{
    "paper_title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC",
    "authors": [
        "Beichen Zhang",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Haodong Duan",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33\\% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code is released at https://github.com/InternLM/ARC-VL."
        },
        {
            "title": "Start",
            "content": "Think Visually, Reason Textually: Vision-Language Synergy in ARC Beichen Zhang1,2, Yuhang Zang2 (cid:66) , Xiaoyi Dong1,2, Yuhang Cao2 Haodong Duan2, Dahua Lin1,2, Jiaqi Wang2,3 (cid:66) 1The Chinese University of Hong Kong 2Shanghai AI Laboratory 3Shanghai Innovation Institute {zhangbeichen,zangyuhang}@pjlab.org.cn Corresponding Authors. (cid:66) 5 2 0 2 6 2 ] . [ 2 3 0 7 5 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Abstract reasoning from minimal examples remains core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from handful of examples, which is key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch SelfCorrection (MSSC), which leverages vision to verify textbased reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code is released at https://github.com/ InternLM/ARC-VL. 1. Introduction The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) [4] has emerged as the leadFigure 1. We propose vision-text co-reasoning in abstract reasoning tasks. It integrates the unique advantages of visual and textual thinking, thereby outperforming uni-modal reasoning. All methods use o4-mini as the base model. ing benchmark for evaluating machine intelligence beyond domain-specific skills. Unlike traditional AI benchmarks that focus on narrow tasks such as natural language processing or image understanding, ARC-AGI measures the ability to learn how to learn: system must identify abstract transformation rules from minimal examples and apply them to entirely new scenarios. This evaluation paradigm mirrors human IQ tests and represents fundamental shift toward assessing artificial general intelligence. Consequently, state-of-the-art models (e.g., GPT-5 [21] and Grok 4 [36]) now highlight ARC-AGI performance as key indicator of their reasoning capabilities. Despite this importance, progress on ARC-AGI remains limited. An intriguing observation is that existing approaches treat ARC-AGI as purely textual task, representing the input-output matrix pairs as nested lists (e.g., [[0,1,2],[3,4,5]]) during both training and inference; see the left part of Fig. 2. This design choice, while computationally convenient, fundamentally contradicts human problemsolving intuition. As shown in the right part of Fig. 2, when humans approach ARC-AGI tasks, they naturally visualize the patterns: color-coded 2D grid immediately reveals spatial relationships like symmetries, rotations, or shape transformations that are tedious to infer from textual coor1 Figure 2. Textual (left) vs. Visual (right) Thinking in the ARC-AGI Task. Previous work treats ARC-AGI as pure text task for training and reasoning, as text allows for precise representation of each element. However, this approach loses the intuitiveness of visual thinking and 2D structural information. In contrast, we organically integrate visual thinking and textual thinking into the ARC-AGI reasoning process, using the complementary strengths of different modalities. dinate descriptions. We argue that this discrepancy between human visual intuition and machine text-centric processing represents critical missed opportunity. However, integrating visual information is not straightforward. Our preliminary experiments reveal counterintuitive paradox: naively rendering ARC-AGI grids as images actually degrades performance compared to textonly baselines. While image-based representations capture global 2D structure effectively, they struggle with precise element-wise operations. For instance, when 20 20 grid is presented as an image, models often fail to reliably identify or manipulate the value at specific position such as (5, 7), conflating it with nearby cells. This highlights fundamental tension: vision excels at recognizing overall spatial patterns, whereas textual encodings naturally provide the discrete precision required for exact rule execution. This observation leads to our core insight: visual and textual modalities have fundamentally complementary strengths in different stages of abstract reasoning. To validate this hypothesis, we systematically decompose ARC-AGI into two sub-tasks: rule summarization (extracting transformation patterns from examples) and rule application (applying the extracted rule to new inputs). We empirically evaluate each modalitys performance on these sub-tasks (detailed in Sec. 3.2). Our analysis of the OpenAI o4-mini model reveals striking differences: vision excels at rule summarization, providing 3.0% improvement through its holistic perception of 2D spatial structures, while text excels at rule application, with vision causing dramatic 20.5% performance drop due to imprecise element-wise manipulation. These findings demonstrate that the question is not whether to use vision or text, but rather when and how to strategically combine them. Guided by our insights, we propose two synergistic strategies that strategically integrate visual and textual modalities throughout the abstract reasoning pipeline: Vision-Language Synergy Reasoning (VLSR) matches each sub-task to its optimal modality. During rule summarization, VLSR visualizes the example input-output matrix pairs as color-coded 2D grids, enabling the model to use holistic spatial perception and efficiently encode global transformation patterns (e.g., all shapes rotate 90 degrees clockwise). During rule application, VLSR switches to textual representation, allowing the model to perform precise element-wise manipulations guided by the extracted rule. This modality-aware decomposition achieves improvements through two mechanisms: (1) divide-andconquer task decomposition reduces individual sub-task complexity, and (2) strategic modality selection exploits each modalitys inherent strengths. Modality-Switch Self-Correction (MSSC) addresses fundamental challenge in intrinsic self-correction: models struggle to identify errors when verifying their own reasoning in the same modality [12, 39]. MSSC breaks this limitation by employing different modalities for forward reasoning and backward verification. After generating candidate output through text-based rule application, MSSC visualizes both the test input and predicted output as images, then uses the visual modalitys strength in pattern consistency verification to check whether the predicted transformation matches the pattern shown in the example images. If inconsistencies are detected, the model receives explicit feedback and performs another round of textual inference. This crossmodal verification enables effective intrinsic self-correction without any external information or ground truth. Extensive experiments demonstrate that our combined approach achieves substantial improvements. When applied to flagship reasoning models such as Gemini-2.5-Pro and o4-mini, it delivers notable improvements of up to 7.25%and 4.5% accuracy respectively on the official ARCAGI evaluation set. On average, our approach yields up to 4.33% improvement over text-only baselines across diverse 2 models (GPT-4o, Gemini-2.5-Pro, o4-mini, Qwen3-VL) and multiple ARC-AGI benchmarks (ARC-AGI, BARC, Re-ARC). Furthermore, our analysis reveals that text-only selfcorrection often fails or degrades performance, while MSSC provides consistent iterative improvements. Our work makes the following contributions: 1) We provide the first systematic study of visual versus textual reasoning in ARC-AGI, identifying four key characteristics of their complementary strengths: holistic vs. independent processing, 2D structure preservation, encoding efficiency, and element-wise precision trade-offs, which is novel perspective for the exploration of visual intelligence. two trainingfree strategies that strategically combine visual and textual modalities to exploit their complementary strengths throughout the reasoning pipeline. 2) We introduce VLSR and MSSC, 3) We demonstrate that our proposed vision-language synergy principle extends naturally to the training paradigm. By fine-tuning separate models for visual rule summarization and textual rule application, our approach achieves 3.5% improvement over text-only fine-tuning on the same training data, enabling small open-source models (Qwen3-8B) to surpass closed-source models like GPT-4o. 2. Related Work ARC-AGI Tasks. ARC-AGI (Abstraction and Reasoning Corpus for Artificial General Intelligence) is benchmark task designed to evaluate the generalization capability of AI Its core objective is to measure whether an AI systems. can abstract rules from minimal number of examples and solve entirely new problems, rather than relying on largescale datasets or pre-trained knowledge. Each task consists of several input-output matrix pairs. The model is required to infer rules from small set of demonstrations and apply these rules to an unseen test input matrix. For humans, ARC-AGI tasks are not difficult, with an accuracy rate of over 97%. However, they remain highly challenging for AI systems and have become one of the most demanding benchmarks currently used to assess whether an AI possesses artificial general intelligence. ARC-AGI Strategies. The Abstraction and Reasoning Corpus (ARC-AGI) task [4] has garnered widespread attention in the academic community since its proposal. Most strategies attempt to enhance ARC-AGI capabilities through training. number of approaches [3, 8, 10, 15 17, 19, 20, 23, 31, 32, 37] generate large number of ARCAGI tasks by permuting and combining predefined transformation rules, and use this synthetic data to fine-tune large language models. Additionally, test-time training [27] is prevalent strategy. These approaches [1, 5, 7, 24, 40, 41] treat the input-output examples provided during testing not only as context to support the models reasoning, but also as data for additional fine-tuning of model before it generates answers. Other strategies attempt to introduce additional hints during the inference process to enhance the models performance on the ARC-AGI task. common type of strategy [18, 25, 26, 30, 34] leverages the characteristics of ARC-AGI tasks by predefining some of the potential transformation rules and then provides prompts and guidance to the model during the reasoning process. Another common strategy [2, 9, 14, 24, 28, 29] involves utilizing the models memory: summarizing observations and insights from previous problems. This strategy consolidates previous attempts into referenceable concepts to guide the model in new reasoning tasks. However, previous strategies have treated ARC-AGI as pure text task for both training and reasoning, neglecting image representations that demonstrate the rules more intuitively. Therefore, the core work of this paper is to integrate visual information into ARC-AGI and analyze the differences between visual thinking and textual thinking in abstract reasoning. Aiding Reasoning with Images. Leveraging visual modalities to augment reasoning has emerged as pivotal research direction. Visual Sketchpad [11] enhances geometric problem-solving by rendering diagrams and introducing auxiliary constructions. ViLaSR [35] further proposes Drawing to Reason in Space paradigm that permits the model to visualize its intermediate spatial hypotheses, achieving measurable improvements on spatial-reasoning benchmarks like maze solving. Owing to its immediacy and spatial expressiveness, visual information is becoming an indispensable component of reasoning pipelines. 3. Method The core of this work lies in integrating visual thinking into ARC-AGI tasks to use the complementary strengths of different modalities. While existing approaches treat ARCAGI as pure text task, we argue that visual and textual modalities excel at different aspects of the reasoning process. Visual thinking provides global perception and 2D structural understanding that text-centric reasoning lacks, while textual thinking enables precise element-wise processing and manipulation. To this end, we propose two core methods for ARC-AGI  (Fig. 3)  : a) Vision-Language Synergy Reasoning (VLSR) and b) Modality-Switch Self-Correction (MSSC). VLSR summarizes rules from exemplar matrices in the visual modality and then applies the inferred rule to the test input in the text modality. MSSC verifies the textual output by re-encoding it as an image to assess visual consistency with the examples; if inconsistent, it triggers another textual pass with targeted feedback. We first formalize the ARC-AGI task and establish notation in Sec. 3.1. Before presenting our method design, we conduct systematic empirical analysis in Sec. 3.2 to under3 Figure 3. Overview of our method. a) Vision-Language Synergy Reasoning decomposes ARC-AGI into two subtasks: Rulesummarization and Rule-application. The former visualizes the provided example matrices as images, using global visual perception and 2D structure to summarize the rule. The latter requires element-wise processing, so rule-application is carried out in the textual modality. b) Modality-Switch Self-Correction visualizes the output matrix to judge rule consistency. The results are fed back to implement the self-correction strategy if necessary. As visual information is more informative in rule verification, the model can repeatedly refine its answers without relying on additional inputs. stand the respective strengths of visual and textual modalities in ARC-AGI reasoning. This provides the empirical foundation for our design choices. Then we present the detailed design of VLSR in Sec. 3.3 and MSSC in Sec. 3.4. 3.1. Problem Setup and Notation to )}, ), . . . , (minput , moutput Each ARC-AGI provided with example is , moutput ), (minput 1 2 Task Formulation. task is associated with specific transformation rule that maps output matrices moutput. input matrices minput The model pairs: {(minput , moutput 1 2 where each matrix ZHW has H, 30 and cell values in {0, 1, . . . , 9}. The goal is to infer the underlying rule from these examples and apply it to new test input minput to generate the corresponding output moutput test Notation. We denote matrices in different modalities as 2) = follows: 1) m: (m): textual representation of the matrix (e.g., nested list: [[0,1,2],[3,4,5],[2,3,5]]). 3) = V(m): visual representation of the matrix, where each cell value (0-9) is mapped to distinct color in grid layout. Details of the visualization process are provided in Sec. in the supplementary materials. in any modality. the matrix, test . The symbols and denote the text and visual transformation functions, respectively. Both transformations are invertible: 1(t) = and 1(i) = m, allowing seamless conversion between modalities. Previous Approaches. Prior works process ARC-AGI tasks purely in the textual modality. Besides, the transformation rule is not explicitly derived, either implicitly outputting it in the chain-of-thought, or skipping it directly to output the transformed matrix. 4 Table 1. Quantitative experiments on using different modalities in rule summarization and application phase. Models Baseline Rule-Sum. Rule-App. text vision text vision GPT-4o Gemini-2.5 o4-mini 8.25 35.0 42.25 10.5 35.25 42.5 13.5 38.75 45.5 13.5 38.75 45. 6.25 23.75 25.0 Formally, previous methods optimize function to directly predict the output matrix in textual form: tpred = (tinput 1 , toutput , , tinput , toutput , tinput test ). (1) This has inherent limitations: (1) textual representation loses crucial 2D structural information, and (2) conflating rule summarization and application in single step prevents the model from fully leveraging modality-specific strengths. 3.2. Comparative Analysis of Vision and Text To inform our method design, we conduct systematic empirical analysis of visual versus textual modalities in ARCAGI reasoning. We decompose the task into two subtasks: rule summarization and rule application, and evaluate each modalitys performance on these subtasks separately across multiple state-of-the-art models. 3.2.1. Quantitative Comparison Experimental Design. To isolate the effect of modality choice on each sub-task, we conduct experiments where we vary the modality used in either the rule summarization or rule application phase while keeping other factors constant. Rule Summarization Phase. We input the provided example matrix pairs {(minput )} , moutput ), . . . , (minput , moutput 1 , ioutput , toutput )} or visual representations {(iinput into the same model using either textual representations {(tinput )}, and require the model to summarize the transformation rule r. To fairly compare the quality of the extracted rules, we then uniformly apply both rules in the textual modality and compare the final accuracy on the test matrix. Rule Application Phase. For this phase, we adopt the same high-quality rule derived from the visual modality (as validated by the previous experiment). The core comparison is whether it is better to represent both the example matrices and the test input matrix as images or as text when applying the rule to generate the output. Baseline. We also include the standard text-only approach commonly used in previous studies, which relies solely on the textual modality without explicit rule extraction, directly outputting the transformed test matrix. Results. As shown in Tab. 1, using the visual modality for rule summarization provides clear advantage, yielding an average improvement of 3.2% across models (e.g., 40.75% vs. 37.25% for Gemini-2.5). However, this effect is reversed in the rule application phase. When applying rules using visual representations instead of textual ones, performance drops dramatically by an average of 15.0% (e.g., from 40.75% to 23.75% for Gemini-2.5). Our results clearly demonstrate that visual and textual modalities have complementary strengths: vision excels at global pattern recognition needed for rule summarization, while text excels at precise element-wise manipulation needed for rule application. 3.2.2. Qualitative Analysis: Understanding the Why To understand the underlying reasons behind our quantitative findings, we conduct an in-depth qualitative analysis of model outputs across both modalities. We identify four key characteristics that explain the performance differences: 1) Visual thinking adopts holistic perception; textual thinking processes elements independently. Visual reasoning demonstrates systematic bias toward encoding relational properties anchored on contiguous spatial structures, such as central blocks, checkerboard patterns, or connected components. In contrast, textual reasoning relies more on type-level statistics (e.g., frequency counts) to identify patterns, treating each element more independently. For rule summarization that requires identifying global spatial relationships, the holistic nature of visual thinking aligns better with task requirements and mirrors human cognitive strategies. 2) Visual thinking preserves 2D structure; textual thinking may lose spatial information. In matrix, two vertically adjacent elements in the same column are perceptually contiguous in visual representation, yet may be separated by dozens of tokens in textual representation (e.g., [[0,1,2],[3],4,5]] where elements in different rows are far apart in the token sequence). Consequently, rules extracted 5 through textual reasoning tend to lack 2D structural characteristics and perform poorly when tasks require capturing inter-row or diagonal regularities. Moreover, when both input and output matrices are transposed, rules discovered via visual representation remain essentially invariant, while those derived from textual representation may be significantly affected by the change in token ordering. 3) Visual representation is more efficient for large matrices. Textual representation requires separately encoding each matrix element along with delimiters (brackets, commas). For large matrices (e.g., 30 30), textual representation may require thousands of tokens. In contrast, visual representation encodes even complex matrices using single image with only few hundred vision tokens. This enables faster reasoning without compromising or even enhancing the reasoning quality. Recent work DeepSeekOCR [33] also explores visual-based approaches for compressing complex document understanding. 4) Visual reasoning lacks fine-grained element-wise precision. Images represent matrices as an integrated whole rather than encoding individual elements separately. While this provides global perspective advantageous for rule summarization, it becomes inadequate when element-wise processing is required during rule application. We observe that when images represent large matrices, models may make errors in basic element localization and value identification (e.g., confusing the value at position (5, 7) with nearby cell). This limitation leads to performance degradation when visual thinking is adopted for the rule application, which demands precise per-element manipulation. Summary. Our analysis reveals that visual and textual modalities have fundamentally different information processing characteristics. Visual thinking provides global perception, preserves 2D spatial structure, and offers efficient representation, making it ideal for rule summarization. Textual thinking enables precise element-wise access and manipulation, making it essential for rule application. These complementary strengths motivate our method design in the following subsections. 3.3. Vision-Language Synergy Reasoning Motivation. We now translate the empirical insights from Sec. 3.2 into our pipeline design. Rather than forcing single modality throughout the reasoning process, we introduce Vision-Language Synergy Reasoning that strategically switches modalities between subtasks. Our key design principle is to route each sub-task to its optimal modality: visual reasoning for rule summarization (exploiting global pattern recognition and 2D structural understanding) and textual reasoning for rule application (exploiting precise elementwise manipulation). This modality-aware decomposition allows us to harness the full potential of LVLMs. Method Overview. Fig. 3 (a) presents our Visionrpred = vision Language Synergy Reasoning (VLSR) pipeline, which strategically employs appropriate modalities at different stages. The complete pipeline consists of two phases: Phase 1: Visual Rule Summarization. We convert all example matrix pairs into visual representations. Each matrix is visualized as an image = V(m), where is visualization function that maps each cell value to distinct color in grid layout. The LVLM then analyzes these visualized examples to derive an explicit transformation rule: , . . . , iinput , ioutput 1 , ioutput ), (2) where vision sum represents the LVLM operating in visual mode with rule summarization prompt. The derived rule rpred in Eq. (2) is expressed in natural language, describing the transformation pattern (e.g., rotate each connected component 90 degrees clockwise). Phase 2: Textual Rule Application. Given the summarized rule rpred, we apply it to the test input using textual representations. All matrices are converted to text format, and the model performs element-wise reasoning: sum (iinput , ioutput , iinput 2 1 1 , toutput 1 test ), (3) , . . . , tinput , toutput , tinput app (rpred, tinput tpred = text where text app represents the same LVLM operating in text mode with rule application prompt. Note that vision sum and text app are the same base model; only the input modality and prompting strategy differ. Key Advantages. Compared with Eq. (1), our approach (1) Task decomposition reduces offers two key benefits: the complexity of individual subtasks through divide-andconquer strategy, and (2) Modality matching ensures each sub-task uses the optimal modality: visual for global pattern recognition, textual for precise manipulation. As we will demonstrate in Sec. 4, our strategy yields consistent improvements across different models and benchmarks. 3.4. Modality-Switch Self-Correction Motivation. Self-correction is cornerstone of human intelligence, yet it remains challenging for LLMs. The fundamental paradox is: if model can identify and fix its own errors, why not generate the correct answer initially? Existing works [12, 39] have shown that intrinsic self-correction (without external feedback) is difficult because models struggle to distinguish correct from incorrect outputs when using the same reasoning modality. We propose Modality-Switch Self-Correction (MSSC), which achieves effective intrinsic self-correction by employing different modalities for reasoning and verification. The key insight is that visual and textual modalities have complementary verification capabilities: while text excels at forward rule application, vision excels at pattern consistency verification. By switching modalities, the model gains fresh perspective that enables it to identify errors that are imperceptible in the original modality. 6 Method Design. Fig. 3 (b) shows the pipeline. After obtaining candidate output tpred from textual rule application in Eq. (3), we perform the following iterative refinement: Step 1: Visualization. Convert the test input-output pair to visual form. Since the predicted output tpred from Eq. (3) is in textual format (nested list), we first parse it back to matrix form mpred, then apply the visualization function V: test = V(tinput iinput test ), ipred = V(tpred), (4) Step 2: Visual Consistency Verification. Present the visualized test pair with the examples to the LVLM as critic: 1 , ioutput 1 , . . . , iinput critic (iinput , iinput , ioutput sconsistent = vision test , ipred), (5) critic assesses whether the test pair (iinput where vision test , ipred) follows the same transformation pattern as the examples. The output sconsistent {yes, no} indicates pattern consistency. Step 3: Iterative Refinement. If sconsistent = no, the model receives feedback about the inconsistency and performs another round of textual inference with error awareness: tpred text inf (rpred, tinput , toutput 1 , . . . , tinput test , feedbackprev), (6) where feedbackprev contains information about the previous attempt. This process repeats until consistency is achieved or the iteration limit is reached (we use Nmax = 3). Key Advantages. MSSC provides two critical benefits: (1) Fresh perspective: switching to visual verification breaks the models confirmation bias that occurs when checking its own textual reasoning, and (2) No external information needed: unlike traditional self-correction that requires ground truth or external critics, MSSC uses the models own multimodal capabilities. As we show in Sec. 4, text-only self-correction often fails or even degrades performance, while MSSC achieves consistent improvements. 4. Experiments 4.1. Experimental setup Models. We evaluate both open-source and closed-source models. For open-source models, we use Qwen3-VL-235BA22B-Instruct [38]. For closed-source models, we use GPT-4o [13], Gemini-2.5-pro-thinking-8192 [6], and o4mini [22]. Benchmarks. We evaluate on three ARC-AGI benchmarks. (1) the official 400-task ARC-AGI [4] evaluation set, (2) 100 randomly sampled tasks from Re-ARC [10] and (3) 100 randomly sampled tasks from BARC [17]. For Re-ARC and BARC, each sampled task contains four input-output pairs: three examples and one test instance. Implementation Details. We report Pass@1 accuracy across all experiments at temperature 0.7. Our prompts are provided in Sec. in the supplementary materials. Table 2. Visual-Language co-reasoning outperforms singlemodality reasoning. Both Vision-Language Synergy Reasoning (VLSR) and Modality-Switch Self-Correction (MSSC) improve over text-only baseline reasoning across models and benchmarks. The combination of both strategies yields the largest gains. Models ARC-AGI BARC-100 Re-ARC GPT-4o +VLSR +MSSC +both (Ours) Gemini-2.5-Pro +VLSR +MSSC +both (Ours) o4-mini +VLSR +MSSC +both (Ours) Qwen3-VL-235B +VLSR +MSSC +both (Ours) 8.25 13.5 12.0 14.5 35.0 38.75 36.5 42.25 42.25 45.5 44.75 46.75 20.25 22.0 21.75 22. 28.0 32.0 30.0 33.0 56.0 58.0 57.0 60.0 59.0 64.0 62.0 65.0 52.0 52.0 54.0 54.0 10.0 13.0 14.0 16.0 30.0 32.0 30.0 33. 36.0 38.0 38.0 39.0 20.0 21.0 21.0 23.0 Table 3. Comparison with training-free inference-time methods. Our All methods use o4-mini as the base model. VLSR+MSSC approach outperforms memory-augmented textonly strategies that retrieve past problem-solving experiences. Method ARC-AGI ARC-AGI-100 Re-ARC Direct Reason Cheatsheet ArcMemo-PS Ours 40.5 38.5 45.25 46.75 41.0 41.0 45.0 46.0 36.0 34.0 39.0 39.0 4.2. Main Results Overall Performance. Tab. 2 shows that both VLSR and MSSC consistently improve performance across all models and benchmarks. VLSR, which uses visual reasoning for rule summarization, improves baseline text-only reasoning by an average of 3.02%. MSSC, which uses visual verification for self-correction, provides an additional 1.82% improvement on average. Combining both strategies yields the largest gains, improving baseline performance by up to 6.25% for GPT-4o and 7.25% for Gemini-2.5-Pro on ARCAGI. These consistent improvements across diverse models demonstrate that visual information provides complementary benefits for both the rule summarization and the iterative refinement stages of abstract reasoning. Comparison with Training-Free Reasoning Methods. We compare against two recent training-free strategies that enhance ARC-AGI reasoning at inference time: 1) DyvisionFigure 4. Qualitative comparison of text-only vs. language synergy reasoning on GPT-4o. Text-only reasoning processes elements without spatial context, leading to an incorrect rule. Vision-language synergy reasoning uses global 2D perception in the rule-summarization phase to identify the correct spatial pattern (retain large connected color blocks). Iterative self-correction comparison. We apply Table 4. Text-Only Self-Correction (TOSC) and Modality-Switch SelfCorrection (MSSC) for up to three rounds (R1-R3) without external feedback. MSSC achieves consistent iterative improvement while TOSC stagnates or degrades. Models Base GPT-4o Gemini o4-mini 8.25 35.0 42.25 TOSC R2 8.0 36.0 42.0 R1 8.25 34.25 42.5 MSSC R1 R2 R3 8.75 35.75 43.25 10.25 35.75 43.5 11.5 36.25 44. 12.0 36.5 44.75 namic Cheatsheet [28] stores strategies and findings from past problem-solving processes as memory and includes 2) ArcMemo-PS them in prompts for new problems. [9] builds concept-level external memory through program synthesis and extracts reusable modular abstract concepts, and selectively retrieves relevant concepts during reasoning. Both methods are text-centric. Using o4-mini as the base model, we evaluate all methods on three test sets: the full ARC-AGI-400, the ARCAGI-100 subset from ArcMemo [9], and the Re-ARC test set we divided. As shown in Tab. 3, our method achieves the highest accuracy across all three benchmarks, outperforming the strongest baseline ArcMemo-PS by 1.5% on ARCAGI. Notably, while ArcMemo-PS and Cheatsheet use retrieved past experiences for text-only reasoning, they cannot access the global 2D structure and spatial pattern information that visual representations provide. Our results suggest that visual information offers complementary benefits that text-based memory retrieval alone cannot capture. 4.3. Analysis Modality-Switch vs. Text-Only Self-Correction Tab. 4 compares traditional text-only self-correction (TOSC) with our Modality-Switch Self-Correction (MSSC) across three iterative rounds. TOSC shows minimal improvement and even degrades performance in some iterations. For GPT4o, TOSC improves by only 0.5 points (8.258.75) across Table 5. Experiments on fine-tuning Results. Vision-Language Synergy Fine-tuning outperforms both text-only fine-tuning (using the same training tasks) and several closed-source and open-source baseline models with much larger parameter sizes. Models ARC-AGI BARC-100 Re-ARC Text-only Reasoning Baseline GPT-4o Gemini-2.5-Pro o4-mini Qwen3-235B-A22B Qwen3-8B +Text-only FT Improvement 8.25 35.75 42.25 20.25 3.25 9.75 +6. 28.0 56.0 59.0 52.0 13.0 37.0 +24.0 Vision-Language Synergy Reasoning Qwen3-VL-8B + Qwen3-8B +VL Synergy FT Improvement 3.5 13.25 +9.75 13.0 43.0 +30.0 10.0 30.0 36.0 20.0 2.0 7.0 +5. 3.0 9.0 +6.0 three rounds, with Round 2 degrading to 8.0. In contrast, MSSC achieves consistent monotonic gains at each round. For GPT-4o, MSSC improves from 8.25 to 10.25 (+2.0) in Round 1, then to 11.5 (+1.25) in Round 2, and finally to 12.0 (+0.5) in Round 3. Similar trends hold for Gemini (+1.5 total) and o4-mini (+2.5 total). We attribute this difference to modality switching: verifying textual outputs with visual representations provides fresh perspective that helps models identify spatial inconsistencies imperceptible when reasoning solely in text. When TOSC uses the same textual modality for both generation and verification, the model exhibits confirmation bias and cannot effectively spot its own errors. By contrast, MSSCs visual verification stage detects pattern violations (e.g., missing symmetry, incorrect spatial relationships) that the textual reasoning stage overlooked, enabling genuine iterative improvement without external feedback. Qualitative Analysis. Fig. 4 presents how visual reasoning corrects systematic errors made by text-only reasoning on GPT-4o. When presented with textual matrix representations, GPT-4o incorrectly summarizes the transformation rule as select the number with the highest count. This frequency-based heuristic fails because the task actually requires identifying large connected spatial structures. With visualized matrices, the model correctly identifies the rule as retain large connected color blocks by using global 2D perception to recognize spatial contiguity patterns. Results show that visual representation enables holistic pattern recognition. Additional examples are provided in Sec. in supplementary materials. 4.4. Extension to Model Fine-tuning Background: Open-source models typically underperform on ARC-AGI due to limited capabilities on abstract reasoning tasks. Prior work [1, 7, 10, 17] constructs large-scale synthetic training data to fine-tune open-source models, but treats ARC-AGI as pure text task without using visual information. We investigate whether vision-language synergy during fine-tuning can improve open-source model performance beyond text-only fine-tuning. Method. We apply the same VLSR task decomposition from Sec. 3.3 during training: vision-language model (Qwen3-VL-8B-Instruct) for visual rule summarization and text-only model (Qwen3-8B) for textual rule application. This decouples the two subtasks, allowing for specialized training of each component. Training Setup. Training data comes from ARC-Heavy200k [17], which provides synthetic ARC-AGI tasks with ground-truth rules. We use approximately 200k training tasks (excluding held-out 100-task test set), with each task split into three example pairs and one test sample. ARCHeavy-200k provides explicit rules for each task, enabling us to train the rule summarization and rule application modules separately. As text-only baseline, we also fine-tune Qwen3-8B on the same 200k tasks using textual matrix representations from ARC-Heavy-200k. Results. As shown in Tab. 5, VL synergy fine-tuning achieves 13.25% on ARC-AGI, outperforming text-centric fine-tuning (9.75%) by 3.5% and surpassing the closedsource baseline GPT-4o (8.25%) by 5.0%. Compared to the Qwen3-8B baseline before fine-tuning, text-only finetuning improves performance by 6.5% while VL synergy fine-tuning improves by 9.75%, which demonstrates the advantage of incorporating visual information during training. Similar trends hold on BARC-100 and Re-ARC. We attribute the advantage of VL synergy fine-tuning to two factors: (1) task decomposition reduces training complexity by separating rule extraction from rule application, and (2) visual information provides global 2D structural cues (e.g., spatial contiguity, symmetry patterns) that are difficult to learn from sequential textual representations alone. 5. Conclusion This paper incorporates visual information into abstract reasoning tasks, which have traditionally been treated as purely textual. By proposing the Vision-Language Synergy Reasoning (VLSR) framework and the Modality-Switch SelfCorrection (MSSC) mechanism, the 2D intuitiveness inherent in the visual modality and the precision of element-wise representation offered by the textual modality can be effectively integrated. Leveraging the distinct advantages of both modalities, our method achieves an average performance improvement of 4.3% compared to text-only reasoning approaches across multiple abstract reasoning tasks and different base models. Furthermore, it outperforms other textcentric training-free strategies, thereby demonstrating the unique value of incorporating visual information. Additionally, this inference strategy can be extended to model fine8 tuning scenarios. The vision-language synergy fine-tuning strategy can yield 3.5% improvement over text-only finetuning methods and outperform several open-source and closed-source models with much larger parameter sizes."
        },
        {
            "title": "References",
            "content": "[1] Ekin Akyurek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising effectiveness of arXiv e-prints, test-time training for abstract reasoning. pages arXiv2411, 2024. 3, 8 [2] Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone, David Zhang, Michael Defferrard, and Taco Cohen. Codeit: Self-improving language models with prioritized hindsight replay. arXiv preprint arXiv:2402.04858, 2024. 3 [3] Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, et al. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles. arXiv preprint arXiv:2505.19914, 2025. 3 [4] Francois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. 1, 3, 6 [5] J. Cole. Community interview jack cole lab42 lab42.global., 2024. 3 [6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6 [7] Daniel Franzen, Jan Disselhoff, and David Hartmann. The llm architect: Solving arc-agi is matter of perspective, 2024. 3, 8 [8] Daniel Franzen, Jan Disselhoff, and David Hartmann. Product of experts with llms: Boosting performance on arc is matter of perspective. arXiv preprint arXiv:2505.07859, 2025. 3 [9] Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Yichi Yang, Zhijian Liu, Zhiting Hu, and Lianhui Qin. Arcmemo: Abstract reasoning composition with lifelong llm memory. arXiv preprint arXiv:2509.04439, 2025. 3, [10] Michael Hodel. Addressing the abstraction and reasoning corpus via procedural example generation. arXiv preprint arXiv:2404.07353, 2024. 3, 6, 8 [11] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. 3 [12] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023. 2, 6 [13] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6 [14] Alexia Jolicoeur-Martineau. Less is more: Recursive reasoning with tiny networks. arXiv preprint arXiv:2510.04871, 2025. 3 [15] Hosung Lee, Sejin Kim, Seungpil Lee, Sanha Hwang, Jihwan Lee, Byung-Jun Lee, and Sundong Kim. Arcle: The abstraction and reasoning corpus learning environment for reinforcement learning. arXiv preprint arXiv:2407.20806, 2024. 3 [16] Solim LeGris, Wai Keen Vong, Brenden Lake, and Todd Gureckis. H-arc: robust estimate of human performance on the abstraction and reasoning corpus benchmark. arXiv preprint arXiv:2409.01374, 2024. [17] Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, et al. Combining induction and transduction for abstract reasoning. arXiv preprint arXiv:2411.02272, 2024. 3, 6, 8 [18] Matthew Macfarlane and Clement Bonnet. Searching latent program spaces. arXiv preprint arXiv:2411.08706, 2024. 3 [19] Michael Moffitt. Arc-gen: mimetic procedural benchmark generator for the abstraction and reasoning corpus. arXiv preprint arXiv:2511.00162, 2025. 3 [20] Arseny Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. arXiv preprint arXiv:2305.07141, 2023. 3 [21] OpenAI. Gpt-5 system card, 2025. 1 [22] OpenAI. Openai o3 and o4-mini system card, 2025. 6 [23] Jaehyun Park, Jaegyun Im, Sanha Hwang, Mintaek Lim, Sabina Ualibekova, Sejin Kim, and Sundong Kim. Unraveling the arc puzzle: Mimicking human solutions arXiv preprint with object-centric decision transformer. arXiv:2306.08204, 2023. 3 [24] Julien Pourcel, Cedric Colas, and Pierre-Yves Oudeyer. Self-improving language models for evolutionary program arXiv preprint synthesis: case study on arc-agi. arXiv:2507.14172, 2025. [25] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Phenomenal yet puzYejin Choi, Nouha Dziri, et al. zling: Testing inductive reasoning capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559, 2023. 3 [26] Kartik Singhal and Gautam Shroff. Conceptsearch: Towards efficient program search using llms for abstraction and reasoning corpus (arc). In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2050620513, 2025. 3 [27] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Test-time training with selfEfros, and Moritz Hardt. supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229 9248. PMLR, 2020. 3 [28] Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet: TestarXiv preprint time learning with adaptive memory. arXiv:2504.07952, 2025. 3, 7 9 [29] Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi arXiv preprint Yadkori. Hierarchical reasoning model. arXiv:2506.21734, 2025. 3 [30] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. Hypothesis search: Inductive reasoning with language models. arXiv preprint arXiv:2309.05660, 2023. 3 [31] Yile Wang and Hui Huang. Improving abstract reasoning ability of large language models through mixture programbased data synthesis. In China National Conference on Chinese Computational Linguistics, pages 208228. Springer, 2025. 3 [32] Yang Wang and Zhejun Zhao. Advancing abstract reasoning in artificial general intelligence with hybrid multicomponent architecture. In 2024 4th International Symposium on Artificial Intelligence and Intelligent Manufacturing (AIIM), pages 867871. IEEE, 2024. 3 [33] Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression, 2025. 5 [34] Johan S. Wind. Dsl solution to the arc challenge, 2020. 3 [35] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 3 [36] xAI Team. Grok 4, 2025. 1 [37] Kevin Xu and Risto Miikkulainen. Neural cellular automata for arc-agi. arXiv preprint arXiv:2506.15746, 2025. 3 [38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [39] Qingjie Zhang, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang, Ke Xu, Hewu Li, Yan Liu, and Han Qiu. Understanding the dark side of llms intrinsic selfcorrection. arXiv preprint arXiv:2412.14959, 2024. 2, 6 [40] Shaoting Zhu, Shuangyue Geng, and Un Lok Chen. T5-arc: Test-time training for transductive transformer models in arcagi challenge. 3 [41] Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. 3 10 Think Visually, Reason Textually: Vision-Language Synergy in ARC"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Prompts Prompt for Text-only Reasoning in ARC-AGI: will provide you with several input and output matrices. You need to find the matrix-changing rule from it and apply it to the new input. Put the output matrix within boxed{}. Example Input 1: [[0, 1, 0, ...], [0, 1, 1, ...], ...] Example Output 1: [[1, 1, 0, ...], [1, 1, 1, ...], ...] Example Input 2: [[0, 2, 4, ...], [0, 2, 2, ...], ...] Example Output 2: [[2, 2, 4, ...], [2, 2, 2, ...], ...] ... Example Input n: [[0, 4, 0, ...], [0, 6, 1, ...], ...] Example Output n: [[4, 4, 0, ...], [6, 6, 1, ...], ...] New Input: [[0, 2, 0, ...], [0, 5, 3, ...], ...] Prompt for Vision-centric Rule Summarization: will now provide you with several input and output images about 2D grids. You need to summarize the grid-changing rule from it. Output the rule you learned within boxed{}. Example Input 1: <Input Image 1> Example Output 1: <Output Image 1> Example Input 2: <Input Image 2> Example Output 2: <Output Image 2> ... Example Input n: <Input Image n> Example Output n: <Output Image n> Prompt for Text-centric Rule Application: will provide you with several input and output matrices. You need to find the matrix-changing rule from it and apply it to the new input. Put the output matrix within boxed{}. Here is possible rule for your reference. Rule: The rule involves removing the colored cross ... Note that the rule is described in color and each color represents value in the matrix: [0:black; 1:blue; 2:red; 3:green; 4:yellow; 5:grey; 6:pink; 7:orange; 8:light blue; 9:brown]. You need to first check the correctness of the rule based on the examples. If the rule is correct, apply it to the new input. Otherwise, summarize new rule and apply it to the new input. Example Input 1: [[0, 1, 0, ...], [0, 1, 1, ...], ...] Example Output 1: [[1, 1, 0, ...], [1, 1, 1, ...], ...] Example Input 2: [[0, 2, 4, ...], [0, 2, 2, ...], ...] Example Output 2: [[2, 2, 4, ...], [2, 2, 2, ...], ...] ... Example Input n: [[0, 4, 0, ...], [0, 6, 1, ...], ...] 1 Example Output n: [[4, 4, 0, ...], [6, 6, 1, ...], ...] New Input: [[0, 2, 0, ...], [0, 5, 3, ...], ...] Prompt for Vision-centric Consistency Verification: will now provide you with several input and output example images, which follows specific changing rule. Then, will give you another input and output pair, determine whether the new pair also follows the same changing rule. Add your final judgment at the end of your replay: boxed{True} or boxed{False}. Example Input 1: <Input Image 1> Example Output 1: <Output Image 1> Example Input 2: <Input Image 2> Example Output 2: <Output Image 2> ... Example Input n: <Input Image n> Example Output n: <Output Image n> New Input: <New Input Image> New Output: <Output Image Pred> B. Matrix-to-Image Visualization We visualize the input-output matrices into color-coded 2D grids to provide 2D spatial information and global view. The detailed visualization process is listed as follows. First, each value is mapped to distinct color. The color map is: [ 0: black; 1: blue; 2: red; 3: green; 4: yellow; 5: grey; 6: pink; 7: orange; 8: light blue; 9: brown ] Furthermore, between the two elements (small colored squares), we add white dividing lines to more clearly indicate the specific number and the structure of the elements contained within block. C. Qualitative Examples We conduct an in-depth analysis of the specific outputs of different models (GPT-4o, Gemini-2.5-Pro-thinking-8192, o4-mini) when employing visual thinking versus textual thinking in the ARC-AGI task. Visual thinking demonstrates numerous unique advantages, such as the integration of 2D structural information  (Fig. 6)  , global perspective(Fig. 5, Fig. 7), and long-range perception capabilities  (Fig. 8)  . Therefore, using visual information during the global rule summarization phase enables more effective extraction of the underlying rules. Figure 5. Visual reasoning possesses global perspective, enabling it to better capture the most critical feature (the colored cross) in the entire image and subsequently summarize the correct rule the underlying rules. Base model is both gpt-4o. 2 Figure 6. Visual reasoning possesses 2D information and can flexibly summarize rules in block-by-block manner, whereas text reasoning tends to adopt row-by-row processing approach, thus failing to derive the correct rules. Base model is both o4-mini. 3 Figure 7. Visual thinking tends to adopt global perspective and thus processes information based on internal and external paths; in contrast, textual thinking focuses more on local information and processes individual elements with reference to their 8-neighbor context. Base model is both Gemini-2.5-Pro-thinking-8192. Figure 8. Visual thinking possesses superior long-range correlation capabilities and can better capture detailed features (the 2-color pairs and the re-color strategy). Base model is both Gemini-2.5-Pro-thinking-8192."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "The Chinese University of Hong Kong"
    ]
}