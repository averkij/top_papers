{
    "paper_title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View",
    "authors": [
        "Fabian Gröger",
        "Shuo Wen",
        "Maria Brbić"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships."
        },
        {
            "title": "Start",
            "content": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View Fabian Groger * 1 2 3 Shuo Wen * 1 Maria Brbic 1 6 2 0 2 6 1 ] . [ 1 6 8 4 4 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The Platonic Representation Hypothesis suggests that representations from neural networks are converging to common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce permutation-based null-calibration framework that transforms any representational similarity metric into calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships. 1. Introduction Quantifying the similarity between neural network representations is central to understanding the geometry of learned representation spaces (Raghu et al., 2017; Nguyen et al., 2021), guiding transfer learning decisions (Kornblith et al., 2019; Neyshabur et al., 2020), and relating artificial representations to neural measurements in neuroscience (Schrimpf et al., 2018). The Platonic Representation Hypothesis (Huh et al., 2024) posits that as neural networks scale, representations across different modalities become increasingly similar, suggesting convergence to shared statistical model of reality. This hypothesis has motivated growing literature that uses representational similarity to Project Page: brbiclab.epfl.ch/aristotelian Code: github.com/mlbio-epfl/aristotelian *Equal contribution 1EPFL 2University of Basel 3HSLU. Correspondence to: Maria Brbic <maria.brbic@epfl.ch>. The Aristotelian Representation Hypothesis Neural networks, trained with different objectives on different data and modalities, are converging to shared local neighborhood relationships. Figure 1. The Aristotelian Representation Hypothesis: Local relations (who is near whom), rather than distances between data points, are preserved across different representation spaces. Representation learning algorithms will converge to shared local neighborhood relationships. study whether scaling produces universal structure across models (Huh et al., 2024; Maniparambil et al., 2024; Tjandrasuwita et al., 2025; Zhu et al., 2026). To measure representational similarity across models, different metrics have been proposed, such as Centered Kernel Alignment (Kornblith et al., 2019), Canonical Correlation Analysis (Weenink, 2003), Representational Similarity Analysis (Kriegeskorte et al., 2008), and mutual k-Nearest Neighbors (Huh et al., 2024). In this work, we identify two pervasive confounders that distort representational similarity measurements. The first is the model width: when the embedding dimension increases relative to the sample size, interaction-matrix-based similarity metrics exhibit systematic positive baseline even when representations are independent. This spurious similarity is general consequence of dimensionality-driven null inflation: the expected similarity under independence does not vanish but instead depends on both the representation dimensionality and the sample size (Figure 2a). As result, wider models can appear more aligned simply because their representations live in higher-dimensional spaces. The second confounder is the model depth. Many analyses do not compare individual layer pairs, because it is unknown where similarity arises (Schrimpf et al., 2018; Huh et al., 1 Revisiting the Platonic Representation Hypothesis: An Aristotelian View resentation Hypothesis still hold once similarity is calibrated? We find that, after calibration, the previously reported convergence in global metrics (Huh et al., 2024; Maniparambil et al., 2024; Tjandrasuwita et al., 2025) largely disappears, suggesting it was driven primarily by width and depth confounders, whereas local neighborhood-based metrics retain significant cross-modal alignment (Figure 2c). However, we also observe that the convergence in local distances is not preserved, suggesting that only local neighborhood relationships are aligned. Motivated by these results, we refine the original Platonic Representation Hypothesis and propose the Aristotelian Representation Hypothesis1: Neural networks, trained with different objectives on different data and modalities, converge to shared local neighborhood relationships (Figure 1). We name it after the Greek philosopher Aristotle, who was student of Plato and, in his Categories, established the principles of relatives (Aristotle, ca. 350 B.C.E). 2. Related work Representational similarity metrics. long line of work compares representation spaces using variety of similarity measures. Canonical Correlation Analysis (CCA) (Hotelling, 1992) and variants such as Singular Vector Canonical Correlation Analysis (SVCCA) (Raghu et al., 2017) and Projection Weighted Canonical Correlation Analysis (PWCCA) (Morcos et al., 2018) compare subspaces up to linear transformations, while Procrustesand shape-based distances compare representations up to restricted alignment classes (Ding et al., 2021; Williams et al., 2021). Centered Kernel Alignment (CKA) (Kornblith et al., 2019) has become dominant tool for comparing deep representations, with kernelized variants extending to nonlinear similarity. Representational Similarity Analysis (RSA) (Kriegeskorte et al., 2008), originating in neuroscience, compares representational dissimilarity matrices rather than feature bases. Neighborhood-based approaches, such as mutual k-Nearest Neighbors (mKNN) (Huh et al., 2024), capture local topological consistency rather than global alignment. However, recent evaluations stress that different metrics encode different invariances and can yield qualitatively different conclusions, motivating more robust reporting practices (Klabunde et al., 2025; Ding et al., 2021; Harvey et al., 2024; Bo et al., 2024). Reliability of representational similarity metrics. In finite-sample, high-dimensional regimes, raw similarity scores can be systematically biased. Recent works (Murphy et al., 2024; Chun et al., 2025) propose debiased CKA, but 1Calling this refinement Aristotelian: it emphasizes learned representations converging on relations among instances (who is near whom) rather than the idea of convergence toward globally matching structure. Figure 2. Null calibration removes width and depth confounders. (a) Width confounder: raw scores exhibit positive null baselines that increase with the ratio of dimension (width) of the spaces and the number of samples; calibration collapses them to zero. (b) Depth confounder: selection-based summaries (max over layers) inflate with search space size; aggregation-aware calibration removes this. (c) After calibration, global metrics lose their convergence trend, while local metrics retain significant alignment. 2024). Instead, they search over all pairs and report summary statistic such as the maximum. Taking maximum over many comparisons inflates the reported score even if there is no similarity, since the expected maximum of independent draws exceeds the mean. This inflation grows with the number of comparisons, so deeper models can appear more aligned simply because more layer pairs are compared (Figure 2b). Together, these confounders undermine the comparative use of representational similarity without calibration. To address these issues, we introduce the null-calibration for representational similarity, general permutation-based framework that transforms any similarity metric into calibrated score with principled null reference, here defined as no relationship (Figure 2). The core idea is to measure how extreme an observed similarity is relative to an empirical null distribution obtained by breaking sample correspondences. For scalar comparisons (i.e., width confounder), we estimate critical threshold from the null distribution and define calibrated score that is zero when the observed similarity falls below this threshold and rescaled to preserve the maximum at one. For selection-based summaries (i.e., depth confounder), we apply aggregation-aware calibration. We compute the null distribution of the same aggregate statistic that is ultimately reported (e.g., the maximum over all layer pairs), thereby calibrating the selection step itself. These observations raise question: Does the Platonic Rep2 Revisiting the Platonic Representation Hypothesis: An Aristotelian View these corrections are metric-specific. For neighborhoodbased metrics, no analogous debiasing methods exist despite distance concentration effects that inflate random k-NN overlap (Beyer et al., 1999; Aggarwal et al., 2001). Other approaches address confounding from input population structure. For instance, Cui et al. (2022) propose regressionstyle deconfounding to remove effects of shared input statistics on RSA/CKA. separate reliability issue arises from layer search, where max or top-k aggregation across many layer pairs introduces multiple-comparison inflation. While resampling-based maxT procedures (Westfall & Young, 1993; Nichols & Holmes, 2002) can calibrate such aggregates, this has not yet been applied in representational similarity studies. Our calibration framework addresses both finite-sample bias and selection inflation in unified, metric-agnostic way. The Platonic Representation Hypothesis. growing body of work examines whether neural networks trained under different conditions converge toward similar representations. The Platonic Representation Hypothesis (Huh et al., 2024) posits that as models scale, their representations increasingly converge across architectures and even across modalities such as vision and language, with convergence reported under both global and local similarity measures. Follow-up work has examined factors influencing these trends, including model size, training duration, and data distribution (Raugel et al., 2025), and has explored analogous convergence effects in broader settings such as video models (Zhu et al., 2026) and comparisons to biological vision (Marcos-Manchon & Fuentemilla, 2025). In this work, we revisit the Platonic Representation Hypothesis using our null-calibration framework that controls for width and depth confounders. 3. Problem setup 3.1. Representation spaces and similarity score Let Rdx and Rdy be two representation spaces, where dx and dy are the respective space dimensions. For set of input samples, let Rndx and Rndy be the corresponding embeddings in and Y. We assume row-wise alignment such that the i-th row of and correspond to paired inputs. We use similarity score s(X , Y) to quantify the agreement between and Y. In practice, we compute it from X, and, by slight abuse of notation, denote it with s(X, Y). We consider generic similarity function s(X, Y) R. Our focus covers three families of metrics: (i) spectral: metrics defined on the spectrum of cross-covariance or Gram matrices (e.g., CKA, CCA), (ii) neighborhood: metrics measuring local topological overlap (e.g., mKNN), and (iii) geometric: second-order isomorphism metrics (e.g., RSA). Section provides definitions of the metrics used in this paper. 3.2. The null hypothesis of independence We claim that similarity score s(X, Y) is uninterpretable without baseline. To provide this baseline, we define the null hypothesis H0 as the absence of relationship between and beyond their marginal statistics. We operationalize H0 via permutation group Πn acting on sample indices: draw π Unif(Πn) independently of (X, Y) and evaluate s(X, π(Y)), where π(Y) permutes the rows of Y. Assumption 3.1 (Exchangeability under the null). Under H0, the joint distribution of paired samples is invariant to relabeling of correspondences. For any permutation π Πn, PH0 (X, Y) = PH0 (X, π(Y)). This assumption implies that if no true relationship exists, the observed pairing is statistically indistinguishable from random shuffling of the data. It allows us to construct an empirical null distribution by holding fixed and shuffling the rows of Y. 3.3. Baseline problem: non-zero null expectations Ideally, under H0, we desire Eπ[s(X, π(Y))] 0. However, for commonly used raw or biased estimators, the expected similarity under the null is not zero, µ0(n, dx, dy) := Eπ[s(X, π(Y))]. (1) This baseline µ0 is metricand preprocessing-dependent and can deviate from zero in finite samples. It also varies with sample size and dimension, thus acting as confounding variable in comparative studies. 4. Theoretical motivation: spurious alignment We motivate and formalize why raw representational similarity metrics fail in cross-scale model comparisons. We identify two distinct sources of confounding: (i) the width confounder driven by representation dimension, and (ii) the depth confounder driven by the number of layers considered when comparing models. 4.1. The width confounder Many spectral-family similarity metrics, e.g., linear/kernel CKA, the RV coefficient, and CCA-based scores (CCA/SVCCA/PWCCA), can be written as functionals of an interaction operator constructed from two representations. One such operator is the (normalized) cross-covariance (cid:101)C = 1 1 Yc Rdxdy , (2) where Xc and Yc denote row-centered representations (Section C.1). Revisiting the Platonic Representation Hypothesis: An Aristotelian View common but misleading intuition is that if and are independent, then (cid:101)C 0 and therefore spectral aggregates should be near zero. In high dimension this fails: the null interaction energy is typically non-zero. Proposition 4.1 (Non-vanishing null interaction energy). Assume the rows are i.i.d. with E[xi] = E[yi] = 0, Cov(xi) = Idx , Cov(yi) = Idy , and xi and yi are independent. Then EH0 (cid:104) (cid:101)C2 (cid:105) = dxdy 1 . (3) Proof. See Section D.4. Since CKA is scaled by the normalized self-similarity terms, which each scale as O( d), the resulting null baseline for the metric is thus O(d/n). This aligns with insights from random matrix theory: in high-dimensional regimes (d n), the null singular spectrum of interaction operators (after centering/whitening) concentrates into non-trivial noise bulk whose upper edge depends on d/n and preprocessing, rather than collapsing to zero (Wachter, 1978; Muller, 2002). Our framework estimates this null baseline directly via permutation, providing metricand pipeline-independent alternative to asymptotic formulas. Neighborhood metrics follow different regime. While spectral metrics have null baselines scaling as O(d/n), neighborhood-based metrics such as mutual k-NN exhibit different behavior, as they rely on set comparisons rather than interactions. Proposition 4.2 (Null baseline for neighborhood metrics). Assume the rows are i.i.d. with xi and yi independent, and that pairwise distances are almost surely distinct (e.g., under absolutely continuous distributions). Then for any < n, (cid:20) (cid:21) mKNN(X, Y) = EH0 1 . (4) ℓ , Y(B) ℓ Sℓ,ℓ := s(X(A) ) be the similarity between layer ℓ of model and layer ℓ of model B. It is common to summarize the similarity between two models by the maximum alignment score Tmax = maxℓ,ℓ Sℓ,ℓ. Let = LALB be the number of layer pairs searched, where LA and LB are the depths of models and B. Even under H0, taking maximum over comparisons inflates the reported score, look-elsewhere effect. This is an instance of the classical multiple comparisons problem (Benjamini & Hochberg, 1995; Bonferroni, 1936): as increases, the probability that at least one null similarity exceeds any fixed threshold grows, inflating the expected maximum. Consequently, when alignment is summarized via max or top-k statistic without correction, unrelated representations can exhibit spuriously high reported similarity, as the inflation depends on model depth, making raw summaries non-comparable across architectures. Characterizing this inflation does not require independence across pairs. It follows from uniform right-tail bound. Assume there exist common mean µ and σ > 0 such that the null fluctuations satisfy, for all (ℓ, ℓ) and all 0, P(Sℓ,ℓ µ t) exp (cid:18) t2 2σ2 (cid:19) . (5) For bounded similarities Sℓ,ℓ [smin, smax], Hoeffdings inequality implies sub-Gaussian right-tail bound of the form Equation (5) with σ (smax smin)/2. This covers many common bounded metrics (e.g., CKA/RSA/mKNN). Crucially, only the right tail is needed for bounding the maximum. Then union bound gives P(Tmax µ t) exp (cid:18) (cid:19) , t2 2σ2 and consequently for constant EH0 [Tmax] µ + σ(cid:112)log . (6) (7) Proof. See Section D.6. Proof. See Section D.5. In particular, neighborhood metrics have null baselines scaling as O(k/n). The difference in null baseline between spectral and neighborhood metrics is substantial: (i) The neighborhood scale can be fixed consistently across experiments, whereas the embedding dimension is determined by the model architecture, making it difficult to control in comparison studies. (ii) The neighborhood metrics are much less confounded since in typical settings. 4.2. The depth confounder subtle yet pervasive issue is the comparison of selection-based alignment summaries across models. Let This creates depth confounder. Deeper models (larger = LALB) can attain higher raw max-alignment scores purely because of larger search space. Correlations across neighboring layers reduce the effective number of comparisons, but the inflation remains monotone in the search space size in typical workflows. Therefore, raw scaling plots of Tmax (or top-k summaries) are not comparable across architectures unless the selection step itself is calibrated. 5. Representational similarity calibration To overcome the issues of the width and depth confounders, we introduce the null-calibration for representational similarity. The key idea is to compare observed similarity scores 4 Revisiting the Platonic Representation Hypothesis: An Aristotelian View against an empirical null distribution obtained by permuting sample correspondences, thereby establishing principled zero point that accounts for finite-sample, high-dimensional artifacts. so the gating rule scal > 0 (where scal is the calibrated score defined in Equation (12), which implies α) is finite-sample α-level declaration of similarity above chance. Proof. Follows directly from Lemma D.2; see Section D.1. 5.1. Null-calibrated similarity We propose null-calibrated similarity measures to correct for width and depth confounders by transforming raw similarity scores into an effect size with principled zero point. Given representations Rndx and Rndy aligned by rows, we operationalize the null hypothesis H0 (no relationship beyond marginal statistics) by permuting sample correspondences. For permutations πk Πn drawn i.i.d. uniformly from Πn and independently of (X, Y), we form null scores s(k) = s(X, πk(Y)), = 1, . . . , K. (8) Let sobs := s(X, Y) denote the observed score. Let s(1) s(2) s(K+1) denote the order statistics of the combined multiset {sobs, s(1), . . . , s(K)} (with ties allowed). We define right-tail rank-based critical value: τα := s((1α)(K+1)), (9) where (1 α)(K + 1) is the (1 α)-quantile of the (K + 1)-sized multiset and the empirical right-tail p-value: = 1 + #{k {1, . . . , K} : s(k) sobs} + 1 . (10) The critical value τα defines robust zero point: values below τα are typical under H0 at level α, while provides an evidence measure that can be combined with multipletesting correction when many comparisons are performed. The proposed calibration framework relies on randomization (permutation) to construct null distribution for any similarity statistic. This yields finite-sample guarantees under an exchangeability condition (Assumption 3.1), and it implies useful invariances that make calibrated scores comparable across metrics and implementations. The permutation p-value in Equation (10) is super-uniform under H0 (i.e., PH0(p α) α for all α [0, 1]), standard consequence of randomization inference (Nichols & Holmes, 2002; Phipson & Smyth, 2010; Good, 2005) (see Section D.1 for formal definitions and proofs). Corollary 5.1 (Type-I control for calibrated scores). Let sobs = s(X, Y) and s(k) = s(X, πk(Y)) for = 1, . . . , K. Define the add-one permutation p-value as in Equation (10), and equivalently define the rank-based critical value τα := s((1α)(K+1)) from the sorted combined set {sobs, s(1), . . . , s(K)}. Under Assumption 3.1, (cid:0)sobs > τα (cid:0)p α(cid:1) α and hence PH0 (cid:1) α, PH (11) 5 Calibrated score (scalar case). While p-values and null percentiles are rank-based and therefore invariant under monotone transformations of the raw score (Proposition D.3; see Section D.2), effect sizes serve complementary purpose: they quantify how much similarity exceeds chance on an interpretable scale. The calibrated score achieves this by rescaling the excess over the null threshold τα to the interval [0, 1]. This rescaling is not monotone-invariant, and this by design. purely rank-based calibration would be equivalent to score shift and would be unable to correct for the scale-dependent null baselines identified in Section 4. The calibrated score instead adapts to the actual null distribution, providing meaningful zero point. For bounded similarity metrics with known maximum smax (often smax = 1), we define max-preserving calibrated score scal = max , . (12) (cid:18) sobs τα smax τα (cid:19) This calibrated score depends on the chosen level α through τα (Equation (9)). We therefore also report the corresponding permutation p-value and/or null percentile for an α-free summary. This score satisfies scal = 0 whenever sobs τα (i.e., below the estimated right-tail critical value of the permutation null), and scal = 1 when sobs = smax (i.e., perfect similarity remains 1). When smax is unknown, or the metric is unbounded, we default to the unnormalized effect size [s τα]+ = max(s τα, 0). 5.2. Aggregation-aware null-calibration To analyze the similarity between two models and with depths LA and LB, common approach is to compute layer-by-layer similarity matrix RLALB by evaluating similarity score for every pair of layers: (cid:16) Sℓ,ℓ = X(A) ℓ , Y(B) ℓ (cid:17) , (13) ℓ Rndℓ and Y(B) where X(A) ℓ Rndℓ are the representations of models and at layers ℓ and ℓ respectively, evaluated on samples, and s(, ) is similarity metric. common practice is then to summarize by selectionbased aggregation operator, such as taking the maximum. These summaries are attractive because they support statements such as there exists layer in that matches some layer in or each layer of best matches layer in B. However, selection introduces statistical effect: even under the null hypothesis of no relationship between representations, selection-based summaries are systematically inflated. Revisiting the Platonic Representation Hypothesis: An Aristotelian View As analyzed in Section 4.2, this inflation grows with the number of layer pairs and makes naıve post-selection pvalues anti-conservative. Our aggregation-aware calibration addresses this by calibrating the reported statistic directly: the null distribution must match the entire analysis pipeline. Let the aggregate score be (S) (e.g., maximum), then the appropriate null is the distribution of (S) under valid null transformation (e.g., permuting sample correspondences). We therefore define an aggregation-aware permutation null. Consistency of permutations across layers. For each draw πk Πn, we apply the same sample permutation to all layers of model and define (cid:16) S(k) ℓ,ℓ := X(A) ℓ ℓ = 1, . . . , LA, (cid:16) (cid:17)(cid:17) Y(B) , πk ℓ ℓ = 1, . . . , LB, , (14) := (S(k)). Let Tobs then compute (k) := (S) denote the observed aggregate. Let T(1) T(K+1) denote the order statistics of the combined set {Tobs, (1), . . . , (K)} (with ties allowed). We define τ agg α := T((1α)(K+1)), (15) where (1 α)(K + 1) is the (1 α)-quantile of the (K + 1)-sized multiset. We report the right-tail permutation p-value pagg = 1 + #{k {1, . . . , K} : (k) Tobs} + 1 , (16) By the same exchangeability argument as for scalar calibration, pagg is super-uniform under H0 (see Proposition D.4). Calibrated score (aggregate case). For bounded similarities with maximum smax (often smax = 1), we report max-preserving calibrated aggregate Tcal = max (cid:18) Tobs τ agg smax τ agg α α (cid:19) , 0 . (17) Use scalar calibration (Section 5.1) when comparing single pair of representations. Use aggregation-aware calibration (Section 5.2) when reporting summary statistic (e.g., maximum) over multiple layer pairs. Section provides pseudocode for both procedures. 6. Experiments We quantify the effects of the width and depth confounders in controlled synthetic experiments and show that our calibration framework effectively removes them. We then revisit the Platonic Representation Hypothesis using our calibration framework, assessing which convergence trends remain robust after controlling for these confounding factors. 6.1. Null-calibration removes width confounder We validate that our calibration eliminates width-related inflation of similarity across metrics, regimes, and noise distributions, without metric-specific derivations. We design controlled synthetic experiments as follows. Under H0, we draw X, Rnd independently from Gaussian and heavy-tailed (Student-t, Laplace) distributions. We sweep the number of samples {128, 256, 512, 1024, 2048, 4096} and the dimension {128, 256, 512, 1024, 2048}. Under H1, we inject shared low-rank signal component and vary the signal-to-noise ratio. We evaluate representative metrics spanning three families. For spectral similarity, we use linear and RBF CKA, as well as CCA/SVCCA/PWCCA; for neighborhood similarity, we use mKNN (with = 10); and for geometric similarities, we use RSA and Procrustes. Figure 3 reports subset of these metrics for readability; additional metrics are reported in Section F.6. For calibration, we use = 200 permutations with α = 0.05. Under H0, uncalibrated scores increase with d/n, while our calibrated scores stay at zero across settings (Figure 3). This confirms that the similarity scores of wider models can arise This score satisfies Tcal = 0 when Tobs τ agg and Tcal = 1 when Tobs = smax. As above, Tcal depends on α via τ agg α ; we therefore report both Tcal (magnitude above null) and pagg (evidence against null), applying multiplicity correction (Holm, 1979; Benjamini & Hochberg, 1995) when many model pairs are evaluated. α 5.3. Summary To compute calibrated similarity score: (i) fix significance level α (e.g., α = 0.05); (ii) generate null scores by permuting sample correspondences; (iii) compute critical value τ as the (1 α)(K + 1)-th order statistic of the combined set (observed + null scores); (iv) return calibrated score, either scal or Tcal. Figure 3. Calibration eliminates spurious similarity across metrics. Raw scores (top) drift with d/n; calibrated scores (bottom) collapse to zero. Results for heavy-tailed distributions and additional metrics are in Section F.6. Revisiting the Platonic Representation Hypothesis: An Aristotelian View purely from high-dimensional finite-sample effects, and our calibration removes this spurious baseline. Importantly, the magnitude of the null baseline is metric-dependent, consistent with our theory: CKAs baseline scales as O(d/n) (Proposition 4.1), while mKNNs baseline scales as O(k/n) (Proposition 4.2). Intuitively, mKNN compares local neighborhood overlap at fixed k, thus only comparing relationships instead of local distances, making its null baseline insensitive to representation width d, which explains the order-of-magnitude gap observed in raw scores. The same pattern holds for heavy-tailed noise (Section F.6). Next, we verify the statistical guarantees of our empirical null calibration. For Type-I error control, rejection rates stay at or below the nominal α = 0.05 across (n, d/n) configurations (Figure 4a). Crucially, our calibration does not sacrifice sensitivity to real alignment: detection rates increase rapidly with signal strength (Figure 4b). Overall, our calibration preserves signal structure: in the high-signal regime, raw and calibrated scores show the same pattern, while in the low-signal regime, calibration correctly gates scores to zero (Section F.2). Furthermore, we verify that our empirical calibration closely matches existing analytical bias corrections for CKA (Murphy et al., 2024), recovering the width correction without metric-specific derivation (Section F.4). Additionally, we perform ablations on different noise distributions used in the synthetic experiments (Section F.1), different calibration approaches (Section F.3), and an ablation on the influence of the number of permutations used for calibration (Section F.5). Figure 4. Statistical guarantees. (Left) Type-I error stays at or below α across configurations. (Right) Power increases rapidly with signal strength; calibration does not sacrifice sensitivity. 6.2. Null-calibration removes depth confounder We validate that our aggregation-aware null-calibration eliminates the depth confounder. To build controlled synthetic setting, we construct two synthetic models, and B, each with layers. Under H0, we sample layer representations ℓ=1, where each Xℓ, Yℓ Rnd {Xℓ}L has i.i.d. (0, 1) entries (independent across layers and between models), using d/n = 8 to match the upper range of the Platonic Representation Hypothesis setting. We then compute the layerwise similarity matrix Sℓ,ℓ = ℓ=1 and {Yℓ}L 7 CKAlin(Xℓ, Yℓ) and summarize it with standard aggregation operators. The uncalibrated max-aggregated scores inflate with layer count even under H0 (Figure 5): raw max-scores are systematically higher at = 128 than at = 1, despite no true signal. Our aggregation-aware calibration eliminates this bias: calibrated aggregates remain stable regardless of depth. We further show that naively calibrating each scalar comparison still leads to inflation, highlighting the importance of calibrating the final statistic. Furthermore, since deeper models tend to be wider as well, raw comparisons are doubly confounded. Figure 5. Aggregation-aware calibration removes depth confounding. Raw max-aggregates of linear CKA scores inflate with layer count under the null; calibrated aggregates are stable and show that naive entry-wise calibration still leads to inflation. 6.3. Revisiting the Platonic Representation Hypothesis central claim behind the Platonic Representation Hypothesis is that, as models become more capable, their representations begin to converge across modalities. We revisit this claim through our calibration framework to determine whether the observed alignment reflects genuine shared representation structure or instead arises from width and depth confounders. We follow the experimental protocol of Huh et al. (2024) using = 1024 imagetext pairs (WIT; Srinivasan et al. (2021)) and embeddings from three language model families (Bloomz, OpenLLaMA, LLaMA) and five vision model families (ImageNet-21K, MAE, DINOv2, CLIP, CLIP-finetuned) across multiple scales. This yields 204 visionlanguage model pairs spanning d/n [0.75, 8]. For each pair, we compute layer-wise similarity and report the maximum across layers, as in the original work. We evaluate both global spectral metrics (CKA linear/RBF) and local neighborhood metrics (mKNN, cycle-kNN, CKNNA). Following Huh et al. (2024), we evaluate mKNN, cycle-kNN, and CKNNA with = 10. We further apply BenjaminiHochberg FDR correction (Benjamini & Hochberg, 1995) to control for multiple comparisons across model pairs. For the global similarity, we find that uncalibrated CKA scores increase with model scale (dotted lines in Figure 6a), reproducing the trend interpreted as evidence of cross-modal Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) CKA RBF: Global spectral alignment. (b) mKNN: Local neighborhood overlap. Figure 6. Revisiting the Platonic Representation Hypothesis. Models are ranked according to their language performance (Huh et al., 2024). Solid lines connect the models within the same family, while semi-transparent lines connect the models across different families. (a) Global spectral metrics lose their convergence trend; calibrated scores show no systematic increase with scale. (b) Local neighborhood metrics keep their trend even after calibration. Full results for all vision families and metrics in Section F.7. model families. Consistent with our previous findings, the global similarity (CKA) shows no trend with model capacity (Figure 7). In contrast, for local similarity (mKNN), clear scaling trend emerges with VideoMAE-Huge, whereas smaller video encoders appear to act as bottleneck, limiting alignment regardless of language model size. This confirms that local neighborhood convergence extends to videolanguage alignment, provided that representations are sufficiently powerful. Section F.8 further compares variety of image models at the frame level on the same dataset, showing the same trend. Taken together, these results suggest refined version of the Platonic Representation Hypothesis. After calibration, we find little evidence that representations converge in global spectral structure as models scale, at least under the considered setting. What reliably persists is local geometric alignment: different models preserve similar neighborhood relationships among inputs. We therefore propose the alternative Aristotelian Representation Hypothesis: As models become capable, their representations converge to shared local neighborhood relationships. 7. Conclusion Representational similarity metrics are widely used to study learned features, but their interpretation is systematically distorted by two artifacts: width-dependent null baselines and depth-dependent selection inflation. We introduced unified null-calibration framework that corrects both, turning similarity scores into effect sizes with principled zero points and valid p-values. Applying our framework to the Platonic Representation Hypothesis reveals that previously reported global spectral convergence is largely confounded by width and depth, whereas local neighborhood alignment remains significant, motivating an Aristotelian Representation Hypothesis. Figure 7. Videolanguage alignment. Extending the Platonic Representation Hypothesis analysis to video encoders (VideoMAE base/large/huge) yields the same pattern: calibrated CKA drops substantially while mKNN retains alignment. convergence (Huh et al., 2024). However, this trend disappears after our calibration (solid lines): calibrated CKA shows no systematic increase with model size. This indicates that global convergence in uncalibrated CKA is largely attributable to width and depth confounders rather than genuine increase in representational similarity. In contrast, for the local similarity, evidence of cross-modal convergence remains strong for neighborhood-based metrics even under our calibration (Figure 6b). The same qualitative conclusion holds for other neighborhood-based measures (cycle-kNN and CKNNA; Section F.7) and different choices of α (Section F.10). Further analysis (Section F.9) reveals that models converge in local neighborhood structure: models increasingly agree on which points are neighbors, but do not agree on the pairwise distances, since CKA-RBF with small bandwidth shows no alignment after calibration. To test whether these findings generalize beyond images and text, we extend our analysis to videolanguage alignment following Zhu et al. (2026). We compare video encoders (VideoMAE base/large/huge) against the same language Revisiting the Platonic Representation Hypothesis: An Aristotelian View"
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Artyom Gadetsky, Siba Smarak Panigrahi, Debajyoti Dasgupta, David Fruhbuss, Shin Matsushima, Rishubh Singh, Adriana Moreno Castan, and Gioele La Manno for their valuable suggestions, which helped improve the manuscript. We are especially grateful to Simone Lionetti for additional input and support. We gratefully acknowledge the support of the Swiss National Science Foundation (SNSF) starting grant TMSGI2 226252/1, SNSF grant IC00I0 231922, and the Swiss AI Initiative. M.B. is CIFAR Fellow in the Multiscale Human Program."
        },
        {
            "title": "References",
            "content": "Aggarwal, C. C., Hinneburg, A., and Keim, D. A. On the surprising behavior of distance metrics in high dimensional space. In International Conference on Database Theory. Springer, 2001. Aristotle. Categories, ca. 350 B.C.E. Feichtenhofer, C. PerceptionLM: Open-access data and models for detailed visual understanding. Advances in Neural Information Processing Systems, 2025. Chun, C., Canatar, A., Chung, S., and Lee, D. D. Estimating Neural Representation Alignment from Sparsely Sampled Inputs and Features. arXiv preprint arXiv:2502.15104, 2025. Cramer, H. Mathematical methods of statistics. Princeton University Press, 1999. Cui, T., Kumar, Y., Marttinen, P., and Kaski, S. Deconfounded representation similarity for comparison of neural networks. Advances in Neural Information Processing Systems, 2022. Diedrichsen, J., Berlot, E., Mur, M., Schutt, H. H., Shahbazi, M., and Kriegeskorte, N. Comparing representational geometries using whitened unbiased-distance-matrix similarity. arXiv preprint arXiv:2007.02789, 2020. Benjamini, Y. and Hochberg, Y. Controlling the false discovery rate: practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, 1995. Ding, F., Denain, J.-S., and Steinhardt, J. Grounding representation similarity through statistical testing. Advances in Neural Information Processing Systems, 2021. Beyer, K., Goldstein, J., Ramakrishnan, R., and Shaft, U. When is nearest neighbor meaningful? In International Conference on Database Theory. Springer, 1999. Embrechts, P., Kluppelberg, C., and Mikosch, T. Modelling extremal events: for insurance and finance. Springer Science & Business Media, 2013. Bo, Y., Soni, A., Srivastava, S., and Khosla, M. Evaluating representational similarity measures from the lens of functional correspondence. arXiv preprint arXiv:2411.14633, 2024. Bolya, D., Huang, P.-Y., Sun, P., Cho, J. H., Madotto, A., Wei, C., Ma, T., Zhi, J., Rajasegaran, J., Rasheed, H. A., Wang, J., Monteiro, M., Xu, H., Dong, S., Ravi, N., Li, S.- W., Dollar, P., and Feichtenhofer, C. Perception encoder: The best visual embeddings are not at the output of the network. Advances in Neural Information Processing Systems, 2025. Bonferroni, C. Teoria statistica delle classi calcolo delle probabilita. Pubblicazioni del istituto superiore di scienze economiche commericiali di firenze, 1936. Cai, M. B., Schuck, N. W., Pillow, J. W., and Niv, Y. Representational structure or task structure? Bias in neural representational similarity analysis and Bayesian method for reducing bias. PLoS Computational Biology, 2019. Cho, J. H., Madotto, A., Mavroudi, E., Afouras, T., Nagarajan, T., Maaz, M., Song, Y., Ma, T., Hu, S., Jain, S., Martin, M., Wang, H., Rasheed, H. A., Sun, P., Huang, P.-Y., Bolya, D., Ravi, N., Jain, S., Stark, T., Moon, S., Damavandi, B., Lee, V., Westbury, A., Khan, S., Kraehenbuehl, P., Dollar, P., Torresani, L., Grauman, K., and Good, P. Permutation, parametric and bootstrap tests of hypotheses. Springer, 2005. Harvey, S. E., Lipshutz, D., and Williams, A. H. What Representational Similarity Measures Imply about Decodable Information. In Proceedings of UniReps: the Second Edition of the Workshop on Unifying Representations in Neural Models. PMLR, 2024. Holm, S. simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 1979. Hotelling, H. Relations between two sets of variates. In Breakthroughs in Statistics: Methodology and Distribution. Springer, 1992. Huh, M., Cheung, B., Wang, T., and Isola, P. Position: The platonic representation hypothesis. In International Conference on Machine Learning, 2024. Klabunde, M., Schumacher, T., Strohmaier, M., and Lemmerich, F. Similarity of neural network models: survey of functional and representational measures. ACM Computing Surveys, 2025. Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. Similarity of neural network representations revisited. In International Conference on Machine Learning, 2019. 9 Revisiting the Platonic Representation Hypothesis: An Aristotelian View Kriegeskorte, N., Mur, M., and Bandettini, P. A. Representational similarity analysisconnecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2008. Raugel, J., Szafraniec, M., Vo, H. V., Couprie, C., Labatut, P., Bojanowski, P., Wyart, V., and King, J.-R. Disentangling the factors of convergence between brains and computer vision models. arXiv preprint arXiv:2508.18226, 2025. Lehmann, E. L. and Romano, J. P. Testing statistical hypotheses. Springer, 2005. Maniparambil, M., Akshulakov, R., Djilali, Y. A. D., El Amine Seddik, M., Narayan, S., Mangalam, K., and OConnor, N. E. Do Vision and Language Encoders Represent the World Similarly? In Conference on Computer Vision and Pattern Recognition, 2024. Robert, P. and Escoufier, Y. unifying tool for linear multivariate statistical methods: the RV-coefficient. Journal of the Royal Statistical Society, 1976. Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., Kar, K., Bashivan, P., Prescott-Roy, J., Geiger, F., et al. Brain-score: Which artificial neural network for object recognition is most brain-like? BioRxiv, 2018. Marcos-Manchon, P. and Fuentemilla, L. Convergent transformations of visual representation in brains and models. arXiv preprint arXiv:2507.13941, 2025. Smilde, A. K., Kiers, H. A., Bijlsma, S., Rubingh, C., and Van Erk, M. Matrix correlations for high-dimensional data: the modified RV-coefficient. Bioinformatics, 2009. Morcos, A., Raghu, M., and Bengio, S. Insights on representational similarity in neural networks with canonical correlation. Advances in Neural Information Processing Systems, 2018. Muller, R. R. random matrix model of communication via antenna arrays. IEEE Transactions on information theory, 2002. Song, L., Smola, A., Gretton, A., Bedo, J., and Borgwardt, K. Feature Selection via Dependence Maximization. Journal of Machine Learning Research, 2012. Srinivasan, K., Raman, K., Chen, J., Bendersky, M., and Najork, M. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021. Murphy, A., Zylberberg, J., and Fyshe, A. Correcting biased centered kernel alignment measures in biological and artificial neural networks. arXiv preprint arXiv:2405.01012, 2024. Tjandrasuwita, M., Ekbote, C., Ziyin, L., and Liang, P. P. Understanding the Emergence of Multimodal Representation Alignment. In International Conference on Machine Learning, 2025. Neyshabur, B., Sedghi, H., and Zhang, C. What is being transferred in transfer learning? Advances in Neural Information Processing Systems, 2020. Nguyen, T., Raghu, M., and Kornblith, S. Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth. In International Conference on Learning Representations, 2021. Nichols, T. E. and Holmes, A. P. Nonparametric permutation tests for functional neuroimaging: primer with examples. Human Brain Mapping, 2002. Phipson, B. and Smyth, G. K. Permutation P-values Should Never Be Zero: Calculating Exact P-values When Permutations Are Randomly Drawn. Statistical Applications in Genetics & Molecular Biology, 2010. Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J. SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. In Advances in Neural Information Processing Systems, 2017. Tong, Z., Song, Y., Wang, J., and Wang, L. VideoMAE: Masked autoencoders are data-efficient learners for selfsupervised video pre-training. Advances in Neural Information Processing Systems, 2022. Wachter, K. W. The strong limits of random matrix spectra for sample matrices of independent elements. The Annals of Probability, 1978. Weenink, D. Canonical correlation analysis. In Proceedings of the Institute of Phonetic Sciences of the University of Amsterdam. University of Amsterdam Amsterdam, 2003. Westfall, P. H. and Young, S. S. Resampling-based multiple testing: Examples and methods for p-value adjustment. John Wiley & Sons, 1993. Williams, A. H., Kunz, E., Kornblith, S., and Linderman, S. W. Generalized Shape Metrics on Neural Representations. In Advances in Neural Information Processing Systems, 2021. Zhu, T., Han, T., Guibas, L., Patraucean, V., and Ovsjanikov, M. Dynamic Reflections: Probing Video Representations with Text Alignment. In International Conference on Learning Representations, 2026. 10 Revisiting the Platonic Representation Hypothesis: An Aristotelian View A. Limitations Permutation calibration is finite-sample valid under Assumption 3.1, which treats the row pairs as exchangeable units. In practice, exchangeability can be violated even without sequential structure (e.g., grouped/clustered samples). In such settings, validity is recovered by using restricted permutations that preserve the dependence structure (e.g., permuting within blocks or permuting block labels) and by re-running under each restricted permutation. B. Existing calibration approaches for representational similarity metrics Table 1. Comparison of prior works. Y=yes, N=no, P=partial/indirect. Debias indicates an explicit null correction of the reported similarity. Bounded indicates whether the corrected score preserves an interpretable upper bound (e.g., 1 for perfect alignment). Agg-aware indicates calibration of selection-based aggregates (e.g., max over layer pairs). Ref Metric(s) Debias? Bounded? Agg-aware? Murphy et al. (2024) Chun et al. (2025) Cui et al. (2022) Diedrichsen et al. (2020) RSA (cv/WUC) Cai et al. (2019) Smilde et al. (2009) CKA CKA RSA/CKA RSA (Bayes) RV / adj. RV Ours Any bounded metric Y N N N N C. Metrics and score definitions This appendix gives the definitions of the similarity metrics s(X, Y) used throughout the paper. The main text focuses on the calibration procedure (Sections 5 and 5.2). Here we provide concrete instantiations of the metrics referenced in Section 3 and Section 6. C.1. Preprocessing and basic notation Let Rndx and Rndy denote row-aligned representations evaluated on the same inputs. We use the centering matrix = In 1n1 , (18) 1 where In Rnn is the identity matrix and 1n Rn is the all-ones vector. We define row-centered representations Xc = HX and Yc = HY. Unless stated otherwise, similarities are computed on centered representations. C.2. Raw similarity metrics This section provides formal definitions of the similarity metrics used throughout the paper. In the main text, we primarily use CKA (linear and RBF kernel) (Kornblith et al., 2019), RSA (Kriegeskorte et al., 2008), and mutual k-NN (Huh et al., 2024) as representative metrics from the spectral, geometric, and neighborhood families, respectively. Additional metrics (SVCCA, PWCCA, cycle-kNN, CKNNA, RV coefficient, Procrustes) are included for completeness and used in supplementary experiments. C.2.1. SPECTRAL METRICS Linear Centered Kernel Alignment (CKA). Linear CKA (Kornblith et al., 2019) can be written as normalized Frobenius energy of the sample cross-covariance operator. With Xc, Yc as above, define the sample (cross-)covariances 1 1 (cid:101)C := (cid:101)ΣXY := 1 1 1 (cid:101)ΣXX := (cid:101)ΣY := Yc, Xc, Yc. X (19) The biased linear Hilbert-Schmidt Independence Criterion (HSIC) energy equals (cid:101)C2 normalization can be written as . The commonly used linear CKA CKAlin(X, Y) = (cid:101)C2 (cid:101)ΣXX (cid:101)ΣY = 11 Yc2 XcF YcF [0, 1], (20) Revisiting the Platonic Representation Hypothesis: An Aristotelian View where the second equality follows by cancellation of common 1/(n 1) factors. Kernel Centered Kernel Alignment. Kernel CKA (Kornblith et al., 2019) generalizes linear CKA by replacing dot products with kernel functions. Let kX : Rdx Rdx and kY : Rdy Rdy be positive semidefinite kernel functions (e.g., RBF kernel kX (x, x) = exp(x x2/2σ2)). Let KX Rnn and KY Rnn be Gram matrices with entries (KX )ij = kX (xi, xj) and (KY )ij = kY (yi, yj). Let (cid:101)KX = HKX and (cid:101)KY = HKY denote centered Gram matrices. Kernel CKA is defined as: CKAkX ,kY (X, Y) = (cid:101)KX , (cid:101)KY (cid:101)KX (cid:101)KY . (21) where A, BF = tr(AB). With positive semidefinite kernels and the biased HSIC estimator, the numerator is nonnegative, and kernel CKA typically lies in [0, 1]. Unbiased Centered Kernel Alignment. The biased HSIC estimator can yield inflated similarity scores at finite sample sizes. Song et al. (2012) derived an unbiased HSIC estimator by recognizing that HSIC can be formulated as U-statistic. Following Kornblith et al. (2019), we substitute the unbiased estimator into the CKA formula. Let (cid:101)KX = HKX be the centered Gram matrix with diagonal set to zero. The unbiased HSIC estimator is: HSICu(KX , KY ) = 1 n(n 3) (cid:32) tr( (cid:101)KX (cid:101)KY ) + 1 (cid:101)KX 1 1 (cid:101)KY 1 (n 1)(n 2) 2 (cid:33) 1 (cid:101)KX (cid:101)KY 1 . (22) Unbiased CKA replaces both numerator and denominator of Equation (21) with this estimator. Unlike the biased version, unbiased CKA can take small negative values at finite n. Canonical Correlation Analysis (CCA)-based similarity. CCA (Weenink, 2003) measures linear subspace alignment. The sample canonical correlations {ρi}r i=1 (with = rank( (cid:101)ΣXY )) are the singular values of the whitened cross-covariance operator 1 2 . Common scalar summaries include the mean canonical correlation 1 SVCCA (Raghu et al., 2017) and PWCCA (Morcos et al., 2018). XX (cid:101)ΣXY (cid:101)Σ (cid:101)TCCA = (cid:101)Σ 1 2 (23) i=1 ρi or weighted average as used in (cid:80)r Singular Vector Canonical Correlation Analysis (SVCCA). SVCCA (Raghu et al., 2017) combines dimensionality reduction via singular value decomposition (SVD) with CCA. First, truncated SVD is applied to each representation to retain the top principal components, yielding Rnp and Rnq. Then CCA is applied to the reduced representations, yielding canonical correlations {ρi}r i=1. The SVCCA similarity is the mean canonical correlation: SVCCA(X, Y) = 1 (cid:88) i= ρi. (24) Projection Weighted Canonical Correlation Analysis (PWCCA). PWCCA (Morcos et al., 2018) improves upon SVCCA by weighting canonical correlations according to their importance in explaining the original representations. Let and hY hX denote the i-th canonical variables (projections onto canonical directions). The weight for the i-th canonical correlation is proportional to how much variance it explains: αi = dx(cid:88) j= hX , X:,j, where X:,j is the j-th column of X. The PWCCA similarity is the weighted mean: PWCCA(X, Y) = (cid:80)r (cid:80)r i=1 αiρi i=1 αi . (25) (26) This weighting ensures that canonical correlations corresponding to principal directions receive higher weight than those corresponding to noise dimensions. 12 Revisiting the Platonic Representation Hypothesis: An Aristotelian View RV coefficient. The RV (Relation between two sets of Variables) coefficient (Robert & Escoufier, 1976; Smilde et al., 2009) is multivariate generalization of the squared Pearson correlation. It measures the similarity between two configuration matrices via their inner-product (Gram) matrices. Let WX = XX and WY = YY be the sample inner-product matrices. The RV coefficient is: [0, 1]. (27) RV(X, Y) = C.2.2. GEOMETRIC METRICS tr(WXWY ) (cid:112)tr(W2 ) tr(W2 ) Representational Similarity Analysis (RSA) via Spearman correlation of dissimilarity matrices. RSA (Kriegeskorte et al., 2008) compares the geometry induced by pairwise dissimilarities. Let δ(, ) be dissimilarity on representation vectors (e.g., correlation distance δ(u, v) = 1 corr(u, v), cosine distance). Define Representational Dissimilarity Matrices (RDMs) (28) and let vec(D) Rn(n1)/2 denote vectorization of the strict upper triangle. RSA is then computed as rank correlation between the two RDM vectors: (DX )ij = δ(xi, xj), (DY )ij = δ(yi, yj), where Spearmans ρ can be expressed as Pearson correlation of ranks, RSA(X, Y) = ρS(vec(DX ), vec(DY )) , ρS(u, v) = corr(rank(u), rank(v)) . (29) (30) Procrustes distance. The orthogonal Procrustes distance (Williams et al., 2021) measures the minimal Euclidean distance between two representations after optimal orthogonal alignment. Assuming dx = dy = d, the optimal orthogonal matrix O(d) is: = argmin QO(d) YQ2 , which has the closed-form solution = VU where UΣV = XY is the SVD. The Procrustes distance is: dProc(X, Y) = YQF . (31) (32) To convert to similarity in [0, 1], one can use 1 d2 Proc/(X2 + Y2 ) after appropriate normalization. C.2.3. NEIGHBORHOOD METRICS Mutual k-Nearest Neighbors (mKNN). mKNN (Huh et al., 2024) focuses on local topology. For each anchor sample i, define the set of its nearest neighbors according to distance measure dist(, ) in and Y, NX(i) = KNNk(i; X), NY(i) = KNNk(i; Y), (33) where KNNk(i; X) denotes the indices of the samples (excluding i) that minimize dist(xi, xj). mKNN is then defined as the average fraction of shared neighbors: mKNNk(X, Y) = 1 n (cid:88) i=1 NX(i) NY(i) [0, 1]. (34) Cycle-kNN (bidirectional k-NN). While mKNN measures one-directional neighborhood overlap, cycle-kNN enforces bidirectional consistency (Huh et al., 2024). pair (i, j) forms cycle if NX(i) and NX(j) (mutual neighbors in X), and similarly for Y. Define the set of bidirectional neighbors: Cycle-kNN measures the overlap of these symmetric neighborhoods: CX(i) = {j : NX(i) and NX(j)}. cycle-kNNk(X, Y) = 1 (cid:88) i= CX(i) CY(i) max(CX(i), 1) [0, 1]. (35) (36) This metric is stricter than mKNN, requiring that shared neighbors be mutually recognized in both representation spaces. Revisiting the Platonic Representation Hypothesis: An Aristotelian View CKA with Neighborhood Alignment (CKNNA). CKNNA (Huh et al., 2024) combines the kernel-based formulation of CKA with local neighborhood structure. Instead of computing CKA on full Gram matrices, CKNNA restricts interaction to k-nearest neighbor graphs. Let AX {0, 1}nn be the adjacency matrix of the k-NN graph on X, with (AX )ij = 1 if NX(i) or NX(j). CKNNA applies the CKA formula (Equation (21)) to the centered adjacency matrices: CKNNAk(X, Y) = HAX H, HAY HF HAX HF HAY HF . D. Theoretical Derivations In this section, we provide the theoretical justification for the confounding factors identified in Section 4. D.1. Permutation validity, super-uniformity, and gating This section formalizes the finite-sample validity of permutation calibration. Definition D.1 (Super-uniformity). p-value is super-uniform under H0 if for all [0, 1], PH0 (p t) t. (37) (38) Equivalently, p-values under H0 are stochastically larger than Unif(0, 1), which is sufficient for valid Type-I error control. Lemma D.2 (Permutation p-values are super-uniform). Under Assumption 3.1, the permutation p-value in Equation (10) satisfies super-uniformity: PH0(p α) α for all α [0, 1] (finite-sample validity). Proof of Lemma D.2. Let sobs = s(X, Y) be the observed statistic and let s(k) = s(X, πk(Y)) for = 1, . . . , be the statistics computed on permuted pairings. Under Assumption 3.1, the vector (sobs, s(1), . . . , s(K)) is exchangeable: its joint distribution is invariant to permutations of the indices. Consider the (upper) rank = 1 + #{k {1, . . . , K} : s(k) sobs} {1, . . . , + 1}. (39) If the scores are almost surely distinct, exchangeability implies that the rank of sobs among {sobs, s(1), . . . , s(K)} is uniform on {1, . . . , + 1}. With possible ties, the add-one p-value of Phipson & Smyth (2010), = + 1 , (40) is conservative, implying PH0(p α) α for all α [0, 1]. Proof of Corollary 5.1. Let sobs = s(X, Y) and s(k) = s(X, πk(Y)) for = 1, . . . , K. Under Assumption 3.1, the vector (sobs, s(1), . . . , s(K)) is exchangeable. Let τα := s((1α)(K+1)) be the (1 α)-quantile defined via the order statistic of the combined multiset {sobs, s(1), . . . , s(K)}. Define the (upper) rank = 1 + #{k {1, . . . , K} : s(k) sobs} {1, . . . , + 1}, and the corresponding add-one p-value = R/(K + 1). By construction of τα, the rejection event {sobs > τα} implies that sobs lies among the largest α(K + 1) values of {sobs, s(1), . . . , s(K)}, hence α(K + 1) and therefore α. By Lemma D.2, PH0(p α) α, which yields PH0 (sobs > τα) PH0(p α) α. 14 Revisiting the Platonic Representation Hypothesis: An Aristotelian View D.2. Monotone invariance of rank-based calibration The following proposition is standard result in randomization inference; we state it here for completeness and to clarify its role in justifying the calibrated score design. Proposition D.3 (Monotone invariance of rank-based calibration (Lehmann & Romano, 2005)). Let : be strictly increasing. Define pg by applying Equation (10) to the transformed statistic using the same permutations. Then pg = p, and likewise the null percentile (the rank of sobs among the combined set) is invariant under g. Proof. Let be strictly increasing. For any two real numbers a, b, we have if and only if g(a) g(b). Therefore, for each permutation draw k, 1{s(k) sobs} = 1{g(s(k)) g(sobs)}. (41) Summing over shows that the permutation rank (and thus the add-one p-value) is unchanged by applying to both the observed and permuted statistics. The same argument applies to the null percentile, since the ordering of samples is preserved under g. D.3. Post-selection inflation and aggregation-aware validity Proposition D.4 (Validity for aggregation-aware calibration). Let be any measurable aggregation operator applied to layer-wise similarity matrix (e.g., max, row-max, top-k). If Tobs = (S) is calibrated against the permutation null {T (S(k))}K k=1 as in Equation (16), then the resulting pagg is super-uniform under H0. Proof of Proposition D.4. Let be any measurable functional of the full data (representations across all layers), producing the scalar report Tobs. Under Assumption 3.1 and consistent layer-wise permutation of sample correspondences, the vector (Tobs, (1), . . . , (K)) is exchangeable. Applying the same rank argument as in Section D.1 yields super-uniformity for the add-one p-value in Equation (16). D.4. The width confounder This appendix provides concrete calculations that justify the width confounder using Random Matrix Theory (RMT): even under independence, interaction operators have non-trivial magnitude and spectrum when is not negligible relative to n. Proof of Proposition 4.1. Let Rndx and Rndy have i.i.d. rows with mean 0, identity covariance, and xi and yi independent. Let = In 1 be the centering matrix, so Xc = HX and Yc = HY. Since is symmetric and idempotent (H2 = H), the sample cross-covariance is 1n1 (cid:101)C = 1 Yc = 1 1 XHY. Denote entry (a, b) as (cid:101)Cab. Expanding via Hij = δij 1 : (cid:101)Cab = 1 1 (cid:88) i=1 XiaYib 1 (cid:16) (cid:88) Xia (cid:17)(cid:16) (cid:88) i=1 j= (cid:17) . Yjb We compute E[ (cid:101)C 2 ab] using independence of xi and yj for all i, j, zero means, and identity covariance. (42) (43) Term 1: E(cid:2)((cid:80) XiaYib)2(cid:3) = (cid:80) i,j E[XiaXja] = E[Xia]E[Xja] = 0. For = j, we have E[X 2 E[XiaXja]E[YibYjb]. For = j, independence across rows and zero mean give ib] = 1. Thus E(cid:2)((cid:80) XiaYib)2(cid:3) = n. ia]E[Y 2 Xja)((cid:80) Ykb)(cid:3) = (cid:80) i,j,k E[XiaXja]E[YibYkb]. This is nonzero only when = and = k, Term 2: E(cid:2)((cid:80) yielding (cid:80) Term 3: E(cid:2)((cid:80) XiaYib)((cid:80) 1 1 = n. Xia)2((cid:80) Yjb)2(cid:3) = E[((cid:80) Xia)2]E[((cid:80) Yjb)2] = = n2. 15 Revisiting the Platonic Representation Hypothesis: An Aristotelian View Combining: (cid:104) (cid:101)C 2 ab (cid:105) = 1 (n 1)2 (cid:18) 2 + (cid:19) n2 n2 = 1 (n 1)2 (n 2 + 1) = 1 1 . Summing over all entries: (cid:104) (cid:101)C2 (cid:105) = dx(cid:88) dy (cid:88) a=1 b= E[ (cid:101)C 2 ab] = dxdy 1 . (44) (45) Interpretation. The null interaction energy is O(dxdy/n). In the common regime dx, dy n, the null energy is O(n) and therefore does not vanish. Since many spectral similarity metrics aggregate singular values (e.g., via (cid:101)C2 ( (cid:101)C)), this already explains positive baseline under H0 and its dependence on (n, dx, dy). = (cid:80) σ2 Why we use permutation rather than closed forms. Closed-form bulk edges are ensembleand normalization-specific and are brittle to the preprocessing used in practice (e.g., centering, whitening, kernelization). Moreover, finite-n corrections can be non-negligible. We therefore estimate the relevant right-tail behavior nonparametrically via permutation. This yields conservative, implementation-faithful estimate of chance fluctuations without relying on fragile analytical formulas. D.5. The depth confounder Here we formalize why selection-based summaries (e.g., maximum similarity over layer pairs) inflate with the size of the search space using Extreme Value Theory (EVT). Let = {Sℓ,ℓ : 1 ℓ LA, 1 ℓ LB} denote the collection of null similarity fluctuations under H0, and let = LALB. Assumption D.5 (Uniform sub-Gaussian right tails and integrability). There exist µ and σ > 0 such that for all (ℓ, ℓ) and all 0, P(Sℓ,ℓ µ t) exp (cid:18) (cid:19) . t2 2σ2 Moreover, each Sℓ,ℓ is integrable: ESℓ,ℓ < for all (ℓ, ℓ). Proposition D.6 (Maximal inequality, no independence required). Under Assumption D.5 and for 2, (cid:104) max ℓ,ℓ Sℓ,ℓ (cid:105) µ + σ(cid:112)log , where > 0 is constant (e.g., one can take = 3). Proof. Let := maxℓ,ℓ Sℓ,ℓ µ. Since < and ESℓ,ℓ < for all (ℓ, ℓ), we have EZ (cid:104) max ℓ,ℓ (cid:105) Sℓ,ℓ + µ (cid:88) ℓ,ℓ ESℓ,ℓ + µ < , so is integrable, and the tail-integration formula applies. By the union bound and Assumption D.5, P(Z t) exp (cid:19) (cid:18) t2 2σ2 for all 0. Using the tail-integration formula for an integrable real-valued random variable Z, E[Z] = (cid:90) 0 P(Z t) dt and the bound P(Z t) 1, we obtain P(Z t) dt (cid:90) 0 P(Z t) dt, (cid:90) (cid:26) E[Z] (cid:90) 0 (cid:18) (cid:19)(cid:27) t2 2σ2 dt. min 1, exp (46) (47) (48) (49) (50) (51) Revisiting the Platonic Representation Hypothesis: An Aristotelian View Let t0 = σ min{1, } switches. Splitting the integral at t0 yields 2 log . This value of t0 is the solution of exp(cid:0)t2 0/2σ2(cid:1) = 1, i.e., the crossover where the bound E[Z] t0 + (cid:90) (cid:18) exp t0 (cid:19) t2 2σ2 dt. Applying the standard Gaussian tail bound (cid:82) t0 et2/(2σ2)dt (σ2/t0)et2 0/(2σ2) gives For 2, the right-hand side is at most 3σ log , proving the claim with = 3. E[Z] σ(cid:112)2 log + σ 2 log . (52) (53) Remark. When the Sℓ,ℓ are i.i.d. (or weakly dependent), classical Extreme Value Theory yields sharper asymptotics. For example, if Sℓ,ℓ (µ0, σ2 0) i.i.d., the centered maximum converges to Gumbel distribution and E[Tmax] µ0 + σ0 (cid:18) 2 ln ln ln + ln 4π 2 2 ln (cid:19) , (54) as stated in standard references (Cramer, 1999; Embrechts et al., 2013). Real layer-wise similarities are dependent, so the approximation above should be treated as heuristic; Proposition D.6 provides dependence-robust upper bound. D.6. Null Baselines for Neighborhood Metrics The preceding analysis focused on spectral metrics whose null baselines scale with d/n. Neighborhood-based metrics such as mutual k-NN follow fundamentally different regime, which we now characterize. Definition D.7 (Mutual k-NN overlap). For representations Rndx , Rndy and neighborhood size < n, let NX(i) {1, . . . , n} {i} denote the indices of the nearest neighbors of sample in (e.g., Euclidean or cosine), and similarly for NY(i). The mutual k-NN overlap is mKNN(X, Y) = 1 n (cid:88) i=1 NX(i) NY(i) . (55) Proposition D.8 (Uniformity of k-NN index sets under i.i.d. sampling). Fix an anchor index {1, . . . , n}. Let x1, . . . , xn Rd be i.i.d. and define the k-NN set NX(i) {1, . . . , n} {i} using fixed distance dist(, ). Assume either (i) {dist(xi, xj)}j=i are almost surely distinct, or (ii) ties are broken by selecting uniformly random k-subset among the set of minimizers. Then NX(i) is uniformly distributed over the (cid:0)n (cid:1) k-subsets of {1, . . . , n} {i}. Proof. Let := {1, . . . , n} {i} be the candidate-neighbor index set. For any permutation π of I, i.i.d. sampling implies (xj)jI d= (xπ(j))jI. The k-NN selection rule depends on the candidate points only through their distances to xi, so permuting the candidate indices permutes the resulting neighbor set. Under either the no-ties assumption or the stated uniform tie-break rule, for any two k-subsets S, there exists permutation π with π(S) = and hence P(cid:0)NX(i) = S(cid:1) = P(cid:0)NX(i) = S(cid:1). Since the events {NX(i) = S} over all = partition the sample space, each has probability (cid:0)n1 Theorem D.9 (Null baseline for mutual k-NN). Let X, Rnd have i.i.d. rows, with independent of Y. Define NX(i) and NY(i) as in Definition D.7, using either almost sure absence of distance ties or uniform random tie-breaking. Then (cid:1)1 . (cid:20) EH mKNN(X, Y) (cid:21) = 1 . Revisiting the Platonic Representation Hypothesis: An Aristotelian View Proof. Fix an anchor i. By Proposition D.8, NX(i) and NY(i) are each uniform random k-subsets of the (n 1)-element set {1, . . . , n} {i}. Moreover, since and are independent and NX(i) (resp. NY(i)) is measurable function of (resp. Y), the sets NX(i) and NY(i) are independent. Therefore NX(i) NY(i) has hypergeometric distribution with population size 1, number of successes k, and draws k, giving (cid:21) (cid:20) EH0 NX(i) NY(i) = k2 . Substituting into the definition of mKNN, (cid:20) (cid:21) mKNN(X, Y) = EH0 1 (cid:88) i=1 EH0 (cid:20) NX(i) NY(i) (cid:21) = 1 (cid:88) i=1 = 1 . Proposition D.10 (Per-anchor variance and generic bounds for mKNN under the null). Under the assumptions of Theorem D.9, for each anchor the intersection size Hi := NX(i) NY(i) is hypergeometric with mean k2/(n 1) and variance Var[Hi] = k2(n 1 k)2 (n 1)2(n 2) . Moreover, since mKNN(X, Y) [0, 1] deterministically, we have the fully general bound Var[mKNN(X, Y)] 1 4 . If one additionally assumes that the per-anchor terms {NX(i) NY(i)/k}n assumption, not consequence of H0), then Var[mKNN(X, Y)] = O(1/n). i=1 are independent (this is modeling Proof. The hypergeometric variance formula gives Var[Hi] = 1 (cid:18) 1 (cid:19) 1 (n 1) (n 1) 1 = k2(n 1 k)2 (n 1)2(n 2) . The bound Var[mKNN] 1/4 follows from mKNN [0, 1]. Under the stated additional independence assumption across anchors, (cid:20) (cid:21) Var (cid:19) (cid:18) H1 1 = 1 nk2 Var[H1], Var mKNN(X, Y) = which is O(1/n) for fixed k. E. Implementation key advantage of null calibration is its simplicity: the framework can be applied to any similarity metric with minimal code changes. This section provides pseudocode for the two main calibration procedures described in the paper. Scalar null calibration. Algorithm 1 shows the complete procedure for calibrating single similarity comparison. The only requirement is function similarity(X,Y) that computes the raw metric. The algorithm returns both permutation p-value and calibrated score with principled zero point. Aggregation-aware calibration for layer-wise comparisons. When comparing models with multiple layers and reporting summary statistic (e.g., maximum similarity across layer pairs), the aggregation step must also be calibrated. Algorithm 2 shows how to extend scalar calibration to this setting. The key insight is that the same sample permutation must be applied consistently across all layers. Computational cost. Scalar calibration requires additional similarity computations. Aggregation-aware calibration requires LA LB computations, which can be parallelized across permutations. In practice, = 200500 permutations suffice for stable p-values and threshold estimation. 18 Revisiting the Platonic Representation Hypothesis: An Aristotelian View π random permutation(n) {Permute sample indices} Algorithm 1 Scalar Null Calibration Require: Representations Rndx , Rndy Require: Similarity function sim(, ), permutations K, significance level α Ensure: Calibrated score scal, p-value 1: sobs sim(X, Y) {Observed similarity} 2: null scores [] 3: for = 1 to do 4: 5: Yπ Y[π, :] {Permute rows of Y} null scores[k] sim(X, Yπ) 6: 7: end for 8: combined [sobs] null scores {Combined set} 9: τα quantile(combined, 1 α) {Critical threshold from combined set} 10: 1+(cid:80)K 11: scal max 12: return scal, 1[null scores[k]sobs] K+1 (cid:16) sobsτα smaxτα {Permutation p-value} , 0 k=1 (cid:17) {Calibrated score (use smax = 1 for bounded metrics)} end for ℓ=1, {Y(ℓ)}LB for ℓ = 1 to LB do ℓ=1 (all samples) S[ℓ, ℓ] sim(X(ℓ), Y(ℓ)) Algorithm 2 Aggregation-Aware Null Calibration Require: Layer representations {X(ℓ)}LA Require: Similarity function sim(, ), aggregator (e.g., max), permutations K, level α Ensure: Calibrated aggregate Tcal, p-value pagg 1: {Compute observed similarity matrix} 2: for ℓ = 1 to LA do 3: 4: 5: 6: end for 7: Tobs (S) {e.g., maxℓ,ℓ S[ℓ, ℓ]} 8: null aggregates [] 9: for = 1 to do 10: 11: 12: 13: 14: 15: 16: 17: end for 18: combined [Tobs] null aggregates {Combined set} 19: τ agg 20: pagg 1+(cid:80)K 21: Tcal max 22: return Tcal, pagg π random permutation(n) {Single permutation for all layers} for ℓ = 1 to LA do end for null aggregates[k] (S(k)) {Aggregate under null} α quantile(combined, 1 α) {Critical threshold from combined set} S(k)[ℓ, ℓ] sim(X(ℓ), Y(ℓ)[π, :]) {Same π for all ℓ} for ℓ = 1 to LB do (cid:16) Tobsτ agg smaxτ agg 1[null aggregates[k]Tobs] K+1 (cid:17) , 0 end for k=1 α α Revisiting the Platonic Representation Hypothesis: An Aristotelian View F. Additional Experimental Results This appendix provides additional analyses that support the main text claims. F.1. Phase diagrams across different noise distributions The theoretical analysis in Section 4 assumes Gaussian entries for tractability, but real neural network activations rarely follow Gaussian distributions. Instead, they often exhibit heavy tails, sparsity, or multimodality. critical question is whether our calibration, which makes no distributional assumptions, remains effective under such deviations. Figure 8 shows phase diagrams under different noise distributions: Gaussian, Student-t (ν = 3), Laplace, and Gaussian mixtures. Each panel shows raw scores (left) and calibrated scores (right) across the (d/n, σ) grid, where σ controls the noise level added to fixed shared signal. At low σ, the signal dominates and both raw and calibrated scores correctly indicate high similarity. At high σ, noise overwhelms the signal, and similarity should approach zero. The key finding is that raw scores remain elevated (around 0.40.6) even at high noise levels where no detectable signal remains, while calibrated scores correctly collapse to near-zero. This pattern holds across all noise distributions tested, confirming that permutation-based calibration adapts to the data-generating process without requiring explicit distributional modeling. (a) Gaussian (b) Student-t (ν = 3) (c) Laplace (d) Gaussian mixture Figure 8. Phase diagrams under different noise types. Calibrated scores (right) collapse to near-zero at high noise levels across the (d/n, σ) grid, while raw scores (left) exhibit systematic positive bias. Calibration remains effective regardless of tail behavior. F.2. SNR sweep heatmaps The experiments of the main paper (Figure 4) demonstrated that calibration eliminates false positives under H0 while preserving sensitivity to fixed signals. This section extends the analysis by characterizing how calibrated similarity varies jointly with signal strength, noise level, and dimensionality ratio, thereby delineating the regimes in which similarity estimation remains reliable. Figure 9 presents heatmaps of raw scores (top row) and calibrated scores (bottom row) across the (Noise level, Signal strength) grid for three signal ranks (r {1, 5, 10}). The results reveal clear phase transition structure. Raw scores (top) show uniformly high values across most of the grid, obscuring the true detection boundary. Calibrated scores (bottom) reveal the underlying signal: high scores concentrate in the low-noise, high-signal corner (bottom-left), while scores correctly collapse to zero as noise increases (moving right) or signal weakens (moving down). The detection boundary shifts rightward (tolerating higher noise) as signal rank increases. This phase structure is meaningful: it delineates when similarity measurements carry information about shared structure versus when they reflect only finite-sample artifacts. Figure 10 provides complementary view by collapsing the 2D heatmaps into 1D curves, plotting calibrated score against noise level for different signal strengths s. As expected, calibrated scores decrease monotonically with noise level: at low 20 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) Rank = (b) Rank = 5 (c) Rank = 10 (d) Rank = 1 (e) Rank = 5 (f) Rank = 10 Figure 9. SNR sweep heatmaps (calibrated scores). Higher-rank signals are detected at higher noise levels. The clear gradient confirms calibration preserves sensitivity to genuine structure. noise, scores are high (reflecting the detectable shared signal), while at high noise, scores collapse to zero (reflecting that the signal is buried). Stronger signals (larger s) maintain elevated scores across wider range of noise levels before eventually succumbing. Higher-rank signals (r = 5, 10) show more gradual decay compared to = 1, consistent with their greater statistical detectability. All curves converge to zero at high noise, confirming that the null floor is correctly calibrated regardless of signal strength or rank. Figure 10. Calibrated scores decay with noise level. Each curve shows calibrated score versus noise level for fixed signal strength s. Stronger signals maintain elevated scores across wider noise ranges; all curves converge to zero at high noise. F.3. Comparing calibration approaches natural question is whether the choice of calibration summary affects the correction. We consider several approaches: (i) gated score, which thresholds at significance level and rescales (α {0.05, 0.1}); (ii) null-centered, subtracting the null mean; (iii) z-score, standardizing by null mean and standard deviation; and (iv) ARI-style, applying the chance-correction formula (s E[s])/(smax E[s]). Figure 11 evaluates these variants across metrics as d/n increases. The results demonstrate that the gated score, null-centered, and ARI-style corrections all successfully collapse to appropriate null baselines across all metrics, regardless of whether the raw metric exhibits severe inflation (CKA, approaching 0.8) or Revisiting the Platonic Representation Hypothesis: An Aristotelian View mild inflation (RSA and mKNN, below 0.1). The z-score calibration, while correcting the mean, can exhibit artifacts when the null distribution is skewed, as occurs for bounded metrics like CKA at high d/n, making it less suitable as universal correction. (a) CKA linear (b) CKA RBF (c) RSA (Spearman) (d) Mutual k-NN Figure 11. Comparing calibration approaches across metrics. Each panel shows raw scores alongside four calibration variants (gated score, null-centered, z-score, ARI-style) as d/n increases. Gated score, null-centered, and ARI-style corrections collapse to appropriate baselines; z-score exhibits artifacts for skewed null distributions. F.4. Comparison with analytical debiasing We validate our empirical null calibration by comparing it to existing analytical bias corrections for CKA. Figure 12 shows the difference between our calibrated CKA and two existing estimators: the debiased CKA of Murphy et al. (2024) and the dep-cols CKA of Chun et al. (2025). Our calibrated CKA closely matches the debiased CKA estimator, indicating that our calibration automatically corrects the dominant width-induced bias without requiring metric-specific derivation. In contrast, dep-cols CKA is designed to correct column dependence, which is not confound in our experimental setup (where columns are independent by construction), and as result, it attenuates the true signal under H1. 22 Revisiting the Platonic Representation Hypothesis: An Aristotelian View Figure 12. Calibration recovers analytical debiasing. Difference between calibrated CKA and existing estimators (n = 1024, d/n swept). (Left) Under signal. (Right) Under null. F.5. Permutation budget analysis Permutation-based calibration introduces computational-statistical tradeoff: more permutations yield more stable threshold estimates but increase runtime. Practitioners need guidance on the minimum budget required for reliable inference. We analyze the stability of threshold estimates τα and calibrated scores as function of the permutation budget across 50 random seeds. Figure 13 shows two panels: the left panel displays threshold estimates, while the right panel shows calibrated scores under H0. Threshold estimates (left) stabilize rapidly, reaching stable values by approximately = 50 for all metrics tested. Calibrated scores (right) exhibit more variability at very low budgets (K < 50), with occasional spikes due to unstable threshold estimation, but converge to near-zero by 100200. Based on these results, we recommend 200. The computational cost scales linearly with K, so this recommendation represents favorable tradeoff between precision and efficiency. Figure 13. Permutation budget analysis. Left: threshold τα stabilizes by 50. Right: calibrated scores under H0 converge to near-zero by 100200. Shaded regions show variability across random seeds. F.6. Full null drift results The main text presents null drift results for representative subset of metrics under Gaussian noise. Here, we present additional results across all metrics evaluated in this work, including RSA, the RV coefficient, and Procrustes distance, as well as results under heavy-tailed noise distributions. Figure 14 presents results under Gaussian noise for all metrics. The severity of null drift varies substantially across metric families: CKA variants exhibit the most severe inflation, followed by RV coefficient and CCA-variants, with neighborhood metrics showing the mildest drift. This reflects the structural sensitivity of the metrics to high-dimensional spurious correlations. Critically, calibration eliminates drift across all metrics, collapsing scores to zero regardless of the raw bias magnitude. Figure 15 extends these results to heavy-tailed noise (Student-t, ν = 3). The qualitative pattern is preserved: all metrics 23 Revisiting the Platonic Representation Hypothesis: An Aristotelian View exhibit positive drift under the null, and calibration eliminates this drift. The magnitude of raw bias is generally higher under heavy-tailed noise, consistent with increased finite-sample variability, yet calibration adapts automatically without requiring distributional knowledge. Figure 14. Full null drift results (Gaussian). Raw scores (top) exhibit systematic positive bias; calibrated scores (bottom) collapse to zero. Figure 15. Full null drift results (heavy-tailed). Student-t (ν = 3) noise. The pattern is consistent across all metrics: calibration eliminates spurious similarity regardless of noise distribution. F.7. Extended PRH alignment results (imagetext) The main text establishes divergence between local and global similarity metrics when applied to the Platonic Representation Hypothesis (PRH): neighborhood-based metrics retain significant cross-modal alignment after calibration, while spectral metrics lose their apparent convergence trend. natural question is whether this finding is robust across model families and metric variants. Here we present comprehensive results across all five vision model families in the PRH setting (DINOv2, CLIP, ImageNet21K, MAE, and CLIP-finetuned) and broad range of metrics spanning the local-to-global spectrum (Figures 16 and 17). The results reinforce and extend the main text findings. Neighborhood metrics (mKNN, cycle-kNN, CKNNA) show consistent alignment trend across all vision families with neighborhood size of 10. This pattern holds for both selfsupervised (DINOv2, MAE) and supervised (ImageNet-21K) pretraining objectives, as well as for both CLIP-aligned and CLIP-finetuned variants. Spectral metrics (CKA linear, CKA RBF, unbiased CKA) show different pattern: raw scores suggest increasing alignment with model scale, but calibrated scores show no such scaling trend. Statistical significance. Beyond calibrated scores, we report permutation p-values to quantify statistical evidence against the null hypothesis of no cross-modal alignment (Figure 18). All 204 visionlanguage model pairs are significant at < 0.05, with most achieving < 0.005 (the minimum achievable with = 200 permutations) for both local and global metrics. This confirms that cross-modal similarity is statistically significant (i.e., has some alignment) across all model pairs. The critical distinction between local and global metrics lies not in statistical significance but in the magnitude and trends of calibrated scores. Local metrics show substantial alignment above the null threshold that persists across scales, whereas global metrics, although significant, show no convergence in calibrated effect sizes. F.8. Extended videolanguage alignment results The main text extends the PRH analysis to videolanguage alignment following Zhu et al. (2026). Here, we provide additional results to verify that the local-vs-global pattern observed for imagelanguage alignment extends to the video 24 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) mKNN: Neighborhood overlap. (b) CKA RBF: Spectral alignment. (c) cycle-kNN: Bidirectional consistency. Figure 16. PRH alignment results (all vision families). All five vision model families are shown (DINOv2, CLIP, ImageNet-21K, MAE, CLIP-finetuned). The divergence between local and global metrics is consistent across all families. (d) Unbiased CKA. Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) CKA linear. (b) CKNNA. Figure 17. Additional PRH metrics (all vision families). CKA linear (a) shows the same loss of convergence trend as CKA RBF. CKNNA (b) shows consistent local alignment across all vision families. (a) mKNN (k = 10). (b) CKA linear. Figure 18. Permutation p-values for PRH alignment. All model pairs are significant at < 0.05, with most achieving < 0.005 for both local (a) and global (b) metrics. The difference between metric families lies in calibrated effect sizes, not significance. 26 Revisiting the Platonic Representation Hypothesis: An Aristotelian View modality. We use 1024 samples from the PVD (Bolya et al., 2025; Cho et al., 2025) test set. We evaluate both video-native models (VideoMAE (Tong et al., 2022)) and image models (DINOv2 and CLIP) applied to the middle frame of each video. We compare these against the same three language model families used in the imagelanguage experiments (BLOOM, OpenLLaMA, LLaMA) at multiple scales. Figure 19 shows results for both spectral (CKA RBF) and neighborhood (mKNN) metrics. The pattern mirrors the imagelanguage findings. For spectral metrics, raw scores suggest alignment, whereas calibrated scores drop significantly, indicating that much of the apparent alignment is attributable to width and depth confounders. In contrast, neighborhood metrics retain significant alignment after calibration, confirming that video and language representations share local topological structure. F.9. Characterizing the locality of cross-modal alignment The main text establishes that local neighborhood metrics retain significant alignment after calibration, while global spectral metrics do not. natural follow-up question is: how local is this alignment? Both mKNN and CKA-RBF have hyperparameters that control their sensitivity to local versus global structure. By varying these parameters, we can characterize the scale at which cross-modal alignment emerges. Experimental setup. We vary two locality parameters: the neighborhood size in mKNN, testing {10, 20, 50, 100} where smaller values focus on immediate neighbors and larger values consider broader local structure, and the RBF kernel bandwidth σ in CKA-RBF, testing σ {0.1, 0.5, 2.0, 5.0}, which controls the length scale over which the kernel assigns significant weight. RBF bandwidth. The RBF (radial basis function) kernel is defined as k(x, y) = exp (cid:0)x y2/2σ2(cid:1). The bandwidth σ determines the length scale of similarity. When σ is small (e.g., 0.1), the kernel is sharply peaked: only very close points contribute significantly to the Gram matrix, making the similarity measure sensitive to exact pairwise distances in the immediate neighborhood. When σ is large (e.g., 5.0), the kernel is broad: even moderately distant points contribute, and the similarity measure aggregates information over larger neighborhoods, becoming sensitive to coarser geometric structure. Neighborhood size. For mKNN, the parameter controls how many nearest neighbors are considered when measuring overlap. Small (e.g., 10) measures agreement on immediate neighbors, i.e., the closest points to each sample, capturing fine-grained local topology. Large (e.g., 100) measures agreement on broader neighborhood. With = 1000 samples and = 100, we ask whether the 10% closest points agree across representations. Crucially, mKNN is rank-based metric: it asks which points are neighbors (ordinal information), not how close they are (cardinal information). mKNN across values. Figure 20 shows the PRH alignment results for mKNN with varying k. consistent pattern emerges: all values show significant alignment after calibration, with calibrated scores remaining well above zero even at = 100. However, the scaling trend is most pronounced at small k. For = 10, raw scores show clear upward trend with model capacity that persists after calibration. At large k, this trend flattens even in raw scores. For = 100, raw scores plateau for larger models, suggesting that broader neighborhood agreement is already saturated across model scales. This pattern indicates that scaling-driven improvement in alignment is concentrated at the finest topological level. CKA-RBF across bandwidth values. Figure 21, and the accompanying p-values in Figure 22, shows results for CKARBF with varying bandwidth σ, revealing different pattern from mKNN. At σ = 0.1 (very local), there is no significant alignment after calibration: raw scores are near 1.0, reflecting the high similarity of any high-dimensional representations under sharply peaked kernel. However, calibrated scores collapse to approximately zero with p-values exceeding 0.05 for most model pairs, indicating that the observed similarity is indistinguishable from chance. At σ = 0.5, alignment emerges, but with flattening trend after calibration. Calibrated scores initially rise with model scale, then plateau and slightly decline for the largest models. At σ = 2.0 and σ = 5.0, significant alignment persists, but the calibrated trend also flattens, resembling the pattern observed for large-k mKNN: alignment exists, but scaling-driven improvement disappears after calibration. 27 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) CKA RBF: spectral alignment. (b) mKNN (k = 10): neighborhood overlap. (c) mKNN (k = 50): neighborhood overlap. (d) CKNNA (k = 10): neighborhood overlap. Figure 19. Videolanguage alignment results. (a) Spectral alignment drops substantially after calibration. (bd) Neighborhood alignment trend remains after calibration. 28 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) mKNN (k = 10) (b) mKNN (k = 20) (c) mKNN (k = 50) Figure 20. PRH alignment with varying neighborhood size for mKNN. All values show significant alignment after calibration. The scaling trend is clearest at small and flattens at large k, suggesting scaling improvements are concentrated at the finest local scale. (d) mKNN (k = 100) 29 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) CKA-RBF (σ = 0.1) (b) CKA-RBF (σ = 0.5) (c) CKA-RBF (σ = 2.0) Figure 21. PRH alignment with varying bandwidth σ for CKA-RBF. At very small σ (a), no significant alignment remains after calibration. Larger σ values (bd) show significant alignment, but the scaling trend flattens after calibration. (d) CKA-RBF (σ = 5.0) 30 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) CKA-RBF (σ = 0.1) (b) CKA-RBF (σ = 0.5) (c) CKA-RBF (σ = 2.0) Figure 22. Significance of PRH alignment with varying bandwidth σ for CKA-RBF. Alignment with σ = 0.1 (a) is not significant for multiple models where larger bandwidths have significance (bd). (d) CKA-RBF (σ = 5.0) Revisiting the Platonic Representation Hypothesis: An Aristotelian View Topological versus metric alignment. The contrasting behavior of mKNN and small-σ CKA-RBF reveals fundamental distinction in what local alignment means. On one hand, mKNN measures topological alignment: do the representations agree on which points are neighbors? This captures ordinal information where the ranking of distances matters but not their absolute values. On the other hand, small-σ CKA-RBF measures metric alignment: do the representations agree on how close neighbors are? This captures cardinal information where exact distance values matter. The fact that mKNN shows alignment at all values while small-σ CKA-RBF shows no alignment reveals that cross-modal representations agree on neighborhood identity (which points are close) but not on exact local distances (how close they are). This finding is consistent with the observation that different training objectives and architectures induce different distance scales in representation space while preserving the relative ordering of neighbors. The Aristotelian Representation Hypothesis should therefore be understood as convergence to shared topological structure rather than shared metric structure. F.10. Sensitivity to significance level α The main text uses significance level of α = 0.05 throughout. natural concern is whether the conclusions of the PRH analysis depend on this particular choice. We repeat the PRH evaluation from Section 6.3 with α {0.01, 0.05, 0.10} for representative global (CKA linear, CKA RBF) and local (mKNN with = 10) metrics. Figures 23 to 25 show that the conclusions are entirely invariant to the choice of α. For global metrics, calibrated scores show no convergence trend at any significance level. For local metrics, calibrated scores retain their alignment trend across all three α values. Stricter thresholds (α = 0.01) produce slightly lower calibrated scores, while more permissive thresholds (α = 0.10) produce slightly higher ones, but the qualitative pattern is unchanged. This confirms that our findings are not an artifact of particular significance level. 32 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) α = 0.01 (b) α = 0.05 (default) Figure 23. Sensitivity to α for CKA linear. Calibrated scores show no convergence trend regardless of significance level. (c) α = 0. 33 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) α = 0.01 (b) α = 0.05 (default) Figure 24. Sensitivity to α for CKA RBF. The same pattern holds: no convergence trend at any significance level. (c) α = 0. 34 Revisiting the Platonic Representation Hypothesis: An Aristotelian View (a) α = 0.01 (b) α = 0.05 (default) Figure 25. Sensitivity to α for mKNN (k = 10). Local alignment and its scaling trend persist across all significance levels. (c) α = 0."
        }
    ],
    "affiliations": [
        "EPFL",
        "HSLU",
        "University of Basel"
    ]
}