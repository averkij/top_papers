{
    "paper_title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
    "authors": [
        "Anshumann",
        "Mohd Abbas Zaidi",
        "Akhil Kedia",
        "Jinwoo Ahn",
        "Taehwak Kwon",
        "Kangwook Lee",
        "Haejun Lee",
        "Joohyung Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B."
        },
        {
            "title": "Start",
            "content": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs Anshumann* and Mohd Abbas Zaidi* and Akhil Kedia* and Jinwoo Ahn and Taehwak Kwon Kangwook Lee and Haejun Lee and Joohyung Lee Samsung Research, Seoul {anshu.mann, abbas.zaidi, akhil.kedia, jinwoo.ahn, taehwak.kwon}@samsung.com 5 2 0 2 1 2 ] . [ 1 0 7 8 6 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Knowledge distillation can be cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method Random Sampling Knowledge Distillation, which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (< 10%) compared to crossentropy based training, while maintaining competitive performance compared to full distillation, across range of model sizes from 300M to 3B."
        },
        {
            "title": "Introduction",
            "content": "Distilling the knowledge from larger teacher into smaller student (Hinton et al., 2015) has been successfully used to train more efficient and stronger models across range of applications (Fukuda et al., 2017; Jiao et al., 2020; Ahn et al., 2019; Tian et al., 2020; Sanh et al., 2019; Bergmann et al., 2020; Zhao et al., 2022; Xu et al., 2024b). As Large Language Models (LLMs) reach increasing adoption, Knowledge Distillation has also been applied to improve smaller LLMs (Sreenivas et al., 2024; Muralidharan et al., 2024; Gu et al., 2024; Wang et al., 2021; Gu et al., 2023; Palo et al., 2024; Boizard et al., 2024; Jiang et al., 2023). Two common categories of Knowledge Distillation are distribution matching, where the teachers final logits or output distribution are learned, and *These authors contributed equally to this work Figure 1: Sparse Knowledge Distillation Pipeline representation matching, where intermediate-layer representations are distilled (Wen et al., 2023). In this work, we focus on the former, in offline logits setting, where the logits from the teacher are precomputed and cached, prior to training the student. Particularly for LLMs, this setting has several advantages The larger, more expensive teacher only needs to run once, and the saved representations can then be used to train family of smaller models of various sizes. Teacher inference can be done on cheaper compute resources without fast multi-node interlinks, and the student can be trained on smaller clusters. Cluster size is further reduced by eliminating the memory footprint of the teacher. Lastly, this makes smaller-scale design experiments and ablations feasible by eliminating the constant large overhead of the teacher. While this is often done for post-training (Shum et al., 2024) or for dataset generation/filtering (Gu et al., 2024; Wen et al., 2023; Gunasekar et al., 2023), extending this to pre-training is challenging. In contrast to vanilla pre-training, knowledge distillation requires the information-dense soft targets (teacher probabilities) to be stored. Due to the large vocabulary size of modern LLMs, naively storing all of these probabilities is infeasible (e.g., requiring 128 PetaBytes of storage for 1T tokens for Llama3 (Grattafiori et al., 2024)). Instead, sparse knowledge distillation approaches store an efficient Top-K subset of logits from the teachers distribution (Raman et al., 2023; Peng et al., 2024). However, these methods still require large number of logits (6400) to be stored, or even observe fall in model performance (Peng et al., 2024). In this work, via theoretical proofs, crossvalidated by empirical analysis, we show that the performance drop in Top-K methods stem from two primary causes - 1) Top-K provides biased estimator of the teachers probability distribution, and 2) It fails to expose the tail of teachers distribution to the student model. These result in the student learning scaled-up and mis-calibrated distribution of the teacher probability. We rectify both of these issues by instead utilizing importance sampling (Elvira and Martino, 2021) to randomly sample from the teachers distribution. We show that our proposed Knowledge Distillation approach 1) Provides an unbiased estimate of the teachers probability distribution, 2) Preserves the gradient in expectation compared to full distillation, and 3) Eliminates the overhead of running the teacher inference, while maintaining model performance to full distillation, using extremely limited storage."
        },
        {
            "title": "2 Top-K Knowledge Distillation",
            "content": "For storing KD logits, previous studies (Raman et al., 2023; Peng et al., 2024; Shum et al., 2024) have proposed to replace the full teacher distribution in knowledge distillation with sub-sampled version ts. The most intuitive way is to use only the top probability values from the teacher (\"Top-K KD\"), specifically ts = 0 otherwise, where ti are the probabilities of the token in t. Note that (cid:80) ts = ti, K, and ts = 1. Theoretically, selecting the top tokens results in the least error from the teacher distribution for single token (Appendix A.3). This may be combined with \"Top-p\" which dynamically adjusts to only keep fixed probability mass p."
        },
        {
            "title": "2.1 How Does Top-K perform compared to",
            "content": "FullKD? To study Top-K KD, we pre-train multiple LLaMA style 300M student models, while varying the number of probabilities used K. We train on web data using well pre-trained 3B teacher (full hyperparameters in Appendix F), using forward KLDivergence loss. As baseline, we use model trained with only Cross Entropy loss (\"CE\"), and as ceiling, model trained using the entire teacher distribution (\"FullKD\") to compare student performance on language modeling tasks. As seen in the table Section 2.1, Top-K training lags behind the FullKD performance on the language modeling task. If small number of Top-K tokens (< 25) are used, the student loss is worse than just than using CE loss without any distillation Only after 300 tokens does the model performance start reaching close to FullKD. Using Top-p allows for the use of fewer tokens, but performance is still only 47% of FullKD. We also measure the Expected Calibration Error (Guo et al., 2017) (\"ECE\") of these models, as prior works (Shum et al., 2024) have shown that calibration is strongly correlated with performance. Even though our teacher model is almost perfectly calibrated, we find that models trained with Top-K are strongly mis-calibrated, with calibration worsening as number of tokens (K) is being reduced. Models trained using CE and FullKD are almost perfectly calibrated, as has also been previously observed (Zhu et al., 2023; Shum et al., 2024; Hebbalaguppe et al., 2024). Unique LM % CE to ECE Tokens Loss FullKD % CE 3 5 12 25 50 *50 57 100 300 2.81 3.04 2.96 2.87 2.82 2.80 2.78 2.79 2.77 2.76 0% 395% 253% 99% 21% 5% 47% 32% 55% 77% 1.2 10.6 7.7 4.7 3.2 2.2 1.7 2.0 1.1 1.5 FullKD 2.75 100% 0.7 Table 1: Vanilla Top-K KD. The row *50 uses Top-p 0.98 with = 100. % CE to FullKD refers to the % gap covered between CE and FullKD models."
        },
        {
            "title": "2.2 Top-K KD Analysis",
            "content": "In this section, we demonstrate fundamental problems with Top-K methods."
        },
        {
            "title": "2.2.1 Up-scaled Teacher Probabilities\nSynthetic Toy Distribution: When only the Top-\nK values are kept from the teacher distribution, the\nprobabilities of the top tokens are inevitably scaled-\nup compared to the original, as the probabilities",
            "content": "(a) Visualizing Target Probabilities (b) Calibration on Synthetic Classes (c) Calibration on CIFAR-100 Figure 2: Comparing different sparse KD methods on synthetic examples (refer to Appendix B). must be normalized to sum to 1. We illustrate this in Figure 2a (see Appendix for pseudo-code), where we simulate synthetic distribution(Copy) (Copy) following Zipf distribution (Kingsley, 1935). Similar bias was also observed in previous research (Zadeh and Schmid, 2021). Gradients from KL-Divergence: When using KL-Divergence loss with Top-K KD, the non TopK tokens are pushed to probability 0 due to restriction of the target distribution to Top-K probabilities. This happens even if one does not normalize the Top-K teacher probabilities. The backward gradients result in the student effectively learning an up-scaled version of the teacher probabilities as targets, with the remaining probability divided among the Top-K tokens. Specifically, if pi and ti are teacher and student probabilities for the ith token, the gradients for the ith logit xi in FullKD are: xi = pi ti (1) But for Top-K KD, as we prove in Appendix A.4, the gradients are: xi (cid:88) = ( jK tj).pi ti (2) Appendix for pseudo-code). As seen in Figure 2b, Top-K KD leads to over-confident models, whereas CE and FullKD are almost perfectly calibrated. The same effect is observed when training toy ResNet (He et al., 2016) model on CIFAR100 (Krizhevsky et al., 2009) dataset, as shown in Figure 2c. Hence, we cannot apply KL-Divergence loss on the Top-K target distribution without explicitly handling the remaining probability."
        },
        {
            "title": "2.2.2 Missing Tail Information",
            "content": "However, only handling the problem of up-scaled teacher probabilities is not sufficient to fully recover the performance (Sections 3.1 and 3.2). In contrast to FullKD training, which utilizes the full distribution, Top-K KD discards the tail information which has been shown to be crucial for model performance (Shumailov et al., 2024). For rare ground truth tokens which fall in the tail of the teacher distribution, Top-K KD throws away the ground truth, providing poor training signal compared to CE training. The tail, even though it contains small probability mass, contains useful information and needs to somehow be learned."
        },
        {
            "title": "3 Proposed Methods",
            "content": "The student will hence be over-confident in the Top-K tokens, and under-confident for the remaining tokens. This over-confidence for the top token is indeed what we observe with top-K pre-training for LLMs (Figure 3a), causing the calibration error in Section 2.1, which worsens as is decreased. Synthetic Classification Task: This calibration error can even be observed in very simple synthetic classification task (similar to Zhang et al., 2023), where we train toy 3-layer MLP for classifying random points with Gaussian noise around class means in 128-dimensional space (see In this section, we first discuss several empirical solutions to the problems discussed above. We apply these fixes to Top-K KD, and provide the corresponding results in Section 3.1. Thereafter, we finally propose Random Sampling KD, theoretically motivated method which overcomes all the drawbacks of the previous approaches."
        },
        {
            "title": "3.1 Label Smoothing",
            "content": "A straightforward solution is to distribute the residual probability over all the classes equally. While this fixes the calibration error, smoothing leads to (a) Calibration of Pre-trained models (b) Expected Calibration Error (c) LM Loss Performance Figure 3: Comparing different sparse KD methods on Language Modeling Pre-Training significant degradation in the performance compared to Top-K KD (Section 3.1). This is expected since real-world token probabilities are not uniformly distributed and are instead hyperbolic (Zipf Kingsley, 1935). While some studies show benefits of using smoothing (Menon et al., 2021), other works (Sultan, 2023; Shum et al., 2024) also find that label smoothing under-performs in KD. Method Top-k New LM % CE to ECE FullKD % Loss Loss 0-shot Score CE Smoothing Ghost Token 2.81 2.80 2.80 - 2.85 2.77 0 73 1.2 0.4 0.4 Naive Fix: Remaining Probability to Ground Truth Top-k 1 Top-k 5 Top-k 10 Top-k 20 Top-k 50 Top-k 100 FullKD 3.37 2.96 2.88 2.83 2.80 2. 2.74 2.81 2.78 2.77 2.76 2.76 2.75 - 5 44 61 73 83 97 100 7.1 4.4 3.3 2.3 1.3 1. 0.7 40.4 41.2 42.9 41.3 42.4 42.4 42.9 42.8 43.0 42.1 Table 2: Naive Fixes for Top-K KD. Smoothing (Label Smoothing) and Ghost Token use 50 tokens."
        },
        {
            "title": "3.2 Ghost Token",
            "content": "Another method to handle residual probability would be to create \"ghost token\" which takes up the accumulated probabilities of non Top-K tokens for both the teacher and the student. We compute loss on the top tokens, between predicted probabilities pi and target ts = ti, and on the \"ghost token\" with probability pghost = 1 (cid:80) iK pi and target ts ghost = 1 (cid:80) iK ti With the ghost token, the Top-K tokens receive the same gradient as FullKD, while the remaining tokens receive gradients proportional to the student confidence (Appendix A.5). This significantly improves both the LM loss and calibration (Section 3.1). However, the performance is still worse compared to FullKD indicating that explicit supervision in the tail is essential to bridge the performance gap. 3.3 Naive Fix trivial candidate for the residual probability of the teacher is the ground truth itself. We label this method as \"Naive Fix\", where the probability of the target token is adjusted to ensure that the target probabilities sum up to 1. One can expect that this will result in probabilities more aligned to the real target (Figure 2a). This method significantly improves both performance and calibration error Section 3.1, however, it still requires 100 tokens to achieve performance comparable to FullKD. The gradients for the logits are linked to the target teacher probability (Appendix A.1 - Equation (4)). The methods above are either biased estimators of the teacher probability distribution, and/or lack adequate supervision in the tail."
        },
        {
            "title": "3.4 Random Sampling KD",
            "content": "For given probability distribution p(x), importance sampling (Elvira and Martino, 2021) allows us to obtain unbiased estimates of function (x), by sampling from different proposal distribution q(x), and reweighing the samples using the likelihood ratio p(x)/q(x). (cid:90) E[f (x)] = (x)p(x)dx = (cid:90) (x) p(x) q(x) q(x)dx If the proposal q(x) = 0 at any where p(x) = 0 (e.g., Top-K), then the estimate is no longer unbiased. While any non-zero proposal distribution q(x) can be used to obtain an unbiased estimate, under certain constraints, the proposal distribution with the lowest variance q(x) is of the form q(x) p(x)f (x) (Salakhutdinov, 2014). Motivated by these findings, we explore q(x) = p(x)t as proposal distribution, where is the sampling temperature. We sample tokens from the proposal distribution q(x) for fixed number of rounds. Each occurrence of token is assigned likelihood ratio p(x)/q(x), which is then normalized to obtain the sub-sampled target probability distribution ts. To ensure fair comparison with Top-K KD methods, the average number of unique tokens remains the same as K. The relationship between the number of sampling rounds and average number of unique tokens is provided in Appendix C. Empirically, we find that for 0.8 < < 1.2, performance does not vary significantly (Section 6.1). We hence use = 1 for simplicity in our experiments, giving us our final method Random Sampling KD."
        },
        {
            "title": "4.1 Calibration",
            "content": "The toy distribution (Figure 2a) demonstrates that our method correctly estimates teacher distribution by providing an unbiased probability estimates, It achieves perfect calibration mirroring FullKD in the synthetic classification tasks (Figure 2b), in toy classification on CIFAR-100 (Figure 2c) and in LLM pre-training (Figure 3a). As compared to the other KD methods discussed above, models trained with Random Sampling KD are much better calibrated, and using fewer tokens does not hurt the calibration (Figure 3b)."
        },
        {
            "title": "4.2 Gradient Similarity",
            "content": "In Appendix A.6, we prove that random sampling preserves the expected gradients at the logits when compared to FullKD. To further verify this empirically, we measure the gradients of the parameters of 300M model trained with FullKD for one global batch. hand, have large angles and significantly different gradient norms even at 300 tokens, compared to just 12 tokens for Random Sampling. 4.3 Variance and Bias of Sampling Methods While sampled distributions using Top-K have the least error for single token (Appendix A.3), they inherently provide biased estimate of the teacher distribution (Section 3.4). This leads to the dissimilar gradients observed in Section 4.2. While our method is always unbiased, it is also crucial for the estimator to exhibit low variance (error). Lower variance will result in better approximation of the teacher distribution and hence better gradient approximation. For example, using = 0 in our proposal (sampling uniformly across the vocabulary) causes training to diverge, as the estimate is too noisy (Section 6.1). Similarly, using fewer tokens (with = 1) will have higher error but 12 tokens seems to be sufficient (Section 5.1), and hence we use 12 tokens in the rest of our experiments."
        },
        {
            "title": "4.4 Speed/Throughput Comparison",
            "content": "In this section, we compare the speed in tokens/sec and TFlops for 300M / 3B student models with 3B / 8B teachers on 8 H100 GPUs. Our (RS-KD) caching implementation is 1.7 to 2.6 times faster than FullKD, and only slightly slower ( 10%) than CE. This overhead stems from computing the loss over the entire vocabulary for distillation compared to single ground truth token for CE. Method Tokens/sec 300M 3B TFlops 300M 3B CE Random Sampling Full KD 2.9x 2.6x 1.0x 1.77x 1.73x 1.00x 330 295 100 544 530 304 Method Angle Norm Ratio Table 4: Speed/Throughput Comparison. Top-K 12 Top-K 50 Top-K 300 Random Sampling 12 58 48 30 4 2.4 1.8 1.3 1.0 Table 3: Comparing sparse KD gradients with FullKD We find that the gradients from using Random Sampling are extremely similar to the gradients obtained from FullKD with an angular difference of 4 and the same norm. Top-K methods on the other"
        },
        {
            "title": "5 Results",
            "content": "Evaluation Tasks We evaluate our method across multiple metrics LM loss on the pretraining dataset, Expected Calibration Error, the acceptance rate on speculative decoding of the teacher model, 0-shot NLU scores (settings detailed in Appendix E.1) before and after Instruction Following training, and 0-shot NLG scores (settings detailed in Appendix E.3). 5.1 Small-Scale Results 5.2 Large-Scale Results We train LLaMA-style 300M student models using 3B teacher (hyper-parameters in Appendix F) for 10B tokens, 1.5x more than Chinchillaoptimal (Hoffmann et al., 2022) number of tokens. Our proposed method achieves very similar performance and calibration compared to FullKD, while using only 12 tokens (Section 5.1). We also measure Speculative Decoding acceptance rate, as Top-1 agreement rate with the teacher has been shown to correlate with performance (Stanton et al., 2021). We find that our method again performs comparable to FullKD. Somewhat surprisingly, as the number of unique tokens is increased, random sampling achieves marginally better performance compared to FullKD. Prior work has found that perturbing teacher logits results in better KD (Zhang et al., 2023), and we conjecture this sampling may achieve something similar. Unique 0-shot LM ECE Speculative Tokens Loss % Accept % Score CE 2.4 5.0 12.1 24.5 57.0 2.81 2.77 2.75 2.75 2.75 2.74 0. 1.0 1.1 0.8 1.1 0.9 59.95 61.47 61.83 61.85 61.93 61.97 FullKD 2.75 0. 62.02 40.4 42.1 42.6 43.0 43.1 42.9 42.1 Table 5: Random Sampling KD (3B 300M) Effect of Longer Training On extending training of the student model for 100B tokens (16x Chinchilla-optimal), our model again achieves performance comparable to FullKD, both in speculative decoding and in 0-shot NLU scores (Section 5.1). Method LM ECE Speculative 0-shot Loss % Accept % Score CE Ours FullKD 2.48 2.48 2.48 0.7 0.3 0. 64.6 65.7 65.8 45.0 46.2 46.2 Table 6: Random Sampling KD 100B toks (3B300M) In order to replicate our findings with open-source LLMs on public datasets, we train student models using the LLaMA-3-8B model on the Finewebedu (Penedo et al., 2024) dataset. First, we train 3B LLaMA-style student using 100B tokens (Section 5.2). The loss gap between Top-K KD and FullKD is much higher in this regime. On the contrary, the student trained using \"Random Sampling KD\" (12 unique tokens) achieves similar loss, calibration and speculative decoding acceptance rate with significantly better downstream and instruction following performance. The improvements observed in our smallscale experiments persist for larger models with much longer training. Method LM ECE Speculative IF SFT Loss % Accept % Score Score 0-shot CE 2.37 2.50 Top-K 12 2.40 Top-K 50 2.35 Ours (12) Ours (12)+ 2. FullKD 2.34 0.3 4.7 1.8 0.2 1.7 0.2 71. 73.0 73.1 73.2 73.5 73.4 55.6 56.6 57.1 57.5 57.9 57.5 54. 57.7 58.3 59.4 59.1 58.4 Table 7: Comparing sparse KD methods, 8B 3B 100B toks. The row Ours (12)+ is described in Section 5.3 Evaluation with LLM-as-a-judge on Generative Tasks We also evaluate the 3B models using Llama 3.1 405B Instruct (Grattafiori et al., 2024) as judge on five instruction following tasks. The student model trained with \"Random Sampling KD\" outperforms all other methods across all the evaluated tasks as seen in Section 5.2. Dataset CE Top-K Top-K Ours FullKD Dolly SelfInst Vicuna S-NI UnNI Avg 64.2 64.6 49.1 62.4 60.4 60.2 12 59.0 60.9 48.9 63.4 58.0 58. 50 65.4 63.4 53.1 62.6 58.3 60.6 12 71.3 73.1 58.2 63.8 61.4 65. 66.1 66.1 56.9 60.7 61.0 62.2 Table 8: Evaluations of 3B models on downstream generative tasks, with LLM-as-judge (8B 3B) Effect of Student Size We also vary the student sizes, training 100M, 300M, 1B and 3B all trained using LLaMA-3-8B as teacher, for 30x model-size tokens. The average performance on 0-shot downstream evaluations using \"Random Sampling KD\" over CE consistently improves as the student model size increases (Figure 4). While similar increasing trends have been previously observed for Top-K pre-training in Peng et al. (2024), they report fall in performance for smaller student models. We conjecture that this may be attributed to the issues with Top-K KD we highlight in this work. Figure 4: Downstream Improvements vs Student Size"
        },
        {
            "title": "5.3 Orthogonal Improvements to KD",
            "content": "Some orthogonal methods have also been proposed in the literature to improve the performance of FullKD. In this section, we show that these approaches can also be applied to \"Random Sampling KD\". Adding combination of KLD and CE losses is often used during training (Gunter et al., 2024; Peng et al., 2024; Zhang et al., 2024) where the final loss is defined as = αLCE +(1α)LKLD where α is the CE weight. Some prior works (Zhong et al., 2024; Zhao et al., 2022; Jiang et al., 2023; Palo et al., 2024) use different training modes for different tokens based on teachers confidence/score in the target, where higher score indicates that token is easy to learn. Setup We apply similar adaptive method to our \"Random Sampling KD\" by categorizing tokens in batch as Easy and Hard based on their target confidence percentile. Hard tokens use higher learning rate (by factor of LR Ratio) compared to the easy tokens during training, while the average LR is kept constant. We train 300M models using 3B teacher, and simultaneously vary the CE weight and the LR ratio together and report the % CE to FullKD metric. Results As seen in Section 5.3, these methods enable \"Random Sampling KD\" to surpass FullKD. The best model is achieved with 0.1 CE weight and 2.0 LR Ratio. We further apply this approach to train 3B student with 8B teacher for 100B tokens. This model (the row Ours (12)+ in Section 5.2), further improves on \"Random Sampling KD\" in LM loss, speculative acceptance rate, and 0-shot NLU scores. Caveats However, this model does not improve as much after Instruction Tuning. We conjecture that up-weighing the \"Hard\" examples in the LR tends to effectively up-weigh the tail of the distribution. This was evidenced by the relatively higher calibration error of this model we find that this model is under-confident in its predictions. While this improves the pre-training scores, it negatively impacts downstream fine-tuning of this model. LR Ratio CE Weight α 0.3 101 124 116 0.2 0.1 0. 98 95 111 111 120 121 124 125 112 1.0 1.5 2.0 Table 9: % CE to FullKD with Orthogonal Improvements to Random Sampling KD (8B 300M)"
        },
        {
            "title": "6.1 Proposal Distributions",
            "content": "Choosing the optimal sampling temperature can reduce the variance of the probability estimates, by allowing trade-off between sampling more varied tokens, vs. obtaining more accurate estimates for higher-probability tokens. While this optimal temperature would depend on the exact shape of the distribution (and hence the teacher model), numerical simulations show that [0.8, 1.2] results in the lowest variance. The post-training performance of these was also comparable (Section 6.1). While better proposal distribution may be obtained following Optimal Experimental Design (Fedorov, 2013), our sampling method performs comparable to FullKD, hence for simplicity we choose proposal with = 1.0 in this work. Sample Unique LM ECE Tokens Loss % Temp Speculative 0-shot Score Accept % 0.0 0.8 1.0 1.2 57 57 54 2.74 2.75 2.74 - 0.7 0.8 0.8 - 42.4 43.0 42.2 - 61.9 61.8 61.9 Table 10: Proposal Temperature Ablation (3B300M) 6.2 Effect of Adapting Teacher Sreenivas et al. (2024) found that if the student is being trained on data distribution different from the teachers pre-training data, the teacher should first be adapted (finetuned) on this data by training for short while. We also observe the same when training 300M student on Fineweb-edu data with the LLaMA-3-8B model as teacher, using the original teacher model directly yields only small improvement over CE (Section 6.2). After teacher adaptation for 50B tokens, this increases significantly. Method LM Loss 0-shot Score CE KD w/o adapt KD adapt 2.99 2.98 2. 40.1 40.2 41.1 Table 11: Adapting Teacher Model on Pre-training Dataset (8B 300M)"
        },
        {
            "title": "6.3 Choice of Loss/Divergence Function",
            "content": "We also experiment with alternative loss/divergence functions, by training 300M students with 8B Llama-3 teacher for 10B tokens. Some prior works (Kim et al., 2021; Wu et al., 2024b; Gu et al., 2023; Ko et al., 2024) find alternative objectives such as Reverse KL Divergence, Mean Squared Error as superior, while other works (Sultan, 2023; Wen et al., 2023; Muralidharan et al., 2024; Peng et al., 2024) have observed the opposite. In Section 6.3, we observe that vanilla forward KLD outperforms other objectives. Metric CE L1 MSE KLD LM Loss 2.81 5. 3.37 2.78 2.75 F+R Table 12: Loss Ablation. and in KLD refer to forward and reverse KLD respectively."
        },
        {
            "title": "7 Related Work",
            "content": "Knowledge Distillation (Hinton et al., 2015) has often been used to improve smaller LLMs (Jiao et al., 2020; Sanh et al., 2019; Sreenivas et al., 2024; Muralidharan et al., 2024; Wang et al., 2021; Gu et al., 2023; Boizard et al., 2024). Many works focus on using teacher models for dataset generation/filtering (Kim and Rush, 2016; Zhang et al., 2023; Wen et al., 2023; Gunasekar et al., 2023; Jiang et al., 2023; Gu et al., 2024; Palo et al., 2024). These methods are somewhat complementary to our method our work is agnostic to the source of the pre-training data corpus, and focuses on distilling the teacher models logits on this data. Similar to our work, Shum et al. (2024) stores the Top-5 teacher probabilities from an LLM for training smaller students. They also observe that distillation with Top-K tokens leads to over-confident students which they solve by employing temperature scaling. By sampling from the teacher distribution, our method offers principled approach of achieving calibrated student (Figure 3b). While they observe mis-calibration of their teacher as well, pretrained LLMs are well-calibrated, but alignment may degrade this calibration (Zhu et al., 2023; Hebbalaguppe et al., 2024). We find both our 3B as well as Llama 8B teachers well calibrated, as they are not instruction-tuned models. Closest to our work are Raman et al. (2023) and Peng et al. (2024). Raman et al. (2023) also observe that distillation improves student model performance but they store Top-5% of the teacher logits, which is prohibitively large for modern LLMs (6400 for the Llama3 model) we successfully bring this down to 12 logits in this work. Peng et al. (2024) explores caching teacher logits in Knowledge Distillation in pre-training of LLMs utilizing Top-K with Top-p. They also conclude that forward KLD outperforms other objectives, adding CE loss improves distillation, and increasing performance improvement on scaling the model size and pre-training corpus. However, they observe fall in performance on smaller students vanilla Top-K may reduce model performance if is not large enough as we show in Section 2.1. Our method remedies this issue, matching FullKD with significantly sparser tokens."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we identified key issues of bias and tail supervision with sparse teacher logits for Knowledge Distillation. We theoretically proved and empirically verified these claims in both synthetic and real-world scenarios, and proposed an importance-sampling based method to rectify them. By preserving gradients and logits distribution in expectation, we enable significantly sparser logits than prior methods. Our method maintains model performance while utilizing only 0.01% of precomputed teacher logits, across range of model sizes, training tokens, and evaluation metrics."
        },
        {
            "title": "Limitations",
            "content": "Due to limited compute resources, we were only able to experiment upto 3B scale models trained for 100B tokens. Training longer with larger models should be explored, but our experiments indicate the benefits of our model only increase with model scale. Representation matching, which distills intermediate activations from the teacher, may improve distillation further. However, caching teacher representations due to limited compute resources was primary requirement for this work, which rendered representation matching infeasible. Lastly, more sophisticated sampling schemes can also be explored, but we did not attempt that as our methods already achieved the desired outcome of matching full KD with low storage requirements."
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023. On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes. Sungsoo Ahn, Shell Xu Hu, Andreas C. Damianou, Neil D. Lawrence, and Zhenwen Dai. 2019. Variational information distillation for knowledge transfer. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 91639171. Computer Vision Foundation / IEEE. allenai. 2025. allenai/olmo-2-hard-coded Datasets at Hugging Face. Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. 2020. Uninformed students: Studentteacher anomaly detection with discriminative latent embeddings. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 4182 4191. IEEE. Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432 7439. AAAI Press. Nicolas Boizard, Kevin El Haddad, Céline Hudelot, and Pierre Colombo. 2024. Towards Cross-Tokenizer Distillation: The Universal Logit Distillation Loss for LLMs. Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, Yulia Tsvetkov, Noah A. Smith, Yejin Choi, and Hanna Hajishirzi. 2025. The Art of Saying No: Contextual Noncompliance in Language Models. Advances in Neural Information Processing Systems, 37:4970649748. Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, and Tagyoung Chung. 2024. REAL Sampling: Boosting Factuality and Diversity of Open-Ended Generation via Asymptotic Entropy. Yevgen Chebotar and Austin Waters. 2016. Distilling knowledge from ensembles of neural networks for speech recognition. In Interspeech 2016, 17th Annual Conference of the International Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, pages 34393443. ISCA. Zhihao Chi, Tu Zheng, Hengjia Li, Zheng Yang, Boxi Wu, Binbin Lin, and Deng Cai. 2023. NormKD: Normalized Logits for Knowledge Distillation. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An OpenSource Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free Dolly: Introducing the Worlds First Truly Open InstructionTuned LLM. Jiequan Cui, Zhuotao Tian, Zhisheng Zhong, Xiaojuan Qi, Bei Yu, and Hanwang Zhang. 2023. Decoupled Kullback-Leibler Divergence Loss. Víctor Elvira and Luca Martino. 2021. Advances in Importance Sampling. V. V. Fedorov. 2013. Theory Of Optimal Experiments. Elsevier. Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata, Samuel Thomas, Jia Cui, and Bhuvana Ramabhadran. 2017. Efficient knowledge distillation from an ensemble of teachers. In Interspeech 2017, 18th Annual Conference of the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017, pages 36973701. ISCA. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. framework for few-shot language model evaluation. Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021. Knowledge Distillation: Survey. International Journal of Computer Vision, 129(6):17891819. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. MiniLLM: Knowledge Distillation of Large Language Models. In The Twelfth International Conference on Learning Representations. Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. MiniPLM: Knowledge Distillation for Pre-Training Language Models. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks Are All You Need. Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, and 136 others. 2024. Apple Intelligence Foundation Language Models. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 13211330. PMLR. Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. 2024. WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770778. IEEE Computer Society. Ramya Hebbalaguppe, Mayank Baranwal, Jatin Prakash, Neelabh Madan, Kartik Anand, and Chetan Arora. 2024. Understanding Calibration Transfer in Knowledge Distillation. OpenReview Preprint. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in Neural Network. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, and 3 others. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2023. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1440914428, Toronto, Canada. Association for Computational Linguistics. Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. 2022. Knowledge distillation from stronger In Advances in Neural Information Proteacher. cessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Brian Kenji Iwana, Ryohei Kuroki, and Seiichi Uchida. 2019. Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 4176 4185. IEEE. Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 110, Beijing, China. Association for Computational Linguistics. Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, and Minlie Huang. 2023. Tailoring language generation models under total variation distance. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, and Nouha Dziri. 2024. WildTeaming at Scale: From In-theWild Jailbreaks to (Adversarially) Safer Language Models. Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. 2023. Lion: Adversarial distillation of proprietary large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31343154, Singapore. Association for Computational Linguistics. Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, and 4 others. 2024b. Tulu 3: Pushing Frontiers in Open Language Model Post-Training. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163 4174, Online. Association for Computational Linguistics. Taehyeon Kim, Jaehoon Oh, Nakyil Kim, Sangwook Cho, and Se-Young Yun. 2021. Comparing kullbackleibler divergence and mean squared error loss in knowledge distillation. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 26282635. ijcai.org. Yoon Kim and Alexander M. Rush. 2016. Sequencelevel knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 13171327, Austin, Texas. Association for Computational Linguistics. Zipf George Kingsley. 1935. The psycho-biology of language: an introduction to dynamic philology. Houghton Mifflin. Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and SeYoung Yun. 2024. DistiLLM: Towards Streamlined Distillation for Large Language Models. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023. Openassistant conversations - democratizing large language model alignment. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Alex Krizhevsky, Geoffrey Hinton, and 1 others. 2009. Learning multiple layers of features from tiny images. \" \". Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, and 4 others. 2024a. Tulu 3: Pushing Frontiers in Open Language Model Post-Training. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, and others. 2024a. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9. Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2024b. TableGPT: Table Fine-tuned GPT for Diverse Table Tasks. Proc. ACM Manag. Data, 2(3). Chengyuan Liu, Fubang Zhao, Kun Kuang, Yangyang Kang, Zhuoren Jiang, Changlong Sun, and Fei Wu. 2024. Evolving knowledge distillation with large language models and active learning. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 67176731, Torino, Italia. ELRA and ICCL. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with EvolInstruct. Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Seungyeon Kim, and Sanjiv Kumar. 2021. statistical perspective on distillation. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 76327642. PMLR. Roy Miles and Krystian Mikolajczyk. 2024. Understanding the role of the projector in knowledge distillation. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 4233 4241. AAAI Press. Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. Compact Language Models via Pruning and Knowledge Distillation. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, and 21 others. 2025. 2 OLMo 2 Furious. Flavio Di Palo, Prateek Singhi, and Bilal Fadlallah. 2024. Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 36753687, Miami, Florida, USA. Association for Computational Linguistics. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15251534, Berlin, Germany. Association for Computational Linguistics. Sangun Park. 2012. Generalized Kullback-Leibler information and its extensions to censored and discrete cases. Journal of the Korean Data and Information Science Society, 23(6):12231229. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. Hao Peng, Xin Lv, Yushi Bai, Zijun Yao, Jiajie Zhang, Lei Hou, and Juanzi Li. 2024. Pre-training Distillation for Large Language Models: Design Space Exploration. Dennis Prangle and Cecilia Viscardi. 2019. Distilling Importance Sampling for Likelihood Free Inference. Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander M. Rush, and Thomas Wolf. 2023. No Robots. Mrigank Raman, Pranav Mani, Davis Liang, and Zachary Lipton. 2023. For Distillation, Tokens Are Not All You Need. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. Ruslan Salakhutdinov. 2014. Deep learning. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 14, New York, NY, USA - August 24 - 27, 2014, page 1973. ACM. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. Arash Shahriari. 2017. Unified backpropagation for ArXiv preprint, multi-objective deep learning. abs/1710.07438. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. KaShun Shum, Minrui Xu, Jianshu Zhang, Zixin Chen, Shizhe Diao, Hanze Dong, Jipeng Zhang, and Muhammad Omer Raza. 2024. FIRST: Teach Reliable Large Language Model Through Efficient Trustworthy Distillation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1264612659, Miami, Florida, USA. Association for Computational Linguistics. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. 2024. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, and 14 others. 2024. Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning. Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. LLM Pruning and Distillation in Practice: The Minitron Approach. Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A. Alemi, and Andrew Gordon Wilson. 2021. Does knowledge distillation really work? In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 614, 2021, virtual, pages 69066919. Md Sultan. 2023. Knowledge Distillation approx Label Smoothing: Fact or Fallacy? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 44694477, Singapore. Association for Computational Linguistics. Shangquan Sun, Wenqi Ren, Jingzhi Li, Rui Wang, and Xiaochun Cao. 2024. Logit Standardization in Knowledge Distillation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1573115740. Akio Suzukawa, Hideyuki Imai, and Yoshiharu Sato. 2001. Kullback-Leibler Information Consistent Estimation for Censored Data. Annals of the Institute of Statistical Mathematics, 53(2):262276. Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive representation distillation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Takuma Udagawa, Aashka Trivedi, Michele Merler, and Bishwaranjan Bhattacharjee. 2023. comparative analysis of task-agnostic distillation methods for compressing transformer language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 2031, Singapore. Association for Computational Linguistics. International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, and Arman Cohan. 2024. SciRIFF: Resource to Enhance Language Model Instruction-Following over Scientific Literature. Abdul Waheed, Karima Kadaoui, and Muhammad Abdul-Mageed. 2024. To Distill or Not to Distill? On the Robustness of Robust Knowledge Distillation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1260312621, Bangkok, Thailand. Association for Computational Linguistics. Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. 2021. MiniLMv2: Multi-head selfattention relation distillation for compressing pretrained transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 21402151, Online. Association for Computational Linguistics. Xinpeng Wang, Leonie Weissweiler, Hinrich Schütze, and Barbara Plank. 2023a. How to distill your BERT: An empirical study on the impact of weight initialisation and distillation objectives. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 18431852, Toronto, Canada. Association for Computational Linguistics. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada. Association for Computational Linguistics. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, and 16 others. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 50855109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. 2023. f-divergence minimization for sequence-level knowlIn Proceedings of the 61st Anedge distillation. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10817 10834, Toronto, Canada. Association for Computational Linguistics. Cindy Wu, Ekdeep Singh Lubana, Bruno Kacper Mlodozeniec, Robert Kirk, and David Krueger. 2024a. What Mechanisms Does Knowledge Distillation Distill? In Proceedings of UniReps: The First Workshop on Unifying Representations in Neural Models, pages 6075. PMLR. Siyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang, and Rui Wang. 2023. AD-KD: Attribution-driven knowledge distillation for language model compression. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84498465, Toronto, Canada. Association for Computational Linguistics. Taiqiang Wu, Chaofan Tao, Jiahao Wang, Runming Yang, Zhe Zhao, and Ngai Wong. 2024b. Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models. Sang Michael Xie and Stefano Ermon. 2019. Reparameterizable subset sampling via continuous relaxations. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 39193925. ijcai.org. Wenda Xu, Rujun Han, Zifeng Wang, Long T. Le, Dhruv Madeka, Lei Li, William Yang Wang, Rishabh Agarwal, Chen-Yu Lee, and Tomas Pfister. 2024a. Speculative Knowledge Distillation: Bridging the TeacherStudent Gap Through Interleaved Sampling. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. 2024b. Survey on Knowledge Distillation of Large Language Models. Shekoufeh Gorgi Zadeh and Matthias Schmid. 2021. Bias in Cross-Entropy-Based Training of Deep Survival Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(9):31263137. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In The Tenth Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, and Chao Zhang. 2023. Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation. Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, and Jinan Xu. 2024. Dual-Space Knowledge Distillation for Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1816418181, Miami, Florida, USA. Association for Computational Linguistics. Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, and Yichen Wei. 2020. Prime-Aware Adaptive Distillation. Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. 2022. Decoupled knowledge distillation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1195311962. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. WildChat: 1M ChatGPT Interaction Logs in the Wild. In The Twelfth International Conference on Learning Representations. Qihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du, and Dacheng Tao. 2024. Revisiting Knowledge Distillation for Autoregressive Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1090010913, Bangkok, Thailand. Association for Computational Linguistics. Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang. 2021. Rethinking soft labels for knowledge distillation: bias-variance tradeoff perspective. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Qinhong Zhou, Zonghan Yang, Peng Li, and Yang Liu. 2023a. Bridging the gap between decision and logits in decision-based knowledge distillation for pretrained language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13234 13248, Toronto, Canada. Association for Computational Linguistics. Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. 2023b. DistillSpec: Improving Speculative Decoding via Knowledge Distillation. Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, and Zhendong Mao. 2023. On the calibration of large language models and alignment. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 97789795, Singapore. Association for Computational Linguistics."
        },
        {
            "title": "A Proofs",
            "content": "A.1 Backward Gradient through Softmax-KL Divergence Loss The output probability is defined in terms of the models logits = Softmax(x) exi (cid:80)V j=1 exj pi = The gradient through Softmax (Iwana et al., 2019) is: pi xj = pi.(1{i = j} pj) Given target probability distribution t, the KL divergence loss is defined as: = (cid:88) i=1 ti log ti pi (3) For Softmax-KL Divergence Loss, the gradient flowing to the jth logit xj can be calculated as follows: xj = (cid:88) i=1 ti 1 pi pi xj = (cid:88) i=1 ti.(pj 1{i = j}) (cid:88) = ( i=1 ti).pj tj (cid:80)V If the full teacher distribution is provided i=1 ti = 1. However, in the most generalized form, the gradient through Softmax-KL divergence loss can be written as: xj (cid:88) = ( i=1 ti).pj tj (4) A.2 Cross Entropy Loss The cross entropy loss defined as follows: LCE = (cid:88) i=1 ti log pi = LKLD (cid:88) i=1 ti log ti Compared to the KLD loss, the additional term ((cid:80)V i=1 tilogti) is independent of the student model. Hence, the gradient for CE loss remains the same as that computed for KL Divergence loss in Equation (3). For cross entropy (and similarly for FullKD with KLD loss), (cid:80)V i=1 ti = 1. Hence, the gradient can be further simplified to: xj = pj tj In this case, the theoretical optima lies at the point where the predicted probabilities become same as target probabilities across the vocabulary, resulting in 0 gradient and minimum loss. A.3 Vanilla Top-K has the Least L1 Error For given distribution t, if only probabilities from must be kept, and they are then normalized to sum to 1, we show that selecting the Top probabilities results in the least L1 error. (cid:80) Let be the set of tokens selected. Let = jK tj. This can be viewed as constructing new distribution v, where normalizing the probabilities , K, vi = ti vi = 0, Then the L1 error between and is L1 = = (cid:88) (cid:88) iK ti vi ti ti/a + (cid:88) iK ti 0 = (1/a 1) (cid:88) iK ti + (1 (cid:88) iK ti) = (1/a 1) + (1 a) = 2 (1 a) Hence L1 will be minimized when is the largest, which will happen when the largest probabilities are selected A.4 Vanilla Top-K KD provides scaled teacher as target We can restrict the target probability to subset of tokens in our vocabulary. If we select as the set of tokens with top-k probabilities, then the loss is defined as follows: This can be viewed as zeroing out the non-top-k target probabilities in the original KLD loss. In this case, the gradient flowing to the logits are (Equation (4)): xj (cid:88) = ( iK ti).pj tj (5) If / K, the gradient is ((cid:80) iK ti).pj . As opposed to the previous case, models optima lies at the point where non-top-k probabilities are 0 and the top-k predicted probabilities are scaled up version of the target probabilities across the ti top-k tokens, pi = jK tj ) . At this optima, the gradient is 0 (but the loss is negative). ((cid:80) A.5 Ghost Token Backward One possible solution to the above discussed problem is to add ghost token which accounts for the remainder of the probability. This ghost token ensures that the sum of probability outside the top-k region is exactly the same for the teacher and student. Ideally, it would ensure that the top-k tokens receive the exact teacher probability as the target. The modified loss function is written below- (cid:32) = (cid:88) iK ti log ti pi + (1 (cid:88) iK ti)log (cid:16) 1 (cid:80) 1 (cid:80) iK ti iK pi (cid:33) (cid:17)"
        },
        {
            "title": "Let us consider the second term in the loss and",
            "content": "find its gradient Lghost = (1 (cid:88) ti)log (cid:16) 1 (cid:80) 1 (cid:80) iK ti iK pi (cid:17) iK (cid:16) 1 (cid:80) 1 (cid:80) Lghost xj = (cid:17) iK ti iK pi (cid:88) (cid:17) . (cid:88) . i=1 pi xj pi.(1{i = j} pj) = (cid:16) 1 (cid:80) 1 (cid:80) iK ti iK pi i=1 The gradient becomes: Lghost xj = (cid:40)(cid:0)1 (cid:80) (cid:16) 1(cid:80) 1(cid:80) (cid:1) pj iK ti (cid:17) iK ti pj iK pi K, (cid:80) iK pi else. = (cid:88) iK ti log ti pi Next we can add the gradient from top-k KD loss Equation (5) and ghost token loss to obtain the final gradient (cid:40)(pj tj) (cid:16) (cid:80) = (cid:17) xj iK (tipi) iK pi For the non top-k tokens, the gradients can be else. 1(cid:80) pj K, rewritten as (cid:17) = pj pj / iK ti iK pi Lghost xj (cid:16) 1 (cid:80) 1 (cid:80) By adding the ghost token, the top-k tokens get the same gradient as KLD loss with FullKD, while the remaining tokens receive gradient in proportion of their predicted probability pi. The target probpj. In ability for non top-k tokens is this case, if the predicted probability distribution is exactly the same as that of teacher probability only for top-k tokens, the gradient becomes 0 and loss becomes minimum. (cid:16) 1(cid:80) 1(cid:80) iK ti iK pi (cid:17) A.6 Unbiased Sampling preserves gradients in expectation For any partial knowledge distillation scheme which sub-samples the full distribution, the expected gradients at the logits will be preserved in expectation if sampling is unbiased. Proof: The gradient gj for the logit xj through the softmax-KL divergence loss is (replacing (cid:80)V i=1 ti = 1 in Equation (4))) gj = pj tj (6)"
        },
        {
            "title": "Taking expectations on both sides",
            "content": "E[gj] = E[pj] E[tj] Similarly, for sub-sampling method which reduced ts, expected gradient is as follows E[gs ] = E[pj] E[ts j] The gradients at the logits are preserved in exj] and the sub-sampling pectation if E[tj] = E[ts process is unbiased."
        },
        {
            "title": "Visualizing Target Probabilities We generate a\nZipf distribution where the probability of ith token",
            "content": "1 is proportional . Next we select tokens and assign them probabilities based on different sparse knowledge distillation methods. We plot these probabilities with the ground truth FullKD probabilities to visualize the alignment of sparse KD target distributions with FullKD. Calibration on Synthetic Classes As discussed in the main paper and the psuedocode (Appendix K), we generate synthetic data by generating random points around randomly chosen class means with Gaussian error distribution. We use simple 3-layer MLP as our model. We train the model using different sparse KD techniques and FullKD and plot the mean accuracy after binning the probabilities. Calibration on CIFAR-100 We follow the exact same methodology as the synthetic classification while using CIFAR-100 task and weaker/smaller version of ResNet-18 model."
        },
        {
            "title": "Number of Effective Tokens",
            "content": "For fair comparison between Top-K KD and random sampling methods, the number of sampling rounds were chosen such that the number of unique tokens sampled match K. The relationship between the two for pre-training data is shown in Figure 5 (log-log scale), and is almost perfectly linear, showing an approximate power-law relationship. Figure 5: Number of unique tokens sampled vs sampling rounds"
        },
        {
            "title": "D Implementation Concerns",
            "content": "D.1 Quantization for Teacher Probabilities For our vocab size = 100000, our token ids require log2(V ) = 17 bits. We store the bytealigned data, which leaves us with 24 17 = 7 bits for teacher probabilities. As probabilities are in range 0..1, for Top-K method, we use the 7 bits to split the 0 to 1 range into 27 equal intervals. This resulted in slightly lower performance compared to storing the probabilities in fp16. Instead switching to ratio encoding with sorted Top-K probabilities resulted in significantly reduced quantization error to almost 0, and results matched that of using unquantized probabilities. In the case of our proposed random sampling, we use 50 sampling rounds, so token probabilities can only be of the form x/50, where is some integer. As this is less than 27, we can store all of these exactly in 7 bits by only storing the numerator. If sampling rounds are increased beyond 128, ratio encoding with sorted probabilities can be used instead. D.2 Efficiency Concerns Naively implementing the sampling and the loss calculation incurred significant memory usage, due to the large vocabulary size. Manual backward and forward for the softmax KLD needed to implemented (via plain Pytorch, custom kernels were not created). Writing and reading the logits needed to be streamlines via shared memory ring buffers and async writer processes, so as to not block the GPU. D.3 Aligning Teacher and Student Sequences In our pre-training, we pack shuffled training documents to maximum sequence length, but we do not mask attention across document boundaries due to efficiency reasons. In our initial implementation, different shuffling seed was used between the teacher (during inference)and student (during training) This resulted in the prefix-context of tokens seen by the teacher and student not being aligned after the first document boundary. This had surprisingly large effect on student model performance, particularly if smaller sequence lengths were used during teacher inference. We conjecture that with longer sequence lengths, far-away tokens from other documents will have less of an impact on the distribution of the logits. After fully aligning the teacher and student sequences, this effect was eliminated, and the offline run was within random error of the online run."
        },
        {
            "title": "E Downstream Evaluation Details",
            "content": "E.1 Natural Language Understanding We evaluate the downstream natural language understanding performance of our trained models using the following benchmarks: HellaSwag (Zellers et al., 2019), Arc-Easy (Clark et al., 2018), LAMBADA (Paperno et al., 2016), and PiQA (Bisk Shuffle Seeds Seq Len LM Loss % CE to online Different Different Same 1024 4096 4096 2.760 2.753 2. 79 90 96 Table 13: Effect of aligning teacher and student sequences, with different/same shuffle seeds and sequence length of the teacher during inference. The last column shows the performance of the offline (cached) implementation relative to an online implementation, where the entire teacher model is run. et al., 2020). We conduct zero-shot evaluation of all benchmarks using LM-Eval-Harness (Gao et al., 2024). In the main paper, we report the average scores obtained across these tasks. E.2 Supervised Finetuning for Instruction Following We used the Olmo2 (OLMo et al., 2025) version of the Tulu (Lambert et al., 2024a) Instruction Following dataset for SFT training after Language Modeling pre-training. E."
        },
        {
            "title": "Instruction Following Evaluation",
            "content": "Similar to Gu et al. (2023), we evaluate the ability of fine-tuned models to follow instructions on five datasets: DollyEval (Conover et al., 2023): 15k humanwritten instruction-response pairs. Following Gu et al. (2023), we use the 500-sample test set for evaluation. SelfInst (Wang et al., 2023b): user-oriented instruction following dataset containing 252 samples. VicunaEval (Chiang et al., 2023): 80 diverse and challenging question-answer pairs. S-NI: The test set of Supernatural Instruction (Wang et al., 2022). We sample 1694 pairs whose ground-truth response length is longer than 11. UnNI: 10k subset of Unnatural Instruction (Honovich et al., 2023). Similar to SNI, we only use pairs where the ground-truth length is longer than 11. We adopt the LLM-as-a-Judge approach, where we use Llama 3.1 405B Instruct (Grattafiori et al., 2024) to score the quality of model responses. For each instruction, we generate the response five times using different seeds and temperature = 1. We prompt the judge model to rate both the groundtruth response and the model-generated response on scale of 1-10, and use the average ratio of the total score of the ground-truth and model-generated responses as the final score. Hyper-parameters The hyper-paramerters for our experiments are described in Appendix Parameters Values Optimizer β1, β2 Effective Batch Size Drop-out (p) Sequence Length Train Iters Learning rate Schedule LR Decay Iterations Warmup steps Min LR Gradient clipping Adam 0.9, 0.95 1024 0.0 1024 10, 000 4 104 Cosine / Constant 100% 4% 4 105 1.0 Table 14: Pre-Training Hyper-Parameters for 300M model. The pre-training dataset was web data, primarily Fineweb-Edu."
        },
        {
            "title": "Parameters",
            "content": "Optimizer β1, β2 Effective Batch Size Drop-out (p) Sequence Length Train Iters Learning rate Schedule LR Decay Iterations Warmup steps Min LR Gradient clipping"
        },
        {
            "title": "Values",
            "content": "Adam 0.9, 0.95 1024 0.0 4096 10, 000 3 104 Cosine 100% 4% 3 105 1.0 Table 15: Training Hyper-Parameters for 3B Llama model Parameters Optimizer β1, β2 Effective Batch Size Drop-out (p) Sequence Length Train Iters Learning rate Schedule LR Decay Iterations Warmup steps Min LR Gradient clipping Values Adam 0.9, 0.95 256 0.0 4096 1, 234 2 105 Cosine 100% 3% 2 106 1. Table 16: SFT Hyper-Parameters for 3B Llama model Parameters 300M Model 3B Model Num Layers Hidden Size FFN Hidden Size Num Attn Heads Num Query Groups 24 1024 2816 8 8/ 28 3072 8192 24 8 Table 17: Student Model Architecture Details. The 100B experiments for 300M model used 4 query groups for efficiency. The pre-training dataset was FineWebEdu (Penedo et al., 2024)"
        },
        {
            "title": "G Package versions",
            "content": "Versions of packages used are described in Appendix G."
        },
        {
            "title": "H Computational Resources",
            "content": "All experiments were carried out on nodes with 8 Nvidia H100 GPUs with 80Gb memory. Most experiments utilized one node or less, while the large scale ones used 2 4 nodes."
        },
        {
            "title": "I Use of AI Assistants",
            "content": "AI assistants were consulted while writing small fraction of the code for this work, but their work was carefully checked, and the majority of the code was handwritten. AI assistants were not used in writing the text of this paper."
        },
        {
            "title": "J Artifacts",
            "content": "We use LLaMA-3-8B (Grattafiori et al., 2024) as the teacher for some of experiments. We also used the Llama-3.1-405b as judge for evaluation. Both Package Version megatron deepspeed flash_attn safetensors scikit-learn scipy sentencepiece torch transformer_engine transformers 0.7.0 0.15.3 2.4.2 0.4.5 1.5.2 1.14.0 0.2.0 2.5.0 1.11.0 4.46.1 Table 18: Package Versions for Pre-training of these uses are permitted under the license of these models. The datasets used here are also permitted for research use, and were only used for research. The pre-training dataset FinewebEdu (Penedo et al., 2024) is primarily composed of English educational-style web data, and so is the SFT data Tulu (Lambert et al., 2024a). Pseudo-code The pseudocode for topk sampling and random sampling approaches is provided below. import torch ## Create downsampled probabilities def create_prob(values, indices, probs): downsampled_probs = torch.zeros_like(probs) downsampled_probs.scatter_(1, indices, values) return downsampled_probs ## Downsampling Functions def downsample_topk(probs, k=50): # Top-k topk_values, topk_indices = probs.topk(k) return create_prob(topk_values, topk_indices, probs) def downsample_ours(probs, N=50): # Sampling sampled_indices = torch.multinomial(probs, N, replacement=True) prob_value = 1.0 / values = torch.full((probs.size(0), N), prob_value, device=probs.device) return create_prob(values, sampled_indices, probs) ## Knowledge distillation loss def distillation_loss(student_logits, teacher_probs, downsample_fn): # Downsample teacher distribution downsampled_teacher_probs = downsample_fn(teacher_probs) # Compute KL divergence loss = torch.nn.functional.kl_div( torch.nn.functional.log_softmax(student_logits, dim=-1), downsampled_teacher_probs, ) return loss ## Training step def train_step(inputs, labels, teacher_model, student_model, downsample_fn, alpha=0.5): # Forward pass through teacher and student with torch.no_grad(): teacher_logits = teacher_model(inputs) teacher_probs = torch.nn.functional.softmax(teacher_logits, dim=-1) student_logits = student_model(inputs) # Compute standard cross-entropy loss ce_loss = torch.nn.functional.cross_entropy(student_logits, labels) # Compute distillation loss kd_loss = distillation_loss(student_logits, teacher_probs, downsample_fn) # Combine losses total_loss = alpha * kd_loss + (1 - alpha) * ce_loss return total_loss The pseudocode for running different sampling strategies on toy distribution. # Set random seed for reproducibility np.random.seed(12345) # Configuration parameters VOCAB_SIZE = 100000 TOP_K = 20 NUM_SAMPLES = 22 NUM_SAMPLING_ROUNDS = 1000 Y_MAX = 50 # Create synthetic data distribution def create_synthetic_data(vocab_size): idx = np.array(range(1, vocab_size + 1)) data_dist = 1 / idx data_dist /= np.sum(data_dist) # Normalize to sum to 1 return idx, data_dist # Generate data idx, data_dist = create_synthetic_data(VOCAB_SIZE) # Top-K method def apply_top_k(data_dist, idx, top_k): top_k_probs = data_dist[:top_k] top_k_probs_redistributed = top_k_probs / np.sum(top_k_probs) # top_k_probs_redistributed = top_k_probs # Create top-k distribution with small offset for visualization top_k_dist = np.zeros_like(data_dist) top_k_dist[:top_k] = top_k_probs_redistributed top_k_dist = list(top_k_dist[:top_k]) + [0] + list(top_k_dist[top_k:]) return top_k_dist data_dist_top_k = apply_top_k(data_dist, idx, TOP_K) # Naive fix method def apply_naive_fix(data_dist, idx, top_k): naive_fix_dist = np.zeros_like(data_dist) naive_fix_dist[:top_k] = data_dist[:top_k] naive_fix_dist += data_dist * (1 - np.sum(naive_fix_dist)) return naive_fix_dist data_dist_remaining_gt = apply_naive_fix(data_dist, idx, TOP_K) # Random sampling method def apply_random_sampling(data_dist, num_samples, num_rounds): random_sampling_dist = np.zeros_like(data_dist) num_samples_effective = 0 for _ in range(num_rounds): current_dist = np.zeros_like(data_dist) samples = np.random.choice(len(data_dist), size=num_samples, p=data_dist) for in samples: current_dist[i] += num_samples_effective += np.count_nonzero(current_dist) current_dist /= num_samples random_sampling_dist += current_dist num_samples_effective /= num_rounds random_sampling_dist /= np.sum(random_sampling_dist) return random_sampling_dist, num_samples_effective data_dist_random_sampling, num_samples_effective = apply_random_sampling(data_dist, NUM_SAMPLES, NUM_SAMPLING_ROUNDS) def plot_probability_distributions(LINE_WIDTH=2.0, MARKER_SIZE=3): plt.plot(idx[:Y_MAX], data_dist[:Y_MAX], label='Ground Truth', color='purple', linewidth=LINE_WIDTH, marker='o', markersize=MARKER_SIZE) # Plot Top-K distribution idx_topk = list(idx[:TOP_K]) + list(idx[TOP_K:]) data_dist_top_k_truncated = list(data_dist_top_k[:TOP_K]) plt.plot(idx_topk[:Y_MAX+1], data_dist_top_k_truncated[:Y_MAX+1], + list(data_dist_top_k[TOP_K:]) label='Top-K (k=20)', color='royalblue', linewidth=LINE_WIDTH, marker='o', markersize=MARKER_SIZE) plt.plot(idx[:Y_MAX], data_dist_remaining_gt[:Y_MAX], label='Naive Fix', color='darkgoldenrod', linewidth=LINE_WIDTH, marker='o', markersize=MARKER_SIZE) plt.plot(idx[:Y_MAX], data_dist_random_sampling[:Y_MAX], label='Random Sampling', color='salmon', linewidth=LINE_WIDTH, marker='o', markersize=MARKER_SIZE) # Add plot details plt.ylim(-0.002, 0.15) plt.legend(fontsize=12, framealpha=0.6) plt.xticks(fontsize=11) plt.yticks(fontsize=11) plt.grid() plt.xlabel(r'Token Index $rightarrow$', fontsize=14) plt.ylabel(r'Teacher Probability $rightarrow$', fontsize=14) plt.savefig(\"images/image.png\", dpi=600, bbox_inches='tight') plot_probability_distributions() print(f\"Effective number of samples: {num_samples_effective:.2f}\") The pseudocode for running different top-k strategies on synthetic classification task. torch.random.manual_seed(1234) torch.set_default_dtype(torch.float64) device='cuda' num_classes = 1024 sigma = 1.5 num_dim = 128 num_hidden_teacher = 128 num_hidden_student = 96 class_centers = torch.rand((num_classes, num_dim), device=device) class_sigma = torch.unsqueeze(torch.rand((num_classes, ), device=device), dim=-1) * sigma class_indices = torch.tensor(range(num_classes), device=device) num_calibration_batches = 100 def get_batch(batch_size=4096): idx = torch.randint(low=0, high=num_classes, size=(batch_size,), device=device) class_centers_batch = class_centers[idx] class_sigma_batch = class_sigma[idx] batch = class_centers_batch + torch.randn((batch_size, num_dim), device=device)*class_sigma_batch return batch, idx def eval(model, method): all_probs = [] all_acc = [] with torch.no_grad(): for in tqdm(range(num_calibration_batches)): model.eval() batch, labels = get_batch() probs = model(batch) probs = torch.nn.functional.softmax(probs, dim=-1) all_probs.append(torch.max(probs, dim=-1)[0]) all_acc.append(torch.argmax(probs, dim=-1).detach() == labels) all_probs = torch.vstack(all_probs) all_acc = torch.vstack(all_acc) print(f'Accuracy for {method}', all_acc.float().mean().item()*100) def train(model, method, teacher=None, lr=2e-3, num_rounds=20000, **kwargs): optimizer = torch.optim.AdamW(params = model.parameters(), lr=lr, weight_decay=0.00) for step in tqdm(range(num_rounds)): optimizer.zero_grad() batch, labels = get_batch() logits = model(batch) if teacher: teacher.eval() logits_teacher = teacher(batch) probs_teacher = torch.nn.functional.softmax(logits_teacher, dim=-1).detach() loss = loss_kd(logits, probs_teacher, method, **kwargs) else: loss = torch.nn.functional.cross_entropy(logits, labels) loss.backward() optimizer.step() eval(model, method) return model def loss_kd(logits, probs_teacher, method, topk=7, to_sample=50): if \"topk\" in method: topk_probs, topk_ids = probs_teacher.topk(topk, dim=-1) probs_teacher *= 0 probs_teacher.scatter_reduce_(dim=-1, index=topk_ids, src=topk_probs, reduce='sum') elif \"random_sampling\" in method: probs_teacher_cumsum = probs_teacher.cumsum(dim=-1) rand_probs = torch.rand(size=(probs_teacher_cumsum.shape[0], to_sample), device=probs_teacher_cumsum.device) rand_probs = rand_probs.sort(dim=-1)[0] sample_token_ids = torch.searchsorted(probs_teacher_cumsum, rand_probs) # Inverse Transform Sampling probs_teacher *= 0 probs_teacher.scatter_reduce_(dim=-1, index=sample_token_ids, src=torch.ones_like(probs_teacher), reduce='sum') probs_teacher.div_(probs_teacher.sum(dim=-1, keepdim=True)) logits_exp = torch.exp(logits) logits_sum_exp = torch.sum(logits_exp, dim=-1) logits_log_sum_exp = torch.log(logits_sum_exp) loss = - probs_teacher * (logits - torch.unsqueeze(logits_log_sum_exp, dim=-1)) loss = torch.sum(loss, dim=-1).mean() return loss class ToyModel(torch.nn.Module): def __init__(self, num_hidden): super().__init__() self.layer1 = torch.nn.Linear(num_dim, num_hidden) self.layer2 = torch.nn.Linear(num_hidden, num_hidden) self.layer3 = torch.nn.Linear(num_hidden, num_classes) def forward(self, x): = torch.nn.functional.gelu(self.layer1(x)) = torch.nn.functional.gelu(self.layer2(x)) = self.layer3(x) return teacher = train(ToyModel(num_hidden_teacher).to(device), 'teacher') student = train(ToyModel(num_hidden_student).to(device), 'student') student_kd = train(ToyModel(num_hidden_student).to(device), 'student_full_kd', teacher=teacher) student_topk = train(ToyModel(num_hidden_student).to(device), 'student_topk', teacher=teacher, topk=7) student_random = train(ToyModel(num_hidden_student).to(device), 'student_random_sampling', teacher=teacher, to_sample=50)"
        },
        {
            "title": "L NLU Tasks Full Scores",
            "content": "Experiment ARC Easy HellaSwag LAMBADA OpenAI LAMBADA Standard PIQA Avg. Base CE Ours (12) FullKD Base CE Top12 Top50 Ours (12) Ours (12)++ FullKD SFT, Tulu CE Top12 Top50 Ours (12) Ours (12)++ FullKD 46.59 50.76 51.56 64.90 65.07 65.66 66.29 68.14 66.08 58.84 63.51 66.58 68.43 66.96 68. 3B Teacher 300M Student 41.18 41.84 41.98 38.85 40.25 40.69 8B Teacher 3B Student 56.35 57.04 57.80 58.93 60.82 58.76 57.51 58.49 59.26 60.14 60.91 59. 45.64 47.76 47.88 47.47 46.83 48.01 45.66 50.92 50.86 52.14 50.71 50.46 30.80 30.70 29.52 38.31 39.86 40.87 40.99 39.80 40.71 37.92 42.97 42.07 42.67 42.23 42.32 67.41 67.46 67. 44.97 46.20 46.20 72.58 73.50 73.50 73.83 73.99 73.88 72.69 72.47 72.91 73.83 74.48 73.01 55.56 56.65 57.14 57.50 57.92 57.49 54.52 57.67 58.34 59.44 59.06 58.72 Table 19: Full performance results on various benchmarks for 300M and 3B experiments."
        }
    ],
    "affiliations": [
        "Samsung Research, Seoul"
    ]
}