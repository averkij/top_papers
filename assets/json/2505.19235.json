{
    "paper_title": "CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models",
    "authors": [
        "Qinsi Wang",
        "Hancheng Ye",
        "Ming-Yu Chung",
        "Yudong Liu",
        "Yueqian Lin",
        "Martin Kuo",
        "Mingyuan Ma",
        "Jianyi Zhang",
        "Yiran Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main."
        },
        {
            "title": "Start",
            "content": "CoreMatching: Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models Qinsi Wang 1 Hancheng Ye 1 Ming-Yu Chung 1 Yudong Liu 1 Yueqian Lin 1 Martin Kuo 1 Mingyuan Ma 1 Jianyi Zhang 1 Yiran Chen"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 5 2 ] . [ 1 5 3 2 9 1 . 5 0 5 2 : r Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, fundamental yet underexplored question remains: Do they truly operate in isolation, or is there deeper underlying interplay that has In this paper, we conyet to be uncovered? duct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-theart baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5 FLOPs reduction and 10 overall speedup. Code is released at https://github.com/wangqinsi1/2025ICML-CoreMatching/tree/main. 1Department of Electrical and Computer Engineering, Duke University, North Carolina, USA. Correspondence to: Qinsi Wang <qinsi.wang@duke.edu>, Jianyi Zhang <jianyi.zhang@duke.edu>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Large language models (LLMs) have achieved outstanding performance in various applications and have exerted significant influence on our daily life (Brown, 2020; Chowdhery et al., 2023; Touvron et al., 2023a; Kuo et al., 2025; Qinsi et al.). As LLMs become increasingly aware of the physical world, researchers have discovered their potential for understanding visual information (Lin et al., 2023; Liu et al., 2024a; 2025b). As result, series of vision-language models (VLMs) such as LLaVA (Liu et al., 2024b), Blip(Li et al., 2022), and Llama(Touvron et al., 2023b) have been introduced, demonstrating impressive performance on tasks like image-based question answering. However, because of the requirement of long image-token inputs, VLMs typically demand more time and memory for inference than LLMs, limiting their practical adoption in real-world scenarios. Token Sparsity, which exploits the high degree of redundancy among image tokens, is promising solution to this challenge(Ye et al., 2024; Huang et al., 2024). Researchers have found that VLMs can retain strong performance using as little as 10% of the total tokens. This has led to extensive interest in determining which tokens are essential. For example, PruMerge (Shang et al., 2024) uses the average attention scores between image tokens and text tokens to measure token importance and retains 20% of the tokens with only minor performance loss; FastV (Chen et al., 2025) uses the total attention scores received by other token and finds that more than half of the tokens can be discarded from the second layer. Nevertheless, it is worth noting that most existing methods rely on attention scores as guide for selecting important tokens, but their validity and accuracy have not been thoroughly examined. Another effective approach to accelerating inference is model-internal sparsity, particularly adaptive Neuron Sparsity. It leverages the highly sparse activations in feedforward network (FFN) layers to skip the computation of inactive neurons, thereby reducing the computational in LLMs. For instance, methods such as Dejavu (Liu et al., 2023) and PowerInfer (Song et al., 2023) employ MLPbased predictors to identify which neurons are activated for given input, achieving up to 93% prediction accuracy. FurCoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Figure 1. Schematic diagram of CoreMatching. In the Pre-filling stage, CoreMatching calculates Core Neurons in the FFN block based on the activation. Core Neurons are the most frequently activated group of neurons. Afterwards, CoreMatching matches the neurons activated by different tokens with the core neurons, and selects group of tokens with the largest intersection as the Core Tokens. Only the Core Tokens are passed to the subsequent layers. During the decoding stage, the model only uses Core Neurons for calculations, and there are only core tokens in the kv cache. CoreMatching achieves comprehensive acceleration for inference of VLMs. ther, CoreInfer (Wang et al., 2024) defines core neurons as the subset of neurons most frequently activated by an input sentence, demonstrating that only these core neurons are needed to maintain performance. While neuron sparsity has shown great promise in LLMs, it remains underexplored and underutilized in VLMs. Although both token sparsity and neuron sparsity can individually accelerate the model, each has limitations in practical applications. Token sparsity primarily speeds up the pre-filling stage and can only provide limited acceleration during decoding by reducing key-value operations. In contrast, neuron sparsity primarily accelerates the decoding stage but cannot achieve high speedup ratio in the prefilling stage due to the large number of tokens and the resulting low level of sparsity. Hence, combining these two forms of sparsity is promising approach to achieving comprehensive acceleration. However, an interesting question that has been overlooked in previous work is: What is the relationship between these two sparse spaces? To answer this question, we first experimentally verify the existence of neuron sparsity in VLMs and the effectiveness of core neurons, which are the most important neurons determined by the activation distribution of all input tokens. Furthermore, we investigate the alignment between core neurons and tokens. By analyzing how core neurons influence token inference, we uncover an insightful matching pattern: tokens whose activated neurons most closely match the core neurons correspond to the most critical part for determining the output. Inspired by this, we define core tokens as the set of tokens that have the largest intersection of activated neurons with the core neurons among all tokens. Building on these insights, we propose CoreMatching, co-adaptive inference framework. As illustrated in Fig. 1, CoreMatching requires single step in the pre-filling stage to jointly compute Core Neurons and Core Tokens, thereby achieving sparsity along both the token and neuron dimensions. Furthermore, we conduct detailed analysis of the theoretical and practical benefits of CoreMatching. Theoretically, we propose projection-guided criterion for evaluating the importance of tokens, which takes into account not only attention scores but also angular information. We analyze the effectiveness and efficiency of this criterion and theoretically analyze the proportional relationship between core tokens and the criterion. Empirically, we implement and evaluate CoreMatching across various tasks and hardware, showing that it exceeds the performance of state-of-the-art token sparsity approaches. CoreMatching delivers comprehensive inference acceleration due to its multi-dimensional sparsity, achieving 2.1 speedup in the pre-filling stage and 9.2 speedup in the decoding stage. In summary, our contributions are as follows: We introduce Core Tokens, which are those tokens that activate the largest number of core neurons. Experiments show that core tokens consistently capture the subset of tokens most relevant to the output. We present Projection-guided Criterion and leverage it to explain why Core Tokens outperform those selected by previous attention score-based methods. We propose CoreMatching, co-adaptive inference framework. Experimental results confirm that it not only surpasses baselines on the performance of various tasks but also achieves comprehensive acceleration. CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Table 1. Verification of the importance of core neurons. We retain different proportions of core neurons on Llava-1.5-7b, and we calculate the accuracy of the model on TextVQA. 0.8 Keep Ratios 0.4 1.0 0. 0.6 Accuracy 45.1% 53.2% 55.8% 56.3% 57.8% Figure 2. Verification of the predictability of core neurons. We visualized the core neurons of the 25-th layer of Llava-1.5-7b when input text token at different lengths. ρ = 0.2, β = 0.4. We selected the first 256 neurons. It can be seen that when the input semantics are sufficient, core neurons are almost unchanged. 2. Intrinsic Relations of Two Paradigms In this section, to delve deeper into the relationship between two sparse spaces, we propose novel interaction paradigm between tokens and neurons. Through this approach, we investigate how important neurons and tokens mutually influence and determine each other. 2.1. Identifying Core Neurons for Tokens In the FFN block in LLMs, there are typically two linear layers, Wu RN1N2, Wd RN2N1. For single token, denote the input representation of the FFN layer as x, the output can be expressed as: = σ(xWu), (1) where σ represents the activation function, such as ReLU or SiLU. Intermediate output = {an}N2 n=1, where an is the activation value of the n-th neuron in Wu. = AWd Core Neurons. The concept of Core Neurons was first introduced in CoreInfer (Wang et al., 2024), which represents group of neurons that are most important for input to maintenance performance. Specifically, for single token x, the token-wise core neurons Cρ(x) are defined as the neurons with the top ρ maximum positive activation values, that is, Cρ(x) = {n an Percentile(A+, ρ)}, (2) where A+ = {an an > 0, an A} represents the set of positively-activated neurons, and Percentile(A+, ρ) denotes the ρ-th percentile of the positive activation. For sentence = [x1, x2, . . . , xM ] containing tokens. The sentence-wise core neurons Cβ ρ (s) are defined as the top β of neurons that appear most frequently in the core neurons of all tokens, (cid:83)M i=1 Cρ(xi), which is formulated as Cβ ρ (s) = {n fρ(n; s) Percentile(fρ(s), β)}, (3) 3 where fρ(s) denotes the count set of each neuron marked as the core neuron across all tokens, as defined in Eq. (4). fρ(s) = {fρ(n; s)}n = { (cid:88) m= I(n Cρ(xm))}n, (4) where I() is an indicator function that returns one if is in Cρ(xm) else zero. Percentile(fρ(s), β) denotes the β-th percentile of fρ(s). Effectiveness. For an input s, core neurons are the subset of neurons that are most frequently activated and exhibit the highest activation values. In LLMs, core neurons have two key characteristics. First, they are crucial for the inference, as only core neurons retained can maintain the performance. Second, they exhibit predictability; when input is sufficiently long and semantically rich, the core neurons identified during the pre-filling stage closely align with those in the decoding stage. To validate that core neurons applicable to VLMs, we conducted experiments on Llava1.5-7b (Liu et al., 2024a). As shown in Tab. 1 and Fig. 2, core neurons also exhibit these two characteristics on Llava. Specifically, by retaining only 40% of the neurons, the performance decreases by merely 3%. This demonstrates that in VLMs, given specific input, we can accelerate inference by preserving only small set of core neurons. Note that the calculation of core neurons takes into account the activation distribution of all tokens. Therefore, core neurons are essentially part of neurons that capture the most critical features, which are determined by all tokens input. 2.2. Deriving Core Tokens from Core Neurons In the previous section, we defined core neurons, which are selected by counting the key activations determined by all input tokens. In this section, we explore how core neurons in turn select important tokens. Matching of Neurons and Tokens. To explore the relationship between core neurons and tokens. We first study the impact of core neurons on different tokens. Specifically, we explore the changes in token inference when only core neruons are retained compared to using all neurons. For single token x, the set of neurons activated by it can be denoted as Γ(x) = { an(x) > 0}. When only the core neurons are retained, token can only activate neurons in the intersection Γ(x) Cβ ρ (s). Consequently, information that token is able to transmit, i.e., I(x), can be regarded as proportional to the number of intersections between the neurons it activates and core neurons, (cid:12)Γ(x) Cβ (5) Therefore, to analyze how much information core neurons retain for different tokens, we can equivalently measure the number of core neurons each token activates. I(x) (cid:12) ρ (s)(cid:12) (cid:12). CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Figure 3. (Upper) Distribution of (cid:12) (cid:12) of image token. The experiment was conducted on Llava-1.5-7b, and we selected the 10th layer. The input image is the rabbit on the left, and the input text is the text token above the image. We use red font to emphasize the key points of the text token. (Note that since core neurons themselves account for 40% of neurons, intersection of about 2000 can be regarded as random sample.) (Lower) Core token under different inputs. The left is the schematic diagram of the maximum geometric distance method to select the threshold. The right side is the core token retained under the distribution of the corresponding image above. (cid:12)Γ(x) Cβ ρ (s)(cid:12) We conduct experiments on LLaVA-1.5-7B, focusing on the number of core neurons activated in the 10th layer for various tokens across different text inputs. The results, illustrated in Fig. 3 (Upper), reveal intriguing patterns. Specifically, image tokens that exhibit stronger correlations with the text prompt significantly activate more core neurons than other image tokens. Notably, even for the same image, the distribution of activated core neurons varies depending on the emphasis of the text prompt. Image tokens most relevant to the text consistently trigger substantially higher activation of core neurons. From the above analysis, we know that core neurons can preserve the information flow of important tokens while suppressing that of unimportant tokens. The subset of tokens that activate the largest number of core neurons are the most critical for the inference. Core Tokens. Motivated by the fact that core neurons can guide the selection of important tokens, we define core tokens here and verify their effectiveness. (cid:110) β ρ (s) = From the previous analysis, tokens that exhibit higher correlation with text tokens activate more core neurons. Therefore, for input s, we can define core tokens β ρ (s) as tokens activate the largest number of core neurons, (cid:12) ρ (s)(cid:12) (cid:12) (cid:12) (6) where Tk is the knee threshold derived from the distribution of (cid:12) (cid:12) for all tokens in using the maximum geometric distance method (Garg & Walker, 1990). Fig. 3 illustrates how Tk is obtained. Notably, since Tk is computed adaptively based on the data distribution, there is no need to manually fix the number of core tokens. (cid:12) (cid:12)Γ(xm) Cβ (cid:12)Γ(x)Cβ (cid:12) Tk ρ (s)(cid:12) (cid:111) , xm Fig. 3 (Lower) shows the retained core tokens under different text inputs. It can be observed that, for various text tokens, core tokens are consistently the most relevant to the text tokens. Moreover, the core tokens adaptively retain the most critical portion of the tokens based on the input, without being constrained by fixed number limit. In Section 4, we empirically demonstrate that core tokens represent the most crucial set of tokens for maintaining performance. By retaining only these core tokens during inference, the model can achieve nearly lossless performance. 3. CoreMatching In this section, building on our previous findings, we propose CoreMatching, co-adaptive inference framework. We further provide theoretical analysis explaining why core neurons can guide core tokens. 3.1. Overall Framework As shown in Fig. 1, CoreMatching precomputes core neurons at each FFN layer during pre-filling, and identifies core tokens at layer and discards non-core tokens. During decoding, the model predicts the next token using the prefetched core neurons and core tokens. CoreMatching pruned unimportant tokens to speed up the pre-filling stage. Meanwhile, the reduced number of neurons also accelerates the decoding stage. Our method is training-free, and plug-and-play. Algorithm is provided in the Appendix .B. 3.2. Theoretical Advantage To analyze why core tokens are effective, we first explore the question: How can we evaluate the importance of image tokens in VLM? In existing research (Shang et al., 2024; Chen et al., 2025), the attention scores between image tokens and text tokens are widely used as metric for evaluation. However, no work has yet analyzed its optimality. Therefore, in this section, we first propose superior criterion to quantify the importance of image tokens. Furthermore, we CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models (a) Forms of OM (b) Attention Score (c) Projection Value Figure 4. Diagram of attention score and projection value. indicates the token is reserved under this matric. indicates discarded. (a) Attention score (b) Projection Value (c) Core Tokens Figure 5. Comparison of three metrics. The input is the rabbit in Fig. 3 and What color clothes is the rabbit wearing?. Experiment is conducted on Llava-1.5-7b and the 10-th layer is selected. mathematically derive and demonstrate that core tokens are proportional to this criterion. Projection-guided Criterion. In the inference of VLM, the only module where tokens influence each other is the attention mechanism. And since the final output is determined by the last token in the input, we only need to focus on how all image tokens affect this last token. For single token, suppose its input to the Attention block is y, where = σ(xWu)Wd. Consider sequence of tokens [y1, y2, . . . , yM ], for the last token yM , its computation in the Attention layer can be expressed as: ˆyM = LayerNorm(yM ), αiM = Sof tmax(cid:0)(ˆyi Wq) (cid:0)ˆyM Wk OM = α1M V1 + α2M V2 + + αMM VM , Vi = ˆyi Wv, (cid:1)T (cid:1), (7) where αiM is the attention score between the i-th and the -th token. OM is the output vector of the -th token. As shown in Fig. 4 (a), when we focus on the OM , we can observe that OM is the sum of multiple vectors. This indicates that OM depends not only on the attention scores αiM , but also on the angle between Vi and OM . Based on this insight, we propose that the Projection-guided Criterion for evaluating the influence of the i-th token on the -th token should be the projection of αiM Vi onto OM , given by (8) (cid:13)αiM Vi (cid:13)ProjOM (cid:0)αiM Vi (cid:13) (cid:13)ProjOM (cid:13) (cid:13) cos(cid:0)(Vi, OM )(cid:1). (cid:1)(cid:13) (cid:13) = (cid:13) where (cid:13) (cid:1)(cid:13) (cid:0)αiM Vi (cid:13) is the projection value of αiM Vi on OM . And cos (cid:0)(Vi, OM )(cid:1) is the cosine value of the angle between the Vi and OM vectors. Fig. 4 provides schematic illustration of this projection-based metric. Furthermore, in Fig. 5 (a) and (b), we compare the results of using the attention score as an evaluation metric with those (a)WqWk (b) WdW Figure 6. (a) (b) Numerical visualization of WqWk and WdW . They are approximately I. (c) Visualization of α. It can be seen that the αii (diagonally) is much higher than the others. (c)Attention Score obtained using the projection magnitude. Obviously, the projection magnitude filters out important tokens more effectively, whereas the attention score alone considers only the vector magnitude and ignores angular information, which introduces more noise. Theoretical Analysis. Although the projection value is superior evaluation metric, its computation is both complex and time-consuming. Therefore, we show that it is actually related to the proposed core token. To demonstrate this, we first introduce two observations: Observation 1. Wq and Wk are nearly approximately orthogonal to each other, i.e., Wq θI. θ is constant. (cid:0)Wo, Wu, Wd (cid:1) are approximately orthogonal matrices, i.e., λI. λ is constant. We present experimental evidence supporting this observation in Fig. 6, and provide comprehensive experiment and more detailed discussion in Appendix C. This is fascinating phenomenon in VLMs. In fact, further investigation reveals that such matrix orthogonality was previously proven to exist and be effective in MLPs and CNNs. For example, (Li et al., 2019; Bansal et al., 2018) have shown that this orthogonality helps improve the training stability and generalization ability of the model. Observation 2. For activation function output vectors Ai and AM , cos((Ai, AM )) is proportional to the number of (cid:12)Γ(xi) Γ(xM )(cid:12) intersections of activated neurons, i.e., (cid:12) (cid:12). This observation is illustrated in Fig. 7. An intuitive understanding is that if xi and xM activate more of the same neurons, Ai and AM will have more positive values in common positions, making cos(Ai, AM ) larger. We provide more visualizations in Appendix C. Building on these two observations, we provide insights into how projection values are influenced in LLMs. Following the inference order, we first analyze the impact of input vector on projection values, then examine how the activation layer affects these input vectors. Theoretical Insight 1. The projection values between the (cid:1)(cid:13) i-th and -th token, i.e., (cid:13) (cid:13), is proportional to the cosine value of the angle between yi and yM . (cid:0)αiM Vi (cid:13)ProjOM Justification. To analyze how yi and yM influence the pro5 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models (a) Layer 5 (b) Layer 10 Figure 7. The distribution of cos(Ai, AM ) and (cid:12) of LLaVA-1.5-7b, clearly shows proportional relationship. (c) Layer 15 (cid:12)Γ(xi) Γ(xM )(cid:12) (cid:12) jection value, we first decompose it into form related to these two tokens. From Eq. 7, OM is sum of vectors in different directions. Since the self-attention score αMM is typically much higher than αiM for other tokens (as shown in Fig. 6 (c)), we can simplify the projection by assuming that OM is primarily determined by αMM VM , i.e., ProjOM (αiM Vi) αiM Vi cos((Vi, VM )), (9) (cid:1)T (cid:1). Since the where αiM = Sof tmax(cid:0)(ˆyi Wq) (cid:0)ˆyM Wk Softmax function is monotonic, that is, Sof tmax(x) x. We can have αiM (ˆyi Wq) (cid:0)ˆyM Wk . Therefore, (cid:1)T αiM Vi cos((Vi, VM )) (ˆyi Wq) (cid:0)ˆyM Wk = ˆyiWq, ˆyM WkVi, VM /VM = (cid:0)ˆyi(WqW )ˆyT (cid:1)(cid:0)ˆyi(WvW )ˆyT (cid:1)T Vi cos((Vi, VM )) (cid:1)/VM (a) What time is it now? (b) Whats the word on the button? (c) Whats the word on the sign?(d) Whats the person wearing? Figure 8. Examples of CoreMatching sampled tokens. Ai, and yi = AiWd. Based on Observation 1, Wd is scalar multiple of unitary self-orthogonal matrix, applying the same rotation to any input while preserving the inner product and angle between any two input vectors (proof is provided in Appendix C). Thus, we can have: cos((yi, yM )) = cos((Ai, AM )) (13) Furthermore, based on Observation 2, cos((Ai, AM )) (cid:12)Γ(xi) Γ(xM )(cid:12) (cid:12) (cid:12). Combined with Eq. 19, we can get cos((yi, yM )) (cid:12) (cid:12)Γ(xi) Γ(xM )(cid:12) (cid:12). (14) which is consistent with Insight 2. (10) Eq. 14 shows that activation layers adjust token angles by controlling the intersections of their activated neurons. More shared activated neurons lead to smaller angles and greater mutual influence. Based on the Observation 1, WqW θI, WvW Therefore, combining Eq. 9 and Eq. 10, we can have λI. ProjOM (αiM Vi) θλˆyi, ˆyM ˆyi, ˆyM /VM (11) Given that ˆy is the result of after LayerNorm, ˆy and share the same direction, and ˆy = 1, it holds that ˆyi, ˆyM = cos((yi, yM )). We can have ProjOM (αiM Vi) cos((yi, yM )), (12) which is consistent with Insight 1. Eq. 12 shows that the angle between different tokens directly affects their mutual interaction in attention layer. The closer the angles of two tokens, the greater their mutual influence. Therefore, the angle should be an essential indicator for token importance evaluation, which has not been fully demonstrated and utilized in previous work. Furthermore, we analyze how the activation influences the relative angle between yi and yM . Theoretical Insight 2. cos((yi, yM )) is proportional to the number of intersections of neurons activated in the previous (cid:12)Γ(xi) Γ(xM )(cid:12) activation layer, i.e., (cid:12) (cid:12). Justification. To analyze how the activation layer affects cos((yi, yM )), we first decompose its computation formula. Suppose the activation output of the i-th token is Correlation with Core Tokens. Based on the predictability of core neurons, when the semantics of the input is stable enough, the neurons activated by xM are approximately the same as those of the core neurons, Γ(xM ) Cβ ρ (s). Therefore, by combining Insight 1 and 2, we can get (αiM Vi) (cid:12) (cid:12)Γ(xi) Cβ ρ (s)(cid:12) (cid:12). ProjOM (15) Eq. 15 shows that the number of intersections between core neurons and the activated neurons in one token reflects how much information this token contains that contributes to the next-token prediction. From the above analysis, we see that using only the attention score does not consider the angular information. In contrast, core neurons are directly proportional to the projection value, incorporating both the absolute influence and the angular component, thus constituting more accurate metric. Fig. 5 shows comparison between using core tokens and using the attention score as metrics. Fig. 8 shows the effectiveness of CoreMatching for different inputs. Moreover, to the best of our knowledge, this is the first work to investigate the intrinsic relationship between token sparsity and neuron sparsity. Our findings reveal the inherent correlation between these two sparsity patterns and highlight that the relative angle between tokens is crucial yet previously overlooked factor in token selection. 6 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Table 2. Comparison with SoTA vision context sparsification methods on vision understanding benchmarks. The best results are bolded. The results of the other methods are from their papers. CoreMatching achieves the best performance in most benchmarks using smaller number of image tokens. The Free indicates whether method is training-free (i.e., can be applied directly to MLLMs without training.) Method Free Token TFLOPs VQAv2 GQA SciQA TextVQA POPE MME MMB SEED VisWiz MM-Vet LLaVA-1.5-7B PruMerge+ FastV VoCo-LLAMA LLaVA-TRIM Dynamic-LLaVA CoreMatching LLaVA-1.5-13B PruMerge+ FastV LLaVA-TRIM Dynamic-LLaVA CoreMatching - - 576 146 144 128 121 115 64 576 146 144 121 115 98 10.1 2.5 3.2 2.2 2.2 2.5 2.1 19.6 4.9 6.0 5.8 4.7 4.3 78.5 76.8 75.1 76.9 76.4 78.0 78.5 80.0 77.8 77.0 75.4 78.8 79. 62.0 57.4 57.5 59.8 61.4 61.4 61.9 63.3 58.2 60.1 59.0 62.5 63.1 66.8 68.3 68.7 - 69.1 69.1 69.5 71.6 71.0 72.8 72.8 72.4 72.8 58.2 57.1 56.2 - - 57.0 54.5 61.3 58.6 59.0 54.8 59.6 58. 85.9 84.0 81.0 - 85.3 85.0 85.9 85.9 84.4 83.2 86.3 86.5 87.0 1510.7 1462.4 1458.9 - 1461.3 1479.8 1506.5 1531.3 1485.5 1470.3 1438.0 1563.3 1529.9 64.3 60.9 63.5 61.0 67.4 64.1 64.6 67.7 65.7 66.9 65.7 66.9 68. 66.0 62.8 62.8 59.1 65.8 64.6 66.1 68.2 - 65.4 65.9 66.5 67.4 50.0 42.5 47.8 - 48.1 50.2 49.8 53.6 49.7 52.3 53.2 52.8 53.2 31.1 25.0 26.3 - 28.0 29.5 29.6 36.1 28.0 31.3 30.3 34.8 34. Table 3. Hardware performance comparison with the most advanced token sparse and neuron sparse methods. Experiments are performed on single NVIDIA Titan Xp (12GB). The input is 610 tokens, and the output is 64 tokens. We test the latency of the baseline method based on the LLaVA framework. For PowerInfer, we migrate the predictor provided by Llama2-7b to the LLM module of LLaVA-1.5-7b. Method Cost Sparisity Memory Pre-filling Decoding Free Neurons Token Kv Cache FFN Total Ratio TFLOPs Lat(s) GFLOPs Lat(s) LLaVA-7B PruMerge+ FastV Dynamic-LLaVA PowerInfer CoreMatching 304.8 89.73 88.98 74.40 304.8 48.99 8.26 8.26 8.26 8.26 1.83 1.65 15.63 15.41 15.41 15.39 9.20 8.62 1.00 1.01 1.01 1.01 1.70 1.82 10.1 2.5 3.2 2.5 2.2 2. 2.23 1.16 1.15 1.15 2.03 1.05 0.845 0.822 0.822 0.820 0.516 0.486 44.2 39.5 39.1 37.6 11.6 4.81 4. Experiments Our experiments are conducted at three levels. First, we evaluate the performance of CoreMatching on various tasks to demonstrate its effectiveness (Sec. 4.1). After that, we deploy CoreMatching on different hardware devices to verify the improvement of hardware performance (Sec. 4.2). Finally, we perform ablation studies to further analyze the effectiveness of each module in CoreMatching (Sec. 4.4). The experimental details are provided in the Appendix D. 4.1. Task Performance. High Accuracy. Tab. 2 presents the performance of CoreMatching on various tasks. As shown, CoreMatching demonstrates minimal performance degradation across all tasks and achieves higher accuracy than state-of-the-art baselines on most tasks. Notably, on LlaVA-1.5-7b, CoreMatching achieves lossless performance on datasets such as VQAv2 and GQA while using only about 10% tokens. Furthermore, on SciQA and MMBench, CoreMatching even outperforms the original model. For LlaVA-1.5-13b, CoreMatching demonstrates clear advantage over other methods, achieving near-lossless accuracy across all datasets. Low Cost. Unlike Dynamic-LLaVA and PruMerge+, which require post-training specific to each model, CoreMatching requires no training or fine-tuning, making it plug-and-play solution for any model. Additionally, CoreMatching does not require pre-specification of the number of tokens to retain, as it dynamically determines the optimal number of tokens based on the input. Consequently, CoreMatching achieves comparable performance with fewer tokens on average. Specifically, CoreMatching only retains 10% and 17% tokens on average for LlaVA-1.5-7b and LlaVA-1.5-13b, respectively, compared to 20% for PruMerge+ and FastV. Moreover, CoreMatching introduces neuron sparsity in addition to token sparsity, further reducing computational requirements during the decoding stage. 4.2. Hardware Performance Reduced Inference Memory. Tab. 3 compares the hardware resources required for inference between CoreMatching and other sparse inference methods on an NVIDIA Titan Xp (12GB). CoreMatching can simultaneous sparsity across both token and neuron dimensions, CoreMatching significantly reduces the memory footprint of the KV cache during inference as well as the memory required for neurons during 7 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Table 4. Peformance comparison with popular acceleration methods on OCR, chart, and document understanding tasks. The basic models are LLaVA-1.5-7B and LLaVA-1.5-13B. All tasks are evaluated on test set. TF indicates TFLOPs. OCRB indicates OCRBench Method Token TF DocVQA InfoVQA ChartQA OCRB AI2D LLaVA-1.5-7B PreMerge+ FastV CoreMathing LLaVA-1.5-13B PreMerge+ FastV CoreMating 576 146 144 48.4 576 146 144 56.3 10.1 2.5 3.2 2.1 19.6 4.9 6.0 4.3 22.2 17.9 20.6 21.9 24.6 14.6 22.9 23.4 22.3 21.1 22.7 22.7 25.1 23.6 24.5 24.8 18.2 15.8 16.2 17.8 18.2 16.9 17.1 17.5 31.3 26.2 28.1 30.5 33.6 27.8 30.1 33. 55.2 47.3 51.2 55.2 59.2 49.1 54.9 59.3 Table 5. Performance of CoreMatching on the Qwen2.5-VL-3B and Qwen2.5-VL-7B models on OCR, chart, and document understanding tasks. The results of the original model are extracted from the Qwen2.5-VL technical article. Since the number of image tokens in Qwen2.5-VL is dynamic, we take the average of different tasks as the number of tokens used. Neur indicates Neurons. Figure 9. Latency comparison of token-only/neurons-only/both sparse on NVIDIA TiTAN Xp. W/T means only core tokens, W/N means only core neurons, and W/TN means CoreMatching. The number on the bar means how many seconds it took. Model Claude-3.5 Sonnet Gemini 1.5 Pro GPT 4o Qwen2.5-VL 3B CoreMatching Qwen2.5-VL 7B CoreMatching Tokens Neur DocVQA InfoVQA ChartQA OCRB AI2D 81.2 74.3 88.4 81.0 84.6 80.7 81.6 77.1 81.6 77.1 83.9 82.6 84.2 82.3 - - - 11008 4403 11008 - - - 235.2 24.1 234.9 23.8 78.8 75.4 73.6 79.7 79.2 86.4 85.6 90.8 87.2 86.7 84.0 83.6 87.3 87.0 95.2 93.1 91.1 93.9 93.2 95.7 94.8 and collected results for FastV and PruMerge on these tasks to ensure fair comparison. As shown in Tab. 4, CoreMatching consistently outperforms other acceleration methods on most tasks while incurring only minimal performance degradation compared to the original models. Notably, for tasks such as DocVQA and InfoVQA, which often require understanding only small portions of text within an image, CoreMatchings dynamic token pruning allows it to achieve near-lossless performance using significantly fewer tokenson average, less than 10% of the original token. Model Generalization. To further evaluate the generalizability and effectiveness of our method on more advanced architectures, we conducted comprehensive experiments on Qwen2.5-VL-3B and Qwen2.5-VL-7B, two representative LVLMs that incorporate dynamic resolution mechanisms to adaptively allocate computational resources based on visual content complexity. These models present more challenging evaluation setting due to their sophisticated token selection and multi-scale processing capabilities. As summarized in Tab. 5, CoreMatching continues to exhibit robust performance under this setting. Notably, it achieves near-lossless performance while utilizing only approximately 10% of the visual tokens, demonstrating its capability to distill and preserve essential visual information with high efficiency. Even more, on the AI2D dataset, CoreMatching not only retains accuracy but actually surpasses the performance of the original full-token model, underscoring its adaptability and potential to enhance model performance through better Figure 10. Latency comparison of token-only/neurons-only/both sparse on NVIDIA RTX 6000. decoding. When the input token length is short, CoreMatching requires only about half the memory of the original model during decoding, eliminating memory-bound limitations on resource-constrained devices and significantly reducing inference latency. For long input token sequences, CoreMatching also can effectively reduce primary memory usage by minimizing the KV cache size. Reduced Inference Latency. In contrast to methods that achieve partial acceleration, such as token sparsity for prefilling time reduction or neuron sparsity for decoding time reduction, CoreMatching delivers comprehensive inference acceleration through multidimensional sparsity. As illustrated in Tab. 3, CoreMatching achieves 2.1 speedup in the pre-filling stage and 9.2 speedup in the decoding. 4.3. Generalization Analysis Task Generalization. To verify the task generalization of our method, we conducted extensive evaluations on LLaVA1.5-7B and LLaVA-1.5-13B across additional OCR and chart/document understanding tasks. We also reproduced CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Table 6. Performance comparison of different LVMs on video reasoning benchmarks. The base model for all acceleration methods is Video-LLaVA. Results of PruMerge and FastV are taken from the original papers. Methods Size FrozenBiLM VideoChat LLaMA-Adapter Video-LLaMA Video-ChatGPT Video-LLaVA FastV PruMerge PruMerge+ CoreMatching 1B 7B 7B 7B 7B 7B 7B 7B 7B 7B MSVD-QA MSRVT-QA Accuracy Score Accuracy Score Accuracy ActivityNet-QA Score 32.2 56.3 54.9 51.6 64.9 70.7 71.0 71.1 71.1 71.6 - 2.8 3.1 2.5 3.3 3.9 3.9 3.9 3.9 3.9 16.8 45.0 43.8 29.6 49.3 59.2 57.0 58.4 59.3 59.8 - 2.5 2.7 1.8 2.8 3.5 3.5 3.5 3.6 3.6 24.7 - 34.2 12.4 35.2 45.3 - 48.3 47.7 48.4 - 2.2 2.7 1.1 2.7 3.3 - 3.4 4.4 3. Table 7. The average number of core tokens for different tasks. Type Tasks Number Multi Choice Questions and Answers Long Text Generate SciQA VQAv2 GQA VisWiz TextVQA POPE MME MM-Vet 27.9 63.5 76.6 76.9 64. 71.1 66.9 93.9 token selection. These results collectively validate that CoreMatching remains effective when applied to state-of-the-art LVLMs with dynamic resolution, reinforcing its potential as general-purpose visual token reduction strategy that scales across model sizes and architectural paradigms. Performance on Video Task. To verify the scalability of CoreMatching on Video tasks, we used Video-LLaVA as the base model and conducted experiments on multiple video tasks including MSVD-QA, MSRVT-QA, and ActivityNet-QA. As shown in Tab. 6, CoreMatching consistently outperforms existing advanced acceleration methods across multiple video reasoning benchmarks. Specifically, on MSRVT-QA, CoreMatching reaches an accuracy of 59.8%, outperforming origianl Video-LLaVA (59.2%). On ActivityNet-QA, CoreMatching improves accuracy (48.4%) compared to the origianl Video-LLaVA (45.3%). This suggests that token redundancy in video frames can be substantial, and strategic pruning not only reduces computational overhead but may also enhance model performance by eliminating noisy or irrelevant tokens. These results collectively highlight the strong potential of CoreMatching as generalpurpose acceleration framework for video LLMs. 4.4. Ablation Study Module Ablation. To further analyze the contributions of token sparsity and neuron sparsity to inference acceleration, we conducted ablation experiments on various hardware. As shown in Fig. 9 and 10, we compared the acceleration achieved by token sparsity alone, neuron sparsity alone, and CoreMatching (sparsity applied to both tokens and neurons) during the pre-filling and decoding stages. Across different devices, models, and output token lengths, CoreMatching consistently achieved significant acceleration, with more 9 (a) 5-th Layer (b) 15-th Layer (c) 25-th Layer Figure 11. Number of tokens required at different layers. pronounced gains as the output token length increased. Token-only sparsity primarily accelerated the pre-filling stage, with minimal impact on the decoding stage (limited to reducing KV computation). Conversely, neuron-only sparsity mainly accelerated the decoding stage, offering little improvement during the pre-filling stage. By simultaneously reducing memory and computational load, CoreMatching achieved higher acceleration rates in both stages. Core Tokens Across Different Tasks. Due to the inputadaptive of CoreMatching, it can automatically determine how many tokens to retain based on the data distribution of each task. Tab. 7 shows the number of core tokens retained for different task types. As observed, CoreMatching adaptively adjusts the required tokens according to task difficulty. For instance, simpler tasks such as multiple-choice question answering (e.g., SciQA where one must choose between options or B) require only 27.9 tokens on average. In contrast, more challenging tasks involving longer text generation (e.g., MM-Vet, which requires generating medical records from images) need around 93.9 tokens on average. Notably, even for the most difficult tasks, CoreMatching needs fewer than 20% of tokens, outperforming other token sparsity methods in terms of token reduction. Tokens Required at Different Layers. Another crucial aspect of token sparsity is understanding how many tokens are needed at each layer. Using our proposed optimal evaluation metric, we visualize and analyze the token requirements across different layers. As illustrated in Fig. 11, image tokens that are closely correlated with text tokens consistently exhibit higher projection values, and as the layer depth increases, the number of tokens with high projection values gradually decreases. This suggests that the model focuses on larger number of tokens in the earlier layers, and more tokens can be pruned in later layers, aligning with results from previous studies. 5. Conclusion This paper introduces CoreMatching, co-adaptive sparse inference framework that accelerates VLMs by leveraging the matching relationship between core neurons and core tokens. Additionally, we also propose more principled token measurement criterion and theoretically derive how activation layers influence token interactions, providing new insights into how VLMs process image and text information. CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models"
        },
        {
            "title": "Acknowledgements",
            "content": "Qinsi Wang, Jianyi Zhang and Yiran Chen disclose the support from NSF 2112562, ARO W911NF-23-2-0224, and NAIRR Pilot project NAIRR240270. We thank area chair and reviewers for their valuable comments."
        },
        {
            "title": "Impact Statement",
            "content": "This paper aims to advance the field of adaptive sparse inference for VLMs. We believe our work has significant potential for applications in deploying large models on resourceconstrained devices. While our research may have various societal implications, we do not find any particular aspect that requires special emphasis in this context."
        },
        {
            "title": "References",
            "content": "Alizadeh, K., Mirzadeh, I., Belenko, D., Khatamifard, K., Cho, M., Del Mundo, C. C., Rastegari, M., and Farajtabar, M. Llm in flash: Efficient large language model inference with limited memory. arXiv preprint arXiv:2312.11514, 2023. Bansal, N., Chen, X., and Wang, Z. Can we gain more from orthogonality regularizations in training deep networks? Advances in Neural Information Processing Systems, 31, 2018. Brown, T. B. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Chen, L., Zhao, H., Liu, T., Bai, S., Lin, J., Zhou, C., and Chang, B. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. In European Conference on Computer Vision, pp. 1935. Springer, 2025. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Garg, A. and Walker, P. Prediction of total knee motion using three-dimensional computer-graphics model. Journal of Biomechanics, 23(1):4558, 1990. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. Huang, W., Zhai, Z., Shen, Y., Cao, S., Zhao, F., Xu, X., Ye, Z., and Lin, S. Dynamic-llava: Efficient multimodal large language models via dynamic vision-language context sparsification. arXiv preprint arXiv:2412.00876, 2024. Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6700 6709, 2019. Kuo, M., Zhang, J., Ding, A., Wang, Q., DiValentin, L., Bao, Y., Wei, W., Li, H., and Chen, Y. H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint arXiv:2502.12893, 2025. Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Li, S., Jia, K., Wen, Y., Liu, T., and Tao, D. Orthogonal deep neural networks. IEEE transactions on pattern analysis and machine intelligence, 43(4):13521368, 2019. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in large visionlanguage models. arXiv preprint arXiv:2305.10355, 2023b. Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., and Yuan, L. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Lin, Y., Wang, Q., Ye, H., Fu, Y., Li, H., Chen, Y., et al. Hippomm: Hippocampal-inspired multimodal memory for long audiovisual event understanding. arXiv preprint arXiv:2504.10739, 2025. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024a. Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. 10 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Wang, Q., Ke, J., Liang, Z., and Zhang, S. Mathnas: if blocks have role in mathematical architecture design. Advances in Neural Information Processing Systems, 36: 4747547486, 2023. Wang, Q., Vahidian, S., Ye, H., Gu, J., Zhang, J., and Chen, Y. Coreinfer: Accelerating large language model inference with semantics-inspired adaptive sparse activation. arXiv preprint arXiv:2410.18311, 2024. Xue, Z., Song, Y., Mi, Z., Chen, L., Xia, Y., and Chen, H. Powerinfer-2: Fast large language model inference on smartphone. arXiv preprint arXiv:2406.06282, 2024. Ye, X., Gan, Y., Huang, X., Ge, Y., Shan, Y., and Tang, Y. Voco-llama: Towards vision compression with large language models. arXiv preprint arXiv:2406.12275, 2024. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2025a. Liu, Y., Sun, J., Lin, Y., Zhang, J., Yin, M., Wang, Q., Zhang, J., Li, H., and Chen, Y. Keyframe-oriented vision token pruning: Enhancing efficiency of large vision language models on long-form video processing. arXiv preprint arXiv:2503.10742, 2025b. Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pp. 2213722176. PMLR, 2023. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Qinsi, W., Ke, J., Tomizuka, M., Keutzer, K., and Xu, C. Dobi-svd: Differentiable svd for llm compression and some new perspectives. In The Thirteenth International Conference on Learning Representations. Shang, Y., Cai, M., Xu, B., Lee, Y. J., and Yan, Y. Llavaprumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Song, Y., Mi, Z., Xie, H., and Chen, H. Powerinfer: Fast large language model serving with consumer-grade gpu. arXiv preprint arXiv:2312.12456, 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023b. Wang, Q. and Zhang, S. Dgl: Device generic latency model for neural architecture search on mobile devices. IEEE Transactions on Mobile Computing, 23(2):19541967, 2023. 11 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Organization In this appendix, we provide in-depth descriptions of the materials that are not covered in the main paper, and report additional experimental results. The document is organized as follows: ARelated Work BAlgorithm CAssumption Explanation DExperiments Settings EAdditional Experiments FVisualization of Results A. Related Work A.1. Activation Sparsity In single Feed-Forward Network (FFN) block in LLMs, there are typically two linear layers, Wu, Wd RN 4N . For single token, denote the input representation of the FFN layer as x, the output can be expressed as: = σ(xWu), = AWd (16) where σ represents the activation function, such as ReLU or SiLU. Intermediate output = [a1, a2, . . . , a4N ], where an is the activation value of the n-th neuron. Previous work (Song et al., 2023; Xue et al., 2024; Wang & Zhang, 2023; Wang et al., 2023; Lin et al., 2025) indicates that individual tokens in LLMs exhibit significant activation sparsity. For example, in the OPT-30B, single token activates only approximately 10% of the neurons (Alizadeh et al., 2023). Therefore, if the activated neurons can be accurately predicted in advance, substantial amount of activation computation can be eliminated, thereby accelerating model inference without compromising performance. This potential has attracted considerable attention from researchers. For example, DejaVu (Liu et al., 2023) inserts an MLP predictor into each FFN block to forecast which neurons will be activated, achieving prediction accuracy of 93%. PowerInfer (Song et al., 2023) introduces the concepts of hot neurons and cold neurons to respectively represent frequently and rarely activated neurons, and accelerates inference by intelligently allocating hardware resources. LLM in Flash (Alizadeh et al., 2023) and PowerInfer2 (Xue et al., 2024) optimize this algorithm for mobile devices, thereby reducing the DRAM requirements for LLM inference on mobile phones. Notably, recent work CoreInfer (Wang et al., 2024) proposed sentence-level adaptive activation sparsity inference method without the need of predictors. CoreInfer identifies set of core neurons that most frequently and strongly activated for each input sentence. Experiments demonstrated that for given input sentence, LLMs require only static set of core neurons to maintain performance. Leveraging sentence-level sparsity, CoreInfer eliminates the need for frequent neuron switching during the decoding stage, achieving 10.33 speedup on NVIDIA Titan Xp GPU with negligible performance loss. The effectiveness of CoreInfer highlights the ability of core neurons to capture the most critical information of the input, underscoring their remarkable potential. In this paper, we further explore the characteristics of core neurons in VLMs, and propose novel one-pass co-adaptive sparsity inference. A.2. Token Sparsity The notorious quadratic complexity in Transformers is well-known issue and one of the key bottlenecks in scaling input sequence lengths. In VLMs, this problem becomes even more pronounced as the length of input image tokens increases. To address this challenge, multitude of adaptive token sparsity methods have been proposed (Huang et al., 2024)(Ye et al., 2024)(Shang et al., 2024)(Chen et al., 2025). For instance, Prumerge (Shang et al., 2024) observed the sparsity in the distribution of attention scores between class token and visual tokens, and employed attention scores as an evaluation metric to determine which tokens should be discarded. FastV (Chen et al., 2025) further explored and demonstrated the inefficiency of visual attention in VLMs, proposing plug-and-play approach that achieved 45% reduction in FLOPs on Llava-1.5-13b. These methods validate that VLMs require only small subset of important tokens to achieve nearly lossless performance. However, the acceleration and efficiency gains from purely token-level sparsity are limited, especially since the speedup benefits during the decoding phase are significantly diminished. In this paper, we thoroughly investigate the intrinsic relationship between token sparsity and neuron sparsity and propose co-adaptive sparsity method. Our method achieves comprehensive acceleration at nearly zero cost. 12 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models B. Algorithm The algorithm of CoreMatching is shown in Algorithm 1. In the pre-filling stage, we calculate core neurons in each FFN block and calculate core tokens in the l-th layer. After calculating the core tokens, we only use the core tokens in the subsequent inference process. In the decoding stage, we only use the retained core neurons for calculation. Algorithm 1 CoreVison: Co-adaptive sparse inference Input: Sentence = [x1, x2, ..., xM ]; Token-pruning layer L, Sparsity hyperparameters ρ, β. Step1: Pre-filling Stage for layer = 1, 2, ..., do Capture the activation of all tokens; for token = x1, x2, ..., xM do Record neurons with the largest ρ% activation as token-wise core neurons, Cρ(x). end Record the most frequent β% neurons in {Cρ(x1), Cρ(x2), . . . , Cρ(xM )} as sentence-wise core neurons Cβ Keep only core neurons Cβ ρ (s) on the device. ρ (s); if l=L then Record the neurons activated by each token Γ(x); ρ (s)(cid:12) Calculate the number of intersections (cid:12) (cid:12)Γ(x) Cβ (cid:12); Use maximum geometric distance to get threshold Tk; ρ (s)(cid:12) Select all xi that (cid:12) Keep only core tokens for inference. (cid:12) Tk as core tokens; (cid:12)Γ(xi) Cβ end end Step2: Decoding Stage for layer = 1, 2, ..., do Use Cβ ρ (s) kept in the pre-filling stage for inference. end C. Assumption Explanation In this section, we provide more comprehensive experimental proofs and discussions for the two observations proposed. For Observation 1, we show WQ@WK.T , WV @WV .T , WD@WD.T of all different layers of LLaVA-1.5-7B in Fig. 12, 13, and 14. It can be seen that different layers have this orthogonal relationship. In fact, the orthogonal relationship of matrices in neural networks has been studied since long time ago. In particular, (Li et al., 2019) proposed new regularization method that encourages the weight matrix of the neural network to maintain orthogonality during training by introducing self-orthogonality module. This method helps to improve the training stability and generalization ability of the model. (Bansal et al., 2018) explores adding orthogonal regularization to weights during training to improve training stability. The author proposed an orthogonal regularization method for weights, aiming to solve the gradient vanishing and explosion problems encountered by deep convolutional neural networks during training. It can be seen that modules with orthogonality are found in various different models to improve the training stability and performance of the model. To the best of our knowledge, we are the first work to intuitively show this orthogonal performance in LLM, which can be more fully explored in subsequent research. 13 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Figure 12. Visualization of WQ@WK .T at different layers in LLaVA-1.5-7b. Figure 13. Visualization of WD@WD.T at different layers in LLaVA-1.5-7b. Figure 14. Visualization of WV @WV .T at different layers in LLaVA-1.5-7b. 14 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models For Observation 2, we show the distribution of 32 layers of LLaVA-1.5-7b in Fig. 15. It can be seen that starting from the 4-th layer, cos((Ai, AM )) is proportional to the number of co-activated neurons. This is actually in line with intuition. On the one hand, the more neurons Ai and AM co-activate, the more non-zero values they have in the same positions, and the larger the product. On the other hand, Ai and AM have positive values in more of the same dimensions, indicating that their directions are closer. Figure 15. Visualization of cos((Ai, AM )) and co-act neurons number at different layers in LLaVA-1.5-7b. For proof of cos((yi, yM )) = cos((Ai, AM )), cos((yi, yM )) can be written as cos((yi, yM )) = AiWd, AM Wd/(yiyM ) )AT = Ai(WdW /(yiyM ) = ηAi, AM /(yiyM ), where η is constant based on Observation 1. Furthermore, since Wd is an orthogonal matrix, we have which means yi = yi2 = AiWd2 = (AiWd)(AiWd)T )AT = ηAiAT ηAi. Substituting this into Eq. 17 we can have = Ai(WdW i = ηAi2. cos((yi, yM )) = ηAi, AM /(ηAiAM ) = cos((Ai, AM )) This shows that the orthogonal matrix Wd does not change the angles between the input vectors. (17) (18) (19) D. Experiments Settings Models and Tasks. Our main experiments are conducted on LlaVA-1.5-7b and LlaVA-1.5-13b, using FP16 for all models. Following the same evaluation setting as LlaVA, we evaluate our methods on ten classical tasks, including VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VizWiz (Gurari et al., 2018), SciQA (Lu et al., 2022), TextVQA (Singh et al., 2019), POPE (Li et al., 2023b), MMBench (en) (Liu et al., 2025a), SEED (image) (Li et al., 2023a), and MM-Vet (Yu et al., 2023). We also provide additional results on more models in the Appendix. Hardware. We conduct experiments on two distinct hardware configurations. NNVIDIA Quadro RTX 6000 (24GB), representing high-performance hardware scenarios. In contrast, NVIDIA TITAN XP GPU (12G), representing lowperformance hardware scenarios. We also provide experimental results on more hardware devices in the Appendix. 15 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models Baselines. Given the multi-dimensional sparsity of CoreMatching, we compare our method against approaches using either token-level sparsity or neuron-level sparsity. For token sparsity, we compare with state-of-the-art token pruning methods such as PruMerge+, FastV, VoCo-LLaMA, LLaVA-HiRED, LLaVA-TRIM, and Dynamic-LLaVA. For neuron-level sparsity, since there has been no prior work specifically on activation sparsity in VLMs, we adapt the predictor from PowerInfer to Llava to emulate the time required for MLP-based activation sparsity on Llava. Implementation Details. CoreMatching uses the same hyperparameter settings across all activation layers for all models. For the computation of core neurons, we follow the hyperparameter setting from CoreInfer with ρ = 0.2, β = 0.4. For core tokens, we set = 2, consistent with FastV. E. Additional Experiments We supplement here the experimental results that are not included in the main text. E.1. Latency on NVIDIA A100 Fig. 16 shows the latency comparison on NVIDIA A100. It can be seen that CoreMatching can achieve excellent acceleration effects even without memory limit. And as the batch size increases, the acceleration effect becomes more and more obvious. Figure 16. Latency comparison of token-only/neurons-only/both sparse on NVIDIA A100. E.2. Experimental Results on LLaMA3.2 We show the experimental results of CoreMatching on LLaVA in the main text. In order to demonstrate the model generalization of CoreMatching, we further show the experimental results on LLaMA3.2 here. Since LLaMA-3.2 uses the crossAttention layer, the activation layer cannot be used to guide the token pruning of attention. We directly perform activation sparseness without token sparseness. The experimental results are shown in Tab. 8. For LLaMA3.2-11B, CoreMatching can still achieve near-lossless performance. This shows the applicability of CoreMatching to models of different architectures. Table 8. Experimental results of CoreMatching on LLaMA-3.2-11b. Methods TextVQA MME VQAV2 VisWiz POPE GQA Number Number 69.7 66.5 1384.5 1381.2 81.4 79.8 52.5 49.8 88.7 87.8 68.6 66. 16 CoreMatching: Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision-Language Models F. Visualization of Results In this section, we visualize the experimental results of CoreMatching. As shown in Fig. 17, for different inputs, CoreMatching can always retain the tokens that are important to the output. (a)What time is it now? (a)Sampled (b) What is the word on the button? (b) Sampled (c)Whats the word on the sign? (c)Sampled (d)What is the person in the picture wearing? (d)Sampled (e)What colour is the Orange? (e)Sampled (f)How many dogs in the picture? (f) Sampled (g)What colour is the cola? (g) Sampled (h)What colour is the bus? (h) Sampled Figure 17. Examples of CoreMatching sampled tokens for different inputs."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, Duke University, North Carolina, USA"
    ]
}