{
    "paper_title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
    "authors": [
        "Xiaolong Li",
        "Youping Gu",
        "Xi Lin",
        "Weijie Wang",
        "Bohan Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA"
        },
        {
            "title": "Start",
            "content": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Xiaolong Li * 1 Youping Gu * 1 Xi Lin * 1 Weijie Wang 1 Bohan Zhuang 1 5 2 0 2 3 ] . [ 1 5 2 0 4 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire keyvalue blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under low compute budget. It works with native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiencyquality tradeoffs. Our code and model weights are publicly available at: http://ziplab.co/PSA *Equal contribution. 1ZIP Lab, Zhejiang University. Email: Xiaolong Li <xiaolong.ziplab@gmail.com>, Youping Gu <youpgu71@gmail.com>, Xi Lin <erix025@outlook.com>, Weijie Wang <wangweijie@zju.edu.cn>, Bohan Zhuang <bohan.zhuang@gmail.com>. Figure 1. Comparison of attention mechanisms under identical compute budget. All methods use the same input Q, K, and tensors extracted from Wan2.11.3B (Wan et al., 2025) denoising process. Computation Pattern (top-left two panels): Normalized block-wise FLOPs distribution. The two panels plot query blocks on the horizontal axis and key blocks on the vertical axis. Despite identical FLOPs (20% full), the proposed Pyramid Sparse Attention (PSA) allows each query block to attend to much larger portion of KV blocks (70% active regions), whereas Block Sparse Attention (BSA) (Dao et al., 2022; Zhang et al., 2025a; Xu et al., 2025) restricts each query to only narrow subset of KV blocks (20% active regions), concentrating FLOPs in limited areas. Attention Output (bottom row): Resulting attention visualizations. PSA closely matches the Full Attention baseline with minimal relative error (< 3%), while BSA shows noticeable distortions due to aggressive pruning. 1. Introduction Recent advances in video generation and video understanding models have substantially increased sequence lengths, often reaching tens of thousands of tokens in modern diffusion transformers and autoregressive architectures (Vaswani et al., 2017; Peebles & Xie, 2023; Lin et al., 2024; Wang et al., 2024; Kong et al., 2025; Wan et al., 2025). However, the quadratic complexity of attention has become major obstacle to serving these models efficiently. In particular, attention computation dominates the prefill stage in long-context video understanding models and the end-to-end sampling process in video generation. For example, when processing high-resolution, long-duration videos, such as generating PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation 720p clip with 81 frames using the Wan2.114B (Wan et al., 2025) model, inference on single NVIDIA H20 GPU can take nearly two hours, with attention operations alone accounting for over 80% of the total runtime. This prohibitive cost underscores the urgent need for more efficient attention mechanisms capable of handling long-context video inputs. To alleviate this computational burden, recent studies exploit the inherent sparsity of the attention map = d(cid:1), where most elements become negliSoftmax(cid:0)QK / gible after softmax normalization (Deng et al., 2024). Sparse attention methods leverage this property by computing only the informative regions of , thereby reducing complexity without sacrificing much accuracy (Zhang et al., 2025a;c; Yang et al., 2025; Xi et al., 2025; Xu et al., 2025). In practice, the dominant paradigm combines mask generation strategy with Block Sparse Attention (BSA) kernel that performs attention only on selected blocks for hardware efficiency (Dao et al., 2022; Dao, 2024; Shah et al., 2024). This paradigm has proven highly effective in reducing attention cost while preserving generation quality, and series of methods following this paradigm such as XAttention (Xu et al., 2025) and SpargeAttention (Zhang et al., 2025a), have demonstrated excellent efficiencyquality trade-offs in large-scale video generation and understanding tasks. However, existing block sparse attention mechanisms suffer from fundamental limitation. At high sparsity levels, their hard binary keep/drop masks severely restrict the keyvalue region visible to each query, forcing the model to discard many informative areas and leading to significant information loss and performance degradation. Recent work such as Sparse VideoGen(SVG) (Xi et al., 2025; Yang et al., 2025), Sliding Tile Attention (Zhang et al., 2025c) introduce token permutation to concentrate more important keyvalue tokens within the limited visible block region of each query, thereby partially alleviating this issue. But this strategy conflicts with the design of the causal attention mask: after permutation, the causal mask becomes highly unstructured, making the algorithm difficult to implement efficiently. Moreover, the additional cost of reordering partially offsets the computational gains it aims to achieve. To this end, we propose Pyramid Sparse Attention (PSA), module that preserves rich contextual information under low compute budget while remaining compatible with both causal and bi-directional attention masking. The design of PSA is motivated by an empirical observation that adjacent key tokens in visual sequences exhibit strong local similarity ( Figure 2), suggesting that nearby keys can be aggregated by average pooling with minimal information loss. Moreover, larger pooling size indicates larger degree of information abstraction. Building on this insight, PSA replaces the binary keep/drop rule in BSA with multi-level pooled KV representations, where important KV Figure 2. Adjacent Key Token Cosine Similarity. High cosine similarity between key tokens (Qwen2.5-VL, Wan2.1-1.3B) motivates hierarchical pooling: nearby visual tokens are highly similar. blocks are assigned to lower (finer) pooling levels and less important blocks to higher (coarser) levels, and the least important blocks are entirely skipped to avoid redundant computation (see Figure 3), resulting in smoother computation allocation under the same compute budget, as shown in Figure 1. An intuitive analogy can be drawn to Feature Pyramid Networks (FPNs) (Lin et al., 2017) in dense prediction tasks, which assign objects of different scales to distinct feature levels. From quantization perspective, PSA generalizes BSAs 1-bit binary mask into multi-bit, fixed-point scheme. Here, each non-zero element indicates specific pooling level for the KV block, while zero value skips the block entirely, enabling finer-grained compute budget allocation. Moreover, PSA allows each query block to allocate computation adaptively by estimating the importance of each query-key block pair based on attention scores and generating multi-level attention mask accordingly. As result, as shown in Figure 1, under the same compute budget, each query in PSA attends to about 70% of the KV blocks on average, substantially expanding its receptive field, while BSA retains only 20% of the blocks and rigidly prunes context. Consequently, PSA produces attention outputs that are much closer to full attention, achieving superior balance between accuracy and efficiency. We empirically observe that PSA exhibits broad applicability across both video understanding and video generation tasks. On Video-MME (Fu et al., 2025a) with Qwen2.5VL (Qwen et al., 2025), PSA matches the full-attention accuracy while achieving the minimal compute budget that preserves quality (approximately 35% full attention FLOPs) among all sparse-attention baselines. For video generation, PSA consistently outperforms prior block-sparse mechanisms in the training-free setting and further improves VBench (Huang et al., 2024) scores when combined with the distillation framework TDM (Luo et al., 2025) on CogVideoX5B (Hong et al., 2023), even when limited to 2 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation only 15% of the full-attention compute budget. These results highlight PSAs strong compatibility and plug-and-play efficiency across diverse architectures and tasks. Our contributions are summarized as follows: (1) Multilevel retention beyond binary masks: PSA constructs pyramid of pooled KV blocks that expands each querys receptive field without increasing FLOPs, thereby mitigating performance degradation under low compute budget. (2) Hardware-friendly implementation: We adopt an efficient mergesplit kernel design that decouples the logical block size from the hardware tile size, making the processing of dynamically varying KV block sizes introduced by multi-level pooling more efficient. This design maintains efficient GPU utilization, supports backpropagation, and is fully compatible with FlashAttention (Dao, 2024; Dao et al., 2022). (3) Versatile applicability: Owing to our design that does not rely on token reordering, PSA can be seamlessly applied to both video understanding and generation models, consistently outperforming all Block-Sparse Attention based methods under low compute budget conditions. 2. Related Work The quadratic computational and memory cost of standard attention presents significant bottleneck for processing long sequences, particularly in tasks like video generation (Kong et al., 2025; Wan et al., 2025; Tan et al., 2025; Polyak et al., 2025; Hong et al., 2023) and understanding (Qwen et al., 2025; Zhang et al., 2023a; Madan et al., 2024; Xu et al., 2021; Yan et al., 2021). To overcome this limitation, sparse attention mechanisms have been developed, which apply mask to the attention matrix to reduce computations. These methods are broadly classified as static or dynamic. Static sparse attention. Static sparsity methods employ predefined, input-agnostic attention patterns (Zhang et al., 2025c; Hassani et al., 2023; Child et al., 2019; Li et al., 2025; Xiao et al., 2024b; Zhang et al., 2023b; Xiao et al., 2025; 2024a; Chen et al., 2025). These include established patterns such as sliding-window attention (Zhang et al., 2025c; Hassani et al., 2023; Zhang et al., 2023b; Xiao et al., 2024a; 2025), attention sink patterns (Zhu et al., 2025; Fu et al., 2025b; Xiao et al., 2024b), and spatiotemporal energy decay patterns (Li et al., 2025). While computationally efficient, the primary drawback of these methods is their inherent rigidity. Because the patterns are fixed and inputagnostic, they risk missing critical long-range dependencies, which can lead to sub-optimal performance and potentially unstable generation quality. Dynamic sparse attention. To address the limitations of static patterns, dynamic sparsity methods were introduced. These methods generate an input-sensitive mask during the forward pass, for instance, by thresholding attention scores (Zhang et al., 2025b; Gu et al., 2025; Xu et al., 2025; Yang et al., 2025; Xi et al., 2025; Lu et al., 2025; Yuan et al., 2025; Song et al., 2025). However, while these contentaware, element-wise adaptive masks offer greater flexibility, they introduce new challenge: hardware inefficiency. The unstructured, sparse matrix operations resulting from these fine-grained masks lead to poor memory access patterns and low hardware utilization, particularly on parallel processors like GPUs. Block sparse attention. The challenge of hardware utilization motivated the development of Block Sparse Attention (BSA) (Dao, 2024; Dao et al., 2022; Xu et al., 2025; Gu et al., 2025; Guo et al., 2024). This approach introduces hardware-aware sparse structure by partitioning the Q, K, and attention matrices into coarse-grained blocks. binary block mask then dictates whether an entire blocks computation is performed or skipped. By operating at this block level, BSA preserves the dense matrix operations within each block, which is essential for achieving high throughput on modern GPUs. However, BSA still relies on rigid, binary decision at the block level, which can lead to significant information loss under high sparsity. In contrast, our work introduces multi-level pooled KV pyramid, allowing each query to access substantially larger receptive field under the same computational budget and critically mitigating the information loss endemic to high-sparsity binary masking. Moreover, our design does not rely on token reordering, ensuring versatile and seamless applicability across both video understanding and generation models while maintaining high GPU throughput via an efficient kernel. 3. Methodology We propose Pyramid Sparse Attention (PSA), hierarchical sparse attention framework that allocates computation adaptively across multi-level pooled key-value (KV) representations. PSA consists of three components: (1) Pyramid KV Blocks that capture coarse-to-fine context, (2) MultiLevel Mask Generator that predicts hierarchical sparsity, and (3) Adaptive Pyramid Attention that fetches KV blocks and computes attention accordingly (see Figure 3). 3.1. Problem Definition and Notation We consider the standard attention formulation with query, key, and value sequences RN d, K, RN d, where is the sequence length and is the hidden dimension. The full attention is computed as Attn(Q, K, ) = Softmax (cid:19) (cid:18) QK V, (1) whose quadratic complexity O(N 2) becomes the bottleneck for long sequences, for example in video understanding or 3 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation along the sequence dimension: 1 = Ki, 1 = Vi, h+1 h+1 = MeanPool(K = MeanPool(V , 2, 2), , 2, 2), (2) (3) where MeanPool(x, k, s) denotes 1D mean pooling with kernel size and stride along the sequence dimension. , 2 , . . . , , . . . , } and {V 1 Through this process, we obtain sets of pyramid KV blocks {K 1 , 2 }, where the hth level block represents token group of size bk 2h1 . As increases, the representation becomes increasingly coarser, summarizing broader contextual information within each block. This hierarchical pooling reduces the effective KV length by factor of 2h1 at pyramid level h, enabling the model to access coarse-to-fine contextual representations and dynamically balance accuracy and efficiency under limited compute budget. Figure 3. Overview of the Pyramid Sparse Attention (PSA) framework. PSA adaptively allocates attention computation across hierarchical KV representations (green; lighter shades denote coarser levels). The multi-level mask (blue) determines which KV level each query block attends to. As illustrated, the current attention block assigned to level 4 uses the coarsest KV representation 4 and 4 . generation. To reduce computation and scale attention to long sequences, block sparse attention is commonly adopted to restrict attention computation to subset of token blocks. The query and KV sequences are divided into non-overlapping blocks of size bq and bk, resulting in nq = N/bq query blocks and nk = N/bk KV blocks. The attention is then computed block-wise between query blocks {Qi}nq i=1 and KV blocks {(Kj, Vj)}nk j=1. binary attention mask {0, 1}nqnk is typically used in block sparse attention, where Mij = 1 indicates that query block Qi attends to KV block (Kj, Vj). However, such binary masking enforces hard keep-or-drop decision, which not only causes information loss from discarded regions but also limits how computation can be flexibly allocated across blocks of varying importance. To address this limitation, we introduce multi-level representation of KV blocks and multi-level mask {0, 1, . . . , H}nqnk , allowing hierarchical and adaptive sparsity control. This serves as the foundation of our proposed PSA, which dynamically allocates compute budget across coarse-to-fine levels. We detail the key design components in the following sections. 3.2. Pyramid KV Blocks To construct multi-level KV representations, we build hierarchical pyramid of levels by progressively pooling 3.3. Multi-Level Mask Generator Given the multi-level KV hierarchy, we estimate the importance of each querykey block pair and generate multilevel mask to guide adaptive attention computation. 3.3.1. BLOCK IMPORTANCE ESTIMATION In this step, we compute lightweight importance score Sij for each querykey block pair (Qi, Kj), which reflects how crucial it is to preserve fine-grained attention information within that region. To leverage domain-specific characteristics, we employ different importance estimators for video generation and understanding tasks. For video generation, we employ sampling-based strategy to approximate block importance efficiently. Specifically, small subset of tokens is randomly sampled from both the query and key blocks, denoted as Qi Rsqd and Kj Rskd, where sq bq and sk bk. The importance score is then estimated as the maximum attention score between the sampled tokens: (cid:32) Sij = max Softmax (cid:32) Qi j (cid:33)(cid:33) . (4) To improve locality and stabilize both the sampling estimation and the pyramid pooling, we apply Hilbert curve permutation (Zhang et al., 2025a) to the base sequences before computation. For video understanding, we adopt the antidiagonal scoring (Xu et al., 2025), and introduce intra-block similarity verification to address the relatively low token similarity in video understanding (Figure 2). This verification compares the cosine similarity of adjacent tokens against level-specific thresholds, which directly sets maximum acceptable pooling level for the block. PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation i=1, pyramid KV blocks j=1,h=1, mask , scale factor 1/ Initialize oi 0, mi , li 0 for each key block Kj do {K , Mij if = 0 then Algorithm 1 Computation of PSA Require: Query blocks {Qi}nq }nk,H Ensure: Output blocks {Oi}nq i=1 1: for each query block Qi do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: Oi oi/li 16: end for 17: return {Oi}nq i=1 end for continue {Skip pruned block} + (h 1) ln 2 end if Sij QiK / mij max(rowmax(Sij), mi) Pij exp(Sij mij) lij li exp(mi mij) + rowsum(Pij) oi oi exp(mi mij) + PijV li lij mi mij, These variants demonstrate that PSA is agnostic to the specific importance estimator and readily adapts to various architectures and tasks. 3.3.2. MULTI-LEVEL MASK ASSIGNMENT Based on the estimated importance scores S, we generate multi-level mask to guide adaptive attention computation, as detailed in Algorithm 2. We introduce multi-level mask to assign pyramid levels dynamically. Each entry Mij {0, 1, 2, . . . , H} specifies which level of pyramid KV block (Kj, Vj) should be fetched when computing attention for the query block Qi: Mij = > 0 use , , Mij = 0 skip. (5) Algorithm 2 Multi-Level Mask Assignment Require: Importance map Rnqnk , thresholds = {τ1, . . . , τH } Sij {Normalize row} i, πi) sort desc(Ei) Ensure: Multi-level mask {0, . . . , H}nqnk 1: zeros like(S) 2: for = 1 to nq do Ei Si/ (cid:80) 3: (E 4: ˆEi cumsum(E i) 5: for = 1 to nk do 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: return πi(j) {Map to sorted index} if ˆEij > τH then Mij min{t ˆEij τt} end if end for Mij 0 else levels, the thresholds are defined as: = τ1, τ2, . . . , τH , 0 τ1 τ2 τH 1. (6) These thresholds specify the score budgets allocated to each level, enabling fine-grained sparsity control. The pyramid level for each querykey block pair is assigned as: Mij = (cid:40) min{ ˆSij τt }, 0, if ˆSij τH if ˆSij > τH . (7) Unlike quantile-based strategies with fixed per-level quotas, our threshold-based strategy dynamically adjusts sparsity according to the cumulative importance score distribution of each query. This adaptive mechanism allows flexible computation allocation and leads to more accurate attention estimation (see Section 4.5 for analysis). larger indicates lower block importance and coarser KV representations, while Mij = 0 corresponds to skipping the block. This formulation allows gradual degradation of precision instead of binary drop, effectively mitigating abrupt information loss at high sparsity. We leverage threshold-based masking strategy to flexibly control the sparsity among query blocks. Specifically, we first normalize the importance scores row-wise, and then compute the cumulative scores ˆSij by summing the descending sorted importance scores for each query block. To translate these cumulative scores into multi-level sparsity, we define set of thresholds as hyper-parameters that determine the pyramid level assignment. For pyramid with 3.4. Adaptive Pyramid Attention With the pyramid KV representations and the multi-level mask constructed, we now compute the attention in blockwise manner. For each block, we fetch the corresponding key/value block based on the assigned pyramid level in the mask according to Equation (5) and compute attention accordingly. However, KV blocks in level have reduced sequence lengths due to pooling, meaning each token in represents an aggregated context of 2h1 original tokens. To maintain consistent probability distribution in attention weights after softmax, we introduce scaling factor of 2h1 to the attention scores. This is efficiently implemented by adding 5 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Figure 4. Qualitative comparison on Wan2.1-1.3B (Text-to-Video, 720p). bias term (h 1) ln 2 to the attention logits before softmax normalization. Specifically, the attention score between query block Qi and key block Kj at level is computed as: (cid:32) Aij = Softmax QiK j (cid:33) + (h 1) ln 2 . (8) The mechanism is detailed in Algorithm 1. Complexity analysis. At pooling level h, the effective sequence length of each KV block is reduced by factor of 2h1, resulting in an attention cost of O(bqbk/2h1) for that attention block. Aggregating across all levels based on the mask distribution yields an overall expected complexity of O(ρN 2), where ρ is the effective sparsity ratio induced by the multi-level mask: ρ = 1 nqnk nq (cid:88) nk(cid:88) i=1 j=1 ρij, with ρij = (cid:40) 2Mij 1 , Mij > 0 Mij = 0 0, (9) Compared with standard block-sparse-based attention mechanisms, PSA distributes the same compute budget more efficientlyassigning finer-grained attention to informative regions and coarser attention to redundant ones. This adaptive allocation allows the model to retain more critical details under the same computational cost, improving representational fidelity without increasing total FLOPs. 6 3.5. Hardware-Optimized Kernel Design To ensure that PSA can be deployed efficiently on modern accelerators, we complement the algorithmic design with hardware-aware implementation. The key challenge is that existing implementations often couple the logical block size with the hardware tile size, although these two hyperparameters serve fundamentally different purposes. The block size captures the logical grouping of tokens while the tile size must be selected to maximize hardware throughput. This mismatch becomes especially pronounced in PSA because KV block pooling produces heterogeneous and often small blocks at coarser pyramid levels. Using these various block sizes directly as the execution tile leads to low compute utilization or warp divergence when the tile configuration changes dynamically. To resolve this issue, we adopt decoupled block-tile design that separates logical sequence blocks from the execution tiles used by the kernel. Building on modified FlashAttention kernel, blocks are flexibly split or merged to match hardware-optimized tile size. This approach allows the block size to follow the attention pattern while the tile size is tuned independently for accelerator efficiency. The design integrates seamlessly with kernel fusion, optimized memory access, and fine-grained parallelization. The decoupled design offers broad compatibility with attention mechanisms for blocked sequences, enabling them to handle heterogeneous block structures while maintaining hardware efficiency. In practice, our kernel preserves high tensor-core utilization even for small pooled blocks, avoids divergence across pooling levels, and achieves up to 10 speedup over naive PSA implementation on NVIDIA H200. PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Table 1. Quantitative comparison on Wan-series models in training-free videogen experiments. We report similarity metrics (PSNR, SSIM, LPIPS) and perceptual quality measures (Aesthetic Quality (Aes.), Background Consistency (Bkg.), and Imaging Quality (Img.)) from VBench (Huang et al., 2024). Latency represents the average generation time per video. For clarity, the best result among all sparse methods under each metric is bolded, and the second-best is underlined. Model Method PSNR SSIM LPIPS Aes. Bkg. Img. Sparsity Latency(s) Wan 2.1 1.3B Wan 2.2 5B (1280 704, 121 frames) Wan 2.1 14B Full SVG2 SVG Sparge STA PSA(Ours) Full SVG2 SVG Sparge PSA(Ours) Full SVG2 SVG Sparge STA PSA(Ours) 25.21 17.57 22.83 20.56 24.36 24.25 18.89 19.53 23.03 24.79 19.84 22.19 20.83 23.83 0.801 0.567 0.736 0.677 0.788 0.818 0.645 0.660 0.794 0.807 0.649 0.737 0.694 0. 0.126 0.399 0.177 0.197 0.121 0.092 0.266 0.229 0.096 0.085 0.300 0.182 0.185 0.105 0.6489 0.6185 0.5039 0.6232 0.6521 0.6686 0.6598 0.6495 0.5539 0.5482 0.6588 0.6918 0.6614 0.5337 0.6083 0.6544 0. 0.9645 0.9548 0.9444 0.9476 0.9419 0.9612 0.9564 0.9518 0.9386 0.9289 0.9569 0.9639 0.9439 0.9501 0.8779 0.9399 0.9261 0.6557 0.5545 0.5974 0.6409 0.6501 0.6607 0.6547 0.6025 0.5877 0.5650 0.6438 0.6247 0.5555 0.5479 0.5977 0.6489 0. 0.91 0.85 0.90 0.83 0.91 0.90 0.86 0.89 0.89 0.87 0.85 0.88 0.83 0.88 327 187 165 165 162 176 168 149 122 124 131 1548 913 830 855 815 4. Experiments 4.1. Experimental Settings Models. We evaluate PSA on open-source video generation models and video understanding models: Wan2.1- {1.3B, 14B} (Wan et al., 2025), Wan2.2-5B (Wan et al., 2025), CogVideoX-5B (Hong et al., 2023), and Qwen2.5VL-7B (Qwen et al., 2025). Baselines. We compare our method, Pyramid Sparse Attention (PSA), against several representative sparse attention baselines, including Sparse VideoGen (SVG) (Xi et al., 2025), Sparse VideoGen2 (SVG2) (Yang et al., 2025), Sliding-Tile Attention (STA) (Zhang et al., 2025c), SpargeAttention (Sparge) (Zhang et al., 2025a), and XAttention (Xu et al., 2025). For brevity, we use abbreviated names (e.g., SVG2, STA, PSA) throughout the remainder of this section. Implementation details. All experiments are conducted on NVIDIA H200 GPUs. Unless stated, videos are generated at 1280768 / 69 frames. All sparse baselines are evaluated using their official implementations without any additional optimization tricks. In video understanding experiments, PSA reuses the same block-importance estimation based on antidiagonal scoring and incorporates an additional similarity-based constraint, as detailed in Section 3.3.1 . All sparse attention mechanisms are applied only during the prefill stage. On sparsity accounting. PSA allocates multi-level compute to KV blocks rather than using binary keep/drop strategy; its reported sparsity thus denotes the sparsity of BSAbased method that uses the same compute budget. Metrics and dataset. For the training-free video generation experiments, we evaluate the similarity between the generated videos and their full-attention counterparts using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018). Beyond these similarity measures, we further adopt three perceptual dimensions from the VBench Score (Huang et al., 2024)widely used in recent video generation benchmarks to assess visual quality: Aesthetic Quality (Aes.), Background Consistency (Bkg.), and Imaging Quality (Img.). For the distillation experiments on CogvideoX-5B (Hong et al., 2023), we primarily use the VBench Score to measure the perceptual quality of videos generated by distilled models. Our distillation process is guided by dataset of 10,000 text prompts, sampled from the JourneyDB benchmark (Sun et al., 2023). To enhance prompt quality and diversity, each sample is refined using the Qwen2.5-3BInstruct model (Qwen et al., 2025). For video understanding, we adopt the Video-MME (1fps) dataset (Fu et al., 2025a) to evaluate the performance of Qwen2.5-VL-7B in video understanding scenarios. 4.2. Training-Free Video Generation qualitative comparison of sparse attention mechanisms is shown in Figure 4. All methods use the same text-to-video scene (a couple walking under red umbrella in the rain) under similar sparsity. STA (Zhang et al., 2025c) exhibits 7 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Table 2. Combining PSA with TDM on CogVideoX-5B. Method Sparsity Sampling Steps VBench Score FullAttn Distill-only Ours 0.85 50 4 4 0.819 0.818 0.826 Table 3. Comparison of attention mechanisms on Video-MME. Method Short Medium Long Overall Sparsity Full Attention XAttention SpargeAttention PSA(Ours) 0.752 0.748 0.749 0.748 0.663 0.661 0.663 0.673 0.537 0.544 0.539 0. 0.651 0.651 0.650 0.654 0.58 0.37 0.65 structure jumps and identity swaps across frames; SVG (Xi et al., 2025)collapses at high sparsity with random color blocks; SVG2 (Yang et al., 2025) is more stable but overly smooth with distorted backgrounds; Sparge (Zhang et al., 2025a) preserves layout yet shows local color distortions; Full Attention remains high-fidelity but costly. In contrast, PSA at 0.91 sparsity maintains sharp details and temporal coherence (stable contours, saturated umbrella, natural reflections), approaching the dense baselines visual quality with far less computation. Table 1 further shows that under comparable high sparsity, PSA consistently surpasses SVG, STA, and Sparge on all similarity metrics (PSNR/SSIM/LPIPS) and on most perceptual axes (Aes./Bkg./Img.) across model sizes. Against the strongest training-free baseline SVG2, PSA delivers clearly better perceptual quality at high sparsityhigher Aesthetic and Imaging scores on all model sizes while keeping similarity metrics within small margins. Together with the visuals in Figure 4, these results indicate that PSA is the most quality-preserving sparse attention mechanism at high sparsity among all compared methods. Additional visual comparisons are provided in the appendix. 4.3. Distillation To explore the generality of PSA and achieve maximum inference acceleration, we combined our method with the data-free distillation technique TDM (Luo et al., 2025; Gu et al., 2025). We integrated PSA into the student model during the distillation training phase. As shown in Table 2, this simple combination achieves 30.2 denoising time speedup over the original 50-step model, without any loss in generation quality. Table 4. Comparison of multi-level vs. binary masking on Wan2.11.3B (480p, training-free). The first 25% of sampling steps do not use sparse attention. = {τ1, . . . , τH } are the cumulative thresholds for level assignment. Method Sparsity PSNR SSIM LPIPS Multi-level mask Binary mask 0.79 0. 23.35 23.11 0.856 0.851 0.116 0.122 Table 5. Ablation of reordering and mask-generation strategy on Wan2.1-1.3B (480p). Mode Reorder Sparsity PSNR SSIM LPIPS Threshold-based Hilbert Hilbert Quantile-based None Quantile-based 0.84 0.85 0.85 23.78 22.54 21.81 0.811 0.775 0. 0.085 0.118 0.201 4.4. Training-Free Video Understanding On Qwen2.5-VL-7B, PSA delivers the best overall performance on Video-MME (Fu et al., 2025a) under the highest sparsity level (0.65). As shown in Table 3, it matches or exceeds the full baseline across most categoriesnotably improving the Medium and Long-Video scoreswhile remaining competitive on Short Video. These results show that PSA preserves (and in some cases improves) video-understanding quality at substantially higher sparsity, demonstrating the strongest quality-retaining sparsity among all compared methods in the training-free setting. 4.5. Ablation Study Effectiveness of multi-level mask vs. binary mask. In terms of configuration, the multi-level mask group is set to = {0.70, 0.80, 0.90, 0.90}, while binary mask uses level ratios = {0.85, 0.85, 0.85, 0.85}, which means that only dense KV blocks are kept and no pooled representation is used. On Wan2.1-1.3B (480p, training-free), we compare our multi-level masking with conventional 0/1 binary mask. The first 25% of sampling steps do not use sparse attention. As shown in Table 4, PSA yields improved PSNR and SSIM, and reduced LPIPS, despite operating under higher sparsity setting than the baselines. Reordering & mask strategy (Videogen). We evaluate different reordering and mask-generation strategies on Wan2.1-1.3B (480p, training-free setting) and the first 25% of sampling steps do not use sparse attention. Our combined approach, using 85% sparsity, achieves the highest VBench score (0.826). This surpasses both the 4step distilled baseline and even the original 50-step model. This result demonstrates that PSA is highly compatible plug-and-play module that can be compounded with other methods like distillation to maximize inference efficiency. We compare two multi-level masking rules (threshold-based vs. quantile-based) and the role of token reordering. The quantile-based rule processes each query row by sorting block importance Si,: and assigning pooling degrees by fixed percentile cut points 0 < < < 1 (e.g., [0, a) 1, [a, b) 2, [b, c) 3, [c, d) 4, (d, 1] 8 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation 0). For reordering, we follow SpargeAttention and apply HilbertCurve permutation to Q, K, (Zhang et al., 2025a). Hilbert denotes Hilbert-curvebased permutation, while None indicates no reordering. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. As shown in Table 5, under near-matched sparsity, Threshold-based + reordering consistently outperforms quantile-based + reordering in reconstruction quality. Removing the reordering step from the quantile-based variant further degrades results. As observed in SpargeAttention (Zhang et al., 2025a), this reordering operation enhances the similarity between adjacent tokens. When incorporated into PSA, this property naturally complements the hierarchical pooling mechanism, as higher local token similarity leads to smoother multi-level aggregation and more stable reconstruction under high sparsity. In contrast to the quantile-based rule, the threshold-based strategy dynamically adjusts the proportion of each mask based on the input distribution, allowing PSA to better adapt to diverse sparsity patterns and preserve visual fidelity. Additional ablations on the similarity constraint and variable block sizes, which also influence the performance of PSA, are presented in the supplementary material 5. Conclusion We propose Pyramid Sparse Attention (PSA), versatile sparse attention mechanism validated across both video understanding and video generation tasks. Our method introduces multi-level sparsity beyond binary keep/skip choices, where higher sparsity levels use larger pooling degrees of KV representations; in this way, unlike block-sparse attention that fully drops low-score blocks, we preserve substantially more information under the same compute budget. As result, PSA demonstrates strong generalization and consistently outperforms all existing Block Sparse Attention based methods under low compute budget, highlighting its robustness across diverse model architectures and application scenarios."
        },
        {
            "title": "References",
            "content": "Chen, P., Zeng, X., Zhao, M., Ye, P., Shen, M., Cheng, W., Yu, G., and Chen, T. Sparse-vdit: Unleashing the power of sparse attention to accelerate video diffusion transformers, 2025. URL https://arxiv.org/abs/2506. 03065. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers, 2019. URL https://arxiv.org/abs/1904.10509. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Deng, Y., Song, Z., and Yang, C. Attention is naturally sparse with gaussian distributed input. CoRR, 2024. Fu, C., Dai, Y., Luo, Y., Li, L., Ren, S., Zhang, R., Wang, Z., Zhou, C., Shen, Y., Zhang, M., et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025a. Fu, T., Huang, H., Ning, X., Zhang, G., Chen, B., Wu, T., Wang, H., Huang, Z., Li, S., Yan, S., et al. Moa: Mixture of sparse attention for automatic large language model compression. In ICLR 2025 Workshop on Foundation Models in the Wild, 2025b. Gu, Y., Li, X., Hu, Y., Chen, M., and Zhuang, B. Blade: Block-sparse attention meets step distillation for efficient video generation, 2025. URL https://arxiv.org/ abs/2508.10774. Guo, J., Tang, H., Yang, S., Zhang, Z., Liu, Z., and Han, S. Block Sparse Attention. https://github.com/ mit-han-lab/Block-Sparse-Attention, 2024. Hassani, A., Walton, S., Li, J., Li, S., and Shi, H. Neighborhood attention transformer. In CVPR, pp. 61856194, June 2023. Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In ICLR, 2023. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., Wu, K., Lin, Q., Yuan, J., Long, Y., Wang, A., Wang, A., Li, C., Huang, D., Yang, F., Tan, H., Wang, H., Song, J., Bai, J., Wu, J., Xue, J., Wang, J., Wang, K., Liu, M., Li, P., Li, S., Wang, W., Yu, W., Deng, X., Li, Y., Chen, Y., Cui, Y., Peng, Y., Yu, Z., He, Z., Xu, Z., Zhou, Z., Xu, Z., Tao, Y., Lu, Q., Liu, S., Zhou, D., Wang, H., Yang, Y., Wang, D., Liu, Y., Jiang, J., and Zhong, C. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. 9 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Li, X., Li, M., Cai, T., Xi, H., Yang, S., Lin, Y., Zhang, L., Yang, S., Hu, J., Peng, K., Agrawala, M., Stoica, I., Keutzer, K., and Han, S. Radial attention: o(n log n) sparse attention with energy decay for long video generation, 2025. URL https://arxiv.org/abs/2506. 19852. Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., and Yuan, L. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 59715984, 2024. Lin, T.-Y., Dollar, P., Girshick, R., He, K., Hariharan, B., and Belongie, S. Feature pyramid networks for object In Proceedings of the IEEE conference on detection. computer vision and pattern recognition, pp. 21172125, 2017. Lu, E., Jiang, Z., Liu, J., Du, Y., Jiang, T., Hong, C., Liu, S., He, W., Yuan, E., Wang, Y., Huang, Z., Yuan, H., Xu, S., Xu, X., Lai, G., Chen, Y., Zheng, H., Yan, J., Su, J., Wu, Y., Zhang, N. Y., Yang, Z., Zhou, X., Zhang, M., and Qiu, J. Moba: Mixture of block attention for long-context llms, 2025. URL https: //arxiv.org/abs/2502.13189. Luo, Y., Hu, T., Sun, J., Cai, Y., and Tang, J. Learning fewstep diffusion models by trajectory distribution matching. In ICCV, 2025. Madan, N., Moegelmose, A., Modi, R., Rawat, Y. S., and Moeslund, T. B. Foundation models for video understanding: survey, 2024. URL https://arxiv.org/ abs/2405.03770. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., Yan, D., Choudhary, D., Wang, D., Sethi, G., Pang, G., Ma, H., Misra, I., Hou, J., Wang, J., Jagadeesh, K., Li, K., Zhang, L., Singh, M., Williamson, M., Le, M., Yu, M., Singh, M. K., Zhang, P., Vajda, P., Duval, Q., Girdhar, R., Sumbaly, R., Rambhatla, S. S., Tsai, S., Azadi, S., Datta, S., Chen, S., Bell, S., Ramaswamy, S., Sheynin, S., Bhattacharya, S., Motwani, S., Xu, T., Li, T., Hou, T., Hsu, W.-N., Yin, X., Dai, X., Taigman, Y., Luo, Y., Liu, Y.-C., Wu, Y.-C., Zhao, Y., Kirstain, Y., He, Z., He, Z., Pumarola, A., Thabet, A., Sanakoyeu, A., Mallya, A., Guo, B., Araya, B., Kerr, B., Wood, C., Liu, C., Peng, C., Vengertsev, D., Schonfeld, E., Blanchard, E., Juefei-Xu, F., Nord, F., Liang, J., Hoffman, J., Kohler, J., Fire, K., Sivakumar, K., Chen, L., Yu, L., Gao, L., Georgopoulos, M., Moritz, R., Sampson, S. K., Li, S., Parmeggiani, S., Fine, S., Fowler, T., Petrovic, V., and Du, Y. Movie gen: cast of media foundation models, 2025. URL https://arxiv.org/abs/2410.13720. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: fast and accurate attention with asynchrony and low-precision. In Proceedings of the 38th International Conference on Neural Information Processing Systems, pp. 6865868685, 2024. Song, E., Chai, W., Yang, S., Armand, E., Shan, X., Xu, H., Xie, J., and Tu, Z. Videonsa: Native sparse attention scales video understanding, 2025. URL https: //arxiv.org/abs/2510.02295. Sun, K., Pan, J., Ge, Y., Li, H., Duan, H., Wu, X., Zhang, R., Zhou, A., Qin, Z., Wang, Y., et al. Journeydb: benchmark for generative image understanding. Advances in neural information processing systems, 36:4965949678, 2023. Tan, X., Chen, Y., Jiang, Y., Chen, X., Yan, K., Duan, N., Zhu, Y., Jiang, D., and Xu, H. Dsv: Exploiting dynamic sparsity to accelerate large-scale video dit training, 2025. URL https://arxiv.org/abs/2502.07590. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models, 2025. URL https: //arxiv.org/abs/2503.20314. 10 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, 2024. Zhang, H., Li, X., and Bing, L. Video-llama: An instructiontuned audio-visual language model for video understanding. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023a. Zhang, J., Xiang, C., Huang, H., Xi, H., Zhu, J., Chen, J., et al. Spargeattention: Accurate and training-free sparse In Fortyattention accelerating any model inference. second International Conference on Machine Learning, 2025a. Zhang, P., Chen, Y., Huang, H., Lin, W., Liu, Z., Stoica, I., Xing, E. P., and Zhang, H. Faster video diffusion with trainable sparse attention. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. Zhang, P., Chen, Y., Su, R., Ding, H., Stoica, I., Liu, Z., and Zhang, H. Fast video generation with sliding tile attention. In Forty-Second International Conference on Machine Learning, 2025c. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., Wang, Z. A., and Chen, B. H2o: Heavy-hitter oracle for efficient generative inference of large language In Oh, A., Naumann, T., Globerson, A., models. Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3466134710. Curran Associates, Inc., 2023b. URL https://proceedings.neurips. cc/paper_files/paper/2023/file/ 6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference. pdf. Zhu, Q., Duan, J., Chen, C., Liu, S., Li, X., Feng, G., Lv, X., Chuanfu, X., Lin, D., and Yang, C. Sampleattention: Near-lossless acceleration of long context llm inference with adaptive structured sparse attention. In Eighth Conference on Machine Learning and Systems, 2025. Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., Chen, J., Stoica, I., Keutzer, K., and Han, S. Sparse videogen: Accelerating video diffusion In Fortytransformers with spatial-temporal sparsity. Second International Conference on Machine Learning, 2025. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., and Sun, M. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. Advances in Neural Information Processing Systems, 37: 119638119661, 2024a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In ICLR, 2024b. Xiao, G., Tang, J., Zuo, J., Yang, S., Tang, H., Fu, Y., Han, S., et al. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. In The Thirteenth International Conference on Learning Representations, 2025. Xu, H., Ghosh, G., Huang, P.-Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., and Feichtenhofer, C. Videoclip: Contrastive pre-training for zero-shot videotext understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 67876800, 2021. Xu, R., Xiao, G., Huang, H., Guo, J., and Han, S. Xattention: Block sparse attention with antidiagonal scoring. In FortySecond International Conference on Machine Learning, 2025. Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. Videogpt: Video generation using vq-vae and transformers, 2021. URL https://arxiv.org/abs/2104.10157. Yang, S., Xi, H., Zhao, Y., Li, M., Zhang, J., Cai, H., Lin, Y., Li, X., Xu, C., Peng, K., Chen, J., Han, S., Keutzer, K., and Stoica, I. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. In NeurIPS, 2025. Yuan, J., Gao, H., Dai, D., Luo, J., Zhao, L., Zhang, Z., Xie, Z., Wei, Y., Wang, L., Xiao, Z., et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2307823097, 2025. 11 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation A. Appendix Due to space limitations in the main paper, we provide additional technical details, ablation studies, and qualitative comparisons in this appendix. The material is organized as follows: Sec. B: Implementation Details of PSA. Detailed description of PSA modules, including the cosine similarity-based pooling constraint, block size variants, and importance pooling operators. Sec. C: Additional Ablations. Comprehensive ablation experiments on cosine similarity, block size, pooling operators, and multi-level allocation strategies across video understanding and video generation tasks. Sec. D: Additional Visual Comparisons. Extended qualitative examples for the training-free video generation evaluation, following the same layout as the main paper. The following sections provide the full details and results. B. Implementation Details of PSA B.1. Cosine Similarity Based Pooling Constraint On top of the importance driven mask , PSA optionally incorporates cosine similarity based pooling constraint. This module is inspired by the cosine similarity constraint design in SpargeAttention (Zhang et al., 2025a), but we generalize it to our multi-level pooling regime. The intuition is that key blocks whose tokens are internally similar can be safely assigned to coarser pyramid levels, whereas blocks with heterogeneous tokens should remain at finer levels. For PSA with levels, we introduce 1 cosine similarity thresholds Ts = {τ (2) , τ (3) , . . . , τ (H) }, where each τ (h) specifies the minimum intra-block cosine similarity required for block to be eligible for level h. Given these thresholds, we compute the per-block maximum admissible level {1, . . . , H}nk using Algorithm 3, which evaluates intra-block similarities at strides corresponding to different pooling sizes. block may enter level only if its stride-2h1 similarity exceeds τ (h) . The final mask used by PSA is obtained by combining this constraint with the importance-driven mask via Mij = min(cid:0)Mij, Lj (cid:1), ensuring that no block is assigned level coarser than what its similarity permits. In our experiments, we consider pyramid with = 4 levels and use different similarity thresholds (τ2, τ3, τ4) depending on the task. For Video-MME, we adopt thresholds of (0.75, 0.70, 0.70), while for the video generation preset PSA (sim) in Table 7, we use (0.70, 0.65, 0.60). The no-sim variant is obtained by setting all thresholds to 1, which yields Lj and disables the similarity constraint entirely. Conversely, the 1-level variant sets all thresholds to 1, enforcing Lj = 1 for all blocks and collapsing the multi-level structure into single fine-grained level. B.2. Block Size and Pooling Variants The query and key block sizes determine the granularity at which importance scores and masks are computed, and are conceptually independent of the hardware tile sizes used within the kernel implementation. Owing to the decoupled block-tile design introduced in Section 3.5, PSA can employ moderate block sizes (bq, bk) (e.g., (32, 32), (64, 64), (128, 64)) while still achieving relatively high tensor-core utilization. To examine the role of the pooling operator in the importance definition (Section 3.3.1), we additionally evaluate meanpooling variant that replaces the max operator with an average over the sampled attention scores, while keeping all other components (sampling, permutation, mask generation, and similarity constraint) identical. The corresponding ablation results are shown in Table 8. 12 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Algorithm 3 Cosine Similarity-Based Pooling Level Constraint j=1, thresholds τ (2) Input: key blocks {Kj}nk Output: maximum admissible levels {1, . . . , H}nk Initialize Lj 1 for all for = 1 to nk do , . . . , τ (H) Interpret Kj as tensor in RBnheadbkd for = 2 to do Compute simh(j) as the mean cosine similarity of stride-2h1 pairs in Kj if simh(j) > τ (h) then Lj max(Lj, h) end if end for end for return Table 6. Effect of the cosine similarity constraint on Video-MME. All sparse variants are matched at 0.65 FLOP-equivalent sparsity. Method Short Medium Long Overall FA2 (dense) 0.752 0.663 0. 0.651 PSA w/o sim 0.747 PSA w/ sim 0.748 0.667 0.673 0.529 0.542 0.647 0.654 Unless otherwise noted, all video generation experiments in this appendix are performed on Wan2.1-1.3B at 480p under the same training-free setup as in the main paper, with sparse attention enabled after the first 25% of sampling steps and sparsity reported as FLOP-equivalent sparsity relative to full attention. C. Additional Ablations C.1. Cosine Similarity (Video Understanding) We first evaluate the cosine similarity-based pooling constraint in the video understanding setting. Table 6 compares dense FlashAttention2 baseline with two PSA variants on Video-MME, where all sparse variants are matched at approximately 0.65 FLOP-equivalent sparsity. Here, PSA w/o sim uses only the importance-based multi-level mask , while PSA w/ sim additionally applies the similaritybased cap from Alg. 3. Under the same sparsity budget, enabling the constraint improves the overall Video-MME score from 0.647 to 0.654. C.2. Cosine Similarity (Video Generation) We next investigate the cosine similarity constraint in the video generation setting. Table 7 compares three presets that share the same PSA framework but differ in how multi-level pooling is assigned and how the similarity thresholds are used. PSA (sim). This preset adopts more aggressive importance-based allocation in order to maintain FLOP-equivalent sparsity close to 0.8 once the similarity cap is applied. Specifically, the thresholds in mask generation are set to = {0.5, 0.7, 0.8, 0.9}, together with similarity thresholds Ts = {0.7, 0.65, 0.6}. Since the cosine constraint may locally force heterogeneous KV blocks to revert to finer levels, this more progressive allocation ensures that the global sparsity remains in the target range while still avoiding the assignment of coarse pooling to blocks whose neighboring tokens are not sufficiently similar. PSA (no-sim). To isolate the effect of the similarity constraint, we use more conservative importance-based assignment with thresholds = {0.7, 0.8, 0.9, 0.9}, but disable the cosine cap by setting Ts = {1, 1, 1}. This keeps larger fraction of blocks at the finest level = 1 and slightly reduces sparsity (0.79 vs. 0.80), yet all blocks are assigned purely based on importance scores without checking whether given KV block is internally homogeneous enough to tolerate 13 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Table 7. Ablation on the cosine similarity constraint for video generation (Wan2.1-1.3B, 480p). All presets operate at comparable FLOP-equivalent sparsity. Preset Sparsity PSNR SSIM LPIPS PSA (sim) PSA (no-sim) PSA (1-level) 0.80 0.79 0.71 23.71 23.36 24.42 0.8615 0.8556 0.8787 0.1086 0.1186 0. Table 8. Ablation on block size and importance pooling for video generation (Wan2.1-1.3B, 480p). All presets operate at an effective sparsity of 0.8. (bq, bk) Pool PSNR SSIM LPIPS (128, 128) Max (128, 128) Mean (32, 32) Max (32, 32) Mean (64, 64) Max (64, 64) Mean 21.43 21.08 21.64 21.71 21.55 21.56 0.8475 0.8337 0.8529 0.8515 0.8526 0. 0.1131 0.1257 0.1102 0.1101 0.1099 0.1179 high-level pooling. PSA (1-level). Finally, PSA (1-level) uses the same importance-based mask ratios as PSA (no-sim), but sets all similarity thresholds to Ts = {1, 1, 1} so that the cosine cap always forces Lj = 1 for any kept block. As result, the combined mask degenerates to PSA with = 1, i.e., no pooling is applied and all kept blocks remain at the finest resolution. This configuration therefore has the highest compute cost among the three and serves as an upper bound on reconstruction quality for this importance pattern. Under comparable sparsity budgets, PSA (sim) achieves better qualitysparsity trade-off than the purely importance-driven PSA (no-sim), indicating that the cosine constraint helps allocate coarse pooling more selectively: it avoids assigning high pyramid levels to KV blocks whose tokens are not poolable, thereby reducing approximation error at fixed sparsity. Meanwhile, PSA (1-level) confirms that further quality gains are possible if we completely abandon pooling and pay significantly higher compute cost, providing useful reference point for the performance/efficiency envelope of PSA. C.3. Block Size and Importance Pooling In this part, we ablate the logical block size (bq, bk) and the pooling operator used inside the importance estimation step under fixed sparsity budget. In PSAs default formulation (Section 3.3.1), the block importance score is computed as i.e., using max-pooling over the sampled token-pair attention weights. In Table 8, we ablate this choice by replacing the outer max with mean operator: Sij = max Softmax (cid:32) Qi j (cid:33)(cid:33) , (cid:32) (cid:32) Smean ij = mean Softmax (cid:32) Qi j (cid:33)(cid:33) , while keeping every other component (sampling strategy, sparsity, mask-generation rule, and multi-level allocation) unchanged. At fixed sparsity of 0.8, PSA is relatively robust to the choice between maxand mean-based importance pooling. Maxpooling yields slightly higher PSNR/SSIM at (128, 128) and (64, 64), while mean-pooling performs comparably at (32, 32). Smaller block sizessuch as (32, 32)consistently yield better generation quality, as the finer partitioning more precisely isolates high-importance regions in the attention map, reducing approximation errors under the same sparsity budget. C.4. Multi-Level Allocation Strategy Finally, we ablate the effect of the multi-level allocation strategy used by the quantile-based method. In this experiment, we fix all other hyper-parameters (block size, overall FLOP-equivalent sparsity, similarity constraint, and sampling strategy) 14 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Table 9. PSA presets: percentage of KV blocks per level. L1/L2/L3 correspond to = 1, 2, 3; Drop corresponds to = 0. Preset L1 L2 L3 Drop Coverage PSA-1 25% 0% 0% 75% PSA-2 0% 0% 100% 0% PSA-3 15% 10% 20% 55% PSA-4 10% 20% 20% 50% PSA-5 10% 10% 40% 40% 0.25 1.00 0.45 0.50 0.60 Table 10. Ablation on the multi-level allocation strategy for PSA on Wan2.11.3B, 480p. We vary only how blocks are distributed across pyramid levels (PSA-1 to PSA-5), while keeping the FLOP-equivalent compute budget (about 0.25 full attention), block size, and all other settings fixed. KV coverage denotes the fraction of KV blocks that are kept (h > 0). Preset KV coverage PSNR SSIM LPIPS PSA-1 PSA-2 PSA-3 PSA-4 PSA-5 0.25 1.00 0.45 0.50 0. 20.94 14.03 22.16 22.13 21.94 0.8365 0.3447 0.8752 0.8683 0.8632 0.1286 0.8445 0.1004 0.1045 0.1070 and vary only how KV blocks are distributed across pyramid levels. All runs are conducted on Wan2.1-1.3B at 480p in the same training-free setting as above, with sparse attention enabled after the first 25% of sampling steps and the compute budget kept at approximately 0.25 full-attention FLOPs. We report the KV coverage ratio, defined as the fraction of KV blocks that are retained (i.e., assigned to levels > 0); For clarity, we define five PSA presets, denoted PSA-1 to PSA-5, which all share the same compute budget but differ in how they allocate KV blocks across the pyramid levels. In this ablation we use three non-dropped levels = 1, 2, 3 (with = 1 the finest and = 3 the coarsest), and dropped level = 0. The percentage of blocks assigned to each level, together with the resulting KV coverage, is summarized in Table 9. The quantitative results of this ablation are reported in Table 10. All presets share the same logical block size (bq, bk) = (128, 64) and do not enable the cosine similarity constraint, so any performance differences can be attributed solely to the multi-level allocation strategy. Among these variants, PSA-2which keeps all blocks but collapses them to single coarse level to stay within the same FLOP budgetleads to severe degradation in all metrics. This shows that, under fixed compute budget, concentrating almost all computation on one very coarse level yields poor reconstructionseven with maximal KV coveragebecause the overly coarse KV representations dominate the performance degradation. PSA-1, which preserves only small portion of blocks at the finest representation and drops all others, performs worse than the multi-level strategies. This shows that allocating all compute to few fine blocks, without using intermediate levels, severely restricts the effective receptive field for each query block and thus leads to degraded performance. In contrast, the more balanced multi-level allocations in PSA-3/4/5 achieve consistently higher PSNR/SSIM and lower LPIPS at the same FLOP-equivalent cost. These strategies jointly trade off where to keep fine-grained representations and how much area to cover with coarser representations, instead of over-committing to single level. PSA-3 offers the best overall trade-off and is therefore adopted as our default multi-level strategy in the main video generation experiments. This ablation supports our design choice that, under fixed compute budget, extremely skewed mask distributions (either too many dropped blocks or too many heavily pooled blocks) hurt quality, whereas balanced allocation across multiple pyramid levels yields superior performance at similar sparsity. Determining the optimal allocation strategy, however, remains an open question and is promising direction for further exploration. D. Additional Visual Comparisons This section provides additional qualitative comparisons for Section 4.2. Each figure follows the same layout as the examples in the main paper. PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Figure 5. plane takes off over city skyline; cut to graffiti-covered steam train pulling into station with billowing smoke. Cinematic colors and motion. Figure 6. lightning bolt strikes the Eiffel Tower, illuminating its metal frame against swirling dark storm clouds. low-angle view highlights the towers grandeur as the electric flash casts dramatic shadows. Figure 7. vintage school bus with retro decals turns dusty rural corner at sunset. Warm golden light fills the scene as children inside read and play, and the focused driver steers through the tight bend. Low-angle, nostalgic cinematography with rolling hills in the background. 16 PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation Figure 8. rider on sleek black motorcycle weaves through neon-lit city streets at night, wearing leather jacket and helmet. Dynamic shots follow their smooth maneuvers through traffic, with vibrant signs and skyscrapers creating an intense urban atmosphere. Figure 9. cheerful hot dog vendor pushes colorful cart down sunny pastel street as smiling woman in floral dress chats with passersby. Balloons, flowers, and lively vendors create bright, carefree summer vibe. Figure 10. puzzled panda student with calculus book in warm, dimly lit classroom, surrounded by desks, books, and attentive classmates."
        }
    ],
    "affiliations": [
        "ZIP Lab, Zhejiang University"
    ]
}