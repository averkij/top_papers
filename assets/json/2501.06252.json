{
    "paper_title": "$\\text{Transformer}^2$: Self-adaptive LLMs",
    "authors": [
        "Qi Sun",
        "Edoardo Cetin",
        "Yujin Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce \\implname, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, \\implname employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \"expert\" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. \\implname demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. \\implname represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 2 5 2 6 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TRANSFORMER2: SELF-ADAPTIVE LLMS Qi Sun1,2*, Edoardo Cetin1*, Yujin Tang1* 1Sakana AI, Japan qisun,edo,yujintang { *Equal contribution 2Institute of Science Tokyo, Japan @sakana.ai }"
        },
        {
            "title": "ABSTRACT",
            "content": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer2, novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer2 employs two-pass mechanism: first, dispatch system identifies the task properties, and then task-specific expert vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Transformer2 demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer2 represents significant leap forward, offering scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems. Our code is available at https://github.com/SakanaAI/self-adaptive-llms"
        },
        {
            "title": "INTRODUCTION",
            "content": "Self-adaptive large language models (LLMs) would represent significant advancement in artificial intelligence, providing framework where models can adjust to varied tasks and dynamic contexts in real time. While compositionality and scalability are crucial for effective adaptation, current LLM training methodologies fall short of achieving both these properties simultaneously. Our research aims to present pioneering solution to realize this vision and address these gaps. Traditionally, LLM post-training has sought to optimize model for wide range of capabilities in single, extensive training session. While this one-shot fine-tuning framework is ideal from simplicity perspective, it is also difficult to achieve in practice. For instance, post-training is still highly resource-intensive, leading to significant computational costs and training times. Additionally, there tends to be notable performance trade-offs when introducing additional breadth to the data, making it challenging to overcome overfitting and task interference at the same time. Figure 1: Overview of Transformer2. In the training phase, we tune the scales of the singular values of the weight matrices to generate set of expert vectors, each of which specializes in one type of tasks. In the inference phase, two-pass process is adopted where the first applies the taskspecific expert and the second generates the answer. In contrast, self-adaptive models offer more flexible and efficient approach. Rather than attempting to train an LLM for all tasks in one step, expert modules can be developed offline and augmented"
        },
        {
            "title": "Preprint",
            "content": "to the base LLM on-demand (Kang et al., 2024). This allows the model to dynamically modify its behavior based on the task at hand, without the need for constant re-tuning. In addition to the benefit of having independent components, this modularity also supports continual learning, enabling the model to add new skills over time without catastrophic forgetting. Moreover, self-adaptive LLMs mirror well-established principle in neuroscience and computational biology, where the brain activates specific regions depending on the task at hand (Loose et al., 2017) and dynamically reconfigures its functional networks in response to changing task demands (Davison et al., 2015). In principle, the first step toward achieving self-adaptive LLMs can be realized through the development of specialized expert modules, each fine-tuned (Kaplan et al., 2020) via techniques such as low-rank adaptation (LoRA) (Hu et al., 2021). These expert modules can then be dynamically composed at runtime based on the task demands, process that can be efficiently managed through Mixture of Experts (MoE)-like systems (Tianlong et al., 2024). However, several challenges need to be addressed to make this approach both scalable and compositional. First, fine-tuning LLMs to create multiple expert modules significantly increases the number of parameters that need to be trained. In practice, even with parameter-efficient methods like LoRA, the cumulative size of these modules can quickly escalate, leading to increased storage and computational demands. Second, these expert modules are often prone to overfitting, phenomenon especially prevalent when training on smaller datasets or narrow task domains. Third, the flexible composition of these expert modules also presents largely unresolved challenges currently posing as open research problems. To overcome these limitations, we first propose Singular Value Fine-tuning (SVF), novel parameter-efficient fine-tuning (PEFT) method to obtain effective building blocks for selfadaptation. SVF works by extracting and tuning only the singular values within the models weight matrices. By focusing on this principled parameterization, our approach mitigates the risk of overfitting, drastically reduces computational demands, and allows for inherent compositionality. We show these properties enable us to cheaply obtain set of effective domain-specific expert vectors by training on narrow datasets with RL, directly optimizing task performance on individual topics. We then introduce our full Transformer2 framework to empower LLMs through the underlying principles of self-adaptation. Given prompt from an unknown task, Transformer2 entails two-pass inference mechanism which we illustrate in Figure 1. During the first pass, Transformer2 executes the model and observes its test-time behavior, gathering the relevant information to understand the necessary skills to tackle the current problem. During the second pass, our framework uses this information to combine the available expert vectors and provide new modification to the base weights of the LLM specifically tailored to its test-time conditions. We design three different adaptation strategies that can be used within Transformer2, which we show provide monotonic performance benefits with increasing access to the test-time conditions. We evaluate SVF and the full Transformer2 framework through extensive experiments across diverse range of LLMs and tasks. First, when trained on domain-specific datasets, we show that SVF consistently outperforms traditional strategies for efficient fine-tuning such as LoRA, and at the same time, with orders of magnitudes fewer parameters. Then we show that Transformer2 is able to push performance far further, effectively adapting the weights of the base model even in entirely out-of-distribution applications such as visual question answering. Finally, we analyze the properties of our new framework, validating that it provides increasing benefits with additional access to its current test-time conditions and even allow for recycling pre-trained SVF experts across model architectures. In summary, our key technical contributions are the following: The development of Transformer2 as pivotal self-adaptation framework for LLMs, providing universal blueprint to dynamically adapt the behavior of LLMs from growing set of pre-trained skills. The introduction of SVF, novel PEFT method trainable with RL on small datasets, producing compact expert vectors with inherent compositionality, all key properties necessary for our scalable self-adaptation framework. The implementation of three adaptation strategies within Transformer2, effectively dispatching SVF-trained experts with properties designed to cope with different requirements and deployment scenarios."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Self-adaptive LLMs We define self-adaptive LLMs as group of LLMs or standalone LLM that can evaluate and modify its behavior in response to changes in its operating environment or internal state, without external intervention. This adaptation can be explored from two perspectives: macroview, where multiple LLMs collaborate and/or compete, and microview, where internal adaptations allow single LLM to specialize in different tasks. Macroview: From this perspective, the system directs queries to LLMs with domain specific expertise, prioritizing outputs from expert models, thereby achieving higher accuracy and task-specific optimization. Such task-specific ensembles can be realized through various mechanisms: multiple LLMs playing distinct roles and coordinate toward shared goal (Zhuge et al., 2023), engaging in mutual listening and debate (Du et al., 2023), or using meticulously crafted prompt constructions (Zhang et al., 2024) to integrate knowledge library and skill planning. Naturally, the improvement in the specialization and adaptive capabilities of individual LLMs in the ensemble enhances the collective performance. Thus, in this paper, we focus on the microview of self-adaptive LLMs. Microview: MoE in LLMs plays critical role in this perspective (Tianlong et al., 2024). In MoE systems, inputs are dynamically routed to subset of specialized modules or layers (e.g., MLPs) containing domain-specific knowledge (Rajbhandari et al., 2022; Fedus et al., 2022). To reduce inference time, researchers introduce sparsely activated MoE where only subset of the experts are selected per token Jiang et al. (2024); Qwen Team (2024). While it is possible to view Transformer2 loosely as type of MoE, there are two major differences. In the aforementioned systems, selfadaptation is achieved through token-level routing, whereas Transformer2 employs sample-level module selection strategy. The second difference lies in the construction of expert modules. In traditional MoE systems, expert modules are either trained from scratch (Fedus et al., 2022; Jiang et al., 2024) or dense models (e.g., upcycling) (Qwen Team, 2024; Zhu et al., 2024), without an auxiliary loss to ensure module specialization. In contrast, Transformer2 specifically trains expert vectors with RL to acquire domain specific-knowledge, making them true experts. Low-rank adaptation PEFT methods such as LoRA (Hu et al., 2021) works by freezing the original models parameters and introducing small trainable low-rank matrices for task-specific updates. It significantly lowers the computational and memory costs while providing performance comparable to full fine-tuning. Inspired by LoRAs design, various modifications have been proposed (Zhang et al., 2023; Kopiczko et al., 2023; Liu et al., 2024; Bałazy et al., 2024; Cetoli, 2024). Transformer2 does not rely on low-rank matrices, and instead scales the singular vectors of the original parameter matrix that span the full rank space. SVD for LLM Fine-tuning SVD is increasingly being used as an inductive bias for PEFT in LLMs. For example, Wang et al. (2024) decompose weight matrix and use the minor singular components, associated with noisy or long-tail information, to initialize low-rank matrices for LoRA fine-tuning. In similar vein, SVD is employed to approximate an original weight matrix with the top singular vectors, corresponding to the highest singular values. small trainable matrix is then introduced on top of the truncated singular value matrix to adjust the magnitude and orientations within this top-r subspace (Bałazy et al., 2024; Cetoli, 2024). However, the drawback of this approach is that retaining only the top singular components can result in the loss of important information, particularly when the singular values distribution is less skewed. The work most similar to ours is concurrent effort by Lingam et al. (2024), where they introduce various sparsification methods that utilize the SVD of the weights. However, it is not for self-adaptive LLMs and does not use RL to enhance learning efficiency."
        },
        {
            "title": "3 METHODS",
            "content": "3.1 PRELIMINARIES Singular value decomposition (SVD) offers fundamental view of matrix multiplications. In the Rnm can be decomposed into three compocontext of neural networks, each weight matrix Rnr together with an nents = ΣV , yielding semi-orthogonal matrices Rrr. ordered vector of singular values (in descending order) arranged in the diagonal matrix Σ The linear operation defined by applying onto x, can be then decomposed into sum of indepenRmr and"
        },
        {
            "title": "Preprint",
            "content": "dent terms, derived from mapping each column vi from into the corresponding column ui from as = (cid:80)r x. Hence, each singular component represented by the rank-1 matrix uiv independently processes the input, providing an orthogonal contribution to the layers outputs, with the singular values σi modulating the degree of the contributions. i=1 σiuiv Cross-entropy method (CEM) is Monte Carlo method for importance sampling and optimization (Rubinstein & Kroese, 2004). The method is based on the concept of minimizing the KL divergence between two probability distributions DKL(P Q), where is the target distribution and is maintained distribution. At its core, CEM repeatedly generates set of samples from Q, evaluates these samples with performance function, and then updates the distribution with the characteristics of the elite samples that have performed best. In the standard setup employed in most applications, is set to diagonal multivariate Gaussian, reducing the problem to simply estimating the empirical mean and standard deviation of the latest elites until stopping criterion is met. We illustrate complete CEM step in the Python pseudocode below. 3.2 TRANSFORMER2 The construction of Transformer2 comprises two main steps, for which we provide an illustrative overview in Figure 2. First, we introduce Singular Value Fine-tuning (SVF), method to learn with RL compact and compositional expert vectors based on the SVD of the base models weights. Then, we describe three different adaptation strategies within Transformer2, inspired by three orthogonal principles, which adaptively combine the SVF-trained expert vectors during inference. We motivate how the properties of SVF are highly complementary to our adaptation strategies, making Transformer2 an effective and scalable framework for the design of new self-adaptive LLMs. Figure 2: Method overview. Left) At training time, we employ SVF and RL to learn the expert vectors zs that scale the singular values of the weight matrices. Right) At inference time, we propose three distinct methods to adaptively select/combine the learned expert vectors. Singular value fine-tuning is key building block in Transformer2. It offers an extremely efficient parameterization for fine-tuning and provides inherent compositionality for adaptation. Conventional fine-tuning techniques often aim to augment pre-trained models with new capabilities by modifying their weight matrices. However, in large-scale transformers, these weights are already rich repositories of abstracted knowledge, thanks to the breadth of the pre-training data and expansive architectural design. In fact, as evidenced in much of the prior literature, the requisite capabilities for solving many downstream tasks appear to already exist within these pre-trained models (Sharma et al., 2023). Therefore, instead of seeking to add new features, an efficient fine-tuning approach should focus on making these latent capabilities more expressible. Motivated by these considera-"
        },
        {
            "title": "Preprint",
            "content": "Rr that provides targeted modifications, for any weight matrix , SVF learns simple vector tions to each singular component of independently, yielding new weight matrix = ΣV , where Σ = Σ diag(z). This essential parameterization enjoys several benefits: Negligible parameters: Learning only vector for each weight matrix allows for very efficient fine-tuning with orders of magnitudes fewer optimized parameters even when compared to prior approaches specifically designed for efficiency. For example, the widely popular LoRA approach learnable parameters per weight matrix, where is hyper-parameter that genrequires (m+n) erally needs to be set large enough for expressivity. While recent extensions, such LoRA-XS (Bałazy et al., 2024), try to push efficiency even further, they often introduce limiting assumptions that curb applicability in several practical scenarios (see examples in Appendix C). In contrast, while SVF only needs = min(m, n) parameters, we show it empirically does not display the same shortcomings thanks to working on highly-meaning space provided by the latent expressiveness compressed in the weights of modern LLMs. SVFs scaling only the singular values may seem to lead to limited expressiveness, we wish to point out that the ability to affect the weight matrix in full-rank manner technically provides more information than low-rank approaches. High compositionality: Decomposing the weights in independent singular components makes the learned vectors highly composable and interpretable, opening numerous possibilities for adaptation via algebraic manipulations. Instead, LoRA-based methods inherently lack these properties. For instance, even if two LoRAs learned on the same task were to learn exactly the same adjustments for each , directly interpolating between their compressed and matrices is unlikely to preserve any of their original behavior, given the countless number of equivalent parameter permutations they might have converged to. Principled regularization: Exclusively modifying the magnitude of pre-existing singular components provides principled and effective form of regularization. In practice, this property enables us to fine-tune for arbitrary downstream tasks with only hundreds of data points without the risk of severe collapse or overfitting. W1, { , zN } , WN } End-to-end optimization with RL. We train set of SVF vectors θz = to finez1, { tune an arbitrary language model πθW parameterized by θW with RL, optimizing directly for task performance. Here, θW = is the set of weight matrices, where is the number of layers and is the number of weight matrices to fine-tune per layer. We use the seminal RED) INFORCE algorithm (Williams, 1992) and label each generated answer yi (for the prompt xi with unitary reward based on its correctness . Inspired by related applications of RL } for optimizing LLMs (Ouyang et al., 2022), we regularize the REINFORCE objective by adding KL penalty for deviating from the original models behavior, weighted by small coefficient λ R+. Thus, our final objective function can be written as: xi)(cid:1) r(ˆyi, yi)(cid:3) J(θz) = (cid:2)log (cid:0)πθW (ˆyi (1) where we use πθW to denote the resulting language model after substituting the original weight matrices with . While RL is generally considered less stable than next-token prediction objectives, we find the regularization properties of SVF avoid many of the failure modes of prior lessconstrained parameterizations (see Section 4.3). Thus, combining these complementary components effectively enables us to avoid relying on expensive fine-tuning procedures with large hand-designed datasets as proxies, and directly maximize task performance end-to-end. λDKL(πθW πθW ), { 1, 1 In general, SVF with RL puts lower requirement on the dataset it trains on. For example, LoRA fine-tuning requires explaining texts to perform next token predictions, which puts higher requirement on the dataset (e.g., imagine LoRA fine-tuning on GSM8K dataset where no reasoning text but only the final number is provided). This benefit allows SVF to be more general and effective. One possible caveat SVF can face is the sparse rewards caused by weak base model, which we discuss this further in Section 5. Self-adaptation is critical mechanism in nature that has established itself as core guiding principle in modern system design (Klos et al., 2015). Our initial efforts toward self-adaptive foundation models focus on the inference stage of LLMs, where we devise simple two-pass adaptation strategy that combines sets of base expert vectors z1:K trained with SVF to provide different kinds of capabilities (e.g., coding, math, etc). The mapping between capability and the dataset we train on can be acquired in the datasets meta data. In the first inference pass, given task or an individual input prompt, Transformer2 executes the model and observes its test-time behavior to derive"
        },
        {
            "title": "Preprint",
            "content": "new vector tailored to its test-time conditions. This adapted is then used in the second inference pass to provide an actual response with the newly adapted weights. The interaction between SVF-trained expert vectors and the adaptation strategies ensures seamless integration, where expert vectors provide modular capabilities, and the adaptation strategies dynamically determine and compose the most suitable combination to address the input task. In this first work, we propose three simple approaches to produce the vector during the first inference pass, implementing selfadaption with distinct methods and requirements. Below, we provide an outline of each method and refer to Appendix for additional implementation details. A) Prompt engineering: Our most basic approach involves constructing new adaptation prompt which we use to directly ask the LLM to categorize the input prompt. Based on its response, we then extract one category out of the set of domain topics used to pre-train each SVF expert and, thus, we select the corresponding directly from z1:K. In our adaptation prompt, we also explicitly provide the option for generic others category, allowing the model to use its base weights in case no expert provides appropriate capabilities. We show the format used to construct the adaptation prompt in Figure 3. (x1,1, 1), B) Classification expert: direct extension of the prompt engineering approach comes from using specialized system to handle task identification. Following the principles of self-adaptation, we apply SVF to fine-tune the base LLM itself to hanIn particular, we collect dataset dle this task. from the SVF , (xi,k, k), = training tasks, where xi,k is the i-th example from the k-th expert task. Each tuple (xi,k, k) then forms an example to pre-train an additional job classification expert zc learned in the same fashion as the others. During the first inference pass, we simply load zc, intending to improve the inherent task classification capabilities of the base model to select more appropriate to handle the input prompt. } { Figure 3: Prompt based adaptation. Selfadaptation prompt used by Transformer2 to classify the task prompt into pre-defined categories. C) Few-shot adaptation: Our third approach leverages additional task information by assuming extended access to its test-time conditions beyond individual prompts. Our approach is inspired by popular few-shot prompting techniques, which have been shown to provide consistent performance improvements and even allow LLMs to in-context learn tasks that were entirely unseen prior to inference (Brown, 2020). For each optimized , our approach entails producing an entirely new = (cid:80)K k=1 αkzk by linearly interpolating between the learned SVF vectors, each weighted by the coefficients αk. We employ CEM to search over the possible values of each αk based on the performance on set of few-shot prompts, which are specifically held out from the rest of the test prompts and used to evaluate CEMs population samples. In the case of multiple population samples obtaining the same score on these held-out prompts, we break ties by favoring the one with the highest average log-likelihood across its own generated correct answers. Crucially, we only need to perform this process once for each target task, avoiding the need to increase the length of each question prompt, relevant downside of traditional few-shot prompting. We refer to Section A.4, for additional details and an extended discussion of this final approach."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We extensively evaluate Transformer2 on multiple tasks and models with the purpose of: (1) assessing the efficiency and effectiveness of SVF; (2) demonstrating self-adaptiveness through the three proposed adaptation strategies; (3) conducting in-depth analysis and ablation studies aimed at understanding and interpreting the properties of our new framework. 4.1 EXPERIMENTAL SETUPS To validate the generality of Transformer2 we consider three pre-trained LLMs ranging across different model families and architecture sizes: LLAMA3-8B-INSTRUCT, MISTRAL-7B-INSTRUCTV0.3, and LLAMA3-70B-INSTRUCT. For each model, we obtain three sets of SVF-trained vectors to maximize performance for GSM8K (Cobbe et al., 2021), MBPP-pro (Austin et al., 2021),"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: SVF learning curves. The dashed lines indicate the performance of LLAMA3-8BINSTRUCT on the test split of each task. SVF effectively fine-tunes to surpass the base performance. While we use the best validation score to select our checkpoint for evaluation (marked by red dots), we present the entire training curve without early stopping to demonstrate SVFs learning capabilities. Tasks with only hundreds of training samples like Coding and Reasoning were stopped early. In our experiments, we update the parameters at the end of each epoch. and ARC-Easy (Clark et al., 2018), respectively. Additionally, we also train set of vectors for LLAMA3-8B-INSTRUCT, when applied as the language backbone for TextVQA (Singh et al., 2019), in order to assess SVFs applicability to the vision-language modeling (VLM) domain. We provide SVFs main learning curves on each of these tasks in Figure 4. Finally, we evaluate the full Transformer2 adaptation framework on four unseen tasks: MATH (Hendrycks et al., 2021), Humaneval (Chen et al., 2021), ARC-Challenge (Clark et al., 2018), and OKVQA (Marino et al., 2019). In all our adaptation experiments, we only consider experts obtained in the pure-language settings, assessing its test-time applicability even for the distinctive vision domain. Please refer to the Appendix for additional details and summary of the hyper-parameters used in the experiments. 4.2 EXPERIMENTAL RESULTS SVF performance We provide results after training on each considered task with the LLAMA38B-INSTRUCT, MISTRAL-7B-INSTRUCT-V0.3, and LLAMA3-70B-INSTRUCT base models in Table 1. Remarkably, we find that SVF provides considerable and consistent performance gains across nearly all tasks and base models. Instead, LoRA experts yield smaller gains and even sporadic performance degradation. (These LoRA experts are trained with next token prediction. While we also have LoRA experts trained with RL in Table 4, RL seems work less well with LoRA than with SVF.) This observed trend extends also to the vision-language domain, as fine-tuning LLAMA3LLAVA-NEXT-8B with SVF bolsters the base models performance by over 39% (see Figure 5). To ensure fair comparison, we provide extensive ablations to both our model and the LoRA baseline considering different architecture and optimization objectives in Appendix 4.3). Due to its essential parameterization, we would like to note that training SVF requires considerably fewer resources, with less than 10% of the training parameters of our LoRA implementation. Adaptation performance With the SVF trained vectors, we assess the self-adaptation capability of Transformer2 on unseen tasks. For fair comparison with LoRA, we record the performance of this baseline using all checkpoints from the considered training tasks and report only its highest performance for each of the test tasks. As shown in Table 2, all of our Transformer2 adaptation strategies demonstrate improvements across all tasks for LLAMA3-8B-INSTRUCT base models, and in at least two out of three tasks for both MISTRAL-7B-INSTRUCT-V0.3 and LLAMA3-70BINSTRUCT. In contrast, even the best training LoRAs only provide marginal improvements on the Table 1: Fine-tuning results. LLM performance on the test splits of math, coding and reasoning. Normalized scores are in the parentheses. Method GSM8K MBPP-Pro ARC-Easy LLAMA3-8B-INSTRUCT + LoRA + SVF (Ours) MISTRAL-7B-INSTRUCT-V0.3 + LoRA + SVF (Ours) 75.89 (1.00) 77.18 (1.02) 79.15 (1.04) 42.83 (1.00) 44.66 (1.04) 49.74 (1.16) 64.65 (1.00) 67.68 (1.05) 66.67 (1.03) 49.50 (1.00) 51.52 (1.04) 51.52 (1.04) 88.59 (1.00) 88.97 (1.00) 89.56 (1.01) 81.65 (1.00) 81.19 (0.98) 85.14 (1.04) LLAMA3-70B-INSTRUCT + LoRA + SVF (Ours) 85.29 (1.00) 77.26 (0.91) 88.32 (1.04) 80.81 (1.00) 68.69 (0.85) 80.81 (1.00) 89.10 (1.00) 88.55 (0.99) 88.47 (0.99) 7 Figure 5: Results for the VLM domain."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Self-adaptation on unseen tasks. Normalized scores are in the parentheses. Method MATH Humaneval ARC-Challenge LLAMA3-8B-INSTRUCT + LoRA + Transformer2 (Prompt) + Transformer2 (Cls-expert) + Transformer2 (Few-shot) MISTRAL-7B-INSTRUCT-V0.3 + LoRA + Transformer2 (Prompt) + Transformer2 (Cls-expert) + Transformer2 (Few-shot) LLAMA3-70B-INSTRUCT + LoRA + Transformer2 (Prompt) 24.54 (1.00) 24.12 (0.98) 25.22 (1.03) 25.18 (1.03) 25.47 (1.04) 13.02 (1.00) 13.16 (1.01) 11.86 (0.91) 11.60 (0.89) 13.39 (1.03) 40.64 (1.00) 25.40 (0.62) 40.44 (1.00) 60.98 (1.00) 52.44 (0.86) 61.59 (1.01) 62.80 (1.03) 62.99 (1.03) 43.29 (1.00) 37.80 (0.87) 43.90 (1.01) 43.90 (1.01) 47.40 (1.09) 78.66 (1.00) 73.78 (0.94) 79.88 (1.02) 80.63 (1.00) 81.06 (1.01) 81.74 (1.01) 81.37 (1.01) 82.61 (1.02) 71.76 (1.00) 75.77 (1.06) 72.35 (1.01) 74.83 (1.04) 75.47 (1.05) 87.63 (1.00) 83.70 (0.96) 88.48 (1.01) ARC-Challenge task and still significantly deteriorate performance on both MATH and Humaneval. This discrepancy suggests that LoRAs parameterization and optimization might be particularly sensitive to overfitting, especially when trained with the smaller GSM8K and MBPP-Pro datasets, the tasks that provide information most related to MATH and Humaneval. In Figure 5, we find similar dichotomy in the OKVQA task, with the performance of the base LLAMA3-LLAVA-NEXT-8B VLM only improving after applying Transformer2. We note that also in this setting, Transformer2 performs self-adaptation only from the expert vectors from GSM8K, MBPP-Pro, and ARC-Easy. Thus, this result further underscores the high flexibility of self-adaptation, transferring knowledge compressed for tasks entirely based on language even for unrelated vision-based problems. Comparing the three proposed adaptation strategies, we highlight clear monotonic trend with more involved strategies and additional information about the test-time condition, self-adaptation appears to be increasingly effective. In particular, Transformer2 with few-shot self-adaptation is almost always the highest-scoring method, providing notable improvements across all tested settings except for LLAMA3-70B-INSTRUCT @MATH, where we have only SVF-tuned half of the layers due to our limited GPU resources. This trend shows that providing additional or different kinds of information seems to be highly beneficial to our framework, suggesting that Transformer2 could provide foundation models with new means to continually improve performance when deployed in lifelong settings. Table 3: Time cost of 2-pass inference in prompt adaptation strategy of Transformer2 for the entire problem set. 1st to 2nd pass inference time ratios are shown in parentheses. Table 3 reports the inference time required by the prompt adaptation strategy of Transformer2, with the time spent on solving the entire problem set presented separately for the 1st and 2nd passes. Notice that the 2nd pass inference time is the time spent on solving the problems, and the 1st pass inference time is the time for self-adaptation, 1st to 2nd pass inference time ratios are in the parentheses. While the additional inference pass might appear to double the overall runtime, it is important to note that inference time primarily depends on the number of (n) where is the tokens generated. In our settings, it is length of the input. ARC-challenges cost ratio is large because they are single choice problems and therefore the cost of the 2nd pass is also (n). In general settings, we think it is reasonable to assume this ratio to be closer to those of MATH and Humaneval. For detailed discussion on improving the efficiency of CEM few-shot adaptation methods, please see Appendix MATH Humaneval ARC-Challenge 42.64 (13%) 2.76 (19%) 13.40 (47%) 321.19 14.28 28.51 2nd (s) 1st (s) Task 4.3 ANALYSIS Lastly, we analyze and discuss the properties of our adaptation strategies for which we provide extensions and further discussion Appendix B. Analysis 1: Job dispatching accuracy In Figure 6 we provide the confusion matrices of our classification-based adaptation strategies. These results validate the effectiveness of both our classification-based adaptation strategies to match each prompt with experts trained in similar domains, as evidenced by the high values along the diagonals. Furthermore, the results from LLAMA3-"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Confusion matrices. These matrices display the classification percentages, where rows represent the task classes (ground truth) and columns indicate the predicted categories. Some samples are misclassified as Others, which is reflected in rows where the totals do not sum to one. 8B-INSTRUCT and MISTRAL-7B-INSTRUCT-V0.3 also show that using the classification expert consistently provides higher classification accuracy than vanilla prompt engineering. While this difference could explain the higher performance of the relative self-adaptation strategy, we also note that domain similarity might not be the only metric relevant to identifying the best expert for each prompt or task. To this end, we believe many further unexplored extensions could be explored in future work, using heuristics such as past expert performance or token-level analysis to further push our frameworks scalability. Analysis 2: Training tasks adaptation contribution In Figure 7, we show the normalized adaptive coefficients ak interpolating between our SVF vectors learned via CEM for LLAMA3-8BINSTRUCT and MISTRAL-7B-INSTRUCT-V0.3 across all the unseen downstream tasks. Intuitively, we find that the expert vectors from the training tasks sharing similar topics to the unseen ones are often the highest contributors to the produced adaptive weights. However, we observe that the MATH task appears as an interesting exception, as the ak for the expert obtained from GSM8K training is actually the lowest out of the three in both models. We hypothesize this reflects the different nature of the mathematics competition problems from MATH as compared to the grade-school problems in GSM8K. In fact, not only is the difficulty of the MATH questions far beyond GSM8K, but large portion of its problems also hinges mainly on logical reasoning, for which task like ARC might actually be more aligned. Furthermore, we also note that the different vectors appear to contribute more uniformly to adaptation in the Llama model. This difference might indicate that, due to its higher base performance, the Llama model does not need to rely on any particular set of skills as much as Mistral, and can harness more holistic benefits from self-adaptation. Note that applying ak uniformly is not universal solution for leveraging expert vectors. This becomes evident when we look at different model and task combinations (e.g. applying ak uniformly on LLAMA3-8BINSTRUCT for MATH tasks only achieves 24.47, while Transformer2 (Few-shot) achieves 25.47). Analysis 3: Ablation studies Module sensitivity: We first compare the performance of SVF when it is applied to different modules (see trials 1-3). Under consistent conditions, both individual MLP and attention updates improve performance, with MLP updates resulting in more pronounced gains. Simultaneous updates to both module types yield even more significant enhancements. Objective function: We are interested in the performance impact from different objective functions, and we compare the RL objective with next-token prediction loss (see trials 2 and 4). For the latter, we use instruction fine-tuning with official GSM8K solutions as target tokens. Results show clear performance gains with RL, demonstrating its effectiveness in task-specific fine-tuning. Conversely, next-token prediction even hinders performance. This highlights RLs ability to handle cases lacking detailed solutions, suggesting its superiority in this context. SVF vs LoRA: Finally, we also evaluate LoRA using the RL objective (see trials 2 and 5). significant performance disparity is observed, primarily attributed to the severe instability of the LoRA training process. Despite exploring wide range of learning rates, LoRAs performance consistently lagged behind. For further illustrations, see Figure 9 in the appendix. Analysis 4: Cross-model compatibility Finally, we explore the potential for our self-adaptation framework to be applied across different LLMs. In particular, we evaluate whether the SVF expert vectors trained on LLAMA3-8B-INSTRUCT can benefit MISTRAL-7B-INSTRUCT-V0.3, and whether we can perform adaptation across the expert vectors of these two models. We present our main findings in Table 5 and refer to Appendix for additional detailed results. Surprisingly, we find that positive transfer occurs across the two models, with visible benefits in 2 out of 3 tasks. We"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Ablation studies. We fine-tune LLAMA3-8B-INSTRUCT on the GSM8K training split with different settings and the results on the test split along with zero-shot transfer results on MATH. #Params () GSM8K () MATH () # Method Objective Function Module 0 1 2 3 4 5 6 7 LLAMA-3-8B-INSTRUCT 75.89 (1.00) 24.54 (1.00) SVF SVF SVF SVF LoRA LoRA LoRA Policy gradient Policy gradient Policy gradient Next token pred Policy gradient Next token pred Next token pred MLP attention MLP + attention attention attention attention MLP + attention 0.39M 0.16M 0.58M 0.16M 6.82M 6.82M 35.13M 78.62 (1.04) 76.19 (1.00) 79.23 (1.04) 60.50 (0.80) 57.92 (0.76) 77.18 (0.98) 75.66 (0.96) 24.20 (0.99) 24.20 (0.99) 25.04 (1.04) 18.52 (0.75) 15.72 (0.64) 24.12 (0.96) 22.12 (0.91) Table 5: Cross-model vector transfer. Results from transferring the expert vectors trained on LLAMA3-8B-INSTRUCT to MISTRAL-7B-INSTRUCT-V0.3 with cross model few-shot adaptation. Method SVF training task MATH GSM8K Humaneval ARC-Challenge MBPP-pro ARC-Easy MISTRAL-7B-INSTRUCT-V0.3 13.02 (1.00) 43.29 (1.00) 71.76 (1.00) + Llama SVF (ordered σi) + Llama SVF (shuffled σi) + Few-shot adaptation (cross-model) 11.96 (0.92) 10.52 (0.81) 12.65 (0.97) 45.12 (1.04) 40.24 (0.93) 46.75 (1.08) 72.01 (1.00) 70.82 (0.99) 75.64 (1.05) note these improvements are due to the inherent ordering of the SVF parameterization, as randomly shuffling each SVF vector before applying it to the Mistral model consistently degrades performance. This operation leads to notable performance degradation across each task. Finally, by performing few-shot adaptation using the SVF vectors collected from both models, the performance of MISTRAL-7B-INSTRUCT-V0.3 further improves across the board. We observe that these gains even surpass the best score from adapting MISTRAL-7B-INSTRUCT-V0.3 with all the SVF vectors in the ARC-Challenge task reported in Table 2. While these results appear promising, we note that the surprising compatibility discovered through our naive transfer approach is potentially tied to the similarity between the architectures of the two considered LLMs. To this end, whether similar transfer can be replicated with models of different scales remains an open research question that could open the doors to disentangling and recycling task-specific skills for newer/larger models, with important implications for democratization and sustainability. Figure 7: αk learned weights."
        },
        {
            "title": "5 CONCLUSION\nIn this paper, we introduced Transformer2, providing a novel blueprint toward realizing self-adaptive\nLLMs. Within this framework, we first proposed SVF, offering superior performance than prior fine-\ntuning recipes, together with reduced costs, high compositionality, and overfitting regularization –\nall crucial properties to achieve scalable self-adaptation. Leveraging a set of SVF experts as building\nblocks, we developed three effective strategies for self-adaptation, each offering unique benefits and\nmonotonic performance benefits with increasing access to the test-time conditions.",
            "content": "While Transformer2 demonstrates promising results, there remain exciting opportunities for future work. One limitation is that the capabilities of SVF experts are tied to the latent components of the base model. To address this, model merging offers promising direction (Yu et al., 2024; Goddard et al., 2024; Akiba et al., 2024), enabling specialized models to be combined into single, more capable model. Additionally, while our CEM-based adaptation effectively balances performance and efficiency, scaling to large number of specialized domains may introduce increased one-time computational costs. However, this trade-off is offset by the benefits of improved performance and enhanced self-adaptation capabilities. Advances in model merging and efficient adaptation techniques have produced models dominating open leaderboards, making them strong candidates as base models for Transformer2 and opening new possibilities for adaptive LLMs."
        },
        {
            "title": "6 AUTHOR CONTRIBUTIONS",
            "content": "Yujin Tang initiated the project. Qi Sun proposed the prompted-based method, developed the evaluation framework, conducted the experiments, and provided contributions to writing. Edoardo Cetin designed the few-shot CEM adaptation strategy, performed the experiment, and made major contributions to manuscript writing. Yujin Tang proposed the core algorithm, conducted initial experiments, made major contributions to the manuscript, and managed the project."
        },
        {
            "title": "REFERENCES",
            "content": "Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. arXiv preprint arXiv:2403.13187, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Klaudia Bałazy, Mohammadreza Banaei, Karl Aberer, and Jacek Tabor. Lora-xs: Low-rank adaptation with extremely small number of parameters. arXiv preprint arXiv:2405.17604, 2024. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Alberto Cetoli. Fine-tuning llms with singular value decomposition. Hugging Face Blog, June 2024. URL https://huggingface.co/blog/fractalego/svd-training. Accessed: 2024-07-01. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Elizabeth Davison, Kimberly Schlesinger, Danielle Bassett, Mary-Ellen Lynall, Michael Miller, Scott Grafton, and Jean Carlson. Brain network adaptability across task states. PLoS computational biology, 11(1):e1004029, 2015. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees mergekit: toolkit for merging large language models. arXiv preprint arXiv:2403.13257, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021."
        },
        {
            "title": "Preprint",
            "content": "Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob Hansen, James Glass, David Cox, Rameswar Panda, Rogerio Feris, and Alan Ritter. Self-moe: Towards compositional large language models with self-specialized experts. arXiv preprint arXiv:2406.12034, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Verena Klos, Thomas Gothel, and Sabine Glesner. Adaptive knowledge bases in self-adaptive system design. In 2015 41st Euromicro Conference on Software Engineering and Advanced Applications, pp. 472478, 2015. doi: 10.1109/SEAA.2015.48. Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random matrix adaptation. arXiv preprint arXiv:2310.11454, 2023. Vijay Lingam, Atula Tejaswi, Aditya Vavre, Aneesh Shetty, Gautham Krishna Gudur, Joydeep Ghosh, Alex Dimakis, Eunsol Choi, Aleksandar Bojchevski, and Sujay Sanghavi. Svft: Parameter-efficient fine-tuning with singular vectors. arXiv preprint arXiv:2405.19597, 2024. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:19501965, 2022. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024. Lasse Loose, David Wisniewski, Marco Rusconi, Thomas Goschke, and John-Dylan Haynes. Switch-independent task representations in frontal and parietal cortex. Journal of Neuroscience, 37(33):80338042, 2017. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual In Proceedings of the IEEE/cvf question answering benchmark requiring external knowledge. conference on computer vision and pattern recognition, pp. 31953204, 2019. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Qwen Team. Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters, March 2024. URL https://qwenlm.github.io/blog/qwen-moe/. Blog post. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In International conference on machine learning, pp. 1833218346. PMLR, 2022. Reuven Rubinstein and Dirk Kroese. The cross-entropy method: unified approach to combinatorial optimization, Monte-Carlo simulation, and machine learning, volume 133. Springer, 2004. Pratyusha Sharma, Jordan Ash, and Dipendra Misra. The truth is in there: Improving reasoning in language models with layer-selective rank reduction. arXiv preprint arXiv:2312.13558, 2023. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019."
        },
        {
            "title": "Preprint",
            "content": "Chen Tianlong, Cheng Yu, Chen Beidi, Zhang Minjia, and Bansal Mohit. Mixture-of-experts in the era of llms: new odyssey. ICML 2024 presentation slides, 2024. International Conference on Machine Learning (ICML). Hanqing Wang, Zeguan Xiao, Yixia Li, Shuo Wang, Guanhua Chen, and Yun Chen. Milora: Harnessing minor singular components for parameter-efficient llm finetuning. arXiv preprint arXiv:2406.09044, 2024. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, et al. Proagent: building proactive cooperative agents with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1759117599, 2024. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameterefficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023. Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. Llama-moe: Building mixture-of-experts from llama with continual pre-training. arXiv preprint arXiv:2406.16554, 2024. Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan Ashley, Robert Csordas, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023."
        },
        {
            "title": "Preprint",
            "content": "A IMPLEMENTATION DETAILS AND HYPER-PARAMETERS A.1 SVF TRAINING We obtain the expert vectors as the base components in Transformer2 by training the SVF finetunes with consistent recipe across the considered training tasks and language models. We divide each dataset to produce equal-sized training and validation splits. We then apply our RL-based 103 with cosine decay, batch approach, optimizing θz with AdamW using learning rate of 2 size of 256, and gradient clipping. We employ early stopping and select the best λ (the coefficient of the KL divergence term) based on validation performance. For the LLAMA3-70B-INSTRUCT and Vision tasks experiments, we apply the SVF on half of the layers to reduce memory usage while maintaining considerable performance improvement. During the training of LLAMA3-8BINSTRUCT on the vision language tasks, we apply small negative reward (-0.1) for training stability. A.2 LORA TRAINING We follow community best practices for LoRA fine-tuning, applying it to query and value projection layers with learning rates 105. We set 200 total iteraaround 5 tions with 256 global batch size for sufficient training. For feasible LoRA instruction training, we collect solutions for all training tasks (GSM8K, MBPP, Arc-Easy, TextVQA) from official sources and append them to question prompts. Table 8 shows sample math problem used for LoRA fine-tuning. Despite extensive hyperparameter tuning, we often observe test performance decay as discussed, which can be attributed to the small number of training samples and potential model requirements for instruction fine-tuning data (specifically, the highly detailed thinking process). Figure 8: Sample problem and answer. Math data sample used for LoRA instruction fine-tuning, text in blue is the unmasked solution. A.3 HYPER PARAMETERS We present summary of the hyperparameters used in our experiments in Table 6. To optimize performance, we conducted sweeps across several hyperparameters and selected the most effective combination based on validation results. For SVF, our primary focus was on adjusting the KL coefficient to enhance training stability. In the case of LoRA, we concentrated on sweeping the learning rate and maximum gradient clip norm to identify optimal settings. A.4 FEW-SHOT ADAPTATION As described in the main text, our few-shot adaptation approach entails producing an entirely new = (cid:80)K k=1 αkzk for each by linearly interpolating between the learned SVF vectors, each RK. We employ CEM to search for αks based on the performance weighted by the coefficients α on the few-shot prompts, which are specifically held out from the rest of the test prompts and used to obtain the elite set at each iteration. In the case of multiple sample solutions obtaining the same score on these held-out samples, we break ties by choosing the sample solution with the highest average log-likelihood across the tokens of its generated correct answers. In all of our main experiments, we reserve only 10 samples of data for self-adaptation and perform up to 100 CEM iterations. For each setting, we consider both per-layer and per-vector adaptation, where the latter strategy has the advantage of greatly simplifying search (as we only have 3 α coefficients). Moreover, we experiment with both normalizing across the α of different tasks (such that their sum would be fixed to 1) or keeping them unconstrained. Due to the lack of validation set, we simply report the performance attained by our best sample from these test configurations at the end of optimization, on the remaining unseen samples for each task."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Hyper-parameters used for SVF and LoRA training. We perform sweep on certain sensitive hyper-parameters across methods for fair comparison. SVF Hyperparameters Initial mean value of Initial variance value of Global batch size Learning rate Clip max norm KL coefficient λ"
        },
        {
            "title": "Rank\nLoRA alpha\nLoRA dropout\nGlobal batch size\nLearning rate\nClip max norm",
            "content": "103 0.1 1 256 103 2 103 1 0.0, 0.1, 0.2, 0.3 LoRA Hyperparameters 16 32 0.05 256 2 1 104, 5 103, 1.0 104, 2 105, 5 105, 2 106. 5 106, Table 7: Additional Comparison Experiment. Normalized scores are in the parentheses. Method GSM8K MBPP-Pro ARC-Easy LLAMA3-8B-INSTRUCT + IA3 + DORA + SVF(Ours) Method 75.89 (1.00) 78.01 (1.03) 78.09 (1.03) 79.15 (1.04) 64.65 (1.00) 67.68 (1.05) 64.65 (1.00) 66.67 (1.03) 88.59 (1.00) 89.10 (1.01) 89.14 (1.01) 89.56 (1.01) MATH Humaneval ARC-Challenge LLAMA3-8B-INSTRUCT + IA3 + DORA + Transformer2 (Prompt) + Transformer2 (Cls-expert) + Transformer2 (Few-shot) 24.54 (1.00) 23.64 (0.96) 24.44 (0.99) 25.22 (1.03) 25.18 (1.03) 25.47 (1.04) 60.98 (1.00) 59.76 (0.98) 52.44 (0.86) 61.59 (1.01) 62.80 (1.03) 62.99 (1.03) 80.63 (1.00) 81.57 (1.01) 81.14 (1.01) 81.74 (1.01) 81.37 (1.01) 82.61 (1.02)"
        },
        {
            "title": "B ADDITIONAL RESULTS",
            "content": "B.1 BASELINE COMPARISON TO MORE PEFT METHODS We conduct additional comparison studies against more parameter-efficient fine-tuning methods, including IA3Liu et al. (2022), DORA. Liu et al. (2024). As Table 7 shows, SVF still outperforms other methods and shows promising generalized performance. B.2 IMPACT FROM NUMBER OF FEW-SHOTS We investigate the relationship between the number of samples available for fewshot adaptation and downstream performance. Our analysis focused on the test task where LLAMA3-8B-INSTRUCT demonstrates the highest baseline performance, to prevent the potential for null signal in our CEM-based search. Table 8: Few-shot adaptation scaling on the ArcChallenge task. Performance varies with number of examples. Method Transformer2 IA3 1000 steps IA3 100 steps LLAMA3-8B-INSTRUCT 80.63 (1.00) 80.63 (1.00) 80.63 (1.00) + 3-shot adaptation + 5-shot adaptation + 10-shot adaptation + 20-shot adaptation 82.18 (1.02) 82.38 (1.02) 82.61 (1.02) 82.61 (1.02) 81.83 (1.01) 80.89 (1.00) 82.00 (1.02) 81.40 (1.01) 79.01 (0.98) 79.41 (0.98) 79.78 (0.99) 79.61 (0.99) As Table 8 shows, substantial benefits of our few-shot strategy are evident with as few as 3 to 5 test samples. Moreover, performance appears to plateau beyond 10 samples, underscoring how our essential and inherently regularized SVF pa-"
        },
        {
            "title": "Preprint",
            "content": "rameterization effectively complements self-adaptation. This efficiency enables optimal use of data to enhance understanding of the test task. For completeness, we have also conducted experiments with identical settings on IA3 (Liu et al., 2022), another method that leverages few-shot examples. All experiments were conducted with full batch size, learning rate of 5 105, with 100 and 1000 training steps. Our results indicate that the performance of IA3 on the unseen test tasks is inferior to CEM-based adaptation for all numbers of few shots considered. We note that in our experiment, we have to considerably limit the number of optimization steps to avoid overfitting the 500,000 parameters of IA3 on the few-shot samples. However, we believe overfitting might still be occurring to some degree even after only 100 steps, as also validated by the models perfect training accuracy on this extremely small dataset. This limitation of fine-tuning-based adaptation highlights the superior generalization capability of our CEM-based adaptation approach in Transformer2. B.3 CROSS-MODEL SVF TRANSFER ON THE TRAINING TASKS We provide complementary results to Table 5 in the main text, where we analyze the SVF crossmodel transfer performance from training on GSM8K, MBPP-pro, and ARC-Easy to our considered test tasks. In Table 9, we show the results in the same transfer setting this time evaluating MISTRAL-7B-INSTRUCT-V0.3 on the same training tasks where the LLAMA3-8B-INSTRUCT SVF vectors were obtained from. Overall, we recognize similar trend, albeit with less consistent improvement from the original model (only in 1 out of 3 tasks), but still much higher performance than the randomly shuffled baseline. These results further confirm that the canonical ordering of the SVF parameterization is key for cross-model transfer, highlighting once more its inherent suitability to empower self-adaptation. Table 9: Cross-model Vector Transfer. Results from transfering the SVF expert vectors trained on LLAMA3-8B-INSTRUCT to MISTRAL-7B-INSTRUCT-V0.3 in the respective training tasks. Method GSM8K MBPP-pro ARC-Easy MISTRAL-7B-INSTRUCT-V0.3 42.83 (1.00) 49.50 (1.00) 81.65 (1.00) + Llama SVF (ordered σi) + Llama SVF (shuffled σi) 42.61 (0.99) 41.93 (0.98) 48.48 (0.98) 46.34 (0.94) 81.78 (1.00) 80.81 (0.99) B.4 TRAINING CURVE OF LORA AND POLICY GRADIENT Figure 9 gives the learning curves for LoRA training on the GSM8K task. Figure 9: Training LoRA with policy gradient. The dashed line shows the performance of LLAMA3-8B-INSTRUCT on the test split. LoRA collapses at the beginning of the training stage and fails to recover, leading to negative effects on test performance. We swept wide range of learn102), and all learning curves were similar to the ing rates (2 one presented. 104, . . . , 2 104, 5 2,"
        },
        {
            "title": "Preprint",
            "content": "C PCA ON LLAMA3 AND MISTRAL To investigate if the singular components that have the highest singular values are able to capture most of the information of weight matrix, we conducted Principle Component Analysis (PCA) on the weight matrices in LLAMA3-8B-INSTRUCT and MISTRAL-7B-INSTRUCT-V0.3 (see Figures 10 and 11). In each figure, we plot the variance that is captured by the top components across all the layers in each type of modules for weight matrix Rmn: ratio = (cid:80)r i=1 σi (cid:80)min(m,n) j=1 σj Here, σs are the ordered (from largest to smallest) singular values on the weight matrix . It is easy to see from the figures that when = 256, less than 50% of the variance is captured by these top components on average. For the MLP layers, this fraction is even lower than 20%. On the other hand, the ranks adopted by LoRA-XS or similar methods are much less than 256, resulting in even more information loss and restrictions in their modeling power that relies mostly on these components. Figure 10: PCA of LLAMA3-8B-INSTRUCT. We show the ratio of the variance captured by the top singular components on the y-axis, and the layer indices on the x-axis. Except for the Query, Key and Value projection matrices, small values only capture tiny fraction of variance in singular values in the parameter matrices."
        },
        {
            "title": "D EFFICIENCY CONSIDERATIONS AND IMPROVEMENTS",
            "content": "Table 10: 3-shot and light variants Performance with different inference-time adaptation budgets. Our CEM-based adaptation method involves running inference on small number of samples for each target task (up to 10 in our experiments). In typical configuration, this process is relatively efficient: for example, our CEM-light approach (3-shot with 10 generations) completes the ARC-Challenge task in approximately 11 minutes. As shown in Table 10, this lighter setup reduces the total number of samples to just 3% of the original setting while still delivering substantial performance improvements over the base model. + CEM 10-shot adaptation + CEM 3-shot (30% of prompts) + CEM light (3% of prompts) 82.61 (1.02) 82.18 (1.02) 82.08 (1.02) LLAMA3-8B-INSTRUCT ARC-Challenge 80.63 (1.00) Method"
        },
        {
            "title": "Preprint",
            "content": "Figure 11: PCA of MISTRAL-7B-INSTRUCT-V0.3. We show the ratio of the variance captured by the top singular components on the y-axis, and the layer indices on the x-axis. Except for the Query, Key and Value projection matrices, small values only capture tiny fraction of variance in singular values in the parameter matrices. We acknowledge that CEM-based adaptation entails trade-off between one-time overhead it spends on searching the optimal combination weights for the SVF-tune vectors and performance. Increasing the number of few-shot samples or the number of generations can yield higher performance, but this comes at the cost of additional computational overhead. However, it is important to note that this adaptation cost is one-time overhead per task. The cost-per-prompt diminishes significantly when applied to tasks with large number of prompts. Moreover, in practical scenarios, CEM-based adaptation offers better scalability than few-shot prompting methods, which require increasing the length of every prompt, leading to much worse scaling as task sizes grow. In contrast, our method focuses on determining optimal expert vector combinations efficiently and avoids repetitive inference-time costs. However, we note that the overhead might be significant for tasks with very few prompts. Thus, the other adaptations methods might be more appropriate for these particular settings. We also highlight two immediate directions for improving efficiency: 1. Reducing the number of few-shot samples: As shown in our ablation study in Appendix B.2, substantial benefits can be seen even in the 3-shot setting, which requires only evaluation of only 30% of the number of prompts per generation. 2. Reducing the number of maximum generations: In the explored settings, the CEM parameters tend to converge early on, being very close to the final values after much lower number of generations than 100. Finally, in this work we only considered CEM due to its simplicity, there exist several different evolution algorithms empirically showing better efficiency and convergence properties that we hope will be explored in future research."
        }
    ],
    "affiliations": [
        "Institute of Science Tokyo, Japan",
        "Sakana AI, Japan"
    ]
}