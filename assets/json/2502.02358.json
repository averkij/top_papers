{
    "paper_title": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm",
    "authors": [
        "Ziyan Guo",
        "Zeyu Hu",
        "Na Zhao",
        "De Wen Soh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/."
        },
        {
            "title": "Start",
            "content": "MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm ZIYAN GUO, Singapore University of Technology and Design, Singapore ZEYU HU, LightSpeed Studios, Singapore NA ZHAO, Singapore University of Technology and Design, Singapore DE WEN SOH, Singapore University of Technology and Design, Singapore 5 2 0 2 6 ] . [ 3 8 5 3 2 0 . 2 0 5 2 : r Fig. 1. Demonstration of our MotionLabs versatility, performance and efficiency. Previous SOTA refer to multiple expert models, including MotionLCM [Dai et al. 2025], OmniControl [Xie et al. 2023], MotionFix [Athanasiou et al. 2024], CondMDI [Cohan et al. 2024] and MCM-LDM [Song et al. 2024]. All motions are represented using SMPL [Loper et al. 2023], where transparent motion indicates the source motion or condition, and the other represents the target motion. More qualitative results are available in the website and appendix. Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide versatile, unified framework capable of handling both human motion generation and editing, we introduce novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, corresponding author Authors addresses: Ziyan Guo, Singapore University of Technology and Design, Singapore, ziyan_guo@mymail.sutd.edu.sg; Zeyu Hu, LightSpeed Studios, Singapore, zeyuhu@global.tencent.com; Na Zhao, Singapore University of Technology and Design, Singapore, na_zhao@sutd.edu.sg; De Wen Soh, Singapore University of Technology and Design, Singapore, dewen_soh@sutd.edu.sg. our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: Project Website. CCS Concepts: Computing methodologies Motion processing; Motion capture; Virtual reality. Additional Key Words and Phrases: Motion Planning, Motion Control"
        },
        {
            "title": "INTRODUCTION",
            "content": "Human motion is crucial component of computer graphics and vision, with applications spanning game development, film production, and virtual reality [Guo et al. 2022a; Tevet et al. 2023]. With the advancements of generative diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; Song et al. 2020], human motion generation has garnered considerable attention, aiming at generating human motion aligned with the input conditions, such as text [Tevet et al. 2022, 2023; Zhang et al. 2022] and trajectory (i.e., joints coordinates) [Athanasiou et al. 2022, 2023; Dai et al. 2025; Fujiwara et al. 2025; Sampieri et al. 2024; Song et al. 2023; Sun et al. 2024; Xie et al. 2023; Zhang et al. 2023a]. On the other hand, in order to modify the motion assets in the industry, significant efforts have been dedicated to motion editing, which intended to modify the properties of prepared motion like motion style transfer [Aberman et al. 2020; Guo et al. 2024b; Jang et al. 2022; Song et al. 2024; Zhong et al. 2025]. As illustrated in Table 1, existing research in this domain predominantly focuses on isolated solutions tailored to specific tasks. 2 Ziyan Guo, Zeyu Hu, Na Zhao, and De Wen Soh Consequently, users are compelled to train multiple distinct models for generating and editing human motion, which is both inefficient and impractical. Although some studies [Fan et al. 2024; Luo et al. 2024; Shrestha et al. 2025; Yang et al. 2024; Zhang et al. 2025; Zhou et al. 2023; Zhou and Wang 2023] have endeavored to unify motionrelated tasks, they merely treat various modalities as conditions for motion generation and naively integrate them into single model. Such approaches not only constrain their editing capabilities but also fail to provide fine-grained trajectory control. Furthermore, these solutions fail to leverage the intrinsic connections between motion generation and editing, thereby hindering knowledge sharing across tasks. Instead, unified framework of generation and editing offers the potential to expand data volume by incorporating datasets from different tasks, thereby enhancing model performance on data-scarce tasks. Motivated by this benefit, as well as the success of large language models (LLMs)[Achiam et al. 2023; Dubey et al. 2024] in unifying diverse tasks to significantly enhance capability and generalization, we pose the following question: Can human motion generation and editing be effectively unified within single framework? To achieve this, it is essential to design an elegant and scalable paradigm. Drawing inspiration from the next-token prediction paradigm [Achiam et al. 2023; Brown et al. 2020], which has revolutionized the NLP field, we propose novel paradigm: MotionCondition-Motion. This paradigm is built upon three concepts source motion, condition, and target motion. Similar to how nexttoken prediction anticipates the subsequent word based on context, the Motion-Condition-Motion paradigm predicts the target motion based on the source motion and specified conditions. For any human motion generation task, the source motion can be treated as none, and the target motion must align with the provided conditions. For any human motion editing task, the target motion is derived from the source motion based on the conditions. By unifying these tasks within this elegant and scalable paradigm, this framework can be seamlessly extended to various human motion tasks and scaled across diverse datasets. Given that human motions are inherently tied to their semantics, trajectories, and styles in practical applications, we aim to unify several key tasks under this framework. These tasks include text-based motion generation and editing [Athanasiou et al. 2024; Tevet et al. 2022, 2023; Zhang et al. 2022], trajectory-based motion generation and editing [Dai et al. 2025; Xie et al. 2023], motion in-between [Cohan et al. 2024] and motion style transfer [Song et al. 2024; Zhong et al. 2025], as illustrated in Figure 1. Despite the proposed paradigm, significant challenges remain in balancing versatility, performance, and efficiency: 1) Unifying various tasks inevitably introduces additional modalities. However, adopting multiple cross-attention mechanisms for each task, as employed in generation-unified frameworks [Fan et al. 2024; Zhang et al. 2025], is both inefficient and suboptimal. This cross-attention approach risks neglecting the rich, deep features of these modalities and fails to establish direct interactions between the modalities and the target motion. 2) More sampling time is required for certain tasks (e.g., trajectory-based motion generation and motion in-between [Xie et al. 2023; Zhong et al. 2025]), as existing methods in these areas involve task-specific posterior guidance [Chung et al. 2022] during inference to improve conditional guidance. 3) Time asynchrony between the source motion and target motion may arise due to the limited scale of the paired editing dataset and the use of implicit positional encoding [Athanasiou et al. 2024; Chen et al. 2023; Cohan et al. 2024; Xie et al. 2023]. 4) Most importantly, naively integrating various motion generation and editing tasks into single framework can lead to task conflicts and catastrophic forgetting, impairing the frameworks overall performance. To address these challenges, we propose powerful generative framework, named MotionLab, built upon our designed MotionFlow Transformer (MFT) as shown in Figure 3. Inspired by MMDiT [Esser et al. 2024], our MFT also leverages rectified flows [Lipman et al. 2022; Liu et al. 2022], but we utilize them to map source motion to target motion based on specified conditions. Furthermore, unlike MM-DiT, which focuses exclusively on text and images, our MFT incorporates multiple modalities: source motion, target motion, text, trajectory, and style. In MFT, each modality is equipped with its own conditioning path for deep feature and fully interacts with the others through joint attention. This design enables MFT to enhance conditional generation and editing without requiring task-specific modules or posterior guidance for certain tasks. To ensure temporal synchronization between source motion and target motion, we incorporate an Aligned ROtational Position Encoding (Aligned ROPE) into MFT, explicitly aligning tokens in corresponding frames between the source and target motion. Additionally, to harmoniously integrate diverse tasks, we propose Task Instruction Modulation, which distinguishes tasks by introducing an additional embedding representing the task into the MFT. To facilitate effective multi-task training, we have devised curriculum-inspired training strategy, termed Motion Curriculum Learning, which accounts for the varying difficulty levels of different tasks. Intuitively, tasks involving fewer modalities or clearer condition information are simpler; for example, the reconstruction of source motion is easier than text-based motion generation. Adhering to the easy-to-hard training principle, we initially introduce the masked source motion reconstruction to extract discriminative motion features. Consequently, the training process in MotionLab is divided into two sequential stages: masked pretraining and supervised fine-tuning. During masked pre-training, the model reconstructs masked source motion to establish foundational understanding of motion features. In the supervised finetuning stage, the model progressively learns new tasks in an easyto-hard sequence. Our Motion Curriculum Learning not only fosters harmonious knowledge sharing across tasks but also supports Classifier-Free Guidance (CFG) [Ho and Salimans 2022] during inference, resulting in improved performance across variety of tasks. The main contributions of this work are as follows: 1) We introduce the Motion-Condition-Motion paradigm, which reformulates human motion generation and editing tasks using three core concepts, enabling unified formulation. 2) We propose the MotionLab framework within the Motion-Condition-Motion paradigm, featuring MotionFlow Transformer to enhance the conditional generation and editing, Aligned ROPE to ensure the synchronization between source motion and target motion, and Task Instruction Modulation for effective multi-task learning. 3) We develop the Motion Curriculum Learning strategy, which enables hierarchical training from MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm Method MDM [Tevet et al. 2023] MLD [Chen et al. 2023] OmniControl [Xie et al. 2023] MotionFix [Athanasiou et al. 2024] CondMDI [Cohan et al. 2024] MCM-LDM [Song et al. 2024] MotionGPT [Jiang et al. 2023] MotionCLR [Chen et al. 2024] Ours text-based generation text-based editing trajectory-based generation trajectory-based editing in-between style transfer Table 1. Summary of different methods focusing on motion generation and editing. indicates that the method has been trained for the task, indicates that the method has not been trained, and indicates that the method has not been trained but can be implemented in zero-shot manner. pre-training to sequential fine-tuning, effectively sharing the knowledge between various tasks. 4) We validate MotionLab on multiple benchmarks, demonstrating its superior versatility, performance, and efficiency over baselines."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Motion Generation and Editing. Motion generation can be classified based on input conditions. Among these, text-based motion generation is one of the most compelling areas [Chen et al. 2023; Guo et al. 2024a, 2022a,c, 2020; Jiang et al. 2023; Kim et al. 2023; Lin et al. 2023; Lu et al. 2023; Petrovich et al. 2021; Plappert et al. 2016; Tevet et al. 2022, 2023; Wang et al. 2024b; Zhang et al. 2023b, 2022], as it trains models to comprehend the semantics of text and generate corresponding pose sequences. To address the fine-grained requirements of practical applications, trajectory-based motion generation has been proposed [Dai et al. 2025; Karunratanakul et al. 2023; Shafir et al. 2023; Xie et al. 2023; Zhang et al. 2023a], where specific motion properties, such as joints reaching designated positions at specified times, are defined. Additionally, motion in-between [Cohan et al. 2024; Jiang et al. 2023; Pinyoanuntapong et al. 2024; Qin et al. 2022; Tevet et al. 2023] focuses on generating complete motion sequences given key poses at keyframes. To enable in-place editing of human motion, MotionFix [Athanasiou et al. 2024] introduces text-based motion editing using paired source and target motions. We extend this approach to trajectory-based motion editing by substituting text with joint trajectories. Meanwhile, style plays crucial role in human motion, leading to motion style-transfer [Aberman et al. 2020; Jang et al. 2022; Song et al. 2024; Zhong et al. 2025]. However, the aforementioned methods concentrate solely on specific tasks, rendering them impractical for real-world applications. Moreover, they overlook the intrinsic connections across different human motion tasks and fail to facilitate knowledge sharing among these tasks. In contrast, our unified framework enhances performance on data-scarce editing tasks through multi-task learning. Unified frameworks for human motion. There are also some efforts in existing methods that try to unify tasks related to human motion. One line of work [Jiang et al. 2023, 2025; Li et al. 2024; Ling et al. 2024; Luo et al. 2024; Wang et al. 2024b; Wu et al. 2024; Zhou et al. 2024] focuses on motion understanding, such as motion captioning or describing human motion in images and videos. Yet, these approaches often rely on GPT-like structures, which requires large amount of training resources and GPU memory. In addition, they fail to provide fine-grained control (e.g., trajectory-based generation and editing) over motion, which is crucial in practical applications. Another line of effort [Fan et al. 2024; Luo et al. 2024; Shrestha et al. 2025; Yang et al. 2024; Zhang et al. 2025; Zhou et al. 2023; Zhou and Wang 2023] highlights generating motion based on more modalities, such as music and speech. However, these approaches just only integrate more modalities into one model and cannot flexibly edit motion, which can cause them suffering from the multi-task learning and limit their scope of use. The closest to our work is MotionCLR [Chen et al. 2024], which does not support fine-grained control (e.g., trajectory-based generation and editing) and requires cumbersome manual adjustments to the attention mechanism."
        },
        {
            "title": "3 PRELIMINARY: RECTIFIED FLOWS",
            "content": "Fig. 2. Demonstration of the difference trajectory between diffusion models and rectified flows. This difference lies in that the trajectory of diffusion models is based on 𝑥𝑡 = (1 𝛼𝑡 )𝑥0+ 𝛼𝑡 𝜖, while the trajectory of rectified flows is based on 𝑥𝑡 = (1 𝑡 )𝑥0 + 𝑡𝑥1. This distinction leads to more robust learning by maintaining constant velocity, contributing to models efficiency [Zhao et al. 2024]. Flow matching methods [Esser et al. 2024; Fei et al. 2024; Lipman et al. 2022; Liu et al. 2022; Ma et al. 2024; Polyak et al. 2024] have recently received significant attention due to their generalizability and efficiency. As shown in Figure 2, these methods directly regress the transport vector field between noise distribution 𝑝1 and data distribution 𝑝0 with the straightest possible trajectories and sample by the corresponding ordinary differential equation (ODE) [Wang et al. 2024a]. Among these methods, rectified flows [Lipman et al. 2022; Liu et al. 2022] aim to learn trajectory from noise 𝑥0 to data 𝑥1, which can be formulated as 𝑥𝑡 = 𝜑 (𝑥0, 𝑥1, 𝑡), and the velocity 4 Ziyan Guo, Zeyu Hu, Na Zhao, and De Wen Soh 𝑣𝑡 = field 𝑣𝑡 of the trajectory 𝑥𝑡 can be defined by: 𝑑𝑥𝑡 𝑑𝑡 𝜑𝑡 (𝑥0, 𝑥1, 𝑡) 𝑡 Once we have learned this velocity field 𝑣𝑡 , we can get 𝑥0 from any 𝑥1 by numerically integrating. Hence, rectified flows 𝑣𝜃 are trained to predict 𝑣𝑡 by given 𝑥𝑡 and 𝑡, and the training object of rectified flows can be represented as: , 𝑡 [0, 1] (1) = L𝑅𝐹 (𝜃 ) = 1 0 E(𝑥0,𝑥1 )(𝑝0,𝑝1 ) [𝑣𝜃 (𝑡, 𝑥𝑡 ) 𝑣𝑡 2 2]𝑑𝑡 (2) Since the trajectory 𝑥𝑡 from 𝑝1 to 𝑝0 should be as straight as possible, it can be reformulated as the linear interpolation between 𝑥0 and 𝑥1, and the velocity field 𝑣𝑡 can be treated as constant, namely: 𝑥𝑡 = (1 𝑡)𝑥0 + 𝑡𝑥1 𝑣𝑡 = 𝑥1 𝑥 (4) (3) Therefore, the training objective can be reformulated as: L𝑅𝐹 (𝜃 ) = 1 E(𝑥0,𝑥1 )(𝑝0,𝑝1 ) [𝑣𝜃 (𝑡, 𝑥𝑡 ) (𝑥1 𝑥0)2 2]𝑑𝑡 (5) After the training of rectified flows is completed, the transfer from 𝑥1 to 𝑥0 can be described via the numerical integration of ODE: Task unconditional generation masked reconstruction reconstruction text-based generation trajectory-based generation motion in-between text-based editing trajectory-based editing style transfer Source Motion masked source motion complete source motion Condition text text/joints coordinates text/poses in keyframes text text/joints coordinates style motion Target Motion source motion source motion Table 2. Structuring human motion tasks within our Motion-ConditionMotion paradigm. Remarks. In particular, trajectory-based motion generation and motion in-between are highly similar, as they both aim to ensure that specific joints reach designated positions at specific times. The primary distinction between them is that the former is sparse in space (i.e., joints) but dense in time, whereas the latter is dense in space (i.e., joints) but sparse in time. To efficiently share the parameters and learned representations between the two tasks, we unify their conditions into single condition. Meanwhile, masked reconstruction is also similar to these two tasks. However, while these two tasks only include the coordinates of joints, the source motion also encompasses the velocity and angular velocity of joints. Therefore, they represent different modalities, and masked reconstruction constitutes distinct task. 𝑥𝑡 1 𝑁 = 𝑥𝑡 + 1 𝑁 𝑣𝜃 (𝑡, 𝑥𝑡 )"
        },
        {
            "title": "5 MOTIONLAB",
            "content": "(6) where 𝑁 is the discretization number of the interval [0,1]."
        },
        {
            "title": "4 MOTION-CONDITION-MOTION",
            "content": "To unify the tasks of human motion generation and editing in an elegant and scalable paradigm, we propose the paradigm of MotionCondition-Motion. As shown in Table 2, all these tasks are unified by three concepts: source motion, condition, and target motion. Motion Generation. For the motion generation tasks, including text/trajectory-based generation and motion in-between, the source motion can be treated as none, with the target motion aligning to the corresponding conditions. For instance, in text-based generation, the generated motion should align with the semantics of the provided text, such as karate kick illustrated in Figure 1. Masked reconstruction, as specific motion generation task, requires the target motion to align with the masked source motion in the specified frames without relying on additional conditions. Notably, the unconditional generation (given zero frames) and reconstruction (given all frames) are special cases of masked reconstruction, thus these three tasks can share the same task instruction as described in Section 5.2. Motion Editing. For motion editing, the source motion must be provided, and the target motion is derived from the source motion based on the specified conditions. In the case of text-based motion editing, the generated motion should originate from the source motion, with modifications applied only to the specified parts as dictated by the provided text, such as use the opposite leg. For trajectory-based editing, the source motion should be aligned with the given joints coordinates, ensuring that the specified joints in the source motion are accurately moved to the designated positions within the specified frames. In motion style transfer, the generated motion should adopt the style of the style motion while preserving the semantics of the source motion. Based on our proposed Motion-Condition-Motion paradigm, we introduce unified framework named MotionLab, as illustrated in Figure 3(a). The core of MotionLab is the MotionFlow Transformer (MFT) (Sec. 5.1), inspired by MM-DiT [Esser et al. 2024], which leverages rectified flow to map source motion 𝑀𝑆 R𝑁 𝐷 to target motion 𝑀𝑆 R𝑁 𝐷 based on the corresponding condition 𝐶 for each task. To enable task differentiation, we propose Task Instruction Modulation (Sec. 5.2), where task-specific instruction 𝐼 R1768 extracted from the CLIP [Radford et al. 2021] is also input into MFT alongside 𝑀𝑆 , 𝑀𝑇 , and 𝐶. At each timestep 𝑡, MFT is trained to predict velocity field 𝑣𝑡 , which is derived via linear interpolation between target motion 𝑀𝑇 and Gaussian noise 𝜖 R𝑁 𝐷 . For effective multi-task training, we adopt Motion Curriculum Learning (Sec. 5.3) which organizes tasks hierarchically to facilitate learning. Once trained, MotionLab can map 𝑀𝑆 to 𝑀𝑇 based on the specified 𝐶, by predicting 𝑣𝑡 in descending order of timestep 𝑡 as described in Sec. 5.4."
        },
        {
            "title": "5.1 MotionFlow Transformer",
            "content": "As shown in the Figure 3 (b), MotionFlow Transformer contains three key components: Joint Attention to interact tokens from different modalities; Condition Path for distinguishing tokens from different modalities and extracting their representations, and Aligned ROPE for position encoding of modalities with time information. Joint Attention. We first adopt the joint attention mechanism [Esser et al. 2024], through which tokens from different modalities can interplay with each other. Specifically, all these tokens will be projected to the query, key, and value representations, and then will be concatenated into sequence of orderly tokens. Subsequently, these orderly tokens are applied by the attention operation, whose output is again split into corresponding token of different modalities. MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm 5 Fig. 3. Illustration of our MotionLab and the detail of its MotionFlow Transformer (MFT). Condition Path. While the joint attention is able to interact tokens from different modalities, there is still need to differentiate different tokens. In addition to the QKV projection and FeedForward Network (FFN) in the attention mechanism, as used in MM-DiT, our MFT incorporates the adaptive Layer Normalization (adaLN) and modulation mechanism [Peebles and Xie 2023] for each modality, enhancing conditional generation and editing capabilities. Aligned Rotational Position Encoding. Considering that the use of absolute position encoding in existing methods [Chen et al. 2023] can weaken the temporal alignment between source motion and target motion due to the limited scale of paired datasets, we adopt relative position encoding method, ROtational Position Encoding (ROPE) [Su et al. 2024]. ROPE explicitly embeds the relative distances between tokens, preserving temporal relationships more effectively. Instead of naively applying 3-dimensional ROPE to distinguish source motion, target motion, and conditions with time information (e.g., trajectory), we propose Aligned ROPE, which encodes these components with appropriate temporal information using 1-dimensional ROPE. This design avoids the confusion caused by 3-dimensional ROPE, where distances between tokens within modality can interfere with cross-modality relationships, ensuring better alignment and representation."
        },
        {
            "title": "5.2 Task Instruction Modulation",
            "content": "MM-DiT incorporates modulation mechanism to enhance textto-image generation by inputting text embedding as modulation. However, in our unified framework, tasks involve diverse and multiple modalities, making simple input strategies like task-specific modalities or learned task token (e.g., [TASK]) insufficient to handle this complexity. Noticing the flexibility of natural language, we leverage text representations learned by foundation models (e.g., CLIP [Radford et al. 2021]) to enable task differentiation in MotionLab. For example, instead of directly inputting the corresponding style for motion style transfer, we use the text embedding of edit source motion by given style. This simple yet effective modulation approach enhances flexibility and scalability, allowing seamless extension to various tasks involving multiple modalities."
        },
        {
            "title": "5.3 Motion Curriculum Learning",
            "content": "To achieve effective multi-task learning and facilitate knowledge sharing between tasks, we propose an easy-to-hard hierarchical training strategy inspired by curriculum learning [Bengio et al. 2009]. Specifically, new tasks are sequentially introduced into the training based on their difficulty , guided by the following assumptions: 1) The fewer modalities task involves, the simpler the task; 2) Editing tasks are easier than generating tasks, as only the conditional difference between source motion and target motion needs to be learned; 3) The more specific the conditional information (e.g., source motion) provided, the simpler the task becomes. The importance of these three criteria decreases in order. Guided by the easy-to-hard training principle, the training process in MotionLab is divided into two stages: self-supervised pre-training and supervised fine-tuning. 6 Ziyan Guo, Zeyu Hu, Na Zhao, and De Wen Soh Pre-training. Intuitively, the reconstruction of masked source motion is the easiest task. Hence, we first train the model based on the masked source motion, independent of the conditions. This approach allows the model to learn prior motion representations independent of conditions, thereby generalizing to different tasks. For the masked ratio of source motion, we propose randomly masking from zero frames to all frames. Compared to the fixed 75% masked ratio like MAE [He et al. 2022], this flexible strategy provides tasks of varying difficulty levels, avoiding overfitting on simple tasks (all frames) and mode collapse (zero frames) on difficult tasks. Furthermore, this strategy seamlessly performs source motion reconstruction (i.e., all frames) and unconditional training (i.e., zero frames), which is crucial for CFG in Section 5.4. Additionally, motion in-between and trajectory-based generation can be viewed as generalized cases of masked reconstruction by masking certain frames or joints coordinates. Since they share the same condition path as discussed in Sec. 4, we remove the text condition and treat their source and target motions as additional reconstruction motion pairs for pre-training. Specifically, we pretrain MotionLab using these three sources for 1,000 epochs. Fine-tuning. In the supervised fine-tuning stage, we train MotionLab on tasks in an easy-to-hard sequence. Specifically, new task is introduced into training every 200 epochs in the following order: ➀ text-based generation, ➁ style-based generation (an auxiliary task for training the condition path of the style, not our primary goal), ➂ trajectory-based editing (without text), ➃ text-based editing, ➄ style transfer, ➅ motion in-between and trajectory-based generation, ➆ trajectory-based editing. This progressive learning strategy ensures effective adaptation and knowledge sharing across tasks. Particularly, ➀ and ➁ are the simplest task because they only include one modality, whereas others include at least two modalities. Among tasks involving two modalities, ➂, ➃, and ➄ take priority over ➅ since they are editing tasks. Additionally, as text is less specific than trajectory but more specific than style, the order is ➂, ➃, and ➄. To mitigate catastrophic forgetting, previous tasks are trained alongside new tasks, based on the probability derived from the FID of the last evaluation. However, the FID scales for different tasks vary due to their differing difficulty levels. Consequently, we use the percentage change compared to the previous evaluation as the probability, which encourages the model to re-learn forgotten tasks or tasks that it has not yet fully mastered. To support classifier-free guidance, we also train the model to unconditionally generate and reconstruct the complete source motion. Empirically, in this stage, 5% probability is allocated for unconditional generation, 5% for reconstructing the complete source motion, 45% for previous tasks, and 45% for the new task. In summary, this training strategy, transitioning from single modality to multiple modalities, enables the model to progressively learn each modalitys representation independently before capturing interactions between multiple modalities. This approach offers three key benefits: 1) adaptability to various tasks; 2) seamless support for CFG during inference; and 3) flexible training management, minimizing the need for retraining due to errors. Method FID R@3 Diversity MModality AITS 0.002 1.087 0.544 1.954 0.473 0.116 0.232 0.254 0.304 0.269 0. GT T2M [Guo et al. 2022b] MDM [Tevet et al. 2023] MotionDiffuse [Zhang et al. 2022] MLD [Chen et al. 2023] T2M-GPT[Zhang et al. 2023b] MotionGPT [Jiang et al. 2023] CondMDI [Cohan et al. 2024] MotionLCM [Dai et al. 2025] MotionCLR [Chen et al. 2024] Ours 0.797 0.736 0.611 0.739 0.772 0.775 0.778 0.6450 0.698 0.831 0.805 Table 3. Evaluation of text-based motion generation on HumanML3D[Guo et al. 2022a] dataset. All AITS have been recalculated on RTX 4090D. The models in bold are the optimal models, and the models in underline are the sub-optimal models. - 0.040 26.04 15.51 0.236 11.24 1.240 57.25 0.045 0.830 0.068 9.503 9.188 9.559 11.10 9.724 9.761 9.528 9.749 9.607 9.607 9.589 2.799 2.090 2.799 0.730 2.413 1.856 2.008 - 2.259 1.985 3.029 MM Dist 2.974 3.340 5.566 2.958 3.196 3.118 3.096 - 3.012 2.806 2. Method Joints FID R@3 Diversity GT GMD [Karunratanakul et al. 2023] PriorMDM [Shafir et al. 2023] OmniControl [Xie et al. 2023] MotionLCM [Dai et al. 2025] Ours OmniControl [Xie et al. 2023] Ours - pelvis pelvis pelvis pelvis pelvis all all 0.002 0.576 0.475 0.212 0.531 0.095 0.310 0.126 0.797 0.665 0.583 0.678 0.752 0.740 0.693 0.765 9.503 9.206 9.156 9.773 9.253 9.502 9.502 9.554 Foot skate ratio 0.000 0.101 - 0.057 - 0.007 0.061 0.002 Average Error - 0.1439 0.4417 0.3226 0.1897 0.0286 0.0404 0.0334 AITS - 137.0 19.83 39.78 0.035 0.133 76.71 0.134 Table 4. Evaluation of trajectory-based motion generation on HumanML3D[Guo et al. 2022a] dataset. AITS of all models have been recalculated on RTX 4090D. AITS Method Condition - text text trajectory trajectory GT TMED [Athanasiou et al. 2024] Ours TMED [Athanasiou et al. 2024] Ours generated-to-target retrieval R@1 R@2 R@3 AvgR 73.15 38.69 56.34 60.01 72.65 Table 5. Evaluation of text-based and trajectory-based motion editing on MotionFix [Athanasiou et al. 2024] dataset. TMED mean that we reimplement the models since original models are trained on the skeleton of SMPL format, while our models are trained on HumanML3D format. AITS of all models have been recalculated on RTX 4090D. Average Error - - - 0.129 0.027 84.09 50.61 70.40 73.33 82.71 89.49 62.23 77.24 82.69 87.89 - 26.57 0.16 30.56 0. 2.09 4.15 3.54 2.67 2."
        },
        {
            "title": "5.4 MotionLab Inference",
            "content": "During inference, Classifier-Free Guidance (CFG) [Ho and Salimans 2022] is incorporated for both motion generation and motion editing to boost sampling quality and align conditions and target motion. For all motion generation tasks, we generate target motion 𝑀𝑇 with the guidance of arbitrary conditions 𝐶: 𝑣𝜃 (𝑀𝑇 , 𝑡, 𝐶) = 𝑣𝜃 (𝑀𝑇 𝑡, ) + 𝜆𝐶 [𝑣𝜃 (𝑀𝑇 𝑡, 𝐶) 𝑣𝜃 (𝑀𝑇 𝑡, )] (7) where 𝑡 is the timestep and 𝜆𝐶 > 1 is hyper-parameter to control the strength of corresponding conditional guidance. For all motion editing tasks, which aim to modify the source motion based on the condition. Hence, we generate the target motion 𝑀𝑇 with source motion 𝑀𝑆 first and then condition 𝐶: 𝑣𝜃 (𝑀𝑇 , 𝑡, 𝑀𝑆, 𝐶) =𝑣𝜃 (𝑀𝑇 𝑡, , ) + 𝜆𝑆 [𝑣𝜃 (𝑀𝑇 𝑡, 𝑆, ) 𝑣𝜃 (𝑀𝑇 𝑡, , )] + 𝜆𝐶 [𝑣𝜃 (𝑀𝑇 𝑡, 𝑆, 𝐶) 𝑣𝜃 (𝑀𝑇 𝑡, 𝑆, )] (8) where 𝜆𝑆 > 1 is hyper-parameter to control the strength of source motion guidance. MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm 7 Fig. 4. Qualitative results of MotionLab on the text-based motion generation. For clarity, as time progresses, motion sequences transit from light to dark colors. Fig. 5. Qualitative results of MotionLab on the text-based motion editing. The transparent motion is the source motion, and the other is the generated motion. Fig. 6. Qualitative results of MotionLab on the trajectory-based motion generation. The red balls are the trajectory of the pelvis, right hand and right foot."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "Datasets. We evaluate our framework using the following datasets. To evaluate the text-based motion generation, the trajectory-based motion generation, motion in-between and motion style transfer, we leverage the HumanML3D [Guo et al. 2022a] dataset, which comprises 14646 motions and 44970 motion annotations. To evaluate on the text-based and trajectory-based motion editing, we utilize MotionFix [Athanasiou et al. 2024] dataset, which is the first dataset for text-based human motion editing including 6730 motion pairs. Evaluation Metrics. We evaluate our framework using the following metrics: 1) To evaluate the text-based motion generation, following the Chen et al., we introduce the FID to evaluate the distribution gap between the generated and original motions; Diversity to calculate the corresponding variance between motions; R-precision (R@K) to measure the proximity of the generated motion to the text or motion; Foot skating ratio to evaluate the physical plausibility of motion; Multi-modal Distance (MM Dist) calculates the distance between motions and texts. We also introduce we Average Inference Time per Sample (AITS) measured in seconds to evaluate the inference efficiency; 2) To evaluate the trajectory-based motion generation and motion in-between, following Xie et al., we introduce the Average Error to measures the mean distance between the generated motion locations and the keyframe locations; 3) To evaluate the text-based and trajectory-based motion editing, following Athanasiou et al., we introduce the AvgR to measure the success rate of retrieval from edited motion to target motion; 4) To evaluate the motion style transfer, following the Song et al., we introduce the Style Recognition Accuracy (SRA) and Content Recognition Accuracy (CRA) to measure the stylistic and content accuracy of the generated motion; Trajectory Similarity Index (TSI) to evaluate the trajectory preservation from source motion. 8 Ziyan Guo, Zeyu Hu, Na Zhao, and De Wen Soh Method w/o rectified flows w/o MotionFlow Transformer w/o Aligned ROPE w/o task instruction modulation w/o motion curriculum learning Ours Ours text gen. (FID) 0.334 0.534 0.246 0.279 2.236 0.209 0. traj. gen. (avg. err.) 0.0359 0.0447 0.0886 0.0401 0.1983 0.0398 0.0334 text edit (R@1) 54.38 51.26 45.39 55.96 28.56 41.44 56.34 traj. edit (R@1) 69.21 65.34 61.99 70.01 36.61 59.86 72.65 in-between (avg. err.) 0.0289 0.0349 0.0756 0.0288 0.1682 0.0371 0.0273 style transfer (SRA) 63.96 53.83 56.59 63.91 34.23 67.55 64.97 Table 6. Ablation studies of our MotionLabs main designs on each task. Ours means that we use this framework to train models in the same size for each task separately. For text-based motion generation, we compare FID; for trajectory-based motion generation and motion in-between. we compare average error; for motion editing, we compare R@1; for motion style transfer, we compare the SRA. Additional ablation studies are available in the supplementary. Implementation Details. In order to fairly compare our model with other models, motions from all datasets have been retargeted into one skeleton following HumanML3D format with 20 fps, where the number of joint 𝐽 is 22 and the dimension of motion feature 𝐷 is 263. The learning rate is set to be 1104. The timesteps are set to 1000 for training and 50 for inference. Our models are trained by four RTX 4090D with each batch of 64. To ensure fair comparison, the AITS of all models are recalculated using one RTX 4090D."
        },
        {
            "title": "6.1 Quantitative Results",
            "content": "Overall Performance. As shown in Table 3 to Table 5, and Figure 7 to Figure 8, MotionLab demonstrates promising performance across all benchmarks, underscoring the effectiveness of our frameworks design. Notably, as MotionLab is unified framework without taskspecific designs, it must balance versatility, performance, and efficiency. While it does not surpass the state-of-the-art in text-based motion generation  (Table 3)  , it achieves competitive performance in this task and excels in other generation and editing tasks. Specifically, as shown in Table 3 and Figure 10, MotionLab achieves superior performance (lowest FID, which is the key metric for generation tasks) with relatively fast inference time (third-lowest AITS). For trajectory-based tasks  (Table 4)  and the motion in-between task (Figure 7), MotionLab achieves lower average error. We believe these improvements stem from the effectiveness of masked pre-training and Aligned ROPE, which ensures spatial and temporal synchronization between the trajectory and target motion."
        },
        {
            "title": "6.2 Qualitative Results",
            "content": "As shown in the Figure 4 from Figure 6, our framework presents its powerful capabilities to generate motion aligned with the conditions and edit source motion based on the condition, demonstrating its versatility and performance. For more visualization results, please kindly refer to the supplementary and project website."
        },
        {
            "title": "6.3 Ablation Studies",
            "content": "In this section, we perform several ablation experiments on our framework to validate the designs in MotionLab: the first variant replaces rectified flows with diffusion models; the second variant uses regular transformer (i.e., without modulation mechanism and adopting cross-attention) instead of MFT. The third variant uses the implicit 1D-learnable encoding used in the past instead of Aligned ROPE; The fourth variant does not adopt the Task Instruction Modulation; the fifth variant directly learns all tasks based on their FID compared to the last evaluation. Meanwhile, we also use the same model to train specialist models for each task. As illustrated in Table 6, the removal of motion curriculum learning markedly diminishes model performance across all tasks, underscoring its pivotal role in facilitating knowledge transfer between diverse tasks. This phenomenon may be attributed to the strategys capacity to enable the model to integrate its comprehension of spatial conditions (e.g., source motion, trajectory, and intermediate states) with abstract conditions (e.g., text and style), given that the latter can be partially represented by the former. Also, as shown in Figure 9 and Table 5, Aligned ROPE is essential for time-related tasks, significantly reducing the average error. It effectively aligns source motion and target motion temporally, contributing to high R-precision in editing tasks. According to the results of Our, our unified framework achieves comparable outcomes to specialist models with the same parameters in each conditional path. Notably, our unified framework outperforms specialist models with lower average error, likely due to the masked pre-training that aligns spatial information. Furthermore, our unified framework also surpasses specialist models in editing tasks, potentially due to the increased data from generation tasks and the learned conditional differences between source motion and target motion. Additionally, we validate our framework on the timesteps of inference. As shown in Figure 10, our framework demonstrates an optimal balance between generation quality and efficiency."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Building on our proposed Motion-Condition-Motion paradigm, we have developed the MotionLab framework to unify human motion generation and editing. We have introduced the MotionFlow Transformer leverage the rectified flows to learn the mapping from source motion to target motion based on specified conditions. Additionally, we have incorporated Aligned Rotational Position Encoding to ensure synchronization between source motion and target motion, Task Instruction Modulation, and Motion Curriculum Learning for effective multi-task learning. Our proposed MotionLab framework demonstrates superior versatility, performance and efficiency compared to existing state-of-the-art methods. However, our work has several limitations for future improvement. Specifically, our framework primarily focuses on limb motion and does not account for finger or facial features. This limitation poses challenges for practical applications and necessitates additional processing by users. Future research may explore the integration of motion tasks involving fingers and faces into MotionLab. MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm"
        },
        {
            "title": "REFERENCES",
            "content": "Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, and Baoquan Chen. 2020. Unpaired motion style transfer from video to animation. ACM Transactions on Graphics (TOG) 39, 4 (2020), 641. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). Nikos Athanasiou, Alpár Ceske, Markos Diomataris, Michael Black, and Gül Varol. arXiv preprint 2024. MotionFix: Text-Driven 3D Human Motion Editing. arXiv:2408.00712 (2024). Nikos Athanasiou, Mathis Petrovich, Michael Black, and Gül Varol. 2022. Teach: Temporal action composition for 3d humans. In 2022 International Conference on 3D Vision (3DV). IEEE, 414423. Nikos Athanasiou, Mathis Petrovich, Michael Black, and Gül Varol. 2023. SINC: Spatial composition of 3D human motions for simultaneous action generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 99849995. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning. 4148. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 18771901. Ling-Hao Chen, Wenxun Dai, Xuan Ju, Shunlin Lu, and Lei Zhang. 2024. Motionclr: Motion generation and training-free editing via understanding attention mechanisms. arXiv preprint arXiv:2410.18977 (2024). Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. 2023. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1800018010. Hyungjin Chung, Jeongsol Kim, Michael Mccann, Marc Klasky, and Jong Chul Ye. 2022. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687 (2022). Setareh Cohan, Guy Tevet, Daniele Reda, Xue Bin Peng, and Michiel van de Panne. 2024. Flexible motion in-betweening with diffusion models. In ACM SIGGRAPH 2024 Conference Papers. 19. Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. 2025. Motionlcm: Real-time controllable motion generation via latent consistency model. In European Conference on Computer Vision. Springer, 390408. Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34 (2021), 87808794. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning. Zhaoxin Fan, Longbin Ji, Pengxin Xu, Fan Shen, and Kai Chen. 2024. Everything2Motion: Synchronizing Diverse Inputs via Unified Framework for Human Motion Synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 16881697. Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. 2024. Flux that plays music. arXiv preprint arXiv:2409.00587 (2024). Kent Fujiwara, Mikihiro Tanaka, and Qing Yu. 2025. Chronologically Accurate Retrieval for Temporal Grounding of Motion-Language Models. In European Conference on Computer Vision. Springer, 323339. Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. 2024a. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19001910. Chuan Guo, Yuxuan Mu, Xinxin Zuo, Peng Dai, Youliang Yan, Juwei Lu, and Li Cheng. 2024b. Generative Human Motion Stylization in Latent Space. arXiv preprint arXiv:2401.13505 (2024). Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022a. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 51525161. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022b. Generating Diverse and Natural 3D Human Motions From Text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 51525161. Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. 2022c. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision. Springer, 580597. Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. 2020. Action2motion: Conditioned generation of 3d human motions. In Proceedings of the 28th ACM International Conference on Multimedia. 20212029. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1600016009. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022). Deok-Kyeong Jang, Soomin Park, and Sung-Hee Lee. 2022. Motion puzzle: Arbitrary motion style transfer by body part. ACM Transactions on Graphics (TOG) 41, 3 (2022), 116. Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. 2023. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems 36 (2023), 2006720079. Biao Jiang, Xin Chen, Chi Zhang, Fukun Yin, Zhuoyuan Li, Gang Yu, and Jiayuan Fan. 2025. Motionchain: Conversational motion controllers via multimodal prompts. In European Conference on Computer Vision. Springer, 5474. Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. 2023. Guided motion diffusion for controllable human motion synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 21512162. Jihoon Kim, Jiseob Kim, and Sungjoon Choi. 2023. Flame: Free-form language-based motion synthesis & editing. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 82558263. Chuqiao Li, Julian Chibane, Yannan He, Naama Pearl, Andreas Geiger, and Gerard PonsMoll. 2024. Unimotion: Unifying 3D Human Motion Synthesis and Understanding. arXiv preprint arXiv:2409.15904 (2024). Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. 2023. Motion-x: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems 36 (2023), 2526825280. Zeyu Ling, Bo Han, Shiyang Li, Hongdeng Shen, Jikang Cheng, and Changqing Zou. 2024. MotionLLaMA: Unified Framework for Motion Synthesis and Comprehension. arXiv preprint arXiv:2411.17335 (2024). Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. 2022. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 (2022). Xingchao Liu, Chengyue Gong, and Qiang Liu. 2022. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003 (2022). Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael In Seminal Graphics Black. 2023. SMPL: skinned multi-person linear model. Papers: Pushing the Boundaries, Volume 2. 851866. Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. 2023. Humantomato: Text-aligned whole-body motion generation. arXiv preprint arXiv:2310.12978 (2023). Mingshuang Luo, Ruibing Hou, Hong Chang, Zimo Liu, Yaowei Wang, and Shiguang Shan. 2024. M3GPT: An Advanced Multimodal, Multitask Framework for Motion Comprehension and Generation. arXiv preprint arXiv:2405.16273 (2024). Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. 2024. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740 (2024). William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. Mathis Petrovich, Michael Black, and Gül Varol. 2021. Action-conditioned 3d human motion synthesis with transformer vae. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1098510995. Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. 2024. Mmm: Generative masked motion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 15461555. Matthias Plappert, Christian Mandery, and Tamim Asfour. 2016. The kit motionlanguage dataset. Big data 4, 4 (2016), 236252. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. 2024. Movie Gen: Cast of Media Foundation Models. arXiv:2410.13720 [cs.CV] https://arxiv.org/abs/2410. 13720 Jia Qin, Youyi Zheng, and Kun Zhou. 2022. Motion In-Betweening via Two-Stage Transformers. ACM Trans. Graph. 41, 6 (2022), 1841. Zixiang Zhou and Baoyuan Wang. 2023. Ude: unified driving engine for human motion generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 56325641. 10 Ziyan Guo, Zeyu Hu, Na Zhao, and De Wen Soh Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. Alessio Sampieri, Alessio Palma, Indro Spinelli, and Fabio Galasso. 2024. Length-Aware Motion Synthesis via Latent Diffusion. arXiv preprint arXiv:2407.11532 (2024). Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit Bermano. 2023. Human motion diffusion as generative prior. arXiv preprint arXiv:2303.01418 (2023). Aayam Shrestha, Pan Liu, German Ros, Kai Yuan, and Alan Fern. 2025. Generating Physically Realistic and Directable Human Motions from Multi-modal Inputs. In European Conference on Computer Vision. Springer, 117. Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020). Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. 2023. Loss-guided diffusion models for plugand-play controllable generation. In International Conference on Machine Learning. PMLR, 3248332498. Wenfeng Song, Xingliang Jin, Shuai Li, Chenglizhao Chen, Aimin Hao, Xia Hou, Ning Li, and Hong Qin. 2024. Arbitrary Motion Style Transfer with Multi-condition Motion Latent Diffusion Model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 821830. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. Haowen Sun, Ruikun Zheng, Haibin Huang, Chongyang Ma, Hui Huang, and Ruizhen Hu. 2024. LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model. In ACM SIGGRAPH 2024 Conference Papers. 19. Guy Tevet, Brian Gordon, Amir Hertz, Amit Bermano, and Daniel Cohen-Or. 2022. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision. Springer, 358374. Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. 2023. Human Motion Diffusion Model. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=SJ1kSyO2jwu Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017). Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. 2024a. Frieren: Efficient Video-to-Audio Generation with Rectified Flow Matching. arXiv preprint arXiv:2406.00320 (2024). Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, and Dan Xu. 2024b. MotionGPT-2: General-Purpose Motion-Language Model for Motion Generation and Understanding. arXiv preprint arXiv:2410.21747 (2024). Qi Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, and Chi-Keung Tang. 2024. MotionLLM: Multimodal Motion-Language Learning with Large Language Models. arXiv preprint arXiv:2405.17013 (2024). Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. 2023. Omnicontrol: Control any joint at any time for human motion generation. arXiv preprint arXiv:2310.08580 (2023). Han Yang, Kun Su, Yutong Zhang, Jiaben Chen, Kaizhi Qian, Gaowen Liu, and Chuang Gan. 2024. UniMuMo: Unified Text, Music and Motion Generation. arXiv preprint arXiv:2410.04534 (2024). Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. 2023b. Generating human motion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1473014740. Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. 2022. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001 (2022). Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. 2025. Large motion model for unified multi-modal motion generation. In European Conference on Computer Vision. Springer, 397421. Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, and Ziwei Liu. 2023a. Finemogen: Fine-grained spatio-temporal motion generation and editing. Advances in Neural Information Processing Systems 36 (2023), 1398113992. Wenliang Zhao, Minglei Shi, Xumin Yu, Jie Zhou, and Jiwen Lu. 2024. FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner. arXiv preprint arXiv:2409.18128 (2024). Lei Zhong, Yiming Xie, Varun Jampani, Deqing Sun, and Huaizu Jiang. 2025. Smoodi: Stylized motion diffusion model. In European Conference on Computer Vision. Springer, 405421. Zixiang Zhou, Yu Wan, and Baoyuan Wang. 2023. unified framework for multimodal, multi-part human motion synthesis. arXiv preprint arXiv:2311.16471 (2023). Zixiang Zhou, Yu Wan, and Baoyuan Wang. 2024. AvatarGPT: All-in-One Framework for Motion Understanding Planning Generation and Beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13571366. MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm 11 encoding are better than 3D-position encoding by avoiding introducing distances between different modalities, and ROPE are better than learnable position encoding by explicit positional encoding. Hence our 1D-ROPE outperforms all others variants, demonstrating its effective to embed the position information into tokens. To further validate the motion curriculum learning, we adopt the variant of removing the masked pre-training and directly supervised fine-tuning in order; the variant of with masked pre-training but supervised fine-tuning all tasks together; the variant of introducing masked reconstruction, motion in-between and trajectory based motion generation in orderly. As shown in Table 8, our proposed motion curriculum learning outperforms all other variants, highlighting the effective of masked pre-training and fine-tuning tasks in order by avoiding gradient conflicts between different tasks. Specifically, the variant of masked pre-training in order demonstrates that necessity of introduce motion in-between and trajectory-based motion generation together, or will greatly weakens the performance of the model in the latter task."
        },
        {
            "title": "C REPRESENTATION FOR EACH MODALITY",
            "content": "We represent the features of all modalities as tokens for the attention mechanism [Vaswani 2017]. Specifically, source motion and target motion are represented as 𝑀𝑆 R𝑁 𝐷 and 𝑀𝑇 R𝑁 𝐷 , and we first ignore timestep 𝑡 here. For the instruction, it is represented as 𝐼 R1768 extracted from the CLIP [Radford et al. 2021]. For available conditions 𝐶, the text is represent as 𝑝 R77768 extracted from the last hidden layer of CLIP, the trajectory is represented as ℎ R𝑁 𝐽 3, and the style is represented as 𝑠 R1512 extracted from Zhong et al.."
        },
        {
            "title": "D INSTRCUTIONS FOR EACH TASK",
            "content": "As shown in the Table9, the instructions in the Task Instruction Modulations for each task are presented, which benefits our framwork to distinguish differents tasks."
        },
        {
            "title": "E CLASSIFIER FREE GUIDANCE FOR EACH TASK",
            "content": "As shown in Table10, strengths of classifier free guidance for each task are presented, which contributing to the results quality during sampling. 3D ASSETS We have borrowed some 3D assets for our video and figure from the Internet, including Dojo Matrix Drunken Wrestlers, Basketball Court, Grandmas Place, DAE Diorama retake Small farm, DAE Diorama retake Small farm, Japanese Small Shrine Temple 0002."
        },
        {
            "title": "A ADDITIONAL QUANTITATIVE RESULTS",
            "content": "Fig. 7. Comparison of the motion in-between with CondMDI [Cohan et al. 2024] on HumanML3D [Guo et al. 2022a], which shows that our model outperforms CondMDI. Fig. 8. Comparison of the motion style transfer with MCM-LDM [Song et al. 2024] on subset of HumanML3D [Guo et al. 2022a]. This shows that our model has stronger ability to preserve the semantics of source motion and stronger ability to learn the style of style motion."
        },
        {
            "title": "B ADDITIONAL ABLATION STUDIES",
            "content": "To further validate the designs in our framework, we perform traditional ablation studies in this section. To further validate the Aligned ROPE, we also introduce the variant of 3D-Learnable and 3D-ROPE to distinguish the source motion, target motion and trajectory. As shown in Table 7, 1D-position 12 Ziyan Guo, Zeyu Hu, Na Zhao, and De Wen Soh Fig. 9. Ablation results of MotionLab on the motion in-between. Beige motion is use 1D-learnable position encoding, purple motion use Aligned ROPE, and gray motions are the poses provided in keyframes, demonstrating the importance of Aligned ROPE. Method 1D-Learnable 3D-Learnable 3D-ROPE 1D-ROPE (ours) text gen. (FID) 0.246 0.346 0.241 0.223 traj. gen. (avg. err.) 0.0886 0.1865 0.0579 0.0334 text edit (R@1) 45.39 35.46 51.34 56.34 traj. edit (R@1) 61.99 53.74 70.00 72.65 in-between (avg. err.) 0.0756 0.1460 0.0354 0.0273 style transfer (SRA) 56.59 58.81 62.46 64. Table 7. Ablation studies of our MotionLabs position encoding on each task. Method random selection based on FID removing the masked pre-training supervised fine-tuning all tasks together masked pre-training in order motion curriculum learning text gen. (FID) 2.236 0.861 1.331 0.256 0.223 traj. gen. (avg. err.) 0.1983 0.0932 0.1317 0.0423 0.0334 text edit (R@1) 28.56 44.99 38.19 56.33 56.34 traj. edit (R@1) 36.61 63.92 55.22 69.31 72. in-between (avg. err.) 0.1682 0.0639 0.1143 0.0264 0.0273 style transfer (SRA) 34.23 57.59 50.59 64.39 64.97 Table 8. Ablation studies of our MotionLabs motion curriculum learning on each task. Task unconditional generation masked source motion generation reconstruct source motion trajectory-based generation (without text) in-between (without text) style-based generation trajectory-based editing text-based editing style transfer in-between (with text) trajectory-based generation (with text) text-based generation Table 9. Instructions in the Task Instruction Modulations for each task. Instruction reconstruct given masked source motion. reconstruct given masked source motion. reconstruct given masked source motion. generate motion by given trajectory. generate motion by given key frames. generate motion by given style. edit source motion by given trajectory. edit source motion by given text. generate motion by the given style and content. generate motion by given text and key frames. generate motion by given text and trajectory. generate motion by given text. Source Motion Guidance Condition Guidance Task trajectory-based generation (without text) in-between (without text) text-based generation style-based generation trajectory-based editing (without text) text-based editing style transfer in-between (with text) trajectory-based generation (with text) trajectory-based editing 1.5 1.5 2.5 2.5 2 2 2.5 2 2 2 Table 10. Strength of classifier free guidance for each task. - - 2 2 2.5 2 Fig. 10. Comparison of the inference time on text-based motion generation. We calculate AITS on the test set of HumanML3D [Guo et al. 2022a] without model or data loading parts. All tests are performed on the same RTX 4090D. The closer the models points are to the lower left corner, the stronger the model is."
        }
    ],
    "affiliations": [
        "LightSpeed Studios, Singapore",
        "Singapore University of Technology and Design, Singapore"
    ]
}