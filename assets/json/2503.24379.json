{
    "paper_title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation",
    "authors": [
        "Shengqiong Wu",
        "Weicai Ye",
        "Jiahao Wang",
        "Quande Liu",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai",
        "Shuicheng Yan",
        "Hao Fei",
        "Tat-Seng Chua"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 9 7 3 4 2 . 3 0 5 2 : r Any2Caption : Interpreting Any Condition to Caption for Controllable Video Generation Shengqiong Wu1,2* Weicai Ye1,(cid:66) Pengfei Wan1 Di Zhang1 Kun Gai1 1Kuaishou Technology Jiahao Wang1 Quande Liu1 Xintao Wang Shuicheng Yan2 Hao Fei2,(cid:66) 2National University of Singapore Tat-Seng Chua2 https://sqwu.top/Any2Cap/ Figure 1. We propose Any2Caption, an efficient and versatile framework for interpreting diverse conditions to structured captions, which then can be fed into any video generator to generate highly controllable videos."
        },
        {
            "title": "Abstract",
            "content": "To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputstext, images, videos, and specialized cues such as region, motion, and camera posesinto dense, structured captions that offer backbone video generators Work done during internship at Kuaishou Technology. (cid:66) Corresponding Author. with better guidance. We also introduce Any2CapIns, large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. 1. Introduction Video serves as fundamental medium for capturing realworld dynamics, making diverse and controllable video generation key capability for modern artificial intelligence (AI) systems. Recently, video generation has gained significant attention, driven by advancements in Diffusion Transformers (DiT) [2, 29, 44, 52, 76], which have demonstrated the ability to generate realistic, long-duration videos from text prompts. These advancements have even led to industrial applications, such as filmmaking. However, we observe that major bottleneck in the current video generation community lies in accurately interpreting user intention, so as to produce high-quality, controllable videos. In text-to-video (T2V) generation, studies [24, 30, 67] have suggested that detailed prompts, specifying objects, actions, attributes, poses, camera movements, and style, significantly enhance both controllability and video quality. Thus, series of works have explored video recaption techniques (e.g., ShareGPT4Video [10], MiraData [30], and InstanceCap [15]) to build dense structured captions for optimizing generative models. While dense captions are used during training, in real-world inference scenarios, users most likely provide concise or straightforward input prompts [15]. Such gap inevitably weakens instruction following and leads to suboptimal generation due to an incomplete understanding of user intent. To combat this, there are two possible solutions, manual prompt refinement or automatic prompt enrichment [15, 67] using large language models (LLMs). Yet, these approaches either require substantial human effort or risk introducing noise from incorrect prompt interpretations. As result, this limitation in precisely interpreting user intent hinders the adoption of controllable video generation for demanding applications such as anime creation and filmmaking. In addition, to achieve more fine-grained controllable video generation, one effective strategy is to provide additional visual conditions besides text promptssuch as reference images [17, 62], identity [22, 46, 69], style [42, 68], human pose [33, 45], or camera [21, 75]or even combinations of multiple conditions together [41, 58, 74]. This multimodal conditioning approach aligns well with real-world scenarios, as users quite prefer interactive ways to articulate their creative intent. Several studies have examined video generation under various conditions, such as VideoComposer [58], Ctrl-Adapter [41], and ControlVideo [74]. Unfortunately, these methods tend to rely on the internal encoders of Diffusion/DiT to parse rich heterogeneous input conditions with intricate requirements (e.g., multiple object IDs, and complex camera movements). Before generation, the model must accurately interpret the semantics of varied visual conditions in tandem with textual prompts. Yet even state-of-the-art (SoTA) DiT backbones have limited capacity for reasoning across different input modalities, resulting in suboptimal generation quality. This work is dedicated to addressing these bottlenecks of any-conditioned video generation. Our core idea is to decouple the first job of interpreting various conditions from the second job of video generation, motivated by two important observations: a) SoTA video generation models (e.g., DiT) already excel at producing high-quality videos when presented with sufficiently rich text captions; a) Current MLLMs have demonstrated robust visionlanguage comprehension. Based on these, we propose Any2Caption, an MLLMbased universal condition interpreter designed not only to handle text, image, and video inputs but also equipped with specialized modules for motion and camera pose inputs. As illustrated in Fig. 1, Any2Caption takes as inputs any/diverse condition (or combination), and produces densely structured caption, which is then passed on to any backbone video generators for controllable, high-quality video production. As Any2Caption disentangles the role of complex interpretation of multimodal inputs from the backbone generator, it advances in seamlessly integrating into wide range of well-trained video generators without the extra cost of fine-tuning. To facilitate the any-to-caption instruction tuning for Any2Caption, we construct Any2CapIns, large-scale dataset that converts concise user prompt and diverse non-text conditions into detailed, structured captions. Concretely, the dataset encompasses four main categories of conditions: depth maps, multiple identities, human poses, and camera poses. Through extensive manual labeling combined with automated annotation by GPT-4V [1], followed by rigorous human verification, we curate total of 337K high-quality instances, with 407K condition annotations, with the short prompts and structured captions averaging 55 and 231 words, respectively. In addition, we devise comprehensive evaluation strategy to thoroughly measure the models capacity for interpreting user intent under these various conditions. Experimentally, we first validate Any2Caption on our Any2CapIns, where results demonstrate that it achieves an impressive captioning quality that can faithfully reflect the original input conditions. We then experiment with integrating Any2Caption with multiple SoTA video generators, finding that (a) the long-form semantically rich prompts produced by Any2Caption are pivotal for generating high-quality videos under arbitrary conditions, and (b) Any2Caption consistently enhances performance across different backbone models, yielding noticeably improved outputs. Furthermore, Any2Caption shows pronounced advantage when handling multiple combined conditions, effectively interpreting and synthesizing intricate user constraints into captions that align closely with user expectations. Our contributions are threefold: We for the first time pioneer novel any-condition-tocaption paradigm of video generation, which bridges the gap between user-provided multimodal conditions and structured video generation instructions, leading to 2 Figure 2. The pipeline for constructing the Any2CapIns dataset involves three key steps: 1) data collection, 2) structured video caption generation, and 3) user-centric short prompt generation. highly-controllable video generation. We propose Any2Caption to effectively integrate and comprehend diverse multimodal conditions, producing semantically enriched, long-form, structured captions, which consistently improve both condition flexibility and video quality. Any2Caption can also be widely integrated as plug-in module for any existing video generation framework. We introduce Any2CapIns, large-scale, highquality benchmark dataset for the any-condition-tocaption task, and also establish suite of evaluation metrics to rigorously assess the quality and fidelity of condition-based caption generation. 2. Related Work Controllable video generation [11, 16, 21, 53] has long been central topic in AI. Recent advanced DiT methods, such as OpenAIs Sora [3] and HunyuanVideo [34], yield photorealistic videos over extended durations. Early work focused on text-controlled video generation [24, 52], the prevalent approach. Yet, text prompts alone may insufficiently capture user intent, spurring exploration of additional inputs including static images [17, 62], sketches [58, 74], human poses [33, 45, 77], camera views [21, 75], and even extra videos [14, 32, 70]. Thus, unifying these diverse conditions into an any-condition framework is highly valuable."
        },
        {
            "title": "Recent works such as VideoComposer",
            "content": "[58], CtrlAdapter [41], and ControlVideo [74] have explored anycondition video generation. However, they face challenges in controlling multiple modalities due to the limited interpretability of text encoders in Diffusion or DiT. Motivated by existing MLLMs multimodal reasoning [39, 43, 56], we propose leveraging an MLLM to consolidate all possible conditions into structured dense captions for better controllable generation. SoTA DiT models already exhibit the capacity to interpret dense textual descriptions as long as the input captions are sufficiently detailed in depicting both the scene and the intended generation goals. Thus, our MLLMbased encoder alleviates the comprehension bottleneck, enabling higher-quality video generation. To our knowledge, this is the first attempt in the field of any-condition video generation. Moreover, as the captioning stage is decoupled from backbone DiT, Any2Caption can integrate with existing video generation solutions without additional retraining. Our approach also relates to video recaptioning, as our system produces dense captions from given conditions. In text-to-video settings, prior work [15, 27, 47, 67] shows that recaptioning yields detailed annotations that improve DiT training. For instance, ShareGPT4Video [10] uses GPT-4V [1] to reinterpret video content, while MiraData [30] and InstanceCap [15] focus on structured and instance-consistent recaptioning. Unlike these methods, we avoid retraining powerful DiT models with dense captions by training an MLLM as an any-condition encoder on pairs of short, dense captions that are easier to obtain. Moreover, recaptioning entire videos can introduce noise or hallucinations that undermine DiT training, whereas our framework sidesteps this risk. Finally, while previous studies rely on dense-captiontrained DiT models, the real-world user concise prompts might create mismatch that degrades generation quality. 3. Any2CapIns Dataset Construction While relevant studies recaption target videos for dense captions for enhanced T2V generation [10, 15, 30], these datasets exhibit two key limitations: (1) the absence of non-text conditions, and (2) short prompts that do not account for interactions among non-text conditions, potentially leading to discrepancies in real-world applications. To address these limitations, we introduce new dataset, Any2CapIns, specifically designed to incorporate diverse multimodal conditions for generating structured video captions. The dataset is constructed through three-step process (cf. Fig. 2), including data collection, structured caption construction, and user-centric short prompt generation. Step-1: Data Collection. We begin by systematically categorizing conditions into four primary types: 1) Spatialwise conditions, which focus on the structural and spatial properties of the video, e.g., depth maps, sketches, and video frames. 2) Action-wise conditions, which emphasize motion and human dynamics in the target video, e.g., human pose, motion. 3) Composition-wise conditions, which focus on scene composition, particularly in terms of ob3 Category #Inst. #Condition #Avg. Len. #Total Len. 182,945 Depth 44,644 Human Pose Multi-Identities 68,255 Camera Movement 41,112 182,945 44,644 138,089 41, 9.87s 8.38s 13.01s 6.89s 501.44h 108.22h 246.69h 78.86h Table 1. Statistics of the collected dataset across four types of conditions. #Inst. means the number of instances, and #Condition denotes the number of unique conditions. #Avg. / #Total Len. indicate the average and total video durations, respectively. Guided by these observations, we employ GPT-4V to infer potential user prompts under condition-specific constraints. Given structured video caption, target video, and associated conditions, we apply tailored constraints to preserve condition invariance when relevant. We also explicitly control the length of the generated prompts to ensure conciseness. Finally, we conduct manual verification and filtering to further refine the dataset. Fig. 3 presents the length distribution of the resulting short and structured prompts. 4. Any2Caption Model In this section, we introduce Any2Caption, novel MLLM explicitly designed to comprehensively model and interpret arbitrary multimodal conditions for controllable video caption generation, as illustrated in Fig. 4(a). Formally, the user provides short text prompt along with non-text conditions = [c1, , cn], where the non-text conditions can be either single condition (n = 1) or combination of multiple conditions. The objective of this task is to generate detailed and structured caption that serves as control signal for video generation. Architecture. Similar to existing MLLMs [39, 43, 63], Any2Caption incorporates an image encoder FI, video encoder FV , motion encoder FM and camera encoder FC} to process non-text conditions. These encoders are then integrated into an LLM backbone FLLM (i.e., Qwen2LLM) to facilitate structured video captioning. Specifically, we leverage ViT-based visual encoder from Qwen2VL as FI and FV for the unified modeling of images and videos, achieving effective interpretation of input conditions represented in image or video formats, such as depth maps and multiple identities. To enable human pose understanding, we represent the extracted human pose trajectories as H={(xk n)k = 1, , K, = 1, , }, where denotes the number of video frames and is the number of keypoints. These trajectories are then visualized within video frames to enable further processing by the motion encoder, which shares the same architectural structure and initialization as the vision encoder. For camera motion understanding, inspired by [21], we introduce camera encoder that processes plucker embedding sequence RN 6HW , where H, are the height and width of the video. This embedding accurately captures camera pose information, enabling pren, yk Figure 3. Distribution of the short/structured caption length (in words) in Any2CapIns. ject interactions and multiple identities in the target video. 4) Camera-wise conditions, which control video generation from cinematographic perspective, e.g., camera angles, movement trajectories. Since it is infeasible to encompass all possible conditions in dataset collection, we curate representative datasets under each category, specifically including depth maps, human pose, multiple identities, and camera motion. During the data collection process, we leverage SoTA tools to construct conditions. For instance, Depth Anything [65] is used to generate depth maps, DWPose [66] provides human pose annotations, and SAM2 [50] is utilized for segmentation construction. In total, we collect 337K video instances and 407K conditions, with detailed statistics of the dataset presented in Tab. 1. Step-2: Structured Video Caption Generation. The granularity of caption, specifically the detailed selection of elements they encompass, plays critical role in guiding the model to produce videos that closely align with the users intentions while preserving coherence and realism. Drawing inspiration from [30], we inherit its structured caption format consisting of (1) Dense caption, (2) Main object caption, (3) Background caption, (4) Camera caption, and (5) Style caption. Furthermore, the action descriptions of the subjects significantly influence the motion smoothness of the videos [57], we explicitly incorporate the (6) Action caption of subjects to form the final structured caption. Technically, following [57], we independently generate each caption component and subsequently integrate them to construct the final structured caption. Step-3: User-centric Short Prompt Generation. In this step, we construct short prompts from user-centric perspective, considering how users naturally express their intentions. Firstly, our analysis highlights three key characteristics of user-generated prompts: 1) Conciseness and Simplicity, where users favor brief and straightforward wording; 2) Condition-Dependent Omission, whereby users often omit explicit descriptions of certain attributes (e.g., camera movement) when such conditions are already specified; and 3) Implicit instruction of Target Video: where users convey their intent indirectly (e.g., specifying multiple identities without explicitly detailing their interactions). VL weights, we fine-tune the model on the Any2CapIns dataset for multimodal condition interpretation. However, direct fine-tuning leads to catastrophic forgetting due to the fixed output structure. To address this, we propose progressive mixed training strategy. Specifically, the model is first trained on single condition to establish strong condition-specific understanding. As new conditions are introduced, we gradually incorporate vision-language instructions such as LLaVA-instruction [43] and Alpaca-52K [54]. This stepwise strategy ensures robust multimodal condition interpretation while preventing knowledge degradation. 5. Evaluation Suite In this section, we introduce the evaluation suite for comprehensively assessing the capability of Any2Caption in interpreting user intent and generating structured captions. Lexical Matching Score. To assess caption generation quality from lexical standpoint, we employ standard evaluation metrics commonly used in image/video captioning tasks, including BLEU [48], ROUGE [40], and METEOR [4]. We also introduce Structural Integrity score to verify whether the generated captions adhere to the required six-component format, thereby ensuring completeness. Semantic Matching Score. To evaluate the semantic alignment of generated captions, we employ BERTSCORE [71], which computes similarity by summing the cosine similarities between token embeddings, effectively capturing both lexical and compositional meaning preservation. Additionally, we utilize CLIP Score [23] to assess the semantic consistency between the input visual condition and the generated videos. Intent Reasoning Score. Evaluating structured captions typically focuses on their linguistic quality, yet it rarely assesses whether the model truly understands user intent and accurately reflects it across aspects such as style, emotion, and cinematic language. To bridge this gap and draw inspiration from [8], we introduce the Intent Reasoning Score (IRSCORE), novel quantitative metric that leverages LLMs to assess whether generated captions accurately capture user intentions. IRSCORE identifies user-specified control factors, then decomposes the generated caption into QA pairs aligned with these factors. The framework has four steps: (1) User Intention Extraction: Analyze provided conditions to categorize user intent into six aspects: subject, background, movement, camera, interaction, and (2) Ground-Truth QA Pair Construction: Forstyle. mulate aspect-specific QA pairs with defined requirements (e.g., for subject-related attributes, emphasize object count, appearance). (3) Predicted Answer Generation: Prompt GPT-4V to parse the predicted caption and generate answers based solely on it. (4) Answer Evaluation: Following [36], GPT-4V outputs two scores (correctness and quality) for Figure 4. Architecture illustration of Any2Caption (a), where Qwen2-LLM serves as the backbone and is paired with text, image, video, motion, and camera encoders to produce structured captions. After alignment learning, we perform progressive mixed training strategy (b), where additional vision/text instruction datasets are progressively added for joint training, and meanwhile, for input short caption, we adopt random-dropout mechanism at the sentence level to enhance robustness. cise modeling of camera trajectories. Finally, in line with Qwen2-VL, we employ special tokens to distinguish nontext conditions from texts. Besides the existing tokens, we introduce <motion start>, <motion end>, <camera start>, <camera end>, to demarcate the start and end of human and camera pose features. Training Recipes. To accurately interpret user generation intent under arbitrary conditions and yield structured target video captions, large-scale pretraining and instruction tuning are required. To this end, we adopt two-stage training procedure: Stage-I: Alignment Learning. In this stage, as image and video encoders have been well-trained in Qwen2-VL, we only focus on aligning human pose features from the motion encoder and camera movement features with the word embeddings of the LLM backbone. To achieve this, we freeze the LLM and vision encoder while keeping the motion encoder trainable and optimizing it on human pose description task. Similarly, for camera movement alignment, we unfreeze the camera encoder and train it on camera movement description task, ensuring that camera-related conditions are embedded into the models latent space. This alignment phase establishes strong foundation for effective representation learning for these conditions. Stage II: Condition-Interpreting Learning Building upon the aligned encoders and pretrained Qwen2-"
        },
        {
            "title": "Entire Structured Caption\nDense Caption\nMain Object Caption\nBackground Caption\nCamera Caption\nStyle Caption\nAction Caption",
            "content": "91.25 - - - - - - B-2 54.99 44.24 38.54 44.65 60.21 41.71 31.91 R-L"
        },
        {
            "title": "Quality",
            "content": "48.63 42.89 47.46 46.73 96.10 47.70 39.83 52.47 49.51 52.48 48.87 94.32 55.9 45.25 91.95 92.42 92.02 92.90 99.31 93.48 91.44 68.15 78.47 56.28 69.37 66.31 63.75 57.98 3.43 3.47 2.74 2.69 3.75 3.05 2.13 Table 2. Quantitative results of structured caption generation quality under four aspects: structural Integrity, lexical matching, semantic matching, and intent reasoning. We demonstrate the overall caption generation capability and the individual component generation performance within the structure. B-2 and R-L denotes BLEU-2 and ROUGE-L, respectively. Caption Enrich Short Cap. Short Cap. w/ Condition Cap. Structured Cap. Text Video Generation CLIP-T Smoothness Aesthetic Integrity 18.31 19.19 19.87 93.46 93. 94.38 5.32 5.41 5.46 55.39 54. 57.47 Table 3. Quantitative results comparing short caption, short caption combined with condition caption, and structured caption for multi-identity video generation. Training Strategy Caption Vieo Generation B-2 Accuracy Smoothness Dynamics Aesthetic Any2Caption 47.69 w/o Two-Stage 33.70 49.24 w/o Dropout 67.35 51.79 69.51 94.60 93.31 94.16 17.67 16.36 14.54 5.53 5.50 5. Table 4. Ablation study on training strategy. w/o stage means alignment learning is not applied during training, while w/o Dropout denotes that short captions are not randomly dropped. each answer, which are then averaged across all QA pairs. More details are in Appendix E. Video Generation Quality Score. The primary objective of generating structured captions is to enhance the video generation quality and controllability. Therefore, we adopt series of metrics to assess the quality of videos generated based on structured captions. Following [26, 30], we evaluate video generation quality across four key dimensions: motion smoothness, dynamic degree, aesthetic quality, and image integrity. To further verify adherence to specific non-text conditions, we use specialized metrics: RotErr, TransErr, and CamMC [21] for camera motion accuracy; Mean Absolute Error (MAE) for depth consistency [18]; DINO-I [51], CLIP-I [51] Score to evaluate identity preservation under multiple identities, and Pose Accuracy (Pose Acc.) [45] to access the alignment in the generated videos. 6. Experiments 6.1. Setups Dataset. We manually construct 200 test cases for each type of condition (i.e., depth, human pose, multiple identities, camera, and compositional conditions) to evaluate the models performance. Additionally, we assess the model on publicly available benchmarks (e.g., [41, 68, 78]). For further details, please refer to the Appendix F. Implementation Details. We leverage Qwen2VL-7B [56] as the backbone of our model, which supports both image and video understanding. The human pose in the input conditions is encoded and processed in the video format. The camera encoder adopts the vision encoder architecture with the following settings: the input channel of 96, the patch size of 16, the depth of 8, and 8 attention heads. During training, to simulate the brevity and randomness of user inputs, we randomly drop sentences from the short caption with dropout rate of 0.6; similar dropout strategy is applied to non-textual conditions. We conducted the training on 8A800 GPUs. For further details on the training parameters for each stage, please refer to the Appendix F. 6.2. Experimental Results and Analyses In this section, we present the experimental results and provide in-depth analyses to reveal how the system advances. Following this, we try to ground the answers for the following six key research questions. RQ-1: How well is the structured caption generation quality? We first evaluate whether our proposed model could accurately interpret user intent and generate highquality structured captions. From caption-generation perspective, we compare the predicted captions with goldstandard captions across various metrics (refer to Tab. 2). We observe that our model successfully produces the desired structured content, achieving 91.25% in structural integrity. Moreover, it effectively captures the key elements of the gold captions, attaining ROUGE-L score of 48.63 and BERTSCORE of 91.95. Notably, the model demonstrates the strongest performance in interpreting camera-related details compared to other aspects. Finally, regarding user intent analysis, we found that the model reliably incorporated user preferences into its structured outputs. 6 Figure 5. Illustrations of generated videos where only the structured captions yielded by Any2Caption are fed into the CogVideoX-2B (Left), and HunyuanVideo (Right). We can observe that some key features of the input identity images, such as the background and main object, can be accurately visualized in the generated videos. Model Text Camera Identities Depth Human Pose Overall Quality CLIP-T RotErr TransErr CamMC DINO-I CLIP-I MAE Pose Acc. Smoothness Dynamic Aesthetic Integrity MotionCtrl [60] + Structured Cap. CameraCtrl [21] + Structured Cap. Ctrl-Adapter [41] + Structured Cap. ControlVideo [74] + Structured Cap. ConceptMaster [25] + Structured Cap. 19.67 20.16 18.89 21.70 20.37 23.30 22.17 24. 16.04 17.15 FollowYourPose [45] + Structured Cap. 21.11 21.39 Camera to Video 1.54 1.45 1.37 0.94 4.49 4.37 3.51 2. 4.80 4.78 4.78 4.37 - - - - - - - - - - - - Depth to Video - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 25.63 21.87 30.11 23.92 Identities to Video 65.31 66. 36.37 39.42 Human Pose to Video - - - - - - - - - - - - - - - - - - 30.47 31.59 96.13 96.16 94.11 95.16 94.53 95.54 92.88 94. 94.71 95.05 91.71 92.87 9.75 11.43 12.59 13.72 20.73 15.14 5.94 18.27 8.18 10.14 14.29 16. 5.40 5.71 4.26 4.66 4.63 5.31 5.29 9.77 5.21 5.68 4.95 5.88 73.69 74.63 71.84 72.47 46.98 54.20 63.85 66. 43.68 49.73 58.84 56.30 Table 5. Performance comparison on four types of conditions (e.g., camera, depth, identities, and human pose) between directly using short captions and integrating structured captions under various video quality evaluation metrics. Better results are marked in bold. To further showcase the models capacity to understand and leverage input conditions, we directly feed the structured captionsderived from our models interpretationinto downstream text-to-video generation systems (e.g., CogvideoX-2B [67] and Hunyuan [34]), as illustrated in Fig. 5. Even without explicit visual conditions (e.g., identities, depth, or camera movement), the resulting videos clearly align with the input prompts, such as the hats color and style, or the womans clothing texture in Example 1, indicating that our structured captions successfully capture intricate visual details. In particular, the model is able to accurately grasp dense conditions, such as depth sequences or compositional requirements in Example 4, ultimately enabling controllable video generation. While certain finegrained elements may not be exhaustively described in text, resulting in occasional discrepancies with the actual visual content, the overall controllability remains robust. RQ-2: Is the video generation quality enhanced with structured caption? Here, we investigate whether integrating structured captions consistently improves controllable video generation in multiple methods. We explore the impact of adding camera, depth, identities, and human pose conditions to various controllable video generation methods. As shown in Tab. 5, all tested models exhibit consistent gains in overall video quality, such as smoothness and frame fidelity, after incorporating structured captions, without requiring any changes to the model architectures or additional training. Moreover, these models show enhanced adherence to the specified conditions, suggesting that our generated captions precisely capture user requirements and lead to more accurate, visually coherent video outputs. More examples can be found in the Appendix G.2. RQ-3: Is the structured caption necessary? We examine whether structured caption design is essential. We 7 Compositional Condition Text Camera Identities Depth Overall Quality CLIP-T RotErr TransErr CamMC DINO-I CLIP-I MAE Smoothness Dynamic Aesthetic Integrity Camera+Identities + Structured Cap. Camera+Depth + Structured Cap. Depth+Identities + Structured Cap. Camera+Identities+Depth + Structured Cap. 14.81 19.03 20.80 21.19 20.01 20.76 18.49 19.52 1.37 1.30 1.57 1.49 - - 2.05 1. 4.04 4.36 3.88 4.41 - - 7.74 7.74 4.24 4.03 4.77 4.84 - - 8.47 8.20 25.63 26.75 - - 35.24 36.25 35.86 38.74 64.14 68.45 - - 57.82 63.48 64.25 64.37 - - 32.15 25.37 23.00 24.78 18.37 17.41 94.43 94.38 95.36 95.40 93.15 92.50 92.02 93. 28.87 34.99 30.12 30.10 32.21 36.43 30.09 32.81 4.99 5.25 4.82 4.96 4.96 5.18 3.91 4.99 59.81 63.02 63.90 65.05 61.21 60.81 60.62 61.22 Table 6. Quantitative comparison of structured captions when handling compositional conditions. Better results are marked in bold. to instruction tuning, both captioning and video generation performance degrade significantly. possible explanation is that bypassing alignment learning disrupts the encoders adaptation process, which has been aligned to the LLM backbone, leading to suboptimal results in subsequent stages. Additionally, we compare the performance of the model without the dropout mechanism. Although removing dropout yields marked improvement in captioning quality, the benefit to video generation is marginal. This suggests that without dropout, the model may rely on shortcuts from the input captions rather than fully understanding the underlying intent, thereby increasing the risk of overfitting. RQ-5: How well does the model perform on compositional conditions? We examine the impact of structured captions under compositional conditions. As shown in Tab. 6, we compare the combined camera, identities, and depth on our customized model and observe that structured captions consistently enhance its performance. Moreover, from the Example 2 and 4 in Fig. 5, our model demonstrates thorough understanding of the interactions among various conditions, for instance, capturing womans hair color and the position of mug, accurately guiding the production of videos that align with the specified requirements. This finding further highlights that our approach can automatically equip existing T2V models with the ability to handle compositional conditions without requiring additional training. RQ-6: How well is the generalization capability of Any2Caption? Finally, we investigate the models generalization ability by evaluating its performance on unseen conditions, including style, segmentation, sketch, and masked images. As demonstrated in Fig. 6, the structured captions generated by our model consistently enhance existing T2V frameworks, offering benefits such as increased motion smoothness, aesthetic quality, and more accurate generation control. We attribute these advantages to two primary factors: the strong reasoning capabilities of our MLLM backbone and our training strategy, i.e., progressive mixed training, which leverages existing vision and text instructions for fine-tuning while mitigating knowledge forgetting, thereby ensuring robust generalization. Figure 6. Quantitative results on unseen conditions (i.e., segmentation [58], style [68], masked image [58], and sketch [58]) when using short and structured captions, respectively. compare our structured caption approach with simpler method, where we first caption the input condition (e.g., multiple identity images) and then concatenate that caption with the original short prompt, as shown in Tab. 3. Our results indicate that merely appending the conditions caption to the short prompt can reduce video smoothness and image quality. One likely reason is that the identity images may contain extraneous details beyond the target subject, potentially conflicting with the original prompt and causing inconsistencies. Consequently, controllability in the final output is compromised. In contrast, our structured caption method accurately identifies the target subject and augments the prompt with relevant information, yielding more controllable video generation (cf. G.2). RQ-4: How effective is the training strategy? Next, we investigate the contribution of the training mechanism, and the results are shown in Tab. 4. During training, we employ two-stage training approach, consisting of alignment learning followed by instruction-tuning. When alignment learning is omitted, and the model proceeds directly 8 7. Conclusion In this work, we focus on addressing the challenge of more accurately interpreting user generation intention from any condition for controllable video generation. We introduce Any2Caption, framework that decouples multimodal condition interpretation from video synthesis. Built based on an MLLM, Any2Caption converts diverse inputs into dense captions that drive high-quality video generation. We further present Any2CapIns, large-scale dataset for effective instruction tuning. Experiments show that our method improves controllability and video quality across various backbones."
        },
        {
            "title": "References",
            "content": "[1] Gpt-4v(ision) system card, 2023. 2, 3, 14 [2] Kuaishou. kling, 2024. 2 [3] Sora, 2024. 3, 13 [4] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In ACL, pages 6572, 2005. 5 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023. 13 [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, pages 2256322575, 2023. 13 [7] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei A. Efros, and Tero Karras. Generating long videos of dynamic scenes. In NeurIPS, 2022. 13 [8] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, and Christopher D. Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. CoRR, abs/2410.03051, 2024. 5 [9] Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and Lei Zhang. Motionllm: Understanding human behaviors from human motions and videos. CoRR, abs/2405.20340, 2024. 14 [10] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Lin Bin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. In NeurIPS, 2024. 2, 3, 14 [11] Yiran Chen, Anyi Rao, Xuekun Jiang, Shishi Xiao, Ruiqing Ma, Zeyu Wang, Hui Xiong, and Bo Dai. Cinepregen: Camera controllable video previsualization via engine-powered diffusion. CoRR, abs/2408.17424, 2024. 3, [12] Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taixe, and Nils Thuerey. Learning temporal coherence via selfsupervision for gan-based video generation. ACM Trans. Graph., 39(4):75, 2020. 13 [13] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report. CoRR, abs/2407.10759, 2024. 14 [14] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-style video editing. In ECCV, pages 183199, 2024. 3, 13 [15] Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, and Ying Tai. Instancecap: Improving text-to-video generation via instanceaware structured caption. CoRR, abs/2412.09283, 2024. 2, 3, 13, 14 [16] Haopeng Fang, Di Qiu, Binjie Mao, Pengfei Yan, and He Tang. Motioncharacter: Identity-preserving and motion controllable human video generation. CoRR, abs/2411.18281, 2024. 3, [17] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, and Chongyang Ma. I2vadapter: general image-to-video adapter for diffusion models. In ACM SIGGRAPH, page 112, 2024. 2, 3, 13 [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In ECCV, pages 330348, 2024. 6 [19] Shir Gur, Sagie Benaim, and Lior Wolf. Hierarchical patch VAE-GAN: generating diverse videos from single sample. In NeurIPS, 2020. 13 [20] Niv Haim, Ben Feinstein, Niv Granot, Assaf Shocher, Shai Bagon, Tali Dekel, and Michal Irani. Diverse generation In ECCV, pages 491 from single video made possible. 509, 2022. 13 [21] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. CoRR, abs/2404.02101, 2024. 2, 3, 4, 6, 7, 13 [22] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, and Jie Zhang. Idanimator: Zero-shot identity-preserving human video generation. CoRR, abs/2404.15275, 2024. 2 [23] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metIn EMNLP, pages 75147528, ric for image captioning. 2021. [24] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In ICLR, 2023. 2, 3, 13 [25] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. CoRR, abs/2501.04698, 2025. 7 [26] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin 9 Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, pages 2180721818, 2024. 6 [27] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. Video reIn CVPR, cap: Recursive captioning of hour-long videos. pages 1819818208, 2024. 3, 13 [28] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. In NeurIPS, 2023. [29] Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu, Kaicheng Yu, Haiyang Sun, Kun Zhan, Peng Jia, and Miao Zhang. Dive: Dit-based video generation with enhanced control. CoRR, abs/2409.01595, 2024. 2 [30] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. In NeurIPS, 2024. 2, 3, 4, 6, 14 [31] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. arXiv preprint arXiv:2503.19907, 2025. 18 [32] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. RAVE: randomized noise shuffling for fast and consistent video editing with diffusion models. In CVPR, pages 65076516, 2024. 3, 13 [33] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion In ICCV, image-to-video synthesis via stable diffusion. pages 2262322633, 2023. 2, 3, 13 [34] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. CoRR, abs/2412.03603, 2024. 3, 7, 13 [35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICM, pages 1973019742, 2023. [36] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. CoRR, abs/2305.06355, 2023. 5 [37] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV, pages 323340, 2024. 14 Jia. Mini-gemini: Mining the potential of multi-modality vision language models. CoRR, abs/2403.18814, 2024. 14 [39] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In EMNLP, pages 59715984, 2024. 3, 4, 13, 14 [40] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, 2004. 5 [41] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrl-adapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. CoRR, abs/2404.09967, 2024. 2, 3, 6, 7, [42] Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang, Xintao Wang, Ying Shan, and Yujiu Yang. Stylecrafter: Taming artistic video diffusion with reference-augmented adapter learning. ACM Trans. Graph., 43(6):251:1251:10, 2024. 2 [43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2628626296, 2024. 3, 4, 5, 13, 14 [44] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. CoRR, abs/2401.03048, 2024. 2 [45] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: PoseIn guided text-to-video generation using pose-free videos. AAAI, pages 41174125, 2024. 2, 3, 6, 7, 13 [46] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. CoRR, abs/2402.09368, 2024. 2 [47] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. CoRR, abs/2407.02371, 2024. 3, 13 [48] Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. Bleu: method for automatic evaluation of machine translation. pages 311318, 2002. [49] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-tovideo generation. In CVPR, pages 66356645, 2024. 13 [50] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross B. Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. CoRR, abs/2408.00714, 2024. 4 [51] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. 6 [38] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya [52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2023. 2, 3, 13 [53] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. CoRR, abs/2411.04928, 2024. 3, 13 [54] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Stanford alpaca: An instruction-following Hashimoto. llama model. https://github.com/tatsulab/ stanford_alpaca, 2023. 5 [55] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Iyer, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, Xichen Pan, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 14 [56] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024. 3, 6, 13 [57] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, Fei Yang, Pengfei Wan, and Di Zhang. Koala36m: large-scale video dataset improving consistency between fine-grained conditions and video content. CoRR, abs/2410.08260, 2024. [58] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. In NeurIPS, 2023. 2, 3, 8, 13 [59] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. Imaginator: Conditional spatio-temporal GAN for video generation. In WACV, pages 11491158, 2020. 13 [60] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH, page 114, 2024. 7 [61] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Zhe Chen, Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, Ping Luo, Yu Qiao, and Jifeng Dai. Visionllm v2: An end-to-end generalist multimodal large language In NeurIPS, model for hundreds of vision-language tasks. 2024. 14 [62] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, pages 75897599, 2023. 2, 3, 13 [63] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng In ICML. Chua. Next-gpt: Any-to-any multimodal LLM. OpenReview.net, 2024. 4, 14 [64] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024. 14 [65] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything V2. In NeurIPS, 2024. 4 [66] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In ICCV, pages 42124222, 2023. 4 [67] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2024. 2, 3, 7, 13 [68] Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your CoRR, video with artistic generation and translation. abs/2412.07744, 2024. 2, 6, 8 [69] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. CoRR, abs/2411.17440, 2024. [70] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. CoRR, abs/2311.04145, 2023. 3, 13 [71] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with BERT. In ICLR, 2020. 5 [72] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In NeurIPS, 2024. 14 [73] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. PIA: your personalized image animator via plug-and-play modules in text-to-image models. In CVPR, pages 77477756, 2024. 13 [74] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun Zhu. Controlvideo: Adding conditional control for one shot text-to-video editing. CoRR, abs/2305.17098, 2023. 2, 3, 7, 13 [75] Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Camera-controlled image-to-video diffusion model. CoRR, abs/2410.15957, 2024. 2, 3, 13 [76] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang 11 You. Open-sora: Democratizing efficient video production for all. CoRR, abs/2412.20404, 2024. 2 [77] Yong Zhong, Min Zhao, Zebin You, Xiaofeng Yu, Changwang Zhang, and Chongxuan Li. Posecrafter: One-shot personalized video synthesis following flexible pose control. In ECCV, pages 243260, 2024. 3, 13 [78] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. ACM Trans. Graph., 37(4): 65, 2018."
        },
        {
            "title": "Appendix Overview",
            "content": "The appendix presents more details and additional results not included in the main paper due to page limitations. The list of items included is: Limitation in Section A. Ethic Statement results in Section B. Extended Related Work in Section C. Extended Dataset Construction Details in Section D. More Statistics Information of IRSCORE in Section E. Detailed Setups in Section F. Extended Experiment Results and Analyses in Section G. A. Limitation Despite the advancement of our proposed framework, several limitations may remain: Firstly, the diversity of annotated data is constrained by the capabilities of the current annotation tools, which may limit the variety of generated content. Moreover, the scarcity of real-world data introduces potential domain gaps, reducing the models generalizability in practical scenarios. Secondly, due to inherent model limitations, hallucinations may occur, resulting in inaccurate structured captions and consequently degrading the quality of generated videos. possible direction to mitigate this issue is to develop an end-to-end approach that jointly interprets complex conditions and handles video generation. Lastly, the additional condition-understanding modules inevitably increase inference time. However, our empirical results suggest that the performance gains from these modules are substantial, and future work may explore more efficient architectures or optimization techniques to balance speed and accuracy. B. Ethic Statement This work relies on publicly available datasets and manually constructed datasets, ensuring that all data collection and usage adhere to established privacy standards. We recognize that automatic annotation processes may introduce biases, and we have taken measures to evaluate and mitigate these biases. Nonetheless, we remain committed to ongoing improvements in this area. By video enhancing generation capabilities, Any2Caption could inadvertently facilitate negative societal impacts, such as the production of deepfakes and misinformation, breaches of privacy, or the creation of harmful content. We, therefore, emphasize the importance robust privacy safeguards, of strict ethical guidelines, and careful dataset curation to minimize these risks and promote responsible research practices. C. Extended Related Work C.1. Text-to-Video Generation The development of video generation models has progressed from early GANand VAE-based approaches [7, 12, 19, 20, 59] to the increasingly popular diffusionbased methods [5, 6, 49, 73]. Among these, diffusionin-transformer (DiT) architectures, such as OpenAIs Sora [3] and HunyuanVideo [34], have demonstrated remarkable performance, producing photorealistic videos over extended durations. Controllable video generation [11, 16, 21, 53] has become an essential aspect of this field. Initially, research efforts centered predominantly on text-to-video generation [24, 52], which remains the most common approach. However, relying solely on text prompts can be insufficient for accurately capturing user intent, prompting exploration into other conditioning inputs such as static images [17, 62], user sketches [58, 74], human poses [33, 45, 77], camera perspectives [21, 75], and even additional videos [14, 32, 70]. Given this diversity of potential conditions, unifying them into single any-condition video generation framework is highly valuable. C.2. Controllable Video Generation Recent methods like VideoComposer [58], Ctrl-Adapter and ControlVideo [74] have investigated any- [41], condition video generation. Nevertheless, they still struggle with comprehensive controllability due to the complexity of multiple modalities and the limited capacity of standard diffusion or DiT encoders to interpret them. Inspired by the strong multimodal reasoning capabilities of modern MLLMs [39, 43, 56], we propose leveraging an MLLM to consolidate all possible conditions into unified reasoning process, producing structured dense captions as inputs to backbone Video DiT. SoTA DiT models already exhibit the capacity to interpret dense textual descriptions, as long as the input captions are sufficiently detailed in depicting both the scene and the intended generation goals. Building on this, our MLLM-based condition encoder directly addresses the comprehension bottleneck, theoretically enabling higher-quality video generation. To our knowledge, this work is the first to develop an MLLM specifically tailored for any-condition video generation. Because the caption-generation mechanism is decoupled from DiT, our proposed Any2Caption can be integrated into existing DiT-based methods without additional retraining. C.3. Video Captioning Our approach is closely related to video recaptioning research, as our MLLM must produce dense captions based In text-to-video settings, prior on the given conditions. work [15, 27, 47, 67] has demonstrated the benefits of recaptioning videos to obtain more detailed textual annota13 tions, thereby improving the training of longer and higherquality video generation via DiT. ShareGPT4Video [10], for example, employs GPT-4V [1] to reinterpret video content and produce richer captions. MiraData [30] introduces structured dense recaptioning, while InstanceCap [15] focuses on instance-consistent dense recaptioning. Although we also pursue structured dense captions to enhance generation quality, our method diverges fundamentally from these previous approaches. First, because DiT models are already sufficiently powerful, we directly adopt an off-theshelf Video DiT without incurring the substantial cost of training it with dense captions. Instead, we train an MLLM as an any-condition encoder at comparatively lower cost; in the text-to-video scenario, for instance, we only need to train on pairs of short and dense captions, which are far easier and more abundant to obtain. Second, prior methods that recapturing the entire video risk introducing noise or even hallucinated content due to the current limitations of MLLMs in video understanding, potentially undermining DiT training quality, whereas our framework avoids this issue. Most importantly, while these approaches may rely on dense-caption-trained DiT models, real-world inference often involves very concise user prompts, creating mismatch that can diminish final generation quality. C.4. Multimodal Large Language Models Recent advances in Large Language Models (LLMs) [64] have catalyzed surge of interest in extending their capabilities to multimodal domains [3739, 61, 72]. number of works integrate vision encoder (e.g., CLIP [43], DINOv2[55], OpenCLIP[38]) with an LLM, often through lightweight connector module (e.g., MLP [43], Qformer [35]), enabling the model to process image and video inputs with minimal additional training data and parameters [39, 43, 63]. These approaches have demonstrated promising performance on tasks such as image captioning, visual question answering, and video understanding. Beyond purely visual data, some researchers have investigated broader modalities, such as 3D motion [9, 28] or audio [13, 63], thereby expanding the application range of multimodal LLMs. Despite these advances, most existing MLLMs focus on limited set of visual modalities and do not readily accommodate more specialized inputs like human pose or camera motion. This gap restricts their ability to handle more diverse and complex conditions in the controllable video generation field. In contrast, our work targets broader spectrum of modalities, aiming to build single model capable of interpreting and unifying image, video, human pose, and camera conditions. Specifically, we augment an existing MLLM with dedicated encoders for motion and camera features, thereby equipping the model to process arbitrary multimodal conditions and facilitate controllable video generation. D. Extended Dataset Details Visualization of Short and Structured Caption. Here, we visualize the short and structured caption in Fig. 7, 8. Notably, the structured caption captures the video content In contrast, in greater detail and across multiple aspects. the short caption deliberately omits any information already supplied by non-text conditionfor example, camera movement is excluded in Fig. 7s short caption but included in Fig. 8 because it is not specified by the provided multiple-identity images. Moreover, we visualize the word distribution of the structured captions in Any2CapIns in Fig. 9. Prompt Visualization for Short Caption Construction. In Tab. 7 and 8, we show the system prompts used by GPT4V to generate short captions. The prompt explicitly instructs GPT-4V to consider the given conditions comprehensively and produce short prompts that focus on information not covered by the non-textual inputs. For instance, when multiple identities are specified, the short prompt should avoid repeating their appearance attributes and instead highlight interactions among the identities. Conversely, when depth is the input condition, the short prompt should include more detailed appearance-related information. E. More Statistics Information of IRSCORE We generate total of 15,378 question-answer (QA) pairs, averaging 19.2 pairs per structured caption. Fig. 10 presents the distribution of constructed questions across different aspects in the structured caption, and Tab. 9 shows representative QA pairs for each aspect. Notably, questions under the main object category emphasize fine-grained details such as clothing color or hairstyles, while action questions focus on object interactions and movements. This level of specificity allows us to rigorously assess whether the generated captions are both complete and precise. F. Detailed Setups F.1. Detailed Testing Dataset Here, we present the statistics of the test in Tab. 10, which covers four types of single conditions (e.g., Depth, Camera, Identities, and Human pose), and four types of compositional conditions(e.g., Camera+Identities, Camera+Depth, Identities+Depth and Camera+Identities+Depth). Each category contains 200 instances. F.2. Detailed Training Procedure We employ two-stage training process to enhance the alignment and interpretability of multimodal conditions in Any2Caption. 14 Figure 7. Illustrations of constructed short and structured captions under the camera-to-video generation. Figure 8. Illustrations of constructed short and structured captions under the multiIDs-to-video generation. Figure 9. Word cloud of different structured captions in Any2CapIns dataset, showing the diversity. Multi-IDs Here is the scenario: We have an MLLM model that supports text image-conditioned intrinsic-video-caption generation task. The system input consists of: 1. reference image composed of 2-3 horizontally stitched images (provided by the user), each stitched image containing one or several target objects for reference); and 2. concise textual prompt (referred to as text B, the users instruction). The models output is detailed descriptive caption (**text A**) that thoroughly describes the video corresponding to the users input prompt (**text B**) in great detail. Your task is to perform reverse engineering. Based on the given reference image (the target objects) and the detailed target video caption (text A), you need to generate **reasonable and concise user prompt (text B)** through your understanding, analysis, and imagination. You must adhere to the following rules: 1. Text is dense caption of video, including all the key objects, their attributes, relationships, background, camera movements, style, and more. Carefully analyze this caption for all relevant details. 2. Analyze the provided reference images in detail to identify the differences or missing details compared to the target video description. These may include environment details, the interaction between objects, the progression of actions, camera movements, style, or any elements not covered by the reference image. Based on these analyses, generate the users instructions. 3. The users prompt must include the following aspects: first, an overall description of where the target objects are and what they are doing, along with the temporal progression of their actions. Then, it should describe the background, style, and camera movements. 4. If the target video introduces new objects not present in the reference images, the users prompt should describe the attributes of the new target objects and their interactions with the other target objects. 5. If the videos style differs from the reference, briefly describe the style in few words. 6. When the background needs to be described, include details about people, settings, and styles present in the background. 7. Avoid repeating information that can be inferred from the reference images, and eliminate redundant descriptions in the user prompt. 8. The user prompt (text B) must be written in simple wording, maintaining concise style with short sentences. 9. The users instructions should vary in expression; For example, prompts do not always need to start with the main subject. They can begin with environmental details, camera movements, or other contextual aspects. Here are three examples representing the desired pattern: ==================================================================================================== [In-context Examples] ==================================================================================================== [Input] Table 7. Demonstration of the prompt used for GPT-4V to generate the short prompt when the input condition is the multi-IDs. Stage-1: Alignment learning. This stage focuses on aligning features extracted by the camera encoder with the LLM feature space. To achieve this, we first extract camera movement descriptions (e.g., fixed, backward, pan to the right) from the camera captions in Any2CapIns to construct camera movement description dataset. We then introduce two specialized tokens, <camera start> and <camera end>, at the beginning and end of the camera feature embeddings. During training, only the camera encoder is optimized, while all other parameters in Any2Caption remain frozen. Similarly, for motion alignment, we construct motion description dataset by extracting action descriptions (e.g., walking, dancing, holding) from the action captions in Any2CapIns. We then freeze all model parameters except those in the motion encoder to ensure the LLM effectively understands motion-related conditions. Stage-2: Condition-Interpreting Learning. After alignment learning, we initialize Any2Caption with the pretrained Qwen2-VL, motion encoder, and camera encoder Depth Here is the scenario: We have an MLLM model that supports text & image-conditioned intrinsic-video-caption generation task. The system input consists of: 1. reference image composed of 3-5 horizontally stitched depth maps in temporal sequence (provided by the user, each map containing depth information for reference); and 2. concise textual prompt (referred to as text B, the users instruction). The models output is detailed descriptive caption (text A) that thoroughly describes the video corresponding to the users input prompt (text B) in great detail. Now, need you to perform reverse engineering task. Based on the given reference image (the depths) and the detailed target video caption (text A), you must generate reasonable and concise user prompt (text B) through your understanding, analysis, and imagination. To ensure accurate and effective outputs, follow these rules strictly: 1. Text is dense caption of video, including all the key objects, their attributes, relationships, background, camera movements, style, and more. Carefully analyze this caption to extract the necessary details. 2. Since the depth information already provides the necessary geometric outlines and layout details. Do not repeat this information in the user prompt. Instead, focus on the aspects not covered by the depth maps. 3. The users instruction should highlight details not included in the depth map, such as environmental details, the appearance of the subjects, interactions between subjects, the progression of actions, relationships between the subjects and the environment, camera movements, and overall style. 4. For dense depth maps (more than 5 maps), assume the maps provide the camera movements and actions between objects, focusing on describing the appearance of the subjects and environment, the atmosphere, and subtle interactions between subjects and their environment. 5. For sparse depth maps (5 maps or fewer), assume the maps only provide scene outlines. Emphasize details about the subjects appearance, environment, interactions between subjects, relationships between subjects and the environment, and camera movements. 6. The user prompt (text B) must be written in simple wording, maintaining concise style with short sentences, with total word count not exceeding 100. 7. Your output should be continuous series of sentences, not list or bullet points. 8. The users instructions should vary in expression; they dont always need to begin with description of the main subject. They could also start with environmental details or camera movements. Here are three examples representing the desired pattern: ==================================================================================================== [In-context Examples] ==================================================================================================== [Input] Table 8. Demonstration of the prompt used for GPT-4V to generate the short prompt when the input condition is the depth. GPUs. G. Extended Experiment Results and Analyses G.1. The Capability for Understanding Complex Instruction. We further examine Any2Captions ability to handle complex user instructions, particularly regarding whether it accurately captures the users intended generation targets. From Fig. 11, we observe that the model focuses precisely on the user-specified main objects, such as woman warrior or background filled with chaos and destructionwhen producing structured captions. In contrast, short caption combined with condition captions often includes extraneous objects or background details present in the identity images, which distract from the users intended targets in the final video generation. Additionally, we assess the models performance on instructions containing implicit objects or actions, as shown in Fig. 12 and 13. In these examples, the model correctly interprets phrases like the most right person as young Black woman with long, curly brown hair, wearing black and white outfit and similarly associates implicitly specified objects with the provided conditions, generating structured captions that align with the users goals. Figure 10. QA pairs proportion in structured captions. weights. We then employ progressive mixed training strategy, updating only the lm head while keeping the multimodal encoders frozen. The training follows sequential order based on condition complexity: identities human pose camera depth. Correspondingly, the integration ratio of additional vision/text instruction datasets is progressively increased, set at 0.0, 0.4, 0.6, and 0.8, ensuring balanced learning process between condition-specific specialization and generalization. F.3. Detailed Implementations In Tab. 11, we list the detailed hyperparameter settings in two stages. All the training is conducted on 8A800 (80G) Lastly, Fig. 16 compares videos generated using different captions. The results indicate that structured captions 17 Aspect QA Pairs Main Object What is the young woman adjusting as she walks down the corridor? Her wide-brimmed hat. Light blue. What color is the young womans T-shirt? How does the young woman feel as she walks down the corridor? Happy and carefree. What is the young woman wearing? What is the young womans hair length? What is the position of the young woman in the frame? What is the main object in the video? large shark. What is the color of the underwater scene? Blue. What are the two scientists wearing? White lab coats and gloves. What is the first scientist using? microscope. In the center of the frame. Long. Light blue t-shirt with pink lettering, blue jeans, and wide-brimmed hat. Background Where is the young woman walking? Down corridor. Camera Style Action Smaller fish. In brightly lit environment with shelves filled with bottles. What time of day does the scene appear to be set? Daytime. What can be seen in the background of the corridor? Beige walls and large windows. What is the weather like in the video? Clear. Where is the shark located? On the ocean floor. What surrounds the shark in the video? Where is the laboratory setting? What detail does the background highlight? The scientific setting with static emphasis. How does the camera follow the young woman? Moving backward What is the cameras height relative to the person? Roughly the same height as the person. What shot type does the camera maintain? Medium close-up shot of the upper body. How does the camera position itself to capture the subject? At higher angle, shooting downward. How does the camera capture the environment? How is the camera positioned? At approximately the same eye level as the subjects, maintaining close-up shot. How does the camera move in the video? What is the style of the video? Casual and candid. What kind of design does the corridor have? Modern and clean design. What style does the video portray? Naturalistic style with clear, vivid visuals. What does the video style emphasize? Clinical, high-tech, and scientific precision. What is the color theme of the lighting? Bright and cool. What kind of atmosphere does the laboratory have? Professional and scientific. From medium distance. It pans to the right. What does the young woman do with both hands occasionally? Adjusts her hat. What is the young woman doing as she moves? Walking forward with her hands on her hat. What is the main action of the shark in the video? What is the movement of the fish like? Calm and occasionally darting. What is the movement of the first scientist at the beginning? Examines microscope. What task is the second scientist engaged in? Handling pipette and beaker filled with green liquid. How does the second scientist transfer the liquid? Carefully using pipette into the beaker. Are there any noticeable movements in the background? Occasional small particles floating. Lying motionless. Table 9. Demonstration of generated question-answer pairs utilized in IRSCORE calculation. Type #Inst. #Condi. Short Cap. #Structured Cap. #Avg. Len. #Avg. Len. 200 Identities 200 Camera 200 Depth 200 Human Pose 200 Camera+Identities 200 Camera+Depth Identities+Depth 200 Camera+Identities+Depth 200 350 200 200 200 622 400 555 756 65.28 50.25 54.22 58.38 53.41 51.43 53.14 58.35 284.97 208.01 225.09 259.03 209.17 208.81 286.83 289.21 Table 10. Statistics of the constructed test datasets. #Inst. denotes the number of instances, and #Condi. indicates the number of unique conditions. Short Cap. #Avg. Len represents the average caption length of short captions, and Structured Cap. #Avg. Len. represents the average caption length of structured captions. G.2. More Video Visualization Here, we compare the results of different customized models [31] after integrating structured captions. Fig. 14 shows camera-controlled video generation results, and Fig. 15 illustrates depth-controlled outcomes. We observe that structured captions improve image quality and motion smoothness by providing richer scene details. For multi-identity-controlled video generation, as depicted in Fig. 16, 17, and 18, structured captions lead to more expressive and realistic representations of the referenced identities, with more accurate colors and smoother actions. significantly improve both the smoothness of motion and the overall consistency of the generated videos. 18 Configuration Optimizer Precision Learning Rate Weight Decay Joint Train Ratio Betas Dropout Possibility Dropout (Short Cap.) Batch Size Per GPU Training Data Camera AdamW bfloat16 5e5 0.01 0.0 (0.9, 0.99) 0.0 0.0 4 Stage-1: Alignment Learning Stage-2: Condition-Interpreting Learning Motion Identities Human pose Camera AdamW bfloat16 5e5 0.01 0.0 (0.9, 0.99) 0.0 0.0 4 AdamW bfloat16 5e5 0.01 0.0 (0.9, 0.99) 0.4 0.6 4 MultiIDs AdamW bfloat16 5e5 0.01 0.4 (0.9, 0.99) 0.6 0.6 4 AdamW bfloat16 5e5 0.01 0.6 (0.9, 0.99) 0.6 0.6 4 Human Pose LLaVA-150K Camera LLaVA-150K Depth AdamW bfloat16 1e5 0.01 0. 0.6 0.6 4 Depth LLaVA-150K Alpaca-50K Camera Movement Description Dataset (Manual) Action Description Dataset (Manual) Table 11. Training recipes for Any2Caption. Figure 11. Illustrations of input short caption, predicted structured caption, and short caption combined with condition captions. The condition captions may introduce noisy information due to the focus distraction of the background of input identities, for example, the Batman, Superman, film crew. Figure 12. Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects for the video generation. Figure 13. Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation. 20 Figure 14. Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation. 21 Figure 15. Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation. 22 Figure 16. Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation. 23 Figure 17. Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation. 24 Figure 18. Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "National University of Singapore"
    ]
}