{
    "paper_title": "GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors",
    "authors": [
        "Xingyilang Yin",
        "Qi Zhang",
        "Jiahao Chang",
        "Ying Feng",
        "Qingnan Fan",
        "Xi Yang",
        "Chi-Man Pun",
        "Huaqi Zhang",
        "Xiaodong Cun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, a novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon a DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: https://github.com/GVCLab/GSFixer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 7 6 6 9 0 . 8 0 5 2 : r GSFIXER: IMPROVING 3D GAUSSIAN SPLATTING WITH REFERENCE-GUIDED VIDEO DIFFUSION PRIORS Xingyilang Yin1,2 Qi Zhang2 Qingnan Fan2 Xi Yang4 Chi-Man Pun1 Huaqi Zhang2 Xiaodong Cun5 1University of Macau 2VIVO 3CUHKSZ 4Xidian University 5GVC Lab, Great Bay University Jiahao Chang3 Ying Feng2 Figure 1: We introduce GSFixer, framework capable of improving 3DGS in both artifact restoration (top) and 3D reconstruction (bottom) under sparse-view settings. Recent generative methods struggle with maintaining consistency between generated and input views. GSFixer guides the video diffusion model conditioned on both 3D and 2D signals to enhance consistency in novel view restoration, thereby improving 3D reconstruction quality."
        },
        {
            "title": "ABSTRACT",
            "content": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: https://github.com/GVCLab/GSFixer. Work done during an internship at VIVO. Equal contribution. Corresponding authors."
        },
        {
            "title": "INTRODUCTION",
            "content": "3D reconstruction and novel view synthesis (NVS) are fundamental tasks in computer vision and graphics, with wide-ranging real-world applications in virtual reality, autonomous driving, and robotics. Recently, 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) has achieved impressive results in both reconstruction quality and rendering efficiency when dense input views are available. However, its performance degrades significantly in sparse-view settings, where limited viewpoint information leads to under-constrained 3D representations. In such cases, 3DGS often suffers from severe artifacts, including distorted geometric structures and incomplete reconstructions, particularly in less-observed regions or extreme novel viewpoints. These limitations hinder its applicability in real-world scenarios where acquiring dense multi-view data is challenging. To alleviate the limitations, some previous regularization methods have been proposed to introduce additional constraints into the 3DGS optimization process, such as monocular depth (Li et al., 2024; Zhu et al., 2024), frequency smoothness (Zhang et al., 2024), and random dropout (Xu et al., 2025). While these approaches can help prevent 3DGS representations from overfitting to sparse input views, they often remain sensitive to noise and yield only marginal improvements in NVS rendering quality. Inspired by the success of ReconFusion (Wu et al., 2024), which introduces diffusion model into NeRF (Mildenhall et al., 2020) optimization, more recent studies (Liu et al., 2024b;a; Wu et al., 2025a;b) explore incorporating 3DGS optimization with powerful generative priors from diffusion models, which are trained on internet-scale data. These strong priors enable the correction of spurious geometry or the inpainting of plausible content in novel views. However, key challenge still remains: maintaining visual and 3D consistency between the generated and original input images, especially when the novel views are far from the observed inputs. Meanwhile, recent advances in controllable video generation have demonstrated the effectiveness of incorporating various conditional signals (Niu et al., 2024; Hu, 2024; He et al., 2025; Cui et al., 2025; YU et al., 2025) to guide video diffusion models in generating high-quality and coherent content. This progress motivates us to investigate appropriate control conditions to guide the video diffusion model in restoring artifact-ridden novel views, aiming for enhanced visual quality and 3D consistency. Given that the input to the sparse-view reconstruction task consists of only few imagestreated as ground-truth viewsit is natural to align the restored novel views with these inputs. Thus, we exploit the information from reference views within the input set to condition the video diffusion model. Since artifacts manifest in 2D image space, their correction should rely on 2D visual priors to ensure semantic consistency with the reference views. At the same time, these artifacts originate from inaccurate 3DGS representations in 3D space, suggesting that effective restoration should also consider 3D structure. Therefore, an effective controllable video diffusion model for this task needs to integrate both 2D and 3D priors from reference views. Inspired by the above motivation, we propose GSFixer, novel generative reconstruction framework built upon DiT-based video diffusion model (Yang et al., 2024), trained on paired artifact 3DGS renders and clean frames, conditioned on additional reference-based conditions. Our key insight is to leverage information of reference views to guide the video diffusion model to restore artifact-prone novel views in geometrically consistent and visually faithful manner. This enables high-quality novel view restoration and 3D reconstruction, as shown in Figure 1. Specifically, we extract tokens via visual encoders (e.g., DINOv2 (Oquab et al., 2024)) as 2D semantic signal to ensure semantic consistency between the fixed novel views and the input observations. To further enforce multi-view consistency, we incorporate features from feed-forward 3D reconstruction networks (e.g., VGGT (Wang et al., 2025)) as 3D geometric condition. Moreover, we introduce reference trajectory sampling strategy into the iterative generative optimization process to fix artifacts in novel views, effectively balancing angular coverage and restoration quality. In addition, to facilitate the evaluation of artifact removal capabilities of generative methods, we present DL3DVRes, benchmark comprising artifact-ridden frames rendered from low-quality 3DGS representations. Empirical results demonstrate that our GSFixer achieves superior sparse-view restoration and reconstruction performance on various challenging scenes. Our main contributions are summarized as follows: We introduce GSFixer, novel generative reconstruction framework tailored for improving the quality of 3DGS representations. It integrates reference-guided video restoration model along with reference-guided trajectory sampling strategy. 2 We propose to incorporate video diffusion model conditioned on both 2D semantic and 3D geometric features extracted from the reference views. This effectively guides the restoration of artifact novel views, achieving both semantically coherent and 3D consistent results. We present DL3DV-Res benchmark for evaluating the performance of existing generative models in 3DGS artifact restoration. Extensive experiments demonstrate that our GSFixer outperforms existing baseslines in video-based 3D artifact restoration and sparse-view 3D reconstruction."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Regularization Sparse-view Novel View Synthesis. Neural Radiance Fields (NeRF) and 3DGS have revolutionized the field of NVS by leveraging advances in neural rendering. While NeRF and 3DGS achieve remarkable photorealistic novel-view synthesis results given dense input views, they often suffer from severe overfitting issues and significant artifacts when training with sparse views. Previous works have attempted to address this limitation by introducing additional constraints and regularization into the per-scene 3D optimize process, such as depth supervision (Deng et al., 2022; Roessle et al., 2022; Wang et al., 2023; Li et al., 2024; Zhu et al., 2024), normal consistency (Yu et al., 2022; Seo et al., 2023), smoothness priors (Niemeyer et al., 2022; Yang et al., 2023; Zhang et al., 2024), semantic coherence (Jain et al., 2021; Truong et al., 2023), and random dropout strategy (Xu et al., 2025). However, these approaches often yield marginal improvements and remain sensitive to some specific scene data. Conditional Video Diffusion Models. Recently, generative models (Rombach et al., 2022; Liu et al., 2023; Blattmann et al., 2023; Chen et al., 2023; 2024; Xing et al., 2024; Yang et al., 2024) have advanced rapidly and demonstrate impressive capabilities in high-quality visual content generation. Building on this progress, recent efforts have extended video diffusion models with various control signals, such as depth (Hu et al., 2025; Zhao et al., 2024), trajectory (Niu et al., 2024; YU et al., 2025), audio (Cui et al., 2025; Kong et al., 2025), camera parameters (Wang et al., 2024; Zhang et al., 2025), and point cloud renderings (Yu et al., 2024; Gu et al., 2025). The additional control signals enable the diffusion models to generate coherent content with the users inputs. Our work builds upon video diffusion framework that leverages 3DGS renderings and features from clean reference frames as condition signals to guide the restoration of artifacts in novel views. Generative Sparse-view Novel View Synthesis. Generative models have demonstrated strong ability to inpaint plausible content in unobserved regions and restore degraded areas with high visual fidelity. Recent approaches (Wu et al., 2024; Liu et al., 2024b;a; Wu et al., 2025a; Bao et al., 2025; Wu et al., 2025b) have made notable progress in the sparse-view NVS task by leveraging priors from generative models. For instance, ReconFusion (Wu et al., 2024) combines image diffusion with PixelNeRF (Yu et al., 2021) as condition to optimize NeRF representations. Similiarly, more recent works (Liu et al., 2024b; Wu et al., 2025a; Bao et al., 2025; Wu et al., 2025b) utilize diffusion priors to enhance low-qualtiy 3DGS representations. Our work is closely related to 3DGSEnhancer (Liu et al., 2024b), GenFusion (Wu et al., 2025b) and DIFIX3D+ (Wu et al., 2025a), all of which finetune diffusion models to correct artifacts in novel views. While following similar direction, our method differs in two key aspects: (i) we finetune DiT-based video diffusion model controlled with additional reference view conditions, and (ii) we incorporate both 2D semantic features and 3D geometric features extracted from visual geometry foundation models to guide the video restoration process, effectively fixing the artifacts in novel views."
        },
        {
            "title": "3 METHODS",
            "content": "3.1 PRELIMINARY 3D Gaussian Splatting and Novel View Synthesis. 3DGS (Kerbl et al., 2023) explicitly represents 3D scene as collection of 3D Gaussian spheres, enabling high-quality 3d reconstructions and efficient novel view synthesis. Each 3D Gaussian sphere is defined by its center location µ, scaling vector s, rotation quaternion q, opacity σ, and spherical harmonic (SH) coefficients sh. Thus, the Gaussian distribution is formulated as: G(x) = 1 2 (xµ)T Σ1(xµ), (1) Figure 2: Pipeline of GSFixer. Given sparse-view images and their corresponding low-quality 3DGS representation, we render artifact-prone novel views between two reference views along reference-guided trajectory. These novel views are fed into reference-guided video restoration model to correct artifacts, and the fixed novel views are then distilled back into the 3DGS representation to improve its quality. The restoration network is finetuned from CogVideoX and trained on paired artifact-ridden 3DGS renders and ground truth frames. It is additionally conditioned on 3D geometric tokens and 2D semantic tokens extracted from the reference views using pretrained VGGT and DINOv2 encoder, respectively. where Σ = RSST RT , denotes the scaling matrix corresponding to and is the rotation matrix determined by q. For rendering novel views, volume rendering integrates these elements using: = (cid:88) ciαi i1 (cid:89) (1 αj), iM j=1 (2) where represents the final pixel color, which is computed via alpha blending of the viewdependent colors ci of the contributing Gaussians, weighted by their opacities αi.. Video Diffusion Model. Video diffusion models (Blattmann et al., 2023; Chen et al., 2023; 2024; Xing et al., 2024) typically consist of two key stages: forward diffusion process, which progressively inject noise ϵ into clean video data x0 Rn3hw, yielding noisy samples xt = αtx0 + σtϵ at each time step t; and reverse denoising process pθ, where noise predictor ϵθ learns to recover the original data by removing the noise. This predictor was traditionally implemented using U-Net architecture and is trained to minimize the following denoising objective: min θ EtU (0,1),ϵN (0,I)[ϵθ(xt, t) ϵ2 2]. (3) Following Sora (Brooks et al., 2024), recent approaches (Yang et al., 2024; Wan et al., 2025) adopt the Diffusion Transformer (DiT) (Peebles & Xie, 2023) architecture for the noise predictor. During training, pretrained 3D VAE encoder compresses videos into latent space = E(x). These latent tokens are then patchified, concatenated with text tokens, and fed into the DiT. At inference time, the model progressively denoises the latent tokens into clean tokens, which are subsequently decoded by the 3D VAE decoder to generate the final video ˆx = D(z). 4 3.2 OVERVIEW OF GSFIXER The illustration of our GSFixer framework is depicted in Figure 2. Given sparse-view RGB images {Ii}i=1,...,K, camera poses {Pi}i=1,...,K, and corresponding trained 3DGS representation of the scene, our goal is to fix the artifacts present in novel views rendered by the low-quality 3DGS representation. We fix the artifact of novel views using our reference-guided video restoration model and distill the fixed images back into the 3DGS representation to improve its quality. In our reference-guided video restoration model, we employ visual encoder Ev and spatial encoder Es to respectively extract 2D semantic tokens T2D and 3D geometry tokens T3D from the reference views {I ref }r=1,...,K (Sec. 3.3). These tokens are subsequently projected, fused and injected into video diffusion process to guide the restoration of artifact-ridden novel views, preserving both semantic fidelity and 3D consistency with the input views (Sec. 3.3). Moreover, we introduce reference-guided trajectory sampling strategy within the iterative generative optimization to further enhance the quality. (Sec. 3.4). 3.3 REFERENCE-GUIDED VIDEO RESTORATION MODEL Motivation. Recent works (Liu et al., 2024b; Wu et al., 2025b) have introduced video diffusion priors to restore artifact-prone novel views into clean frames. However, the generated frames often lack visual and 3D consistency with the input sparse views, leading to suboptimal 3D reconstruction performance. To address this, considering the artifacts finally lie in the 2D image space and are caused by suboptimal 3DGS representations in 3D space, we propose injecting both 2D semantic and 3D geometric control signals of reference views to guide the video diffusion process, enabling semantic and 3D consistency in restorating the artifact novel views. As illustrated in Figure 2, our reference-guided video restoration model is built upon DiT-based video diffusion model CogVideoX (Yang et al., 2024), an image-to-video diffusion model capable of animating an input image to generate video. We utilize its 3D Variational Autoencoder (VAE) for video compression and decompression. Additionally, we use BLIP (Li et al., 2022) and T5 (Raffel et al., 2020) encoder to caption video and extract the text tokens t1D . In our video-to-video restoration task, which aims to restore artifact frames into clean ones, we replace the original image condition with the 3DGS renders {I nov }n=1,...,N between two reference views {Ii, Ij}. Both the artifact frames and reference views are encoded using the 3D VAE encoder E, concatenated with per-frame initial noise, and projected into view tokens tview. These view tokens are then combined with text tokens ttext and fed into the DiT blocks for denoising. Reference-based Conditions. Given set of sparse input images, we select two images {Ii, Ij} as reference views, set of novel views {I nov }n=1,...,N along arbitrary trajectories between them, we aim to learn conditional p(xI nov, {Ii, Ij}) to guide the video diffusion model. To incorporate 2D semantic guidance into the video diffusion process, we employ pretrained DINOv2 (Oquab et al., 2024) model as 2D visual tokenizer. For each reference view Ii, DINOv2 encoder E2D divides it into patches and extracts robust 2D visual features as sequence of tokens: = E2D(Ii), t2D t2D RLC, (4) where t2D tokens and the feature dimension, respectively. denotes the resulting 2D semantic tokens, with and representing the number of To further control our video diffusion with 3D geometric priors, we adopt the pretrained Visual Geometry Grounded Transformer (VGGT) (Wang et al., 2025) as 3D geometric tokenizer. VGGT employs 3D geometric encoder E3D, which composes several frame-wise and global self-attention layers, to encode multi-view reference views {Ii, Ij}: t3D , t3D = E3D(Ii, Ij), t3D RLC, t3D RLC, (5) , t3D where t3D represents the extracted 3D geometric tokens, with and representing the number of tokens and the feature dimension, respectively. VGGT leverages these tokens to produce all key 3D attributes of scene, including camera parameters, depth maps, point maps, and 3D point tracks through various prediction heads. Consequently, the 3D geometric tokens include rich and robust 3D geometric priors of the scene. 5 Figure 3: Illustration of different trajectories. (a) Interpolation trajectory: blue curve. (b) Ellipse trajectory: green curve. (c) Reference-guided trajectory: orange and green curve. Reference-guided Generation. As previously mentioned, the fixed frames may still lack visual and 3D consistency with the reference images. To address this, we fuse 3D geometric tokens and 2D semantic tokens of reference views to fusion tokens as additional condition: tf usion = rojector3D(t3D ) + rojector2D(t2D ), (6) where rojector3D and rojector2D are implemented using linear and normalization layers. To inject the reference fusion tokens to control the video diffusion process, we augment each DiT block by adding cross-attention layer after the 3D attention layer. In this cross-attention mechanism, the view tokens serve as queries, while the fusion tokens act as keys and values: tview = CrossAttention(tview, tf usion). (7) This allows the rich 2D and 3D information from the reference fusion tokens to be effectively injected into the view tokens, enabling direct alignment with the reference views and enhancing both semantic and geometric consistency when restoring artifact-prone novel views to clean ones. 3.4 REFERENCE-GUIDED GENERATIVE RECONSTRUCTION Given rendered artifact-prone novel views and reference views, our trained reference-guided restoration diffusion model can produce consistent, artifact-free frames aligned with the input sparse views. Leveraging this capability, we supervise 3DGS optimization using both the input sparse views and the fixed novel views in an iterative training manner Haque et al. (2023); Wu et al. (2025b;a). Specifically, we begin by constructing an initial low-quality 3DGS representation from the sparse views. We then render novel views along new camera trajectories and feed these artifact-prone frames into our reference-guided video restoration model to obtain artifact-free outputs, as illustrated in Figure 2. These fixed novel views are subsequently added to the training set to further supervise the 3DGS optimization in iterative way. Reference-guided Trajectory. Trajectory sampling is critical in this iterative optimization process. Common sampling strategies, such as interpolation trajectory between input poses (Figure 3 (a)) or ellipse trajectory along spherical path across all the camera poses (Figure 3 (b)) , have limitations. For our GSFixer, The interpolation trajectory yields high-quality fixed novel views but lacks angular diversity, while the ellipse trajectory provides broader view coverage but results in suboptimal fixed views. To balance these trade-offs, we propose reference-guided trajectory sampling strategy, as shown in Figure 3 (c). Specifically, (i) we first interpolate from reference view to its nearest viewpoint on the spherical path, (ii) sample additional views along the sphere path, (iii) and then interpolate to the next nearest reference view. This hybrid sampling strategy achieves high-quality fixed views and angle coverage, leading to better 3DGS representation. During optimization, we freeze the video diffusion model and supervise the 3DGS representation using loss function composed of two components: the reconstruction loss Lrecon between rendered images and the input sparse views, and the generative loss Lgen between the rendered novel views and the corresponding fixed novel views. Specifically, we adopt simple photometric losses: Lrecon = λl1 Ll1 + λSSIM LSSIM . (8) 6 Artifact (Kerbl et al., 2023) Difix3D+ (Wu et al., 2025a) GenFusion (Wu et al., 2025b) GSFixer (Ours) PSNR 14.12 14.14 14.56 16.72 SSIM 0.405 0.419 0.453 0.520 LPIPS 0.509 0.455 0.486 0.399 I2V SC 0.9382 0.9288 0.8916 0. I2V BC 0.9524 0.9481 0.9258 0.9644 OC 0.2066 0.2393 0.2372 0.2407 TF 0.9058 0.9038 0.9135 0.9233 MS 0.9548 0.9473 0.9596 0.9665 Table 1: Quantitative comparison on DL3DV-Res Dataset. We compare the video 3D artifact restoration results with baselines. The best results are highlighted in bold. Figure 4: Qualitative comparison on DL3DV-Res Benchmark. We compare 3DGS artifact restoration quality of the existing generative methods. The generative loss Lgen consists of the same components as Lrecon but is applied to the rendered and fixed novel views: = Lrecon + λ Lgen, (9) where we employ an annealing strategy Wu et al. (2025b) to gradually increase the weight of the generative loss λ during training."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Training Dataset and Evaluation Benchmarks. We train our GSFixer on random selection of 1,000 scenes from the DL3DV-10K dataset (Ling et al., 2024). To construct paired artifact-laden and clean 3DGS renders, we employ sparse-view 3D reconstruction strategy. Specifically, we use few of input views (e.g., 3, 6, or 9) to reconstruct low-quality 3DGS representations. We then select camera trajectories from the dataset that include both the input views (used as reference views) and novel viewpoints (serving as ground truth frames). By rendering the low-quality 3DGS along these trajectories, we can generate the corresponding artifact-containing frames. Additionally, we extract the 1D text captions, 2D semantic tokens and 3D geometric tokens of the reference views by the pretrained BLIP, DINOv2 and VGGT encoders, respectively. We evaluate our method and other baselines on 137 scenes from our proposed DL3DV-Res benchmark for the 3DGS artifact restoration task, and 28 scenes from the DL3DV-Benchmark (Ling et al., 2024) and 9 scenes from the Mip-NeRF 360 dataset (Barron et al., 2022) for the sparse-view 3D reconstruction task. For evaluation, we report PSNR, SSIM, and LPIPS metrics and Overall Consis7 3DGS (Kerbl et al., 2023) Difix3D+ (Wu et al., 2025a) GenFusion (Wu et al., 2025b) GSFixer (Ours) PSNR SSIM 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view 0.293 0.547 13.72 0.259 0.589 15.07 0.314 0.610 14.64 0.307 0.625 16.21 0.372 0.329 0.374 0.360 19.05 20.12 20.32 20.60 0.625 0.656 0.688 0.675 17.11 18.26 18.36 19.11 0.521 0.473 0.493 0. 0.410 0.481 0.498 0.536 LPIPS Table 2: Quantitative comparison on DL3DV-Benchmark Dataset. We compare the rendering quality with baselines given 3, 6 and 9 views. Figure 5: Qualitative comparison on DL3DV-Benchmark. We compare the novel view with baselines rendering quality using 3, 6, and 9 input views. tency (OC), Temporal Flickering (TF), Motion Smoothness (MS) scores of VBench (Huang et al., 2024b) and I2V Subject Consistency (I2V SC), I2V Background Consistency (I2V BC) scores of VBench++ (Huang et al., 2024c) protocol. The DL3DV-Res benchmark is constructed from all available scenes in the DL3DV-Benchmark, using similar sparse-view reconstruction protocol. We train 3DGS models with extremely sparse input views (e.g., 3) and render the reconstructions along the original camera trajectories of each scene. This produces novel views with severe artifacts, paired with high-quality ground truth images. For fair comparison, all the methods initialize with the COLMAP (Schonberger & Frahm, 2016) point cloud in all the experiments and we filter and retain only the visible points from the input sparse training views. Implementation Details. We implement and initialize the parameters of our reference-guided video diffusion model based on the pretrained CogVideoX-5B-I2V (Yang et al., 2024). During training, the frame resolution is fixed at 480720, and the video length is set to 49 frames. The training stage is conducted for 10,000 iterations with learning rate of 2105, incorporating warm-up strategy, and optimized using the AdamW optimizer. The proposed model is trained on 8 NVIDIA H20 GPUs with batch size of 8 for about 4 days. 4.2 COMPARISON WITH OTHER METHODS 3DGS Artifact Restoration. To evaluate the effectiveness of existing models in 3DGS artifact restoration, we compare our GSFixer with recent generative methods, including Difix3D+ (Wu et al., 2025a) and GenFusion (Wu et al., 2025b) on our proposed DL3DV-Res benchmark. As shown in Table 1, the quantitative results demonstrate that GSFixer significantly outperforms both Difix3D+ and GenFusion in correcting artifacts in novel views across all pixel-wise metrics. Specifically, GSFixer achieves improvements of 2.16 in PSNR, 0.067 in SSIM, and 0.087 in LIPIS compared to GenFusion. Furthermore, our method consistently outperforms the baselines across all VBench and VBench++ metrics, showcasing state-of-the-art performance in 3DGS artifact restoration. It is 8 Zip-NeRF (Barron et al., 2023) FreeNeRF (Yang et al., 2023) SimpleNeRF (Somraj et al., 2023) ZeroNVS (Sargent et al., 2024) ReconFusion (Wu et al., 2024) 3DGS (Kerbl et al., 2023) 2DGS (Huang et al., 2024a) FSGS (Zhu et al., 2024) Difix3D+ (Wu et al., 2025a) GenFusion (Wu et al., 2025b) GSFixer (Ours) PSNR LPIPS SSIM 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view 0.633 0.284 12.77 0.695 0.283 12.87 0.676 0.312 13.27 0.655 0.337 14.44 0.511 0.401 15.50 0.446 0.355 13.06 0.449 0.338 13.07 0.468 0.415 14.17 0.391 0.382 13.92 0.440 0.430 15.03 0.420 0.426 15.61 14.30 14.59 15.15 15.99 18.19 16.79 16.67 17.94 17.54 18.29 18.63 0.312 0.319 0.354 0.350 0.432 0.447 0.423 0.492 0.452 0.489 0.481 13.61 13.35 13.67 15.51 16.93 14.96 15.02 16.12 15.94 16.90 17.27 0.663 0.717 0.721 0.663 0.544 0.505 0.506 0.517 0.468 0.494 0. 0.705 0.715 0.741 0.680 0.585 0.576 0.580 0.578 0.578 0.578 0.559 0.271 0.260 0.283 0.316 0.358 0.251 0.243 0.318 0.298 0.357 0.370 Table 3: Quantitative Comparison on Mip-NeRF 360 Dataset. We compare the rendering quality with baselines given 3, 6 and 9 views. denotes results reproduced by ReconFusion and GenFusion, while indicates results reproduced by us on their official implementation. Figure 6: Qualitative comparison on Mip-NeRF 360. We compare the novel view rendering quality with baselines using 3, 6, and 9 input views. noteworthy that the I2V SC and I2V BC scores of Difix3D+ and GenFusion are even lower than the original artifact frames after their artifact correction, suggesting that these methods struggle to maintain consistency when fixing artifacts in novel views. The qualitative comparisons in Figure 4 further emphasize GSFixers superior quality and consistency in fixing the artifacts of novel views. For instance, Difix3D+ and GenFusion fail to restore the content consistently, such as the foreground statue and background buildings. See supplement for more details. In Domain Sparse View Reconstruction. We next compare our method with baseline approaches for sparse-view 3D reconstruction on the DL3DV-Benchmark. As shown in Table 2 and Figure 5, our GSFixer achieves more realistic novel view synthesis across 3, 6, and 9 input views. For example, under the extremely sparse 3-view setting, GSFixer significantly improves 3DGS quality by 3.55 dB in PSNR, 0.119 in SSIM, and 0.034 in LPIPS. Moreover, our GSFixer outperforms state-of-theart methods such as Difix3D+ and GenFusion by substantial margin. Difix3D+ fails to generate plausible content in occluded or missing regions, as illustrated in the building structure in the first It also does not adequately fix geometry-distorted Gaussians, as seen in the row of the figure. road reconstruction in the fifth and sixth rows. Additionally, inconsistent novel views generated by Difix3D+s image diffusion process degrade overall quality, particularly noticeable in the statue (first row) and in the building and sky regions (third row). Similarly, GenFusion exhibits notable artifacts in fixing 3DGS representation. For example, its video diffusion process generates foggy 9 Ours w/o 3D tokens Ours w/o 2D tokens Ours full model PSNR 16.36 16.48 16.72 SSIM 0.510 0.516 0.520 LPIPS 0.414 0.409 0.399 I2V SC 0.9527 0.9540 0. I2V BC 0.9630 0.9635 0.9644 OC 0.2403 0.2405 0.2407 TF 0.9218 0.9225 0.9233 MS 0.9663 0.9665 0.9665 Table 4: Ablation study about different conditions. We report the PSNR, SSIM, LPIPS, VBench and VBench++ metrics for the full model and its ablated versions on DL3DV-Res. Figure 7: Effectiveness of reference view conditions. We compare our full model with two alternatives: variant without 3D conditions (top), and variant without 2D conditions (bottom). The red boxes highlight the most prominent differences. geometry in the background (statue in first row and building in fifth row), failing to produce 3Dconsistent content in missing regions. This results in blurred renderings, as shown in the third and fourth rows. In contrast, our GSFixer effectively inpaints plausible content in missing regions and performs both semantically and geometrically consistent fixes in novel views, leading to high-quality and coherent novel view renderings. Out of Domain Sparse View Reconstruction. To demonstrate the generalizability of our method for out-of-distribution dataset, we further evaluate the methods on the challenging Mip-NeRF 360 dataset with 3, 6, and 9 input views. The quantitative results presented in Table 5 show that our GSFixer outperforms the baseline approaches, highlighting its strong generalization performance on unseen and complex scenes. For instance, our GSFixer surpasses generative 3DGS-based methods such as Difix3D+ and GenFusion. Moreover, we observe that our GSFixer even outperforms generative NeRF-based ReconFusion in sparse view settings. This is particularly notable as 3DGS is generally more prone to overfitting on sparse input views compared to NeRF. Qualitative results shown in Figure 6 further illustrate the semantic and 3D consistent generative capability of our GSFixer. For detailed per-scene results, please refer to the supplementary material. 4.3 ABLATION STUDIES Effectiveness of Reference View Conditions. We conduct an ablation study on one of our key contributions: the reference-guided video restoration model, which controls the 3DGS artifact restoration process using 3D and 2D tokens extracted from reference views. We compare the video restoration results of three variants on the DL3DV-Res benchmark: our full model, version without 3D tokens, and one without 2D tokens. As quantitatively results presented in Table 4, our full model consistently outperforms the other two variants across all metrics. The qualitative comparisons in Figure 7 further support this finding. From the visualization, we observe that removing the 3D 10 Figure 8: Effectiveness of reference-guided trajectory. We compare with widely used interpolation trajectory and ellipse trajectory. The red boxes highlight the most prominent differences. Interpolation trajectory Ellipse trajectory Reference-guided trajectory PSNR 14.50 15.46 15.61 SSIM 0.353 0.362 0.370 LPIPS 0.565 0.563 0.559 Table 5: Ablation study about trajectory sampling. We report the PSNR, SSIM, and LPIPS metrics for different trajetories on Mip-NeRF 360. condition leads to poor 3D consistency with the reference views, e.g., misalignment in the fence and cars (highlighted in red boxes). This is because 3DGS renders under sparse-view settings often exhibit severe geometric distortions, making it difficult for the diffusion model to generate geometrically consistent content without strong 3D priors. Similarly, without the 2D condition, the model fails to restore semantically plausible details, such as the table leg, baffle, socket, and chair marked in red boxes. In contrast, our full model successfully restores artifact-ridden novel views to frames that are both 3D-consistent and semantically aligned with the reference views, demonstrating the effectiveness of injecting both 3D and 2D conditions into the video diffusion model. Effectiveness of Reference Guided Trajectory. To validate the effectiveness of our reference guided-trajectory sampling strategy, we further conduct an ablation study on the challenging MipNeRF 360 dataset using 3-view 3D reconstruction setting. Qualitative results are presented in Figure 8, with the most notable differences highlighted in red boxes. As shown in the figure, the interpolation trajectory lacks sufficient angular coverage, leading to incomplete reconstructions with missing regions. Meanwhile, the ellipse trajectory results in degraded rendering quality, as the novel views to be restored are not temporally sequential with the reference views. In contrast, our reference-guided trajectory ensures both coverage and quality in the reconstruction process. The quantitative results in Table 5 further confirm the effectiveness of the reference-guided trajectory sampling strategy."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "We present GSFixer, novel pipeline for enhancing the quality of 3D Gaussian Splatting in sparseview 3D reconstruction and novel view synthesis. Our reference-guided video restoration model integrates both 3D geometric and 2D semantic condition signals from reference views, ensuring semantically coherent and geometrically consistent artifact correction in novel views. Furthermore, we introduce reference-guided trajectory sampling strategy within the iterative reconstruction process, enabling high-quality 3D reconstruction. In addition, we propose the DL3DV-Res benchmark to evaluate the 3DGS artifact restoration capability. Limitations and Future Work. Our GSFixer bulids upon DiT-based video diffusion model that requires 50 denoising steps, which may limit the efficiency. As 3D enhancement model, its performance is inherently limited by the quality of the initial 3DGS representation. Future work may explore improving GSFixer with advanced single-step video diffusion models and improved 3D representation, enabling efficient and high-fidelity novel view synthesis."
        },
        {
            "title": "REFERENCES",
            "content": "Chong Bao, Xiyu Zhang, Zehao Yu, Jiale Shi, Guofeng Zhang, Songyou Peng, and Zhaopeng Cui. Free360: Layered gaussian splatting for unbounded 360-degree view synthesis from extremely sparse and unposed views. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1637716387, 2025. Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 54705479, 2022. Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: In Proceedings of the IEEE/CVF International Anti-aliased grid-based neural radiance fields. Conference on Computer Vision, pp. 1969719705, 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73107320, 2024. Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2108621095, 2025. Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1288212891, 2022. Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. In SIGGRAPH, 2025. Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. In Proceedings of the IEEE/CVF InInstruct-nerf2nerf: Editing 3d scenes with instructions. ternational Conference on Computer Vision, pp. 1974019750, 2023. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for video diffusion models. In International Conference on Learning Representations, 2025. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81538163, 2024. Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 20052015, 2025. Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH 2024, pp. 111, 2024a. 12 Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024b. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024c. Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on diet: Semantically consistent fewIn Proceedings of the IEEE/CVF International Conference on Computer shot view synthesis. Vision, pp. 58855894, 2021. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, pp. 1391, 2023. Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, and Lin Gu. Dngaussian: Optimizing sparse-view 3d gaussian radiance fields with global-local depth normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2077520785, 2024. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 1288812900, 2022. Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2216022169, 2024. Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024a. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 92989309, 2023. Xi Liu, Chaoyi Zhou, and Siyu Huang. 3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with view-consistent 2d diffusion priors. In Advances in Neural Information Processing Systems, pp. 133305133327, 2024b. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, pp. 405421, 2020. Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 54805490, 2022. Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofavideo: Controllable image animation via generative motion field adaptions in frozen image-tovideo diffusion model. In European Conference on Computer Vision, pp. 111128, 2024. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, pp. 131, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, pp. 167, 2020. Barbara Roessle, Jonathan Barron, Ben Mildenhall, Pratul Srinivasan, and Matthias Nießner. In Proceedings of the Dense depth priors for neural radiance fields from sparse input views. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1289212901, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 94209429, 2024. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103, 2024. Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 41044113, 2016. Seunghyeon Seo, Yeonjin Chang, and Nojun Kwak. Flipnerf: Flipped reflection rays for few-shot novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2288322893, 2023. Nagabhushan Somraj, Adithyan Karanayil, and Rajiv Soundararajan. Simplenerf: Regularizing sparse input neural radiance fields with simpler solutions. In SIGGRAPH Asia, pp. 111, 2023. Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural In Proceedings of the IEEE/CVF Conference on radiance fields from sparse and noisy poses. Computer Vision and Pattern Recognition, pp. 41904200, 2023. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 90659076, 2023. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH, pp. 111, 2024. Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, and Huan Ling. Difix3d+: Improving 3d reconstructions with single-step diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2602426035, 2025a. Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2155121561, 2024. 14 Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, and Anpei Chen. Genfusion: Closing the loop between reconstruction and generation via videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 60786088, 2025b. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pp. 399417, 2024. Yexing Xu, Longguang Wang, Minglin Chen, Sheng Ao, Li Li, and Yulan Guo. Dropoutgs: Dropping out gaussians for better sparse-view rendering. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 701710, 2025. Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82548263, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 45784587, 2021. Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. arXiv preprint arXiv:2503.05638, 2025. Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, TienTsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: ExIn Advances in ploring monocular geometric cues for neural implicit surface reconstruction. Neural Information Processing Systems, pp. 2501825032, 2022. David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 20502062, 2025. Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, and Eric Xing. Fregs: 3d gaussian splatting with progressive frequency regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2142421433, 2024. Sijie Zhao, Wenbo Hu, Xiaodong Cun, Yong Zhang, Xiaoyu Li, Zhe Kong, Xiangjun Gao, Muyao Niu, and Ying Shan. Stereocrafter: Diffusion-based generation of long and high-fidelity stereoscopic 3d from monocular videos. arXiv preprint arXiv:2409.07447, 2024. Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In European Conference on Computer Vision, pp. 145163, 2024. 15 APPENDIX: GSFIXER: IMPROVING 3D GAUSSIAN SPLATTING VIA REFERENCE-GUIDED VIDEO DIFFUSION PRIORS REFERENCE-GUIDED VIDEO DIFFUSION MODEL DETAILS Our reference-guided video diffusion model is built upon CogVideoX-5B-I2V, pretrained DiTbased video diffusion model capable of generating 49-frame videos at resolution 480720 from single input image. In our video-to-video restoration task, which aims to restore artifact frames into clean ones, we replace the original image condition with the 3DGS renders {I nov }n=1,...,47 between two reference views {I1, I2}. In the training process, each training sample consists of sequence of 47-frame artifact-prone 3DGS renders, two reference images, and 49-frame ground truth RGB video. The artifact frames (493480720) and ground truth frames (493480720) are first encoded into latent features using 3DVAE, resulting in latent dimensions of 16136090. The ground truth latent features are then perturbed with noise for the diffusion training process. For the view tokens, we concatenate, patchify and project the latent features to 175503072 view tokens. For the text tokens, we use BLIP to generate text captions of the reference views and use T5 to extract 2264096 feature embedding, which are projected to 2263072 text tokens. To extract geometric and semantic conditioning signals, the two reference views are first resize to 350518 to feed into VGGT and DINOv2 encoder to produce 29302048 geometric tokens and 29301024 semantic tokens. We abondon 252048 camera token from VGGT, and remove 211024 class token and 241024 registration token from DINOv2, resulting in 29252048 3D geometric tokens and 29251024 2D semantic tokens. These are projected to 29253072 using rojector3D and rojector2D, respectively. rojector3D is linear layer mapping from 2048 to 3072 dimensions, followed by LayerNorm. rojector2D is linear layer mapping from 1024 to 3072 dimensions, followed by LayerNorm. Finally, the 3D geometric and 2D semantic tokens are combined and reshaped into fusion tokens of dimension 18503072, which serve as rich 3D and 2D priors to guide the restoration process of diffusion model. During inference, we employ DDIM sampling with classifier-free guidance to modulate condition adherence strength."
        },
        {
            "title": "B DETAILS OF COMPARISION BASELINES",
            "content": "We compare our method with (i) regularization methods and (ii) generative methods for sparse-view reconstruction and 3DGS artifact restoration tasks. Specifically, we evaluate NeRF-based per-scene regularization methods including Zip-NeRF Barron et al. (2023), FreeNeRF Yang et al. (2023) and SimpleNeRF Somraj et al. (2023); NeRF-based generative methods such as ZeroNVS Sargent et al. (2024) and ReconFusion Wu et al. (2024); 3DGS-based per-scene regularization method FSGS Zhu et al. (2024); and 3DGS-based generative methods Difix3D+ Wu et al. (2025a) and GenFusion Wu et al. (2025b). Difix3D+ incoporates generative priors from the image diffusion model SD-turbo Sauer et al. (2024), finetuned on paired artifact renders and clean frames. In contrast, GenFusion bulids upon the video diffusion model DynamicCrafter Xing et al. (2024) that condition video frames on artifact-prone RGBD renders."
        },
        {
            "title": "C MORE QUANTITATIVE AND QUALITATIVE RESULTS",
            "content": "We provide extensive per-scene experimental results in Figure 9, Table 6, Table 7, Table 8, Table 9, Table 10, and Table 11. 16 Figure 9: Qualitative comparision of 3DGS artifact restoration on DL3DV-Res. 17 Artifact Difix3D+ GenFusion GSFixer (Ours) PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 0.196 17.43 0.458 12.65 0.499 13.09 0.620 11.50 0.123 20.37 0.355 17.24 0.472 13.08 0.273 15.59 0.290 15.13 0.486 11.08 0.552 12.41 0.536 12.38 0.437 12.13 0.474 13.37 0.384 14.59 0.458 16.49 0.345 17.91 0.459 12.21 0.417 13.27 0.165 19.19 0.292 14.95 0.363 14.40 0.546 11.63 0.561 12.54 0.540 10.72 0.465 12.30 0.231 16.33 0.421 13.57 0.324 16.87 0.327 15.35 0.518 12.02 0.321 14.94 0.247 18.72 0.392 13.14 0.432 13.66 0.457 12.67 0.480 12.77 0.297 14.81 0.255 15.94 0.376 14.98 0.310 12.16 0.425 14.08 0.348 13.87 0.526 12.17 0.449 13.07 0.404 14.93 0.406 15.72 0.227 14.46 0.469 13.61 0.453 14.39 0.408 13.51 0.448 14.48 0.286 13.64 0.568 10.68 0.180 17.45 0.214 18.02 0.337 14.65 0.384 12.39 0.378 13.83 0.468 12.44 0.322 15.33 0.377 17.59 0.348 13.67 0.238 17.08 0.406 13.76 0.467 14.24 0.348 15.45 0.467 12.92 0.467 13.75 0.522 12.22 0.429 14.24 0.499 13.16 0.300 14.88 0.318 15.44 0.463 15.69 0.345 14.17 0.573 0.250 0.526 0.17 0.662 0.578 0.452 0.529 0.491 0.432 0.452 0.245 0.491 0.389 0.429 0.559 0.666 0.243 0.434 0.629 0.378 0.385 0.304 0.352 0.272 0.305 0.595 0.475 0.662 0.410 0.381 0.447 0.614 0.375 0.556 0.437 0.367 0.658 0.463 0.525 0.377 0.495 0.565 0.262 0.386 0.518 0.604 0.455 0.441 0.396 0.384 0.478 0.395 0.262 0.592 0.592 0.587 0.445 0.602 0.445 0.524 0.779 0.387 0.673 0.398 0.564 0.549 0.472 0.350 0.417 0.432 0.342 0.427 0.532 0.551 0.418 0.412 0.562 0.507 0.626 0.303 0.419 0.552 0.441 0.440 0.579 0.563 0.565 0.521 0.552 0.462 0.472 0.431 0.533 0.522 0.319 0.438 0.495 0.561 0.615 0.562 0.570 0.395 0.482 0.455 0.457 0.529 0.430 0.394 0.528 0.503 0.487 0.512 0.375 0.380 0.466 0.486 0.508 0.472 0.537 0.504 0.476 0.478 0.487 0.505 0.501 0.487 0.485 0.437 0.629 0.338 0.356 0.457 0.500 0.446 0.543 0.430 0.415 0.479 0.373 0.504 0.513 0.474 0.494 0.518 0.551 0.498 0.576 0.494 0.420 0.461 0.469 0.375 0.536 0.500 0.604 0.269 0.391 0.540 0.403 0.402 0.560 0.563 0.538 0.498 0.491 0.420 0.453 0.403 0.473 0.501 0.197 0.364 0.448 0.552 0.572 0.557 0.541 0.314 0.462 0.401 0.417 0.549 0.411 0.337 0.469 0.435 0.472 0.505 0.393 0.330 0.441 0.468 0.474 0.429 0.529 0.491 0.457 0.459 0.424 0.496 0.470 0.474 0.477 0.400 0.604 0.280 0.274 0.418 0.490 0.423 0.529 0.385 0.404 0.423 0.367 0.469 0.482 0.425 0.477 0.490 0.548 0.469 0.520 0.424 0.387 0.448 0. 0.510 0.218 0.407 0.129 0.680 0.530 0.445 0.507 0.467 0.397 0.331 0.205 0.396 0.360 0.374 0.522 0.606 0.238 0.385 0.707 0.388 0.353 0.249 0.345 0.232 0.268 0.559 0.380 0.610 0.408 0.269 0.416 0.569 0.317 0.523 0.337 0.316 0.561 0.429 0.473 0.347 0.434 0.504 0.232 0.378 0.470 0.520 0.417 0.380 0.328 0.318 0.453 0.384 0.192 0.563 0.612 0.518 0.370 0.536 0.352 0.496 0.714 0.398 0.613 0.362 0.400 0.473 0.450 0.318 0.304 0.365 0.288 0.382 0.496 0.500 0.387 0.447 0.567 0.525 0.613 0.321 0.442 0.587 0.454 0.466 0.660 0.599 0.568 0.554 0.576 0.492 0.498 0.491 0.512 0.558 0.279 0.457 0.510 0.571 0.609 0.570 0.580 0.425 0.519 0.482 0.478 0.586 0.478 0.418 0.551 0.502 0.516 0.526 0.458 0.426 0.502 0.515 0.520 0.492 0.563 0.520 0.499 0.502 0.502 0.524 0.517 0.527 0.525 0.470 0.661 0.389 0.347 0.500 0.530 0.492 0.573 0.459 0.472 0.480 0.454 0.524 0.559 0.512 0.521 0.524 0.592 0.528 0.551 0.521 0.448 0.493 0.477 17.04 12.42 13.13 11.34 20.06 17.33 12.85 15.27 15.24 11.23 12.45 12.32 12.12 13.59 14.87 16.36 18.43 12.10 13.30 20.09 15.30 14.19 11.57 12.22 10.57 11.99 16.98 13.67 16.75 15.14 12.10 14.86 19.11 12.96 13.58 12.83 12.57 14.92 15.92 15.13 11.84 13.90 13.71 12.18 12.80 14.84 15.74 14.62 13.51 14.58 13.65 14.22 13.40 10.83 17.94 18.24 14.75 12.20 13.90 12.52 15.48 17.93 13.38 17.34 13.48 14.90 15.48 12.83 13.42 12.35 14.40 13.26 14.90 15.33 15.44 13.90 0.521 0.214 0.429 0.121 0.706 0.549 0.422 0.517 0.491 0.355 0.360 0.195 0.428 0.330 0.396 0.527 0.636 0.238 0.396 0.732 0.406 0.371 0.251 0.313 0.231 0.271 0.606 0.409 0.637 0.414 0.283 0.428 0.604 0.332 0.545 0.357 0.317 0.594 0.449 0.508 0.365 0.455 0.536 0.207 0.361 0.488 0.554 0.438 0.384 0.345 0.338 0.473 0.391 0.168 0.618 0.627 0.563 0.390 0.570 0.365 0.522 0.731 0.403 0.651 0.369 0.435 0.520 0.455 0.321 0.327 0.386 0.299 0.404 0.511 0.519 0.395 17.03 12.79 14.76 12.19 18.95 17.55 13.49 15.41 15.89 14.27 13.13 13.43 12.77 14.00 15.47 16.29 17.97 12.74 13.62 18.32 15.13 14.33 12.5 12.52 11.67 12.74 16.49 14.28 16.48 15.18 13.86 15.29 17.99 13.39 12.75 13.92 13.76 16.06 16.57 15.00 11.85 14.28 13.86 13.65 13.24 14.87 15.75 14.38 14.33 15.28 14.67 13.87 13.75 13.23 17.04 17.51 15.02 12.70 13.90 13.47 15.29 18.92 13.22 17.50 13.94 16.32 15.12 13.48 13.67 13.04 14.28 13.63 14.38 15.13 15.52 14.27 23.07 15.32 14.47 12.53 25.59 19.51 14.96 19.11 20.23 14.87 13.93 14.86 15.65 15.65 16.86 16.80 20.34 15.09 16.08 24.18 18.42 17.60 13.15 13.15 12.27 14.58 20.11 15.83 20.56 18.05 13.98 17.79 21.89 15.93 15.08 14.99 14.12 18.12 17.93 16.99 16.33 16.21 16.33 14.82 15.26 16.16 17.55 20.25 15.20 16.41 16.35 15.53 17.23 13.42 22.53 21.28 18.39 15.46 15.85 14.42 18.51 18.49 16.16 20.64 16.10 17.47 18.36 13.78 15.43 13.76 16.48 15.26 20.39 17.94 15.70 16. 0.709 0.330 0.513 0.181 0.860 0.625 0.493 0.670 0.623 0.402 0.468 0.267 0.558 0.383 0.477 0.571 0.728 0.342 0.506 0.820 0.529 0.484 0.311 0.355 0.313 0.373 0.725 0.511 0.734 0.558 0.388 0.534 0.723 0.461 0.619 0.460 0.393 0.710 0.573 0.593 0.564 0.579 0.663 0.282 0.435 0.559 0.640 0.644 0.459 0.421 0.453 0.532 0.547 0.240 0.767 0.732 0.696 0.543 0.654 0.474 0.613 0.760 0.529 0.762 0.466 0.576 0.612 0.504 0.376 0.410 0.472 0.381 0.617 0.616 0.547 0.529 032dee9fb0 0569e83fdc 06da796666 073f5a9b98 07d9f9724c 0853979305 093ef327b4 0a1b7c20a9 0a485338bb 0bfdd020cf 119fd56d37 1264931635 14eb48a50e 15ff83e253 165f5af8bf 183dd248f6 1ba74c2267 1d6a9ed47c 1da888bded 1de58be515 2385549d39 26fd23358f 286239bd0d 2991a75d1f 2b65ba886e 2beaca3189 2cbfe28643 2f3e1c0f68 32c2b92fac 341b4ff3df 35317e6219 35872363e1 374ffd0c5f 387eeb925b 389a460ca1 3b16a10ec9 3b7529dccc 3bb3bb4d3e 3bb894d193 41036716da 444da1b4a3 457e9a1ae7 484c0aca40 493816813d 4ae797d07b 4ff8650b5c 50c46cf8b8 513e4ea2e8 54bf355ca7 56452d9cd9 565553aa89 599ca3e04c 5a27c00f52 5a69d1027b 5c3af58102 5f0041e53d 63798f5c6f 669c36225b 66fd66cbed 6d22162561 6d81c5ab0d 6e11e7f4fe 70eac6ff18 71b2dc8a2a 75fbbe4673 7705a2edd0 7a9f97660b 7da3db9905 800cf88687 8324b3ca22 85cd0e9211 8b9fb9d9f1 8cb2e97d26 8fdc5130f0 90cb7ef953 917e9c8985 Table 6: Per-scene 3DGS artifact restoration quantitative comparison on DL3DV-Res (Part 1). 18 Artifact Difix3D+ GenFusion GSFixer (Ours) PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 0.437 13.96 0.287 15.67 0.391 15.73 0.569 10.43 0.456 12.32 0.319 15.54 0.265 14.06 0.473 12.76 0.510 12.77 0.528 11.94 0.343 13.93 0.490 12.30 0.444 14.73 0.339 15.87 0.479 12.33 0.529 11.04 0.326 16.08 0.349 13.97 0.434 13.90 0.511 11.97 0.171 18.20 0.248 17.50 0.196 17.44 0.508 12.10 0.453 13.50 0.437 16.43 0.341 14.85 0.290 17.30 0.420 12.40 0.445 12.86 0.531 13.59 0.555 10.99 0.272 15.82 0.556 12.10 0.456 13.47 0.558 11.63 0.503 12.74 0.210 15.33 0.273 14.38 0.581 11.35 0.450 12.63 0.425 17.44 0.376 14.02 0.281 15.52 0.340 13.12 0.453 13.90 0.491 12.51 0.284 14.38 0.516 12.13 0.478 13.75 0.353 13.81 0.375 13.68 0.483 14.50 0.361 17.15 0.210 14.48 0.336 14.75 0.440 13.23 0.416 13.11 0.284 14.93 0.588 11.78 0.395 12.35 0.399 14.12 0.483 0.484 0.562 0.267 0.296 0.638 0.401 0.375 0.432 0.414 0.406 0.606 0.481 0.588 0.338 0.332 0.502 0.380 0.566 0.274 0.787 0.653 0.640 0.280 0.342 0.569 0.455 0.545 0.351 0.467 0.404 0.440 0.491 0.199 0.376 0.411 0.380 0.564 0.535 0.164 0.310 0.628 0.286 0.427 0.563 0.506 0.241 0.529 0.335 0.476 0.506 0.569 0.277 0.381 0.631 0.534 0.254 0.260 0.530 0.180 0.455 0.453 0.490 0.385 0.489 0.669 0.543 0.412 0.389 0.517 0.564 0.555 0.494 0.481 0.508 0.484 0.494 0.582 0.414 0.487 0.468 0.562 0.255 0.333 0.295 0.545 0.508 0.438 0.511 0.411 0.510 0.528 0.523 0.554 0.447 0.560 0.503 0.556 0.559 0.346 0.472 0.587 0.553 0.479 0.532 0.397 0.452 0.537 0.546 0.486 0.565 0.532 0.497 0.389 0.551 0.498 0.343 0.461 0.529 0.530 0.381 0.630 0.493 0.486 0.480 0.337 0.432 0.635 0.529 0.340 0.343 0.489 0.539 0.550 0.420 0.481 0.470 0.438 0.479 0.585 0.371 0.429 0.469 0.524 0.243 0.312 0.291 0.516 0.479 0.417 0.442 0.324 0.492 0.496 0.508 0.540 0.405 0.540 0.492 0.552 0.535 0.329 0.450 0.579 0.514 0.458 0.481 0.338 0.430 0.497 0.534 0.438 0.521 0.518 0.489 0.415 0.500 0.443 0.317 0.421 0.499 0.473 0.375 0.610 0.481 0. 0.515 0.436 0.506 0.697 0.550 0.412 0.435 0.524 0.572 0.572 0.502 0.530 0.508 0.523 0.509 0.593 0.441 0.487 0.537 0.559 0.348 0.394 0.373 0.547 0.537 0.465 0.498 0.413 0.539 0.529 0.538 0.576 0.481 0.565 0.520 0.568 0.570 0.409 0.510 0.599 0.557 0.506 0.529 0.425 0.321 0.545 0.562 0.499 0.554 0.543 0.534 0.484 0.549 0.515 0.405 0.490 0.534 0.529 0.469 0.622 0.533 0.509 0.409 0.454 0.523 0.246 0.256 0.627 0.390 0.338 0.358 0.317 0.400 0.463 0.394 0.526 0.310 0.267 0.461 0.384 0.429 0.256 0.705 0.612 0.608 0.247 0.287 0.539 0.433 0.530 0.300 0.401 0.374 0.341 0.425 0.171 0.320 0.322 0.315 0.516 0.503 0.137 0.293 0.563 0.277 0.402 0.473 0.415 0.206 0.491 0.319 0.417 0.452 0.476 0.214 0.331 0.594 0.528 0.230 0.247 0.442 0.132 0.432 0.405 0.415 0.473 0.561 0.200 0.259 0.640 0.407 0.351 0.371 0.332 0.412 0.510 0.398 0.559 0.303 0.274 0.496 0.388 0.443 0.262 0.744 0.633 0.630 0.253 0.289 0.533 0.450 0.555 0.313 0.419 0.367 0.362 0.456 0.164 0.321 0.320 0.320 0.543 0.535 0.132 0.297 0.580 0.285 0.421 0.504 0.447 0.209 0.513 0.319 0.431 0.481 0.509 0.217 0.338 0.624 0.551 0.224 0.253 0.481 0.125 0.420 0.419 13.86 16.25 16.07 10.36 12.11 15.38 14.09 12.63 12.67 11.81 14.00 13.11 15.19 15.86 12.01 10.76 16.23 13.75 14.29 11.74 19.01 17.57 18.14 11.94 13.30 16.11 14.64 17.85 12.41 12.66 13.36 11.03 16.10 11.74 13.12 11.52 12.89 15.59 14.11 11.21 12.28 17.23 13.91 15.60 13.26 13.93 12.45 14.17 11.84 13.53 13.50 13.77 15.04 17.32 14.51 14.38 13.06 13.14 15.61 11.72 12.81 14.14 0.504 0.574 0.626 0.262 0.382 0.703 0.613 0.395 0.457 0.428 0.545 0.592 0.504 0.691 0.365 0.400 0.579 0.506 0.560 0.303 0.842 0.721 0.738 0.298 0.377 0.555 0.588 0.655 0.438 0.521 0.382 0.408 0.623 0.206 0.395 0.399 0.408 0.698 0.700 0.190 0.401 0.658 0.442 0.589 0.662 0.550 0.296 0.687 0.362 0.526 0.606 0.575 0.293 0.465 0.760 0.635 0.328 0.388 0.608 0.189 0.516 0.520 16.52 18.61 17.48 13.96 14.95 17.25 18.59 13.24 14.64 13.86 17.54 14.36 17.92 20.09 13.27 12.61 18.68 16.73 16.88 12.71 22.30 20.59 23.28 13.39 14.81 15.94 18.03 20.60 14.82 14.87 13.70 11.90 21.05 12.86 14.60 13.22 14.71 19.76 18.62 12.58 14.55 18.78 17.45 20.31 17.58 15.40 14.71 18.52 12.67 15.67 16.37 14.99 17.07 19.97 19.79 17.21 15.63 16.34 19.14 13.41 17.72 16. 15.08 16.06 15.29 11.78 12.76 14.37 14.69 12.90 13.49 13.27 13.63 14.38 15.82 15.68 12.80 11.83 15.98 13.60 17.50 12.31 20.26 17.87 18.54 12.83 14.13 16.31 14.32 17.08 12.82 13.04 14.65 12.07 16.51 12.45 13.70 12.94 13.57 16.54 13.56 12.26 12.66 17.80 13.90 15.80 14.42 13.25 13.32 14.27 12.31 14.12 13.51 14.16 15.89 17.71 14.21 13.7 13.72 13.76 16.38 12.80 14.28 14.56 91afb9910b 946f49be73 9641a1ed79 9c8c0e0fad 9cbc554864 9e9a89ae6f 9fb0588ff0 a17a984ca9 a401469cb0 a62c330f54 a62f9a1c63 a726c1112a adb95f29c1 adf35184a1 af0d7039e6 b2076bc723 b3bf9079b4 b4f53094fd b5faa2a8ce b6d1134cb0 b92b499c9b ba55c875d2 c076929db6 c37109a55e c37726ce77 c455899acf cbd44beb04 cc08c0bdc3 cd9c981eeb ceb252f5d4 d1b3a0b37a d3812aad53 d3af8212ae d4fbeba016 d8de66037b d904ae2998 d9b6376623 d9f4c746e6 dac9796dd6 dafa9c7cbd ddfdcfdf02 ded5e4b46a df04f58064 df29c22586 df4f9d9a0a e5684b3292 e78f8cebd2 e8ce51b6ab e9360e7a89 eb4cf52988 ec1e44d4dc ec305787b7 ed16328235 ef59aac437 f004c810d9 f477ffc4b3 f71ac346cd f7aaea9ac6 fb2c0499c2 fb3b73f1d3 ff59239865 average Table 7: Per-scene 3DGS artifact restoration quantitative comparison on DL3DV-Res (Part 2). 19 Difix3D+ GSFixer (Ours) GenFusion PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 0.476 0.511 15.96 0.526 0.380 14.33 0.465 0.407 14.70 0.623 0.306 11.50 0.455 0.506 15.66 0.441 0.580 17.78 0.485 0.576 14.09 0.439 0.496 16.49 0.393 0.669 18.62 0.416 0.640 16.28 0.527 0.496 12.01 0.495 0.615 15.37 0.513 0.418 13.76 0.496 0.414 14.91 0.394 0.653 18.31 0.550 0.452 14.07 0.396 0.583 15.43 0.397 0.600 17.18 0.520 0.456 13.90 0.568 0.344 12.94 0.443 0.559 13.90 0.624 0.224 11.68 0.318 0.585 19.31 0.526 0.446 14.50 0.413 0.511 14.23 0.573 0.306 14.83 0.325 0.757 17.53 0.574 0.443 12.56 0.478 0.498 15.07 0.498 0.401 0.359 0.260 0.528 0.550 0.508 0.476 0.639 0.583 0.449 0.610 0.400 0.358 0.630 0.427 0.555 0.613 0.443 0.439 0.561 0.169 0.538 0.463 0.587 0.281 0.729 0.416 0.4816 0.510 0.546 0.514 0.648 0.489 0.437 0.481 0.458 0.404 0.417 0.539 0.517 0.526 0.485 0.417 0.565 0.394 0.421 0.523 0.593 0.453 0.643 0.370 0.508 0.435 0.606 0.321 0.583 0.493 0.494 0.423 0.452 0.605 0.509 0.383 0.518 0.395 0.418 0.458 0.576 0.621 0.549 0.404 0.368 0.538 0.374 0.386 0.527 0.647 0.518 0.497 0.238 0.490 0.515 0.485 0.314 0.547 0.473 17.04 14.78 16.56 13.77 16.57 18.03 16.20 16.85 20.03 18.33 13.28 16.46 14.93 16.49 18.62 15.24 16.79 18.10 14.78 13.06 15.79 12.64 21.13 14.82 15.44 15.83 18.85 13.51 16.21 15.16 13.31 15.49 12.10 12.74 16.96 15.14 15.63 18.72 17.13 11.78 15.19 13.74 14.91 17.21 13.09 15.07 16.02 13.39 10.92 14.29 11.81 19.84 13.52 13.37 13.58 17.76 12.09 14. 0.526 0.457 0.435 0.343 0.565 0.578 0.589 0.516 0.692 0.660 0.503 0.612 0.420 0.612 0.671 0.485 0.590 0.635 0.486 0.416 0.610 0.250 0.633 0.492 0.640 0.355 0.763 0.467 0.536 0bfdd020cf 2beaca3189 5a69d1027b 9c8c0e0fad 032dee9fb0 85cd0e9211 91afb9910b 165f5af8bf 374ffd0c5f 457e9a1ae7 669c36225b 9641a1ed79 56452d9cd9 493816813d 0853979305 adb95f29c1 b3bf9079b4 ba55c875d2 d1b3a0b37a d904ae2998 dac9796dd6 dafa9c7cbd df29c22586 e9360e7a89 ec305787b7 ed16328235 f004c810d9 ff59239865 average Table 8: Comparison of per-scene 3D reconstruction results of DL3DV with 3 input views. 20 Difix3D+ GSFixer (Ours) GenFusion PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 0.399 0.584 17.54 0.361 0.638 17.56 0.365 0.501 17.35 0.472 0.378 15.15 0.285 0.727 20.48 0.266 0.692 20.60 0.365 0.673 17.56 0.349 0.570 18.06 0.227 0.827 23.98 0.343 0.752 19.48 0.412 0.300 15.42 0.419 0.668 18.05 0.373 0.511 16.35 0.401 0.515 16.89 0.260 0.807 22.35 0.383 0.526 16.78 0.301 0.696 18.84 0.265 0.764 20.78 0.450 0.566 16.22 0.526 0.446 14.36 0.279 0.767 19.83 0.541 0.270 13.32 0.216 0.737 24.24 0.430 0.559 15.80 0.309 0.733 18.53 0.502 0.398 16.18 0.194 0.872 22.77 0.378 0.611 16.89 0.360 0.610 18.26 0.547 0.592 0.471 0.342 0.690 0.673 0.622 0.551 0.807 0.694 0.566 0.658 0.422 0.482 0.771 0.516 0.662 0.742 0.516 0.480 0.735 0.226 0.713 0.540 0.701 0.358 0.859 0.549 0.5895 0.394 0.250 0.322 0.425 0.265 0.258 0.346 0.302 0.195 0.347 0.400 0.470 0.409 0.326 0.212 0.435 0.253 0.227 0.435 0.541 0.285 0.418 0.150 0.372 0.310 0.382 0.157 0.339 0.329 18.21 18.37 18.47 16.14 21.13 22.32 18.64 18.71 24.50 20.93 16.64 18.47 18.84 17.54 22.71 18.87 18.56 21.37 16.66 13.97 20.87 14.21 24.57 16.09 19.01 16.50 23.62 19.05 19.11 0.582 0.627 0.516 0.399 0.722 0.754 0.672 0.576 0.819 0.735 0.612 0.651 0.562 0.522 0.789 0.583 0.676 0.751 0.538 0.480 0.768 0.279 0.724 0.569 0.723 0.400 0.874 0.600 0.625 0.405 0.359 0.407 0.535 0.293 0.337 0.366 0.374 0.224 0.322 0.396 0.418 0.420 0.406 0.245 0.482 0.296 0.273 0.426 0.533 0.278 0.573 0.256 0.426 0.299 0.504 0.202 0.403 0. 17.39 18.39 16.97 14.69 19.65 20.14 18.53 17.76 24.29 20.48 16.24 18.17 17.70 16.93 22.39 15.73 18.29 20.18 16.82 13.10 20.63 13.29 24.64 15.63 19.27 16.29 22.94 17.59 18.36 0bfdd020cf 2beaca3189 5a69d1027b 9c8c0e0fad 032dee9fb0 85cd0e9211 91afb9910b 165f5af8bf 374ffd0c5f 457e9a1ae7 669c36225b 9641a1ed79 56452d9cd9 493816813d 0853979305 adb95f29c1 b3bf9079b4 ba55c875d2 d1b3a0b37a d904ae2998 dac9796dd6 dafa9c7cbd df29c22586 e9360e7a89 ec305787b7 ed16328235 f004c810d9 ff59239865 average Table 9: Comparison of per-scene 3D reconstruction results of DL3DV with 6 input views. 21 Difix3D+ GSFixer (Ours) GenFusion PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 0.352 0.635 19.15 0.286 0.725 19.79 0.316 0.578 18.69 0.436 0.442 16.76 0.209 0.829 22.93 0.262 0.778 22.88 0.295 0.737 20.45 0.316 0.617 19.41 0.160 0.886 26.67 0.299 0.800 21.66 0.362 0.690 17.03 0.389 0.702 18.85 0.372 0.594 17.32 0.324 0.581 18.35 0.199 0.856 24.54 0.383 0.598 18.29 0.254 0.762 19.37 0.200 0.825 22.98 0.355 0.660 18.41 0.430 0.548 16.80 0.231 0.815 22.06 0.512 0.297 14.33 0.172 0.782 25.66 0.352 0.668 17.95 0.224 0.806 21.07 0.414 0.477 18.44 0.157 0.911 25.11 0.333 0.658 18.37 0.307 0.688 20.12 0.582 0.694 0.542 0.411 0.787 0.762 0.715 0.597 0.871 0.761 0.636 0.675 0.473 0.563 0.835 0.557 0.717 0.808 0.607 0.548 0.807 0.276 0.759 0.626 0.776 0.460 0.899 0.613 0.656 19.70 20.52 18.80 16.67 22.73 22.60 20.28 19.12 27.01 22.05 17.91 19.30 19.01 18.11 24.66 18.17 19.76 22.79 18.97 16.03 22.00 13.92 26.14 18.66 21.65 17.98 25.50 18.88 20.32 0.366 0.284 0.356 0.482 0.211 0.267 0.301 0.330 0.163 0.281 0.336 0.389 0.370 0.338 0.192 0.399 0.247 0.207 0.332 0.443 0.232 0.533 0.228 0.339 0.220 0.417 0.157 0.369 0.314 20.03 20.20 19.83 17.35 22.89 22.48 20.51 19.67 26.67 22.21 17.60 19.32 19.29 18.88 24.86 18.88 19.87 23.79 19.06 16.71 22.42 14.80 26.01 18.15 21.54 18.18 25.40 20.13 20.60 0.605 0.706 0.581 0.442 0.801 0.757 0.733 0.611 0.874 0.777 0.656 0.676 0.561 0.596 0.841 0.584 0.737 0.818 0.623 0.553 0.812 0.309 0.774 0.643 0.790 0.467 0.906 0.654 0. 0.320 0.185 0.270 0.380 0.169 0.174 0.248 0.255 0.120 0.262 0.324 0.419 0.393 0.249 0.152 0.343 0.212 0.170 0.315 0.408 0.207 0.385 0.115 0.289 0.203 0.295 0.126 0.272 0.259 0bfdd020cf 2beaca3189 5a69d1027b 9c8c0e0fad 032dee9fb0 85cd0e9211 91afb9910b 165f5af8bf 374ffd0c5f 457e9a1ae7 669c36225b 9641a1ed79 56452d9cd9 493816813d 0853979305 adb95f29c1 b3bf9079b4 ba55c875d2 d1b3a0b37a d904ae2998 dac9796dd6 dafa9c7cbd df29c22586 e9360e7a89 ec305787b7 ed16328235 f004c810d9 ff59239865 average Table 10: Comparison of per-scene 3D reconstruction results of DL3DV with 9 input views. 22 GenFusion PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS GSFixer (Ours) Difix3D+ bicycle bonsai counter flowers garden kitchen room stump treehill average bicycle bonsai counter flowers garden kitchen room stump treehill average bicycle bonsai counter flowers garden kitchen room stump treehill average 13.02 13.86 13.96 11.81 14.34 15.56 14.15 15.35 13.19 13.92 15.58 16.30 16.09 12.66 17.58 17.95 15.07 17.51 14.70 15. 16.99 18.54 17.42 13.97 19.18 20.08 17.96 18.50 15.25 17.54 0.177 0.397 0.394 0.154 0.258 0.371 0.465 0.215 0.253 0.298 0.253 0.525 0.490 0.178 0.360 0.534 0.525 0.284 0.293 0.382 0.288 0.661 0.552 0.218 0.453 0.610 0.626 0.340 0.322 0.452 0.632 0.549 0.519 0.693 0.573 0.537 0.427 0.620 0.649 0.578 0.594 0.427 0.413 0.572 0.363 0.310 0.460 0.485 0.588 0. 0.498 0.343 0.352 0.499 0.282 0.243 0.343 0.425 0.537 0.391 3 Views 14.75 13.87 15.20 12.86 16.13 16.14 16.23 16.51 13.57 15.03 0.247 0.408 0.470 0.202 0.289 0.419 0.568 0.296 0.311 0.357 6 Views 16.16 17.08 17.06 13.69 18.62 18.62 17.73 17.77 15.36 16. 0.292 0.545 0.545 0.224 0.396 0.556 0.628 0.321 0.353 0.430 9 Views 16.89 19.43 18.18 14.50 19.79 20.56 19.72 19.09 16.42 18.29 0.313 0.661 0.607 0.253 0.471 0.635 0.700 0.376 0.387 0.489 0.632 0.549 0.519 0.693 0.573 0.537 0.427 0.620 0.649 0.578 0.570 0.426 0.426 0.632 0.452 0.388 0.387 0.570 0.585 0. 0.541 0.343 0.368 0.593 0.393 0.316 0.317 0.522 0.566 0.507 15.80 13.42 15.25 13.80 17.42 16.61 15.54 17.62 15.03 15.61 17.32 17.29 17.12 14.10 18.98 18.78 16.76 18.65 16.46 17.27 17.89 19.53 18.82 14.98 20.10 20.58 18.96 19.59 17.24 18.63 0.284 0.434 0.472 0.288 0.326 0.424 0.540 0.320 0.319 0.370 0.302 0.556 0.530 0.216 0.400 0.555 0.606 0.328 0.341 0. 0.322 0.654 0.600 0.244 0.473 0.623 0.666 0.378 0.366 0.481 0.613 0.554 0.505 0.654 0.537 0.503 0.441 0.603 0.619 0.559 0.549 0.440 0.425 0.594 0.429 0.371 0.403 0.543 0.546 0.478 0.511 0.356 0.362 0.534 0.367 0.314 0.333 0.496 0.507 0.420 Table 11: Per-scene 3D sparse view reconstruction quantitative comparison on Mip-NeRF 360."
        }
    ],
    "affiliations": [
        "CUHKSZ",
        "GVC Lab, Great Bay University",
        "University of Macau",
        "VIVO",
        "Xidian University"
    ]
}