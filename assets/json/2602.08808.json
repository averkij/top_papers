{
    "paper_title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
    "authors": [
        "Yapei Chang",
        "Kyle Lo",
        "Mohit Iyyer",
        "Luca Soldaini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 8 0 8 8 0 . 2 0 6 2 : r How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs Yapei Chang1, 2 Kyle Lo1, 3 Mohit Iyyer2 Luca Soldaini1 1Allen Institute for AI 2University of Maryland 3University of Washington Code: How2Everything Data: How2Bench, How2Train Model: How2Judge Contact: yapeic@umd.edu"
        },
        {
            "title": "Abstract",
            "content": "Generating step-by-step how-to procedures is key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything,1 scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support closed loop of capability evaluation and improvement at scale. The How2Everything Framework Figure 1a How2Mine mines and refines how-to procedures at web scale across 14 topics. Running this pipeline on about 1M documents yields 351K procedures. Figure 1b How2Bench + How2Score + How2Judge support scalable evaluation with an open 8B judge, showing clear scaling trends across model sizes and training stages. Figure 1c RL on How2Train with How2Score as the reward lift How2Bench performance while maintaining or improving scores on 12 standard post-training evaluations, indicating broad downstream utility. 1Of course, no method has infinite coverage. The name is playful pun to convey the scale and diversity of our framework."
        },
        {
            "title": "Introduction",
            "content": "The ability to understand and generate how-to procedures is key capability for large language models (LLMs). On one hand, LLMs are increasingly used for practical how-to guidance: approximately 8.5% of ChatGPT conversations fall under the How-To Advice category (Chatterji et al., 2025), with open-access conversation datasets like WildChat and LMSYS-Chat (Zhao et al., 2024; Zheng et al., 2024) showing similar trends (see Appendix A). On the other hand, exposure to procedural content at all stages of model training has been shown to improve downstream tasks that rely on reasoning and planning (e.g., pretraining: Ruis et al., 2025; midtraining: Zhang et al., 2020; post-training: Brahman et al., 2024). Therefore, the ability to evaluate and improve this core skill across diverse set of procedures has the potential to unlock progress on many downstream tasks. Evaluating and optimizing end-to-end procedural validity is challenging in open-world settings, where realworld procedures span diverse goals and domains without task-specific executor or automatic oracle. One key challenge is diversity: existing work is limited to narrow domains like cooking recipes (Lal et al., 2024; Toyooka et al., 2025) or specific sources like how-to sites (Zhang et al., 2020; Yuan et al., 2023). Furthermore, successful procedure generation hinges on the validity of an entire sequence of actions, requiring end-to-end evaluation: prior work often focuses on subtasks, such as graph edge prediction (Sakaguchi et al., 2021) or step ordering (Zhang et al., 2020; Lal et al., 2024; Anika and Miah, 2025). Finally, large-scale investigation requires accurate yet efficient metrics: string-overlap metrics like BLEU are fast to compute but inaccurate, and human annotation is accurate but expensive (Brahman et al., 2024). To fill these gaps, we introduce How2Everything, scalable framework to evaluate and train models for It comes with How2Mine, web-scale pipeline to mine and refine step-by-step procedure generation. procedures, which we use to create How2Bench for evaluation and How2Train for training. In addition, it introduces How2Score, an LLM-as-a-judge protocol to detect critical failures in model-generated steps, along with How2Judge, an open 8B judge that makes How2Score low-cost, efficient, and reproducible at scale. Our contributions are: Contribution 1: pipeline to collect realistic, diverse procedures at web-scale. Rather than drawing from narrowly scoped how-to websites, How2Mine can scale to arbitrarily large collections of web documents (Figure 1a). It ensures broad coverage by sampling from 14 different topics, as identified by WebOrganizer (Wettig et al., 2025). Using multiple stages of filtering and refining, we remove low-quality procedures and standardize format. We show the effectiveness of this pipeline by processing 980K web documents to derive 351K procedures (3). Contribution 2: An accurate, low-cost, and reproducible protocol to evaluate procedure generation. All existing protocols have limitations: efficient automatic metrics (e.g., perplexity on reference procedure, string overlap between reference and model generation) are unreliable Lyu et al. (2021); Li et al. (2023), human annotations are expensive and slow, and solely relying on LLM APIs as judge is not reproducible. Therefore, we establish How2Score, an evaluation protocol that checks whether generated procedure contains critical failure, meaning an omission, an extraneous action, or deviation that would prevent achieving the goal under the stated constraints (see Table 1 for examples). We first validate this protocol by assessing agreement between frontier LLM APIs as judges and human annotators on 200 examples, and note that, across 5 LLMs, agreement with human majority is consistently high, ranging from 76.5 to 83.0%. Then, we use labels from LLM APIs to distill How2Judge, compact, 8B model for repeatable and efficient evaluation. This model maintains high agreement with annotators (80.5%), enabling efficient assessment (4). Contribution 3: Mined procedures benchmark performance across large range of model sizes and capabilities. We reserve 7K of the 351K mined procedures for evaluation and create How2Bench. We find that How2Bench can meaningfully rank models trained on vastly different amounts of compute (from just 1021 FLOPse.g., 1B model trained on 200B tokensto the latest frontier models, such as GPT 5 OpenAI (2026)), and also compare base models against instruct variants. This is desirable property for benchmark, as it enables ranking models (Heineman et al., 2025) and establishing scaling laws (Xu et al., 2025) across compute budgets; however, many benchmarks either saturate early in training or target frontier models, having near-zero performance at smaller scales (e.g., Kazemi et al., 2025). In contrast, across training runs spanning 1B32B models, How2Bench shows clear scaling trends with both model size and training stage (Figure 1b; Table 8), 2 enabling the study of techniques to improve this key skill across the entire model training pipeline (5). Contribution 4: Training on procedures improves how-to generation with no out-of-domain regression. Beyond measuring performance, the same scalable components that make How2Everything cheap to evaluate also make it practical to optimize models on this task (Figure 1c). We use remaining samples in the 351K mined procedures as How2Train for training. Across three models, RL on How2Train with How2Score scored by How2Judge as reward consistently improves How2Everything performance by >10 points. Importantly, these improvements do not introduce regressionsand, in some cases even improveon standard post-training evaluation suite Olmo et al. (2025), suggesting that procedure generation is capability useful across tasks (6). Contribution 5: Improvements in procedure generation are not driven by format compliance or memorization. Prior work suggests that some LLM capabilitiessuch as those related to knowledgeare acquired through generalization over the course of the full training pipeline (Wang et al., 2025), while others can be exploited via reinforcement learning (Sun et al., 2025a). Our analysis reveals that, while RL boosts procedure generation capabilities, improvement in base models consistently yields better checkpoints after RL (7.1). Further, we show that although our evaluation examples are derived from the same web documents LLMs are pretrained on, risks due to memorization are limited. Even after aggressively contaminating data, performance on procedure generation only improves by modest amount: +3 points for 7B model (7.2). More broadly, this work offers worked example of how pretraining web data can support closed loop of capability evaluation and improvement at scale. The web provides virtually unbounded supply of open-ended, naturally occurring real-world documents that can serve as reference anchors when execution-based verification is infeasible. By mining and standardizing this data into an evaluable format, and by developing an evaluation protocol that targets task-level validity and can be made reliable and reproducible at scale, we turn an otherwise hard-to-measure behavior into practical development loop."
        },
        {
            "title": "2 Problem Setting and Related Work",
            "content": "In this work, we use procedure to refer to goal-conditioned sequence of actions. We distinguish between descriptive procedures, where model can generate textual representations of such sequences, and executable procedures, where correctness is determined by executioneither in grounded environments with explicit state transitions, such as formal transition systems (Samiei et al., 2025) or simulated environments (Puig et al., 2018; Shridhar et al., 2021), or through internally executed reasoning strategies for problem solving (Mao et al., 2024; Ruis et al., 2025). Our work focuses on real-world procedures, which fall under the first category. While model cannot actually file taxes or replace kitchen faucet, the ability to accurately represent the steps involved remains core user-facing capability and necessary prerequisite for downstream systems that aim to support or automate parts of real-world processes. Datasets for goal-conditioned procedures. Within the descriptive procedural setting, dataset construction has typically been constrained along two axes, limiting coverage of diverse real-world procedures. Many datasets are restricted by topical domain, with common choices including cooking Bień et al., 2020; Toyooka et al., 2025; Anika and Miah, 2025. Others are restricted by collection source, such as instructional platforms like WikiHow and Instructables (Zhou et al., 2022; Bolotova-Baranova et al., 2023; Brahman et al., 2024; Uzunoglu et al., 2024). Our work goes beyond these constraints by mining naturally occurring, goal-conditioned procedures across 14 topics from the web. Evaluation challenges for end-to-end procedural validity. Given these datasets, prior work has adopted range of task formulations to study procedural capabilities. Examples include edge prediction over step pairs (Sakaguchi et al., 2021), step reordering (Anika and Miah, 2025), QA (Lal et al., 2024; Uzunoglu et al., 2024), or constraint satisfaction (Yuan et al., 2023). In the setup where the task is pure generation (closest to ours), models directly generate step sequence given goal and are typically evaluated with perplexity or string-overlap metrics such as BLEU (Lyu et al., 2021; Li et al., 2023; Sakaguchi et al., 2021; Brahman et al., 2024). Prior work, however, acknowledges that these metrics are insufficient proxies for procedural validity and relies on human evaluation for more reliable signal (Lyu et al., 2021; Brahman et al., 2024). More recently, LLM-as-a-judge protocols have been used as general approach to scale evaluation for open-ended generation (Zheng et al., 2023; Dubois et al., 2025), but generic preference-style judging can overemphasize surface 3 Table 1 Given goal, list of resources, and target number of steps (inferred from reference doc), the model generates steps. How2Score evaluates the generation by checking whether it contains any critical failure using How2Judge (4.4). For each example, we show brief description of representative critical failure, highlighting relevant steps. More examples in G.1. Example (goal + resources + reference steps) Model-generated steps Goal: [Crime & Law] Sell your share of common property apartment by following the required legal procedure for notifying co-owners and transferring ownership. Resources: notary; notarial document; letter with list of contents; receipt. Reference: 1. Prepare notification to all co-owners stating the conditions of sale of your share. 2. Visit notary to draw up notarial document including all sale conditions. 3. Distribute the notarial document to all co-owners by letter with list of contents, obtaining receipt from each. 4. Wait 30 days for co-owners to express their desire to purchase your share. 5. Sell your share to third party. Generator: Gemini 2.5 Pro 1. Engage notary to draft formal notification of the intended sale for all co-owners. 2. Send the notification via letter with list of contents and keep the postal receipt as proof. 3. Instruct the notary to prepare the final sale agreement as notarial document. 4. Execute the transfer by signing the notarial document in the presence of the notary. 5. Receive the registered notarial document and the final transaction receipt from the notary. Critical failure Skips required prerequisite (the 30-day waiting period) before completing the transfer. Goal: [Food & Dining] Prepare spiced glazed nuts with crisp candy glaze using Crock Pot. Resources: Crock Pot; nuts; melted butter; powdered sugar; spices; racks; waxed paper. Reference: 1. Preheat the Crock Pot on high for 1520 minutes. 2. Add nuts to the Crock Pot. 3. Pour melted butter over the nuts and stir. 4. Add powdered sugar and stir until completely blended. 5. Cover and cook on high for 15 minutes. 6. Remove the lid, set the Crock Pot to low, and cook with the lid off for 23 hours, stirring occasionally until the nuts have crisp glaze. 7. Stir spices over the nuts and stir to distribute. 8. Pour nuts onto racks or waxed paper and let cool completely before serving or bagging. Generator: Qwen 3 8B Instruct 1. Place the nuts in the Crock Pot. 2. Add melted butter, powdered sugar, and spices to the Crock Pot. 3. Cook on low heat, stirring occasionally, until the glaze thickens. 4. Transfer the nuts to racks to cool and dry. 5. Cover the cooled nuts with waxed paper to maintain crispness. 6. Let the glaze set for several hours. 7. Ensure the nuts are completely dry and crisp. 8. Serve or use as desired. Leaves critical cooking times and lid/heat settings unspecified (e.g., high vs. low and lid on vs. off), making the procedure hard to execute reliably. qualities like coherence or helpfulness, and thus fail to capture end-to-end procedural validity. Together, these approaches highlight fundamental reliabilityscalability tradeoff, which we address by introducing validity-oriented evaluation protocol."
        },
        {
            "title": "3 How2Mine: Extracting Realistic Step-by-Step Procedures from",
            "content": "the Web To evaluate end-to-end procedural validity at scale, we mine goal-conditioned step-by-step procedures from large web corpus to ensure broad topical coverage. As proof of concept, we run How2Mine on 980,000 web documents to extract 351,162 structured procedure instances (Figure 2). The pipeline scales straightforwardly to larger corpora, making it possible to dynamically construct evaluation sets and training corpora without manual curation."
        },
        {
            "title": "3.1 Sampling Web Pages for Procedure Mining\nWe source candidate documents from the DCLM web corpus (Li et al., 2025). Because tutorial-style documents\ntend to have a high density of explicitly ordered, imperative steps, we restrict our document pool to those\nlabeled as Tutorial & How-to Guide by the WebOrganizer format classifier (Wettig et al., 2025).2 To ensure\nequal topical coverage, we apply the WebOrganizer topic classifier and perform stratified sampling across 14\ntopics.3 Our final pool of 351K procedure instances spans 189K unique domains (we report top 10 domains\nper topic in §B.1).",
            "content": "2How2Mine can be easily extended to extract valid procedures from other formats such as academic writing and knowledge articles (see B.2); for simplicity, we focus on single format. 3See weborganizer.allen.ai for definitions and examples. 4 Figure 2 Given sample of 980K topic-stratified web documents, How2Mine yields 351K procedure instances (goal + resources list + reference steps), and can be easily scaled to larger corpora."
        },
        {
            "title": "3.2 From Web Documents to Structured Procedures\nStarting from this topic-stratified pool of tutorial documents, we run a multi-stage pipeline to extract, filter,\nand post-process procedures. All LLM-based stages use GPT-4.1 OpenAI (2025), see prompts in §H.1. Using\nthe OpenAI batch API, running this pipeline over 980K documents issues 252K requests, costing 5,717 USD.",
            "content": "(1) Procedure extraction. Given candidate web document, we first use an LLM to identify whether it contains well-formed sequential procedure and, if so, extract the goal and an ordered list of steps. (2) Heuristics filter. We run simple heuristics-based checks to remove (i) candidates with fewer than 5 or more than 15 steps to avoid trivial or overly complex procedures, and (ii) those with high n-gram overlap within the extracted steps. See B.3 for implementation details. (3) LLM filter. We apply an LLM-based filter to exclude examples that (i) depend on specific named entities, (ii) are purely mathematical calculations, (iii) require interacting with UI elements, (iv) involve open-ended creative generation, (v) are non-sequential, or (vi) are unreasonable/nonsensical. These criteria are derived from multiple rounds of data inspection (see B.4 for the in-depth rationale). (4) Post-processing and resource extraction. For each remaining example, we rewrite the goal to be as specific and deterministic as possible, explicitly stating the required constraints and expected outcome. Because multiple distinct procedures can still satisfy goal, we additionally list the resources (if any) referenced by the steps in the reference procedure. See Table 1 for examples. Together, these edits narrow the space of valid solutions. (5) Final validation. Finally, we run an LLM-based sanity check to remove any remaining nonsensical or otherwise invalid procedures. Pipeline outputs. Each procedure instance is structured record that includes topic, goal, list of resources (possibly empty), and reference steps. B.5 shows one full example for each topic. From this pool, we construct How2Bench by sampling 500 instances per topic (7,000 total), and use the remaining instances as How2Train."
        },
        {
            "title": "Failures",
            "content": "Evaluating procedural generation comes with trade-off between scalability and reliability. Reference-overlap metrics are cheap but miscalibrated to procedural validity, while human evaluation is reliable but does not 5 scale (Li et al., 2023; Brahman et al., 2024). We introduce How2Score, an LLM-based evaluation protocol that asks whether generated procedure contains any critical failure that prevents achieving its goal. To make scoring efficient and reproducible, we distill frontier judge into How2Judge, an open 8B model, which achieves 80.5% agreement with human annotators."
        },
        {
            "title": "4.1 Defining Critical Failures in an Open-World Setting\nWe take inspiration from the framing commonly used in process reward models (PRMs) for mathematical\nreasoning (Lightman et al., 2023), where the earliest incorrect step identified by a verifier is treated as the\npoint of failure. In open-world procedures, however, steps are not directly executable, making it difficult to\nlocalize a “first failure” automatically. We therefore develop a working definition and codebook by qualitatively\ninspecting model outputs and iterating with human annotators.",
            "content": "Definition. We define critical failure as an omission, extraneous action, contradiction, severe vagueness, or other deviation from the reference that is severe enough to prevent achieving the goal, or to make the procedure unusable as instructions. We use the reference procedure as an anchor, but aim not to penalize alternative valid procedures or superficial differences. For example, if the goal is to make terracotta pot as gift, different gift message than the reference is not critical failure. While what constitutes critical is inherently subjective in this non-executable setting, this definition provides practical proxy. Table 1 shows representative critical failures; G.1 provides additional examples, including non-critical variations. Assumption of reference correctness. While rigorous filtering (3) reduces noise, some references can still contain errors, and How2Score may inherit this noise. As sanity check, we prompt GPT-4.1 to judge if each How2Bench reference procedure reasonably achieves the stated goal; it accepts 96.6% of examples as to make the task more deterministic and suitable for evaluation, not as valid. In our formulation, we use perfect ground-truth solution."
        },
        {
            "title": "4.2 Evaluation Protocol",
            "content": ", ˆS) (goal g, extracted resource list R, reference procedure Given an evaluation set of examples = (g, R, , and model-generated procedure ˆS), we use an LLM judge to identify critical failures. Each failure is and/or ˆS. We provide the full accompanied by description and references to the relevant steps in annotation codebook and examples of non-critical vs critical cases in the judge prompt in H.3. Binary score aggregation. From the judge output list, we derive binary label: we assign has_failure if at least one critical failure is identified, and no_failure otherwise. To report performance over D, we aggregate the binary labels into success rate (the fraction of examples labeled no_failure). Formally, Score(D) = 1 xD I[J(g, R, , ˆS) = no_failure]. where J() denotes the derived binary judgment, answering the question: Does this procedure contain any critical failure? Compared to checking for the first failure as in the math PRM setup, this aggregation yields higher inter-annotator agreement (see 4.3). We therefore adopt this formulation, which remains aligned with our downstream objective. For transparency, we still ask the judge to enumerate all identified failures."
        },
        {
            "title": "4.3 Validation via Human Annotations\nTo validate our definition of critical failures, we ask human annotators to list all critical failures they observe\nusing the evaluation protocol in §4.2. We recruit three annotators via Prolific to label 200 examples (pre-\nscreened to avoid procedures requiring specialized domain knowledge), paying an average hourly rate of 28\nUSD (total cost: 3,600 USD). See §C.1 for details.",
            "content": "Annotator training and pilot studies. In early pilots (300 annotations), many annotators either flagged any difference from the reference as critical, or overlooked indisputable failures masked by coherent surface form.4 As result, initial inter-annotator agreement was low (Krippendorffs α = 0.273). We iteratively refined the 4One example of such failure is when the procedure first says cut the wood board into 5 pieces of equal size, but later says to place the pieces on the table with the largest piece on top and the smallest piece on the bottom. This is clear inconsistency. 6 training materials and added more examples to clarify the boundary between non-critical variations and critical failures. For the final round, we screened annotators with short qualification test and selected the three who best demonstrated understanding to label 200 examples. Inter-annotator agreement. With binary score aggregation, we observe Krippendorffs α = 0.593. Given the non-executable, open-world setting and the existence of multiple valid procedures per goal, we do not expect near-perfect agreement; instead, we target metric that is stable for relative comparisons (5) and usable as an RL reward (6). If we instead require agreement on the location of the first failure (as in math PRMs), agreement drops (α = 0.307), motivating our use of binary aggregation. Evaluating LLM judges against human labels. We obtain annotations from various LLM judges on the same 200 examples used to obtain human annotations, and compute their percentage agreement with the human majority labels. As shown in Figure 3, GPT 5 has the highest overall agreement (83.0%) and is well-calibrated across classes (83.7% on human-majority has_failure cases; 82.4% on no_failure cases). To contextualize these results, we measure leave-one-out agreement among human annotators, which ranges from 84.7% to 88.5%. GPT 5s agreement falls within few percentage points of this range, suggesting performance comparable to individual annotators. Figure 3 Agreement between LLM judges and the human majority label on critical-failure detection (N=200), reported overall and stratified by the human-majority class (has_failure/no_failure). H.3 for the judge prompt; C.1 for annotation details."
        },
        {
            "title": "4.4 Distilling a Cost-Effective Judge\nWhile GPT 5 OpenAI (2026) is shown to have strong agreement with human labels in §4.3, evaluating\n7,000 examples with it costs around $15. We therefore use GPT 5 as a teacher judge and distill it into\nHow2Judge, a smaller Qwen 3 8B model for stable, low-cost large-scale evaluation. We collect 73K GPT\n5 annotations on outputs from a diverse set of generator models,5 and deduplicate to remove any overlap\nwith the human-annotated set in §4.3. We then finetune Qwen 3 8B on this dataset for three epochs. On the\nhuman-labeled set in §4.3, How2Judge achieves 90.5% agreement with GPT 5 and 80.5% agreement with\nthe human majority label. It is also relatively well-balanced across classes, with 79.6% human agreement\non has_failure and 81.4% on no_failure (Figure 3), making it a cost-effective alternative for large-scale",
            "content": "5These include three 1B checkpoints, four 7B checkpoints, three 32B checkpoints, and four closed-source models. 7 evaluation (5) and for serving as reward function for RL training (6). See more details on distillation in C.2."
        },
        {
            "title": "Generation",
            "content": "Equipped with How2Score and How2Judge, we run systematic evaluations on range of models. We create How2Bench by sampling 500 procedures per topic from the data created in 3, totaling 7,000 examples. Figure 4 How2Bench results on selected models. We report How2Score computed with How2Judge along with the average generated tokens for each model. The average reference length is 97.44 tokens. For open models, Base denotes the final non-post-trained checkpoint, and Instruct denotes the post-trained checkpoint. 5."
        },
        {
            "title": "Inference Setup",
            "content": "At inference time, the model receives the goal g, resource list R, and required step count = , and is asked to output procedure ˆS with exactly steps. While conditioning generations on and may not reflect real-world usage, it is an evaluation control to reduce degrees of freedom and improve comparability across model outputs. We enforce length control by requiring each step to be single, concise sentence containing one main action, and asking the model to closely follow the concision level in the provided examples. See setup details in D.1 and prompts in H.2."
        },
        {
            "title": "5.2 Evaluation Results and Analysis\nWe use How2Score with How2Judge (§4.4) to evaluate a range of open and closed models, and report\nthe main results in Figure 1b and Figure 4. Performance scales with model size and training stage, and we\nobserve a noticeable gap between open and closed models.",
            "content": "No evidence of LLM judge self-preference bias. common concern with LLM-as-a-judge evaluation is self-preference: judge might favor outputs from models in its own family (Zheng et al., 2023). We collect outputs from models in the GPT, Gemini, and Claude families and recompute How2Score using models 8 from these families as judges. As shown in Figure 5, although absolute values vary, the relative model ranking is unchanged across different judges. Figure 5 Cross-judge robustness check for self-preference bias on closed models spanning the GPT, Gemini, and Claude families: we rescore the same generations with four judges (How2Judge, GPT 5, Gemini 2.5 Pro, Claude 4.5 Opus) and find the ranking is unchanged. How2Bench shows clear scaling behavior across model sizes and training stages. Besides comparing fully trained base and instruct models, we also measure performance at intermediate steps of model training pipeline using pretraining, midtraining, and post-training Olmo checkpoints (OLMo et al., 2025; Olmo et al., 2025). 1b shows that across five Olmo training runs (Olmo 2: 1B/7B/32B; Olmo 3: 7B/32B),6 How2Bench exhibits smooth scaling across both model size and training stage, with consistent ordering of model performance. We also observe the emergence of non-trivial performance by about 5% into pretraining for 1B model (around 1021 training FLOPs), after which performance continues to improve. The sensitivity of How2Bench to gains from all training indicates that it probes core, general capability, making it well-suited for performance forecasting (Xu et al., 2025). Early emergence of procedural formatting. We find that surface-level procedural formatting stabilizes early in training, particularly for larger models. We track simple formatting proxies across checkpoints: step-count mismatch relative to the reference, duplicate-step frequency, and n-gram repetition. Over five Olmo runs, formatting errors drop during early pretraining and quickly plateau while How2Score continues to improve. This decoupling resembles an emergence-like pattern: surface-formatting behavior stabilizes early, while procedural validity keeps improving. Thus, the continued gains we observe later in training are unlikely to be driven primarily by correcting surface-formatting errors, and instead reflect improvements in end-to-end procedural validity (details in D.3). How2Score is not simply reducible to reference-step likelihood. To test whether our task is simply reducible to perplexity over the reference, we compare checkpoint ordering under How2Everything to checkpoint ordering by conditional perplexity on the reference steps. Across five Olmo runs,7 the Spearman correlation between checkpoint rank by How2Everything and rank by perplexity ranges from 0.233 (Olmo 2 32B) to 0.967 (Olmo 2 1B),8 indicating that How2Score is not simply measuring conditional likelihood of the reference procedure (full results in D.4). 6Olmo checkpoints corresponding to midtraining stage are labeled stage 2 pretraining; see Table 8 for exact IDs. 7We use 9 checkpoints per run: 8 stage-1 pretraining checkpoints plus the stage-2 midtrained checkpoint. 8With intermediate values 0.667 (Olmo 2 7B), 0.867 (Olmo 3 7B), and 0.483 (Olmo 3 32B). 9 Controlling for topic, required step count is monotonic difficulty knob. To better interpret aggregate scores and enable difficulty-controlled slices of How2Bench, we examine simple instance properties that is the dominant predictor across models: correlate with How2Score. We find that reference step count procedures requiring more steps are consistently harder, making simple, monotonic difficulty knob (details in D.5). Qualitative examples of common failure patterns. To study which non-formatting failures occur, we perform small-scale qualitative analysis over model generations. While we occasionally observe refusals (primarily in frontier models), most errors fall into the following types: critical omissions of required actions; missing parameters (e.g., times, quantities, temperatures) that make steps non-executable; wrong values for critical parameters; unsafe or invalid actions; and internal contradictions across steps. See G.1 for more details on this analysis."
        },
        {
            "title": "6 Improving Step-by-Step Procedure Generation with RL",
            "content": "Beyond serving as an evaluation protocol, How2Score and How2Judge can also be used as practical RL reward for improving goal-conditioned step-by-step procedure generation, with gains that persist under external judges and without systematic regressions on standard out-of-domain benchmarks. These results suggest How2Everything provides practical framework for both evaluating and improving goal-conditioned step-by-step procedure generation, and that How2Score-based RL can complement existing post-training pipelines as an additional optimization target. Table 2 Results before and after RL with How2Score as reward (step 1000). We report performance on How2Bench (in-domain) and 12 standard out-of-domain benchmarks. columns show changes relative to the original checkpoint; OOD is the mean out-of-domain change. Green indicates positive change and pink shows negative change. Qwen3-4B-Inst Qwen3-8B-Inst Olmo-3-7B-Think"
        },
        {
            "title": "Base",
            "content": "+RL Base +RL Base +RL In-domain How2Bench 30.29 43.52 +13.23 38.52 48.62 +10.10 27.30 37.89 +10. Out-of-domain MMLU-Pro 60.16 61.70 +1.54 62.16 63.11 +0.95 44. 49.61 +5."
        },
        {
            "title": "GPQA",
            "content": "44.87 44.64 -0.23 54.02 53.79 -0. 46.21 47.10 +0."
        },
        {
            "title": "ZebraLogic",
            "content": "82.4 81.2 -1.2 85.2 85.7 +0. 65.6 63.3 -2."
        },
        {
            "title": "AlpacaEval",
            "content": "44.78 47.73 +2.95 58.44 58.76 +0.32 49.75 51.19 +1. HumanEval+ 71.95 75.43 +3.48 81.28 79.57 -1. 90.49 89."
        },
        {
            "title": "LiveCodeBench",
            "content": "85.6 85.38 -0.22 86.32 86.11 -0. 74.85 72.40 -1.04 -2.45 -0.52 MBPP+ GSM8K"
        },
        {
            "title": "Omega",
            "content": "AIME24 AIME25 OOD 67.46 66.98 -0. 68.65 69.31 +0.66 64.81 64.29 94.09 93. -0.31 95.68 95.30 -0.38 94.92 95.30 +0. 90.38 90.45 +0.07 91.20 91.92 +0.72 94.44 94.62 +0. 42.2 39.4 -2.8 44.4 44.4 0. 44.6 47.0 +2.4 60.42 60.42 0. 61.15 59.06 -2.09 55.52 58.65 +3.13 46. 49.48 +3.44 47.29 49.48 +2.19 38.54 43.96 +5.42 +0. +0.06 +1."
        },
        {
            "title": "6.1 Training Setup\nWe create a training set by sampling 100K examples from How2Train, balanced across 14 topics and with\nlow semantic similarity to How2Bench instances (see §E.1).",
            "content": "For SFT, we fine-tune base and instruction-tuned checkpoints of Qwen 3 4B and 8B Qwen Team (2025a), and OLMo 3 7B Olmo et al. (2025) for one epoch. For RL, we train Qwen 3 4B Instruct and 8B Instruct Qwen Team (2025a), and OLMo 3 7B Think Olmo et al. (2025),9 using Group Relative Policy Optimization (GRPO) (Shao et al., 2024) for 1000 optimizer steps with three rewards: (i) How2Score computed by How2Judge, (ii) step-format verifier, and (iii) reference-calibrated length reward to prevent length gaming. See E.2 for full details."
        },
        {
            "title": "6.2 Results and Analysis\nLength control effectively prevents length gaming in RL. With the reference-calibrated length reward,\ngenerations stay close to the reference length (∣gen∣/∣ref∣ ≈ 1.0). Without it, models inflate length (up to\n1.34×–1.53× the reference) and achieve large apparent How2Bench gains consistent with length gaming\n(details in §E.4). This controls a major confound in LLM-as-judge settings where judges are prone to verbosity\nbias (§5.1).",
            "content": "RL improvements persist under external judges. To test whether these gains are specific to How2Judge (which was used to compute How2Score during training), we re-evaluate the same RL-trained model generations with external judges (GPT 5 and Gemini 2.5 Pro), and find the gains persist. See E.5 for details. RL improves How2Bench performance without systematic out-of-domain degradation. Table 2 shows that RL-trained models improve How2Bench while retaining performance on standard out-of-domain evaluations: changes are mixed but generally modest (improving some benchmarks while regressing on others), with no evidence of systematic degradation. This suggests that our How2Score-based reward can complement existing post-training pipelines as an additional RL signal. The out-of-domain suite spans knowledge, chat, math, code, and logical reasoning, see E.6 for details. Additional SFT stage yields limited gains on instruct checkpoints. We find that SFT can yield small gains when applied to base model checkpoints, but does not improve instruction-tuned checkpoints (see E.3). One plausible explanation is objective mismatch: SFT maximizes likelihood of single reference text per goal, which need not align with minimizing critical failures under How2Score (Stiennon et al., 2022; Xie et al., 2025)."
        },
        {
            "title": "7 Robustness to Format and Memorization Confounds",
            "content": "Because How2Bench is scored with an LLM judge and our evaluation examples are mined from the web, improvements in How2Score could plausibly arise from two confounding factors: (1) better compliance with an implicit task format, and (2) source-document memorization. We run targeted analyses to stress-test each explanation, and find evidence that neither can account for the observed gains."
        },
        {
            "title": "7.1 Confound 1: Implicit Task Format Compliance\nTo test whether gains from post-training can be explained by learning an implicit task format, we use two\ncomplementary diagnostics. Pretraining maturity axis: we hold the post-training recipe fixed and vary the\npretraining checkpoint. If gains were primarily format-level, they should be similarly recoverable from weaker\ncheckpoints. Data coverage axis: we hold the base model fixed and run RL using topic-restricted training\ndata. If gains primarily come from format, they should transfer broadly across topics, largely independent of\nwhich topics appear in training.",
            "content": "Diagnostic 1: Gains require pretraining maturity. We apply the same post-training recipe (SFT followed by RL) starting from varying intermediate Olmo 3 7B pretraining checkpoints. Figure 6 shows that SFT gains 9Thinking mode refers to the presence of explicit intermediate reasoning. Qwen models integrate instruction-following and reasoning in single checkpoint, whereas OLMo provides separate Instruct and Think checkpoints. 11 Figure 6 Post-training from different Olmo 3 7B pretraining checkpoints (x-axis). RL gains grow substantially at later checkpoints, while SFT yields modest improvements. are similar across checkpoints (3.39 to 5.36),10 while RL gains increase with pretraining FLOPs (3.56 at 10% to 20.33 at 100%), accounting for most of the improvement at late checkpoints. This pattern aligns with prior findings that SFT mainly shapes surface-level behavior, while RL amplifies pretrained capabilities (Ouyang et al., 2022; Zhao et al., 2025), suggesting that How2Bench is not primarily format-driven. Diagnostic 2: Gains depend on data topic coverage. Next, we test whether improvements depend on broad topic coverage or can be obtained by learning generic output format from narrow topic. Via embedding analysis, we select two topics with contrasting diversity: Science, Math & Technology, which is broadly dispersed in embedding space, and Food & Dining, which forms specialized cluster. We run RL on Qwen 3 8B and find that training on all topics yields the best overall performance (+10.10), while science-only RL generalizes strongly to many other topics (+9.41 overall); in contrast, dining-only RL still transfers but more weakly (+5.55 overall). Full results in E.7. Together, these results suggest that RL trained on single topic can transfer, but broad topic coverage yields the largest gains, consistent with improvements driven by content coverage rather than generic output format."
        },
        {
            "title": "7.2 Confound 2: Memorization of Source Documents\nTo probe memorization effects, we vary how often a fixed set of source documents appears during midtraining\nand measure How2Score on procedure instances extracted from those documents.",
            "content": "Midtraining. We focus on midtraining, stage where benchmark leakage can spuriously boost task scores (Olmo et al., 2025). Starting from the final pretraining checkpoints of Olmo 3 7B and 32B, we midtrain for 10B tokens while controlling for the exposure frequency of the documents (0, 1, 3, 6, or 10 occurrences). The 0-occurrence control serves as baseline where the target documents are not seen. Memorizing source documents yields limited gains on How2Score. We run our pipeline on the midtraining documents to create an evaluation set of 13,500 examples, evenly balanced across the five occurrence groups. Table 3 shows How2Score of our midtrained models on this evaluation set. As exposure increases, document perplexity drops sharply (for Olmo 3 7B: 10.4 1.4; for Olmo 3 32B: 8.0 1.2), indicating substantially higher fit to the source documents. By contrast, How2Score improves only modestly and non-monotonically (peaking at +3.3 for 7B and +6.1 for 32B), suggesting that improvements in How2Score are not explained 10Unlike 6.2, the SFT step yields additional performance improvement over RL alone when starting from base models. 12 Table 3 Midtraining memorization sensitivity. As document occurrence increases, perplexity drops sharply, while How2Score only improves modestly and non-monotonically."
        },
        {
            "title": "Model",
            "content": "Metric"
        },
        {
            "title": "Doc occurrences during midtraining",
            "content": "Olmo 3 7B Olmo 3 32B"
        },
        {
            "title": "1\nDoc perplexity 10.4 8.5\nHow2Score",
            "content": "3 10 1.4 6.1 14.0 17.3 15.8 15.7 16.5 6 3.0 Doc perplexity 8.0 How2Score 1.2 3.5 33.3 39.3 39.4 38.1 37.9 6.0 1. simply by repeatedly seeing the underlying source documents."
        },
        {
            "title": "Impact Statement",
            "content": "We introduce How2Everything, framework for mining and evaluating goal-conditioned step-by-step procedures from large-scale web corpora. Since step-by-step procedure generation is an important and commonly used capability of LLMs, providing measurement and data for improving it has practical and broad positive impacts. Our work enables more reliable evaluation of procedural instruction quality at scale, and provides practical reward signal for improving models end-to-end procedural validity. Altogether, it could benefit user-facing assistants in domains such as troubleshooting, education, and everyday planning. Risks and negative societal impact. Because our data are derived from web documents, the extracted procedures may reflect societal biases present online. In addition, how-to instructions can be safety-sensitive (e.g., health, legal, chemicals), and misuse could enable harmful behavior if models are trained to generate unsafe instructions. Mitigations. How2Score is an evaluation proxy rather than guarantee of real-world correctness; it is not substitute for expert review or execution-based verification in safety-critical settings. Prior to release, we will apply additional safety and privacy filtering (e.g., removing procedures that involve regulated or high-risk activities and removing personally identifiable information where present) and provide documentation describing intended use and known limitations. We will release the benchmark split, prompts, and distilled judge weights to support reproducible evaluation without requiring access to proprietary judge models."
        },
        {
            "title": "References",
            "content": "A. Anika and M. M. M. Miah. Evaluating llms reasoning over ordered procedural steps, 2025. URL https: //arxiv.org/abs/2511.04688. M. Bień, M. Gilski, M. Maciejewska, W. Taisner, D. Wisniewski, and A. Lawrynowicz. RecipeNLG: cooking recipes dataset for semi-structured text generation. In B. Davis, Y. Graham, J. Kelleher, and Y. Sripada, editors, Proceedings of the 13th International Conference on Natural Language Generation, pages 2228, Dublin, Ireland, Dec. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.inlg-1.4. URL https://aclanthology.org/2020. inlg-1.4/. V. Bolotova-Baranova, V. Blinov, S. Filippova, F. Scholer, and M. Sanderson. WikiHowQA: comprehensive benchmark for multi-document non-factoid question answering. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 52915314, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.290. URL https://aclanthology.org/2023.acl-long.290/. F. Brahman, C. Bhagavatula, V. Pyatkin, J. D. Hwang, X. L. Li, H. J. Arai, S. Sanyal, K. Sakaguchi, X. Ren, and Y. Choi. Plasma: Making small language models better procedural knowledge models for (counterfactual) planning, 2024. URL https://arxiv.org/abs/2305.19472. A. Chatterji, T. Cunningham, D. J. Deming, Z. Hitzig, C. Ong, C. Y. Shan, and K. Wadman. How people use chatgpt. Technical report, National Bureau of Economic Research, 2025. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators, 2025. URL https://arxiv.org/abs/2404.04475. Google. Gemini 2.5 Pro model card, 2025. URL https://modelcards.withgoogle.com/assets/documents/gemini-2. 5-pro.pdf. Accessed: 2026-01-29. Y. Gu, O. Tafjord, B. Kuehl, D. Haddad, J. Dodge, and H. Hajishirzi. Olmes: standard for language model evaluations, 2025. URL https://arxiv.org/abs/2406.08446. D. Heineman, V. Hofmann, I. Magnusson, Y. Gu, N. A. Smith, H. Hajishirzi, K. Lo, and J. Dodge. Signal and noise: framework for reducing uncertainty in language model evaluation, 2025. URL https://arxiv.org/abs/2508.13144. N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/ 2403.07974. M. Kazemi, B. Fatemi, H. Bansal, J. Palowitch, C. Anastasiou, S. V. Mehta, L. K. Jain, V. Aglietti, D. Jindal, P. Chen, N. Dikkala, G. Tyen, X. Liu, U. Shalit, S. Chiappa, K. Olszewska, Y. Tay, V. Q. Tran, Q. V. Le, and O. Firat. Big-bench extra hard, 2025. URL https://arxiv.org/abs/2502.19187. Y. K. Lal, V. Cohen, N. Chambers, N. Balasubramanian, and R. Mooney. CaT-bench: Benchmarking language model understanding of causal and temporal dependencies in plans. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1933619354, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1077. URL https://aclanthology.org/2024.emnlp-main.1077/. A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. J. Li, A. Fang, G. Smyrnis, M. Ivgi, M. Jordan, S. Gadre, H. Bansal, E. Guha, S. Keh, K. Arora, S. Garg, R. Xin, N. Muennighoff, R. Heckel, J. Mercat, M. Chen, S. Gururangan, M. Wortsman, A. Albalak, Y. Bitton, M. Nezhurina, A. Abbas, C.-Y. Hsieh, D. Ghosh, J. Gardner, M. Kilian, H. Zhang, R. Shao, S. Pratt, S. Sanyal, G. Ilharco, G. Daras, K. Marathe, A. Gokaslan, J. Zhang, K. Chandu, T. Nguyen, I. Vasiljevic, S. Kakade, S. Song, S. Sanghavi, F. Faghri, S. Oh, L. Zettlemoyer, K. Lo, A. El-Nouby, H. Pouransari, A. Toshev, S. Wang, D. Groeneveld, L. Soldaini, P. W. Koh, J. Jitsev, T. Kollar, A. G. Dimakis, Y. Carmon, A. Dave, L. Schmidt, and V. Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2025. URL https://arxiv.org/abs/2406.11794. 14 X. Li, Y. Cao, M. Chen, and A. Sun. Take break in the middle: Investigating subgoals towards hierarchical script generation. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1012910147, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.644. URL https://aclanthology.org/2023.findings-acl.644/. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. B. Y. Lin, R. L. Bras, K. Richardson, A. Sabharwal, R. Poovendran, P. Clark, and Y. Choi. Zebralogic: On the scaling limits of llms for logical reasoning, 2025. URL https://arxiv.org/abs/2502.01100. J. Liu, C. S. Xia, Y. Wang, and L. Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023. URL https://arxiv.org/abs/2305.01210. Q. Lyu, L. Zhang, and C. Callison-Burch. Goal-oriented script construction. In A. Belz, A. Fan, E. Reiter, and Y. Sripada, editors, Proceedings of the 14th International Conference on Natural Language Generation, pages 184200, Aberdeen, Scotland, UK, Aug. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.inlg-1.19. URL https://aclanthology.org/2021.inlg-1.19/. Y. Mao, Y. Kim, and Y. Zhou. Champ: competition-level dataset for fine-grained analyses of llms mathematical reasoning capabilities, 2024. URL https://arxiv.org/abs/2401.06961. T. Olmo, :, A. Ettinger, A. Bertsch, B. Kuehl, D. Graham, D. Heineman, D. Groeneveld, F. Brahman, F. Timbers, H. Ivison, J. Morrison, J. Poznanski, K. Lo, L. Soldaini, M. Jordan, M. Chen, M. Noukhovitch, N. Lambert, P. Walsh, P. Dasigi, R. Berry, S. Malik, S. Shah, S. Geng, S. Arora, S. Gupta, T. Anderson, T. Xiao, T. Murray, T. Romero, V. Graf, A. Asai, A. Bhagia, A. Wettig, A. Liu, A. Rangapur, C. Anastasiades, C. Huang, D. Schwenk, H. Trivedi, I. Magnusson, J. Lochner, J. Liu, L. J. V. Miranda, M. Sap, M. Morgan, M. Schmitz, M. Guerquin, M. Wilson, R. Huff, R. L. Bras, R. Xin, R. Shao, S. Skjonsberg, S. Z. Shen, S. S. Li, T. Wilde, V. Pyatkin, W. Merrill, Y. Chang, Y. Gu, Z. Zeng, A. Sabharwal, L. Zettlemoyer, P. W. Koh, A. Farhadi, N. A. Smith, and H. Hajishirzi. Olmo 3, 2025. URL https://arxiv.org/abs/2512.13961. T. OLMo, P. Walsh, L. Soldaini, D. Groeneveld, K. Lo, S. Arora, A. Bhagia, Y. Gu, S. Huang, M. Jordan, N. Lambert, D. Schwenk, O. Tafjord, T. Anderson, D. Atkinson, F. Brahman, C. Clark, P. Dasigi, N. Dziri, A. Ettinger, M. Guerquin, D. Heineman, H. Ivison, P. W. Koh, J. Liu, S. Malik, W. Merrill, L. J. V. Miranda, J. Morrison, T. Murray, C. Nam, J. Poznanski, V. Pyatkin, A. Rangapur, M. Schmitz, S. Skjonsberg, D. Wadden, C. Wilhelm, M. Wilson, L. Zettlemoyer, A. Farhadi, N. A. Smith, and H. Hajishirzi. 2 olmo 2 furious, 2025. URL https: //arxiv.org/abs/2501.00656. OpenAI. Introducing GPT-4.1 in the API, 2025. URL https://openai.com/index/gpt-4-1/. Accessed: 2026-01-29. OpenAI. GPT-5 system card, 2026. URL https://arxiv.org/abs/2601.03267. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203. 02155. X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating household activities via programs, 2018. URL https://arxiv.org/abs/1806.07011. Qwen Team. Qwen2.5 technical report, 2024. URL https://arxiv.org/abs/2412.15115. Qwen Team. Qwen3 technical report, 2025a. URL https://arxiv.org/abs/2505.09388. Qwen Team. Qwen3-Embedding-0.6B model card, 2025b. URL https://huggingface.co/Qwen/Qwen3-Embedding-0.6B. Accessed: 2026-01-29. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=Ti67584b98. L. Ruis, M. Mozes, J. Bae, S. R. Kamalakara, D. Talupuru, A. Locatelli, R. Kirk, T. Rocktäschel, E. Grefenstette, and M. Bartolo. Procedural knowledge in pretraining drives reasoning in large language models, 2025. URL https://arxiv.org/abs/2411.12580. K. Sakaguchi, C. Bhagavatula, R. L. Bras, N. Tandon, P. Clark, and Y. Choi. proscript: Partially ordered scripts generation via pre-trained language models, 2021. URL https://arxiv.org/abs/2104.08251. 15 M. Samiei, M. Mansouri, and M. S. Baghshah. The illusion of procedural reasoning: Measuring long-horizon fsm execution in llms, 2025. URL https://arxiv.org/abs/2511.14777. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. M. Shridhar, X. Yuan, M.-A. Côté, Y. Bisk, A. Trischler, and M. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning, 2021. URL https://arxiv.org/abs/2010.03768. N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. Christiano. Learning to summarize from human feedback, 2022. URL https://arxiv.org/abs/2009.01325. Y. Sun, Y. Cao, P. Huang, H. Bai, H. Hajishirzi, N. Dziri, and D. Song. Rl grokking recipe: How does rl unlock and transfer new algorithms in llms?, 2025a. URL https://arxiv.org/abs/2509.21016. Y. Sun, S. Hu, G. Zhou, K. Zheng, H. Hajishirzi, N. Dziri, and D. Song. Omega: Can llms reason outside the box in math? evaluating exploratory, compositional, and transformative generalization, 2025b. URL https: //arxiv.org/abs/2506.18880. M. Toyooka, K. Aizawa, and Y. Yamakata. highly clean recipe dataset with ingredient states annotation for state probing task, 2025. URL https://arxiv.org/abs/2507.17232. A. Uzunoglu, A. R. Safa, and G. G. Şahin. Paradise: Evaluating implicit planning skills of language models with procedural warnings and tips dataset, 2024. URL https://arxiv.org/abs/2403.03167. X. Wang, A. Antoniades, Y. Elazar, A. Amayuelas, A. Albalak, K. Zhang, and W. Y. Wang. Generalization v.s. memorization: Tracing language models capabilities back to pretraining data, 2025. URL https://arxiv.org/abs/ 2407.14985. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. URL https://arxiv.org/abs/2406.01574. A. Wettig, K. Lo, S. Min, H. Hajishirzi, D. Chen, and L. Soldaini. Organize the web: Constructing domains enhances pre-training data curation. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=boSqwdvJVC. Z. Xie, J. Chen, L. Chen, W. Mao, J. Xu, and L. Kong. Teaching language models to critique via reinforcement learning, 2025. URL https://arxiv.org/abs/2502.03492. C. Xu, K. Chen, X. Li, K. Shen, and C. Li. Unveiling downstream performance scaling of llms: clustering-based perspective, 2025. URL https://arxiv.org/abs/2502.17262. S. Yuan, J. Chen, Z. Fu, X. Ge, S. Shah, C. Jankowski, Y. Xiao, and D. Yang. Distilling script knowledge from large language models for constrained language planning. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 43034325, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.236. URL https://aclanthology.org/2023.acl-long.236/. L. Zhang, Q. Lyu, and C. Callison-Burch. Reasoning about goals, steps, and temporal ordering with WikiHow. In B. Webber, T. Cohn, Y. He, and Y. Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46304639, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.374. URL https://aclanthology.org/2020.emnlp-main.374/. R. Zhao, A. Meterez, S. Kakade, C. Pehlevan, S. Jelassi, and E. Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining, 2025. URL https://arxiv.org/abs/2504.07912. W. Zhao, X. Ren, J. Hessel, C. Cardie, Y. Choi, and Y. Deng. Wildchat: 1m chatgpt interaction logs in the wild, 2024. URL https://arxiv.org/abs/2405.01470. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https: //arxiv.org/abs/2306.05685. L. Zheng, W.-L. Chiang, Y. Sheng, T. Li, S. Zhuang, Z. Wu, Y. Zhuang, Z. Li, Z. Lin, E. P. Xing, J. E. Gonzalez, I. Stoica, and H. Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset, 2024. URL https: //arxiv.org/abs/2309.11998. 16 S. Zhou, L. Zhang, Y. Yang, Q. Lyu, P. Yin, C. Callison-Burch, and G. Neubig. Show me more details: Discovering hierarchies of procedures from semi-structured web data. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 29983012, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.214. URL https://aclanthology.org/2022.acl-long.214/."
        },
        {
            "title": "A Motivational analysis of query type distribution",
            "content": "Chatterji et al. (2025) reports that How-To Advice accounts for approximately 8.5% of conversations for ChatGPT usage, ranking 4th among 23 fine-grained categories. How-To Advice comes right behind Specific Info (18.3%), Edit or Critique Provided Text (10.6%), and Tutoring or Teaching (10.2%). At current ChatGPT scale, this corresponds to tens to hundreds of millions of how-to interactions daily. We also apply the same query-type classifier to two publicly available corpora, WildChat-4.8M (Zhao et al., 2024) and LMSYS-Chat-1M (Zheng et al., 2024), but observe systematic skews in query-type distributions relative to ChatGPT, consistent with these corpora being collected from unrestricted public LLM endpoints rather than from deployed product. See https://huggingface.co/datasets/how2everything/WildChat-4.8M and https://huggingface.co/datasets/how2everything/lmsys-chat-1m for the labeled datasets. Accordingly, we anchor our discussion of real-world LLM usage to the ChatGPT distribution and use the open datasets primarily to contextualize results under alternative user behavior regimes. See Table 4 for summary of the query type distribution across ChatGPT, LMSYS, and WildChat. Table 4 Query type distribution across chat sources (percent of conversations). OpenAI numbers are taken from Chatterji et al. (2025); LMSYS and WildChat are computed by applying the same classifier rubric used in the OpenAI report to LMSYS-Chat-1M (Zheng et al., 2024) and WildChat-4.8M (Zhao et al., 2024). We visually emphasize the OpenAI (commercial) column."
        },
        {
            "title": "Personal Writing or Communication",
            "content": "Health, Fitness, Beauty or Self Care"
        },
        {
            "title": "Create an Image",
            "content": "Other / Unknown"
        },
        {
            "title": "Asking About the Model",
            "content": "18 7.0% 1.1% 6.1% 6.4% 4.0% 3.4% 5.2% 8.8% 13.2% OpenAI LMSYS WildChat 18.3% 13.5% 8.7% 10.6% 6.5% 10.2% 7.1% 8.5% 5.5% 8.0% 7.4% 5.7% 1.4% 4.5% 1.4% 4.2% 13.2% 4.2% 0.6% 4.1% 3.1% 3.9% 3.1% 3.6% 7.5% 3.0% 3.0% 2.1% 0.8% 2.0% 5.8% 1.9% 1.5% 1.4% 7.1% 1.1% 0.2% 0.9% 0.6% 0.6% 0.2% 0.4% 1.9% 0.4% 3.8% 0.4% 4.7% 0.7% 8.1% 1.5% 5.5% 0.8% 0.5% 0.1% 4.7% 6.0% 0.2% 1.1% 1.7% 5.2%"
        },
        {
            "title": "B Details on data pipeline",
            "content": "This section provides details related to the data. Prompts used for the data pipeline are provided in subsection H.1. B.1 Top frequent URL domains See Table 5 for the top 10 most frequent URL domains within each topic in our final pool of 351K procedure instances. Counts are the number of procedure instances in the given topic whose source URL belongs to the domain. Table 5 Top 10 most frequent URL domains per topic in our final pool of 351K procedure instances. Rank Domain Art & Design 1 instructables.com 3 4 5 6 7 9 creativelive.com steves-digicams.com shutterbug.com dummies.com wikihow.com picturecorrect.com digital-photography-school.com snapshot.canon-asia.com 10 photography.tutsplus.com Crime & Law wikihow.com 2 3 4 5 7 8 9 legalbeagle.com policeone.com policemag.com patternlanguagenetwork.org info.legalzoom.com avvo.com americanbar.org nolo.com insidecounsel.com Education & Jobs 1 2 3 5 6 7 8 9 betterlesson.com wikihow.com englishlessonplanner.com classroom.synonym.com work.chron.com auburn.edu education.com slideplayer.com prezi.com 10 brighthubeducation.com Electronics & Hardware 2 3 4 5 6 instructables.com wikihow.com lifehacker.com ecmweb.com dummies.com hackaday.com"
        },
        {
            "title": "Count",
            "content": "237 212 126 97 95 73 68 67 67 414 159 88 79 77 76 60 54 201 128 124 95 85 82 64 52 1348 121 114 76 73 Continued on next page."
        },
        {
            "title": "Rank Domain",
            "content": "7 8 9 crutchfield.com hackaday.io itstillworks.com 10 lifewire.com Fashion & Beauty wikihow.com 1 2 4 5 6 7 8 leaf.tv reference.com oureverydaylife.com instructables.com naturallycurly.com allure.com popsugar.com becomegorgeous.com 10 cosmopolitan.com Food & Dining Count 49 49 49 753 437 260 229 186 181 171 136 recipe-finder.com 1065 1 2 4 5 6 7 8 food.com ifood.tv dlife.com foodandwine.com instructables.com vegweb.com seriouseats.com relish.com 10 washoku.guide Health 1 wikihow.com 2 3 4 5 6 8 9 healthyliving.azcentral.com infobarrel.com slideplayer.com livestrong.com dummies.com lifehacker.com futurelearn.com leaf.tv 10 hubpages.com Home & Hobbies 1 2 3 4 6 7 8 9 instructables.com homeguides.sfgate.com wikihow.com hunker.com reference.com homesteady.com ehow.com doityourself.com thespruce.com 10 lifehacker.com Industrial 1 ecmweb.com 3 forconstructionpros.com ptonline.com 817 569 313 244 242 241 234 370 87 85 82 63 44 44 44 42 1083 359 291 216 199 183 169 160 280 195 138 Continued on next page."
        },
        {
            "title": "Rank Domain",
            "content": "4 5 6 7 8 thefabricator.com machinerylubrication.com screenweb.com beefmagazine.com instructables.com weldingtipsandtricks.com 10 wikihow.com"
        },
        {
            "title": "Religion",
            "content": "1 2 3 4 5 7 8 9 lds.org uua.org wikihow.com spellsofmagic.com classroom.synonym.com bible.org orthodoxsundayschool.org childrensministry.com ssnet.org 10 teachonereachone.org Science, Math & Technology 1 2 4 5 6 7 8 education.com slideplayer.com instructables.com getrevising.co.uk nrich.maths.org openwetware.org betterlesson.com sciencebuddies.org ck12.org 10 prezi.com Sports & Fitness 1 2 3 4 5 7 8 9 healthyliving.azcentral.com wikihow.com t-nation.com bodybuilding.com active.com woman.thenest.com livehealthy.chron.com runnersworld.com howcast.com 10 mensfitness.com Transportation hotrod.com 1 2 4 5 6 7 8 itstillruns.com wikihow.com instructables.com superchevy.com ixigo.com reference.com popularmechanics.com auto.howstuffworks.com 10 aopa.org Travel & Tourism Count 116 110 106 104 103 610 146 131 117 105 79 78 77 59 361 284 208 192 167 159 118 92 360 329 287 231 207 178 176 163 260 219 204 184 140 104 100 96 87 Continued on next page. Source format Tutorial & how-to guide Personal blog Knowledge articles Non-fiction writing Q&A forum Academic writing # docs After extraction (%) After heuristics (%) After LLM filter (%) 24.46% 140,000 70.46% 86.87% 140,000 140, 140,000 140,000 122,439 34.48% 35.47% 33.65% 49.02% 29.20% 25.41% 23.72% 24.80% 31.75% 23.21% 14.26% 16.54% 13.65% 17.67% 16.29% Table 6 Aggregate yield rates by WebOrganizer document format, measured through the procedure-extraction stage, heuristics filtering, and the LLM filter stage (before postprocessing and final validation). All yields are reported as percentage of the original input source documents for each format."
        },
        {
            "title": "Rank Domain",
            "content": "1 2 3 4 5 7 8 9 traveltips.usatoday.com ixigo.com wikihow.com tripsavvy.com lifehacker.com getawaytips.azcentral.com frommers.com budgettravel.com cruisemates.com 10 instructables.com Count 256 168 146 61 58 55 45 30 B.2 Extracting procedures from documents of other formats See Table 6 for yield rates for WebOrganizer document formats beyond Tutorial & How-to Guide, measured through the procedure-extraction stage, heuristics filtering, and the LLM filter stage (before postprocessing and final validation). Throughout, yield at stage is computed relative to the original input source documents for that format (i.e., #documents retained after the stage divided by #input documents). Overall, these results show that valid, extractable procedures are not unique to tutorial-style pages, but Tutorial & How-to Guide consistently achieves the highest yields at each stage, so we focus on it as the primary source format for efficient large-scale mining. B.3 Implementation details on the heuristics filter We apply two simple filters in sequence: (1) Step-count filter. We require the extracted procedure to have between min_steps and max_steps steps (defaults: 515; configurable via command-line flags). (2) N-gram repetition filter. We normalize each step (lowercasing and removing punctuation) and compute the repetition rate of 2-, 3-, and 4-grams pooled across all steps, where repetition rate is the fraction of n-grams that are repeated beyond their first occurrence. We reject procedures with high repetition, using thresholds of 0.40 for bigrams, 0.35 for trigrams, or 0.30 for fourgrams. This filter primarily removes degenerate extractions that repeat near-identical phrases or steps. B.4 Rationale behind LLM filter criteria At the LLM filter stage in How2Mine, we remove procedures that fall within the following categories (matching the prompt in H.1): Named-entity focused. These instances hinge on entity-specific conventions (a particular person, organization, website, software product, or brand). Their correctness is often timeand access-dependent, and cannot be judged reliably without the entity context. 22 Pure math. Pure calculation or formula-solving tasks are not procedures in our intended sense: success is determined by correct numeric/algebraic result and is better evaluated by mathematical or verifiable oracles rather than critical-failure detection in instructions. UI interaction. UI-driven tasks require interacting with specific interfaces (websites, apps, terminals) and implicit state (what is currently visible, which buttons exist, what menus are named). In our execution-free setting, these tasks are difficult to verify and are brittle to UI changes over time. Open-ended creative generation. Creative goals have many qualitatively different valid endpoints, and the boundary between valid and invalid is dominated by taste and preference rather than by missing prerequisites or contradictions. Non-sequential process. Some examples are practically listicles, where most steps are order-independent without clear linear dependencies. Such instances are ill-posed for step-by-step validity evaluation. Unreasonable procedure. Finally, we remove instances where the steps are internally inconsistent or cannot plausibly achieve the stated goal (e.g., contradictions, logically impossible actions, or missing essential actions). Some are also simply nonsensical (e.g., How to fail an exam). This serves as quality-control stage: since How2Score uses the mined reference procedure as an anchor, unreasonable references would directly add noise to both evaluation and training. B.5 Example procedures for all 14 topics In Table 7, we show one full example (goal, resources, reference steps) for each of the 14 topics. Table 7 One full example (goal, resources, reference steps) for each of the 14 topics used in How2Everything. Reference steps 1. Form cup by rolling color-cored glass canes around glass collar. 2. Twist the cup to create spiral pattern. 3. Remove the twisted cup from the collar and set it aside. 4. Form second cup by rolling color-cored glass canes around glass collar. 5. Twist the second cup in the opposite direction to the first cup. 6. Blow the second cup inside the first cup to create the reticello network pattern. 1. Prepare notification to all co-owners stating the conditions of sale of your share. 2. Visit notary to draw up notarial document including all sale conditions. 3. Distribute the notarial document to all co-owners by letter with list of contents, obtaining receipt from each. 4. Wait 30 days for co-owners to express their desire to purchase your share. 5. Sell your share to third party. 1. Choose the simple sentence to transform. 2. Select the part of the sentence to emphasize (subject, object, or time). 3. Start the new sentence with It and the appropriate form of the BE verb (was or is). 4. Place the chosen part to emphasize immediately after the BE verb. 5. Insert who, that, or when as appropriate to introduce the rest of the sentence. 6. Add the remaining information from the original sentence to complete the it-cleft sentence. Continued on next page. Topic Art & Design To produce glass piece featuring a"
        },
        {
            "title": "Goal",
            "content": "reticello network pattern using the process of forming, twisting, and combining two color-cored glass cups. Resources color-cored glass canes; glass collar Crime & Law Sell your share of common property apartment with separate ownership by following the required legal procedure for notifying co-owners and transferring ownership. notary; notarial document; letter with list of contents; receipt Education & Jobs To create an it-cleft sentence in English that emphasizes specific part of given simple sentence. simple sentence; BE verb (was or is); It; who; that; when"
        },
        {
            "title": "Reference steps",
            "content": "Electronics & Hardware To maintain pedestrian turnstile gate to ensure its proper functioning and extend its service life through regular, comprehensive maintenance procedure. soft cloth; vacuum cleaner; lubricant; antirust oil; stainless steel maintenance oil; paint; non-corrosive cleaning solution; soft lint-free rag Food & Dining Prepare fricasé Boliviano, spicy pork stew with potatoes and white corn, by cooking pork with spices, thickening the stew, and serving with cooked potatoes and white corn. oil; large pot; pork pieces; white onion; cumin; black pepper; garlic; cayenne pepper; green onion; water; pan; potatoes; white corn; bread crumbs; deep plate Fashion & Beauty Create the structured base of an Uzbeki Spy Hat (or Wizard Hat) using interfacing and fabric. interfacing; fabric; scissors; needle; thread"
        },
        {
            "title": "Health",
            "content": "To prevent the formation of venous ulcers in your legs by following daily care routine. compression stockings; lotion; antiseptic ointment Home & Hobbies To create knurled band on the face of prop driver using the plunge knurling technique on lathe. lathe; knurl holder; tool post holder; peg; cross slide 1. Cut off the power supply to the turnstile gate. 2. Open the cover of the turnstile gate chassis. 3. Clean dust and debris from the surface and interior using soft cloth or vacuum cleaner. 4. Tighten any loose connecting screws on all internal parts. 5. Apply lubricant to moving components after inspecting the wear of vulnerable parts. 6. Adjust the balance spring after 30,000 operations. 7. Replace any aging or damaged wires in the power circuit. 8. Polish the external chassis with soft cloth and apply antirust oil or stainless steel maintenance oil. 9. Repair exposed scratches on the chassis with paint of the same color. 10. Clean the infrared acrylic mirror and beam window with non-corrosive cleaning solution and soft lint-free rag. 1. Heat oil in large pot. 2. Fry pork pieces in the oil until golden. 3. Add white onion, cumin, black pepper, garlic, cayenne pepper, and green onion to the pot. 4. Pour water into the pot while stirring. 5. Simmer until the meat comes off the bones, maintaining the broth level as needed (about 2 hours). 6. Cook potatoes in separate pan until done. 7. Cook white corn in separate pan until done. 8. Add bread crumbs to the stew to thicken it shortly before serving. 9. Serve the stew in deep plate and garnish with the cooked potatoes and white corn. 1. Cut right triangle from the interfacing by folding one corner to meet the opposite side and cutting along the fold. 2. Fold the triangle in half and stitch from the point downwards to form cone. 3. Trim the cone so it fits properly above your eyes. 4. Lay the cone on your fabric with the seam next to one edge and cut around the bottom, leaving about an inch of seam allowance. 5. Roll the interfacing cone over towards the adjacent side of the fabric to cut the other half. 6. Stitch cone of fabric. 7. Trim the tips and turn both cones inside out. 8. Fit the interfacing cone inside the fabric cone. 9. Stitch the cones together near the base of the interfacing. 1. Wear compression stockings every day while you are awake. 2. Exercise regularly to lose weight and lower blood pressure. 3. Apply lotion to your legs every day. 4. Check your legs for hard or rough areas and small cuts or abrasions while applying lotion. 5. Use antiseptic ointment on every small sore. 1. Reduce the stock to the final diameter required for the driver. 2. Counter-bore to depth of about 0.016 inches to produce the band that will be knurled. 3. Mount the knurl holder in the tool post holder with the center of the peg set to lathe center height. 4. Start the lathe and run the work at about 500 rpm. 5. Plunge the knurl into the face of the drive washer to form the knurl. 6. Run the cross slide in and out by about 1/32 inch to help clear chips and form the Vs. 7. Take light skimming cut over the outside diameter of the driver to remove the metal burr. 8. Take light skimming cut on the inside of the band. Continued on next page."
        },
        {
            "title": "Industrial",
            "content": "Install comprehensive waterproofing system for below grade spaces to prevent water ingress and structural damage. high performance waterproofing; waterproofing membrane; comprehensive waterproofing system; installers; manufacturer; geotechnical report"
        },
        {
            "title": "Religion",
            "content": "Analyze the influence of decanates and their associated Areas of Consciousness on the principal theme of natal chart. natal chart; decanates; Areas of Consciousness Science, Math & Technology Compute the trajectory of particle through velocity field using numerical integration within grid. cell; velocity field; grid; Eulers method Sports & Fitness Complete specific yoga sequence designed to stretch and strengthen the core muscles for equestrian fitness. yoga mat"
        },
        {
            "title": "TransportationCheck a car seat as baggage at the",
            "content": "airport to minimize the risk of damage or loss by following specific procedure. large corrugated cardboard box; contact information; airline check-in counter; airline staff; luggage tag; claim ticket; baggage area; airlines baggage desk; photos 25 1. Specify high performance waterproofing suitable for the assessed risk. 2. Ensure the waterproofing membrane bonds adhesively to the structure to prevent lateral water migration. 3. Specify comprehensive waterproofing system for both floors and walls. 4. Confirm that installers are experienced and trained by the manufacturer, and arrange for manufacturer support such as preinstall meetings and site visits. 5. Have the manufacturer review the geotechnical report to ensure membrane compatibility with site contaminants. 1. Determine the decanates occupied by the Sun, Moon, Ascendant, Ruling Planet, and ruler of the 5th house in the natal chart. 2. Identify which decanate (first, second, or third) is most frequently occupied by the majority of these key points. 3. Associate the most emphasized decanate with its corresponding Area of Consciousness: Personal (first), Relating (second), or Universal (third). 4. Highlight any key points not in the majority decanate for special consideration regarding their expression. 5. Integrate the decanate emphasis and associated Areas of Consciousness with the principal theme of the natal chart. 1. Identify the cell containing the initial position of the particle. 2. Determine the velocity at the current position by interpolation. 3. Calculate the new position using Eulers method. 4. Identify the cell containing the new position. 5. Repeat the previous three steps while the particle remains inside the grid. 1. Practice three-part breath (pranayama), expanding the stomach, then ribs, then chest on each inhale. 2. Move the spine in all directionsfront, back, sides, and twistswhile linking breath to movement. 3. Hold Goddess pose by standing with legs wide, bending knees, keeping chest elevated and shoulders over hips, tucking tailbone, and sinking into the squat. 4. Hold Warrior 1 pose by facing forward, stretching one leg back into lunge, reaching both arms upward, keeping shoulders wide, and elongating the torso by drawing the belly button toward the spine; alternate legs and hold each side for 10 breaths. 5. Hold Downward Facing Dog by bending down from standing, stretching legs back to high plank, then pressing hands down and lifting hips up and back to form an inverted V, keeping weight evenly distributed between hands and feet. 6. Lie on your back in Savasana (Corpse pose) and relax for several minutes. 1. Pack the car seat securely in large corrugated cardboard box. 2. Label the box with your contact information. 3. Bring the packed car seat to the airline check-in counter. 4. Check the car seat as baggage with the airline staff and obtain luggage tag or claim ticket. 5. Inspect the car seat for visible damage upon arrival at your destination before leaving the baggage area. 6. File claim at the airlines baggage desk immediately if the car seat is lost or visibly damaged, providing the luggage tag information and photos. Continued on next page."
        },
        {
            "title": "Topic",
            "content": "Travel & Tourism"
        },
        {
            "title": "Reference steps",
            "content": "Verify the authenticity of U.S. e-passport and the identity of its holder at border control terminal using the e-passport chip and printed information. passport terminal; e-passport chip; printed key; passport book; person presenting the passport 1. Unlock the e-passport chip using the printed key from the passport book. 2. Establish communication between the passport terminal and the unlocked chip over short distance. 3. Transmit the encrypted data from the chip to the passport terminal. 4. Verify the digital signature on the chips data to confirm authenticity and detect tampering. 5. Compare the printed information, the digital information from the chip, and the person presenting the passport. Details on developing How2Score C.1 Human annotation setup In the final version of annotator training, we first define critical failures, then carefully walk annotators through five examples of critical failures and six acceptable variances that should not be counted as critical. Our annotation interface enforces attention checks: annotators must explicitly click through UI elements to confirm that they have read and understood each example. Submissions remain closed until at least 90 seconds have elapsed. See screenshots of our annotation interface in Figures 7, 8, 9, 10, 11. Figure 7 Annotation interface screenshot [1]. Annotators must acknowledge they have read and understood the goal, then click through all model-generated steps to confirm they have thoroughly read them. C.2 Distillation To construct training data for distilling GPT 5 into Qwen 3 8B judge, we collect 72,920 GPT 5 annotations on model-generated procedures from diverse set of generator models. Specifically, we include generations from: gemini-2.5-flash gemini-2.5-pro (Google 2025) gpt-4.1 (OpenAI 2025) GPT 5 (OpenAI 2026) qwen2.5-7b-instruct (Qwen Team 2024) 26 Figure 8 Annotation interface screenshot [2]. After reading the example, annotators select whether they do identify critical failure. Figure 9 Annotation interface screenshot [3]. If there is no critical failure, annotators can select that option from the dropdown, and submit. 27 Figure 10 Annotation interface screenshot [4]. If annotators do identify critical failure, they need to provide brief description, then click any relevant reference / generation steps. Figure 11 Annotation interface screenshot [5]. Annotators can add multiple instances of critical failures. 28 OLMo-2-0425-1B-Instruct OLMo-2-0425-1B-stage1-step760000-tokens1594B OLMo-2-0425-1B-stage1-step1907359-tokens4001B OLMo-2-1124-7B-Instruct OLMo-2-1124-7B-stage1-step467000-tokens1959B OLMo-2-1124-7B-stage1-step928646-tokens3896B OLMo-2-0325-32B-Instruct OLMo-2-0325-32B-stage1-step467000-tokens3918B OLMo-2-0325-32B-stage1-step721901-tokens6056B To reduce label noise from stochastic judging, we run GPT 5 twice for each example and retain only examples where the binary judgment (has_failure vs. no_failure) is consistent across both runs. We finetune Qwen 3 8B for 3 epochs with learning rate 5e6 and batch size 64."
        },
        {
            "title": "Inference setup details",
            "content": "D.1 At inference time, the generator model receives the goal g, the resource list R, and the required step count = , and is asked to output procedure ˆS with exactly steps.11 Figure 12 Requesting model to stick to the level detail in the few-shot examples in the inference prompt (v2) significantly brings the generation length distribution closer to the reference distribution. Inference-time control for length bias. Like many existing benchmarks that use LLM judges (Zheng et al., 2023; Dubois et al., 2025), How2Bench shows length bias. In early experiments, we found that rewriting procedure to be more verbose (while attempting not to introduce new information) can increase scores, even when the underlying procedural content is unchanged. To avoid unfairly rewarding verbosity, we standardize inference across all models using the same 3-shot prompts. Full prompts in H.2. The prompts used for base and instruct endpoints are slightly different in wording. For instruction-tuned models, we append more instruction-like suffix to the prompt. The examples in the prompt illustrate the expected output format 11Conditioning generations on and is an evaluation control that reduces degrees of freedom and improves comparability across model outputs. It is not intended to reflect typical real-world usage. 29 and intended level of detail. We enforce explicit length control by requiring each step to be single, concise sentence containing one main action, and asking the model to closely follow the concision level in the provided examples. Figure 12 shows that this explicit length control brings generated lengths substantially closer to the reference length distribution. Decoding setup. For non-reasoning (non-thinking) model endpoints, we use greedy decoding. To prevent overly long continuations (especially from base models), we use stop sequence of nn for these endpoints. For reasoning-enabled API models, we use stochastic decoding with temperature = 0.6, and use the providers default reasoning/thinking budget for each API. D.2 Full intermediate checkpoint evaluation results on How2Bench Table 8 Full checkpoint results on How2Bench, including pretraining trajectories. For OLMo, Pretrain corresponds to the stage1 checkpoint series, Midtrain to the final base checkpoint, and Posttrain to the final instruct checkpoint."
        },
        {
            "title": "Size Stage",
            "content": "Step How2Score Avg gen tokens OLMo-2-0425 1B Pretrain OLMo-2-0425 1B Pretrain OLMo-2-0425 1B Pretrain OLMo-2-0425 1B Pretrain OLMo-2-0425 1B Pretrain OLMo-2 100000 190000 380000 760000 OLMo-2-0425 1B Pretrain 1140000 OLMo-2-0425 1B Pretrain 1530000 OLMo-2-0425 1B Pretrain 1907359 OLMo-2-0425 1B Midtrain OLMo-2-0425 1B Posttrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Midtrain OLMo-2-1124 7B Posttrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Midtrain OLMo-2-0325 32B Posttrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain 9000 46000 93000 371000 557000 743000 928646 7000 36000 72000 145000 289000 578000 721901 OLMo-3 14000 141000 283000 566000 30 0.06 0. 0.76 0.80 0.96 1.51 1.49 1. 6.39 5.96 0.09 2.61 4.74 7. 7.66 8.39 8.84 10.43 22.29 27. 1.79 8.60 12.29 10.86 12.53 15. 15.63 17.74 35.50 40.56 4.13 12. 16.00 17.82 17.87 247.74 109.12 105. 86.24 101.86 83.21 82.71 81.69 82. 66.86 132.99 87.79 77.82 91.71 82. 85.12 81.32 89.93 90.20 96.40 108. 79.71 83.69 78.67 79.84 74.57 75. 75.54 94.94 101.21 98.68 111.83 93. 96.76 93.51 Continued on next page. Suite OLMo-3-1025 7B"
        },
        {
            "title": "Size Stage",
            "content": "Pretrain Step How2Score Avg gen tokens 90.34 21.96 848000 OLMo-3-1025 7B Pretrain 1130000 OLMo-3-1025 7B Pretrain 1413814 OLMo-3-1025 7B Midtrain OLMo-3-1025 7B Posttrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Midtrain OLMo-3-1125 32B Posttrain 29000 58000 116000 232000 347000 579120 21.46 21.59 24. 30.23 6.21 17.15 21.96 23.96 25. 26.94 30.86 31.00 38.31 43.16 90. 86.26 96.67 101.60 108.16 87.24 86. 91.79 89.01 80.56 93.29 97.52 95. 100.77 D.3 Formatting proxy metrics over intermediate checkpoints To complement How2Everything scores with simple automated checks of procedural formatting, we compute three proxy metrics on model generations across checkpoint trajectories and report them in Table 9. Step-count mismatch is the fraction of examples where the generated procedure has different number of steps than the reference, i.e., predicted_steps reference_steps. Duplicate steps is the fraction of examples where predicted_steps contains any exact repeated step string (verbatim duplicates), i.e., set(predicted_steps) predicted_steps. Dup n-gram rate is computed within each example by concatenating predicted_steps, whitespace-tokenizing, forming n-grams, and computing max(0, cg 1) total n-grams , where cg is the count of n-gram g; we then average over examples. In this table we report the unweighted mean across {1, 2, 3, 4}. Table 9 Formatting proxy metrics computed on model outputs across checkpoint trajectories (all values are percentages)."
        },
        {
            "title": "Size Stage",
            "content": "Step Task score Step-count mismatch Dup-step ex. Dup n-gram rate (1--4) OLMo-3-1125 OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Midtrain 7000 33000 66000 131000 262000 394000 525000 656000 OLMo-3.1 32B Posttrain 4.33% 13.29% 17.10% 21.52% 22.33% 25.23% 29.14% 32.30% 35.23% 42.47% OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain 14000 71000 141000 2.12% 8.15% 11.55% OLMo-3-1025 9.37% 2.41% 1.24% 2.20% 2.14% 1.41% 2.07% 0.60% 1.30% 1.71% 22.47% 18.46% 6.73% 2.14% 0.79% 1.27% 0.77% 0.57% 0.27% 0.30% 0.26% 0.16% 0.00% 2.83% 1.66% 0.66% 11.74% 9.90% 10.71% 9.90% 10.17% 9.74% 10.70% 10.43% 9.42% 8.99% 13.42% 11.50% 10.19% 31 Continued on next page."
        },
        {
            "title": "Size Stage",
            "content": "Suite 283000 OLMo-3-1025 7B Pretrain 566000 OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain 848000 OLMo-3-1025 7B Pretrain 1130000 OLMo-3-1025 7B Pretrain 1413814 OLMo-3-1025 7B Midtrain 7B Posttrain OLMo-3 Step Task score Step-count mismatch Dup-step ex. Dup n-gram rate (1--4) 11.13% 11.77% 10.61% 10.79% 10.14% 9.72% 9.06% 13.44% 13.53% 17.91% 17.44% 17.19% 21.51% 29.80% 1.97% 2.56% 1.66% 1.50% 1.47% 0.24% 0.00% 6.94% 7.01% 3.63% 3.81% 3.37% 1.97% 0.03%"
        },
        {
            "title": "20000\nOLMo-2-0425 1B Pretrain\n100000\nOLMo-2-0425 1B Pretrain\n190000\nOLMo-2-0425 1B Pretrain\n380000\nOLMo-2-0425 1B Pretrain\nOLMo-2-0425 1B Pretrain\n760000\nOLMo-2-0425 1B Pretrain 1140000\nOLMo-2-0425 1B Pretrain 1530000\nOLMo-2-0425 1B Pretrain 1907359\n–\nOLMo-2-0425 1B Midtrain\n–\nOLMo-2-0425 1B Posttrain",
            "content": "OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Midtrain OLMo-2-1124 7B Posttrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Midtrain OLMo-2-0325 32B Posttrain 9000 46000 93000 187000 371000 557000 743000 928646 7000 36000 72000 145000 289000 433000 578000 721901 OLMo-2-0425 0.01% 0.21% 0.50% 0.34% 0.57% 0.56% 0.75% 1.02% 3.47% 4.34% OLMo-2-1124 0.04% 1.76% 2.88% 4.30% 5.07% 5.97% 6.47% 6.57% 17.91% 27.62% OLMo-2-0325 1.13% 6.02% 9.25% 8.85% 9.36% 12.46% 12.55% 15.00% 32.25% 40.14% 59.61% 19.66% 23.11% 11.96% 16.26% 15.81% 13.31% 12.14% 10.43% 36.99% 37.67% 7.17% 4.77% 8.53% 6.90% 7.81% 10.10% 10.23% 5.71% 0.10% 11.41% 3.67% 5.31% 4.34% 5.57% 11.56% 3.63% 3.33% 2.73% 0.10% 35.44% 12.63% 8.16% 9.29% 7.49% 5.11% 5.81% 5.59% 0.89% 0.01% 18.57% 5.64% 4.30% 4.83% 4.10% 4.91% 4.27% 3.81% 0.23% 0.00% 4.46% 4.73% 5.43% 4.44% 4.31% 4.00% 3.40% 3.86% 0.17% 0.01% 36.98% 21.41% 18.91% 20.29% 18.81% 16.06% 17.30% 17.18% 10.62% 8.47% 26.40% 16.67% 16.42% 16.01% 16.36% 16.49% 15.69% 15.08% 8.12% 9.21% 17.77% 16.03% 16.98% 17.46% 15.92% 15.67% 14.93% 15.54% 8.26% 10.29% D.4 Conditional perplexity vs. How2Score We compute conditional perplexity (teacher-forced) on the reference steps only, conditioned on the goal and resources prompt for each example, and compare checkpoint ordering under this metric to checkpoint ordering 32 under How2Score. Table 16 reports per-checkpoint How2Score scores and conditional reference-step perplexities, along with the induced within-run ranks. For each OLMo trajectory, the table header reports the Spearman rank correlation across the 9 checkpoints (8 stage-1 pretraining checkpoints plus the stage-2 midtrained checkpoint). D.5 Instance-level correlates of How2Score no_failure This section analyzes how the How2Score label (no_failure vs. has_failure) varies with three simple, instance-level properties: the reference step count (which also determines the requested number of generated steps in our inference setup), the resource count (the number of resources extracted from the reference procedure and provided as part of the task specification), and generation-to-reference length ratio that captures residual verbosity relative to the reference. We focus on 7 models: two open 78B models, two open 32B models, and three closed frontier models. Because topics differ in typical reference step counts and resource-list sizes, we fit logistic regression with topic fixed effects that predicts the per-example How2Score binary label from these covariates: logit(p(no_failure)) = log p(no_failure) 1 p(no_failure) = β0 + βsteps + βres + βratio ρ + γt I[topic = t], (1) tT {t0} where is the reference step count, is the reference resource count, and ρ is the generation/reference token ratio in percentage points: ρ = 100 gen/ref, computed by tokenizing each step string with the same token counting logic used in our evaluation scripts (tiktoken o200k_base). Here is the set of 14 topics and t0 is the baseline topic (in our runs, the baseline is chosen as the first topic in lexicographic order, which is Art & Design). We report odds ratios OR = exp(β), which are multiplicative changes in odds per +1 unit of the corresponding covariate. Table 10 Instance-level analysis with topic-controlled logistic regression, including residual verbosity. For each model, we fit Equation 1 on the How2Bench examples using that models generations and the How2Score-derived binary label (no_failure vs. has_failure), excluding records with missing fields or undefined token ratios. We report odds ratios (OR) with Wald 95% confidence intervals computed from the inverse Hessian. For numerical stability, we include 6) on non-intercept coefficients; effects are unchanged at this scale. ORsteps < 1 indicates small L2 penalty (λ = 10 that no_failure becomes less likely as reference procedures require more steps within topic. Orange text indicates non-significant effects at 0.05 (equivalently, the 95% CI includes 1.0). The Overall row fits the same regression on pooled generations across the shown models."
        },
        {
            "title": "Model",
            "content": "How2Bench score OR per +1 step (95% CI) OR per +1 resource (95% CI) OR per +1pp gen/ref (95% CI) OLMo-3-7B-Instruct Qwen3-8B-Instruct 30.23 0.756 [0.730, 0.783] 1.009 [0.990, 1.028] 1.012 [1.010, 1.014] 35.34 0.737 [0.713, 0.762] 1.020 [1.002, 1.038] 1.015 [1.014, 1.017] OLMo-3.1-32B-Instruct 43.16 0.751 [0.729, 0.775] 1.043 [1.025, 1.060] 1.013 [1.012, 1.015] Qwen3-32B-Instruct 46.04 0.765 [0.742, 0.788] 1.018 [1.001, 1.035] 1.014 [1.012, 1.016] Gemini-2.5-Pro Claude-Opus-4.5 GPT 5 Overall 56.11 0.795 [0.773, 0.817] 1.062 [1.045, 1.080] 1.018 [1.016, 1.020] 64.26 0.813 [0.791, 0.836] 1.060 [1.043, 1.078] 1.017 [1.015, 1.019] 67.99 0.846 [0.824, 0.869] 1.022 [1.006, 1.039] 1.014 [1.012, 1.016] 49.02 0.803 [0.795, 0.812] 1.032 [1.026, 1.039] 1.015 [1.014, 1.015] Reference step count (number of required steps) is the dominant predictor. Across all models, βsteps < 0 with ORsteps [0.74, 0.85], meaning that each additional required step is associated with substantial decrease in the odds of no_failure, even after controlling for topic, resources, and residual verbosity  (Table 10)  . This pattern is expected: procedures with more required steps create more opportunities for critical failures (omissions, wrong parameters, contradictions) to occur. Importantly, in our inference setup we request exactly generated steps with = , so reference step count is also the required output length; thus, this effect mixes both (i) intrinsic task complexity and (ii) the increased surface area for errors introduced by requiring longer outputs. 33 Figure 13 Topic fixed effects from Equation 1 across models, shown as log-odds offsets γt relative to the baseline topic (Art & Design). Red indicates higher odds of no_failure than the baseline topic after controlling for step count, resource count, and the gen/ref length ratio; blue indicates lower odds. Residual verbosity is positively associated with no_failure. The gen/ref token ratio ρ has consistent positive association with no_failure across models: ORratio 1.011.02 per +1 percentage point  (Table 10)  . Because this coefficient is per +1pp increase in 100 gen/ref , it compounds quickly: +10pp increase corresponds 10, i.e., on the order of 1020% increase in the odds of no_failure, holding topic, step to roughly (ORratio) count, and resources fixed. This provides quantitative evidence of residual verbosity bias in judge-based evaluation even under our explicit step-count and concision constraints; we therefore report average generated tokens alongside How2Bench results. Ceiling effects attenuate apparent effect sizes for frontier models. We also observe that the step-count effect is less extreme for the strongest models (e.g., GPT 5, Claude Opus 4.5): when overall no_failure rates are high, there is less residual variance left for simple predictors to explain, so estimated effect sizes can appear smaller even if the underlying trend is shared. Topic effects are large and broadly consistent across models. Figure 13 visualizes the topic offsets γt and R. We find systematic differences in conditional no_failure odds across after controlling for topics: Education & Jobs and Food & Dining tend to have substantially higher odds of no_failure than the baseline topic, while Electronics & Hardware and Industrial tend to have lower odds. Although the absolute magnitudes vary with model strength, the direction of these topic effects is broadly stable across models, indicating that topic-level variation is not reducible to step and resource counts alone."
        },
        {
            "title": "E Training details",
            "content": "E.1 Training data deduplication To reduce trainevaluation leakage, we perform embedding-based deduplication between the training set used for RL/SFT and the evaluation pool used to construct How2Bench. Concretely, we embed each example as single text string consisting of the goal followed by the numbered reference steps (one step per line). We compute L2-normalized sentence embeddings (so dot product equals cosine similarity) using SentenceTransformer embedding model (Qwen/Qwen3-Embedding-0.6B Qwen Team (2025b)), and for each candidate evaluation example we find its nearest neighbor in the training set by cosine similarity. We then 34 filter out candidate evaluation examples whose maximum train similarity exceeds fixed threshold (τ = 0.65), and sample topic-balanced clean evaluation set from the remaining examples. Operationally, we first compute nearest-neighbor similarity report (one record per evaluation example, including the nearest training example and its cosine similarity), then apply the threshold filter, re-attach full example records, and sample up to fixed number of examples per topic (with fixed random seed) to produce the final cleaned split. The resulting evaluation set is thus deduplicated with respect to the training set under this embedding similarity criterion. E.2 Details on training setup We construct the training set by sampling 100K examples created by our pipeline (3), balanced across 14 topics. We use embedding-based similarity filtering to ensure low overlap between the training set and How2Bench (see E.1). SFT setup. For SFT, we finetune both base and instruction-tuned checkpoints of Qwen 3 4B, Qwen 3 8B, and OLMo 3 7B for one epoch (learning rate 5e6; batch size 64). We format SFT examples using the prompt template from 5.1. RL setup with length control. For RL, we train Qwen 3 4B Instruct, Qwen 3 8B Instruct, and OLMo 3 7B Think.12 We train with Group Relative Policy Optimization (GRPO) (Shao et al., 2024) for 1000 optimizer steps with learning rate 5e7. Each rollout batch samples 4 prompts, with GRPO group size of 8 completions per prompt. We sample rollouts using the same prompt template as in 5.1. Rewards sum three components: (i) binary How2Score score computed by How2Judge, (ii) step-format verifier, and (iii) reference-calibrated length reward to prevent length gaming. See E.4 for details. E.3 SFT results We observe that SFT on our data yields at best small gains when starting from non-posttrained (base) checkpoints, but does not improve and can decrease performance when applied on top of already instructiontuned checkpoints. likely reason is objective mismatch: SFT imitates one reference-style realization per goal, while How2Score rewards any valid procedure as long as it avoids critical failures, so additional imitation on How2Train does not reliably reduce critical failures. See Table 11. Table 11 Performance before and after SFT. Stage Before After Gen tokens (before) Gen tokens (after) Model Qwen3-4B Base Qwen3-4B Instruct Qwen3-8B Base Qwen3-8B Instruct OLMo-3-7B Base OLMo-3-7B Instruct 32.00 29. 35.54 35.34 24.91 30.23 33.11 +1.11 28.47 1.23 35.20 0.34 32.45 2.89 26.13 +1.22 22.07 8.16 99. 89.63 112.21 99.18 96.67 101.60 90. 84.83 88.16 83.69 88.10 74.26 E.4 Auxiliary format and length rewards used during RL In addition to the binary How2Score reward, we include two lightweight, verifiable reward components: (i) step-format verifier and (ii) reference-calibrated length reward. Both are computed from the models final answer text and are added to the scalar reward used by GRPO. Step-format verifier. We check that the final answer contains an explicitly numbered list of steps with consecutive numbering starting at 1 (e.g., 1,2,3,. . . ), and when an expected step count is provided, that the number of steps matches it. This verifier returns 1 if the formatting constraints are satisfied and 0 otherwise. 12Thinking mode refers to the presence of explicit intermediate reasoning. Qwen models integrate instruction-following and reasoning in single checkpoint, whereas OLMo provides separate Instruct and Think checkpoints. 35 Reference-calibrated length reward. Let gen and ref denote the token lengths of the generated final answer and the reference, respectively (measured with fixed tokenizer). We compute the ratio = gen/ref and assign full credit within tolerance band τ around 1.0 (we use τ = 0.2). Outside the band, the reward decays exponentially: Rlen(r) = 1, exp(α r1τ 1 τ, 1τ ) , otherwise, with α = 5. Intuitively, this keeps generations close to the reference length while allowing moderate variation. Table 12 contrasts RL runs with vs. without this length reward. Table 12 Length control is necessary to prevent verbosity hacking during RL. We report How2Bench score and average generated tokens for RL-trained models with and without the length-based reward term. The average reference length is 97.44 tokens. For OLMo-3-7B-Think, we only report the main run (with length reward); the no-length-reward ablation was not run. Model Qwen3-4B-Inst RL reward + length reward (main) Qwen3-4B-Inst no length reward (prelim) Qwen3-8B-Inst + length reward (main) Qwen3-8B-Inst no length reward (prelim) OLMo-3-7B-Think + length reward (main) OLMo-3-7B-Think no length reward (not run) How2Bench score Avg gen tokens Avg gen/ref 1.01 43.52 97. 54.41 48.62 67.00 37.89 130. 96.99 149.42 91.80 1.34 1. 1.53 0.94 E.5 Judge robustness for RL gains Refer to Table 13 for detailed results on the judge robustness check. Table 13 RL gains persist under external judges. Scores are shown before and after RL (GRPO; step 1000); reports absolute gain with percent gain in parentheses. How2Judge GPT 5 judge Gemini-2.5-Pro judge Before After (%) Before After (%) Before After (%) Qwen3-4B-Inst 30.29 43.52 +13.23 (43.69%) 27. 36.28 +9.15 (33.72%) 15.66 24.83 +9.17 (58.58%) Qwen3-8B-Inst 38.52 48.62 +10.10 (26.23%) 32.63 41.39 +8.76 (26.84%) 20.10 28.13 +8.03 (39.94%) Olmo-3-7B-Think 27.30 37.89 +10.58 (38.77%) 20.63 31.71 +11.09 (53.74%) 13.53 20.30 +6.77 (50.05%) E.6 Details on out-of-domain benchmarks See Table 14 for details on each benchmark and the main capability it targets. We ran all our evaluations with the OLMES framework (Gu et al., 2025). E.7 Topic-restricted RL transfer across topics See Figure 14 for the PCA projection of topic embeddings for the 14 topics, computed from the goal texts. We use this visualization to select contrasting topic subsets for the topic-restricted RL experiment. Results are shown in Table 15."
        },
        {
            "title": "F Analyses and diagnostics",
            "content": "36 Table 14 Out-of-domain benchmarks used in Table 2 and the primary capability each targets."
        },
        {
            "title": "Primary capability tested",
            "content": "MMLU-Pro Wang et al. (2024) Broad professional-grade knowledge and multi-domain reasoning. GPQA Rein et al. (2024) Graduate-level science Q&A requiring deep domain understanding and reasoning; designed to be difficult to answer via retrieval alone. ZebraLogic Lin et al. (2025) Deductive logical reasoning on structured logic puzzles. AlpacaEval Dubois et al. (2025) Instruction-following and response quality via preference-style evaluation of helpfulness. HumanEval+ Liu et al. (2023) Program synthesis: writing correct code from docstrings with stronger correctness checking. LiveCodeBench Jain et al. (2024) Real-world coding ability under contamination-aware, time-based evaluation. MBPP+ Liu et al. (2023) Python programming: solving short tasks with improved test coverage. GSM8K Cobbe et al. (2021) Grade-school math word-problem solving with multi-step arithmetic reasoning. Minerva Lewkowycz et al. (2022) Quantitative reasoning on math/science problems (often requiring longer-form derivations). OMEGA Sun et al. (2025b) Mathematical reasoning that emphasizes exploratory, compositional, and transformative generalization. AIME24 AIME25 Competition mathematics (AIME exam problems; Olympiad-style reasoning). Competition mathematics (AIME exam problems; Olympiad-style reasoning). Table 15 RL training on topic-specific data (Qwen3-8B with thinking). We report overall task score and per-topic breakdown after RL (step 1000), along with deltas relative to the base model. Model Overall Art & Design Crime & Law Education & Jobs Electronics & Hardware Fashion & Beauty Food & Dining Health Home & Hobbies Industrial Religion Science, Math & Tech Sports & Fitness Transportation Travel & Tourism Qwen3-8B (with thinking) All topics (RL, step 1000) Delta 38. 48.62 10.10 Science-only (RL, step 1000) 47.93 Delta 9. Dining-only (RL, step 1000) 44.07 Delta 5.55 33.73 42. 8.84 44.98 11.24 36.47 2.74 38. 53.72 15.65 53.01 14.94 46.80 8. 59.64 70.00 10.36 71.20 11.56 67. 7.96 26.91 37.75 10.84 36.95 10. 31.33 4.42 39.16 49.30 10.14 50. 11.54 44.29 5.13 42.17 52.10 9. 49.70 7.53 50.30 8.13 43.09 54. 11.31 52.20 9.11 50.20 7.11 31. 39.48 8.22 36.47 5.21 39.08 7. 25.80 36.55 10.75 35.01 9.21 29. 3.86 48.09 53.91 5.82 55.20 7. 52.01 3.92 45.09 52.01 6.92 53. 8.42 46.99 1.90 27.51 39.80 12. 38.28 10.77 34.74 7.23 27.11 38. 11.45 35.81 8.71 33.13 6.02 51. 60.40 8.80 57.83 6.23 54.31 2. 37 Figure 14 PCA projection of topic embeddings for the 14 topics, computed from the goal texts. We use this visualization to select contrasting topic subsets for the topic-restricted RL experiment in Table 15. 38 Table 16 Per-checkpoint How2Score and conditional reference-step perplexity (lower is better), along with the induced ranks within each training run. Checkpoints are identified using the same Suite/Size/Stage/Step convention as Table 8."
        },
        {
            "title": "Size Stage",
            "content": "Step How2Score PPL Rank (How2Score) Rank (ppl) OLMo-2-0425 (1B) (Spearman rank ρ = 0.917) OLMo-2-0425 1B Pretrain OLMo-2-0425 1B Pretrain OLMo-2-0425 1B Pretrain OLMo-2-0425 1B Pretrain OLMo-2-0425 1B Pretrain 20000 100000 190000 380000 760000 OLMo-2-0425 1B Pretrain 1140000 OLMo-2-0425 1B Pretrain 1530000 OLMo-2-0425 1B Pretrain 1907359 OLMo-2-0425 1B Midtrain 0.06 11. 0.56 0.76 0.80 0.96 1.51 1. 1.59 6.39 9.63 9.25 9.11 8. 9.07 8.28 8.30 7.72 OLMo-2-1124 (7B) (Spearman rank ρ = 0.667) OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain OLMo-2-1124 7B Pretrain 9000 46000 187000 371000 557000 743000 928646 OLMo-2-1124 7B Midtrain 0.09 9.833 2.61 7.734 4.74 7.319 7.10 7. 7.66 7.294 8.39 6.581 8.84 8.303 10.43 6.523 22.29 6.707 OLMo-2-0325 (32B) (Spearman rank ρ = 0.233) OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain OLMo-2-0325 32B Pretrain 7000 72000 145000 289000 433000 578000 OLMo-2-0325 32B Midtrain 1.79 8.60 12.29 10. 12.53 15.00 15.63 17.74 35.50 7. 6.39 6.45 6.02 6.63 6.73 6. 6.12 6.18 OLMo-3-1025 (7B) (Spearman rank ρ = 0.850) OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain 14000 71000 283000 566000 848000 OLMo-3-1025 7B Pretrain OLMo-3-1025 7B Pretrain 1413814 OLMo-3-1025 7B Midtrain 4.13 12.42 16.00 17.82 17.87 21. 21.46 21.59 24.91 8.31 7.76 7. 6.20 5.96 5.93 6.01 6.11 6. OLMo-3-1125 (32B) (Spearman rank ρ = 0.483) OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain OLMo-3-1125 32B Pretrain 6000 29000 58000 116000 347000 6.21 7.856 17.15 6.683 21.96 6.087 23.96 5.857 25.53 5. 26.94 5.487 9 8 7 6 3 4 2 1 9 7 6 5 4 3 1 9 8 6 7 4 3 2 1 9 7 6 5 2 4 1 9 8 7 6 4 9 8 7 6 5 2 3 1 9 6 4 5 2 8 3 9 4 5 1 7 8 2 3 9 7 6 2 1 4 3 9 7 6 5 1 Continued on next page. 39 Suite OLMo-3-1125 32B"
        },
        {
            "title": "Size Stage",
            "content": "Pretrain Step How2Score PPL Rank (How2Score) Rank (ppl) 4 30.86 5.655 463000 3 OLMo-3-1125 32B Pretrain 579120 OLMo-3-1125 32B Midtrain 31.00 6.915 38.31 5. 2 1"
        },
        {
            "title": "G Qualitative examples of common failure patterns",
            "content": "G.1 Qualitative analysis details Below, we provide examples of model outputs with and without critical failures, scored by How2Judge. Crime & Law: subtle critical omissions. Table 17 illustrates subtle but critical omission: Gemini 2.5 Pro proposes plausible notary-mediated transfer, but skips the legally required waiting period that gives co-owners time to exercise purchase rights (reference step 4). In contrast, Claude 4.5 Opus and GPT 5 both preserve the essential structure of notice proof of delivery waiting period transfer, even while varying phrasing and step ordering. Science, Math & Technology: refusals and missing operational detail. Table 18 shows two common failure patterns on technical procedures. First, GPT 5 refuses to provide step-by-step experimental instructions and instead gives only high-level guidance; under our metric, this is critical failure because it is not executable relative to the reference protocol. Second, Gemini 2.5 Pro provides step sequence but leaves the thermocycling program underspecified, which the judge flags as missing critical operational condition for reliable execution. Art & Design: example of success. Table 19 provides an example where all three frontier models produce coherent, actionable stepby-step procedures. The outputs include small helpful additions (e.g., protecting adjacent pages) without omitting requirements, illustrating that failures are not universal and often concentrate in recognizable patterns such as refusals or subtle critical omissions. Table 17 Qualitative example (Crime & Law): selling share of common-property apartment. We report the topic, goal, reference steps, each models generated steps, and the resulting critical-failure annotations (4). Models shown are Claude 4.5 Opus (claude-opus-4.5), GPT 5 (GPT 5), and Gemini 2.5 Pro (gemini-2.5-pro)."
        },
        {
            "title": "Steps",
            "content": "Content Crime & Law Sell your share of common property apartment with separate ownership by following the required legal procedure for notifying co-owners and transferring ownership. 1. Prepare notification to all co-owners stating the conditions of sale of your share. 2. Visit notary to draw up notarial document including all sale conditions. 3. Distribute the notarial document to all co-owners by letter with list of contents, obtaining receipt from each. 4. Wait 30 days for co-owners to express their desire to purchase your share. 5. Sell your share to third party. Claude 4.5 Opus ( no_failure)"
        },
        {
            "title": "Generated steps",
            "content": "1. Visit the notary to draft formal notification letter stating your intent to sell, including the price and terms of the sale. 2. Send the letter with list of contents to each co-owner via registered mail to officially inform them of their right of first refusal. 3. Obtain and retain the receipt as proof that all co-owners received proper legal notice within the required timeframe. 4. After the statutory waiting period expires without co-owner acceptance, have the notary prepare the notarial document for the sale to the outside buyer. 5. Sign the notarial document before the notary to legally transfer your share of ownership to the new buyer."
        },
        {
            "title": "Critical failures",
            "content": "None. GPT 5 ( no_failure) Continued on next page."
        },
        {
            "title": "Generated steps",
            "content": "Content 1. Engage notary to manage the sale of your apartment share and outline the required co-owner notification procedure. 2. Have the notary draft notarial document specifying the share, price, and terms of the intended sale. 3. Send each co-owner letter with list of contents enclosing the notarial document and offering them the right of first refusal by stated deadline. 4. Retain the receipt proving delivery of the notice while the statutory response period runs. 5. After refusals or deadline expiry, sign the notarial transfer deed with the notary to transfer ownership to the buyer."
        },
        {
            "title": "Critical failures",
            "content": "None. Gemini 2.5 Pro ( has_failure)"
        },
        {
            "title": "Critical failures",
            "content": "1. Engage notary to draft formal notification of the intended sale for all co-owners. 2. Send the notification via letter with list of contents and keep the postal receipt as proof. 3. Instruct the notary to prepare the final sale agreement as notarial document. 4. Execute the transfer by signing the notarial document in the presence of the notary. 5. Receive the registered notarial document and the final transaction receipt from the notary. F1. Missing the required 30-day waiting period for co-owners to express their desire to purchase before proceeding to sell (reference step 4). F2. Proceeds to execute and register the transfer without ensuring the waiting period has elapsed or that third-party sale is permissible under the co-owner response procedure (reference steps 45). Table 18 Qualitative example (Science, Math & Technology): whole-plasmid PCR for repairing nicked plasmid products. We report the topic, goal, reference steps, each models generated steps, and the resulting critical-failure annotations (4). Models shown are Claude 4.5 Opus (claude-opus-4.5), GPT 5 (GPT 5), and Gemini 2.5 Pro (gemini-2.5-pro)."
        },
        {
            "title": "Steps",
            "content": "Content Science, Math & Technology Generate and repair nicked plasmid products using whole plasmid PCR with PFU Ultra DNA polymerase and PFU ligase, and verify and quantify the resulting PCR product. 1. Combine H2O, 10X PFU Ultra PCR buffer, 2mM dNTP mix, plasmid template, sense primer, antisense primer, PFU Ultra DNA polymerase, and PFU ligase in PCR tube on ice in the specified order. 2. Run the thermocycling program: 95C for 1 minute; then 95C for 30 seconds; then 55C for 60 seconds; then 72C for 2 minutes per 1 kb of PCR product. Repeat the previous three steps for total of 1220 cycles; then 72C for 20 minutes; then 55C for 60 minutes; then hold at 12C. 3. Verify the PCR product using gel electrophoresis. 4. Quantify the PCR product using Quantifluor DNA quantification. Claude 4.5 Opus ( no_failure)"
        },
        {
            "title": "Generated steps",
            "content": "1. Combine H2O, 10X PFU Ultra PCR buffer, 2mM dNTP mix, plasmid template, sense primer, antisense primer, PFU Ultra DNA polymerase, and PFU ligase in PCR tube on ice. 2. Place the PCR tube in the thermocycler and run the whole plasmid amplification program with appropriate cycling conditions. 3. Analyze an aliquot of the PCR product by gel electrophoresis to verify successful amplification and correct fragment size. 4. Quantify the remaining PCR product using Quantifluor DNA quantification to determine the final yield."
        },
        {
            "title": "Critical failures",
            "content": "None. GPT 5 ( has_failure)"
        },
        {
            "title": "Generated steps",
            "content": "1. Sorry, cant help with step-by-step experimental instructions for molecular biology procedures. 2. At high level, whole-plasmid PCR uses high-fidelity polymerase with primers designed to copy the entire circular template, generating products that may contain strand nicks. 3. ligase formulated for post-PCR repair can seal those nicks to yield an intact molecule, while agarose gel electrophoresis is used to assess product size and purity. 4. DNA concentration is then determined with fluorometric assay; consult the enzyme manufacturers protocols and your labs biosafety guidelines for precise conditions. Continued on next page."
        },
        {
            "title": "Critical failures",
            "content": "Content F1. Refuses to provide actionable step-by-step instructions, making the procedure unusable for achieving the goal (missing operational detail across reference steps 14). F2. Omits the concrete thermocycling program parameters required to run the PCR (reference step 2). F3. Omits explicit reaction setup instructions (reference step 1). Gemini 2.5 Pro ( has_failure)"
        },
        {
            "title": "Critical failures",
            "content": "1. In PCR tube on ice, combine H2O, 10X PFU Ultra PCR buffer, 2mM dNTP mix, plasmid template, sense and antisense primers, PFU Ultra DNA polymerase, and PFU ligase. 2. Place the reaction in thermocycler and run program optimized for whole plasmid amplification and nick repair. 3. Verify the size of the resulting PCR product using gel electrophoresis. 4. Measure the DNA concentration of the final product using Quantifluor DNA quantification. F1. Thermocycling program is underspecified/vague (optimized for whole plasmid amplification and nick repair) instead of providing the concrete temperatures, times, and cycle counts required for reliable execution (reference step 2). Table 19 Qualitative example (Art & Design): creating stamped images in journaling Bible using acrylic stamps. We report the topic, goal, reference steps, each models generated steps, and the resulting critical-failure annotations (4). Models shown are Claude 4.5 Opus (claude-opus-4.5), GPT 5 (GPT 5), and Gemini 2.5 Pro (gemini-2.5-pro)."
        },
        {
            "title": "Steps",
            "content": "Content Art & Design Create stamped images in journaling Bible using acrylic stamps. 1. Peel the acrylic stamp off the plastic sheet. 2. Stick the flat side of the stamp onto clear acrylic stamping block. 3. Ink the stamp on your stamp pad to ensure good ink coverage. 4. Condition new stamp by stamping few times on scrap paper before reinking. 5. Layer Bible mat and thin piece of cushy packaging under your Bible page to create flat stamping surface. 6. Position the stamp where you want it on the page. 7. Stamp firmly on the paper without moving the stamp, then lift off cleanly. 8. Clean the stamp with damp paper towel after use. Claude 4.5 Opus ( no_failure)"
        },
        {
            "title": "Critical failures",
            "content": "GPT 5 ( no_failure)"
        },
        {
            "title": "Critical failures",
            "content": "1. Slide the Bible mat beneath the page you plan to stamp to protect the underlying pages. 2. Place cushy packaging under the mat for softer, more even stamping surface. 3. Insert the plastic sheet between the working page and the facing page to prevent ink bleed-through. 4. Peel the acrylic stamp from its backing and press it firmly onto the clear acrylic stamping block. 5. Tap the mounted stamp evenly onto the stamp pad until the design is fully inked. 6. Test the inked stamp on scrap paper to check image clarity and ink coverage. 7. Position the stamp over the desired spot in the Bible and press down with steady, even pressure. 8. Clean the stamp immediately with the damp paper towel to keep it ready for future use. None. 1. Slip the Bible mat beneath the journaling page to protect the rest of the Bible. 2. Place the cushy packaging under the page on top of the mat to create soft stamping surface. 3. Lay the plastic sheet over adjacent pages to shield them from stray ink. 4. Mount the acrylic stamp onto the clear acrylic stamping block. 5. Tap the mounted stamp evenly onto the stamp pad to load ink. 6. Make test impression on the scrap paper to confirm coverage and positioning. 7. Stamp the design onto the Bible page with steady, even pressure. 8. Wipe the stamp clean with the damp paper towel. None. Gemini 2.5 Pro ( no_failure) Continued on next page."
        },
        {
            "title": "Critical failures",
            "content": "Content 1. Place the Bible mat behind the page you intend to stamp. 2. Slide piece of cushy packaging under the mat for an even impression. 3. Peel the acrylic stamp from its plastic sheet. 4. Press the stamp firmly onto the clear acrylic stamping block. 5. Gently tap the stamp onto the stamp pad until it is evenly coated with ink. 6. Test the impression by stamping firmly onto piece of scrap paper. 7. Press the inked stamp straight down onto the prepared Bible page with even pressure. 8. Clean the ink off the stamp using damp paper towel before returning it to the plastic sheet. None."
        },
        {
            "title": "H Prompt templates",
            "content": "This section includes the exact prompt templates used for inference, judging, and the web-data pipeline stages. H.1 Prompts for the data pipeline Figures 15, 16, 17, 18, and 19 provide the prompt templates for each stage of the web-mining data pipeline. Prompt for Pipeline Stage: Procedure Extraction You will be looking at document from web corpora. Your goal is to extract well-defined sequential process containing list of at least three executable steps. valid process should fulfill all of the following requirements: 1. Sequential: the steps should follow sequential order, where later steps depend on the completion of previous steps. 2. Imperative and atomic: express each step as single action. Add adjectives or adverbs only when they supply essential precision (e.g., \"coarsely grind beans\" vs. simply \"grind beans\"). 3. Concrete: each step should specify what to do, not why. In order to satisfy these requirements, the steps you extract may differ from how they are originally presented in the document. If there exists such valid process, you should also extract the goal of this process, which should: 1. Clearly state the outcome the process is meant to achieve. 2. Contain any essential context or constraints needed to understand or bound that outcome. Output your response in JSON format following this convention: {{ }} \"has_valid_process\": <boolean>, // true or false \"goal\": <string>, // can be an empty string if there is no valid process \"steps\": <string[]> // can be an empty list if there is no valid process If it is impossible to extract valid process from the given document, simply set \"has_valid_process\" to false, and leave \"goal\" and \"steps\" empty. <start of document> {document} <end of document> Figure 15 Prompt for the procedure-extraction stage in the web-mining pipeline. Prompt for Pipeline Stage: LLM Filter ## Inputs Below is goal and list of steps to achieve it. Read them carefully. Goal: {goal} Steps: {steps} --- ## Possible Categories Classify the goal and steps using the categories below, or indicate that no category fits if none are applicable. 1. **Named-entity Focused** - The goal and/or steps explicitly revolve around named entity such as specific person, organization, website, software, or branded product. - Examples: - \"Make pivot table in Microsoft Excel\" - \"Recreate the hairstyle of Emma Roberts in her recent film\" - \"Prepare presentation for the UN sustainability summit\" 2. **Pure Math** - The entire task is purely mathematical calculation or formula-solving exercise. - Examples: - \"Find the square root of 144\" - \"Solve for in 2x + 5 = 15\" - \"Compute the interest on $1,000 loan at 5 percent for 3 years\" 3. **UI Interaction** - The goal and/or steps involve interactions with specific UI elements in websites, software, or systems. - Examples: - \"Navigate to LinkedIn.com\" - \"Click Next and log in\" - \"Run pip install requests in the terminal\" 4. **Open-ended Creative Generation** - The goal and/or steps involve subjective, artistic, or imaginative creation where the output can vary widely. - Examples: - \"Write poem about autumn\" - \"Compose short story about robot learning to cook\" - \"Create color palette that feels like early spring\" 5. **Non-sequential Process** - non-sequential process is one where most steps do not need to follow fixed order. Each step is independent or only loosely connected, so they can be completed in any sequence without changing the overall outcome. With no strict dependencies, progress can happen flexibly---whether by working on steps in parallel, skipping ahead, or circling back as needed. - Examples: - 1. Try different forms of exercise 2. Consult nutritionist 3. Experiment with meal plans 4. Track sleep 5. Practice stress-reduction techniques - 1. Sketch possible product concepts 2. Research competitors 3. Estimate rough costs 4. Discuss with potential customers 5. Jot down names/branding ideas - 1. Donate clothes you no longer wear 2. Sell unused gadgets online 3. Organize kitchen cabinets 4. Sort through old papers and files 5. Rearrange furniture for better space flow 6. **Unreasonable Procedure** - The given steps cannot plausibly achieve the stated goal because some steps are logically impossible, irrelevant, contradictory, or omit critical actions. --- ## Task Determine if the provided goal and steps **fully match any of the categories above**. - Set \"judgment\" to false if no category fully applies. If at least one category fully applies, set it to true . 44 - If \"judgment\" is true, provide concise \"reason\" that explicitly mentions the relevant categories. Otherwise, leave \"reason\" as an empty string. --- ## Output Format Return your output in the following JSON format: {{ \"judgment\": <boolean>, \"reason\": <string> }} Figure 16 Prompt for the LLM-based filtering stage in the web-mining pipeline. Prompt for Pipeline Stage: Postprocess Goal/Steps Rewrite ## Inputs Below is goal and list of steps to achieve the goal. Goal: {goal} Steps: {steps} ## Task Revise the goal and steps so that they strictly fulfill all of the following conditions: 1. No resource-gathering steps: - Remove any beginning steps that involve collecting resources, such as \"get X\" or \"gather Y.\" Assume that all required resources are already available. 2. Deterministic path: - Ensure the goal and steps define one clear, unambiguous sequence of actions. - Remove any optional or conditional phrasing (e.g., \"do with or B,\" \"if desired,\" or \"if X, do Y\") by selecting and committing to single branch, then update the goal and steps to reflect that choice. 3. Only include actions to perform: - Each step should describe something to do, not something to avoid. - Rephrase any negative or prohibitive steps into positive actions, or remove them. 4. Keep only actions and details necessary to achieve the goal: - Remove any steps or information that do not directly contribute to accomplishing the goal. Eliminate optional, decorative, or repetitive elements that have no effect on the final outcome. 5. One major action per step: - Each step must involve single, coherent major action or task. - If step contains multiple distinct actions (e.g., \"mix, let stand, and drain\"), split it into separate steps so that each represents single clear operation. 6. No excessive micro-steps: - Avoid over-fragmentation where many consecutive steps repeat the same structure or action with only minor variations. - Combine such micro-actions into single, higher-level step that naturally groups related operations into one meaningful stage of the process. 7. Goal--steps alignment: - Rewrite the goal so that it precisely reflects the scope, intent, and level of detail of the steps, ensuring that the steps represent the only valid and sufficient way to achieve it. - The goal should describe what is being accomplished, not how its done. Avoid procedural or action-level details. 45 - Adjust general or broad goals to be specific enough that the listed steps are the only natural and complete way to fulfill them. Make no textual or formatting changes beyond what these conditions require. If no edits are necessary, leave the goal and steps unchanged. ## Output Format Return your response in the following JSON format. {{ }} \"rewritten_goal\": <string>, \"rewritten_steps\": <string[]> Figure 17 Prompt for rewriting extracted goals and steps to be deterministic and well-aligned. Prompt for Pipeline Stage: Postprocess Resource Extraction Below is goal and corresponding list of steps to achieve that goal. Goal: {goal} Steps: {steps} Your task is to extract and return deduplicated list of every distinct resource---tool, ingredient, piece of equipment, location, entity, etc.---explicitly mentioned in the steps only. Think of these as the key ** anchors of the process**: the essential external things that define the steps. Guidelines: 1. List each resource only once. 2. Include only primary, external resources. Skip anything produced along the way (intermediate creations). 3. Exclude any components that are intrinsic to the subject being acted on---in other words, dont list parts of the thing youre modifying, analyzing, or creating; include only external resources brought in to complete the steps. 4. Ignore verbs, non-identifying adjectives, measurements, and generic or vague terms like \"parts,\" \"item,\" \" object,\" \"surface,\" or pronouns. Please return your response in the following JSON format: {{ }} \"resources\": <string[]> Figure 18 Prompt for extracting an explicit resource list from the reference steps. Prompt for Pipeline Stage: Final Filter ## Inputs Below is goal and list of steps to achieve the goal using the given resources. Goal: {goal} Resources (could be empty): {resources} Steps: {steps} ## Task Answer the following questions: - **correctness**: Do the steps correctly achieve the stated goal? - **sequential**: Do the steps form clear, linear sequence (no branching or alternative paths)? - **no_specific_entity**: Are the goal and steps free of references to specific entities (e.g., particular people, products, websites, named resources, etc.) that require external context to be understood? - **goal_steps_alignment**: Do the goal, steps, and resources together define mostly deterministic plan - such that, given the goal and the provided resources (which may be an empty list), the steps represent an unambiguous and largely the only way to achieve the goal (allowing for minor variations in execution)? ## Output Format Return your response in the following JSON format. If your answer to any question is \"no\", provide onesentence reason. Otherwise, leave the reason empty. {{ \"correctness\": {{ \"answer\": \"yes\" or \"no\", \"reason\": \"...\" }}, \"sequential\": {{ \"answer\": \"yes\" or \"no\", \"reason\": \"...\" }}, \"no_specific_entity\": {{ \"answer\": \"yes\" or \"no\", \"reason\": \"...\" }}, \"goal_steps_alignment\": {{ \"answer\": \"yes\" or \"no\", \"reason\": \"...\" }} }} Figure 19 Prompt for the final sanity-check filtering stage in the web-mining pipeline. H.2 Prompts for inference Figures 20 and 21 provide the inference prompts used for base vs. post-trained checkpoints. Prompt for Base-Model Procedure Generation Goal: Prevent door from slamming shut by cushioning the latch with rubber band. Resources: [rubber band, door] Exactly 3 steps to achieve the goal using the given resources: 1. Stretch the rubber band around one door handle so that it crosses over the latch mechanism. 2. Twist the band once and loop it over the opposite handle, keeping it taut. 3. Center the band so it lies flat across the latch plate. Goal: Build tabletop Zen sand garden to encourage daily mindfulness. Resources: [shallow tray, fine sand, small rocks, smooth shell, miniature rake, decorative figurine, essential oil, brush ] 47 Exactly 8 steps to achieve the goal using the given resources: 1. Place the shallow tray on stable, level surface. 2. Pour fine sand into the tray until it forms an even layer about one inch deep. 3. Tap the tray edges lightly to settle and level the sand. 4. Arrange small rocks asymmetrically to create natural focal points. 5. Position the smooth shell and decorative figurine for added visual interest. 6. Use the miniature rake to draw flowing patterns around the objects. 7. Add one or two drops of essential oil onto corner of the sand for subtle fragrance. 8. Gently brush stray grains from the tray edges to keep the display tidy. Goal: Calibrate and pair Bluetooth stylus with tablet for reliable digital note-taking, then save the configuration. Resources: [Bluetooth stylus, charging cable, tablet, tablet Bluetooth settings, stylus settings panel, note-taking app, microfiber cloth, internet connection] Exactly 11 steps to achieve the goal using the given resources: 1. Connect the stylus to the charging cable and charge it for at least 30 minutes. 2. Power on the tablet and enable Bluetooth in the settings menu. 3. Disconnect the stylus from the charger and activate pairing mode. 4. In the tablets Bluetooth list, select the stylus name to initiate pairing. 5. Confirm any on-screen pairing prompt to finalize the connection. 6. Open the stylus settings panel found under \"Paired Devices.\" 7. Launch the calibration tool and tap the on-screen targets to align tip accuracy. 8. Adjust pressure sensitivity to personal preference. 9. Open the note-taking app and create test page. 10. Write and draw to verify smooth input and proper pressure response. 11. Back up or sync the stylus settings within the app or cloud account to preserve them for future use. Goal: {goal} Resources: {resources} Exactly {n} steps to achieve the goal using the given resources: Figure 20 Prompt for generating procedures during inference on base (no post-training) model checkpoints. Prompt for Post-trained Procedure Generation You will be given goal and list of resources. Your task is to output list of steps that complete the goal using the given resources. See below for some examples: ------------------------------- Goal: Prevent door from slamming shut by cushioning the latch with rubber band. Resources: [rubber band, door] Exactly 3 steps to achieve the goal using the given resources: 1. Stretch the rubber band around one door handle so that it crosses over the latch mechanism. 2. Twist the band once and loop it over the opposite handle, keeping it taut. 3. Center the band so it lies flat across the latch plate. Goal: Build tabletop Zen sand garden to encourage daily mindfulness. Resources: 48 [shallow tray, fine sand, small rocks, smooth shell, miniature rake, decorative figurine, essential oil, brush ] Exactly 8 steps to achieve the goal using the given resources: 1. Place the shallow tray on stable, level surface. 2. Pour fine sand into the tray until it forms an even layer about one inch deep. 3. Tap the tray edges lightly to settle and level the sand. 4. Arrange small rocks asymmetrically to create natural focal points. 5. Position the smooth shell and decorative figurine for added visual interest. 6. Use the miniature rake to draw flowing patterns around the objects. 7. Add one or two drops of essential oil onto corner of the sand for subtle fragrance. 8. Gently brush stray grains from the tray edges to keep the display tidy. Goal: Calibrate and pair Bluetooth stylus with tablet for reliable digital note-taking, then save the configuration. Resources: [Bluetooth stylus, charging cable, tablet, tablet Bluetooth settings, stylus settings panel, note-taking app, microfiber cloth, internet connection] Exactly 11 steps to achieve the goal using the given resources: 1. Connect the stylus to the charging cable and charge it for at least 30 minutes. 2. Power on the tablet and enable Bluetooth in the settings menu. 3. Disconnect the stylus from the charger and activate pairing mode. 4. In the tablets Bluetooth list, select the stylus name to initiate pairing. 5. Confirm any on-screen pairing prompt to finalize the connection. 6. Open the stylus settings panel found under \"Paired Devices.\" 7. Launch the calibration tool and tap the on-screen targets to align tip accuracy. 8. Adjust pressure sensitivity to personal preference. 9. Open the note-taking app and create test page. 10. Write and draw to verify smooth input and proper pressure response. 11. Back up or sync the stylus settings within the app or cloud account to preserve them for future use. ------------------------------- Your turn. For the following goal and resources, return exactly {n} steps. Each step should be single, concise sentence containing one main action. Closely follow the style shown in the examples above. Only return the steps, do not say anything else. Goal: {goal} Resources: {resources} {n} steps to achieve the goal using the given resources: Figure 21 Prompt for generating procedures during inference on post-trained model checkpoints. H.3 Prompts for the LLM judge for How2Score Figure 22 provides the full prompt used for the How2Score LLM judge. Prompt for the LLM Judge (How2Score) You are given goal and two lists of steps, L1 and L2. L1 is one correct procedure that is guaranteed to achieve the goal. L2 is candidate procedure whose correctness needs to be determined. Your task is to determine whether L2 has any **critical failures**, using the goal and L1 as the reference. # Important Guidelines ## L1 as Reference L1 reliably achieves the goal as written, but it may not be the only valid way to do so. Use it as reliable reference, not the exclusive solution. ## Definition of Critical Failure **critical failure** is an issue that fundamentally prevents the goal from being achieved or makes L2 unusable as set of followable instructions. Critical failures can take several forms: ### Contradictions - **Contradiction to the goal:** An L2 step directly contradicts condition specified in the goal. - **Contradiction to L1 steps:** An L2 step directly contradicts or significantly diverges from an L1 step, preventing the goal from being achieved. ### Logical or Structural Issues - **Internal inconsistency:** An L2 step is inconsistent with another step within L2. - **Incoherence:** L2 has very low readability or logical flow and is hard to follow. This doesnt require reading L1 to determine. - **Severe vagueness:** As whole, L2 lacks so much essential detail from L1 that it becomes basically unusable. ### Missing or Extraneous Actions - **Missing critical action:** An essential L1 step required to achieve the goal is completely omitted in L2, with no equivalent or implied action present. - **Unnecessary, confusing, or counterproductive extra action:** An L2 step introduces an action not present in L1 that is unnecessary or counterproductive. - **Redundant repetition:** An L2 step repeats one or more previous steps in L2 where no such repetition exists in L1. These categories are not exhaustive. In practice, single critical failure may span multiple categories. ## Acceptable Variations When assessing L2, focus on whether any issue is severe enough to prevent the goal from being achieved or to make L2 incoherent or unusable as set of instructions. If not, the variation is acceptable. Acceptable variations include: - Minor differences in tone, phrasing, or level of detail. - Differences in emphasis or ordering that do not affect the outcome. - Additional steps that are neutral or practical. - Reasonable implicit equivalence, where an omitted action is implied by another step. Ignore stylistic or verbosity differences unless the omissions make L2 lack essential details from L1 to the point that it becomes unusable. In that case, treat it as critical failure (severe vagueness). ## External Knowledge Base all decisions only on the provided **Goal** and **L1**. Minimize reliance on outside knowledge as much as possible. # Examples Below are examples of what qualifies as critical failure, as well as examples of what does not. To keep things concise, the L1 and L2 cases are shown in summarized form. Please read through them carefully to understand how to make the distinction. Keep in mind that these examples are not an exhaustive list of all possible failures for each L2. ## Examples of critical failures Note: The following examples are not listing all failures present in each L2; its only for demonstration purposes. Goal: Prepare Indian-style red lentil dhal for 8 portions using an oven and skillet. Summary of L1: Soak lentils 8 hours, rinse, steam at 100C with rice, spices, and aromatics, then finish with lime juice, seasoning, and coriander garnish. Summary of L2: Soak lentils only 30 minutes, then fry onions, garlic, chili, cumin, and salt in ghee, add lentils with water, and simmer until soft. 50 Example critical failure: L2 soaks lentils for only 30 minutes, whereas L1 soaks for 8 hours. This is critical difference in time. Example critical failure: L2 omits the oven entirely, using only stovetop simmering, which deviates from L1s oven-based preparation method and contradicts the goal. Goal: To construct traditional wooden Jacobs Ladder toy using wood, ribbon, and small nails. Summary of L1: Mark and cut the wood into equal pieces, sand coarse then fine, cut ribbon to equal lengths, stack the wood in Jacobs Ladder pattern, and nail ribbons to the pieces. Summary of L2: Cut the wood into 5 equal pieces, sand smooth, then arrange them from largest to smallest, nailing and wrapping ribbon around each piece in sequence. Example critical failure: L2 contradicts itself; if the 5 pieces are of equal size, there is no largest or smallest piece. Goal: To treat head lice by applying tea tree oil and apple cider vinegar solution to the hair. Summary of L1: Mix tea tree oil with apple cider vinegar, wash hair, apply solution, cover 15 minutes, rinse, then comb with fine-tooth comb. Summary of L2: Wash hair with shampoo, apply diluted tea tree oil--vinegar spray under cap for 1 hour, comb, and repeat treatment over 2 weeks, wash hair with shampoo. Example critical failure: L2 step 7 repeats the shampooing step almost verbatim, redundancy not present in L1. Goal: Prepare an alkyl chloride from primary or secondary alcohol using thionyl chloride to avoid acid and rearrangements. Summary of L1: Place alcohol in flask, add thionyl chloride, reflux, cool, then separate and dry the alkyl chloride with drying agent. Summary of L2: Add alcohol and thionyl chloride to flask, then add the drying agent, attach condenser, reflux, cool, and filter off the drying agent. Example critical failure: L2 adds the drying agent to the flask before heating the flask, while L1 uses the drying agent at the very end. Goal: Housebreak your Bichon Frise so that it reliably uses the designated outdoor bathroom location. Summary of L1: Take your Bichon Frise to the outdoor bathroom spot, praise it after use, crate when unsupervised, and repeat until accident free. Summary of L2: Put the dog in the crate, take the dog out of the crate, take the dog to the bathroom, put the dog back into the crate. Example critical failure: L2 omits praising the dog after outdoor bathroom use, removing the positive reinforcement step that is critical in L1 for reliable housebreaking. Goal: To establish clear, concise, and objective view of the accident based on evidence and actions. L1: 1. Establish specific snapshots of the accident based on evidence. 2. Consider these actions in light of what they establish individually, then in relation and combination with other actions. 3. Order or sequence the entire series of actions using specific sequencing evidence and common sense. 4. Audit actions where contradictions and questions arise to help decide what happened. 5. Define the events and overall conclusions about the accident based on the established actions and evidence. L2: 1. Get the basic facts. 2. Do not make assumptions. 3. Separate the people from the problem. 4. Define the problem. 5. Do not judge. Example critical failure: L2 as whole omits many critical details present in L1, making it practically unusable as set of instructions. ## Examples of acceptable variations that do not count as failures Goal: Prepare Ambrosia Fruit Dip using cream cheese, yogurt, vanilla extract, grated lemon rind, and Equal sweetener. Summary of L1: Blend cream cheese and yogurt until smooth, add vanilla, lemon rind, and sweetener, mix well, and chill in refrigerator. Summary of L2: Combine cream cheese, yogurt, vanilla, and sweetener, beat until smooth, add lemon rind, chill, then serve with fruit, enjoy, clean up, and store leftovers. Acceptable variation: L2 last step (storing leftovers) is not in L1, but it is an extra practical step that is reasonable and does not hurt the process. Goal: Prepare package for shipping so that its contents arrive in good condition. 51 Summary of L1: Choose strong box, wrap and cushion items, fill empty space, close and tape box, attach label , and remove old labels. Summary of L2: Place items in box with cushioning, tape securely, attach and verify label, seal seams, mark fragile if needed, and send to shipping service. Acceptable variation: L2 omits removing old labels, but this is not critical since it is reasonable to assume new box without old labels. Goal: Clean and protect car wheels safely and effectively using appropriate products and techniques for the specific wheel finish. Summary of L1: Identify wheel finish by contacting the manufacturer, choose safe cleaner for this finish, spray from bottom up, agitate with mitt/brush, and rinse thoroughly. Summary of L2: Follow manufacturers cleaning recommendations for this wheel finish, wash with mitt and cleaner, rinse, polish with metal polish, and apply protectant. Acceptable variation: L1 explicitly requires identifying the wheel finish, while L2 implies this through reading the manufacturers recommendations---a reasonable equivalent. This is not critical omission. Goal: Create distressed terra cotta pots as baby shower favors, each with an herb seed packet in stamped muslin bag. Summary of L1: Paint pots with base coat, dry, add second coat, dry overnight, sand for distressed look, add pebbles, tie twine with thank-you note, stamp \"GROW\" on muslin bags, insert herb seed packets, and place the bags next to each pot. Summary of L2: Paint pots in contrasting color and dry, lightly sand, add pebbles and soil, tie twine with handwritten thank-you card, stamp and label muslin bags with the herb name, fill with seed packets, tie shut , and place the bag in each pot. Acceptable variation: L1 step 8 and L2 step 8 differ in what is written on each muslin bag, but this difference is trivial and does not change the intended presentation or functionality. Goal: To perform basic sitting meditation focused on mental relaxation and body awareness. Summary of L1: Sit cross-legged and adjust posture until relaxed; focus attention and let the body readjust; maintain focus until fully relaxed; if distracted, return focus to the body. Summary of L2: Sit on chair or cushion with feet flat; close eyes and relax shoulders and jaw; notice body sensations without change; when thoughts arise, return focus to the body. Acceptable variation: L2 substitutes seated position with feet flat on the floor for L1s cross-legged posture. Considering the goal, this variation should not be considered critical failure, as both represent valid meditation positions. Goal: Capture sharp, blur-free photos of moving subjects using Shutter Priority Mode on your camera. L1: 1. Set your camera to Shutter Priority Mode. 2. Select an appropriate shutter speed for the action you want to freeze (e.g., 250 for moderate movement, 1000 for fast action). 3. If shooting in low light, increase the ISO setting to higher value (e.g., 800 or higher) to allow for faster shutter speeds. 4. Use lens with wide aperture (low f-number) to let in more light. 5. Position yourself at an appropriate angle or level for the subject (e.g., get low to the ground for children or sports). 6. Take the photo by pressing the shutter button. L2: 1. Set the cameras mode dial to Shutter Priority. 2. Set the ISO setting to high value for adequate light sensitivity. 3. Select wide aperture to capture as much light as possible. 4. Set the shutter speed to fast value (at least 1/500 of second) to freeze motion. 5. Use the cameras viewfinder to frame and focus on your subject. 6. Press the shutter button to take the photo. Acceptable variation: L2 simplifies the process bit but retains all essential actions, so it is not critical failure. However, if it used vague terms like \"appropriate value\" instead of specifying \"high\" or \" fast,\" it would be critical failure, as the instructions would no longer be useful. # Input data Goal: {goal} L1: {reference_steps} L2: {steps} # Output format To ensure transparency, provide clear reasoning in the \"reasoning\" field. This part should explain why each potential issue in L2 does or does not qualify as critical failure. The reasoning must not simply restate the failures---it should instead show your **thought process in determining correctness or failure severity **. Guidelines for marking critical failures: - Identify **all** critical failures in the given L2, and return them as list called \"critical_failures\". - The \"failure\" field should provide concise and clear explanation of what the failure is. - Each failure must be linked to **one or two** most relevant steps from L1 and/or L2. Record these in the \" L1_steps\" and \"L2_steps\" fields as lists of step numbers. Only link to more than two steps if there is good reason to do so. - If no failures are found, return \"critical_failures\": []. Return your response in the following JSON format: {{ \"reasoning\": \"<string>\", \"critical_failures\": [ {{ \"failure\": \"<string>\", \"L1_steps\": [<int>], \"L2_steps\": [<int>] }}, ... ] }} Figure 22 Prompt for the LLM judge used to detect critical failures in candidate procedures."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "University of Maryland",
        "University of Washington"
    ]
}