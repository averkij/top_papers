{
    "paper_title": "Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation",
    "authors": [
        "Shufan Li",
        "Jiuxiang Gu",
        "Kangning Liu",
        "Zhe Lin",
        "Zijun Wei",
        "Aditya Grover",
        "Jason Kuen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Lavida-O, a unified Masked Diffusion Model (MDM) for multimodal understanding and generation. Unlike existing multimodal MDMs such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O presents a single framework that enables image-level understanding, object grounding, image editing, and high-resolution (1024px) text-to-image synthesis. Lavida-O incorporates a novel Elastic Mixture-of-Transformers (Elastic-MoT) architecture that couples a lightweight generation branch with a larger understanding branch, supported by token compression, universal text conditioning and stratified sampling for efficient and high-quality generation. Lavida-O further incorporates planning and iterative self-reflection in image generation and editing tasks, seamlessly boosting generation quality with its understanding capabilities. Lavida-O achieves state-of-the-art performance on a wide range of benchmarks including RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive models and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable speedup at inference. These advances establish Lavida-O as a new paradigm for scalable multimodal reasoning and generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 2 4 4 2 9 1 . 9 0 5 2 : r Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation Shufan Li1,2,, Jiuxiang Gu1, Kangning Liu1, Zhe Lin1 Zijun Wei1, Aditya Grover2, Jason Kuen1 1Adobe 2UCLA * Work done primarily during internship at Adobe Research Figure 1: We propose Lavida-O, unified masked diffusion model capable of multi-modal understanding and generation."
        },
        {
            "title": "Abstract",
            "content": "We propose Lavida-O, unified Masked Diffusion Model (MDM) for multimodal understanding and generation. Unlike existing multimodal MDMs such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O presents single framework that enables image-level understanding, object grounding, image editing, and high-resolution (1024px) text-to-image synthesis. Lavida-O incorporates Preprint. Under review. novel Elastic Mixture-of-Transformers (Elastic-MoT) architecture that couples lightweight generation branch with larger understanding branch, supported by token compression, universal text conditioning and stratified sampling for efficient and high-quality generation. Lavida-O further incorporates planning and iterative self-reflection in image generation and editing tasks, seamlessly boosting generation quality with its understanding capabilities. Lavida-O achieves state-of-the-art performance on wide range of benchmarks including RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive models and continuous diffusion models such as Qwen2.5VL and FluxKontext-dev, while offering considerable speedup at inference. These advances establish Lavida-O as new paradigm for scalable multimodal reasoning and generation."
        },
        {
            "title": "Introduction",
            "content": "The abilities to understand and generate images have been two essential objectives of image modeling research. Traditionally, these tasks are handled by diverse set of specialist models, such as detection models for object localization (Liu et al., 2024b; Li et al., 2023a), Visual Question Answering (VQA) models for question-answering (Li et al., 2022), and diffusion models for text-to-image generation (Esser et al., 2024; Podell et al., 2023; Rombach et al., 2022). Recently, the rise of unified multi-modal models such as GPT-4o (OpenAI, 2024) has introduced new paradigm: using single generalist model to perform wide range of image understanding and generation tasks. Not only is this unified approach more aligned with the goal of developing versatile multi-task Artificial General Intelligence (AGI), but it also demonstrates strong empirical performance by allowing understanding and generation capabilities to mutually benefit each other (Deng et al., 2025). This is especially notable in tasks requiring both understanding and generation capabilities, such as image editing, where unified models show unparalleled advantages over generation specialists. Most current unified models are built on Autoregressive (AR) large language models. Some works, such as BLIP3o (Chen et al., 2025a) and BAGEL (Deng et al., 2025), employ AR modeling for text generation and continuous diffusion modeling for image generation (AR+diff), while others, such as Janus (Chen et al., 2025c), first tokenize images into sequences of discrete tokens and then employ unified AR next-token prediction objective for both image and text modalities. Recently, Masked Diffusion Models (MDMs) (Lou et al., 2023; Sahoo et al., 2024) have emerged as competitive alternative to AR models. Unlike AR models, MDMs treat token generation as diffusion process over discrete tokens. In the forward process, the tokens of sequence are gradually masked. At inference, we start with sequence of mask tokens and gradually unmask them to obtain sequence of meaningful tokens. Large-scale experiments in language modeling (Nie et al., 2025; Ye et al., 2025a) show that MDMs can achieve comparable performance to AR language models while offering many advantages, such as better speed-quality tradeoffs, controllability, and bidirectional context. Several recent works extend MDMs to multi-modal understanding and generation tasks (Li et al., 2025a; Yu et al., 2025b; Yang et al., 2025; Shi et al., 2025). Compared with the AR+diff setup, unified MDMs avoid the need to carefully tune the balance between AR and diffusion losses by offering unified objective, resulting in greater simplicity and scalability. Compared with unified AR modeling, unified MDMs offer significantly faster sampling speeds by allowing parallel decoding of multiple tokens. Despite these advantages, the latest unified MDMssuch as MMaDa (Yang et al., 2025) and Muddit (Shi et al., 2025)still lag behind state-of-the-art unified AR and AR+diffusion models, both in the breadth of tasks they support and in benchmark performance. There are three main challenges in developing high-performing unified MDMs. First, unified models are expensive to train due to the large size of their language backbones. For example, to build unified MDM with image generation capability, MMaDa pretrains an 8B model jointly on text and image generation, which is costly. This challenge is further exacerbated by the limited literature on training large-scale masked image generative models. In contrast, many open-source large-scale continuous diffusion models such as Flux (Labs, 2024) are readily available. Second, open-source resources for masked image generative models (MIGMs) are scarce, and the literature on their training techniques and sampling processes is less developed than that for continuous diffusion models. Even the best open-source MIGM, Meissonic-1B (Bai et al., 2024), significantly underperforms continuous diffusion models 2 of comparable size (Xie et al., 2025a). Lastly, while these models can perform both understanding and generation tasks, they lack explicit mechanisms to leverage image understanding capabilities to improve generation quality. In fact, MMaDa and Muddit cannot even perform image editing tasks, which require both understanding and generation capabilities. These models simply concatenate text-to-image data and image understanding data during training. To bridge this gap, we propose Lavida-O, unified multi-modal Masked Diffusion Model (MDM) capable of both image understanding and generation tasks. To mitigate the cost of training large diffusion models, Lavida-O introduces several techniques such as Elastic Mixture-of-Transformers (Elastic-MoT), progressive upscaling (gradually increasing the image resolution during training), and token compression that enable efficient scaling. To improve generation quality, Lavida-O employs stratified sampling and universal text conditioning. To fully leverage the potential of unified multi-modal model, Lavida-O incorporates planning and self-reflection mechanisms that explicitly utilize its understanding capabilities to enhance generation outputs. We highlight Lavida-Os capabilities compared with previous multi-modal MDMs in Table 1. Through extensive experiments, we show that Lavida-O achieves state-of-the-art performance on wide range of benchmarks such as RefCOCO object grounding (Kazemzadeh et al., 2014), GenEval text-to-image generation (Ghosh et al., 2023), and ImgEdit (Ye et al., 2025b) image editing, outperforming existing autoregressive and continuous diffusion models such as Qwen2.5-VL (Bai et al., 2025) and Flux .1 Kontext dev (Labs et al., 2025), while offering up to 6.8 speedup. Overall, our contributions can be summarized as follows: We propose the first multi-modal MDM that achieves state-of-the-art performance on textto-image generation, image editing, and grounding tasks, outperforming existing MDMs, AR models, and continuous diffusion models. We propose several efficient and effective training and inference techniques for large-scale masked image generative models and unified multi-modal models, such as the Elastic-MoT architecture, universal text conditioning, and stratified sampling, significantly advancing the literature. We introduce novel paradigm that explicitly leverages the understanding capabilities of unified model to improve its generation through planning and self-reflection. Table 1: Capabilities of different multimodal MDMs. Lavida-O uniquely supports localized understanding, high-resolution image synthesis, image editing and interleaved generation. Understanding Generation Model Image-level Object-level Text-to-image Image-editing Interleaved LaViDa, Dimple, LLaDa-V Muddit MMaDa LaViDa-O 5122 5122 10242 *"
        },
        {
            "title": "2 Background and Related Works",
            "content": "2.1 Masked Diffusion Models Masked Generative Modeling (MGM) has emerged as an alternative to AR models for modeling sequences of discrete tokens. Early works such as BERT (Devlin et al., 2019) used MGM as representation learning objective. Later works (Chang et al., 2022, 2023) such as MaskGIT explored using MGM for generative modeling. In this setup, sequence is initialized with only mask tokens, which are then gradually unmasked to generate the desired output. In these works, discrete tokenizers like VQGAN (Esser et al., 2021) are used to convert images into discrete tokens. More recently,MDMs (Austin et al., 2021; Sahoo et al., 2024; Lou et al., 2023) have further developed the theory of MGM by formalizing the masking and unmasking process as the forward and 1Muddit showed examples of simple editing through inpainting. It does not have instruction-based editing capabilities. reverse diffusion processes in discrete space. This provides principled framework for training and sampling from these models. MDMs have renewed interest in masked modeling for language generation, offering theoretical advantages over AR models, such as better speed-quality tradeoffs and improved controllability. Notably, LLaDa-8B and Dream-8B (Nie et al., 2025; Ye et al., 2025a) demonstrated that MDMs can achieve competitive performance compared to AR models at scale. Several follow-up works (Li et al., 2025a; Yu et al., 2025b; You et al., 2025; Yang et al., 2025) such as LaViDa extend MDMs to multi-modal tasks such as image understanding and text-to-image generation. Their capabilities are summarized in Table 1. Formally, given sequence of discrete tokens X0 = [X 1 0 ], the forward process q(XtXs) gradually masks the tokens over the time interval [0,1], with 1 0. At = 1, the sequence X1 consists entirely of masked tokens, denoted by [M ]. neural network pθ is used to model the reverse process p(XsXt). The masked diffusion objective is defined as: 0 , . . . , 0 , 2 LMDM = Et,X0,Xt (cid:21) log pθ(X0Xt) (cid:20) 1 (1) where pθ(X0Xt) is factorized into (cid:81)L 0Xt) based on independence assumptions (Sahoo et al., 2024). At inference time, the model starts from fully masked sequence X1 = [M, M, . . . , ] and progressively applies the learned reverse process log pθ(X0Xt) to recover the original tokens. We provide more detailed formulation of MDMs in Appendix A.1. i=1 pθ(X 2.2 Unified Multi-modal Models Unified multi-modal models such as GPT-4o (OpenAI, 2024) are capable of both image understanding and generation tasks, leading to strong performance on tasks requiring both capabilities, such as image editing. Generally, there are two dominant types of unified models based on their modeling objectives. The first type, such as BAGEL (Deng et al., 2025), employs an AR objective for text generation and diffusion objective for image generation (AR+diff). However, this design involves two different training objectives with distinct numerical scales and training dynamics, often requiring careful tuning of loss weighting and data mixtures. In contrast, the second type of models employ unified objectives for both image and texts. Early works like Janus-Pro (Chen et al., 2025c) employ unified AR modeling objective. Recent works like MMaDa (Yang et al., 2025) explore unified MDM objective. Despite some success, significant performance gap remains between these unified MDMs and state-of-the-art unified models in the AR and AR+diff categories. Architecturally, unified models also fall into two main categories. The first type, such as Janus and MMaDa, uses single dense transformer to output both image and text tokens. The second type, such as BAGEL and MetaQueries, employs separate parameter sets for handling image and text modalities. common design in this category is the mixture-of-transformers (MoT) architecture (Liang et al., 2024), where image and text inputs are processed by different parameter sets but can interact through joint attention mechanisms. These designs are illustrated in Figure 3. While being more flexible, training MoT experts can be expensive due to their large parameter counts."
        },
        {
            "title": "3 Method",
            "content": "3.1 Model Architecture Lavida-Os model architecture is built on LaViDa (Li et al., 2025a), diffusion model capable of only image understanding tasks. LaViDa uses SigLIP (Zhai et al., 2023) vision encoder to convert input images into continuous semantic embeddings Ci, which are concatenated with token embeddings of text prompts Ct to form the final conditional embeddings = Concat(Ci, Cp) for visual understanding tasks. At each inference step, the diffusion model uses the partially unmasked answer Xt and the conditional embedding to predict the clean text answer X0. For image understanding tasks, Lavida-O maintains this exact setup of LaViDa. To incorporate visual generation tasks, we extend LaViDas design by representing target images as sequences of discrete tokens using VQ-Encoder (Esser et al., 2021). When performing these tasks, X0 and Xt contain not only text tokens, but also VQ tokens that represent images. For image editing and interleaved generation tasks, we additionally incorporate VQ tokens of input images Cv as part of 4 Figure 2: Overall Pipeline of Lavida-O. Given an input image and text prompt, we first concatenate the image semantic embedding Ci, image VQ embedding Cv, and text prompt embedding Cp to form the conditioning embedding C. The combined embedding is then passed to the model alongside the partially masked sequence Xt. The model then predicts the fully-unmasked sequence X0. the conditional embedding = Concat(Ci, Cv, Cp), since using semantic embeddings Ci alone can degrade the low-level details needed for editing. To reduce the number of tokens and improve computational efficiency, we introduce token compression module that reduces the number of VQ tokens by factor of 4. The overall pipeline is illustrated in Figure 2. 3.1.1 Elastic Mixture-of-Transformers (ElasticMoT) Our goal is to find an efficient method that can equip an understanding-only diffusion model with visual generation capabilities. However, both of the existing common choices described in Section 2.2dense models and MoTare very expensive. Dense models use the same set of parameters for all tasks, requiring mix of understanding and generation data during training to prevent catastrophic forgetting, which is not data-efficient. While the MoT setup allows freezing the understanding branch and training only the generation branch for image generation, its architecture doubles the total parameter count, leading to considerable computational overhead. Moreover, given an 8B base understanding model, both setups require training at least 8B parameters for generation tasks from scratch, which is prohibitively expensive. To address these limitations, we propose Elastic-MoT, novel architecture design that efficiently adapts an understanding-only model for image generation tasks. Compared with the vanilla MoT architecture, Elastic-MoT introduces two major modifications. First, instead of using equally sized branches, the generation branch has smaller hidden size. This reduces the parameter count and enables efficient training. We make this design choice based on the observation that many text-toimage models can generate high-quality images with only 24B parameters, suggesting that generation tasks may not require as much capacity as understanding tasks (Xie et al., 2025a,b). Second, given an -layer model, instead of having joint attention at all layers, we only allow text and image modalities to interact in the first layers. In the remaining = layers, text and image tokens interact only within their modality through self-attention. This design activates only partial parameters for different tasks. For example, in Lavida-Os final design, the generation branch has 2.4B new parameters and the understanding branch 8B parameters from LaViDa. With = 32 layers and = = 16, image generation activates only 6.4B parameters (2.4B from generation + 4B from the first 16 understanding layers). During text-to-image pretraining, only the 2.4B generation branch is trainable, further improving the efficiency. Similarly, understanding tasks use 8B active parameters, while interleaved tasks requiring both branches use 10.4B. The full Elastic-MoT design is shown in Figure 3, with further details in Appendix A.2 and B.2. 5 Figure 3: Design of Elastic MoT. Elastic-MoT introduces two major modifications to standard MoT. First, the generation branch has smaller hidden size. Second, given an -layer model, we only allow text and image modalities to interact in the first layers. These two designs allow us to flexibly load only portion of parameters depending on tasks, improving the efficiency. 3.1.2 Modality-aware Masking One of the challenges in adapting MoT architecture for MDMs is routingthe mechanism to determine which branch should be activated for each token. This is trivial for unified AR MoT models, where the model can simply learn to generate special token (e.g., [img start]) to indicate that the next token should use the generation branch. However, MDMs decode tokens in parallel and must decide in advance which mask tokens should be routed to the understanding branch and which to the generation branch. naive solution is to let the user specify the number and location of text and image tokens, but this is difficult for interleaved generation, such as image generation with self-reflection. To address this issue, we design modality-aware masking process. Given sequence of text tokens and image VQ tokens, the vanilla forward diffusion process gradually converts it into + mask tokens during the time interval [0, 1]. By contrast, our modality-aware forward process introduces special timestamp texp [0, 1], at which fully masked image VQ tokens are collapsed into special [exp] text token. This process is illustrated in Figure 4a (Bottom-up). At inference, we assume all mask tokens are text tokens at the beginning. When [exp] token is generated, we replace it with sequence of Limg mask tokens, and specify that these tokens will be processed by the generation branch for image synthesis in subsequent forward calls. This process is also illustrated in Figure 4a (Top-down). We provide additional details in Appendix A.3. 3.2 Task-Specific Designs In this section, we describe several additional technical innovations that improve the effectiveness and efficiency on newly incorporated tasks such as image generation, image editing and grounding. Universal Text Conditioning. common approach to improving the quality of text-to-image models is micro-conditioning (Podell et al., 2023), which conditions the image generation process on extra parameters such as original image resolution, crop coordinates, and image quality scores. This is typically achieved via specialized embeddings. However, since unified model has strong language understanding and reasoning capabilities, we can simply append these conditions as plain text (e.g., SCORE: 5.40) to the end of user prompts. In addition to common conditions, we also incorporate image luminance and contrast as micro-conditions. This simple and effective design not only improves image quality by biasing generation toward high-scoring distributions, but also gives users more refined control over outputs. We provide additional details in Appendix A.4. Stratified Random Sampling. Most MDMs use confidence-based sampling, unmasking highconfidence tokens first. In image generation, high-confidence tokens tend to cluster around already 6 (a) Modality-Aware Masking (b) Stratified Sampling Figure 4: Design choices of Lavida-O. (a) Forward diffusion process with modality-aware masking. (b) Visualization of the unmasking order in the proposed stratified random sampling process. unmasked tokens. This negatively affecting image quality because adjacent tokens are highly correlated, which contradicts the independence assumption of MDMs. To mitigate this, we introudced stratified sampling process. Starting with 2 2 grid, we unmask one token per region to ensure broad spatial coverage. Each region is then recursively subdivided into four smaller subregions, and we continue unmasking one token from each new region. This process repeats until all tokens are revealed, producing balanced, evenly distributed unmasking pattern across the entire image. This is illustrated in Figure 4b. More details and analysis are provided in Appendix A.5 and B.3. Planning and Reasoning. While existing unified MDMs integrate image understanding and generation tasks with single objective, they do not incorporate mechanisms that use understanding to improve generation, except for the assumption that joint training benefits both tasks. To address this, we introduce two explicit mechanisms that leverage understanding to improve generation: planning and reflection. With planning, the model first generates layout of the image represented by bounding boxes, then creates the actual image accordingly. For image editing tasks, it first identifies the desired edit region before generating the edited image. With reflection, the model evaluates its own generation using its understanding capability and determines whether it satisfies the users request. If misalignment is detected, the model generates new image correcting the error. Examples are shown in Figure 1, with additional technical results and analysis in Appendix A.7 and B.5. Object Grounding with Coordinate Quantization. The bi-directional context of MDMs naturally allows parallel decoding of bounding box coordinates. While Lavida-O can represent numbers as plain text, we adopt specialized scheme that normalizes all bounding box coordinates to [0, 1] and quantizes them into 1025 discrete tokens representing 1024 . This ensures each bounding box is represented by exactly four tokens. At inference, we construct multiple query input with masked tokens such as dog [m][m][m][m]; cat [m][m][m][m], and unmask all coordinates in parallel. This design allow us to decode multiple bounding boxes in single diffusion step, greatly boosting the efficiency. We provide further details in Appendix A.6 1024 , ..., 1024 0 1024 ,"
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Setup We start with LaViDa (Li et al., 2025a) and extend it with 2.4B image generation branch using the ElasticMoT architecture described in Section 3.1.1. The training consists of three stages: Stage 1: We continue training the base model on object grounding and image-level understanding tasks. Stage 2: We incroprate an 2.4B image generation and pretrain for text-to-image generation. We start with resolution of 256 and progressively increase it to 512 and 1024 during training. Stage 3: In the final stage, we jointly train the entire 2.4B + 8B model end-to-end on image understanding, text-to-image generation, image editing, and interleaved generation tasks such as planning and selfreflection. More details on the training data and process are provided in Appendix B.1. 7 4.2 Main Results Image Understanding. We report the performance of image understanding tasks in Table 2. LavidaO outperforms the previous state-of-the-art unified diffusion model, MMaDa, by considerable margin on MMMU (Yue et al., 2024), MME (Fu et al., 2023), and MMB (Liu et al., 2024c). Compared with the base model LaViDa, Lavida-O achieves substantial improvements on most benchmarks such as ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), ScienceQA (Lu et al., 2022), and MathVista (Lu et al., 2023), due to the scaling of the training data. Table 2: Quantitative results on image-level understanding tasks.*Evaluated by us. Model MMMU MME-P MME-C MMB ChartQA DocVQA InfoVQA Sci.QA AI2D M.Vista M.Verse LLaVa-1.6-7B (Liu et al., 2024a) Qwen2.5-VL-7B (Bai et al., 2025) Intern-VL-3-8B (Li et al., 2024a) BAGEL (Deng et al., 2025) Show-O (Xie et al., 2024) Dimple (Yu et al., 2025b) LaViDa (Li et al., 2025a) Muddit (Shi et al., 2025) MMaDa (Yang et al., 2025) LaViDa-O 35.1 58.6 65.6 55.3 27.4 45.2 43.6 - 30.2 45.1 1519.3 - - 323 - - AR Und. Only 54.6 83.5 83.4 64.9 84.9 86.6 1687 1233 1514 1366 1104 1410 1431 AR Unified Und. and Gen. 701 - 85 - - - - 74.6 70.5 Masked Und. Only 63.4 64.6 432 341 Masked Unified Und. and Gen. - 242* - 68.5 76.4 - 9.8* 80.0 74.4 82.6 92.7 - - - 59.0 - 10.9* 73. 37.1 - 76.8 - - - 34.2 - 14.9* 44.6 73.2 - - - - 77.1 80.2 - 55.8* 84.6 66.6 83.9 85.2 - - 74.4 70.0 - 66.6* 76. 34.4 68.2 75.2 73.1 42.3 44.8 - 33.7* 56.9 14.3 49.2 39.8 - - 27.2 - 13.5* 36.9 Text-to-Image Generation. We report text-to-image generation results on the GenEval (Ghosh et al., 2023) and DPG (Hu et al., 2024) benchmarks, and FID scores on 30k prompts from the MJHQ (Li et al., 2024b) dataset. We compare against text-to-image models including Flux-dev (Labs, 2024), SD3-Medium (Esser et al., 2024), Meissonic (Bai et al., 2024) and DALLE-3 (OpenAI, 2023), unified models such as BAGEL (Deng et al., 2025), MMaDa (Yang et al., 2025) and Muddit (Shi et al., 2025). Lavida-O significantly outperforms the state-of-the-art Meissonic masked image generation model, as well as unified models such as MMaDa and Muddit. Planning and reflection further enhance prompt-following performance. We did not activate planning and reflection on MJHQ due to its large size and that FID does not reflect prompt-following capabilities. Table 3: Quantative results on text-to-image generation tasks. *Evaluated by us. GenEval DPG-Bench FID-30k Method Parms. Flux-dev (Labs, 2024) SD3-Medium (Esser et al., 2024) DALLE-3 (OpenAI, 2023) Meissonic (Bai et al., 2024) 12B 2B - 1B Type Gen. Only Continuous Continuous Continuous Masked BAGEL (Deng et al., 2025) OmniFlow (Li et al., 2024c) Show-o (Xie et al., 2024) Muddit (Shi et al., 2025) MMaDA (Yang et al., 2025) LaViDa-O +Planning +Reflection Unified Und. and Gen. 7B+7B 3.4B 1.3B 1B 8B 4B+2.4B 8B+2.4B 8B+2.4B Continuous Continuous Masked Masked Masked Masked Masked Masked 0.68 0.74 0.67 0.54 0.82 0.62 0.67 0.61 0.63 0.77 0.85 0.89 84.0 84.1 83.5 - - - - - 53.4* 81.8 82.9 83.2 10.15 11.92 - - - - 15.18 - 32.85* 6.68 - - Object Grounding. We evaluate the object grounding capabilities of Lavida-O on RefCOCO Referring Expression Comprehension (REC) tasks, reporting the Precision@0.5 metric. Lavida-O outperforms autoregressive vision-language models such as Qwen2.5-VL-7B (Bai et al., 2025) and InternVL3-8B (Zhu et al., 2025), as well as specialist models such as Grounding-DINO-L (Liu et al., 2024b) and SegLLM-7B (Wang et al., 2025a). Image Editing. We report image editing results on Image-Edit Bench in Table 5. Lavida-O outperforms state-of-the-art unified models such as BAGEL and specialized models like FluxKontext-dev. Most notably, Lavida-O even outperforms the state-of-the-art closed-source model GPT4-o(OpenAI, 2024) on replacing and removing objects, which requires localized understanding. This underscores the effectiveness of Lavida-Os design in integrating object-grounding capabilities. Table 4: Precision@0.5 on RefCOCO, RefCOCO+, and RefCOCOg REC tasks. Model RefCOCO RefCOCO+ RefCOCOg SegLLM-7B(Wang et al., 2025a) Qwen2.5-VL-7B (Bai et al., 2025) GroundingDINO (Liu et al., 2024b) InternVL3-8B (Zhu et al., 2025) LaViDa-O (4-step) LaViDa-O (1-step) val 90.0 90.0 90.6 92.5 92.3 91.9 testA testB 86.2 92.1 85.4 92.5 88.2 93.2 88.0 94.6 89.0 94.8 88.4 94.6 val 82.2 84.2 88.2 88.2 88.7 87.4 testA testB 76.1 85.5 76.9 89.1 75.9 89.0 81.8 92.5 83.3 92.5 82.2 91.7 val 83.9 87.2 86.1 89.6 90.0 89. test 85.9 87.2 87.0 90.0 90.6 89.8 Model GPT-4o (OpenAI, 2024) Qwen2.5VL+Flux (Wang et al., 2025b) FluxKontext dev (Labs et al., 2025) OmniGen2 (Wu et al., 2025b) UniWorld-V1 (Lin et al., 2025) BAGEL (Deng et al., 2025) Step1X-Edit (Liu et al., 2025) OmniGen (Xiao et al., 2025) UltraEdit (Zhao et al., 2024) AnyEdit (Yu et al., 2025a) InstructAny2Pix(Li et al., 2023b) MagicBrush (Zhang et al., 2023) Instruct-Pix2Pix(Brooks et al., 2023) LaViDa-O + Planning Table 5: Per-Category and overall scores on Image-Edit benchmark. Add Adjust Extract Replace Remove Background 4.35 4.61 4.13 4.07 3.98 3.76 3.74 3.57 3.47 3.82 3.30 3.56 3.40 3.88 2.94 3.47 2.96 3.44 2.47 3.18 2.54 2.55 1.97 2.84 2.01 2.45 4.39 4.04 4.40 4.11 Style Hybrid Action Overall 4.93 4.84 4.38 4.81 4.21 4.49 4.63 4.19 3.76 2.85 3.51 2.38 3.55 4.82 4.75 4.33 3.79 3.45 3.06 3.64 3.31 3.14 3.04 2.81 2.95 1.83 1.58 1.83 3.62 3.67 3.66 3.89 2.94 3.20 3.24 2.62 2.41 2.43 1.45 2.23 1.17 1.58 1.50 3.98 4. 2.90 2.04 2.15 1.77 2.27 1.70 1.76 1.71 2.13 1.88 2.10 1.51 1.44 2.01 2.04 4.57 3.90 3.78 3.57 2.99 3.24 3.16 3.21 2.83 2.24 2.01 1.75 1.44 4.06 4.00 4.89 4.52 4.26 4.68 2.74 4.17 2.52 3.38 2.98 2.65 1.98 1.22 1.46 3.54 4.04 4.20 3.80 3.52 3.44 3.26 3.20 3.06 2.96 2.70 2.45 2.12 1.90 1.88 3.71 3.80 3.96 3.04 2.96 2.52 2.96 2.38 2.64 2.24 1.91 1.56 1.42 1.62 1.20 2.94 3.10 4.3 Training and Inference Speed In Figure 5, we benchmark the inference efficiency of Lavida-O across three tasks: text-to-image generation, object grounding, and math reasoning. We measure end-to-end latency in seconds per image. Lavida-O is significantly faster than autoregressive models. Notably, we achieve 6.8 speedup on object grounding tasks compared to Qwen2.5-VL-7B (Bai et al., 2025). We also report the training efficiency measured by per-step latency and compare our Elastic-MoT design with BAGEL-style standard MoT design, Elastic-MoT improves the training speed by 3.17. Specifically, reducing the size of generation branch leads to speedup of 2.23, and decoupling the attention operation in the last 16 layers lead to an additional speedup of 1.44, We provide additional analysis on the speed-quality tradeoff at inference time in Appendix B.6 and analysis on the training efficiency of Elastic-MoT design in B.2. Figure 5: Training and Inference Speed of Lavida-O. We compare the end-to-end inference latency of Lavida-O on three tasks, as well as pretraining efficiency measured by per-step latency. 4.4 Additional Qualitative Results Finally, we provide additional qualitative examples demonstrating Lavida-Os capabilities on diverse prompts and editing instructions. Figure 6 shows text-to-image generation, and Figure 7 shows image editing results. 9 Figure 6: Qualitative examples of text-to-image generation. We provide additional examples of text-to-image generation outputs on diverse prompts. 10 Figure 7: Qualitative examples of image editing. We provide additional examples of image editing outputs on diverse instructions."
        },
        {
            "title": "5 Conclusion",
            "content": "the first multi-modal masked diffusion model In summary, we proposed Lavida-O, that achieves state-of-the-art performance on text-to-image generation, image editing, and grounding taskscompetitive with the best specialist models and autoregressive unified models. We also introduced novel paradigm of interleaved generation, which explicitly leverages understanding capabilities to improve generation results in unified multi-modal model through planning and selfreflection. In developing Lavida-O, we proposed several efficient training and inference techniques, including the ElasticMoT architecture, universal text conditioning, and stratified random sampling, providing valuable insights for future work in masked diffusion models and unified multi-modal systems."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution textto-image synthesis. arXiv preprint arXiv:2410.08261, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Victor Besnier, Mickael Chen, David Hurych, Eduardo Valle, and Matthieu Cord. Halton scheduler for masked generative image transformer. arXiv preprint arXiv:2503.17076, 2025. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon https://github.com/kakaobrain/ Image-text pair dataset. Coyo-700m: Kim. coyo-dataset, 2022. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025b. Lin Chen and Long Xing. Open-llava-next: An open-source implementation of llava-next sehttps://github.com/ ries for facilitating the large multi-modal model community. xiaoachen98/Open-LLaVA-NeXT, 2024. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. 13 Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. Minghui Hu, Chuanxia Zheng, Heliang Zheng, Tat-Jen Cham, Chaoyue Wang, Zuopeng Yang, Dacheng Tao, and Ponnuthurai Suganthan. Unified discrete diffusion for simultaneous visionlanguage generation. arXiv, 2022. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu Ella. Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 5(7):16, 2024. Yiming Jia, Jiachen Li, Xiang Yue, Bo Li, Ping Nie, Kai Zou, and Wenhu Chen. Visualwebinstruct: Scaling up multimodal instruction data through web search. arXiv preprint arXiv:2503.10582, 2025. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787798, 2014. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, et al. mplug: Effective and efficient vision-language learning by crossmodal skip-connections. arXiv preprint arXiv:2205.12005, 2022. Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024b. Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel Ni, and Heung-Yeung Shum. Mask dino: Towards unified transformer-based framework for object detection and segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 30413050, 2023a. Shufan Li, Harkanwar Singh, and Aditya Grover. Instructany2pix: Flexible visual editing via multimodal instruction following. arXiv preprint arXiv:2312.06738, 2023b. 14 Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Zichun Liao, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Omniflow: Any-to-any generation with multi-modal rectified flows. arXiv preprint arXiv:2412.01169, 2024c. Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025a. Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Reflect-dit: Inference-time scaling for text-to-image diffusion transformers via in-context reflection. arXiv preprint arXiv:2503.12271, 2025b. Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, et al. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. arXiv preprint arXiv:2411.04996, 2024. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pp. 3855. Springer, 2024b. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024c. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL https://aclanthology.org/2022.findings-acl.177. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. OpenAI. Dalle 3. https://openai.com/index/dall-e-3/, 2023. 15 OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. URL https://arxiv. org/abs/2410.21276. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large multimodal model. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Christoph Schuhmann. Laion-aesthetics. laion-aesthetics/, 2022. Accessed: 2024 - 03 - 06. https://laion.ai/blog/ Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in neural information processing systems, 36:4965949678, 2023. XuDong Wang, Shaolun Zhang, Shufan Li, Kehan Li, Konstantinos Kallidromitis, Yusuke Kato, Kazuki Kozuka, and Trevor Darrell. Segllm: Multi-round reasoning segmentation with large language models. In The Thirteenth International Conference on Learning Representations, 2025a. Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang arXiv preprint Xie. Gpt-image-edit-1.5 m: million-scale, gpt-generated image dataset. arXiv:2507.21033, 2025b. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Wei Wu, Kecheng Zheng, Shuailei Ma, Fan Lu, Yuxin Guo, Yifei Zhang, Wei Chen, Qingpei Guo, Yujun Shen, and Zha Zheng-Jun. Lotlip: Improving language-image pre-training for long text understanding. In arXiv, 2024. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023. 16 Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025a. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inferencetime compute in linear diffusion transformer, 2025b. URL https://arxiv.org/abs/ 2501.18427. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025a. URL https://hkunlp.github.io/blog/2025/dream. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025b. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2612526135, 2025a. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025b. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning. arXiv preprint arXiv:2504.16080, 2025."
        },
        {
            "title": "A Additional Technical Details",
            "content": "A.1 Formulation of Masked Diffusion Models 0 , 2 0 , . . . , Masked Diffusion Models (MDMs) model the generation process of discrete token sequences through continuous-time Markov chain (CMTC). Formally, given sequence of discrete tokens X0 = [X 1 0 ] of length L, the forward process q(XtXs) gradually converts it into sequence of mask tokens [M ], denoted by X1 = [X 1 1 ], over the continuous time interval [0, 1], with 1 0. Each token belongs to fixed-size vocabulary set . In our setup, consists of text tokens, image VQ tokens, and the special mask token [M ]. This forward process is formally defined as 1 , . . . , 1 , q(X X s) = (cid:26)Cat(X Cat(X ; M), ; 1t 1s Xi + ts 1s M), if if = [M ] = [M ], (2) RV are probability vectors, with where Cat() denotes categorical distribution, and M, Xi denoting the vocabulary size. In particular, is one-hot vector representing the mask token [M ]. This forward process yields the following marginal distribution: 0, Xi q(X X 0) = Cat(X ; (1 t)Xi 0 + tM). (3) MDLM (Sahoo et al., 2024) demonstrated that the posterior of the reverse process p(XsXt, X0) has the following form: p(X sX , 0) = (cid:26)Cat(X Cat(X s; Xi t), s; ts Xi 0 + M), if if = [M ] = [M ]. (4) In practice, we replace Xi reverse process, which gives the following transition: 0 with the neural network prediction pθ(X 0Xt) when sampling from the pθ(X sXt) = (cid:26)Cat(X Cat(X s; Xi t), s; ts pθ(X 0Xt) + M), if if = [M ] = [M ]. (5) 1 = = Sampling process. At inference time, we initialize X1 as sequence of mask tokens, with 1 1 = 2 1 = [M ]. We discretize the continuous time interval [0, 1] into discrete timesteps 0 = t0 < t1 < < tK = 1, and iteratively sample Xtk1 pθ(Xtk1 Xtk ) using Equation 5. We start with = and end when we obtain mask-free sequence X0. At each step, we sample each token position independently, assuming that pθ(Xtk1 Xtk ) factorizes as (cid:81)L Xtk ), following previous works (Nie et al., 2025; Sahoo et al., 2024; Lou et al., 2023). i=1 pθ(X tk1 Training process. At each training step, given clean sequence X0, we sample random timestep [0, 1] and obtain Xt q(XtX0) through the forward process defined in Equation 3. The loss is then computed using Equation 1 from Section 2.1. In this section, we have documented the standard training and inference process for typical MDMs. Our modality-aware masking design introduces several modifications to the above processes, which are described in Section 3.1.2 of the main paper. Additional details are provided in Appendix A.3. A.2 Elastic-MoT Architecture In this section, we document the detailed design of the Elastic-MoT architecture described in Section 3.1.1. As discussed in the main paper, the proposed Elastic-MoT architecture has two key differences compared to standard MoT: generation branch with variable size and decoupled joint attention in the later layers. Variable-sized generation branch. In standard MoT models such as BAGEL (Deng et al., 2025), the generation branch is initialized as an exact copy of the understanding branch. For models in the 710B scale, this leads to substantial increase in parameter count and compute overhead, 18 Figure 8: Activated parameters of Lavida-O under different task settings. Elastic-MoT Design allow Lavida-O to dynamically loads its parameters depending on the tasks. For understanding-only tasks, we only load the 8B generation branch. For text-to-image generation tasks, we load the first = 16 layers of the understanding branch, which consists of 4B parameters, and the full 2.4B generation branch. For interleaved tasks, we load all 2.4B+8B parameters. limiting the scalability of MoT models. Motivated by the success of many medium-sized, highquality text-to-image generation models, we explore using smaller generation branch in the ElasticMoT design. Since we still want modalities to interact with each other through the joint attention mechanism, it is important to keep the dimensions of the query and key vectors consistent. We provide detailed breakdown of parameter sizes in Table 6. To initialize the generation branch with smaller dimensions than the understanding branch, we truncate the weights of the understanding branch and copy them to the generation branch. Table 6: Comparison of understanding (Und) branch and generation (Gen) branch configurations. The projection sizes are in the format [output size, input size]. Und Branch Gen Branch Attention 4096 [4096, 4096] [4096, 4096] [4096, 4096] [4096, 4096] 2048 [4096, 2048] [4096, 2048] [4096, 2048] [4096, 2048] MLP 4096 4096 12288 2048 2048 8192 2048 norm proj size proj size proj size attn out norm input size hidden size output size Decoupled attention. In standard MoT, understanding and generation tokens can interact with each other in all transformer layers through the joint attention mechanism. We decouple the attention in the last layers and only allow tokens of the same type to interact with each other. In the first = layers, all tokens can still interact with each other as in the standard MoT architecture. This design is motivated by two factors. First, it prevents text and image tokens from interfering with each others representations in the later stages of generation. Second, and more importantly, it allows us to load only 4B out of 8B parameters for text-to-image generation tasks, greatly improving the scalability of pretraining while also reducing compute cost at inference time. We visualize the activated parameters for different tasks in Figure 8. For understanding-only tasks, we activate only the understanding branch in all = + layers. For generation-only tasks, we activate the understanding branch in the first layers and the generation branch in all = + layers. For interleaved tasks with both text and image outputs, we activate all parameters. In our setup, we choose = = 16, yielding = 32 layers in total. 19 Figure 9: Training and inference with modality-aware masking. We visualize the sampling process with modality-aware masking on the left and the training process on the right. During the training, the loss is applied on either X0 or 0 depending on the value of with respect to texp. A.3 Modality-Aware Masking In this section, we provide details of the changes to the training and sampling process introduced by modality-aware masking, as described in Section 3.1.2. Recall that in adapting the MoT architecture for MDMs, one of the main challenges is routing tokens. In particular, while we can easily decide which branch should process unmasked tokens based on whether they are image VQ tokens or text tokens, it is difficult to make such decisions for masked tokens, especially in interleaved generation tasks where the final output contains both images and text. Modality-aware masking addresses this problem by processing all tokens with the understanding branch by default and dynamically deciding when and where to invoke the generation branch during the sampling process. Sampling Process. For convenience, we denote masked tokens that will be processed by the understanding branch as Mund and masked tokens that will be processed by the generation branch as Mgen. With this distinction, the routing policy becomes simple: all text tokens plus Mund are processed by the understanding branch, while all image VQ tokens plus Mgen are processed by the generation branch. We introduce special text token [exp] to indicate when an image should be generated. When [exp] token is generated in the unmasking process, it is automatically replaced with sequence of Mgen tokens. The number of Mgen tokens representing each image is determined by prespecified output resolution. These tokens are then processed by the generation branch in subsequent rounds. For example, each 1024 1024 image is represented by 1024 VQ tokens. This process is documented in Algorithm 1 and illustrated in Figure 9 (Left). Mund Algorithm 1 Interleaved Generation with Modality-Aware Masking Input: Initial Generation Length L, discrete timestamps 0 = t0 < t1 < < tK = 1, prompt 1: Initialize 2: Initialize 1:L 3: for = to 1 do 4: 5: 6: 7: 8: 9: end for 10: return Fully unmasked sequence Sample Xti1 pθ(Xti1 Xtk , C) if [exp] token is generated in Xti1 then // These Mgen will be routed to the generation branch in subsequent rounds Replace it with sequence of Mgen tokens // Eq. 5 end if Training. consequence of modality-aware masking is that the partially masked sequence Xt will have varying length depending on t, making the loss described in Equation 1 not directly applicable. In particular, when sampling from the forward process q(Xt X0), there is special timestep texp 20 Figure 10: Effect of Universal Text Conditioning. On the left side, we visualize the text format used in Universal Text Conditioning. On the right side, we visualize generation results under different choices of universal text conditioning. at which sequence of VQ image tokens is collapsed into single [exp] text token. As illustrated in Figure 9 (Right), when < texp, Xt has shorter sequence length than X0. To apply the loss properly, we construct new sequence 0 by collapsing all sequences of image VQ tokens into [exp] tokens in X0. We then modify the loss in Equation 1 to the following: LMDM = Et,X0,Xt 1 (cid:88) {iX =[M ]} log pθ( ˆX 0 Xt) , where ˆX0 = (cid:26)X0, 0, if (texp, 1) if (0, texp) (6) (7) This change is also highlighted in blue in Figure 9 (Right). Understanding-Only and Generation-Only Tasks. We activate modality-aware masking only for interleaved tasks, since these require both the understanding and generation branches in our Elastic-MoT architecture. For computational efficiency, we do not use modality-aware masking for understanding-only tasks such as image captioning, or for generation-only tasks such as text-toimage generation (without planning and reflection). This allows us to best utilize the flexibility of Elastic-MoT and avoid loading unnecessary model parameters. A.4 Universal Text Conditioning Universal text conditioning is inspired by the micro-conditioning approach (Podell et al., 2023) employed in many text-to-image models. These models interoperate special conditioning embeddings to incorporate non-text conditions such as the original image resolution or aesthetic score. Since Lavida-O is unified model with mathematical reasoning capabilities, we can represent these conditions directly as plain text. In particular, we include source image resolution, crop coordinates, aesthetic scores (Schuhmann, 2022), and HPS scores (Wu et al., 2023), following existing works (Podell et al., 2023; Bai et al., 2024). Additionally, we incorporate luminance (brightness) and contrast to give users greater control over the generated images. Each condition is represented as simple string of the form [KEY] : [VALUE]. During training, each condition is randomly dropped with some probability. At inference, users may specify all conditions or only subset. This design is illustrated in Figure 10. By modifying these universal text conditioning parameters at inference time, users can flexibly control various image properties such as brightness. Notably, when brightness and contrast are set to very high values, the generated images become highly stylized in order to satisfy the constraints. 21 Figure 11: Visualization of different sampling processes. We compare the unmasking order of the stratified sampler, Halton sampler, and uniform random sampler. Uniform random sampler produces the least desirable spatial pattern, with many unmasked tokens clustered together. Halton sampler is less ideal than stratified sampler because it does not guarantee perfectly stratified coverage. For example, when the number of unmasked tokens is 4, the upper-right quadrant remains unoccupied. A.5 Stratified Random Sampling In this section, we provide detailed descriptions of the stratified random sampling process introduced in Section 3.2. In the vanilla sampling process described in Equation 5, each token is unmasked independently. In practice, this often leads to suboptimal generation quality. Instead of unmasking tokens randomly, several works adopt alternative sampling strategies in which the unmasking order of tokens is determined by heuristics such as the models confidence at each token position (Nie et al., 2025; Ye et al., 2025a; Chang et al., 2022). In image generation, tokens with high confidence are frequently adjacent to one another. As result, confidence-based unmasking tends to reveal many adjacent tokens in single step. Since tokens that are spatially adjacent are often highly correlated, this violates the independence assumption pθ(Xtk1 Xtk ) = (cid:81)L Xtk ) stated in Section A.1. To address this, we design stratified sampling process that ensures the unmasked tokens are spatially dispersed. Specifically, we enforce that the first 4 unmasked tokens occupy the four quadrants of the image; the first 16 unmasked tokens occupy all 16 subregions obtained by dividing the image into 4 4 grid; and so forth. The algorithm is formally described below: i=1 pθ(X tk1 Algorithm 2 Stratified Unmasking Order Partition the image into 2d 2d grid cells for each grid cell in random order do Input: Image size Output: list of coordinates (i, j) indicating unmasking order 1: Initialize an empty list 2: for = 1, 2, . . . , log2 do 3: 4: 5: 6: 7: 8: 9: 10: end for 11: return Sample (ig, jg) uniformly within cell Append (ig, jg) to if = Ø then end for end if 22 Figure 12: Coordinate Quantization. We normalize bounding box coordinates into the range [0,1] and discretize them into 1025 bins. This ensures that each bounding box is represented by exactly 4 tokens, allowing efficient parallel decoding of multiple bounding boxes in single step. Our design is inspired by the stratified sampling process commonly used in numerical integration and computer graphics. It also follows similar motivation to the recent Halton mask scheduler, which uses the low-discrepancy Halton sequence to ensure that unmasked tokens are spatially dispersed (Besnier et al., 2025). We illustrate the differences among stratified sampling, Halton sampling, and uniform random sampling in Figure 11. As shown in the figure, uniform random sampling produces the least desirable spatial pattern, with many unmasked tokens clustered together. Compared with our proposed stratified sampling process, Halton sampling is less ideal because it does not guarantee perfectly stratified coverage. For example, when the number of unmasked tokens is 4, the upper-right quadrant remains unoccupied. The benefits of stratified sampling are also reflected in FID scores, which we document in Section B.3. A.6 Object Grounding with Coordinate Quantization In this section, we provide detailed descriptions of Lavida-Os design for object grounding tasks. Given an image and referring expression describing an object, the grounding task requires locating the described object in the image by predicting its bounding box coordinates. In autoregressive vision-language models such as Qwen2.5-VL (Bai et al., 2025), bounding boxes are represented as plain text strings, such as [123, 232, 300, 1021]. At inference, the coordinates are generated sequentially from left to right. This design has several limitations. First, since the model only sees padded and resized image, it is difficult for the model to predict absolute pixel coordinates that depend on the original resolution of the input image. Second, the sequential generation order is slow and inefficient. 1 1024 , To address these issues, we normalize the bounding box coordinates and quantize them into discrete bins. Specifically, given an image of size , we first pad it to square image of size D, where = max(H, ), and normalize the bounding boxes in the padded image to the range [0,1] by dividing the raw pixel coordinates by D. This step makes the coordinates independent of the original input resolution. We then round each coordinate into 1025 bins representing 0 1024 , 1024 and represent them with special tokens. This reduces the number of tokens needed to represent each bounding box to exactly 4. Finally, since Lavida-O is masked diffusion model with bi-directional attention mask and parallel decoding capabilities, we can predict multiple bounding boxes simultaneously. For example, if we want to obtain the bounding boxes of both cute dog and boy, we can initialize text sequence cute dog [m][m][m][m]; boy [m][m][m][m] and perform parallel unmasking of multiple bounding box coordinates. This design is illustrated in Figure 12. 1024 , ..., 1024 2 A.7 Reflection and Planning The unique advantage of unified understanding and generation models is that they can leverage their understanding capabilities to improve generation results. Several works on unified models show that 23 Figure 13: Interleaved Generation with Planning and Reflection. We provide visual examples of interleaved generation, including text-to-image generation with planning (Top), text-to-image generation with reflection (Middle), and image editing with planning (Bottom). We always enable planning during the reflection process. The layout traces is omitted in the middle figure for clarity and better presentation. simple joint training on combination of understanding and generation tasks improves performance on generation tasks (Xie et al., 2024; Deng et al., 2025), particularly in instruction-following capabilities. Lavida-O pushes this paradigm further by introducing two explicit mechanisms to exploit understanding capabilities: planning and reflection. Planning. To improve prompt-following capabilities in text-to-image generation, we ask the model to first generate layout design of objects, which consists of (object, bounding box) pairs, before generating the final image. Such interleaved generation is achieved through the modality-aware masking process described in Section A.3. We illustrate this process in Figure 13 (Top). As shown, planning enables Lavida-O to follow challenging and unintuitive prompts, such as horse above an astronaut. Similarly, we can adopt planning for image editing tasks. Given an input image and an edit instruction, the model can first leverage its grounding capabilities to identify the regions that need to be edited before generating the edited image. This process is illustrated in Figure 13 (Bottom). Reflection. We can improve text-to-image generation performance by leveraging Lavida-Os understanding capability to achieve self-critique and iterative self-improvement. Given an input prompt, the model first generates an image, then performs self-critique step to evaluate whether the generated image matches the prompt. If it does, the generation process terminates. Otherwise, the model generates revised image and attempts to fix the identified issues. This cycle is repeated until an image passes the self-critique process or the maximum number of rounds is reached. At each round, we also invoke the planning capability. Since Lavida-Os context length is limited to 8192 tokens, we truncate the history when necessary to include at most three rounds. This process is illustrated in Figure 13 (Middle). Similar designs have been explored for generation-only models in the context of inference-time scaling, such as Reflect-DiT (Li et al., 2025b) and ReflectionFlow (Zhuo et al., 2025). However, unlike these works, which require an external vision-language model as reward model, LavidaO uniquely unifies layout planning, self-critique, and iterative self-improvement in single model through unified generation process."
        },
        {
            "title": "B Additional Experiment Details and Results",
            "content": "In this section, we document experiment details for better reproducibility, including data pipeline, training hyperparameters, and compute cost. In addition, we also provide additional experimental results on the effectiveness of various design choices employed by Lavida-O, such as the ElasticMoT design, stratified sampling, and the data pipeline. B.1 Setup Pretrained Weights. We use LaViDa (Li et al., 2025a) to initialize the understanding branch and semantic encoder. For the VQ-encoder, we adopt the encoder of Meissonic (Bai et al., 2024). The image generation branch is initialized from the truncated weights of the understanding branch, as described in Section A.2. Data Pipeline. Unlike many frontier models, our model does not make use of any proprietary images or documents. Our training data consists of the following components: A: Text-to-Image Pairs. We source data from LAION-2B (Schuhmann et al., 2022) and COYO-700M (Byeon et al., 2022). We additionally include SA-1B (Kirillov et al., 2023), JourneyDB (Sun et al., 2023), BLIP3o-60k (Chen et al., 2025a), and ShareGPT4o-Image (Chen et al., 2025b). Each dataset is heavily filtered to remove NSFW prompts, low CLIP scores (Radford et al., 2021), low aesthetic scores (Schuhmann, 2022), and low-resolution images. This results in 200M images in our final mix. Where available, we use captions generated by VLMs instead of raw alt-texts. These captions are sourced from existing works including Recap-LAION, Recap-COYO (Wu et al., 2024), and BLIP-3o (Chen et al., 2025a). B: Image-level Understanding Data. We include LLaVA-OneVision (Li et al., 2024a), Open-LLaVA-Next (Chen & Xing, 2024), MAmmoth-VL (Guo et al., 2024), and VisualWebInstruct (Jia et al., 2025). C: Region-level Understanding Data. We include GranD (Rasheed et al., 2024) and RefCOCO (Kazemzadeh et al., 2014). D: Image Editing Data. We include ShareGPT4o-Image (Chen et al., 2025b), GPT-Edit1.5M (Wang et al., 2025b), and the image editing subset of UniWorld-V1 (Hu et al., 2022). E: Interleaved Planning and Reflection Data. For planning data, we manually construct layout dataset by running an open-vocabulary object detector, GroundingDino-L (Liu et al., 2024b), on the outputs of image generation and editing datasets, including BLIP-3o (Chen et al., 2025a), ShareGPT4o-Image (Chen et al., 2025b), and GPT-Edit-1.5M (Wang et al., 2025b). For reflection data, we leverage existing datasets including ReflectDiT (Li et al., 2025b) and ReflectionFlow (Zhuo et al., 2025). 25 Table 7: Training configurations across three stages. We use letters A-E to represent different dataset following Section B.1. Learning Rate Steps β1 β2 optimizer Stage 1 5 106 80k 0.99 0.999 AdamW Stage 2 1 104 400k 0.99 0.999 AdamW Stage 3 2 105 100k 0.99 0.999 AdamW Dataset Used Loaded Parameters Trainable Parameters Und. resolution Gen. resolution B,C 8B 8B 384 {(1, 3), (2, 2)} - 6.4B 2.4B 384 {(1, 3), (2, 2)} 256 512 A,B,C,D,E 10.4B 10.4B 384 {(1, 3), (2, 2)} 1024 Semantic Encoder VQ Encoder Gen. Branch Und. Branch"
        },
        {
            "title": "Trainable\nLoaded\nTrainable\nTrainable",
            "content": "Training Setup. Training consists of three stages. In the first stage, we extend LaViDa to regionlevel tasks such as grounding. In the second stage, we perform large-scale pretraining on text-toimage generation tasks. In the final stage, we jointly train the model on mix of understanding, generation, and interleaved tasks. We document the training hyperparameters, datasets used, active parameter count, and other relevant details in Table 7. In addition, we implement dataset mix scheduler that dynamically adjusts the sampling weight of each dataset throughout training to address data imbalance. Specifically, we assign high weight to new capabilities at the start of each training stage and gradually decay the weight over time. For example, in Stage 1 we have fewer than 1M grounding samples but over 10M image-level understanding samples. To enable efficient acquisition of grounding capability while preventing overfitting, we set the grounding-to-understanding ratio to 3:1 initially, which is gradually decreased to 1:3. We provide further analysis of the scheduler in Section B.4. B.2 Ablation Studies on Elastic-MoT Design In this section, we report ablation results of the Elastic-MoT design, including the size of the generation branch and the number of joint attention layers. Size of Generation Branch. We report the performance of Lavida-O with different sizes of the generation branch during text-to-image pretraining (Stage 2) in Table 8. Results are obtained after 50k training steps with global batch size of 1024. We also document the maximum per-GPU batch size, gradient accumulation steps, and training latency to measure efficiency. Results show that models of different sizes achieve comparable performance after 50k steps. Smaller models (1B, 2B) converge slightly faster and achieve marginally higher performance than larger models (4B, 8B). In terms of latency, smaller models are considerably faster. The 2B model achieves the best balance between performance and efficiency, attaining the highest GenEval and DPG scores while being 3.17 faster. Number of Joint Attention Layers. To study the effect of varying the number of joint attention layers, we conducted two ablation experiments. The first experiment was performed during Stage 2 pretraining. We started with the Stage 1 checkpoint with = 32 layers in the understanding branch and fixed the generation branch size to 4B. We then varied , the number of layers with joint attention, among {8,16,24,32}. The number of non-joint layers, K, is automatically determined by = . Results after 100k training steps are shown in Table 9. Among the four choices, = {16, 24, 32} yield comparable performance, while = 8 shows substantial drop. This suggests that sufficient number of joint attention layers is necessary for strong text-to-image performance, but additional layers beyond threshold provide little benefit. Training latency also decreases when 26 Table 8: Comparison of different model sizes on GenEval, DPG, and training efficiency. We report the performance of Lavida-O with different sizes of the generation branch during the textto-image pretraining (Stage-2) after 50k training steps. We also report the per-GPU batch size and training latency. Architecture Performance Efficiency Parm. Hidden Size GenEval DPG Batch Size Accum. Step Latency (s/it) 4B+1B 4B+2B 4B+4B 4B+8B 1536 2048 3072 4096 0.56 0.57 0.48 0.55 60.8 63.1 55.3 58.6 16 16 8 1 1 2 2 1.98 3.67 8.42 11.64 is smaller (i.e., larger K), since fewer joint layers need to be loaded. = 16 achieves the best balance of speed and performance. We conducted second experiment in Stage 3, where interleaved generation and editing tasks may benefit more from joint attention. Starting from Stage 2 checkpoint pretrained with = 16 layers for 400k steps, we trained for 50k steps under = {16, 24, 32}. Results show two key observations: (1) text-to-image tasks converge faster than image-editing tasks, reaching near-final performance after 50k steps, while editing tasks lag behind; (2) increasing does not significantly improve performance, even for interleaved editing. This may be due to token interference at later layers or the Stage 2 model being optimized with only 16 joint layers. Due to compute constraints, we were unable to retrain Stage 2 with alternative values of . Nevertheless, keeping = 16 is reasonable choice given our setup. Finally, in Stage 3 the efficiency difference is less pronounced, since all 10.4B parameters must be loaded for interleaved training and inference. Table 9: Effect of varying choices of and in partially-decoupled attention design. Efficiency is measured in Stage-2 training. For stage-3 training, we need to load all layers since the data contain mix of text, image, and interleaved generation tasks. Pretraining (Stage-2) SFT (Stage-3) Efficiency GenEval DPG GenEval DPG ImageEdit Latency (s/it) 8 16 24 32 24 16 8 0.57 0.63 0.63 0.61 69.3 75.0 73.3 71.2 - 0.89 0.81 0.85 - 83.2 83.0 83.2 - 3.66 3.60 3.55 2.45 3.67 4.12 5. Weight Initialization. We initialized the 2.4B generation branch with truncated weights from the understanding branch (Section A.2). We also explored initializing from scratch. Figure 14 shows validation loss during the first 20k steps of Stage 2. Truncated initialization converges faster and yields lower loss. B.3 Ablation Studies on Stratified Sampling We compared image generation quality under different sampling strategies on the MJHQ-30k dataset (Li et al., 2024b) with 64 sampling steps. We evaluate the proposed stratified sampler against confidence-based sampling (Chang et al., 2022), uniform random sampling, and Halton sampling (Besnier et al., 2025). Results are reported in Table 10. The stratified sampler achieves the best performance. B.4 Ablation Studies on Data Pipeline Effect of Task Scheduler. To study the effect of the dataset scheduler described in Section B.1, we compare three dataset mixing strategies in Stage 1 training. The goal of Stage 1 is to equip LaViDa with region-level understanding capabilities such as grounding. At this stage, training data includes fewer than 1M grounding samples but over 10M image-level understanding samples. To Figure 14: Effect of truncated initialization. Validation loss comparison of truncated initialization vs. training from scratch during Stage 2. Truncated initialization converges faster and achieves lower loss. Table 10: Performance of Different Samplers in Text-to-Image Generation Tasks. We report the FID scores on MJHQ-30K dataset using different samplers. The proposed stratified sampler achieves the best outcome. Method FID-30k Confidence Uniform Halton Stratified 11.42 8.22 7.38 6. mitigate imbalance, we employ scheduler that dynamically adjusts the sampling weights for new (grounding) and existing (image-level) capabilities. Each batch is drawn from single dataset. For example, when New:Old=1:3, on average 1 4 contain imagelevel data. 4 of batches contain grounding data and 3 We initialize the ratio as New:Old=3:1 and gradually reduce it to 1:3. We compare against fixed ratios of 1:3 and 3:1, reporting results after 20k steps in Table 11. Fixing New:Old=1:3 under-trains grounding, while fixing New:Old=3:1 improves grounding but causes forgetting on image-level understanding. By contrast, the dynamic scheduler achieves strong performance on both. Notably, it even outperforms the fixed 3:1 setup on image-level understanding, suggesting it also mitigates overfitting caused by the small grounding dataset. Table 11: Comparison of different task scheduling during Stage 1 Training. We compare the performance under different dataset sampling weights of new capabilities (grounding) and old capabilities (image-level understanding). We explored two fixed sampling ratio 1:3 and 3:1 for New:Old. For the dynamic scheduler, the New:Old ratio is initialized as 3:1 and gradually decreased to 1:3. Method New Capabilities Existing Capabilities RefCOCO RefCOCO+ RefCOCOg MME ChartQA ScienceQA New:Old = 1:3 New:Old = 3:1 Dynamic 83.2 88.8 92.0 74.6 82.4 86.9 78.3 85.7 89. 449 349 436 72.6 65.0 73.4 84.3 75.8 86.4 Does understanding data help generation tasks? To examine whether incorporating understanding data benefits generation, we experimented with removing all grounding data from Stage 3. Results are shown in Table 12. Even without invoking explicit planning, grounding data improves 28 text-to-image and editing tasks, suggesting synergy between the two. When planning is enabled, the gains are further amplified. Table 12: Effect of Grounding Data in Stage 3 Training. To study the synergy between understanding and generation tasks, we explored removing object grounding in Stage 3 Training. This leads to worse overall performance, suggesting jointly training on both tasks imporve the generation performance. Method GenEval DPG ImageEdit w/o grounding data w/ grounding data + planning 0. 0.77 0.85 82.0 81.8 82.9 3.60 3.71 3.80 B.5 Ablation Studies on Reflection and Planning Breakdown of Performance Improvements. We provide detailed breakdown of planning and reflection gains. Table 13 shows results on GenEval. Planning yields large improvements in object positioning (+0.19), while reflection additionally improves counting and attribution. On Image-Edit  (Table 14)  , planning improves adding/removing objects, subject actions, and hybrid instructions. The largest gains are in action (+0.50) and hybrid (+0.16). However, global edits (e.g., style, background) degrade slightly, as these tasks are less aligned with grounding. promising direction for future works is to let the model dynamically decide whether to invoke planning. Table 13: Breakdown of performance improvements on GenEval Dataset. We report the improvements of the planning and reflection mechanism on each category of the text-to-image generation tasks from GenEval Dataset. Baseline +Planning vs. Baseline Single 0.99 0.99 = +Reflection vs. Baseline 1.00 +0.01 Two 0. 0.94 +0.09 0.95 +0.10 Position Counting Color Attribution Overall 0.65 0.84 +0.19 0.89 +0. 0.71 0.75 +0.04 0.85 +0.14 0.86 0.90 +0.04 0.90 +0. 0.58 0.68 +0.10 0.74 +0.16 0.77 0.85 +0.08 0.89 +0. Table 14: Breakdown of performance improvements on Image-Edit Dataset. We report the improvements of the planning mechanism on each category of the image editing tasks from ImageEdit Dataset. Model Baseline + Planning vs. Baseline Add Adjust Extract Replace Remove Background Style Hybrid Action Overall 4.04 4.11 +0.07 3.62 3.67 +0.05 2.01 2.04 +0. 4.39 4.40 +0.01 3.98 4.05 +0.07 4.06 4.00 -0. 4.82 4.75 -0.07 2.94 3.10 +0.16 3.54 4.04 +0. 3.71 3.80 +0.09 Effect of Inference-time Scaling. We evaluate reflection scaling by varying , the maximum number of images generated per prompt. Table 15 shows results. Even one reflection step (N = 2) improves performance. Gains saturate at = 8, with little benefit beyond. Latency grows sublinearly with since simple prompts often trigger early stopping. For example, when = 20, the model may obtain satisfying output and terminate the generation process after generating just two images. B.6 SpeedQuality Tradeoff key advantage of masked diffusion models over autoregressive models is the speedquality tradeoff enabled by parallel decoding. We study this in the unified setting by evaluating Lavida-O on Table 15: Performance and Latency at different numbers of reflection rounds . When = 1, we only perform planning. Num. of Reflection Rrounds N=1 N=2 N=4 N=8 N=12 N=16 N=20 GenEval Score 0.848 0.864 0.875 0.882 0.890 0. 0.886 Latency (s/image) 27.2 32.6 39.3 47. 53.4 58.3 62.2 Figure 15: Speedquality tradeoff on generation, grounding, and reasoning. Latency (s/sample) and benchmark scores are shown. For MJHQ: FID (lower is better). For RefCOCO: Precision@0.5 (higher is better). For MathVista: accuracy (higher is better). On MathVista, the maximum generation length is capped at 256 tokens. MJHQ-30k text-to-image generation (Li et al., 2024b), RefCOCO grounding (Kazemzadeh et al., 2014), and MathVista reasoning (Lu et al., 2023). For MJHQ and RefCOCO, we vary the number of diffusion steps. For MathVista, we employ FastDLLM (Wu et al., 2025a), which adaptively unmasks multiple tokens per step. The tradeoff is controlled via its threshold hyperparameter. Results are shown in Figure 15. For MJHQ we report FID (lower is better), for RefCOCO Precision@0.5 (higher is better), and for MathVista accuracy (higher is better). We compare against several baselines: Flux (Labs, 2024) on T2I, Qwen2.5-VL-7B (Bai et al., 2025) on grounding, and Qwen2.5-VL/Open-LLaVA-Next-8B (Chen & Xing, 2024) on reasoning. LavidaO achieves faster inference and stronger quality on image generation and grounding. For grounding, it reaches up to 6.8 speedup while surpassing Qwen2.5-VL-7B in precision. On MathVista, while less accurate than state-of-the-art AR models, Lavida-O is much faster, and still stronger than popular AR baselines such as Open-LLaVA-Next-8B. Performance also exceeds the base LaViDa (56.9 vs. 44.8)."
        },
        {
            "title": "C Compute Cost",
            "content": "All experiments are conducted on 8 nodes, each equipped with 8 A100 GPUs. The total training amounts to 34.2 days measured by wall clock time, or 53k GPU hours."
        },
        {
            "title": "D Limitations",
            "content": "In this section, we discuss several limitations of Lavida-O. Text Rendering. Since the image generation branch is trained from scratch and we did not explicitly include datasets for text rendering, Lavida-O capability to render and edit text is very limited. We also find that the VQ image tokenizer we use can not faithfully reconstruct small texts. We aim to address this issue in future works by incroprating additional text rendering data and finetune the VQ image tokenizer on screenshots of documents. 30 Pixel Shift. Our image editing datasets, such as GPT-Image-Edit-1.5M (Wang et al., 2025b) contains images distilled from generative models like GPT-4o, which is know to have pixel shift problems. Specifically, even if the instruction only requires editing specific region, the rest of image may still experience small but noticeable changes. As consequence, Lavida-O inherit this problem. We aim to mitigate this by obtaining more clean and high-quality image-editing data. Math Reasoning. The focus of Lavida-O is to build unified multi-modal MDMs capable of both understanding and generation tasks. While its math reasoning capabilities has improved from the base model LaViDa thanks to additional training, there remain considerable gap when compared against state-of-the-art models. We left further improvements on math reasoning tasks to future works. Hallucination. Like all generative models, our model may suffer from hallucination problems at inference. We recommend treating model output with caution instead of blind trust."
        },
        {
            "title": "E Boarder Impact",
            "content": "Lavida-O has strong text-to-image generation capabilities and image-editing capabilities, which may be abused to create various harmful and offensive content. We strongly caution the community against such usecases. Additonally, our model may inherit the biases embedded in the base model LaViDa, as well as biases incorporated in the images and texts of the training data. Our model is intended to be used by researchers to build strong diffusion model for multi-modal applications and explore methods of building future multi-modal foundational models. We do not recommend it be used for any other purposes."
        }
    ],
    "affiliations": [
        "Adobe",
        "UCLA"
    ]
}