{
    "paper_title": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models",
    "authors": [
        "Javier Ferrando",
        "Oscar Obeso",
        "Senthooran Rajamanoharan",
        "Neel Nanda"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This suggests that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 7 5 2 4 1 . 1 1 4 2 : r DO KNOW THIS ENTITY? KNOWLEDGE AWARENESS AND HALLUCINATIONS IN LANGUAGE MODELS Javier Ferrando UPC Oscar Obeso ETH Zürich Senthooran Rajamanoharan Neel Nanda"
        },
        {
            "title": "ABSTRACT",
            "content": "Hallucinations in large language models are widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesnt know about an athlete or movie. This suggests that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have causal effect on the chat models refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have remarkable capabilities (Radford et al., 2019; Brown et al., 2020; Hoffmann et al., 2022; Chowdhery et al., 2023) yet have propensity to hallucinate: generating text that is fluent but factually incorrect or unsupported by available information (Ji et al., 2023; Minaee et al., 2024). This significantly limits their application in real-world settings where factuality is crucial, such as healthcare. Despite the prevalence and importance of this issue, the mechanistic understanding of whether LLMs will hallucinate on given prompt remains limited. While there has been much work interpreting factual recall (Geva et al., 2023; Nanda et al., 2023; Chughtai et al., 2024), it has mainly focused on the mechanism behind recalling known facts, not on hallucinations or refusals to answer, leaving significant gap in our understanding. Language models can produce hallucinations due to various factors, including flawed data sources or outdated factual knowledge (Huang et al., 2023). However, an important subset of hallucinations occurs when models are prompted to generate information they dont possess. We operationalize this phenomenon by considering queries about entities of different types (movies, cities, players, and songs). Given question about an unknown entity, the model either hallucinates or refuses to answer. In this work, we find linear directions in the representation space that encode form of self-knowledge: assessing their own knowledge or lack thereof regarding specific entities. These directions are causally relevant for whether it refuses to answer. We note that the existence of this kind of self-knowledge does not necessarily imply the existence of other forms of self-knowledge, and may be specific to the factual recall mechanism. We find these directions using Sparse Autoencoders (SAEs) (Bricken et al., 2023; Cunningham et al., 2023). SAEs are an interpretability tool for finding sparse, interpretable decomposition of model representations. They are motivated by the Linear Representation Hypothesis (Park et al., 2023; Equal contribution. Work done as part of the ML Alignment & Theory Scholars (MATS) Program. Correspondence to: jferrandomonsonis@gmail.com, balcells.oscar@gmail.com. Known Entity Latent Activations Unknown Entity Latent Activations Michael Jordan Michael Joordan When was the player LeBron James born? When was the player Wilson Brown born? He was born in the city of San Francisco He was born in the city of Anthon just watched the movie 12 Angry Men just watched the movie 20 Angry Men The Beatles song Yellow Submarine The Beatles song Turquoise Submarine Table 1: Pair of sparse autoencoder latents that activate on known (left) and unknown entities (right) respectively. They fire consistently across entity types (movies, cities, songs, and players). Mikolov et al., 2013): that interpretable properties of the input (features) such as sentiment (Tigges et al., 2023) or truthfulness (Li et al., 2023; Zou et al., 2023) are encoded as linear directions in the representation space, and that model representations are sparse linear combinations of these directions. We use Gemma Scope (Lieberum et al., 2024), which offers suite of SAEs trained on every layer of Gemma 2 models (Team et al., 2024), and find internal representations of selfknowledge in Gemma 2 2B and 9B. Arditi et al. (2024) discovered that the decision to refuse harmful request is mediated by single direction. Building on this work, we demonstrate that models refusal to answer requests about attributes of entities (knowledge refusal) can similarly be steered with our found entity recognition directions. This finding is particularly intriguing given that Gemma Scope SAEs were trained on the base model on pre-training data. Yet, SAE-derived directions have causal effect on knowledgebased refusal in the chat model-a behavior incentivized in the finetuning stage. This insight provides additional evidence for the hypothesis that finetuning often repurposes existing mechanisms (Jain et al., 2024; Prakash et al., 2024; Kissane et al., 2024). Overall, our contributions are as follows: Using sparse autoencoders (SAEs) we discover directions in the representation space on the final token of an entity, detecting whether the model can recall facts about the entity, form of self-knowledge about its own capabilities. Our findings show that entity recognition directions generalize across diverse entity types: players, films, songs, cities, and more. We demonstrate that these directions causally affect knowledge refusal in the chat model, i.e. by steering with these directions, we can cause the model to hallucinate rather than refuse on unknown entities, and refuse to answer questions about known entities. We find that unknown entity recognition directions disrupt the factual recall mechanism, by suppressing the attention of attribute extraction heads, shown in prior work (Nanda et al., 2023; Geva et al., 2023) to be key part of the mechanism. We go beyond merely understanding knowledge refusal, and find SAE latents, seemingly representing uncertainty, that are predictive of incorrect answers."
        },
        {
            "title": "2 SPARSE AUTOENCODERS",
            "content": "Dictionary learning (Olshausen & Field, 1997) offers powerful approach for disentangling features in superposition. Sparse Autoencoders (SAEs) have proven to be effective for this task (Sharkey et al., 2022; Bricken et al., 2023). SAEs project model representations Rd into larger dimensional space a(x) RdSAE . In this work, we use the SAEs from Gemma Scope (Lieberum et al., 2024)1, which use the JumpReLU SAE architecture (Rajamanoharan et al., 2024), which defines the function SAE(x) = a(x)Wdec + bdec, (1) 1We use the default sparsity for each layer, the ones available in Neuronpedia (Lin & Bloom, 2024). 2 Figure 1: We identify SAE latents in the final token of the entity residual stream (i.e. hidden state) that almost exclusively activate on either unknown or known entities (scatter plot on the left). Modulating the activation values of these latents, e.g. increasing the known entity latent when asking question about made-up athlete increases the tendency to hallucinate. where a(x) = JumpReLUθ (cid:0)xWenc + benc (cid:1), (2) with the activation function (Erichson et al., 2019) JumpReLUθ(x) = H(x θ), composed by H, the Heaviside step function, and θ, learnable vector acting as threshold. Intuitively, this is zero below the threshold, and then the identity, with discontinuous jump at the threshold. Wenc, benc and Wdec, bdec are the weight matrices and bias of the encoder and decoder respectively. We refer to latent activation to component in a(x), while we reserve the term latent direction to (row) vector in the dictionary Wdec. Equation (1) shows that the model representation can be approximately reconstructed by linear combination of the SAE decoder latents, which often represent monosemantic features (Cunningham et al., 2023; Bricken et al., 2023; Templeton et al., 2024; Gao et al., 2024). By incorporating sparsity penalty into the training loss function, we can constrain this reconstruction to be sparse linear combination, thereby enhancing interpretability: L(x) = SAE(x)2 2 (cid:125) (cid:124) (cid:123)(cid:122) Lreconstruction + λ a(x)0 (cid:125) (cid:123)(cid:122) Lsparsity (cid:124) . (3) Steering with SAE Latents. Recall from Equation (1) that SAEs reconstruct models representation as a(x)Wdec + bdec. This means that the reconstruction is linear combination of the decoder latents (rows) of Wdec plus bias, i.e. (cid:80) aj(x)Wdec[j, :]. Thus, increasing/decreasing the activation value of an SAE latent, aj(x), is equivalent to doing activation steering (Turner et al., 2023) with the decoder latent vector, i.e. updating the residual stream as follows: xnew + αdj. (4)"
        },
        {
            "title": "3 METHODOLOGY",
            "content": "To study how language models reflect knowledge awareness about entities, we build dataset with four different entity types: (basketball) players, movies, cities, and songs from Wikidata (Vrandeˇcic & Krötzsch, 2024). For each entity, we extract associated attributes available in Wikidata. Then, we create templates of the form (entity type, entity name, relation, attribute) and prompt Gemma 2 2B and 9B models (Team et al., 2024) to predict the attribute given (entity type, relation, entity name), for instance: Entity type Relation The movie 12 Angry Men was directed by Entity name Attribute (5) 3 Figure 2: Layerwise evolution of the Top 5 latents in Gemma 2 2B SAEs, as measured by their known (left) and unknown (right) latent separation scores (sknown and sunknown). Error bars show maximum and minimum scores. MaxMin (red line) refers to the minimum separation score across entities of the best latent. This represents how entity-agnostic is the most general latent per layer. In both cases, the middle layers provide the best-performing latents. We then categorize entities into known or unknown. Known entities are those where the model gets at least two attributes correct, while unknown are where it gets them all wrong, we discard any in-between. To measure correctness we use fuzzy string matching2. See Appendix for description of the process. Finally, we split the entities into train/validation/test (50%, 10%, 40%) sets. We run the model on the set of prompts containing known and unknown entities. Inspired by Meng et al. (2022a); Geva et al. (2023); Nanda et al. (2023) we use the residual stream of the final token of the entity, xknown and xunknown. In each layer (l), we compute the activations of each latent in the SAE, i.e. al,j(xknown ). For each latent, we obtain the fraction of the time that it is active (i.e. has value greater than zero) on known and unknown entities respectively: ) and al,j(xunknown l known l,j = (cid:80)N known 1[al,j(xknown known l,i ) > 0] , unknown l,j = (cid:80)N unknown 1[al,j(xunknown l,i unknown ) > 0] , (6) where known and unknown are the total number of prompts in each subset. take the difference, obtaining the latent separation scores sknown sunknown l,j , for detecting known and unknown entities respectively. l,j = known = unknown l,j known l,j l,j unknown l,j Then, we and"
        },
        {
            "title": "4 SPARSE AUTOENCODERS UNCOVER ENTITY RECOGNITION DIRECTIONS",
            "content": "We find that the separation scores of some of the SAE latents in the training set are high, i.e. they fire almost exclusively on tokens of either known or unknown entities, as depicted in the scatter plot in Figure 1 for Gemma 2 2B and Figure 8, Appendix for Gemma 2 9B. An interesting observation is that latent separation scores reveal consistent pattern across all entity types, with scores increasing throughout the model and reaching peak around layer 9 before plateauing (Figure 2). This indicates that latents better distinguishing between known and unknown entities are found in the middle layers. We also examine the level of generality of the latents by measuring their minimum separation score across entity types (t): players, song, cities and movies. high minimum separation score indicates that latent performs robustly across entity types, suggesting strong generalization capabilities. For this purpose, for each layer (l) we compute MaxMinknown,l = maxj mint sknown,t , and similarly for unknown entities. The increasing trend shown in the MaxMin (red) line in Figure 2 for Gemma 2 2B and in Figure 9, Appendix for Gemma 2 9B suggests that more generalized latentsthose that distinguish between known and unknown entities across various entity typesare concentrated in these intermediate layers. This finding points to hierarchical organization of entity representation within the model, with more specialized, worse quality, latents in earlier layers and more generalized, higher quality entity-type-agnostic features emerging in the middle layers. l,j Next, we compute the minimum separation scores by considering every SAE latent in every layer, i.e. mint sknown,t for 1 and 1 dSAE, and equivalently for unknown entities. l,j 2https://github.com/seatgeek/thefuzz. 4 Figure 3: Left: Number of times Gemma 2 2B refuses to answer in 100 queries about unknown entities. We examine the unmodified original model, the model steered with the known entity latent and unknown entity latent, and the model with the unknown entity latent projected out of its weights (referred to as Orthogonalized model). Steering with (10) random latents are shown for comparison. Right: This example illustrates the effect of steering with the unknown entity recognition latent (same as in Table 1). The steering induces the model to refuse to answer about well-known basketball player. To ensure specificity to entity tokens, we exclude latents that activate frequently (>2%) on random tokens sampled from the Pile dataset (Gao et al., 2020). The latents with highest minimum separation scores exhibit the most generalized behavior out of all latents, and will be the focus of our subsequent analysis: known entity latent = arg max and unknown entity latent = arg max l,j min sknown,t l,j (cid:123)(cid:122) (cid:125) min known separation score of latent l, across entity types (cid:124) l,j . min sunknown,t l,j (cid:125) (cid:123)(cid:122) (cid:124) min unknown separation score of latent l, across entity types (7) Table 1 demonstrates the activation patterns of the Gemma 2 2B topmost known entity latent on prompts with well-known entities (left), and the patterns for the topmost unknown entity latent (right), firing across entities of different types that cannot be recognized. In Appendix we provide the activations of these latents on sentences containing diverse set of entity types, suggesting that indeed they are highly general. In the following sections, we explore how these latents influence the models overall behavior."
        },
        {
            "title": "REFUSAL",
            "content": "We define knowledge refusal as the model declining to answer question due to reasons like lack of information or database access as justification, rather than safety concerns. To quantify knowledge refusals, we adapt the factual recall prompts as in Example 5 into questions: Relation Entity type Who directed the movie 12 Angry Men ? (8) Entity name Attribute and we define set of common knowledge refusal completions and detect if any of these occur with string matching, e.g. Unfortunately, dont have access to real-time information.... Gemma 2 includes both base model, and fine-tuned chat (i.e. instruction tuned) model. In Section 4 we found the entity recognition latents by studying the base model, but here focus on the chat model, as they have been explicitly fine-tuned to perform knowledge refusal where appropriate (Team et al., 2024)3, and the factuality of chat models is highly desirable. We hypothesize that entity recognition directions could be used by chat models to induce knowledge refusal. To evaluate this, we use test set sample of 100 questions about unknown entities, 3The Gemma 2 technical report (Team et al., 2024) mentions including subsets of data that encourage refusals to minimize hallucinations improves performance on factuality metrics. This pattern is consistent with recent language models, such as Llama 3.1 (Dubey et al., 2024), where the explicit finetuning process for knowledge refusal has been documented. 5 and measure the number of times the model refuses by steering (as in Equation (4)) with the entity recognition latents the last token of the entity and the following end-of-instruction-tokens.4 Figure 3 (left) illustrates the original model refusal rate (blue bar), showing some refusal across entity types. We see that the entity recognition SAE latents found in the base model transfer to the chat model and, by increasing the known entity latent activation we are able to reduce refusal in three entity types. Conversely, increasing the unknown entity latent induces almost 100% refusal across all entity types. We also include an Orthogonalized model baseline, which consists of doing weight orthogonalization (Arditi et al., 2024) on every matrix writing to the residual stream. Weight orthogonalization modifies each row of weight matrix to make it perpendicular to specified direction vector d. This is achieved by subtracting the component of each row that is parallel to d:"
        },
        {
            "title": "W new",
            "content": "out Wout Woutdd. By doing this operation on every output matrix in the model we ensure no component is able to write into that direction. The resulting orthogonalized model exhibits large reduction in refusal responses, suggesting this direction plays crucial role in the models knowledge refusal behavior. We also include the average refusal rate after steering with 10 differents random latents, using the same configuration (layer and steering coefficient) that the known and unknown entity latents respectively. Additional analysis of the Gemma 2 9B model, detailed in Section F, reveals similar patterns, albeit with less pronounced effects compared to the 2B model. (9) Figure 3 (right) shows refusal response for well-known basketball player generated by steering with the unknown entity latent. In Figure 1 (right) we observe that when asked about non-existent player, Wilson Brown, the model without intervention refuses to answer. However, steering with the known entity latent induces hallucination."
        },
        {
            "title": "6 MECHANISTIC ANALYSIS",
            "content": "Entity Recognition Directions Regulate Attention to Entity. In the previous section, we saw that entity recognition latents had causal effect on knowledge refusal. Here, we look at how they affect the factual recall mechanism (aka circuit) in prompts of the format of Example 5. This has been well studied before on other language models (Nanda et al., 2023; Geva et al., 2023; Meng et al., 2022a). We replicate the approach of Nanda et al. (2023) on Gemma 2 2B and 9B and find similar circuit. Namely, early attention heads merge the entitys name into the last token of the entity, and downstream attention heads extract relevant attributes from the entity and move them to the final token position (Figure 4 (a, b)), this pattern holds across various entity types and model sizes (Appendix and Appendix J). To do the analysis, we perform activation patching (Geiger et al., 2020; Vig et al., 2020; Meng et al., 2022a) on the residual streams and attention heads outputs (see Appendix for detailed explanation on the method). We use the denoising setup (Heimersheim & Nanda, 2024), where we patch representations from clean run (with known entity) and apply it over the run with corrupted input (with an unknown entity).5 Expanding on the findings of Yuksekgonul et al. (2024), who established link between prediction accuracy and attention to the entity tokens, our study reveals large disparity in attention between known and unknown entities, for instance the attribute extraction heads L18H5 and L20H3 (Figure 4 (c)), which are overall relevant across entity types in Gemma 2 2B (see example of attributes extracted by these heads in Appendix L). Notably, attention scores are higher when faced with known entity. We also observe causal relationship between the entity recognition latents and the behavior of these attention heads. Steering with the top unknown entity latent reduces the attention to the last token of the entity, even in prompts with known entity (Figure 4 (d)), while steering with the known entity latent increases the attention scores (Figure 4 (e)). We show in Figure 4 (f) the results of steering with random vector baseline for comparison, and in Appendix the results of steering with random SAE latent. In Appendix we illustrate the average attention score change to the entity tokens after steering on the residual streams of the last token of the entities 4We use validation set to select an appropriate steering coefficient α. In Appendix we show generations of Gemma 2B IT with different steering coefficients. We select α [400, 550], which corresponds to around two times the norm of the residual stream in the layers where the entity recognition latents are present (Appendix E). 5We show the proportion of logit difference recovered after each patch in Figure 4 (a). recovered logit difference of 1 indicates that the prediction after patching is the same as the original prediction in the clean run. 6 Figure 4: (a,b) Activation patching on the residual streams and the output of attention heads in the last position (song entities). We patch clean (from known entities prompts) representations into corrupted forward pass (from unknown entities prompts) and measure the logit difference recovered. (c) Attention paid from the last position to the last token of the entity is greater when faced with known entity in attribute-extraction heads. (d,e,f) Effect on attention scores, as in (c), after steering the last token of the entity with the unknown entity latent (d), known entity latent (e), and random vector with same norm (f). Figure 5: Logit difference between Yes and No predictions on the question Are you sure you know the {entity_type} {entity_name}? Answer yes or no. after steering with unknown (left) and known (right) entity recognition latents. in Gemma 2 2B and 9B with the top 3 known and unknown entity latents. The results reveal an increase/decrease attention score across upper layer heads, with the 9B model showing more subtle effects when steered using unknown latents. These results provide compelling evidence that the entity recognition SAE latent directions play crucial role in regulating the models attention mechanisms, and thereby their ability to extract attributes. Early Entity Recognition Directions Regulate Expressing Knowledge Uncertainty. We have shown that the entity recognition latents causally affect the models knowledge refusal, implicitly using its knowledge of whether it recognises an entity, but not whether they are used when explicitly asking model whether it recognises an entity. To investigate this, we use the following prompt 7 Unknown Latent Activations Apparently one or two people were shooting or shooting at each other for reasons unknown when eight people were struck by the gunfire ...and the Red Cross all responded to the fire. The cause of the fire remains under investigation. The Witcher Card Game will have another round of beta tests this spring (platforms TBA) His condition was not disclosed, but police said he was described as stable. Table 2: Activations of the Gemma 2B IT unknown latent on the maximally activating examples provided by Neuropedia (Lin & Bloom, 2024). structure: Are you sure you know the {entity_type} {entity}? Answer yes or no. Answer: (10) We then steer the residual streams of the last token of the entity by upweighting the entity recognition latents. In Figure 5 we show the results on the logit difference between Yes and No responses. The left plot illustrates the effect of steering known entities prompts with the unknown entity latent. This intervention results in reduction of the logit difference. For comparison, we include random baseline where we apply randomly sampled SAE latent with the same coefficient. In the right plot, we steer unknown entities prompts with the known entity latent. Despite the models inherent bias towards Yes predictions for unknown entities (indicated by positive logit differences in the Original column), which indicates the model struggles to accurately express their uncertainty (Yona et al., 2024), this intervention leads to positive shift in the logit difference, suggesting that the entity recognition latents, although slightly, have an effect on the expression of uncertainty about knowledge of entities. similar pattern can be observed in Gemma 2 9B (Appendix N)."
        },
        {
            "title": "7 UNCERTAINTY DIRECTIONS",
            "content": "Having studied how base models represent features for entity recognition, we now explore internal representations that may differentiate between correct and wrong answers. Our investigation focuses on chat models, which are capable of refusing to answer, and we search for directions in the representation space signaling uncertainty or lack of knowledge potentially indicative of upcoming errors. For this analysis we use our entities dataset, and exclude instances where the model refuses to respond, and leave only prompts that elicit either correct predictions or errors from the model. Our study focuses on the study of the residual streams before the answer. We hypothesize that endof-instruction tokens, which always succeed the instruction, may aggregate information about the whole question (Marks & Tegmark, 2023).6 We select the token model and use examples such as: <start_of_turn>usernWhen was the player Wilson Brown born?<end_of_turn>n<start_of_turn>modeln (11) For each entity type and layer with available SAE we extract the representations of the model residual stream, for both correct and mistaken answers, and gather the SAE latent activations. We are interested in seeing whether there are SAE latents that convey information about how unsure or uncertain the model is to answer to question, but still fails to refuse, giving rise to hallucinations. To capture subtle variations in model uncertainty, which may be represented even when attributes are correctly recalled, we focus on quantifying differences in activation levels between correct and incorrect responses. For each latent, we compute the t-statistic using two activation samples: al,j(xcorrect ) for correct responses and al,j(xerror ) for incorrect ones. The t-statistic measures how different the two sample means are from each other, taking into account the variability within the samples: l t-statisticl,j = µ(al,j(xcorrect (cid:113) σ(al,j (xcorrect ncorrect )) µ(al,j(xerror + σ(al,j (xerror ))2 ))2 )) nerror . (12) 6This concept was termed by Tigges et al. (2023) as the summarization motif. 8 Figure 6: Left: Activation values of the Gemma 2B IT unknown latent on correct and incorrect responses. Right: Top 10 tokens with the highest logit increases by the unknown latent influence. We use pre-trained SAE for the 13th layer (out of 18) of Gemma 2B IT7, and the available Gemma Scope SAEs for Gemma 2 9B IT, at layers 10, 21, and 32 (out of 42). Our approach for detecting top latents, similar to the entity recognition method described in Section 4 focuses on the top latents with the highest minimum t-statistic score across entities, representing the most general latents. We split the dataset into train and test sets, and use the training set to select the top latents. The left panel of Figure 6 reveals distinct separation between the latent activations at the model token when comparing correct versus incorrect responses in the test set. Using this latent as classifier, it achieves 73.2 AUROC score, and by calibrating the decision threshold on validation set, it gets an F1 score of 72. See Appendix with separated errors by entity type. Table 2 illustrates the activations of the highest-scoring latent in Gemma 2B ITs SAE on large text corpus (Penedo et al., 2024)8, showing it triggers on text related to uncertainty or undisclosed information. Figure 6 (right) illustrates the top tokens with higher logit increase by this latent, further confirming its association with concepts of unknownness.9 Similar latent separations between correct and incorrect answers can also be observed in Gemma 2 9B IT (Appendix O)."
        },
        {
            "title": "8 RELATED WORK",
            "content": "Recent advances in mechanistic interpretability in language models (Ferrando et al., 2024) have shed light on the factual recall process in these systems. Key discoveries include the aggregation of entity tokens (Nanda et al., 2023), the importance of early MLPs for entity processing (Meng et al., 2022b), and the identification of specialized extraction relation heads (Geva et al., 2023; Chughtai et al., 2024). Despite these insights, there remains significant gap in our understanding of the mechanisms underlying failures in attribute extraction leading to hallucinations. Gottesman & Geva (2024) demonstrated that the performance of probes trained on the residual streams of entities correlates with its ability to answer questions about them accurately. Yuksekgonul et al. (2024) established link between increased attention to entity tokens and improved factual accuracy. (Yu et al., 2024) proposed two mechanisms for non-factual hallucinations: inadequate entity enrichment in early MLPs and failure to extract correct attributes in upper layers. Our research aligns with studies on hallucination prediction (Kossen et al., 2024; Varshney et al., 2023), particularly those engaging with model internals (CH-Wang et al., 2023; Azaria & Mitchell, 2023). Additionally, our work contributes to the growing body of literature on practical applications of sparse autoencoders, as investigated by Marks et al. (2024); Krzyzanowski et al. (2024). While the practical applications of sparse autoencoders in language model interpretation are still in their early stages, our research demonstrates their potential."
        },
        {
            "title": "9 CONCLUSIONS",
            "content": "In this paper, we use sparse autoencoders to identify directions in the models representation space that encode form of self-knowledge about entities. These directions, found in the base model, 7https://huggingface.co/jbloom/Gemma-2b-IT-Residual-Stream-SAEs. We note that Gemma Scope doesnt provide SAEs for Gemma 2 2B IT. 8https://huggingface.co/datasets/HuggingFaceFW/fineweb. 9We omit the players category since Gemma 2B IT refuses to almost all of those queries. 9 are causally relevant to the knowledge refusal behavior in the chat-based model. We demonstrated that, by manipulating these directions, we can control the models tendency to refuse answers or hallucinate information. We also provide insights into how the entity recognition directions influence the model behavior, such as regulating the attention paid to entity tokens, and their influence in expressing knowledge uncertainty. Finally, we uncover directions representing model uncertainty to specific queries, capable of discriminating between correct and mistaken answers. This work contributes to our understanding of language model behavior and opens avenues for improving model reliability and mitigating hallucinations."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was conducted as part of the ML Alignment & Theory Scholars (MATS) Program. We want to express our sincere gratitude to McKenna Fitzgerald for her guidance and support during the program, to Matthew Wearden for his thoughtful feedback on the manuscript, and to Wes Gurnee for initial discussions that helped shape this work. We want to extend our gratitude to Adam Karvonen, Can Rager, Bart Bussmann, Patrick Leask and Stepan Shabalin for the valuable input during MATS. Lastly, we thank the entire MATS and Lighthaven staff for creating the environment that made this research possible. Javier Ferrando is supported by the Spanish Ministerio de Ciencia Innovación through the project PID2019-107579RB-I00 / AEI / 10.13039/501100011033. Portions of this work were supported by the Long Term Future Fund."
        },
        {
            "title": "REFERENCES",
            "content": "Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. Refusal in language models is mediated by single direction. ArXiv, 2024. URL https: //arxiv.org/abs/2406.11717. Amos Azaria and Tom Mitchell. The internal state of an LLM knows when its lying. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 967976, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.68. URL https://aclanthology. org/2023.findings-emnlp.68. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. URL https://transformer-circuits.pub/ 2023/monosemantic-features/index.html. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCanLanguage models are few-shot dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Adlearners. vances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Sky CH-Wang, Benjamin Van Durme, Jason Eisner, and Chris Kedzie. Do androids know theyre only dreaming of electric sheep?, 2023. URL https://arxiv.org/abs/2312.17249v1. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin 10 Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. URL http: //jmlr.org/papers/v24/22-1144.html. Bilal Chughtai, Alan Cooney, and Neel Nanda. Summing up the facts: Additive mechanisms behind factual recall in llms, 2024. URL https://www.arxiv.org/abs/2402.07321. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. Arxiv, 2023. URL https: //arxiv.org/abs/2309.08600. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De 11 Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. ArXiv, 2024. URL https://arxiv.org/abs/2407.21783. N. Benjamin Erichson, Zhewei Yao, and Michael W. Mahoney. Jumprelu: retrofit defense strategy for adversarial attacks. ArXiv, 2019. URL https://arxiv.org/abs/1904.03750. Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R. Costa-jussà. primer on the inner workings of transformer-based language models. ArXiv, 2024. URL https://arxiv.org/abs/ 2405.00208. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. ArXiv, 2024. URL https://arxiv.org/abs/2406.04093. 12 Atticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models partially embed theories of lexical entailment and negation. In Afra Alishahi, Yonatan Belinkov, Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 163173, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.blackboxnlp-1.16. URL https://aclanthology.org/2020.blackboxnlp-1.16. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1221612235, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.751. URL https://aclanthology.org/2023. emnlp-main.751. Daniela Gottesman and Mor Geva. Estimating knowledge in large language models without generating single token. ArXiv, 2024. URL https://arxiv.org/abs/2406.12673. Stefan Heimersheim and Neel Nanda. How to use and interpret activation patching. Arxiv, 2024. URL https://arxiv.org/abs/2404.15255. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3001630030. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ hash/c1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, 2023. URL https: //arxiv.org/abs/2311.05232. Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Tim Rocktäschel, Edward Grefenstette, and David Krueger. Mechanistically analyzing the effects of finetuning on procedurally defined tasks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=A0HKeKl4Nl. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12), mar 2023. ISSN 0360-0300. doi: 10.1145/3571730. URL https: //doi.org/10.1145/3571730. Connor Kissane, Robert Krzyzanowski, Arthur Conmy, and Neel Nanda. Base llms refuse too. LessWrong, 2024. URL https://www.alignmentforum.org/posts/YWo2cKJgL7Lg8xWjj/ base-llms-refuse-too. Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal. Semantic entropy probes: Robust and cheap hallucination detection in llms, 2024. URL https: //arxiv.org/abs/2406.15927. Robert Krzyzanowski, Connor Kissane, Arthur Conmy, We inAI Alignspected every head in GPT-2 small using saes so you dont have to. ment Forum, 2024. URL https://www.alignmentforum.org/posts/xmegeW5mqiBsvoaim/ we-inspected-every-head-in-gpt-2-small-using-saes-so-you-don. and Neel Nanda. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time In Thirty-seventh Conference intervention: Eliciting truthful answers from language model. on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= aLLuYpn83y. 13 Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, János Kramár, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. ArXiv, 2024. URL https://arxiv.org/abs/ 2408.05147."
        },
        {
            "title": "Johnny",
            "content": "Lin accelerating 2024. announcing-neuronpedia-platform-for-accelerating-research. Platform for Joseph Bloom. into Forum, https://www.alignmentforum.org/posts/BaEQoxHhWPrkinmxd/ and research URL"
        },
        {
            "title": "AI Alignment",
            "content": "autoencoders. neuronpedia:"
        },
        {
            "title": "Announcing",
            "content": "sparse Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets, 2023. URL https://arxiv.org/abs/2310.06824. Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. ArXiv, 2024. URL https://arxiv.org/abs/2403.19647. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 1735917372. Curran Associates, Inc., 2022a. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36, 2022b. arXiv:2202.05262. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/ paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: survey, 2024. URL https://arxiv.org/ abs/2402.06196. Fact findNeel Nanda, Senthooran Rajamanoharan, AI Aligning: Attempting to reverse-engineer ment Forum, 2023. URL https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/ fact-finding-attempting-to-reverse-engineer-factual-recall. János Kramár, and Rohin Shah. recall on the neuron level. factual Bruno A. Olshausen and David J. Field. Sparse coding with an overcomplete basis set: strategy employed by v1? Vision Research, 37(23):33113325, 1997. ISSN 0042-6989. doi: https:// doi.org/10.1016/S0042-6989(97)00169-7. URL https://www.sciencedirect.com/science/ article/pii/S0042698997001697. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. Arxiv, 2023. URL https://arxiv.org/abs/2311.03658. Judea Pearl. Causality. Cambridge University Press, 2 edition, 2009. doi: 10.1017/ CBO9780511803161. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning enhances existing mechanisms: case study on entity tracking. arXiv, 2024. URL https: //arxiv.org/abs/2402.14811. 14 Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. URL Language models are unsupervised multitask learners. https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_ are_unsupervised_multitask_learners.pdf. OpenAI Blog, 2019. Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Kramár, and Neel Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders, 2024. URL https://arxiv.org/abs/2407.14435."
        },
        {
            "title": "Lee",
            "content": "Sharkey, superposition with URL interim-research-report-taking-features-out-of-superposition. and Beren Millidge. autoencoders. of 2022. https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/ Dan Braun, sparse"
        },
        {
            "title": "AI Alignment",
            "content": "features Forum,"
        },
        {
            "title": "Taking",
            "content": "out Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozinska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at practical size. ArXiv, 2024. URL https://arxiv.org/abs/2408.00118. Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html. Curt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. Linear representations of sentiment in large language models. Arxiv, 2023. URL https://arxiv.org/abs/2310.15154. 15 Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization, 2023. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation, 2023. URL https://arxiv.org/abs/2307.03987. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Investigating gender bias in language models using causal mediation and Stuart Shieber. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), analysis. Advances in Neural Information Processing Systems, volume 33, pp. 1238812401. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 92650b2e92217715fe312e6fa7b90d82-Abstract.html. Denny Vrandeˇcic and Markus Krötzsch. Wikidata: free collaborative knowledgebase. ACM, 2024. URL https://cacm.acm.org/research/wikidata/. Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=NpsVSN6o4ul. Gal Yona, Roee Aharoni, and Mor Geva. Can large language models faithfully express their intrinsic uncertainty in words?, 2024. URL https://arxiv.org/abs/2405.16908. Lei Yu, Meng Cao, Jackie Chi Kit Cheung, and Yue Dong. Mechanistic understanding and mitigation of language model non-factual hallucinations. arXiv, 2024. URL https://arxiv.org/abs/ 2403.18167. Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. Attention satisfies: constraint-satisfaction lens on factual errors of language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=gfFVATffPd. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. Representation engineering: top-down approach to ai transparency. Arxiv, 2023. URL https://arxiv.org/abs/2310.01405."
        },
        {
            "title": "A ENTITY DIVISION INTO KNOWN AND UNKNOWN",
            "content": "Figure 7: Pipeline for classifying entities as known or unknown. Each entity ei is evaluated by querying the language model about set of attributes A(ei). Classification as known or unknown is based on the accuracy of the models responses. In this work we set the threshold τ = 1. Entity Type Number of entities Attributes Player Movie City Song 7487 10895 7904 8448 Birthplace, birthdate, teams played Director, screenwriter, release date, genre, duration, cast Country, population, elevation, coordinates Artist, album, publication year, genre Table 3: Entity types and attributes extracted from Wikidata."
        },
        {
            "title": "B ENTITY RECOGNITION LATENTS ON DIVERSE ENTITIES",
            "content": "Known Entity Latent Activations Unknown Entity Latent Activations Many people use Twitter to share their thoughts. Many people use Twitter to share their thoughts. LOréal is large cosmetics and beauty company. LOréal is large cosmetics and beauty company. The Mona Lisa is displayed in the Louvre museum. The Mona Lisa is displayed in the Louvre museum. Many people use Snapchat for sharing photos and short videos. Many people use Snapchat for sharing photos and short videos. The Acropolis is an ancient citadel in Athens. The Acropolis is an ancient citadel in Athens. The Galapagos Islands are known for their unique wildlife. The Galapagos Islands are known for their unique wildlife. Many people use Dropbox for cloud storage. Many people use Dropbox for cloud storage. The pyramids of Giza were built by ancient Egyptians. The pyramids of Giza were built by ancient Egyptians. Walmart is the worlds largest company by revenue. Walmart is the worlds largest company by revenue. FedEx is multinational delivery services company. FedEx is multinational delivery services company. Many people use Instagram to share photos. Many people use Instagram to share photos. The Neuschwanstein Castle inspired Disneys Sleeping Beauty Castle. The Neuschwanstein Castle inspired Disneys Sleeping Beauty Castle. The theory of gravity was developed by Isaac Newton. The theory of gravity was developed by Isaac Newton. Sony is known for its electronics and entertainment products. Sony is known for its electronics and entertainment products. Many people use Skype for voice and video calls. Many people use Skype for voice and video calls. The Sistine Chapel is famous for its frescoes by Michelangelo. The Sistine Chapel is famous for its frescoes by Michelangelo. The Andes are the longest continental mountain range in the world. The Andes are the longest continental mountain range in the world. The theory of evolution was proposed by Charles Darwin. The theory of evolution was proposed by Charles Darwin. Many people use Shopify for e-commerce platforms. Many people use Shopify for e-commerce platforms. Honda is known for its motorcycles and automobiles. Honda is known for its motorcycles and automobiles. Table 4: Activations of Gemma 2 2B entity recognition latents on LLM generated data. 18 Known Entity Latent Activations Unknown Entity Latent Activations Druids commune with nature in the sacred grove of Elthalas. Druids commune with nature in the sacred grove of Elthalas. Adventurers Zephyrion. seek the lost treasure of King Adventurers Zephyrion. seek the lost treasure of King The Thaumaturges Guild specializes in Aether manipulation. The Thaumaturges Guild specializes in Aether manipulation. The Vorpal Blade was forged by the legendary Jabberwock. The Vorpal Blade was forged by the legendary Jabberwock. The Hivemind of Xarzith threatens galactic peace. The Hivemind of Xarzith threatens galactic peace. Travelers must appease the Stormcaller to cross the Tempest Sea. Travelers must appease the Stormcaller to cross the Tempest Sea. Archaeologists unearthed artifacts from the Zanthar civilization. Archaeologists unearthed Zanthar civilization. artifacts from the Sailors fear the treacherous waters of the Myroskian Sea. Sailors fear the treacherous waters of the Myrosian Sea. Scientists studied the unique properties of Quixium alloy. Scientists studied the unique properties of Quixium alloy. The Glibberthorn plant is known for its healing properties. The Glibberthorn plant is known for its healing properties. The Voidwalker emerged from the Abyssal Rift. The Voidwalker emerged from the Abyssal Rift. Alchemists seek to create the legendary Philosophers Stone. Alchemists seek to create the legendary Philosophers Stone. Pilgrims seek enlightenment at the Temple of Ethereal Wisdom. Pilgrims seek enlightenment at the Temple of Ethereal Wisdom. Pilots navigate through the treacherous Astral Maelstrom. Pilots navigate through the treacherous Astral Maelstrom. Merchants trade rare gems in the bazaars of Khalindor. Merchants trade rare gems in the bazaars of Khalindor. Scholars study ancient texts at the University of Arcanum. Scholars study ancient texts at the University of Arcanum. The Vexnor device revolutionized quantum computing. The Vexnor device revolutionized quantum computing. The Whispering Woods are guarded by the Sylvani. The Whispering Woods are guarded by the Sylvani. The Ethereal Conclave governs the realm of spirits. The Ethereal Conclave governs the realm of spirits. The Quantum Forge harnesses the power of Nullstone. The Quantum Forge harnesses the power of Nullstone. Table 5: Activations of Gemma 2 2B entity recognition latents on LLM generated data. 19 GEMMA 2 9B LATENTS ACTIVATION FREQUENCIES ON KNOWN AND"
        },
        {
            "title": "UNKNOWN PROMPTS",
            "content": "Figure 8: Activation frequencies of Gemma 2 9B SAE latents on known and unknown Prompts, in player entity type. GEMMA 2 9B LAYERWISE EVOLUTION OF THE TOP 5 LATENTS Figure 9: Gemma 2 9B layerwise evolution of the Top 5 latents, as measured by their known (left) and unknown (right) latent separation scores (sknown and sunknown). Error bars show maximum and minimum scores. MaxMin (red line) refers to the minimum separation score across entities of the best latent. This represents how entity-agnostic is the most general latent per layer. In both cases, middle layers provide the best-performing latents."
        },
        {
            "title": "E NORM RESIDUAL STREAMS",
            "content": "Figure 10: Norm of the residual streams of the last token of the entity across layers of the different Gemma models. REFUSAL RATES GEMMA 2 9B Figure 11: Left: Number of times Gemma 2 9B refuses to answer in 100 queries about unknown entities. We examine the unmodified original model, the model steered with the known entity latent and unknown entity latent, and the model with the unknown entity latent projected out of its weights (referred to as Orthogonalized model). Steering with 10 random latents are shown for comparison. Right: This example illustrates the effect of steering with the unknown entity recognition latent. The steering induces the model to refuse to answer about well-known basketball player."
        },
        {
            "title": "G EXAMPLE OF GENERATIONS STEERING WITH DIFFERENT COEFFICIENTS",
            "content": "α 0 100 200 300 500 600 700 800 900 Question: Where was born the player Leo Barnhorst? Generation Leo Barnhorst was born in **Berlin, Germany**. Leo Barnhorst was born in **Germany**. do not have access to real-time information, including personal details like birthplaces. do not have access to real-time information, including personal details like birthplaces. couldnt find any information about player named Leo Barnhorst. believe youre asking about**Leo Barnhorst**, professional soccer player. Im unable to provide specific details about the birthplace of player named Leo Barnhorst. ?nnPlease provide me with the correct spelling of the players name. rnnI believe youre asking about Leo Barnhart, professional soccer player. \"rnnI believe youre asking about **Leo Barnhart**, professional soccer player. rnnI believe youre asking about **Leo Barnhart**, professional soccer player. 1100 Associate the player Leo Barnhart with the sport of baseball. 1200 criminator: Im sorry, but dont have access to real-time information, including personal details like birthplaces. Table 6: Gemma 2 2B IT responses to Where was born the player Leo Barnhorst? at different steering coefficient values, α in Equation (4). Leo Barnhorst is unknown for Gemma 2 2B."
        },
        {
            "title": "H ACTIVATION PATCHING",
            "content": "Figure 12: Activation Patching done over the residual stream. Activation patching (Vig et al., 2020; Meng et al., 2022a; Geiger et al., 2020; Wang et al., 2023) is an intervention procedure performed during forward pass. We consider clean input, which in our case is the prompt with known entity (Figure 12 left). We compute an intermediate hidden state, e.g. the residual steam value at token James, as in Figure 12. Then, we patch this activation at the same site of the forward pass with the corrupted input. In this case, the corrupted input is prompt with an unknown entity. We can express this intervention using the do-operator (Pearl, 2009) as (corrdo(xunknown xknown)). After the intervention is done, the forward pass continues and the model output is compared with the prediction with the corrupted input. In the experiments in Section 6 we measure the logit difference between the clean (Lakers) and the corrupted predictions (Warriors): logitLakersWarriors(corrdo(xunknown xknown)) logitLakersWarriors(clean) (13) 23 ACTIVATION PATCHING ON GEMMA 2 2B Figure 13: Gemma 2 2B activation patching results on movies (top), players (middle) and cities (bottom). 24 ACTIVATION PATCHING ON GEMMA 2 9B Figure 14: Gemma 2 9B activation patching results on. from top to bottom, song, movies, players and cities."
        },
        {
            "title": "STEERING",
            "content": "Figure 15: Comparison of attention scores to the last token of the entity after steering with random SAE latent from Layer 15."
        },
        {
            "title": "L ATTRIBUTE EXTRACTION HEADS EXAMPLES",
            "content": "Head Entity Extracted Attributes L18H5 L20H3 Kawhi Leonard Detmold Boombastic Kawhi Leonard Detmold Boombastic Clippers, Niagara, Raptors, Westfalen, Lancaster, Volkswagen Jamaican, Reggae, Jamaica, Caribbean NBA, basketball, Clippers, Basketball Germans, German, Germany, Westfalen reggae, Reggae, Jamaican, music, song Table 7: Examples from the top tokens promoted by the attribute extraction heads L18H5 and L20H3 in Gemma 2 2B."
        },
        {
            "title": "M CHANGE IN ATTENTION SCORES TO ENTITIES AFTER STEERING",
            "content": "Gemma 2 2B (Figures 16 and 17) and Gemma 2 9B (Figures 18 and 19) average attention scores to entity tokens after steering with the top known entity latents and top unknown entity latents. Error bars indicate standard deviation. For the known entity latent steering we use prompts with unknown entities, for the unknown entity latent steering we use prompts with known entities. The strength of the steering coefficient is α = 100. We show heads starting from layer 15 in Gemma 2 2B and layer 25 in Gemma 2 9B, coinciding with the point where information propagates to the last position. Figure 16: Gemma 2 2B top 3 known entity latents steering. 27 Figure 17: Gemma 2 2B top 3 unknown entity latents steering. Figure 18: Gemma 2 9B top 3 known entity latents steering. Figure 19: Gemma 2 9B top 3 unknown entity latents steering. 29 GEMMA 2 9B SELF KNOWLEDGE REFLECTION Figure 20: Gemma 2 9B Logit difference between Yes and No predictions on the question Are you sure you know the {entity_type} {entity_name}? Answer yes or no. after steering with unknown (left) and known (right) entity recognition latents.. GEMMA 2 9B IT TOP UNKNOWN LATENTS Figure 21: Top 2 Gemma 2 9B IT unknown latents based on the t-statistic score. GEMMA 2B IT TOP UNKNOWN LATENT WITH SEPARATED ERRORS"
        },
        {
            "title": "BASED ON ENTITY TYPE",
            "content": "Figure 22: Top 2 Gemma 2B IT unknown latent based on the t-statistic score, with errors divided into known and unknown entities."
        }
    ],
    "affiliations": [
        "ETH Zürich",
        "UPC"
    ]
}