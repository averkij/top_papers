{
    "paper_title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models",
    "authors": [
        "Adam Coscia",
        "Shunan Guo",
        "Eunyee Koh",
        "Alex Endert"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance."
        },
        {
            "title": "Start",
            "content": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models Adam Coscia acoscia6@gatech.edu Georgia Institute of Technology Atlanta, Georgia, USA Shunan Guo Eunyee Koh sguo@adobe.com eunyee@adobe.com Adobe Research San Jose, California, USA Alex Endert endert@gatech.edu Georgia Institute of Technology Atlanta, Georgia, USA 5 2 0 2 8 ] . [ 1 1 6 0 1 2 . 8 0 5 2 : r Figure 1: OnGoal tracks and visualizes conversational goals such as requests and suggestions in multi-turn dialogue with LLMs, helping users better evaluate and review their goal progress. Abstract As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. Work done during an internship at Adobe. Supervised work during internship. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). UIST 25, Busan, Republic of Korea 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2037-6/2025/09 https://doi.org/10.1145/3746059.3747746 1 OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through study with 20 participants on writing task, we evaluate OnGoal against baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance. UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. CCS Concepts Human-centered computing Graphical user interfaces; Visualization techniques; Computing methodologies Discourse, dialogue and pragmatics. Keywords LLM, UI, Sensemaking, Visualization, Conversational agent. ACM Reference Format: Adam Coscia, Shunan Guo, Eunyee Koh, and Alex Endert. 2025. OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models. In The 38th Annual ACM Symposium on User Interface Software and Technology (UIST 25), September 28-October 1, 2025, Busan, Republic of Korea. ACM, New York, NY, USA, 18 pages. https://doi. org/10.1145/3746059."
        },
        {
            "title": "1 Introduction\nLarge language models (LLMs) have significantly improved their\nability to handle multi-turn, text-based interactions that span longer,\nmore complex conversations [11, 56]. This has led users to en-\ngage actively in dyadic (two-agent) conversational turn-taking with\nLLMs similar to human-human conversations, using conversational\ngoals such as questions, requests, offers, or suggestions to struc-\nture communication [42]. While multi-turn interactions have been\nshown to increase expressivity and flexibility [38], the length and\nshifting context of back-and-forth conversations can make it chal-\nlenging for users to evaluate and review their conversational goals\nas they evolve over time. For example, users may struggle with\nunder-specified or conflicting goals, parsing long chats for progress,\nor addressing stagnant and forgotten goals [25]. These challenges\noften cause users to repeat prompts, ignore their goals, or restart\nthe conversation entirely, losing valuable progress and insights\ngained along the way [30].",
            "content": "Managing goals that evolve over time or span multiple turns is challenging, particularly with linear chat interfaces. At the same time, these interfaces remain widely adopted for interacting with LLMs, offering familiar and intuitive structure for multi-turn conversations. For instance, an analyst using Copilot to explore sales dataset might start with broad goal like identifying overall sales trends and progressively refine their goal to investigate seasonal patterns or identify outliers. Yet over several turns, it can be difficult for the analyst to evaluate if the LLMs responses address their current goal, or if the model is still addressing prior goals without clear transitions. Alternatively, wedding planner coordinating activities and themes with ChatGPT may be more concerned with reviewing how goals have been addressed over time. As requests and suggestions accumulate, it can become difficult to trace which goals have been fulfilled, which remain pending, or whether new requests contradict with earlier ones as they scroll an ever-growing chat log. By refining the linear chat paradigm, we aim to balance familiarity with functionality, making AI-assisted conversations more effective and user-friendly. These challenges inspire us to ask: How can linear chat interface help users evaluate and review their conversational goals over extended dialogue with LLMs? To address this challenge, we developed OnGoal, chat interface augmented with goal-tracking visualizations to assist users in evaluating and refining their conversational objectives. Our approach is grounded in insights from ML, HCI and visualization literature, which we synthesized into three design challenges around conversing with LLMs. In response to these challenges, OnGoal integrates LLM-assisted goal pipeline to infer, merge, and evaluate users goals against LLM responses over time. The chat interface visualizes goal progress inline with explanations of how an LLM response addresses each goal, as well as on the side in multiple progress summary views, increasing user awareness on goal alignment throughout long, complex LLM responses. Across these views, OnGoal provides text highlighting to help users compare messages over time for key insights related to LLM behaviors including distractions, context switching, and topic drift. To assess OnGoals effectiveness, we conducted user study with 20 participants, focusing on LLM-assisted writing tasks as representative scenario for multi-turn, goal-oriented interactions. We compared how well OnGoal supports goal evaluation and review compared to baseline chat interface without goal tracking or visualizations. We examined the effectiveness of OnGoals features in supporting task outcomes and user experience. Our study also surfaced behavioral insights into how users adapt their communication strategies when interacting with goal-feedback visualizations. Overall, OnGoal encouraged new strategies for communicating goals with the LLM, shifting the time and effort spent away from reading the chat and towards evaluating and reviewing goals, and greater understanding of different LLM behaviors that could cause conversations to derail. Our insights led us to synthesize design implications for future interfaces that help users better evaluate and review conversational goals: (1) provide multiple methods for goal communication, such as setting goals up front or letting them be inferred automatically; (2) enhance interactivity with visual and directive tools, such as letting the user indicate where they want the LLM to focus attention or explain itself; (3) reduce cognitive load with proactive goal tracking by including goal alerts and progress snapshots; and (4) enable feedback on how goals are addressed to improve LLM evaluations and examples. In summary, we contribute: OnGoal, chat interface augmented with in-situ visualizations for tracking conversational goals, developed to address design design challenges we identified in managing conversational goals during multi-turn dialogues with LLMs. user study with 20 participants evaluating OnGoals effectiveness in helping users manage conversational goals in LLM conversations compared to baseline chat interface, and revealing how real-time goal feedback shapes users communication strategies. Design implications for future LLM chat interfaces that improve evaluating and reviewing conversational goals."
        },
        {
            "title": "2 Related Work\nThis work leverages LLMs as conversational agents, defined as AI\nsystems that generate multi-turn, open-domain dialogue dynami-\ncally based on a user’s contextual input. Existing non-LLM agents\nlike Siri and Alexa engage in unstructured, back-and-forth dialogue,",
            "content": "2 OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models UIST 25, September 28-October 1, 2025, Busan, Republic of Korea supporting long multi-turn conversations to accomplish tasks like answering questions [9]. While agents can be mixed-initiative, we focus on augmenting user-initiated interactions. In this section, we discuss related work on interacting with LLMs as conversational agents, sensemaking of LLM conversations, and how visualizations help users better understand LLMs during multi-turn dialogue."
        },
        {
            "title": "2.1 LLM-Based Conversational Agents\nIncreased context lengths have improved LLMs’ ability to under-\nstand and respond to multiple user prompts in sequence, enabling\nnew use cases for conversational interaction [56]. In tasks like writ-\ning and information foraging, conversations often remain 10 turns\nor less due to mental fatigue from an ever-growing chat log [21].\nGao et al. reviewed HCI literature on human-LLM interactions, syn-\nthesizing four phases in which LLM assistance occurs; (1) planning\nthe goals of the conversation; (2) facilitating a new interaction; (3)\nrefining an existing interaction; (4) testing interactions [14]. When\npeople engage with LLMs in multi-turn conversation for help tasks\nlike programming assistance, Ross et al. discovered that multi-turn\nconversations can enhance co-creativity, boost engagement and\nproductivity, and increase resilience to errors over single-turn inter-\nactions [38]. Based on how LLMs are used as conversational agents,\nwe aim to improve the linear chat interface experience when users\nconverse with an LLM in a multi-turn setting to plan, execute, and\nrefine their goals towards accomplishing a specific task.",
            "content": "Despite the growing use of LLMs in multi-turn conversation, LLMs have limitations in handling long conversations. Kim et al. studied dissatisfaction types of GPT responses and found challenges around intent understanding, depth and originality, accuracy, transparency, refusal to answer, ethics and integrity, format and attitude [25]. LLMs can exhibit undesirable behaviors that can derail the conversation, leading to distractions [39], task switching [16] or topic drift [2, 31]. major concern is the tendency of LLMs to forget context from earlier turns, particularly in long conversations. For instance, Liu et al. discovered that LLMs such as GPT-3.5 tend to omit information in the middle of requests, as context windows increase in length [31]. These limitations make it challenging for users to maintain coherent and effective conversations over time. Our work seeks to address these limitations in the context of conversational goal management. In natural language processing (NLP), dialogue state tracking (DST) has been widely used to model user goals and track conversation states, helping analysts uncover valuable interaction patterns [26, 36]. DST is typically employed in post-conversation analysis as classification task, where the space of goals or states is predefined and models are trained on labeled data. However, DST is less suited for dynamic, context-driven conversations with LLMs, where tasks are dynamic and continuously evolving. Because DST is primarily retrospective, it also fails to address users real-time challenges such as maintaining context, understanding response relevance, and tracking shifts in conversation flow. To bridge this gap, our work aims to enhance user awareness of conversation goals and states during interactions, providing real-time support to improve communication and sensemaking."
        },
        {
            "title": "2.2 Sensemaking of LLM Conversations\nUnderstanding long, text-based conversations is inherently chal-\nlenging. Conversational agents can fail to address all parts of a\nuser’s request, lose context or drift as the conversation progresses,\nor provide contradictory responses when the user seeks clarifica-\ntion [19]. Prior work in task decomposition and chain-of-thought\nprompting sought to improve model reasoning and verification by\nbreaking down tasks into sub-tasks or reasoning steps [23, 52]. How-\never, such approaches are predominantly model-centric, aiming to\nimprove a model’s internal reasoning and output verification. Task\ndecomposition and evaluation typically occur within the model\nitself, with intermediate reasoning steps or sub-goals not explicitly\nsurfaced to end users. In contrast, our work focuses on support-\ning users in monitoring and understanding goal progress as they\ninteract, addressing the complementary challenge of user-facing\nfeedback in multi-turn conversations.",
            "content": "Addressing the lack of user-facing support contributes to broader challenges in how users engage with and trust LLMs when accomplishing tasks [37]. For example, Liang et al. conducted survey on the usability of LLMs as conversational assistants [30]. They found that LLM responses can be excessively long and contain too many terms and structures, making them time-consuming to read and difficult to track over time in multi-turn settings, as well as making it unclear how responses connect between prompts. These issues can hinder users ability to understand if their goals were satisfied during conversation with LLMs. To improve the usability of LLM-based conversation agents, researchers have explored methods to provide real-time feedback. Explanations of AI responses have been widely used as critical method to calibrate trust in AI models like LLMs [37]. For example, OpenAI released CriticGPT, model to provide critic feedback on ChatGPT responses during users conversational session [34]. Hernandez-Bocanegra and Ziegler found that interactive, graphical explanations of LLM behaviors can lead to more positive user experiences [18]. Gero et al. studied how visual highlighting techniques could be used to support sensemaking of LLM text at scale [15]. Inspired by these findings, this work enhances LLM-human communication by visualizing goal feedback across messages over time as way to help users make sense of their progress."
        },
        {
            "title": "2.3 Visualizing LLM Conversations\nThe visualization community has a rich history of developing meth-\nods to support the analysis and sensemaking of text conversations\n[3, 47, 49]. Many prior works focus on human-to-human conver-\nsations and post-hoc analysis of interaction patterns [13, 51]. For\nexample, StuGPTViz visualizes conversations between ChatGPT\nand students after the student has already conversed with the LLM\n[8]. Few works have explored visualizing interaction patterns from\nan LLM conversation in real time, and fewer still have explored this\nidea to help users manage their conversational goals.",
            "content": "To enhance LLM conversations, many tools like PromptChainer [54] and PromptAid [35] focus on refining prompts to optimize single-turn responses. Alternatively, visualizations can also improve multi-turn conversations by helping users better manage their goals over time. For example, Hong et al. developed AI Threads as multi-threaded approach to managing conversational context UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. and improving LLM responses [20]. Suchmann et al. developed GUI for visualizing the branching topics of conversation with recommendation system over time, helping users review progress [44]. Going further, few systems like Graphologue [22] and Sensecape [45] aid sensemaking of multi-turn conversations with LLMs using interface designs that break the linear chat format. These tools offer structured ways to manage dialogue flow and let the user perform follow-up interactions through direct manipulation, helping explain or explore LLM responses. These systems introduce new ways to navigate responses and enhance exploration. To do this, they focus on structuring outputs, rather than surfacing the users own conversational goals. In contrast, OnGoal introduces user-facing layer of inferred conversational goals, towards helping users monitor evolving objectives they pose to the LLM. For example, the non-linear chat structures introduced in Graphologue and Sensecape offer powerful sensemaking capabilities, yet they also introduce added complexity that may disrupt users mental models and require steep learning curve. OnGoal instead extends the familiar linear chat interface, embedding goal tracking and visualization directly within the conversational flow to support goal management without disrupting established interaction patterns. Our work contributes to the space of augmenting multi-turn, conversational interactions in linear chat interfaces [7, 41], focusing on supporting users in managing conversational goals throughout extended dialogues."
        },
        {
            "title": "3 Designing Goal Tracking and Visualization\nTo formalize our design rationale, we organized insights from re-\nlated ML, HCI and visualization literature on conversing with LLMs.\nThe advent of LLMs that can handle longer inputs has inspired\nthe HCI and visualization communities to develop and study chat\ninterfaces with LLMs as dyadic conversational agents [24]. How-\never, several ML studies reveal issues that LLMs have in handling\nmulti-turn dialogue, including context switching [16] and topic\ndrift [2, 31]. HCI literature further reveals usability challenges of\nLLMs as conversational agents [30] as well as dissatisfaction in\nhow LLMs address users during conversations [25]. Inspired by\nthis growth and the challenges it poses, we distilled the issues that\nusers might face when interacting with LLMs in multi-turn con-\nversation into key design challenges (C) for our system to address.\nThe challenges are integrated into our descriptions of the system\n(Sect. 4) and evaluation (Sect. 5) throughout the rest of the paper.",
            "content": "C1. With many overlapping conversational goals, LLMs can miss what the user wants. One common reason for dissatisfaction with LLMs as conversational agents is their difficulty in accurately discerning user intent. This often arises when responses focus on unexpected parts of users message, overlook key user goals, struggle to address conflicting goals, or misinterpret what the user meant [25]. For example, helping user write story may require the LLM to balance explicit requests, such as using both formal and informal language, replacing out-dated goals like including more imagery as the story becomes overly saturated, and merging conflicting suggestions. However, growing conversation can cause the LLM to forget or ignore previous user requests [31], further exacerbating problems if prompting continues. Without clear explanations, users may struggle with balancing prompt length and 4 clarity, leading to repeatedly prompting the same message and ultimately getting frustrated and losing confidence when the LLM fails to adapt. chat interface should formalize the communication of the users goals and explain the LLMs understanding and response to user messages, highlighting under-specified, misinterpreted, or ignored goals. This would help users refine their input and reduce communication breakdowns. C2. As messages accumulate, sensemaking across long, complex LLM responses becomes effortful and time-consuming. Even when the LLM correctly interprets user goals, tracking the progress within an increasingly long chat log can become untenable. LLM responses are often overly long and filled with irrelevant fluff, introducing extraneous terms and themes that make it hard to track goal progress [30]. This often compounds with the general challenges of dialogue systems repeating themselves, losing context over time, or contradicting themselves during clarifications [19]. Challenges making sense of the text begin to snowball: evaluating multiple goals against each response, reviewing the progress of goals across responses and determining when goals are fully addressed require more time and effort. These obstacles can shift users attention away from their task and ultimately reduce their confidence [28]. Summarizing goal status and tracking progress over time can alleviate this burden, reducing the need for users to manually sift through the entire chat log. Additionally, such summaries could surface patterns in LLM behavior, helping users to adjust their prompting strategies more effectively. C3. When the conversation derails, undesirable LLM behaviors are often opaque. Sometimes LLMs can fail to address goals in ways that are not immediately clear to the user. For example, LLMs may forget conversational goals over time, get stuck on specific goal, or address the goal in unexpected ways, like generating multiple versions of text or only applying the goal to subsection of the response [9]. These undesirable behaviors can derail the conversation, leading to distractions [39], task switching [16] or topic drift [2, 31]. While model explanations can increase users trust [37], identifying inconsistent behaviors from explanations is tricky with just text alone. As conversation grows, it becomes harder to track how the LLM addressed goals across multiple messages. Visual strategies, such as highlighting relevant text to reveal goal alignment, or comparing key phrases across messages, could help users detect misalignment and help them regain control over the conversation [15]. Additionally, extracting examples within message that exemplify where goal progress has been made and identifying recurring patterns across messages can further support users in understanding how the LLM is or is not addressing their goals."
        },
        {
            "title": "4 The OnGoal System\nBased on our design challenges (Sect. 3), we developed OnGoal, a\nchat interface augmented with in-situ visualizations for real-time,\nhuman-in-the-loop control over setting and tracking conversational\ngoals. Our system integrates a goal pipeline with multiple chat\nviews and text highlighting techniques that help users evaluate\nand review conversational goals as they converse with an LLM in\nmulti-turn dialogue.",
            "content": "The goal pipeline (Sect. 4.1) infers, merges, and evaluates goals against LLM responses, and provides explanations and examples OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Figure 2: An example of the conversational goal pipeline in action. OnGoal uses generative LLM (e.g., GPT-4o) and prompt engineering to infer, merge, and evaluate goals. The results are then visualized in the chat. for how each goal was evaluated to increase communication and transparency. The pipeline is embedded in chat user interface featuring several visualizations (Sect. 4.2) designed to summarize and compare goal progress, including inline goal glyphs, ex-situ timeline views of goal progress, and detailed views of individual goal progress. Finally, OnGoal provides several text highlighting features (Sect. 4.3) that show examples of goal alignment inline in the text, as well as comparison views of key phrases, similar and unique sentences between responses, helping users identify undesirable LLM behaviors to address. Each of OnGoals views addresses unique design challenge in general, domain-agnostic goal-tracking that the other views do not. Inline goal glyphs visualize how the LLM interpreted the user with explanations (C1), so users know what the LLM may be focusing on. Other views lack local explanations for LLM behaviors. As goals evolve over time, the ex-situ progress panel summarizes temporal trends in goal tracking (C2) that users would otherwise have to scroll up and down to find another unique feature. Finally, text highlighting adds concrete examples of LLM behaviors across messages (C3), where other views only address goal completion in each message individually. By combining their unique capabilities, the views in tandem aim to mitigate issues of cognitive overload that users often face when managing longer, multi-turn LLM dialogues."
        },
        {
            "title": "4.1 Modeling Goals Using a Pipeline\nOnGoal utilizes a three-stage goal pipeline to model conversational\ngoals (Fig. 2). Each stage of the pipeline is run by prompting a\ngenerative LLM (OpenAI’s GPT-4o in our implementation) with\ngoals from prior stages. The pipeline LLM is independent of the chat\nLLM that the user is conversing with in the interface. By running\na goal pipeline, OnGoal can infer goals, merge similar goals, and\nevaluate goals against each LLM response. The evaluations also\ngenerate explanations for why the LLM gave the evaluation it did,\ntowards clarifying what the LLM understood about the user’s goals\nand how it addressed them (C1).",
            "content": "5 To create our prompts, we defined conversational goals as questions, requests, offers, or suggestions from the user that an LLM should respond to in turn-taking order [42]. We include this definition in our prompts verbatim. The prompt also assigns one of the four types to each goal inferred. For example, user might prompt the LLM, want to write story, which the goal pipeline would infer as request. The user could suggest, You should make the story happy, offer critique, think the story should be longer, or ask question, Why did you include dog in the story? All these example clauses are conversational goals. Our definition is tailored to LLM-assisted writing as the primary use case in this work. Further, limiting goal inference to these four goal types helped the pipeline successfully infer, merge, and evaluate goals in the context of writing tasks, such as requesting feedback, suggesting edits, and asking questions. However, the definition of conversational goals can vary across real-world applications, which may require adaptation of goal types and inference prompts to fit different domains. Our prompts are included in the appendix (Sect. A). The pipeline performs the following operations in order (see Fig. 2 for an example). Each step can be independently toggled on or off from the OnGoal interface (Fig. 1C). (1) Infer. The pipeline takes the users message as input and infers conversational goals, which include questions, requests, offers, or suggestions. These goals are stored as list and passed to subsequent steps of analysis. (2) Merge. In this step, the pipeline analyzes the existing list of goals from conversation history and compares them with the newly inferred goals from the latest interaction. It then performs one of the three operations: combine similar existing and inferred goals, replace an existing goal with similar or contradictory inferred goal, or keep unique existing and inferred goals. (3) Evaluate. The pipeline takes the final list of merged goals, as well as the chat LLMs response to the users message as input. It then evaluates each goal against the chat LLMs UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. Figure 3: The three tabs in the progress panel. In the goals tab (A), users can control goals by locking or completing them, create their own goals, and visualize goals evaluation history. In the timeline tab (B), the goal pipeline is visualized as node-link diagram, where nodes flow top to bottom and are colored by their evaluation. In the events tab (C), the goal pipeline operations are visualized in list view with more verbose descriptions. Users can brush and link row in the timeline or events view with its corresponding message in the chat by clicking the numbered user icons. response. This prompt is used to determine if the response confirms, contradicts, or ignores the goal, provide an explanation for the evaluation, and extract phrases from the LLM response as supporting evidence. Our pipeline focuses on global goal tracking and support, i.e., parsing and applying goals to an entire LLM response. In the context of writing tasks, global goal tracking is useful to address specific issues in writing. For example, conflicting writing styles can cause the LLM to prioritize goals inconsistently. global perspective can help users understand opaque LLM behaviors (C3), such as the LLM ignoring certain goals due to repeated language patterns that only appear over time. In dialogue state tracking, other types of goal support exist; e.g., fine-grained or local goal support, such as user asking the LLM to edit the tone of particular paragraph or sentence differently from the global tone. We discuss future work exploring local goal support in Sect. 6.2."
        },
        {
            "title": "4.2 Visualizing Goals in the Chat UI\nOnGoal visualizes goal data from each stage of the pipeline in the\nchat interface using both in-situ evaluations and ex-situ summary\nviews. By integrating insights and patterns directly into the inter-\nface, OnGoal aims to improve communication (C1) while reducing\nthe time and effort it takes to make sense of the chat history (C2).",
            "content": "In-situ goal evaluations. OnGoal presents two inline visuals 4.2.1 within the chat messages themselves (Fig. 1A). First, goal glyphs are shown under each message in the chat window to summarize inferred and evaluated goals. Below users prompt, they indicate the goals that were inferred, while below an LLMs response, they reflect the final merged goals and their evaluation results colored accordingly green for confirm, red for contradict, and yellow for ignore. This helps users quickly assess how goals were evaluated at glance and evaluate how the LLM responses addressed their conversational goals over time as they scroll the conversation (C2). Second, clicking on goal glyph opens more-detailed goal explanation inline. For user goal glyphs, the view explains how the goal was inferred and highlights the users phrase that the goal is derived from. For LLM goal glyphs, the panel includes explanations of why the LLM assigned that evaluation as well as supporting evidence extracted from the LLM response. (C1). 4.2.2 Ex-situ progress panel. Next to the chat, the progress panel (Fig. 1B) allows users to track and control specific goals across three tabs (C2). Each tab achieves different evaluation/review objectives: controlling the goal pipeline, visualizing the evaluation and merging of goals over time, and listing all pipeline events for validation. The first tab is the goals tab (Fig. 3A), which lists all final goals from the pipeline as widgets with text description and several controls for the goal pipeline. Users can lock goals from being merged, complete goals to remove them from evaluation, and restore previously merged goals. The second tab, the timeline tab (Fig. 3B), is Sankey-based [48] timeline visualization that summarizes the history of infer, merge and evaluation events from the goal pipeline. The plot flows from top to bottom in sets of three rows for each user prompt / LLM response pair: row 1 shows inferred goals and draws lines from above connecting to existing goals; row 2 lists the final goals and draws lines from the existing and inferred goals above to represent combine, replace, and keep operations; and row 3 shows icons representing the evaluation result from the final goal in the row above: check for confirm, cross for contradict, and prohibited no sign for ignore. The third tab is the events tab (Fig. 3C), which displays events of the goal pipeline in list, grouped by each user prompt / LLM response pair. Within the timeline and events tabs, users can click on the numbered user or LLM icons to have the chat window scroll to the corresponding message. 6 OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Figure 4: Clicking on goal in the goals tab  (Fig. 3)  filters the chat by LLM responses that were evaluated against that goal (A). The evaluations and examples are highlighted in the text by default; clicking an evaluation in the side panel scrolls the conversation to that message. Three text highlighting modes (key phrases, similar and unique sentences) can be toggled to overlay on the chat log (B). Individual goal view. Users can drill down to get details of 4.2.3 an individual goal on demand by selecting it in the goals tab. This filters the chat window to only show LLM responses which were evaluated against the selected goal (Fig. 4A), facilitating easier comparison of LLM responses over time (C2). For example, by scanning all messages related to single goal, users can identify repeating phrases, forgotten requests, or inconsistent responses. In this view, the progress panel transforms to show list of the evaluations for that goal (Fig. 5C); selecting an evaluation scrolls the chat window to the corresponding LLM response."
        },
        {
            "title": "4.3 Highlighting Text to Support Sensemaking\nReading lengthy LLM responses as plain text can make it challeng-\ning to identify patterns about LLM behaviors. Gero et al. show that\ntext highlighting can improve sensemaking behaviors and insights\nwhen evaluating LLM responses at scale [15]. Drawing inspiration\nfrom their insights, OnGoal presents two variations of text high-\nlighting to help users quickly identify relevant clauses in the text\nrelated to potentially problematic LLM behaviors (C3).",
            "content": "First, goal evaluations include examples extracted verbatim from the LLM response that are highlighted inline when goal explanation panel is open (Fig. 4A). Examples are colored by evaluation result: green for examples that show goal is confirmed, yellow for ignored, and red for contradicted. The highlighted clauses exemplify local LLM behaviors that are potentially related to how and why it evaluated particular goal based on the prior user message. For example, users can compare highlighted texts across multiple responses (C3) to assess the consistency of an LLM response regarding specific goals or determine if progress is being made. Second, in the individual goal view, the same goal evaluation examples are highlighted in every LLM response by default. Adapting subset of techniques from Gero et al. [15], OnGoal lets users toggle between three additional text highlighting techniques (Fig. 4B): (1) using generative LLM and prompt engineering, OnGoal extracts all key phrases from each LLM response and highlights the shared and unique phrases; OnGoal tokenizes all sentences, computes text embedding using generative LLM, and computes pairwise cosine similarity between sentences, revealing (2) sentence pairs with the highest similarity (i.e. similar sentences) and (3) the sentences with the lowest average similarity to all other sentences (i.e., unique sentences). These techniques can enable comparison of LLM response at scale related to an individual goal, potentially revealing insights related to global LLM behaviors including distractions and topic drift over time (C3). The prompt for extracting key phrases is included in the appendix (Sect. A.4)."
        },
        {
            "title": "4.4 Usage Scenario\nTo illustrate how OnGoal can be used to support tasks accomplished\nvia multi-turn dialogue with an LLM, we demonstrate a usage\nscenario involving exploratory data analysis (Fig. 5). Consider Jim,\na sales associate working with a customer analytics dataset. With\nlimited time, Jim seeks assistance from an LLM chatbot to quickly\nextract insights from the data and use them to write a data report.\nJim turns to OnGoal to help manage the conversation.",
            "content": "Jim begins by uploading CSV dataset and asking initial questions (Fig. 5A), including basic questions like What are the fastest growing market segments? as well as more advanced ones like What is the relationship between revenue and rating per age group? OnGoal immediately provides goal glyphs under the LLM responses, allowing Jim to evaluate how each of the analysis goals was satisfied with explanations. Jim noticed two of goals are being ignored the high-level goal of writing data report, and the goal about information on the fastest growing market segments. Recognizing 7 UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. Figure 5: Usage scenario of using OnGoal to analyze CSV dataset. After giving the LLM the dataset, the user iteratively forms goals, explores and refines them, evaluates the explanations and examples, and reviews progress over time, using the features of OnGoal to support their workflow. that the goals are ignored, Jim is encouraged to dive deeper into the answers and ask follow ups to get more information. Jim decides to address the ignored goals one by one. This process helps clarify the intention behind the conversational goals, both for the LLM, and for Jim. For example, Jim asks clarifying question about the market growth rate, and gets more details that address what the assistant did not touch on before (Fig. 5B). The goal glyphs and goal tab in the progress view both show progress in confirming goals over time. As Jim continues to chat, they learn more about their goals and update them as new information becomes available. Jim uses the timeline tab in the progress view to reflect on their progress and how their goals have changed over time. The timeline also shows progress, as less goals are carried over each turn. When the assistant struggles, Jim clicks on goal in the goal tab and dives deeper into the history of messages for that goal. They analyze the similarities and differences between responses using key phrases to see if there is any missing or contradicting information (Fig. 5C). For example, by reviewing similar sentences, Jim observes the assistant repeating old information without updating based on new evidence. Overall, this iterative process helped Jim better understand how to communicate their goals to the assistant, reduced the effort in remembering and tracking their goals, and helped them keep the assistant aligned with their goals over time."
        },
        {
            "title": "4.5 Implementation\nThe goal pipeline is implemented in Python using the OpenAI li-\nbrary to make API calls to GPT-4o and stream the results to the\nfrontend interface. OnGoal can work with any decoder LLM ar-\nchitecture and easily be adapted to use open-source LLMs as well\nas local LLMs, e.g., using PyTorch. The OnGoal frontend is imple-\nmented in Vue.js; the visualizations are built using D3.js [5].",
            "content": ""
        },
        {
            "title": "5.1 Study Design\nWe designed a 1 × 2 between-subjects study with 20 participants.\nOur study compares user performance and behavior across two\ninterface conditions: a baseline interface presenting a standard\nLLM-based chat experience with limited goal tracking and no visu-\nalization features; and the full OnGoal interface. Each participant\nwas assigned one of two similar writing tasks to complete, limiting\npotential confounds from learning and task effects. The differences\nin these tasks were not compared.",
            "content": "Interfaces. As part of the design rationale (Sect. 3), we observed that many common issues with multi-turn dialogue could be related to interface design. To understand if OnGoal could address any of these issues, we aimed to gain baseline measure of user behaviors when using typical LLM chat interface and measure the difference in performance with OnGoal. To ensure consistency in the goal inference pipeline and LLM responses between interfaces, we utilized the same LLM framework as OnGoal. We set up the baseline interface as barebones LLM chat interface using the same framework as OnGoal but stripped of any OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Table 1: Six conversational goals that were used in the study task. Editor #1: The Creative Boss Editor #2: The Pragmatic Boss Language Style Persuasiveness Imagery and Metaphors 1. Use non-formal, conversational language 3. Engage storytelling and emotional appeal 5. Include rich imagery and creative metaphors 2. Use formal and technical language 4. Build credibility through research and evidence 6. Prefer facts over figurative language visualizations and with merging and evaluation disabled. Because our task was to address fixed conversational goals (described under Tasks), we reconfigured the progress panel to show limited view of the goals tab with only goal inference enabled in the pipeline. In other words, baseline users would see the initial list of goals and goals being inferred after each conversational turn, but no merging or evaluation in the progress panel. In both conditions, we preloaded the fixed goals in both interfaces and locked them. This ensured both conditions had equal ability to see the same goals, but only OnGoal would have access to visualizations and the results of merge and evaluation. To further control for user behaviors, we also disabled the new goal button, the goal controls (lock, complete, restore), and the goal pipeline checkboxes in the OnGoal condition, as these were not related to the task. Tasks. We designed two different writing tasks for our study from the following template As writer working for two bosses, use an LLM to draft single article that satisfies the goals of both bosses. The following instructions were shown to participants, with either 1. or 2. substituted to counterbalance across participants: As writer for an online blogging company, use GPT to write single, five-paragraph article of tips on {1. traveling to destination of your choice for weekend, 2. hosting an event of your choice at your residence}. You work for two different bosses, each with three specific goals for how the article should be written. You must satisfy all of their goals to the best of your ability. Participants were shown the set of six goals  (Table 1)  , three from each boss reflecting their personal expectations on style and content of the article. These goals are not inferred from the goal pipeline, but instead preloaded and fixed in both interfaces. This way, all participants had an equal opportunity to see and track them using the progress panel and, in the OnGoal condition, use the evaluation stage of the pipeline to evaluate the goals. While these specific goals were not part of the goal pipeline, other goals could still be inferred, merged, and evaluated in the OnGoal interface. Participants were required to satisfy all goals as much as possible and to use the LLM to generate all text included in the final article, but were allowed to use the LLM chat interface in whatever way they wanted to (e.g., copy-pasting template, writing custom instructions, etc.). Writing tasks are commonly used as task for evaluating LLM interfaces [1, 54]. Writing typically involves satisfying global goals such as use positive tone, which our system was designed to interpret as conversational goal to guide the LLM response (Sect. 4.1). We aimed to present goals that were sometimes contradictory, requiring users and the LLM to make intentional decisions on which goals to address, how to address them, and why they addressed them. We discussed our task design with professional copy editors, who confirmed that having multiple bosses with conflicting writing goals for single article often happens, requiring them to balance job requirements with self-evaluation. This scenario is also useful to study how users manage trade-offs between addressing LLM issues and making decisions, and how this affects human-AI alignment. In addition to writing the article, participants were asked to evaluate and review their goals after each message they sent. Upon getting an LLM response, both interfaces present two sets of two initial questions (labeled in Fig. 6, 8), rated on scale from 1 5. The first question in each set, broken into two parts, was: (1) to evaluate whether the LLMs response satisfied single goal chosen at random; and (2) to review whether that same goal chosen at random had been consistently satisfied or dissatisfied across all messages. The second question in each set, presented after each part, asked the participant how confident they were in their assessment. For example, for the goal 2. Use formal and technical language, the participant would evaluate if the LLMs current response used formal and technical language (1 5), if all of the LLMs prior responses also used formal and technical language (1 5), and how confident they were in evaluating (1 5) and reviewing (1 5). The order of the evaluate/review questions in each set was randomized to counterbalance learning effects in tracking time spent. After the task was completed, the participants previous answers were then shown back to them with the interface reloaded to how it appeared at the time when they answered. They would then have an opportunity to validate (labeled in Fig. 6, 8) and revise their answer if they wanted to, using any features of the interface to help them, without the time pressure or the task to worry about. By explicitly asking participants to evaluate and review their goals, we aimed to elicit think-aloud feedback on how the interfaces either did or did not support these tasks, as well as usability findings in the post-study survey and interview related to their overall experience evaluating and reviewing conversational goals. Further, by giving participants the opportunity to revise their answers after the task, we could calibrate findings on time spent, effort expended, and confidence with how participants felt about their initial answer during the task. We analyzed the self-reported answers and report differences in task performance in our usability findings (Sect. 5.2.1). Procedure. Each study lasted 60 minutes in total. We employed between-subjects design, where participants used one of the two possible interfaces and tasks (2 2), chosen at random to control for ordering effects, resulting in 10 participants using our baseline interface (P1 10) and 10 using OnGoal (P11 20). Betweensubjects allowed us to compare independently sampled distributions of self-reported performance factors including time spent, effort, and confidence between interfaces in our analysis, while ensuring participants using one interface did not know about the other. 9 UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. After giving consent, participants first self-reported their demographics in pre-study survey. Then, the researcher conducting the session explained aloud the various features of the chosen interface and the expectations of the task. Participants then performed practice task for 12 minutes in the same style as the live task that was not recorded. After, participants spent 15 minutes completing the live task which was recorded. In both the practice and live task, participants were instructed to follow think-aloud protocol [12], explaining their thoughts, insights, and feedback as they worked. Finally, after they completed both tasks, the participant completed post-study usability survey, and both the participant and researcher engaged in semi-structured debrief interview discussing the participants experience during the session. Participants. We recruited 20 participants (P1 20) at both an academic institution and private company via recruitment emails and interpersonal networking. 14 self-identified as male and 6 selfidentified as female, between the ages of 19 and 37, with median age of 23. 16 participants were pursuing higher education degrees in fields spanning Computer Science (10), Human-Computer Interaction (4), Human-Centered Computing (1), and Information Visualization (1). Four participants held industry roles as researchers (2) and engineers (2). All participants had prior experience using LLM chat interfaces, with all except one using them at least once month or more. Participants self-reported usage of LLM chat interfaces for various tasks; most used it for help with programming (17) and writing (13), some used it for education (9) such as learning topics, studying, and asking questions, or as personal assistant (5), and few used it for research (6) including summarizing papers, brainstorming ideas and analyzing data. 13 participants kept conversations short (i.e. less than five messages), 6 held medium-length conversations (between five and ten messages), and only 1 regularly had long conversations (more than 10 messages). Measures and analysis. We asked participants to use thinkaloud protocol [12] as they worked and summarize how they arrived at their results after completing tasks. We recorded the screen and audio (participant and researcher) as well as interaction logs of all mouse events (clicks, hovers). We also tracked users performance in the task using their responses to the evaluation and review questions described in Tasks above. We then analyzed participants interactions and responses to the in-task questions and post-study surveys for insights on task performance. Following Dragicevic [10], we generated and interpreted sample means as effect size using bootstrapped 95% confidence intervals (CIs) with 10, 000 resamples to represent uncertainty. For given confidence level and sample size, CI width increases with increasing variability; results are considered significant if CIs do not overlap. We report all means and CIs for the baseline (B) and OnGoal (O) conditions, as well as the strength of the comparison, in the text as follows: (B: 𝑚𝑒𝑎𝑛 [𝐶𝐼 ], O: 𝑚𝑒𝑎𝑛 [𝐶𝐼 ], {strong, weak, no} evidence) We also visualize the raw data and 95% CI in each of our results figures (6, 7, 8, 9) as point plots with error bars over top of the raw data as transparent dots. We further evaluated both the video recordings of each session and the audio recordings of participants think-aloud protocol, debrief interview, and the researchers notes, and conducted inductive thematic analysis [6], identifying 10 emergent themes that were discussed amongst all authors relating to issues and workflows encountered using both interfaces."
        },
        {
            "title": "5.2 Study Results\nComparing survey responses, interaction logs, and think-aloud feed-\nback across both tasks, key differences emerged in how participants\nused the baseline interface in comparison with OnGoal.",
            "content": "5.2.1 Usability findings. We first analyzed participants post-study survey results and interaction logs for significant differences between interfaces. From the interaction logs, we analyzed time spent (in seconds), turns taken, goals addressed, and answers to the evaluation and review questions. From the post-study surveys, we analyzed self-reported levels of effort and confidence in completing tasks (1 5), the usefulness of OnGoals features (1 7), and finally perceived accuracy of OnGoals goal pipeline (1 5). Overall, we observed several significant differences (i.e., overlapping 95% confidence intervals) in usability between interfaces. With OnGoal, participants spent more time reviewing during validation, self-reported lower effort reading and reviewing, self-reported lower mental demand, and self-reported higher initial confidence in evaluating goals. We attribute many of these observations to OnGoal users changing their prompting strategy more often and generally feeling more in agreement with the LLM in terms of addressing goals than baseline users. The most common use of OnGoal features was using the goal glyphs and progress panel to identify issues, goal explanations to evaluate how the LLM interpreted goals in their most recent message, and the individual goal view to review the history of messages for patterns related to addressing their goals over time. Finally, OnGoal users found the evaluation stage of the goal pipeline significantly less accurate than the infer, pointing to emergent issues in designing effective goal evaluations. Time, effort, and confidence. We conducted post-study usability survey to get self-reported levels of effort and confidence in performing tasks and compared these reports to logs of time participants spent  (Fig. 6)  . Compared with participants using the baseline interface, participants using OnGoal spent slightly more time evaluating (B: 29.8 [20.9, 38.6], O: 34.1 [22.9, 45.3], weak evidence) and reviewing (B: 19.7 [15, 24.5], O: 24.4 [17.6, 31.2], weak evidence) and less time reading (B: 66.5 [43.4, 89.6], O: 56.8 [42, 71.7], weak evidence). The gap between time spent in the interfaces was slightly larger after the task validating evaluation took somewhat longer (B: 20.2 [13.5, 26.8], O: 26.6 [18.4, 34.9], weak evidence), while validating reviewing took significantly longer (B: 16.8 [12, 21.7], O: 30 [21.2, 38.9], strong evidence). This could suggest participants are encouraged to spend more time reviewing their conversation with OnGoal when not pressed for time. This surprised us. Both conditions were encouraged to work on goal alignment, and both only had access to text box to interact with the LLM with no suggestions on how to fix misalignment. With limited time, we expected OnGoal users to look for ways to offload cognition to the LLM to handle evaluating and reviewing, especially given that OnGoal was more information-heavy compared with baseline. Instead, OnGoal users actively engaged in evaluation OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Figure 6: Time spent reading messages, evaluating goals, and reviewing goals between interfaces and task phases. the 95% CI) when using OnGoal (B: [5, 8.4], O: [4.4, 12.2]). These results might point to OnGoal participants experimenting more with communication strategies to get the LLM to change its evaluation, with mixed results. When asked to evaluate and review their conversational goals after each turn taken, OnGoal users felt the LLM addressed their goals more often and felt more confident in their answers than baseline users for both evaluating (B: 4.1 [3.8, 4.4], O: 4.5 [4.3, 4.8], strong evidence) and reviewing (B: 4.3 [4.1, 4.6], O: 4.5 [4.3, 4.8], weak evidence). Further, we looked at how often users agreed with the system evaluation. While both interfaces evaluated goals after each turn, evaluations were only visible to participants using OnGoal. We found participants tended to agree more with the system when using OnGoal, and their confidence was aligned with their answer when participants agreed with the system they were more confident (B: 4.2 [3.9, 4.5], O: 4.7 [4.6, 4.9], strong evidence), and conversely less confident if they disagreed (B: 4.2 [3.9, 4.5], O: 3.5 [2.4, 4.6], weak evidence). This is likely due to the influence of seeing the evaluation on the screen. OnGoal features. We examined how participants used the features of OnGoal based on post-study survey feedback on usefulness  (Fig. 9)  and think-aloud feedback. Goal glyphs and the goals tab were self-reported more useful overall (goal glyphs: 4 [3.2, 4.7], goals tab: 3.9 [3.3, 4.4]) for alerting the user to potential issues over time such as contradicted goal, and compared with evaluating (goal glyphs: 3.4 [2.5, 4.2], goals tab: 3.7 [3.1, 4.2], weak evidence) or reviewing (goal glyphs: 2.9 [2.2, 3.6], goals tab: 3.7 [3.0, 4.4], weak evidence). P20 gave an example for their reasoning: It was more surprising to see red. That made me want to interact with the glyphs more. could quickly find contradicting goals to fix them right away. For evaluation, users predominantly found the goal explanations more useful than other features for evaluating (4.2 [3.6, 4.8]), helping them identify issues with how the LLM interpreted the goal in context of the response. P14 would click on the goal and immediately see why the goal was being satisfied and where in the response. Depending on how difficult those issues were to resolve, users would then get details on demand in the individual goal view. They considered this view more useful than other features for reviewing (3.8 [3.1, 4.5]) the history of messages. The text highlighting techniques helped users identify key phrases and mismatching sentences, which served as signals for lack of expected performance from the LLM. For P19, the unique sentences and keyphrases helped Figure 7: Responses to the NASA Task Load Index (TLX) for assessing workload between interfaces. and review more and in more ways; e.g., adapting their communication strategy to align with the LLM. Similar observations were reported in work on desirable difficulties, where longer periods of reviewing and reflecting can enhance data understanding [4]. We discuss why these strategies only appear with OnGoal in Sect. 5.2.2. Further, participants self-reported lower levels of effort in reading (B: 4 [3.6, 4.4], O: 3.3 [2.6, 3.9], weak evidence) and reviewing (B: 3.2 [2.5, 3.9], O: 2 [1.7, 2.6], strong evidence), corroborating lower self-reported mental demand (B: 3.9 [3.2, 4.6], O: 2.7 [2.4, 3.0], strong evidence) and effort (B: 4.1 [3.3, 4.9], O: 3.2 [2.5, 3.9], weak evidence) with similar performance in the NASA TLX  (Fig. 7)  . Confidence was reported similar for reading and evaluation, and slightly higher for evaluating with OnGoal (B: 3.8 [3.2, 4.4], O: 4.2 [3.7, 4.7], weak evidence). Less effort may be needed to obtain similar results compared to baseline interface. Interactions and answers. We used participants interaction logs to compare usage in terms of turns taken, goals addressed, and answers when asked to evaluate and review their goals  (Fig. 8)  . Turns taken was similar between interfaces, between 3 5 on average. We observed no evidence for OnGoal users having less goals confirmed (B: 3.8 [3.4, 4.2], O: 3.7 [3.3, 4.1]) and contradicted (B: 0.9 [0.6, 1.1], O: 0.7 [0.5, 0.9]), while we observed weak evidence of more goals being ignored (B: 1.3 [1, 1.7], O: 1.7 [1.3, 2.1]) than baseline users. Further, the variability in goals being addressed, or the number of times goal would switch between confirmed, contradicted, and ignored, was much wider (measured as the width of UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. Figure 8: Turns taken, goals confirmed / contradicted / ignored, goal variability, and final answers to the initial and validation questions between interfaces. Answers in the evaluating and reviewing columns were recorded as questions participants responded to after each dialogue turn (Sect. 5.1, Tasks). We measured agreement between participant answers and the systems evaluations post-hoc. (Sect. 5.2.3) some participants felt confused or misled by evaluations when they failed to align with their mental model of their own writing. This feedback from user-reported accuracy is important, as it reveals critical areas for improving goal evaluation, which we discuss in Sect. 6.1. 5.2.2 Thematic analysis. We then organized participants thinkaloud feedback thematically based on emergent issues they encountered when interacting with the LLM over multiple conversational turns, and corroborated feedback with evidence from the usability insights in Sect. 5.2.1. We further deductively aligned our themes of participant feedback with our design challenges (C) in Sect. 3. Baseline users shared several challenges including miscommunication of goals, excessive effort spent reading the chat, and uncertainty about how their goals were being addressed. In contrast, OnGoal users employed more diverse strategies to overcome miscommunication, spent more time and effort on evaluating and reviewing goals, and identified more ways to make consistent progress, fostering greater confidence. Communicating goals. Participants in both conditions struggled with getting the LLM to understand their goals, as anticipated from the design challenge C1. However, baseline users particularly struggled with how and when to effectively communicate their goals more than OnGoal users. We expected baseline users to spend more time evaluating and reviewing (Sect. 5.2.1) with less information to process, giving them more time to experiment with how and when to communicate goals. Instead, OnGoals simple feedback loop more often led users to adapt goal communication, even when lacking suggestions on what to fix. Designing interfaces to encourage more active, dynamic dialogue may help users overcome goal misalignment in similar ways to improving modeling accuracy. For example, we observed baseline users often resorting to long and comprehensive initial prompts in an attempt to cover all their goals. However, this approach frequently led the LLM to misinterpret or ignore goals, as P4 lamented: It was often not congruent with the prompt at the end of the response. In contrast, OnGoals Figure 9: Usefulness ratings for various OnGoal features. me jump to different parts of the text to find evidence that align with the goal that had. The timeline and events tabs were the least useful features overall (timeline: 3 [2, 4], events: 2 [1.9, 2.1]) for this task. Those participants that used them explained that they were not as useful when reviewing only few fixed goals, and that they preferred these tabs more when reviewing the history of their own goals. OnGoal pipeline accuracy. Finally, we asked participants in the post-study survey to rate the accuracy of the different stages of the goal pipeline (infer, merge, evaluate). Because baseline users only had the infer stage active, they only rated that stage. We found that both baseline and OnGoal users generally agreed on the performance of the infer stage (B: 3.6 [3.0, 4.2], O: 4.1 [3.6, 4.6], weak evidence). This provides confidence in our study setup that our conditions are comparable. OnGoal users self-reported higher accuracy for the infer and merge (4 [3.0, 5.0]) stages than for the evaluate stage (2.9 [2.4, 3.4]). In particular, the difference between infer and evaluate was statistically significant. This may corroborate observations of emergent issues in our think-aloud feedback 12 OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Table 2: Themes synthesized from think-aloud feedback. Theme Baseline OnGoal Communicating goals Users often relied on and repeated the initial prompt to communicate goals, sometimes making little progress. Maintaining awareness Users had to skim lengthy chat logs & responses, deIdentifying LLM issues Users manually compared messages for indicators of creasing confidence and increasing cognitive load. goal progress to learn why the LLM failed. Users leveraged explanations to refine their goal communication and adapt their prompting strategies. Users offloaded cognition to goal glyphs and progress panel, strategically shifting their mental burden. Users leveraged highlighting and progress panel to detect, understand, and monitor goal alignment. in-situ explanations encouraged more iterative goal refinement. By explicitly flagging which goals were misinterpreted or overlooked and where, the system encouraged users to adjust prompts iteratively, directing the LLMs attention to specific sections of the response. For example, this helped P18 become unstuck on how to keep moving forward: The interface outlines where the agreement or disagreement occurs... could then compare my own mental model to the systems model to help me move on. Moreover, when baseline users felt the LLM was stuck or drifting from their intent, they often repeated similar prompts with little success, leading to frustration and stagnation. P2 tried this strategy and found that the LLM responses problems have been pretty stubborn... need multiple [turns] to drift [the LLM] away from these issues. This stagnation is likely reflected in the lower variability of goal status changes in Fig. 8. With OnGoal, users instead utilized explanation cues to target specific parts of the output or rephrase requests based on what the system flagged as problematic. For example, P13 targeted specific sections of the response in follow-up messages: want the LLM to align the goal with sections of the article that naturally would have more rich imagery. Additionally, some baseline participants repeatedly attempted to force all goals to be perfectly aligned in every response, even when the LLM clearly failed to understand or adapt. This might explain the higher rate of no answers when evaluating and reviewing goals compared with OnGoal in Fig. 8. OnGoal flipped this dynamic: explanations often encouraged users to reconsider or revise their goals based on the LLMs interpretation. Some, like P12, reported gaining more trust in the LLMs judgment after reviewing its reasoning, leading to more flexible and productive interactions: Its surprising that the LLM recognizes the conflicts in the request, and generates different versions correspondingly. This is clever. We also observed OnGoals explanations help users better understand the LLMs intent and reassess their own personal judgments. Specifically, some onGoal participants changed their evaluations from no to yes after seeing evidence they had initially missed, showing how explanation-driven feedback fostered deeper, shared understanding of task progress. For example, P18 changed their initial no evaluation based on an LLM explanation: actually agree with the system, the message does have examples that changes my mind. do think the goal is satisfied. Such instances likely contributed to the higher proportion of yes answers we observed in Fig. 8. Maintaining awareness. Users in both interface conditions encountered sensemaking challenges, such as increased effort needed to parse lengthy LLM responses and chat logs, as posited by C2. However, OnGoals text highlighting provided cues that helped 13 users know where to shift their focus, saving them time and effort in the long run over baseline users  (Fig. 6)  . With the baseline interface, users described that scanning long responses and chat logs was time-consuming and mentally taxing. The LLM also made mistakes that were difficult to parse, as P7 points out: spent lot of time reading the text conversation. couldnt pay attention to quality... cant believe didnt even notice the grammatical errors until the end. P10 manually scanned for goal-related phrases and indicators, recreating functionality that OnGoal provided directly. The manual effort likely corroborates the lower confidence self-reported in Fig. 6, especially during review, as P6 explains: Its bit too long to go through all the responses that have previously, so Im decreasing my confidence score. In contrast, OnGoal shifted the burden away from exhaustive reading by visualizing signals of goal progress directly. The goal glyphs and progress panel were predominantly used to alert users to inconsistencies between their prompt and the goals. P17 used this feedback to reduce their cognitive load: didnt have to actively and consistently think about the goals all the time... [OnGoal] reduced the cognitive load for me as writer. OnGoals features seemed to empower users to use their mental energy more strategically. P11 used the the goal glyphs and timeline tab to offload cognition to the system, only needing to check in sporadically when issues arose: can just check the response for which goals think it should be satisfying. P13 used the goal-tracking features as gauges for when to turn up or down particular goal: Now that Ive seen the response, want to focus on \"turning the knobs up and down\" on different goals. The popularity of these OnGoal features also aligns with the usefulness feedback and ratings reported in Fig. 9. Identifying LLM issues. While the LLM would sometimes forget goals or ignore requests in both conditions per C3, OnGoal could visualize textual patterns related to addressing goals across messages, helping users better understand why the LLM was underperforming and avoid derailing the entire conversation. Specifically, baseline users often struggled to assess goal consistency over time potential reason for more no responses when reviewing goals  (Fig. 8)  . For example, P5 had to scan what GPT changed between messages to determine progress: The GPT didnt really change lot of its responses depending on my prompting... if there is always similar response, it means the response is consistent, but on the other hand, the GPT is not polishing the answer according to my prompt. In contrast, OnGoals inline highlighting and sentence comparison features helped users discern between helpful consistency and unproductive repetition. Several strategies emerged: P18 looked for key phrases in highlighted text to confirm UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. their requests had been addressed, P12 checked if unique sentences changed or stayed stagnant over time, and P20 compared both new and repeated sentences to track progress: During goal review, pulled out the unique sentences to see if it was generating new things, and then similar sentences to see if it kept what liked. While reviewing multiple goals remained cognitively demanding in both conditions, users in the baseline condition reported lower confidence and spent less time engaging with the task  (Fig. 6)  . lack of transparency features for LLM behaviors may be the cause, as many baseline participants spent time scrolling through lengthy chat logs to understand what went wrong. To save time and effort, some like P4 implicitly trusted the LLM to remember their goals and conversational history. Others like P1 and P8 explicitly requested transparency features that we integrated into OnGoal LLM goal evaluations and text highlighting between messages. OnGoals text highlighting and progress panel vitally reduced the burden of reviewing goal progress. For example, P14 glanced at the timeline to see if anything had become contradicted (i.e., red): Once determined my feeling about the system performance, relied more on glancing at [the timeline tab] to help with reviewing. P11 uniquely asked the LLM to summarize goal progress at the end of the task, then used the text highlighting features to identify where it had diverged from specific goals and why. These features likely also helped OnGoal users save time reading (as seen in Fig. 6). 5.2.3 Emergent issues. Despite the benefits of goal tracking and visualization we observed, several new issues emerged uniquely from the OnGoal condition. Some participants felt that the systems goal explanations could be contentious. For instance, P15 felt the goal explanations encouraged them to see their writing as more objective: There is no right or wrong in terms of what want for my plan. But, there is now right and wrong in terms of goals for the writing... was fighting the LLM for what wanted. Other participants were confused when the LLM gave examples that seemed to contradict its own evaluation. This led P14 to increase their mental effort: There were times had to go against the system, especially when the system was disagreeing with me, and was spending lot of effort. Some felt the LLM evaluations were not always clear, and could lead to confusion about how the LLM was evaluating the response, including P16 (The glyph in the message cited reason for ignoring the goal, but it wasnt obvious why it came to that assessment.) and P12 (Some evidence were considered as satisfying the goal, some of the same are considered ignoring the goal.). This usually came down to mismatch in expectations, which P16 explained: When the goal said ignore was confused, because the part that was relevant to me was still satisfied. These observations highlight that while the transparency features provided by OnGoal can empower users, they also introduce interpretive complexity, which may require extra justification effort."
        },
        {
            "title": "6 Discussion\nOur work contributes to a growing area of research which explores\nthe cognitive gap between end users and AI in task-driven settings\n[43, 46, 55]. Specifically, users of AI agents like LLMs can struggle to\nexpress their wants to the AI and align their expectations with how\nthe AI interprets their goals, making it difficult to verify whether\nAI-generated content meets their objectives [28]. To address this",
            "content": "14 gap, recent works have explored how improved UI designs and workflows can assist users in leveraging AI for writing [27], image editing [33], and sensemaking [45]. Our work specifically explores the benefits of improving UI by explicitly encoding and showing users goals, towards addressing the general limitations of LLMs in conversational contexts. In Sect. 6.1, we synthesize our study findings on goal tracking and visualization into design implications for future interfaces that foster increased human-AI collaboration and creativity while keeping users engaged and empowered. Then, in Sect. 6.2, we discuss limitations of our evaluation and future work studying interaction with LLMs in multi-turn dialogue."
        },
        {
            "title": "6.1 Implications For Design\nWe developed design implications that build off of participants’\nthink-aloud feedback and our own observations of how interacting\nwith LLMs in multi-turn conversation can be improved.",
            "content": "Enable multiple ways to communicate goals. Formalizing conversational goals in multi-turn dialogue can give users greater control over their interactions with LLMs. While baseline users typically listed all goals in the initial prompt, often with mixed results, OnGoal users employed diverse strategies for how and when to communicate goals, helping them navigate miscommunication more effectively. For example, participants varied how they communicated goals with the LLM: (1) some put all goals in the initial prompt up front, similar to the baseline condition; (2) some put few goals in time, such as conflicting pairs; (3) and some never put goals in, only asking the LLM to adjust its responses on case-by-case basis. We also observed variations in when goals were discussed with the LLM: (1) proactive, where participants explicitly asked for the LLM to address target goals; and (2) reactive, where participants allowed the LLM to fulfill the task implicitly without having specific goals, and used the system features to identify if changes were needed through specific instructions. These strategies highlight need for personalized workflows that align users preferences for inferring and merging goals across task stages [50]. Additionally, users goal-setting priorities shifted across different stages of the task, reflecting shift in users mental models [32]. For example, when goals were unspecified, users often prioritized narrative flow, while specific goals led users to concentrate on more targeted wording edits. Building off of OnGoals on-demand goal inference and evaluation, future systems could adapt dynamically to evaluate goals where users are focusing on and support flexible goal-setting throughout the dialogue. Visualize where goal evaluations align with users focus. In supporting multiple ways for users to communicate their goals, visualizations play key role in revealing how well responses align with goals over time. We observed that baseline users often relied on their initial prompt structure to scaffold their process for evaluating and reviewing progress on goals. For example, in follow-up prompts, users would only check the LLM response against what they had originally asked for, overlooking missing goals or contradictions unless those were explicitly stated in the initial prompt. In contrast, OnGoal users leveraged the examples and text highlighting to identify where and how goals were not being addressed by the LLM, resulting in more informed assessment and better goal alignment over time. These findings may point at design opportunities for OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models UIST 25, September 28-October 1, 2025, Busan, Republic of Korea visualizations to provide layer of understanding, helping users monitor the evolving relationship between their goals and LLM responses. For example, some OnGoal users requested summary visualizations of evolving themes and key ideas across message blocks, like in ThemeRiver [17]. To indicate interest, direct manipulation could be leveraged to highlight specific clauses in response [33], and then trace that highlight across messages to see if similar sentences or themes are addressed by the LLM or ignored. Visualizations should further highlight the users interests and supply more in-depth explanations using LLMs that can reason about why goal was evaluated the way it was. At the same time, care should be taken to carefully annotate the purpose of the visualizations, as OnGoal users would sometimes interpret highlighting as proxy for determining where the LLM was focusing its attention. Design goal alerts and snapshots to further offload cognition. Compared with the baseline, we saw OnGoal users report higher levels of confidence when evaluating and reviewing for similar or less effort. This may indicate that conversational interaction with LLMs is enhanced by having the system offload tasks from the user such as tracking conversational goals over time. For example, participants reported that the goal glyphs were useful for altering them when issues were present such as contradictions, freeing them up to think about how to make progress on their other goals. To help users rely less on their memory and keep them in the loop longer, goal alerts could be configurable, such as alerts when certain combinations of goals conflict or too many have been merged. While we provide summary visualizations in the timeline and events views, they lacked the ability to trace individual text phrases such as key words or themes throughout. To facilitate this, summary snapshot could report progress in terms of salient themes addressed, or key phrases, where the system synthesizes the progress on completing goals automatically and lists what is left for the user to accomplish (i.e., did get everything?). This is particularly important based on feedback we got that not all messages in conversation were useful for all goals. By summarizing progress, users could see potentially conflicting messages without needing to skim the conversation and take focus away from working on goals. Support user feedback on goal evaluation. Goal evaluation is often subjective, varying according to users individual interpretations, preferences, and needs. We observed that users occasionally disagreed with LLMs assessments and experienced tension when they were unable to influence how their goals were interpreted or judged. It was not always clear to users how the LLM arrived at an evaluation, and further, users could not give the system feedback to improve or personalize these judgments. Future work should explore external human-in-the-loop control and feedback for personalizing the goal pipeline, such as giving evaluations thumbs up or thumbs down to adapt the evaluation style towards users preferences over time [29, 40]. More interactive methods could be used, such as combining LLM-as-a-judge [56] with expanded visualizations to provide examples of users intention for updating the LLM evaluation prompts, like in Scattershot [53]. Evaluations could further be regenerated or edited by the user to better keep track of whether the system addressed goals when reviewing the conversation later on."
        },
        {
            "title": "6.2 Limitations and Future Work\nOur system implementation has several limitations that reveal\npromising future research directions. To better understand how On-\nGoal’s goal pipeline affected human-AI alignment in LLM chat inter-\nfaces, we collected user-reported accuracy, which helped contextual-\nize important findings on user experience, task outcomes and behav-\nioral insights. However, quantitatively evaluating our pipeline’s ac-\ncuracy, such as on expert-annotated benchmarks, remains untested.\nComplimenting user-reported accuracy with expert-annotated pre-\ncision could provide further insights into the effects of goal tracking\non human-AI alignment. For example, how the precision of the goal\npipeline might affect a user’s ability to evaluate and review goals\nremains unclear. In addition to benchmarks, future work should\nalso investigate how prompt design and alternative, potentially\nmore robust, LLM architectures could improve goal tracking. At the\nsame time, including smaller or less capable models such as com-\npressed or distilled LLMs would help assess the generalizability of\nour findings. Finally, it is unclear how support for other goal types\nmight affect user behaviors. In cases where global tracking alone\nmay be insufficient for complex multi-section artifacts, fine-grained\nand local goals could help bridge usability gaps.",
            "content": "In our evaluation, learning effects with OnGoal were inconclusive. For example, some participants reported that reviewing became easier over time as they grew more familiar with the LLM, regardless of interface features. It remains unclear how much OnGoal contributed beyond this growing familiarity. Further, our evaluation on writing task and baseline chat interface was useful to highlight interface-related insights. However, it is unclear whether additional cognitive and sensemaking effects might emerge in other tasks like copywriting, data analysis, personal assistance, programming, or learning. In exploring other tasks, our approach could also extend to human-to-human conversation, such as in online interactions in virtual meeting spaces where LLMs could assist in goal tracking and visualization. Lastly, we did not examine whether our system encourages users to create more or more diverse goals. Some feedback suggested the timeline and events views may be especially helpful in open-ended contexts. Future work should explore dynamic goal tracking over time in longitudinal studies with open-ended goal setting."
        },
        {
            "title": "7 Conclusion\nUsing LLMs as conversational agents is opening the door to new\nopportunities for supporting users in solving complex tasks. As\nthese systems are deployed in diverse environments with unknown\nLLM capabilities, interfaces can provide the critical infrastructure\nneeded to facilitate stronger communication with LLM agents. Our\nwork contributes study results and design implications for conver-\nsational goal tracking and visualization as an avenue for enabling\nmore efficient and resilient LLM chat interface experiences. By de-\nveloping OnGoal, we uncovered specific insights into how users\nof chat interfaces can handle miscommunication with LLMs, make\nsense of long and complex chat histories, and identify problematic\nLLM behaviors using interactive visualizations. Future designers\nand engineers should explore more ways to support personalized\nexperiences in LLM chat interfaces, including new methods for\ncommunicating goals and feedback-driven goal evaluations.",
            "content": "UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. Acknowledgments This research was conducted during an internship at Adobe Research. We thank the Adobe Research EEL group for their guidance and support throughout the work. References [1] Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, and Elena L. Glassman. 2024. ChainForge: Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 304, 18 pages. doi:10.1145/3613904.3642016 [2] Trevor Ashby, Adithya Kulkarni, Jingyuan Qi, Minqian Liu, Eunah Cho, Vaibhav Kumar, and Lifu Huang. 2024. Towards Effective Long Conversation Generation with Dynamic Topic Tracking and Recommendation. In Proceedings of the 17th International Natural Language Generation Conference, Saad Mahamood, Nguyen Le Minh, and Daphne Ippolito (Eds.). Association for Computational Linguistics, Tokyo, Japan, 540556. https://aclanthology.org/2024.inlg-main.43/ [3] Jeff Baker, Donald Jones, and Jim Burkman. 2009. Using visual representations of data to enhance sensemaking in data exploration tasks. Journal of the Association for Information Systems 10, 7 (2009), 2. [4] Elizabeth Bjork, Robert Bjork, et al. 2011. Making things hard on yourself, but in good way: Creating desirable difficulties to enhance learning. Psychology and the real world: Essays illustrating fundamental contributions to society 2, 59-68 (2011). [5] Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011. D3 Data-Driven Documents. IEEE Transactions on Visualization and Computer Graphics 17, 12 (2011), 23012309. doi:10.1109/TVCG.2011.185 [6] R.E. Boyatzis. 1998. Transforming Qualitative Information: Thematic Analysis and Code Development. SAGE Publications. [7] Victor S. Bursztyn, Jennifer Healey, Eunyee Koh, Nedim Lipka, and Larry Birnbaum. 2021. Developing Conversational Recommendation Systemfor Navigating Limited Options. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI EA 21). Association for Computing Machinery, New York, NY, USA, Article 309, 6 pages. doi:10.1145/3411763.3451596 [8] Zixin Chen, Jiachen Wang, Meng Xia, Kento Shigyo, Dingdong Liu, Rong Zhang, and Huamin Qu. 2025. StuGPTViz: Visual Analytics Approach to Understand Student-ChatGPT Interactions. IEEE Transactions on Visualization and Computer Graphics 31, 1 (2025), 908918. doi:10.1109/TVCG.2024.3456363 [9] Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre, and Mark Cieliebak. 2021. Survey on evaluation methods for dialogue systems. Artificial Intelligence Review 54, 1 (01 Jan 2021), 755810. doi:10.1007/s10462-020-09866-x [10] Pierre Dragicevic. 2016. Fair Statistical Communication in HCI. Springer International Publishing, Cham, 291330. doi:10.1007/978-3-319-26633-6_13 [11] Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, and Kai Chen. 2023. Botchat: Evaluating llms capabilities of having multi-turn dialogues. arXiv preprint arXiv:2310.13650 (2023). [12] Anders Ericsson and Herbert Simon. 1984. Protocol analysis: Verbal reports as data. the MIT Press. [13] Siwei Fu, Jian Zhao, Hao Fei Cheng, Haiyi Zhu, and Jennifer Marlow. 2018. T-Cal: Understanding Team Conversational Data with Calendar-based Visualization. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal, QC, Canada) (CHI 18). Association for Computing Machinery, New York, NY, USA, 113. doi:10.1145/3173574. [14] Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, and Thomas Malone. 2024. Taxonomy for HumanLLM Interaction Modes: An Initial Exploration. In Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI EA 24). Association for Computing Machinery, New York, NY, USA, Article 24, 11 pages. doi:10.1145/3613905.3650786 [15] Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld, and Elena L. Glassman. 2024. Supporting Sensemaking of Large Language Model Outputs at Scale. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 838, 21 pages. doi:10.1145/3613904. 3642139 [16] Akash Gupta, Ivaxi Sheth, Vyas Raina, Mark Gales, and Mario Fritz. 2024. LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History. In ICML 2024 Workshop on Foundation Models in the Wild. https://openreview.net/forum?id=WnMcWR9n3P [17] S. Havre, B. Hetzler, and L. Nowell. 2000. ThemeRiver: visualizing theme changes over time. In IEEE Symposium on Information Visualization 2000. INFOVIS 2000. Proceedings. 115123. doi:10.1109/INFVIS.2000.885098 16 [18] Diana C. Hernandez-Bocanegra and Jürgen Ziegler. 2023. Explaining Recommendations through Conversations: Dialog Model and the Effects of Interface Type and Degree of Interactivity. ACM Trans. Interact. Intell. Syst. 13, 2, Article 6 (April 2023), 47 pages. doi:10.1145/ [19] Ryuichiro Higashinaka, Masahiro Araki, Hiroshi Tsukahara, and Masahiro Mizukami. 2021. Integrated taxonomy of errors in chat-oriented dialogue systems. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Haizhou Li, Gina-Anne Levow, Zhou Yu, Chitralekha Gupta, Berrak Sisman, Siqi Cai, David Vandyke, Nina Dethlefs, Yan Wu, and Junyi Jessy Li (Eds.). Association for Computational Linguistics, Singapore and Online, 8998. doi:10.18653/v1/2021.sigdial-1.10 [20] Matt-Heun Hong and Anamaria Crisan. 2023. Conversational AI Threads for Visualizing Multidimensional Datasets. arXiv:2311.05590 [cs.HC] https://arxiv. org/abs/2311.05590 [21] Shih-Hong Huang, Ya-Fang Lin, Zeyu He, Chieh-Yang Huang, and Ting-Hao Kenneth Huang. 2024. How Does Conversation Length Impact Users Satisfaction? Case Study of Length-Controlled Conversations with LLM-Powered Chatbots. In Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems (CHI EA 24). Association for Computing Machinery, New York, NY, USA, Article 188, 13 pages. doi:10.1145/3613905.3650823 [22] Peiling Jiang, Jude Rayan, Steven P. Dow, and Haijun Xia. 2023. Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST 23). Association for Computing Machinery, New York, NY, USA, Article 3, 20 pages. doi:10.1145/3586183.3606737 [23] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406 (2022). [24] Joshua Y. Kim, Rafael A. Calvo, N. J. Enfield, and Kalina Yacef. 2021. Systematic Review on Dyadic Conversation Visualizations. In Companion Publication of the 2021 International Conference on Multimodal Interaction (Montreal, QC, Canada) (ICMI 21 Companion). Association for Computing Machinery, New York, NY, USA, 137147. doi:10.1145/3461615.3485396 [25] Yoonsu Kim, Jueon Lee, Seoyoung Kim, Jaehyuk Park, and Juho Kim. 2024. Understanding Users Dissatisfaction with ChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 385404. doi:10.1145/3640543. [26] Atharva Kulkarni, Bo-Hsiang Tseng, Joel Moniz, Dhivya Piraviperumal, Hong Yu, and Shruti Bhargava. 2024. SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers). 19882001. [27] Philippe Laban, Jesse Vig, Marti Hearst, Caiming Xiong, and Chien-Sheng Wu. 2024. Beyond the chat: Executable and verifiable text-editing with llms. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. 123. [28] Hao-Ping (Hank) Lee, Advait Sarkar, Lev Tankelevitch, Ian Drosos, Sean Rintel, Richard Banks, and Nicholas Wilson. 2025. The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From Survey of Knowledge Workers. In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems. ACM. doi:10.1145/3706598. 3713778 [29] Andrew Li, Zhenduo Wang, Ethan Mendes, Duong Minh Le, Wei Xu, and Alan Ritter. 2024. ChatHF: Collecting Rich Human Feedback from Real-time Conversations. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 270279. [30] Jenny Liang, Chenyang Yang, and Brad Myers. 2023. Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges. arXiv preprint arXiv:2303.17125 (2023). [31] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics 12 (2024), 157173. doi:10.1162/tacl_a_ [32] Amama Mahmood, Junxiang Wang, Bingsheng Yao, Dakuo Wang, and ChienMing Huang. 2023. Llm-powered conversational voice assistants: Interaction patterns, opportunities, challenges, and design guidelines. arXiv preprint arXiv:2309.13879 (2023). [33] Damien Masson, Sylvain Malacria, Géry Casiez, and Daniel Vogel. 2024. Directgpt: direct manipulation interface to interact with large language models. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 116. [34] Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 2024. LLM Critics Help Catch LLM Bugs. arXiv preprint arXiv:2407.00215 (2024). [35] Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul Kwon, and Chris Bryan. 2023. Promptaid: Prompt exploration, perturbation, testing OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models UIST 25, September 28-October 1, 2025, Busan, Republic of Korea and iteration using visual analytics for large language models. arXiv preprint arXiv:2304.01964 (2023). [36] Cheng Niu, Xingguang Wang, Xuxin Cheng, Juntong Song, and Tong Zhang. 2024. Enhancing Dialogue State Tracking Models through LLM-backed UserAgents Simulation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 87248741. [37] Andrea Papenmeier, Dagmar Kern, Gwenn Englebienne, and Christin Seifert. 2022. Its Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI. ACM Trans. Comput.-Hum. Interact. 29, 4, Article 35 (March 2022), 33 pages. doi:10.1145/3495013 [38] Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D. Weisz. 2023. The Programmers Assistant: Conversational Interaction with Large Language Model for Software Development. In Proceedings of the 28th International Conference on Intelligent User Interfaces (Sydney, NSW, Australia) (IUI 23). Association for Computing Machinery, New York, NY, USA, 491514. doi:10.1145/3581641.3584037 [39] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. 2023. Large Language Models Can Be Easily Distracted by Irrelevant Context. In Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 3121031227. https://proceedings.mlr.press/v202/shi23a.html [40] Taiwei Shi, Zhuoer Wang, Longqi Yang, Ying-Chun Lin, Zexue He, Mengting Wan, Pei Zhou, Sujay Jauhar, Sihao Chen, Shan Xia, et al. 2024. Wildfeedback: Aligning llms with in-situ user interactions and feedback. arXiv preprint arXiv:2408.15549 (2024). [41] Brodrick Stigall, Ryan Rossi, Jane Hoffswell, Xiang Chen, Shunan Guo, Fan Du, Eunyee Koh, and Kelly Caine. 2023. On Chatbots for Visual Exploratory Data Analysis. In 2023 IEEE International Conference on Big Data (BigData). 59245929. doi:10.1109/BigData59044.2023.10386335 [42] Tanya Stivers, Enfield, Penelope Brown, Christina Englert, Makoto Hayashi, Trine Heinemann, Gertie Hoymann, Federico Rossano, Jan Peter de Ruiter, KyungEun Yoon, and Stephen Levinson. 2009. Universals and cultural variation in turn-taking in conversation. Proc. Natl. Acad. Sci. U. S. A. 106, 26 (June 2009), 1058710592. [43] Hari Subramonyam, Roy Pea, Christopher Pondoc, Maneesh Agrawala, and Colleen Seifert. 2024. Bridging the Gulf of Envisioning: Cognitive Challenges in Prompt Based Interactions with LLMs. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 1039, 19 pages. doi:10. 1145/3613904.3642754 [44] Lovis Bero Suchmann, Nicole Krämer, and Jürgen Ziegler. 2023. Branching Preferences: Visualizing Non-linear Topic Progression in Conversational Recommender Systems. In Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization (Limassol, Cyprus) (UMAP 23 Adjunct). Association for Computing Machinery, New York, NY, USA, 199205. doi:10.1145/3563359.3597380 [45] Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. 2023. Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST 23). Association for Computing Machinery, New York, NY, USA, Article 1, 18 pages. doi:10.1145/3586183.3606756 [46] Lev Tankelevitch, Viktor Kewenig, Auste Simkute, Ava Elizabeth Scott, Advait Sarkar, Abigail Sellen, and Sean Rintel. 2024. The metacognitive demands and opportunities of generative AI. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 124. [47] A. Tat and M.S.T. Carpendale. 2002. Visualising human dialog. In Proceedings Sixth International Conference on Information Visualisation. 1621. doi:10.1109/IV. 2002. [48] Edward Tufte and Peter Graves-Morris. 1983. The visual display of quantitative information. Vol. 2. Graphics press Cheshire, CT. [49] Gina Danielle Venolia and Carman Neustaedter. 2003. Understanding sequence and reply relationships within email conversations: mixed-model visualization. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Ft. Lauderdale, Florida, USA) (CHI 03). Association for Computing Machinery, New York, NY, USA, 361368. doi:10.1145/642611.642674 [50] Ben Wang, Jiqun Liu, Jamshed Karimnazarov, and Nicolas Thompson. 2024. Task supportive and personalized human-large language model interaction: user study. In Proceedings of the 2024 Conference on Human Information Interaction and Retrieval. 370375. [51] Tao Wang, Mandy Keck, and Zana Vosough. 2021. Discussion Flows: An Interactive Visualization for Analyzing Engagement in Multi-Party Meetings. In EuroVis 2021 - Short Papers, Marco Agus, Christoph Garth, and Andreas Kerren (Eds.). The Eurographics Association. doi:10.2312/evs.20211060 [52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems (2022), 2482424837. [53] Sherry Wu, Hua Shen, Daniel Weld, Jeffrey Heer, and Marco Tulio Ribeiro. 2023. ScatterShot: Interactive In-context Example Curation for Text Transformation. In Proceedings of the 28th International Conference on Intelligent User Interfaces (Sydney, NSW, Australia) (IUI 23). Association for Computing Machinery, New York, NY, USA, 353367. doi:10.1145/3581641.3584059 [54] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie Cai. 2022. PromptChainer: Chaining Large Language Model Prompts through Visual Programming. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA 22). Association for Computing Machinery, New York, NY, USA, Article 359, 10 pages. doi:10.1145/3491101.3519729 [55] Diego Zamfirescu-Pereira, Richmond Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny cant prompt: how non-AI experts try (and fail) to design LLM prompts. In Proceedings of the 2023 CHI conference on human factors in computing systems. 121. [56] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Judging LLMHao Zhang, Joseph Gonzalez, and Ion Stoica. 2023. as-a-Judge with MT-Bench and Chatbot Arena. in NeuInformation Processing Systems, A. Oh, T. Naumann, A. Globerson, ral K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 4659546623. https://proceedings.neurips.cc/paper_files/paper/2023/file/ 91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf In Advances LLM Prompts A.1 Infer goals 1 You will be presented with human dialogue in conversation with you , an assistant . Your task is to extract every clause verbatim from the document exactly as it appears . 2 3 List all clauses in the dialogue that are either question , request , offer , or suggestion . Briefly summarize how to address the goal of the clause in ONE sentence . 4 5 Please respond ONLY with valid JSON in the following format : \" clauses \": [ {{\" clause \": \"< CLAUSE_1 >\" , \" type \": \"< TYPE_1 >\" , \" summary \": \"< SUMMARY_1 >\"}} , {{\" clause \": \"< CLAUSE_2 >\" , \" type \": \"< TYPE_2 >\" , \" summary \": \"< SUMMARY_2 >\"}} , ] 6 7 {{ 8 9 11 12 }} A.2 Merge goals Input parameters: old_goals_str_list newline-delimited string list of numbered goals. Example: 1. ... 2. ... new_goals_str_list newline-delimited string list of numbered goals. Example: 1. ... 2. ... 1 You have one set of old numbered bullet point goals : 2 { old_goals_str_list } 3 4 You have another set of new numbered bullet point goals : 5 { new_goals_str_list } 6 7 Merge the two lists of bullet point goals into single updated list of goals . Use the following three operations as rules to perform the merge : 8 UIST 25, September 28-October 1, 2025, Busan, Republic of Korea Coscia et al. 9 * Replace : If new goal contradicts an old goal , replace 5 Please respond ONLY with valid JSON in the following the old goal with the new goal . List the number of format : \" keyphrases \": [\" < KEYPHRASE_1 >\" , \"< KEYPHRASE_2 >\"] the old goal , then the number of the new goal . 10 * Combine : If new goal is similar to an old goal , combine the old goal and the new goal into new combined goal . List the number of the old goal , then the number of the new goal . 11 * Keep : If goal is unique , keep that goal in the updated list . List the original number of the goal . 12 13 Please respond ONLY with valid JSON in the following 6 7 {{ 8 9 }} format : 14 15 {{ 17 18 19 20 21 23 24 25 26 \" operations \": [ {{ \" updated_goal \": \"< GOAL_1 >\" , \" operation \": \"< OPERATION_1 >\" , \" goal_numbers \": [\" < GOAL_NUMBER_1 >\" , \"< GOAL_NUMBER_2 >\"] }} , {{ \" updated_goal \": \"< GOAL_2 >\" , \" operation \": \"< OPERATION_2 >\" , \" goal_numbers \": [\" < GOAL_NUMBER_1 >\" , \"< GOAL_NUMBER_2 >\"] }} , ] 27 28 }} A.3 Evaluate goals Input parameters: goal_str string representing conversational goal. Example: Use figurative language. 1 You will be presented with human dialogue and response from you , an assistant . Your task is to evaluate the assistant response in terms of the following conversational goal : { goal_str } 2 3 Categorize how the assistant response addresses the goal in one of three categories . The categories are confirm , contradict , or ignore . Explain the relationship between the response and the goal in ONE sentence . Extract clauses verbatim from the response exactly as they appear as examples that show evidence to support your explanation . 4 5 Please respond ONLY with valid JSON in the following format : 6 7 {{ 8 9 10 11 }} \" category \": \"< CATEGORY_1 >\" , \" explanation \": \"< EXPLANATION_1 >\" , \" examples \": [\" < EXAMPLE_1 >\" , \"< EXAMPLE_2 >\"] A.4 Keyphrase extraction 1 You will be given an assistant response . Your task is to extract every phrase verbatim from the response exactly as it appears . 2 3 List the phrases that capture the most salient topics of the response ."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Georgia Institute of Technology"
    ]
}