{
    "paper_title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
    "authors": [
        "Bencheng Liao",
        "Hongyuan Tao",
        "Qian Zhang",
        "Tianheng Cheng",
        "Yingyue Li",
        "Haoran Yin",
        "Wenyu Liu",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 5 4 1 3 1 . 2 0 5 2 : r Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Bencheng Liao * 1 2 Hongyuan Tao * 2 Qian Zhang 3 Tianheng Cheng 2 Yingyue Li 2 Haoran Yin 3 Wenyu Liu 2 Xinggang Wang 2 Figure 1: Comprehensive comparison of mmMamba. (a) Our mmMamba can build linear-complexity and hybrid decoder-only VLM by distilling the knowledge in Transformer to Mamba-2. (b) By distilling from the quadratic-complexity decoder-only VLM HoVLE, our mmMamba-linear achieves competitive performance against existing linear and quadraticcomplexity VLMs with fewer parameters (e.g., 2 fewer than EVE-7B), while mmMamba-hybrid surpasses them across all benchmarks and approaches the teacher model HoVLEs performance. (c)-(d) We compare the speed and memory of mmMamba-linear and mmMamba-hybrid with the teacher model HoVLE on the same single NVIDIA 4090 GPU. mmMamba-linear maintains consistently low latency and memory usage, while mmMamba-hybrids resource consumption scales significantly better than HoVLE. At 103K tokens, mmMamba-linear demonstrates 20.6 speedup compared to HoVLE and saves 75.8% GPU memory, while mmMamba-hybrid achieves 13.5 speedup and saves 60.2% GPU memory."
        },
        {
            "title": "Abstract",
            "content": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, framework for developing linearcomplexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computa- *Equal contribution Intelligence, Huazhong University of Science & Technology 2School of EIC, Huazhong University of Science & Technology 3Horizon Robotics. Correspondence to: Xinggang Wang <xgwang@hust.edu.cn>. 1Institute of Artificial tional resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoderonly HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMambahybrid further improves performance significantly, Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation approaching HoVLEs capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6 speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5 speedup and 60.2% memory savings. Code and models are released at https: //github.com/hustvl/mmMamba 1. Introduction in"
        },
        {
            "title": "Large",
            "content": "advances Language Models Recent (LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023a;b; Dubey et al., 2024; Bai et al., 2023a; Jiang et al., 2023; Bi et al., 2024; Javaheripi et al., 2023) have catalyzed significant research interest in expanding their capabilities beyond text to encompass multimodal understanding, particularly in processing both visual and textual information simultaneously. This expansion has given rise to Multimodal Large Language Models (MLLMs), with Vision Language Models (VLMs) emerging as prominent subset. Notable examples such as LLaVA (Liu et al., 2024a), BLIP (Li et al., 2022), Qwen-VL (Bai et al., 2023b), InternVL (Chen et al., 2024b), and Monkey (Li et al., 2024b) have demonstrated remarkable success in enhancing LLMs visual comprehension capabilities through the integration of pre-trained vision encoders and specialized connectors that bridge the modality gap between vision and language. While these encoder-based compositional VLMs have achieved state-of-the-art (SOTA) performance and established themselves as the de-facto paradigm, they face two critical limitations. First, processing long contexts becomes prohibitively expensive due to the quadratic computational complexity and linearly growing Key-Value (KV) cache with respect to sequence length. This limitation becomes particularly problematic given the increasing demand for long chain-of-thought reasoning (Muennighoff et al., 2025; DeepSeek-AI et al., 2025; Team et al., 2025; Xu et al., 2024) and high-resolution image/video understanding (Chen et al., 2023; Li et al., 2024a). Second, their heterogeneous architecture heavily relies on pre-trained vision encoders, introducing significant complexity in both training procedures and deployment scenarios (Chen et al., 2024a). Current research efforts to address these limitations have followed two distinct paths. One approach focuses on developing linear-complexity VLMs by adhering to the conventional encoder-based recipe, which requires both pre-trained vision encoders and pre-trained linear-complexity language models (Hou et al., 2024; Qiao et al., 2024). The alternative approach aims to enhance decoder-only VLMs through increased model scale and expanded training datasets, achieving performance competitive with encoder-based counter2 parts (Bavishi et al., 2023; Diao et al., 2024; Tao et al., 2024; Team, 2024; Wang et al., 2024b). the development of Despite these advances, linearcomplexity decoder-only MLLMs remains an understudied yet critical challenge. Addressing this gap holds substantial value for three key reasons: (1) Unified multimodal understanding: Such models could seamlessly integrate multimodal reasoning within single architecture, eliminating the need for heterogeneous, modality-specific frameworks. (2) Practical efficiency: Linear-complexity models inherently reduce computational demands during both training and inference, lowering costs and enabling deployment on resource-constrained edge devices. (3) Untapped Potential: While recent linear-time models like Mamba-2 demonstrate high text-processing capabilities, their ability to handle multimodal tasksparticularly in cross-modal alignment and reasoningremains largely unexplored. The research of linear-complexity decoder-only MLLMs could unlock scalable, cost-effective multimodal systems without sacrificing performance. straightforward solution is to synergize the recipe of decoder-only VLMs and linear-complexity encoder-based VLMs. This integration requires pre-trained linearcomplexity LLM and performs image-caption alignment pre-training (PT) and supervised fine-tuning (SFT) using text instructions and image prompts. However, this integrated recipe faces two significant challenges: (1) It demands the curation of different large-scale multimodal datasets for different purposes (i.e., PT and SFT) and requires substantial computational resources. (2) The overall performance is inherently limited by the capabilities of pre-trained linear-complexity LLMs, which consistently underperform mainstream SOTA Transformer-based LLMs in language understanding tasks. In this paper, we propose novel distillation-based recipe to develop linear-complexity decoder-only VLMs, which requires only moderate academic resources while circumventing the limitations of pre-trained linear-complexity LLMs. Our method leverages the fundamental similarity between the Transformer attention mechanism and the Mamba-2 state space model (SSM) mechanism. We introduce an initialization scheme that enables direct parameter transfer from Transformer to Mamba-2 layers, effectively converting the attention mechanism into the SSM function while carefully initializing SSM-specific parameters to mimic attention behavior. This approach enables the direct transformation of pre-trained Transformer-based VLMs into linearcomplexity Mamba-2-based VLMs without relying on underperforming pre-trained linear-complexity LLMs. While this parameter inheritance and initialization strategy provides promising starting point, the transformed Mamba2-based VLM requires further distillation to recover robust Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation multimodal conversation capabilities. To enhance alignment with the Transformer-based teacher VLM, we develop three-stage progressive distillation strategy: (1) Stage-1: we first train the SSM-specific parameters while freezing inherited parameters, and align layer-wise behavior using MSE distillation loss; (2) Stage-2: we then optimize complete Mamba-2 layer behavior by enabling the training of inherited Transformer parameters; (3) Stage-3: we finally perform complete model alignment using KL-divergence loss on final output logits to recover the teacher models multimodal understanding capabilities through end-to-end distillation. The proposed distillation recipe enables two distinct architectural variants: mmMamba-linear, which converts all Transformer layers into Mamba-2 layers, achieving full linear complexity, and mmMamba-hybrid, which strategically transforms fixed intervals of Transformer layers into Mamba-2 layers. The hybrid design systematically preserves Transformer layers at critical feature hierarchies while leveraging Mamba-2s linear complexity for the majority of computations, striking balance between efficiency and capability. During the final end-to-end distillation stage, we can flexibly adjust the number of interleaved Transformer layers, enabling precise control over the computation-performance trade-off. This architectural flexibility makes our approach highly adaptable to diverse deployment scenarios, allowing optimization for specific computational constraints while maintaining desired performance. By distilling from the recent Transformer-based decoderonly VLM, HoVLE, we demonstrate that mmMamba achieves competitive performance across multiple visionlanguage benchmarks while significantly improving computational efficiency. Our pure Mamba-2-based linearcomplexity variant, mmMamba-linear, achieves comparable performance to existing quadratic/linear-complexity VLMs like Mobile-VLM-3B (Chu et al., 2023), VisualRWKV3B (Hou et al., 2024), VL-Mamba-3B (Qiao et al., 2024) while eliminating the need for separate vision encoders. mmMamba-pure also matches the performance of the previous SOTA Transformer-based decoder-only EVE-7B with 2 fewer parameters. The hybrid variant, mmMambahybrid, significantly improves performance on all benchmarks compared to mmMamba-pure, approaching the teacher model HoVLE. Notably, at the context length of 103K tokens, mmMamba-linear demonstrates 20.6 speedup compared to HoVLE and saves 75.8% GPU memory, while mmMamba-hybrid achieves 13.5 speedup and saves 60.2% GPU memory. These results and extensive ablation studies validate the effectiveness of our distillation recipe and highlight the potential for practical applications. Our main contributions can be summarized as follows: We present novel three-stage progressive distillation recipe for building native multimodal state space models without the reliance on underperforming pre-trained linear-complexity LLMs, enabling effective knowledge transfer from quadratic to linear architectures. With the proposed distillation recipe, we propose the first decoder-only multimodal state space models that include two distinct architectural variants: mmMambalinear with purely linear complexity and mmMambahybrid offering flexible performance-efficiency tradeoffs. Extensive experimental results demonstrate competitive performance with significantly improved computational efficiency across various vision-language tasks, achieving up to 20.6 speedup and 4.2 memory reduction for long sequence modeling on NVIDIA 4090 GPU. 2. Related Work Decoder-only VLM. The remarkable success of Large Language Models (LLMs) has inspired the research community to extend their capabilities to multi-modal Vision-Language Models (VLMs). While compositional encoder-based architectures (Liu et al., 2024a; Chen et al., 2024b; Li et al., 2024b; 2022), leveraging pre-trained foundation vision encoders (Fang et al., 2023; 2024; Sun et al., 2023; Zhai et al., 2023) and additional connectors, have dominated the field. Recently, pioneering work Fuyu-8B (Bavishi et al., 2023) demonstrated that single unified decoder-only Transformer can achieve competitive performance against encoder-based VLMs, offering an appealing alternative due to its architectural simplicity and deployment efficiency. This breakthrough has sparked researchers interest in decoder-only VLM. SOLO (Chen et al., 2024a) proposed systematic training recipe tailored for decoder-only VLM by adapting pre-trained LLMs to vision-language tasks. EVE (Diao et al., 2024) advanced this approach by introducing visionlanguage pre-alignment and auxiliary visual representation supervision during fine-tuning to enhance the performance of decoder-only VLM. To better preserve the inherited LLMs language capabilities, HoVLE (Tao et al., 2024) introduces an extra Transformer-based decoder-only holistic embedding module that aligns language and vision modalities before LLM processing multi-modal input tokens. Despite these advances, existing decoder-only VLMs remain constrained by the quadratic computational complexity of Transformer architectures, resulting in substantial training and deployment costs. In contrast, our proposed mmMamba addresses these limitations by converting Transformer layers to linear-complexity Mamba-2 layers through progressive distillation, enabling both pure linear and hybrid architectural variants. 3 Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Linear-complexity VLM. The development of linearcomplexity RNN-based LLMs (e.g., Mamba (Gu & Dao, 2023), Mamba-2 (Dao & Gu, 2024), RWKV (Peng et al., 2023)) has inspired increasing interest in addressing the quadratic complexity limitations of Transformer-based VLMs. VL-Mamba (Qiao et al., 2024) follows the recipe of LLaVA by incorporating Vision Selective Scan connector with the pre-trained Mamba LLM. Similarly, Cobra (Zhao et al., 2024) enhances pre-trained Mamba LLMs visual capabilities by integrating DINOv2 (Oquab et al., 2023) and SigLIP (Zhai et al., 2023) vision encoders. MLMamba (Huang et al., 2024) introduces Mamba-2 Scan Connector to process visual tokens between the pre-trained vision encoder and the pre-trained Mamba-2 LLM. Instead of relying on Mamba, VisualRWKV (Hou et al., 2024) leverages CLIP ViT-L/14 (Radford et al., 2021) as the vision encoder and pre-trained RWKV LLM (Peng et al., 2023; 2024) with 2D image scanning mechanism for visual sequence processing. However, the above works remain constrained by their reliance on pre-trained RNN-based LLMs and vision encoders, following the compositional encoderbased paradigm. In contrast, our proposed mmMamba eliminates the dependency on pre-trained RNN-based LLMs and vision encoders, and enables training flexible hybrid architecture that interleaves Mamba with Transformer layers with minimal training cost. This capability enables customizable trade-offs between performance and efficiency, making it adaptable to diverse practical applications. Transformer to RNN distillation. Instead of training RNNbased LLMs from scratch, recent studies propose to linearize the pre-trained Transformer-based LLMs into RNNbased LLMs through distillation, which can significantly reduce the training cost for building RNN-based LLMs. Kasai et al. (2021) pioneered this approach by using linear attention and initializing linear attention parameters using pre-trained LLM weights, exploiting the inherent similarities with Transformers softmax attention. Zhang et al. (2024b) propose to add loss for matching softmax attention to approximate more closely the base transformer. Mercat et al. (2024) advanced the field by replacing softmax attention with linear RNN kernel coupled with novel normalization strategy. Building upon these foundations, Bick et al. (2024), Wang et al. (2024a), and Zhang et al. (2024a) developed multi-stage distillation approaches for more effective Transformer to RNN distillation. Inspired by these advances, we extend this distillation paradigm to VLMs through the proposed novel multi-stage distillation strategy. Our approach first aligns the newly added parameters of the linearized LLM at each layer, followed by layer-wise distillation, and concludes with end-to-end distillation. This progressive pipeline ensures efficient transfer from quadratic knowledge to linear knowledge while maintaining performance. 3. Preliminary We firstly give brief background on quadratic-complexity sequence modeling Transformer and linear-complexity sequence modeling Mamba-2. Given an input sequence = [x1, . . . , xT ] RT d, where is the sequence length and is the hidden dimension. The above two sequence modeling layers will compute the output sequence = [y1, . . . , yT ] RT d. Transformer The standard autoregressive Transformer used in LLM employs attention mechanism (Vaswani, 2017) by interacting with all historical positions in the sequence, which is defined as: qt, kt, vt = xtW Q, xtW K, xtW , yt = (cid:80)t i=1 exp(qtk )vi (cid:80)t i=1 exp(qtk ) , (1) where Q, K, Rdd are the learnable parameters. The current output token ot is computed by performing attention over the growing sequence of historical keys {ki}t i=1 and values {vi}t i=1. Mamba-2 Instead of interacting with all historical positions, Mamba-2 (Dao & Gu, 2024) compresses the historical information into fixed-size matrix-shaped hidden state, which is defined as: qt, kt, vt = xtW Q, xtW K, xtW , γt = exp (softplus(xtW γ)exp(a)) , St = γtSt1 + vtk , yt = Stqt, (2) where Q, K, Rdd, γ Rd1 and are the learnable parameters. St is the fixed-size matrixshaped hidden state, γt is the data-dependent gating term to control the information flow by dynamically decaying the historical information St1. 4. Method Our method consists of three key components. First, we detail the seeding strategy, which carves the Mamba-2 architecture from pre-trained Transformer by inheriting parameters and carefully initializing the newly introduced SSMspecific parameters in Sec. 4.1. Building upon this seeding strategy, we present the proposed progressive distillation pipeline in Sec. 4.2, Sec. 4.3 and Sec. 4.4 to effectively transfer knowledge from Transformer to Mamba-2. With the designed distillation training recipe, we then instantiate two model variants in Sec. 4.5: mmMamba-linear using only Mamba-2 layers, and mmMamba-hybrid incorporating interleaved Transformer and Mamba-2 layers. 4 Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation the original trained Transformer layer. Specifically, we instantiate the trained Transformer-based VLM as teacher model, and the transferred Mamba-2 VLM model as student model. The only difference lies in the sequence mixer layer. We feed the multimodal sequence into the teacher model. To keep the layerwise alignment and diminish the accumulated error of the cascading layers, we input the i-th Mamba-2 layer with the output of the 1-th Transformer layer, i.e., i-th Mamba-2 layer and i-th Transformer layer have the same input. And we align the layerwise behavior by applying the MSE distillation loss between the output of the i-th Mamba-2 layer and the output of the i-th Transformer layer: stage1 = {ai, ϕi γ, conv, G}, min stage1}L i=1 {ϕi LMSE(Attn(Xi), Mamba-2ϕi stage1 (Xi)), (3) (cid:88) i=1 where ϕi stage1 is the trainable parameters of the i-th Mamba-2 layer, which only includes the introduced extra parameters ai, G. Xi is the input sequence to the i-th Mamba-2 layer and Transformer layer, Attn(Xi) is the output of the i-th teacher Transformer layer, (Xi) is the output of the i-th student MambaMamba-2ϕi 2 layer. conv, γ, stage1 4.3. Stage-2: Layerwise Distillation for the Whole Mamba-2 Parameters After the stage-1 distillation, we have obtained good initialization of the introduced extra parameters, and we further train all the Mamba-2 parameters to better align the layerwise behavior of the student Mamba-2 with the teacher Transformer. The only difference between the stage-1 and stage-2 is that we further include the parameters of Q, K, for optimizing the distillation loss: Stage2 = {ai, ϕi γ, conv, G, Q, K, V }, min Stage2}L i=1 {ϕi LMSE(Attn(Xi), Mamba-2ϕi Stage (Xi)), (4) (cid:88) i=1 4.4. Stage-3: End-to-End Distillation Beyond the layerwise alignment, the final stage-3 distillation aims to align the end-to-end behavior of the student Mamba2 with the teacher Transformer. Specifically, we input the same multi-modal sequence to both the teacher Transformer and the student Mamba-2 without sharing the intermediate output. For the output of the teacher model and the student model, we apply the word-level KL-Divergence loss, in other words, they are used as soft labels, we enforce the output logits of the student model to be close to the output 5 Figure 2: Initialize Mamba-2 from Transformer. By comparing the mechanism similarity in Sec. 3, we directly inherit Q, K, , parameters (blue) from trained Transformer layer and carefully initialize the extra parameters (orange) including a, γ, conv, and in Mamba2 to initially mimic the Transformers behavior, providing strong foundation for subsequent distillation. 4.1. Seeding: Initialize Mamba-2 from Transformer To transfer as much knowledge as possible from quadratic Transformer to linear Mamba-2, we initialize Mamba-2 from Transformer at each layer. By comparing Eq. 1 and Eq. 2, we can find that Mamba-2 shares the similarity with Transformer, which means we can directly inherit Q, K, and projection parameters at each layer instead of building from scratch. Furthermore, we need to introduce extra parameters γ and for state space modeling, replacing the attention mechanism. For better replacement and ease the training difficulty (Trockman et al., 2024), we initialize γ and to make the gating term γt close to 1 at the beginning of training, which means we begin by memorizing all historical information without selectivity. Beyond the core SSM mechanism, we also introduce extra causal convolution and output gating for enhanced positional awareness and expressiveness. To eliminate the initial impact of causal convolution, we initialize the weights and biases to make it function as an identity layer (i.e., the output of causal convolution is the same as the input) without affecting the original function of SSM at the beginning of training. The other parts of the model such as the MLP layers and text and image patch embedding layers are directly inherited from the original Transformer-based VLM and kept as frozen. 4.2. Stage-1: Layerwise Distillation for the Newly Introduced SSM Parameters We first perform layerwise distillation for the introduced extra parameters to align the proposed Mamba-2 layer with Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Figure 3: Progressive distillation pipeline of our mmMamba. We keep MLP layers, text and image patch embedding layers and freeze them in subsequent distillation training stages. Stage-1: Train the newly-introduced SSM-specific parameters while freezing inherited Transformer parameters in layer-wise manner. Stage-2: Train all parameters to align Mambas state representation with Transformer in layer-wise manner. Stage-3: Train all the Mamba layers of the model to align the end-to-end behavior with the teacher Transformer-based VLM. logits of the teacher model: ϕStage-3 = {ai, min ϕStage-3 conv, LKL(Teacher-(X0), StudentϕStage3 (X0)), Q, G, γ, K, V }L i=1, (5) where X0 is the same multi-modal input sequence to the teacher model and the student model, ϕStage3 is the trainable parameters of the student model. 4.5. Architecture Our mmMamba builds upon HoVLE, decoder-only VLM that consists of 32 Transformer layers. For mmMambalinear, we convert each Transformer layer into Mamba2 layer while preserving the MLP layers, resulting in linear-complexity decoder-only VLM. To enhance model expressiveness, we adopt multi-head design in our Mamba2 layers by partitioning the SSM into multiple groups and implementing shared queries across groups, consistent with the grouped query attention used in HoVLE. For mmMamba-hybrid, we introduce systematic layer conversion scheme. Specifically, within every fixed number of consecutive layers, we preserve the first layer as Transformer and convert the remaining layers to Mamba-2. This hybrid scheme maintains the Transformers modeling capacity at critical feature hierarchies while leveraging Mamba2s linear complexity for the majority of computation. Such design enables an effective and flexible trade-off between computational efficiency and model capability, suitable for various deployment scenarios with varied requirements. In this paper, we set the interval as 4, building mmMambahybrid with 8 Transformer layers and 24 Mamba-2 layers in total. 6 5. Experiment 5.1. Implementation Detail Training. All models are trained using 8 NVIDIA A800 80GB GPUs with BF16 precision and DeepSpeed ZeRO2 (Rajbhandari et al., 2020; Rasley et al., 2020). The distillation process utilizes SOLOs (Chen et al., 2024a) supervised fine-tuning dataset, comprising 1.7M samples across both language-only and image-text paired instances. We employ the AdamW (Loshchilov, 2017) optimizer with β = (0.9, 0.999), gradient clipping at 5.0, and WarmupStable-Decay (WSD) scheduler with 10% warmup and 10% decay periods. For stages-1 and stage-2 distillation, we use batch size of 128, train for 20K steps, and set weight decay to 0.05, with learning rates of 1 103 and 5 104 respectively. Stage-3 distillation employs reduced batch size of 64, continues for 20K steps with weight decay at 0.05, and uses learning rate of 5 105. Evaluation benchmarks. We evaluate our model on 9 diverse public benchmarks, encompassing 6 general VLM benchmarks and 3 visual question answering tasks. The general VLM benchmarks include: MME (Yin et al., 2023), which evaluates visual perception and reasoning through true/false questions; MMBench (Liu et al., 2024b), which assesses model robustness through multiple-choice questions; POPE (Li et al., 2023b), which evaluates object hallucination; SEED (Li et al., 2023a), which gauges open-world multi-modal understanding; MMMU (Yue et al., 2024), which scrutinizes models with college-level multi-discipline reasoning tasks; and MM-Vet (Yu et al., 2023), which evaluates the model on 16 emergent tasks from core visual and linguistic capabilities. The visual question answering benchmarks comprise: TextVQA (Singh et al., 2019), which evalMultimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Method Recipe Complexity # P. # T.P. MME MMB POPE SEED MMMU MM-Vet TQA SQA-I GQA Encoder-based VLMs OpenFlamingo (Awadalla et al., 2023) PT, SFT PT, SFT MiniGPT-4 (Zhu et al., 2023) PT, SFT Qwen-VL (Bai et al., 2023b) PT, SFT LLaVA-Phi (Zhu et al., 2024) PT, SFT MobileVLM-3B (Chu et al., 2023) PT, SFT VisualRWKV (Hou et al., 2024) PT, SFT VL-Mamba (Qiao et al., 2024) PT, SFT Cobra (Zhao et al., 2024) - - - Quadratic 9B 96.6% - 4.6 Quadratic 13B 94.8% 581.7 23.0 Quadratic 7B 100.0% - 38.2 Quadratic 3B 90.0% 1335.1 59.8 85.0 Quadratic 3B 90.0% 1288.9 59.6 84.9 3B 90.0% 1369.2 59.5 83.1 3B 90.0% 1369.6 57.0 84.4 88.4 3.5B 82.6% - Linear Linear Linear - - - 56.3 - - - - - Decoder-only VLMs Fuyu-8B (HD) (Bavishi et al., 2023) SOLO (Chen et al., 2024a) Chameleon-7B (Team, 2024) EVE-7B (Diao et al., 2024) Emu3 (Wang et al., 2024b) HoVLE (Tao et al., 2024) mmMamba mmMamba PT, SFT PT, SFT PT, SFT PT, SFT PT, SFT - Quadratic 8B 100.0% 728.6 10.7 74.1 64.4 Quadratic 7B 100.0% 1001.3 Quadratic 7B 100.0% 170 30.6 Quadratic 7B 100.0% 1217.3 49.5 83.6 61.3 58.5 85.2 68.2 Quadratic 8B 100.0% - DT, PT, SFT Quadratic 2.6B 100.0% 1433.5 71.9 87.6 70.7 2.7B 14.7% 1303.5 57.2 85.2 62.9 2.7B 11.2% 1371.1 63.7 86.7 66.3 Linear Hybrid - 31.1 DT DT - - - - - - - - - - - - 25.4 32.3 31.6 33.7 30.7 32.3 - 22.1 - 28.9 - - 32.6 - 21.4 - 8.3 25.6 37.2 44.3 31.1 36.9 33.6 - - - - 32.2 63.8 67.1 59.3 48.6 68.4 47.5 61.0 59.0 48.7 65.3 59.6 48.9 65.4 56.2 62.3 - 58. - - - - - - 73.3 - 4.8 47.2 51.9 63.0 60.8 64.7 89.2 60.3 66.0 94.8 60.9 47.7 79.2 57.4 55.1 86.9 59.3 Table 1: Comparison with existing VLMs on general VLM benchmarks. Recipe denotes the adopted training recipe. PT, SFT, and DT denote the pre-training, supervised fine-tuning, and distillation training, respectively. Complexity denotes the model computation complexity with respect to the number of tokens. # P. denotes the number of total parameters. # T.P. denotes the percentage of trainable parameters ( trainable paramters total parameters ). The best performance is highlighted in bold and the second-best result is underlined. Model LLM Backbone Vision Encoder Total Params Visual Tokens Output Tokens Speed (tokens/s) Total (s) LLaVA-Phi Phi2-2.7B MobileVLM-3B LLaMA-2.7B CLIP ViT-L/14 CLIP ViT-L/ HoVLE 32-layer Transformer Cobra-3.5B Mamba-2.8B DINOv2 + SigLIP ViT-SO VisualRWKV-3B RWKV6-3B mmMamba-linear mmMamba-hybrid 24-layer Mamba2 + 8-layer Transformer 32-layer Mamba CLIP ViT-L/14 3.1B 3.1B 2.6B 3.5B 3.4B 2.7B 2.7B 576 144 768 729 577 768 768 256 256 256 256 256 256 26.92 35.26 33.03 99.22 41.34 132.43 134.77 9.51 7.26 7.75 2.58 6.19 1.93 1.90 Table 2: Inference efficiency comparison under same multimodal prompt and fixed decode length. We compare with VLMs of the similar parameter scale (3B) across encoder-based, decoder-only, quadratic-complexity, and linear-complexity. The results highlight the speed advantage of mmMamba-linear/hybrid. The benchmark recipe directly follows Cobra, and we report the results on the same single NVIDIA RTX 4090 GPU. Note that Total Time includes the time of both prefilling and decoding, and Speed = Output Tokens / Total Time. uates optical character recognition (OCR) capabilities and text-based reasoning; ScienceQA (Lu et al., 2022), which tests scientific image comprehension; and GQA (Hudson & Manning, 2019), which assesses real-world visual reasoning and compositional question answering. For the specific score in the comparison, we report the MMEperception score as the MME score, MMB score is calculated on the MMBench-EN split, and the POPE score is calculated by averaging across its three categories. 5.2. Main Comparison In Tab. 1, we compare mmMamba with previous encoderbased and decoder-only VLMs on 9 popular benchmarks. We highlight the following findings: mmMamba only performs distillation as the training recipe, which requires much lower training cost in two aspects: (1) dataset collectionunlike other methods require separate curated datasets for pre-training (PT) and supervised fine-tuning (SFT), our distillation recipe only needs single SFT dataset; (2) trainable parametersour method only updates 14.7% parameters for mmMamba-linear and 11.2% parameters for mmMamba-hybrid during training, while other methods require training most of parameters. mmMamba-linear previous SOTA surpasses Transformer-based decoder-only VLM EVE-7B on 6/9 benchmarks (i.e., MME, MMB, POPE, SEED, MM-Vet, ScienceQA), while matching the performance on the left 3 benchmarks with 2 fewer parameters. Even compared with encoder-based VLMs (e.g., MobileVLM-3B, LLaVA-phi), mmMamba-linear still demonstrates comparable performance, while the computation complexity is reduced to linear complexity. Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation ID Stage1 Stage2 Stage3 MME POPE TextVQA SQA-I Attention Layers MME POPE TextVQA SQA-I 1 2 3 4 5 6 7 8 NAN NAN 969.8 70.6 1007.1 72.9 1188.4 83.0 1108.9 75.3 1263.1 84.0 1255.5 83.5 1303.5 85.2 Table 3: Ablation for training stages. NAN 13.47 25.5 40.0 28.0 42.5 41.1 47.7 NAN 40.8 52.1 63.4 59.3 77.1 72.1 79.2 Init Strategy MME POPE TextVQA SQA-I from scratch inherit Q,K,V 1214.0 1222.6 inherit Q,K,V + mimic 1303.5 83.1 84.0 85. 40.0 41.9 47.7 67.4 73.3 79.2 Table 4: Ablation for parameter initialization. encoder-based mmMamba-linear matches the performance of linear-complexity VLMs recent (VisualRWKV-3B and VL-Mamba-3B), while significantly outperforming them on the ScienceQA benchmark. By interleaving with the Transformer layers, mmMamba-hybrid achieves improved performance on all benchmarks over mmMamba-linear, significantly narrowing the gap with the teacher Transformer-based HoVLE and outperforming the linear complexity encoder-based VLMs (VisualRWKV-3B and VL-Mamba-3B). 5.3. Efficiency Analysis Fixed prompt and fixed decode length. We directly follow the benchmark recipe of Cobra in Tab. 2, where we prompt the VLM model with the same example image and question Describe the image specifically, and set the number of output tokens to 256. We record the total time of the VLM model, which includes the image/text prompt prefilling time and the decoding time. The speed (tokens/s) is calculated by the number of output tokens (i.e., 256) divided by the total time. We compare our method with 3 transformer-based VLMs and 2 linear-complexity encoder-based VLMs of similar parameter scale. All the evaluations are conducted on the same single NVIDIA RTX 4090 GPU. Thanks to the fixed hidden state rooted in linear-complexity modeling, mmMamba-linear/hybrid achieve nearly 4 faster inference speed than all the Transformer-based VLMs. mmMamba-linear/hybrid also outperforms the linear complexity encoder-based VLMs (Cobra-3.5B and VisualRWKV-3B) by large margin (about 30 tokens/s and 3 faster, respectively) due to the simple decoder-only architecture. Increasing context length. Long context processing has emerged as crucial capability in modern VLMs, becom8 0 1 2 4 8 32 1303.5 1304.3 1318.4 1329.1 1371. 1433.5 85.2 85.5 86.3 86.8 86.7 87.6 47.7 48.0 48.4 51.5 55. 66.0 79.2 79.3 79.9 82.8 86.9 94.8 Table 5: Ablation for the number of interleaved attention layers. 0 denotes mmMamba-pure, 8 denotes mmMamba-hybrid, 32 denotes the full Transformer model HoVLE. Hybrid strategy MME POPE TextVQA SQA-I Tail-stacked Head-stacked Tail-interleaved Head-interleaved 1305.5 1329.4 1308.3 1371.1 85.9 85.9 86.1 86. 53.7 55.0 55.0 55.1 79.4 80.8 86.5 86.9 Table 6: Ablation for hybrid strategy. ing increasingly important for high-resolution image/video understanding (Chen et al., 2023; Li et al., 2024a) and long chain-of-thought multimodal reasoning (Xu et al., 2024; Lightman et al., 2023; DeepSeek-AI et al., 2025; Muennighoff et al., 2025; Team et al., 2025), which often require processing sequences of thousands of tokens. To showcase the efficiency of the proposed mmMamba in this application, we compare our model with Transformer-based HoVLE in the same single NVIDIA RTX 4090 GPU. We report the GPU memory usage and the latency of the model for decoding the next token. As shown in Fig. 1, thanks to the efficient implementation of FlashAttention2, HoVLE demonstrates stable and low latency under 4K token length. As the context token length reaches 8K and beyond, the latency and memory of HoVLE increase linearly with the token length due to the growing Key-Value cache, when the token length reaches 128K, HoVLE squeezes out of the GPU memory and fails to decode. On the contrary, mmMamba-linear exhibits low and stable latency and memory usage with increasing token length, and the inference cost of mmMambahybrid increases much slower than HoVLE, which can still decode at the 128K token length. Specifically, at the 103K token length, mmMamba-linear demonstrates 20.6 speedup compared to HoVLE and saves 75.8% GPU memory, while mmMamba-hybrid achieves 13.5 speedup and saves 60.2% GPU memory. 5.4. Ablation Study Stage importance. As shown in Tab. 3, direct weight transfer from Transformer to Mamba-2 without distillation (Sec. 4.1) lost the multi-modal understanding ability. By progressively adding the designed distillation stages, the models performance is increasingly improved. When comMultimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation paring ID-7 and ID-8, we can see that the proposed extra parameter distillation stage-1 decouples the optimization and eases the training, leading to better alignment, with consistent improvements across all metrics (48 in MME, 1.7 in POPE, 6.6 in TextVQA, 7.1 in ScienceQA). Parameter initialization. In Tab. 4, We compare with the from scratch strategy used in Phi-Mamba (Bick et al., 2024), which replace the trained Transformer layer with directly initialized Mamba-2 layer without inheriting the parameters, and the inherit Q,K,V strategy used in LoLCATs (Zhang et al., 2024a) and Mamba in the LLaMA (Wang et al., 2024a), which exploit the similarity and only inherit the parameters of Q,K,V,O from Transformer to Mamba-2. The results validate the superiority of our proposed parameter initialization strategy, which should not only inherit the trained parameters but also initialize the extra introduced parameters by mimicking the original attention mechanism. Hybrid architecture. The proposed distillation recipe is more flexible than the previous training recipe used in building linear-complexity encoder-based VLM, which requires the trained linear-complexity LLM and can not modify the architecture. As shown in Tab. 5, we can build hybrid architecture with varied interleaved Transformer layers, enabling the flexible trade-off between performance and efficiency. By increasing the number of Transformer layers, the performance is gradually improved. The hybrid architecture with 24 Mamba-2 layers and 8 Transformer layers can achieve comparable performance with minor decrease compared to the full Transformer model HoVLE. Hybrid strategy. In Tab. 6, we explore specific hybrid strategies while fixing the number of interleaved Transformer layers to 8. We study 4 interleaving strategies: (1) Tail-stacked: stacking all 8 Transformer layers at the top of the network. (2) Head-stacked: stacking all 8 Transformer layers at the bottom of the network; (3) Tail-interleaved: interleaving Transformer layer at the tail of every 4-layer block; (4) Head-interleaved: interleaving Transformer layer at the head of every 4-layer block; The results demonstrate that the Head-interleaved strategy is the most effective, achieving the best performance across all metrics 6. Conclusion We presented mmMamba, novel framework for building linear-complexity decoder-only VLMs with only moderate academic resources through the proposed distillation recipe, eliminating the need for pre-trained linear-complexity LLMs and vision encoders. Our recipe enables both pure linear and hybrid architectures, achieving competitive performance while significantly reducing computational costs. Experimental results demonstrate that mmMamba-linear matches or exceeds the performance of existing linear-complexity and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance through flexible efficiencyperformance trade-offs with interleaved Transformer layers. At 103K tokens, mmMamba-linear achieves up to 20.6 speedup and 75.8% memory reduction compared to Transformer-based teacher HoVLE, while mmMambahybrid achieves 13.5 speedup and saves 60.2% GPU memory. These results validate the effectiveness of our distillation recipe for building linear-complexity decoder-only VLMs suitable for long-context applications."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: frontier large visionlanguage model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023b. Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., and Tasırlar, S. Introducing our multimodal models, 2023. URL https://www.adept. ai/blog/fuyu-8b. Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Bick, A., Li, K. Y., Xing, E. P., Kolter, J. Z., and Gu, A. Transformers to ssms: Distilling quadratic knowledge to subquadratic models. arXiv preprint arXiv:2408.10189, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, G., Zheng, Y.-D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et al. Videollm: 9 Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292, 2023. Chen, Y., Wang, X., Peng, H., and Ji, H. single transformer for scalable vision-language modeling. arXiv preprint arXiv:2407.06438, 2024a. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024b. Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang, B., Wei, X., et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. Dao, T. and Gu, A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J.-M., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B.-L., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D.-L., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S.-K., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W.-X., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X.-C., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y.-J., He, Y., Xiong, Y., Luo, Y.-W., mei You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., guo Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z.-A., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Diao, H., Cui, Y., Li, X., Wang, Y., Lu, H., and Wang, X. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1935819369, 2023. Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., and Cao, Y. Eva-02: visual representation for neon genesis. Image and Vision Computing, 149:105171, 2024. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Hou, H., Zeng, P., Ma, F., and Yu, F. R. Visualrwkv: Exploring recurrent neural networks for visual language models. arXiv preprint arXiv:2406.13362, 2024. Huang, W., Pan, J., Tang, J., Ding, Y., Xing, Y., Wang, Y., Wang, Z., and Hu, J. Ml-mamba: Efficient multi-modal large language model utilizing mamba-2. arXiv preprint arXiv:2407.19832, 2024. Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6700 6709, 2019. Javaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck, S., Mendes, C. C. T., Chen, W., Del Giorno, A., Eldan, R., Gopi, S., et al. Phi-2: The surprising power of small language models. Microsoft Research Blog, 1(3):3, 2023. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G., Pappas, N., Mao, Y., Chen, W., and Smith, N. A. Finetuning pretrained transformers into rnns. arXiv preprint arXiv:2103.13076, 2021. Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Li, J., Chen, D., Cai, T., Chen, P., Hong, Y., Chen, Z., Shen, Y., and Gan, C. Flexattention for efficient high-resolution In European Conference on vision-language models. Computer Vision, pp. 286302. Springer, 2024a. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in large visionarXiv preprint arXiv:2305.10355, language models. 2023b. Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., and Bai, X. Monkey: Image resolution and text label are important things for large multi-modal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2676326773, 2024b. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024a. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024b. Loshchilov, I. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Mercat, J., Vasiljevic, I., Keh, S., Arora, K., Dave, A., Gaidon, A., and Kollar, T. Linearizing large language models. arXiv preprint arXiv:2405.06640, 2024. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Grella, M., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Du, X., Ferdinan, T., Hou, H., et al. Eagle and finch: Rwkv with matrixvalued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. Qiao, Y., Yu, Z., Guo, L., Chen, S., Zhao, Z., Sun, M., Wu, Q., and Liu, J. Vl-mamba: Exploring state space models for multimodal learning. arXiv preprint arXiv:2403.13600, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 35053506, 2020. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Evaclip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. Tao, C., Su, S., Zhu, X., Zhang, C., Chen, Z., Liu, J., Wang, W., Lu, L., Huang, G., Qiao, Y., et al. Hovle: Unleashing the power of monolithic vision-language models with holistic vision-language embedding. arXiv preprint arXiv:2412.16158, 2024. 11 Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation Zhang, M., Arora, S., Chalamala, R., Wu, A., Spector, B., Singhal, A., Ramesh, K., and Re, C. Lolcats: On lowrank linearizing of large language models. arXiv preprint arXiv:2410.10254, 2024a. Zhang, M., Bhatia, K., Kumbong, H., and Re, C. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. arXiv preprint arXiv:2402.04347, 2024b. Zhao, H., Zhang, M., Zhao, W., Ding, P., Huang, S., and Wang, D. Cobra: Extending mamba to multi-modal large language model for efficient inference. arXiv preprint arXiv:2403.14520, 2024. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Zhu, Y., Zhu, M., Liu, N., Xu, Z., and Peng, Y. Llavaphi: Efficient multi-modal assistant with small language model. In Proceedings of the 1st International Workshop on Efficient Multimedia Computing under Limited, pp. 1822, 2024. Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Trockman, A., Harutyunyan, H., Kolter, J. Z., Kumar, S., and Bhojanapalli, S. Mimetic initialization helps arXiv preprint state space models learn to recall. arXiv:2410.11135, 2024. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Wang, J., Paliotta, D., May, A., Rush, A. M., and Dao, T. The mamba in the llama: Distilling and accelerating hybrid models. arXiv preprint arXiv:2408.15237, 2024a. Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Xu, G., Jin, P., Hao, L., Song, Y., Sun, L., and Yuan, L. Llava-cot: Let vision language models reason step-bystep. arXiv preprint arXiv:2411.10440, 2024. Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1197511986, 2023."
        }
    ],
    "affiliations": [
        "Horizon Robotics",
        "Institute of Artificial Intelligence, Huazhong University of Science & Technology",
        "School of EIC, Huazhong University of Science & Technology"
    ]
}