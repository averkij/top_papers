{
    "paper_title": "EMOv2: Pushing 5M Vision Model Frontier",
    "authors": [
        "Jiangning Zhang",
        "Teng Hu",
        "Haoyang He",
        "Zhucun Xue",
        "Yabiao Wang",
        "Chengjie Wang",
        "Yong Liu",
        "Xiangtai Li",
        "Dacheng Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterparts have been recognized by attention-based design. Our work rethinks the lightweight infrastructure of efficient IRB and practical components in Transformer from a unified perspective, extending CNN-based IRB to attention-based models and abstracting a one-residual Meta Mobile Block (MMBlock) for lightweight model design. Following neat but effective design criterion, we deduce a modern Improved Inverted Residual Mobile Block (i2RMB) and improve a hierarchical Efficient MOdel (EMOv2) with no elaborate complex structures. Considering the imperceptible latency for mobile users when downloading models under 4G/5G bandwidth and ensuring model performance, we investigate the performance upper limit of lightweight models with a magnitude of 5M. Extensive experiments on various vision recognition, dense prediction, and image generation tasks demonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g., EMOv2-1M/2M/5M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-order CNN-/Attention-based models significantly. At the same time, EMOv2-5M equipped RetinaNet achieves 41.5 mAP for object detection tasks that surpasses the previous EMO-5M by +2.6. When employing the more robust training recipe, our EMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates the performance of 5M magnitude models to a new level. Code is available at https://github.com/zhangzjn/EMOv2."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 4 7 6 6 0 . 2 1 4 2 : r IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE EMOv2: Pushing 5M Vision Model Frontier Jiangning Zhang, Teng Hu, Haoyang He, Zhucun Xue, Yabiao Wang, Chengjie Wang, Yong Liu, Xiangtai Li, Dacheng Tao AbstractThis work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterparts have been recognized by attention-based design. Our work rethinks the lightweight infrastructure of efficient IRB and practical components in Transformer from unified perspective, extending CNN-based IRB to attention-based models and abstracting one-residual Meta Mobile Block (MMBlock) for lightweight model design. Following neat but effective design criterion, we deduce modern Improved Inverted Residual Mobile Block (i2RMB) and improve hierarchical Efficient MOdel (EMOv2) with no elaborate complex structures. Considering the imperceptible latency for mobile users when downloading models under 4G/5G bandwidth and ensuring model performance, we investigate the performance upper limit of lightweight models with magnitude of 5M. Extensive experiments on various vision recognition, dense prediction, and image generation tasks demonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g., EMOv2-1M/2M/5M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-order CNN-/Attention-based models significantly. At the same time, EMOv2-5M equipped RetinaNet achieves 41.5 mAP for object detection tasks that surpasses the previous EMO-5M by +2.6. When employing the more robust training recipe, our EMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates the performance of 5M magnitude models to new level. Code is available at https://github.com/zhangzjn/EMOv2. Index TermsComputer Vision, Lightweight Vision Backbone, Vision Architecture Design"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Lightweight models are particularly crucial in resource-constrained scenarios, drawing many research efforts [1], [2], [3], [4], [5], [6], [7] in various fields. Early work primarily can be divided into two categories: 1) models with fewer FLOPs and faster hardwarespecific inference speeds [8], [9], [10], [11], [12], which do not emphasize parameter counts and perform poorly in high-resolution downstream tasks; 2) models that balance FLOPs and performance under limited parameter counts [2], [13], resulting in more compact models. With the development of computational devices, most current models achieve throughput of several thousand and latency within real-time 20ms [1], [2], [14], where computational power is not the bottleneck for small model applications, even if we strive to reduce their computational requirements. Additionally, edge applications iterate models rapidly, as seen in short video platforms like TikTok, where effects frequently update lightweight real-time detection algorithms and small-scale generation models. Considering the imperceptible delay in downloading models under 4G/5G bandwidth and ensuring model performance, lightweight model of 5M magnitude is recommended as an appropriate size [15], [16]. Therefore, this paper explores the upper limits of lightweight model performance with fixed parameter count, using 5M lightweight model as typical representative. MobileNetv2 [9] introduces an efficient Inverted Residual Block (IRB) based on Depth-Wise Separable Convolution (DW-Conv), which is widely regarded as the foundation of efficient models [10], [12], [17]. However, constrained by the natural induction bias of static convolution operations, the accuracy of CNN-based J. Zhang, Y. Wang, and C. Wang are with Youtu Lab, Tencent, China. T. Hu is with Shanghai Jiao Tong University, Shanghai, China. H. He, Z. Xue, and Y. Liu are with the Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China. X. Li and D. Tao are with the Nanyang Technological University, Singapore. lightweight models is suboptimal due to the lack of global modeling capabilities. This motivates us to explore the construction of stronger fundamental block that surpasses the IRB by introducing global modeling capabilities. On the other hand, benefiting from the dynamically global modeling capability of Multi-Head Self-Attention (MHSA), Vision Transformer (ViT) [18] and its derivatives [19], [20], [21], [22], [23], [24], [25], [26] have achieved significant improvements over CNNs. Some works attempt to address the quadratic computational complexity of MHSA by designing variants with linear complexity [27], [28], reducing the spatial resolution of features [19], [29], [30], rearranging channels [31], and employing local window attention [21], [22], among other strategies. Recently, researchers have introduced MHSA into certain layers of lightweight CNN models to improve complex blocks [2], [14], [17], [32], [33], [34] or have used multiple hybrid blocks. However, such designs lack uniformity, require meticulous design, and pose higher demands for adaptation to mobile device deployment. So far, no works explore MHSA-based counterparts as IRB, and this inspires us to think: can we build lightweight IRB-like infrastructure for attention-based models with only basic operators? Based on the motivation above, we rethink the efficient IRB in MobileNetv2 [9] and the MHSA / FFN modules in Transformer [35] from unified perspective, expecting to integrate their advantages at the infrastructure design level. As shown in Fig. 2-Left, while working to bring one-residual IRB with inductive bias into the attention model, we observe that MHSA/FFN submodules in two-residual Transformer share similar metastructure to IRB. Thus, we inductively abstract one-residual Meta Mobile Block (MMBlock in Sec. 3.2.1) that takes parametric arguments expansion ratio λ and efficient operator to instantiate different modules, i.e., IRB, MHSA, and FFN. MMBlock reveals the consistent essence expression of the above three modules and can be regarded as an improved lightweight concentrated IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 parameters on Kinetics-400 for video classification that surpasses UniFormer-XXSs 63.2 with 9.8M parameters. In addition, we enhance the recently popular UNet and DiT architectures for image segmentation and generation across multiple downstream tasks based on this module (Sec. 3.3.3). E.g., U-EMO-v2 obtains 88.3mAcc with 21.3M parameters on HRF; D-EMO-v2 achieves 46.3/9.6 FID in generating 256256 ImageNet images with 400K training steps on S/XL scales, which significantly surpasses DiTs 68.4/19.5. In summary, we make the following significant extensions over the preliminary conference version (EMO [13] at ICCV23): 1) Based on the abstracted one-residual Meta Mobile Block for lightweight model design, we extend the iRMB to powerful i2RMB block. Specifically, we design parametersharing spanning attention mechanism, enabling interaction between neighborhood and distant spatial features within single module without increasing the models parameter count. This mechanism is also compatible with EW-MHSA, achieving efficient feature modeling for mobile applications. Additionally, we improve the post-attention and large local kernel structures to further enhance model performance. 2) We construct 4-stage EMOv2 backbone solely based on the deduced i2RMB block. This model significantly improves performance while maintaining the similar parameter count as EMOv1. For instance, EMOv2-5M achieves +1.0 improvement over EMOv1-5M in classification tasks. The performance gap widens further in high-resolution downstream tasks, with improvements of +1.7 and +2.6 mAP using SSDLite and RetinaNet, respectively. We also explore the impact of stronger training strategies on model performance, validating the models scaling capability, with EMOv2-5M reaching up to 82.9 Top-1 accuracy. 3) Thanks to the general, neat, and powerful design of i2RMB, we can easily extend it to series of tasks, constructing various lightweight versions of different types of structures and achieving significant improvements. Finally, we provide detailed studies and experimental analysis to build our attention-based lightweight models in Sec. 4.3. 4) We re-write the entire draft and add more comprehensive discussion on close related works. We open-source our EMOv2 for the community."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Lightweight CNN Models. With the increasing demands of neural networks for mobile vision applications, efficient model design has attracted extensive attention from researchers in recent years. SqueezeNet [37] replaces 3x3 filters with 1x1 filters and decreases channel numbers to reduce model parameters, while Inceptionv3 [38] factorizes the standard convolution into asymmetric convolutions. Later, MobileNet [8] introduces depth-wise separable convolution to alleviate large amount of computation and parameters, followed in subsequent lightweight models [6], [9], [11], [39]. Besides the above hand-craft methods, researchers exploit automatic architecture design in the pre-defined search space [1], [10], [12]. Specifically, RepViT [40] leverages the reparameterization technique to enhance model performance, while recent GhostNetV3 [41] has further incorporated Knowledge Distillation (KD) strategy. MobileNetv4 [42] employs both NAS algorithm and KD strategy to achieve impressive results, where strong training recipe has already become trend in lightweight Fig. 1: Top: Performance vs. Parameters with concurrent methods. Our EMOv2 achieves significant accuracy with fewer parameters. Superscript : The comparison methods employ more robust training strategies described in their papers, while ours uses the strategy mentioned in Tab. 17(e). Bottom: The range of token interactions varies with different window attention mechanisms. Our EMOv2, with parameter-shared spanning attention in Sec. 3.3.1, has larger and correspondingly stronger Effective Receptive Field (ERF). aggregate of Transformer. Furthermore, neat yet effective Inverted Residual Mobile Block (iRMB) is deduced that only contains fundamental Depth-Wise Convolution and the improved EWMHSA (c.f ., Sec. 3.2.2). And we build ResNet-like 4-phase Efficient MOdel (EMOv1) with only iRMBs (c.f ., Sec. 3.2). Even though EMOv1 [13] achieves promising results, it is limited by window attention that can only model the interaction of neighbor information within local window, as shown in Fig. 1-Bottom. This modeling approach leads to suboptimal performance in high-resolution downstream tasks due to the lack of distant information interaction. For instance, RetinaNet [36] using EMOv1-5M only achieves 38.9 mAP that does not even reach 40. Recently, MobileViT [17] attempts to model long-range attention but performs moderately due to the loss of local dynamic modeling capability and significant increase in FLOPs with higher resolutions. Thus, more balanced efforts between longrange modeling and lower GFlops are needed. To overcome these challenges, we explore the procedure of attention computation and discover that the neighbor window attention map can be reused to model the correlation between distant positions. Based on this, we design novel spanning mechanism Sec. 3.3.2 (i.e., SEW-MHSA) that simultaneously models neighbor and distant features. As shown in Fig. 1, this mechanism does not increase the number of parameters and only adds small number of FLOPs. It significantly enhances the models effective receptive field, thereby improving performance in high-resolution downstream tasks (Sec. 4.2). Additionally, we improve the detailed structure of i2RMB to enhance the performance further and explore different training strategies to maximize the models potential in mainstream image classification tasks. Detailed comparison with state-of-theart methods can be viewed in Fig. 1. Due to the neat structural design, i2RMB can be easily extended to various downstream tasks, achieving significant and consistent performance improvements. Specifically, we apply EMOv2 to the temporal dimension for video recognition, and V-EMO-v2 obtains 65.2 Top-1 accuracy with 5.9M IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE model research. We draw on lightweight design principles from the CNN domain, such as depth-wise convolution and inverted residual designs, and integrate them with attention mechanisms to construct stronger hybrid module. Hugging Vision Transformer with CNN. Since ViT [18] first introduces Transformer structure [35] into visual tasks, massive improvements have successfully been developed. DeiT [43] provides benchmark for efficient transformer training, subsequent works [19], [21] employ ResNet-like [44] pyramid structure to form pure Transformer-based models for dense prediction tasks. However, the absence of 2D convolution will potentially increase the optimization difficulty and damage the model accuracy for lacking local inductive bias, so researchers [45], [46] concentrate on how to better integrate convolution into Transformer for obtaining stronger hybrid models. E.g., work [47] incorporates convolution design into FFN, works [48], [49] regard convolution as the positional embedding for enhancing inductive bias of the model, and works [29] for attention and QKV calculations, respectively. Recently, MogaNet [50] encapsulates conceptual convolutions and gated aggregation into compact module, and SHViT [51] uses depthwise convolution layer for local feature aggregation or conditional position embedding. However, the above methods are still confined to the MetaFormer [52] architecture, where each block contains two residual connections. EMOv1 studies how to build neat but effective lightweight model based on an improved oneresidual attention block. In contrast, this paper further investigates the parameter-sharing mechanism for window attention, enabling it to simultaneously model neighbor and distant information interactions, thereby significantly enhancing the performance of downstream tasks. Effective Transformer Improvements. Researchers [2], [53] have started to lighten Transformer-based models for low computational power. Tao et al. [53] introduces additional learnable tokens to capture global dependencies efficiently, and Chen et al. [53] design parallel structure of MobileNet and Transformer with twoway bridge in between. Works [54], [55] improve an efficient Transformer block by borrowing convolution operation, while EdgeNeXt [2] absorbs effective Res2Net [56] and transposed channel attention [57]. MobileVit series [14], [17], [32] fuse improved MobileViT blocks with Mobile blocks [9]. Recent EfficientFormerV2 [1] uses the NAS algorithm to search hardwarefriendly modules, while ViG [58] introduces gating mechanism to facilitate the interaction of sequential and spatial information. However, most current approaches require elaborate complex modules, which limits the mobility and usability of the model. How to balance parameters, computation, and accuracy while designing easy-to-use lightweight models still needs further exploration. RNN-reinvented Models. Due to the quadratic growth in computational complexity of Transformers with the number of tokens, some RNN-based models [59], [60], [61] have gradually gained attention, with Mamba [62] and RWKV [63] being the primary representatives. Zhu et al. [64] proposes vision Mamba, which applies SSM to visual tasks, while Duan et al. [60] also introduces vision version based on RWKV. Recently, works [65], [66] explore the application of Mamba in lightweight visual tasks. These methods can seamlessly integrate into our proposed Meta Mobile Block, yielding favorable results. However, considering the verified stable performance of transformers across various fields, this paper explores improvements to the attention module based on windowed operation. TABLE 1: Criterion comparison for current efficient models. ➀: Usability; ➁: Uniformity; ➂: Efficiency and Effectiveness; ➃: Generalization. : Satisfied. : Partially satisfied. : Unsatisfied. Method vs. Criterion MobileNet Series [8], [9], [32] MobileViT Series [14], [17], [32] EdgeNeXt [2] EdgeViT [55] RepViT [40] EfficientFormerV2 [1] EfficientVMamba [65] MogaNet [50] EMOv1 EMOv"
        },
        {
            "title": "3 METHODOLOGY",
            "content": "➀ ➁ ➂ ➃"
        },
        {
            "title": "3.1 Criteria for General Lightweight Model",
            "content": "When designing light-weight visual models for mobile usages, we advocate the following criteria subjectively and empirically that an efficient model should satisfy as much as possible: ➀ Usability. Neat implementation that does not use complex operators and is easy to optimize for applications. ➁ Uniformity. As few core modules as possible to reduce model complexity and accelerate deployment. ➂ Efficiency and Effectiveness. Balancing parameters and calculations with accuracy trade-off. ➃ Generalization. Easily applied to perception tasks such as classification, detection, and segmentation, as well as to generative tasks, while compatible with architectures like ResNet and UNet. We make summary of current efficient models in Tab. 1: 1) Performance of MobileNet series [8], [9], [32] is now seen to be slightly lower, and its parameters are slightly higher than counterparts. 2) Recent MobileViT series [14], [17], [32] achieve notable performances, but they suffer from higher FLOPs and slightly complex modules. 3) EdgeNeXt [2] and EdgeViT [55] obtain pretty results, but their basic blocks also consist of elaborate modules. 4) RepViT [40] employs multiple fundamental modules and introduces re-parameterization strategy, while EfficientFormerV2 [1] utilizes NAS to search for hardware-friendly models, and EfficientVMamba [65] introduces new SSM module. 5) MogaNet [50] achieves balance between performance and efficiency without introducing new complex operators. Comparably, the design principle of our EMO/v2 follows the above criteria without introducing complicated operations (c.f ., Sec. 3.3.2) while still obtaining impressive results on multiple vision tasks (c.f ., Sec. 4). Additionally, EMOv2 can be easily transferred to other models for various tasks, such as video classification, UNet-based image segmentation, and diffusion-based image generation (c.f ., Sec. 3.3.2)."
        },
        {
            "title": "3.2.1 Meta Mobile Block",
            "content": "Motivation. 1) Recent Transformer-based works [21], [68], [69], [70], [71], [72], [73] are dedicated to improving spatial token mixing under the MetaFormer [52] for high-performance network. CNN-based Inverted Residual Block [9] (IRB) is recognized as the infrastructure of efficient models [9], [12], but little work has been done to explore attention-based counterpart. This inspires us to build lightweight IRB-like infrastructure for attentionbased models. 2) While working to bring one-residual IRB with inductive bias into the attention model, we stumble upon two underlying sub-modules (i.e., FFN and MHSA) in two-residual IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4 Fig. 2: Left: Abstracted unified Meta-Mobile Block from Multi-Head Self-Attention, Feed-Forward Network [35], and Inverted Residual Block [9] (c.f . Sec 3.2.1). The inductive block can be deduced into specific modules using different expansion ratio λ and efficient operator . Middle: We construct family of vision models based on our i2RMB module: 4-stage EMOv2, composed solely of the deduced i2RMB (c.f . Sec 3.2.2), for various perception tasks (image classification, detection, and segmentation in Sec. 4.2). Additionally, we introduce the temporally extended V-EMO for video classification, the U-EMO based on an encoder-decoder architecture, and D-EMO to replace the Transformer block in DiT [67]. These downstream models are typically built based on the i2RMB. Right: Performance comparison with different SoTAs on various tasks. Transformer that happen to share similar structure to IRB. This inspires us to integrate these elements into unified block representation, thereby constructing more shallow foundational visual backbone. Compared to each ViT block, which contains two residual connections, our approach simplifies the architecture. Induction. We rethink Inverted Residual Block in MobileNetv2 [9] with core MHSA and FFN modules in Transformer [35], and inductively abstract general Meta Mobile Block (MMBlock) in Fig. 2, which takes parametric arguments expansion ratio λ and efficient operator to instantiate different modules. We argue that the MMBlock can reveal the consistent essence expression of the above three modules, and MMBlock can be regarded as an improved lightweight concentrated aggregate of Transformer. Also, this is the basic motivation for our elegant and easy-touse EMO/v2, which only contains one deduced iRMB/i2RMB absorbing advantages of lightweight CNN and Transformer. Taking image input X( RCHW ) as an example, MMBlock firstly use an expansion MLPe with output/input ratio equaling λ to expand channel dimension: Xe = MLPe(X)( RλCHW ). (1) Then, intermediate operator enhance image features further, e.g., identity operator, static convolution, dynamic MHSA, etc.. Considering that MMBlock is suitable for efficient network design, we present as the concept of efficient operator, formulated as: Xf = (Xe)( RλCHW ). (2) Finally, shrinkage MLPs with inverted input/output ratio equaling λ to shrink channel dimension: Xs = MLPs(Xf )( RCHW ), (3) where residual connection is used to get the final output = + Xs( RCHW ). For clarity, notice that normalization and activation functions are omitted. Relation to MetaFormer. We reveal the differences between our Meta Mobile Block and MetaFormer [52] in Fig. 3. 1) From the structure, two-residual MetaFormer contains two sub-modules with two skip connections, while our Meta Mobile Block contains only one sub-module that covers one-residual IRB in the field Fig. 3: Meta-paradigm comparison between our MMBlock and MetaFormer [52]. We integrate into expended FFN to construct more streamlined and shallower single-module block. of lightweight CNN. Also, shallower depths require less memory access and save costs [74] that is more general and hardwarefriendly for optimization. 2) From the motivation, MetaFormer is the induction of high-performance Transformer/MLP-like models, while our Meta Mobile Block is the induction of efficient IRB in MobileNetv2 [9] and effective MHSA/FFN in Transformer [18], [35] for designing lightweight infrastructure. 3) Inductive oneresidual Meta Mobile Block can be regarded as conceptual extension of two-residual MetaFormer in the lightweight field. We hope our work inspires more future research dedicated to lightweight model design domain based on attention. 4) From the result, our instantiated EMOv2-5M (w/ 5.1M #Params and 1.0G FLOPs) exceeds instantiated PoolFormer-S12 (w/ 11.9M #Params and 1.8G FLOPs) by +2.1, illustrating that stronger efficient operator makes advantage. We further replace Token Mixer in MetaFormer with in iRMB and build 5.3M model. Compared with EMOv1-5M, it only achieves 77.5 Top-1 on ImageNet-1k that is -0.9 than our model, meaning that our proposed Meta Mobile Block has better advantage for constructing lightweight models than two-residual MetaFormer."
        },
        {
            "title": "3.2.2 Micro Designs for Deducted iRMB",
            "content": "Based on the inductive Meta Mobile Block, we instantiate an effective modern Inverted Residual Mobile Block (iRMB) for lightweight architecture design from microscopic view in Fig. 4. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 2: Complexity and Maximum Path Length analysis of modules. Input/output feature maps are in RCW , = 2, = w2, and are feature map size and window size, while and are kernel size and group number. Module MHSA W-MHSA Conv DW-Conv #Params 4(C + 1)C 4(C + 1)C (Ck2/G + 1)C (k2 + 1)C FLOPs 8C2L + 4CL2 + 3L2 8C2L + 4CLl + 3Ll (2Ck2/G)LC (2k2)LC MPL O(1) O(Inf ) O(2W/(k 1)) O(2W/(k 1)) Design principle. Following criteria in Sec. 3.1, in iRMB is modeled as cascaded MHSA and Convolution operations, formulated as () = Conv(MHSA()). This design absorbs CNN-like efficiency to model local features and Transformer-like dynamic modeling capability to learn long-distance interactions. However, naive implementation can lead to unaffordable expenses for two main reasons: 1) λ is generally greater than one that the intermediate dimension would be multiple to input dimension, causing quadratic λ increasing of parameters and computations. Therefore, components of should be independent or linearly dependent on the number of channels. 2) FLOPs of MHSA is proportional to the quadratic of total image pixels, so the cost of naive Transformer is unaffordable for downstream application. The specific influences can be seen in Tab. 2. Expanded Window MHSA. Parameters and FLOPs for obtaining Q,K in Window MHSA (W-MHSA) [21] is quadratic of the channel. Given the input ( RCHW ), we obtain channelunexpanded and ( RCHW ) to compute the attention matrix more efficiently, while the expanded ( RλCHW ) is used to capture finer-grained visual features. The essence of this expanding mechanism is that models only the spatial positional relationships and is independent of the number of channels in . This improvement is termed EW-MHSA, which is more applicable. Specifically, Window Partition operation flattens each feature map {Q, K, } into non-overlapping patches with each sequence length =w h, where =H W/P . The corresponding dimensional transformation can be described by the following formula: [B, C, H, ] [BHW/P, C, ], and vice versa for the Window Reverse operation. To put it more directly, w=4, h=4, =16, and =4 for example in Fig. 4. Structural deduction. Combining lightweight Depth-Wise Convolution (DW-Conv) and efficient EW-MHSA to trade-off model cost and accuracy, the process of the designed iRMB can be formulated as follows: () = DW-Conv(EW-MHSA()). (4) This cascading manner can increase the expansion speed of the receptive field and reduce the maximum path length of the model to O(2W/(k 1 + 2w)), which has been experimentally verified with consistency in Sec. 4.3. Flexibility. Empirically, current transformer-based methods [1], [2], [49], [50], [75] reach consensus that inductive CNN in shallow layers while global Transformer in deep layers composition could benefit the performance. Unlike recent EdgeNeXt that employs different blocks for different depths, our iRMB satisfies the above design principle using only two switches to control whether two modules are used (Code level is also concise in #Supp). Therefore, we can easily implement the use of EW-MHSA for more semantic modeling only in the deeper layers, i.e., stage-3 and stage-4. 5 Fig. 4: Detailed implementation comparison of the Inverted Residual Mobile Block (iRMB in Sec. 3.2.2) and the improved version (i2RMB in Sec. 3.3.1). i2RMB designs parameter-sharing spanning window attention mechanism that simultaneously models the interaction of distant and close window information. TABLE 3: Toy experiments for assessing iRMB and i2RMB. Model DeiT-Tiny [43] DeiT-Tiny / iRMB DeiT-Tiny / i2RMB PVT-Tiny [19] PVT-Tiny / iRMB PVT-Tiny / i2RMB #Params 5.7M 4.9M 5.0M 13.2M 11.7M 11.9M FLOPs 1.3G 1.1G 1.3G 1.9G 1.8G 1.9G Top-1 72.2 74.3 +2.1% 75.0 +2.8% 75.1 75.4 +0.3% 76.1 +1.0% Efficient equivalent implementation. MHSA is typically employed in channel-consistent projection (λ=1), indicating that the FLOPs of multiplying the attention matrix by the expanded Xe (λ>1) will increase by factor of λ - 1. Fortunately, the information flow from to the expanded (Xe) involves only linear operations, allowing us to derive an equivalent proposition: \"When the number of groups in MLPe equals the number of heads in EW-MHSA, the result of the multiplication remains unchanged when the order is exchanged.\" To reduce FLOPs, matrix multiplication before MLPe is used by default, referred to as preattention. Boosting naive transformer. To assess iRMB performance, we set λ to 4 and replace standard Transformer structure in columnar DeiT [43] and pyramidal PVT [19]. As shown in Tab. 3, we surprisingly found that iRMB can improve performance with fewer parameters and computations in the same training setting, especially for the columnar ViT. And the newly proposed i2RMB further boosts the performance significantly. This proves that the one-residual iRMB/i2RMB has obvious advantages over the tworesidual Transformer in the lightweight model. Parallel design of . We also implement the parallel structure of DW-Conv and EW-MHSA with half the number of channels in each component, and some configuration details are adaptively modified to ensure the same magnitude. Comparably, this parallel IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6 model gets 78.1 (-0.3) Top-1 in ImageNet-1k dataset with 5.1M parameters and 964M FLOPs (+63M than EMOv1-5M), but its throughput will slow down by about -7%. Manner Parallel Cascaded (Ours) #Params. 5.1M 5.1M FLOPs 964M 903M Top1 78.1 78.3 Throughput 1618.4 1731.7 This phenomenon is also discussed in the work [74] that: \"Network fragmentation reduces the degree of parallelism\"."
        },
        {
            "title": "3.3 Parameter-Efficient Extension (EMOv2)",
            "content": "Even though EMOv1 achieves satisfactory results, it only models the interaction of neighbor information within local window, which has limited effective receptive field (ERF) (see Fig. 1). This limitation leads to suboptimal performance in high-resolution downstream tasks. We further explore the performance frontier of lightweight models based on this module with negligible increase in model parameters. Specifically, we leverage the principles of attention computation to reuse the neighbor window attention map for uniform sampling over global window size, resulting in novel spanning module termed SEW-MHSA. This mechanism simultaneously models both neighbor and distant features without increasing the number of parameters. Additionally, we elaborately improve structural details to further enhance the models performance."
        },
        {
            "title": "3.3.1 Improved Inverted Residual Mobile Block (i2RMB)\nTo avoid a significant increase in the number of parameters, we\noptimize the EW-MHSA and DW-Conv modules to construct a\nmore powerful i2RMB module in Fig. 4.\nSpanning attention for EW-MHSA. This paper explores the po-\ntential of lightweight models under limited parameters, i.e., mainly\n5M for most mobile scenarios. We observe that in EW-MHSA, the\nattention map only computes feature interactions within windows.\nWhile this alleviates the computational explosion of global atten-\ntion, it inevitably reduces the flow of the receptive field. Therefore,\nwe extend the computation of the attention map to a parallel fusion\nof neighbor and distant window attention, introducing Spanning\nWindow Partition and Reverse steps to achieve this goal. Compared\nto the naive Window Partition described in Sec. 3.2.2, this operation\ninvolves two parallel window partitions that separately segment\nthe shared Q, K, and V into neighbor and distant partitions. In\nthe former, each window contains only adjacent features. In the\nlatter, feature selection within the window is performed based\non a stride of [H/h, W/w]. This allows for feature interac-\ntion at different distances simultaneously, and its transformation\ncan be described by the following formula: [B, C, H, W ] →\n{[BHW/P, C, P ]neibor, [BHW/P, C, P ]distant}. Followed by\ntwo parameter-shared MHSA, this powerful improvement is termed\nSEW-MHSA. The computation of Q and K remains in the non-\nextended dimension, following iRMB. This approach has two\nbenefits: 1) A single module can accommodate global information\nin one forward pass, which is advantageous for downstream\ntasks requiring high resolution. 2) The parallel operation does\nnot introduce additional parameters, reusing the parameters and\ncomputations of K, Q, and V, and only adds an extra attention\nmap computation, thereby enhancing model accuracy with minimal\ncomputational cost.\nNon-linearity for post-attention. We introduce a nonlinear acti-\nvation function in the V computation of the attention mechanism,\nfurther filtering features before multiplying them with the attention\nmap. This differs from the pre-attention described in Sec. 3.2.2,",
            "content": "referred to as post-attention, which improves model performance without increasing the number of parameters. Large kernel for local modeling. iRMB uses kernel size of 3 for the DW-Conv in local modeling. Smaller values limit the models receptive field. i2RMB further investigates the impact of large kernels on accuracy. Considering the depth-wise modeling approach, this does not significantly increase the number of model parameters. Additionally, this structure provides the model with positional information, allowing it to achieve downstream structures without additional position embedding design. Structural deduction. Combining lightweight Depth-Wise Convolution (DW-Conv) and efficient EW-MHSA to trade-off model cost and accuracy, the process of the designed iRMB can be formulated: () = DW-Conv(SEW-MHSA()). (5) Accessibility analysis. Due to the fact that i2RMB only includes convolution and multi-head self-attention operators, the constructed EMOv2 is built by stacking identical standard modules without employing hardware-aware search structures, and it uses serial structure without multiple branches. This design is highly compatible with hardware acceleration, potentially offering strong generalizability for different hardware platforms and applications."
        },
        {
            "title": "3.3.2 Macro Design of EMOv2 for Dense Prediction",
            "content": "Based on the above criteria, we design ResNet-like 4-phase Efficient MOdel (EMO) based on series of iRMBs for dense applications in our previous work [13]. In this extension work, we build stronger vision backbone EMOv2 by the powerful i2RMBs, as shown in Fig. 2-Right. 1) For the overall framework, EMOv2 consists of only i2RMB without diversified modules➁, which is departure from recent efficient methods [2], [17] in terms of designing idea. 2) For the specific module, i2RMB consists of only convolution and multi-head self-attention without other complex operators➀. Also, benefitted by DW-Conv, i2RMB can adapt to down-sampling operation through the stride and does not require any position embeddings for introducing inductive bias to MHSA➁. The comparison of the requirements for embedding across different methods is shown in Tab. A1. 3) For the configuration of different-scale models, we employ gradually increasing expansion rates and channel numbers, and detailed configurations are shown in Tab. 4. Results for basic classification and downstream dense prediction tasks in Sec. 4 demonstrate the superiority of our i2RMB over SoTA lightweight methods on magnitudes of 1M, 2M, and core-focused 5M➂. 4) i2RMB can be easily extended to other foundational architectures and accomplish corresponding tasks➃, such as temporal extension, UNet variant, and DiT-like model in Sec. 3.3.3. Configuration details. Since MHSA is better suited for modeling semantic features for deeper layers, we only turn it on at stage3/4 following previous works [2], [49], [75]. Note that this never violates the uniformity criterion, as the shutdown of MHSA was special case of i2RMB structure. To further increase the stability of EMO, BN [76]+SiLU [77] are bound to DW-Conv while LN [78]+GeLU [77] are bound to SEW-MHSA, and i2RMB is competent for down-sampling operations. Importance of instantiated efficient operator. Our defined efficient operator contains two core modules, i.e., (S)EW-MHSA and DW-Conv. In Tab. 5, we conduct an ablation experiment IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7 TABLE 4: Core configurations of EMOv2 variants. TABLE 6: Performance of our EMOv1/v2 with different lightweight model training recipes. Items Depth Emb. Dim. Exp. Ratio EMOv2-1M [ 2, 2, 8, 3 ] [ 32, 48, 80, 180 ] [ 2.0, 2.5, 3.0, 3.5 ] EMOv2-2M [ 3, 3, 9, 3 ] [ 32, 48, 120, 200 ] [ 2.0, 2.5, 3.0, 3.5 ] EMOv2-5M [ 3, 3, 9, 3 ] [ 48, 72, 160, 288 ] [ 2.0, 3.0, 4.0, 4.0 ] TABLE 5: Ablation study on components in iRMB/i2RMB. EMOv1 [13] EMOv2 EW-MHSA DW-Conv SEW-MHSA DW-Conv Top-1 73.5 76.6 +3.1 77.6 +4.1 78.4 +4.9 Top-1 73.5 77.7 +4.2 78.1 +4.6 79.4 +5.9 to study the effect of both modules in iRMB/i2RMB. The first row means that neither (S)EW-MHSA nor DW-Conv is used, i.e., the model is almost composed of MLP layers with several DWConv for down-sampling, and degenerates to Identity operation. Surprisingly, this model still produces respectable result, i.e., 73.5 Top-1. Comparatively, results of the second and third rows demonstrate that each component contributes to the performance, e.g., +3.1 and +4.1 when adding DW-Conv and EW-MHSA for EMO, respectively, while +4.2 and +4.6 for EMOv2. Our approach achieves the best result when both components are used. Besides, this experiment illustrates that the specific instantiation of iRMB/i2RMB is very important to model performance. Order of operators. Based on EMOv1-5M, we switch the order of DW-Conv/EW-MHSA and find slight -0.6, and similar -0.7 drop is also observed in EMOv2 when switching DW-Conv/SEWMHSA. Therefore, (S)EW-MHSA performs first by default. Performance gains over EMOv1. The improved EMOv2-5M achieves Top-1 accuracy of 79.4, surpassing EMOv1-5M by +1.0, without significantly increasing parameters and FLOPs. it demonstrates notable Additionally, improvements across various highresolution downstream tasks. For instance, in popular detection and segmentation tasks, as shown in Fig. 5, EMOv2 consistently achieves an enhancement of 13 points across different frameworks. Omni-Task Fig. 5: Downstream gains of EMOv2-5M over EMOv1-5M. 3.3.3 i2RMB-Centric Transformation Thanks to the general, neat, and powerful i2RMB design, we can easily extend it to various tasks in this extension work, as illustrated in Fig. 2: 1) video classification (V-EMO) extends the i2RMB to the temporal dimension, 2) UNet-based image segmentation (U-EMO) replaces the original convolutional blocks with i2RMB, and 3) diffusion-based image generation (D-EMO) replaces naive Transformer blocks with i2RMB. We construct various lightweight versions of different types of structures and conduct extensive experiments to demonstrate the effectiveness and generalizability of i2RMB in Sec. 4.2."
        },
        {
            "title": "4 EXPERIMENTAL RESULTS\n4.1 Image Classification",
            "content": "Setup. Different SoTA methods use various training recipes that could lead to potentially unfair comparisons, and we have Recipe EMOv1 [13] EMOv2 MNetv3 [10] NaN NaN DeiT [43] 78.1 78.8 EdgeNeXt [2] 78.3 79.1 Vim [64] 77.9 78. Ours 78.4 79.4 summarized and compared these training strategies in Tab. A1. In contrast, our training strategy is weaker, yet it achieves impressive results without employing strong training tricks. All experiments are conducted on the ImageNet-1K dataset [79] without using additional datasets or pre-trained models. Each model is trained for standard 300 epochs from scratch at resolution of 224224 by default. The AdamW [80] optimizer is employed with betas (0.9, 0.999), weight decay of 5e2, learning rate of 6e3, and batch size of 2,048. We use Cosine scheduler [81] with 20 warmup epochs, Label Smoothing 0.1 [82], stochastic depth [83], and RandAugment [84] during training. However, LayerScale [85], Dropout [86], MixUp [87], CutMix [88], Random Erasing [89], Position Embeddings [18], Token Labeling [90], and Multi-Scale training [17] are disabled. EMOv2 is implemented based on TIMM [91]. Results analysis. We evaluate our method against SoTA models on three small magnitudes, and the quantitative results are presented in Tab. 7. Notably, our method achieves the best results without utilizing complex modules and strong training recipes employed by recent works, such as NAS in MobileNetv4 [42] and reparameterization in RepViT [40]. For example, the smallest EMOv2-1M achieves SoTA Top-1 accuracy of 72.3, surpassing the CNN-based MobileNetv3-L-0.50 [10] by +3.5 with nearly half the parameters, and the Transformer-based MobileViTv20.5 [14] by +2.1 with only 61% of the FLOPs. The larger EMOv2-2M achieves SoTA Top-1 accuracy of 75.8 with only 487M FLOPs, nearly half of MobileVit-XS [17] but with +1.0 improvement. Comparatively, the latest EdgeViT-XXS [55] achieves lower Top-1 accuracy of 74.4 while requiring +78% more parameters and +14% more FLOPs, whereas tiny-MOAT0 [75] requires +48% more parameters and +64% more FLOPs to achieve similar result. Consistently, EMOv2-5M demonstrates superior trade-off between #Params. (5.1M), FLOPs (1.0G), and accuracy (79.4), proving to be more efficient than contemporary counterparts. For example, it achieves +0.9 over EATFormerTiny [24] with better efficiency. When we further employ the KD training strategy (TResNet [92] with 83.9 accuracy as the teacher model), our three-magnitude EMOv2 models achieve 73.5, 76.7, and 80.9 Top-1 accuracy, respectively. This represents an increase of +2.0, +1.6, and +2.5 compared to our previous conference method [13]. Moreover, these results significantly exceed the latest models using strong training strategies, such as RepViT [40], EfficientFormerV2 [1], GhostNetV3 [41], and MobileNetv4 [42]. Training recipes matters. We evaluate EMO [13] and EMOv2 with different mainstream training recipes presented in Tab. 6. We find that our simple training recipe is enough to get impressive results, while existing stronger recipes (especially used by EdgeNeXt [2]) will not improve performance further. NaN indicates that the model did not train well for the possibly unadapted hyper-parameters."
        },
        {
            "title": "4.2 Downstream Applications\nThanks to the structural design of spanning attention in i2RMB,\nour EMOv2 can simultaneously model global and local information\ninteractions, which significantly enhances the performance of",
            "content": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8 TABLE 7: Classification performance comparison among different kinds of backbones on ImageNet-1K dataset in terms of 5Mmagnitude, as well as 1M-magnitude and 2M models. White, grey, orange, and blue backgrounds indicate CNN-based, Transformerbased, RNN-based, and our EMO series, respectively. This kind of display continues for all subsequent experiments. Gray indicates the results obtained from the original paper. Comprehensive suggested models are marked in bold. Unit: #Params with (M) and FLOPs with (M). Abbreviations: MNet MobileNet; MViT MobileViT; MFormer MobileFormer. : Neural Architecture Search (NAS) for elaborate structures. : Using knowledge distillation. : Re-parameterization strategy. : Using stronger training strategy displayed in Tab. 17(e). t a 1 t a 2 t a Model MNetv1-0.50 [8] MNetv3-L-0.50 [10] MViTv1-XXS [17] MViTv2-0.5 [14] EdgeNeXt-XXS [2] EATFormer-Mobile [24] EMOv1-1M [13] EMOv2-1M EMOv2-1M MNetv2-1.40 [9] MNetv3-L-0.75 [10] FasterNet-T0 [93] GhostNetV3-0.5x [41], MNetv4-Conv-S [42] MoCoViT-1.0 [94] PVTv2-B0 [20] MViTv1-XS [17] MFormer-96M [33] EdgeNeXt-XS [2] EdgeViT-XXS [55] tiny-MOAT-0 [75] EfficientViT-M1 [95] EfficientFormerV2-S0 [1] EATFormer-Lite [24] EMOv1-2M [13] EMOv2-2M EMOv2-2M MNetv3-L-1.25 [10] EfficientNet-B0 [12] FasterNet-T2 [93] RepViT [40] RepViT [40], GhostNetV3-1.3x [41], MNetv4-Conv-M [42] DeiT-Ti [43] XCiT-T12 [57] LightViT-T [53] MViTv1-S [17] MViTv2-1.0 [14] EdgeNeXt-S [2] PoolFormer-S12 [52] MFormer-294M [33] MPViT-T [96] EdgeViT-XS [55] tiny-MOAT-1 [75] EfficientViT-M5 [95] EfficientFormerV2-S1 [1] ViG-T [58] SHViT-S3 [51] EATFormer-Tiny [24] Vim-Ti [64] EfficientVMamba-T [65] EfficientVMamba-S [65] VRWKV-T [60] MSVMamba-S [97] MambaOut-Femto [98] EMOv1-5M [13] EMOv2-5M EMOv2-5M EMOv2-5M #Params 1.3 2.6 1.3 1.4 1.3 1.8 1.3 1.4 1.4 6.9 4.0 3.9 2.7 3.8 5.3 3.7 2.3 4.6 2.3 4.1 3.4 3.0 3.5 3.5 2.3 2.3 2.3 7.5 5.3 15.0 6.8 6.8 8.9 9.2 5.7 6.7 9.4 5.6 4.9 5.6 11.9 11.4 5.8 6.7 5.1 12.4 6.1 6.0 14.2 6.1 7.0 6.0 11.0 6.2 7.0 7.0 5.1 5.1 5.1 5.1 FLOPs Reso. 2242 2242 2562 2562 2562 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2562 2242 2562 2562 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2562 2562 2242 2242 2242 2242 2562 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 5122 149 69 364 466 261 360 261 285 285 585 155 340 48 200 147 572 986 96 538 557 800 167 400 910 439 487 487 356 399 1910 1100 1100 269 1000 1258 1254 700 2009 1851 965 1823 294 1654 1136 1200 522 650 900 601 1410 1500 800 1300 1200 900 1200 903 1035 1035 5627 Top-1 63.7 68.8 69.0 70.2 71.2 69.4 71.5 72.3 73.5 74.7 73.3 71.9 69.4 73.8 74.5 70.5 74.8 72.8 75.0 74.4 75.5 68.4 75.7 75.4 75.1 75.8 76.7 76.6 77.1 78.9 78.6 80.0 79.1 79.9 72.2 77.1 78.7 78.4 78.1 78.8 77.2 77.9 78.2 77.5 78.3 77.1 79.0 77.2 77.4 78.4 76.1 76.5 78.7 75.1 77.3 78.9 78.4 79.4 80.9 82.9 Venue arXiv1704 ICCV19 ICLR22 arXiv22 ECCVW22 IJCV24 ICCV23 - - CVPR18 ICCV19 CVPR23 arXiv2404 arXiv2404 arXiv22 CVM22 ICLR22 CVPR22 ECCVW22 ECCV22 ICLR23 CVPR23 ICCV23 IJCV24 ICCV23 - - ICCV19 ICML19 CVPR23 CVPR24 CVPR24 arXiv2404 arXiv2404 ICML21 NeurIPS21 arXiv22 ICLR22 arXiv22 ECCVW22 CVPR22 CVPR22 CVPR22 ECCV22 ICLR23 CVPR23 ICCV23 arXiv2405 CVPR24 IJCV24 ICML24 arXiv2403 arXiv2403 arXiv2403 arXiv2405 arXiv2405 ICCV23 - - - TABLE 8: Object detection performance by SSDLite [10] on MSCOCO 2017 [99] dataset at 320320 resolution. Abbreviated MNet/MViT: MobileNet/MobileViT. : 512 512 resolution. Backbone MNetv1 [8] MNetv2 [9] MNetv3 [10] MViTv1-XXS [17] MViTv2-0.5 [14] EMOv1-1M [13] EMOv2-1M EMOv2-1M MViTv2-0.75 [14] EMOv1-2M [13] EMOv2-2M EMOv2-2M ResNet50 [44] MViTv1-S [17] MViTv2-1.25 [14] EdgeNeXt-S [2] EMOv1-5M [13] EMOv2-5M EMOv2-5M #Params 5.1 4.3 5.0 1.7 2.0 2.3 2.4 2.4 3.6 3.3 3.3 3.3 26.6 5.7 8.2 6.2 6.0 6.0 6.0 FLOPs 1.3G 0.8G 0.6G 0.9G 0.9G 0.6G 0.7G 2.3G 1.8G 0.9G 1.2G 4.0G 8.8G 3.4G 4.7G 2.1G 1.8G 2.4G 8.0G mAP 22.2 22.1 22.0 19.9 21.2 22.0 22.3 26.6 24.6 25.2 26.0 30.7 25.2 27.7 27.8 27.9 27.9 29.6 34.8 TABLE 9: Object detection results by RetinaNet [36] on MSCOCO 2017 [99] dataset. Backbone ResNet-50 [44] PVTv1-Tiny [19] PVTv2-B0 [20] EdgeViT-XXS [55] EMOv1-5M EMOv2-5M #Params mAP mAP 55.3 36.3 56.9 36.7 57.2 37.2 59.0 38.7 59.8 38.9 62.7 41.5 37.7 23.0 13.0 13.1 14.4 14.4 50 mAP 38.6 38.9 39.5 41.0 41.0 44.1 75 mAP 19.3 22.6 23.1 22.4 23.8 25.7 mAP 40.0 38.8 40.4 42.0 42.2 45. mAP 48.8 50.0 49.7 51.6 51.7 55.5 downstream tasks. It is noteworthy that current lightweight models have only reported limited results on downstream tasks, and different methods lack unified experimental standard. Therefore, we have endeavored to find overlapping results from the original papers for fair comparison. Additionally, we report the detailed results of our method with different magnitudes on multiple downstream tasks in the supplementary materials. Object detection. We evaluate our EMOv2 (pre-trained on ImageNet-1K) with other SoTA methods on MS-COCO 2017 [99] dataset, using the lightweight SSDLite [10] and heavy RetinaNet [36] / Mask RCNN [100]. Considering fairness and friendliness for the community, we employ standard MMDetection library [101] for experiments and replace the optimizer with AdamW [80] without tuning other parameters. Comparison results on SSDLite are shown in Tab. 8, and our EMOv1surpasses corresponding counterparts by apparent advantages and the improved EMOv2 further boosts the performance. For example, SSDLite equipped with EMOv1-1M achieves 22.0 mAP with only 0.6G FLOPs and 2.3M parameters, which boosts +2.1 compared with SoTA MobileViT [17] with only 66% FLOPs. Consistently, EMOv1-5M obtains the highest 27.9 mAP so far with much fewer FLOPs, e.g., 53% (1.8G) of MobileViT-S [17] (3.4G) and 0.3G less than EdgeNeXt-S (2.1G). EMOv2-5M further achieves 29.6 mAP with no significant increase in parameters, surpassing EMOv1-5M by +1.7. We also conduct experiments on heavy detection frameworks. Tab. 9 and Tab. 10 present the results of different lightweight backbones on the RetinaNet [36] and Mask RCNN [100] methods, respectively. Our EMOv2 consistently achieves superior results compared to its counterparts, e.g., +5.2 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 10: Object detection results by Mask RCNN [100] on MS-COCO 2017 [99] dataset. TABLE 11: Semantic segmentation results by DeepLabv3 [102], Semantic FPN [103], SegFormer [104], and PSPNet [105] on ADE20K [106] dataset at 512512 resolution. 9 Backbone #Params PVT-Tiny [19] PVTv2-B0 [20] 33.0 23.0 PoolFormer-S12 [52] 31.0 MPViT-T [96] 28. EATFormer-Tiny [24] 25.9 EMOv1-5M EMOv2-5M 24.8 24. mAP mAP mAP mAP 50 mAP 50 mAP 75 mAP 75 mAP mAP mAP mAP mAP 36.7 35.1 38.2 36.2 37.3 34.6 42.2 39.0 42.3 39.0 39.3 36.4 42.3 39. 59.2 56.7 60.5 57.8 59.0 55.8 64.2 61.4 64.7 61.5 61.7 58.4 64.3 61.4 39.3 37.3 40.7 38.6 40.1 36.9 45.8 41.8 46.2 42.0 42.4 38.7 46.3 42.1 - - - - - - - - 25.5 22.4 23.5 18.2 25.8 20.0 - - - - - - - - 45.5 42.0 42.3 39.0 45.6 41.8 - - - - - - - - 55.1 52.7 51.1 52.6 56.3 57.0 mAP over the CNN-based ResNet-50, +2.8 mAP over the Transformer-based EdgeViT-XXS, and +2.6 mAP over our previous EMOv1under the RetinaNet framework. For the Mask RCNN framework, our EMOv2-5M obtains highly competitive results compared to the recently designed EATFormer for heavy architectures, with improvements of +3.0 mAP and +2.6 mAP over the previous generation EMOv1-5M model. Semantic segmentation. ImageNet-1K pre-trained EMOv2 is integrated with DeepLabv3 [102], Semantic FPN [103], SegFormer [104], and PSPNet [105] to adequately evaluate its performance on challenging ADE20K [106] dataset at 512512 resolution. We employ the standard MMSegmentation library [107] with official configurations without tuning other parameters. Due to the fact that different methods only report results on certain segmentation frameworks, we strive to find sufficient comparable models of similar magnitude under each method. Detailed results are presented in Tab. 11. For lightweight models at the 1M/2M/5M magnitude, our method demonstrates significant advantages over comparative methods (including CNN, Transformer, and hybrid architectures), achieving balance between parameters, computational cost, and performance. Notably, our conference version model (i.e., EMO [13]) achieves highly competitive results, and the improved EMOv2 model further significantly enhances the metrics. For instance, under the Deeplabv3 framework, our EMOv2-1M/2M/5M achieved 34.6/36.8/39.8 mIoU, respectively, representing improvements of +1.1/+1.5/+2.0 over EMOv1with fewer parameters. Similarly, under the Semantic FPN framework, our EMOv2-1M/2M/5M achieves 37.1/39.9/42.3 mIoU, respectively, representing improvements of +2.9/+2.6/+1.9 over EMOv1without increasing the number of parameters. More detailed results can be found in the supplementary materials. Previous studies have demonstrated the effectiveness of EMOv2 in classification and mainstream downstream detection/segmentation tasks. To further validate the superiority of EMOv2, we additionally extend it to UNet-like architectures, as well as video classification and DiT-based image generation. UNet-based vision segmentation (U-EMO). Furthermore, we replace the basic convolutional block in UNet with the i2RMB block to construct more powerful U-EMO architecture, as described in Fig. 2, and we conduct experiments on the downstream segmentation task to demonstrate the generalizability of the proposed method across different architectures. Tab. 12 presents results of U-EMO, UNet [108], and the adapted EdgeNeXt [2] method on Backbone MViTv2-0.5 MViTv3-0.5 EMOv1-1M EMOv2-1M MNetv2 MViTv2-0.75 MViTv3-0.75 EMOv1-2M EMOv2-2M MViTv2-1.0 MViTv3-1.0 EMOv1-5M EMOv2-5M ResNet-18 EMOv1-1M EMOv2-1M ResNet-50 PVTv1-Tiny PVTv2-B0 EMOv1-2M EMOv2-2M ResNet-101 ResNeXt-101 PVTv1-Small EdgeViT-XXS EdgeViT-XS PVTv2-B1 EMOv1-5M EMOv2-5M MiT-B0 EMOv2-2M MiT-B1 EMOv2-5M MNetv2 MViTv2-0.5 EMOv1-1M EMOv2-1M MViTv2-0.75 EMOv1-2M EMOv2-2M MViTv2-1.0 EMOv1-5M EMOv2-5M #Params 6.3 6.3 5.6 5.6 18.7 9.6 9.7 6.9 6.6 13.4 13.6 10.3 9.9 15.5 5.2 5.3 28.5 17.0 7.6 6.2 6.2 47.5 47.1 28.2 7.9 10.6 17.8 8.9 8.9 3.8 2.6 13.7 5.3 13.7 3.6 4.3 4.2 6.2 5.5 5.2 9.4 8.5 8. FLOPs 26.1G - 2.4G 3.3G 75.4G 40.0G - 3.5G 5.0G 56.4G - 5.8G 9.1G 32.2G 22.5G 23.4G 45.6G 33.2G 25.0G 23.5G 25.1G 65.1G 64.7G 44.5G 24.4G 27.7G 34.2G 25.8G 29.1G 8.4G 10.3G 15.9G 14.4G 53.1G 15.4G 2.1G 2.9G 26.6G 3.1G 4.6G 40.3G 5.3G 8.6G mIoU 31.9 33.5 33.5 34.6 34.1 34.7 36.4 35.3 36.8 37.0 39.1 37.8 39.8 32.9 34.2 37.1 36.7 35.7 37.2 37.3 39.9 38.8 39.7 39.8 39.7 41.4 42.5 40.4 42.3 37.4 40.2 42.2 43.0 29.7 31.8 33.2 33.6 35.2 34.5 35.7 36.5 38.2 39.1 3 L D ] 2 0 1 [ c a S ] 3 0 1 [ r e ] 4 0 1 [ P ] 5 0 1 [ TABLE 12: Semantic segmentation results by UNet [108] on HRF [109] dataset at 256256 resolution. Backbone UNet-S5-D16 EdgeNeXt-S [2] U-EMOv2-5M #Params 29.0 23.7 21.3 FLOPs 204G 221G 228G mDice 88.9 89.1 89.5 aAcc 97.0 97.1 97.1 mAcc 86.2 87.5 88. the HRF [109] dataset at 256256 resolution. Our improved UEMO achieves higher performance with fewer parameters without meticulous adjustments to the architecture and training recipes. Video classification (V-EMO). By simply extending the temporal dimension of the convolution and spanning attention in the i2RMB block, we obtain basic i2RMB-3D block for video processing. This allows us to replace modules while maintaining structure similar to 2D EMOv2, resulting in the V-EMO model. We use ImageNet-1K pretrained weights with temporal repetition to initialize the video classification model. Tab. 13 presents comparison of our method with UniFormer-XXS [49] and the adapted EdgeNeXt [2] method on the Kinetics-400 [110] dataset. Our V-EMO-5M achieves Top-1 accuracy of 65.2 with only IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10 TABLE 13: Comparison with the state-of-the-art on Kinetics400 [110] dataset with four input frames. TABLE 15: Efficiency and performance comparison of different depth and channel configurations. Backbone UniFormer-XXS EdgeNeXt-S [2] V-EMOv2-5M #Params 9.8 6.8 5.9 FLOPs 1.0G 1.2G 1.3G Top-1 63.2 64.3 65.2 TABLE 14: Comparison with DiT [67] for 400K training steps in generating 256256 ImageNet [79] images. Depth [2, 2, 10, 3] [2, 2, 12, 2] [4, 4, 8, 3] [3, 3, 9, 3] [2, 2, 12, 3] [2, 2, 8, 2] Channels [48, 72, 160, 288] [48, 72, 160, 288] [48, 72, 160, 288] [48, 72, 160, 288] [48, 72, 160, 288] [48, 72, 224, 288] #Params 5.3M 5.0M 5.1M 5.1M 5.1M 5.1M Top-1 FLOPs 1038M 79.1 1127M 78.9 1132M 79.4 1035M 79.4 1136M 79.1 1117M 79.0 Model DiT-S-2 SiT-S-2 D-EMOv2-S-2 DiT-B-2 SiT-B-2 D-EMOv2-B-2 DiT-L-2 SiT-L-2 D-EMOv2-L-2 DiT-XL-2 SiT-XL-2 D-EMOv2-XL-2 #Params 33.0 33.0 24.6 130.5 130.5 96.1 458.1 458.1 334.8 675.1 675.1 492.7 FLOPs 5.5G 5.5G 5.4G 21.8G 21.8G 19.9G 77.5G 77.5G 69.3G 114.5G 114.5G 101.5G FID 68.4 57.6 46.3 43.5 33.5 24.8 23.3 18.8 11.2 19.5 17.2 9.6 TABLE 16: Comparisons of throughput on CPU/GPU and running speed on mobile iPhone15 (ms). Method EdgeNeXt-XXS EMOv1-1M EMOv2-1M EdgeNeXt-XS EMOv1-2M EMOv2-2M EdgeNeXt-S EMOv1-5M EMOv2-5M #Params 1.3M 1.3M 1.4M 2.3M 2.3M 2.3M 5.6M 5.1M 5.1M CPU FLOPs 261M 73.1 261M 158.4 285M 147.1 538M 69.1 439M 126.6 487M 118.2 965M 54.2 903M 106.5 1035M 93. GPU 2860.6 3414.6 3182.2 1855.2 2509.8 3312.4 1622.5 1731.7 1607.8 iPhone15 10.2 3.0 3.6 17.6 3.7 4.3 22.5 4.9 5.9 Top-1 71.2 71.5 72.3 75.0 75.1 75.8 78.8 78.4 79.4 5.9M parameters, outperforming UniFormer-XXS, which has 9.8M parameters, by +2.0. DiT-based image generation (D-EMO). The primary design goal of the i2RMB is to simplify the Transformer block structure, making it suitable for mobile architecture design by reducing the depth of individual blocks while improving the modeling of both distant and neighboring features. Thanks to its plug-and-play characteristic, i2RMB can easily replace the Transformer block in the DiT model for image generation tasks. Specifically, we fully adhere to the DiT [67] training framework, and the results on the 256256 ImageNet generation task are shown in Tab. 14. Compared to the baseline DiT [67] and the SiT [111] with improved training strategies, our D-EMO model, which replaces the basic Transformer block with i2RMB, requires fewer parameters and computational resources while achieving significantly better FID scores. This demonstrates the advantage of spanning attention in downstream image generation task."
        },
        {
            "title": "4.3 Structural Ablation and Analysis",
            "content": "This section uses EMOv2-5M as the research backbone to ablate the proposed method modules and training hyperparameters, while also analyzing the model structure and results. Depth and channel configurations. Using EMOv2-5M as the baseline, we evaluate the impact of different depth configurations on model performance, as shown in the upper part of Tab. 15. The selected depth configuration yields relatively better performance. Furthermore, we assess the performance of slimmer and wider models with similar number of parameters, as shown in the lower part of Tab. 15. These models, despite having an increased computational load, do not result in further performance improvements, demonstrating the rationality of the current structural configuration. Throughput comparison. Tab. 16 presents throughput evaluation results compared with the state-of-the-art EdgeNeXt [2], which effectively balances parameters, computational load, and performance. The test platforms are an AMD EPYC 7K62 CPU and V100 GPU, with resolution of 224224 and batch size of 256. Results indicate that EMOv1achieves faster speeds on both platforms with higher Top-1 accuracy. For instance, EMOv1-1M achieves speed boosts of +20% on the GPU and +116% on the CPU compared to EdgeNeXt-XXS with the same FLOPs. The improved EMOv2 maintains nearly the same parameter count as EMOv1but significantly enhances performance with slight increase in computational load. This performance gap is further widened on mobile devices (following the official classification project [112] on iPhone15), where our EMOv2 is 2.8 , 4.1 , and 3.9 faster than the state-of-the-art EdgeNeXt [2]. This improvement is attributed to our simple and device-friendly i2RMB block, which does not rely on other complex structures such as the Res2Net module [56], transposed channel attention [57], etc. Attention mode. The proposed i2RMB in Sec. 3.3.1 includes two components: distant and neighbor window attention with shared parameters. Tab. 17a evaluates the models performance under different attention modes. When neighborhood and distant attention are added separately, the model shows significant improvement compared to the baseline model. It also outperforms models of similar magnitude without attention, especially in downstream task metrics, demonstrating the effectiveness of the proposed basic EW-MHSA (Sec. 3.2.2). Thanks to the shared parameter design, the model with integrated spanning attention achieves better Top-1 classification results without any additional parameters. This is particularly evident in detection and segmentation tasks, further proving the effectiveness of the spanning mechanism in i2RMB. Used stages of spanning attention. Tab. 17b shows the changes in model accuracy when applying spanning attention to different stages based on EMOv2-5M. As spanning attention is gradually added from the fourth stage (S-4) to all four stages (S-1234), the models performance significantly increases (S-34) and then saturates and slightly decreases (S-234). Considering that more stages require additional parameters and computational resources, spanning attention is by default injected only in the last two stages. Interestingly, in the conference version of EMO [13], the accuracy of the model increases with the number of stages to which spanning IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11 TABLE 17: Ablation studies and comparison analysis on ImageNet [79]. All the experiments use EMOv2-5M as default structure. (a) Attention mode analysis on classification and downstream RetinaNet [36] / DeepLabv3 [102]. (b) Applied stages of spanning attention. (c) Influence of DPR and BS hyperparameters. Mode None None (Scaling to 5.1M) Neighborhood Attention Remote Attention Spanning Attention #Params FLOPs Top-1 mAP mIoU 37.2 802M 77.9 37.7 991M 78.4 39.0 967M 78.8 38.6 967M 79.0 39.8 1035M 79.4 4.3M 5.1M 5.1M 5.1M 5.1M 39.3 39.6 40.4 39.9 41.5 Stage S-4 S-34 S-234 S-1234 #Params FLOPs Top-1 832M 78.5 1035M 79.4 1096M 79.3 1213M 79.1 4.7M 5.1M 5.1M 5.2M DPR Top-1 BS Top-1 78.9 0.00 79.2 0.03 79.4 0.05 79.4 0.10 0.20 79.4 256 512 1024 2048 79.1 79.2 79.4 79.3 79.1 (d) Convolution type. K: kernel size. D: Dilation. (e) Training strategies: image resolution, knowledge distillation, and 1000 training epochs. Size K-1 K-3 K-5 K-7 K-9 K-5 + D-2 K-5 + D-3 K-5 + DCNv2 [113] #Params FLOPs Top-1 4.8M 4.9M 5.1M 5.3M 5.5M 5.1M 5.1M 6.7M 969M 991M 1035M 1102M 1184M 1035M 1035M 1625M 78.6 79.0 79.4 79.2 79.3 79.3 79.1 78.5 Resolution 224 256 224 224 512 512 512 KD Long Training #Params. 1.0G 1.4G 1.0G 1.0G 5.6G 5.6G 5.6G FLOPs 5.1M 5.1M 5.1M 5.1M 5.1M 5.1M 5.1M Top-1 79.4 79.9 80.8 80.4 81.5 82.4 82.9 attention is applied. This discrepancy may be due to the structure of i2RMB, where EMOv2-5M is closer to the performance upper limit for models with this parameter count. Effect of training hyper-parameters. Tab. 17c discusses the two most influential hyperparameters in model training. The proposed EMOv2-5M exhibits strong robustness to the drop path rate (DPR) hyperparameter within the range of [0, 0.2], where the Top-1 accuracy fluctuates within 0.3, achieving the best result at drop path rate of 0.05. Meanwhile, smaller batch size (BS) of 256 slightly affects the models performance, with the performance peaking at batch size of 1024 and then stabilizing. Considering memory efficiency, default batch size of 1024 is suggested. These ablation experiments demonstrate the robustness of EMOv2 to the above hyperparameter variations. Neighborhood kernel size in i2RMB. The size of the DW-Conv affects the local receptive field of i2RMB, which significantly impacts the models classification ability and perception capability in downstream tasks. As shown in Tab. 17d-Top, when the kernel size gradually increases from 1 to 5, the models performance improves from 78.6 to 79.4. However, further increases in kernel size do not yield noticeable gains and instead incur additional parameter and computational costs. Convolution type in i2RMB. Tab. 17d-Bottom illustrates the impact of different convolution variants on EMOv2, which extend the receptive field. The use of dilated convolutions does not further improve the models performance; in fact, when the dilation rate is set to 3, the models performance slightly decreases. Deformable convolution significantly increases the models parameter count and computational load. Therefore, we replace the DW-Conv in EMOv2-1M with DCNv2 [113] with group size of 1 to maintain similar scale of the model. The results indicate that this substitution actually reduces the models performance. Stronger training strategy. Tab. 17e presents three training strategies that enhance model performance without altering the model architecture or parameters. When employing higher resolutions (up to 512 in this paper), knowledge distillation (KD) with naive logit distribution (TResNet [92] in Sec. 4.1), and long training durations (up to 1000 epochs), the models performance improves significantly. When all strategies are combined, the EMOv2-5M TABLE 18: Core configurations of scaled EMOv2 variants. Items Depth Emb. Dim. Exp. Ratio EMOv2-20M [ 3, 3, 13, 3 ] [ 64, 128, 320, 448 ] [ 2.0, 3.0, 4.0, 4.0 ] EMOv2-50M [ 5, 8, 20, 7 ] [ 64, 128, 384, 512 ] [ 2.0, 3.0, 4.0, 4.0 ] TABLE 19: Evaluation of scaling capabilities of EMOv2 at 20M/50M magnitudes on ImageNet-1K dataset. Model ResNet-50 [44], [114] ConvNeXt-T [115] PVTv2-B2 [20] Swin-T [21] PoolFormer-S36 [52] ViTAEv2-S [116] EATFormer-Small [24] EMOv1-20M [13] EMOv2-20M ResNet-152 [44], [114] Swin-B [21] PoolFormer-M48 [52] ViTAEv2-48M [116] EATFormer-Base [24] EMOv2-50M #Params 25.5 28.5 25.3 28.2 30.8 19.3 24.3 20.5 20.1 60.1 87.7 73.4 48.6 49.0 49.8 FLOPs Reso. 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 2242 4.1G 4.5G 4.0G 4.5G 5.0G 5.7G 4.3G 3.8G 4.0G 11.5G 15.5G 11.6G 13.4G 8.9G 8.8G Top-1 80.4 82.1 82.0 81.3 81.4 82.6 83.1 82.0 83.3 82.0 83.5 82.5 83.8 83.9 84.1 Venue CVPR16 CVPR22 ICCV21 ICCV21 CVPR22 IJCV23 IJCV24 ICCV23 - CVPR16 ICCV21 CVPR22 IJCV23 IJCV24 - 0 u g 0 8 0 5 t a achieves the best 82.9 Top-1 accuracy. This performance notably surpasses that of Swin-Transformer-T (28.2M with 81.3 Top-1) and ResNet-152 (60.1M with 82.0 Top-1). Scale up assessment We scale up EMOv2 to 20M/50M magnitudes to evaluate its scaling capability. The specific structure is presented in Tab. 18, and the comparison results with current backbones of similar magnitudes are shown in Tab. 19. The results demonstrate that EMOv2 can be easily extended to large-scale models and achieve highly competitive results. This scaling capability is also reflected in Tab. 14, proving the structural effectiveness and generalization of i2RMB."
        },
        {
            "title": "4.4 Visual Analysis between EMOv1/v2",
            "content": "Quantitative downstream visualization. Fig. 6-Top presents the detection visualization results based on SSDLite. Compared to EMOv1, the improved EMOv2 demonstrates accurate classification IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12 Fig. 6: Qualitative comparisons between EMOv1/v2 on downstream SSDLite [10] and DeepLabv3 [102]. EMOv2 demonstrates higher accuracy in class and boundary detection. Zoom in for more details. Fig. 7: Visualizations by Grad-CAM. EMOv2 generates sharper and higher confidence attention maps than EMOv1. and localization capabilities, even generalizing to objects that are missed in the ground truth. Thanks to the spanning attention mechanism, EMOv2 also achieves significant performance improvements in pixel-level dense prediction, as shown in Fig. 6-Bottom. Class activation mapping comparison. Fig. 7 presents the visualization results of Grad-CAM. The improved EMOv2 generates high-confidence class activations that are more closely aligned with the image subjects."
        },
        {
            "title": "4.5 Summary",
            "content": "Starting from the EMOv1 baseline [13], we progressively explore factors influencing EMOv2 performance from the perspectives of structural design and training strategy. As shown in Fig. 8, the model parameters are controlled at 5.1M, and each structural improvement incrementally enhances the models performance without additional parameter increase: 1) larger kernel size improves the models performance at the cost of only 0.016M parameters. 2) Post attention increases the Top-1 accuracy by 0.5 Fig. 8: Overall incremental trajectory from baseline to modern EMOv2 at the 5M magnitude. Each line is based on modification of the immediately preceding line. Detailed ablations in Sec. 4.3. Parameters and FLOPs are marked in green and yellow. with an additional 0.1G FLOPs. 3) Spanning attention further enhances the model accuracy to 79.4, surpassing the baseline by +1.0. Additionally, this operation significantly improves the performance of EMOv2 on downstream tasks, as shown in Fig. 5. We use the structure at the end of the structural design phase as our default EMOv2-5M, while higher resolution, extended training, and naive knowledge distillation strategies are employed to investigate the performance upper limits of our EMOv2 in the 5M parameter magnitude. The detailed structure can be viewed in the attached source code. Limitation discussion. This study focuses on lightweight vision backbones and proposes EMOv2 model, extending them to the 20M and 50M parameter scales due to resource constraints. However, its Transformer-compatible architecture design potentially allows application to larger-scale vision backbones. Additionally, the spanning mechanism can be extended to the domain of large language models (LLMs), which warrants further exploration."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. Li, J. Hu, Y. Wen, G. Evangelidis, K. Salahi, Y. Wang, S. Tulyakov, and J. Ren, Rethinking vision transformers for mobilenet size and speed, in ICCV, 2023. 1, 2, 3, 5, 7, 8, 16, 17 [2] M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. W. Zamir, R. M. Anwer, and F. S. Khan, Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications, in ECCVW, 2022. 1, 3, 5, 6, 7, 8, 9, 10, 16, 17 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 13 [3] [4] [5] [6] [7] [8] H. Shu, W. Li, Y. Tang, Y. Zhang, Y. Chen, H. Li, Y. Wang, and X. Chen, Tinysam: Pushing the envelope for efficient segment anything model, arXiv preprint arXiv:2312.13789, 2023. 1 C. Zhou, X. Li, C. C. Loy, and B. Dai, Edgesam: Prompt-in-theloop distillation for on-device deployment of sam, arXiv preprint arXiv:2312.06660, 2023. 1 S. Xu, H. Yuan, Q. Shi, L. Qi, J. Wang, Y. Yang, Y. Li, K. Chen, Y. Tong, B. Ghanem et al., Rap-sam: Towards real-time all-purpose segment anything, arXiv preprint arXiv:2401.10228, 2024. 1 X. Li, A. You, Z. Zhu, H. Zhao, M. Yang, K. Yang, and Y. Tong, Semantic flow for fast and accurate scene parsing, in ECCV, 2020. 1, 2 P. Lu, T. Jiang, Y. Li, X. Li, K. Chen, and W. Yang, RTMO: Towards high-performance one-stage real-time multi-person pose estimation, 2023. 1 A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, Mobilenets: Efficient convolutional neural networks for mobile vision applications, arXiv preprint arXiv:1704.04861, 2017. 1, 2, 3, [9] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, Mobilenetv2: Inverted residuals and linear bottlenecks, in CVPR, 2018. 1, 2, 3, 4, 8 [10] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan et al., Searching for mobilenetv3, in ICCV, 2019. 1, 2, 7, 8, 12, 16, 17 [11] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu, and C. Xu, Ghostnet: More features from cheap operations, in CVPR, 2020. 1, 2 [12] M. Tan and Q. Le, Efficientnet: Rethinking model scaling for convolu- [13] tional neural networks, in ICML. PMLR, 2019. 1, 2, 3, 8 J. Zhang, X. Li, J. Li, L. Liu, Z. Xue, B. Zhang, Z. Jiang, T. Huang, Y. Wang, and C. Wang, Rethinking mobile block for efficient attentionbased models, in ICCV, 2023. 1, 2, 6, 7, 8, 9, 10, 11, 12 [14] S. Mehta and M. Rastegari, Separable self-attention for mobile vision [15] transformers, TMLR, 2023. 1, 3, 7, 8, 16, 17 J. Nielsen, The need for speed in ai, 2023, accessed: 2023-10-03. [Online]. Available: https://www.uxtigers.com/post/ai-response-time 1 [16] , Usability engineering. Morgan Kaufmann, 1994. 1 [17] S. Mehta and M. Rastegari, Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer, in ICLR, 2022. 1, 2, 3, 6, 7, 8, 16, 17 [18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2021. 1, 3, 4, 7, 16, [19] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, Pyramid vision transformer: versatile backbone for dense prediction without convolutions, in ICCV, 2021. 1, 3, 5, 8, 9 [20] , Pvt v2: Improved baselines with pyramid vision transformer, CVM, 2022. 1, 8, 9, 11 [21] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in ICCV, 2021. 1, 3, 5, 11 [23] [22] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao, Z. Zhang, L. Dong et al., Swin transformer v2: Scaling up capacity and resolution, in CVPR, 2022. 1 J. Zhang, C. Xu, J. Li, W. Chen, Y. Wang, Y. Tai, S. Chen, C. Wang, F. Huang, and Y. Liu, Analogous to evolutionary algorithm: Designing unified sequence model, NeurIPS, 2021. 1 J. Zhang, X. Li, Y. Wang, C. Wang, Y. Yang, Y. Liu, and D. Tao, Eatformer: improving vision transformer inspired by evolutionary algorithm, IJCV, 2024. 1, 7, 8, 9, [24] [25] X. Li, H. Ding, W. Zhang, H. Yuan, G. Cheng, P. Jiangmiao, K. Chen, Z. Liu, and C. C. Loy, Transformer-based visual segmentation: survey, TPAMI, 2024. 1 [26] D. Li, J. Hu, C. Wang, X. Li, Q. She, L. Zhu, T. Zhang, and Q. Chen, Involution: Inverting the inherence of convolution for visual recognition, in CVPR, 2021. 1 [27] N. Kitaev, L. Kaiser, and A. Levskaya, Reformer: The efficient transformer, in ICLR, 2020. 1 [28] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller, Rethinking attention with performers, in ICLR, 2021. [29] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, Cvt: Introducing convolutions to vision transformers, in ICCV, 2021. 1, 3 [30] J. Li, X. Xia, W. Li, H. Li, X. Wang, X. Xiao, R. Wang, M. Zheng, and X. Pan, Next-vit: Next generation vision transformer for efficient deployment in realistic industrial scenarios, arXiv preprint arXiv:2207.05501, 2022. 1 [31] S. Mehta, M. Ghazvininejad, S. Iyer, L. Zettlemoyer, and H. Hajishirzi, Delight: Deep and light-weight transformer, in ICLR, 2021. 1 [32] S. N. Wadekar and A. Chaurasia, Mobilevitv3: Mobile-friendly vision transformer with simple and effective fusion of local, global and input features, arXiv preprint arXiv:2209.15159, 2022. 1, 3 [33] Y. Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan, and Z. Liu, Mobile-former: Bridging mobilenet and transformer, in CVPR, 2022. 1, 8 [34] Y. Li, G. Yuan, Y. Wen, J. Hu, G. Evangelidis, S. Tulyakov, Y. Wang, and J. Ren, Efficientformer: Vision transformers at mobilenet speed, NeurIPS, 2022. [35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, in NeurIPS, 2017. 1, 3, 4 [36] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, Focal loss for dense object detection, in ICCV, 2017. 2, 8, 11, 16, 17 [37] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size, arXiv preprint arXiv:1602.07360, 2016. 2 [38] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, in CVPR, 2016. 2 [39] X. Li, J. Zhang, Y. Yang, G. Cheng, K. Yang, Y. Tong, and D. Tao, Sfnet: Faster, accurate, and domain agnostic semantic segmentation via semantic flow, IJCV, 2023. 2 [40] A. Wang, H. Chen, Z. Lin, H. Pu, and G. Ding, Repvit: Revisiting mobile cnn from vit perspective. arxiv 2023, arXiv preprint arXiv:2307.09283, 2023. 2, 3, 7, 8, 16, [41] Z. Liu, Z. Hao, K. Han, Y. Tang, and Y. Wang, Ghostnetv3: Exploring the training strategies for compact models, arXiv preprint arXiv:2404.11202, 2024. 2, 7, 8, 16, 17 [42] D. Qin, C. Leichner, M. Delakis, M. Fornoni, S. Luo, F. Yang, W. Wang, C. Banbury, C. Ye, B. Akin et al., Mobilenetv4-universal models for the mobile ecosystem, arXiv preprint arXiv:2404.10518, 2024. 2, 7, 8, 16, 17 [43] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou, Training data-efficient image transformers & distillation through attention, in ICML, 2021. 3, 5, 7, 8, 16, 17 [44] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in CVPR, 2016. 3, 8, 11 [45] M. Hassanin, S. Anwar, I. Radwan, F. S. Khan, and A. Mian, Visual attention methods in deep learning: An in-depth survey, arXiv preprint arXiv:2204.07756, 2022. [46] K. Islam, Recent advances in vision transformer: survey and outlook of recent work, arXiv preprint arXiv:2203.01536, 2022. 3 [47] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu, Incorporating convolution designs into visual transformers, in ICCV, 2021. 3 [48] X. Chu, Z. Tian, B. Zhang, X. Wang, and C. Shen, Conditional positional encodings for vision transformers, in ICLR, 2023. 3 [49] K. Li, Y. Wang, G. Peng, G. Song, Y. Liu, H. Li, and Y. Qiao, Uniformer: Unified transformer for efficient spatial-temporal representation learning, in ICLR, 2022. 3, 5, 6, 9 [50] S. Li, Z. Wang, Z. Liu, C. Tan, H. Lin, D. Wu, Z. Chen, J. Zheng, and S. Z. Li, Moganet: Multi-order gated aggregation network, in ICLR, 2024. 3, 5, 16, 17 [51] S. Yun and Y. Ro, Shvit: Single-head vision transformer with memory efficient macro design, in CVPR, 2024. 3, [52] W. Yu, M. Luo, P. Zhou, C. Si, Y. Zhou, X. Wang, J. Feng, and S. Yan, Metaformer is actually what you need for vision, in CVPR, 2022. 3, 4, 8, 9, 11 [53] T. Huang, L. Huang, S. You, F. Wang, C. Qian, and C. Xu, Lightvit: Towards light-weight convolution-free vision transformers, arXiv preprint arXiv:2207.05557, 2022. 3, 8 [54] Q. Zhang and Y.-B. Yang, Rest: An efficient transformer for visual [55] [56] recognition, in NeurIPS, 2021. 3 J. Pan, A. Bulat, F. Tan, X. Zhu, L. Dudziak, H. Li, G. Tzimiropoulos, and B. Martinez, Edgevits: Competing light-weight cnns on mobile devices with vision transformers, in ECCV, 2022. 3, 7, 8 S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and P. Torr, Res2net: new multi-scale backbone architecture, IEEE TPAMI, 2019. 3, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14 [57] A. Ali, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin, I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek et al., Xcit: Crosscovariance image transformers, in NeurIPS, vol. 34, 2021. 3, 8, 10 [86] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, Dropout: simple way to prevent neural networks from overfitting, JMLR, 2014. 7 [58] B. Liao, X. Wang, L. Zhu, Q. Zhang, and C. Huang, Vig: Linearcomplexity visual sequence learning with gated linear attention, arXiv preprint arXiv:2405.18425, 2024. 3, 8 [59] Q. He, J. Zhang, J. Peng, H. He, X. Li, Y. Wang, and C. Wang, Pointrwkv: Efficient rwkv-like model for hierarchical point cloud learning, arXiv preprint arXiv:2405.15214, 2024. [60] Y. Duan, W. Wang, Z. Chen, X. Zhu, L. Lu, T. Lu, Y. Qiao, H. Li, J. Dai, and W. Wang, Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures, arXiv preprint arXiv:2403.02308, 2024. 3, 8 [61] H. Yuan, X. Li, L. Qi, T. Zhang, M.-H. Yang, S. Yan, and C. C. Loy, Mamba or rwkv: Exploring high-quality and high-efficiency segment anything model, arXiv preprint arXiv:2406.19369, 2024. 3 [62] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, arXiv preprint arXiv:2312.00752, 2023. 3 [63] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, M. Grella et al., Rwkv: Reinventing rnns for the transformer era, arXiv preprint arXiv:2305.13048, 2023. 3 [64] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, Vision mamba: Efficient visual representation learning with bidirectional state space model, arXiv preprint arXiv:2401.09417, 2024. 3, 7, 8, 16, 17 [65] X. Pei, T. Huang, and C. Xu, Efficientvmamba: Atrous selective scan for light weight visual mamba, arXiv preprint arXiv:2403.09977, 2024. 3, 8 [66] H. He, J. Zhang, Y. Cai, H. Chen, X. Hu, Z. Gan, Y. Wang, C. Wang, Y. Wu, and L. Xie, Mobilemamba: Lightweight multi-receptive visual mamba network, arXiv preprint arXiv:2411.15941, 2024. [67] W. Peebles and S. Xie, Scalable diffusion models with transformers, [68] in ICCV, 2023. 4, 10 J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao, Focal attention for long-range interactions in vision transformers, in NeurIPS, 2021. 3 [69] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo, Cswin transformer: general vision transformer backbone with cross-shaped windows, in CVPR, 2022. 3 [70] C. Si, W. Yu, P. Zhou, Y. Zhou, X. Wang, and S. YAN, Inception transformer, in NeurIPS, 2022. [71] H. Liu, Z. Dai, D. So, and Q. V. Le, Pay attention to mlps, NeurIPS, [72] 2021. 3 I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al., Mlp-mixer: An all-mlp architecture for vision, NeurIPS, 2021. 3 [73] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave, G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek et al., Resmlp: Feedforward networks for image classification with data-efficient training, T-PAMI, 2022. 3 [74] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, Shufflenet v2: Practical guidelines for efficient cnn architecture design, in ECCV, 2018. 4, 6 [75] C. Yang, S. Qiao, Q. Yu, X. Yuan, Y. Zhu, A. Yuille, H. Adam, and L.-C. Chen, Moat: Alternating mobile convolution and attention brings strong vision models, ICLR, 2023. 5, 6, 7, [76] S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, in ICML. PMLR, 2015. 6 [77] D. Hendrycks and K. Gimpel, Gaussian error linear units (gelus), arXiv [78] [79] [80] preprint arXiv:1606.08415, 2016. 6 J. L. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization, arXiv preprint arXiv:1607.06450, 2016. 6 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in CVPR, 2009. 7, 10, 11, 16 I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in ICLR, 2019. 7, [81] , SGDR: Stochastic gradient descent with warm restarts, in ICLR, 2017. 7 [82] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, in CVPR, 2016. 7 [83] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, Deep networks with stochastic depth, in ECCV, 2016. 7 [84] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, Randaugment: Practical automated data augmentation with reduced search space, in CVPRW, 2020. 7 [85] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jégou, Going deeper with image transformers, in ICCV, 2021. 7 [87] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, mixup: Beyond empirical risk minimization, in ICLR, 2018. 7 [88] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, Cutmix: Regularization strategy to train strong classifiers with localizable features, in ICCV, 2019. 7 [89] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, Random erasing data augmentation, in AAAI, 2020. [90] Z.-H. Jiang, Q. Hou, L. Yuan, D. Zhou, Y. Shi, X. Jin, A. Wang, and J. Feng, All tokens matter: Token labeling for training better vision transformers, in NeurIPS, vol. 34, 2021. 7 [91] R. Wightman, Pytorch image models, https://github.com/rwightman/ pytorch-image-models, 2019. 7 [92] T. Ridnik, H. Lawen, A. Noy, E. Ben Baruch, G. Sharir, and I. Friedman, Tresnet: High performance gpu-dedicated architecture, in CACV, 2021. 7, 11 J. Chen, S.-h. Kao, H. He, W. Zhuo, S. Wen, C.-H. Lee, and S.-H. G. Chan, Run, dont walk: chasing higher flops for faster neural networks, in CVPR, 2023. 8 [93] [94] H. Ma, X. Xia, X. Wang, X. Xiao, J. Li, and M. Zheng, Mocovit: Mobile convolutional vision transformer, arXiv preprint arXiv:2205.12635, 2022. [95] X. Liu, H. Peng, N. Zheng, Y. Yang, H. Hu, and Y. Yuan, Efficientvit: Memory efficient vision transformer with cascaded group attention, in CVPR, 2023. 8 [96] Y. Lee, J. Kim, J. Willette, and S. J. Hwang, Mpvit: Multi-path vision transformer for dense prediction, in CVPR, 2022. 8, 9 [97] Y. Shi, M. Dong, and C. Xu, Multi-scale vmamba: Hierarchy in hierarchy visual state space model, arXiv preprint arXiv:2405.14174, 2024. 8 [98] W. Yu and X. Wang, Mambaout: Do we really need mamba for vision? arXiv preprint arXiv:2405.07992, 2024. [99] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, Microsoft coco: Common objects in context, in ECCV, 2014. 8, 9, 16, 17 [100] K. He, G. Gkioxari, P. Dollár, and R. Girshick, Mask r-cnn, in ICCV, 2017. 8, 9, 16, 17 [101] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Xu, Z. Zhang, D. Cheng, C. Zhu, T. Cheng, Q. Zhao, B. Li, X. Lu, R. Zhu, Y. Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. C. Loy, and D. Lin, MMDetection: Open mmlab detection toolbox and benchmark, arXiv preprint arXiv:1906.07155, 2019. 8 [102] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, Rethinking atrous convolution for semantic image segmentation, arXiv preprint arXiv:1706.05587, 2017. 9, 11, 12, 16, 17 [103] A. Kirillov, R. Girshick, K. He, and P. Dollár, Panoptic feature pyramid networks, in CVPR, 2019. 9, 16, 17 [104] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, Segformer: Simple and efficient design for semantic segmentation with transformers, in NeurIPS, 2021. 9, 16, 17 [105] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, Pyramid scene parsing network, in CVPR, 2017. 9, 16, 17 [106] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba, Semantic understanding of scenes through the ade20k dataset, IJCV, 2019. 9, 16, 17 [107] M. Contributors, MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark, https://github.com/open-mmlab/ mmsegmentation, 2020. [108] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in MICCAI, 2015. 9 [109] A. Budai, R. Bock, A. Maier, J. Hornegger, and G. Michelson, Robust vessel segmentation in fundus images, IJBI, 2013. 9 [110] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev et al., The kinetics human action video dataset, arXiv preprint arXiv:1705.06950, 2017. 9, 10 [111] N. Ma, M. Goldstein, M. S. Albergo, N. M. Boffi, E. Vanden-Eijnden, and S. Xie, Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers, arXiv preprint arXiv:2401.08740, 2024. [112] A. Inc., Optimize your core ml usage, https://developer.apple.com/ documentation/vision/classifying_images_with_vision_and_core_ml, 2022. 10 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 15 [113] X. Zhu, H. Hu, S. Lin, and J. Dai, Deformable convnets v2: More deformable, better results, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 93089316. 11 [114] R. Wightman, H. Touvron, and H. Jégou, Resnet strikes back: An improved training procedure in timm, in NeurIPSW, 2021. 11 [115] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, convnet for the 2020s, in CVPR, 2022. 11 [116] Q. Zhang, Y. Xu, J. Zhang, and D. Tao, Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond, IJCV, 2023. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 16 APPENDIX OVERVIEW The supplementary material presents more comprehensive results of our EMOv2 to facilitate the comparison of subsequent methods: Appendix provides detailed training recipes of various lightweight models trained on ImageNet-1K [79] dataset. Appendix provides more detailed object detection results using different frameworks on MS-COCO 2017 [99] dataset. Appendix provides more detailed semantic segmentation results using Mask R-CNN [100] for multiple magnitudes of EMOv2 on ADE20K [106] dataset. .1 Detailed Training Recipes Different SoTA lightweight methods [1], [2], [10], [14], [17], [18], [40], [41], [42], [43], [50], [64] use various training recipes that could lead to potentially unfair comparisons, and we have summarized and compared these training strategies in Tab. A1. Our training strategy is weaker, yet it achieves impressive results without employing strong training tricks. .2 Detailed Object Detection Results Tab. A2 shows more detailed object detection results using SSDLite [10] and RetinaNet [36] of our EMOv2 on MS-COCO 2017 [99] dataset, while Tab. A3 provide detailed object detection results using Mask R-CNN [100]. .3 Detailed Semantic Segmentation Results Tab. A4 shows more detailed semantic segmentation results using DeepLabv3 [102], Semantic FPN [103], SegFormer [104], and PSPNet [105] of our EMOv2 on ADE20K [106] dataset, while Tab. A5 provide detailed semantic segmentation results by adapting UNet with i2RMB. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 17 TABLE A1: Comparison of training recipes among popular and contemporary methods and we employ the same setting in all experiments. Please zoom in for clearer comparisons. Abbreviations: MNet MobileNet; MViT MobileViT; EFormerv2 EfficientFormerv2; GNet GhostNet; NAS: Neural Architecture Search; KD: Knowledge Distillation; #Repre.: Re-parameterization strategy. Super-Params. Epochs Batch size Optimizer Learning rate Learning rate decay Warmup epochs Label smoothing Drop out rate Drop path rate RandAugment Mixup alpha Cutmix alpha Erasing probability Position embedding Multi-scale sampler NAS KD #Repre. MNetv3 [10] ICCV19 300 512 RMSprop 6.4e2 1e5 3 0.1 9/0.5/1 0.2 ViT [18] ICLR21 300 4096 DeiT [43] ICML21 300 1024 AdamW AdamW 3e3 3e1 3.4 1e3 5e2 5 0.1 9/0.5/1 0.8 1.0 0.25 MViTv1 [17] ICLR22 300 1024 AdamW 2e3 1e2 2.4 0.1 MViTv2 [14] arXiv22 300 1024 AdamW 2e3 5e2 16 0.1 9/0.5/1 0.8 1.0 0.25 EdgeNeXt [2] arXiv22 300 4096 AdamW 6e3 5e2 20 0.1 9/0.5/1 EFormerv2 [1] ICCV23 300 1024 AdamW 1e3 2.52 5 0.1 9/0.5/1 0.8 1.0 0.25 RepViT [40] CVPR24 300 2048 AdamW 4e3 2.52 5 0.1 9/0.5/1 0.8 1.0 0.25 MogaNet [50] ICLR24 300 1024 AdamW 1e3 42 5 0.1 7/0.5/1 0.1 1.0 0.25 Vim [64] ICLR24 300 1024 AdamW 1e3 11 5 0.1 9/0.5/1 0.8 1.0 0.25 GNetv3 [41] arXiv2404 600 2048 LAMB 5e3 52 3 0.1 9/0.5/1 MNetv4 [42] arXiv2404 500 4096 AdamW 4e3 11 5 0.1 15/0.7/2 - EMOv1/v2 Ours 300 2048 AdamW 6e3 5e2 20 0.1 0.1 9/0.5/1 TABLE A2: Detailed object detection performance using SSDLite [10] and RetinaNet [36] of our EMOv2 on MS-COCO 2017 [99] dataset. : 512 512 resolution. TABLE A4: Detailed semantic segmentation performance using DeepLabv3 [102], Semantic FPN [103], SegFormer [104], and PSPNet [105] to adequately evaluate our EMOv2 on ADE20K [106] dataset. 75 mAP Backbone EMOv2-1M EMOv2-1M EMOv2-2M EMOv2-2M EMOv2-5M EMOv2-5M EMOv2-20M EMOv2-20M L S ] 0 1 [ #Params FLOPs mAP mAP 37.5 44.4 43.0 49.8 47.6 54.7 51.9 58.4 0.7G 2.3G 1.2G 4.0G 2.4G 8.0G 9.1G 30.3G 22.3 26.6 26.0 30.7 29.6 34.8 33.1 38.3 2.4 2.4 3.3 3.3 6.0 6.0 21.2 21.2 50 mAP 22.4 27.5 26.5 31.7 30.1 36.4 33.9 40. mAP 21.3 31.4 26.6 37.1 32.2 42.0 36.8 45.2 mAP 43.4 43.0 50.2 47.3 54.8 52.0 57.3 54.6 2.0 7.3 3.6 9.9 5.5 13.7 8.9 17.9 EMOv2-1M EMOv2-2M EMOv2-5M EMOv2-20M a e ] 6 3 [ 10.5 11.5 14.4 29.8 142G 146G 158G 220G 36.9 39.3 41.5 43.8 57.1 60.0 62.7 65.0 39.0 41.4 44.1 47.1 22.1 23.9 25.7 28. 39.8 43.1 45.5 47.4 49.5 51.6 55.5 59.0 TABLE A3: Detailed object detection performance using Mask RCNN [100] of our EMOv2 on MS-COCO 2017 [99] dataset. Backbone 3 EMOv2-1M EMOv2-2M EMOv2-5M EMOv2-20M EMOv2-1M EMOv2-2M EMOv2-5M EMOv2-20M EMOv2-1M EMOv2-2M EMOv2-5M EMOv2-20M EMOv2-1M EMOv2-2M EMOv2-5M EMOv2-20M #Params 5.6 6.6 9.9 26.0 5.3 6.2 8.9 23.9 1.4 2.6 5.3 20.4 4.2 5.2 8.1 23.6 FLOPs mIoU 34.6 36.8 39.8 43.3 37.1 39.9 42.3 46.8 37.0 40.2 43.0 47.3 33.6 35.7 39.1 43.4 3.3G 5.0G 9.1G 31.6G 23.4G 25.1G 29.1G 51.5G 5.0G 10.3G 14.4G 36.8G 2.9G 4.6G 8.6G 30.9G aAcc mAcc 45.5 75.9 48.6 77.1 51.5 78.3 56.0 79.6 47.6 78.2 51.1 79.3 53.4 80.8 58.3 82.2 47.5 77.7 51.1 79.0 53.9 80.5 58.7 82.1 44.8 75.8 47.0 76.7 51.0 78.2 55.7 79.6 Backbone #Params FLOPs EMOv2-1M 21.2 165G EMOv2-2M 22.1 170G EMOv2-5M 24.8 181G EMOv2-20M 39.8 244G 50 mAP 50 mAP 75 mAP 75 mAP mAP mAP mAP mAP 49.5 51.8 39.9 37.2 mAP mAP mAP mAP 37.1 35. 59.2 56.4 39.5 36.9 42.3 39.0 44.2 40.6 41.8 61.8 58.9 64.3 61. 66.2 63.6 64.9 39.6 37.0 42.4 39.4 46.3 42.1 48.7 43.4 45.0 21.8 16. 22.9 17.7 25.8 20.0 27.4 21.7 21.1 43.0 39.4 45.6 41.8 47.6 43.4 45. 52.6 53.8 56.3 57.0 58.7 59.1 60.5 TABLE A5: Detailed semantic segmentation performance by adapting UNet with i2RMB on ADE20K [106] dataset. Backbone UNet-S5-D16 EMOv2-5M #Params 29.0 21. FLOPs mIoU 88.9 89.5 204G 228G aAcc mAcc 86.2 97.0 88.3 97."
        }
    ],
    "affiliations": []
}