{
    "paper_title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models",
    "authors": [
        "Hieu Man",
        "Nghia Trung Ngo",
        "Viet Dac Lai",
        "Ryan A. Rossi",
        "Franck Dernoncourt",
        "Thien Huu Nguyen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 4 7 8 0 0 . 1 0 5 2 : r LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models Hieu Man1, Nghia Trung Ngo1, Viet Dac Lai2, Ryan A. Rossi2, Franck Dernoncourt2, Thien Huu Nguyen1 1Dept. of Computer Science, University of Oregon, OR, USA 2Adobe Research, USA {hieum,nghian,thienn}@uoregon.edu {viet.lai,ryrossi,franck.dernoncourt}@adobe.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vectorbased retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFERs architecture combines multilingual encoder, serving as languageuniversal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through minimal set of trainable parameters that act as connector, effectively transferring the multilingual encoders language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and lowresource languages, without requiring explicit multilingual training data."
        },
        {
            "title": "Introduction",
            "content": "Text embeddings, which provide dense vector representations of textual content (Mikolov et al., 2013; Devlin et al., 2019), have become fundamental building blocks in modern natural language processing. These embeddings encode semantic information and serve as an important component for numerous downstream applications, ranging from information retrieval and document reranking to classification, clustering, and semantic textual similarity assessment. Recently, the significance 1 of high-quality embeddings has been further amplified by their crucial role in retrieval-augmented generation (RAG) systems (Lewis et al., 2020b). RAG architectures enable large language models (LLMs) to dynamically access and integrate external or proprietary knowledge without the need for model parameter updates, substantially enhancing their adaptability and accuracy (Wang et al., 2023; Liu et al., 2024b; Gao et al., 2024). The evolution of embedding models has witnessed remarkable advancements, progressing from static word embeddings (Robertson et al., 2009) through contextualized representations (Reimers and Gurevych, 2019; Gao et al., 2021b; Ni et al., 2021a) to state-of-the-art LLM-based embedding models (Wang et al., 2024b) that harness the sophisticated semantic understanding capabilities of large language models. These developments have substantially enhanced performance across various embedding tasks (Luo et al., 2024), achieving unprecedented accuracy in semantic similarity and retrieval applications. However, critical limitation remains: the predominant focus on English in LLM-based embedding models has created significant disparity in multilingual capabilities. This gap is especially pronounced in medium and lowresource languages, where English-centric models exhibit substantial performance degradation due to insufficient language-specific training data (Wang et al., 2020; Thakur et al., 2024). While recent advances in multilingual embedding models, particularly those leveraging multilingual pre-trained architectures, have demonstrated promising results in multilingual embedding tasks (Li et al., 2023; Wang et al., 2024c; Chen et al., 2024), their reliance on explicit multilingual supervision for embeddings constrains their applicability primarily to languages with abundant training resources, leaving the challenge of true language-agnostic representation largely unaddressed. To address this challenge, we present LUSIFER, novel zero-shot approach that adapts English LLM-based embedding models for multilingual tasks without requiring explicit multilingual supervision. Drawing inspiration from recent advances in multimodal integration (Liu et al., 2024a; Lu et al., 2024), LUSIFER employs unique architecture that bridges the gap between multilingual understanding and specialized embedding capabilities. At its core, LUSIFER leverages the robust multilingual representations from XLM-R (Conneau et al., 2020) and introduces learnable connector mechanism to interface with English-optimized LLM embedding models. This approach enables LUSIFER to effectively transfer the multilingual understanding of XLM-R to the target LLM while inheriting advanced embedding capabilities of the LLM. In this way, LUCIFER can achieve effective multilingual representation capabilities without requiring explicit multilingual training data. We conduct comprehensive evaluations of LUSIFER through extensive experiments across 123 diverse datasets spanning 14 languages, focusing on five fundamental embedding tasks: Classification, Clustering, Reranking, Retrieval, and Semantic Textual Similarity (STS). Our experimental results demonstrate that LUSIFER substantially enhances the performance of English-centric LLMbased embedding models, achieving average improvements of 3.19 points across all tasks, with particularly significant gains observed for medium and low-resource languages (up to 22.15 improvement). To validate LUSIFERs broader applicability and cross-lingual capabilities, we extend our evaluation to cross-lingual tasks using four comprehensive datasets that encompass over 100 languages, including several critically low-resource languages. LUSIFER significantly outperforms existing English-centric embedding models by 5.75 on average in cross-lingual scenarios. These results demonstrate the effectiveness of our approach in enhancing multilingual representation capabilities without explicit multilingual supervision. The theoretical foundation for LUSIFERs effectiveness lies in its ability to create languageagnostic universal space through the integration of multilingual encoder (Pires et al., 2019; Libovický et al., 2020). We hypothesize that this universal space serves as bridge between different languages, enabling the target language model to process semantic information independently of the input language. By mapping these languageneutral representations to the target models input space, we conjecture that the target LLM can grasp the semantics of these representations, thereby improving the quality of output embeddings across multiple languages. This mechanism allows the model to become less dependent on the specific language of the input, enabling it to better capture semantic information for embedding tasks in languages it rarely encountered during pretraining. Our empirical analysis using t-SNE visualization supports this hypothesis."
        },
        {
            "title": "2.1 English-centric Embedding Models",
            "content": "Text embedding models have experienced significant advancement in recent years, driven by the evolution of pre-trained language models. Early successes with BERT-based architectures, as demonstrated in Sentence-BERT (Reimers and Gurevych, 2019), SimCSE (Gao et al., 2021b), and DPR (Karpukhin et al., 2020), established the foundation for modern embedding approaches. The field has since progressed to leverage LLMs, with recent works (Wang et al., 2024b; Muennighoff et al., 2024; BehnamGhader et al., 2024; Lee et al., 2024; Man et al., 2024) demonstrating substantial improvements in embedding quality and task performance through the enhanced representational capacity of LLMs (Luo et al., 2024). However, these advances primarily benefit high-resource language applications, as most state-of-the-art LLMbased embedding models are derived from Englishcentric foundation models (Jiang et al., 2023; Touvron et al., 2023) and trained predominantly on English or high-resource language datasets (Wang et al., 2024b). This bias has resulted in significant performance gap between high-resource and low-resource languages, limiting the global applicability of these models. Our proposed method, LUSIFER, addresses this limitation by enabling effective multilingual representation without multilingual training data."
        },
        {
            "title": "2.2 Zero-shot Multilingual Embedding",
            "content": "Multilingual Embedding has evolved through several distinct methodological approaches, each addressing the fundamental challenge of bridging language gaps in embedding tasks. Early successful approaches relied on translation models to enable multilingual understanding (Liu et al., 2020; Shi et al., 2021; Zhang and Misra, 2022). While effective, these methods introduced operational com2 plexity by requiring external translation systems, limiting their practical deployment and scalability. The emergence of multilingual pre-trained language models, particularly XLM-R (Conneau et al., 2020), opened new possibilities for multilingual transfer. Recent works have demonstrated promising results by fine-tuning such models with contrastive learning objectives on multilingual data (Wang et al., 2024c; Chen et al., 2024; Sturua et al., 2024). However, these approaches face two key limitations: they require substantial multilingual training data, and moreover, they do not exploit the sophisticated semantic representations afforded by contemporary English-centric LLM architectures, which have demonstrated superior performance in capturing nuanced semantic relationships. Recent advances in aligning multilingual and English-centric representations could offer solution. By combining independently pre-trained representations, paradigm that has shown remarkable success in multimodal alignment research (Alayrac et al., 2022; Liu et al., 2024a; Lu et al., 2024), these works bridge the gap between visual encoders and language models to enhance visual comprehension. As such, similar principles can be applied to align multilingual representations with LLM-based semantic spaces. While related efforts have explored aligning multiple LLMs for improved reasoning capabilities in multilingual settings (Bansal et al., 2024; Yoon et al., 2024), these approaches primarily target generation tasks and typically require large-scale alignment data. Our work extends these efforts by focusing on embedding tasks and leveraging minimal set of parameters to align multilingual and English-centric representations, enabling enhanced multilingual representation capabilities without requirement for large-scale multilingual training data."
        },
        {
            "title": "2.3 Multilingual Embedding Benchmarks",
            "content": "The evaluation landscape for multilingual embedding models has historically been fragmented across various benchmarks, each with significant limitations. While existing benchmarks have made they often exhibit convaluable contributions, strained scope: MINERS (Winata et al., 2024) provides evaluation across multiple languages but is limited to classification and STS tasks with only 11 datasets; XNLI (Conneau et al., 2018), XQuAD (Artetxe et al., 2020), and SIB-200 (Adelani et al., 2024) offer broad language coverage but focus exclusively on classification tasks; and MTEB (Muennighoff et al., 2023), despite its diverse task selection, primarily addresses high-resource languages. To address these limitations, we introduce comprehensive evaluation framework that encompasses 5 fundamental embedding tasksClassification, Clustering, Reranking, Retrieval, and STSacross an extensive collection of 123 datasets spanning 14 languages. This holistic approach enables systematic evaluation across both task and language dimensions, providing unprecedented insights into models multilingual capabilities. Furthermore, our benchmark extends beyond traditional multilingual evaluation by incorporating cross-lingual tasks, featuring coverage of over 100 languages, including critically low-resource languages that have been historically underrepresented in existing benchmarks. This extensive coverage allows for more nuanced understanding of embedding models performance across the global linguistic landscape."
        },
        {
            "title": "3 Methodology",
            "content": "Previous works demonstrate that representations of multilingual encoder models exhibit inherent language-agnostic properties, facilitating zero-shot multilingual transfer (Pires et al., 2019; Libovický et al., 2020). Building upon this foundation, we propose LUSIFER, an embedding framework that aligns multilingual encoder model with target English-centric LLMs representational space, enabling the target to encode semantics across multiple languages without extensive multilingual training. This section details our architectural design and two-stage training process for LUSIFER."
        },
        {
            "title": "3.1 Model Architecture",
            "content": "The core development of LUSIFER lies in its novel approach to enabling multilingual encoding of target LLMs through efficient representation mapping. As illustrated in Figure 1, LUSIFERs architecture consists of three key components: (1) multilingual encoder that functions as languageuniversal learner, capturing semantic information for diverse languages, (2) language-agnostic connector that serves as minimal parametric bridge between representations, and (3) target LLM optimized for embedding-specific tasks. The multilingual encoder processes input from various languages into shared semantic space, while the connector, designed with minimal trainable parameters, aligns these universal representations 3 Figure 1: Overview of LUSIFER. Left: Align multilingual encoder with the target English-centric LLM only using English data and minimal set of trainable parameter. Center: End-to-end representation finetune through contrastive learning on English text-embedding tasks using LoRA. Right: During inference, LUSIFER successfully processes text-embedding tasks across multiple languages. with the target LLMs native representational space. This alignment enables the target LLM embedding model to effectively leverage multilingual understanding without requiring extensive multilingual training data or architectural modifications. Following successful approaches in multimodal alignment (Alayrac et al., 2022; Liu et al., 2024a; Lu et al., 2024), we implement the connector as 2-layers feed-forward network, FF, augmented with single trainable token appended to the multilingual encoders hidden states. Formally, given input tokens Xinput (with necessary padding), the multilingual encoders hidden states Henc are transformed to align with the target LLMs representational space. The resulting aligned hidden states Halign maintain dimensionality compatibility with the target LLMs hidden states while extending the sequence length by one (Xinput + 1): Halign = [FF(Henc); t], where FF is the feed-forward network to align the multilingual encoders hidden states with dimension de to the target LLMs hidden states with dimension dt, and Rdt is the trainable token. Moreover, we employ masking mechanism to mask any original padding tokens in Henc to prevent their influence on the target LLMs processing, ensuring the model focuses on meaningful tokens."
        },
        {
            "title": "3.2 Training Pipeline",
            "content": "LUSIFER employs two-stage training process to achieve optimal multilingual representation capabilities. Both stages only require training on English data, leveraging the multilingual encoders inherent language-agnostic properties and embedding advantages of LLMs to facilitate zero-shot multilingual transfer. Stage 1: Alignment Training. The initial training stage aligns the multilingual encoders representations with the target LLMs embedding space. Specifically, we optimize the connector parameters θc and the multilingual encoder parameters θe while keeping the target LLMs parameters fixed, ensuring stable convergence. The training employs two complementary objectives: (1) masked reconstruction task where we randomly mask k% of input tokens such that Xinput = mask(X, k), training the model to recover the original sequence Xlm = X. (2) An autoregressive completion task that focuses on next-token prediction, where the model learns to generate the target sequence Xlm conditioned on the input context Xinput. The training objective for both tasks is formulated as language modeling objective to generate the target sequence Xlm given the input sequence Xinput. This objective enables local token-level alignment through masked reconstruction task where the model learns to predict the masked tokens by leveraging the context. In addition, it exploits global semantic alignment through autoregressive completion task that encourages the model to capture semantic information of the input sequences to generate the target sequence. As such, our training strategy learns to align the multilingual encoders representations with the target LLMs embedding space while preserving important semantic information of multilingual input sequences. Our training process is conducted using the standard crossentropy loss function. Stage 2: Representation Finetuning. The second stage improves text representations through contrastive learning process, effectively teaching the model to distinguish between positive and 4 Figure 2: Overview of tasks and datasets in our benchmark. Crosslingual datasets are marked with blue shade. negative examples. Our approach leverages both in-batch negatives sampled from the current training batch and hard-negative examples specifically curated to enhance model training. Additionally, we incorporate bidirectional attention mechanisms within the target LLM, following recent advances in LLMs representation learning (Muennighoff et al., 2024; BehnamGhader et al., 2024; Lee et al., 2024; Man et al., 2024). This bidirectional context modeling significantly enhances the quality of learned representations by enabling the model to capture both forward and backward dependencies in the input sequence. During this stage, we finetune all components of LUSIFER, including the target LLM, the multilingual encoder, and the connector parameters, to optimize the models representation quality for embedding-specific tasks. The goal of this stage is to improve the quality of text representations by leveraging the advanced embedding capabilities of the target LLM while maintaining the multilingual understanding provided by the multilingual encoder."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we first introduce the benchmark datasets and evaluation metrics in Section 4.1. Then, we describe the experimental setup, including the model implementation, training data, and training details in Section 4.2. Afterward, we present the main results in Section 4.3, and analyze the effectiveness of LUSIFERs components in Section 4.6. Finally, we visualize the LUSIFERs representations in multilingual space to obtain insights into its lingual-agnostic capabilities in Section 4.7."
        },
        {
            "title": "4.1 Benchmark",
            "content": "Figure 2 illustrates the tasks and datasets in our benchmark. Following (Muennighoff et al., 2023), our benchmark includes five fundamental embedding tasks, with the evaluation protocol for each task adapted from the respective original papers. The benchmark involves 123 diverse datasets, including 48 Classification datasets, 24 Clustering datasets, 24 Retrieval datasets, 22 Semantic Textual Similarity STS datasets, and 5 Reranking datasets. The main metrics for each task are as follows: Classification: Accuracy, Clustering: Vmeasure (Rosenberg and Hirschberg, 2007), Retrieval: nDCG@10, STS: Pearson correlation based on cosine similarity (Reimers et al., 2016), and Reranking: MAP. Following (Lai et al., 2023), our benchmark covers 14 languages including 5 high-resource languages: English (en), Spanish (es), Russian (ru), French (fr), Vietnamese (vi); 6 medium-resource languages: Persian (fa), Indonesian (id), Arabic (ar), Finnish (fi), Korean (ko), Hindi (hi); 3 low-resource languages: Bengali (bn), Telugu (te), Swahili (sw). Additionally, we evaluate models on crosslingual retrieval tasks where the models need to perform text embedding tasks with queries and documents in different languages. These tasks feature 5 datasets, including Belebele (Bandarkar et al., 2024), MLQA (Lewis et al., 2020a), STS17, STS22 (Agirre et al., 2016), and IndicCrosslingualSTS (Ramesh et al., 2022), covering over 100 languages, including critically low-resource languages. 5 Baselines En Es Ru Fr Vi Fa Id Ar Fi Ko Hi Bn Te Sw Avg. Jina-embeddings-v3* (Sturua et al., 2024) mGTE-base* (Zhang et al., 2024) BGE-M3* (Chen et al., 2024) Multilingual-E5-large* (Wang et al., 2024d) UDEVER-Bloom-7B* (Zhang et al., 2023) SimCSE (Gao et al., 2021b) Contriever (Izacard et al., 2022) GTE-large (Li et al., 2023) BGE-en-1.5 (Xiao et al., 2023) E5-large (Wang et al., 2024a) ST5-XXL (Ni et al., 2021c) GTR-XXL (Ni et al., 2021b) E5-Mistral (Wang et al., 2024b) 59.84 60.40 60.09 61.91 55.83 51.92 49.29 62.29 63.27 60.12 58.81 58.12 66.64 61.23 59.65 60.60 61.97 56. 51.81 44.26 51.66 51.65 52.41 60.35 54.39 61.84 62.88 61.02 62.37 62.91 59.73 24.90 26.55 33.49 32.79 26.81 44.42 41.94 61.30 58.94 56.20 57.34 59.40 54.38 46.95 44.05 50.13 50.84 51.00 58.50 53.21 59.65 66.74 65.81 70.69 71.30 64. 31.18 33.03 38.88 38.50 37.99 41.81 37.96 58.58 78.35 73.46 78.97 78.08 68.70 37.12 39.66 44.67 49.73 39.47 24.66 24.67 72.55 58.51 56.55 58.78 55.21 48.97 39.27 38.33 43.07 43.28 43.86 53.43 50.08 58.25 64.71 61.97 64.12 63.41 55. 29.46 32.36 30.27 30.81 31.32 25.30 25.14 54.43 73.57 68.96 75.60 76.53 67.60 41.64 45.76 51.98 51.16 53.59 52.46 53.88 66.97 64.96 61.22 64.72 66.55 58.54 26.23 26.47 27.02 31.11 28.84 15.43 15.23 62.82 64.19 60.81 64.61 63.75 55. 25.17 23.27 20.38 25.28 24.57 18.07 17.35 56.23 61.54 58.24 65.31 63.67 55.13 21.54 22.61 22.97 26.34 23.48 17.10 15.92 55.10 68.96 63.58 69.85 67.32 61.00 26.71 22.64 22.75 23.02 22.03 21.63 22.12 47.15 49.20 52.57 54.20 51.55 47. 38.36 39.26 41.40 41.96 43.25 38.81 40.57 50.61 63.83 61.46 64.80 64.54 57.78 35.16 34.82 38.64 39.98 38.48 37.91 36.47 59.44 LUSIFER (Ours) 57.20 60. 59.82 59.24 67.69 76.17 59.70 55. 72.83 65.23 62.37 58.43 69.30 53. 62.63 Table 1: Comparative analysis of model performance across multiple languages and tasks. The table presents average metrics for each model, with the highest score for each language emphasized in bold. * denotes the models trained on extensive multilingual data."
        },
        {
            "title": "MLQARetrieval BelebeleRetrieval",
            "content": "STS17 STS"
        },
        {
            "title": "IndicCrosslingual",
            "content": "Avg. SimCSE (Gao et al., 2021b) Contriever (Izacard et al., 2022) GTE-large (Li et al., 2023) BGE-en-1.5 (Xiao et al., 2023) E5-large (Wang et al., 2024a) ST5-XXL (Ni et al., 2021c) GTR-XXL (Ni et al., 2021b) E5-Mistral (Wang et al., 2024b) LUSIFER (Ours) 7.41 9.75 16.99 16.64 17.04 20.82 20.19 31.54 36.68 18.35 22.94 31.82 31.19 31.12 41.68 38.02 54. 57.81 39.71 34.55 37.57 40.40 37.90 56.19 50.83 81.12 81.09 37.95 41.72 53.79 50.77 54.31 59.02 60.11 71.37 70.49 0.18 0.03 1.59 1.11 1.83 1.76 2.74 21. 43.40 20.72 21.80 28.35 28.02 28.44 35.89 34.38 52.14 57.89 Table 2: Cross-lingual evaluation results. The table presents average metrics for each model over all languages of the datasets, with the highest score for each language emphasized in bold."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "Implementation Details. LUSIFER encompasses three key components: multilingual encoder, connector, and target LLM. We employ XLM-Rlarge (Conneau et al., 2020) as the multilingual encoder, Mistral-7B (Jiang et al., 2023) as the Englishcentric target LLM, and 2-layer feed-forward network with one trainable token as the connector. To facilitate efficient training, we leverage the LoRA framework (Hu et al., 2022) for training of LUSIFERs components. Furthermore, we employ GradCache (Gao et al., 2021a), gradient checkpointing, mixed precision training, and FSDP (Zhao et al., 2023) to minimize GPU memory requirements. The LUSIFER architecture and its training code are built on top of the Hugging Face Transformers (Wolf et al., 2020) and Pytorch Lightning libraries (Falcon and team, 2024). We detail the training hyper-parameters for each stage in Table 4 of Appendix A. Training Data. We only train LUSIFER on diverse public English datasets. For alignment training, we use the combination of the English Wikipedia and questions-answering datasets. Specifically, we use subset of Wikitext-103 (Merity et al., 2017) and MSMARCO (Bajaj et al., 2018) for the masked reconstruction and autoregressive completion tasks, respectively. For representation finetuning, we adopt the retrieval datasets as follows: MS MARCO (Bajaj et al., 2018), NQ (Kwiatkowski et al., 2019), PAQ (Lewis et al., 2021), HotpotQA (Yang et al., 2018), SNLI (Bowman et al., 2015), SQuAD (Rajpurkar et al., 2016), ArguAna (Wachsmuth et al., 2018), FiQA (Maia et al., 2018) and FEVER (Thorne et al., 2018). To address the lack of hard negatives in these datasets, we leverage an encoder-based model (Wang et al., 2024a) to select the hard negatives on those datasets. Refer to Table 5 for the number of samples used in each dataset. Baselines. We evaluate LUSIFERs performance across the five fundamental embedding tasks on the benchmark datasets. We make comparisons with variety of baseline models for embedding tasks which only trained/finetuned on mainly English data. Baselines include the following categories: dense retrieval models with Small Language Model (SLM) backbone: SimCSE (Gao et al., 2021b), Contriever (Izacard et al., 2022), GTE-large (Li et al., 2023), BGE-en-1.5 (Xiao et al., 2023), E5-large (Wang et al., 2024a); and dense retrieval models with Large Language Model 6 (a) Classification tasks (b) Clustering tasks Figure 3: Performance comparison of LUSIFER and baseline models on Classification and Clustering tasks. (LLM) backbone: GTR-XXL (Ni et al., 2021b), ST5-XXL (Ni et al., 2021c), E5-Mistral (Wang et al., 2024b). Moreover, we include the following state-of-the-art multilingual embedding models which are trained on extensive multilingual data for reference: Jina-embeddings-v3 (Sturua et al., 2024), mGTE-base (Zhang et al., 2024), BGE-M3 (Chen et al., 2024), Multilingual-E5-large (Wang et al., 2024d), and UDEVER-Bloom-7B (Zhang et al., 2023)."
        },
        {
            "title": "4.3 Main Results",
            "content": "Table 1 presents the main results of LUSIFER and baseline models on the benchmark datasets. LUSIFER achieves state-of-the-art performance in 10 out of 14 languages, with an average score of 62.63 across all languages, 3.19 points improvement over the previous best-performing baseline, E5-Mistral (59.44) (Wang et al., 2024b). Note that E5-Mistral is essentially the Mistral model fine-tuned on extensive proprietary synthetic data and supplemented with some multilingual data for training. Our results demonstrate that LUSIFER significantly enhances the multilingual capabilities of English-centric embedding LLM by aligning it with multilingual encoder, enabling effective multilingual representation without requiring explicit multilingual training data. The improvements are particularly pronounced for medium and lowresource languages, with Telugu (te) showing the largest gain of 22.15 points over E5-Mistral. This highlights LUSIFERs effectiveness in improving representation capabilities for traditionally underrepresented languages. Additionally, LUSIFER significantly outperforms the embedding models with SLM backbones, such as E5-large (38.48) and BGE-en-1.5 (39.98) which are trained on English data only, thus further demonstrating the benefits of combining multilingual encoder and LLMs English-centric for text-embedding tasks in multilingual settings. Furthermore, even without explicit multilingual supervision, LUSIFER achieves competitive performance (62.63) compared to stateof-the-art multilingual models that require extensive multilingual training data, such as BGE-M3 (64.80) (Chen et al., 2024) and Multilingual-E5large (64.54) (Wang et al., 2024d). These results further demonstrate the benefits of LUCIFER for multilingual representation learning while avoiding expensive multilingual data for text embeddings."
        },
        {
            "title": "4.4 Cross-Lingual Evaluation",
            "content": "Table 2 presents the results of LUSIFER and baseline models on the cross-lingual tasks. LUSIFER achieves the highest average score of 57.89, outperforming the previous best-performing baseline, E5Mistral (52.14), by 5.75 points. Notably, LUSIFER demonstrates significant improvements in lowresource languages, as evidenced by its performance on the IndicCrosslingual dataset, where it achieves score of 43.40, substantially higher than the next best baseline, E5-Mistral (21.92). These results underscore LUSIFERs effectiveness in enhancing cross-lingual capabilities through efficient multilingual representation alignment, enabling the model to process text-embedding tasks across mul7 Baselines LUSIFER (Full) En Es Ru Fr Vi Fa Id Ar Fi Ko Hi Bn Te Sw Avg. 57.20 60.14 59.82 59.24 67. 76.17 59.70 55.60 72.83 65.23 62. 58.43 69.30 53.12 62.63 LUSIFER (Connector Only) LUSIFER (Frozen Multilingual Encoder) LUSIFER (Alignment Only) LUSIFER (Representation Finetuning Only) 35.53 50.99 43.32 49. 33.98 58.77 38.94 58.76 42.95 58.30 45.12 58.08 33.54 52.73 36.75 51.01 35.68 62.24 41.96 62.11 57.86 75.88 64.60 74.01 35.55 58.11 38.38 57. 27.60 41.66 33.07 40.95 48.72 70.75 52.78 68.47 34.45 59.53 38.08 57.81 47.57 62.48 53.06 59.74 41.85 55.53 47.84 53.53 46.50 66.24 48.34 63. 34.66 49.12 40.03 47.03 44.18 58.74 44.45 57.28 Table 3: Ablation study results of LUSIFERs components. The table presents average metrics for each model, with the highest score for each language emphasized in bold. the performance of LUSIFER with the following ablated versions: (1) LUSIFER with only finetuning connector in both alignment training and representation finetuning stages, (2) LUSIFER with frezzing the multilingual encoder while training the connector and the target LLM in both stages, (3) LUSIFER with only alignment training, i.e., alignment training without representation finetuning, (4) LUSIFER with only representation finetuning without alignment training. Table 3 presents the results of the ablation study. The full LUSIFER model achieves the highest average score of 62.63 across all languages, outperforming the ablated versions. Notably, the alignment training and representation finetuning stages both contribute to the models performance, with the representation finetuning stage showing more substantial impact on the models performance. These results underscore the importance of each component in LUSIFERs architecture and training process, highlighting the models effectiveness in enhancing multilingual representation capabilities."
        },
        {
            "title": "4.7 Model Representation Visualization",
            "content": "Figure 6 shows 2D scatter plots of representations from different models for 200 randomly sampled examples from the SIB200 dataset, visualized using t-SNE. The points are colored by the language of the samples. The t-SNE representation of E5-Mistral demonstrates clearer separation between languages, with distinct clusters for each language. In contrast, the visualization of LUSIFER presents more mixed distribution of languages, with overlapping clusters across different languages. This observation provides insights into LUSIFERs lingual-agnostic capabilities, highlighting the models ability to bridge the gaps between representation spaces of different languages. These results suggest that LUSIFERs alignment strategy enables the model to comprehend semantics across multiple languages effectively, facilitating zero-shot multilingual transfer. Overall, our experiments confirm the advantages of the representation alignment strategies in LUCIFER to efFigure 4: Performance comparison of LUSIFER and baseline models on Reranking tasks. tiple languages effectively."
        },
        {
            "title": "4.5 Task-Specific Performance",
            "content": "Figure 3, 4, 5 present the performance comparison of LUSIFER and baseline models on Classification, Clustering, Reranking, Retrieval, and STS tasks. LUSIFER consistently outperforms the baseline models across 4 out of 5 tasks, with the largest improvements observed in Clustering and Retrieval tasks, especially in the medium and lowresource languages. However, the performance of LUSIFER in the Reranking tasks is slightly worse than the baseline models. This discrepancy may be attributed to the tasks complexity and the information loss in the alignment process between the multilingual encoder and the target LLM. Nevertheless, LUSIFERs strong performance across variety of tasks and languages highlights its ability to enhance multilingual representations without relying on explicit multilingual training data."
        },
        {
            "title": "4.6 Ablation Study",
            "content": "To evaluate the effectiveness of LUSIFERs components and training procerdure, we conduct an ablation study to analyze the impact of each component on the models performance. We compare 8 (a) Retrieval tasks (b) STS tasks Figure 5: Performance comparison of LUSIFER and baseline models on Retrieval and STS tasks. multilingual training data. In future work, we plan to explore additional alignment strategies and further investigate the impact of LUSIFERs components on multilingual representation quality. (a) E5-Mistral (language) (b) LUSIFER (language)"
        },
        {
            "title": "References",
            "content": "Figure 6: t-SNE representation of 200 randomly samples from the SIB200 dataset. The points are colored by the languages. fectively enable zero-shot multilingual transfer for LLM-based embedding methods."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose LUSIFER, novel framework that enables effective multilingual representation without explicit multilingual training data. LUSIFER aligns multilingual encoder with target English-centric LLM through minimal set of trainable parameters, facilitating zero-shot multilingual transfer. Our experimental results demonstrate that LUSIFER achieves state-of-the-art performance across diverse languages and tasks, outperforming existing baseline models. Moreover, LUSIFER significantly enhances cross-lingual capabilities, enabling the model to process textembedding tasks across multiple languages effectively. Our work provides promising direction for enhancing multilingual representation capabilities in English-centric embedding models, enabling global applicability without requiring extensive David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O. Alabi, Yanke Mao, Haonan Gao, and Annie En-Shiun Lee. 2024. Sib-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. Preprint, arXiv:2309.07445. Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 497511, San Diego, California. Association for Computational Linguistics. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems, volume 35, pages 23716 23736. Curran Associates, Inc. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 46234637, Online. Association for Computational Linguistics. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. Ms marco: human generated machine reading comprehension dataset. Preprint, arXiv:1611.09268. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2024. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 749775. Association for Computational Linguistics. Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, and Partha Talukdar. 2024. LLM augmented LLMs: Expanding capabilities through composition. In The Twelfth International Conference on Learning Representations. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. Llm2vec: Large language models are secretly powerful text encoders. Preprint, arXiv:2404.05961. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632642, Lisbon, Portugal. Association for Computational Linguistics. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Preprint, arXiv:2402.03216. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 24752485, Brussels, Belgium. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. William Falcon and The PyTorch Lightning team. 2024. Pytorch lightning. Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. 2021a. Scaling deep contrastive learning batch size under memory limited setup. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 316321, Online. Association for Computational Linguistics. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 68946910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: survey. Preprint, arXiv:2312.10997. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Preprint, arXiv:2112.09118. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics. 10 Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Viet Dac Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. ChatGPT beyond English: Towards comprehensive evaluation of large language models in multilingual learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1317113189, Singapore. Association for Computational Linguistics. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. Preprint, arXiv:2405.17428. Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020a. MLQA: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315 7330, Online. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledgeIn Advances in Neural Inforintensive nlp tasks. mation Processing Systems, volume 33, pages 9459 9474. Curran Associates, Inc. Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. PAQ: 65 million probably-asked questions and what you can do with them. Transactions of the Association for Computational Linguistics, 9:10981115. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. Preprint, arXiv:2308.03281. Jindˇrich Libovický, Rudolf Rosa, and Alexander Fraser. 2020. On the language neutrality of pre-trained multilingual representations. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 16631674, Online. Association for Computational Linguistics. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024a. Visual instruction tuning. Advances in neural information processing systems, 36. Jiapeng Liu, Xiao Zhang, Dan Goldwasser, and Xiao Wang. 2020. Cross-lingual document retrieval with smooth learning. In Proceedings of the 28th International Conference on Computational Linguistics, pages 36163629, Barcelona, Spain (Online). International Committee on Computational Linguistics. Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. 2024b. Chatqa: Surpassing gpt-4 on conversational qa and rag. arXiv preprint arXiv:2401.10225. Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis: Structural embedding alignment for multimodal large language model. Preprint, arXiv:2405.20797. Kun Luo, Minghao Qin, Zheng Liu, Shitao Xiao, Jun Zhao, and Kang Liu. 2024. Large language models as foundations for next-gen dense retrieval: comprehensive empirical assessment. Preprint, arXiv:2408.12194. Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018. Www18 open challenge: Financial opinion mining and question answering. In Companion Proceedings of the The Web Conference 2018, WWW 18, page 19411942, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee. Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, and Thien Huu Nguyen. 2024. Ullme: unified framework for large language model embeddings with generation-augmented learning. Preprint, arXiv:2408.03402. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In International Conference on Learning Representations. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. Preprint, arXiv:1310.4546. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative representational instruction tuning. Preprint, arXiv:2402.09906. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, Dubrovnik, Croatia. Association for Computational Linguistics. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. 2021a. Large dual encoders are generalizable retrievers. arXiv preprint arXiv:2112.07899. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2021b. Large dual encoders are generalizable retrievers. Preprint, arXiv:2112.07899. Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang. 2021c. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. Preprint, arXiv:2108.08877. Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 49965001, Florence, Italy. Association for Computational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Srihari Nagaraj, Deepak Kumar, Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Shantadevi Khapra. 2022. Samanantar: The largest publicly available parallel corpora collection for 11 indic languages. Trans. Assoc. Comput. Linguistics, 10:145162. Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016. Task-oriented intrinsic evaluation of semantic textual similarity. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 8796, Osaka, Japan. The COLING 2016 Organizing Committee. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410 420, Prague, Czech Republic. Association for Computational Linguistics. Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2021. Cross-lingual training of dense retrievers for document retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 251 253, Punta Cana, Dominican Republic. Association for Computational Linguistics. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, and Han Xiao. 2024. jina-embeddingsv3: Multilingual embeddings with task lora. Preprint, arXiv:2409.10173. Nandan Thakur, Jianmo Ni, Gustavo Hernández Ábrego, John Wieting, Jimmy Lin, and Daniel Cer. 2024. Leveraging llms for synthesizing training data across many languages in multilingual dense retrieval. Preprint, arXiv:2311.05800."
        },
        {
            "title": "James",
            "content": "Andreas Vlachos, Christos Thorne, Christodoulopoulos, and Arpit Mittal. 2018. FEVER: large-scale dataset for fact extraction In Proceedings of the 2018 and VERification. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809819, New Orleans, Louisiana. Association for Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971. Henning Wachsmuth, Shahbaz Syed, and Benno Stein. 2018. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 241251, Melbourne, Australia. Association for Computational Linguistics. Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan CatanInstructretro: Instruction tuning post zaro. 2023. arXiv preprint retrieval-augmented pretraining. arXiv:2310.07713. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2024a. Text embeddings by weakly-supervised contrastive pre-training. Preprint, arXiv:2212.03533. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024b. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1189711916, Bangkok, Thailand. Association for Computational Linguistics. 12 Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024c. Multilingual e5 text embeddings: technical report. Preprint, arXiv:2402.05672. Xin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. 2023. Language models are universal embedders. arXiv preprint arXiv:2310.08232. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. 2024. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval. Preprint, arXiv:2407.19669. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. 2023. Pytorch fsdp: Experiences on scaling fully sharded data parallel. Preprint, arXiv:2304.11277. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024d. Multilingual e5 text embeddings: technical report. arXiv preprint arXiv:2402.05672. Zihan Wang, Karthikeyan K, Stephen Mayhew, and Dan Roth. 2020. Extending multilingual BERT to lowresource languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 26492656, Online. Association for Computational Linguistics. Genta Indra Winata, Ruochen Zhang, and David Ifeoluwa Adelani. 2024. Miners: Multilingual language models as semantic retrievers. Preprint, arXiv:2406.07424. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium. Association for Computational Linguistics. Dongkeun Yoon, Joel Jang, Sungdong Kim, Seungone Kim, Sheikh Shafayat, and Minjoon Seo. 2024. LangBridge: Multilingual reasoning without multilingual In Proceedings of the 62nd Annual supervision. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75027522, Bangkok, Thailand. Association for Computational Linguistics. Bryan Zhang and Amita Misra. 2022. Machine translation impact in E-commerce multilingual search. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 99109, Abu Dhabi, UAE. Association for Computational Linguistics."
        },
        {
            "title": "A Training Details",
            "content": "The code and dataset for training are available at https://github.com/hieum98/lusifer. Hyperparameter Alignment Training Representation Finetuning Batch size Learning rate Learning rate scheduler Learning rate warm-up ratio Weight decay Grad norm clipping Epochs Optimizer Float precision LoRA rank LoRA alpha Random mask ratio Number of hardnegatives 256 1.5e-4 cosine 0.1 0.01 1.0 2 AdamW bf16-mixed 16 32 0.5 - 256 5e-5 cosine 0.1 0.01 1.0 1 AdamW bf16-mixed 16 32 - Table 4: Training hyperparameters for each stage. Stage Dataset Number of Samples Alignment Training Wikitext-103 (Merity et al., 2017) MSMARCO (Bajaj et al., 2018) Representation Finetuning MS MARCO (Bajaj et al., 2018) FEVER (Thorne et al., 2018) PAQ (Lewis et al., 2021) SNLI (Bowman et al., 2015) HotpotQA (Yang et al., 2018) SQuAD (Rajpurkar et al., 2016) FiQA (Maia et al., 2018) NQ (Kwiatkowski et al., 2019) ArguAna (Wachsmuth et al., 2018) 100,000 100,000 100,000 100,000 100,000 100,000 97,800 97,400 6,420 3,420 1,280 Table 5: Number of samples used in each dataset for training. The number of negative samples is included in the total number of samples."
        },
        {
            "title": "B Detailed Results",
            "content": "In this section, we provide detailed results of LUSIFER and E5-Mistral on all benchmark datasets for each language."
        },
        {
            "title": "Es Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "AmazonReviewsClassification MassiveIntentClassification MassiveScenarioClassification MTOPIntentClassification MultilingualSentimentClassification TweetSentimentClassification SpanishNewsClassification PawsXPairClassification XNLI SpanishNewsClusteringP2P MLSUMClusteringP2P MLSUMClusteringS2S SIB200ClusteringS2S MultiEURLEXMultilabelClassification BelebeleRetrieval MintakaRetrieval STS17 STS22 STSBenchmarkMultilingualSTS Avg. 42.69 69.67 74.63 72.16 87.91 49.73 89.5 61.19 77.34 42.28 47.54 47.11 31.01 6.16 83.92 48.77 87.18 71.79 84.31 61.84 50.41 68.93 73.41 80.13 91.01 58.55 87.81 62.82 60.49 43.85 44.36 41.56 44.42 3.87 81.4 18.17 80.84 70.66 79.89 60. Table 6: Detailed results of E5-Mistral and LUSIFER on the Spanish benchmark datasets."
        },
        {
            "title": "En Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "AmazonCounterfactualClassification AmazonPolarityClassification AmazonReviewsClassification Banking77Classification EmotionClassification ImdbClassification MassiveIntentClassification MassiveScenarioClassification MTOPDomainClassification MTOPIntentClassification ToxicConversationsClassification TweetSentimentExtractionClassification SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus ArxivClusteringP2P ArxivClusteringS2S BiorxivClusteringP2P BiorxivClusteringS2S MedrxivClusteringP2P MedrxivClusteringS2S RedditClustering RedditClusteringP2P StackExchangeClustering StackExchangeClusteringP2P TwentyNewsgroupsClustering AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverflowDupQuestions ArguAna ClimateFEVER CQADupstackTexRetrieval DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID STS12 STS13 STS14 STS15 STS16 STS17 STS22 BIOSSES SICK-R STSBenchmark SummEval Avg. 78.69 95.91 55.79 88.23 49.77 94.78 80.57 82.39 96.12 86.11 69.59 63.72 95.66 81.62 87.75 50.45 45.5 43.53 40.24 38.19 37.45 57.71 66.49 73.1 45.91 54.31 66.98 32.6 86.33 54.91 61.88 38.4 42.97 48.9 87.8 56.62 75.7 43.1 38.59 63.5 89.62 16.27 76.41 26.39 87.33 79.65 88.43 84.54 90.42 87.68 91.75 67.28 82.64 80.76 88.6 31.4 67.69 72.45 94.3 55.46 87.33 74 92.52 75.64 78 96.81 87.34 82.84 72.74 90.99 68.49 85.35 35.6 22.25 39.93 29.3 41.2 35.53 39.94 53.4 46.41 39.7 38.5 60.56 24.55 34.94 46.04 74.15 29.24 23.22 17.98 82.77 14.91 49.04 56.43 5.48 42.95 89.1 5.53 66.09 6.33 18.22 74.26 84.2 77.5 84.95 82.21 81.67 71.25 84.22 78 84.18 32.36 57. Table 7: Detailed results of E5-Mistral and LUSIFER on the English benchmark datasets."
        },
        {
            "title": "Ru Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "GeoreviewClassification HeadlineClassification InappropriatenessClassification KinopoiskClassification MassiveIntentClassification MassiveScenarioClassification RuReviewsClassification RuSciBenchGRNTIClassification RuSciBenchOECDClassification GeoreviewClusteringP2P RuSciBenchGRNTIClusteringP2P RuSciBenchOECDClusteringP2P TERRa RiaNewsRetrieval RuBQRetrieval RuSTSBenchmarkSTS STS22 Avg. 46.92 76.52 59.35 60.67 72.06 76.64 64.10 60.19 46.30 69.87 52.96 46.54 57.45 71.39 38.04 81.79 61.32 61.30 43.79 79.26 63.15 60.57 71.29 74.49 67.40 59.51 46.41 59.20 55.00 49.95 54.24 49.61 43.48 78.20 61.44 59. Table 8: Detailed results of E5-Mistral and LUSIFER on the Russian benchmark datasets."
        },
        {
            "title": "Fr Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "AmazonReviewsClassification MTOPIntentClassification MassiveIntentClassification MassiveScenarioClassification TweetSentimentClassification SIB200Classification FrenchBookReviews PawsXPairClassification RTE3 XNLI MasakhaNEWSClusteringP2P MasakhaNEWSClusteringS2S MLSUMClusteringP2P MLSUMClusteringS2S HALClusteringS2S SIB200ClusteringS2S MultiEURLEXMultilabelClassification BelebeleRetrieval MintakaRetrieval OpusparcusPC STS17 SICKFr STS22 STSBenchmarkMultilingualSTS SummEvalFr Avg. 43.36 70.39 71.12 74.68 50.23 72.45 46.77 62.15 88.45 76.60 50.96 52.08 42.69 42.60 24.21 29.94 5.00 84.66 52.60 94.58 84.66 79.12 76.50 83.98 31.38 59.65 49.96 79.14 70.88 73.96 62.62 79.51 48.07 65.93 87.62 62.75 48.59 63.12 42.70 41.51 24.16 43.30 3.51 83.76 18.88 90.63 82.19 74.22 73.77 78.42 31.91 59. Table 9: Detailed results of E5-Mistral and LUSIFER on the French benchmark datasets."
        },
        {
            "title": "Vi Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "MassiveIntentClassification MassiveScenarioClassification MultilingualSentimentClassification SIB200Classification VieStudentFeedbackClassification XNLI SIB200ClusteringS2S BelebeleRetrieval MLQARetrieval VieQuADRetrieval Avg. 66.36 70.69 69.30 70.20 73.02 71.32 32.93 79.20 32.43 20.35 58.58 71.38 74.82 81.30 78.58 77.39 61.30 46.79 85.51 54.61 45.20 67."
        },
        {
            "title": "Fa Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "MassiveScenarioClassification\nMassiveIntentClassification\nMultilingualSentimentClassification\nFarsTail\nWikipediaRerankingMultilingual\nWikipediaRetrievalMultilingual",
            "content": "Avg. 76.37 71.98 80.07 63.49 75.60 67.77 72.55 77.94 73.32 80.54 67.98 78.75 78.49 76.17 Table 11: Detailed results of E5-Mistral and LUSIFER on the Farsi benchmark datasets."
        },
        {
            "title": "Id Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "IndonesianMongabayConservationClassification MassiveIntentClassification MassiveScenarioClassification SIB200Classification indonli SIB200ClusteringS2S BelebeleRetrieval SemRel24STS Avg. 24.72 69.51 72.89 80.88 50.00 46.46 81.10 40.40 58.25 25.27 71.38 74.62 80.44 50.22 47.50 87.56 40.57 59. Table 12: Detailed results of E5-Mistral and LUSIFER on the Indonesian benchmark datasets."
        },
        {
            "title": "Ar Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "TweetEmotionClassification ArEntail XNLI MintakaRetrieval MLQARetrieval STS17 STS22 Avg. 53.74 77.63 68.00 17.15 28.32 75.13 61.01 54.43 49.03 84.15 58.58 16.59 47.90 71.44 61.54 55. Table 13: Detailed results of E5-Mistral and LUSIFER on the Arabic benchmark datasets."
        },
        {
            "title": "Fi Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "FinToxicityClassification MassiveIntentClassification MassiveScenarioClassification MultilingualSentimentClassification SIB200Classification WikipediaRerankingMultilingual BelebeleRetrieval WikipediaRetrievalMultilingual OpusparcusPC FinParaSTS Avg. 53.78 64.15 67.79 72.42 66.57 86.85 73.89 71.90 91.41 20.97 66.97 62.23 70.77 75.02 83.59 77.06 82.65 85.18 82.94 91.63 17.24 72. Table 10: Detailed results of E5-Mistral and LUSIFER on the Vietnamese benchmark datasets. Table 14: Detailed results of E5-Mistral and LUSIFER on the Finnish benchmark datasets."
        },
        {
            "title": "Ko Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "MassiveIntentClassification MassiveScenarioClassification KorSarcasmClassification SIB200Classification KorHateSpeechMLClassification PawsXPairClassification KLUE-TC SIB200ClusteringS2S Ko-StrategyQA BelebeleRetrieval KLUE-STS KorSTS STS17 Avg. 70.42 75.12 57.64 72.70 8.49 53.10 60.58 31.04 63.81 80.09 83.48 79.28 80.97 62.82 69.79 75.60 55.28 77.89 7.54 54.97 63.95 46.58 68.66 84.69 84.17 78.36 80.55 65. Table 15: Detailed results of E5-Mistral and LUSIFER on the Korean benchmark datasets."
        },
        {
            "title": "Te Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "IndicNLPNewsClassification IndicSentimentClassification MassiveIntentClassification MassiveScenarioClassification SIB200Classification TeluguAndhraJyotiNewsClassification IndicReviewsClusteringP2P SIB200ClusteringS2S BelebeleRetrieval IndicQARetrieval IndicCrosslingualSTS SemRel24STS Avg. 89.46 61.53 47.34 51.67 46.23 67.40 34.02 10.81 42.46 33.67 8.36 72.83 47.15 98.90 90.63 68.69 74.17 74.56 76.24 43.62 42.11 80.32 57.61 43.76 80.99 69."
        },
        {
            "title": "Hi Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "Table 18: Detailed results of E5-Mistral and LUSIFER on the Telugu benchmark datasets. MTOPIntentClassification SentimentAnalysisHindi MassiveIntentClassification MassiveScenarioClassification SIB200Classification TweetSentimentClassification XNLI IndicReviewsClusteringP2P SIB200ClusteringS2S WikipediaRerankingMultilingual BelebeleRetrieval MintakaRetrieval MLQARetrieval WikipediaRetrievalMultilingual IndicCrosslingualSTS SemRel24STS Avg. 68.84 58.98 64.69 69.71 68.43 37.70 65.04 40.04 27.32 85.22 69.73 18.60 35.37 74.62 42.30 73.14 56.23 79.93 73.92 71.01 75.42 75.98 40.78 60.26 42.40 45.62 78.17 66.76 21.53 54.54 75.25 58.97 77. 62.37 Table 16: Detailed results of E5-Mistral and LUSIFER on the Hindi benchmark datasets."
        },
        {
            "title": "Bn Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "BengaliDocumentClassification BengaliHateSpeechClassification MassiveIntentClassification MassiveScenarioClassification XNLIV2 IndicReviewsClusteringP2P SIB200ClusteringS2S WikipediaRerankingMultilingual BelebeleRetrieval IndicQARetrieval WikipediaRetrievalMultilingual IndicCrosslingualSTS Avg. 50.78 54.67 59.51 64.57 63.66 38.20 23.88 82.66 60.17 56.59 71.05 35.42 55.10 48.00 51.43 66.65 70.91 60.01 45.68 43.96 76.39 55.77 68.06 72.47 41.86 58. Table 17: Detailed results of E5-Mistral and LUSIFER on the Bengali benchmark datasets."
        },
        {
            "title": "Sw Datasets",
            "content": "E5-Mistral"
        },
        {
            "title": "LUSIFER",
            "content": "AfriSentiClassification MasakhaNEWSClassification MassiveIntentClassification MassiveScenarioClassification SwahiliNewsClassification XNLI MasakhaNEWSClusteringP2P MasakhaNEWSClusteringS2S Avg. 39.67 72.96 52.84 61.09 63.95 58.86 34.15 21.34 50.61 46.47 74.79 52.79 58.59 61.56 57.82 36.95 35.97 53. Table 19: Detailed results of E5-Mistral and LUSIFER on the Swahili benchmark datasets."
        }
    ],
    "affiliations": [
        "Adobe Research, USA",
        "Dept. of Computer Science, University of Oregon, OR, USA"
    ]
}