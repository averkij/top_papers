{
    "paper_title": "Kling-Omni Technical Report",
    "authors": [
        "Kling Team",
        "Jialu Chen",
        "Yuanzheng Ci",
        "Xiangyu Du",
        "Zipeng Feng",
        "Kun Gai",
        "Sainan Guo",
        "Feng Han",
        "Jingbin He",
        "Kang He",
        "Xiao Hu",
        "Xiaohua Hu",
        "Boyuan Jiang",
        "Fangyuan Kong",
        "Hang Li",
        "Jie Li",
        "Qingyu Li",
        "Shen Li",
        "Xiaohan Li",
        "Yan Li",
        "Jiajun Liang",
        "Borui Liao",
        "Yiqiao Liao",
        "Weihong Lin",
        "Quande Liu",
        "Xiaokun Liu",
        "Yilun Liu",
        "Yuliang Liu",
        "Shun Lu",
        "Hangyu Mao",
        "Yunyao Mao",
        "Haodong Ouyang",
        "Wenyu Qin",
        "Wanqi Shi",
        "Xiaoyu Shi",
        "Lianghao Su",
        "Haozhi Sun",
        "Peiqin Sun",
        "Pengfei Wan",
        "Chao Wang",
        "Chenyu Wang",
        "Meng Wang",
        "Qiulin Wang",
        "Runqi Wang",
        "Xintao Wang",
        "Xuebo Wang",
        "Zekun Wang",
        "Min Wei",
        "Tiancheng Wen",
        "Guohao Wu",
        "Xiaoshi Wu",
        "Zhenhua Wu",
        "Da Xie",
        "Yingtong Xiong",
        "Yulong Xu",
        "Sile Yang",
        "Zikang Yang",
        "Weicai Ye",
        "Ziyang Yuan",
        "Shenglong Zhang",
        "Shuaiyu Zhang",
        "Yuanxing Zhang",
        "Yufan Zhang",
        "Wenzheng Zhao",
        "Ruiliang Zhou",
        "Yan Zhou",
        "Guosheng Zhu",
        "Yongjie Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 6 7 7 6 1 . 2 1 5 2 : r Kling-Omni Technical Report Kling Team, Kuaishou Technology We present Kling-Omni, generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond content creation tool, we believe Kling-Omni is pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds. Date: December 18, 2025 Access: https://app.klingai.com/global/omni/new Model ID: Kling-Omni (Kling-O1)"
        },
        {
            "title": "1 Introduction",
            "content": "It has been long-term vision in artificial general intelligence to create multimodal assistants capable of perceiving, reasoning, and creating across all sensory domains and generating visual outputs that mirror human communication through language [2, 15, 28], visual demonstration [23, 34], and temporal dynamics [1, 24]. Ideally, such systems should seamlessly process diverse inputs, whether text, images, or video, and produce corresponding outputs. Recent breakthroughs in unified modeling [12, 18] have brought this vision closer to reality. Pioneering works in image-text unification have successfully bridged the gap between understanding and generation by jointly optimizing these capabilities within unified architecture. Models like Gemini 3 Pro Image [4] have further accelerated this paradigm shift, evolving from specialized single-task solvers into comprehensive systems that integrate computer vision, reasoning, and content creation. These advances signal decisive move away from fragmented \"expert models\" toward powerful, general-purpose unified systems. Despite this progress, integrating video understanding and generation remains significant challenge due to the following reasons. Firstly, the current landscape of video generation [7, 8, 30] is still dominated by fragmented approaches. Most state-of-the-art video models are narrowly focused on specific tasks, such as text/image-to-video synthesis, and often rely on static text encoders that struggle to capture complex visual details. On the other hand, video editing and understanding frequently depend on separate, task-specific pipelines or external adapters, which complicates scaling and integration. As result, advanced capabilities that require deep synergy between perception and creationsuch as multimodal in-context generation, precise visual editing through reasoning, and responding to interleaved video-text instructionsare still out of reach for existing video architectures. Secondly, the interaction paradigm towards unified video generation system remains significant bottleneck. Relying solely on natural language prompts often fails to capture the nuances of visual imagination; text is inherently limited in describing precise spatial relationships, visual 1 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language references, and temporal dynamics, leading to gap between user intent and model output. Finally, current models [1] lack deep, native intelligence. While they excel at pixel-level synthesis, they often struggle with semantic reasoning and understanding the underlying physics or logic of scene, acting more as passive generators than intelligent agents capable of inferring complex user intentions. In this work, we introduce Kling-Omni, generalist framework designed to tackle these challenges by unifying diverse video generation, editing, and intelligent creation tasks. Kling-Omni employs straightforward architecture, representing an important step from specialized expert models to unified system that seamlessly integrates these capabilities and removes task boundaries. To achieve this, we propose multi-modal vision language (MVL) as new interaction paradigm, revolutionizing how users interact with video generation models. Unlike traditional approaches, MVL constructs unified input representation by combining natural language as semantic skeleton with multi-modal descriptions. This enhances the models foundational understanding and control by treating text and visual signals as cohesive language. Moreover, Kling-Omni represents an advancement towards multi-model intelligence. The introduction of MVL does not merely refine instruction following; it empowers the model to deeply understand and infer user intentions. By exploring this inference potential, Kling-Omni moves beyond rote generation, demonstrating unexpected reasoning capabilities. The remainder of this report is organized as follows. Sec. 2 presents the methodology, introducing the key components, training strategies, training optimization, and inference optimization. Sec. 3 focuses on data engineering, outlining the data collection and processing processes. Sec. 4 provides comprehensive analysis of Kling-Omnis capabilities, including human evaluation, multi-modal referencing, interactive editing, and the broader potential of the model in intelligent reasoning and generation. Finally, we conclude with discussion and acknowledges the contributions of the authors involved in the project."
        },
        {
            "title": "2.1 Model Architecture Overview",
            "content": "Figure 1 Overview of Kling-Omni, generalist framework that introduces multimodal visual language as the interaction mechanism, supporting diverse tasks including video generation, editing, and intelligent reasoning. We present Kling-Omni, generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language (MVL) inputs. Adopting an end-to-end perspective, Kling-Omni moves beyond disjointed pipeline approaches, integrating instruction understanding, visual generation and refinement into holistic system. The architecture is designed to accept diverse range of user inputsincluding text instructions, reference images, and video contextsprocessing them through unified interface to produce cinematic-quality video content creation and editing with high intelligence. 2 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language As illustrated in the framework, the architecture comprises three key components underpinned by robust training and infrastructure ecosystem. First, to bridge the gap between heterogeneous user inputs and the models representations, Prompt Enhancer (PE) module employs an MLLM to comprehend complex user inputs and synthesize them with learned world knowledge. By doing so, it infers the creators specific creative intent and reformulates the prompt accordingly. These refined features serve as input for the Omni-Generator, which processes visual and textual tokens within shared embedding space, enabling deep cross-modal interaction, ensuring robust visual consistency and precise instruction adherence. The generated content is subsequently refined by Multimodal Super-Resolution module, which conditions on original MVL signals to refine high-frequency details. The entire system is empowered by progressive multi-stage training strategy, ranging from instruction pre-training, supervised fine-tuning to reinforcement learning (RL), and operates on highly optimized infrastructure utilizing 3D parallelism and model distillation to improve training and inference efficiency."
        },
        {
            "title": "2.2 Training Strategies of Omni-Generator",
            "content": "2.2.1 Pre-training In the pre-training phase, we harness large-scale text-video paired data to instill robust instruction-based text-to-video generation capabilities into the model. To ensure the model can adapt to wide spectrum of user inputs, we curate captions ranging from concise prompts to elaborate narratives, thereby laying solid groundwork for comprehending diverse instructional formats. Furthermore, to catalyze the models sensitivity to multi-modal vision-language (MVL) contexts, we infuse image-to-video tasks into the training mixture, establishing an early synergy between visual and textual modalities. 2.2.2 Supervised Fine-tuning Continue-training. This stage focuses on deeply aligning the model with complex MVL inputs. We introduce comprehensive curriculum that includes reference-to-video generation, image/video editing, and suite of specialized tasks for semantic understanding. These tasks feature highly interleaved formats of image, video, and text conditioning. By exposing the model to such heterogeneous and information-rich data, we effectively enhance its ability to interpret intricate instructions and perform preliminary reasoning. Quality-tuning. To further enhance the generation quality and multimodal understanding capacity of the model, we have meticulously constructed high-quality dataset characterized by balanced task distribution and exceptional video standards. Each data sample is paired with precise instruction annotations. Through iterative fine-tuning on this premium dataset, we progressively optimize the models output distribution, steering it towards domain of superior visual quality and understanding capacity. 2.2.3 Reinforcement Learning To bridge the gap between model outputs and human aesthetic preferences, we employ Direct Preference Optimization (DPO) [21]. We favor DPO over alternative algorithms like GRPO[26] because it bypasses the computationally expensive trajectory sampling required by the latter, offering streamlined one-step diffusion forward process. Our optimization objectives are centered on key perceptual metrics, specifically motion dynamics and visual integrity. For data construction, we sample diverse array of MVL conditions to form candidate pool, subsequently generating multiple video variations using distinct random noise. These variations are then subjected to human evaluation to identify preference pairsdistinguishing between the optimal (preferred) and suboptimal (dispreferred) outcomes. During training, these preference pairs, along with their corresponding noise and timesteps, are utilized to compute the DPO loss. Through multiple rounds of this preference-aligned training, the model achieves significant improvements in video generation quality, aligning more closely with human intent. 2.2.4 Model Acceleration (Distillation) We develop two-stage distillation methodology to substantially reduce the computational cost of inference while preserving output fidelity. The acceleration pipeline incorporates both trajectory matching distillation 3 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language and distribution matching distillation, compressing the model inference to 10 Number of Function Evaluations (NFE), which originally costs 150 NFE to synthesize single video sample before distillation. In the first stage, the procedure follows the principle of trajectory matching distillationexemplified by PCM [31], HyperSD [22], and related methodsto ensure closer adherence to the teacher trajectory at the early training phase. Specifically, we employ phase-wise temporal structuring of the training objective with the timestep scheduler partitioned into several phases. The student model is supposed to predict temporally consistent denoising outputs that align with the designated phase endpoint at any reverse step. Different from common practice that initially distills student model into an intermediate state whose NFE is reduced yet still exceeds the expected NFE, we directly make the student execute with the target scheduler of 10 sampling step in this stage. To further enhance the generation performance, distribution matching distillation is conducted as the second stage training. Unlike other score-based distillation algorithms such as DMD[35] and SiD[37] that formulate the student as stochastic differential equation (SDE) process, we adopt the insights of TDM [17] and distill the student to perform few-step ordinary differential equation (ODE) sampling, which has been empirically demonstrated to be more suitable for our tasks. In addition, the trajectory matching objective is preserved at this stage, serving as \"regularization\" mechanism to prevent the model from deviating significantly from the reference trajectory. The similar operation has also been reported in [3]."
        },
        {
            "title": "2.3 Prompt Enhancer",
            "content": "To address the ambiguity and high variance inherent in user inputs, we introduce Prompt Enhancer (PE) module for Kling-Omni. The primary function of the PE is to map diverse user prompts onto distribution that is consistent with the models training data. This alignment is critical for enhancing generative quality, specifically in terms of identity preservation, spatial coherence, and color fidelity, while simultaneously improving physical plausibility via textual reasoning [29, 33]. The PE is built upon Multimodal Large Language Model (MLLM) to accommodate multi-modal user inputs. Since general-purpose MLLMs are not optimized for our specific generation tasks, we constructed specialized multilingual dataset. Our training pipeline involves two phases: initially, we utilize Supervised Fine-Tuning (SFT) to enable the models reasoning chain (or \"thinking process\"). This is followed by Reinforcement Learning (RL), where the reward function is designed to maximize factual correctness, content richness, and semantic plausibility, as well as the similarity between the processed prompts and our high-quality training data. Experiments indicate that the PE module significantly boosts Kling-Omnis performance, resulting in videos with greater dynamism and detail. Furthermore, the PE demonstrates strong generalization potential, empowering the model with intelligent creativity."
        },
        {
            "title": "2.4 Multimodal Super-Resolution",
            "content": "To improve the training and inference efficiency of generator, we propose cascaded diffusion framework for Video Super-Resolution (VSR). Conditioned on both Low-Resolution (LR) latents from the base model and Multi-modal Vision-Language (MVL) signals, our VSR model operates as unified framework. This cohesive design enables the synthesis of high-fidelity, fine-grained visual details and textures, catering to diverse range of applications. We adopt the architecture of the base model and initialize our VSR module using its pre-trained weights. To address the computational overhead imposed by long temporal contexts and high-resolution inputs, we exploit the inherent spatio-temporal redundancy of video data. Specifically, we replace standard full attention mechanisms with local window attention. To prevent receptive field isolation, we implement shifted window strategy in every odd-numbered layer, offsetting the window by half its size, to facilitate information flow between adjacent non-overlapping windows, as illustrated in Fig. 2. To further minimize inference latency, we introduce an asymmetric attention mechanism. In this configuration, condition tokens (serving as queries) are restricted to self-attention, whereas noisy tokens attend to the full sequence. This decoupling allows us to cache the Key-Value (KV) features of the condition tokens, enabling 4 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 2 Attention maps in Multimodal Super-Resolution. The left panel illustrates the map for even-numbered layers, while the right panel shows the map for odd-numbered layers. Skipping the computation for the shaded regions leads to substantial reduction in computational load and supports accelerated inference with KV cache. their reuse across subsequent sampling steps. This strategy boosts generation efficiency with negligible impact on visual performance."
        },
        {
            "title": "2.5 Training Optimization",
            "content": "We develop an end-to-end training system that optimizes multimodal data processing, parallel execution, and computation kernels for large-scale pre-training. 2.5.1 Multimodal Data Pipeline and Load Balancing To handle significant sequence length variation across text, image, and video data, we employ heuristic scheduling strategy to reduce imbalance bubble across pipeline-parallel (PP) [6, 911, 19, 20] and data-parallel (DP) groups. As shown in Fig. 3, the training loop is divided into two stages: online VAE/text encoder inference and DiT training. central scheduler assigns samples to DP groups to ensure balanced workloads. For VAE/text encoder inference, tokens are dynamically partitioned across PP stages to balance encoding workloads and improve utilization. To further address dynamic sequence lengths, we introduce microbatch-level elastic ulysses-parallel (UP) [13, 16] switching mechanism[32], as shown in Fig.4. An online adaptive scheduler with asynchronous pipeline predetermines the UP degree per microbatch and dynamically adjusts assignments to DP ranks, reducing load imbalance. To mitigate network congestion from cross-node all-to-all communication, we adopt two-tier all-to-all strategy (intra-node aggregation followed by inter-node exchange) to distribute traffic and alleviate spine switch workload. 2.5.2 Efficient Multimodal Framework and Activation Reduction In DiT training, inputs are flattened into 1D sequences with minimal padding[5], and the computation graph is restructured to preserve modality-independent computation, minimizing redundant data movement and layout transform overhead. packing version of multimodal FlashAttention [25] operator (MM-FlashAttention) is developed to support arbitrary cross-modal masks and variable-length sequences within single kernel while maintaining high performance. For activation reduction, we selectively recompute [36] the most cost-effective operators, and pipeline-aware 5 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 3 Online training data pipeline. Raw data is distributed across DP/PP groups using an inference scheduler. After inference, training scheduler reorders data for balanced workload. Figure 4 The pipeline schedule in Kling-Omni. The inference pass of VAE/TE are distributed across both dataand pipeline-parallelism, following an interleaved 1F1B pipeline schedule. Pipeine-aware offloading and onloading are introduced to reduce GPU memory consumption without blocking forward or backward pass, and an online load balance scheduler is running on CPU to determine the ulysses parallel size and the workload for each microbatch. offloading [36] further reduces GPU memory by moving activations to CPU. Kernel fusions cut down memory traffic and launch overhead, which is crucial for packing phase. virtual-pipeline-stage-aware mechanism reuses activations across model chunks with identical inputs, slashing memory and computation in multi-view, multi-stream scenarios. 2.5.3 Reliability and High-Availability We achieve 97% Effective Training Time Ratio by compressing recovery time. An automated fault detection system monitors RDMA traffic to detect hangs within minute, reducing worst-case exit time to minute-level. custom TCP synchronization layer and concurrent artifact loading from NVMe enable sub-minute restarts. Parallelized warmup overlaps NCCL initialization and kernel compilation with I/O, reducing first-iteration overhead to second-level. To ensure runtime stability, I/O operations are optimized through request-training overlap. Random reads from dataset shuffling are converted to sequential access via pre-shuffled Parquet files. Non-blocking asynchronous checkpointing and hardware isolation prevent interference. unified observability stack correlates MFU drops with data shifts and kernel stalls for automated root-cause analysis."
        },
        {
            "title": "2.6 Inference Optimization\nModel Parallelism. To mitigate the substantial GPU memory consumption and inference latency associ-\nated with long-sequence video generation, we adopt a hybrid parallel inference strategy, including Ulysses\nparallelism[14], and tensor parallelism[27]. In addition, to reduce communication overhead, we design a\ncomputation–communication overlap scheme, which can hide most of the communication cost and has almost\nno impact on computation.",
            "content": "Quantization. To further reduce inference latency and lower memory usage, we designed comprehensive hybrid quantization scheme that achieves nearly lossless acceleration. The scheme has three main features: 6 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Wide quantization coverage. Most GEMM operations and self-attention modules in the model are quantized to FP8. Zero-overhead quantization. All quantization and dequantization operators are fused into other kernels, minimizing the additional overhead introduced by quantization FP8 communication. Using FP8 for communication further reduces communication overhead. When combined with communication-overlap techniques, most communication overhead can be effectively hidden. Cache. The Kling-Omni model takes large number of reference images and reference videos as input, and these long conditional inputs significantly increase inference time. We designed cache scheme tailored for Kling-Omni, achieving roughly 2 speedup. In addition, we developed cache-offload solution that greatly alleviates the potential memory pressure introduced by the caching mechanism."
        },
        {
            "title": "3 Data System",
            "content": "This section delineates the data methodology underlying our unified video generation and editing framework, structured around two pivotal dimensions: data collection and data processing system. Figure 5 Cross-modal and cross-task data distribution in our constructed data system. Driven by the requirements of high-fidelity video synthesis, specifically regarding temporal consistency, semantic stability, multi-image reference alignment, and complex editing constraints, we have engineered holistic data infrastructure. This system spans two key dimensions: cross-modality (image/text/video) and cross-task (image-to-video, video-to-video, editing, and reference-based generation, etc.), ensuring robust foundation for model training, as illustrated in Fig. 5. 7 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language"
        },
        {
            "title": "3.1 Data Collection",
            "content": "To construct training corpus characterized by high diversity, consistency, and controllability, our data collection system integrates large-scale real-world data acquisition with task-oriented synthetic data construction. Real-World Data Acquisition. We curated comprehensive collection of videos and images data to ensure broad scenario coverage. These sources provide essential natural priors, spanning diverse subjects, complex scenes, and stylistic variations. To expand task coverage beyond static datasets, we developed an automated pipeline for large-scale internet data mining. Utilizing an in-house embedding model, the pipeline identifies and constructs cross-modal samples that are semantically related or subject-consistent to enhance the models generalization across diverse generation scenarios. Synthetic Data Construction. Since relying solely on real-world data is insufficient for learning precise controllability, we employed synthesis pipeline driven by expert models. We utilized in-house image editing and video understanding models to produce high-quality samples for tasks such as editing and multi-image referencing. Furthermore, to support high-fidelity video generation tasks, we constructed an automatic reverse synthesis strategy. These approaches constructs robust reference-to-video training examples that preserve the temporal consistency of natural videos while providing explicit control signals."
        },
        {
            "title": "3.2 Data Processing",
            "content": "In large-scale multimodal training, data quality directly dictates the models temporal consistency, semantic stability, and cross-modal alignment capabilities. We constructed three-tier processing system covering basic governance, temporal stability, and cross-modal alignment to ensure that the training data exhibits stable, clean, and interpretable distribution, as illustrated in Fig. 6. Basic Filtering. To establish robust quality baseline, we implemented rigorous governance protocol that filters unusable or compromised samples. This process begins with strict resolution and duration thresholds to ensure visual validity, followed by deduplication mechanism using frame-wise and temporal fingerprinting to prevent model bias from redundant content. Additionally, we applied audio-visual corruption detection to eliminate samples with structural errors and enforce content safety protocols to exclude NSFW material. This foundational layer guarantees the hygiene of the raw data pool, preventing the training process from being disrupted by noise. Temporal Quality Assessment. Given the critical importance of temporal continuity in video generation, we employed specialized screening mechanisms for visual and temporal stability. We utilized quality scoring metrics to identify and penalize artifacts such as blur, jitter, and compression noise. To prevent the model from learning unnatural discontinuities, the system detects and removes abrupt scene changes and incoherent shot transitions. Furthermore, we filtered out videos with excessively low action semantic density, thereby improving the effective training ratio for dynamic content and ensuring the model learns high-quality temporal coherence. VideoText and ImageVideo Alignment. To support the unified modeling of text, images and videos, we established systematic cross-modal alignment detection mechanism. This involves evaluating the semantic consistency between video captions and actual visual content, as well as assessing the fidelity of reference images to target videos for generation tasks. We further verified the alignment between editing instructions and their execution results. Crucially, for human-centric tasks, we enforced strict character identity consistency checks. These strategies ensure the model learns accurate mapping relationships across modalities, facilitating robust performance in complex editing and generation scenarios."
        },
        {
            "title": "4 Model Performance",
            "content": "This section presents comprehensive evaluation and capability analysis of Kling-Omni. Specifically, Sec. 4.1 details the internal evaluation protocol, including the overall benchmark design, absolute scoring procedures and comparative evaluation using the GoodSameBad (GSB) metric, with the summarized results provided in Figure 7. Sec. 4.2 investigates the performance of the model in spectrum of core functionalities, such as image/video reference generation, video editing, and the synergistic effects that arise from the composition of 8 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 6 Data filtering pipeline for video and image samples, illustrating the stages of quality control, temporal consistency, and multimodal alignment. multiple capabilities. Sec. 4.3 further examines the extended potential of the model, highlighting its proficiency in more advanced interactive and reasoning-enhanced generative tasks."
        },
        {
            "title": "4.1 Human Evaluation",
            "content": "4.1.1 Benchmarks To validate the performance of Kling-Omni compared with other leading video generation and editing models, we constructed the OmniVideo-1.0 Benchmark, which encompasses comprehensive and representative set of scenarios. We collected large amount of high-quality multimodal dataset, including images, subjects, and videos as elements. Utilizing this dataset, we designed over 500 cases to comprehensively evaluate the models capability of referencing, integrating, and editing diverse elements. We meticulously constructed the evaluation set across multiple dimensions, including : Subject Categories, which include humans, cartoon characters, animals, clothing, and props; Application Scenarios, such as professional video production, ecommerce advertising, and social media content creation; and Additional Challenges, involving complex actions, wide-angle perspectives, emotional expressions, cross-style integration, and multi-element fusion. 4.1.2 Metrics We engaged with creators ranging from professional directors to general users. By collecting the requirements from different user groups, we constructed an evaluation system that is comprehensive, structured, and interpretable to evaluate the overall capabilities of model. This system primarily comprises the following core metrics: Dynamic Quality. This metric assesses the temporal performance of the model, focusing on the continuity between frames, the stability of attributes, and the plausibility of motion relative to physical laws and commonsense dynamics. It evaluates the naturalness of movement, the seamlessness of subjectbackground integration, and the adequacy of motion amplitude. Additionally, it considers higher-level behaviors, such as multi-character interactions and narrative camera movements, ensuring the generated sequence exhibits stable, realistic, and expressive motion. Prompt Following. Reflecting the users creative intent, this metric measures the models instruction adherence. It evaluates how accurately the generated video captures and executes the semantic information and specific constraints detailed in the input prompt. Identity Consistency. This metric evaluates the models ability to preserve the identity and structural features of reference subjects (e.g., persons, objects, or styles). It assesses stability across variations in camera angles, expressions, complex movements, and lighting conditions throughout the video. Video Consistency: Specific to video editing tasks, this metric measures the models faithfulness to unedited 9 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 7 Quantitative comparison of Kling-Omni against SOTA methods on reference-based video generation and video editing tasks. Overall GSB is computed over all evaluation metrics. regions. It assesses the ability to sustain the identity and structure of key subjects while performing content modifications or style transfers, ensuring visual smoothness and coherence alongside the execution of editing instructions. 4.1.3 Evaluation Results We conducted double-blind human evaluation based on the OmniVideo-Benchmark 1.0, inviting domain experts and professional annotators to compare Kling-Omni against industry-leading models. Evaluators performed side-by-side qualitative assessments based on the defined dimensions, classifying the relative performance into three categories: (Good): The performance of Kling-Omni is significantly superior to the competing model. (Same): The performance of Kling-Omni is comparable to the competing model. (Bad): The performance of Kling-Omni is significantly inferior to the competing model. The aggregated GSB metric distributions for Image-Reference and Video-Editing tasks are presented in Fig. 7. We compared Kling-Omni against Veo 3.1 [8] for image-referencing tasks and Runway-Aleph [24] for video editing tasks. As illustrated, Kling-Omni demonstrates varying degrees of superiority over its competitors across all evaluated dimensions, validating its robustness and reliability in complex generation and editing scenarios."
        },
        {
            "title": "4.2 Unleash Imagination via Kling-Omni",
            "content": "This section demonstrates the capabilities of Kling-Omni. Table 1 lists the representative features, including but not limited to reference-based generation, instruction-driven editing, video reference, frame-condition 10 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Table 1 Comparison of model capabilities: Kling-Omni vs. SOTA Video Generation and Editing Models. Category Capability Kling-Omni (Ours) Google Veo 3.1 Runway Aleph Image/Element Library Reference Image Reference Element Library Reference Image + Element Library Ref. Addition Removal Replacement Stylization Attribute Manipulation Special Effects Video Matting Multi-image Editing Subject-driven Editing Next Shot Generation Prev. Shot Generation New Camera Angle Generation Motion Transfer Camera Motion Transfer First Frame to Video First & Last Frame Instruction Editing Video Reference Frame-cond. Generation Text-to-Video Compositional Generation Visuao Prompt Understanding Reasoning-enhanced Generation generation, compositional generation, visual prompt understanding, intelligent reasoning via generation, etc. Qualitative analysis for representative features are provided below. 4.2.1 Multi-Modal and Multi-Dimensional Precise Referencing Kling-Omni enables fine-grained and reliable control through multi-modal and multi-dimensional referencing, as shown in Table 1. The model supports flexible conditioning based on diverse input formsimages, videos, and textand allows users to specify reference information across multiple dimensions, including but not limited to identity, status, style, shot composition, and actions. Unlike single-image-per-subject referencing approaches, Kling-Omni incorporates subject library mechanism, where multiple images of the same subject (e.g., the same person with different viewpoints, poses, expressions, or lighting conditions) can be jointly provided. This design improves the ability of the model to establish consistent and robust subject representation, allowing more stable identity preservation. By integrating these multi-source and multi-dimensional cues, Kling-Omni achieves precise alignment with user intent while maintaining visual and semantic coherence in complex generation scenarios, including image/element library reference, new camera angle generation, motion transfer, camera motion transfer, next-shot generation, previous-shot generation and flexible referencing dimension like sketch, as shown in Fig. 8 to Fig. 14. This flexible referencing paradigm also leaves room for users to explore richer combinations 11 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language of reference dimensions beyond the predefined set, further exploring the potential of Kling-Omni. 4.2.2 Temporal Narrative This feature enables the model to interpret group of related imageswhether they depict continuous single shot or complex multi-shot sequenceand generate comprehensive video presentation, as shown in Fig. 15 and Fig. 16. By intelligently bridging the visual gaps between frames, the model constructs cohesive, chronologically flowing narrative that transforms static storyboard into dynamic video experience. 4.2.3 High-Degree-of-Freedom Interactive Editing In addition to conventional edit operations such as addition, removal, and replacement of content, Kling-Omni enables unconstrained interactive manipulation that is free from temporal and spatial limitations, allowing users to control video content along arbitrary dimensionsincluding elements, styles, scenes, and shots, as shown in Fig. 17 to Fig. 23. 4.2.4 Flexible Task Combination As shown in Fig. 24 and Fig. 25, the model has the ability to handle combined complex instructions within single generation process, without requiring sequential task execution or manual decomposition. This unified approach not only simplifies the workflow but also avoids the accumulation of errors that typically occur in sequential editing, ensuring more consistent and accurate results while improving overall generation efficiency."
        },
        {
            "title": "4.3 Broader Potential of Kling-Omni∗",
            "content": "4.3.1 Controllable Generation via Visual Signals Moving beyond traditional text-based prompting, we conduct an experimental investigation of video generation driven by visual signals. We take advantage of powerful visionlanguage reasoning model to explore new possibilities in customized video synthesis. We adopt workflow in which users express their intent through visual annotationssuch as drawing arrows to indicate character trajectories or using bounding boxes to specify interactions. As shown in Fig. 26, by interpreting these visual cues, the model translates abstract user concepts into concrete generation constraints. This example demonstrates the promising potential of advanced visionlanguage systems to achieve fine-grained control over character identity and scene dynamics. 4.3.2 Reasoning-enhanced Generation We conduct an exploratory study on intelligent reasoningenhanced generation, integrating more powerful visionlanguage reasoning engine to bridge the gap between abstract user prompts and concrete visual execution. As shown in Fig. 27, the system leverages world knowledge, such as interpreting GPS coordinates or inferring temporal dynamics, to ground user instructions in real-world context. For example, it can decode raw geographic coordinates to retrieve associated landmark knowledge (e.g., the Eiffel Tower), enabling context-aware scene synthesis. Furthermore, as illustrated in Fig. 28, the system demonstrates reasoning abilities. These include geometric and relational inference for sorting tasks, as well as semantic structural reasoning for completing visual puzzles. Together, these capabilities push video generation beyond mere depiction toward dynamic, intelligent problem-solving."
        },
        {
            "title": "5 Conclusion",
            "content": "In this report, we present Kling-Omni, generalist generative model that bridges the traditional boundaries between video generation, editing, and multimodal reasoning. By leveraging diffusion transformer aligned with vision-language model, Kling-Omni establishes shared embedding space that enables deep cross-modal interaction. Kling-Omni effectively replaces fragmented expert models with single, holistic system capable Features described in this section are not yet supported in the online version. 12 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language of processing Multi-modal Visual Language (MVL) inputs to produce high-fidelity, physically plausible video content. Our contributions extend beyond model architecture to encompass robust training and data infrastructure. We constructed comprehensive data engineering pipeline ensuring temporal stability and semantic alignment, and implemented highly optimized infrastructure to ensure scalability and efficiency. Extensive evaluations demonstrate that Kling-Omni achieves state-of-the-art performance in complex tasks. Looking forward, Kling-Omni represents foundational step toward building multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds."
        },
        {
            "title": "6 Contributors",
            "content": "All contributors are listed in alphabetical order by their last names. Jialu Chen, Yuanzheng Ci, Xiangyu Du, Zipeng Feng, Kun Gai, Sainan Guo, Feng Han, Jingbin He, Kang He, Xiao Hu, Xiaohua Hu, Boyuan Jiang, Fangyuan Kong, Hang Li, Jie Li, Qingyu Li, Shen Li, Xiaohan Li, Yan Li, Jiajun Liang, Borui Liao, Yiqiao Liao, Weihong Lin, Quande Liu, Xiaokun Liu, Yilun Liu, Yuliang Liu, Shun Lu, Hangyu Mao, Yunyao Mao, Haodong Ouyang, Wenyu Qin, Wanqi Shi, Xiaoyu Shi, Lianghao Su, Haozhi Sun, Peiqin Sun, Pengfei Wan, Chao Wang, Chenyu Wang, Meng Wang, Qiulin Wang, Runqi Wang, Xintao Wang, Xuebo Wang, Zekun Wang, Min Wei, Tiancheng Wen, Guohao Wu, Xiaoshi Wu, Zhenhua Wu, Da Xie, Yingtong Xiong, Yulong Xu, Sile Yang, Zikang Yang, Weicai Ye, Ziyang Yuan, Shenglong Zhang, Shuaiyu Zhang, Yuanxing Zhang, Yufan Zhang, Wenzheng Zhao, Ruiliang Zhou, Yan Zhou, Guosheng Zhu, Yongjie Zhu. Project Lead 13 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 8 Examples of image-reference-based video generation. 14 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 9 Examples of element library reference. Kling-Omni supports multi-expression references for the same subject. 15 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 10 Examples of image reference together with element library reference. 16 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 11 Examples of new camera angle generation and motion transfer in video reference. 17 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 12 Examples of camera motion transfer in video reference. 18 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 13 Examples of next shot generation and previous shot generation in video reference. 19 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 14 Examples of flexible image and video reference, e.g, sketch reference. The top example shows video generation controlled by the sketch drawing in the reference image, while the bottom example illustrates video stylization that integrates color references into the sequential sketch reference of video. 20 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 15 Examples of temporal narrative in image reference. The input is multi-grid image. 21 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 16 Examples of temporal narrative in image reference. The input is multi-grid image. 22 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 17 Examples of addition, removal, and replacement in video editing. 23 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 18 Examples of reference-imageguided addition and replacement in video editing. 24 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 19 Examples of background replacement in video editing. 25 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 20 Examples of video stylization in video editing. 26 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 21 Examples of attribute manipulation in video editing. 27 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 22 Examples of special effects in video editing. 28 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 23 Examples of weather change in video editing. 29 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 24 Example of task composition: Kling-Omni combines the element library of girl, reference images, and an video stylization prompt to generate consistent stylized video. 30 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 25 Two examples of task composition: (top) generating new camera angle while adding referenced headband; (bottom) replacing the background, adding train platform element, and converting the video to claymation style. 31 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 26 Examples of visual-signal-guided video generation, which supports intelligent interpretation of user intent from images containing visual signals. 32 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 27 Examples of reasoning-enhanced generation leveraging world knowledge. The top one demonstrates geospatial reasoning by synthesizing subject into the specific location defined by GPS coordinates (the Eiffel Tower). The bottom one showcases temporal reasoning, where the model accurately adjusts environmental lighting and shadows on mountain landscape based on the instruction \"6 hours later.\" 33 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language Figure 28 Examples of reasoning-enhanced generation for logical tasks. The top one demonstrates sorting geometric shapes (tetrahedron, cube, octahedron) in ascending order of face count. The bottom one shows solving linguistic puzzle by selecting and placing the correct character block to complete two intersecting Chinese idioms. 34 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language"
        },
        {
            "title": "References",
            "content": "[1] Video generation models as world simulators. OpenAI, 2024. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Hanbo Cheng, Peng Wang, Kaixiang Lei, Qi Li, Zhen Zou, Pengfei Hu, and Jun Du. From structure to detail: Hierarchical distillation for efficient diffusion model. arXiv preprint arXiv:2511.08930, 2025. [4] Google Deepmind. https://deepmind.google/models/gemini-image/pro/. [5] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. [6] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, et al. Dapple: pipelined data parallel approach for training large models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 431445, 2021. [7] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [8] Google. https://aistudio.google.com/models/veo-3. [9] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training. arXiv preprint arXiv:1806.03377, 2018. [10] Mincong Huang, Chao Wang, Chi Ma, Yineng Zhang, Peng Zhang, and Lei Yu. Re-evaluating the memory-balanced pipeline parallelism: Bpipe. arXiv preprint arXiv:2401.02088, 2024. [11] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. [12] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [13] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. [14] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models, 2023. https://arxiv.org/abs/2309.14509. [15] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [16] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. [17] Yihong Luo, Tianyang Hu, Jiacheng Sun, Yujun Cai, and Jing Tang. Learning few-step diffusion models by trajectory distribution matching. arXiv preprint arXiv:2503.06674, 2025. [18] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77397751, 2025. [19] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 79377947. PMLR, 2021. [20] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language 35 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language model training on gpu clusters using megatron-lm. performance computing, networking, storage and analysis, pages 115, 2021. In Proceedings of the international conference for high [21] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [22] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. Advances in Neural Information Processing Systems, 37:117340117362, 2024. [23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [24] Runway. https://app.runwayml.com/. [25] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. [26] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv. org/abs/2402.03300, 2(3):5, 2024. [27] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. https://arxiv. org/abs/1909.08053. [28] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [29] Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, et al. Thinking with video: Video generation as promising multimodal reasoning paradigm. arXiv preprint arXiv:2511.04570, 2025. [30] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [31] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. [32] Yujie Wang, Shiju Wang, Shenhan Zhu, Fangcheng Fu, Xinyi Liu, Xuefeng Xiao, Huixia Li, Jiashi Li, Faming Wu, and Bin Cui. Flexsp: Accelerating large language model training via flexible sequence parallelism. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS 25, pages 421436, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400710797. doi: 10.1145/3676641.3715998. https://doi.org/10.1145/3676641.3715998. [33] Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. [34] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [35] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. [36] Tailing Yuan, Yuliang Liu, Xucheng Ye, Shenglong Zhang, Jianchao Tan, Bin Chen, Chengru Song, and Di Zhang. Accelerating the training of large language models using efficient activation rematerialization and optimal hybrid parallelism. In 2024 USENIX Annual Technical Conference (USENIX ATC 24), pages 545561, Santa Clara, CA, July 2024. USENIX Association. ISBN 978-1-939133-41-0. https://www.usenix.org/conference/atc24/ presentation/yuan. 36 Kling-Omni: Omni Video Generation Model from Multi-modal Visual Language [37] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024."
        }
    ],
    "affiliations": [
        "Kuaishou Technology"
    ]
}