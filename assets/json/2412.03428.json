{
    "paper_title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction",
    "authors": [
        "Wanting Zhang",
        "Haodong Xiang",
        "Zhichao Liao",
        "Xiansong Lai",
        "Xinghui Li",
        "Long Zeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 8 2 4 3 0 . 2 1 4 2 : r 2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction Wanting Zhang Haodong Xiang Zhichao Liao Xiansong Lai Xinghui Li Long Zeng Tsinghua University https://valentina-zhang.github.io/2DGS-Room/ Figure 1. 2DGS-Room achieves high-fidelity geometric reconstructions for indoor scenes. We introduce seed points to guide the distribution of 2D Gaussians coupled with geometric constraints, leading to clearer structures and more accurate geometry."
        },
        {
            "title": "Abstract",
            "content": "The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction. 1. Introduction 3D reconstruction from multi-view RGB images is fundamental task in the fields of computer vision and computer graphics. The reconstructed models can be utilized in wide range of applications, including virtual reality, video games, autonomous driving, and robotics. Reconstructing indoor scenes is challenging task in the field of 3D reconstruction, as indoor environments often contain large textureless regions. MVS-based methods [13] often yield incomplete or geometrically flawed reconstructions, primarily due to the geometric ambiguities arising from the presence of textureless regions."
        },
        {
            "title": "Recent advancements",
            "content": "in neural-radiance-field-based methods [48] that utilize signed distance fields (SDF) for scene modeling have enabled accurate and complete mesh reconstruction in indoor environments. This progress is attributed to the continuity of neural SDFs and the integration of monocular geometric priors [6]. Although neuralradiance-field-based methods achieve high-quality reconstruction, they are computationally expensive due to the need for dense ray sampling, resulting in long optimization times. Fortunately, 3D Gaussian Splatting (3DGS) [9] enhances the optimization and rendering efficiency of neural rendering through its differentiable rasterization technique, offering new possibilities for 3D scene reconstruction. 2DGS [10] build upon 3DGS by using 2D-oriented planar Gaussians as primitives, significantly improving surface reconstruction quality. Despite these advances, Gaussian splatting-based methods still often produce floating artifacts and incomplete reconstructions in indoor scenes, due to the lack of structured geometric constraints. In this work, we present novel approach named 2DGSRoom, aiming to achieve high-fidelity geometric reconstruction for indoor scenes based on 2D Gaussian Splatting. Considering the scenes underlying structure, we propose seed-guided mechanism to control the distribution and density of 2D Gaussians. Specifically, we introduce seed-guided initialization to generate 2D Gaussians, ensuring their alignment with scene surfaces to improve geometric accuracy. To further refine the reconstruction, we propose seed-guided optimization strategy that dynamically adjusts seed point density through gradient-guided growth and contribution-based pruning, enabling efficient representation of fine details. Additionally, we incorporate monocular depth and normal priors to provide crucial geometric constraints. The depth prior addresses distortions in detailed areas, while the normal prior ensures accurate surface estimation in textureless regions. Furthermore, we introduce multi-view consistency constraints to address residual artifacts, which enforces both geometric and photometric consistency across multiple views. Extensive qualitative and quantitative experiments show that compared with Gaussian-based methods, 2DGS-Room achieves start-of-the-art performance in indoor scenarios. In summary, our contributions are as follows: We propose 2DGS-Room, novel method for indoor scene reconstruction based on 2DGS, which leverages the seed points maintaining the scene structure to guide the distribution and density of 2D Gaussians. We introduce monocular depth and normal priors to provide geometric cues, improving the reconstruction of detailed areas and textureless regions respectively. We employ multi-view constraints incorporating geometric and photometric consistency to further enhance the reconstruction quality. Our method achieves high-quality surface reconstruction for indoor scenes. Extensive experiments on indoor scene datasets show that our method achieves state-of-the-art in multiple evaluation metrics. 2. Related work 2.1. Multi-View Stereo Multi-view stereo (MVS) methods [1, 1113] estimate the 3D coordinates of pixels and explicitly reconstruct objects and scenes by matching features across collection of posed images. The surface is then obtained through the application of Poisson surface reconstruction [14]. In indoor scenes, particularly in large texture-less regions, these methods frequently encounter difficulties due to the scarcity of features. Voxel-based approaches [1518] optimize spatial occupancy and color within voxel grid, thus avoiding the challenges of feature matching. However, highresolution memory constraints degrade reconstruction quality. Learning-based multi-view stereo methods [2, 3, 1925] implicitly match corresponding multi-view features through neural networks, enabling end-to-end 3D reconstruction. Nonetheless, even with extensive training data, errors may still occur in the results when handling occlusions, complex lighting, or regions with subtle textures. 2.2. Neural Radiance Field Neural Radiance Fields (NeRF) [26] employs multi-layer perceptron (MLP) to model continuous volumetric function of density and color, enabling novel view synthesis through volume rendering. Methods such as Mip-NeRF [2729] enhance rendering quality by improving the ray sampling strategy. Other works [3034] accelerate training and rendering through techniques such as multi-resolution hash encoding or resizing MLPs. Some studies aim to enhance rendering quality by incorporating regularization terms. For example, depth regularization [35, 36] explicitly supervises ray termination to minimize unnecessary sampling time. Other approaches focus on enforcing smoothness constraints on rendered depth maps [37] or utilizing multi-view consistency regularization in sparse-view scenarios [38, 39]. Some research explores the use of alternative implicit functions to enhance the geometric reconstruction capabilities of NeRF, such as occupancy grids [40, 41] and signed distance functions (SDFs) [4, 5, 34, 42, 43], replacing NeRFs volumetric density field. To further enhance reconstruction quality, [44, 45] suggest regularizing optimization with SfM points, while [6, 46] incorporate priors like the Manhattan world assumption and pseudo depth supervision. However, these approaches often lead to incomplete reconstructions and require extensive optimization time. Figure 2. Overview of 2DGS-Room. Given multi-view posed images, we improve 2DGS to achieve high-fidelity geometric reconstruction for indoor scenes. (a) Starting from an SfM-derived point cloud, we generate set of seed points through voxelization, establishing stable foundation for guiding the distribution and density of 2D Gaussians. We further introduce an adaptive growth and pruning strategy to optimize seed points. (b) We incorporate depth and normal priors, addressing the challenges of detailed areas and textureless regions. (c) We introduce multi-view consistency constraints to further enhance the quality of the indoor scene reconstruction. 2.3. Gaussian Splatting 3D Gaussian Splatting [9] explicitly represents 3D scenes using learnable Gaussian primitives, enabling high-quality novel view synthesis with short training times and high rendering frame rates. The 3DGS method is solely responsible for the image loss, and after initializing with sparse point clouds generated by SfM [47], no further constraints are applied to the Gaussian primitives. This leads to disorganized distribution of the optimized Gaussian primitives, resulting in poor geometric properties. Works such as DNSplatter [48], GaussianRoom [49] and GSDF [50] introduce geometric priors or leverage the accurate geometric information from SDFs to supervise the optimization of Gaussians. SuGaR [51], PGSR [52] and RaDe-GS [53] use Flatten Gaussians to represent scenes, enhancing surface reconstruction capabilities. In contrast, 2DGS [10] directly applies 2D oriented planar Gaussians instead of 3D Gaussian primitives to represent 3D scenes, achieving better surface reconstruction results. However, it still encounters poor reconstruction in indoor scenes due to Gaussian primitives lacking geometric constraints. 3. Preliminary The key innovation of 2DGS [10] lies in its transformation of 3D volumetric Gaussians into flat 2D Gaussians, or surfels, for scene representation. It directly models scenes with 2D elliptical disks, simplifying the representation process and yielding more accurate geometry without extra mesh refinement. Each 2D Gaussian disk, defined in local tangent plane, is parameterized by central point pk, two orthogonal tangential vectors tu and tv, and scaling vector (su, sv) that controls the variances along each direction. The normal tw of each Gaussian disk is computed as tw = tu tv and this orientation can be arranged into rotation matrix = [tu, tv, tw]. The scaling factors can be arranged into 3 3 diagonal matrix = [su, sv, 0]. Then 2D Gaussian can be parameterized: (u, v) = pk + sutuu + svtvv = H(u, v, 1, 1), (1) where 4 4 is homogeneous transformation matrix representing the geometry of the 2D Gaussian: = (cid:20)sutu svtv 0 pk 1 0 0 (cid:21) = (cid:21) (cid:20)RS pk 0 . (2) In the Gaussians tangent frame (u, v), the 2D Gaussian value G(u) at point = (u, v) is evaluated as: (cid:18) G(u) = exp u2 + v2 2 (cid:19) . (3) For efficient rendering, each 2D Gaussian is projected onto the image plane by general 2D-to-2D mapping in ho3 mogeneous coordinates. Given world-to-screen transformation matrix W, the screen space points can be derived from: = (xy, yz, z, z) = WH(u, v, 1, 1). (4) where represents homogeneous ray emitted from the camera and passing through pixel (x, y) and intersecting the splat at depth z. To avoid numerical instability, ray-splat intersection is calculated explicitly by finding the intersection of three non-parallel planes in the 3D scene. Given an image coordinate = (x, y), the ray of pixel can be defined by the x-plane the intersection of two homogeneous planes: hx = (1, 0, 0, x) and the y-plane hy = (0, 1, 0, y). To compute the intersection with the Gaussian splat, both planes are transformed to uv-space: hu = (WH)hx, hv = (WH)hy. (5) By homography, the two planes are used to find the intersection point (u(x), v(x)) with the 2D Gaussian splats, given by: h4 h2 uh2 uh1 , v(x) = uh1 h4 uh2 h1 h1 h2 uh4 uh1 , (6) u(x) = uh4 h2 uh2 h1 and hi where hi in the Gaussians tangent frame. are components of the transformed planes 4. Methods Given multi-view posed images, our goal is to optimize 2DGS [10] to accurately reconstruct the geometry of indoor scenes. To this end, we first propose seed-guided mechanism, which leverages seed points to control the distribution and density of 2D Gaussians, thereby improving the accuracy and efficiency of scene representation in indoor scenes (Sec. 4.1). To further improve geometric accuracy, we incorporate depth and normal priors, which enhance the representation of detailed areas and textureless regions, respectively (Sec. 4.2). Finally, to mitigate floating artifacts caused by lighting variations in indoor scenes, we introduce multi-view consistency constraints, further enhancing the quality of the indoor scene reconstruction (Sec. 4.3). An overview of our framework is provided in Fig. 2. 4.1. Seed Points Guidance Existing methods [9, 10] tend to optimize Gaussians relying on each training view, ignoring the underlying structure of the scene. As illustrated in Fig. 3 (a) and (b), the Gaussian primitives fail to align with the surfaces. To overcome this limitation, we propose seed-guided mechanism to control the distribution of 2D Gaussians. Specifically, we utilize set of seed points to provide stable foundation for generating 2D Gaussians, ensuring that the reconstruction reflects 4 the underlying scene structure more accurately. Additionally, we introduce an adaptive growth and pruning strategy to dynamically adjust the density of seed points. Seed-Guided Initialization. Starting from an SfM-derived point cloud RM 3, we first filter some unreliable outliers. We define confidence measure Opi for each individual point pi in the point cloud. This measure is expressed as follows: (cid:40) Opi = 1 0 if ϵ if < ϵ , (7) where represents the number of image feature matches associated with pi, and ϵ is predefined threshold. Points with number of matched features below ϵ are deemed unreliable and removed from the point cloud to ensure more accurate reconstruction. Following the filtering process, we apply voxelization to generate set of seed points RN 3 by selecting the center points of each voxel grid to represent the seed points: = (cid:26)(cid:22) δ (cid:23) (cid:27) δ , (8) where δ denotes the voxel grid size. Each seed point serves as the basis for deriving several 2D Gaussians, which are positioned based on learnable offsets from the seed point. This initialization ensures that the distribution of Gaussians is closely aligned with the underlying geometry of the scene, thereby improving the overall robustness of the reconstruction quality. For each seed point V, we initialize set of 2D Gaussians {Gi,j}, where Gi,j denotes the j-th Gaussian associated with the i-th seed. The position of each Gaussian is determined by learnable offset Oi,j from the seed point location: pi,j = vi + Oi,j, (9) where pi,j R3 represents the global position of the Gaussian, and Oi,j R3 is learnable offset which is optimized during training to adjust each Gaussians local position for better alignment with the scene. Expect for the center position, each 2D Gaussian is parameterized by the scaling R2, rotation R2, appearance R3 and opacity α R. At initialization, the scaling and rotation are aligned with the local geometry derived from the point cloud, which provides starting approximation that reflects the scenes spatial distribution. During training, these parameters are iteratively optimized to refine the representation. Seed-Guided Optimization. In order to capture different levels of detail in complex indoor scenes, we develop an adaptive approach to dynamically adjust seed point density by combining gradient-guided growth and contributionbased pruning. reference depths to address potential inconsistencies that may arise due to relative scaling differences in complex scenes. Then we adjust the predicted depth map to obtain the aligned prediction: ˆDaligned = ˆD + t. The depth loss Ld consists of two terms: data term that minimizes the mean squared error (MSE) between the aligned rendered depths ˆDaligned and the reference depths D, and regularization term for gradient consistency that encourages local smoothness in the depth rendering. Formally, the depth loss is defined as: Ld = (cid:88) 1 Vd ˆDaligned D2 + λgrad Lgrad, (10) where Vd represents the number of pixels with valid depths, and Lgrad is spatial regularization term that penalizes abrupt depth variations across neighboring pixels. Monocular Normal Supervision. Additionally, the normal prior plays crucial role in addressing the reconstruction challenges of textureless or planar regions like walls and floors. So we also incorporate normal supervision to enforce smooth and realistic surface orientation throughout the scene. Let ˆN denote the reference normals derived from pretrained model [56], and represents the rendered normals. We first use the L1 norm loss to quantify the absolute difference in magnitude between the rendered and reference normals, promoting consistency in the length of the vectors: L1 = 1 Vn (cid:88) (cid:12) (cid:12)N ˆN (cid:12) (cid:12) (cid:12) (cid:12) , (11) where Vn is the number of pixels with valid reference normals. To further encourage the alignment of with ˆN , we use cosine similarity loss that penalizes angular differences between the two normal vectors: Lcos = (cid:88) 1 Vn (cid:32) 1 ˆN ˆN (cid:33) . (12) The final normal supervision loss Ln is defined as: Ln = λ1 L1 + λcos Lcos. (13) 4.3. Multi-View Consistency Constraints The strategies outlined above significantly improve the accuracy of indoor scene reconstruction, but we observe that some small floaters may still persist in certain scenarios. These cases are likely caused by the complex lighting variations and subtle spatial structures typical in indoor environments. Therefore, we introduce multi-view consistency constraints to further refine the reconstruction by reducing the inconsistencies that occasionally manifest across different views. Specifically, as shown in Figure 2, given Figure 3. Ground truth scene surface and Gaussian primitives distribution. Compared with 3DGS and 2DGS, our method significantly reduces scattered floaters in the non-surface areas, benefitting from our designed structured geometric constraints. We utilize gradient-guided growth strategy to increase seed point density adaptively, especially in areas with high structural complexity or fine details. For each voxel, we compute the average gradient of the included 2D Gaussians across Ng training iterations, using it as an indicator of structural complexity. When exceeds threshold θg, additional seed points are introduced to enhance representation. This growth occurs within multi-resolution voxel structure, with thresholds that adapt according to the resolution level, ensuring higher seed density in regions requiring more detail. Moreover, we implement contribution-based pruning strategy that selectively removes low-impact seed points. For each seed, we calculate the cumulative opacity αv of the connected 2D Gaussians over Nα iterations. If αv is below predefined threshold θα, the seed point is pruned, as its minimal contribution to scene opacity suggests the limited impact on the overall representation. This strategy allows us to allocate Gaussians to regions of higher structural significance, enhancing both computational efficiency and reconstruction quality. 4.2. Monocular Cues Supervision While the control of seed points enhances the structural consistency of the scene, it remains insufficient for achieving highly accurate geometry, particularly in detailed or textureless regions which are common in indoor environments. Therefore, we incorporate depth and surface normal priors, providing geometric constraints to further improve the scene reconstruction. Monocular Depth Supervision. The depth prior is leveraged to mainly refine the spatial alignment of objects in the scene by aligning the rendered depths with reference depths predicted from pre-trained model [54]. We incorporate depth supervision by aligning the rendered depths with reference depths through scale-and-shift-invariant loss [55], compensating for relative scaling discrepancies that may arise in the representation of complex indoor geometries. Given the rendered depths ˆD, we first compute optimal scale and shift values to minimize discrepancies in scale and translation between our rendered depths and the 5 reference view Vr, we select neighboring view Vn and enforce geometric consistency and photometric consistency between the two views. Geometric Consistency Constraint. To ensure consistent geometry across views, we define pixel-wise geometric consistency loss that penalizes discrepancies in the forward and backward projections for each individual pixel. We compute transformation Hrn to represent the homography matrix mapping pixel pr from Vr to the corresponding pixel pn in Vn: (cid:18) Hrn = Kn Rrn (cid:19) TrnN Dr 1 , (14) where denotes the cameras intrinsic matrix. Rrn and Trn are the relative rotation and translation from the reference frame to the neighboring frame. For each pixel pr , we project it forward from Vr to Vn using Hrn, and then back-project from Vn to Vr using Hnr. The resulting multi-view geometric consistency loss Lgeo is formulated as: Lgeo = 1 Ve (cid:88) prVe pr HnrHrnpr, (15) where Ve is set of valid pixels excluding those with high forward and backward projection errors. Photometric Consistency Constraint. To account for local variations in texture and illumination, we also enforce photometric consistency which is measured using the normalized cross-correlation (NCC) [59], penalizing differences in pixel intensity distributions between the views. Focusing on geometric details, we convert color images into grayscale and the photometric consistency loss Lpho is defined as: Lpho = 1 Ve (cid:88) prVe (1 NCC(Gr(pr), Gn(Hrnpr))) , (16) where Gr and Gn denote the grayscale intensities of the patches in Vr and Vn, respectively. Finally, the total multi-view consistency loss Lmv is given by: Lmv = λgeoLgeo + λphoLpho. (17) 4.4. Optimization In summary, with Lrgb representing the photometric supervision that minimizes the difference between rendered and input images proposed in the original 2DGS, our final training loss is given by: = Lrgb + λd Ld + λn Ln + Lmv, (18) where λd and λn control the relative contributions of depth and normal supervision, respectively. 5. Experiments 5.1. Experimental Setup Dataset. We evaluate the performance of our approach on reconstruction quality across 12 real-world indoor scenes from publicly available datasets: 8 scenes from ScanNet(V2) [57] and 4 scenes from ScanNet++ [58]. Implementation Details. Our training strategy and hyperparameters are consistent with the baseline 2DGS method to ensure comparability. We set = 10, λ1 = 0.01, λcos = 0.01, λgrad = 0.5, λgeo = 0.05, λpho = 0.2, λd = 1.0, λn = 1.0, in all our experiments. We render depth maps for all training views and then adopt TSDF fusion [60] for mesh extraction. We train all models for 30k iterations. All experiments are conducted on an NVIDIA RTX 4090 GPU to ensure consistent processing. Metrics. Consistent with existing methods [5, 6], five standard metrics are employed to evaluate the quality of reconstructed meshes: Accuracy, Completion, Precision, Recall, and F-score. Method Acc. Comp. Prec. Recall F-score Acc. Comp. Prec. Recall F-score ScanNet [57] ScanNet++ [58] NeuS [4] Neuralangelo [34] 3DGS [9] SuGaR [51] 2DGS [10] PGSR [52] RaDe-GS [53] 2DGS-Room (Ours) 0.105 0.185 0.338 0.167 0.157 0.125 0.167 0. 0.124 0.223 0.406 0.148 0.151 0.117 0.205 0.092 0.448 0.252 0.129 0.361 0.336 0.420 0.309 0.648 0.378 0.260 0.067 0.373 0.347 0.433 0.307 0.518 0.409 0.255 0.085 0.366 0.341 0.426 0.306 0.575 0.160 0.363 0.144 0.158 0.359 0.204 0.284 0.262 0.224 0.264 0.990 0.178 0.228 0.202 0.252 0. 0.294 0.172 0.322 0.383 0.230 0.353 0.171 0.450 0.221 0.120 0.066 0.349 0.160 0.217 0.179 0.498 0.251 0.141 0.104 0.361 0.183 0.249 0.166 0.464 Table 1. Quantitative reconstruction comparison on ScanNet and ScanNet++ dataset. Averaged results are reported over 8 scenes and 4 scenes, respectively. 2DGS-Room achieves the best F-score. 6 Figure 4. Qualitative reconstruction comparisons. For each indoor scene, the first row is the top view of the whole room, and the second row is the details of the masked region. Baselines. We compare our approach with several stateof-the-art methods, covering both neural volume rendering and Gaussian splatting techniques. The baselines include (1) Neural volume rendering methods: NeuS [4] and NeuralAngelo [34]; (2) Gaussian splatting methods: 3DGS [9], SuGaR [51], RaDe-GS [53], PGSR [52], and 2DGS [10]. 5.2. Results Analysis Qualitative Results. To show the visualized reconstruction results of our method, we compare our 2DGS-Room with different reconstruction methods, including NeuS [4], SuGaR [51], RaDe-GS [53], PGSR [52], 2DGS [10], and the ground truth. As illustrated in Figure 4, our method ex7 hibits significantly clearer scene structures, which is largely attributed to the seed-guided strategy. Additionally, thanks to the incorporation of depth and normal priors, the overall quality of our reconstructions is noticeably higher. In comparison with Gaussian-based methods, our method obtains more visually coherent and accurate representation of the indoor scenes, with well-defined surfaces and consistent details across different views. Quantitative Results. Quantitative results are presented in Table 1, showing comprehensive comparison in geometry metrics on indoor scene datasets. On the ScanNet dataset, our method achieves the best results in all metrics. Compared to NeRF-based methods [4, 34] which typically require over 20 hours to train scene, our method significantly reduces training time, being approximately 30 times faster. Since our method directly uses 2D Gaussians to represent scene surfaces, allowing the Gaussian splat to better adhere to the surface geometry, it outperforms 3DGS-based methods [9, 51]. Furthermore, while 2DGS [10] and some other methods [52, 53] that employ depth strategies do improve geometric reconstruction quality, they still struggle in indoor scenes due to the complexity of spatial structures and the prevalence of textureless regions. By integrating seedguided strategies and geometric constraints, our method enhances the accuracy of scene structure capture and achieves higher reconstruction quality, resulting in superior metrics. As shown in Fig. 4, some methods [4, 51] produce noisy reconstructions with scattered floaters, and fail to represent the actual surfaces accurately due to the lack of geometric constraints. However, they may cover more ground truth data and thus achieve higher Accuracy than 2DGS on the ScanNet++ dataset in Table 1. Our method improves the structural coherence of the reconstruction, leading to more accurate representation of the scene and significant improvement in the Accuracy metric compared to 2DGS. 5.3. Ablation Studies To assess the individual contributions of each component in our model, we perform ablation studies on the ScanNet dataset. The quantitative results are reported in Table 2 and Figure 5 shows the qualitative results. These allow us to isolate the impact of key elements on the overall reconstruction quality. Method Acc. Comp. Prec. Recall F-score 0.128 w/o Seed w/o Depth 0.084 w/o Normal 0.066 0.055 w/o MV 0.055 Full model 0.152 0.139 0.102 0.092 0. 0.336 0.510 0.596 0.644 0.648 0.284 0.386 0.463 0.508 0.518 0.307 0.438 0.520 0.566 0.575 Table 2. Results of the ablation study on ScanNet dataset. The best results are marked in bold. Seed Points Guidance. Figure 5 shows that without seed points guidance, the scene lacks clear structural organization, leading to significantly inflated and disorganized reconstruction. Adding this module enables our method to better capture the underlying geometric framework of indoor scenes, improving the F-score by 87.3% in Table 2. Monocular Depth Supervision. As shown in Figure 5, removing depth supervision leads to spatial misalignments and unrealistic arrangements. Incorporating depth supervision significantly enhances geometric accuracy, achieving 31.3% F-score increase as reported in Table 2. Monocular Normal Supervision. Removing normal supervision results in surface inconsistencies as shown in Figure 5, with certain planar areas like walls, floors, and doors misaligned. Adding this module improves surface alignment, increasing the F-score by 10.6% in Table 2. Multi-View Consistency Constraints. Figure 5 reveals some Gaussians fail to align with the correct areas with the absence of multi-view constraints. Introducing this component reduces view-dependent inconsistencies to certain degree, further enhancing the reconstruction quality. Figure 5. Qualitative results of ablation study. 6. Conclusion We propose 2DGS-Room, novel method for indoor scene reconstruction based on 2D Gaussian splatting by incorporating structural information from the scene to generate seed points, which guide the local Gaussian distributions. By leveraging geometric priors, we enhance the reconstruction quality of textureless regions and fine details in complex indoor environments. We also utilize multi-view consistency to reduce view-dependent inconsistencies to certain degree. Extensive experiments show our method achieves superior performance compared with existing methods on multiple metrics and various indoor scenes."
        },
        {
            "title": "References",
            "content": "[1] Johannes Schonberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 501518. Springer, 2016. 1, 2 [2] Wenjie Luo, Alexander Schwing, and Raquel Urtasun. Efficient deep learning for stereo matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56955703, 2016. 2 [3] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pages 767783, 2018. 1, 2 [4] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021. 2, 6, 7, 8 [5] Jiepeng Wang, Peng Wang, Xiaoxiao Long, Christian Theobalt, Taku Komura, Lingjie Liu, and Wenping Wang. Neuris: Neural reconstruction of indoor scenes using normal priors. In European Conference on Computer Vision, pages 139155. Springer, 2022. 2, [6] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. Advances in neural information processing systems, 35:2501825032, 2022. 2, 6 [7] Xinghui Li, Yikang Ding, Jia Guo, Xiansong Lai, Shihao Ren, Wensen Feng, and Long Zeng. Edge-aware neural imIn 2023 IEEE International plicit surface reconstruction. Conference on Multimedia and Expo (ICME), pages 1643 1648. IEEE, 2023. [8] Xinghui Li, Yuchen Ji, Xiansong Lai, Wanting Zhang, and Long Zeng. Fine-detailed neural indoor scene reconstruction using multi-level importance sampling and multi-view consistency. In 2024 IEEE International Conference on Image Processing (ICIP), pages 34773483. IEEE, 2024. 2 [9] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4):114, 2023. 2, 3, 4, 6, 7, 8 [10] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. arXiv preprint arXiv:2403.17888, 2024. 2, 3, 4, 6, 7, 8 [11] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan Goldman. Patchmatch: randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24, 2009. [12] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Gipuma: Massively parallel multi-view stereo reconstruction. Publikationen der Deutschen Gesellschaft fur Photogrammetrie, Fernerkundung und Geoinformation e. V, 25 (361-369):2, 2016. [13] Robust Multiview Stereopsis. Accurate, dense, and robust multiview stereopsis. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 32(8), 2010. 2 [14] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics (ToG), 32(3):113, 2013. 2 [15] Adrian Broadhurst, Tom Drummond, and Roberto Cipolla. probabilistic framework for space carving. In Proceedings eighth IEEE international conference on computer vision. ICCV 2001, pages 388393. IEEE, 2001. 2 [16] Jeremy De Bonet and Paul Viola. Poxels: Probabilistic voxelized volume reconstruction. In Proceedings of International Conference on Computer Vision (ICCV), page 2. Citeseer, 1999. [17] Steven Seitz and Charles Dyer. Photorealistic scene International journal of reconstruction by voxel coloring. computer vision, 35:151173, 1999. [18] Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng Cui. Dist: Rendering deep implicit signed distance function with differentiable sphere tracing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20192028, 2020. 2 [19] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 50385047, 2017. 2 [20] Sergey Zagoruyko and Nikos Komodakis. Learning to compare image patches via convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 43534361, 2015. [21] Gernot Riegler, Ali Osman Ulusoy, Horst Bischof, and Andreas Geiger. Octnetfusion: Learning depth fusion from data. In 2017 International Conference on 3D Vision (3DV), pages 5766. IEEE, 2017. [22] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multiview stereopsis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28212830, 2018. [23] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution In Proceedings of multi-view stereo depth inference. the IEEE/CVF conference on computer vision and pattern recognition, pages 55255534, 2019. [24] Zehao Yu and Shenghua Gao. Fast-mvsnet: Sparse-todense multi-view stereo with learned propagation and gaussIn Proceedings of the IEEE/CVF connewton refinement. ference on computer vision and pattern recognition, pages 19491958, 2020. [25] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. arXiv preprint arXiv:2008.07928, 2020. 2 [26] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: 9 Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [27] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58555864, 2021. 2 [28] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Pointnerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54385448, 2022. [29] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Zip-nerf: Anti-aliased Srinivasan, and Peter Hedman. In Proceedings of the grid-based neural radiance fields. IEEE/CVF International Conference on Computer Vision, pages 1969719705, 2023. 2 [30] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. 2 [31] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:1565115663, 2020. [32] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55015510, 2022. [33] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and In European Hao Su. Tensorf: Tensorial radiance fields. Conference on Computer Vision, pages 333350. Springer, 2022. [34] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84568465, 2023. 2, 6, 7, 8 [35] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster trainIn Proceedings of the IEEE/CVF Conference ing for free. on Computer Vision and Pattern Recognition, pages 12882 12891, 2022. 2 [36] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 56105619, 2021. 2 [37] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54805490, 2022. 2 [38] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 90659076, 2023. [39] Yixing Lao, Xiaogang Xu, Xihui Liu, Hengshuang Zhao, et al. Corresnerf: Image correspondence priors for neural radiance fields. Advances in Neural Information Processing Systems, 36, 2024. 2 [40] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35043515, 2020. 2 [41] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 55895599, 2021. 2 [42] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing Systems, 33:24922502, 2020. 2 [43] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems, 34:48054815, 2021. 2 [44] Qiancheng Fu, Qingshan Xu, Yew Soon Ong, and Wenbing Tao. Geo-neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction. Advances in Neural Information Processing Systems, 35:34033416, 2022. 2 [45] Jingyang Zhang, Yao Yao, Shiwei Li, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. Critical regularizations for neural surface reconstruction in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62706279, 2022. 2 [46] Haoyu Guo, Sida Peng, Haotong Lin, Qianqian Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. Neural 3d scene reconstruction with the manhattan-world assumption. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55115520, 2022. 2 [47] Johannes Schonberger and Jan-Michael Frahm. StructureIn CVPR, pages 41044113, 2016. from-motion revisited. [48] Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, and Juho Kannala. Dn-splatter: Depth and normal priors for gaussian splatting and meshing. arXiv preprint arXiv:2403.17822, 2024. 3 [49] Haodong Xiang, Xinghui Li, Xiansong Lai, Wanting Zhang, Zhichao Liao, Kai Cheng, and Xueping Liu. Gaussianroom: Improving 3d gaussian splatting with sdf guidance and monocular cues for indoor scene reconstruction. arXiv preprint arXiv:2405.19671, 2024. 3 [50] Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, and Bo Dai. Gsdf: 3dgs meets sdf for improved rendering and reconstruction. arXiv preprint arXiv:2403.16964, 2024. 3 10 [51] Antoine Guedon and Vincent Lepetit. Sugar: Surfacealigned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. arXiv preprint arXiv:2311.12775, 2023. 3, 6, 7, 8, [52] Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, and Guofeng Zhang. Pgsr: Planar-based gaussian splatting for efficient and high-fidelity surface reconstruction. arXiv preprint arXiv:2406.06521, 2024. 3, 6, 7, 8 [53] Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, and Ping Tan. Rade-gs: Rasterizing depth in gaussian splatting. arXiv preprint arXiv:2406.01467, 2024. 3, 6, 7, 8, 2 [54] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. 5 [55] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. 5 [56] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Estimating and exploiting the aleatoric uncertainty in surface normal estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1313713146, 2021. 5 [57] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. 6, [58] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. 6, 1 [59] Jae-Chern Yoo and Tae Hee Han. Fast normalized crosscorrelation. Circuits, systems and signal processing, 28:819 843, 2009. 6 [60] Brian Curless and Marc Levoy. volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 303312, 1996. 6 11 2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction"
        },
        {
            "title": "Supplementary Material",
            "content": "Training details. For all scenes, our seed-guided optimization is performed between 1,500 and 15,000 iterations. We set Ng = 100 for the gradient-guided growth and Nα = 100 for the pruning strategy. Depth supervision and normal supervision are applied consistently from the first iteration through to the end of training, providing continuous geometric constraints. The multi-view consistency constraint is introduced after 7,000 iterations, once the foundational structure has been established, to further improve view alignment. C. Additional Qualitative Results C.1. Additional Ablation Results To complement the local detail comparisons in the main paper, we provide additional ablation results focusing on the overall scene structure in Figure 6. These visualizations highlight the contributions of key components, including the seed points guidance, monocular depth supervision, and monocular normal supervision. The multi-view consistency constraints are primarily designed to further mitigate floating artifacts in certain scenarios, which have limited impact on the overall structure. Therefore, they are not included in these structural comparisons. Their effectiveness is instead reflected in the qualitative results shown in Figure 5 and the quantitative metrics presented in Table 2 of the main paper. When the seed points guidance strategy is removed, the reconstructed objects appear fused together, with unclear boundaries, compromising the scenes structural clarity. In this supplementary material, we provide the following components: Definitions of the 3D geometry metrics used to evaluate reconstruction quality in Sec. A. Additional details of the datasets, training configuration, and the iteration schedule for key modules in Sec. B. Additional qualitative results, including mesh comparison, ablation results, and rendering comparison in Sec. C. A. Definitions of Eevaluation Metrics We evaluate our method using five widely-used 3D geometry metrics: Accuracy, Completion, Precision, Recall, and F-score, defined in Table 3. These metrics collectively assess the geometric fidelity of the reconstructed point clouds by measuring the alignment between the predicted and ground truth point clouds. Accuracy measures the average distance between reconstructed points and the ground truth, with smaller values indicating better alignment. Completion assesses how well the reconstruction covers the ground truth, where lower values are better. Precision and Recall evaluate the proportion of points within set threshold, with higher values indicating better performance. F-score, the harmonic mean of Precision and Recall, provides balanced measure of reconstruction quality, where higher values reflect superior results. Metric Acc. Comp. Prec. Recall zoF-score Definition meancC(mincC c) meancC (mincC c) meancC(mincC c < .05) meancC (mincC c < .05) 2PrecRecall Prec+Recall Table 3. Definitions of 3D metrics. and are the predicted and ground truth point clouds. B. Additional Implementation Details Datasets. As described in the main paper, the quantitative evaluation metrics are derived from results tested two datasets. Specifically, we select 8 scenes from the scene0085 00, ScanNet dataset scene0114 02, scene0603 00, and 4 scene0616 00, scenes from the ScanNet++ dataset [58]: 8b5caf3398, 8d563fc2cc, 41b00feddb, b20a261fdf. scene0580 00, scene0050 00, scene0721 00, scene0617 00, [57]: Figure 6. Additional qualitative results of ablation study. 1 Without depth supervision, objects exhibit depth misalignments, leading to unrealistic spatial arrangements. Similarly, excluding normal supervision results in uneven surfaces, especially on planar regions like walls, where visible curvature or misalignment artifacts occur. C.2. Additional Qualitative Comparison In addition to the four indoor scenes shown in the main paper, we further include qualitative reconstruction comparison results of the different methods [4, 10, 5153] on additional scenes from ScanNet and ScanNet++. As demonstrated in Figure 7, our method significantly outperforms other approaches in capturing global structures, preserving fine-grained details as well as reducing artifacts in textureless regions. C.3. Rendering Comparison We also provide extensive rendering results comparing our 2DGS-Room with 2DGS across various scenes and viewpoints from the ScanNet and ScanNet++ datasets in Figures 8, 9, and 10. Rendered RGB, depth, and normal maps are shown for visual comparison. Our method achieves significant improvements in the rendering quality of depth and normal maps, showcasing smoother transitions and more accurate surface details. Furthermore, the quality of the RGB images rendered by our method remains robust and shows clear advantages over 2DGS in challenging scenarios, such as handling fine details and varying lighting conditions. This demonstrates the effectiveness of our method in achieving superior geometric reconstructions while maintaining photometric accuracy. Figure 7. Additional qualitative reconstruction comparison. For each indoor scene, the first row is the top view of the whole room and the second row is the details of the masked region. 2 Figure 8. Rendering comparison on the ScanNet dataset (scene0580 and scene0050). 3 Figure 9. Rendering comparison on the ScanNet dataset (scene0085 and scene0617). Figure 10. Rendering comparison on the ScanNet++ dataset (8d563fc2cc and 41b00feddb)."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}