{
    "paper_title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights",
    "authors": [
        "Mathieu Andreux",
        "Breno Baldas Skuk",
        "Hamza Benchekroun",
        "Emilien Biré",
        "Antoine Bonnet",
        "Riaz Bordie",
        "Matthias Brunel",
        "Pierre-Louis Cedoz",
        "Antoine Chassang",
        "Mickaël Chen",
        "Alexandra D. Constantinou",
        "Antoine d'Andigné",
        "Hubert de La Jonquière",
        "Aurélien Delfosse",
        "Ludovic Denoyer",
        "Alexis Deprez",
        "Augustin Derupti",
        "Michael Eickenberg",
        "Mathïs Federico",
        "Charles Kantor",
        "Xavier Koegler",
        "Yann Labbé",
        "Matthew C. H. Lee",
        "Erwan Le Jumeau de Kergaradec",
        "Amir Mahla",
        "Avshalom Manevich",
        "Adrien Maret",
        "Charles Masson",
        "Rafaël Maurin",
        "Arturo Mena",
        "Philippe Modard",
        "Axel Moyal",
        "Axel Nguyen Kerbel",
        "Julien Revelle",
        "Mats L. Richter",
        "María Santos",
        "Laurent Sifre",
        "Maxime Theillard",
        "Marc Thibault",
        "Louis Thiry",
        "Léo Tronchon",
        "Nicolas Usunier",
        "Tony Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, a new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 tops generalist User Interface (UI) benchmarks as well as our new web UI localization benchmark, WebClick. When powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on WebVoyager, striking a Pareto-optimal balance between accuracy and cost-efficiency. To accelerate research advancement in agentic systems, we are open-sourcing both our WebClick evaluation dataset and the Holo1 model weights."
        },
        {
            "title": "Start",
            "content": "Surfer-H Meets Holo1 Cost-Efficient Web Agent Powered by Open Weights 5 2 0 2 3 ] . [ 1 5 6 8 2 0 . 6 0 5 2 : r M. Andreux, B. Baldas Skuk, H. Benchekroun, E. Bire, A. Bonnet, R. Bordie, M. Brunel, P.-L. Cedoz, A. Chassang, M. Chen, A.D. Constantinou, A. dAndigne, H. de La Jonqui`ere, A. Delfosse, L. Denoyer, A. Deprez, A. Derupti, M. Eickenberg, M. Federico, C. Kantor, X. Koegler, Y. Labbe, M.C.H. Lee, E. Le Jumeau de Kergaradec, A. Mahla, A. Manevich, A. Maret, C. Masson, R. Maurin, A. Mena, P. Modard, A. Moyal, A. Nguyen Kerbel, J. Revelle, M. L. Richter, M. Santos, L. Sifre, M. Theillard, M. Thibault, L. Thiry, L. Tronchon, N. Usunier, and T. Wu Company - Alphabetical order Abstract We present Surfer-H, cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 tops generalist User Interface (UI) benchmarks as well as our new web UI localization benchmark, WebClick. When powered by Holo1, Surfer-H achieves 92.2% state-of-the-art performance on WebVoyager, striking Pareto-optimal balance between accuracy and cost-efficiency. To accelerate research advancement in agentic systems, we are open-sourcing both our WebClick evaluation dataset and the Holo1 model weights."
        },
        {
            "title": "1 Introduction",
            "content": "Building AI agents requires designing systems capable of acting in and adapting to dynamic digital environments in real time. In this context, Large Language Models (LLMs) have made remarkable progress in reasoning and problem solving, rivaling or even surpassing human experts in domain-specific tasks [12, 32]. However, in their most fundamental form, LLMs are confined to static, pre-trained world: they cannot act, verify, or access up-to-date information. For instance, they cannot answer questions about current events, book restaurant table, or avoid hallucination [30, 35]. To circumvent their limitations, research has focused on enhancing LLMs with tool-use capabilities, enabling them to execute code snippets [7, 29], query Application Programming Interfaces (APIs) [18, 31], or retrieve information at scale with multi-step reasoning [33, 38, 24, 26]. These systems, often referred to 1 as agents, extend LLMs into more capable virtual assistants [36]. However, their real-world utility remains bounded by the available predefined tools and the engineering effort required to expand them [13]. Approaching this problem from another angle, computer use agents have recently emerged as new paradigm in which agents interact with software directly through Graphical User Interfaces (GUIs) [1, 8, 11, 15, 17, 23, 39], i.e. using the same interface humans are presented with. This approach avoids relying on custom integrations or APIs, opening the door to more adaptable general-purpose agents with higher potential and broader real-world utility. Here we present Surfer-H1, visual web retrieval agent designed to be easily trained through reinforcement learning techniques. Surfer-H comprises three main modules: policy, localizer, and validator, which act in sequence (see Section 2). These modules are compatible with any VLM capable of proposing and evaluating actions. Our agent only uses screenshots from websites and does not require the Document Object Model (DOM) or the accessibility tree of the websites. To deliver the best cost-performance ratio, we introduce Holo1, family of lightweight VLMs specialized in taking and evaluating actions and localizing UI elements. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 is publicly available on Hugging Face2. Localization, the ability to identify the precise coordinates of User Interface (UI) components in screenshot, is core capability for effective web navigation and interaction. Existing benchmarks, such as Screenspot [4, 16, 34] and GroundUI [28], primarily focus on general UI localization across apps and platforms, but are not tailored to the unique challenges of web-based environments. These include complex, dynamic components like calendars and nested menus that frequently appear during web navigation. To fill this gap, we introduce WebClick, new benchmark specifically designed for web localizers, which we make publicly available on Hugging Face3. As detailed in Section 4, WebClick features specialized UI elements representative of the modern web, extracted from human-annotated data and agentic on-policy interactions. Our results show that Holo1 models excel at the aforementioned benchmarks. We evaluate Surfer-H on WebVoyager [11], mainstream benchmark for web retrieval, and compare the performance of Holo1 models against external baselines in Section 5. Our results show that the accuracy and cost-efficiency of the Holo1 models enable Surfer-H to achieve state-of-the-art performance at Pareto optimality."
        },
        {
            "title": "2 Surfer-H",
            "content": "As displayed in Figure 1, Surfer-H relies on three trainable modules: policy, localizer, and validator. The policy proposes actions that are executed sequentially. Web actions are executed in browser by simulating human-like interactions. If the policy generates an action that requires interacting with specific element on webpage, such as like button, it generates textual description of the element, and the localizer provides its 2D coordinates. The policy can also decide that the task is complete and generate textual answer via dedicated action. When the policy emits an answer, it is passed through the validator. The validator generates feedback about the answer and decides whether it is suitable for the user. If the answer is valid, it is returned to the user. Otherwise, the feedback is incorporated to the agents memory, and the agent continues its execution until either completion or reaching time or cost budget. We describe the three modules and the role of the memory in greater detail below. Action Space and Policy Our agent is equipped with small action space: it can click on or type text into particular web components, scroll up and down, wait for page to load, refresh the page, go to given URL, go back, or return an answer. For given task and memory, Surfer-H decides which action to execute using its policy, specialized VLM. Each action is preceded by thought and, if deemed necessary by the agent, note-taking. The thoughts, notes, and actions are stored in natural language so they can be easily interpreted. They are created using chain-of-thought prompting [33] and structured generation [5]. 1https://www.surferh.com 2https://huggingface.co/collections/Hcompany/holo1-683dd1eece7eb077b96d0cbd 3https://huggingface.co/datasets/Hcompany/WebClick 2 Figure 1: Surfer-H operates via screenshots and limited action set. It maintains an internal memory with the task, recent screenshots, and thought history. The policy generates thoughts and selects the next action. If necessary, the localizer refines the coordinates for clicks or typing. The validator filters answers, gives feedback, and decides whether task is complete. Memory The past actions are stored in the agents internal memory, along with the most recent screenshots, thoughts, notes (i.e. information gathered by Surfer-H throughout the episode), and the current browser view. Surfer-H maintains and iteratively updates its internal memory, and uses it to produce one action at each timestep. Localizer For click and write actions requiring an element (e.g. button or search bar), localizer identifies and integrates the elements coordinates into the action. The localizer is specialized UI model optimized for coordinate generation. Validator If Surfer-H believes it has successfully completed the user request, it will call the answer action and produce textual answer with supporting screenshots. The validator will then be summoned to review the generated answer for approval. If approved, the agent run is terminated and the answer is returned to the user. Otherwise, Surfer-H will gather feedback from the validator in its notepad and continue browsing. For each attempt, we set maximum number of steps, at which point it is forced to produce an answer. Models Surfer-H modules can be powered by generalist foundation models or fine-tuned specialists. Note that the different modules can be served through different VLMs, or may rely on single model using different prompting strategies, see Section 5. In Section 3, we describe how we trained Holo1 to be used in any of these modules to deliver the best performance."
        },
        {
            "title": "3.1 Overview",
            "content": "Training Surfer-H means training its constituent modules to perform their required tasks optimally within web-browsing environment. Our goal in training is therefore to imbue our models, the Holo1 family, with deep understanding of complex information on webpages and precise state-to-action mapping. We achieve 3 this using large-scale mixture of diverse datasets designed to capture the breadth and complexity of the modern web. This mixture spans real-world web pages, synthetic UIs, document visualizations, and agentbased behavioral traces. In this way, we encourage models to develop actionable understanding beyond surface-level recognition and enable generalization across broad range of web interfaces. Table 1: Dataset Distribution with mixture group breakdown (tokens in billions)."
        },
        {
            "title": "Mixture Group",
            "content": "Tokens (B) Percentage"
        },
        {
            "title": "Complex Visual Understanding",
            "content": "Behavior Learning Grand Total WebCrawl Open-source datasets WebSynthetic Total"
        },
        {
            "title": "Coordinate Validation\nUI Extraction\nVQA\nTotal",
            "content": "Policy Validator Total 12.19 3.42 0.37 15.98 2.70 5.93 1.52 10.16 4.87 0.46 5.32 38.76 10.87 1.17 50.79 8.59 18.86 4.84 32. 15.48 1.45 16.93 31.46 100."
        },
        {
            "title": "3.2 Data Composition Summary",
            "content": "The different elements of the training mixture described in Table 1 reflect the various model capabilities that are expressed in the localizer, policy and validator modules. The foundation of the training mixture is GUI grounding data and is composed of web-crawled and synthetic pages, labeled for the detection of UI elements based on visual cues, as well as open-source datasets. This data source forms 50.79% of the total tokens and consists mainly of proprietary data, distinguishing our models from those trained solely on public datasets. We enhance our overall dataset mixture with data for Complex Visual Understanding, which covers tasks such as assessing the localizer outputs, extracting the interactable elements from web pages, and Visual Question Answering (VQA). This more specialized data amounts to 32.28% of our training mixture. The third tranche of the mixture comprises data collected from our agents in action, enabling our models to learn from past behavior, and amounts to 16.93% of the overall training data. It contains set of action datasets based on past successful agent traces, which represents 15.48% of training tokens. It allows the model to learn complex patterns of memory management, paired thinking and action generation, and an understanding of states in the context of broader task. Additionally, the mixture also contains examples of evaluation of an agents answer against task based on textual and visual evidence. This represents 1.45% of the tokens used during training."
        },
        {
            "title": "3.3 GUI Grounding",
            "content": "The localizer is essential for bridging the visual and action spaces: it enables the agent to determine the coordinates of an interface element selected for interaction based on visual cues. To be effective, this capability must generalize across the vast diversity of web pages, which vary widely in language, layout, visual style, content density, and interactive complexity. WebCrawl Dataset To address this, we constructed the WebCrawl dataset, large-scale collection of web pages sampled from the public Internet. The HTML content of each page was parsed, and elements that allow for interaction (e.g. click, text input, selection) were extracted. These elements are then mapped to an intent, which ranges from simple text content to high-level intents (e.g., submit search query or open user 4 settings) that require reasoning about UI functionality beyond literal appearance. These abstract action descriptions are synthetically generated using frontier models. We collected click interaction data from 4 million web pages, amounting to 89 million clicks in total. Furthermore, we rely on open datasets such as OS-Atlas [34] to complement and diversify the mixture. Each sample pairs an image and an intent with precise click coordinates, which are used as labels. WebSynthetic Dataset We strategically augment the generalist mixture with proprietary synthetic datasets to address known challenges in UI grounding. These carefully crafted resources include the following: Custom-developed websites with calendars and relevant intents, addressing task known to challenge web agents. Synthetic tables with relational data structures, targeting known weakness in current models that struggle to properly interpret tabular information spread across multiple rows and columns. synthetic dataset focused on icon interpretation on the web, enabling improved recognition and functional understanding of ambiguous UI elements that standard datasets consistently misclassify. These proprietary synthetic sources specifically target failure cases observed in conventional models, serving as adversarial training data that significantly enhances model robustness beyond what is possible with standard datasets."
        },
        {
            "title": "3.4 Complex Visual Understanding",
            "content": "More complex visual understanding capabilities are introduced in the models by training on specialized datasets which enable grounding analysis, precise information extraction, and Visual Question Answering (VQA). Coordinate Validation Data We introduce novel dataset for the judgment of grounding proposition. Here, given triplet consisting of an image, intent, and coordinates, models predict whether the click action aligns with the stated intent. This leverages Set-of-Marks [37] approach to highlight the area of interest and trains models to evaluate the match between textual intent and visual targets. This dataset contains more than 5 million triplets for Coordinate Validation. UI Extraction Data Models are trained to exhaustively extract every clickable, selectable, or inputtable element on page. Given screenshot, the model outputs the (a) location and (b) label of each interactable element. This goes beyond standard Optical Character Recognition (OCR) tasks by emphasizing cues that signal interactivity, such as affordances in fonts, frames, and styling, and encourages models to be exhaustive and non-redundant in extraction. Our UI Extraction dataset contains close to 7 million pages. VQA We use common datasets for visual understanding and question answering; ingesting, remapping, and filtering them to extract their most valuable components. We focus on chart, table, and document understanding, and parts of Cauldron by Huggingface [14], totaling 600,000 images. We enrich the training mix with internal datasets tailored for complex visual understanding. These datasets focus on interpreting charts, dashboards, tables, and dense reports, enabling the extraction of numerical, relational, and scientific information. Together, these datasets contain 150M tokens and 300,000 images, giving our model an advantage over those trained on public visual data alone."
        },
        {
            "title": "3.5 Behavior Learning on Multimodal Traces",
            "content": "Multimodal Traces Data Crucially, our training dataset includes multimodal traces from agent executions. These elements allow the model to bridge the gap between vision and action by representing action messages grounded in visual inputs. They also encourage memory understanding and planning by learning 5 actions as function of past observations and actions. Finally, they represent the grounding of actions in thinking pattern by jointly predicting thought, notes and action pairs: (cid:0)thoughtt+1, notest+1, actiont+1 These sequences train for the exact policy VLM used in the Surfer-H logic, and represent the learning of (cid:1) π (task, {thoughtk, actionk, notesk, screenshott3<k t}) . (1) mapping between memory and action defined in Equation 1. Offline Reinforcement Learning These trajectories teach models to behave as agents: reasoning over long contexts, understanding goals, and predicting the next action based on task history. Following Filtered Behavioral Cloning (FBC) approach, only successful traces are retained in the final mixture. This component is essential for upgrading the model from passive understanding to active, interactive web navigation. Each agent execution contains up to 30 policy steps, resulting in large amount of training tokens. Agent trajectories for this dataset were generated using two task corpora. The first is WebVoyager [11], which comprises 643 tasks on 15 common websites, mostly consumer-facing. The second is new corpus we generated, WebVoyagerExtended, with 15,000 tasks spanning 330 websites. To construct it, we identified websites similar in function, features, and design to those in WebVoyager, then synthetically generated tasks mirroring WebVoyagers style. The introduction of the former traces demonstrates the self-play and self-learning capability of our system, i.e. our agents ability to improve from past executions. The latter extend the robustness of the policy capabilities of the model by promoting data diversity. We investigate the relative impact of self-learning and learning on broader tasks in Section 5.2."
        },
        {
            "title": "3.6 Feedback and Validation Learning",
            "content": "Validation Data Format The ability of Surfer-H to inspect and validate proposed answer before submission is crucial. This is formalized by function (Equation(2)), which takes as input the task, textual answer, and supporting screenshots. The validator outputs boolean indicating task success, along with an explanation justifying the decision. This explanation guides subsequent attempts by Surfer-H. (success, explanation) (task, answer, {screenshott3<kt}) . (2) Learning from Past Validations Similar to Section 3.5, we generate more than 1 million validation input and output pairs, based on real agent executions and answers, on the aforementioned websites. The proposed validation inputs therefore represent realistic agent answers, together with evidence in the form of screenshots gathered on agent trajectories. The output explanations and validation Boolean are generated by frontier VLMs, prompted to evaluate the validity of the answer, and the grounding of the answer in provided screenshots."
        },
        {
            "title": "3.7 Training Strategy",
            "content": "Holo1 models are trained using mixture of text completion and tool call samples, encouraging them to follow instructions, leverage context, and predict actionable outputs in both passive (extraction) and active (interaction) tasks. While data is organized around layered capabilities, we transform each sample into chatlike example with system, user and assistant messages, with one or multiple images per input. Consequently, the training dataset is effectively multi-task and multi-modal chat dataset mixture. Regarding the Holo1 models themselves, we start from Qwen 2.5-VL-Instruct [2] weights that we fine-tune using our proprietary training codebase. Instead of one model per module, each Holo1 model is trained on the entire dataset to cover all module capabilities (policy, localizer, validator), as well as other standard VLM capabilities. In doing so, we allow our models to handle both low-level (localization) and high-level (policy, validation) operations with variable model size. This allows us to measure and control the cost-effectiveness of our agents. We used the ToxiGen dataset [10] to evaluate the toxicity of the Holo1 model outputs. We found that only 2.1% and 1.5% of the responses were flagged, for Holo1-3B and Holo1-7B respectively. As reference, we found that Qwen2.5-VL 3B and 7B score 3.7% and 0.5%, respectively. This suggests that the safety of the initial models was well preserved by the training procedure."
        },
        {
            "title": "4.1 Overview",
            "content": "Localization is key skill for the real-world utility of our VLMs. The ability to identify precise coordinates on UI determines the success of click or write action and thus the capacity to complete task. To assess this capability, we evaluated our Holo1 models on several established localization benchmarks, including Screenspot [4], Screenspot-V2 [34], Screenspot-Pro [16], GroundUI-Web [28], and our own newly introduced benchmark, WebClick, described in Section 4.2. For comparison, we also evaluated current state-of-theart VLMs: UI-TARS [27], and the Qwen-2.5-VL and UGround-V1 families [2, 9]."
        },
        {
            "title": "4.2 WebClick: A Specialized Web Localization Benchmark",
            "content": "We introduce new web localization benchmark called WebClick to accurately measure and track model performance on localization and, by extension, web interaction capabilities. This benchmark follows screenspot-like format, wherein each datapoint includes web screenshot, an instruction, and bounding box that marks the interactive element to be clicked in order to complete the task. The VLM being tested receives screenshot and instruction, and is asked to respond with interaction coordinates; correct answers are those that fall within the bounding box. We carefully curated this benchmark dataset from three sources: (1) data collected by our agents while attempting to solve WebVoyager tasks [11], (2) human interactions with the Web during everyday tasks, and (3) human interactions with calendar interfaces. The calendar data, subset of human interactions with the Web, was deliberately isolated and developed into its own dataset, as we identified calendar navigation as setting in which many contemporary VLMs underperform. Moreover, accurately using and understanding calendars is particularly important for enhancing the practical utility of our agents. Through manual curation, we ensured this benchmark includes challenges commonly observed as points of failure in state-of-the-art models, such as understanding UI conventions or combining textual instruction with visual reasoning. Calendar tasks can be particularly difficult, requiring models to interpret structural elements and account for regional variations in date formats. In total, the benchmark contains 1,639 screenshots from over 100 websites. The benchmark is publicly released under the Apache-2 license, and can be found at https://huggingface.co/datasets/Hcompany/WebClick. Table 2: Click Accuracy (%) across models and benchmarks. Model Screenspot GroundUI WebClick (ours) Avg v1 [4] v2 [34] Pro [16] Web [28] agent calendar human Qwen2.5-VL-3B-Instruct [2] UGround-V1-2B [9] UI-TARS-2B [27] Holo1-3B (ours) Qwen2.5-VL-7B-Instruct [2] UGround-V1-7B [9] UI-TARS-7B [27] Holo1-7B (ours) 82.78 77.12 66.82 85.93 85.53 85.69 84.20 87.42 84.34 79.31 69.39 88. 88.04 84.26 86.70 89.85 7.91 21.32 16.38 23.66 10.12 30.93 23.53 26.06 70.50 78.60 80.75 74.75 78.75 82.70 81.00 78.50 76.26 84.41 78.68 83. 78.47 92.37 90.47 89.77 51.70 50.76 42.05 65.91 59.09 68.75 63.45 72.92 85.07 78.50 70.33 88.80 85.22 84.84 87.03 88.80 65.51 67.15 60.63 73. 69.32 75.65 73.77 76."
        },
        {
            "title": "4.3 Localization Benchmarks Results",
            "content": "Overall, we find that our Holo1 model family outperforms state-of-the-art models of similar size, as reported in Table 2 and Figure 2. Both Holo1-3B and Holo1-7B achieve the highest average localization performance for models of their size, scoring 73.55% and 76.16%, respectively. The Holo1-3B model demonstrates strong performance across both public and internal benchmarks. For example, we observe significant improvement 7 Figure 2: Holo1 as Localizer: comparison against competitors, for external and internal benchmarks. Holo1 models reach state-of-the-art average localization performance at all model scales. over Qwen2.5-VL-3B, UGround-V1-2B and UI-TARS-2B on the human-based elements of WebClick, as well as on all Screenspot variants. Furthermore, Holo1-3B not only achieves the highest average localization performance of the 2B and 3B models, but also outperforms the larger Qwen2.5-VL-7B by 4.23 percentage points, and is competitive with UGround-V1-7B, with less than 0.5 percentage points separating their scores. The Holo1-7B variant improves upon the performance of the Holo1-3B model and maintains its position ahead of its competitors, validating the scalability of the Holo1 training framework. Across two of the three Screenspot benchmarks, and two of the three WebClick datasets, it surpasses UGround-V1-7B. Despite falling slightly behind its competitors on GroundUI, with score of 78.50%, it still achieves the highest average score of 76.19%. In summary, our Holo1 models excel in localization, which stands them in good stead for their use in Surfer-H, which we explore and evaluate in Section 5."
        },
        {
            "title": "5.1 Methodology",
            "content": "WebVoyager Setup We evaluate Surfer-H and external competitors on the WebVoyager benchmark [11], using all 643 tasks from 10 different websites. For date-dependent tasks, we adjust the original dates so that they are always in the future relative to when the benchmark is executed, preventing invalid lookups, such as attempting to book cruise that set sail the month before last. Success is computed as the majority vote 8 Figure 3: Pareto-Optimality of Surfer-H+Holo1. Surfer-H success is plotted against cost for varying maximum allowed attempts before the agent must respond, and different underlying policy modules (Holo1, GPT, Gemini, or Qwen2.5-VL). For BrowserUse [3], OpenAI Operator [23] and Project Mariner [8], we use reported numbers (* superscript). Surfer-H powered by Holo1 models reaches state-of-the-art performance while being the most cost-efficient. from three samples of GPT-4o. Baselines We use BrowserUse [3, 17], Project Mariner [8], and OpenAI Operator [23] as external baselines. It should be noted that these reported scores were We use reported numbers for all of these baselines. computed at different time, with slightly different websites, website contents, and evaluation functions. As baselines for the Holo1 family within Surfer-H, we use the GPT-4 [25] and Qwen2.5 [2] model suites, along with Gemini-Flash-2.0 [7]. Surfer-H Configuration For each task, Surfer-H is permitted up to 30 steps to produce an answer. If no valid response is generated within this limit, final forced response is issued. When response is rejected by the validator, Surfer-H attempts to retry, with maximum of 10 attempts per task. Table 3: Inference Cost Per Model Type, using official external providers costs and internal cost estimation for the Holo1 and Qwen2.5-VL models. Model GPT-4o [21] GPT-4o-mini [22] GPT-4.1 [19] GPT-4.1-mini [20] Gemini-2.0-Flash [6] Holo1-3B Holo1-7B/Qwen2.5-VL-7B-Instruct Qwen2.5-VL-32B-Instruct Cost($) / input tokens Cost($) / output tokens Token count / 1200x1200 image 10 0.6 8 1.6 0.4 0.4 0.6 2 772 25508 772 2348 1290 1280 1280 1280 2.5 0.15 2 0.4 0.1 0.1 0.15 0.5 9 Metrics We track the average WebVoyager success accuracy as function of the average cost per run (i.e. across all 643 tasks) in U.S. dollars. This cost is estimated based on the total usage of agent modules throughout the completion of the task. For modules powered by external APIs, we use the official pricing of the providers. For modules implemented using Holo1 or Qwen2.5-VL models, we rely on internal estimates of inference cost based on model size. The per token prices we use are available in Table 3, along with the typical per image token counts. Both are needed for accurate and meaningful comparisons, especially as external providers have very distinct pricing models. Overall, this cost reflects both task difficulty (harder tasks require more steps and attempts) and model complexity (larger models incur higher inference costs). Metrics are reported as function of the maximum number of attempts executed."
        },
        {
            "title": "5.2 WebVoyager Results",
            "content": "Pareto-Optimality of Surfer-H+Holo1 Figure 3 displays the performance of Surfer-H for different policy modules (Holo1-3B or Holo1-7B) and set validator (GPT-4o). The Localizer selection is reported in Table 5, along with exact benchmark numbers. Overall, Surfer-H+Holo1 agents sit on the Pareto front, both at the 3B and 7B parameter models, and for all attempt values. This guarantees that using any of our models is always optimal. Surfer-H+Holo1-7B achieves score of 92.2% after 10 attempts, on par with Surfer-H+GPT-4.1 at 92.0%, at fraction of the cost ($0.13/task vs $0.54/task). Both Holo1-3B and Holo1-7B outperform Operator and Mariner after just 5 attempts, and match Browser-Uses performance after 10 attempts. Powering SurferH with either Gemini-2.0-Flash or Qwen2.5-VL-7B-Instruct leads to comparable performance (76.9% and 78.2%, respectively, after 10 attempts). Using Qwen2.5-VL-32B-Instruct boosts the score to 85.9%, few points below Holo1-3B (89.7%). Holo1 as Validator As our training mixture includes validation data, we also benchmark Surfer-H using Holo1 for the validator, focusing on Pareto-optimal agents, and compiling scores in 5. This reduces the run cost, but the benchmark score drops by 16 percentage points and 12 percentage points at the 3B and 7B scales, respectively, after 10 attempts. These fully Holo1-powered agents remain close to the Pareto front, but the performance decline, in particular its reduction from 3B to 7B, is significant. This suggests that validation is harder than policy making or localization, or perhaps that it requires more cognitive power, and thus larger models. Performance on Unseen Tasks We investigate how training on agent trajectories affects performance, depending on whether the evaluation tasks overlap with those seen during training. To this end, we create version of our training data mixture that contains agent traces obtained from the WebVoyagerExtended task set only, and use it to train Holo1-7B-WVE, an alternative to Holo1-7B. For this experiment, Surfer-H is allowed 10 attempts and uses GPT-4o as Validator. As shown in Table 4, Holo1-7B-WVE yields 9.5 percentage points boost over its foundation Qwen2.5VL-7B-Instruct. The performance difference between Holo1-7B-WVE and Holo1-7B (4.5 percentage points) illustrates the added benefit of in-domain experience. These results highlight the dual benefits of targeted (in-domain) fine-tuning and broad (cross-domain) exploration. When incorporated into the training of the policy model, these strategies substantially enhance agent performance, laying foundation for reinforcement learning driven by large-scale agent executions. Table 4: Impact of the Training Mixtures. While the maximum performance is achieved with indomain knowledge (Holo1-7B), web navigation benefits from cross-domain exploration (Holo1-7B-WVE). Policy Training Tasks Policy Module WebVoyager Accuracy (%) None WebVoyagerExtended only WebVoyager + WebVoyagerExtended Qwen2.5-VL-7B-Instruct Holo1-7B-WVE Holo1-7B 78.2 87.7 92. 10 Table 5: Surfer-H WebVoyager Performance for various modules configurations and against external agents. The superscript * indicates reported numbers. All others are computed internally."
        },
        {
            "title": "Localizer Validator Attempts",
            "content": "Accuracy (%) Cost ($/task) GPT-4o Holo1-3B GPT-4o GPT-4o-mini Holo1-3B GPT-4o GPT-4.1 Holo1-3B GPT-4o GPT-4.1-mini Holo1-3B GPT-4o Gemini-2.0-Flash Holo1-3B GPT-4o Surfer-H Qwen2.5-VL-7B-Instruct Holo1-3B GPT-4o Qwen2.5-VL-32B-Instruct Holo1-3B GPT-4o Holo1-3B Holo1-3B GPT-4o Holo1-3B Holo1-7B Holo1-7B GPT-4o Operator Mariner BrowserUse Holo1-7B 1 2 5 10 1 2 5 10 1 2 5 10 1 2 5 10 1 2 5 10 1 2 5 10 1 2 5 10 1 2 5 10 1 2 5 10 1 2 5 10 1 2 5 10 68.2 75.2 82.1 84.3 59.6 71.6 80.5 86.1 75.4 84.2 89.7 92.0 69.2 79.8 85.0 88.8 45.0 60.4 73.0 76.9 44.7 59.4 71.3 78.2 59.1 72.5 82.5 85.9 62.9 77.3 87.0 89.7 50.7 63.8 70.8 73.2 69.6 80.8 88.2 92.2 55.4 66.3 75.7 80.4 87.0* 83.5* 89.1* 0.20 0.31 0.51 0.71 0.18 0.30 0.53 0.74 0.17 0.26 0.40 0.54 0.09 0.13 0.20 0.26 0.04 0.06 0.10 0.14 0.06 0.09 0.14 0.20 0.09 0.14 0.22 0.30 0.05 0.07 0.09 0.11 0.01 0.02 0.03 0.04 0.05 0.07 0.10 0.13 0.02 0.03 0.05 0."
        },
        {
            "title": "6 Conclusion",
            "content": "Surfer-H and Holo1 exemplify how powerful and cost-efficient web agents can be constructed on top of foundation models by tightly integrating modular architecture, targeted training strategies, and rich and diverse data mixture. By directly interacting with the web through the browser GUI, Surfer-H offers broad applicability across real-world tasks without the need for domain-specific integrations. The introduction of the Holo1 models, highly specialized vision models trained to serve as Surfer-H modules, enables both high performance and cost-effective deployment. Holo1 achieves state-of-the-art on both well-established and the newly introduced WebClick localization benchmarks. Integrated into Surfer-H, these models lead to Pareto-optimal performance on the WebVoyager agentic benchmark. We hope that the release of our model weights and benchmark data will help catalyze future advances in agent research."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku. https://www. anthropic.com/news/3-5-models-and-computer-use, 2024. Accessed: May 23, 2025. [2] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-VL Technical Report, 2025. [3] Browser Use Team. Browser use: Sota technical report. https://browser-use.com/posts/ sota-technical-report, 2024. Accessed May 23, 2025. [4] K. Cheng, Q. Sun, Y. Chu, F. Xu, L. YanTao, J. Zhang, and Z. Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93139332, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. [5] Y. Dong, C. F. Ruan, Y. Cai, R. Lai, Z. Xu, Y. Zhao, and T. Chen. Xgrammar: Flexible and efficient structured generation engine for large language models. arXiv preprint arXiv:2411.15100, 2024. [6] Google Deepmind. Gemini 2.0 Flash Docs. https://ai.google.dev/gemini-api/docs/pricing# gemini-2.0-flash. Accessed: May 23, 2025. [7] Google DeepMind. Introducing gemini 2.0: our new ai model for the agentic era. https://blog.google/ technology/google-deepmind/google-gemini-ai-update-december-2024/, December 2024. Accessed: May 23, 2025. [8] Google Deepmind. Project mariner: agents that can help you accomplish complex tasks. https: //blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/ #agents-for-developers, 2024. Accessed May 23, 2025. [9] B. Gou, R. Wang, B. Zheng, Y. Xie, C. Chang, Y. Shu, H. Sun, and Y. Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. [10] T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, and E. Kamar. Toxigen: large-scale machinegenerated dataset for adversarial and implicit hate speech detection. arXiv preprint arXiv:2203.09509, 2022. [11] H. He, W. Yao, K. Ma, W. Yu, Y. Dai, H. Zhang, Z. Lan, and D. Yu. WebVoyager: Building an endto-end web agent with large multimodal models. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68646890, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. 12 [12] D. M. Katz, M. J. Bommarito, S. Gao, and P. Arredondo. Gpt-4 passes the bar exam. Philosophical Transactions of the Royal Society A, 2024. [13] J. Y. Koh, R. Lo, L. Jang, V. Duvvur, M. C. Lim, P.-Y. Huang, G. Neubig, S. Zhou, R. Salakhutdinov, and D. Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. [14] H. Laurencon, L. Tronchon, M. Cord, and V. Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. [15] LaVagueAI. Lavague: Web agent framework for builders. https://docs.lavague.ai/en/latest/, 2025. Accessed: May 23, 2025. [16] K. Li, Z. Meng, H. Lin, Z. Luo, Y. Tian, J. Ma, Z. Huang, and T.-S. Chua. ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use. arXiv preprint arXiv:2504.07981, 2025. [17] M. Muller and G. ˇZuniˇc. Browser use: Enable ai to control your browser. https://github.com/ browser-use/browser-use, 2024. [18] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2022. [19] OpenAI. Gpt-4.1 documentation. https://platform.openai.com/docs/models/gpt-4.1. Accessed: May 23, 2025. [20] OpenAI. Gpt-4.1-mini documentation. https://platform.openai.com/docs/models/gpt-4.1-mini. Accessed: May 23, 2025. [21] OpenAI. Gpt-4o documentation. https://platform.openai.com/docs/models/gpt-4o. Accessed: May 23, 2025. [22] OpenAI. Gpt-4o-mini documentation. https://platform.openai.com/docs/models/gpt-4o-mini. Accessed: May 23, 2025. [23] OpenAI. https://openai.com/index/introducing-operator/. OpenAI Blog, Jan. 2025. [24] OpenAI. Introducing deep research. https://openai.com/index/introducing-deep-research/, Feb. 2025. [25] OpenAI et al. GPT-4 Technical Report, 2024. [26] Perplexity Team. Introducing perplexity deep research. https://www.perplexity.ai/hub/blog/ introducing-perplexity-deep-research, 2025. Accessed May 23, 2025. [27] Y. Qin, Y. Ye, J. Fang, H. Wang, S. Liang, S. Tian, J. Zhang, J. Li, Y. Li, S. Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [28] C. Rawles, S. Clinckemaillie, Y. Chang, J. Waltz, G. Lau, M. Fair, A. Li, W. E. Bishop, W. Li, F. Campbell-Ajala, D. K. Toyama, R. J. Berry, D. Tyamagundlu, T. P. Lillicrap, and O. Riva. Androidworld: dynamic benchmarking environment for autonomous agents. In The Thirteenth International Conference on Learning Representations, 2025. [29] Replit Team. Meet replit ghostwriter, your partner in code. https://blog.replit.com/ghostwriter, October 2022. Accessed: May 23, 2025. [30] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko. Object hallucination in image captioning. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 40354045, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. 13 [31] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv:2302.04761, 2023. [32] Q. Shi, M. Tang, K. Narasimhan, and S. Yao. Can language models solve olympiad programming? arXiv preprint arXiv:2404.10952, 2024. [33] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [34] Z. Wu, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, K. Cheng, Z. Ding, L. Chen, P. P. Liang, and Y. Qiao. OSATLAS: Foundation action model for generalist GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. [35] Y. Xiao and W. Y. Wang. On hallucination and predictive uncertainty in conditional language generation. In P. Merlo, J. Tiedemann, and R. Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 27342744, Online, Apr. 2021. Association for Computational Linguistics. [36] F. F. Xu, Y. Song, B. Li, Y. Tang, K. Jain, M. Bao, Z. Z. Wang, X. Zhou, Z. Guo, M. Cao, M. Yang, H. Y. Lu, A. Martin, Z. Su, L. Maben, R. Mehta, W. Chi, L. Jang, Y. Xie, S. Zhou, and G. Neubig. Theagentcompany: Benchmarking llm agents on consequential real world tasks, 2025. [37] J. Yang, H. Zhang, F. Li, X. Zou, C. Li, and J. Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [38] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv preprint arXiv:2210.03629, 2023. [39] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, U. Alon, and G. Neubig. WebArena: realistic web environment for building autonomous agents. ICLR, 2024."
        }
    ],
    "affiliations": [
        "Company"
    ]
}