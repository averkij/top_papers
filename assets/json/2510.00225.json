{
    "paper_title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks",
    "authors": [
        "Yue Meng",
        "Fei Chen",
        "Chuchu Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO"
        },
        {
            "title": "Start",
            "content": "TGPO: TEMPORAL GROUNDED POLICY OPTIMIZATION FOR SIGNAL TEMPORAL LOGIC TASKS Yue Meng Massachusetts Institute of Technology {mengyue,feic,chuchu}@mit.edu Fei Chen Chuchu Fan 5 2 0 2 0 ] . [ 1 5 2 2 0 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Learning control policies for complex, long-horizon tasks is central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO"
        },
        {
            "title": "INTRODUCTION",
            "content": "Signal Temporal Logic (STL) is powerful framework for specifying tasks with temporal and spatial constraints in real-world robotic applications. However, designing controllers to satisfy these specifications is difficult, especially for systems with complex dynamics and long task horizon. While Reinforcement Learning (RL) excels in handling these dynamical systems, directly deploying RL for STL specifications poses significant challenges. The history-dependent nature of STL breaks the Markovian assumption for the common RL algorithms. Furthermore, the reward based on the STL satisfaction is extremely sparse for long-horizon tasks, making RL struggle to learn effectively. Existing model-free RL approaches for STL tasks typically leverage state augmentation with reward shaping. τ -MDP (Aksaray et al., 2016) encodes histories explicitly in the augmented spaces and F-MDP (Venkataraman et al., 2020) designs flags to bookkeep the satisfaction of STL subformulas. However, these techniques only work on limited STL fragments with up to two temporal layers. While model-based RL (Kapoor et al., 2020; He et al., 2024) has fewer restrictions on the STL formulas, learning the system (latent space) dynamics can be challenging, and the estimation error accumulates over long horizons. Additionally, the planning often relies on Monte Carlo Tree Search or sampling action sequences, which may not be tractable for high-dimensional systems. We argue that the primary barrier for RL to efficiently solve STL tasks is the difficulty of designing dense, stage-wise reward function. This challenge stems directly from the unspecified temporal variables governing the reach-type tasks in STL formulas, which prevents direct decomposition of STL into sequence of executable subgoals. For example, for an STL F[0,160]A F[0,160]B 1 (Eventually reach and eventually reach within the time interval [0, 160]), the time assignments for reaching and reaching determine the order of visiting these regions. If we can ground the variables into concrete values (e.g., reach at 35, and reach at 120), the problem can be cast into sequence of goal-reaching problems, which is much easier to solve by RL. Inspired by this observation, we propose hierarchical RL framework to solve STL tasks by iteratively conducting Temporal Grounding and Policy Optimization (TGPO). The high-level component assigns values for the time variables to form the sequenced subgoals, and the low-level time-conditioned policy learns to achieve the task guided by the dense, stage-wise rewards derived from these subgoals. To efficiently bind values for multiple time variables, we carry out high-level temporal search with critic that predicts STL satisfaction. MetropolisHastings sampling is used to guide exploration toward more promising time allocations. During inference, we sample time variable assignments and evaluate them using the critic. The most promising schedule is then executed by the low-level policy to generate the final solution trajectory for the STL specification. We conduct extensive experiments over five simulation environments, ranging from 2D linear dynamics to 29D Ant navigation tasks. Compared to other baselines, TGPO* (with Bayesian time variable sampling) achieves the highest overall task success rate. The performance gains are significant, especially in high-dimensional systems and long-horizon tasks. Furthermore, our time-conditioned design offers key benefits: our critic offers interpretability by identifying promising temporal plans, and the policy can generate diverse, multi-modal behaviors to satisfy single STL specification. Our main contributions are summarized as follows: (1) Hierarchical RL-STL framework: To the best of our knowledge, we are the first to develop hierarchical model-free RL algorithm capable of solving general, nested STL tasks over long horizons. (2) Critic-guided Bayesian sampling: We introduce critic-guided temporal grounding mechanism that, together with STL decomposition, yields subgoals and invariant constraints. This mechanism constructs an augmented MDP with dense, stage-wise rewards and thus overcomes the sparse reward challenges that have hindered existing RL approaches. (3) Interpretability: By explicitly grounding subgoals and invariant constraints in the STL structure using critic-guided Bayesian sampling, our approach offers more interpretable learning process, where progress can be directly traced to logical task components. (4) Complex dynamics and reproducibility: TGPO demonstrates strong performance over other baselines and fits for complex dynamics, which supports the effectiveness of the design. All the code (the algorithm, the simulations and STL tasks) will be open-sourced to advance STL planning."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 SIGNAL TEMPORAL LOGIC TASKS Signal Temporal Logic (STL) offers powerful framework for specifying robotics tasks (Donze, 2013). Unlike Linear Temporal Logic (LTL), STL operates over continuous signals with time intervals and lacks an automaton representation, making it challenging to conduct planning (Finucane et al., 2010). Traditional approaches for STL include sampling-based methods (Vasile et al., 2017; Karlsson et al., 2020; Linard et al., 2023; Sewlia et al., 2023), Mixed-integer Programming (Sun et al., 2022; Kurtz & Lin, 2022) and trajectory optimization (Leung et al., 2023). More recently, learning-based methods emerged, such as differentiable policy learning (Liu et al., 2021; 2023; Meng & Fan, 2023), imitation learning (Puranic et al., 2021; Leung & Pavone, 2022; Meng & Fan, 2024; 2025), and reinforcement learning (RL) (Liao, 2020). 2.2 REINFORCEMENT LEARNING FOR TEMPORAL LOGIC TASKS Temporal logic RL has been extensively studied in Linear Temporal Logic (LTL) and some Signal Temporal Logic (STL) fragments (Liao, 2020), where the key challenge is designing suitable rewards. For LTL, existing methods (Sadigh et al., 2014; Li et al., 2017; Hasanbeig et al., 2018; 2020) typically convert the formula into Limit-Deterministic Buchi Automata (LDBA) (Sickert et al., 2016) or reward machines (Icarte et al., 2018), while LTL2Action (Vaezipoor et al., 2021) uses progression (Bacchus & Kabanza, 2000) to assign dense reward, and SpectRL (Jothimurugan et al., 2019) devises composable specification language for complex objectives. In contrast, STL poses additional challenges due to its explicit time constraints and real-value predicates. Early approaches augment the state space via temporal abstractions using history segments (Aksaray et al., 2016; 2 Ikemoto & Ushio, 2022) or flags (Venkataraman et al., 2020; Wang et al., 2024), while bounded horizon nominal robustness (BHNR) (Balakrishnan & Deshmukh, 2019) offers intermediate reward approximations. Recent work uses model-based learning to solve STL tasks with evolutionary strategies (Kapoor et al., 2020) and Monte-Carlo Tree Search in value function space (He et al., 2024). However, most of these methods are restricted to STL structures and systems (limited temporal nesting, fixed-size time windows, or grid-like environments). Instead, our method can handle more general STLs and efficiently designs augmented states along with dense, stage-wise rewards."
        },
        {
            "title": "3.1 SIGNAL TEMPORAL LOGIC (STL)",
            "content": "Consider discrete-time system xt+1 = (xt, ut) where xt Rn and ut Rm denote the state and control at time t. Starting from an initial state x0, signal σ = x0, ..., xT is generated via controls u0, ..., uT 1. STL specifies properties via the following rules (Donze et al., 2013): ϕ ::= µ(x) 0 ϕ ϕ1 ϕ2 ϕ1U[a,b]ϕ2. (1) Here the boolean-type operators split by are the building blocks to compose an STL: means true, µ denotes predicate function Rn R, and , , , [a,b] are negation, conjuction, until and the time interval from to b. Other operators are disjunction: ϕ1ϕ2 = (ϕ1ϕ2), eventually: F[a,b]ϕ = U[a,b]ϕ and always: G[a,b]ϕ = F[a,b]ϕ. We denote σ, = ϕ if the signal σ from time satisfies the STL formula (the evaluation of ϕ returns True). In particular, we simply write σ = ϕ if the signal is evaluated from = 0. For operators , µ 0, , and , the evaluation checks for the signal state at time t. As for temporal operators (Maler & Nickovic, 2004): σ, = F[a,b]ϕ [t+a, t+b], σ, = ϕ; and σ, = G[a,b]ϕ [t+a, t+b], σ, = ϕ; and σ, = ϕ1U[a,b]ϕ2 [t + a, + b], σ, = ϕ2, [0, t], σ, = ϕ1. In plain words, ϕ1U[a,b]ϕ2 means ϕ1 holds until ϕ2 happens in [a, b]. Robustness score (Donze & Maler, 2010) ρ(σ, t, ϕ) measures how well signal σ satisfies ϕ. We have ρ 0 iff σ, = ϕ. The score ρ is: ρ(σ, t, µ) = µ(σ(t)), ρ(σ, t, ) = 1, ρ(σ, t, ϕ1 ϕ2) = min{ρ(σ, t, ϕ1), ρ(σ, t, ϕ2)}, ρ(σ, t, F[a,b]ϕ) = sup r[a,b] ρ(σ, + r, ϕ), (cid:26) ρ(σ, t, ϕ1U[a,b]ϕ2) = sup t[t+a,t+b] min ρ(σ, t, ϕ2), inf t[t,t] ρ(σ, t, ϕ1) (cid:27) . ρ(σ, t, ϕ) = ρ(σ, t, ϕ), ρ(σ, t, G[a,b]ϕ) = inf ρ(σ, + r, ϕ), (2) r[a,b] 3.2 MARKOV DECISION PROCESS Markov Decision Process (MDP) is defined by the tuple = (S, A, P, R, γ) where: and represent the sets of states and actions, respectively, : [0, 1] is the probabilistic transition function where (ss, a) denotes the probability of the next state given current state and action a, : is the reward function, and γ [0, 1) is the discount factor. The agent decision is made by policy π : which maps states to probability distribution over actions. The objective is to find an optimal policy π that maximizes the expected discounted cumulative (cid:20) (cid:80) t=0 γtR(st, at)(cid:12) (cid:12)s0 (cid:21) with at π(st) and st+1 (st, at). reward from starting state s0: Eπ 3.3 PROBLEM FORMULATION Consider discrete-time system with state space , control space and the initial state set X0. Given an STL formula ϕ defined in Eq. 1, our objective is to first formulate an MDP (S, A, P, R, γ) and then learn policy: π : to maximize the satisfaction probability, max (σ = ϕ). x0X0 π Remarks. It is tempting to treat the control system state as the MDP state S, and the control input as the actions A. However, for STL tasks, the policy also depends on the history1, making 1E.g., if an STL task is to Eventually reach region and then reach B, the policy needs to remember whether it has already visited the region in order to proceed to reach B. 3 the problem non-Markovian. Thus, we need to augment the state to keep history data. Besides, the satisfaction of an STL is checked over the full trajectory, making it difficult to define dense rewards (unlike LTL, where stage-wise rewards (Camacho et al., 2017; Vaezipoor et al., 2021) can be defined). Thus, we need to design dense rewards under the augmented state space to learn efficiently."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "We propose TGPO, Temporal Grounded Policy Optimization, to address the problem considered. The entire framework is illustrated in Fig. 1, and we explain each component in detail below. Figure 1: Framework: STL decomposition and critic-guided temporal grounding yield subgoals and invariant constraints that guide an augmented MDP with dense rewards for policy optimization. 4.1 STL SUBGOAL DECOMPOSITION Our method of decomposing STL into subgoals with invariant constraints is inspired by Kapoor et al. (2024); Liu et al. (2025). The essence is to first translate the STL into set of subtasks, where each subtask has checker µ on the trace σ and belongs to one of the following types: Reachability task: achieve µ(σ(τ )) 0 at time instant τ , denoted as Reach(µ, τ ). Invariance task: keep µ(σ(τ )) 0 for all time τ in an interval , denoted as Inv(µ, ). For basic STL formulas, the time instants and the time intervals can be concrete values or variables: e.g., the formula G[a,b]µ can be written as Inv(µ, [a, b]) with concrete [a, b], whereas the formula F[a,b]µ can be written as Reach(µ, τ ) with the time variable τ [a, b], and µ1U[a,b]µ2 can be written as {Reach(µ2, τ ), Inv(µ1, [a, τ ])} with the time variable τ [a, b]. For nested STL, we follow top-down approach to flatten it into reachability and invariance tasks governed by time variables. We denote Reach(ϕ, τ ) for ρ(σ, τ, ϕ) 0 and use Inv(ϕ, ) to represent ρ(σ, τ, ϕ) 0, τ . For any STL ϕ we can write it as Reach(ϕ, 0) and then we rewrite with tasks using its subformulas. The subformula will always carry time variables from its ancestor operators, and we repeat the process until all the tasks are represented as atomic propositions (APs) corresponding to µ or its negation µ. For example, for ϕ = F[a,b]ϕ0 G[c,d]µ0 where ϕ0 = µ1 G[a2,b2]µ2 F[a3,b3]µ3 is subformula, we can represent ϕ as {Reach(ϕ0, τ ), Inv(µ0, [c, d])} with domain {τ [a, b]}, then we can pass τ into ϕ0 to represent the STL as {Reach(µ1, τ ), Inv(µ2, [τ +a2, τ +b2]), Reach(µ3, τ + τ ), Inv(µ0, [c, d])} with domains {τ [a, b], τ [a3, b3]}. An illustration of the decomposition is depicted in Fig. 2. In this work, we do not consider disjunctions or temporal structures of the form G(F . . .). Such STLs can be represented by introducing additional binary variables to select the disjunction branch and more time variables for each instant in the time domain of the operator. From the reachability and invariance tasks, we further denote subgoals (reach or stay) as tasks that are either reachability task (e.g., Subgoal 1 in Fig. 2) or an invariance task (e.g., Subgoal 2 in 4 Fig. 2) with atomic proposition µ (we assume all the APs are for reaching certain regions). The remaining invariance tasks associated with negation of APs (e.g., Inv(µ0, [c, d])) are treated as invariant constraints (avoidance). Through this decomposition, complex STL formula ϕ reduces to Ng subgoals ϕg , Wi), Θg := {1, , Ng} and Nc invariant constraints ϕc j, Wj), Θc := {1, , Nc}. Each subgoal / constraint has starting time and an ending time [t, t] which is [τ, τ ] (or ). Denote all the time variables in this STL as t. Next, we will show how this decomposition guides our state augmentation and reward shaping. with Reach(µg , τi) or Inv(µg with Inv(µc Time variables = (τ , τ ) AP Starting time Ending time Task Subgoal 1 µ1 Subgoal 2 µ2 Subgoal 3 µ3 Invariant µ0 τ τ + a2 τ + τ τ τ + b2 τ + τ Time variable τ [a, b] Reach(ϕ, 0) Reach(ϕ0, τ ) Inv(µ0, [c, d]) Invariant constraint Time variable τ [a3, b3] Reach(µ1, τ ) Subgoal Inv(µ2, [τ + a2, τ + b2]) Subgoal 2 Reach(µ3, τ + τ ) Subgoal 3 Figure 2: STL decomposition of ϕ = F[a,b](µ1 G[a2,b2]µ2 F[a3,b3]µ3) G[c,d]µ0. 4.2 TEMPORAL GROUNDED STATE AUGMENTATION AND REWARD DESIGN Given concrete time variables assignment t, the problem is now structured as reaching sequence of subgoals sorted by their starting time with invariant constraints satisfied during execution. For brevity, we assume the subgoal indices are already sorted. We augment our state as: = (x, τ, pprev, p, r, χ) (3) Here Rn stands for the original state, τ {0, 1, , } represents the time index, {0, 1, , Ng} represents the progress index and pprev records the previous progress, records the certificate to proceed to the next subgoal, χ {0, 1}Nc maintains the satisfaction status for the invariant constraints. For the k-th subgoal (or invariant constraint), denote the starting time tg (or k) and the ending time tg tc (cid:26)x = (x, u), k). The augmented state transition can be written as: = + 1(r = 2) = χk 1((tc χ prev = p, µc = h(r, x, τ , p), k(x) < 0)) = 0, 1, ..., Nc (or tc τ = τ + 1, τ tc (4) where: h(r, x, τ , p) = 0, 1, 2, r, if = 2 = tg if tg if (r = 1 tg otherwise τ = tg = tg µg p) (τ = tg p(x) 0 µg p(x) 0) (5) The variable acts as certificate (or flag) that keeps track of whether the reach-and-stay (F G) condition has been satisfied. It encodes the progress toward establishing that the predicate holds both at the entry time and the exit time of the required interval. To guide the agent to achieve these subgoals in proper time window while satisfying the invariant constraints, we design the reward: R(s) = λ1Rdist + λ2Rprogress + λ3Rsuccess + λ4Rinv (6) where Rdist = µg p(x) is distance-based reward shaping to encourage the agent to reach the current p]), Rprogress = 1(pprev = p) subgoal (and stay at the current subgoal within the time window [tg encourages the agent to achieve more subgoals, Rsuccess = 1(p = Ng χ = 1) encourages the agent to finish all subgoals without violating any invariant constraints, and Rinv = 1(χk = 0) penalizes for violating invariant constraints. The robustness score is also used at the final time step to encourage the agent to satisfy the STL. In this way, the agent is incentivized to reach all the subgoals while obeying the invariant constraints. We use Proximal Policy Optimization (PPO) (Schulman et al., 2017) to train the agent. The policy network and the critic receive the augmented state and the time variable assignment as the input, and output the action and the critic value correspondingly. At p, tg the beginning of each training epoch, we sample the time variables and collect episodes to update the network parameters. During inference, we sample time variables and use the trained critic to find the most effective assignment. The most naive way to sample these time variables will be randomly sampling from their feasible intervals, but we will present better solution in the following section."
        },
        {
            "title": "4.3 CRITIC-GUIDED BAYESIAN TIME ALLOCATION",
            "content": "The key challenge in our framework is efficiently searching for time variable assignments. naive uniform sampling strategy might waste huge effort on assignments that lead to infeasible or lowreward trajectories. To address this, we propose Bayesian sampling strategy to find promising time assignments. We do not need to learn an extra surrogate function, as the value function learned by the PPO agent already provides powerful heuristic. We employ Metropolis-Hastings (MH) algorithm to sample time variables from exp(Vψ(s0, t)) for initial state s0. The MH performs guided random walk over the discrete time variable space and prefers to stay in regions that yield high critic values. To mitigate the risk of the sampler converging to local optima and the fact that the initial critic might not be accurate, we adopt hybrid approach: In each epoch, we use an MH sampler to obtain ratio ηmcmc of the time variables and sample ratio ηuniform through uniform sampling. To further leverage knowledge across training epochs, we maintain replay buffer containing the top ηelite ratio of elite time variable assignments that yield the highest STL robustness scores. This combination creates robust and efficient mechanism for discovering effective temporal plans. The full training procedure is detailed in Algo. 1, and the ablation study for each component is shown in Sec. 5.4. Algorithm 1 TGPO with Hybrid Time Variable Sampling 1: Input: STL formula ϕ (subgoals and invariant constraints), elite buffer size K, batch size NB 2: Initialize policy πθ(as, t), critic Vψ(s, t), and elite time variable buffer 3: for iteration = 1, . . . , do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: return Trained policy πθ, critic Vψ, and elite buffer B. Tuniform Sample ηuniformNB time variables uniformly from the valid domain Tmcmc Run Metropolis-Hastings guided by Vψ to generate ηmcmcNB time variables. Telite Top ηeliteNB time variables from elite buffer Tbatch Tuniform Tmcmc Telite Collect trajectories Di = {(σj, ρϕ Update πθ and Vψ using the PPO algorithm on Di Update with time variables from Di corresponding to top-K STL robustness score , tj)} by executing πθ with time variables from Tbatch 1. High-level Temporal Grounding 2. Low-level Policy Optimization"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 IMPLEMENTATION DETAILS Baselines. We consider the following approaches. RNN: Train RL with recurrent neural network (RNN) to handle history and use the STL robustness score as the rewards. CEM: Cross-Entropy Method (De Boer et al., 2005) that optimizes the policy network with the STL robustness score as the fitness score. Grad: gradient-based method (Meng & Fan, 2023) that trains the policy with differentiable STL robustness score. τ -MDP: An RL method (Aksaray et al., 2016) which augments the state space with trajectory segment to handle history data. F-MDP: An RL approach (Venkataraman et al., 2020) that augments the state space with flags. We denote our base algorithm as TGPO and the enhanced version with Bayesian time sampling as TGPO*. Benchmarks. We evaluate TGPO across five environments shown in Fig. 3 with varying dynamics and dimensionality: (1) Linear: 2D point-mass linear system. (2) Unicycle: non-holonomic 4D system for wheeled robot. (3) Franka Panda: 7-DoF robot arm. (4) Quadrotor: 12D, full dynamic model of quadrotor. (5) Ant: 29D quadruped robot for locomotion tasks. The agent starts from an initial set, and we specify the regions that the agent needs to reach, stay, or avoid using STL. For each benchmark, we designed 10 STL tasks of varying difficulty. Five of these STLs 6 (a) Linear (b) Unicycle (c) Franka Panda (d) Quadrotor (e) Ant Figure 3: Simulation benchmarks. are two-layered (e.g., F[0,T ]G[0,5](Reach A)), solvable by all the methods. The rest are multi-layer STLs with deeper nesting, which cannot be solved by F-MDP. Details can be found in App. A.7. Training and evaluation. For the main comparisons, the task horizon is fixed at =100 except for Ant (T =200). We trained each model with 7 random seeds to ensure statistical significance. All the methods are implemented in JAX (Bradbury et al., 2018) and trained with 512 parallel environments for 10004000 epochs. All experiments were conducted on Amazon Web Services (AWS) g6e.2xlarge instances. single experiment (a specific set of environment, method, STL, and random seed) took 5 to 90 minutes, depending on the environment and method complexity. In the testing stage, we sample 512 initial states. For each initial state, each baseline is given 10 attempts to generate the solution, and the trajectory with the highest STL robustness score is selected. For our approach, we attempt to select the best time assignment only once, based on the critic value, and then roll out the trajectory (we avoid the use of the STL score as feedback to choose the trajectory). The Success rate is the average performance over all the initial states and the STLs. We also measured Training time, as shown in App. A.5, which is the time to train each model (averaged over STLs). 5.2 MAIN RESULTS Figure 4: Main comparison. Our method has higher task success rate compared to other baselines. Figure 5: Main comparison for the STL success rate evaluation along the training process. As shown in Fig. 4 (top row), TGPO achieves the leading performance in most benchmarks, and with Bayesian time variable sampling, TGPO* achieves the highest overall success rate across all benchmarks, indicating the strong empirical performance. Our advantage becomes clearer as the system dimension and the planning difficulty increase, especially in Quadrotor and Ant, where most of the baselines achieve less than 10% success rate, whereas TGPO* can achieve 86.46% and 61.57% success rate, respectively. Under Linear system, the best baseline τ -MDP (84.11%) performs competitively compared to TGPO* (87.53%), but τ -MDPs performance drops drastically on the other benchmarks. The Grad method is strong baseline on Franka Panda, however, its success rate decreases by large margin on Quadrotor due to its complex nonlinear dynamics, and it cannot work at all on Ant, which is likely caused by the discrepancy between the simulators approximated gradients and the true non-differentiable dynamics. These findings showcase TGPOs strong performance and great adaptation to high-dimensional and non-differentiable environments. If we look at different types of STLs (Fig. 4, bottom row), on low-dimensional cases (Linear and Unicycle), most baselines work well under the simple STL tasks (two-layer STLs) but they struggle on the harder STLs (multi-layer STLs, note that F-MDP can only handle two-layer STLs). Whereas our approaches (both TGPO and TGPO*) excel at working on these complex STLs and perform consistently well. This shows our approachs strength in handling complex STLs. In Fig. 5, we show the task success rate in training. Our approach can achieve high task success rate eventually, whereas other baselines show plateauing early in the training."
        },
        {
            "title": "5.3 SOLVING STL WITH DIFFERENT HORIZON-LENGTHS",
            "content": "Figure 6: Solving STL in Linear environment over varied task horizons. Our method performs the best and maintains high success rate in long horizons where the RL baselines performance degrades. Beyond system complexity and task difficulty, our methods also show resilient adaptivity for longhorizon tasks. Here, we consider only the two-level STLs and we scale the task horizon to different lengths (50, 200, 300, 800 and 1000). As shown in Fig. 6, our methods (TGPO and TGPO*) keep high success rate over varied time lengths, whereas for RL methods τ -MDP, F-MDP and RNN, which are strong baselines for shorter horizons (T =50 and 100), experience huge drop in success rate as the horizon increases. It is interesting that CEM and Grad can maintain their performance as the horizon expands 10 times, which may be attributed to their trajectory optimization formulation. 5.4 ABLATION STUDIES Table 1: Ablation studies for TGPO on the linear dynamics environment. (a) Different time variables sampling strategies. (b) Different state augmentation and rewards. Method Rand. Bay. Elite Test(%) State aug. / Reward Test(%) Ours OursBay OursElite OursmixBay OursmixElite OursBayElite Ours* 80.33 8.84 53.79 7.99 61.49 10.02 81.18 9.72 86.62 8.67 81.04 11.00 88.99 9. 11.73 2.67 t+flags / STL 46.85 10.53 t+flags / STL+Inv t+flags / STL+Inv+Prog 49.80 7.72 84.59 7.88 t+flags / STL+Inv+Dist 11.43 3.48 / STL+Inv+Dist 47.51 7.86 / STL+Inv+Dist Ours* (all / all) 88.99 9.60 8 We conduct thorough ablation study under Linear (all 10 STLs) for the analysis. We first study different sampling strategies. As shown in Tbl. 1a, our base model with random sampling (Ours) can already achieve 80.33% success rate ( indicates the standard deviation over 7 random seeds). However, naively using Bayesian sampling (OursBay) or Elite variable replay buffer (OursElite) will hurt the performance, likely due to the myopic exploration at the beginning of the training, which restricts the agent from seeking more promising assignments. Hence, we mix the two sources of the time variables together and witness certain improvement (OursmixBay, OursmixElite, and OursBayElite) compared to Ours. Finally, by combining all these together, Ours* achieves the best performance. In Tbl. 1b we study how state augmentation and reward shaping foster an efficient multi-stage RL. For the reward design, we consider to just using parts of the reward terms introduced before, and the results (the first 4 rows) show that, just using STL robustness score will only result in 11.73% success rate, whereas by gradually adding invariance penalty, progress reward and the distance reward, the performance will get improved (the most improvement comes from using the distance reward term) and finally becomes 88.99% for Ours*. Regarding the state augmentation, removing the flags in the augmented state will result in 41.48% drop in success rate, and if further removing the time index counter, the performance will drop to 11.43%. The combined findings validate our design. 5.5 VISUALIZATION FOR INTERPRETABILITY AND MULTI-MODAL BEHAVIOR Figure 7: Critic value visualization and simulation for the ant environment under STL F[0,160](A1) F[0,160](A2) G[0,200]B. From the two feasible regions on the critic heatmap, we can see that the corresponding conditioned policy generates two behaviors to fulfill the task specification. TGPO can generate diverse behaviors to fulfill the STL specifications, which can also be reflected from the critic values. We consider an example under the Ant environment for the STL task F[0,160](A1) F[0,160](A2) G[0,200]B. The ant starts from the lower left, and there is an obstacle in the middle of the scene. The time variables here correspond to Reach A1 (the cyan region in the scene) and Reach A2 (green). After the training, we plot the critic value heatmap across different time variable assignments for the initial state. As shown in Fig. 7, the lower-left Lshape region is in low critic value as it is dynamically infeasible to reach the first subgoal in short time (0 τ 40). The diagonal line region also receives low critic value, because the two subgoal regions cannot be visited in such short time. The diagonal line splits the promising time variable regions (yellow) into two parts, from which we can generate two different ways to fulfill the STL task (as shown from the time-elapsed simulation plot on the right). This shows that we can leverage the time variables as the condition to generate multi-modal solutions to solve the STL problem. 5.6 LIMITATIONS While our method achieves strong empirical performance, it lacks formal guarantees on convergence to global optimum. TGPO is effective on STLs with conjunctions and temporal operators, but it might not efficiently handle STLs with disjunctions or infinite-horizon task requirements like Always-Eventually (G(F)). In our paper, we have tested TGPO with 5 time variables; its scalability towards more complex STLs remains an open question. We aim to address these in future work."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce Temporal Grounded Policy Optimization (TGPO), novel reinforcement learning framework for solving long-horizon Signal Temporal Logic tasks. By using STL decomposition, time variable sampling, state augmentation and reward design, TGPO can effectively handle general and complex STL tasks. Our experiments demonstrate that TGPO significantly outperforms existing baselines across various robotic environments and STL formulas. Future work will focus on extending TGPO to handle broader class of STL formulas and improving its scalability."
        },
        {
            "title": "REFERENCES",
            "content": "Derya Aksaray, Austin Jones, Zhaodan Kong, Mac Schwager, and Calin Belta. Q-learning for robust satisfaction of signal temporal logic specifications. In 2016 IEEE 55th Conference on Decision and Control (CDC), pp. 65656570. IEEE, 2016. Fahiem Bacchus and Froduald Kabanza. Using temporal logics to express search control knowledge for planning. Artificial intelligence, 116(1-2):123191, 2000. Anand Balakrishnan and Jyotirmoy Deshmukh. Structured reward shaping using signal temporal logic specifications. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 34813486. IEEE, 2019. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. Jax: Composable transformations of python+ numpy programs, version 0.3. 13, 2018, 2018. Alberto Camacho, Oscar Chen, Scott Sanner, and Sheila McIlraith. Decision-making with nonIn Proceedings of the Multimarkovian rewards: From ltl to automata-based reward shaping. disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279283, 2017. Siddhartha Chib and Edward Greenberg. Understanding the metropolis-hastings algorithm. The american statistician, 49(4):327335, 1995. Pieter-Tjerk De Boer, Dirk Kroese, Shie Mannor, and Reuven Rubinstein. tutorial on the cross-entropy method. Annals of operations research, 134(1):1967, 2005. Alexandre Donze. On signal temporal logic. In Runtime Verification: 4th International Conference, RV 2013, Rennes, France, September 24-27, 2013. Proceedings 4, pp. 382383. Springer, 2013. Alexandre Donze and Oded Maler. Robust satisfaction of temporal logic over real-valued signals. Formal Modeling and Analysis of Timed Systems, pp. 92, 2010. Alexandre Donze, Thomas Ferrere, and Oded Maler. Efficient robust monitoring for stl. In Computer Aided Verification: 25th International Conference, CAV 2013, pp. 264279. Springer, 2013. Cameron Finucane, Gangyuan Jing, and Hadas Kress-Gazit. Ltlmop: Experimenting with language, In 2010 IEEE/RSJ International Conference on Intelligent temporal logic and robot control. Robots and Systems, pp. 19881993. IEEE, 2010. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. arXiv preprint Braxa differentiable physics engine for large scale rigid body simulation. arXiv:2106.13281, 2021. Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Logically-constrained reinforcement learning. arXiv preprint arXiv:1801.08099, 2018. Mohammadhosein Hasanbeig, Daniel Kroening, and Alessandro Abate. Deep reinforcement learning with temporal logics. In Formal Modeling and Analysis of Timed Systems: 18th International Conference, FORMATS 2020, Vienna, Austria, September 13, 2020, Proceedings 18, pp. 122. Springer, 2020. 10 Yiting He, Peiran Liu, and Yiding Ji. Scalable signal temporal logic guided reinforcement learning via value function space optimization. arXiv preprint arXiv:2408.01923, 2024. Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In International Conference on Machine Learning, pp. 21072116. PMLR, 2018. Junya Ikemoto and Toshimitsu Ushio. Deep reinforcement learning under signal temporal logic constraints using lagrangian relaxation. IEEE Access, 10:114814114828, 2022. Kishor Jothimurugan, Rajeev Alur, and Osbert Bastani. composable specification language for reinforcement learning tasks. Advances in Neural Information Processing Systems, 32, 2019. Parv Kapoor, Anand Balakrishnan, and Jyotirmoy Deshmukh. Model-based reinforcement learning from signal temporal logic specifications. arXiv preprint arXiv:2011.04950, 2020. Parv Kapoor, Eunsuk Kang, and Rˆomulo Meira-Goes. Safe planning through incremental decomposition of signal temporal logic specifications. In NASA Formal Methods Symposium, pp. 377396. Springer, 2024. Jesper Karlsson, Fernando Barbosa, and Jana Tumova. Sampling-based motion planning with temporal logic missions and spatial preferences. IFAC-PapersOnLine, 53(2):1553715543, 2020. Vincent Kurtz and Hai Lin. Mixed-integer programming for signal temporal logic with fewer binary variables. IEEE Control Systems Letters, 6:26352640, 2022. Karen Leung and Marco Pavone. Semi-supervised trajectory-feedback controller synthesis for signal temporal logic specifications. In 2022 American Control Conference (ACC), pp. 178185. IEEE, 2022. Karen Leung, Nikos Arechiga, and Marco Pavone. Backpropagation through signal temporal logic specifications: Infusing logical structure into gradient-based methods. The International Journal of Robotics Research, 42(6):356370, 2023. Xiao Li, Cristian-Ioan Vasile, and Calin Belta. Reinforcement learning with temporal logic rewards. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3834 3839. IEEE, 2017. Hsuan-Cheng Liao. survey of reinforcement learning with temporal logic rewards. preprint, 2020. Alexis Linard, Ilaria Torre, Ermanno Bartoli, Alex Sleat, Iolanda Leite, and Jana Tumova. Realtime rrt* with signal temporal logic preferences. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 86218627. IEEE, 2023. Ruijia Liu, Ancheng Hou, Xiao Yu, and Xiang Yin. Zero-shot trajectory planning for signal temporal logic tasks. arXiv preprint arXiv:2501.13457, 2025. Wenliang Liu, Noushin Mehdipour, and Calin Belta. Recurrent neural network controllers for signal temporal logic specifications subject to safety constraints. IEEE Control Systems Letters, 6:9196, 2021. Wenliang Liu, Wei Xiao, and Calin Belta. Learning robust and correct controllers from signal temporal logic specifications using barriernet. In 2023 62nd IEEE Conference on Decision and Control (CDC), pp. 70497054. IEEE, 2023. Oded Maler and Dejan Nickovic. Monitoring temporal properties of continuous signals. Formal Techniques, ModellingandAnalysis of Timed and Fault-Tolerant Systems, pp. 152, 2004. Yue Meng and Chuchu Fan. Signal temporal logic neural predictive control. IEEE Robotics and Automation Letters, 8(11):77197726, 2023. Yue Meng and Chuchu Fan. Diverse controllable diffusion policy with signal temporal logic. IEEE Robotics and Automation Letters, 2024. 11 Yue Meng and Chuchu Fan. Telograf: Temporal logic planning via graph-encoded flow matching. In Forty-second International Conference on Machine Learning, 2025. Yash Vardhan Pant, Houssam Abbas, and Rahul Mangharam. Smooth operator: Control using the smooth robustness of temporal logic. In 2017 IEEE Conference on Control Technology and Applications (CCTA), pp. 12351240. IEEE, 2017. Aniruddh Puranic, Jyotirmoy Deshmukh, and Stefanos Nikolaidis. Learning from demonstrations using signal temporal logic. In Conference on Robot Learning, pp. 22282242. PMLR, 2021. Dorsa Sadigh, Eric Kim, Samuel Coogan, Shankar Sastry, and Sanjit Seshia. learning based approach to control synthesis of markov decision processes for linear temporal logic specifications. In 53rd IEEE Conference on Decision and Control, pp. 10911096. IEEE, 2014. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Mayank Sewlia, Christos Verginis, and Dimos Dimarogonas. Cooperative sampling-based motion planning under signal temporal logic specifications. In 2023 American Control Conference (ACC), pp. 26972702. IEEE, 2023. Salomon Sickert, Javier Esparza, Stefan Jaax, and Jan Kˇretınsk`y. Limit-deterministic buchi automata for linear temporal logic. In International Conference on Computer Aided Verification, pp. 312332. Springer, 2016. Dawei Sun, Jingkai Chen, Sayan Mitra, and Chuchu Fan. Multi-agent motion planning from signal temporal logic specifications. IEEE Robotics and Automation Letters, 7(2):34513458, 2022. Abdelhamid Tayebi and Stephen McGilvray. Attitude stabilization of vtol quadrotor aircraft. IEEE Transactions on control systems technology, 14(3):562571, 2006. Pashootan Vaezipoor, Andrew Li, Rodrigo Toro Icarte, and Sheila Mcilraith. Ltl2action: Generalizing ltl instructions for multi-task rl. In International Conference on Machine Learning, pp. 1049710508. PMLR, 2021. Cristian-Ioan Vasile, Vasumathi Raman, and Sertac Karaman. maximally-satisfying controllers for temporal logic specifications. tional Conference on Intelligent Robots and Systems (IROS), pp. 38403847. IEEE, 2017. Sampling-based synthesis of In 2017 IEEE/RSJ InternaHarish Venkataraman, Derya Aksaray, and Peter Seiler. Tractable reinforcement learning of signal temporal logic objectives. In Learning for Dynamics and Control, pp. 308317. PMLR, 2020. Siqi Wang, Xunyuan Yin, Shaoyuan Li, and Xiang Yin. Tractable reinforcement learning for signal temporal logic tasks with counterfactual experience replay. IEEE Control Systems Letters, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ALGORITHM HYPERPARAMETERS All the main hyperparameters used during training are shown in Table 2. Table 2: Hyperparameters assignments used for training TGPO*. Hyperparameter Linear; Unicycle; FrankaPanda; Quadrotor; Ant Network hidden units Optimizer Learning rate Weight decay Grad norm clip Random seeds Batch size Epochs Time steps Time duration Distance reward λ1 Progress reward λ2 Success reward λ3 Invariance penalty λ4 Number of MCMC steps NM CM Number of warmup steps Nwarmup Number of MCMC chains MM CM Ratio of Randomly-sampled time variables ηmcmc Ratio of MCMC-sampled time variables ηunif orm Ratio of Elite time variables ηelite Elite buffer size A.2 SIMULATION ENVIRONMENT DETAILS (512, 512, 512) Adam 3 104 0.1 0.5 1007,1008,1009,1010,1011,1012,1013 512 1000 (L, U); 2000 (F, A); 4000(Q) 100 (L, U, F, Q); 200 (A) 0.2 (L, U); 0.05 (F, A); 0.1 (Q) 0.5 20.0 20.0 -3.0 (L, F, Q); -3.5 (U); -1.5 (A) 500 200 512 0.5 0.4 0.1 512 In this paper, we conduct experiments on five simulation environments (Linear, Unicycle, Franka Panda, Quadrotor, and Ant). The first four environments were implemented in plain JAX code by writing out the system dynamics, whereas the last one was adopted from the Mujoco JAX implementation. Detailed implementations are listed as follows. A.2.1 LINEAR We use single-integrator dynamics model. The 2D state (x, y)T represents the 2D coordinates on xy-plane, and the 2D control input (v, w)T reflects the velocities in these two directions. The system dynamics is described as: (cid:26)xt+1 = xt + vtt yt+1 = yt + wtt (7) We set the time step duration = 0.2s. A.2.2 UNICYCLE We use car-like dynamics model. The 4D state (x, y, θ, v)T represents the 2D coordinates on the xy-plane, the heading angle of the robot and the velocity of the robot, respectively. The 2D input (ω, a)T represents the angular velocity and the acceleration. The system dynamics can be described as: xt+1 = xt + vt cos(θt)t yt+1 = yt + vt sin(θt)t θt+1 = θt + ωtt vt+1 = vt + att 13 (8) We set the time step duration = 0.2s. The control actuation is limited at [1rad/s, +1rad/s] [4m/s2, +4m/s2]. The scene layout is [5m, +5m] [5m, +5m] on the xy-plane. A.2.3 FRANKA PANDA We use 7 DoF Franka Panda robot arm model to conduct the simulation. The 7D state (θ1, θ2, ..., θ7)T represents the angle for all the joints where θ7 is for the end-effector joint. The 7D control input (ω1, ω2, ..., ω7)T represents the angular velocity for all the joints. The dynamics follows simple single-integrator case: θi,t+1 = θi,t + ωi,tt, for = 1, 2, ..., 7. We set the time step duration = 0.05s. A.2.4 QUADROTOR We use full quadrotor dynamics model (Tayebi & McGilvray, 2006) to conduct the simulation. The 12D state (x, y, z, vx, vy, vz, ϕ, θ, ψ, ωx, ωy, ωz)T represents the 3D coordinate = (x, y, z)T , the velocity vector = (vx, vy, vz)T , the orientation vector Θ = (ϕ, θ, ψ)T , and the angular velocity ω = (ωx, ωy, ωz)T , respectively. The 4D control input (f1, f2, f3, f4)T represents the lifting force from the four motors. The full dynamics are: pt+1 = pt + vtt vt+1 = vt + (ge3 Θt+1 = Θt + ωtt ωt+1 = ωt + 1(τ ωt (Iωt))t (cid:35) 0 Rz(ψ)Ry(θ)Rx(ϕ)e3)t cos(ϕ) sin(ϕ) cos(ϕ) sin(ϕ) , Ry(θ) = (9) (cid:34) cos(θ) 0 sin(θ) 0 1 0 (cid:35) , sin(θ) 0 cos(θ) and and τ are the total thrust and the torques derived with the rotation matrices Rx(ϕ) = and Rz(ψ) = (cid:34)cos(ψ) sin(ψ) cos(ψ) 0 sin(ψ) 0 (cid:34)1 0 0 (cid:35) 0 0 1 from the motor input with the Coriolis effect considered to the angular velocity vector. We set the time step duration = 0.10s, adapt the gravity coefficient = 9.81m/s2 with the corresponding gravity vector e3 = (0, 0, 1)T , set the total mass of the quadrotor = 0.2kg and set the diagonal line of the quadrotor inertia matrix as (0.01kg m2, 0.01kg m2, 0.02kg m2)T . A.2.5 ANT In this case, the agent is 8-DoF quadruped robot with the complex dynamics implemented in Brax (Freeman et al., 2021). The observation space is 29-dimension (3-dimension for xyz coordinates, 4-dimension for the torso orientation (in Quaternion representation), 3-dimension velocity vector and 3-dimension angular velocity for the torso, 8-dimension for the joints angles and another 8-dimension for the joints angular velocities). The original control input is 8-dimension for the torques applied to each of the 8 joints. To ease the RL training, we first train goal-reaching policy, enabling the ant to learn and move to specified target location. Then, for the baselines and our methods, the problem becomes planning the waypoints so that the ant can satisfy the STL tasks specified. A.3 BASELINE IMPLEMENTATION DETAILS A.3.1 CEM We use the Cross Entropy Method baseline mentioned in (Meng & Fan, 2023), which belongs to the evolutionary search algorithm mentioned in (Salimans et al., 2017). We denote the initial neural network policy parameters as θ(0). At j-th iteration, we draw samples θ1, ...θN from (θ(j), σ(j)2 ) where σ(j) is the preset standard deviation, then we rollout the trajectories and compute their robustness score. We pick the top-K candidates parameters θE1, ...θEk . Then we update the estimate for the neural network parameters θ(j+1) = 1 (cid:80) i=1 (cid:115) θEi and σ(j+1) = 1 k1 (cid:80) i=1 (θEi θ(j+1))2. We 14 repeat this process for iterations to get the final parameters. We set the size for the elite pool to be = 32 and set the population sample size to be = 512. The number of iteration steps is the same as our method (L = 1000 for Linear and Unicycle, = 2000 for Franka Panda and Ant, and = 4000 for Quadrotor.) A.3.2 τ -MDP τ -MDP is an RL method introduced in (Aksaray et al., 2016) to solve STL tasks under the discrete state space. The original method appends history to the state space, and uses Q-learning to solve short-horizon tasks with 2-layer STL specifications. Here, we extend it to handle general STL formulas by augmenting the entire trajectory into the state space with STL robustness score as the terminal reward to guide the agent to satisfy STL tasks. We also changed the RL backbone from Q-learning to PPO for better scalability to longer-horizon tasks (The original Q-learning tabular formulation will not work on continuous space for = 100). A.3.3 -MDP -MDP is an improved RL method introduced in (Venkataraman et al., 2020) to solve STL tasks under the discrete state space more efficiently. This approach considers the 2-layer STL specifications, and introduces flag for each of the subformulas in the STL. They defined the state transition rules and reward mechanism for and G-based subformulas based on these flags and show that the Q-learning under this augmentation can learn more efficiently than the Q-learning under τ -MDP (Aksaray et al., 2016). We re-implemented -MDP in PPO for our comparison. A.3.4 RNN In this case, similar to (Liu et al., 2021), we use an RNN to encode the history data and then use the robustness score as the final reward to guide the agent to satisfy the tasks. The issue of this implementation is that it is much more time-consuming compared to the other baselines. A.3.5 GRAD In this case, similar to (Leung & Pavone, 2022) and (Meng & Fan, 2023), we use neural network policy to roll out the trajectory (in deterministic manner, rather than sampling from the learned Gaussian distribution). At each time step, the network receives the state (and the time index) and generates the action, which is then sent to the environment to derive the next state. We repeat this process times to roll out the full trajectory, which preserves the gradient through the differentiable system dynamics. We use the approximated robustness score mentioned in (Pant et al., 2017) to ensure the score is differentiable. We then conduct backpropagation-through-time (BPTT) to update the neural network parameters. A.4 TEMPORAL SAMPLING ALGORITHM DETAILS The Metropolis-Hastings algorithm (Chib & Greenberg, 1995) is Markov Chain Monte Carlo (MCMC) method for sampling from probability distribution, commonly used when directly sampling from the distribution is hard. In our approach TGPO*, we use discrete version of the M-H algorithm to sample time variables that are likely to yield high critic values Vψ(s0, t), where s0 is the initial state. We use exp(Vψ(s0, t)) as proxy for the unnormalized probability of the promising temporal variables. The algorithm proceeds by starting with an initial set of temporal variables t0 and iteratively proposing to move on grids to new set based on proposal distribution g(tt). The move is then accepted or rejected based on the acceptance ratio α, which compares the critic value exponentials of the new and the current variables. The process is detailed in Algorithm. 2. Propose new temporal variables for chain Sample from g(ttm,i1) for all chain {1, . . . , Mchain} do Initialize temporal variables tm,0 randomly Initialize samples list Sm [] Qcurrent Vψ(s0, tm,i1) Qnew Vψ(s0, t) α min(1, exp(Qnew Qcurrent)) Algorithm 2 Metropolis-Hastings for time variable sampling (with multiple chains and warm-up) 1: Input: Initial state s0, Critic network Vψ(s, t), Proposal distribution g(tt) 2: Input: Iterations Nmcmc, Number of chains Mchain, Number of warm-up steps Nwarmup 3: for all chain {1, . . . , Mchain} do 4: 5: 6: end for 7: for = 1 to Nmcmc do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end for 24: 25: for = Nwarmup + 1 to Nmcmc do 26: 27: 28: 29: end for 30: Return: Sampled time variables {tt Si, = 1, 2, . . . , } Sample from Uniform(0, 1) if < α then tm,i for all chain {1, . . . , Mchain} do Add tm,i to Sm tm,i tm,i1 end for end for end if else Accept or reject the new sample for chain Collect samples after the warm-up period Calculate the acceptance ratio α Reject and keep the old sample Accept the new sample In our approach, we set Nmcmc = 500, Nwarmup = 200, Mchain = 512 and pick the time variable from each Si with the highest critic value to form the time variable set Tmcmc used in Alg. 1. For the proposal distribution g(tt), we use uniform distribution over the local neighborhood of the current temporal variables t: we first uniformly sample an index from the dimensions of and then uniformly sample move direction {1, +1}. The proposed new set of variables is generated by applying this change to the selected index but also ensure that the new value is within the valid range (otherwise we keep unchanged). A.5 TRAINING TIME COMPARISON Figure 8: Comparison for training time. For each method under each environment, the result is averaged over 10 STLs and 7 random seeds. TGPOs training time is on par with leading baselines. As shown in Fig. 8, TGPO and TGPO* have similar runtime compared to τ -MDP, -MDP and Grad baselines, whereas the CEM baseline is normally 20.8%35.8% higher than TGPO*. The most time-consuming baseline is RNN, where TGPO* is 1.96X6.11X faster in training speed. This shows that our approach is as scalable as other top RL baselines in training time, but our method can achieve higher task success rate. A.6 CORRELATION BETWEEN THE CRITIC AND THE STL ROBUSTNESS SCORE (a) STL-01 (b) STL-02 (c) STL-03 (d) STL- (e) STL-05 (f) STL-06 (g) STL-07 (h) STL-08 (i) STL-09 (j) STLFigure 9: Correlation analysis between the critic and the STL score (seed=1007). (a) STL-01 (b) STL-02 (c) STL-03 (d) STL-04 (e) STL- (f) STL-06 (g) STL-07 (h) STL-08 (i) STL-09 (j) STL-10 Figure 10: Correlation analysis between the critic and the STL score (seed=1008). To validate that our learned critic in TGPO can really reflect the promising time variables that lead to STL satisfaction, in the Linear environment, for the TGPO algorithm, we randomly sample 4096 points from the pretrained critic and rollout the corresponding trajectories to generate the STL robustness score. We plot the (critic value, STL score) scatter plot, together with the cumulative STL success rate curve for samples with critic value greater than x. As shown in Fig. 9 (for seed=1007) and Fig. 10 (for seed=1008) from the blue scatter plots, whenever the critic value (left) is higher, the STL score is more likely to be higher, and hence more likely to satisfy STL. If we look at the orange curve, as the Critic value increases, in most cases the probability for the corresponding traces satisfying the STL score is monotonously increasing or plateau at 100%, which indicates that our critic is learned correctly (note that if the critic is not learned well, it could learn for some time variables that bring in high critic value but result in low STL scores, like STL-09 in Fig. 10) and can be used to find promising time variable assignments. A.7 STL TASK DETAILS Under each simulation environment, we make 10 STL formulas in two different categories (twolevel and multi-level). Here we only consider predicates related with Reach, Stay, Avoid certain objects in the scene. They are listed as follows. 17 A.7.1 STLS IN LINEAR ENVIRONMENT (a) STL- (b) STL-02 (c) STL-03 (d) STL-04 (e) STL-05 Figure 11: Scene for Linear: STL tasks 01 to 05 STL-01 (Two-layer): G[0:90](B3) G[0:90](B4)) F[5:7](F[50:85](A) G[0:90](B5) G[0:90](B1) G[0:90](B2) STL-02 (Two-layer): G[0:90](B2) G[0:90](B3) G[0:90](B4)) F[5:10](F[0:50](A) G[60:80](C) G[0:90](B5) G[0:90](B1) STL-03 (Two-layer): G[0:90](B0) G[0:90](B1) G[0:90](B2) G[0:90](B3) G[0:90](B4)) F[5:10](F[0:50](A) F[40:60](C) G[70:80](D) G[0:90](B5) STL-04 (Two-layer): G[0:90](E) G[0:90](B1) G[0:90](B2) G[0:90](B3) G[0:90](B4)) F[5:10](F[0:50](A)F[40:50](C)F[70:80](F )G[50:60](D)G[0:90](B) F[5:10](F[0:30](A)F[30:50](C)F[70:80](F )F[75:88](H)G[50:60](D) STL-05 (Two-layer): G[0:90](B) G[0:90](E) G[0:90](G) G[0:90](B1) G[0:90](B2) G[0:90](B3) G[0:90](B4)) (a) STL-06 (b) STL-07 (c) STL-08 (d) STL- (e) STL-10 Figure 12: Scene for Linear: STL tasks 06 to 10 STL-06 (Multi-layer): G[0:100](B4) G[0:100](B5) F[10:90](A) G[0:100](B1) G[0:100](B2) G[0:100](B3) STL-07 (Multi-layer): G[0:100](B3) G[0:100](B4) G[0:100](B5) F[0:90](A1) F[40:80](A2) G[0:100](B1) G[0:100](B2) STL-08 (Multi-layer): G[0:100](B2) G[0:100](B3) G[0:100](B4) G[0:100](B5) F[0:90](A1) F[40:80](A2 F[10:20](G[0:10](A3))) G[0:100](B1) (Multi-layer): F[5:20](A1 F[10:20](G[0:5](A2) F[10:30](G[0:5](A3)) STL-09 F[10:30](G[0:10](A4)))) G[0:100](B1) G[0:100](B2) G[0:100](B3) G[0:100](B4) G[0:100](B5) G[0:100](B6) G[0:100](B7) G[0:100](B8) G[0:100](B9) (D1)U[0:100](K1) (D2)U[0:100](K2) (D3)U[0:100](K3) STL-10 (Multi-layer): F[80:90](G[0:5](G)) G[0:100](B1) G[0:100](B2) G[0:100](B3) G[0:100](B4) G[0:100](B5) A.7.2 STLS IN UNICYCLE ENVIRONMENT STL-01 (Two-layer): G[0:90](B3) G[0:90](B4)) F[5:7](F[50:85](A) G[0:90](B5) G[0:90](B1) G[0:90](B2) 18 (a) STL-01 (b) STL-02 (c) STL- (d) STL-04 (e) STL-05 Figure 13: Scene for Unicycle: STL tasks 01 to 05 STL-02 (Two-layer): G[0:90](B2) G[0:90](B3) G[0:90](B4)) F[5:10](F[0:50](A) G[60:80](C) G[0:90](B5) G[0:90](B1) STL-03 (Two-layer): G[0:90](B0) G[0:90](B1) G[0:90](B2) G[0:90](B3) G[0:90](B4)) F[5:10](F[0:50](A) F[40:60](C) G[70:80](D) G[0:90](B5) STL-04 (Two-layer): G[0:90](E) G[0:90](B1) G[0:90](B2) G[0:90](B3) G[0:90](B4)) F[5:10](F[0:50](A)F[40:50](C)F[70:80](F )G[50:60](D)G[0:90](B) F[5:10](F[0:30](A)F[30:50](C)F[70:80](F )F[75:88](H)G[50:60](D) STL-05 (Two-layer): G[0:90](B) G[0:90](E) G[0:90](G) G[0:90](B1) G[0:90](B2) G[0:90](B3) G[0:90](B4)) (a) STL-06 (b) STL- (c) STL-08 (d) STL-09 (e) STL-10 Figure 14: Scene for Unicycle: STL tasks 06 to 10 STL-06 (Multi-layer): G[0:100](B4) G[0:100](B5) F[10:90](A) G[0:100](B1) G[0:100](B2) G[0:100](B3) STL-07 (Multi-layer): G[0:100](B3) G[0:100](B4) G[0:100](B5) F[0:90](A1) F[40:80](A2) G[0:100](B1) G[0:100](B2) STL-08 (Multi-layer): G[0:100](B2) G[0:100](B3) G[0:100](B4) G[0:100](B5) F[0:90](A1) F[40:80](A2 F[10:20](G[0:10](A3))) G[0:100](B1) (Multi-layer): F[5:20](A1 F[10:20](G[0:5](A2) F[10:30](G[0:5](A3)) STL-09 F[10:30](G[0:10](A4)))) G[0:100](B1) G[0:100](B2) G[0:100](B3) G[0:100](B4) G[0:100](B5) G[0:100](B6) G[0:100](B7) G[0:100](B8) G[0:100](B9) (D1)U[0:100](K1) (D2)U[0:100](K2) (D3)U[0:100](K3) STL-10 (Multi-layer): F[80:90](G[0:5](G)) G[0:100](B1) G[0:100](B2) G[0:100](B3) G[0:100](B4) G[0:100](B5) A.7.3 STLS IN FRANKA PANDA ENVIRONMENT STL-01 (Two-layer): G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6)) F[5:7](F[50:85](A) G[0:90](B5) G[0:100](W1) G[0:100](W2) STL-02 (Two-layer): G[0:100](W2) G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6)) F[5:10](F[0:50](A) G[60:80](C) G[0:90](B5) G[0:100](W1) 19 Figure 15: Scene for Franka Panda: STL task 01 Figure 16: Scene for Franka Panda: STL task 02 Figure 17: Scene for Franka Panda: STL task 03 20 F[5:10](F[0:50](A) F[40:60](C) G[70:80](D) G[0:90](B5) STL-03 (Two-layer): G[0:90](B0)G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5) G[0:100](W6)) Figure 18: Scene for Franka Panda: STL task 04 F[5:10](F[0:50](A)F[40:50](C)F[70:80](F )G[50:60](D)G[0:90](B) STL-04 (Two-layer): G[0:90](E)G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5) G[0:100](W6)) Figure 19: Scene for Franka Panda: STL task 05 F[5:10](F[0:30](A)F[30:50](C)F[70:80](F )F[75:88](H)G[50:60](D) STL-05 (Two-layer): G[0:90](B) G[0:90](E) G[0:90](G) G[0:100](W1) G[0:100](W2) G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6)) Figure 20: Scene for Franka Panda: STL task 06 STL-06 (Multi-layer): G[0:100](W2) G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6) F[10:90](A) G[0:100](B1) G[0:100](B2) G[0:100](W1) Figure 21: Scene for Franka Panda: STL task 07 21 F[0:90](A1) F[40:80](A2) G[0:100](B1) G[0:100](B2) STL-07 (Multi-layer): G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5)G[0:100](W6) Figure 22: Scene for Franka Panda: STL task 08 F[0:90](A1) F[40:60](A2 F[15:30](G[0:5](A3))) G[0:100](B1) STL-08 (Multi-layer): G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5)G[0:100](W6) Figure 23: Scene for Franka Panda: STL task 09 (Multi-layer): F[25:30](A1 F[20:28](G[0:5](A2) F[10:30](G[0:5](A3)) STL-09 F[10:30](G[0:10](A4)))) G[0:100](B1) G[0:100](B6) G[0:100](B7) G[0:100](B8) G[0:100](B9)G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5) G[0:100](W6) Figure 24: Scene for Franka Panda: STL task 10 (D1)U[0:100](K1) (D2)U[0:100](K2) (D3)U[0:100](K3) STL-10 (Multi-layer): F[80:90](G[0:5](G)) G[0:100](B5) G[0:100](B1) G[0:100](B2) G[0:100](B3) G[0:100](B4)G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5) G[0:100](W6) A.7.4 STLS IN QUADROTOR ENVIRONMENT STL-01 (Two-layer): G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6)) F[5:7](F[50:85](A) G[0:90](B5) G[0:100](W1) G[0:100](W2) STL-02 (Two-layer): G[0:100](W2) G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6)) F[5:10](F[0:50](A) G[60:80](C) G[0:90](B5) G[0:100](W1) F[5:10](F[0:50](A) F[40:60](C) G[70:80](D) G[0:90](B5) STL-03 (Two-layer): G[0:90](B0)G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5) G[0:100](W6)) Figure 25: Scene for Quadrotor: STL task 01 Figure 26: Scene for Quadrotor: STL task 02 Figure 27: Scene for Quadrotor: STL task 03 Figure 28: Scene for Quadrotor: STL task 04 23 F[5:10](F[0:50](A)F[40:50](C)F[70:80](F )G[50:60](D)G[0:90](B) STL-04 (Two-layer): G[0:90](E)G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5) G[0:100](W6)) Figure 29: Scene for Quadrotor: STL task 05 F[5:10](F[0:30](A)F[30:50](C)F[70:80](F )F[75:88](H)G[50:60](D) STL-05 (Two-layer): G[0:90](B) G[0:90](E) G[0:90](G) G[0:100](W1) G[0:100](W2) G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6)) Figure 30: Scene for Quadrotor: STL task 06 STL-06 (Multi-layer): G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6) F[10:90](A) G[0:100](B1) G[0:100](W1) G[0:100](W2) Figure 31: Scene for Quadrotor: STL task STL-07 (Multi-layer): G[0:100](W2) G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6) F[0:90](A1) F[40:80](A2) G[0:100](B1) G[0:100](W1) Figure 32: Scene for Quadrotor: STL task 08 F[0:90](A1) F[40:60](A2 F[15:30](G[0:5](A3))) G[0:100](B1) STL-08 (Multi-layer): G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5)G[0:100](W6) 24 Figure 33: Scene for Quadrotor: STL task (Multi-layer): F[25:30](A1 F[20:28](G[0:5](A2) F[10:30](G[0:5](A3)) STL-09 F[10:30](G[0:10](A4)))) G[0:100](B1) G[0:100](B6) G[0:100](B7) G[0:100](B8) G[0:100](B9)G[0:100](W1)G[0:100](W2)G[0:100](W3)G[0:100](W4)G[0:100](W5) G[0:100](W6) Figure 34: Scene for Quadrotor: STL task 10 (D1)U[0:100](K1) (D2)U[0:100](K2) (D3)U[0:100](K3) STL-10 (Multi-layer): F[80:90](G[0:5](G)) G[0:100](B5) G[0:100](W1) G[0:100](W2) G[0:100](W3) G[0:100](W4) G[0:100](W5) G[0:100](W6) A.7.5 STLS IN ANT ENVIRONMENT (a) STL- (b) STL-02 (c) STL-03 (d) STL-04 (e) STL-05 Figure 35: Scene for Ant: STL tasks 01 to 05 STL-01 (Two-layer): G[0:180](B3) G[0:180](B4)) F[10:14](F[100:170](A) G[0:180](B5) G[0:180](B1) G[0:180](B2) STL-02 (Two-layer): G[0:180](B2) G[0:180](B3) G[0:180](B4)) F[10:20](F[0:100](A) G[120:160](C) G[0:180](B5) G[0:180](B1) STL-03 (Two-layer): G[0:180](B0) G[0:180](B1) G[0:180](B2) G[0:180](B3) G[0:180](B4)) F[10:20](F[0:100](A) F[80:120](C) G[140:160](D) G[0:180](B5) F[10:20](F[0:100](A) F[80:100](C) F[140:160](F ) G[100:120](D) STL-04 (Two-layer): G[0:180](B) G[0:180](E) G[0:180](B1) G[0:180](B2) G[0:180](B3) G[0:180](B4)) F[10:20](F[0:60](A) F[60:100](C) F[140:160](F ) F[150:176](H) STL-05 (Two-layer): G[100:120](D) G[0:180](B) G[0:180](E) G[0:180](G) G[0:180](B1) G[0:180](B2) G[0:180](B3) G[0:180](B4)) 25 (a) STL-06 (b) STL-07 (c) STL-08 (d) STL- (e) STL-10 Figure 36: Scene for Ant: STL tasks 06 to 10 STL-06 (Multi-layer): G[0:200](B4) G[0:200](B5) F[20:180](A) G[0:200](B1) G[0:200](B2) G[0:200](B3) STL-07 (Multi-layer): G[0:200](B3) G[0:200](B4) G[0:200](B5) F[0:180](A1) F[80:160](A2) G[0:200](B1) G[0:200](B2) STL-08 (Multi-layer): G[0:200](B2) G[0:200](B3) G[0:200](B4) G[0:200](B5) F[0:180](A1) F[80:160](A2 F[20:40](G[0:20](A3))) G[0:200](B1) F[10:40](A1 F[20:40](G[0:10](A2) F[20:60](G[0:10](A3)) STL-09 (Multi-layer): F[20:60](G[0:20](A4)))) G[0:200](B1) G[0:200](B2) G[0:200](B3) G[0:200](B4) G[0:200](B5) G[0:200](B6) G[0:200](B7) G[0:200](B8) G[0:200](B9) (D1)U[0:200](K1) (D2)U[0:200](K2) (D3)U[0:200](K3) STL-10 (Multi-layer): F[160:180](G[0:10](G)) G[0:200](B1) G[0:200](B2) G[0:200](B3) G[0:200](B4) G[0:200](B5)"
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology"
    ]
}