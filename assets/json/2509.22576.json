{
    "paper_title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning",
    "authors": [
        "Xu Wujiang",
        "Wentian Zhao",
        "Zhenting Wang",
        "Li Yu-Jhe",
        "Jin Can",
        "Jin Mingyu",
        "Mei Kai",
        "Wan Kun",
        "Metaxas Dimitris"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 7 5 2 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Under review",
            "content": "EPO: ENTROPY-REGULARIZED POLICY OPTIMIZATION FOR LLM AGENTS REINFORCEMENT LEARNING Wujiang Xu1, Wentian Zhao2, Zhenting Wang1, Yu-Jhe Li2, Can Jin1, Mingyu Jin1, Kai Mei1, Kun Wan2, Dimitris N. Metaxas1 1 Rutgers University 2 Adobe Inc."
        },
        {
            "title": "ABSTRACT",
            "content": "Training LLM agents in multi-turn environments with sparse rewards, where completing single task requires 30+ turns of interaction within an episode, presents fundamental challenge for reinforcement learning. We identify critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training. The code is available at the URL 1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning (RL) (Shao et al., 2024; Schulman et al., 2017) has become an important approach for post-training Large Language Models (LLMs), enabling LLMs to acquire reasoning ability with single-turn verified reward (Guo et al., 2025; Yang et al., 2025). LLM agents start to achieve success in versatile scenarios, such as coding (Anthropic, 2025; Kipruto & Salva, 2025; OpenAI, 2025b; Wang et al., 2025b), computing using (OpenAI, 2025a; Xie et al., 2024; Zheng et al., 2024), and searching (Team et al., 2025; Zeng et al., 2025), benefiting from LLMs powerful reasoning ability. Multi-turn LLM agent scenarios present unique challenges compared to traditional LLM RL settings, as episodes can span 30+ turns (Shridhar et al., 2021; Wang et al., 2022) with extended trajectories and sparse rewards. Unlike conventional RL, where rewards are distributed throughout the episode, LLM agents typically receive feedback only at completion, meaning the policy is updated based on the shared reward across long-horizon turns within an episode, making the exploration-exploitation trade-off (Sutton, 1988) crucial for stable training. Traditional RL approaches (Haarnoja et al., 2018; Mnih et al., 2016; Williams & Peng, 1991) employ entropy regularization by adding the entropy of the policy to the objective function, discouraging premature convergence to suboptimal deterministic policies through hyperparameter-controlled regularization term. In RL training for LLM, researchers have adapted these mechanisms by incorporating entropy into the advantage function to reward high-entropy token generation (Cui et al., 2025; Dong et al., 2025; He et al., 2025; Wang et al., 2025a), encouraging exploration and preventing entropy collapse in later training stages. However, entropy-controlled LLM methods prove fundamentally inadequate for multi-turn agent environments because they cannot resolve the exploration-exploitation Contact: wujiang.xu@rutgers.edu 1https://github.com/WujiangXu/EPO"
        },
        {
            "title": "Under review",
            "content": "Figure 1: The exploration-exploitation cascade failure in multi-turn agent training. The cascade failure manifests in two distinct phases clearly visible in the figure: (1) Phase 1 - Excessive Early Exploration (0-40 steps): PPOs early trajectory steps (pink dashed line) exhibit rapid, uncontrolled entropy growth that creates unstable behavioral foundations, while rewards remain stagnant, indicating ineffective exploration-to-reward conversion. (2) Phase 2 - Uncertainty Propagation (40-120 steps): The instability from early steps cascades to late trajectory steps (red dotted line), maintaining dangerously high entropy oscillations that prevent coherent strategy formation and result in reward plateaus despite continuous exploration. This two-phase pattern demonstrates why standard entropy methods fail in multi-turn environments. In contrast, our EPO method maintains stable, controlled entropy levels across both early and late trajectory steps throughout training, achieving significantly lower final entropy values and consistent reward improvement, preventing the cascade failure. cascade failure. This failure cascade proceeds in two distinct phases, as illustrated in Figure 1. First, sparse and delayed reward signals induce excessive early-stage exploration, where standard entropy regularization causes uncontrolled entropy growth in early trajectory steps rather than premature convergence to low-entropy states. This blind exploration creates unstable foundations that systematically commit to suboptimal behavioral patterns. Having locked into these flawed early trajectories, the agent then enters late-stage uncertainty propagation, where accumulated uncertainty from early steps compounds in late steps, maintaining dangerously high entropy levels that prevent coherent strategy formation and further degrade performance (seen from PPOs persistently high entropy in Figure 1). In contrast, our EPO method controls cross-step entropy propagation, maintaining stable entropy levels and achieving consistent reward improvement. Therefore, how to design exploration mechanisms that navigate the exploration-exploitation tradeoff without triggering cascade failure remains an important problem for multi-turn agent training. To address this cascade issue, we propose Entropy-regularized Policy Optimization (EPO), novel framework that combines entropy regularization with specialized mechanisms for stable on-policy training under sparse reward conditions. Our framework is guided by the central insight that standard entropy regularization is insufficient because it lacks temporal awareness. We find that anchoring the policys entropy to dynamically adjusted historical bound provides the necessary stability to halt the exploration-exploitation cascade failure without sacrificing essential exploration. Our contributions are fourfold. ❶ We identify and formally characterize the exploration-exploitation cascade failure in multi-turn sparse-reward environments, revealing how excessive early-stage exploration leads to unstable behavioral foundations that compound into late-stage uncertainty propagation, phenomenon that standard entropy methods cannot address. ❷ We adapt entropy regularization to the multi-turn setting by computing entropy across all turns within trajectories and averaging over trajectory batches, providing foundation that captures the unique temporal structure of agent interactions. ❸ We introduce an entropy smoothing regularizer that penalizes deviations from historical entropy averages, effectively dampening the severe oscillations between overconfidence and over-exploration that plague sparse reward environments. ❹ We develop an adaptive weighting scheme that dynamically balances exploration and exploitation across training phases, starting with conservative exploration, transitioning through balanced exploration-exploitation, and ending with strong stabilization to ensure convergence. Together, these components create theoretically grounded and general framework that guarantees monotonically decreasing entropy variance and optimal exploration-exploitation trade-offs while being compatible with any on-policy optimiza-"
        },
        {
            "title": "Under review",
            "content": "tion method. We validate EPO on challenging benchmarks ScienceWorld (Wang et al., 2022) and ALFWorld (Shridhar et al., 2021), achieving up to 152% performance improvement with significantly more stable training dynamics, transforming previously untrainable sparse-reward scenarios into smoothly converging optimization problems."
        },
        {
            "title": "2.1 REINFORCEMENT LEARNING FOR LLMS",
            "content": "RLHF (Ouyang et al., 2022) and DPO (Rafailov et al., 2023) have become foundational approaches for aligning LLMs with human preferences, with both methods significantly improving model alignment and instruction-following capabilities. Recent RL methods such as GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025) further enhance LLM reasoning abilities during post-training through verified rewards. In contrast to PPO (Schulman et al., 2017), these methods leverage batch-wise advantage computation from identical prompts, obviating the critic model and substantially improving the computational tractability of large-scale RL training for LLMs. However, primary challenge in applying RL to LLMs is the phenomenon of policy entropy collapse: models rapidly reduce their stochasticity, converging on narrow, over-optimized behaviors (Cui et al., 2025; Dong et al., 2025; Wang et al., 2025a; Deng et al., 2025; He et al., 2025; Cheng et al., 2025a;b). In response, recent work has focused on integrating entropy control mechanisms into the optimization process to preserve policy diversity. For instance, Cui et al. (Cui et al., 2025) regulate the impact of high-covariance tokens by applying clipping function and KL penalty. Meanwhile, Cheng et al. (Cheng et al., 2025b) augment the standard advantage function with clipped, gradient-detached entropy term, which encourages deeper reasoning chains without altering the original policy optimization direction. 2.2 REINFORCEMENT LEARNING FOR LLM AGENTS To enhance their autonomy, LLM agents are designed to interact with external environments using diverse toolsets (OpenAI, 2025a; Team, 2025). However, training agents to complete multi-step tasks with these tools presents significant challenges for standard reinforcement learning, including sparse rewards and credit assignment problems. To address these issues, seminal works have introduced advanced training paradigms such as hierarchical RL (Zhou et al., 2024), autonomous learning (Bai et al., 2024), and off-policy Q-learning (Bai et al., 2025). Meanwhile, another line of research employs supervised fine-tuning (SFT) to directly enhance the models decision-making abilities, training them on vast datasets of high-quality tool-use trajectories to master complex environments and APIs (Xi et al., 2024; Zhang et al., 2024; Qin et al., 2024). Recently, to leverage the training stability of GRPO (Shao et al., 2024), researchers have extended single-turn GRPO to multi-turn training settings with various training techniques to improve performance (Jin et al., 2025; Feng et al., 2025; Wang et al., 2025c). To guide learning in long-horizon scenarios, RLVMR (Zhang et al., 2025) introduces verifiable meta-reasoning rewards that provide dense, intermediate feedback on the agents reasoning process. However, these methods overlook the critical exploration-exploitation dynamics unique to multi-turn settings, failing to address the exploration-exploitation cascade failure where policies undergo early-stage premature convergence followed by late-stage excessive exploration with high entropy that makes training vulnerable to noise. This fundamental gap leads to unstable training dynamics and prevents agents from effectively learning long-horizon strategies."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "3.1 ON-POLICY OPTIMIZATION On-policy optimization is fundamental paradigm in reinforcement learning where the agent learns to improve its policy by directly optimizing the expected return using trajectories sampled from the current policy. Given parameterized stochastic policy πθ(as) with parameters θ Rd, the objective is to maximize the expected return,given by J(θ) = Eτ πθ [R(τ )], where τ denotes trajectory and R(τ ) = (cid:80)T t=0 γtrt is the discounted return. On-policy optimization methods build on:"
        },
        {
            "title": "Under review",
            "content": "θJ(θ) = Eτ πθ (cid:34) (cid:88) (cid:35) θ log πθ(atst)Aπθ (st, at) t=0 (1) where Aπθ (st, at) = Qπθ (st, at) πθ (st) is the advantage function. While the policy gradient provides an unbiased estimator of θJ(θ), directly optimizing this objective can lead to instability due to large policy updates. To address this, modern on-policy methods employ surrogate objective functions that approximate the policy gradient while ensuring stable learning. The standard policy gradient can be reformulated as the surrogate objective LP G(θ) = Eτ πθold . However, this surrogate objective is only valid for infinitesimally small updates. Proximal Policy Optimization (PPO) (Schulman et al., 2017) addresses this limitation by constraining the policy ratio through clipped surrogate objective: (cid:104) πθ(atst) πθold (atst) ˆAt (cid:105) LCLIP (θ) = Eτ πθold (cid:104) min (cid:16) rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17)(cid:105) (2) πθold (atst) is the importance sampling ratio, Eτ πθold where rt(θ) = πθ(atst) [] denotes expectation over trajectories sampled under πθold , and ϵ defines the trust region bounds. Group Relative Policy Optimization (GRPO) (Shao et al., 2024) extends PPO by modifying the advantage function computation. Instead of using standard advantages ˆAt, GRPO employs group-relative advantages, computed as At = Rtµg σg+δ . where Rt is the return from timestep t, µg and σg are the mean and standard deviation of returns within group g, and δ is small constant for numerical stability. This group-based normalization provides more stable gradient estimates. 3.2 PROBLEM FORMULATION We formalize the multi-turn task as sequential decision-making reinforcement learning problem. single LLM agent πθ executes task through turns over trajectory τ = (s0, a0, r0, . . . , sT , aT , rT ). The reward is sparse, with rt = 0 for all intermediate turns and only the final turn receiving the task outcome reward, such that the total return is R(τ ) = (cid:80)T t=0 γtrt = rT , where rT {0, 1} represents the binary task outcome. In our experimental settings, specifically ALFWorld (Shridhar et al., 2021) and SciWorld (Wang et al., 2022), we assume an undiscounted formulation where γ = 1. All turns within the same task share the final outcome reward, creating credit assignment challenge across sequential turns. The multi-turn policy optimization differs from standard RL in that losses accumulate across all turns before parameter updates: LM (θ) = Eτ πθold [EtT [min (rt(θ)At, clip(rt(θ), 1 ϵ, 1 + ϵ)At)]] (3) where rt(θ) = πθ(atst) πθold (atst) and At represents the advantage estimate at turn within the multi-turn trajectory. Et[] denotes expectation over timesteps within turn t, while the outer expectation is over trajectories sampled under πθold."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "4.1 ENTROPY REGULARIZATION To address the exploration-exploitation cascade failure, we first adapt entropy regularization to capture the temporal structure of multi-turn interactions. Unlike traditional RL where entropy is computed per-step, we recognize that in multi-turn environments, early trajectory decisions compound across subsequent turns. Therefore, we compute entropy across all turns within each trajectory and average over the batch of trajectories. The entropy-regularized policy loss is formulated as LER(θ) = LM (θ) λLH (θ), where λ is the entropy coefficient, and the entropy loss is averaged over the batch of trajectories: LH (θ) = 1 (cid:88) j= 1 1 (cid:88) t=0 1 τj,t τj,t (cid:88) i= Hj,t,i (4)"
        },
        {
            "title": "Under review",
            "content": "where is the batch size (number of trajectories), is the number of turns per trajectory, Hj,t,i is the entropy at token position in turn of trajectory j, and τj,t represents the sequence length at turn of trajectory j. The token-level entropy is computed from the models probability distribution over the vocabulary at each position as = (cid:80) vV p(vw<t) log p(vw<t), where p(vw<t) is the probability of token from vocabulary given the preceding context w<t."
        },
        {
            "title": "4.2 ENTROPY SMOOTHING REGULARIZER",
            "content": "To break the cascade failures two-phase pattern, excessive early exploration followed by late-stage uncertainty propagation, we introduce an entropy smoothing mechanism that prevents the dangerous oscillations observed in sparse reward settings. We maintain an entropy history window Wk = { H0, . . . , Hm, . . . , Hk1} for RL step k, storing the average entropy Hm across all trajectories at the token level for each previous RL step m. The historical entropy reference is computed as the Hm. This historical anchoring prevents the uncontrolled entropy growth that mean Wk = 1 characterizes the early exploration phase of cascade failure. We apply token-wise penalty based on acceptable entropy ranges relative to this historical average: (cid:80)k1 m=0 Pn,t,i = (cid:26)0, α, if κl Wk Hn,t,i κr Wk , otherwise. (5) The boundary coefficients κl and κr define the acceptable range, while α provides the penalty weight for tokens with entropy outside the desired range. By bounding entropy within historical averages, we prevent both the blind exploration of early stages and the chaotic uncertainty propagation of late stages. Aggregating these penalties across all tokens, turns, and trajectories yields the smoothing loss: Lsmooth(θ) = 1 (cid:88) n=1 1 T 1 (cid:88) t=0 1 τn,t τn,t (cid:88) i=1 Pn,t,i (6) The complete entropy-smoothed policy optimization loss is then defined as LEP O(θ) = LM (θ) λ[LH (θ) βkLsmooth(θ)], where the dynamic coefficient βk follows an exponential schedule that directly counteracts the cascade failures progression: βk = βstart + (1 βstart) 1 + (βend 1) (cid:16) 1 eλd kkmid Kkmid kmid (cid:19) 1 eλd (cid:18) (cid:17) , if kmid , if > kmid (7) This adaptive schedule specifically addresses each phase of the cascade failure: it begins with conservative exploration to prevent early-stage behavioral lock-in, transitions through balanced exploration-exploitation at mid-training (kmid = K/2), and increases smoothing strength in later phases to prevent uncertainty propagation and ensure stable convergence. The parameters βstart, βend, λd, and control the transition dynamics and training duration. Algorithm 1 presents the full optimization procedure, incorporating these components."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTS SETUP This section outlines our experimental setup, including the benchmarks, evaluation protocol, baselines, and implementation details. Further details can be found in Appendix B. Benchmark. We evaluate on two challenging benchmarks that require different reasoning capabilities, ScienceWorld (Wang et al., 2022) and ALFWorld (Shridhar et al., 2021). ScienceWorld focuses on text-based scientific experimentation, demanding systematic hypothesis testing and structured exploration. ALFWorld is an embodied environment containing 4,639 household task instances across six categories, requiring multi-step decision-making and spatial reasoning. To improve the generalizability of our approach across multiple scenarios, we finetune the foundation model directly on the environment using RL, rather than employing trajectory finetuning for initialization."
        },
        {
            "title": "Under review",
            "content": "Algorithm 1 Entropy-smoothed Policy Optimization (EPO) Require: Initial policy parameters θ, entropy coefficient λ, smoothing penalty α, entropy bounds κl, κr, and schedule parameters for βk. 1: Initialize policy parameters θ0. 2: Initialize historical entropy window W0 = . 3: for each training step = 0, 1, 2, . . . , 1 do Collect batch of trajectories Dk = {τj}B 4: Compute advantages ˆAt for all timesteps in Dk. Compute the multi-turn policy loss LM (θ) using Equation 3. j=1 using the old policy πθold. Compute the trajectory-aware entropy loss LH (θ) over the batch using Equation 4. if > 0 then Compute the historical entropy mean Wk from the window Wk. Compute the smoothing loss Lsmooth(θ) based on Wk using Equation 6. Start of EPO specific computations else 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end for Ensure: Optimized policy parameters θK. Lsmooth(θ) 0. end if Update the dynamic coefficient βk according to the schedule in Equation 7. Combine losses to form the final EPO objective: LEP O(θ) = LM (θ) λ[LH (θ) βkLsmooth(θ)]. Update the policy parameters θ by optimizing LEP O(θ). Update the historical entropy window Wk+1 Wk { Hcurrent batch}. Evaluation Setting. To evaluate generalization capabilities, we focus on two key evaluation scenarios: IID (in-distribution) covers seen task variants and categories, while OOD (out-of-distribution) evaluates on unseen task variants within seen categories. This design allows us to measure both optimization effectiveness and generalization robustness, which are crucial for practical deployment. We employ dual success rate metrics to capture different aspects of performance: Succ. reports the average of maximum success rates across random seeds, while Succ. measures average performance after convergence, reflecting practical reliability. Given the high variance inherent in RL, final performance scores alone can be misleading. We therefore present averaged curves to provide more robust comparison and illustrate the performance evolution throughout the training process. Baselines. We conduct comprehensive comparisons across multiple paradigms: (1) Promptingbased approaches such as ReAct (Yao et al., 2023) that utilize in-context learning without parameter optimization; (2) Trajectory-based and platform methods including supervised fine-tuning (SFT) through expert trajectory imitation and AgentGym (Xi et al., 2024) which provides unified framework with behavioral cloning and self-evolution mechanisms; (3) General reinforcement learning approaches encompassing standard on-policy methods (PPO (Schulman et al., 2017), GRPO (Shao et al., 2024)); (4) Agent RL approaches including recent methods specifically designed for agent training (GiGPO (Feng et al., 2025), RLVMR (Zhang et al., 2025)). Our proposed EPO methodology is architected as general enhancement framework that can be seamlessly integrated with existing RL paradigms, as exemplified through our PPO+EPO and GRPO+EPO implementations. Implementation Details. To optimize performance for each benchmarks complexity, we employ Qwen2.5-3B-Instruct for ALFWorld and the larger Qwen2.5-7B-Instruct for ScienceWorlds more complex scientific reasoning tasks. Constrained by computational resources, we adopt single foundation model per task whose size is sufficient to ensure proper convergence, as smaller models consistently fail to converge. We conduct our own implementations and experiments for our proposed method, PPO, and GRPO baselines across three random seeds to ensure statistical reliability. Results for other baseline methods (ReAct, AgentGym, SFT, GiGPO, RLVMR) are sourced from the RLVMRs (Zhang et al., 2025) paper. To account for their different convergence characteristics, we trained the model for 120 RL steps on ScienceWorld and 150 steps on ALFWorld."
        },
        {
            "title": "Under review",
            "content": "Table 1: For PPO and GRPO baselines with our EPO method: better performance indicates improvement over baseline, worse performance indicates degradation. Highlighted values represent the best performance among other baseline methods. shows the relative improvement (%) when applying our method. Results for other baseline methods (ReAct, AgentGym, SFT, GiGPO, RLVMR) are sourced from the RLVMR (Zhang et al., 2025) paper. We ran our own implementations of PPO, GRPO, and EPO, tuning hyperparameters across multiple trials to obtain stable results. Method LLM IID OOD LLM ScienceWorld ALFWorld IID OOD ReAct GPT-4o ReAct DeepSeek-R1 ReAct Qwen2.5-7B AgentGym LLaMa2-7B SFT Qwen2.5-7B GiGPO Qwen2.5-7B RLVMR Qwen2.5-7B PPO Qwen2.5-7B +EPO Qwen2.5-7B GRPO +EPO Qwen2.5-7B Qwen2.5-7B - - - - - - - 38.4 96. 49.2 31.4 11.3 33.6 32.0 35.2 43.0 58.3 100.0 Succ. Succ. Succ. Succ. 45.4 22.2 7.8 46.9 36.7 53.4 67.2 64.6 100.0 54.8% 152.1% 71.5% 146.0% 80.9 93.8 95.8 81.3 2.1% 2.7% 4.5% 0.5% - - - - - - - 39.1 96.2 91.7 95.8 81.6 83.8 GPT-4o DeepSeek-R1 Qwen2.5-7B LLaMa2-7B Qwen2.5-7B Qwen2.5-7B Qwen2.5-7B Qwen2.5-3B Qwen2.5-3B Qwen2.5-3B Qwen2.5-3B 66.0 70.2 28.5 63.3 57.0 90.2 91.8 87.5 91.7 - - - - - - - 72.3 73.4 Succ. Succ. Succ. Succ. 57.3 68.8 23.1 76.6 63.3 89.5 91.4 95.8 85.4 - - - - - - - 70.9 74.3 -10.9% 1.5% 4.8% 4.8% 63.5 87.5 91.7 75.4 4.8% 19.8% 7.6% 18.7% 63.3 75. 83.3 89.6 (a) Training Rewards (ScienceWorld) (b) IID Success Rate (ScienceWorld) (c) OOD Success Rate (ScienceWorld) (d) Training Rewards (ALFWorld) (e) IID Success Rate (ALFWorld) Figure 2: Training dynamics and generalization performance analysis. We present the evolution of training rewards and validation success rates across both in-distribution (IID) and out-of-distribution (OOD) evaluation settings. (a-c) ScienceWorld experimental results contrasting PPO and PPO+EPO performance across training reward accumulation, IID validation, and OOD validation metrics. (df) ALFWorld experimental results contrasting GRPO and GRPO+EPO under identical evaluation criteria. Our EPO enhancement exhibits significantly improved training stability and substantial performance gains across both IID and OOD evaluation scenarios against baseline methods. (f) OOD Success Rate (ALFWorld) 5.2 PERFORMANCE COMPARISON Quantitative Results Analysis. Table 1 presents comprehensive performance comparisons across both ScienceWorld and ALFWorld environments. Our EPO enhancement demonstrates substantial improvements when integrated with existing RL methods. Notably, PPO+EPO achieves remarkable 152.1% improvement in averaged success rates (Succ.) on ScienceWorld IID tasks, significantly outperforming agent-specialized RL methods including GiGPO(53.4%) and RLVMR(67.2%). The dramatic improvement of PPO stems from addressing its inherent tendency toward aggressive policy updates that can cause severe entropy collapse in multi-turn interactions, particularly problematic in ScienceWorlds sparse reward environment where maintaining exploration is crucial. Our entropy smoothing regularization directly mitigates this issue by stabilizing policy entropy across sequential turns. In contrast, GRPOs more conservative update mechanism makes it less susceptible to such"
        },
        {
            "title": "Under review",
            "content": "(a) Training Rewards (ScienceWorld) (b) IID Success Rate (ScienceWorld) (c) OOD Success Rate (ScienceWorld) (d) Training Rewards (ALFWorld) (e) IID Success Rate (ALFWorld) Figure 3: Ablation studies on entropy regularization components. (a-c) ScienceWorld comparison of EPO versus EPO-Base without entropy smoothing, demonstrating that smoothing is essential for stable convergence in sparse reward settings. (d-f) ALFWorld comparison of EPO with dynamic βk versus EPO W/O DW using constant β, showing that adaptive weighting significantly accelerates early training progress. (f) OOD Success Rate (ALFWorld) entropy instabilities, resulting in more modest but consistent improvements (9.5% on ALFWorld IID tasks) when combined with EPO. We emphasize the Succ. metric as it represents performance averaged across multiple evaluation episodes, providing more robust and fair comparison by reducing variance from individual runs. Training Dynamics Analysis. Figure 2 illustrates the training dynamics and validation performance, revealing key insights into our methods effectiveness. The training reward curves demonstrate that EPO-enhanced methods achieve substantially higher reward accumulation while maintaining superior stability. In ScienceWorld, PPO+EPO reaches approximately 2x higher training rewards (15 vs. 8) with smooth monotonic trajectories, while ALFWorld shows consistent improvements with GRPO+EPO maintaining steady upward trends throughout training. The validation performance curves reveal the most significant improvements in early training phases and overall convergence behavior. On ScienceWorld, our EPO variants achieve rapid convergence to high success rates (>0.8 for both IID and OOD) within 40 training steps, compared to baseline methods that struggle to exceed 0.4 even after 100 steps. On ALFWorld, while baseline methods exhibit substantial oscillations and instability, our EPO-enhanced approaches demonstrate more consistent performance with reduced variance, particularly evident in the OOD evaluation where baselines frequently drop below 0.2 while EPO variants maintain performance above 0.4. The key observable pattern across both environments is the elimination of the characteristic oscillations between premature convergence and over-exploration that plague standard RL methods. This stability translates to more reliable training outcomes and enhanced generalization capabilities, directly validating our entropy regularization frameworks effectiveness in addressing the exploration-exploitation dilemma in multi-turn LLM agent training. 5.3 ABLATION STUDY Figure 3 presents ablations on two key components: the entropy smoothing regularizer and its dynamic weighting coefficient βk. On ScienceWorld, removing the smoothing regularizer (EPOBase) severely degrades performance. EPO-Base exhibits delayed convergence with minimal rewards until step 40 and plateaus at 0.5-0.6 success rate, while full EPO achieves meaningful learning by step 20 and reaches 0.8-1.0 success rate, demonstrating 50-60% improvement. ScienceWorlds sparse reward structure induces severe oscillations between exploration and exploitation, which the smoothing regularizer effectively stabilizes by maintaining entropy within historical bounds defined by κl and κr. On ALFWorld, replacing the dynamic coefficient βk with constant weight (EPO W/O DW) yields similar final performance around 0.7-0.8 success rate but results in slower convergence during the initial 40-60 episodes and increased training variance. The adaptive schedule in Equation 7,"
        },
        {
            "title": "Under review",
            "content": "(a) EPO-Base vs EPO-Decay (b) EPO vs EA Success Rate Figure 4: Model studies on ScienceWorld. (a) Consistent entropy regularization outperforms decaying schedules, preventing cascade failure. (b) EPO achieves near-perfect success while EA plateaus at 0.50.6 due to reasoning degradation. (c) Decay schedules prematurely suppress crucial early-turn exploration, triggering late-stage uncertainty propagation. (c) Early vs Late Entropy which progresses from βstart through unity to βend, automatically modulates regularization intensity across training phases, thereby improving convergence efficiency while preserving final performance quality. For more detailed ablation study, it can refer to Appendix B.3. 5.4 MODEL STUDY 5.4.1 STUDY OF ENTROPY REGULARIZATION We compare our standard method, PPO+EPO-Base, which applies consistent entropy regularization throughout training, against PPO+EPO-Decay, variant employing dynamic schedule that assigns higher entropy weight during initial training phases to promote exploration and systematically reduces it in later phases to encourage exploitation. Counter-intuitively, empirical results in Figure 4 show this decay strategy consistently underperforms across all metrics. While the decay schedule successfully reduces policy entropy in later training stages, it prematurely suppresses exploration in the crucial initial turns of each episode. Panel (c) reveals that comparing average entropy of the first 10 tokens (Early Steps) with the last 10 tokens (Late Steps) shows insufficient early exploration locks agents into suboptimal strategies from which they cannot recover, even as the policy becomes more deterministic. Insight: Directly changing loss weights fails in LLM agent scenarios due to the explorationexploitation cascade failure in multi-turn settings. Unlike single-turn tasks, multi-turn environments exhibit strong temporal dependencies where early steps fundamentally shape late-step outcomes. The decay schedule triggers excessive early-stage exploration that creates unstable foundations, systematically committing to suboptimal behavioral patterns. These flawed early trajectories then lead to late-stage uncertainty propagation, where accumulated uncertainty compounds and prevents coherent strategy formation. Therefore, for complex multi-turn sparse-reward tasks, maintaining robust and consistent exploration pressure throughout all trajectory steps is essential to avoid cascade failure, rather than following conventional exploration-to-exploitation scheduling. 5.4.2 STUDY OF ENTROPY-SHAPED ADVANTAGE We also compare our Entropy-smoothed Policy Optimization (EPO) with the Entropy-based Advantage (EA) shaping method from (Cheng et al., 2025b). As shown in Figure 4(b), while PPO+EA improves over the baseline, PPO+EPO achieves substantially superior performance in both final performance and convergence speed, reaching near-perfect success rates (1.0) while EA plateaus at 0.50.6. The primary difference lies in the gradient signal and how it affects the underlying LLMs capabilities. EA uses detached entropy term acting as an indirect intrinsic reward, providing no gradient signal to explicitly increase entropy. In contrast, EPO integrates entropy directly into the policy loss, enabling direct gradient signals θLH (θ) to guide the policy towards more exploratory behavior. Furthermore, EAs hard clipping on the advantage bonus can induce training instability and its myopic nature considers only instantaneous entropy. Insight: Directly modifying the policy loss for LLM agent RL can severely impair the models reasoning abilities in agent scenarioscapabilities that were not developed during pretraining. Since LLMs are not pretrained on agent-specific tasks, aggressive entropy regularization directly injected into the policy loss disrupts the models learned representations and reasoning pathways. Our EPO"
        },
        {
            "title": "Under review",
            "content": "method addresses this by using temporal smoothing with historical entropy windows, which preserves the LLMs inherent reasoning capabilities while providing exploration guidance. This decoupled regularization maintains the integrity of the value signal and the pretrained knowledge, leading to more robust and effective learning without degrading the models fundamental abilities."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we identified and addressed the exploration-exploitation cascade failure which is fundamental challenge unique to training LLM agents in multi-turn environments with sparse rewards. Our proposed EPO framework addresses this through trajectory-aware entropy computation, entropy smoothing regularization, and adaptive phase-based weightingmechanisms that together prevent the dangerous entropy oscillations that destabilize training. Empirical results demonstrate up to 152% performance improvement on ScienceWorld and 19.8% on ALFWorld, transforming previously untrainable scenarios into smoothly converging optimization problems. This work establishes that multi-turn LLM agent training requires fundamentally different entropy control than traditional RL, opening new directions for developing effective training methods for LLM Agents."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude code: Deep coding at terminal velocity, 2025. URL https://www. anthropic.com/claude-code. Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. Hao Bai, Yifei Zhou, Li Erran Li, Sergey Levine, and Aviral Kumar. Digi-q: Learning q-value functions for training device-control agents. arXiv preprint arXiv:2502.15760, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025a. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025b. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng Gao, Wayne Xin Zhao, and Ji-Rong Wen. From trial-and-error to improvement: systematic analysis of llm exploration mechanisms in rlvr. arXiv preprint arXiv:2508.07534, 2025. Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, et al. Rl-plus: Countering capability boundary collapse of llms in reinforcement learning with hybrid-policy optimization. arXiv preprint arXiv:2508.00222, 2025. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference on Language Modeling, 2025. URL https://openreview. net/forum?id=Rwhi91ideu. Jerop Kipruto and Ryan J. Salva. Get coding help from gemini code assist now for free, March 2025. URL https://blog.google/technology/developers/ gemini-code-assist-free/. Sara Klein, Simon Weissmann, and Leif Döring. Beyond stationarity: Convergence analysis of stochastic softmax policy gradient methods. arXiv preprint arXiv:2310.02671, 2023. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 19281937. PmLR, 2016."
        },
        {
            "title": "Under review",
            "content": "OpenAI. Computer-using agent: Introducing universal interface for ai to interact with the digital world. 2025a. URL https://openai.com/index/computer-using-agent. OpenAI."
        },
        {
            "title": "Introducing",
            "content": "codex, 2025b. URL https://openai.com/index/ introducing-codex/. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=dHng2O0Jjr. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Han Shen. On entropy control in llm-rl algorithms. arXiv preprint arXiv:2509.03493, 2025. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. URL https://arxiv.org/abs/2010.03768. Richard Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3 (1):944, 1988. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Tongyi DeepResearch Team. Tongyi-deepresearch. https://github.com/Alibaba-NLP/ DeepResearch, 2025. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1127911298, 2022. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025a. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=OJd3ayDDoF."
        },
        {
            "title": "Under review",
            "content": "Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025c. Ronald Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241268, 1991. Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, et al. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151, 2024. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1364313658, 2024. Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. arXiv preprint arXiv:2507.22844, 2025. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. In International Conference on Machine Learning, pp. 6134961385. PMLR, 2024. Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. ArCHer: Training language model agents via hierarchical multi-turn RL. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=b6rA0kAHT1."
        },
        {
            "title": "CONTENTS",
            "content": "1 Introduction 2 Related Work"
        },
        {
            "title": "2.2 Reinforcement Learning for LLM Agents",
            "content": ". . . . . . . . . . . . . . . . . . . . . . 3 Preliminary"
        },
        {
            "title": "3.1 On-policy Optimization .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.2 Problem formulation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Methodology 4.1 Entropy Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Entropy Smoothing Regularizer . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Experiments 5.1 Experiments Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Performance Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Ablation Study . 5.4 Model Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 Study of Entropy Regularization . . . . . . . . . . . . . . . . . . . . . . . 5.4. Study of Entropy-Shaped Advantage . . . . . . . . . . . . . . . . . . . . . 6 Conclusion Theoretical Analysis of Entropy-Smoothed Policy Optimization Experiments Core Implementation Pseudocode System Prompts Limitation and Future Work Use of Large Language Models 1 3 3 3 3 4 4 4 5 5 7 8 9 9 10 15 19 28 31"
        },
        {
            "title": "Under review",
            "content": "A THEORETICAL ANALYSIS OF ENTROPY-SMOOTHED POLICY OPTIMIZATION A.1 PRELIMINARY SETUP We build upon the analytical framework from the reference work (Shen, 2025; Sutton et al., 1999; Klein et al., 2023), leveraging their established lemmas without reproducing full proofs. Lemma 1 (Performance Difference). For any two policies π and π, and any state s0: π(s0) π (s0) = Eπ (cid:34)H1 (cid:88) t=0 Lemma 2 (Entropy Gradient). For softmax policy πθ: (cid:35) Aπ (cid:12) (cid:12) (st, at) (cid:12) (cid:12) s0 H(πθ) = Eπθ (cid:34)H1 (cid:88) h=0 log πθ(ahsh) (cid:35) log πθ(atst) H1 (cid:88) t=h Lemma 3 (Entropy Bias). For the entropy-regularized objective with optimal policy π arg maxπ π λ = λ (D): π (s0) πθ (s0) π λ λ (s0) πθ λ (s0) + λH log (s0) 1 H (s0) denote the set of optimal action sequences for given initial state s0 and horizon of where H. Lemma 4 (Performance Bound under Gradient Norm). If πθ λ (D) ϵ: π λ (s0) πθ λ λ (s0) 1 2λ D2 λ (s0) πθ ϵ2 A.2 THE EPO PERFORMANCE BOUND The EPO objective πθ λ,β can be understood as three-fold optimization target: maximizing returns (exploitation), encouraging policy diversity (exploration), and constraining exploration behavior over time (stabilization). This addresses the core challenge that standard maximum entropy RL solves \"whether to explore,\" but not \"how to explore stably.\" Definition 5 (EPO Objective). The EPO-regularized objective is: πθ λ,β(D) := πθ (D) + λH(πθ) λβkLsmooth(θ) where Lsmooth(θ) is defined as in Equation 6: with penalty function: Lsmooth(θ) = 1 (cid:88) j=1 1 1 (cid:88) t=0 1 τj,t τj,t (cid:88) i=1 Pj,t,i Pj,t,i = (cid:26)0, α, if κl Wk Hj,t,i κr Wk otherwise Proposition 6 (Improved Performance Bound with EPO). Assume the policy follows softmax parameterization. If the EPO policy gradient satisfies πθ λ,β(D) ϵ, then for any query s0, the policy suboptimality is bounded by: π (s0) πθ (s0) ϵ2 D2 πθ 2λ λ,β(s0) (cid:124) (cid:125) (cid:123)(cid:122) Optimization Error where: + λH log (s0) 1 (cid:125) (cid:123)(cid:122) Standard Entropy Bias (cid:124) λβkΦπθ,π (cid:124) (s0) (cid:125) (cid:123)(cid:122) Corrective Bias from Smoothing"
        },
        {
            "title": "Under review",
            "content": "πθ λ,β(s0) is problem-dependent constant (specified below) Φπθ,π (s0) := Eπθ [Lsmooth(θ)s0] Eπ [Lsmooth(θ)s0] represents the smoothing gap Proof. Let π λ,β = arg maxπ π λ,β(D) be the optimal policy under EPO. By optimality: π λ,β (s0) π λ,β λ,β(s0) Expanding the EPO objectives: π λ,β (s0) πθ λ,β λ,β(s0) λ,β(s0) π λ,β(s0) πθ (cid:16) π (s0) + λH(πs0) λβkEπ [Lsmooths0] = (cid:0)V πθ (s0) + λH(πθs0) λβkEπθ [Lsmooths0](cid:1) (cid:17) Rearranging: π (s0) πθ (s0) λ,β λ,β(s0) π λ,β (s0) πθ + λ (H(πθs0) H(πs0)) λβk (cid:0)Eπθ [Lsmooths0] Eπ [Lsmooths0](cid:1) (8) (9) (10) We use two fundamental bounds on policy entropy. First, the entropy of an optimal policy π is bounded by the maximum possible entropy over its support set, the optimal trajectories (s0). Second, the entropy of any policy πθ is bounded by the sum of the maximum possible single-step entropies over the horizon H. Hmax = (cid:88) τ H (s0) = (s0) 1 (s0) log 1 (s0) (cid:18) 1 (s0) ( log H (s0)) (cid:19) = log (s0) H(πθs0) = H(a0, a1, . . . , aH1s0) = H1 (cid:88) t=0 H(atst) We then obtained bounds showing that the entropy of the optimal policy concentrates on optimal actions: H(πs0) log (s0) and established maximum entropy bound: H(πθs0) log A. Therefore: H(πθs0) H(πs0) log log (s0) = log (s0)1/H This gives us: π (s0) πθ (s0) λ,β π λ,β (s0) πθ λ,β(s0) + λH log (s0)1/H λβkΦπθ,π (s0) (11)"
        },
        {
            "title": "Under review",
            "content": "We adapt the proof technique from Lemma 1. The key modification is that our Q-function now includes the smoothing term. Given the deterministic transitions in our LLM setting (where st+1 is fully determined by (st, at)), we have: Qπθ t,λ,β(s, a) = r(s, a) + πθ t+1,λ,β(st+1) where the value function incorporates smoothing penalties. Following the derivation in the reference (equations leading to their Lemma 4), we express the performance gap as: π λ,β (s0) πθ λ,β λ,β(s0) H1 (cid:88) t=0 sP π λ,β (s0) [λDKL(πθ(s)πθ,β(s))] where πθ,β(as) exp(Qπθ t,λ,β(s, a)/λ) is the soft-optimal policy. The KL divergence can be bounded using the softmax parameterization: DKL(πθ(s)πθ,β(s)) 1 2λ2 (cid:13) (cid:13)Qπθ (cid:13) t,λ,β(s, ) λθs, c(s)1 (cid:13) 2 (cid:13) (cid:13) where c(s) is normalizing constant and 1 is the all-ones vector. The gradient of the EPO objective is: πθ λ,β(D) = H1 (cid:88) (cid:88) t=0 Pπθ (s) (cid:16) πθ(s) θs, Qπθ t,λ,β(s, ) λθs, (cid:17) Let us define the soft advantage term Aπθ t,λ,β(s) as: Aπθ t,λ,β(s) := Qπθ t,λ,β(s, )/λ θs, c(s)1 (12) where c(s) is state-dependent normalizing constant. Adapting the bounding technique from the reference, which combines the Cauchy-Schwarz inequality with properties of the softmax Jacobian, yields the following lower bound on the squared gradient norm: πθ λ,β(D)2 λ2C πθ λ,β(s0) H1 (cid:88) sP π λ,β (s0) (cid:13) Aπθ (cid:13) (cid:13) (cid:13) 2 (cid:13) t,λ,β(s) (cid:13) (13) t=0 This inequality is crucial as it connects the global gradient norm to the sum of local, per-state soft advantages. where: πθ λ,β(s0) = 2 min t,sS(s0) Pπθ (s)(min s,a πθ(as))2 min t,sS(s0) Pπθ (s) Pπ λ,β (ss0) with Cd = (AH )1/2 and S(s0) being the set of all states reachable from s0. Therefore: π λ,β (s0) πθ λ,β λ,β(s0) D2 2λC πθ λ,β(s0) πθ λ,β(D)2 (14) Substituting Equation 14 into Equation 11, and using the assumption πθ λ,β(D) ϵ: π (s0) πθ (s0) D2 λ,β(s0) πθ ϵ2 2λ + λH log (s0)1/H λβkΦπθ,π (s0) The bias reduction term λβkΦπθ,π (s0) effectively counteracts standard entropy bias when the optimal policy exhibits stable, low-variance entropy and the current policy experiences entropy violations. This correction becomes increasingly effective through the dynamic coefficient βk, which strengthens as training progresses to enable smooth transition from exploration-focused early training to stability-focused convergence. Consequently, EPO achieves strictly better bounds than standard maximum entropy RL when βkΦπθ,π (s0) > 0, directly addressing the large bias issue that becomes prohibitive in LLM settings."
        },
        {
            "title": "Under review",
            "content": "A.3 CORE PROOF STEPS FOR LEMMAS Lemma 1 (Performance Difference). performance difference between any two policies π and π is: For any {0, 1, . . . , 1} and state S, the (s) π π (s) = Eπ (cid:34)H1 (cid:88) t=h Aπ (cid:12) (cid:12) (st, at) (cid:12) (cid:12) (cid:35) sh = Proof. (s) π π (s) = Eπ = Eπ = Eπ = Eπ (cid:34)H1 (cid:88) t=h (cid:34)H1 (cid:88) t=h (cid:34)H1 (cid:88) t=h (cid:34)H1 (cid:88) t=h r(st, at) (cid:35) sh = (cid:12) (cid:12) (cid:12) (cid:12) π (s) (15) r(st, at) + H2 (cid:88) t=h π t+1(st+1) H2 (cid:88) t=h (cid:12) (cid:12) π t+1(st+1) (cid:12) (cid:12) (cid:35) sh = π (s) Qπ (st, at) H1 (cid:88) t=h (cid:12) (cid:12) π (st) (cid:12) (cid:12) (cid:35) sh = Aπ (cid:12) (cid:12) (st, at) (cid:12) (cid:12) (cid:35) sh = (16) (17) (18) The key insight is telescoping the value functions and using Qπ (st, at) = r(st, at) + π t+1(st+1). Lemma 2 (Entropy Bias). It holds that: π (s0) πθ (s0) π λ λ (s0) πθ λ (s0) + λH log (s0) 1 A where π λ = arg maxπ π By optimality of π λ (D) and π arg maxπ π(D). λ for the entropy-regularized objective: λ π λ (s0) π π λ (s0) π λ (s0) πθ λ λ (s0) (s0) πθ (s0) + λ(H(πs0) H(πθs0)) The key bounds on entropy: H(πs0) log H(πθs0) log H (s0) (optimal policy is concentrated) (maximum entropy bound) (19) (20) (21) (22) Therefore: H(πs0) H(πθs0) log (s0) log = log (s0)1/H Lemma 3 (Performance Bound under Gradient Norm). πθ λ (D) ϵ: Assume the policy is softmax. If π λ (s0) πθ λ λ (s0) 1 2λ D2 λ (s0) πθ ϵ2 First, express the performance gap using KL divergence: π λ (s0) πθ λ λ (s0) H1 (cid:88) t=0 sPπ λ [λDKL(πθ(s)πθ(s))] (23) (s0)"
        },
        {
            "title": "Under review",
            "content": "where πθ(as) exp(Qπθ t,λ(s, a)/λ). For softmax policies, the KL divergence bound: DKL(πθ(s)πθ(s)) 1 2λ2 (cid:13) (cid:13)Qπθ (cid:13) t,λ(s, )/λ θs, c(s)1 (cid:13) 2 (cid:13) (cid:13) The gradient norm lower bound (using Cauchy-Schwarz): πθ λ (D)2 (cid:88) H1 (cid:88) sS(s0) t=0 (Pπθ (s))2(min πθ(as))2 (cid:13) (cid:13)Qπθ (cid:13) t,λ(s, )/λ θs, c(s)1 (cid:13) 2 (cid:13) (cid:13) Combining these with the constant: πθ λ (s0) = 2 min t,s Pπθ (s)(min πθ(as))2 min t,s Pπθ (s) Pπ (ss0) λ Lemma 4 (Entropy Gradient). For the softmax policy: H(πθ) = Eπθ (cid:34)H1 (cid:88) h=0 log πθ(ahsh) (cid:35) log πθ(atst) H1 (cid:88) t=h Starting from the entropy definition: H(πθ) = (cid:88) s0,a0,...,aH P(s0) H1 (cid:89) h=0 πθ(ahsh) H1 (cid:88) t= log πθ(atst) Taking the gradient and decomposing: H(πθ) = (cid:88) (cid:89) P(s0) πθ(ahsh) (cid:32) (cid:88) (cid:33) log πθ(atst) s0,... (cid:88) s0,... (cid:32) P(s0) (cid:33) πθ(ahsh) (cid:89) (cid:88) log πθ(atst) The first term vanishes because: (cid:88) (cid:32)H1 (cid:89) a0...aH h=0 (cid:33) πθ(ahsh) = (1) = 0 For the second term, using the product rule and the fact that Eaπθ(s)[ log πθ(as)] = 0: H(πθ) = Eπθ = Eπθ (cid:34)H1 (cid:88) h=0 (cid:34)H1 (cid:88) h=0 log πθ(ahsh) log πθ(ahsh) (cid:35) (cid:35) log πθ(atst) log πθ(atst) H1 (cid:88) t= H1 (cid:88) t=h (24) (25) (26) (27) (28) (29) The last equality follows from the tower property of expectation, where terms with < have zero expectation."
        },
        {
            "title": "B EXPERIMENTS",
            "content": ""
        },
        {
            "title": "Under review",
            "content": "B.1 DETAILED EXPERIMENT SETUP B.1.1 BENCHMARK We evaluate our method on two challenging and complementary benchmarks, ScienceWorld and ALFWorld, which test distinct yet crucial reasoning capabilities. ScienceWorld (Wang et al., 2022) is dynamic, text-based environment simulating grade-school science lab, where the agent must solve open-ended scientific tasks. Success demands systematic hypothesis testing, common-sense reasoning about object properties, and deep understanding of cause and effect. Its curriculum is divided into over 30 task types sourced from official study guides, primarily spanning: Physics, with tasks such as powering electrical circuits, testing the conductivity of materials, and manipulating states of matter; Chemistry, including identifying acids and bases or observing chemical reactions; and Life Science, which involves classifying organisms based on their traits. These tasks rigorously test an agents abstract knowledge and procedural reasoning. In contrast to the abstract challenges in ScienceWorld, ALFWorld (Shridhar et al., 2021) tests embodied reasoning in visually-rich environment. It requires an agent to interpret high-level natural language instructions and decompose them into long sequences of low-level actions within simulated household settings. The benchmark is structured around seven main task categories designed to test long-horizon planning and language grounding: (1) Pick & Place, for simple object relocation (e.g., \"Put mug in the coffee maker\"); (2) Pick Two & Place, for handling multiple objects; (3) Pick & Cool/Heat, requiring state changes using appliances; (4) Pick & Clean, involving interaction with sinks; and more complex compositional tasks like (5) Look At Obj & Pick and (6) Examine In Light. Success in ALFWorld hinges on multi-step task planning, spatial awareness, and the ability to ground language in physical context. Together, these two benchmarks provide comprehensive testbed for our agents capabilities, spanning from abstract knowledge application in ScienceWorld to embodied task execution in ALFWorld. B.1.2 EVALUATION SETTING We employ dual success rate metrics to capture different aspects of performance: Succ. reports the average of maximum success rates across random seeds, while Succ. measures average performance after convergence, reflecting practical reliability. To calculate the average converged success rate (Succ.), we first identify convergence period where performance stabilizes. In the ALFWorldenvironment, we observed that all methods exhibit similar convergence trends, with success rates plateauing after step 125. Therefore, we compute Succ. by averaging the success rates from step 125 onward (inclusive) across three random seeds. In contrast, the ScienceWorldenvironment exhibited more varied convergence behaviors across different random seeds, necessitating per-run analysis. Specifically, the epoch ranges for computing the converged success rate in the ScienceWorldenvironment were determined as follows: In our comparison with GRPO, the evaluation windows for the three random seeds of the GRPO baseline were set to epochs 70-120, 90-120, and 90-120. In stark contrast, for our method (GRPO with EPO), these windows began significantly earlier, spanning epochs 60-80, 80-120, and 25-120, respectively. similar trend was observed in the PPO comparison. The PPO baselines convergence was identified late in training, with windows of 100-120, 105-120, and 105-120. When enhanced with EPO, the model converged much faster, with its evaluation periods set to 70-120, 60-120, and 60-120 for the three seeds. This detailed breakdown confirms that EPO consistently accelerates convergence across different algorithms and seeds. Given the high variance inherent in RL, final performance scores alone can be misleading. We therefore present averaged curves to provide more robust comparison and illustrate the performance evolution throughout the training process. We apply wandbs default running average (window size of 10) to smooth all training curves. This standard practice avoids visualization-specific tuning and ensures fair comparison of the underlying learning trends. Additionally, we scale the variance by factor of 0.8 for better visual clarity."
        },
        {
            "title": "Under review",
            "content": "B.1.3 BASELINES We conduct comprehensive comparisons across multiple paradigms to evaluate the effectiveness of our proposed EPO methodology. These baselines are grouped into four categories, each representing distinct approach to training large language model (LLM) agents. Prompting-based Approaches This paradigm focuses on leveraging the in-context learning capabilities of LLMs without any parameter optimization. The ReAct framework (Yao et al., 2023) synergizes reasoning and acting in language models. Its core innovation is the interleaving of textual reasoning traces with actions that interact with an external environment. Unlike prior methods that treated reasoning and acting as separate processes, ReAct allows the model to create and adjust high-level plans while grounding them in reality by gathering information from the environment. As prompting-based method, ReActs performance relies on the quality of the in-context examples and the inherent capabilities of the base model. Its operational scope is confined to single-pass inference, without mechanisms for parameter optimization or learning from experiences across multiple episodes to discover novel policies. Trajectory-based and Platform Methods This category includes methods that rely on imitating expert trajectories and platforms designed for agent development. SFT is fundamental approach for adapting pre-trained language models to specific tasks by imitating expert-provided trajectories. The effectiveness of SFT is contingent upon the availability of comprehensive dataset of high-quality expert demonstrations. The resulting agents policy is inherently bounded by the scope of behaviors observed within this dataset, constraining its ability to explore novel strategies beyond the demonstrated examples. AgentGym (Xi et al., 2024) is comprehensive framework for building and evaluating generalist LLM-based agents, introducing self-evolution method, AgentEvol. While AgentGym provides valuable framework for evaluation, its AgentEvol method operates through form of behavioral cloning. The evolution of the agent is thus guided by the quality and diversity of the initial trajectory data, which influences its sample efficiency in exploring the environment. General Reinforcement Learning Approaches This group consists of well-established reinforcement learning algorithms that are not specifically designed for LLM agents but are widely used in the field. PPO (Schulman et al., 2017) is state-of-the-art on-policy reinforcement learning algorithm known for its stability and ease of implementation. It uses clipped surrogate objective function to constrain policy updates. When applied to multi-turn, sparse-reward environments, credit assignment in standard PPO is performed based on the terminal reward signal. This structure can present challenges in distributing credit across long sequence of actions, potentially leading to instabilities such as the entropy oscillation phenomenon. GRPO (Shao et al., 2024) is critic-free reinforcement learning algorithm. Instead of relying on learned value function, it compares the performance of group of trajectories generated from the same initial state. GRPO performs credit assignment at the trajectory level, evaluating the collective outcome of an entire episode. This design provides holistic signal for policy updates, rather than turn-by-turn feedback, which is consideration for learning complex, multi-step tasks. Agent RL Approaches This category includes recent reinforcement learning methods that are specifically designed for training LLM agents. GIGPO (Feng et al., 2025) extends GRPO by introducing two-level hierarchical structure for advantage estimation. It refines the trajectory-level credit assignment of GRPO by introducing micro-level analysis based on anchor states. The utility of this mechanism is related to the frequency with which these anchor states are revisited, and learning is guided by the single reward signal provided at the conclusion of each episode. RLVMR (Zhang et al., 2025) is framework that integrates dense, process-level supervision into the reinforcement learning loop by rewarding verifiable, meta-reasoning behaviors. RLVMR shapes the agents behavior by leveraging teacher model (e.g., GPT-4) to annotate expert trajectories with meta-reasoning tags. The learning process is thus guided by the quality and potential biases inherent in these teacher-provided annotations."
        },
        {
            "title": "Under review",
            "content": "B.1."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "All of our experiments were conducted on single server node equipped with eight NVIDIA H100 or A100 GPUs to ensure consistent and reproducible results. The training times varied based on the environments complexity, the RL algorithm, and the GPU architecture. On the computationally intensive SciWorld benchmark, full PPO training run required approximately 23 hours on H100s and 31 hours on A100s. The more efficient GRPO baseline was faster, completing in 16 hours on H100s and 20 hours on A100s. For the ALFWorld environment, the PPO baseline took 23 hours with H100s and 31 hours with A100s, while the GRPO baseline required 18 hours and 25 hours, respectively. Crucially, our proposed EPO method is designed for efficiency and introduces negligible computational overhead. The training times for our EPO-enhanced variants (PPO+EPO and GRPO+EPO) are effectively identical to their corresponding PPO and GRPO baselines under the same hardware configuration. The detailed hyperparameter configurations for each setup are presented in Table 3 and Table 2. Table 2: Hyperparameter settings for the SciWorld environment. Hyperparameter PPO GRPO PPO+EPO GRPO+EPO Foundation Model Total RL Steps (K) Max Prompt Length Max Response Length Test Frequency (steps) General Setup Qwen2.5-7B-Instruct 125 2048 512 5 Optimizer & LR Scheduler Optimizer LR Scheduler Learning Rate Warmup / Min LR Ratio AdamW Cosine (num_cycles = 0.5) 3 106 5 106 3 106 5 106 0.1 / 0.2 Batch Sizes & PPO-Specific Setup Mini-batch Size Micro-batch Size Critic Micro-batch Size λ βstart βend κl, κr λd λk 64 8 4 EPO 128 16 64 8 4 128 16 0.001 2.0 1.0 0.0, 2.0 3.0 0.05 B.2 PERFORMANCE COMPARISON Figure 5 demonstrates the training dynamics and generalization performance of our EPO enhancement across two environments and algorithms. Our EPO variants exhibit superior convergence characteristics in both settings. In ScienceWorld, GRPO+EPO achieves early convergence around step 60 with higher peak rewards than baseline GRPO ( Figure 5(a)). Similarly, PPO+EPO in ALFWorld maintains more consistent reward accumulation with reduced oscillations ( Figure 5(b)). This improved stability stems from EPOs entropy regularization guiding exploration toward productive policy regions. For ScienceWorld, GRPO+EPO demonstrates clear advantages across both IID and OOD settings, achieving success rates exceeding 0.8 within 40 steps while baseline GRPO struggles to surpass 0.6 ( Figure 5(c),(e)). In ALFWorld, PPO+EPO prioritizes OOD robustness over IID performance. While showing comparable IID results ( Figure 5(d)), PPO+EPO maintains consistent OOD success rates above 0.6 compared to baseline PPOs frequent drops below 0.4 ( Figure 5(f))."
        },
        {
            "title": "Under review",
            "content": "Table 3: Hyperparameter settings for the ALFWorld environment. Hyperparameter PPO GRPO PPO+EPO GRPO+EPO General Setup Foundation Model Total RL Steps (K) Max Prompt Length Max Response Length Test Frequency (steps) Qwen2.5-3B-Instruct 150 2048 512 Optimizer & LR Scheduler Optimizer LR Scheduler Learning Rate Warmup / Min LR Ratio AdamW Cosine (num_cycles = 0.5) 5 106 0.1 / 0.2 Batch Sizes & PPO-Specific Setup Mini-batch Size Micro-batch Size Critic Micro-batch Size 256 32 λ βstart βend κl, κr λd λk EPO 256 32 0.001 2.0 1.0 0.0, 2.0 3.0 0.1 The key advantage of EPO lies in variance reduction and elimination of training oscillations. Across both environments, EPO variants show tighter confidence intervals and more reliable convergence patterns. This stability particularly benefits OOD scenarios where baseline methods exhibit substantial performance degradation. These results validate our entropy regularization approach for addressing exploration-exploitation challenges in multi-turn LLM agent training, demonstrating simultaneous improvements in generalization capability and convergence reliability. B.3 ABLATION STUDY Figure 6 extends our ablation analysis to both GRPO and PPO variants across ScienceWorld and ALFWorld. In ScienceWorld (a,c,e), the entropy smoothing regularizer proves essential: GRPO+EPOBase exhibits severely delayed learning with rewards remaining near 2 until step 40 and success rates plateauing at 0.6, while GRPO+EPO achieves immediate progress reaching 7-8 rewards and 0.7-0.85 success rates with 40-50% relative improvement that persists throughout training. This pattern holds across both GRPO and PPO, confirming the mechanisms algorithm-agnostic benefits. ALFWorld (b,d,f) shows markedly different dynamics: both PPO variants converge to similar final performance ( 12.5 reward, 0.8 success rate), with PPO+EPO primarily demonstrating 20-episode faster convergence. This differential impact validates our theoretical frameworkScienceWorlds extreme sparsity (30+ actions before feedback) creates pathological exploration-exploitation oscillations that the smoothing regularizer effectively breaks by maintaining entropy within historical bounds. ALFWorlds structured feedback naturally prevents such oscillations, making smoothing beneficial for speed but not essential for convergence. The consistent improvements across both GRPO and PPO in sparse settings confirm that entropy smoothing addresses fundamental challenge in multi-turn optimization rather than algorithmspecific weaknesses. The adaptive βk weighting enables this by providing strong stabilization when oscillations are detected while relaxing constraints during natural convergence, transforming intractable sparse reward problems into smoothly converging optimization processes."
        },
        {
            "title": "Under review",
            "content": "(a) Training Rewards (ScienceWorld) (b) Training Rewards (ALFWorld) (c) IID Success Rate (ScienceWorld) (d) IID Success Rate (ALFWorld) (e) OOD Success Rate (ScienceWorld) (f) OOD Success Rate (ALFWorld) Figure 5: Training dynamics and generalization performance analysis. We present the evolution of training rewards and validation success rates across both in-distribution (IID) and out-ofdistribution (OOD) evaluation settings. (a,c,e) ScienceWorld experimental results contrasting GRPO and GRPO+EPO performance across training reward accumulation, IID validation, and OOD validation metrics. (b,d,f) ALFWorld experimental results contrasting PPO and PPO+EPO under identical evaluation criteria. Our EPO enhancement exhibits significantly improved training stability and substantial performance gains across both IID and OOD evaluation scenarios against baseline methods. B.4 MODEL STUDY B.4.1 STUDY OF ENTROPY REGULARIZATION To analyze the exploration-exploitation trade-off, we compare our standard method, PPO+EPOBase, which applies consistent entropy regularization coefficient throughout training, against an experimental variant, PPO+EPO-Decay. This variant was designed to test the hypothesis that dynamic schedule could improve performance. It employs formula to modulate the entropy weight over time: assigning higher weight during initial training phases to promote exploration, and systematically reducing the weight in later phases to encourage exploitation."
        },
        {
            "title": "Under review",
            "content": "(a) Training Rewards (ScienceWorld) (b) Training Rewards (ALFWorld) (c) IID Success Rate (ScienceWorld) (d) IID Success Rate (ALFWorld) - PPO (e) OOD Success Rate (ScienceWorld) - PPO (f) OOD Success Rate (ALFWorld) - PPO (g) Training Rewards (ALFWorld) - GRPO (h) IID Success Rate (ALFWorld) - GRPO (i) OOD Success Rate (ALFWorld) - GRPO Figure 6: Impact of the entropy smoothing regularizer on training dynamics and performance. This ablation study contrasts our full method (EPO) with variant that excludes the entropy smoothing regularizer (EPO-base). The comparison on ScienceWorld (a,c,e) and ALFWorld (b,d,f) demonstrates that the smoothing mechanism is essential for stable RL training progression. Contrary to our hypothesis, the empirical results in Figure 7 show this strategy is counterproductive. The PPO+EPO-Decay variant consistently underperforms the baseline across all metrics, including episodic reward (a), in-distribution success rate (b), and out-of-distribution success rate (c). Panel (d) provides insight into this failure by analyzing the intra-episode entropy, comparing the average entropy of the first 10 tokens (Early Steps) with the last 10 tokens (Late Steps). While the decay schedule successfully reduces the policys entropy in the later stages of training, it does so at significant cost. The schedule prematurely suppresses exploration in the crucial initial turns of each episode. This insufficient early exploration locks the agent into suboptimal strategies from which it cannot recover, even as the policy becomes more deterministic. This finding underscores that for complex, multi-turn tasks, maintaining robust and consistent exploration pressure is more effective than manually scheduling transition towards exploitation. B.4.2 STUDY OF ENTROPY-SHAPED ADVANTAGE We compare our Entropy-smoothed Policy Optimization (EPO) with the Entropy-based Advantage (EA) shaping method from Cheng et al. (Cheng et al., 2025b). As shown in Figure 8, while PPO+EA improves over the baseline, our PPO+EPO is substantially superior in both final performance and convergence speed. The primary difference lies in the gradient signal. The EA method uses detached entropy term , which acts as an indirect intrinsic reward rather than direct, optimizable objective. Consequently, the policy receives no gradient signal to explicitly increase its entropy. In contrast, our EPO formulation integrates entropy directly into the policy loss, enabling direct gradient θLH (θ) to explicitly guide the policy towards more exploratory behavior. Furthermore, EAs hard clipping on the advantage bonus can induce training instability, and its myopic nature considers only instantaneous entropy. Our EPO method promotes smoother and more consistent updates by using continuous smoothing"
        },
        {
            "title": "Under review",
            "content": "(a) Training Rewards (ScienceWorld) (b) IID Success Rate (ScienceWorld) (c) OOD Success Rate (ScienceWorld) (d) Entropy Comparison (ScienceWorld) Figure 7: Performance comparison of our standard PPO+EPO-Base against PPO+EPO-Decay, which uses decaying entropy coefficient, on the ScienceWorld benchmark. Panels (a-c) demonstrate that the dynamic decay schedule consistently degrades performance across episodic rewards and success rates. Panel (d) analyzes the intra-episode entropy for early versus late tokens, revealing that the decay schedule prematurely suppresses crucial early-turn exploration, which negatively impacts overall performance. regularizer that leverages historical entropy window. This temporal consistency is critical for long-horizon reasoning tasks. These theoretical advantages explain the empirical gap: PPO+EPO converges to near-optimal success rate of almost 1.0, while PPO+EA plateaus far lower at 0.5-0.6. We posit that EAs direct advantage modification distorts the credit assignment process. In contrast, EPOs decoupled regularization preserves the integrity of the value signal, leading to more robust and effective learning."
        },
        {
            "title": "Under review",
            "content": "(a) ScienceWorld (b) ScienceWorld (c) ScienceWorld (d) ScienceWorld (e) ScienceWorld (f) ScienceWorld Figure 8: Performance comparison on ScienceWorld environment: vanilla PPO, PPO with Entropybased Advantage shaping (PPO+EA) from Cheng et al. (Cheng et al., 2025b), and our PPO with Entropy-smoothed Policy Optimization (PPO+EPO). Results show episodic rewards (a,b), validation IID success rates (c,d), and OOD success rates (e,f). PPO+EPO consistently outperforms both baselines, achieving near-perfect success rates (1.0) compared to PPO+EAs plateau at 0.5-0.6. Curves show mean values with shaded standard error across multiple seeds."
        },
        {
            "title": "C CORE IMPLEMENTATION PSEUDOCODE",
            "content": "Algorithm 2 presents PyTorch-style pseudocode outlining the core computational steps for our proposed Entropy-Smoothed Policy Optimization (EPO) loss. This implementation directly corresponds to the methodology described in Section 3, detailing how the base policy loss, entropy regularization, and the entropy smoothing regularizer are combined to form the final training objective for single policy update step. Algorithm 2 PyTorch-style Pseudocode for EPO Loss Calculation 1 # Given: policy policy_pi, batch data, current epoch 2 # data contains: old_log_probs, advantages, response_mask, entropy_history 3 # Hyperparameters: lambda_, kappa_l, kappa_r, alpha, 4 5 # 1. Forward pass for current log probabilities and token-level entropy 6 logits = policy_pi(data.input_ids, data.attention_mask) 7 log_prob, entropy = get_logprob_and_entropy(logits, data.responses) 8 9 # 2. Compute the base multi-turn policy loss L^MT (e.g., PPO objective) 10 pg_loss = compute_policy_loss( 11 13 14 log_prob=log_prob, old_log_prob=data.old_log_probs, advantages=data.advantages, response_mask=data.response_mask, clip_ratio=0.2 28 15 16 ) 17 18 # 3. Compute the entropy regularization loss L^H (Eq. 6) 19 entropy_loss = agg_loss(entropy, data.response_mask) 20 21 # 4. Compute the entropy smoothing regularizer 22 # 4a. Calculate historical average entropy from window W_k 23 historical_avg_entropy = data.entropy_history.mean() 24 25 # 4b. Generate token-wise penalty mask based on historical avg (Eq. 8) 26 penalty_mask = generate_entropy_penalty( 27 current_entropy=entropy, historical_avg_entropy=historical_avg_entropy, min_ratio=kappa_l, max_ratio=kappa_r, penalty_weight=alpha 29 30 ) 31 32 # 4c. Calculate smoothing loss L^smooth by aggregating penalties (Eq. 9) 33 smoothing_loss = agg_loss(penalty_mask, data.response_mask) 34 35 # 4d. Get dynamic coefficient beta_k for the current step (Eq. 11) 36 beta_k = calculate_dynamic_beta(current_step=k, total_steps=K) 37 38 # 5. Combine entropy and smoothing terms 39 # Corresponds to [L^H(theta) - beta_k * L^smooth(theta)] 40 entropy_term = entropy_loss - beta_k * smoothing_loss 41 42 # 6. Compute the final EPO loss (Eq. 10) 43 # L^EPO = L^MT - lambda * [L^H - beta_k * L^smooth] 44 final_loss = pg_loss - lambda_ * entropy_term"
        },
        {
            "title": "D SYSTEM PROMPTS",
            "content": "This appendix details the system prompts used to guide the language model agents in the ALFWorld and ScienceWorld environments. For each environment, we provide two versions of the prompt: one that includes historical context (previous actions and observations) and one that omits it for the initial turn. The placeholders in curly braces, such as {current_observation}, are dynamically replaced with environment-specific information at runtime. D.1 ALFWORLD PROMPTS Listing 1: ALFWorld prompt (without history) You are an expert agent operating in the ALFRED Embodied Environment. Your current observation is: {current_observation} Your admissible actions of the current situation are: (cid:44) [{admissible_actions}]. Now its your turn to take an action. You should first reason step-by-step about the current situation. This (cid:44) reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible (cid:44) action for current step and present it within <action> </action> (cid:44) tags. Listing 2: ALFWorld prompt (with history) You are an expert agent operating in the ALFRED Embodied Environment. (cid:44) Your task is to: {task_description} Prior to this step, you have already taken {step_count} step(s). Below (cid:44) are the most recent {history_length} observations and the (cid:44) corresponding actions you took: {action_history} You are now at step {current_step} and your current observation is: { (cid:44) current_observation} Your admissible actions of the current situation are: (cid:44) [{admissible_actions}]. Now its your turn to take an action. You should first reason step-by-step about the current situation. This (cid:44) reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an admissible (cid:44) action for current step and present it within <action> </action> (cid:44) tags. D.2 SCIENCEWORLD PROMPTS Listing 3: ScienceWorld prompt (without history) You are an expert agent operating in the ScienceWorld environment, which (cid:44) is text-based virtual environment centered around accomplishing (cid:44) tasks from the elementary science curriculum. Your current task is: {task_description} Your current observation is: {current_observation} Here are the actions you may take: [ {\"action\": \"open OBJ\", \"description\": \"open container\"}, {\"action\": \"close OBJ\", \"description\": \"close container\"},"
        },
        {
            "title": "Under review",
            "content": "{\"action\": \"activate OBJ\", \"description\": \"activate device\"}, {\"action\": \"deactivate OBJ\", \"description\": \"deactivate device\"}, {\"action\": \"connect OBJ to OBJ\", \"description\": \"connect electrical (cid:44) components\"}, {\"action\": \"disconnect OBJ\", \"description\": \"disconnect electrical (cid:44) components\"}, {\"action\": \"use OBJ [on OBJ]\", \"description\": \"use device/item\"}, {\"action\": \"look around\", \"description\": \"describe the current room\"}, {\"action\": \"look at OBJ\", \"description\": \"describe an object in detail\"}, {\"action\": \"look in OBJ\", \"description\": \"describe containers (cid:44) contents\"}, {\"action\": \"read OBJ\", \"description\": \"read note or book\"}, {\"action\": \"move OBJ to OBJ\", \"description\": \"move an object to (cid:44) container\"}, {\"action\": \"pick up OBJ\", \"description\": \"move an object to the (cid:44) inventory\"}, {\"action\": \"put down OBJ\", \"description\": \"drop an inventory item\"}, {\"action\": \"pour OBJ into OBJ\", \"description\": \"pour liquid into (cid:44) container\"}, {\"action\": \"dunk OBJ into OBJ\", \"description\": \"dunk container into (cid:44) liquid\"}, {\"action\": \"mix OBJ\", \"description\": \"chemically mix container\"}, {\"action\": \"go to LOC\", \"description\": \"move to new location\"}, {\"action\": \"eat OBJ\", \"description\": \"eat food\"}, {\"action\": \"flush OBJ\", \"description\": \"flush toilet\"}, {\"action\": \"focus on OBJ\", \"description\": \"signal intent on task (cid:44) object\"}, {\"action\": \"wait\", \"description\": \"take no action for 10 iterations\"}, {\"action\": \"wait1\", \"description\": \"take no action for 1 iteration\"}, {\"action\": \"task\", \"description\": \"describe current task\"}, {\"action\": \"inventory\", \"description\": \"list your inventory\"} ] Current available actions: {available_actions} Now its your turn to take an action. You should first reason step-by-step about the current situation. This (cid:44) reasoning process MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an appropriate (cid:44) action for the current step and present it within <action> (cid:44) </action> tags. Listing 4: ScienceWorld prompt (with history) You are an expert agent operating in the ScienceWorld environment, which (cid:44) is text-based virtual environment centered around accomplishing (cid:44) tasks from the elementary science curriculum. Your current task is: {task_description} Prior to this step, you have already taken {step_count} step(s). Below (cid:44) are the most recent {history_length} observations and the (cid:44) corresponding actions you took: {action_history} You are now at step {current_step} and your current observation is: { (cid:44) current_observation} Here are the actions you may take: [ {\"action\": \"open OBJ\", \"description\": \"open container\"}, {\"action\": \"close OBJ\", \"description\": \"close container\"}, {\"action\": \"activate OBJ\", \"description\": \"activate device\"}, {\"action\": \"deactivate OBJ\", \"description\": \"deactivate device\"}, {\"action\": \"connect OBJ to OBJ\", \"description\": \"connect electrical (cid:44) components\"},"
        },
        {
            "title": "Under review",
            "content": "{\"action\": \"disconnect OBJ\", \"description\": \"disconnect electrical (cid:44) components\"}, {\"action\": \"use OBJ [on OBJ]\", \"description\": \"use device/item\"}, {\"action\": \"look around\", \"description\": \"describe the current room\"}, {\"action\": \"look at OBJ\", \"description\": \"describe an object in detail\"}, {\"action\": \"look in OBJ\", \"description\": \"describe containers (cid:44) contents\"}, {\"action\": \"read OBJ\", \"description\": \"read note or book\"}, {\"action\": \"move OBJ to OBJ\", \"description\": \"move an object to (cid:44) container\"}, {\"action\": \"pick up OBJ\", \"description\": \"move an object to the (cid:44) inventory\"}, {\"action\": \"put down OBJ\", \"description\": \"drop an inventory item\"}, {\"action\": \"pour OBJ into OBJ\", \"description\": \"pour liquid into (cid:44) container\"}, {\"action\": \"dunk OBJ into OBJ\", \"description\": \"dunk container into (cid:44) liquid\"}, {\"action\": \"mix OBJ\", \"description\": \"chemically mix container\"}, {\"action\": \"go to LOC\", \"description\": \"move to new location\"}, {\"action\": \"eat OBJ\", \"description\": \"eat food\"}, {\"action\": \"flush OBJ\", \"description\": \"flush toilet\"}, {\"action\": \"focus on OBJ\", \"description\": \"signal intent on task (cid:44) object\"}, {\"action\": \"wait\", \"description\": \"take no action for 10 iterations\"}, {\"action\": \"wait1\", \"description\": \"take no action for 1 iteration\"}, {\"action\": \"task\", \"description\": \"describe current task\"}, {\"action\": \"inventory\", \"description\": \"list your inventory\"} ] Current available actions: {available_actions} Now its your turn to take an action. You should first reason (cid:44) step-by-step about the current situation. This reasoning process (cid:44) MUST be enclosed within <think> </think> tags. Once youve finished your reasoning, you should choose an appropriate (cid:44) action for the current step and present it within <action> (cid:44) </action> tags."
        },
        {
            "title": "E LIMITATION AND FUTURE WORK",
            "content": "While EPO effectively addresses the exploration-exploitation cascade failure in multi-turn sparsereward environments, our approach does not fully leverage memory systems (Xu et al., 2025) to enhance learning from past trajectories. Currently, EPO uses historical entropy information solely for regularization, but does not incorporate explicit memory mechanisms that could help agents recall and reuse successful behavioral patterns from previous episodes. In multi-turn settings where sparse rewards make successful trajectories particularly valuable, memory-augmented approach could potentially accelerate learning by allowing agents to explicitly store and retrieve relevant past experiences, especially those leading to rare positive rewards. Future work could extend EPO to vision-language model (VLM) agents operating in multi-turn visual environments, where the cascade failure may manifest differently due to the multimodal nature of observations and actions. The interplay between visual and textual entropy in VLM agents presents unique challengesvisual observations might require different entropy bounds than textual responses, and the temporal dependencies across modalities could amplify or dampen the cascade failure."
        },
        {
            "title": "F USE OF LARGE LANGUAGE MODELS",
            "content": "We utilized Large Language Models (LLMs), such as Claude, exclusively for ancillary support in two main areas: (i) language editing and polishing of the manuscript, and (ii) coding assistance for minor"
        },
        {
            "title": "Under review",
            "content": "boilerplate tasks, such as generating plotting scripts and small utilities. All model-generated outputs were thoroughly reviewed, modified, and rigorously tested by the authors to ensure their accuracy and appropriateness. The core intellectual contributions of this workincluding all research ideas, algorithmic designs, experimental methodologies, data analysis, and conclusionswere conceived and validated entirely by the authors. Critically, LLMs were not used to generate any experimental results, create annotations or ground truth data, or influence methodological decisions. The authors assume full and sole responsibility for all content presented in this paper."
        }
    ],
    "affiliations": [
        "Adobe Inc.",
        "Rutgers University"
    ]
}