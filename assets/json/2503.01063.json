{
    "paper_title": "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding",
    "authors": [
        "David Noever"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance."
        },
        {
            "title": "Start",
            "content": "AI-Invented Tonal Languages: Preventing Machine Lingua Franca Beyond Human Understanding David A. Noever PeopleTec, Inc., Huntsville, AL david.noever@peopletec.com ABSTRACT This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned unique frequency calculated as = 220 2^((i-32)/12) Hz, creating logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves (log₂(50175.42/220) 7.83), with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance. Keywords: Machine-to-Machine Communication, AI Language Invention, Ultrasonic Communication, Semitone Mapping, Cross-Modal Encoding, Private AI Languages, Frequency-Based Encoding, Human Perception Boundaries Can AI agents autonomously invent and productively employ their own private languages? This paper seeks to test the limits of that question. Can modern language models (LLMs) create private tonal speech using human-machine mapping, and if so, what would that invented encode language outside human auditing? resemble large and can arise translatable cryptophasia, For as many as 50% of human twins [1], this kind of lingual self-invention, or as spontaneous, private communication not to any parental tongues. With 1.6 million twins born every year, this constant statistical introduction of novel sounding languages may affect nearly half of twin births-- or one in every 84 total births that introduce secret one-time language to our species. Their twin-speech as invented or emergent language is not universal, nor does it seem to symbolize any common mother tongue for humanity as whole. Rather cryptophagia shares elements of idioglossia [2], which refers to isolated forms of communication invented to serve one or only few people. In addition to these rare twin languages, common tonal Figure 1. Machine-only communication (blue boundary) based on tonal alphabet as agents accelerate information sharing without human interpretability languages [3] such as Mandarins primary 4 tones, Cantonese 6-9 tones and Vietnamese 5-6 tones, represent humanintroduced nuances which share identical syllables but meaning different conceptual things when spoken in high, mid, low, rising or falling tone in speech. This research poses the question: are there equivalent shorthand methods of rapid communication that LLMs might routinely develop for their own use and efficiency? Is human-incomprehensible language also potential emergent agentic property? For the AI community, this kind of inventive machine-to-machine (M2M) communication highlights long technical history [4-15]. For instance, one emergent capability of agent behavior might involve highly compact M2M conversations. In 2017, two negotiating agents from Facebook (Alice and Bob) showed degenerative repetition equivalent to rewarded statement of deal done to conclude bidding transaction [4]. At first this cooperative behavior spawned notions that machine language invention might spontaneously arise to speed up rewards. However, their cooperation would not qualify as an invented communication as much as an undertrained and misattributed degeneration of the recurrent neural network [4-5]. From former Google CEO, Eric Schmidt, more cautionary 2024 warning [6] implies that human AI creators should pull the plug when faced with this language invention phenomenon. When LLMs begin to create private languages inaccessible to human understanding or auditing, they should be terminated. \"Your agent and my agent and her agent and his agent will all combine to solve new problemsAt some point, these agents will develop their own language and that's when we don't understand what we're doingYou know what we should do when that happens? Pull the plug. Literally unplug the computer. It's really problem when agents start communicating in ways we as humans do not understand. Thats the limit There's every reason to think some version of this scenario will occur within five years, maybe sooner.\" [6] The present research approach focuses on creating purely tonal alphabetical (English) map, then transferring that lookup table of frequencies as part of the multimodal access for foundational LLMs. The motivation shares some traditional features for M2M methods, including Morse code [7-8] and Bell Labs vocoder for speech compression [9Figure 2. Alternative Working User Interfaces for Private Audio or Tonal Language Creation with Novel LLM Intermediaries. 10]. The MIDI (Musical Instrument Digital Interface) of the 1980s inspired the tonal representation of musical notes as discrete frequency values, although not typically assigned to language values or conceptual linguistic systems [11]. The musical notation version, ABC, converts numerical MIDI to alphabetic notes, which potentially provides longterm storage capability to any tonal alphabet (Figure 2). The Chirp protocol [12-13] has also proposed peer-to-peer audio transmission, like more rapid touchless transfer between mobile devices than Bluetooth or QR codes. User surveys report higher satisfaction with sonic acknowledgements rather than visual or purely electronic silence alone. Most recent 2024 attempts seek to compress audio inputs [13-14] into recognized frozen token weights of LLMs, thus sharing common goal to represent sound [15] as new foreign language, and LLMs can learn the new foreign language with several demonstrations. The present efforts invert these attempts to convert sound into machinereadable representation, and instead derive minimal set of audio units to accelerate machine communication in the absence of human supervision or interpretability. By studying the requirements, one goal can be understood as recognizing the potential differences between genuine language invention and degenerate model babel or gibberish. The research seeks to understand how to recognize invented LLM communications protocols first by simulating example candidates. Lets assume any 2025 foundational LLM possesses this full spectrum of historical M2M context, both its successes, shortcomings and adoption rates in practice (see Supplemental Material II). As thought experiment of how AI might implement future machine-to-machine language of its own invention, we offer simple frequency-based lettering system of semitones. The hypothetical language offers adjustable tones and information speeds. By understanding the language requirements, we study possible pathways that future AI might attempt self-improvement in agentic interactions. Many similar language experiments (Supplemental Material II) provide key plot points with imagined extraterrestrial translation and inter-species communication (e.g. whale sound translation). In software, the proposed tonal language system implements direct mapping between text characters and auditory frequencies using equal temperament scaling. For demonstrating this ASCII-to-tone system, the frequency mapping represents precise logarithmic progression based on Western music's equal temperament system. Each ASCII character from 32 (space) to 126 (tilde) corresponds to unique frequency determined by applying semitone increase from the previous value. The progression by semitone intervals yields frequency ratio of 2(1/12). For character at position in the ASCII table (starting at position 32 for space), the frequency is calculated as equation (1): = 220 2((i-32)/12) Hz (1). This selection creates logarithmic frequency distribution that aligns with human auditory perception. This mapping spans approximately 7.8 octaves, extending from 220.00 Hz to 50175.42 Hz (E11), deliberately transcending normal human hearing thresholds, which typically range from 20 Hz to approximately 20 kHz. The logarithmic distribution aligns with the Weber-Fechner law of human perception [16], where perceived differences between stimuli correspond to proportional changes in their physical magnitudes. The human auditory system naturally perceives frequency ratios rather than absolute frequency differences, making semitone intervals particularly appropriate for encoding discrete information. The system's frequency range demonstrates crossmodal compatibility between visual representation (text), auditory (frequency), and representation computational representation (ASCII values). Lower frequencies remain within the range of human auditory perception, while higher frequencies extend into suggesting design range, considerations for both human and machine recipients. This design creates communication channel compatible with human perception while maintaining machine-optimized necessarily properties, becomes M2M-private as ultrasonic transmission occurs outside adult hearing above 20kHz. the ultrasonic but Figure 3. Closer view of spectrogram and tonal grid to show the visual representation of audio cryptophasia From first principles, there are motivations behind why LLM might choose this hidden language for agentic communication. From an information theory perspective, there are reasons two AI agents might prefer this form of communication for speed and compact efficiency. complete ASCII message can be transmitted rapidly using brief tonal segments, resulting in information densities that exceed normal speech. The exclusive frequency assignment to each character ensures unambiguous translation and decodability through Fourier analysis or similar frequency domain methods. The semitone progression also encodes messages with inherent musical characteristics that may help memorization, pattern recognition, and cross-cultural processing. This tone-based encoding considers how crossmodal information representation can potentially serve as an interlingua, or Lingua franca, that addresses translation challenges in computational linguistics while maintaining mathematical consistency throughout the entire character range. If two AI agents chose this conversational mode, round-trip communication is summarized graphically in Figure 1. Human speech-to-text software (A) provides English input to tonal map that spans the frequency range (200-50,000 Hz) as shown in Supplemental Materials spanning ASCII characters. Text-to-tone software (B) then transmit the input to tone-to-text receiver (C) which translates the text to its multimodal transformer architecture and returns nexttoken contextual responses. Two options exist to build the multimodal transformer, either as text only or combined audio-visual training data. The resulting output (D) then transfer tonal messaging either to another AI agent (E) or back to text or speech content for human perception (F, G). The demonstration of D-E stages highlight M2M communication, potentially in more compact or imperceptible language that humans would not recognize as spoken or written conversation. To complete the tonal language, the software offers the ABC notation for MIDI notes such that the machine-only version is both auditory and archived for future machine reference in digital print format. The boundary in Figure 1 corresponds to the focus of the present software effort to replicate. The human access to the invented tonal alphabet, like the Chirp protocol or voice-coder, remain limited by our senses. To illustrate the approach, Figure 2 shows two alternative user-interfaces that encode the functional diagram stages of Figure 1. In working prototype [17], we represent the human boundary as traditional speech-to-text and text-tospeech exit points. The machine boundary begins with one-to-one conversion of ASCII text to logarithmically spaced tones, which are represented architecturally as text-to-tone or tone-to-ABC notation. The two user interfaces in Figure 2 showcase the tones as either one-dimensional (wave) spectra or two-dimensional spectrograms. In analogy to the audio mixing board concept, the second prototype also lights colors in tonal grid as visual cues. Actions within the tonal language are largely inaccessible to human perception without specialized audio instrumentation. For example, ultrasonic demodulation, or non-linear microphones, would be required to perceive certain letters. At an accelerated data rate, the understanding of agentic communication might resemble trained Morse code translator jotting down dashes and dots. While not full implementation of cryptophasic invention, the software highlights the minimal requirements for M2M tones and rapid invention of hidden instructions (Figure 3). The comprehensive semitone-based ASCII mapping represents an evolution of previous approaches, combining the character-level precision of digital encoding systems with the perceptual advantages of musical frequency relationships, extending the frequency range significantly beyond previous systems to accommodate both human and machine processing capabilities. Several limitations constrain the current implementation and theoretical framework. The frequency mapping system depends heavily on Western musical conventions, potentially introducing cultural biases into supposedly universal machine communication. Additionally, ultrasonic components of the encoding system face practical challenges in real-world environments, including signal degradation, environmental noise, and hardware limitations of current audio systems. The mapping also lacks semantic compression, as each character requires equivalent encoding resources regardless of its informational significance. Furthermore, the current approach does not address how multiple AI agents might develop shared conventions beyond the predetermined mapping, limiting exploration of truly emergent communication properties. Importantly, the ethical dimensions of enabling AI systems to communicate in ways partially or wholly inaccessible to humans require further examination beyond the technical implementation. Future research should address several promising directions. First, the development of advanced detection and translation mechanisms for ultrasonic AI communications will be crucial for maintaining human oversight of M2M interactions. Second, investigating whether emergent linguistic properties appear in these tonal systems when used by multiple AI agents could reveal if more complex grammatical structures evolve organically. Third, exploring information compression techniques specifically optimized for tonal transmission could further enhance efficiency, potentially achieving even greater data transfer rates between AI systems. Fourth, examining the robustness of tonal languages against environmental interference or adversarial attacks would strengthen practical applications. Finally, developing ethical frameworks and governance models for regulating private AI languages will be essential as these technologies mature. This paper has demonstrated proof-of-concept for how AI systems could develop tonal languages as efficient communication channels. By implementing comprehensive frequency mapping for textual data that extends into ultrasonic ranges, we show that LLMs could theoretically engage in M2M communications partially inaccessible to human perception. This possibility raises significant implications for AI transparency, oversight, and governance. While our system does not represent true cryptophasic invention by AI, it illustrates the minimal requirements for such capabilities to emerge. As Schmidt warns, the development of private AI languages may represent critical threshold in AI development requiring careful monitoring and potentially intervention. The tonal language system we describe provides useful experimental framework for studying these possibilities before they emerge spontaneously. By understanding how such systems might function and their inherent capabilities and limitations, we can better prepare for future where AI communication becomes increasingly sophisticated and potentially opaque. The cross-modal nature of our approach, connecting text, sound, and computational representation, may also offer insights for human-AI interfaces that leverage multiple sensory channels, even as we remain vigilant about maintaining meaningful human oversight of AI systems."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "The authors thank the PeopleTec Technical Fellows program for research support."
        },
        {
            "title": "REFERENCES",
            "content": "[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] Bishop, D. V., & Bishop, S. J. (1998). \" Twin Language\" Risk Factor for Language Impairment?. Journal of Speech, Language, and Hearing Research, 41(1), 150-160. Kolesnikova, S. M., Burskaya, E. A., Shatalova, O. V., & Ledeneva, V. V. Means of Representation of the Idioglossas Children,. Family,Life in the Novel by FM Dostoevsky The Brothers Karamazov. Vestnik Volgogradskogo gosudarstvennogo universiteta. Seriya, 2, 47-62. McCawley, J. D. (1978). What is tone language?. In Tone (pp. 113-131). Academic Press. Lammin, H. (2022). What are Bob and Alice Saying?[Mis] communication and Intermediation Between Language and Code. Language GamesLeonardo Electronic Almanac, 23(1). Bagga, P., Paoletti, N., Alrayes, B., & Stathis, K. (2020). deep reinforcement learning approach to concurrent bilateral negotiation. arXiv preprint arXiv:2001.11785. Schmidt, E. (2024) When Should We Stop AI? Noema Magazine, https://www.youtube.com/watch?v=lUErzvvKuaY Gilbert, E. N. (1969). How good is Morse code?. Information and Control, 14(6), 559-565. Levine, S., Gauger, J., Bowers, L., & Khan, K. (1986). comparison of Mouthstick and Morse code text inputs. Augmentative and Alternative Communication, 2(2), 51-55. Dudley, H. (1940). The vocoderElectrical re-creation of speech. Journal of the Society of Motion Picture Engineers, 34(3), 272-278. Schroeder, M. R., & Schroeder, M. R. (2004). Speech Compression. Computer Speech: Recognition, Compression, Synthesis, 107-127. HekmatiAthar, S., & Anwar, M. (2021). Music embedding: tool for incorporating music theory into computational music applications. arXiv preprint arXiv:2104.11880. [12] Mehrabi, A., Mazzoni, A., Jones, D., & Steed, A. (2020). Evaluating the user experience of acoustic data [13] [14] transmission: study of sharing data between mobile devices using sound. Personal and Ubiquitous Computing, 24(5), 655-668. Schürmann, D., & Sigg, S. (2011). Secure communication based on ambient audio. IEEE Transactions on mobile computing, 12(2), 358-370. Li, D., Tang, C., & Liu, H. (2024, July). Audio-LLM: Activating the Capabilities of Large Language Models to Comprehend Audio Data. In International Symposium on Neural Networks (pp. 133-142). Singapore: Springer Nature Singapore. [15] [16] [17] Yang, D., Guo, H., Wang, Y., Huang, R., Li, X., Tan, X., ... & Meng, H. (2024). Uniaudio 1.5: Large language model-driven audio codec is few-shot audio task learner. arXiv preprint arXiv:2406.10056. Dehaene, S. (2003). The neural basis of the WeberFechner law: logarithmic mental number line. Trends in cognitive sciences, 7(4), 145-147. Noever, D. (2025) Cryptophasia Language Prototype, https://github.com/reveondivad/cryptophasia SUPPLEMENTAL MATERIAL I: Frequency Table for ASCII Characters Character ASCII (space) ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ D H L 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 Frequency (Hz) 220.00 Hz 233.08 Hz 246.94 Hz 261.63 Hz 277.18 Hz 293.66 Hz 311.13 Hz 329.63 Hz 349.23 Hz 369.99 Hz 392.00 Hz 415.30 Hz 440.00 Hz 466.16 Hz 493.88 Hz 523.25 Hz 554.37 Hz 587.33 Hz 622.25 Hz 659.26 Hz 698.46 Hz 739.99 Hz 783.99 Hz 830.61 Hz 880.00 Hz 932.33 Hz 987.77 Hz 1046.50 Hz 1108.73 Hz 1174.66 Hz 1244.51 Hz 1318.51 Hz 1396.91 Hz 1479.98 Hz 1567.98 Hz 1661.22 Hz 1760.00 Hz 1864.66 Hz 1975.53 Hz 2093.00 Hz 2217.46 Hz 2349.32 Hz 2489.02 Hz 2637.02 Hz 2793.83 Hz 2959.96 Hz 3135.96 Hz Musical Note F#3 G3 G#3 A4 A#4 B4 C4 C#4 D4 D#4 E4 F4 F#4 G4 G#4 A5 A#5 B5 C5 C#5 D5 D#5 E5 F5 F#5 G5 G#5 A6 A#6 B6 C6 C#6 D6 D#6 E6 F6 F#6 G6 G#6 A7 A#7 B7 C7 C#7 D7 D#7 Character R V Z [ ] ^ _ ` d h l p t x { } ASCII 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 Frequency (Hz) 3322.44 Hz 3520.00 Hz 3729.31 Hz 3951.07 Hz 4186.01 Hz 4434.92 Hz 4698.64 Hz 4978.03 Hz 5274.04 Hz 5587.65 Hz 5919.91 Hz 6271.93 Hz 6644.88 Hz 7040.00 Hz 7458.62 Hz 7902.13 Hz 8372.02 Hz 8869.84 Hz 9397.27 Hz 9956.06 Hz 10548.08 Hz 11175.30 Hz 11839.82 Hz 12543.85 Hz 13289.75 Hz 14080.00 Hz 14917.24 Hz 15804.27 Hz 16744.04 Hz 17739.69 Hz 18794.55 Hz 19912.13 Hz 21096.16 Hz 22350.61 Hz 23679.64 Hz 25087.71 Hz 26579.50 Hz 28160.00 Hz 29834.48 Hz 31608.53 Hz 33488.07 Hz 35479.38 Hz 37589.09 Hz 39824.25 Hz 42192.33 Hz 44701.21 Hz 47359.29 Hz 50175.42 Hz Musical Note F7 F#7 G7 G#7 A8 A#8 B8 C8 C#8 D8 D#8 E8 F8 F#8 G8 G#8 A9 A#9 B9 C9 C#9 D9 D#9 E9 F9 F#9 G9 G#9 A10 A#10 B10 C10 C#10 D10 D#10 E10 F10 F#10 G10 G#10 A11 A#11 B11 C11 C#11 D11 D#11 E11 SUPPLEMENTAL MATERIAL II: Example Tonal Language Creation Use Cases from Fiction and Speculative Scientific AI Studies"
        }
    ],
    "affiliations": [
        "PeopleTec, Inc., Huntsville, AL"
    ]
}