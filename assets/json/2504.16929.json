{
    "paper_title": "I-Con: A Unifying Framework for Representation Learning",
    "authors": [
        "Shaden Alshammari",
        "John Hershey",
        "Axel Feldmann",
        "William T. Freeman",
        "Mark Hamilton"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners."
        },
        {
            "title": "Start",
            "content": "Published as conference paper at ICLR 2025 I-CON: UNIFYING FRAMEWORK FOR REPRESENTATION LEARNING Shaden Alshammari1 1 MIT 2 Google 3 Microsoft John Hershey2 Axel Feldmann1 William T. Freeman1,2 Mark Hamilton1, 5 2 0 2 3 2 ] . [ 1 9 2 9 6 1 . 4 0 5 2 : r https://aka.ms/i-con Figure 1: periodic table of representation learning methods unified by the I-Con framework. By choosing different types of conditional probability distributions over neighbors, I-Con generalizes over 23 commonly used representation learning methods."
        },
        {
            "title": "ABSTRACT",
            "content": "As the field of representation learning grows, there has been proliferation of different loss functions to solve different classes of problems. We introduce single information-theoretic equation that generalizes large collection of modern loss functions in machine learning. In particular, we introduce framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners."
        },
        {
            "title": "INTRODUCTION",
            "content": "Over the past decade the field of representation learning has flourished, with new techniques, architectures, and loss functions emerging daily. These advances have powered state-of-the-art models in vision, language, and multimodal learning, often with minimal human supervision. Yet as the field 1 Published as conference paper at ICLR 2025 expands, the diversity of loss functions makes it increasingly difficult to understand how different methods relate, and which objectives are best suited for given task. In this work, we introduce general mathematical framework that unifies wide range of representation learning techniques spanning supervised, unsupervised, and self-supervised approaches under single information-theoretic objective. Our framework, Information Contrastive Learning (ICon), reveals that many seemingly disparate methods including clustering, spectral graph theory, contrastive learning, dimensionality reduction, and supervised classification are all special cases of the same underlying loss function. While prior work has identified isolated connections between subsets of representation learning methods, typically linking only two or three techniques at time (Sobal et al., 2025; Hu et al., 2023; Yang et al., 2022; Bohm et al., 2023; Balestriero & LeCun, 2022), I-Con is the first framework to unify over 23 distinct methods under single objective. This unified perspective not only clarifies the structure of existing techniques but also provides strong foundation for transferring ideas and improvements across traditionally separate domains. Using I-Con, we derive new unsupervised loss functions that significantly outperform previous methods on standard image classification benchmarks. Our key contributions are: We introduce I-Con, single information-theoretic loss that generalizes several major classes of representation learning. We prove 15 theorems showing how diverse algorithms emerge as special cases of I-Con. We use I-Con to design debiasing strategy that improves unsupervised ImageNet-1K accuracy by +8%, with additional gains of +3% on CIFAR-100 and +2% on STL-10 in linear probing."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Representation learning spans wide range of methods for extracting structure from complex data. We review approaches that I-Con builds upon and generalizes. For comprehensive surveys, see (Le-Khac et al., 2020; Bengio et al., 2013; Weng, 2021). Feature Learning aims to derive informative low-dimensional embeddings using supervisory signals such as pairwise similarities, nearest neighbors, augmentations, class labels, or reconstruction losses. Classical methods like PCA (Pearson, 1901) and MDS (Kruskal, 1964) preserve global structure, while UMAP (McInnes et al., 2018) and t-SNE (Hinton & Roweis, 2002; Van der Maaten & Hinton, 2008) focus on local topology by minimizing divergences between joint distributions. I-Con adopts similar divergence-minimization view. Contrastive learning approaches such as SimCLR (Chen et al., 2020a), CMC (Tian et al., 2020), CLIP (Radford et al., 2021), and MoCo v3 (Chen* et al., 2021) use positive and negative pairs, often built via augmentations or aligned modalities. I-Con generalizes these losses within unified KL-based framework, highlighting subtle distinctions between them. Supervised classifiers (e.g., ImageNet models (Krizhevsky et al., 2017)) also yield effective features, which I-Con recovers by treating class labels as discrete contrastive points, bridging supervised and unsupervised learning. Clustering methods uncover discrete structure through distance metrics, graph partitions, or contrastive supervision. Algorithms like k-Means (Macqueen, 1967), EM (Dempster et al., 1977), and spectral clustering (Shi & Malik, 2000) are foundational. Recent methods, including IIC (Ji et al., 2019), Contrastive Clustering (Li et al., 2021), and SCAN (Gansbeke et al., 2020), leverage invariance and neighborhood structure. Teacher-student models such as TEMI (Adaloglou et al., 2023) and EMA-based architectures (Chen et al., 2020b) enhance clustering further. I-Con encompasses these by aligning clustering-induced joint distribution with target distribution derived from similarity, structure, or contrastive signals. Unifying Representation Learning has been explored through connections between contrastive learning and t-SNE (Hu et al., 2023; Bohm et al., 2023), equivalences between contrastive and cross-entropy losses (Yang et al., 2022), and relations between spectral and contrastive methods (Balestriero & LeCun, 2022; Sobal et al., 2025). Other efforts, like Bayesian grammar models (Grosse et al., 2012), offer probabilistic perspectives. Tschannen et al. (Tschannen et al., 2019) emphasized estimator and architecture design in mutual information frameworks but stopped short of broader unification. Published as conference paper at ICLR 2025 (a) High-level I-Con architecture. (b) Illustrative examples of distribution families for pθ or qϕ. Figure 2: Overview of the I-Con framework. (a) Alignment of learned and supervisory distributions. (b) Common distribution families in I-Cons formulation. While prior work links subsets of these methods, I-Con, to our knowledge, is the first to unify supervised, contrastive, clustering, and dimensionality reduction objectives under single loss. This perspective clarifies their shared structure and opens paths to new learning principles."
        },
        {
            "title": "3 METHODS",
            "content": "The I-Con framework unifies multiple representation learning methods under single loss function: minimizing the average KL divergence between two conditional neighborhood distributions that define transition probabilities between data points. This information-theoretic objective generalizes techniques from clustering, contrastive learning, dimensionality reduction, spectral graph theory, and supervised learning. By varying the construction of the supervisory distribution and the learned distribution, I-Con encompasses broad class of existing and novel methods. We introduce I-Con and demonstrate its ability to unify techniques from diverse areas and orchestrate the transfer of ideas across different domains, leading to state-of-the-art unsupervised image classification method. 3.1 INFORMATION CONTRASTIVE LEARNING Let i, be elements of dataset , with probabilistic neighborhood function p(ji) defining transition probability. To ensure valid probability distributions, p(ji) 0 and (cid:82) jX p(ji) = 1. We parameterize this distribution by θ Θ, to create learnable function pθ(ji). Similarly, we define another distribution qϕ(ji) parameterized by ϕ Φ. The core I-Con loss function is then: L(θ, ϕ) = (cid:90) iX DKL (pθ(i)qϕ(i)) = (cid:90) (cid:90) iX jX pθ(ji) log pθ(ji) qϕ(ji) . (1) In practice, is typically fixed supervisory distribution, while qϕ is learned by comparing deep network representations, prototypes, or clusters. Figure 2a illustrates this alignment process. The optimization aligns qϕ with p, minimizing their KL divergence. Although most existing methods optimize only qϕ, I-Con also allows learning both pθ and qϕ, although one must take care to prevent trivial solutions. Published as conference paper at ICLR"
        },
        {
            "title": "3.2 UNIFYING REPRESENTATION LEARNING ALGORITHMS WITH I-CON",
            "content": "Despite the incredible simplicity of Equation 1, this equation is rich enough to generalize several existing methods in the literature simply by choosing parameterized neighborhood distributions pθ and qϕ as shown in Figure 1. We categorize common choices for pθ and qϕ in Figure 2a. Table 1 summarizes some key choices which recreate popular methods from contrastive learning (SimCLR, MOCOv3, SupCon, CMC, CLIP, VICReg), dimensionality reduction (SNE, t-SNE, PCA), clustering (K-Means, Spectral, DCD, PMI), and supervised learning (Cross-Entropy and Harmonic Loss). Due to limited space, we defer proofs of each of these theorems to the supplemental material. We also note that Table 1 is not exhaustive, and we encourage the community to explore whether other learning frameworks implicitly minimize Equation 1 for some choice of and q."
        },
        {
            "title": "3.2.1 EXAMPLE: SNE, SIMCLR, AND K-MEANS",
            "content": "While I-Con unifies broad range of methods, we illustrate how different choices of and recover well-known techniques such as SNE, SimCLR, and K-Means. Full details are in the appendix. SNE as neighbors remain neighbors. Stochastic Neighbor Embedding (SNE) is classic example. Given Rdn with points in dimensions, SNE learns low-dimensional representation ϕ Rmn, typically d. i) is deTo preserve local structure, p(j fined by placing Gaussian around each highdimensional point xi, and qϕ(j i) by placing Gaussian around ϕi. Minimizing the average KL divergence between these distributions ensures that points close in the original space remain close in the embedded space. SimCLR as augmentations of the same image are neighbors. Contrastive learning methods like SimCLR and SupCon instead use class labels. Here, p(j i) = 1 if is an augmentation of (and 0 otherwise). In the embedding space, qϕ(j i) is defined via Gaussian-like distribution based on cosine similarity. Minimizing their KL divergence encourages images from the same scene to cluster together. K-Means as points that are close are members of the same clusters. Clustering-based approaches like K-Means and DCD follow similar recipe. The distribution p(j i) is again Gaussian-based in the original space, while qϕ(j i) reflects whether points are assigned to the same cluster in the learned representation. Minimizing KL divergence aligns these cluster assignments with the actual neighborhood structure in the data. Methods like K-Means include an entropy penalty to enforce hard probabilistic assignments, as shown in Theorem 13, whereas methods like DCD do not include it. (a) SNE (dimensionality reduction) (b) SimCLR (contrastive learning) (c) K-Means (clustering) Figure 3: Examples of methods as special cases of I-Con via different choices of and q, with corresponding code-style configurations. 3.3 CREATING NEW REPRESENTATION LEARNERS WITH I-CON The I-Con framework unifies various approaches to representation learning under single mathematical formulation and, crucially, facilitates the transfer of techniques among different domains. 4 Published as conference paper at ICLR 2025 Method Choice of pθ (j i) (A) Dimensionality Reduction Choice of qϕ(j i) SNE (Hinton & Roweis, 2002) Theorem 1 Gaussian over data points, xi t-SNE (Van der Maaten & Hinton, 2008) Corollary exp(xi xj 2/2σ2 ) k=i exp(xi xk2/2σ2 ) (cid:80) (cid:80) Gaussian over learned low-dimensional points, ϕi exp(ϕi ϕj 2) k=i exp(ϕi ϕk2) Cauchy distribution over ϕi (1 + ϕi ϕj 2)1 k=i(1 + ϕi ϕk2)1 Wide Gaussian on linear projection features, fϕ(xi) (cid:80) PCA (Pearson, 1901) Theorem InfoNCE Loss (Bachman et al., 2019) Theorem 3 Triplet Loss (Schroff et al., 2015) Theorem 5 t-SimCLR, t-SimCNE (Hu et al., 2023; Bohm et al., 2023) Corollary 2 VICReg* without covariance term (Bardes et al., 2021) Theorem 4 SupCon (Khosla et al., 2020) Theorem 6 X-Sample (Sobal et al., 2025) Theorem 7 LGSimCLR (El Banani et al., 2023) CMC & CLIP (Tian et al., 2020) Theorem 8 Supervised Cross Entropy (Good, 1963) Theorem 9 Harmonic Loss (Baek et al., 2025) Theorem 10 Masked Lang. Modeling (Devlin et al., 2019) Theorem Probabilistic k-Means (Macqueen, 1967) Theorem 13 Spectral Clustering (Ng et al., 2001) Corollary 4 Normalized Cuts (Shi & Malik, 2000) Theorem 14 PMI Clustering (Adaloglou et al., 2023) Theorem 15 1[ = ] lim σ (cid:80) exp(fϕ(xi)fϕ (xj )2/2σ2 ) k=i exp(fϕ (xi)fϕ (xk )2/2σ2 ) (B) Contrastive Learning 1 1[i and are positive pair] Gaussian on deep normalized features exp(cid:0)fϕ (xi)fϕ (xj )(cid:1) (cid:80) k=i exp(cid:0)fϕ (xi)fϕ (xk )(cid:1) Gaussian on deep features (1 neg. sample, σ 0) exp(cid:0)fϕ (xi)fϕ (xj )2 /2σ2(cid:1) (cid:80) k{i+ , } exp(cid:0)fϕ (xi)fϕ (xk )2/2σ2(cid:1) (cid:80) Student-T on deep features (1+ϕiϕj 2 /ν)(ν+1)/2 k=i(1+ϕiϕk 2/ν)(ν+1)/2 Wide Gaussian on learned features exp(fϕ(xi)fϕ (xj )2/2σ2 ) k=i exp(fϕ (xi)fϕ (xk )2/2σ2 ) lim σ (cid:80) 1 1[i and have same class] Gaussian on corresponding embeddings exp(cid:0)gθ (xi) gθ (xj )(cid:1) (cid:80) k=i exp(cid:0)gθ (xi) θ (xk)(cid:1) 1[xi is among xj nearest neighbors] 1 1 1[i,j pos. pairs, Vi = Vj ] (C) Supervised Learning Indicator over classes 1(cid:2)i belongs to class j(cid:3) 1 #(cid:2)Context precedes token j(cid:3) (D) Clustering Intra-cluster uniform probability (cid:88) c=1 p(cid:0)fθ (xi) and fθ (xj ) in c(cid:1) E[size of cluster c] Intra-cluster uniform probability weighted by degree (cid:88) c=1 p(cid:0)fθ (xi) and fθ (xj ) in c(cid:1) dj E[degree of cluster c] 1 1[j is k-NN of i] Gaussian on deep normalized features exp(cid:0)fϕ(xi) fϕ(xj )(cid:1) (cid:80) k=i exp(cid:0)fϕ(xi) fϕ(xk)(cid:1) exp(cid:0)fϕ(xi) fϕ(xj )(cid:1) (cid:80) kVj exp(cid:0)fϕ(xi) fϕ(xk)(cid:1) exp(cid:0)fϕ(xi) ϕj kC exp(cid:0)fϕ(xi) ϕk Student-T on deep features and class prototypes (cid:80) (cid:1) (cid:1) lim σ0 (cid:80) (σ2+fϕ (xi)ϕj 2)n kC (σ2+fϕ(xi)ϕk )n exp(cid:0)fϕ(xi) ϕj kC exp(cid:0)fϕ(xi) ϕk (cid:1) (cid:1) (cid:80) (cid:80) Gaussians on datapoints exp(xixj 2/2σ2 ) k=i exp(xixk 2/2σ2 Gaussians on spectral embeddings exp(xixj 2/2σ2 ) k=i exp(xixk 2/2σ2 ) ) (cid:80) Gaussians on graph weigths exp(wij /dj ) exp(wik/dk) (cid:80) Intra-Cluster Uniform Probability p(cid:0)fθ (xi) and fθ (xj ) in c(cid:1) (cid:88) E[size of cluster c] c=1 (1 α)p(cid:0)fθ (xi) and fθ (xj ) in c(cid:1) E[size of cluster c] + α Debaised InfoNCE Clustering (ours) Debiased Graph through Uniform Distribution and Neighbor Propagation (cid:88) c=1 Table 1: I-Con unifies representation learners under different choices of pθ(ji) and qϕ(ji). Proofs of the propositions in this table can be found in the supplement. 5 Published as conference paper at ICLR 2025 (a) Continuous distance-based distributions control neighborhood width via hyperparameters. (b) Graph-based distributions expand neighborhoods through structural strategies. Figure 4: Neighborhood adaptation in continuous and discrete settings. (a) Distance-based distributions modulate neighborhood width via parameters such as σ. (b) Graph-based approaches modify the connectivity directly, often via random walks or added edges, thereby broadening each nodes neighborhood. For instance, trick from contrastive learning can be applied to clusteringor vice versa. In this paper, we demonstrate how surveying modern representation methods enables the development of clustering and unsupervised classification algorithms that surpass previous performance levels. Specifically, we integrate insights from spectral clustering, t-SNE, and debiased contrastive learning (Chuang et al., 2020) to build state-of-the-art unsupervised image classification pipeline. 3.3.1 DEBASING Debiased Contrastive Learning (DCL) addresses the mismatch caused by random negative sampling in contrastive learning, especially when the number of classes is small. Randomly chosen negatives can turn out to be positives, introducing spurious repulsive forces between similar examples. Chuang et al. (2020) rectify this by subtracting out such false repulsion terms and boosting attractive forces, substantially improving representation quality. However, their method modifies the softmax itself, implying that qji is no longer genuine probability distribution and making it more difficult to extend the approach to clustering or supervised tasks. Our view, grounded in the I-Con framework, suggests simpler and more general alternative: rather than adjusting the learned distribution qji, we incorporate additional uncertainty directly into the supervisory distribution p(ji). This preserves qji as valid distribution and keeps the method applicable to tasks beyond contrastive learning. 3.3.2 DEBIASING THROUGH UNIFORM DISTRIBUTION Our first example adopts simple uniform mixture: p(ji) = (1 α) p(ji) + α , where is the local neighborhood size, and α specifies the degree of mixing. This approach assigns small probability mass α to each negative sample, thereby mitigating overconfident allocations. In supervised contexts, this is analogous to label smoothing (Szegedy et al., 2016). In contrast, Chuang et al. (2020) adjust the softmax function itself while retaining one-hot labels. Another way to view this method is through the lens of heavier-tailed or broader distributions. By adding uniform component, we mirror the idea in t-SNEs Student-t distribution (Van der Maaten & Hinton, 2008), which allocates greater mass to distant points. In both cases, expanding the distribution reduces the chance of overfitting to narrowly defined set of neighbors. 6 Published as conference paper at ICLR Figure 5: Left: Debiasing cluster learning improves performance on ImageNet-1K across batch sizes. Center: Distribution of maximum predicted probabilities for the biased model (α = 0) showing poor calibration, with overconfident predictions. Right: Distribution of maximum predicted probabilities for the debiased model (α = 0.4), demonstrating improved probability calibration. Debiased training alleviates optimization stiffness by reducing the prevalence of saturated logits, mitigating vanishing gradient issues, and fostering more robust and well-calibrated learning dynamics. Empirical results in Tables 3, Figures 5, and 6 show that this lightweight modification consistently improves performance across various tasks and batch sizes. It also relaxes overconfident distributions, much like label smoothing in supervised cross entropy, thereby guarding against vanishing gradients. 3.3.3 DEBIASING THROUGH NEIGHBOR PROPAGATION second strategy applies graph-based expansions. As shown in Table 1, replacing k-Means Gaussian neighborhoods with degree-weighted k-nearest neighbors recovers spectral clustering, which is known for robust, high-quality solutions. Building on this idea, we train contrastive learners with KNN-based neighborhood definitions. Given the nearest-neighbor graph, we can further expand it by taking longer walks, analogous to Word-Graph2Vec or tsNET (Li et al., 2023; Kruiger et al., 2017), process we term neighbor propagation. Formally, let be the conditional distribution matrix whose entries Pij = p(xj xi) define the probability of selecting xj as neighbor of xi. Interpreting as the adjacency matrix of the training data, we can smooth it by summing powers of up to length k: + 2 + + k. We can further simplify this by taking uniform distribution over all points reachable within steps, denoted by: PU I(cid:2) + 2 + + > 0(cid:3), where I[] is the indicator function. This walk-based smoothing broadens the effective neighborhood, allowing the model to learn from denser supervisory signal. Tables 3 and 4 confirm that adopting such propagation-based approach yields significant improvements in unsupervised image classification, underscoring the effectiveness of neighborhood expansion as debiasing strategy."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we demonstrate that the I-Con framework offers testable hypotheses and practical insights into self-supervised and unsupervised learning. Rather than aiming only for state-of-the-art performance, our goal is to show how I-Con can enhance existing unsupervised learning methods 7 Published as conference paper at ICLR 2025 by leveraging unified information-theoretic approach. Through this framework, we also highlight the potential for cross-pollination between techniques in varied machine learning domains, such as clustering, contrastive learning, and dimensionality reduction. This transfer of techniques, enabled by I-Con, can significantly improve existing methodologies and open new avenues for exploration. We focus our experiments on clustering because it is relatively understudied compared to contrastive learning, and there are variety of techniques that can now be adapted to this task. By connecting established methods such as k-Means, SimCLR, and t-SNE within the I-Con framework, we uncover wide range of possibilities for improving clustering methods. We validate these theoretical insights experimentally, demonstrating the practical impact of I-Con. We evaluate the I-Con framework using the ImageNet-1K dataset (Deng et al., 2009), which consists of 1,000 classes and over one million high-resolution images. This dataset is considered one of the most challenging benchmarks for unsupervised image classification due to its scale and complexity. To ensure fair comparison with prior works, we strictly adhere to the experimental protocol introduced by (Adaloglou et al., 2023). The primary metric for evaluating clustering performance is Hungarian accuracy, which measures the quality of cluster assignments by finding the optimal alignment between predicted clusters and ground truth labels via the Hungarian algorithm (Ji et al., 2019). This approach provides robust measure of clustering performance in an unsupervised context, where direct label supervision is absent during training. For feature extraction, we utilize the DiNO pre-trained Vision Transformer (ViT) models in three variants: ViT-S/14, ViT-B/14, and ViT-L/14 (Caron et al., 2021). These models are chosen to ensure comparability with previous work and to explore how the I-Con framework performs across varying model capacities. The experimental setup, including training protocols, optimization strategies, and data augmentations, mirrors those used in TEMI to ensure consistency in methodology. The training process involved optimizing linear classifier on top of the features extracted by the DiNO models. Each model was trained for 30 epochs, using ADAM (Kingma & Ba, 2017) with batch size of 4096 and an initial learning rate of 1e-3. We decayed the learning rate by factor of 0.5 every 10 epochs to allow for stable convergence. We do not apply additional normalization to the feature vectors. During training, we applied variety of data augmentation techniques, including random re-scaling, cropping, color jittering, and Gaussian blurring, to create robust feature representations. Furthermore, to enhance the clustering performance, we pre-computed global nearest neighbors for each image in the dataset using cosine similarity. This allowed us to sample two augmentations and two nearest neighbors for each image in every training batch, thus incorporating both local and global information into the learned representations. We refer to our derived approach as InfoNCE Clusting in Table 2. In particular, we use supervisory neighborhood comprised of augmentations, KNNs (k = 3), and KNN walks of length 1. We use the shared cluster likelihood by cluster neighborhood from k-Means (See table 1 for more detailed Equation) as our learned neighborhood function to drive cluster learning. 4.1 BASELINES We compare our method against several state-of-the-art clustering methods, including TEMI, SCAN, IIC, and Contrastive Clustering. These methods rely on augmentations and learned representations, but often require additional regularization terms or loss adjustments, such as controlling cluster size or reducing the weight of affinity losses. In contrast, our I-Con-based loss function is self-balancing and does not require such manual tuning, making it cleaner, more theoretically grounded approach. This allows us to achieve higher accuracy and more stable convergence across three different-sized backbones. 4.2 RESULTS Table 2 compared the Hungarian accuracy of Debiased InfoNCE Clustering across different DiNO variants (ViT-S/14, ViT-B/14, ViT-L/14) and several other modern clustering methods. The I-Con framework consistently outperforms the prior state-of-the-art method across all model sizes. Specifically, for the DiNO ViT-B/14 and ViT-L/14 models, debiased InfoNCE clustering achieves significant performance gains of +4.5% and +7.8% in Hungarian accuracy compared to TEMI, the prior state-of-the-art ImageNet clusterer. We attribute these improvements to two main factors: 8 Published as conference paper at ICLR"
        },
        {
            "title": "Method",
            "content": "DiNO ViT-S/14 DiNO ViT-B/14 DiNO ViT-L/14 k-Means Contrastive Clustering SCAN TEMI Debiased InfoNCE Clustering (Ours) 51.84 47.35 49.20 56.84 57.8 0.26 52.26 55.64 55.60 58.62 64.75 0.18 53.36 59.84 60.15 67.52 0.28 Table 2: Comparison of methods on ImageNet-1K clustering with respect to Hungarian Accuracy. Debiased InfoNCE Clustering significantly outperforms the prior state-of-the-art TEMI. Note that TEMI does not report results for ViT-L. Self-Balancing Loss: Unlike TEMI or SCAN, which require hand-tuned regularizations (e.g., balancing cluster sizes or managing the weight of affinity losses), I-Cons loss function automatically balances these factors without additional regularization hyper-parameter tuning as we are using the exact same clustering kernel used by k-Means. This theoretical underpinning leads to more robust and accurate clusters. Cross-Domain Insights: I-Con leverages insights from contrastive learning to refine clustering by looking at pairs of images based on their embeddings, treating augmentations and neighbors similarly. This approach, originally successful in contrastive learning, translates well into clustering and leads to improved performance on noisy high-dimensional image data. 4.3 ABLATIONS We conduct several ablation studies to experimentally justify the architectural improvements that emerged from analyzing contrastive clustering through the I-Con framework. These ablations focus on two key areas: the effect of incorporating debiasing into the target and embedding spaces and the impact of neighbor propagation strategies. Method DiNO ViT-S/14 DiNO ViT-B/14 DiNO ViT-L/ Baseline + Debiasing + KNN Propagation + EMA 55.51 57.27 0.07 58.45 0.23 57.8 0.26 63.03 63.72 0.09 64.87 0.19 64.75 0.18 65.70 66.87 0.07 67.25 0.21 67.52 0.28 Table 3: Ablation study of new techniques discovered through the I-Con framework. We compare ImageNet-1K clustering accuracy across different sized backbones. Method Baseline + KNNs + 1-walks on KNN + 2-walks on KNN + 3-walks on KNN DiNO ViT-S/14 DiNO ViT-B/14 DiNO ViT-L/14 55.51 56.43 58.09 57.84 57.82 63.03 64.26 64.29 64.27 64.15 65.72 65.70 65.97 67.26 67. Table 4: Ablation Study on Neighbor Propagation. Adding both KNNs and walks of length 1 or 2 on the KNN graph achieves the best performance. We perform experiments with different levels of debiasing in the target distribution, denoted by the parameter α, and test configurations where debiasing is applied to the target side, both sides (target and learned representations), or none. As seen in Figure 6, adding debiasing improves performance, with the optimal value typically around α = 0.6 to α = 0.8, particularly when applied to both sides of the learning process. This method is similar to how debiasing work in contrastive learning by assuming that each negative sample has non-zero probability (α/N ) of being incorrect. Figure 5 shows how changing the value of α improves performance across different batch sizes. 9 Published as conference paper at ICLR 2025 Figure 6: Effects of increasing the debias weight α on the supervisory neighborhood (blue line) and both the learned and supervisory neighborhood (red line). Adding some amount of debiasing helps in all cases, with double debiasing yielding the largest improvements. In second set of experiments, shown in Table 4, we examine the impact of neighbor propagation strategies. We evaluate clustering performance when local and global neighbors are included in the contrastive loss computation. Neighbor propagation, especially at small scales (s = 1 and = 2), significantly boosts performance across all model sizes, showing the importance of capturing local structure in the embedding space. Larger neighbor propagation values (e.g., = 3) offer diminishing returns, suggesting that over-propagating neighbors may dilute the information from the nearest, most relevant points. Note that only DiNO-L/14 showed preference for large step size, and this is likely due to its higher k-nearest neighbor ability, so the augmented links are correct. Our ablation studies highlight that small adjustments in the debiasing parameter and neighbor propagation can lead to notable improvements that achieve state-of-the-art result with simple loss function. Additionally, sensitivity to α and propagation size varies across models, with larger models generally benefiting more from increased propagation but requiring fine-tuning of α for optimal performance. We recommend using α 0.6 to α 0.8 and limiting neighbor propagation to small values for balance between performance and computational efficiency."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In summary, we have developed I-Con: single information-theoretic equation that unifies broad class of machine learning methods. We provide over 15 theorems that prove this assertion for many of the most popular loss functions used in clustering, spectral graph theory, supervised and unsupervised contrastive learning, dimensionality reduction, and supervised classification and regression. We not only theoretically unify these algorithms but show that our connections can help us discover new state-of-the-art methods, and apply improvements discovered for particular method to any other method in the class. We illustrate this by creating new method for unsupervised image classification that achieves +8% improvement over prior art. We believe that the results presented in this work represent just fraction of the methods that are potentially unify-able with I-Con, and we hope the community can use this viewpoint to improve collaboration and analysis across algorithms and machine learning disciplines. Acknowledgments This research was partially sponsored by the Department of the Air Force Artificial Intelligence Accelerator and was conducted under Cooperative Agreement Number FA875019-2-1000, as well as NSF CIF 1955864 (Occlusion and Directional Resolution in Computational Imaging). We also acknowledge support from Quanta Computer. Additionally, we would like thank Phillip Isola, Andrew Zisserman, Yair Weiss, Justin Kay, and Shivam Duggal for valuable discussions and feedback. 10 Published as conference paper at ICLR"
        },
        {
            "title": "BIBLIOGRAPHY",
            "content": "Nikolas Adaloglou, Felix Michels, Hamza Kalisch, and Markus Kollmann. Exploring the limits of deep image clustering using pretrained models. arXiv preprint arXiv:2303.17896, 2023. Philip Bachman, Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019. David Baek, Ziming Liu, Riya Tyagi, and Max Tegmark. Harmonic loss trains interpretable ai models. arXiv preprint arXiv:2502.01628, 2025. Randall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods. Advances in Neural Information Processing Systems, 35:2667126685, 2022. Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):17981828, 2013. David Blei, Alp Kucukelbir, and Jon McAuliffe. Variational inference: review for statisticians. Journal of the American statistical Association, 112(518):859877, 2017. Jan Niklas Bohm, Philipp Berens, and Dmitry Kobak. Unsupervised visualization of image datasets using contrastive learning. International Conference on Learning Representations, 2023. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers, 2021. URL https: //arxiv.org/abs/2104.14294. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 15971607. PMLR, 2020a. Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b. Xinlei Chen*, Saining Xie*, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021. Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive learning, 2020. URL https://arxiv.org/abs/2007.00224. Keenan Crane, Clarisse Weischedel, and Max Wardetzky. Geodesics in heat: new approach to computing distance based on heat flow. ACM Transactions on Graphics (TOG), 32(5):111, 2013. Arthur Dempster, Nan Laird, and Donald Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series (methodological), 39(1): 122, 1977. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL https://arxiv.org/ abs/1810.04805. Mohamed El Banani, Karan Desai, and Justin Johnson. Learning visual representations via In Proceedings of the IEEE/CVF Conference on Computer Vision language-guided sampling. and Pattern Recognition, pp. 1920819220, 2023. 11 Published as conference paper at ICLR Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan: Learning to classify images without labels, 2020. URL https://arxiv.org/ abs/2005.12320. Irving Good. Maximum entropy for hypothesis formulation, especially for multidimensional contingency tables. The Annals of Mathematical Statistics, pp. 911934, 1963. Roger Grosse, Ruslan Salakhutdinov, William Freeman, and Joshua Tenenbaum. Exploiting compositionality to explore large space of model structures. arXiv preprint arXiv:1210.4856, 2012. Geoffrey Hinton and Sam Roweis. Stochastic neighbor embedding. Advances in neural information processing systems, 15, 2002. Tianyang Hu, Zhili Liu, Fengwei Zhou, Wenjia Wang, and Weiran Huang. Your contrastive learning is secretly doing stochastic neighbor embedding. In International Conference on Learning Representations, 2023. Xu Ji, Joao Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 98659874, 2019. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:1866118673, 2020. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. URL https://arxiv.org/abs/1412.6980. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):8490, 2017. Johannes Kruiger, Paulo Rauber, Rafael Messias Martins, Andreas Kerren, Stephen Kobourov, and Alexandru Telea. Graph layouts by t-sne. In Computer graphics forum, volume 36, pp. 283294. Wiley Online Library, 2017. Joseph Kruskal. Multidimensional scaling by optimizing goodness of fit to nonmetric hypothesis. Psychometrika, 29(1):127, 1964. Phuc Le-Khac, Graham Healy, and Alan Smeaton. Contrastive representation learning: framework and review. Ieee Access, 8:193907193934, 2020. Wenting Li, Jiahong Xue, Xi Zhang, Huacan Chen, Zeyu Chen, Feijuan Huang, and Yuanzhe Cai. Word-graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk technique, 2023. URL https://arxiv.org/abs/2301.04312. Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 85478555, 2021. Macqueen. Some methods for classification and analysis of multivariate observations. In Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Probability/University of California Press, 1967. Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 14, 2001. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559572, 1901. 12 Published as conference paper at ICLR Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational In International Conference on Machine Learning, pp. 5171 bounds of mutual information. 5180. PMLR, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815823, 2015. Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and machine intelligence, 22(8):888905, 2000. Vlad Sobal, Mark Ibrahim, Randall Balestriero, Vivien Cabannes, Diane Bouchacourt, Pietro Astolfi, Kyunghyun Cho, and Yann LeCun. X-sample contrastive loss: Improving contrastive learning with sample similarity graphs. International Conference on Learning Representations, 2025. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. RethinkIn Proceedings of the IEEE conference on ing the inception architecture for computer vision. computer vision and pattern recognition, pp. 28182826, 2016. Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pp. 776794. Springer, 2020. Michael Tschannen, Josip Djolonga, Paul Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. Lilian Weng. Contrastive representation learning. lilianweng.github.io, May 2021. URL https: //lilianweng.github.io/posts/2021-05-31-contrastive/. Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. Unified contrastive learning in image-text-label space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1916319173, 2022. Published as conference paper at ICLR"
        },
        {
            "title": "APPENDIX",
            "content": "A Additional Experiments on Debiasing Feature Learning Proofs for Unifying Dimensionality Reduction Methods Proofs for Unifying Feature Learning Methods Proofs for Unifying Clustering Methods I-Con as Variational Method Why do we need to unify representation learners? How to choose neighborhood distributions for your problem Comparing I-Con, MLE, and the KL Divergence On I-Cons Hyperparameters 15 17 24 27 28 29 30 14 Published as conference paper at ICLR"
        },
        {
            "title": "A ADDITIONAL EXPERIMENTS ON DEBIASING FEATURE LEARNING",
            "content": "The following experiments aim to test the effect of our debiasing approach in feature learning. We followed the experimental setup introduced by Hu et al. (Hu et al., 2023). The architecture consisted of ResNet-34 backbone paired with two-layer multilayer perceptron (MLP) feature extractor. The MLP included hidden layer with 512 units and an output layer with 64 units, without batch normalization. CIFAR-10 & CIFAR-100. The models were trained on the CIFAR-10 dataset for 1000 epochs using the AdamW optimizer with the following hyperparameters: β1 = 0.9, β2 = 0.999, learning rate of 1 103, batch size of 1024, and weight decay of 1 105. The learned kernel was either Gaussian or Students t-distribution with degrees of freedom df = 2. For evaluation, we used two methods: (1) linear probing on the 512-dimensional embeddings from the MLPs hidden layer, and (2) k-nearest neighbors (k = 3) classification based on the same embeddings for CIFAR-10 (in-distribution) and CIFAR-100 (out-of-distribution). STL-10 & Oxford-IIIT Pet. With similar setup, the models were trained contrastively on STL-10 (in distribution) without labels using the same hyperparameters as in the CIFAR experiments. For evaluation, we performed (1) linear probing for the STL-10 classification task and Oxford-IIIT Pet binary classification, and (2) k-nearest neighbors classification based on the same embeddings for STL-10 and Oxford-IIIT Pet with = 10. Method CIFAR10 (in distribution) CIFAR100 (out of distribution) Linear Probing Linear Probing KNN KNN qϕ is Gaussian Distribution SimCLR (Chen et al., 2020a) DCL (Chuang et al., 2020) Our Debiasing α = 0.2 Our Debiasing α = 0.4 Our Debiasing α = 0.6 77.79 78.32 79.50 79.07 79. 80.02 83.11 84.07 85.06 85.90 31.82 32.44 32.53 32.53 30.67 qϕ is Students t-distribution t-SimCLR(Hu et al., 2023) DCL (Chuang et al., 2020) Our Debiasing α = 0.2 Our Debiasing α = 0.4 Our Debiasing α = 0.6 90.97 Diverges 91.31 92.70 92.86 88.14 Diverges 88.34 88.50 88. 38.96 Diverges 41.62 41.98 38.92 40.27 42.10 43.19 43.29 29.79 30.75 Diverges 32.88 34.26 32.51 Table 5: Contrastive feature learning evaluation results for CIFAR10 and CIFAR100 datasets with various debasing α factors. Adding some amount of debasing helps raising accuracy in both linear probing and KNN classification. Method STL-10 (in distribution) Oxford-IIIT Pet (out of distribution) Linear Probing Logistic Regression KNN KNN SimCLR (Chen et al., 2020a) DCL (Chuang et al., 2020) 77.71 78.32 74.92 75. qϕ is Students t-distribution t-SimCLR(Hu et al., 2023) Our Debiasing α = 0.2 Our Debiasing α = 0.4 Our Debiasing α = 0.6 85.11 85.94 86.13 87.18 83.05 83.15 84.14 83.58 74.80 74.41 83.40 84.11 84.07 84. 71.48 70.22 81.41 81.15 84.13 83.04 Table 6: Contrastive feature learning evaluation results for STL10 (in distribution) and OxfordIIIT Pet (out of distribution) with various debasing α factors. Similar to the other experiments, our debasing helps raising accuracy in both linear probing and KNN classification. 15 Published as conference paper at ICLR 2025 (a) STL-10 embeddings for SimCLR & DCL (b) CIFAR-10 embeddings for SimCLR & DCL (c) CIFAR10 embeddings for models trained on with Gaussian distribution qϕ (d) CIFAR10 features for models trained with Students t-distribution qϕ (e) STL-10 features for models trained with Students t-distribution qϕ Figure 7: t-SNE visualizations of learned embeddings on CIFAR10 and STL10 datasets. (a) and (b) display embeddings from the DCL (Chuang et al., 2020) method before and after applying debiasing, showing tendency to heavily cluster data points, which may hinder out-of-distribution generalization (Hu et al., 2023). (c) and (d) show embeddings with Students t-distribution, where the debiasing factor α enhances clustering and separation, resulting in improved data representation. Published as conference paper at ICLR"
        },
        {
            "title": "B PROOFS FOR UNIFYING DIMENSIONALITY REDUCTION METHODS",
            "content": "We begin by defining the setup for dimensionality reduction methods in the context of I-Con. Let xi Rd represent high-dimensional data points, and ϕi Rm represent their corresponding lowdimensional embeddings, where d. The goal of dimensionality reduction methods, such as Stochastic Neighbor Embedding (SNE) and t-Distributed Stochastic Neighbor Embedding (t-SNE), is to learn these embeddings such that neighborhood structures in the high-dimensional space are preserved in the low-dimensional space. In this context, the low-dimensional embeddings ϕi can be interpreted as the outputs of mapping function fθ(xi), where fθ is essentially an embedding matrix or look-up table. The I-Con framework is well-suited to express this relationship through KL divergence loss between two neighborhood distributions: one in the high-dimensional space and one in the low-dimensional space. Theorem 1. Stochastic Neighbor Embedding (SNE) (Hinton & Roweis, 2002) is an instance of the I-Con framework. Proof. This is one of the most straightforward proofs in this paper, essentially based on the definition of SNE. The target distribution (supervised part), described by the neighborhood distribution in the high-dimensional space, is given by: pθ(ji) = exp (cid:0)xi xj2/2σ2 k=i exp (xi xk2/2σ2 ) (cid:1) (cid:80) , while the learned low-dimensional neighborhood distribution is: exp (cid:0)ϕi ϕj2(cid:1) k=i exp (ϕi ϕk2) qϕ(ji) = (cid:80) . The objective is to minimize the KL divergence between these distributions: (cid:88) = DKL(pθ(i)qϕ(i)) = (cid:88) (cid:88) pθ(ji) log pθ(ji) qϕ(ji) . The embeddings θi are learned implicitly by minimizing L. The mapper is an embedding matrix, as SNE is non-parametric optimization. Therefore, SNE is special case of the I-Con framework, where pθ(ji) and qϕ(ji) represent the neighborhood probabilities in the highand low-dimensional spaces, respectively. Corollary 1 (t-SNE (Van der Maaten & Hinton, 2008)). t-SNE is an instance of the I-Con framework. Proof. The proof is similar to the one for SNE. While the high-dimensional target distribution pθ(ji) remains unchanged, t-SNE modifies the low-dimensional distribution to Students tdistribution with one degree of freedom (Cauchy distribution): qϕ(ji) = (1 + ϕi ϕj2)1 k=i(1 + ϕi ϕk2)1 . (cid:80) The objective remains to minimize the KL divergence. Therefore, t-SNE is an instance of the I-Con framework. Proposition 1. Let := {xi}n i=1, then the following cohesion variance loss Lcohesion-var = 1 (cid:88) ij wijfϕ(xi) fϕ(xj)2 2Var(X) is an instance of Con in the special case wij = p(ji) and qϕ is Gaussian as with large width as σ . 17 Published as conference paper at ICLR Proof. By using AM-GM inequality, we have 1 (cid:88) k=1 ezk (Πn k=1ezk ) 1 = 1 (cid:88) k=1 ezk (e (cid:80)n k=1 zk ) 1 which implies that log (cid:88) k=1 ezk log log (cid:16) (cid:80)n k=1 zk (cid:17) 1 = log (cid:88) k=1 ezk 1 (cid:88) k= zk + log(n) Alternatively, this can be written as log (cid:88) k=1 ezk 1 (cid:88) k=1 zk log(n) Now assume that we have Gaussian Kernel qϕ exp (cid:0)fϕ(xi) fϕ(xj)2/σ2(cid:1) k=i exp (fϕ(xi) fϕ(xk)2/σ2) Therefore, given the inequality of exp-sum that we showed above, we have qϕ(ji) = (cid:80) , log qϕ(ji) = fϕ(xi) fϕ(xj)2 σ2 (cid:88) log (cid:18) exp fϕ(xi) fϕ(xk)2 σ2 (cid:19) k=i 1 nσ2 = 1 σ2 fϕ(xi) fϕ(xj)2 + 1 σ2 (fϕ(xi) fϕ(xj)2 + fϕ(xi) fϕ(xk)2 log(n) fϕ(xi) fϕ(xk)2) log(n) (cid:88) k=i (cid:88) k=i 1 Therefore, the cross entropy H(pθ, qϕ), is bounded by H(pθ, qϕ) = 1 (cid:88) (cid:88) p(ji) log qϕ(ji) (cid:88) (cid:88) p(ji) 1 1 σ2 (fϕ(xi) fϕ(xj)2 + 1 (cid:88) k=i fϕ(xi) fϕ(xk)2) log(n) p(ji)fϕ(xi) fϕ(xj)2 1 n2 (cid:88) ijk p(ji)fϕ(xi) fϕ(xk)2 log(n) p(ji)fϕ(xi) fϕ(xj)2 2Var(X) + log(n) p(ji)fϕ(xi) fϕ(xj)2 2Var(X) + log(n) 1 1 1 j (cid:88) ij (cid:88) ij (cid:88) ij 1 σ2 1 σ2 1 σ2 = = = = 1 σ2 Lcohesion-var + log(n) On the other hand, the L.H.S. can be upper bounded by using second order bound ez 1z+z2/2, which implies that log (cid:88) k=1 ezk log(1 1 (cid:88) k=1 zk + 1 (cid:88) k=1 z2 k) log(n) 18 Published as conference paper at ICLR On the other hand, log(1 + u) u2/2, therefore, log (cid:88) k=1 ezk (1 1 n (cid:88) k=1 zk + 1 (cid:88) k= z2 k) 1 2 (1 1 (cid:88) k= zk + 1 (cid:88) k=1 k)2 log(n) z2 Therefore, in the limit σ , the bounds become tighter and the I-Con loss approaches the cohesion variance loss. Theorem 2. Principal Component Analysis (PCA) is an asymptotic instance of the I-Con. Proof. By using Proposition 1. When pji = 1[i = j], we have the following expression for = 1 = 1 (cid:88) ij (cid:88) pjifϕ(xi) fϕ(xj)2 2Var(X) fϕ(xi) fϕ(xi)2 2Var(X) = 2Var(X) Therefore, minimizing is equivalent to maximizing the variance which is the equivalent of the PCA objective. Intuitivily, the KL divergence is asking fϕ(xi) fϕ(xi)2 = 0 to be the maximum in comparison to fϕ(xi) fϕ(xj)2 to match the supervisory indicator function, which implies the minimization of the sum of fϕ(xi) fϕ(xj)2, which is maximizing the variance. If we restrict fϕ to be linear projection map, then minimizing would be equivalent to PCA."
        },
        {
            "title": "C PROOFS FOR UNIFYING FEATURE LEARNING METHODS",
            "content": "We now extend the I-Con framework to feature learning methods commonly used in contrastive learning. Let xi Rd be the input data points, and fϕ(xi) Rm be their learned feature embedding. In contrastive learning, the goal is to learn these embeddings such that similar data points (positive pairs) are close in the embedding space, while dissimilar points (negative pairs) are far apart. This setup can be expressed using neighborhood distribution in the original space, where neighbors are defined not by proximity in Euclidean space, but by predefined relationships such as data augmentations or class membership. The learned embeddings fϕ(xi) define new distribution over neighbors, typically using Gaussian kernel in the learned feature space. We show that InfoNCE is natural instance of the I-Con framework, and many other methods, such as SupCon, CMC, and Cross Entropy, follow from this. Theorem 3 (InfoNCE (Bachman et al., 2019)). InfoNCE is an instance of the I-Con framework. Proof. InfoNCE aims to maximize the similarity between positive pairs while minimizing it for negative pairs in the learned feature space. In the I-Con framework, this can be interpreted as minimizing the divergence between two distributions: the neighborhood distribution in the original space and the learned distribution in the embedding space. The neighborhood distribution pθ(ji) is uniform over the positive pairs, defined as: pθ(ji) = (cid:26) 1 0 if xj is among the positive views of xi, otherwise. where is the number of positive pairs for xi. The learned distribution qϕ(ji) is based on the similarities between the embeddings fϕ(xi) and fϕ(xj), constrained to unit norm (fϕ(xi) = 1). Using temperature-scaled Gaussian kernel, this distribution is given by: qϕ(ji) = exp (fϕ(xi) fϕ(xj)/τ ) k=i exp (fϕ(xi) fϕ(xk)/τ ) , (cid:80) 19 Published as conference paper at ICLR 2025 where τ is the temperature parameter controlling the sharpness of the distribution. Since fϕ(xi) = 1, the Euclidean distance between fϕ(xi) and fϕ(xj) is 2 2(fϕ(xi) fϕ(xj)). The InfoNCE loss can be written in its standard form: LInfoNCE = (cid:88) log )/τ (cid:1) exp (cid:0)fϕ(xi) fϕ(x+ (cid:80) exp (fϕ(xi) fϕ(xk)/τ ) , where j+ is the index of positive pair for i. Alternatively, in terms of cross-entropy, the loss becomes: LInfoNCE pθ(ji) log qϕ(ji) = H(pθ, qϕ), (cid:88) (cid:88) where H(pθ, qϕ) denotes the cross-entropy between the two distributions. Since pθ(ji) is fixed, minimizing the cross-entropy H(pθ, qϕ) is equivalent to minimizing the KL divergence DKL(pθqϕ). By aligning the learned distribution qϕ(ji) with the target distribution pθ(ji), InfoNCE operates within the I-Con framework, where the neighborhood structure in the original space is preserved in the embedding space. Thus, InfoNCE is direct instance of I-Con, optimizing the same divergence-based objective. Corollary 2. I-Con framework. t-SimCLR and t-SimCNE (Hu et al., 2023; Bohm et al., 2023) are instances of the Given the proof of Theorem 3, we can see that t-SimCLR is equivelant by having the same pθ but qϕ would change from Gaussian distribution over cosine similarity to Student-T distribution over Euclidean distance. qϕ(ji) = (cid:0)fϕ(xi) fϕ(xj)2/τ (cid:1)1 k=i (fϕ(xi) fϕ(xk)2/τ )1 , (cid:80) Theorem 4. VICReg Bardes et al. (2021) without covariance term is an instance of the I-Con framework. Given Proposition 1, we know that any loss in the cohesion variance form is an instance of I-Con: = 1 (cid:88) ij pjifϕ(xi) fϕ(xj)2 2Var(X) If we choose pji to be an indicator over positive pairs, and i+, we obtain = 1 (cid:88) fϕ(xi) fϕ(xi+)2 2Var(X) which is the VICReg loss without the covariance term and with an invariance-to-variance term ratio of 1:2. Observe that VICReg does not have negative pairs because it applies an equal repulsion force to all points. This is equivalent to taking σ in the conditional Gaussian distribution over the embeddings. Theorem 5 (Triplet Loss (Schroff et al., 2015)). Triplet Loss can be viewed as an instance of the I-Con framework with the following distributions pθ(ji) and qϕ(ji): pθ(ji) = (cid:26) 1 0 if xj is among the positive views of xi, otherwise, qϕ(ji) = (cid:16) exp (cid:17) fϕ(xi)fϕ(xj )2 σ2 (cid:16) fϕ(xi)fϕ(xk)2 σ2 (cid:17) , (cid:80) k=i exp particularly in the special case where only two neighbors are considered: one positive view and one negative view. 20 Published as conference paper at ICLR 2025 Proof. The idea of this proof was first presented at (Khosla et al., 2020) using Taylor Approximation; however, in this proof we present stronger bounds for this result. For simplicity, we set σ = 1 (the general bounds for other σ values are provided at the end of the proof). ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) (cid:88) qϕ(ji) log exp (cid:0)fϕ(xi) fϕ(xj)2(cid:1) k=i exp (fϕ(xi) fϕ(xk)2) (cid:80) . In the special case where each anchor xi has exactly one positive x+ the denominator simplifies to: (cid:88) exp (cid:0)fϕ(xi) fϕ(xk)2(cid:1) = exp (cid:0)fϕ(xi) fϕ(x+ and one negative example, )2(cid:1) + exp (cid:0)fϕ(xi) fϕ(x )2(cid:1) . k=i = fϕ(xi) fϕ(x+ Let d+ function, we obtain: )2 and i = fϕ(xi) fϕ(x )2. Substituting these into the loss ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) log exp (cid:0)d+ (cid:1) (cid:1) + exp (cid:0)d (cid:33) exp (cid:0)d+ (cid:32) 1 (cid:1) (cid:88) log 1 = = 1 (cid:88) 1 + exp (cid:0)d (cid:1) d+ (cid:1)(cid:1) . i log (cid:0)1 + exp (cid:0)d+ Recognizing that the expression inside the logarithm is the softplus function, we can leverage its well-known bounds: max(z, 0) log (1 + exp(z)) max(z, 0) + log(2). 1 (cid:88) By letting = d+ , we substitute into the bounds to obtain: max(d+ , 0) (cid:88) max(d+ i , 0) + log(2), 1 where the left-hand side is the Triplet loss LTriplet = 1 the following bounds: (cid:80) max(d+ d , 0). Therefore, we obtain For general σ, the inequality bounds are as follows: log(2) LTriplet L. Lσ σ2 log(2) LTriplet Lσ, where Lσ = σ2 (cid:88) (cid:88) qϕ(ji) log (cid:80) As σ approaches 0, LTriplet approaches Lσ. (cid:16) exp k=i exp (cid:17) fϕ(xi)fϕ(xj )2 σ2 (cid:16) fϕ(xi)fϕ(xk)2 σ2 (cid:17) . Theorem 6. The Supervised Contrastive Loss (Khosla et al., 2020) is an instance of the I-Con framework. Proof. This follows directly from Theorem 3. Define the supervisory and target distributions as: qϕ(j i) = exp (fϕ(xi) fϕ(xj)/τ ) k=i exp (fϕ(xi) fϕ(xk)/τ ) (cid:80) , pθ(j i) = 1 Ki 1 1[i and share the same label], where fϕ is the mapping to deep feature space and Ki is the number of samples in the class of i. Substituting these definitions into the I-Con framework recovers the Supervised Contrastive Loss. 21 Published as conference paper at ICLR 2025 Theorem 7. The X-Sample Contrastive Learning Loss (Sobal et al., 2025) is an instance of the I-Con framework. Proof. Consier the following distribution over corresponding features (e.g. caption embeddings for images): exp(cid:0)gθ(xi) gθ(xj)(cid:1) exp(cid:0)gθ(xi) θ (xk)(cid:1) (cid:80) k=i where could be either parametric or non-parametric mapper to the corresponding embeddings gθ(xi). On the other hand, similar to most feature learning methods, the learned distribution is Gaussian over learned embeddings with cosine distance qϕ(j i) = exp(cid:0)fϕ(xi) fϕ(xj)(cid:1) exp(cid:0)fϕ(xi) fϕ(xk)(cid:1) (cid:80) k=i where fϕ is the mapping to deep feature space. Theorem 8. Contrastive Multiview Coding (CMC) and CLIP are instances of the I-Con framework. Proof. Since we have already established that InfoNCE is an instance of the I-Con framework, this corollary follows naturally. The key difference in Contrastive Multiview Coding (CMC) and CLIP is that they optimize alignment across different modalities. The target probability distribution pθ(ji) can be expressed as: pθ(ji) = 1 1[i and are positive pairs and Vi = Vj], where Vi and Vj represent the modality sets of xi and xj, respectively. Here, pθ(ji) assigns uniform probability over positive pairs drawn from different modalities. The learned distribution qϕ(ji), in this case, is based on Gaussian similarity between deep features, but conditioned on points from the opposite modality set. Thus, the learned distribution is defined as: qϕ(ji) = (cid:80) exp (cid:0)fϕ(xi) fϕ(xj)2(cid:1) exp (fϕ(xi) fϕ(xk)2) kVj . This formulation shows that CMC and CLIP follow the same principles as InfoNCE but apply them in multiview setting, fitting seamlessly within the I-Con framework by minimizing the divergence between the target and learned distributions across different modalities. Theorem 9. Cross-Entropy classification is an instance of the I-Con framework. Proof. Cross-Entropy can be viewed as special case of the CMC loss, where one view corresponds to the data point features and the other to the class logits. The affinity between data point and class is based on whether the point belongs to that class. This interpretation has been explored in prior work, where Cross-Entropy was shown to be related to the CLIP loss (Yang et al., 2022). Theorem 10. Harmonic Loss for classification is an instance of the I-Con framework. Proof. This is the equivalent of moving from Gaussian distribution for q(ji) in Cross-Entropy to Student-T distribution analogs to moving from SNE to t-SNE. More specifically, let be the set of data points, the set of class prototypes, ϕi be the learned class prototype for class i, and be the harmonic loss degree. Consider the following p, which is data-label indicator p(ji) = 1(cid:2)i belongs to class j(cid:3) 22 Published as conference paper at ICLR 2025 and the following q, which is Student-T distribution with 2n 1 degrees for freedom."
        },
        {
            "title": "It can be rewritten as",
            "content": "lim σ0 (1 + fϕ(xi) ϕj2/((2n 1)σ2))n kC(1 + fϕ(xi) ϕk2/((2n 1)σ2))n (cid:80) lim σ0 (((2n 1)σ2) + fϕ(xi) ϕj2)n kC(((2n 1)σ2) + fϕ(xi) ϕk2/)n (cid:80) As σ , the loss function approaches = (cid:88) iC (fϕ(xi) ϕj2)n kC(fϕ(xi) ϕk2/)n (cid:80) whichs the Harmonic Loss for classification as introduced by 10 Theorem 11. Masked Language Modeling (MLM) (Devlin et al., 2019) loss is an instance of the I-Con framework. Proof. In Masked Language Modeling, the objective is to predict masked token given its surrounding context xi. This setup fits naturally within the I-Con framework by defining appropriate target and learned distributions. The target distribution pθ(ji) is the empirical distribution over contexts and tokens j, defined as: pθ(ji) = 1 # [Context precedes token j] , where # [Context precedes token j] counts the number of times token follows context xi in the training corpus and is normalization constant ensuring that (cid:80) pθ(ji) = 1. The learned distribution qϕ(ji) is modeled using the neural networks output logits for token predictions. It is defined as softmax over the dot product between the context embedding fϕ(xi) and the token embeddings ϕj: qϕ(ji) = exp (fϕ(xi) ϕj) kV exp (fϕ(xi) ϕk) (cid:80) , where fϕ(xi) is the embedding of the context xi produced by the model, ϕj is the embedding of token j, and is the vocabulary of all possible tokens. The MLM loss aims to minimize the cross-entropy between the target distribution pθ(ji) and the learned distribution qϕ(ji): LMLM = (cid:88) (cid:88) pθ(ji) log qϕ(ji) = H(pθ, qϕ). Since in practice, for each context xi, only the true masked token bution simplifies to: is considered, the target distriwhere δj,j is the Kronecker delta function, equal to 1 if = and 0 otherwise. Substituting this into the loss function, the MLM loss becomes: pθ(ji) = δj,j , LMLM = (cid:88) log qϕ(j xi). 23 Published as conference paper at ICLR"
        },
        {
            "title": "D PROOFS FOR UNIFYING CLUSTERING METHODS",
            "content": "The connections between clustering and the I-Con framework are more intricate compared to the dimensionality reduction methods discussed earlier. To establish these links, we first introduce probabilistic formulation of K-means and demonstrate its equivalence to the classical K-means algorithm, showing that it is zero-gap relaxation. Building upon this, we reveal how probabilistic K-means can be viewed as an instance of I-Con, leading to novel clustering kernel. Finally, we show that several clustering methods implicitly approximate and optimize for this kernel. Definition 1 (Classical K-means). Let x1, x2, . . . , xN Rn denote the data points, and µ1, µ2, . . . , µm Rn be the cluster centers. The objective of classical K-means is to minimize the following loss function: (cid:88) (cid:88) Lk-Means = 1(c(i) = c)xi µc2, i=1 where c(i) represents the cluster assignment for data point xi, and is defined as: c=1 c(i) = arg min xi µc2. PROBABILISTIC K-MEANS RELAXATION In probabilistic K-means, the cluster assignments are relaxed by assuming that each data point xi belongs to cluster with probability ϕic. In other words, ϕi represents the cluster assignments vector for xi Proposition 2. The relaxed loss function for probabilistic K-means is given by: LProb-k-Means = (cid:88) (cid:88) i=1 c=1 ϕicxi µc2, and is equivalent to the original K-means objective Lk-Means. The optimal assignment probabilities ϕic are deterministic, assigning probability 1 to the closest cluster and 0 to others. Proof. For each data point xi, the term (cid:80)m probabilities ϕic are deterministic, i.e., (cid:26)1 0 ϕic = if = arg minj xi µj2, otherwise. c=1 ϕicxi µc2 is minimized when the assignment With these deterministic probabilities, LProb-k-Means simplifies to the classical K-means objective, confirming that the relaxation introduces no gap. CONTRASTIVE FORMULATION OF PROBABILISTIC K-MEANS Definition 2. Let {xi}N i=1 be set of data points. Define the conditional probablity qϕ(ji) as: qϕ(ji) = (cid:88) c= ϕicϕjc k=1 ϕkc (cid:80)N , where ϕi is the soft-cluster assignments for xi. Given qϕ(ji), we can reformulate probabilistic K-means as contrastive loss: Theorem 12. Let {xi}N Define the objective function as: i=1 Rn and {ϕic}N i=1 be the corresponding assignment probabilities. = (cid:88) i,j (xi xj) qϕ(ji). Minimizing with respect to the assignment probabilities {ϕic} yields optimal cluster assignments equivalent to those obtained by K-means. 24 Published as conference paper at ICLR 2025 Proof. The relaxed probabilistic K-means objective LProb-k-Means is: LProb-k-Means = (cid:88) (cid:88) i=1 c=1 ϕicxi µc2. Expanding this, we obtain: LProb-k-Means = (cid:88) (cid:32) (cid:88) c=1 i=1 (cid:33) ϕic µc2 (cid:88) (cid:32) (cid:88) c=1 i=1 (cid:33) ϕicxi µc + (cid:88) i=1 xi2. The cluster centers µc that minimize this loss are given by: µc = (cid:80)N i=1 ϕicxi (cid:80)N i=1 ϕic . Substituting µc back into the loss function, we get: = (cid:88) i,j (xi xj) qϕ(ji), which proves that minimizing this contrastive formulation leads to the same clustering assignments as classical K-means. Corollary 3. The alternative loss function: = (cid:88) i,j xi xj2 qϕ(ji), yields the same optimal clustering assignments when minimized with respect to {ϕic}. Proof. Expanding the squared norm in the loss function gives: = (cid:88) i,j (cid:0)xi2 2xi xj + xj2(cid:1) qϕ(ji). The terms involving xi2 and xj2 simplify since (cid:80) qϕ(ji) = 1, reducing the loss to: = (cid:88) i,j xi xjqϕ(ji) , which is equivalent to the objective in the previous theorem. PROBABILISTIC K-MEANS AS AN I-CON METHOD In the I-Con framework, the target and learned distributions represent affinities between data points based on specific measures. For instance, in SNE, these measures are Euclidean distances in highand low-dimensional spaces, while in SupCon, the distances reflect whether data points belong to the same class. Similarly, we can define measure of neighborhood probabilities in the context of clustering, where two points are considered neighbors if they belong to the same cluster. The probability of selecting xj as xis neighbor is the probability that point, chosen uniformly at random from xis cluster, is xj. More explicitly, let qϕ(ji) represent the probability that xj is selected uniformly at random from xis cluster: qϕ(ji) = (cid:88) c= ϕicϕjc k=1 ϕkc (cid:80)N . Theorem 13 (K-means as an instance of I-Con). Given data points {xi}N hood probabilities pθ(ji) and qϕ(ji) as: i=1, define the neighborpθ(ji) = exp (cid:0)xi xj2/2σ2(cid:1) exp (xi xk2/2σ2) (cid:80) , qϕ(ji) = (cid:88) c= ϕicϕjc k=1 ϕkc (cid:80)N . 25 Published as conference paper at ICLR 2025 Let the loss function Lc-SNE be the sum of KL divergences between the distributions qϕ(ji) and pθ(ji): Lc-SNE = DKL(qϕ(i)pθ(i)). (cid:88) Then, Lc-SNE = 1 2σ2 LProb-k-Means (cid:88) H(qϕ(i)), where H(qϕ(i)) is the entropy of qϕ(i). Proof. For simplicity, assume that 2σ2 = 1. Denote (cid:80) exp (cid:0)xi xk2(cid:1) by Zi. Then we have: Let Li be defined as (cid:80) xi xj2 qϕ(ji). Using the equation above, Li can be rewritten as: log pθ(ji) = xi xj2 log Zi. Li = (cid:88) xi xj2 qϕ(ji) = = = (cid:88) (log(pθ(ji)) + log(Zi))qϕ(ji) (cid:88) (cid:88) qϕ(ji) log(pθ(ji)) + (cid:88) qϕ(ji) log(Zi) qϕ(ji) log(pθ(ji)) + log(Zi) = H(qϕ(i), pθ(i)) + log(Zi) = DKL(qϕ(i)pθ(i)) + H(qϕ(i)) + log(Zi). Therefore, LProb-KMeans, as defined in Corollary 3, can be rewritten as: (cid:88) (cid:88) (cid:88) xi xj2 qϕ(ji) = Li LProb-KMeans = i DKL(qϕ(i)pθ(i)) + H(qϕ(i)) + log(Zi) (cid:88) = (2) (3) (4) (5) (6) (7) (8) (9) = Lc-SNE + (cid:88) H(qϕ(i)) + constant. (10) Therefore, Lc-SNE = LProb-KMeans (cid:88) H(qϕ(i)). If we allow σ to take any value, the entropy penalty will be weighted accordingly: Lc-SNE = 1 2σ2 LProb-KMeans (cid:88) H(qϕ(i)). Note that the relation above is up to an additive constant. This implies that minimizing the contrastive probabilistic K-means loss with entropy regularization minimizes the sum of KL divergences between qϕ(i) and pθ(i). Corollary 4. Spectral Clustering is an instance of the I-Con framework. Proof. From Theorem 13, we know that K-Means clustering can be formulated as an instance of the I-Con framework, where the clustering assignments depend on the inner products of the data points. Spectral Clustering extends this idea by first embedding the data into lower-dimensional space using the top eigenvectors of the normalized Laplacian derived from the affinity matrix A. The affinity matrix is constructed using similarity measure (e.g., an RBF kernel) and encodes the probabilities of assignments between data points. Given this transformation, spectral clustering is an instance of I-Con on the projected embeddings. 26 Published as conference paper at ICLR 2025 Theorem 14. Normalized Cuts (Shi & Malik, 2000) is an instance of I-Con. Proof. The proof for this follows naturally from our work on K-Means analysis. The loss function for normalized cuts is defined as: LNormCuts = (cid:88) c=1 cut(Ac, Ac) vol(Ac) , where Ac is subset of the data corresponding to cluster c, Ac is its complement, and cut(Ac, Ac) represents the sum of edge weights between Ac and Ac, while vol(Ac) is the total volume of cluster Ac, i.e., the sum of edge weights within Ac. Similar to K-Means, by reformulating this in contrastive style with soft-assignments, the learned distribution can be expressed using the probabilistic cluster assignments ϕic = p(cxi) as: qϕ(ji) = (cid:88) c=1 ϕicϕjcdj k=1 ϕkcdk (cid:80)N , where dj is the degree of node xj, and the volume and cut terms can be viewed as weighted sums over the soft-assignments of data points to clusters. This reformulation shows that normalized cuts can be written in manner consistent with the ICon framework, where the target distribution pθ(ji) and the learned distribution qϕ(ji) represent affinity relationships based on graph structure and cluster assignments. Thus, normalized cuts is an instance of I-Con, where the loss function optimizes the neighborhood structure based on the cut and volume of clusters in manner similar to K-Means and its probabilistic relaxations. Theorem 15. Mutual Information Clustering is an instance of I-Con. Proof. Given the connection established between SimCLR, K-Means, and the I-Con framework, this result follows naturally. Specifically, the target distribution pθ(ji) (the supervised part) is uniform distribution over observed positive pairs: pθ(ji) = (cid:26) 1 0 if xj is among the positive views of xi, otherwise. On the other hand, the learned embeddings ϕi represent the probabilistic assignments of xi into clusters. Therefore, similar to the analysis of the K-Means connection, the learned distribution is modeled as: qϕ(ji) = (cid:88) ϕicϕjc k=1 ϕkc (cid:80)N . c=1 This shows that Mutual Information Clustering can be viewed as method within the I-Con framework, where the learned distribution qϕ(ji) aligns with the target distribution pθ(ji), completing the proof. I-CON AS VARIATIONAL METHOD Variational bounds for mutual information are widely explored and have been connected to loss functions such as InfoNCE, where minimizing InfoNCE maximizes the mutual information lower bound (Oord et al., 2018; Poole et al., 2019). The proof usually starts by rewriting the mutual information: I(X; ) = Ep(x,y) (cid:20) log (cid:21) q(xy) p(x) + Ep(y) [DKL (p(xy) q(xy))] This expression is typically used to derive lower bound for I(X; ). The proof usually begins by assuming that is uniform over discrete data points = {xi}N i=1 (i.e., we use uniform sampling 27 Published as conference paper at ICLR 2025 for data points). By using the fact that p(xi) = 1 mutual information lower bound becomes , we can write p(x, y) = 1 p(xy). Therefore, the I(X; ) Ep(x,y) [log q(xy)] Ep(x,y) [log p(x)] = Ep(x,y) [log q(xy)] + log(N ) = ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) x,yX (cid:88) (cid:88) yX xX p(xy) log q(xy) + log(N ) p(xy) log q(xy) + log(N ) = (p(xy), q(xy)) + log(N ) Therefore, maximizing the cross-entropy between the two distributions maximizes the mutual information between samples. On the hand, Variational Bayesian (VB) methods are fundamental in approximating intractable posterior distributions p(z x) with tractable variational distributions qϕ(z). This approximation is achieved by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior: KL(qϕ(z)p(z x)) = Eqϕ(z) (cid:20) log qϕ(z) p(z x) (cid:21) . The optimization objective, known as the Evidence Lower Bound (ELBO), is given by: ELBO = Eqϕ(z) [log p(x, z)] Eqϕ(z) [log qϕ(z)] . (11) (12) Maximizing the ELBO is equivalent to minimizing the KL divergence, thereby ensuring that qϕ(z) closely approximates p(z x) (Blei et al., 2017). VB can be framed within the I-Con framework by making specific mappings between the variables and distributions. Let correspond to the data point x, and correspond to the latent variable z. We can set the supervisory distribution pθ(z x) to be the true posterior p(z x). This allow us to define the learned distribution qϕ(z x) to be independent of x, i.e., qϕ(z x) = qϕ(z). Under these settings, the I-Con loss simplifies to: (cid:90) L(ϕ) = KL (p(z x)qϕ(z)) dx = Ep(x) [KL(p(z x)qϕ(z))] . (13) xX INTERPRETATION Global Approximation: In VB, qϕ(z) serves as global approximation to the posterior p(z x) across all data points x. Similarly, in I-Con, when qϕ(j i) = qϕ(j), the learned distribution provides uniform approximation across all i. Variational Alignment: Both frameworks employ variational techniques to align tractable distribution qϕ with an intractable or supervisory distribution p. This alignment ensures that the learned representations capture essential information from the target distribution. Framework Generalization: I-Con generalizes VB by allowing qϕ(j i) to depend on i, enabling more flexible and data-specific alignments. VB is recovered as special case where the learned distribution is uniform across all data points. WHY DO WE NEED TO UNIFY REPRESENTATION LEARNERS? I-con not only provides deeper understanding of these methods but also opens up the possibility of creating new methods by mixing and matching components. We explicitly use this property to discover new improvements to both clustering and representation learners. In short, I-Con acts like periodic table of machine learning losses. With this periodic table we can more clearly see the implicit assumptions of each method by breaking down modern ML losses into more simple components: pairwise conditional distributions and q. 28 Published as conference paper at ICLR 2025 One particular example of how this opens new possibilities is with our generalized debiasing operation. Through our experiments we show adding slight constant linkage between datapoints improves both stability and performance across clustering and feature learning. Unlike prior art, which only applies to specific feature learners, our debiasers can improve clusterers, feature learners, spectral graph methods, and dimensionality reducers. Finally it allows us to discover novel theoretical connections by compositionally exploring the space, and considering limiting conditions. We use I-Con to help derive novel theoretical equivalences between K-Means and contrastive learning, and between MDS, PCA, and SNE. Transferring ideas between methods is standard in research, but in our view it becomes much simpler to do this if you know methods are equivalent. Previously, it might not be clear how exactly to translate an insight like changing Gaussian distributions to Cauchy distributions in the upgrade from SNE to T-SNE has any effect on clustering or representation learning. In I-Con it becomes clear to see that similarly softening clustering and representation learning distributions can improve performance and debias representations."
        },
        {
            "title": "G HOW TO CHOOSE NEIGHBORHOOD DISTRIBUTIONS FOR YOUR PROBLEM",
            "content": "PARAMETERIZATION OF LEARNING SIGNAL Parametric: (Learn network to transform data points to representations). Use parametric method to quickly represent new datapoints without retraining. Use parametric method if there is enough features in the underlying data to properly learn representation. Use this option with datasets with sparse supervisory signal in order to share learning signal through network parameters. Nonparametric: (Learn one representation per data point). Use nonparametric method if datapoints are abstract and dont contain natural features that are useful for mapping. Use this option to better optimize the loss of each individual datapoint. Do not use this in sparse supervisory signal regimes (Like augmentation based contrastive learning), as there are not enough links to resolve each individual embedding. CHOICE OF SUPERVISORY SIGNAL Gaussians on distances in the input space: though this is common choice and underlies methods like k-means, with enough data it is almost always better to use k-neighbor distributions as they better capture local topology of data. This is the same intuition that is used to justify spectral clustering over k-means. K-neighbor graphs distributions: If your data can be naturally put into graph instead of just considering Gaussians on the input space we suggest it. This allows the algorithm to adapt local neighborhoods to the data, as opposed to considering all points neighborhoods equally shaped and sized. This better aligns with the manifold hypothesis. Contrastive augmentations: When possible, add contrastive augmentations to your graph - this will improve performance in cases where quantities of interest (like an image class) are guaranteed to be shared between augmentations. General kernel smoothing techniques: Use random walks to improve the optimization quality. It connects more points together and in some cases mirrors geodesic distance on the manifold (Crane et al., 2013). Debiasing: Use this if you think negative pairs actually have small chance of aligning positively. For small number of classes this parameter scales like the inverse of the number of classes. You can also use this to improve stability of the optimization. CHOICE OF REPRESENTATION: Any conditional distribution on representations can be used, so consider what kind of structure you want to learn, tree, vector, cluster, etc. And choose the distribution to be simple and meaningful for that representation. Published as conference paper at ICLR 2025 Discrete: Use discrete cluster-based representations if interpretability and discrete structure are important Continuous Vector: Use vector representation if generic downstream performance is concern as this is bit easier to optimize than discrete variants. COMPARING I-CON, MLE, AND THE KL DIVERGENCE There are many connections between KL divergence and maximum likelihood estimation. We highlight the differences between standard MLE approach and I-Con. In short, although I-Con has maximum likelihood interpretation, its specific functional form allows it to unify both unsupervised and supervised methods in way that elucidates the key structures that are important for deriving new representation learning losses. This is in contrast to the commonly known connection between MLE and KL divergence minimization, which does not focus on pairwise connections between datapoints and does not provide as much insight for representation learners. To see this we note that the conventional connection between MLE and KL minimization is as follows: θMLE = arg min θ DKL( ˆP Qθ), where the empirical distribution, ˆP , is defined as: ˆP (x) = 1 (cid:88) i=1 δ(x xi), where δ(x xi) is the Dirac delta function. The classical KL minimization fits parameterized model family to an empirical distribution. In contrast the I-Con equation: L(θ, ϕ) = (cid:90) iX DKL (pθ(i)qϕ(i)) Operates on conditional distributions and captures an average KL divergence instead of single KL divergence. Secondly, I-Con explicitly involves computation over neighboring datapoints which does not appear in the aforementioned equation. This decomposition of methods into their actions on their neighborhoods makes many methods simpler to understand, and makes modifications of these methods easier to transfer between domains. It also makes it possible to apply this theory to unsupervised problems where empirical supervisory data does not exist. Furthermore some methods, like DINO, do not share the exact functional form of I-Con, and suffer from various difficulties like collapse which need to be handled with specific regularizers. This shows that I-Con is not just catchall reformulation of MLE, but is capturing specific functional form shared by several popular learners. ON I-CONS HYPERPARAMETERS One important way that I-Con removes hyperparameters from existing works is that it does not rely on things like entropy penalties, activation normalization, activation sharpening, or EMA stabilization to avoid collapse. The loss is self-balancing in this regard as any way that it can improve the learned distribution to better match the target distribution is fair game. This allows one to generalize certain aspects of existing losses like InfoNCE. In I-Con info NCE looks like fixed-width Gaussian kernels mediating similarity between representation vectors. In I-Con its trivial to generalize these Gaussians to have adaptive and learned covariances for example. This allows the network to select its own level of certainty in representation learning. If you did this naively, you would need to ensure the loss function doesnt cheat by making everything less certain. Nevertheless I-Con defines space of methods depending on the choice of and q. The choice of these two distributions becomes the main source of hyperparameters we explore. In particular our experiments change the structure of the supervisory signal (often p). For example, in clustering experiment changing from Gaussians with respect to distance to graph adjacency transforms 30 Published as conference paper at ICLR 2025 K-Means into Spectral clustering. Its important to note that K-means has benefits over Spectral clustering in certain circumstances and vice-versa, and theres not necessarily singular right choice for in every problem. Like many things in ML, the different supervisory distributions provide different inductive biases and should be chosen thoughtfully. We find that this design space makes it easier to build better performing supervisory signals for specific important problems like unsupervised image classification on ImageNet and others."
        }
    ],
    "affiliations": [
        "Google",
        "MIT",
        "Microsoft"
    ]
}