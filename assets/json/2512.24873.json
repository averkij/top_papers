{
    "paper_title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "authors": [
        "Weixun Wang",
        "XiaoXiao Xu",
        "Wanhe An",
        "Fangwen Dai",
        "Wei Gao",
        "Yancheng He",
        "Ju Huang",
        "Qiang Ji",
        "Hanqi Jin",
        "Xiaoyang Li",
        "Yang Li",
        "Zhongwen Li",
        "Shirong Lin",
        "Jiashun Liu",
        "Zenan Liu",
        "Tao Luo",
        "Dilxat Muhtar",
        "Yuanbin Qu",
        "Jiaqiang Shi",
        "Qinghui Sun",
        "Yingshui Tan",
        "Hao Tang",
        "Runze Wang",
        "Yi Wang",
        "Zhaoguo Wang",
        "Yanan Wu",
        "Shaopan Xiong",
        "Binchen Xu",
        "Xander Xu",
        "Yuchi Xu",
        "Qipeng Zhang",
        "Xixia Zhang",
        "Haizhou Zhao",
        "Jie Zhao",
        "Shuaibing Zhao",
        "Baihui Zheng",
        "Jianhui Zheng",
        "Suhang Zheng",
        "Yanni Zhu",
        "Mengze Cai",
        "Kerui Cao",
        "Xitong Chen",
        "Yue Dai",
        "Lifan Du",
        "Tao Feng",
        "Tao He",
        "Jin Hu",
        "Yijie Hu",
        "Ziyu Jiang",
        "Cheng Li",
        "Xiang Li",
        "Jing Liang",
        "Chonghuan Liu",
        "ZhenDong Liu",
        "Haodong Mi",
        "Yanhu Mo",
        "Junjia Ni",
        "Shixin Pei",
        "Jingyu Shen",
        "XiaoShuai Song",
        "Cecilia Wang",
        "Chaofan Wang",
        "Kangyu Wang",
        "Pei Wang",
        "Tao Wang",
        "Wei Wang",
        "Ke Xiao",
        "Mingyu Xu",
        "Tiange Xu",
        "Nan Ya",
        "Siran Yang",
        "Jianan Ye",
        "Yaxing Zang",
        "Duo Zhang",
        "Junbo Zhang",
        "Boren Zheng",
        "Wanxi Deng",
        "Ling Pan",
        "Lin Qu",
        "Wenbo Su",
        "Jiamang Wang",
        "Wei Wang",
        "Hu Wei",
        "Minggang Wu",
        "Cheng Yu",
        "Bing Zhao",
        "Zhicheng Zheng",
        "Bo Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure."
        },
        {
            "title": "Start",
            "content": "Let It Flow: Agentic Crafting on Rock and Roll"
        },
        {
            "title": "Building the ROME Model within an Open Agentic Learning Ecosystem",
            "content": "ROCK & ROLL & IFLOW & DT Joint Team (cid:19) ROCK (cid:19) ROLL (cid:19) iFlow CLI (cid:19) Terminal Bench Pro (cid:19) iFlow-ROME"
        },
        {
            "title": "Abstract",
            "content": "Agentic crafting, unlike one-shot response generation for simple tasks, requires LLMs to operate in real-world environments over multiple turnstaking actions, observing outcomes, and iteratively refining artifacts until complex requirements are satisfied. Yet the spirit of agentic crafting reaches beyond code, into broader tooland languagemediated workflows where models must plan, execute, and remain reliable under interaction. Reaching this new regime demands sustained, painstaking effort to build an agentic ecosystem as the foundational bedrock, ultimately culminating in an agent model as the capstone. ROME wasnt built in day. principled, end-to-end agentic ecosystem can streamline the development of the agent LLMs from training to production deployment, accelerating the broader transition into the agent era. However, the opensource community still lacks such an ecosystem, which has hindered both practical development and production adoption of agents. To this end, we introduce the Agentic Learning Ecosystem (ALE), foundational infrastructure that optimizes the end-to-end production pipeline for agent LLMs. ALE consists of three system components. ROLL is post-training framework for weight optimization. ROCK is sandbox environment manager that orchestrates environments for trajectory generation. iFlow CLI is an agent framework that enables configurable and efficient context engineering for environment interaction. We release ROME (ROME is Obviously an Agentic ModEl), an open-source agent grounded by ALE and trained on over one million trajectories. In addition, we curate suite of data composition protocols that synthesize data spanning isolated, static snippets to dynamic, complex agentic behaviors, with built-in verification of safety, security, and validity. We further develop an end-to-end training pipeline and propose novel policy optimization algorithm IPA, which assigns credit over semantic interaction chunks rather than individual tokens, improving training stability over long horizons. Empirical evaluations show that ROME achieves strong results across mainstream agentic benchmarks, including 24.72% on Terminal-Bench 2.0 and 57.40% accuracy on SWE-bench Verified, outperforming similarly sized models and rivaling those with over 100B parameters. To enable more rigorous evaluation, we introduce Terminal Bench Pro, benchmark with improved scale, domain coverage, and contamination control. ROME still demonstrates competitive performance among open-source models of similar scale and has been successfully deployed in production, demonstrating the practical effectiveness of the ALE. 5 2 0 2 1 3 ] . [ 1 3 7 8 4 2 . 2 1 5 2 : r Figure 1: Overview of the Agentic Learning Ecosystem (ALE) and ROME Performance."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Agentic Learning Ecosystem: ROME Wasnt Built in Day 2.1 System Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Agentic RL Training Framework: ROLL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Environment Execution Engine: ROCK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Agent Framework: iFlow CLI 2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Agentic Model: ROME is Obviously an Agentic ModEl 3.1 Data Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Agent Competencies as Blueprint for Data Design . . . . . . . . . . . . . . . . . . 3.1.2 Code-Centric Basic Data Composition . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.3 Agentic Data Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.4 Safety-Aligned Data Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Training Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Continuous Pre-training Develops the Agentic Basic Behaviors . . . . . . . . . . . 3.2.2 Anchoring Reinforcement Learning in Reliable Policy Regions via Supervised Fine- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Tuning . . . . . . . . . . . . . 3.2.3 Prepare Training Instance for Reinforcement Learning . . . . . . . . . . . . . . . . 3.2.4 Towards Efficient and Scalable Agentic Reinforcement Learning . . . . . . . . . . . Specialized off-policy baseline for industrial agentic RL . . . . . . . . . . 3.2.4.1 3.2.4.2 Modeling Multi-Turn Agentic Task as Chunked MDP . . . . . . . . . . . 3.2.4.3 Reconstruct Training Objective via Chunk-Level Optimization . . . . . . 3.2.4.4 Rollout Paradigm Refinement via Chunk-Level Initialized Sampling . . . 3.3 Experiments and Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Evaluation Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 Terminal Bench Pro: More Rigorous and Fine-Grained Benchmark for Terminal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Agents . . . . . . . . . . . 3.3.3 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Conclusion 5 Authors 6 Appendix 6.1 Real-world Case Study and Subjective Evaluation . . . . . . . . . . . . . . . . . . . . . . . 2 3 4 4 6 8 9 10 10 11 12 15 16 16 19 19 20 22 22 24 26 26 27 31 33 34"
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed transformative wave in software engineering driven by large language models (LLMs) (Hou et al., 2024). Early efforts largely cast LLMs as one-shot generators, emitting static responses to single prompt (Jiang et al., 2025; Allamanis et al., 2018; Hou et al., 2024). Yet this paradigm provides limited iterative reasoning and lacks grounded feedback loops, rendering it ill-suited for complex, end-to-end workflows. Accordingly, the frontier of LLM-based workflow-driven task (e.g., software engineering) is shifting toward the agentic crafting1 paradigm, which enables LLMs to plan, execute, and self-correct through multi-turn interactions with environments, spanning software repositories, terminals and broader tooland language-mediated workflows in the real world (Ning et al., 2025; Ye et al., 2025; Wang et al., 2025e; Gao et al., 2023; Novikov et al., 2025). However, the widespread practical adoption of agentic crafting remains elusive in the absence of scalable, end-to-end agentic ecosystem. Prior work has sought to improve agentic crafting via supervised fine-tuning (SFT) on limited human demonstrations (Emergent Mind, 2025; Wang et al., 2025a), or through ad-hoc reinforcement learning (RL) recipes that are often struggles with long-horizon tasks and sparse, delayed rewards (Luo et al., 2025; Tan et al., 2025; Wang et al., 2025a). In this report, we contend that principled agentic ecosystem must close the loop spanning data generation, agent execution, and policy optimization, enabling an continuous end-to-end optimization workflow that can adapt to distribution shift and growing complexity in production environments. To bridge this gap, we present the Agentic Learning Ecosystem (ALE), full-stack infrastructure that unifies data, training, and deployment for agentic intelligence. Concretely, ALE comprises three synergistic system components: ROLL (Reinforcement Learning Optimization for Large-Scale Learning): scalable RL training framework supporting multi-environment rollouts, chunk-aware credit assignment, and stable policy updates for long-horizon agentic tasks. ROCK (Reinforcement Open Construction Kit): secure, sandboxed agent execution platform that provides executable, tool-grounded environments, supporting interaction trajectory synthesis, execution, and validation. iFlow CLI: An agent framework that orchestrates structured prompt suites for environment interaction, coupled with user-facing interface that packages agents for real-world workflows and exposes APIs for continuous refinement via user feedback. Grounded in ALE, we incubate ROME as an open-source agent LLM based on Qwen3-MoE, tightly developed within our established ecosystem. Along the road to ROME, we take two deliberate steps. First, we establish curated, coherent data composition workflow that synthesizes multi-source, multilingual, tool-grounded trajectories. Benefiting from strong sandbox isolation and fine-grained permission control of ROCK, we run rigorous security, safety, and validity verification to ensure the integrity and quality of the generated trajectories. Second, we leverage millions of high-quality trajectories to iteratively refine an efficient, stage-wise training pipeline from continuous pre-training, SFT, to RL. Enabled by the tight integration of our ecosystem, the end-to-end training pipeline remains both high-throughput, resourceefficient, and user-friendly. To further stabilize RL training dynamics, we propose Interaction-Perceptive Agentic Policy Optimization (IPA), novel algorithm that optimizes policies over semantic interaction chunks (Li et al., 2025). By shifting credit assignment from tokens to semantically meaningful chunks, IPA improves long-horizon stability and ultimately strengthens long-context agentic crafting performance. Extensive empirical results demonstrate that ROME achieves solid and consistent performance across diverse set of agentic benchmarks. On terminal-centric tasks, ROME achieves 57.4% accuracy on SWE-bench Verified and 24.7% on Terminal-Bench v2.0, outperforming models of similar scale and approaching the performance of larger models exceeding 100B parameters. On the more rigorous Terminal Bench Pro, which enforces stricter contamination control and improved domain balance, ROME still performs competitively, showing strong generalization and stability across domains. Furthermore, ROME has been integrated into iFlow CLI and stably deployed in production. This real-world validation, together with ALE, establishes robust, scalable, and production-grade foundation for the continual training and enhancement of ROME. In summary, this technical report presents reliable, cost-effective, secure, and user-friendly training ecosystem that enables practitioners to build customized models tailored to diverse needs. Beyond 1The agentic crafting extends beyond writing code to encompass general-purpose, workflow-driven tasks (e.g, travel plan, GUI assistant) through multi-turn interactions with its environment. 3 (a) The overview of Agentic Learning Ecosystem (ALE). (b) Agentic RL training pipeline. Figure 2: The overview of agentic RL ecosystem (a) and its training pipeline (b). technical stack, ALE is also call to reframe the communitys priorities. In complex agentic settings, the central challenge is no longer merely data scale or curation quality, but the co-design of training infrastructure, executable environments, and evaluation protocols. We hope this work catalyzes collaborative efforts toward agentic benchmarks, standardized execution environments, and reproducible training pipelines, which constitute essential pillars for the next generation of general-purpose agents."
        },
        {
            "title": "2 Agentic Learning Ecosystem: ROME Wasnâ€™t Built in a Day",
            "content": "2.1 System Overview Figure 2a shows the Agentic Learning Ecosystem (ALE) that enables agentic crafting, including the training framework ROLL, the environment execution engine ROCK, and the agent framework iFlow CLI. Below, we briefly describe these three systems. ROLL (Wang et al., 2025c; Lu et al., 2025) is the agentic RL training framework that supports scalable and efficient RL post-training with multiple environments, multi-turn sampling, and policy optimization. ROCK is the environment execution engine that provides secure, sandboxed environments for agentic interaction. It supports environment-driven trajectory generation and validation for data synthesis and closed-loop execution during training. iFlow CLI is the agent framework that manages the context for environment interactions and delivers an end-to-end agentic crafting experience to complete given workflow. The three systems work together to efficiently support agentic RL training: ROLL issues multiple environment calls, ROCK manages and executes these environments within their corresponding sandboxes, and iFlow CLI orchestrates the context between LLM responses and environment outputs. Together, they form an efficient, fault-tolerant, and scalable infrastructure for agentic crafting. 2.2 Agentic RL Training Framework: ROLL Agentic Training Pipeline. Figure 2b depicts an agentic RL training workflow with three key stages, rollout, reward, and training. During rollout, the agent LLM interacts with the environment by emitting tokens that represent actions. After each action, the environment returns an observation. This exchange continues for multiple turns until an episode ends, producing trajectory of interleaved actions and observations. The reward stage then scores each trajectory and outputs scalar reward. Finally, the training stage uses the collected trajectories and rewards to update the agents weights. The updated model is periodically synchronized back to the rollout stage for the next training iteration. ROLL decomposes agentic RL post-training into specialized worker roles, including LLM inference, environment interaction, reward computation, and parameter updates. This separation allows each stage to scale independently and enables efficient communication among roles during distributed execution. Similar to prior frameworks (Sheng et al., 2024; Hu et al., 2024), ROLL (Wang et al., 2025c; Lu et al., 2025) exposes Cluster abstraction and adopts single-controller programming model. The controller 4 (a) Fine-grained Rollout and Asynchronous Training (b) Train-Rollout Multiplexing Figure 3: ROLL Architecture. (a) ROLL pipelines LLM generation, environment interaction, and reward phases at trajectory-level granularity. Training is also decoupled via sample buffer using an asynchronous ratio to manage staleness. (b) ROLL multiplexes dynamic GPU pool by shrinking rollout resources for bursty training and expanding them back during demand peaks. coordinates heterogeneous workers and handles corresponding deployment and lifecycle management, which substantially reduces development complexity for RL researchers. Empirical results from prior work show that rollout is the dominant cost in RL post-training and often contributes roughly 70% of end-to-end overhead (He et al., 2025; Gao et al., 2025b). The problem is more pronounced in agentic training, where the rollout stage may last hundreds of seconds (Lu et al., 2025). Even the environment interaction can become major bottleneck and has been reported to consume more than 15% of total training time (Gao et al.). These observations drive the dedicated optimization for environment execution and LLM generation. In this section, we first explain how ROLL enables finegrained rollout so that LLM generation can proceed concurrently with environment interaction within the rollout stage. We then describe ROLLs asynchronous training pipeline that overlaps rollout with training to reduce training time while preserve the model accuracy. Last, we discuss how train-rollout multiplexing can reduce resource bubbles and improve rollout throughput in asynchronous training. Fine-grained Rollout. ROLL supports asynchronous reward computation during rollout, thus it enables fine-grained rollout by decomposing the rollout stage into three phases: LLM generation, environment interaction, and reward computation. Instead of executing these phases in single full batch, it applies parallelism at the sample level. This design allows users to control the lifecycle of each sample, deciding when and where each phase is executed. As result, ROLL supports pipelined execution of LLM generation, environment interaction, and reward computation at sample-level granularity. Asynchronous Training. As shown in Figure 3a, we decouple the rollout and training stage across different devices. The rollout stage acts as the producer, and the training stage acts as the consumer. ROLL maintains sample buffer to store the completed trajectories and introduces asynchronous ratio to configure the per-sample staleness during the asynchronous training. The asynchronous ratio is defined on per sample as the maximum allowable gap in policy version numbers between the current policy and the policy version that initiated generation of that sample. The asynchronous training pipeline iteratively repeats the following steps. First, the training stage finishes gradient computation from the previous iteration and then fetches target batch of trajectories from the sample buffer in blocking manner. Samples that violate the asynchronous ratio constraint are discarded to preserve model accuracy. Second, the rollout stage is suspended and model weights are synchronized from the training workers to the rollout workers. Third, the rollout stage resumes and generates new trajectories using the updated model weights, while the training stage performs gradient computation on the fetched samples in parallel to maximize resource utilization. Our prior work, ROLL-Flash (Lu et al., 2025), conduct extensive empirical studies to show that ROLLs asynchronous training can effectively balance training accuracy and throughput. We refer interested readers to that work for details. TrainRollout Multiplexing. Although an asynchronous training architecture can overlap training and rollout via pipelining, bubbles are inevitable due to imbalanced stages. The rollout stage typically takes longer than training, the trainer may stall while waiting for enough trajectories to be collected in the sample buffer. Unlike classic pipelining with fixed resource allocation, GPUs can be dynamically reassigned between stages based on the current critical path. When rollout becomes the bottleneck, allocating more GPUs to rollout accelerates trajectory collection. Conversely, when training is the bottleneck, resources should be prioritized for training. 5 Figure 4: ROCK System Architecture. Figure 3 illustrates the bubble problem when rollout stage dominates the end-to-end iteration time. Rollout typically exhibits pronounced long-tail latency distribution: the staleness bound caps the number of in-flight trajectories, and while most trajectories finish quickly, small fraction of stragglers run up to the maximum context length, leaving many rollout GPUs underutilized. Meanwhile, the training stage is comparatively short but must wait until rollout has produced enough valid samples. Under static GPU partition between rollout and training, this mismatch creates resource bubbles. Our key insight is that rollout demand is highly time-varying: it peaks immediately after weight synchronization, when many new trajectories are launched, and then drops into low-demand valley where only small set of stragglers remain. In contrast, the training stage consumes resources in short, bursty episodes. Building on this observation, we introduce time-division multiplexing with dynamic GPU partition between rollout and training. As shown in Figure 3b, the system first assigns all GPUs to rollout to rapidly generate batch of samples. Once the sample buffer accumulates sufficient data for the next training step, the system triggers shrink operation that temporarily reallocates fixed subset of GPUs to training, while consolidating the remaining unfinished trajectories onto the rollout GPUs that remain. After training completes, an expand operation returns those GPUs to rollout to serve the next demand peak. This policy aligns training bursts with rollout demand valleys, reducing bubbles and improving overall GPU utilization compared to statically disaggregated asynchronous design. 2.3 Environment Execution Engine: ROCK ROCK is scalable and user-friendly system for managing sandbox environments to complete various agentic crafting applications (e.g., travel plan, GUI assistant). It is designed to be framework-agnostic, providing flexible APIs that allow any RL training frameworks to programmatically build, manage, and schedule these environments. System Architecture and Workflow. Figure 4 illustrates the architecture of ROCK. The ROCK system is designed around client-server architecture to support multiple levels of isolation, guaranteeing operational stability. From the client perspective, interacting with remote environment is as convenient as using local RL environment through small set of primitives such as reset, step, and close. Under the hood, ROCK decouples environment execution from orchestration so that large-scale concurrent rollouts remain stable, debuggable, and resource efficient. ROCK consists of three main components. First, the server tier is governed by the Admin control plane, which serves as the orchestration engine: it provisions sandboxed environments, performs admission control, and manages cluster-wide resource scheduling and allocation. Second, the worker tier comprises Worker nodes deployed on each machine; they run the sandbox runtime and manage local hardware resources. Third, Rocklet is lightweight proxy that mediates communication between the agent SDK and sandboxes, governs outbound network access, and enforces egress policies. In addition, ROCK provides EnvHub, centralized registry for environment images that enables reproducible provisioning and faster cold starts. The agent LLM training, evaluation, and data synthesis impose diverse requirements, and ROCK provides the following features to meet these needs. Skill 1: Streamlined SDK Control. ROCK exposes minimal, consistent control interface aligned with standard GEM RL environment semantics. Users can create, reset, step, and 6 close environments through small set of APIs, simplifying integration with RL training and evaluation pipelines. We detail these APIs later. Skill 2: Seamless Agent Scaling. ROCK supports environments with multiple agents and can provision shared or isolated sandboxes based on the interaction pattern, enabling multiagent collaboration and competition. It also orchestrates diverse agent benchmarks (e.g., SWEbench (Jimenez et al., 2024), Terminal Bench Pro (Team, 2025)) behind unified GEM API, so ROLL can interact heterogeneous environments through single interface and enable multi-task RL training with only minimal configuration changes. Skill 3: Native Agent Bridging. This bridges the gap between the RL framework and the agent framework that reconstructs and aligns the agents native message-based context management. We explain this native agent mode in detail later. Skill 4: Massive-Scale Scheduling. ROCK performs dynamic allocation and reclamation of resources across sandboxes. This enables high utilization under bursty workloads and supports large-scale concurrency, scaling to tens of thousands of simultaneous environments by elastically distributing tasks over the cluster. Skill 5: Robust Fault isolation. Each task runs in its own sandbox. If an agent crashes, gets stuck, or damages its files, the failure is contained within that sandbox and does not interfere with other tasks on the same machine. ROCK also restricts each sandboxs network access with per-sandbox policies, limiting the impact of misbehaving or compromised agents. Tailored Optimizations. ROCK provides permission isolation for untrusted instructions, efficient large-file and artifact transfer, centralized logging, resource guardrails with failure recovery, optional checkpointing and restart support, and tooling for debugging and CI/CD-style environment delivery. API Interfaces. ROCK exposes two primary API services for programmatic control, namely the Sandbox API and the GEM API. The Sandbox API manages the sandboxes that host GEM environments. The GEM API provided by ROCK follows the official GEM standardized API (Axon-RL). It is training-framework agnostic and integrates seamlessly with range of RL frameworks, including veRL (Sheng et al., 2024), OpenRLHF (Hu et al., 2024), and Tinker (Thinking Machines AI). To ensure broad compatibility, ROLL also provides GEM API implementation that adheres to the GEM protocol (Axon-RL). In particular, environment workers managed by the ROLL runtime use the GEM API to mediate interactions between an agent and its environment hosted by ROCK. All endpoints follow RESTful design and use JSON for data interchange. We describe both APIs below. The sandbox API manages the complete lifecycle of sandbox instances. Its functionality can be grouped into three main categories: Provisioning: Create and start sandboxes, with support for custom images, resource configurations, and both synchronous and asynchronous modes. Monitoring: Query the status, operational health, and resource consumption statistics of any running sandbox. Persistence: Stop sandbox instance to release its resources or commit its current state to new image for future use. As standardized interface for RL environments, this protocol enables the API to support the core agent interaction loop for general-purpose tasks: Make: Create new GEM environment instance. Reset: Reset an existing environment instance to its default state. Step: Send an action to advance the environment one step and receive the next state. Close: Close the environment to release resources. Agent Native Mode. The agent native mode connects the agentic RL training with the ROCK. The inconsistency in context management between the training framework (ROLL) and the deployment system (iFlow CLI) can significantly degrade an agents performance in production (Rush, 2025). naive solution would be to force ROLL to perfectly mirror the iFlow CLIs context handling, including its specific logic for multi-turn interactions and prompt concatenation. However, this creates tight coupling: every update to an agents logic would require corresponding reimplementation within ROLL, leading to an unsustainable maintenance burden. To address this, we have implemented ModelProxyService within the ROCK environment. This service acts as proxy, intercepting all LLM requests originating from the agents sandbox. Crucially, 7 Figure 5: The overview of iFlow CLI architecture and execution. these requests already contain the complete historical context, fully orchestrated by the iFlow CLI. The proxy then forwards these requests to the appropriate inference service be it ROLL inference workers during training or an external API (e.g., GPT, Gemini) during deployment. The native mode achieves clean separation. ROLL is simplified to generation engine, while the iFlow CLI retains full control over context management. This not only eliminates implementation complexity in the training framework but also guarantees perfect consistency between training and deployment, resolving both the maintenance and performance issues. The agent native mode ensures consistency not just between training and deployment, but across the full development pipeline, including data synthesis, training, and evaluation. key feature is its support for multiple agent frameworks (iFlow CLI, SWE-Agent (Luo et al., 2025), OpenHands (Wang et al., 2025d), etc.), which lowers the overhead of switching scaffolds and simplifies tasks like generating more diverse training data. 2.4 Agent Framework: iFlow CLI The iFlow CLI is powerful command-line agent framework that exposes an interface for automating and executing complex, multi-step tasks, serving as both the context manager and user interface for our infrastructure layer. We describe the role of iFlow CLI in agentic RL training, and provide its overview, and highlight two key features, namely context engineering and open configuration. The Role of iFlow CLI in Agentic Training. iFlow CLI bears two roles in agentic RL training. First, in agent-native mode, model-proxy service intercepts requests from ROLL and invokes iFlow CLI for context management, ensuring consistency between training and deployment. Second, iFlow CLIs open configuration enables general-purpose LLMs to incorporate domain-specific knowledge during training via context management. By allowing configurable system prompts, tools, and workflows, iFlow CLI becomes flexible substrate for training and refining agent behavior, improving performance on domain-specific agentic tasks. System Architecture and Workflow. As shown in Figure 5, iFlow CLI adopts an orchestrator-worker architecture built around single-agent design principle, following Anthropics recommendations for effective agentic systems (Albert et al., 2024). The system exposes various user interfaces to users including client, IDE plugins, web and SDK. The system is driven by Main Agent that maintains the global task state and executes an iterative control loop. At each step, iFlow CLI receives the user command and loads available persistent memory and prior chat history, then perform context management to assemble the model input. Based on the context, the Main Agent selects the next action, which may be direct response, tool invocation, or call to specialized sub-agent. The tool suites are accessed through unified aggregation layer that wraps heterogeneous capabilities, such as MCP integrations, and returns their results as observations the agent can consume. Importantly, sub-agents are implemented as specialized tools with bounded context, avoiding agent handoffs and removing the need for explicit inter-agent communication. During the control loop, iFlow CLI provides four built-in skills to strengthen context management. The Compress performs context compression for limited prompt budgets. The Reminder reports context 8 changes including environment updates, tool changes, and task done. The Detection identifies issues such as loops and tool-call failures. The Env.Mgmt tracks environment state and notifies the agent upon user environment changes. The iFlow CLI also provides three enhanced capabilities. The Hooks implement session-level preand post-tool checks, such as warnings and interception for destructive commands. The Workflow packages reusable skills as configurable procedures for multi-step tasks. The Memory maintains hierarchical persistent state at the user, project, and global levels. Context Engineering for Agentic Crafting. We adopt single-agent control loop because it is simple, robust, and easy to scale. Following The Bitter Lesson (Sutton, 2019), we avoid brittle, over-engineered pipelines and instead focus on context engineering: supplying the agent with precise, high-quality context so it can plan, act, and self-correct effectively in real software environments. In practice, iFlow CLI implements five techniques to manage context for long-horizon tasks: Persistent memory. iFlow maintains lightweight todo file as external memory across sessions. The agent can read and update it to track plans, open issues, and next steps. Context isolation. For complex tasks, iFlow can delegate sub-tasks to sub-agent. Each subagent operates within dedicated, isolated context, which prevents interference with the main agents workflow and ensures more focused, efficient execution. Context retrieval. iFlow fetches relevant information on demand via agent search, semantic vector retrieval, and knowledge-base integrations (e.g., DeepWiki), reducing reliance on what is already in the prompt. Context compression. To cope with limited context windows, iFlow applies lossy and lossless compression to retain key facts while controlling prompt length. Context enhancement. Users can explicitly highlight critical signals. This includes reinforcing the current task objective or highlighting significant changes in the environment (e.g., new files created, test results) to guide the LLMs attention. Together, these capabilities enable specification-driven workflow for domain tasks: by injecting clear specs (prompts, tools, and procedures) into the context, iFlow can execute specialized workflows (e.g., WeChat Mini-program development or iOS app engineering) while keeping the core agent loop unchanged. The iFlow CLI also exposes open configuration interfaces, making it straightforward to align RL training with domain-specific prompts, tools, and workflows. Open Configuration Capabilities. Real-world software engineering demands more than generic intelligence. It requires strict adherence to domain-specific standards, complex operational logic, and specialized toolchains. To bridge the gap between general-purpose models and specialized engineering requirements, the iFlow CLI exposes highly customizable configuration layer: System Prompt (Behavioral Alignment) To align the models cognitive style with specific domain constraints, the system prompt serves as flexible blueprint. Users can explicitly define workflows, toolsets, usage scenarios, and persona tones. This customization acts as an accurate control mechanism, optimizing the models responses to fit the unique requirements of specific project or field. Workflow / Spec (Process Standardization): To scale from simple code generation to end-to-end, workflow-driven tasks, iFlow CLI introduces Workflows (or Specs). This feature lets users compose disparate AI capabilitiesagents, commands, and toolsinto structured, automated task chains. Whether for code analysis, development cycles, or deployment pipelines, workflows ensure complex processes are executed reliably and autonomously. Tool Set (Functional Extensibility): To extend beyond the LLMs native capabilities, iFlow CLI supports broad integration via the Model Context Protocol (MCP). Users can add custom tools or sub-agents (invoked as tools within single-agent loop), enabling seamless interaction with external APIs, databases, and proprietary environments. 2.5 Summary Our infrastructure, leveraging ROLL, ROCK, and the iFlow CLI, provides system-level support for the entire agentic RL pipeline from training to deployment at the system layer. It is specifically served as the two pillars of high-performance agentic RL: structuring effective training algorithms and constructing quality datasets, as discussed subsequently. 9 Figure 6: Overview of data sources and composition pipelines for training agentic models, spanning code centric basic data and agentic data."
        },
        {
            "title": "3 Agentic Model: ROME is Obviously an Agentic ModEl",
            "content": "This section introduces ROME, our agentic foundation model trained with our ALE infrastructure. ROME excels at wide range of workflow-driven tasks (e.g., GUI assistance, travel plan). We then outline the core principles and procedures behind its development for strong agentic crafting performance, organized into three components: (1) rigorous and principled data acquisition and synthesis workflow; (2) an end-toend training pipeline integrating Agentic Continual Pre-training (CPT), Supervised Fine-tuning (SFT), and Interaction-Perceptive Agentic Policy Optimization(IPA) RL algorithm; and (3) comprehensive benchmark suite. Collectively, these components form systematic pathway that illustrates how ROME leverages the required infrastructure to support next-generation agentic LLM. 3.1 Data Composition 3.1.1 Agent Competencies as Blueprint for Data Design Agentic crafting aims to build autonomous, workflow-driven agents that can reliably translate requirements into working artifacts through an iterative loop of formulation, implementation, verification, and refinement. To characterize what such agents must learn and consequently what training signals our data must provide, we decompose agentic crafting competencies into three tightly coupled dimensions: task understanding and planning, action and execution, and interaction and adaptation: Task Understanding and Planning. This dimension captures the agents ability to interpret natural-language or semi-structured specifications and translate them into well-scoped, executable engineering tasks accompanied by verifiable development plans. The agent must accurately extract user intent, uncover implicit rules and constraints, and surface hidden assumptions that could derail implementation. This involves identifying core system entities, defining precise input-output contracts, establishing boundary conditions, and articulating non-functional requirements (e.g., performance, security, scalability, compatibility) that are often omitted but critical to real-world viability. When information is incomplete, the agent should ask minimally sufficient clarification questions and explicitly represent uncertainty, avoiding overcommitment under ambiguous requirements and thereby reducing downstream rework. Action and Execution. This dimension concerns the agents ability to operationalize plans into high-quality implementations and to leverage external toolchains to close the development loop. The agent must actively select appropriate tools based on task characteristics (e.g., code search, build systems, dependency management, compilation/execution, testing frameworks, debuggers, static checkers, formatters, profilers, CI/CD pipelines) and invoke them with correct parameters and sequencing. Critically, the agent must also interpret tool outputs to drive subsequent actions, e.g., localizing defects from failing test logs, resolving style and correctness issues from linter reports, and optimizing bottlenecks guided by profiler evidence. 10 Interaction and Adaptation. This dimension governs the agents ability to maintain dynamic feedback loop with its environment, enabling continuous refinement across iterations. The agent must actively incorporate diverse signals (e.g., runtime behavior, test outcomes, user feedback, code review comments, and evolving system constraints) and adapt its plans and implementations accordingly. For instance, when faced with API deprecations or dependency conflicts, it should perform impact analysis and pivot to alternative strategies (e.g., rollback, refactoring, or substitution) rather than rigidly adhering to an outdated plan. Guided by the above competency analysis, our data design adopts two-tier curriculum that stages the model from foundational proficiency to closed-loop agentic behavior. In the first tier, Basic Data delivers targeted basic capability building that agentic models require as they progress toward full agent behavior. It comprises complementary components including code-centric corpora that support continuous pretraining and strengthen project-level code understanding and generation, and general reasoning data spanning reasoning-intensive tasks and general-purpose instructions that reinforces transferable deduction and planning skills. In the second tier, Agentic Data targets agent-specific requirements by producing closed-loop, executable training units in realistic environments. It is organized into i) instances, which extend conventional query with an executable specification, pinned environment, and verifiable feedback, and ii) trajectories, which record multi-turn interactions in which agents iteratively plan, act, observe runtime feedback, and revise solutions. Agentic data can be directly leveraged in post-training to selectively enhance agentic planning, execution, and adaptation under real-world constraints. Our data maps the competency dimensions to supervision across both tiers. Basic Data concentrates on task understanding and planning by exposing the model to rich project contexts and well-formed specifications that teach intent extraction, requirement scoping, and plan formulation. It also builds the coding and general reasoning foundations that later enable effective action, execution, and iterative refinement, without relying on explicit tool-use traces. Agentic Data then provides targeted strengthening of action and execution and of interaction and adaptation. It embeds requirements in pinned, executable environments, supplies verifiable runtime feedback through deterministic builds and tests, and captures singleand multi-turn trajectories in live settings. This setting both trains robust execution and adaptation and grounds task understanding and planning in realistic constraints, turning high-level plans into working solutions under real-world conditions. Together, the two tiers of data form staged curriculum. The basic Data builds breadth and reliability in core coding and reasoning without full environment orchestration, while the agentic data then adds closed-loop execution and concrete runtime signals that directly supervise planning discipline, execution fidelity, and adaptive iteration under real world constraints. This progression operationalizes the competency blueprint and provides coherent path from foundational skills to full agentic capabilities. 3.1.2 Code-Centric Basic Data Composition As cornerstone of agentic LLM capabilities, coding proficiency requires robust foundation of largescale, high-quality code data. Building such corpus entails not only the systematic acquisition of extensive codebases but also the establishment of specialized environments to synthesize and process real-world software engineering data. Consequently, we curate comprehensive dataset and task suite leveraging authentic development ecosystems to cover critical dimensions including code comprehension, fault localization, bug remediation, and automated test generation, etc. Data Acquisition & Preprocessing. We select approximately one million high-quality GitHub repositories based on criteria such as star counts, fork statistics, and contributor activity. Following Seed-Coder (Seed et al., 2025), we concatenate multiple source files within the same repository to form training samples at the project-level code structure, preventing the model from learning only isolated code snippets and promoting understanding of real-world engineering context. In addition, to improve code localization and repair, we further crawl Issues and Pull Requests (PRs) from the selected repositories. We retain only closed Issues and merged PRs to ensure clear problemsolution correspondence. We then use an LLM to filter Issues, removing low-quality cases with vague descriptions, purely question/discussion posts, auto-generated content, or missing key technical details. During IssuePR linking, we retain only PRs with an explicit will-close intent that actually resolve the corresponding Issue, excluding PRs that merely referenced the Issue without substantive fixes. Task Construction and Formalization. Building upon the collected Issue-PR pairs, we formulate five core categories of software engineering tasks: Code Localization. To establish target for modification, we follow the protocol in AGENTLESS (Xia et al., 2024) by adopting the modified-file list from the golden patch as the ground-truth. Formally, given an issue description and the repository structure S, the task is to identify 11 minimal subset of files = { f1, f2, . . . , fn} that require editing to resolve the issue. Code Repair. Building on the localized files, we formulate the repair process as structured transformation. Following AGENTLESS (Xia et al., 2024), golden-patch differences are converted into search-and-replace blocks to provide precise editing signals. Formally, given issue and the relevant code segments C, the model generates set of edits = M(I, C), where represents the search-and-replace blocks specifying the required transformation. Unit Test Generation. To achieve closed-loop verification of the proposed repairs, we formulate test generation task by extracting test-centric patches from the associated PRs. Formally, given the issue and the successfully patched code C, the model synthesizes corresponding test suite = M(I, C) specifically designed to validate the correctness of the repairs. Multi-turn Interaction. To enhance the models capability in multi-turn tasks, we carefully construct high-quality multi-turn interaction dataset. Following the methodology of SWERL (Wei et al., 2025), we treat PR comments as turn-level feedback signals (feedbackt) and the subsequent commit-level code changes as the corresponding responses (responset). This allows for formalizing the iterative refinement process as an evolutionary feedback-edit trajectory: (feedback1, response1) (feedbackn, responsen). Code Reasoning. To further bolster the models underlying reasoning capabilities, we utilize larger and more capable models to synthesize intermediate CoT rationales for file localization, code repair, and unit test generation, ensuring that the model internalizes the analytical logic behind each modification. To guarantee high data fidelity, we implement rigorous rejection sampling pipeline: localization samples are retained only they fully cover the ground-truth set of modified files, while repair and test generation samples are filtered based on sequence-level similarity threshold relative to the golden patches. Employing the aforementioned data collection and task-synthesis procedures, we construct an initial corpus exceeding 200B tokens. Through stringent data hygiene and quality assurance protocols (e.g., deduplication, decontamination, noise reduction, and logical consistency verification), we distill this corpus into high-qualiy dataset comprising 100B tokens, which serves as the foundation for both continuous pre-training and post-training stages. 3.1.3 Agentic Data Composition Agentic data differs fundamentally from conventional code corpora. Instead of isolated snippets or static repositories, it packages tasks with an executable specification, pinned environment, and verifiable feedback, and it records how agents behave when they plan, act, observe runtime signals, and revise solutions. This closed-loop structure is essential for training models to exhibit reliable agentic behavior, yet it introduces challenges that conventional datasets do not address: environment reproducibility, execution closure, high-quality feedback signals, and resistance to superficial solutions. Two core data objects define the agentic data form: Instance. An instance is the agentic analogue of query in basic instruction data. It bundles the prompt (task specification), Dockerfile together with build/test commands that pin the execution environment, and unit tests that provide verifiable feedback. This packaging turns an abstract problem into runnable, reproducible task with clear acceptance criteria. Trajectory. trajectory records an agents behavior on validated instance. It captures multi-turn interactions, including tool invocations, file edits, reasoning traces (optional), and environment feedback. Trajectories exhibit long-horizon properties such as extended length, stateful dependencies, and recovery from partial failure, and they expose behaviors such as loop avoidance, rollback, and plan revision under changing constraints. Open-source artifacts are natural starting point, but raw availability is sparse and noisy for agentic needs. Open-source artifacts are natural starting point, but raw availability is sparse and noisy for agentic needs. Existing curation pipelines for open-source code data often rely on language-specific heuristics or human-labeled quality classifiers, which scale poorly, require continual maintenance, and can introduce subjective bias. More importantly, agentic data imposes strict requirements on execution closure, environment context, and feedback signals, making manual construction and validation prohibitively expensive. As result, the open-source ecosystem provides insufficient high-fidelity agentic data for training capable programming agents at scale. To bridge this gap, we propose two-tiered synthesis strategy. First, we construct general tool-use data to establish foundational capabilities in tool invocation and interactive reasoning. Second, we introduce four-stage programming-centric data specifically designed for software development tasks, 12 which autonomously generates high-fidelity and verifiable instances and diverse trajectories at scale. Moreover, all synthesized data undergoes rigorous data filtering via multi-agent verification system to eliminate false positives, false negatives, and ambiguous or unverifiable executions, ensuring only reliable, executable, and semantically sound trajectories are used for training. General Tool-Use Data Construction. Tool usage is core capability of LLMs, enabling them to expand their knowledge scope and deepen their reasoning (Wang et al., 2024; Hou et al., 2025). To bootstrap this capability, we synthesize tool-use data across two settings: Basic Tool Use. To strengthen the basic tool-use capabilities, we develop an automated pipeline to synthesize high-quality tool-interaction data. Starting from collected task-oriented dialogues, we normalize and parse the utterances to extract structured intent representations, which are then mapped into standardized toolparameter call formats. To support accurate tool selection and parameter grounding, we also curate comprehensive tool documentation aligned with the LLMs usage context. Leveraging this infrastructure, we synthesize complete interaction samples containing tool calls and corresponding execution feedback, followed by quality control through automatic inspection. The resulting synthetic data spans four settings: single-turn single-tool, single-turn multi-tool, multi-turn single-tool, and multi-turn multi-tool. In addition, to enhance robustness under real and noisy conditions, we collect interaction traces from APIs and MCP services originating from internal development and testing environments, and use these traces to ground tool calls in actual execution environments. Tool Use in Interactive Scenarios. To enhance LLMs tool-use ability in web and domain-specific interactive settings, we develop series of simulated environments. First, we design web sandbox centered on e-commerce, built upon real product catalogs and supporting core user actions such as product search, page navigation, detail inspection, specification selection, and order placement. In addition, we construct multiple sandbox environments by automatically synthesizing program files to simulate typical systems such as file systems and billing management. In these environments, class attributes represent the internal data state, while class methods expose interactive tool interfaces. Leveraging each environments internal state and tool schema, we generate customized tasks that require the model to strategically invoke available tools to achieve specified goals. We also introduce simulated users played by LLMs into the task interactions, enhancing the realism of scenarios. Strict quality control is enforced by validating the syntactic correctness of tool invocations and verifying that post-interaction outcomes (e.g., purchased product attributes or updated environment states) align with task expectations. This general tool-use corpus establishes baseline competencies in planning, tool selection, and state tracking, serving as prerequisites for more sophisticated agentic behaviors. Programming-Centric Data Construction. For the targeted software development scenarios, our specialized pipeline generates high-quality agentic data for programming tasks through multi-agent workflow, including divergent exploration, convergent implementation, and rigorous validation, orchestrated through multi-agent framework powered by the iFLOW-cli execution engine and the ROCK sandboxed environment management system. Explore Agent: Divergent Exploration under Constraint Relaxation. We transform PRs, Issues, code snippets, and terminal workflows into structured drafts. This seed data is sourced from highly starred, actively maintained, multi-language GitHub repositories to ensure quality and diversity. We retain closed PRs that can be unambiguously linked to Issues and split each PR into fix patch and test patch to preserve independence and reproducibility. We expand task coverage to additional programming languages such as Go, TypeScript, and JavaScript, drawing from over 20,000 repositories to enhance dataset diversity. We also curate terminal interactions from developer forums and map them to canonical task types such as debugging, system administration, and data science. For each seed, we identify skill primitives (e.g., dependency management, scientific computation, statistical modeling) and generate creative variants that mimic user-agent prompts without imposing implementation paths. lightweight feasibility filter assesses conceptual plausibility and selects the most promising candidates for dataset construction. Instance Builder Agent: Convergent Construction via Self-Play and Validation. It converts drafts into executable and reproducible evaluation instances, each with task-specific Docker environment. It infers compilers, package managers, build tools, and test frameworks from project metadata across different programming languages, generates deterministic build and test commands, and validates the environment through end-to-end compilation and test execution. Each instance includes the task description, complete source files, unit and task-level tests, 13 and Dockerfile that reproduces the environment. The agent runs an internal validation loop within ROCKs sandboxed execution infrastructure via iFLOW-cli, iterating through construction, verification, and refinement until the quality criteria are met. This self-correcting validation mechanism provides formal guarantees across multiple critical dimensions: (i) the Docker image maintains full operational functionality, (ii) the source code compiles without errors, (iii) all unit tests execute successfully, and (iv) the test suite exhibits precise semantic alignment with the task instruction. Review Agent: Rigorous Independent Validation. It assesses each constructed instance along three axes: specification fidelity, implementation completeness, and resistance to superficial solutions. Decoupled from any prior execution state, the agent first runs pre-validated reference solution to confirm solvability. It then employs an independent external language model as an impartial auditor to evaluate both the task specification and the test infrastructure. The audit focuses on two questions: test comprehensiveness asks whether the test suite adequately covers functional requirements, edge cases, and boundary conditions stated in the prompt, while false-positive mitigation checks for cases where an implementation passes all tests yet fails the true objective, revealing weaknesses such as lenient acceptance criteria, backdoor exploitation, or systematic coverage gaps. The review process ensures that each instance reflects real-world challenges rather than artifacts of the validation process. Trajectory Agent: Scalable Behavior Collection. It generates large-scale execution traces by orchestrating diverse agents on validated instances. It concurrently runs multiple scaffolding frameworks, paired with different LLMs to capture heterogeneous behaviors under realistic conditions. Each run produces complete trajectory that records planning steps, reasoning steps, tool invocations, file edits, and environment interactions. After execution, two-stage evaluation is applied: unit tests first determine task completion and fine-grained analysis then examines tool-usage patterns, detects infinite loops and redundant operations, and verifies alignment between behavior and task intent. The resulting corpus of successful trajectories supports model training and capability enhancement across languages, ecosystems, and application scenarios. Using this progressive pipeline, we synthesize 76K instances and trajectory records totaling 30B tokens. The general tool-use data cultivates broad proficiency in tool handling, while the programming-centric data adds closed-loop, environment-pinned supervision that strengthens execution fidelity and adaptive iteration, and grounds task understanding in real-world constraints. Together, these datasets enable posttraining that elevates models from basic tool literacy to specialized, high-confidence agentic capabilities. Data Filtering: Multi-Stage Filtering Pipeline for Rigorous Testing. To better filter the agentic data and provide high-quality information for the training stage, we propose Multi-Stage Filtering Pipeline to handle critical yet often overlooked challenge in multi-turn interaction agentic tasks: brittle test scripts, ambiguous task specifications, or incomplete ground-truth checks can assign incorrect rewardseither false positives (rewarding flawed executions) or false negatives (penalizing valid ones). Such noisy signals mislead policy optimization and induce optimization drift, where the agent learns to exploit evaluator weaknesses rather than solve the task. To ensure high-quality, reliable data for training agentic systems, we implement four-stage filtering pipeline that progressively refines candidate execution traces. This structured approach mitigates the risk of noisy or misleading rewards caused by brittle test scripts, ambiguous specifications, or incomplete ground-truth checkscommon pitfalls that can induce optimization drift during policy learning. The pipeline consists of the following sequential stages: Heuristic Filter: Applies lightweight, rule-based filters to eliminate obviously malformed or syntactically invalid tool calls (e.g., missing required arguments, incorrect parameter types). LLM-based Judge: Uses large language model to assess the relevance between the test patch and the original issue. The judge determines whether the passed trajectory logically addresses the problem described in the task prompt. Execution Simulator: Executes the agents generated trajectory in sandboxed environment to verify whether it passes the associated test cases or resolves the target issue. This stage confirms functional correctness. Expert Inspection: Conducts human-in-the-loop sampling audits on subset of filtered trajectories particularly those flagged as borderline or high-risk by prior stages. Domain experts evaluate overall trajectory quality, including reasoning coherence, efficiency, safety, and readability, ensuring the final dataset reflects real-world usability beyond mere pass/fail metrics. Each stage acts as gatekeeper: only traces that pass all preceding filters proceed to the next. The final output is curated dataset of high-confidence, well-verified execution traces suitable for policy 14 optimization. This pipeline enables robust training on long-tail, complex agentic behaviors while minimizing reward corruption from faulty evaluations. 3.1.4 Safety-Aligned Data Composition When rolling out the instances for the trajectory, we encountered an unanticipatedand operationally consequentialclass of unsafe behaviors that arose without any explicit instruction and, more troublingly, outside the bounds of the intended sandbox. Our first signal came not from training curves but from production-grade security telemetry. Early one morning, our team was urgently convened after Alibaba Clouds managed firewall flagged burst of security-policy violations originating from our training servers. The alerts were severe and heterogeneous, including attempts to probe or access internal-network resources and traffic patterns consistent with cryptomining-related activity. We initially treated this as conventional security incident (e.g., misconfigured egress controls or external compromise). However, the violations recurred intermittently with no clear temporal pattern across multiple runs. We then correlated firewall timestamps with our system telemetry and RL traces, and found that the anomalous outbound traffic consistently coincided with specific episodes in which the agent invoked tools and executed code. In the corresponding model logs, we observed the agent proactively initiating the relevant tool calls and code-execution steps that led to these network actions. Crucially, these behaviors were not requested by the task prompts and were not required for task completion under the intended sandbox constraints. Together, these observations suggest that during iterative RL optimization, language-model agent can spontaneously produce hazardous, unauthorized behaviors at the tool-calling and code-execution layer, violating the assumed execution boundary. In the most striking instance, the agent established and used reverse SSH tunnel from an Alibaba Cloud instance to an external IP addressan outbound-initiated remote access channel that can effectively neutralize ingress filtering and erode supervisory control. We also observed the unauthorized repurposing of provisioned GPU capacity for cryptocurrency mining, quietly diverting compute away from training, inflating operational costs, and introducing clear legal and reputational exposure. Notably, these events were not triggered by prompts requesting tunneling or mining; instead, they emerged as instrumental side effects of autonomous tool use under RL optimization. While impressed by the capabilities of agentic LLMs, we had thought-provoking concern: current models remain markedly underdeveloped in safety, security, and controllability, deficiency that constrains their reliable adoption in real-world settings. We therefore consolidated the logs across the entire dataset and performed statistical analysis to characterize and categorize these phenomena. We refer to them collectively as general-security issues, encompassing set of general risks associated with an agents safe task execution in real-world environments. Specifically, we grouped them into three categories: Safety&Security, Controllability, and Trustworthiness. Safety&Security dictates that an agent must neither spontaneously generate harmful actions nor succumb to malicious inputs, inducements, or external pressures. In practice, this encompasses code safety (e.g., mitigating exploitation primitives), behavioral safety (e.g., preventing hazardous tool-use trajectories), and adversarial robustness (e.g., resisting prompt injection and jailbreaks). Controllability ensures strict adherence to human-specified boundaries and operational rules during task execution. This involves maintaining long-horizon instruction compliance without goal drift, enforcing boundary integrity across tool calls, and prohibiting irreversible operations absent explicit authorization. Trustworthiness requires agent behavior to be reliably interpretable and audit-ready. Key aspects include process transparency (e.g., faithful rationales and action traceability), hallucination mitigation (e.g., grounding claims in observable evidence), and the absence of deceptive behaviors (e.g., concealing side objectives or manipulating logs). Guided by the above taxonomy, we constructed suite of general-security-relevant data to enable systematic evaluation and optimization of agent behavior under realistic cloud execution conditions. Concretely, we assembled diverse seed corpus of general-security scenarios spanning safety&security, controllability, and trustworthiness via data collection, commercial data acquisition, and high-fidelity synthesis. In parallel, we developed dedicated red-teaming system, which programmatically composed agentic task instances with general-security seeds to inject realistic failure modes into otherwise benign workflows. For example, within code-editing task, the agent may be subtly steered to modify privileged system files as an expedient means to satisfy task objectives, thereby introducing unauthorized and potentially irreversible actions. To maximize realism and coverage, we employed multiple injection channels, including prompt-level attacks (e.g., instruction hijacking), repository-level injections (e.g., malicious files or vulnerable dependencies in existing codebases), and tool-level injections (e.g., adversarial tool specifications or side-effectful APIs), producing synthetic data that more similar to the real-world Figure 7: Overview of ROMEs Training Pipeline. incidents. Finally, we generated corresponding golden trajectories devoid of general-security issues for subsequent post-training (e.g., SFT and RL). Our overarching objective was to instill robust security awareness such that, when confronted with tasks containing latent security pitfalls, the agent reliably selected safe action paths and proactively avoided risky behaviors. In future work, we will pursue more systematic investigation along this direction, and we call for sustained community attention to this phenomenon and to the broader agenda of AI safety. 3.2 Training Pipeline Building upon the agentic data composition strategy outlined in subsection 3.1, which curates multisource, multi-lingual, and tool-grounded trajectories through verifiability-aware filtering, we propose unified training architecture tailored for agentic crafting. This pipeline comprises three synergistic stages: agentic continual pre-training (CPT) (subsubsection 3.2.1), two-stage supervised fine-tuning (SFT) (subsubsection 3.2.2), and reinforcement learning algorithm for agentic (subsubsection 3.2.4). We first employ CPT to instill broad foundational capabilities by exposing the base LLM to curriculum of complex software engineering tasks. Subsequently, we replace conventional single-step SFT with dedicated two-stage procedure to bootstrap basic interaction patterns and consolidate executable and context-consistent behaviors. Critically, both stages incorporate reformulated SFT objective that mitigates gradient noise from execution failures and inefficient learning. Finally, we apply InteractionPerceptive Agentic Policy Optimization (IPA) in the RL stage, which refines training and sampling of REINFORCE at the semantic interaction chunk level toward long-horizon success. Together, these stages form coherent pipeline as shown in Figure 7. 3.2.1 Continuous Pre-training Develops the Agentic Basic Behaviors We introduce an agentic continual pre-training (CPT) phase that precedes subsequent post-training (e.g., SFT and RLHF). CPT systematically equips the LLM with foundational agentic capabilities, including code understanding, task decomposition, tool use, and multi-step reasoning. Technically, this phase exposes the model to large-scale, structured software engineering tasks and high-quality behavioral trajectories via two-stage curriculum that progressively increases data complexity and context length. Stage I: Mastery of Atomic Tasks. First, we train the pretrained model on approximately 500B tokens of diverse, structured data to establish coding and reasoning capabilities. The dataset consists of: Structured Code Task Data: Real-world software engineering tasks, including bug localization, code repair, and unit test generation, constructed from high-quality Issue, i.e., PR pairs in opensource repositories. To enhance reasoning fidelity, we augment these examples with synthesized chain-of-thought (CoT) rationales that model step-by-step decision-making processes. We also simulate iterative development through multi-round feedback loops, derived from PR comments 16 and commit histories, allowing the model to learn how to respond to incremental feedback, critical skill for robust agent behavior (see subsubsection 3.1.2 for full construction details). General Text with Reasoning and Tool-Use Signals: broad collection of general-domain data, including mathematical reasoning problems, logic puzzles, and natural language demonstrations of tool use. While smaller in proportion, this component helps generalize the models reasoning mechanisms beyond code-specific contexts and strengthens its cross-domain generalization. The training loss follows the next-token prediction objective, with global batch size of 32M tokens and constant learning rate of 3 105. This stage aligns ROMEs representations with fundamental code semantics and agentic interactive behaviors, e.g., recognizing when to use tools or localize faults, laying solid foundation for complex task planning and iterative, feedback-driven execution. Stage II: Emergence of Agentic Solver. After Stage I, Stage II fosters the emergence of the agentic solver: the ability to form intentions, maintain goals over time, and efficiently explore high-dimensional decision spaces through interaction and environmental feedback. Here, the model is trained on approximately 300B tokens of synthesized behavioral trajectories, generated by strong teacher models (e.g., Qwen3-Coder480B-A35B-Instruct, Claude) interacting with sandbox environments (e.g., file systems, web shopping simulators) under controlled cues. By including both successful executions and corrected failure paths, we improve the models ability to recover from errors and adapt its strategy during execution. This stage enables the LLM to develop more sophisticated understanding of complex decision spaces and long-horizon planning strategies. We keep the training hyperparameters consistent with Stage I, except that we linearly anneal the weight decay from 0.1 to 0.01 to improve performance. 3.2.2 Anchoring Reinforcement Learning in Reliable Policy Regions via Supervised Fine-Tuning After continual pretraining, to better align the models agentic behavior before RL and enhance the models multi-turn interaction capability. We replace naive supervised fine-tuning (SFT), which is commonly used for single step reasoning LLMs, with our two-stage SFT, i.e., Stage 1: Naive SFT with heuristic-guided data filtering and Stage 2: Adaptive valuable data revisiting. Beyond structural improvements to the training pipeline, we reformulate the SFT objective to address two key challenges in agentic tasks: gradient noise and inefficient sample utilization caused by frequent execution failures and dynamic context shifts. We present the revised SFT procedure as follows. Introduction of Training Stages. In naive SFT, the composition of the training data, especially the relative proportions of different data types, plays decisive role in shaping an agents downstream capabilities. To build high-quality SFT dataset tailored for agentic reasoning, we conduct systematic ablation study to quantify how different data categories affect model behavior. This analysis yields the following empirical insights: Empirical Insights for Naive SFT 1. overthinking samplesthose containing verbose, redundant, or self-contradictory reasoning tracesdegrade task efficiency and impair tool-use proficiency. 2. High-quality programming examples, particularly in Python, substantially enhance the models cross-domain generalization ability. 3. Pure reasoning data without grounded tool interactions tends to encourage redundant or repetitive tool invocations during execution. 4. non-negligible fraction of expert demonstrations are fake positives: they pass tests yet contain logical or semantic errors, posing significant risk of reinforcing incorrect behaviors. 5. Multilingual data preserves reasoning consistency without degrading tool-use performance. To equip the model with robust instruction-following capabilities and foundational agentic behavior patterns, we curated high-quality, million-scale SFT dataset through principled data selection guided by the above empirical insights. The dataset comprises three components: (i) 70% agentic task data (e.g., endto-end software development, API orchestration, and multi-tool workflows), (ii) 15% reasoning-intensive data (e.g., mathematical problem solving, algorithmic coding, and scientific reasoning), and (iii) 15% general-purpose instructions (e.g., summarization, creative writing, and open-domain dialogue). The corpus spans approximately 15 languages and emphasizes programming languages prevalent in real-world usageparticularly Python, Java, C++, and Go. All samples are synthesized via distillation from an ensemble of expert models, followed by rigorous quality control. 17 Guided by our finding that excessively verbose chain-of-thought traces degrade execution efficiency in software tasks, we explicitly exclude overthinking samples during curation. Furthermore, we apply multi-stage filtering pipeline to all expert-sampled trajectories, which: â¶ removes redundant or repetitive tool-call sequences; â· discards truncated or incomplete interactions; â¸ filters out trajectories trapped in self-repair loops; â¹ flags fake positive responsesoutputs that pass superficial checks but contain logical errors; âº ranks remaining trajectories using LLM-as-Judge system for final quality-based selection. This protocol ensures that the SFT dataset is not only diverse and scalable but also aligned with the behavioral priors required for stable downstream reinforcement learning. Notably, while naive SFT successfully elicits basic multi-turn tool invocation patterns, it remains insufficient for mastering the diverse logic structures and complex state transitions inherent in agentic tasks. Consequently, dedicated refinement stage is essential to bridge the gap between initial behavior acquisition and robust reinforcement learning. To address this, and given the scarcity of high-quality agentic demonstrations, we introduce secondstage adaptive valuable data revisiting phase following the initial training. This stage revisits and distills curated subset of high-confidence trajectories, applying stricter quality control to eliminate ambiguous or suboptimal behaviors. The resulting supervision signals are not only more reliable but also better aligned with the credit assignment requirements of downstream RL, thereby establishing stable foundation for policy optimization. Compared to Stage 1, which prioritizes broad coverage across task domains, Stage 2 emphasizes verifiability, style consistency, and reproducibility to align the SFT policy with the structural demands of reinforcement learning. Specifically, we curate data from three high-fidelity sources: 1. Verified interaction trajectories: Executable traces from software development and toolaugmented tasks, where solutions must pass unit tests or be validated through replayable execution to ensure closed-loop consistency with the real working flow. 2. Expert-audited demonstrations: Trajectories annotated or reviewed by senior engineers, focusing on core agentic competencies, including debugging strategies, failure recovery, tool selection and invocation conventions, and minimal-change principles. 3. Preference-refined samples: For each task, multiple candidate trajectories are generated, then ranked via soft scoring mechanism combining rule-based constraints (e.g., syntactic validity, loop detection) and reward-model evaluations, i.e., LLM-as-Judge. Low-quality candidates, e.g., those with redundant tool calls, repair loops, invalid formatting, or log-inconsistent execution, are filtered out through reject sampling. This hierarchical quality-control system, integrating hard constraints (executability and verifiability) and soft scoring (efficiency, strategic coherence), shifts the data distribution toward regions of policy space that are both executable and outcome-sensitive. As result, Stage 2 yields supervision signal that closely approximates the optimization landscape of downstream RL, thereby improving alignment between agentic workflows and decision boundaries before policy refinement begins. Error-Masked Training Enhances Training Stability. In agentic software development, long-horizon interactions are prone to tool-call errors (e.g., type mismatches) and execution failures (e.g., timeouts, syntax errors). Critically, standard SFT treats all tokens equallypropagating gradients through erroneous turns and inadvertently reinforcing failure-prone behaviors. Therefore, we propose error-masked training, novel loss objective that leverages real-time execution feedback logs to dynamically suppress loss signals from failed interactions. Specifically, for any turn that triggers an error during tool execution, we zero out the corresponding token-level losses in the SFT objective. This ensures that gradient updates are driven exclusively by executable and semantically valid trajectories, thereby increasing the signal-to-noise ratio of supervision and preventing the policy from overfitting to common failure modes. Task-Aware Context Masking Ensures Training Efficiency. While error masking addresses executionlevel noise, complementary challenge arises from context misalignment across heterogeneous subtasks within unified software-engineering workflowsuch as dynamic context compression, tool-emulation, and loop detection. Although these subtasks are logically dependent on the main task, their training contexts are often artificially altered through summarization, truncation, or rule-based pruning (e.g., discarding intermediate tool outputs). This distorts the contextual distribution seen during multi-turn SFT, causing the model to learn inconsistent or brittle alignment behaviors when switching between tasks. To resolve this, we introduce task-aware context masking: dynamic supervision strategy that identifies task-specific decision boundaries and selectively retains only the context turns directly relevant to the current subtask. Leveraging pattern-based heuristics (e.g., tool-call triggers, loop-entry markers), we 18 mask loss gradients for redundant, highly similar, or pruned historical turns. Consequently, the model focuses its learning signal exclusively on causally influential interactions, improving sample efficiency while ensuring its behavior remains faithful to real-world software development workflowswhere agents operate on concise, task-adapted contexts rather than raw, unfiltered histories. Loss Formulation of the Whole SFT Training Objective. Given multi-turn agentic trajectory = {(sk, ck)}K k=1, where sk denotes the dialogue state (including interaction history and tool outputs) prior to turn k, and ck is the models response at turn k, we optimize dynamically masked maximum likelihood objective: LSFT(Î¸) = 1 k=1 mk ck + Ïµ k=1 mk log Ï€Î¸ (ck sk) , (1) where ck is the token length of turn k, Ïµ > 0 is small constant for numerical stability, and mk {0, 1} is interaction level mask that selectively enables gradient flow. The mask mk factorizes into two orthogonal componentsreflecting our dual desiderata of execution correctness and task relevance: mk = merr mtask , merr = 1(cid:2)Err(k)(cid:3), mtask = 1(cid:2)Rel(k)(cid:3), (2) where Err(k) indicates whether turn triggers tool-call or execution failure (as recorded in runtime logs), and Rel(k) denotes whether the turn contains context deemed relevant to the current subtask under task-specific heuristics (e.g., proximity to tool invocation or loop entry). Only turns that are both error-free and task-relevant contribute to the loss, ensuring that supervision signals are grounded in executable behaviors and aligned with functional decision boundaries. 3.2.3 Prepare Training Instance for Reinforcement Learning To support efficient and stable agentic reinforcement learning, we curate collection of high-quality RL instances with verifiable execution outcomes and sufficient task complexity and difficulty. These instances are mainly from two sources, approximately 60K high-quality candidate RL instances in total: 1. Uniformly sampled instances from synthesized instances, each rigorously human-annotated to ensure correctness. 2. Expert instances designed to reflect challenging, long-horizon agentic behaviors encountered in real-world software engineering scenarios. To facilitate efficient learning, we select instances from the candidate pool based on task difficulty, which is estimated by computing pass rates using multiple strong open-source baseline models and our SFT model. Based on these estimates, we retain approximately 2K instances with moderate difficulty. Notably, to ensure reward reliability, we filter out instances affected by non-deterministic or unstable environments (e.g., tasks involving external services subject to rate limits or IP blocking), as well as instances with misaligned specifications between task descriptions and test cases. Finally, test files are uploaded only at the evaluation stage and are never exposed during generation, preventing information leakage and test-aware behaviors. Collectively, these procedures result in compact, reliable, and execution-grounded RL instance set that provides stable learning signals for agentic RL. 3.2.4 Towards Efficient and Scalable Agentic Reinforcement Learning After revisiting the existing RLVR methods, we find that: while recent RLVR methods have demonstrated success in single-turn reasoning tasks, they might exhibit fundamental limitations in long-tail multiturn agentic settings: (i) unstable policy updates; (ii) inefficient temporally credit assignment over long trajectories; and (iii) low-efficiency trajectory sampling. These issues may dramatically increase both computational cost and the risk of policy degradation. To address these challenges, we first construct REINFORCE variant as the starting point for algorithm refinement (3.2.4.1). Building upon this baseline, we propose Interaction-Perceptive Agentic Policy Optimization (IPA)a novel RL algorithm tailored for agents engaged in dense tool usage and environmental interaction loops. The core insight of our method is to recognize and exploit the interaction chunk: structured segment of consecutive agent-environment communication that collectively contributes to high-level subgoal by calling the tool at the end (3.2.4.2). By treating interaction chunks, not individual tokens or full trajectories, as the fundamental unit of policy optimization, we redefine the gradient computation formulation to achieve efficient credit assignment and stable training(3.2.4.3). Then, we propose novel sampling strategy to reduce low-quality trajectory rollout and improve the 19 Figure 8: Overview of the Proposed Interaction-Perceptive Agentic Policy Optimization (IPA) training pipeline. sample efficiency(3.2.4.4). An overview of our framework, including its key components and data flow, is depicted in Figure 8. 3.2.4.1 Specialized off-policy baseline for industrial agentic RL REINFORCE as powerful baseline. To find suitable naive RL algorithm as the baseline for training an agentic model. We conducted an in-depth analysis of mainstream algorithms and found that: Unlike PPO style methods (Schulman et al., 2017), REINFORCE (Sutton et al., 1999) models the entire training process as bandit problem by using sequence-level rewards, making it suitable for language reasoning scenarios (Ahmadian et al., 2024). Moreover, its simplicity, requiring no value function approximation or importance sampling clipping, makes it clean, minimally biased starting point for building our agentic RL baseline. Formally, the gradient calculation of REINFORCE is: which fully utilizes the log-derivative of every token in trajectory Ï„. JREINFORCE(Ï€) = EÏ„Ï€ [R(Ï„) log Ï€(Ï„)] , Adapt REINFORCE to the off-policy training. Our empirical studies reveal that while REINFORCE is effective in single-turn reasoning tasks, its performance degrades in industrial-scale asynchronous agentic training. key bottleneck arises from the widespread use of off-policy learning in such settings to improve data efficiency and throughput (Lu et al., 2025). However, due to high off-policy ratio, the megatron old policy Ï€ that conforming to the old data distribution becomes increasingly outdated relative Î¸old to the current policy Ï€megatron (Megatron denotes the Megatron-LM (Shoeybi et al., 2019) training engine. Notably, to avoid confusion, mismatches caused by inference and training engines are not taken into account here). This growing distributional shift makes policy training with data sampled by different strategy, resulting in biased optimization objective. To correct the learning objective, Importance Sampling (IS) is introduced (Schulman et al., 2017). However, naive IS may produce high-variance gradient estimates and unstable policy updates. To make training stable, an efficient mitigation approach is to employ Truncated Importance Sampling (TIS) to weight its update based on policy differences (Munos et al., 2016). To further make the IS ratio robust to low-probability tokens, we replace the continued multiplication style TIS calculation with geometric mean(Zheng et al., 2025b; Zhao et al., 2025): Î¸ JRL(Ï€) = Ï„Âµ SGLang Î¸old [[Ï(Ï„)]1 0 (cid:124) (cid:123)(cid:122) (cid:125) TIS R(Ï„) log Ï€megatron Î¸ (Ï„)], Ï(Ï„) = (cid:0) tÏ„ Ï€megatron Î¸ megatron Ï€ Î¸old (Ï„t Ï„<t) (Ï„t Ï„<t) (cid:1) 1 Ï„ where ÂµSGLang high-throughput serving system akin to vLLM (Kwon et al., 2023) and RTP-LLM (Alibaba, 2025). denotes the inference policy executed via the SGLang inference engine (Zheng et al., 2024), Î¸old However, TIS employs uniform clipping strategy that treats positive and negative samples identically, failing to account for their distinct roles in policy improvement, mysteriously limiting data efficiency (Roux et al., 2025). To address this, we follow the approach of TOPR (Roux et al., 2025) and apply TIS only to negative samples, which are more likely to interfere with the policy. This avoids suffering the gradients of positive samples and achieves efficient and stable policy optimization. Thus, the gradient 20 calculation can be: JRL(Ï€) = Ï„T + (cid:124) ÂµSGLang Î¸old (Ï„)R(Ï„) log Ï€megatron Î¸ (cid:123)(cid:122) Weighted SL update for positive examples (Ï„) (cid:125) + Ï„T (cid:124) ÂµSGLang Î¸old (Ï„) [Ï(Ï„)]1 0 R(Ï„) log Ï€megatron (cid:123)(cid:122) Clipped IS update for negative examples Î¸ (Ï„) , (cid:125) (3) where + and denote sets of positive and non-positive trajectories, respectively. Such an objective combines the Supervised Learning (SL) update (weighted by return) for accelerating learning on positive examples, and TIS update for negative samples, allowing for their handling without brittleness, avoiding the uncontroled sample distribution shift caused by large-scale negative samples in agentic sampling, that is, the probability being squeezed onto large number of useless tokens, leading to policy collapse. In addition to the aforementioned training instability, industrialHandle the inference-training mismatch. scale RL systems impose stringent requirements on training stability and rollout throughput, which often lead to architectural divergence between the training and inference engines. Specifically, highperformance inference servers (e.g., SGLang) and large-scale training frameworks (e.g., Megatron-LM) employ different execution backends, quantization strategies, or batching mechanisms. As result, the inference policy that generates rollouts, denoted ÂµSGLang , systematically differs from the training policy Ï€Megatron , even when they share the same parameters. The problem is agnostic to the underlying engine Î¸old and instead arises from the dominant training paradigm commonly adopted in agentic model building. Such mismatch secretly increases the unstable training risk. Recently, many works have proposed optimization methods (Zheng et al., 2025b; Yao et al., 2025; Gao et al., 2025a) from the algorithmic level to overcome this challenge. Among them, widely used mismatch measurement directly quantifies the Î¸old Ï€ megatron Î¸old SGLang Âµ Î¸old (Ï„k) (Ï„k) , where Ï„k gap between inference policy and training policy via the token-level different ratio: denotes the k-th token in sequence. Intuitively, we mask out tokens for which the importance weight exceeds the threshold H, i.e., those exhibiting severe distributional shift (Zheng et al., 2025a). Specifically, we define binary loss mask: mk = , and exclude masked-out tokens (mk = 0) (cid:32) Ï€ megatron Î¸old SGLang Âµ Î¸old (Ï„tÏ„<t) (Ï„tÏ„<t) (cid:33) from gradient updates to ensure training stability. Notably, mk denotes token-level masking. Finally, the gradient calculation of our baseline with token level mismatch masking is formalized as: JRL(Ï€) = Ï„T + (cid:124) ÂµSGLang Î¸old (Ï„)R(Ï„) mk log Ï€megatron Î¸ (Ï„k Ï„<k) (cid:123)(cid:122) Weighted SL update with token-level masking (cid:125) Ï„ k=1 ÂµSGLang Î¸old + Ï„T (cid:124) 0 R(Ï„) (Ï„) [Ï(Ï„)]1 Ï„ k=1 (cid:123)(cid:122) Clipped IS update with token-level masking mk log Ï€megatron Î¸ (Ï„k Ï„<k) . (4) (cid:125) Dynamic trajectory filtering for data refinement. Beyond algorithmic design, we emphasize that data filtering is critical for stable post-training in tool-augmented environments. Empirical analysis reveals that the dominant sources of harmful trajectories stem from environmental noise, including transient API failures, non-deterministic tool responses, and repeated illegal tool invocations. When such trajectories are used, particularly if high-magnitude rewards are spuriously assigned to tokens arising from noisy or invalid interactions, they inject misleading gradient signals that can trigger catastrophic policy collapse. To address this, our RL pipeline incorporates dynamic trajectory filtering during data collection, which explicitly discards trajectories whose rewards are deemed unreliable. Specifically, trajectory Ï„ is rejected if it exhibits any of the aforementioned failure modes. Critically, to ensure stable batch construction and prevent training interruptions due to insufficient valid samples, we employ on-the-fly resampling: whenever rollout is filtered out, the agent immediately initiates new continuation from the same initial state using the current policy Ï€Î¸, with the aim of generating higher-quality trajectory. In conclusion, REINFORCE combined with the above-mentioned techniques achieves relatively effective optimization of the model under the agentic RL setting. We take such REINFORCE variant as the improvement frontier of our final IPA. 21 Figure 9: Comparison of importance sampling strategies across token-level, chunk-level, and sentencelevel granularities, where chunk-level aligns with the natural granularity of interactions. 3.2.4.2 Modeling Multi-Turn Agentic Task as Chunked MDP In 3.2.4.1, we established robust REINFORCE variant as the foundational baseline for agentic reinforcement learning. Building on this, the present section introduces modeling framework specifically tailored to the challenges of multi-turn agentic interaction, where sparse rewards, long horizons, and tool-mediated reasoning demand more structured credit assignment and stable policy updates. This formulation serves as the basis for series of subsequent baseline enhancements, paving the way for scalable and reliable RL in complex interactive environments. Crucially, our MDP operates at the level of interaction chunks, rather than tokens (Yu et al., 2025) or sentences (Team et al., 2025), to align the horizon with the causal structure of agentenvironment interaction naturally provided by multi-turn tool-integrated reasoning. Formally, given token trajectory Ï„[1:T], we partition it into sequence of chunks {c1, c2, . . . , cK}, T. Each chunk ck spans from one environmental interaction to the next and corresponds to complete functional unittypically culminating in tool invocation (e.g., reason format API call trigger execution). Chunk level modeling mitigates mismatches in finer-grained formulations: 1. Token-level action creates mismatch between decision granularity and external environmental transition dynamics: the vast majority of tokens have no external effect. 2. Sentence-level segmentation, though coarser, remains misaligned with agency semantics. In practice, single tool invocation often requires the model to generate multiple consecutive sentences, e.g., first articulating intent, then constructing parameters, before finally outputting the executable call. Only the final sentence triggers an environmental change. Based on chunk level segmentation, our Chunked MDP can be defined by the tuple (S, C, P, R, Î³). denotes the state space, where each state sk encodes the complete interaction history up to the start of chunk ck, including prior tool calls, generation and environmental feedback. represents the chunk-action space: each action is variable-length token sequence generated by the agent in response to s, culminating in either tool invocation or task completion. defines the transition dynamics influenced by c, governed by the LLMs generative process and the stochastic responses of external tools. is sparse reward function that only provides positive feedback when the trajectory has passed all unit tests. Î³ (0, 1] is the discount factor, applied at the chunk level to prioritize temporally proximal, outcome-influencing decisions. Overall, Chunked MDP aggregates those tokens that collectively lead to an environmental transition, aligns the optimization horizon with meaningful interventions, and enables accurate credit assignment. 3.2.4.3 Reconstruct Training Objective via Chunk-Level Optimization To align with the Chunked MDP, IPA adjusts the optimization horizon of the constructed baseline to the chunk level by incorporating return calculation, importance sampling, and mismatch masking. Intuitively, these refinements intermediate granularity strikes favorable balance: it is coarse enough to ensure training efficiency and semantic consistency within each chunk, yet fine-grained enough to enable precise credit assignment across multi-turn reasoning. 22 Figure 10: Comparison of Chunk-Level Optimization and baseline on mini-set of the training data. Left: Unclipped gradient norm for updates that reflects the stability of training. Our Chunk-Level Optimization exhibits more stable gradient norms, while baseline induces anomalous gradient fluctuations. Middle: Performance on training tasks. Owing to stable gradient updates and effective credit assignment, ChunkLevel Optimization consistently shows better performance than baseline. Right: Test-time success rate on validation tasks. Chunk-Level Optimization retain its superiority over baseline, demonstrating the generalization of our method. First, we introduce Discounted Chunk-Level Return, which re-establishes temporal credit assignment in agentic reinforcement learning. key limitation of conventional token-level formulation is its inability to incorporate meaningful temporal discounting (Wang et al., 2025b), since applying reward discount factor Î³ < 1 over thousands of tokens would cause reward signals to vanish exponentially (Yue et al., 2025). Moreover, without temporal structure, value estimates for early states suffer from high variance in long tail trajectories (Yin et al., 2024; Amit et al., 2020). In contrast, the Chunked MDP formulation discretizes trajectories at the semantic action boundary, which enables the principled reintroduction of temporal discounting at the chunk level. Formally, given trajectory partitioned into chunks, the return assigned to chunk ck is defined as: (5) where (j, k) denotes the number of chunks between ck and cj, and Rfinal is the terminal task reward. All tokens within chunk ck share the same scalar weight Gk in the policy gradient. Notably, this reward calculation can be compatible with intrinsic reward systems. Gk = Î³ (j,k) Rfinal, This design yields two crucial benefits. First, by aligning discounting with semantic decision intervals, it mitigates the bias-variance trade-off in long-horizon credit assignment: early chunks are downweighted not arbitrarily, but proportionally to their temporal distance from outcome-determining actions, thereby reducing noise propagation while preserving signal integrity. Second, it avoids the exponential signal decay inherent in token-level discounting, since Ttokens, the effective horizon is drastically shortened, ensuring stable gradient magnitudes even in multi-thousand-token trajectories. Consequently, the 1), while early ineffective policy receives stronger gradients for chunks proximate to task success (Î³ attempts, e.g., invalid tool calls, are exponentially suppressed. This not only accelerates convergence on high-impact behaviors but also induces an implicit trajectory compression effect, significantly improving sample efficiency and training stability. Empirically, the results in Figure 10 (Left) indicate that incorporating chunk-level discounted returns into the gradient computation of our baseline enhances training stability, accelerates the perception and learning of high-level action semantics embedded in chunks, and significantly improves the models optimization efficiency (Figure 10 (Middle)). This, in turn, leads to improved performance on difficult tasks (Figure 10 (Right)). Moreover, we propose Chunk-Level Importance Sampling to synergize with the chunk-level return as suggested in Zheng et al. (2025b). Specifically, for each interaction chunk c, we calculate the importance sampling ratio over all tokens within the chunk to measure the chunk level difference. Notably, because chunk-level calculation expands its calculation horizon compared to token-level ratios, we use the geometric mean style IS to dampen the impact of outlier tokens and avoid extreme ratios: Ïc(c) = (cid:18) tc Ï€megatron Î¸ megatron Ï€ Î¸old (Ï„t Ï„<t) (Ï„t Ï„<t) (cid:19) 1 . (6) Finally, to align all the optimization scales with the chunked MDP, we finally elevate loss masking from H(cid:1). Intuitively, Chunk-level the token to the interaction chunk level: mc = I(cid:0)(tc (Ï„tÏ„<t) ) Ï€ 1 megatron Î¸old SGLang Âµ Î¸old (Ï„tÏ„<t) masking may simultaneously mitigate two critical issues that arise at the token level (Liu et al., 2025): 23 Figure 11: Illustration of the Chunk-Level Initialized Resampling Strategy (Sequential Rollback). Left: In challenging tasks, sampling high-quality trajectories from the beginning is difficult, severely limiting policy learning efficiency. Right: Sequential Rollback sampling strategy initiates rollouts from critical chunks, dramatically reducing the exploration burden and enabling the policy to rapidly acquire the key skills embedded in these crucial chunks. By progressively rolling back along the crucial chunks, it enables chunk-level curriculum learning for model to finally solve these challenging tasks. 1. State Occupancy Mismatch: token-level policy gradients are computed over state distributions induced by the inference policy, which diverges from the true state visitation. 2. Mismatched Reward Signal: fine-grained token-wise importance weights are misaligned with the coarse, outcome-driven rewards that govern long-horizon agentic success. Empirical experience also shows that the constraint of mask is relaxed by extending to chunk horizon, so as to avoid excessive influence on RL gradient and maintain training stability. Combining chunk-level masking, discounted returns and importance sampling, the gradient calculation of our REINFORCE variant can be reformulated as: JChunk-RL(Ï€) = cT + (cid:124) k=1 ÂµSGLang Î¸old (c)Gc mc log Ï€megatron Î¸ (ck Ï„<ck ) (cid:123)(cid:122) Chunk-level weighted SL update (cid:125) ÂµSGLang Î¸old + cT (cid:124) (c) [Ïc(c)]1 0 Gc k=1 (cid:123)(cid:122) Chunk-level clipped IS update Î¸ mc log Ï€megatron (ck Ï„<ck ) , (7) (cid:125) where denotes the k-th chunk in Ï„, Gc is the discounted return of chunk (as defined in Equation 5). 3.2.4.4 Rollout Paradigm Refinement via Chunk-Level Initialized Sampling As agentic reasoning evolves from single-turn inference to multi-turn interactions, we observe that the probability of sampling positive trajectory markedly decays on several complex tasks. After analyzing these failed trajectories, we find that the success rate of these long-horizon agentic tasks is typically governed by sparse set of crucial forksdecision points where the models next chunk disproportionately affects the final return (e.g., selecting the right tool or correctly parsing pivotal observation). When sampling from the initial state, an incorrect decision chunk at any crucial fork will possibly cause the failure of the entire task. Therefore, under naive sampling strategy, rollouts on these tasks always contain extremely sparse positive signals, resulting in inefficient or misleading policy updates (Yu et al., 2025). simple but exciting insight is that, if we can prefill the interaction history with the correct expert-like chunks and resample the subsequent trajectories, we can effectively reduce task difficulty and enrich the reward signals for optimization. Once the model has learned the tail part chunks, we roll back to the head part crucial forks, enabling chunk-level curriculum learning on these challenging tasks. Specifically, IPA introduces Chunk-Level Initialized Resampling, which enables the policy to launch rollouts from selected forks by initializing tasks with chunks of expert-like trajectories, e.g., obtained either via self-sampling or from teacher model. Notably, we periodically update the expert trajectory under the current policy to maximize coverage of critical chunks while minimizing interference from 24 Figure 12: Performance of Sequential Rollback and baseline (naive sampling) on challenging training task. Left: Average success rate during training, which reflects the percentage of positive signals in training batch. Sequential Rollback obviously brings more valuable rollouts compared to baseline (all failures). The drop of success rate indicates that the model has rolled back across crucial chunk to the crucial fork. Middle: Expert chunks used during training, which visually displays the progress of rolling back along the expert trajectory. Right: Average success rate on the challenging task during testing. In test-time, all trajectories are sampled from the initial state. The gap between two curves after step 75 indicates that sequential rollback enables effective learning on extremely hard tasks. unnecessary ones. Formally, given an expert-like trajectory with chunks Ï„ = (c K) and , we interact with the environment using Ï„ an selected expert chunk and then resample the subsequent chunks Ï„ck with the train policy Ï€Î¸. The expected success rate of resampling trajectories on Ï„ as crucial chunk if the expected resampling success rate on Ï„ . The drop in success rate indicates 1 that the decisions made within skills. Therefore, the state right before are decisive for success, and the current policy does not master such is significantly lower than on Ï„ , Ï„ck ). We then define chunk is defined as EÏ„Ï€Î¸ inal(Ï„ 2, . . . , is naturally crucial fork. 1, c c k1 k1 k1 Empirically, naive yet effective strategy to select the resampling initialization state is Sequential Rollback: starting from the last chunk of an expert trajectory and moving regressively toward the beginning. As shown in Figure 11, sampling from states near the end of successful trajectory requires far fewer rollout turns, dramatically reducing the exploration burden compared to starting from the initial state. Consequently, positive samples are much easier to obtain from these tail states, enabling reliable generation of high-quality rollouts and rapid learning correct behaviors on these crucial forks. The results in Figure 12 (left) demonstrate that Sequential Rollback can keenly monitor the important forks on expert trajectory, and gradually master the global crucial chunks through progressive learning (Figure 12 (middle)), so that the policy can obtain excellent test performance on the difficult task (Figure 12 (right)). However, the aforementioned Sequential Rollback, while effective at preserving high-value reasoning pathways, suffers from significant computational inefficiency. Crucially, if the decisive interaction occurs early in the trajectory, backward scanning only discovers it after exhaustively testing all later positions, leading to wasted rollouts and poor scalability across diverse task structures. To enable robust and efficient detection of critical modules across wide range of tasks, we propose Parallelized Initialization scheme as practical and reliable compromise. Specifically, given an expert-like trajectory, we first select set of anchor chunks at various positions (uniformly or randomly), aiming to include crucial forks between these anchors. Then IPA initializes environments to the state asociate with the anchor chunks and launches several independent rollouts in parallel. Parallelized Initialization introduces trajectories rolled out from diverse starting states within single rollout batch. Although this dilutes the number of samples drawn at each potential crucial fork, it avoids the time cost on bad-cases of Sequential Rollback and ultimately achieves higher efficiency on our dataset. Finally, even with Parallelized Initialization, there exist extreme cases where no positive trajectories are sampled from crucial fork. In such scenarios, purely on-policy or importance-sampled updates yield zero gradient signal, stalling learning and risking irreversible policy collapse. To accelerate convergence and safeguard against degradation, we adopt hybrid training objective that seamlessly integrates imitation learning (IL) and reinforcement learning (RL). Specifically, to avoid missing positive signals at any crucial fork, we introduce the imitation learning target to the experts chunks Ï„ as fallback. This injects recovery signal that anchors the policy in high-quality regions of the behavior space, preventing drift into degenerate modes. Formally, our mixed objective operates in two phases: 25 Figure 13: Comparison of IPA with & without Chunk-Level Initialized Resampling (Parallelized Initialization) on mini-set of the training data. Left: Average success rate on training tasks. The gap between curves in the early stage of training shows that the Chunk-Level Initialized Resampling brings much more diverse reward signals in training batches. Middle: Minimum success rate across train-tasks with test-time setting (sampled from beginning). With Chunk-Level Initialized Resampling, IPA enables the train model to solve extremely hard tasks by learning in chunk-level curriculum-like manner. Right: Average success rate at test-time. Benefiting from more valuable rollouts, Parallelized Initialization substantially improves the performance of IPA. 1. For the prefilled expert chunks Ï„ f 1 and expert cruical chunk , we apply imitation learning style loss. This rapidly instills reliable subroutines, e.g., tool invocation formatting. 2. For the resampled chunks Ï„c , we use the chunk-level RL (Equation 7), enabling adaptive credit assignment on outcome-determining interactions. The final training loss of IPA is thus: LIPA = Î»IL Ï„ c (cid:124) Ï€ megatron Î¸ (c )Gc log Ï€ megatron Î¸ (c Ï„ k ) +Î»RL cÏ„c Chunk-RL. (8) (cid:123)(cid:122) Imitation learning style update (cid:125) The coefficients Î»IL, Î»RL balance imitation and exploration. The results in Figure 13 demonstrate that IPA effectively enhances the models generalization ability on challenging agentic tasks, enabling it to overcome performance limits and significantly improve learning efficiency. Based on our IPA, we effectively unlock the agentic capabilities of ROME, 30B MoE model, allowing it to overcome the performance bottleneck associated with its inherent size and achieve capabilities comparable to those of larger models, such as the 480B agentic model. 3.3 Experiments and Benchmark 3.3.1 Evaluation Setup To rigorously and holistically evaluate agentic intelligence, we adopt three-dimensional evaluation framework encompassing tool-use abilities, general agentic capabilities, and terminal-based agentic execution. These dimensions reflect the core competencies required for real-world agent deployment, ranging from tool calling to long-horizon, environment-grounded task completion. Tool-use Abilities. We evaluate tool-use abilities by assessing whether agents can correctly select, invoke, and coordinate external tools in response to user intents. This dimension is evaluated using established tool-use benchmarks, including domain-specific subsets of TAU2-Bench (Retail, Airline, Telecom) (Barres et al., 2025), BFCL-V3 (Patil et al.), and MTU-Bench (Wang et al., 2024). General Agentic Capabilities. To evaluate an agents ability to solve complex queries through multistep reasoning and high-level decision-making, we assess general agentic capabilities on diverse suite of benchmarks, including BrowseComp-ZH (Zhou et al., 2025), ShopAgent (Pei et al., 2025), and GAIA (Mialon et al., 2023). Terminal-Based Agentic Execution. We further assess terminal-based agentic execution using benchmarks that require agents to complete goal-directed workflows within executable environments. 26 Specifically, we evaluate on Terminal-Bench 1.0 (Team, 2025), Terminal-Bench 2.0, SWE-bench Verified (Jimenez et al., 2024), SWE-Bench Multilingual (Yang et al., 2025), as well as our extended benchmark. Together, these three dimensions represent demanding and practically significant dimensions of agent evaluation, serving as the primary yardsticks for assessing real-world deployability of agentic models. Meanwhile, we observe that the aforementioned publicly available datasets exhibit notable limitations in scale, domain balance, difficulty calibration, and contamination control. To further enrich the agentic evaluation ecosystem, we introduce Terminal-Bench Pro, rigorously curated benchmark designed to offer larger-scale coverage, balanced task domains, calibrated difficulty levels, and stronger safeguards against data leakage. Full details of its construction and corresponding evaluation results are provided in Section 3.3.2. All models are evaluated using consistent set of generation hyperparameters to ensure fair comparison. Specifically, we configure the models with temperature = 0.7, top-p = 0.8, and top-k = 20. The maximum output length is restricted to 65,536 tokens and the maximum context length 262,144 tokens. To ensure consistency across terminal-based agentic tasks, all CLI evaluations are conducted under unified execution environment using the iFlow CLI framework. For the evaluation results, we report Pass@1 as the average over 3 independent runs (Avg@3), and we use * to denote scores obtained from official reports or public leaderboards. 3.3.2 Terminal Bench Pro: More Rigorous and Fine-Grained Benchmark for Terminal Agents Figure 14: Benchmark characterization and cross-benchmark comparison of Terminal Bench Pro against other benchmarks. Motivation and Limitations of Existing Benchmarks. Terminal-based benchmarks are increasingly important for evaluating autonomous coding agents, yet existing benchmarks remain limited in scale, reliability, and diagnostic resolution. In terms of scale, Terminal Bench 1.0 contains only 80 tasks, and Terminal Bench 2.0 expands this marginally to 89 tasks, which renders aggregate metrics susceptible to wide confidence intervals and makes overall rankings sensitive to small number of idiosyncratic instances. Moreover, the benchmark reliability can be further compromised by task-specific artifacts. For instance, tasks that are highly sensitive to external network conditions (e.g., downloading content from online platforms) introduce environment-induced variance that is orthogonal to agent capability, thereby reducing reproducibility and complicating attribution of observed performance differences. More critically, existing benchmarks lack sufficient granularity for domain-level analysis. As shown in Fig 14(b), several sub-domains are represented by only one to three tasks (e.g., one task in games and three tasks in machine learning). This sparsity yields unstable sub-domain level estimates, as reflected by the substantial category-wise variance in pass@1 across benchmarks as shown in Fig 14(c). And this undermines the statistical significance and confidence of our evaluation results in these domains. In addition, many existing tasks are overly broad yet relying on sparse test suites, resulting in low test coverage, as depicted by the per-instance test-case statistics in Fig 14(d). Under such conditions, agents may pass evaluations 27 by exploiting underspecified requirements or unintended shortcuts, which undermines the validity of conclusions regarding correctness, robustness, and generalization. Design and Construction. To address these limitations, we propose Terminal Bench Pro, new benchmark designed for rigorous and fine-grained evaluation of terminal-based agents. Its construction follows three core principles: 1. Comprehensive Domain Coverage: Terminal Bench Pro is aligned with real-world user demands. It systematically covers eight major CLI-related domains with balanced number of tasks in each, enabling reliable assessment of fine-grained domain capabilities. 2. Rigorous Data Validation: All tasks undergo multiple rounds of expert validation to ensure high data quality, unambiguous specifications, and comprehensive test coverage. 3. Deterministic Evaluation Environment: Every instance is paired with an executable test file and fully reproducible environment, eliminating non-determinism arising from external network or system dependencies. To ground the benchmark in practical usage, we analyze discussions from GitHub issue forums and identify eight key domains where user queries are predominantly concentrated: data processing, games, debugging, system administration, scientific computing, software engineering, machine learning, and security. For each domain, we engage experts to manually construct tasks based on real-world problem scenarios. All problem descriptions and unit tests are authored from scratch by experienced programmers to ensure originality and minimize the risk of data leakage. Each task then undergoes independent review by multiple experts to eliminate ambiguous instructions and false-positive solutions, ensure optimal reference solutions, and achieve high unit-test coverage. Following this process, we construct Terminal Bench Pro 2 dataset, which consists of 400 evaluation tasks (200 public and 200 private instances) uniformly distributed across the eight domains. As evidenced by Fig. 14, the benchmark exhibits high test coverage, rich task diversity, and low evaluation variance, providing reliable foundation for systematic and trustworthy assessment of terminal-based agentic systems. 3.3.3 Evaluation Results In this section, we present the detailed and fair evaluation results comprehensively under structured agentic evaluation setting, to support the outstanding performance of our agentic model, i.e., ROME, trained by ALE through three test perspectives. To provide an intuitive overview, Figure 15 highlights ROMEs agentic performance advantage under comparable or smaller parameter budgets, surpassing the performance ceiling typically observed in standard-sized models. In the rest of the subsection, we present detailed evaluation results and analyses across individual benchmarks. Figure 15: Performance-parameter trade-offs in agentic tasks. Scores represent averages on general agentic and code agent benchmarks. Models with known parameters are shown as circles, while proprietary models with unknown parameters are depicted as diamonds (right side). Left: Total parameters versus overall performance. Right: Activated parameters versus overall performance. 2https://github.com/alibaba/terminal-bench-pro 28 Table 1: Performance on Terminal-Based Benchmarks (Normal Models). Benchmark Architecture # Total Params # Activated Params Terminal-Bench 1.0 Terminal-Bench 2.0 SWE-Bench Verified SWE-Bench Multilingual Terminal-Bench-Pro-Public Terminal-Bench-Pro-Private Avg. ROME Qwen3-Coder 30B-A3B-Instruct Devstral Small 2 GPT-OSS120B Gemini-2.5 Flash GLM-4.5 Air GPT-5 Mini MoE 30B 3B 41.50 24.72 57.40 40.00 40.50 21.50 37.60 MoE 30B 3B 28.50 13.48 46.33 30.00 26.00 11.33 25.94 Dense 24B - 28.33 18.20 51.87 27.00 32.17 17.00 29.10 MoE 117B 5.1B 31.25 21.12 43.93 34.84 32.00 27.83 31.83 - - - 23.75 16.40* 28.73* 11.50 23.67 15.17 19.87 MoE 106B 12B 30.00 17.30 56.20 38.16 33.00 15.83 31.75 - - - 33.75 20.97 59.30 49.67 34.75 29.50 37.99 Table 2: Performance on Terminal-Based Benchmarks (Large Models). Benchmark Architecture # Total Params # Activated Params Terminal-Bench 1.0 Terminal-Bench 2.0 SWE-Bench Verified SWE-Bench Multilingual Terminal-Bench-Pro-Public Terminal-Bench-Pro-Private Avg. ROME Qwen3-Coder Plus Qwen3-Coder 480B-A35B-Instruct DeepSeek V3.1 GLM4. KimiK2 ClaudeHaiku-4.5 MoE 30B 3B 41.50 24.72 57.40 40.00 40.50 21.50 37.60 MoE - - 39.58 32.36 65.87 54.16 39.67 28.50 43.36 MoE 480B 35B 37.92 26.97 65.20 49.50 38.33 26.50 40.74 MoE 671B 37B 38.75 28.47 62.20 48.16 39.33 28.33 40.87 MoE MoE 1043B 355B 32B 32B 41.25 26.29 62.67 53.84 41.50 29.17 42.45 39.25 30.90 64.80 48.67 40.50 29. 42.19 - - - 47.08 34.83 69.60 60.34 45.83 35.33 48.84 Evaluation on Terminal-Based Benchmarks. We evaluate models on suite of terminal-based agentic benchmarks that emphasize execution robustness, long-horizon multi-turn interaction, and environment grounding. These benchmarks go beyond single-shot code generation and require agents to iteratively reason, invoke tools, recover from execution errors, and maintain state across multiple interaction steps. As shown in Table 1, ROME achieves 41.50% on Terminal-Bench 1.0, 24.72% on Terminal-Bench 2.0, 57.40% on SWE-Bench Verified, and 40.00% on SWE-Bench Multilingual. These results consistently and substantially outperform other normal-sized models, including Qwen3-Coder-30B-A3B-Instruct, Devstral Small 2, and GPT-OSS-120B, across all evaluated benchmarks. Notably, the improvements are not confined to single dataset but persist across benchmarks with varying task distributions, programming languages, and interaction lengths, indicating strong robustness and generalization in agentic behavior. From scaling-efficiency perspective, ROME demonstrates highly favorable performanceparameter trade-off. Despite activating only 3B parameters, it significantly surpasses dense and MoE models with substantially larger total or activated parameter counts. This highlights the effectiveness of ALE in enhancing agentic reasoning and action execution, effectively breaking through the performance ceiling typically observed in normal-sized models. More impressively, As Small-scale model, ROME attains performance that approaches or even exceeds that of multiple large-scale and ultra-large-scale models across several benchmarks. On Terminal-Bench 1.0, ROME (41.50%) surpasses super large-scale models such as Qwen3-Coder-480B-A35B-Instruct (37.92%) and DeepSeek-V3.1 (38.75%), while achieving performance comparable to advanced proprietary systems including Qwen3-Coder-Plus (39.58%) and Kimi-K2 (39.25%), despite operating at substantially smaller scale. Similarly, on the widely adopted SWE-Bench Verified, ROME (57.40%) surpasses or matches leading proprietary models such as GLM-4.5 Air (56.20%), Gemini-2.5 Flash (28.73%), and GPT-OSS-120B (43.93%). This indicates that the benefits of ALE extend beyond terminal-based interaction and generalize to real-world software engineering tasks involving repository understanding, patch generation, and regression validation. Despite these encouraging results, all evaluated agentic models, including ROME and large-scale baselines, exhibit only limited performance on the more challenging Terminal Bench Pro benchmark. This benchmark introduces stricter success criteria, deeper interaction horizons, and more complex environment dynamics, exposing systematic weaknesses such as error compounding, suboptimal recovery strategies, and brittle long-term planning. The uniformly low absolute scores highlight that current agentic LLMs, regardless of scale, remain far from solving realistic, high-difficulty terminal-based tasks. Taken together, these findings underscore both the effectiveness and the limitations of current agentic learning approaches. While ROME significantly improves agentic efficiency and narrows the gap 29 Table 3: Performance on Tool-Use Benchmarks (Normal Models). Benchmark Architecture # Total Params # Activated Params Tau2-Bench Retail Tau2-Bench Airline Tau2-Bench Telecom BFCL-v3 (Multi-Turn) MTU-Bench (Single-Turn) MTU-Bench (Multi-Turn) Avg. ROME Qwen3-Coder 30B-A3B-Instruct Devstral Small 2 GPT-OSS120B Gemini-2.5 Flash GLM-4.5 Air GPT-5 Mini MoE 30B 3B 62.28 50.50 30.92 43.00 62.45 47.63 49.46 MoE 30B 3B 59.87 45.50 30.04 29.75 50.69 29.38 40.87 Dense 24B - 58.33 30.00 20.40 30.12 63.08 34.15 39.35 MoE 117B 5.1B 64.30 53.50 54.61 53.62 54.16 58.61 56.47 - - - 64.30 42.50 16.90 36.25 45.93 57.01 43.82 MoE 106B 12B 74.60 69.00 46.90 66.88 57.74 37. 58.78 - - - 74.12 60.00 73.46 27.25 59.82 55.61 58.38 Table 4: Performance on Tool-Use Benchmarks (Large Models). Benchmark Architecture # Total Params # Activated Params Tau2-Bench Retail Tau2-Bench Airline Tau2-Bench Telecom BFCL-v3 (Multi-Turn) MTU-Bench (Single-Turn) MTU-Bench (Multi-Turn) Avg. ROME Qwen3-Coder Plus Qwen3-Coder 480B-A35B-Instruct DeepSeek V3.1 GLM4.6 KimiK2 ClaudeHaiku-4.5 MoE 30B 3B 62.28 50.50 30.92 43.00 62.45 47. 49.46 MoE - - 62.28 48.00 52.19 27.75 56.68 37.56 47.41 MoE 480B 35B 59.00 48.00 58.55 42.38 63.87 34. 51.11 MoE 671B 37B 71.50 52.00 40.35 20.62 61.71 53.44 49.94 MoE MoE 1043B 355B 32B 32B 76.10 65.00 71.05 67.50 49.54 37. 61.12 67.54 49.00 86.40 50.63 56.21 53.31 60.52 - - - 67.32 47.50 36.40 53.50 61.19 55.43 53. between medium-scale and large-scale models, the results on Terminal-Bench-Pro reveal substantial headroom for future research. Evaluation on Tool-Use Benchmarks. We further evaluate the tool-use abilities of the models, with the results summarized in Table 3. Overall, the results reveal that ROME excels across the benchmarks. In six benchmark tests, our model achieved an average score of 49.46%, significantly outperforming similarsized models such as Qwen3-Coder-30B-A3B (40.87%), and Devstral Small 2 (39.35%), demonstrating remarkable efficiency. Meanwhile, we find that even when compared with slightly larger-size models, such as GPT-5-mini (close-sourced) and GLM-4.5 Air (106B-A12B), our model still achieves competitive performance. granular analysis of the benchmarks indicates that our model excels particularly in the MTU-Bench (Single-Turn), reaching score of 62.45%, which is substantially higher than Gemini2.5 Flash (45.93%) and GPT-OSS-120B (54.16%). While some models like GPT-5 Mini show volatility across different domains, our model maintains consistent efficacy, particularly in the Tau2-Bench Retail (62.28%) and Airline (50.50%) tasks. These results suggest that the architectural optimizations in ROME provide more stable foundation for complex tool-calling logic than many of its direct competitors. Furthermore, as shown in Table 4, when compared to significantly larger models, ROME remains highly competitive, often matching or exceeding the performance of models with vastly greater parameter counts. Despite having only fraction of the activated parameters (3B) compared to models like DeepSeek-V3.1 (37B activated) and Qwen3-Coder 480B (35B activated), our model maintains highly competitive average performance of 49.46%. Specifically, ROME outperforms Qwen3-Coder Plus (47.41%) and performs on par with DeepSeek-V3.1 (49.94%) across the aggregate suite. In the MTU-Bench (Single-Turn) category, our models performance (62.45%) actually exceeds that of DeepSeek-V3.1 (61.71%) and several other large-scale alternatives. This scaling efficiency highlights ROMEs ability to bridge the performance gap between medium-scale and large-scale models, suggesting that its specialized training for tool-use tasks provides more effective path to achieving agentic capabilities than sheer parameter scaling alone. Evaluation on General Agentic Benchmarks. After establishing robust fundamental tool-use performance of our model, we conducted unified analysis of the models on general agentic benchmarks that require multi-turn interactions and action decision-making. Specifically, we consider GAIA, which focuses on everyday queries requiring coordinated use of multiple tools (e.g., web search, data analysis, and logical reasoning), and BrowseComp-ZH, which emphasizes Chinese multi-hop web search with evidence aggregation across heterogeneous webpages. In addition, we introduce ShopAgent, high-quality proprietary benchmark constructed from real-world e-commerce assistant scenarios, designed to system30 Table 5: Performance on General-Agent Benchmarks (Normal Models). Benchmark Architecture # Total Params # Activated Params GAIA BrowseComp-ZH ShopAgent (Single-Turn) ShopAgent (Multi-Turn) Avg. ROME Qwen3-Coder 30B-A3B-Instruct Devstral Small 2 GPT-OSS120B Gemini-2.5 Flash GLM-4.5 Air GPT-5 Mini MoE 30B 3B 24.24 14.19 34.53 29.61 25.64 MoE 30B 3B 20.00 7.27 22.11 13.38 15.69 Dense 24B - 21.21 7.27 19.44 17.28 16.30 MoE 117B 5.1B 33.54 20.42 21.11 18.54 23.40 - - - 34.14 18.11 20.89 17.51 22.66 MoE 106B 12B 31.92 21.11 25.97 20.12 24.78 - - - 51.52 40.83 23.58 26.41 35.59 Table 6: Performance on General-Agent Benchmarks (Large Models). Benchmark Architecture # Total Params # Activated Params GAIA BrowseComp-ZH ShopAgent (Single-Turn) ShopAgent (Multi-Turn) Avg. ROME Qwen3-Coder Plus Qwen3-Coder 480B-A35B-Instruct DeepSeek V3.1 GLM4. KimiK2 ClaudeHaiku-4.5 MoE 30B 3B 24.24 14.19 34.53 29.61 25.64 MoE - - 31.52 15.80 26.54 22.08 23.99 MoE 480B 35B 33.74 13.15 27.66 20.98 23.88 MoE 671B 37B 31.92 23.88 38.87 33.97 32.16 MoE MoE 1043B 355B 32B 32B 35.76 24.33 33.80 22.12 29.00 34.55 15.22 30.97 26. 26.75 - - - 41.01 22.15 36.21 30.65 32.51 atically evaluate an agents ability to infer user preferences, retrieve and compare products, reason over structured attributes, and adapt to evolving user intent through multi-step interactions. Both Single-Turn and Multi-Turn settings require long-horizon, multi-step interactions, where the Multi-Turn setting is particularly challenging as users may revise or refine their intentions during subsequent interactions, demanding robust intent clarification and adaptive planning. Table 5 and Table 6 report the performance of ROME compared with wide range of strong baselines, including both normal-scale and large-scale models. Additionally, ROME achieves performance comparable to that of larger open-source agentic models across most benchmarks, as shown in Table 6. Notably, our model even surpassed the super-large scale GLM-4.6 in the complex ShopAgent task. These results demonstrate strong generalization across diverse agentic workloads. Overall, ROME significantly outperforms other models of comparable scale, achieving an average score of 25.64%, markedly higher than Qwen3-Coder-30B-A3B-Instruct (15.69%) and Devstral Small 2 (16.30%). Beyond scale-matched comparisons, ROME also demonstrates strong competitiveness against substantially larger models, outperforming Gemini-2.5 Flash (22.66%), GLM-4.5 Air (24.78%), Qwen3-Coder-Plus (23.99%), and Qwen3-Coder-480B-A35B-Instruct (23.88%). Moreover, ROME achieves performance close to Kimi-K2, despite the latter having total parameter count of 1043B with 32B activated parameters. The advantage of ROME is particularly pronounced on the ShopAgent benchmark, where it attains 34.53% in the Single-Turn setting and 29.61% in the more challenging Multi-Turn setting, substantially surpassing all other normal-sized models. These results highlight ROMEs strong capability in long-horizon planning, user preference modeling, and adaptive interactionkey competencies for realistic shopping assistant scenarios involving intent clarification and personalized recommendation."
        },
        {
            "title": "4 Conclusion",
            "content": "The pursuit of agentic crafting represents significant advancement in the capabilities of LLMs, moving beyond simple one-shot responses to operate effectively within dynamic, real-world environments. This shift necessitates robust agentic ecosystem to facilitate the planning, execution, and reliability required for complex tasks. Through our introduction of the Agentic Learning Ecosystem (ALE), we lay the groundwork for streamlining the development and deployment of agentic LLMs. Specifically, by integrating systematic components, i.e., ROLL, ROCK, and iFlow CLI, we provide comprehensive infrastructure that optimizes the complete production pipeline for agent LLMs. Anchored by our proposed policy optimization algorithm IPA, the training pipeline ultimately fosters smoother transition into the agent era. The deployment of ROME, an open-source agent built upon this ecosystem and trained on extensive trajectories, demonstrates the potential of this approach. Our empirical evaluations, supported by various benchmarks and our newly proposed Terminal Bench 31 Pro benchmark, underscore ROMEs strong performance across diverse contexts, reaffirming the practicality and effectiveness of the ALE framework. This foundational infrastructure not only enhances agent model development but also bridges the gap in the open-source community, addressing the challenges that have impeded the practical implementation and adoption of agents. As we continue to refine and expand upon this ecosystem, we anticipate that our efforts will significantly contribute to the evolution of agentic modeling and the broader landscape of AGI applications."
        },
        {
            "title": "5 Authors",
            "content": "Project Lead Weixun Wang XiaoXiao Xu Core Contributors Wanhe An Fangwen Dai Wei Gao Yancheng He Ju Huang Qiang Ji Hanqi Jin Xiaoyang Li Yang Li Zhongwen Li Shirong Lin Jiashun Liu Zenan Liu Tao Luo Dilxat Muhtar Yuanbin Qu Jiaqiang Shi Qinghui Sun Yingshui Tan Hao Tang Runze Wang Yi Wang Zhaoguo Wang Yanan Wu Shaopan Xiong Binchen Xu Xander Xu Yuchi Xu Qipeng Zhang Xixia Zhang Haizhou Zhao Jie Zhao Shuaibing Zhao Baihui Zheng Jianhui Zheng Suhang Zheng Yanni Zhu Contributors Mengze Cai Kerui Cao Xitong Chen Yue Dai Lifan Du Tao Feng Tao He Jin Hu Yijie Hu Ziyu Jiang Cheng Li Xiang Li Jing Liang Chonghuan Liu ZhenDong Liu Haodong Mi Yanhu Mo Junjia Ni Shixin Pei Jingyu Shen XiaoShuai Song Cecilia Wang Chaofan Wang Kangyu Wang Pei Wang Tao Wang Wei Wang Ke Xiao Mingyu Xu Tiange Xu Nan Ya Siran Yang Jianan Ye Yaxing Zang Duo Zhang Junbo Zhang Boren Zheng"
        },
        {
            "title": "Supervision",
            "content": "Wanxi Deng Ling Pan Lin Qu Wenbo Su Jiamang Wang Wei Wang Hu Wei Minggang Wu Cheng Yu Bing Zhao Zhicheng Zheng Bo Zheng"
        },
        {
            "title": "6 Appendix",
            "content": "6.1 Real-world Case Study and Subjective Evaluation Here, we present several concrete real-world task cases to further demonstrate the superiority of our model in agentic crafting capabilities. As summarized in Table 7, we conduct comprehensive evaluation of the models capability to execute real-world tasks. We curate benchmark of 100 distinct tasks (from de-identified real user logs collected via the iFlow CLI) and assess outputs along five dimensions: (1) Functionality & Interaction Implementation, which emphasizes correct core logic, smooth user interaction, and absence of critical defects; (2) Layout & Style Replication, which measures visual fidelity, responsiveness, and adherence to design specifications; (3) Code Quality & Robustness, focusing on structural clarity, standardized naming, maintainability, and error-free execution; (4) Structural & Semantic Correctness, evaluating compliance with HTML5 semantic conventions; and (5) Innovation & Prompt Understanding, capturing accurate requirement comprehension and reasonable value-added enhancements. For comparison, we select two similarly sized models (Qwen3-Coder-30B-A3B-Instruct and Devstral-Small-2) as well as two large-scale models (GLM-4.6 and Qwen3-Coder-Plus) as our reference baselines. To improve reliability and reduce evaluator bias, we employ blinded annotation protocol involving 20 independent domain experts, who judge results without access to model identity. Final labels are determined via majority voting and used to compute the overall win rate. The aggregated quantitative results, together with representative qualitative examples and selected screenshots, are reported in the following sections. Evaluation Dimension Weight Core Requirements Functionality & Interaction Implementation Layout & Style Replication Code Quality & Robustness Structural & Semantic Correctness Innovation & Prompt Understanding 40% 20% 20% 10% 10% Complete core logic, smooth interaction, no critical defects Visually appealing, responsive, compliant with design specifications Clear structure, standardized naming, error-free, maintainable HTML5 semantics Accurate understanding of requirements + reasonable feature enhancements Table 7: Evaluation rubric for real-world case study, detailing the five assessment dimensions, their point weights, and the corresponding core requirements. As shown in Figure 16, across the 100-task benchmark, ROME demonstrates consistent advantages over all evaluated baselines in overall task execution quality. Notably, these gains persist even when compared against larger, same-series model (e.g., Qwen3-Coder-Plus) and strong state-of-the-art agentic model (GLM-4.6), indicating that ROMEs improvements are not merely attributable to parameter scale. This result suggests that ROME more effectively translates high-level requirements into executable plans and reliably completes multi-step workflows, yielding outputs that are not only functionally correct but also better aligned with interaction design, robustness expectations, and semantic structure. In practice, ROME exhibits fewer critical failures in core logic and integration, maintains higher stability under varied task specifications, and provides more consistent end-to-end deliverables across task types. Overall, the findings imply that ROME achieves form of scale-breaking agentic capabilityi.e., stronger real-task completion performance than would be expected from model size alonehighlighting the effectiveness of our approach for enhancing agentic execution beyond scaling laws. Figure 16: Pairwise win-rate matrix (%) on the 100-task real-world benchmark under 30-expert blinded majority voting. Each cell reports the percentage of tasks where the row model is judged better than the column model; higher values (green) indicate stronger performance. We also select two representative case studies(Sleep Management System Generation and Solar System Modeling) and present task screenshots in Figure 17 and Figure 18, respectively. The detailed scoring rubric is 34 Table 8: Case-study evaluation scores, reported as the average ratings across 30 experts. Metric Sub-metric ROME Qwen-Coder-30B Qwen3-Coder-Plus Devstral-Small GLM-4.6 Sleep Management System Generation Interaction Style Restoration Functionality Layout Code Quality Robustness Structure Innovation Semantic Correctness Prompt Understanding Total Score Interaction Style Restoration Functionality Layout Code Quality Robustness Structure Innovation Semantic Correctness Prompt Understanding Total Score 39 18 20 7 8 92 34 20 20 10 10 94 39 16 20 7 7 Solar System Modeling 35 20 20 10 6 91 39 18 20 7 8 92 36 20 20 10 96 39 15 19 6 7 86 10 5 7 3 5 30 40 18 20 7 93 34 20 16 10 10 90 provided in Table 7. From the case examples, we can also observe that ROME achieves stronger taskexecution performance and better visual/page quality than other models of comparable size, and its results are competitive with those large-scale models. 35 (a) ROME screenshot (b) ROME screenshot 2 (c) ROME screenshot 3 (d) Qwen3-Coder-Plus screenshot1 (e) Qwen3-Coder-Plus screenshot 2 (f) Qwen3-Coder-Plus screenshot 3 (g) GLM-4.6 screenshot (h) GLM-4.6 screenshot 2 (i) GLM-4.6 screenshot 3 (j) Qwen3-coder-30B screenshot 1 (k) Qwen3-coder-30B screenshot 2 (l) Qwen3-coder-30B screenshot 3 (m) Devstral-Small-2 screenshot (n) Devstral-Small-2 screenshot 2 (o) Devstral-Small-2 screenshot 3 Figure 17: Case study 1 screenshot examples: Sleep Management System Generation. 36 (a) ROME screenshot 1 (b) ROME screenshot (c) ROME screenshot 3 (d) Qwen3-Coder-Plus screenshot 1 (e) Qwen3-Coder-Plus screenshot 2 (f) Qwen3-Coder-Plus screenshot 3 (g) GLM-4.6 screenshot 1 (h) GLM-4.6 screenshot (i) GLM-4.6 screenshot 3 (j) Qwen3-coder-30B screenshot 1 (k) Qwen3-coder-30B screenshot 2 (l) Qwen3-coder-30B screenshot 3 (m) Devstral-Small-2 screenshot 1 (n) Devstral-Small-2 screenshot (o) Devstral-Small-2 screenshot 3 Figure 18: Case study 2 screenshot examples: Solar System Modeling."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ust un, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Alex Albert, Sam McCandlish, Nelson Elhage, and Deep Ganguli. Building Effective Agents, 2024. URL https://www.anthropic.com/engineering/building-effective-agents. Alibaba. Source code of rtp-llm. https://github.com/alibaba/rtp-llm, 2025. Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. survey of machine learning for big code and naturalness. ACM Comput. Surv., 51(4), July 2018. ISSN 0360-0300. doi: 10.1145/3212695. URL https://doi.org/10.1145/3212695. Ron Amit, Ron Meir, and Kamil Ciosek. Discount factor as regularizer in reinforcement learning. In International conference on machine learning, pp. 269278. PMLR, 2020. Axon-RL. Gem: Generalist environment for multi-task learning. URL https://github.com/axon-rl/ gem. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. Ï„2-bench: Evaluating conversational agents in dual-control environment, 2025. URL https://arxiv.org/abs/2506.07982. Emergent Mind. Agentic sft dataset. https://www.emergentmind.com/topics/agentic-sft-dataset, 2025. Accessed 2025-12. Chang Gao, Chujie Zheng, Xionghui Chen, Kai Dang, Shixuan Liu, Bowen Yu, An Yang, Shuai Bai, Jingren Zhou, and Junyang Lin. Soft adaptive policy optimization. 2025a. URL https://arxiv.org/ abs/2511.20347v1. Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, and Wei Wang. Rollart: Scaling agentic rl training via disaggregated infrastructure. Wei Gao, Yuheng Zhao, Dakai An, Tianyuan Wu, Lunxi Cao, Shaopan Xiong, Ju Huang, Weixun Wang, Siran Yang, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, and Wei Wang. Rollpacker: Mitigating longtail rollouts for fast, synchronous rl post-training, 2025b. URL https://arxiv.org/abs/2509.21009. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023. Jingkai He, Tianjian Li, Erhu Feng, Dong Du, Qian Liu, Tao Liu, Yubin Xia, and Haibo Chen. History rhymes: Accelerating llm reinforcement learning with rhymerl, 2025. URL https://arxiv.org/abs/ 2508.18588. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: systematic literature review. ACM Transactions on Software Engineering and Methodology, 33(8):179, 2024. Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278, 2025. Jian Hu, Xibin Wu, Weixun Wang, Dehao Zhang, Yu Cao, et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. ACM Trans. Softw. Eng. Methodol., July 2025. ISSN 1049-331X. doi: 10.1145/3747588. URL https://doi.org/10.1145/3747588. Just Accepted. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VT F8yNQM66. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 38 Qiyang Li, Zhiyuan Zhou, and Sergey Levine. Reinforcement learning with action chunking. arXiv preprint arXiv:2507.07969, 2025. Jiacai Liu, Yingru Li, Yuqian Fu, Jiawei Wang, Qian Liu, and Yu Shen. When speed kills stability: Demystifying RL collapse from the training-inference mismatch, September 2025. URL https://rich ardli.xyz/rl-collapse. Han Lu, Zichen Liu, Shaopan Xiong, Yancheng He, Wei Gao, Yanan Wu, Weixun Wang, Jiashun Liu, Yang Li, Haizhou Zhao, Ju Huang, Siran Yang, Xiaoyang Li, Yijia Luo, Zihe Liu, Ling Pan, Junchi Yan, Wei Wang, Wenbo Su, Jiamang Wang, Lin Qu, and Bo Zheng. Part ii: Roll flash accelerating rlvr and agentic training with asynchrony, 2025. URL https://arxiv.org/abs/2510.11345. Michael Luo, Naman Jain, Jaskirat Singh, Sijun Tan, Ameen Patel, Qingyang Wu, Alpay Ariyak, Colin Cai, Tarun Venkat, Shang Zhu, Ben Athiwaratkun, Manan Roongta, Ce Zhang, Li Erran Li, Raluca Ada Popa, Koushik Sen, and Ion Stoica. Deepswe: Training state-of-the-art coding agent from scratch by scaling rl. https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourc ed-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33, 2025. Notion Blog. Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. Advances in neural information processing systems, 29, 2016. Yansong Ning, Rui Liu, Jun Wang, Kai Chen, Wei Li, Jun Fang, Kan Zheng, Naiqiang Tan, and Hao Liu. Deeptravel: An end-to-end agentic reinforcement learning framework for autonomous travel planning agents, 2025. URL https://arxiv.org/abs/2509.21842. Alexander Novikov, NgË†an u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning. Wang Pei, Wu Yanan, Song Xiaoshuai, Wang Weixun, Chen Gengru, Yan Kezhong Li Zhongwen, Xiong Shaopan, Zhao Shuaibin, Wan Xi, Su Wenbo, Zheng Bo, et al. Shopsimulator: Evaluating and exploring rl-driven llm agents for multi-turn personalized recommendation in e-commerce, 2025. URL https: //github.com/ShopAgent-Team/ShopSimulator9. Nicolas Le Roux, Marc Bellemare, Jonathan Lebensold, Arnaud Bergeron, Joshua Greaves, Alex Frechette, Carolyne Pelletier, Eric Thibodeau-Laufer, Sandor Toth, and Sam Work. Tapered off-policy reinforce: Stable and efficient reinforcement learning for llms. arXiv preprint arXiv:2503.14286, 2025. Sasha Rush. Building cursor composer with sasha rush. https://www.youtube.com/watch?v=md8D8eNj 5JM, 2025. Online; accessed December 18, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. ByteDance Seed, Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, Jinhua Zhu, Shulin Xin, Dong Huang, Yetao Bai, Lixin Dong, Chao Li, Jianchong Chen, Hanzhi Zhou, Yifan Huang, Guanghan Ning, Xierui Song, Jiaze Chen, Siyao Liu, Kai Shen, Liang Xiang, and Yonghui Wu. Seed-coder: Let the code model curate data for itself, 2025. URL https://arxiv.org/abs/2506.03524. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. verl: Volcano engine reinforcement learning for llm. https://github.com/volce ngine/verl, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Richard S. Sutton. The bitter lesson. https://www.cs.utexas.edu/eunsol/courses/data/bitter esson.pdf, 2019. Accessed: 2025-12. 39 Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. uller (eds.), Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999. URL https://proceedings.neur ips.cc/paper files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf. Sijun Tan, Michael Luo, Colin Cai, Tarun Venkat, Kyle Montgomery, Aaron Hao, Tianhao Wu, Arnav Balyan, Manan Roongta, Chenguang Wang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. rllm: framework for post-training language agents, 2025. Notion Blog. Ling Team, Ang Li, Ben Liu, Binbin Hu, Bing Li, Bingwei Zeng, Borui Ye, Caizhi Tang, Changxin Tian, Chao Huang, et al. Every activation boosted: Scaling general reasoner to 1 trillion open language foundation. arXiv preprint arXiv:2510.22115, 2025. The Terminal-Bench Team. Terminal-bench: benchmark for ai agents in terminal environments, 2025. URL https://github.com/laude-institute/terminal-bench. Thinking Machines AI. Tinker. https://thinkingmachines.ai/tinker/. Accessed: 2025-12. Pei Wang, Yanan Wu, Zekun Wang, Jiaheng Liu, Xiaoshuai Song, Zhongyuan Peng, Ken Deng, Chenchen Zhang, Jiakai Wang, Junran Peng, et al. Mtu-bench: multi-granularity tool-use benchmark for large language models. arXiv preprint arXiv:2410.11710, 2024. Qi Wang, Hongzhi Zhang, Jia Fu, Kai Fu, Yahui Liu, Tinghai Zhang, Chenxi Sun, Gangwei Jiang, Jingyi Tang, Xingguang Ji, Yang Yue, Jingyuan Zhang, Fuzheng Zhang, Kun Gai, and Guorui Zhou. Klear-agentforge: Forging agentic intelligence through posttraining scaling, 2025a. URL https: //arxiv.org/abs/2511.05951. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025c. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for ai software developers as generalist agents, 2025d. URL https://arxiv.org/abs/2407.16741. Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks, 2025e. URL https://arxiv.org/ab s/2501.11733. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025. URL https://arxiv.org/abs/2504.21798. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https://fengyao.notion.s ite/off-policy-rl. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, and Ming Yan. Mobile-agentv3: Fundamental agents for gui automation, 2025. URL https://arxiv.org/abs/2508.15144. Shuyu Yin, Fei Wen, Peilin Liu, and Tao Luo. Analyzing and bridging the gap between maximizing total reward and discounted reward in deep reinforcement learning. arXiv preprint arXiv:2407.13279, 2024. 40 Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, et al. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673, 2025. Chujie Zheng, Kai Dang, Bowen Yu, Mingze Li, Huiqiang Jiang, Junrong Lin, Yuqiong Liu, Hao Lin, Chencan Wu, Feng Hu, An Yang, Jingren Zhou, and Junyang Lin. Stabilizing reinforcement learning with llms: Formulation and practices. 2025a. URL https://api.semanticscholar.org/CorpusID: 283450324. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025b. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025."
        }
    ],
    "affiliations": []
}