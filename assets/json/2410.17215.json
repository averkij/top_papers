{
    "paper_title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models",
    "authors": [
        "Yuxian Gu",
        "Hao Zhou",
        "Fandong Meng",
        "Jie Zhou",
        "Minlie Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher's knowledge. For efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MiniPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at https://github.com/thu-coai/MiniPLM."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 2 5 1 2 7 1 . 0 1 4 2 : r MINIPLM: Knowledge Distillation for Pre-Training Language Models Yuxian Gu1,2, Hao Zhou2, Fandong Meng2, Jie Zhou2, Minlie Huang1 1The CoAI Group, Tsinghua University 2WeChat AI, Tencent Inc., China"
        },
        {
            "title": "Abstract",
            "content": "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MINIPLM, KD framework for pre-training LMs by refining the training data distribution with the teachers knowledge. For efficiency, MINIPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MINIPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MINIPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MINIPLM boosts the student LMs performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MINIPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MINIPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at https://github.com/thu-coai/MiniPLM. (a) Computation Scaling (1.8B 500M) (b) Model Size Scaling Figure 1: Computation (a) and model size (b) scaling curves of student LMs pre-trained from scratch with Vanilla KD1 and MINIPLM. The teacher LM has 1.8B parameters. 1.8B500M means we use 500M student LM. Training-time computation is kept constant for LMs of the same size in model scaling. The y-axis represents the LMs zero-shot performance on 9 downstream NLP tasks. Contribution during an internship at Tencent Inc. guyx21@mails.tsinghua.edu.cn 1Vanilla KD [65, 58] minimizes the token-level forward Kullback-Leibler divergence between the output distributions of the teacher LM and student LM."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in language models (LMs; 35, 8, 60, 78) have largely been driven by scaling up model sizes, but this comes with high inference costs for deployment. At the same time, training small, deployment-friendly LMs faces training computation challenges, as small models are typically far from compute-optimal configurations according to Scaling Laws [37]. This has spurred growing interest in exploring the limits of small LMs under the constraint of pre-training computation [51]. Knowledge Distillation (KD; 36), where small student model learns from large teacher model, is promising approach for training high-performing small LMs. While KD is effective for fine-tuning LMs on specific tasks [88], its role in improving pre-training, the critical stage for small LMs to acquire versatile foundation knowledge, remains under-explored. It is non-trivial to apply KD during pretraining with the methods in fine-tuning stages, which can be categorized as online KD and offline KD. In online KD, the teacher LM has to perform inference during the student LM pre-training to provide token-level probability supervision2 [65, 32], introducing additional training-time computation overhead. As shown in Figure 2, while online KD enhances performance within the same training steps, its benefits diminish if the extra computation was instead used to extend pre-training without KD. In addition, most online KD methods require the teacher and student LMs to share tokenization, limiting its flexibility across different model families. Offline KD, on the other hand, avoids extra training-time computation and allows for KD across model families, as student LMs are trained on the data offline generated by the teacher LM [41, 33]. However, ensuring sufficient difficulty and diversity in the teacher-generated data for pre-training is challenging without extensive human expertise. This often causes the student LM to overfit easy and common language patterns, hindering its generalization to versatile downstream tasks [70], as demonstrated in Figure 2. Figure 2: Results of applying KD methods in fine-tuning to pre-train 200M student LM, using 1.8B teacher LM. See Section 3.1 for method and evaluation details. When the training FLOPs are controlled, all KD methods perform similar or worse than Pre-Train w/o KD. To address these challenges, we propose MINIPLM, an efficient, flexible, and effective KD framework for pre-training LMs, as illustrated in Figure 3. Intuitively, MINIPLM distills the teacher LMs knowledge into the pre-training distribution through Difference Sampling, which samples training instances based on the difference between large and small LMs. Student LMs are then pre-trained from scratch on the refined distribution. To ensure efficiency, as shown in Figure 3(a), Difference Sampling performs offline teacher LM inference, allowing MINIPLM to distill knowledge into multiple student LMs without incurring additional training-time costs. For flexibility, MINIPLM operates solely on the training corpus, enabling KD across model families and ensuring seamless integration with highly optimized training pipelines [15]. In terms of effectiveness, Difference Sampling samples training instances that the teacher LM prefers but that small reference LM assigns low probabilities to, promoting data difficulty and diversity. As depicted in Figure 3(b), this design down-samples easy and common patterns, up-samples hard and diverse instances, and filters out noisy or harmful data points from the pre-training corpus, which encourages student LMs to acquire versatile and sophisticated knowledge, ultimately improving downstream generalization. We apply MINIPLM to pre-train 200M, 500M, and 1.2B student LMs from scratch, using 1.8B teacher LM. We show that MINIPLM surpasses various baselines in improving student LMs zeroshot performance on 9 widely used downstream tasks, enhancing language modeling capabilities, and reducing pre-training computation. By extrapolating the test loss with the Scaling Law [37], we observe that MINIPLMs benefit remains consistent for LMs trained on 10T tokens. MINIPLM also facilitates KD across model families, improving Llama3.1 [19] and Mamba [27] with teacher LM from the Qwen family [3]. Further analysis shows that MINIPLM enhances pre-training data utilization, reducing the data demand by 2.4 times, which mitigates the quick exhaustion of webcrawled corpora [80]. 2Pre-computing this supervision is impractical, as it requires 30PB of storage for 50B tokens with 150K vocabulary. Therefore, teacher LMs probabilities are typically computed online during the student LM training. 2 (a) MINIPLM Training Framework (b) Effect of Difference Sampling Figure 3: MINIPLM. (a): Training framework. MINIPLM distills the knowledge of the teacher LM into the student LM by adjusting the pre-training corpus of the student LM (qθ) through offline Difference Sampling, based on the output probability discrepancy between the teacher LM (p) and small reference LM (pref). (b): Illustration of the effect of Difference Sampling, which down-samples common easy instances, up-samples hard valuable instances, and removes noisy harmful instances."
        },
        {
            "title": "2 MINIPLM: KD for Pre-training LMs",
            "content": "We consider pre-training an LM with an output distribution qθ on large-scale corpus consisting of text sequences, where θ represents the model parameters. KD aids pre-training by incorporating the knowledge of teacher LM with output distribution p, and training θ to minimize the discrepancy between and qθ [65]. MINIPLM formulates KD as reward maximization problem [34], which trains the student LM to generate diverse texts receiving high preference from the teacher LM (Section 2.1). To efficiently and effectively optimize the reward, as illustrated by Figure 3, MINIPLM employs Difference Sampling to refine the pre-training corpus (Section 2.2). This process improves the pre-training distribution in an offline manner with the teacher LMs knowledge and preserves the data diversity and difficulty with the help of small reference model. The student LM is then pre-trained from scratch on the refined corpus (Section 2.3). 2.1 KD as Reward Maximization Recent works [32, 1, 42] have shown the effectiveness of minimizing the reverse Kullback-Leibler divergence (KLD) between and qθ for KD of LMs in the fine-tuning stage, which avoids qθ from over-estimating the low-probability regions of p. We reformulate this objective as reward maximizing problem [34], which is suitable for designing efficient offline optimization methods [44]: θ = arg min θ KL [qθp] = arg min θ Exqθ log qθ(x) p(x) = arg max θ Exqθ r(p, qθ, x), (1) where the reward is defined as r(p, qθ, x) = log p(x) qθ (x) . Intuitively, Eq. (1) trains qθ to generate text that receives high reward values, indicating preference from the teacher LM (i.e., high log p(x) values), while also ensuring high diversity (i.e., low Exqθ qθ(x) values). To optimize Eq. (1), simple yet effective approach is Best-of-N [71, 4, 79], where set Dqθ containing candidates is first sampled from qθ and instances with the highest rewards are selected from these candidates to form new dataset qθ : qθ = top-K{ r(p, qθ, x) Dqθ }, (2) where Dqθ = { xm xm qθ, 1 }. The student LM is then trained on qθ to learn to generate texts with large r(p, qθ, x) values. Although Best-of-N achieves performing the teacher LMs inference prior to the student LM training similar to offline KD [41, 33], it still lacks the efficiency advantage of these methods because obtaining qθ requires (1) sampling data from qθ and (2) computing the reward values with qθ, making qθ non-transferable for pre-training other student LMs. It is also hard to ensure the diversity and difficulty of the candidates sampled from qθ without careful prompt engineering with human expertise. In the following, we show that these issues can be effectively and efficiently addressed by Difference Sampling, which is the basis of the MINIPLM training algorithm. 3 2.2 Difference Sampling As shown in Figure 3(a), Difference Sampling refines the pre-training corpus based on the discrepancy between and the output distribution pref from tiny reference LM, which eliminates the dependency of Eq. (2) on qθ, making the sampled corpus reusable in training multiple student LMs. Top-K Sampling From D, not Dqθ . To avoid sampling data from qθ, we sample instances with high r(p, qθ, x) values from the pre-training corpus D, rather than from the Dqθ generated by the student LM as in Eq. (2). This way, changing the student LM does not affect the candidate set, and contains enough diverse and hard examples to be sampled for pre-training. The following proposition offers theoretical support for this approach, showing that the sampled training instances from Dqθ and are highly likely to be the same when the sizes of Dqθ and are sufficiently large: Proposition 2.1. Let be the sample space of two distributions p1 and p2, X1, X2, , XN p1 be i.i.d random variables, and Y1, Y2, , YM p2 be i.i.d random variables. Let r() : (cid:55) be any injective function. Assume that S, p1(x) > 0, p2(x) > 0. For fixed satisfying 1 min {N, }, when +, +, we have (top-K { r(Xn) 1 } = top-K { r(Ym) 1 }) 1. The proof of Proposition 2.1 is provided in Appendix A. In our context, p1 represents the data distribution of D, p2 = qθ, which is the data sampling distribution of Dqθ , and r(x) = r(p, qθ, x). Intuitively, Proposition 2.1 reveals the fact that when = and Dqθ = are sufficiently large, the top-K instances selected from both sets tend to overlap. This is well-suited to our scenario, as the pre-training corpus is typically large-scale, and it is advantageous to sample as many candidates from Dqθ as possible to find high-reward instances. (3) Decoupling the Student and the Reward-Computing LM. To avoid computing reward values with qθ, we replace it with pref, the output distribution of tiny reference LM, typically smaller than the student LM, for reward computation. The reference LM is pre-trained on small subset Dref, uniformly sampled from D, allowing pref to approximate qθ with minimal computation. This is reasonable approximation because qθ evaluates the difficulty of instances, and the relative data difficulties generally remain consistent across different models [21]. As result, the reward function in Eq. (2) is replaced with r(p, pref, x) = log p(x) pref(x) . In summary, Difference Sampling constructs pre-training corpus from Dref as follows: p(x) pref(x) which is independent of qθ. As shown in Figure 3(b), Difference Sampling essentially refines the data distribution of by comparing the difference between and pref, increasing difficulty and diversity while filtering out noise. detailed discussion of these effects is provided in Section 2.5. Dref = top-K log (cid:12) (cid:12) (cid:12) (cid:12) (cid:26) (cid:27) (4) , 2.3 Pre-Training on Difference-Sampled Corpus As illustrated in Figure 3(a), we pre-train the student LM from scratch on the difference-sampled corpus with the cross-entropy loss for next-token prediction, which is similar to standard pretraining. The loss function L(qθ, D) is given by L(qθ, D) = (cid:88) 1 1 x (cid:88) log qθ(xtx<t), (5) xD where is the length of x, xt is the tth token, and x<t denotes the prefix of with 1 tokens. t=1 2.4 MINIPLM Training Pipeline The general training pipeline of MINIPLM is as follows: (1) Uniformly sample subset Dref from the pre-training corpus D, ensuring Dref D. (2) Train reference LM from scratch on Dref using the cross-entropy loss to obtain its output distribution pref. (3) Perform Difference Sampling with pref and the teacher LMs output distribution p, generating pre-training corpus from Dref using Eq. (4). The size of is controlled by sampling ratio α, where = αD Dref. (4) Pre-train student LMs from scratch on with the cross-entropy loss defined in Eq. (5). 2.5 Discussion Efficiency and Flexibility of MINIPLM. As shown in Figure 3(a), MINIPLM relies only on p(x) and pref(x), the teacher and reference LMs probability over the entire sequence, which can be computed and stored offline because each instance in is associated with single floating-point number, amounting to only 200MB storage for 50B tokens with 1,024 sequence length. Once is difference-sampled based on p(x) and pref(x), multiple student LMs can be efficiently pretrained under the teacher LMs guidance without extra computational cost. In addition, this process modifies only the pre-training data, imposing no restrictions on the architecture or tokenization of the student LM, making MINIPLM highly flexible for integration into optimized pre-training frameworks [15] and suitable for KD across model families. In contrast, online KD [58, 32] relies on per-token distributions, which are infeasible to store offline, as it takes 50B 150K 4byte = 30PB storage for an LM with 150K vocabulary trained on 50B tokens, making online teacher LM inference necessary. Aligning per-token distributions also demands matching tokenizers between the teacher and student [7], which complicates the re-implementation and optimization of pre-training workflows. Effectiveness of MINIPLM. In essence, as illustrated in Figure 3(b), MINIPLM distills the teacher LMs knowledge into the student LMs pre-training distribution via Difference Sampling, producing three effects: (1) Down-sampling easy and common patterns that both the teacher and reference LM fit well, where p(x) pref(x) and log p(x) 0. (2) Up-sampling hard and diverse knowledge, pref(x) which the larger teacher LM has mastered but the smaller reference LM struggles with, and thus p(x) pref(x) and log p(x) pref(x) 0. (3) Discarding noisy and harmful instances that the teacher LM assigns lower probabilities to than the reference LM, where log p(x) pref(x) < 0. We provide examples of these effects in Appendix C.5. Training on the distribution with these effects encourages the student model to focus more on the sophisticated world knowledge learned by the teacher without being distracted by the noise. Note that without the comparison between and pref, the effect (1) disappears, leading to pre-training corpus dominated by common patterns. This explains the effectiveness of MINIPLM against prior offline KD methods [41, 63], where maintaining the data difficulty and diversity, critical to pre-training [70], is challenging without extensive human efforts on prompt engineering [33]."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup Model. We adopt the Qwen-1.5 [3] architecture in our experiments. We use the officially released 1.8B Qwen-1.5 model as the teacher LM and distill its knowledge into students with 200M, 500M, and 1.2B parameters. Detailed model configurations are provided in Appendix B. Pre-Training. We construct pre-training corpora from the Pile [23]. To control the computation in experiments, we pre-train all LMs on maximum of 50B tokens, where documents are merged to construct instances with sequence lengths of 1,024. For online KD methods that incur additional train-time computation, we reduce their training steps to align the total training computation with pre-training without KD or offline KD methods. See Appendix for more pre-training details. Baselines. We compare MINIPLM with 4 baselines: Pre-Train w/o KD pre-trains the student LM on 50B corpus uniformly sampled from the Pile dataset, without the guidance of the teacher LMs knowledge. Vanilla KD [58] minimizes the token-level forward KLD between and qθ, which requires online inference of the teacher LM to obtain the token-level output distributions. SeqKD [41] trains the student LM on the teacher-generated data. Since it is infeasible to generate all 50B tokens, we approximate Kim and Rush [41] by using the first 768 tokens of each instance from the training corpus in Pre-Train w/o KD as the prompts and let the teacher LM generate the remaining tokens offline. MiniLLM [32] minimizes the reverse KLD between and qθ with PPO [68], which requires online inference of the teacher LM and online sampling from the student LM. We treat the first 768 tokens of the instances from the training corpus of Pre-Train w/o KD as the prompts and sample 256 tokens from qθ during the exploration of PPO. 5 HS LAM Wino OBQA ARC-e ARC-c PIQA SIQA Story Avg. Pre-Train w/o KD 31.1 Vanilla KD 30.4 30.2 MiniLLM 30.5 SeqKD 32.7 MINIPLM Pre-Train w/o KD 35.8 37.0 Vanilla KD MiniLLM 33.0 34.9 SeqKD 39.0 MINIPLM Pre-Train w/o KD 39.4 40.7 Vanilla KD MiniLLM 36.1 38.5 SeqKD 42.8 MINIPLM 32.4 31.0 29.4 31.0 35.4 40.1 39.9 35.4 37.9 42. 44.5 43.3 42.5 41.4 46.2 1.8B Teacher 200M Student 49.9 51.4 50.0 51.3 51.4 27.6 26.6 26.6 27.4 27.2 38.9 40.1 39.0 39.3 40.6 23.1 23.1 21.3 22.4 23. 1.8B Teacher 500M Student 51.0 51.7 51.2 50.7 52.2 30.2 29.4 27.5 28.6 30.2 41.7 45.1 42.1 42.7 45.8 24.4 24.2 24.2 23.6 24.9 1.8B Teacher 1.2B Student 51.8 53.2 51.2 51.9 53.3 28.4 29.8 28.5 29.2 31.0 46.0 46.1 44.1 46.5 46.8 25.7 25.5 25.3 25.1 26.9 61.8 62.2 60.5 61.3 63.3 65.4 65.8 62.3 65.0 67. 67.0 67.3 65.8 66.3 68.3 36.4 36.9 36.6 36.9 37.0 38.2 38.0 37.3 38.4 39.0 39.5 39.2 37.9 39.0 39.8 58.1 57.3 57.6 57.4 60.0 61.4 61.6 60.2 58.9 62. 62.2 63.5 61.4 61.0 64.0 39.9 39.9 39.0 39.7 41.3 43.2 43.6 41.5 42.3 44.8 44.9 45.4 43.6 44.3 46.6 Table 1: Zero-shot accuracy scores on 9 widely-used downstream tasks and the average scores (Avg.). We use the Qwen-1.5 1.8B LM [3] as the teacher and Qwen LMs with 200M, 500M, and 1.2B parameters as the student. Student LMs with the same sizes consume the same training-time computation. The best scores of each model size are boldfaced. MINIPLM. We employ 104M reference LM trained on 5B tokens. In Section 3.2 and 3.3, we consider setting where is sufficiently large, containing 105B tokens uniformly sampled from the Pile corpus. We reserve 5B tokens as Dref, and conduct Difference Sampling as per Eq. (4) on the other 100B tokens by setting α = 0.5 to construct 50B-token corpus D. In this way, the student LM is pre-trained on for one epoch. We use the loss difference of the teacher and reference LM to sampled instances from D, which is equivalent to Eq. (4) as all instances have 1,024 tokens. In Section 3.4, we evaluate MINIPLM in data-limited setting, where is controlled to contain 50B tokens and the student LM is trained on the difference-sampled data for multiple epochs. See Appendix C.4 for the ablation studies on the reference LM and sampling ratio α. Evaluation. We assess the zero-shot accuracy of LMs trained with different methods on 9 downstream tasks widely used in examining the foundation abilities of base models [78, 26]. We also test the language modeling capability of the LMs on subset of DCLM [45], high-quality corpus carefully curated with complex pipelines, to examine how well LMs capture broad and diverse knowledge. See Appendix for more evaluation details. 3.2 Main Results MINIPLM Improves Downstream Performance. Table 1 shows zero-shot accuracy on downstream tasks for LMs trained by different methods, leading to three key observations. First, among all the baselines, only Vanilla KD outperforms Pre-Train w/o KD for relatively large student LM, given constant computation budget. This highlights the room for improvement in KD for pre-training, especially when the gap between the teacher and the student is substantial. Second, MINIPLM-trained model achieves the best performance across most of the tasks. Compared to Pre-Train w/o KD, MINIPLM effectively leverages the teacher LMs knowledge to improve the student LM pre-training. Compared to online methods like Vanilla KD and MiniLLM, MINIPLM, as shown in Figure 3(a), incurs no additional training-time overhead, allowing the student LM to be optimized for more steps and leading to higher performance. Compared to offline methods like SeqKD, MINIPLM, as shown in Figure 3(b), uses reference LM to ensure sufficient difficulty and diversity of the pre-training corpus, which is essential for the student LM to learn versatile sophisticated knowledge during pretraining and generalize across various downstream tasks. Finally, the improvements of MINIPLM against Vanilla KD scale well with the student LM size, which is also illustrated in Figure 1(b). 6 (a) 1.8B Teacher 200M Student (b) 1.8B Teacher 500M Student (c) 1.8B Teacher 1.2B Student Figure 4: Language modeling loss on the DCLM [45] subset. We distill the knowledge of the 1.8B Qwen model [3] into student LMs from the Qwen family with 200M, 500M, and 1.2B parameters. We control the total training-time FLOPs of different methods to be the same. MINIPLM Helps Language Modeling. Figure 4, compares the language modeling performance of the MINIPLM-trained LMs and baselines on the DCLM [45] subset, diverse and high-quality dataset curated from web corpora. The results show that, given the same training-time FLOPs, LMs trained with MINIPLM achieve the lowest test losses. In Table 2, we extrapolate the test losses with the Scaling Law [37] to simulate pre-training on 1T and 10T tokens (details in Appendix C.2), showing thatMINIPLM maintains its advantages at the scales of pre-training recent large LMs [78, 19]. critical stage in DCLMs datacleaning pipeline involves removing easy and common patterns, thereby enhancing challenging and diverse signals. Therefore, lower test loss on DCLM suggests that, with the teacher LMs guidance and the reference LM, the MINIPLM-trained LMs learn the diverse and hard knowledge better due to the up-sampling of the corresponding parts in the pre-training distribution, as illustrated in Figure 3(b)."
        },
        {
            "title": "Nstu Method",
            "content": "L1T Pre-Train w/o KD 3.35 Vanilla KD 3.39 3.28 MINIPLM Pre-Train w/o KD 3.12 3.12 Vanilla KD 3.06 MINIPLM Pre-Train w/o KD 2.98 2.95 Vanilla KD 2.92 MINIPLM L10T 3.32 3.35 3.26 3.08 3.07 3.04 2.94 2.91 2. 200M 500M 1.2B Table 2: Test loss predictions using the Scaling Law [37]. Nstu: the student LM size. L1T, L10T: the loss when Pre-Train w/o KD and MINIPLM process 1T and 10T tokens, with Vanilla KD consuming the same training FLOPs. MINIPLM Reduces Training Computation. We plot the 500M student LMs average zero-shot accuracy scores on the downstream tasks in Figure 1(a) with respect to its pre-training FLOPs. MINIPLM achieves the same performance as Vanilla KD while reducing computational costs by 2.2 times. Similar trends are observed for other student LMs (Figure 7) and in the test loss curves on DCLM corpus (Figure 4). The efficiency gains are more pronounced when comparing MINIPLM with Pre-Train w/o KD on the 500M and 1.2B models. We attribute this acceleration to Difference Sampling, which down-samples common patterns and filters out noisy signals from the pre-training corpus, as shown in Figure 3(b). As result, the model trained with MINIPLM avoids wasting computation on learning the easy knowledge quickly memorized during the early training stage and is less distracted by the noisy outliers that slow the convergence down. 3.3 KD Across Model Families noticeable advantage of MINIPLM over Vanilla KD and MiniLLM is its flexibility to distill the knowledge of teacher LM into student LMs with completely different tokenizers and architectures without additional strategies like Boizard et al. [7]. In Table 3, we illustrate the performance when using the LMs from the Qwen [3] family as the teacher and the reference LM to distill knowledge into 212M Llama3.1 [19] model and 140M Mamba [27] model. The results demonstrate the promising performance of MINIPLM in KD across model families, outperforming Pre-Train w/o KD and the existing offline KD baseline (SeqKD). This allows emerging LMs with novel architectures [47, 73] or advanced tokenization [74, 25] to inherit knowledge from existing LMs, thereby facilitating the development of more efficient and higher-performed models. 7 Llama3."
        },
        {
            "title": "Mamba",
            "content": "Acc. Loss Acc. Loss Pre-Train w/o KD 41.0 40.8 SeqKD"
        },
        {
            "title": "MINIPLM",
            "content": "41.8 3.52 3.54 3.43 41.6 41.0 42.6 3.24 3. 3.15 Table 3: Results of KD across model families. We use the teacher and reference LM from the Qwen family to distill the Llama3.1 and Mamba models. The average zero-shot accuracies on the downstream tasks and the losses on the DCLM corpus are reported. Note that Vanilla KD and MiniLLM cannot be applied when the teacher and student LMs use different tokenizations. Figure 5: MINIPLM in the data-constrained setting. We fix to contain 50B tokens and alter the sampling ratio α to obtain with Difference Sampling, which will be trained on for multiple epochs to achieve the constant total trained tokens. The y-axis represents the test loss on the DCLM corpus. 3.4 Data-Limited Setting αDDref We evaluate MINIPLM in data-limited setting where is constrained to contain 50B tokens. To this end, the student LM should be trained on the difference-sampled over multiple epochs to ensure the total computation and trained tokens remain consistent with Pre-Train without KD. We split Dref containing 1B tokens to train the reference LM. Therefore, for sampling ratio α, the α epochs, given that Dref D. In Figure 5, we plot student LM should be trained for the loss curves of the 200M student LMs on the DCLM corpus when using α [0.5, 0.25, 0.125] and training the LM for around 2, 4, and 8 epochs, respectively. We can see that difference-sampling 25% data (with α = 0.25) and training the student LM for 4 epochs yields the best performance, which aligns with the observations in Muennighoff et al. [57]. The corpus sampled with higher α does not achieve the best quality and diversity offered by Difference Sampling, while lower α leads to rapid over-fitting of the student LM. These findings suggest that MINIPLM is promising approach to enhance data utilization when high-quality web corpora become scarce [80]. By extrapolating the loss curve of Pre-Train w/o KD using the Scaling Law in data-constrained setting [57], we estimate that it would require an additional 68B training tokens to match the performance of MINIPLM (α = 0.25, 4 epochs), which means MINIPLM reduces the pre-training data requirement by 2.4 times. See Appendix C.2 for more details on this extrapolation. 3.5 Analysis Impact of Teacher Model. Intuitively, larger teacher LMs will lead to better KD results, which is observed in recent works [32]. However, early works have also shown that an excessively large gap between teacher and student models can hinder effective KD [55]. In Figure 6, we plot the performance of Vanilla KD and MINIPLM when distilling teacher LMs with different sizes into 200M student model. We observe similar phenomenon to Mirzadeh et al. [55] on Vanilla KD and MINIPLM that larger teacher LMs are not necessarily more helpful for KD. However, we attribute this to different factors for Vanilla KD and MINIPLM. In Vanilla KD, the overhead introduced by larger LMs during training diminishes the benefits of distillation. As for MINIPLM, 500M teacher LM proves most effective for distilling into 200M student LM. Smaller teacher LMs (e.g., 300M) lack the capacity to identify the hard but valuable parts of the pre-training distribution, weakening the effectiveness of Difference Sampling as shown in Figure 3(b). On the other hand, the value scale of log p(x) from oversized LMs (e.g., 4B) becomes too small compared to that of the reference LM, which tends to degenerate Difference Sampling into sampling with pref(x) only, losing the effect of the teacher LM. Future research could focus on optimizing teacher model size for MINIPLM or mitigating the impact of differing log p(x) and log pref(x) value scales. Diversity of Difference-Sampled Data. To further verify the effectiveness of Difference Sampling on improving the diversity of the pre-training corpus, in Table 4, we follow Friedman and Dieng [22] to compute the semantic diversity of the difference-sampled corpus and the data used in other baseline pre-training approaches. The results show that the difference-sampled corpus has the highest diversity, 8 Pre-Training Corpus Usage"
        },
        {
            "title": "Original",
            "content": "Teacher-Generated Difference-Sampled MINIPLM Pre-Train w/o KD &Vanilla KD SeqKD 32.25 30.16 36.70 Figure 6: Impact of the teacher LMs sizes on Vanilla KD and MINIPLM, with the pre-training FLOPs aligned. The y-axis represents the average zero-shot accuracy on the downstream tasks. Table 4: Semantic diversity of the original pre-training corpus used in Pre-Train w/o KD and Vanilla KD, the teacher-generated corpus used in SeqKD, and the difference-sampled corpus in MINIPLM. Difference Sampling increases the diversity of the refined pre-training distribution, which helps LM pre-training. despite that Difference Sampling is derived from minimizing the reverse KLD, which exhibits the mode-seeking behavior [54]. We suspect the reason is that Difference Sampling down-samples the easy parts of the corpus containing repeated contents while up-sampling the hard parts consisting of diverse texts. These two components, with large p(x) values as seen in Figure 3(b), constitute the major modes of the teacher LM that qθ seeks during optimizing Eq. (1). Therefore, the mode-seeking behavior helps remove the noisy parts of the pre-training distribution, and the loss of diversity due to noise reduction is compensated by the up-sampling of the hard and diverse data points. We provide case study in Appendix C.5 to further explain our argument. Combining Vanilla KD and MINIPLM. As shown in Table 1 and Figure 4, Vanilla KD improves the 500M and 1.2B student LM performance compared to Pre-Train w/o KD, suggesting the potential of further improving MINIPLM by combining it with Vanilla KD. This combined approach, termed MINIPLM + Vanilla KD, applies Vanilla KD to the differencesampled corpus used in MINIPLM. In Table 5, we compare MINIPLM + Vanilla KD with the individual use of Vanilla KD and MINIPLM, using 1.8B teacher LM. The results show that when pre-training student LMs with 500B and 1.2B parameters, MINIPLM + Vanilla KD further improves the performance, given the same training-time FLOPs. This demonstrates that MINIPLM and Vanilla KD complement each other: MINIPLM distills the coarse-grained sequence-level knowledge of the teacher LM into the student LM via the pre-training data, while Vanilla KD directly aligns the token-level probability distribution between and qθ, providing fine-grained token-level signals. Nstu Method Acc. 200M 500M 1.2B 39.9 Vanilla KD 41.3 MINIPLM MINIPLM + Vanilla KD 40.7 43.6 Vanilla KD MINIPLM 44.8 MINIPLM + Vanilla KD 44.9 45.4 Vanilla KD MINIPLM 46.6 MINIPLM + Vanilla KD 48.1 Table 5: Average accuracy on downstream tasks when combining MINIPLM and Vanilla KD. MINIPLM + Vanilla KD: applying Vanilla KD to pre-train student LMs on the difference-sampled corpus in MINIPLM. Nstu: the size of student LMs."
        },
        {
            "title": "4 Related Work",
            "content": "Language Model Pre-Training. Pre-training is the critical phase for language models (LMs; 10, 60, 77, 15, 78, 79, 19) to obtain their foundation abilities for various downstream tasks. To improve pre-training, some works focus on data curation, such as adjusting domain mixing [86, 89], selecting valuable data points relevant to desired tasks [9, 20, 31], or transforming the instances based on downstream requirements [13, 28, 29]. Another line of work improves the optimization during pretraining by solving better data reweighting strategies [30], designing more effective optimizers [48, 69], or discovering better training recipes [39, 37]. Variations in model architectures [87] and training objectives [75] are also explored to boost pre-training stability and final LM performance. In this work, we utilize the knowledge from existing LMs to enhance pre-training. Small Language Models. Given the high computational demands of large LMs during inference, there has been growing interest in pre-training small LMs [67]. However, achieving high performance 9 with limited parameter sizes remains challenging because the training computation that small LMs need to match the capabilities of large LMs often exceeds Chinchilla-optimal [37] and scales as power law with respect to the model size gap [40]. Despite these challenges, recent efforts have made promising progress by data quality improvement [52, 5, 91] or model pruning [58, 85]. We explore knowledge distillation as complementary approach. Knowledge Distillation. Knowledge distillation (KD;36) uses large teacher model to improve the performance of small student model, which is widely used to build efficient neural network systems [62, 17, 64]. In NLP, early works primarily apply KD to encoder-only models [18, 49] for text classification by aligning token-level distribution [65], hidden states [72], and attention matrices [81, 82]. For generative LMs, straightforward KD approach is training small LMs on the texts generated by large LMs [14, 63, 38]. Other works [32, 1, 83, 46, 84] explore better optimization objectives. However, these works focus on KD for fine-tuning LMs, while pre-training is critical for establishing core LM capabilities [2]. Therefore, we investigate KD in the pre-training stage to develop strong small base LMs."
        },
        {
            "title": "5 Conclusion",
            "content": "Summary. In this work, we find it non-trivial to adapt existing KD approaches for fine-tuning LMs to the pre-training stage because of the high overhead brought by the teacher LM inference in online KD and the tendency of losing data difficulty and diversity in offline KD. Therefore, we propose MINIPLM to address these issues through Difference Sampling, which refines the training distribution by down-sampling easy patterns, up-sampling hard instances, and filtering out noisy data points, with the knowledge of the difference between the large teacher LM and small reference LM. The offline nature of MINIPLM makes it both efficient and flexible to distill student LMs with diverse configurations. The use of the large-small-model differences ensures the difficulty and diversity of the refined pre-training distribution. Using 1.8B LM as the teacher to guide the pre-training of 200M, 500M, and 1.2B LMs, we demonstrate that MINIPLM improves the student LMs performance on 9 downstream tasks, enhances their language modeling capability, and reduces pre-training computation. Additionally, MINIPLM improves the utilization of limited pre-training data and can distill teacher LMs knowledge into student LMs from completely different families. Limitations. One limitation of MINIPLM is that it requires the large LMs probabilities on texts from the pre-training corpus, making black-box KD for close-source LMs [60, 76] with MINIPLM challenging. For some APIs [59], this issue can be solved by specifying user-provided bias in the softmax operation, which allows obtaining the probabilities of given tokens one by one [12], at the expense of large number of API calls. Future Work. promising future direction to explore is applying the difference-sampled corpus to pre-train LMs larger than the teacher LM, enabling weak-to-strong generalization [11]. Since data properties are critical for pre-training LMs across various sizes, the improvement of data diversity and difficulty is likely to be beneficial for LMs larger than the difference-sampling models."
        },
        {
            "title": "References",
            "content": "[1] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In Proceedings of ICLR, 2024. URL https://openreview.net/ forum?id=3zKtaqxLhW. [2] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. In Proceedings of ICML, 2024. URL https://openreview.net/forum?id= 5x788rqbcj. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. URL https://arxiv.org/pdf/2309.16609. 10 [4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. URL https: //arxiv.org/abs/2212.08073. [5] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable LM 2 1.6B technical report. arXiv preprint arXiv:2402.17834, 2024. URL https://arxiv.org/abs/ 2402.17834. [6] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of AAAI, 2020. URL https://ojs.aaai. org/index.php/AAAI/article/view/6239/6095. [7] Nicolas Boizard, Kevin El-Haddad, Céline Hudelot, and Pierre Colombo. Towards the universal logit distillation loss for llms. arXiv preprint cross-tokenizer distillation: arXiv:2402.12030, 2024. URL https://arxiv.org/abs/2402.12030. [8] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. URL https://arxiv.org/abs/2108.07258. [9] David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan Richard Schwarz, and Sham Kakade. CoLoR-Filter: Conditional loss reduction filtering for targeted language model pretraining. arXiv preprint arXiv:2406.10670, 2024. URL https://arxiv.org/abs/2406. 10670. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. Language models are fewshot learners. In Proceedings of NeurIPS, 2020. URL https://papers.nips.cc/paper/ 2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. [11] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. URL https://openai.com/index/weak-to-strong-generalization/. [12] Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, et al. Stealing part of production language model. In Proceedings of ICML, 2024. URL https://arxiv.org/abs/2403.06634. [13] Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. Instruction pre-training: Language models are supervised multitask learners. In Proceedings of EMNLP, 2024. URL https://arxiv.org/abs/2406.14491. [14] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. [15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311. [16] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. URL https://arxiv.org/abs/1803.05457. [17] Wojciech Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. Distilling policy distillation. In Proceedings of AISTATS, 2019. URL http://proceedings.mlr.press/v89/czarnecki19a/czarnecki19a.pdf. 11 [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, 2019. URL https://aclanthology.org/N19-1423.pdf. [19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407. 21783. [20] Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024. URL https://arxiv.org/abs/ 2401.12926. [21] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with v-usable information. In Proceedings of ICML, 2022. URL https://proceedings.mlr. press/v162/ethayarajh22a/ethayarajh22a.pdf. [22] Dan Friedman and Adji Bousso Dieng. The vendi score: diversity evaluation metric for machine learning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=g97OHbQyk1. [23] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. URL https: //arxiv.org/abs/2101.00027. [24] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, et al. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. [25] Nathan Godey, Roman Castagné, Éric de la Clergerie, and Benoît Sagot. MANTa: Efficient gradient-based tokenization for end-to-end robust language modeling. In Findings of EMNLP, 2022. URL https://aclanthology.org/2022.findings-emnlp.207. [26] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. OLMo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. URL https:// arxiv.org/abs/2402.00838. [27] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. URL https://arxiv.org/abs/2312.00752. [28] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. PPT: Pre-trained prompt tuning for fewshor learning. In Proceedings of ACL, 2022. URL https://arxiv.org/abs/2109.04332. [29] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Pre-training to learn in context. Proceedings of ACL, 2023. URL https://aclanthology.org/2023.acl-long.267/. In [30] Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, and Furu Wei. Towards optimal learning of language models. arXiv preprint arXiv:2402.17759, 2024. URL https: //arxiv.org/abs/2402.17759. [31] Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, and Minlie Huang. Data selection via optimal control for language models. arXiv preprint arXiv:2410.07064, 2024. URL https://arxiv.org/abs/2410.07064. [32] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language models. In Proceedings of ICLR, 2024. URL https://openreview.net/forum? id=5h0qf7IBZZ. [33] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. 12 [34] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Proceedings of ICML, 2017. URL http://proceedings. mlr.press/v70/haarnoja17a.html?ref=https://githubhelp.com. [35] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, et al. Pre-trained models: Past, present and future. AI Open, 2021. URL https://www.sciencedirect.com/science/article/pii/ S26666510210002319. [36] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. URL https://arxiv.org/pdf/1503.02531.pdf. [37] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor TrainIn Proceedings of NeurIPS, 2022. https://proceedings.neurips.cc/paper_files/paper/2022/file/ Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, et al. ing compute-optimal URL c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf. large language models. [38] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the ACL, 2023. URL https://aclanthology.org/2023.findings-acl.507/. [39] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. MiniCPM: Unveiling the potential of small language In Proceedings of COLM, 2024. URL https: models with scalable training strategies. //openreview.net/forum?id=3X2L2TFr0f&noteId=QvwPc5chyd. [40] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. URL https://arxiv.org/abs/ 2001.08361. [41] Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In Proceedings of EMNLP, 2016. URL https://aclanthology.org/D16-1139.pdf. [42] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. Distillm: Towards streamlined distillation for large language models. In Proceedings of ICML, 2024. URL https: //openreview.net/forum?id=lsHZNNoC7r. [43] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Proceedings of KR, 2012. URL https://dl.acm.org/doi/10.5555/3031843.3031909. [44] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. URL https://arxiv.org/pdf/2005.01643.pdf. [45] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. DataComp-LM: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024. URL https: //arxiv.org/abs/2406.11794. [46] Yixing Li, Yuxian Gu, Li Dong, Dequan Wang, Yu Cheng, and Furu Wei. Direct preference knowledge distillation for large language models. arXiv preprint arXiv:2406.19774, 2024. URL https://arxiv.org/abs/2406.19774. [47] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. URL https: //arxiv.org/abs/2403.19887. [48] Hong Liu, Zhiyuan Li, David Leo Wright Hall, Percy Liang, and Tengyu Ma. Sophia: scalable stochastic second-order optimizer for language model pre-training. In Proceedings of ICLR, 2024. URL https://openreview.net/forum?id=3xHDeA8Noi. 13 [49] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019. URL https://arxiv.org/ abs/1907.11692. [50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of ICLR, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. [51] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas Lane, and Mengwei Xu. Small language models: Survey, measurements, and insights. arXiv preprint arXiv:2409.15790, 2024. URL https://arxiv.org/abs/2409.15790. [52] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. OpenELM: An efficient language model family with open-source training and inference framework. arXiv preprint arXiv:2404.14619, 2024. URL https://arxiv.org/abs/2404.14619. [53] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In CProceedings of EMNLP, 2018. URL https://api.semanticscholar.org/CorpusID:52183757. [54] Tom Minka et al. Divergence measures and message passing. Technical report, Citeseer, 2005. URL https://www.microsoft.com/en-us/research/wp-content/uploads/ 2016/02/tr-2005-173.pdf. [55] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of AAAI, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/5963/5819. [56] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of NAACL-HLT, 2016. URL https: //aclanthology.org/N16-1098/. [57] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. In Proceedings of NeurIPS, 2023. URL https://openreview.net/forum?id= j5BuTrEj35. [58] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. arXiv preprint Compact arXiv:2407.14679, 2024. URL https://www.arxiv.org/abs/2407.14679. language models via pruning and knowledge distillation. [59] OpenAI. OpenAI: Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt. [60] OpenAI. GPT-4 technical report, 2023. URL https://cdn.openai.com/papers/gpt-4. pdf. [61] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of ACL, 2016. URL https://aclanthology.org/P16-1144.pdf. [62] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of CVPR, 2019. URL https://openaccess.thecvf.com/content_CVPR_ 2019/papers/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.pdf. [63] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277, 2023. URL https://arxiv.org/abs/2304. 03277. 14 [64] Tim Salimans and Jonathan Ho. diffusion models. 3b30857a628099896b6123e85d6cf04c59abe77b.pdf. In ICLR, 2022."
        },
        {
            "title": "Progressive distillation for",
            "content": "sampling of URL https://openreview.net/pdf/ fast [65] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. URL https://arxiv.org/pdf/1910.01108.pdf. [66] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of EMNLP, 2019. URL https://aclanthology.org/D19-1454. [67] Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. In Proceedings of ICML, 2024. URL https://openreview.net/forum?id=0bmXrtTDUu. [68] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. URL https://arxiv. org/pdf/1707.06347.pdf. [69] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Proceedings of ICML, 2018. URL https://proceedings.mlr.press/v80/ shazeer18a/shazeer18a.pdf. [70] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022): 755759, 2024. URL https://www.nature.com/articles/s41586-024-07566-y. [71] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. In Proceedings of NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/ file/1f89885d556929e98d3ef9b86448f951-Paper.pdf. [72] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model compression. In Proceedings EMNLP, 2019. URL https://aclanthology.org/D19-1441. [73] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. URL https://arxiv.org/abs/2307.08621. [74] Yi Tay, Vinh Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via gradient-based subword tokenization. In Proceedings of ICLR, 2022. URL https:// openreview.net/forum?id=JtBRnrlOEFN. [75] Yi Tay, Mostafa Dehghani, Vinh Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning In Proceedings of ICLR, 2023. URL https://openreview.net/pdf?id= paradigms. 6ruVLB727MC. [76] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. URL https: //arxiv.org/abs/2312.11805. [77] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. URL https://arxiv.org/abs/2403.08295. [78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. URL https://arxiv.org/pdf/2302.13971.pdf. 15 [79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https: //arxiv.org/abs/2307.09288. [80] Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325, 2022. URL https://arxiv.org/abs/2211.04325. [81] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In Proceedings of NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/2020/ file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [82] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. MiniLMv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of ACL, 2021. URL https://aclanthology.org/2021.findings-acl.188. [83] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level knowledge distillation. In Proceedings of ACL, 2023. URL https://aclanthology.org/ 2023.acl-long.605.pdf. [84] Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, and Ngai Wong. Rethinking kullbackarXiv preprint leibler divergence in knowledge distillation for large language models. arXiv:2404.02657, 2024. URL https://arxiv.org/abs/2404.02657. [85] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. In Proceedings of ICLR, 2024. URL https://openreview.net/pdf?id=6s77hjBNfS. [86] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. DoReMi: Optimizing data mixtures In Proceedings of NeurIPS, 2024. URL https: speeds up language model pretraining. //openreview.net/forum?id=lXuByUeHhd. [87] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In Proceedings of ICML, 2020. URL https://openreview.net/forum?id=B1x8anVFPr. [88] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. URL https://arxiv.org/abs/2402.13116. [89] Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024. URL https://arxiv.org/abs/2403.16952. [90] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can In Proceedings of ACL, 2019. URL https:// machine really finish your sentence? aclanthology.org/P19-1472/. [91] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. URL https://arxiv.org/abs/ 2401.02385. 16 Proof of Proposition 2.1 To prove Proposition 2.1, We start from the = 1 case, corresponding to selecting an instance with the maximal reward and Eq. (3) becomes (cid:18) arg max 1nN r(Xn) = arg max 1mM r(Ym) 1, (cid:19) which is equivalent to (cid:18) (cid:88) xS arg max 1nN r(Xn) = and arg max 1mM r(Ym) = 1. (cid:19) (6) (7) Since Xn and Ym are independent random variables for 1 and 1 , Eq. (7) can be further written as (cid:18) (cid:88) xS arg max 1nN r(Xn) = arg max 1mM r(Ym) = 1. (8) (cid:19) (cid:18) (cid:19) We focus on the term (cid:18) arg max 1nN (cid:19) r(Xn) = , which can be expanded as follows based on the fact that X1, X2, , XN are i.i.d random variables: (cid:18) arg max 1nN r(Xn) = (cid:19) =P (r(Xn) r(x), for 1 and at least one variable in X1, X2, , XN equals x) =P (r(Xn) r(x), for 1 ) (r(Xn) r(x) and Xn = x, for 1 ) = (cid:89) n=1 (r(Xn) r(x)) (cid:89) n=1 [P (r(Xn) r(x)) (Xn = x)] (9) =P (r(X1) r(x))N [P (r(X1) r(x)) p1(x)]N =PX(x)N (cid:34) (cid:20) 1 1 (cid:21)N (cid:35) , p1(x) PX(x) where PX(x) = (r(X1) r(x)). Note that 0 < p1(x) = (X1 = x) (r(X1) r(x)) = PX(x), we have 0 < p1(x) PY(x) = (r(Y1) r(x)), we have PX(x) 1, and thus = 0. Similarly, by setting 1 p1(x) PX(x) lim N+ (cid:105)N (cid:104) (cid:18) arg max 1mM r(Ym) = (cid:19) = PY(x)M (cid:34) (cid:20) 1 1 (cid:21)M (cid:35) , p2(x) PY(x) (10) (cid:104) (cid:105)M and 1 p2(x) PY(x) lim M+ r(x). Therefore, PX(x) = PY(x) = 1 and = 0. Let = arg max xS PX(x) < 1, PY(x) < 1 for = x. When N, +, we have PX(x)N = PY(x)M = 1, and PX(x)N , PY(x)M 0 for = x. Therefore, we have (cid:19) (cid:20) lim N,M+ (cid:88) xS (cid:18) (cid:19) (cid:18) arg max 1nN r(Xn) = arg max 1mM r(Ym) = (cid:88) = xS lim N,M+ PX(x)N PY(x)M lim N,M+ (cid:34) (cid:20) 1 1 p1(x) PX(x) (cid:21)N (cid:35) (cid:34) = lim N,M+ PX(x)N PY(x)M + (cid:88) xS,x=x lim N,M+ PX(x)N PY(x)M =1, 17 1 1 (cid:21)M (cid:35) p2(x) PY(x) (11)"
        },
        {
            "title": "Model Size",
            "content": "104M 200M 300M 500M 1.2B dmodel 512 768 768 1,024 1,536 dFFN nlayers nhead dhead 1,408 2,112 2,112 2,816 4,224 8 12 18 24 24 8 12 12 16 16 64 64 64 64 96 learning rate 6 104 6 104 6 104 3 104 2.5 10 Table 6: Model configurations and corresponding learning rates. which proves Eq. (6). For > 1, the equality of two top-K subsets requires the elements ranked from 1 to to be equal, respectively. Therefore, the left hand of Eq. (3) can be decomposed using Bayess Law. Let = {X1, X2, , XN }, = {Y1, Y2, , YM }, = arg max r(Xn), 1nN and = arg max r(Ym), we have (X = Y) 1 (Eq. (6)) and 1mM (top-2 { r(Xn) 1 } = top-2 { r(Ym) 1 }) =P arg max r(X) = arg max = (X = Y). YY{Y} Since X1, X2, , XN and Y1, Y2, , YM are i.i.d. random variables, we can still decompose XX {X} (cid:19) (12) (cid:12) (cid:12) r(Y) (cid:12) (cid:12) (cid:18) (cid:18) = as Eq. (9), which means when N, +: (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) the term arg max XX {X} (cid:18) r(X) = arg max XX {X} r(X) = arg max YY{Y} r(Y) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:19) 1. (13) Eq. (13) means the elements with the secondary large r(x) values are highly likely to be the same. The decomposition in Eq. (12) can be conducted times for the elements ranked from 1 to K, and each decomposed term approaches 1 similar to Eq. (13). So far, we have proved that the probability of and having the same top-K subsets measured by r(x) approaches to 1 when N, +, which is formally written as (top-K { r(Xn) 1 } = top-K { r(Ym) 1 }) 1. (14) This completes the proof of Proposition 2.1."
        },
        {
            "title": "B More Experimental Details",
            "content": "Model and Training Configurations. We mostly follow Brown et al. [10] to set the model and learning rate configurations, as summarized in Table 6. The 500M, 1.8B, and 4B teacher models are the officially released Qwen-1.5 checkpoints 3 and the 300M teacher model used in Section 3.5 to analysis the effect of teacher LM sizes Is pre-trained on 200B tokens from our pre-training corpus. We train all the LMs with the AdamW [50] optimizer, with β1 = 0.9, β2 = 0.98, and 0.1 weight decay. We set the batch size to 512 and the max sequence length to 1,024, corresponding to 100K total training steps for roughly 50B tokens in Pre-Train w/o KD, SeqKD, and MINIPLM. For MiniLLM and Vanilla KD, we limit the training steps to align their training computation with Pre-Train w/o KD. Specifically, we assume the computation of forward and backward pass are 2N and 4N D, respectively, where is the model size and is the number of trained tokens. The training steps of MiniLLM and Vanilla KD are listed in Table 7. We linearly warm up the learning rate for 2K steps and apply cosine learning rate decay until 1/10 of the max values. All experiments are conducted on NVIDIA 40G A100 and NVIDIA 32G V100 GPUs. Evaluation Details. Our downstream datasets for evaluation include Hellaswag (HS; 90), LAMBADA (LAM; 61), Winograde (Wino; 43), OpenbookQA (OBQA; 53), ARC-Easy/Challange (ARCe/c; 16), PIQA [6], SIQA [66], and StoryCloze (Story; 56). We apply the LM-Eval-Harness [24] 4 framework to conduct zero-shot evaluation. We sample 10K documents from the DCLM [45] corpus to construct our test set for language modeling evaluation. 3https://huggingface.co/Qwen 4 18 Vanilla KD MiniLLM Formula Student Model Size Nstu 3Nstu 3Nstu+Ntch 200M 500M 1.2B 3Nstu 4Nstu+2Ntch 200M 500M 1.2B Training Steps 25K 45K 65K 15K 30K 40K Table 7: Training steps in Vanilla KD and MiniLLM, which is set to ensure training-time computation to be the same as Pre-Train w/o KD. Nstu and Ntch are model sizes of student and teacher LMs respectively. Ntch = 1.8B in our experiments. = 100K is the training steps in Pre-Train w/o KD. (a) 1.8B 200M (b) 1.8B 500M (c) 1.8B 1.2B Figure 7: Computation scaling curves of student LMs when trained with Pre-Train w/o KD, Vanilla KD, and MINIPLM. The y-axis represents the average zero-shot accuracy on downstream tasks."
        },
        {
            "title": "C More Results",
            "content": "C.1 Computation Scaling Curves of More Sizes In Figure 7, we plot the scaling curves of average accuracy on the downstream tasks with respect to the pre-training FLOPs for student LMs with 200M and 1.2B parameters. We can see that MINIPLM saves pre-training computation for both student LM sizes and constantly outperforms Vanilla KD. C.2 Test Losses Extrapolation with Scaling Laws Data-Unlimited Setting. In Table 2, we follow Hoffmann et al. [37] to fit the scaling law curves with the test losses on the DCLM corpus. Then, we used the fitted constants to predict the test losses for 1T and 10T training data in Pre-Train w/o KD and MINIPLM, and that for Vanilla KD when it consumes the same training FLOPs. Specifically, we consider the following power law suggested by Hoffmann et al. [37]: L(Nm, Nd) = Lirr + Am αm + Ad αd (15) where Nm is the model sizes, Nd is the number of trained tokens, and Lirr is the irreducible test loss. Since the required computation of training on token of Vanilla KD and other methods are different, we re-write Eq. (15) using the fact that NmNd for fixed model size: L(C) = + Ac αc , (16) where and Ac depends on the model size Nm, and αc = αd. In this way, we can fit the loss curves in Figure 4 with Eq. (16). Table 8 includes the fitted values of L, Ac, and αc. We also include the total FLOPs of Pre-Train w/o KD and MINIPLM on 1T (C1T) and 10T (C10T) tokens, which Vanilla KD aligns with. The numbers in Table 8 can be computed with Eq. (16) and the constants in Table 8. Data-Limited Setting. In Section 3.4, we extrapolate the loss curve of Pre-Train w/o KD with the Data-Constrained Scaling Law [57], which is almost the same as Eq. (15), except that Nd represents the total number of tokens in the pre-training corpus repeated 4 times, as suggested by Muennighoff 19 Nstu Method Ac 200M 500M 1.2B Pre-Train w/o KD 2.19107 9.77107 Vanilla KD 8.561010 MINIPLM Pre-Train w/o KD 2.73108 3.14108 Vanilla KD 6.64109 MINIPLM Pre-Train w/o KD 1.88108 1.101010 Vanilla KD 4.29108 MINIPLM αc 0.41 0.44 0.59 0.45 0.45 0.52 0.43 0.52 0.45 C1T (FLOPs) C10T (FLOPs) 3.30 3.34 3.25 3.06 3.05 3.03 2.91 2.90 2.86 1.261021 1.261022 3.14 3.141021 7.301021 7.301021 Table 8: Scaling Law constants in Eq. (16) fitted using the loss curves in Figure 4. Nstu means the student LM size. C1T and C10T are the compute spent on processing 1T and 10T tokens in Pre-Train w/o KD and MINIPLM, which Vanilla KD aligns with. et al. [57]. Therefore, after Nd is solved by letting the loss of Pre-Train w/o KD equal that of MINIPLM (α = 0.25, 4 Eps.), we divide the value by 4, resulting in training tokens number of 118B. This means 68B extra training tokens are needed for Pre-Train w/o KD to achieve the performance of MINIPLM using 50B training tokens. C.3 Difference Sampling with Proxy Model In this section, we show that the offline computational overhead of Difference Sampling Motivation. can be further reduced by conducting teacher and reference LM inference on small proxy dataset and then transferring the value r(p, pref, x) to the entire corpus with proxy model. Specifically, we first uniformly sample proxy subset Dprx from D, satisfying Dprx D. Then, we compute the r(p, pref, x) value for each instance in Dprx. Note that since Dprx D, the computational overhead of teacher LM inference is significantly reduced. After that, we fine-tune small proxy model on Dprx to fit the r(p, pref, x) values, which is used to infer the values for instances from D. Finally, the Top-K operation in Eq. (4) is based on the inferred values. Method. To test this approach, we uniformly sample Dprx containing 0.1B tokens from the 50B-token D. Computing log p(x) pref(x) values on Dprx takes only 0.2% computation of that on D. Then, we employ the reference LM as the proxy model and fine-tune it with the following regression loss: w, b, θ ref = arg min w,b,θref 1 Dprx (cid:88) xDprx (cid:2)wh(x, θref) + r(p, pref, x)(cid:3)2 , (17) where h(x, θref) Rd is the average output hidden states of the reference LM, with representing the models hidden size and θref representing the parameters of the reference LM. Rd, are the parameters of linear head outputting scalar as the predicted r(p, pref, x) values, given the average hidden states. The inferred values on are given by ˆr(x) = wh(x, θ ref)+b. Since this inference process is based on the reference LM, it still saves computation compared to inference with the teacher LM. The difference-sampled corpus is then obtained by selecting = αD instances from with the highest ˆr(x) values. Method Vanilla KD MINIPLM MINIPLMprx FLOPs Online 2 1020 9 1018 Acc. 39.9 41.3 40.9 Table 9: Offline FLOPs and average accuracy (Acc.) on downstream tasks of MINIPLM using proxy model, compared with that of standard MINIPLM and Vanilla KD. Results. We term this method as MINIPLMprx and compare its performance with Vanilla KD and MINIPLM in Table 9. We can see that MINIPLMprx requires much less offline inference computation compared to standard MINIPLM while still maintaining substantial improvement over Vanilla KD. C.4 Ablation Study 20 Figure 8: Impact of the reference model size. We use the 1.8B LM as the teacher and the 200M LM as the student. We report the average zeroshot accuracy on the downstream tasks of the LMs trained with MINIPLM and compare it with Vanilla KD. Figure 9: Impact of the difference sampling ratio α. We report the average zero-shot accuracy on the downstream tasks of the LMs trained with MINIPLM, using α [0.3, 0.4, 0.5, 0.6, 0.7, 0.9] and compare it with Vanilla KD. Impact of the Reference Model. In Figure 8, we show the impact of the reference model size on the performance of LMs trained with MINIPLM. We can see that larger reference models lead to better performance of MINIPLM, but the improvement saturates as the reference LM grows larger. Difference Sampling Ratio. In Figure 9, we plot the impact of the difference sampling ratio on the student LMs performance in the data-unlimited setting. We can see that small sampling ratios result in better zero-shot accuracy on downstream tasks. However, to ensure that the difference-sampled data contains 50B tokens, we need larger original corpus D. To control the computation overhead, we mainly use α = 0.5 in our experiments. C.5 Case Study We present case study in Table 10, 11, and 12 to show the instances corresponding to the three parts of data distribution shown in Figure 3(b). 21 p(x) pref(x): Easy and common instances Instance #1 log p(x) = 1.24 log pref(x) = 1. log p(x) pref(x) = 0."
        },
        {
            "title": "Discarded",
            "content": "<node id=-659 action=modify visible=true lat= 0.05467069248579575 lon=0.014892969670168166 /> <node id=-657 action=modify visible=true lat= 0.05243834625645345 lon=0.023237696125627347 /> <node id=-655 action=modify visible=true lat= 0.04940873338995851 lon=0.03152927145717611 /> <node id=-653 action=modify visible=true lat= 0.04786735135170292 lon=0.040405509151806455 /> <node id=-651 action=modify visible=true lat= 0.04733584029593642 lon=0.04513595918070425 /> Instance #2 log p(x) = 0.44 log pref(x) = 0.51 log p(x) pref(x) = 0."
        },
        {
            "title": "Discarded",
            "content": "{sumlimits_{j = 1}ˆ{n - 1}operatorname{size}left( {mathcal{X},j} right) cdot operatorname{size} left( {mathcal{Z},n - j} right)quad} & {mathcal{I} = mathcal{M}_{1},} {sumlimits_{j = 1}ˆ{n - 1}operatorname{size}left( {mathcal{X},j} right) cdot operatorname{size} left( {mathcal{X},n - j} right)quad} & {mathcal{I} = mathcal{M}_{2},} {sumlimits_{j = 1}ˆ{n - 1}operatorname{size}left( {mathcal{X},j} right) cdot operatorname{size} left( {mathcal{A},n - j} right)quad} & {mathcal{I} = mathcal{M}_{3},} {sumlimits_{j = 1}ˆ{n - 1}operatorname{size}left( {mathcal{A},j} right) cdot operatorname{size} left( {mathcal{Z},n - j} right)quad} & {mathcal{I} = mathcal{M}_{4},} Instance #3 log p(x) = 2.83 log pref(x) = 3.86 log p(x) pref(x) = 1.02 Selected \"I felt bad for Coach Rod to have to deal with it,\" said senior tight end Mike Massey, St. grad. didnt even talk about it.\" \"But for the players it was non-issue. Ignatius We But some other people in college football did. \"[Boren] was legacy guy and when he makes comments like [that], thats like getting kicked square in the shorts when youre Rich Rodriguez,\" ESPN analyst and former OSU quarterback Kirk Herbstreit said Thursday. \"With all the other things that are happening, when you get that comment from legacy guy, guy whose dad started three or four years for Bo, all of sudden everybodys tentacles go up little bit. Table 10: Easy and common instances down-sampled by Difference Sampling. Instance #1 and #2: HTML and LaTeX code data that contain repeated patterns, which is easy to fit by both the reference and teacher LM. Instance #3: Dialogues from story, which is relative easy for LMs but are still selected by Difference Sampling to help student LMs learn basic language skills. 22 p(x) pref(x): Hard and valuable instances Instance #1 log p(x) = 1.26 log pref(x) = 4.20 log p(x) pref(x) = 2."
        },
        {
            "title": "Selected",
            "content": "Dumpster Nonetheless, there are legal Legal along with Environmental Responsibility! rentals in the user side may seem as fundamental as placing phone, having dumpster sent and hurling all your disposals inside to be carted away. issues attached to appropriate disposal connected with certain products which tie up into environmental issues. Dumpster For Rent in Pocahontas customer or perhaps demolition purchaser should be informed about these issues by means of careful screening so as to reduce firms liability which inturn keeps firms overhead all the way down and makes for prompt fall off, pick up along with disposal of the dumpster and its articles. The 10 Yard Instance #2 log p(x) = 2.36 log pref(x) = 5.59 log p(x) pref(x) = 3.23 Selected 有利 you3li4 yˇoulì advantageous; beneficial 谨慎 jin3shen4 jˇınshèn cautious; prudent 甲 jia3 jiˇa one; armor (1st Heavenly Stem) 犹豫 you2yu4 yóuyù hesitate; hesitant; undecided 从此 cong2ci3 cóngcˇı from now on; since then 企业 qi3ye4 qˇıyè company; business; firm 下载 xia4zai3 xiàzˇai to download 狮子 shi1zi5 shızi lion 青少年 qing1shao4nian2 qıngshàonián teenager Instance #3 log p(x) = 0.16 log pref(x) = 2.73 log p(x) pref(x) = 2. Selected function WritableState(options, stream) { var Duplex = require(./_stream_duplex); options = options {}; // the point at which write() starts returning false // Note: 0 is valid value, means that we always return false if // the entire buffer is not flushed immediately on write() var hwm = options.highWaterMark; var defaultHwm = options.objectMode?16:16*1024; this.highWaterMark = (hwm hwm === 0) ? defaultHwm; hwm : // object stream flag to indicate whether or not this stream // contains buffers or objects. this.objectMode = !!options.objectMode; ... } Table 11: Hard and valuable instances up-sampled by Difference Sampling. Instance #1: High-quality long documents containing versatile world knowledge. Instance #2: instance that contains translation tasks, presented in an in-context learning form. Instance #3: High-quality code data with detailed comments. 23 p(x) pref(x): Noisy and harmful instances Instance #1 log p(x) = 9.50 log pref(x) = 6.60 log p(x) pref(x) = 2."
        },
        {
            "title": "Discarded",
            "content": "]{} ********************************************** . ... ... . Instance #2 log p(x) = 1.01 log pref(x) = 0. log p(x) pref(x) = 0.11 Discarded (91.00,60.00) (1.00,35.00) (2.00,35.00)[(1,0)[13.00]{}] {} (16.00,35.00) (16.70,34.30)[(1,-1)[8.60]{}]{} (26.00,25.00) (26.00,11.00) (26.00,12.00)[(0,1)[12.00] {}]{} (36.00,35.00) (26.00,45.00) (16.70,35.70) (35.30,34.30)[(-1,-1)[8.60]{}]{} (56.00,35.00) (66.00,25.00) (76.00,35.00) (66.00,45.00) (66.00,46.00) Instance #3 log p(x) = 2.53 log pref(x) = 0.26 log p(x) pref(x) = 2.26 Discarded *Z* = 8 - - Data collection #tablewrapdatacollectionlong =============== fine-focus sealed tube - Bruker APEXII area-detector diffractometer Radiation source: graphite and scans Absorption correction: (*SADABS*; Sheldrick, 2004) *T*min= 0.970, *T*max= 0.981 20408 measured reflections - multi-scan Table 12: Noisy and harmful instances discarded by Difference Sampling. Instance #1: irregular symbols. Both the reference LM and the teacher LM are hard to fit. Instance #2: string primarily made of meaningless numbers. Both the reference LM and the teacher LM are easy to fit, by predicting numbers. Instance #3: data collection in scientific paper, but lack of contexts. The reference LM fits the patterns, which the teacher LM finds useless."
        }
    ],
    "affiliations": [
        "The CoAI Group, Tsinghua University",
        "WeChat AI, Tencent Inc., China"
    ]
}