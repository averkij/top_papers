{
    "paper_title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
    "authors": [
        "Huibin Li",
        "Haoran Liu",
        "Mingzhe Liu",
        "Yulong Xiao",
        "Peng Li",
        "Guibin Zan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN."
        },
        {
            "title": "Start",
            "content": "U-Net-Like Spiking Neural Networks for Single Image Dehazing Huibin Li College of Computer Science and Cyber Security, (Chengdu University of Technology) School of Data Science and Artificial Intelligence (Wenzhou University of Technology) Chengdu, Wenzhou, China lihuibin@stu.cdut.edu.cn Yulong Xiao Department of Energy (Politecnico di Milano) Milano, Italy yulong.xiao@mail.polimi.it Haoran Liu College of Nuclear Technology and Automation Engineering (Chengdu University of Technology) School of Data Science and Artificial Intelligence (Wenzhou University of Technology) Chengdu, Wenzhou, China liuhaoran@cdut.edu.cn Peng Li College of Nuclear Technology and Automation Engineering (Chengdu University of Technology) Chengdu, China lipeng@stu.cdut.edu.cn Mingzhe Liu* School of Data Science and Artificial Intelligence (Wenzhou University of Technology) Wenzhou, China liumz@cdut.edu.cn Guibin Zan* Sigray, Inc. Concord, USA gbzan@sigray.com AbstractImage dehazing is critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with longrange dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an that integrates U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN. innovative architecture KeywordsImage Dehazing, Leaky-Integrate-and-Fire, Remote Sensing, Spiking Neural Networks I. INTRODUCTION Single image dehazing is vital technique in image processing designed to enhance the visibility and quality of images diminished by atmospheric haze, which can obscure details and distort color accuracy. Traditional dehazing methods extract information from single hazy image, utilizing models of light propagation and atmospheric scattering to estimate and eliminate haze effects [1]. However, recent advancements in deep learning techniques have demonstrated significantly superior performance, effectively dominating this field. By analyzing contrast and consistency within the hazy image, these algorithms can enhance clear regions while accurately reconstructing obscured details. This process is particularly valuable in applications such as remote sensing [2], photography [3], and computer vision [4], where clarity is crucial for precise analysis and interpretation. Convolutional Neural Networks (CNNs) [4] and Transformers [5] stand out as the two most prominent *Corresponding author. Fig. 1: Comparisons between our DehazeSNN and other state-of-the-art dehazing methods on the RESIDE-ITS set. Circle size is proportional to the number of model parameters. backbones for image dehazing architectures. CNNs have been widely adopted due to their exceptional ability to capture local features and patterns within images, making them effective for various image processing tasks, including dehazing. They rely on convolutional layers to analyze image segments, providing robust feature extraction. On the other hand, Transformers have gained traction for their capacity to model global dependencies through selfattention mechanisms. This allows them to consider contextual information across the entire image, which is crucial for effectively restoring hazy scenes. CNN-based models were among the first deep learning methods applied to the field of image dehazing. These models initially built upon traditional algorithms that utilize the atmospheric scattering model, estimating atmospheric light or transmission depth to reconstruct haze-free images through the resolution of physical systems [7]. However, early methods often produced images with high saturation levels and encountered issues such as color distortion and halo artifacts. As research progressed, it became evident that using CNN-based models to directly estimate haze-free image could yield superior performance [8], resulting in more streamlined dehazing approach that eliminates the need for complex physical modeling. However, due to the limited size of convolutional kernels, CNN-based models often fall short in capturing long-range dependencies and global image feature extraction. As result, recent research has increasingly emphasized the use of multi-scale fusion [6, 7], feature pyramids [8, 9], and dilated convolutions [10, 11] to enhance the overall capability of image information processing. These approaches aim to improve the models' ability to gather comprehensive contextual information, leading to better performance in image dehazing tasks. utilize Transformers-based models attention mechanisms to effectively capture global information within an image. By establishing direct mapping between hazy and haze-free images, these methods often outperform CNN-based models [15]. However, this remarkable capability for managing long-range dependencies comes at the cost of computationally intensive cross-attention calculations. As result, Transformers typically require large number of parameters and an even greater number of Multiply-Accumulate Operations (MACs). To address this computational burden, recent research has focused on optimizing the attention process, leading to the development of models such as DehazeFormer [12] and SwinIR [13], which aim to deliver high performance while reducing computational demands. To address the limitations of models based on CNNs and Transformers, this study proposes DehazeSNN, which combines U-Net-like architecture with Spiking Neural Networks (SNNs). The classic U-Net structure is employed to decompose hazy images into multi-scale feature map channels, allowing DehazeSNN to manipulate image features across various scales and effectively capture and restore objects of different sizes. Additionally, we implement an Orthogonal Leaky-Integrate-and-Fire (LIF) module, termed OLIFBlock, to facilitate cross-channel enables information DehazeSNN to leverage global image information and longterm dependencies. Consequently, DehazeSNN efficiently processes local features and patterns, akin to CNNs, while managing long-range dependencies similar to Transformers, but with greater efficiency than traditional cross-attention mechanisms. communication. This design [14], inherits Moreover, from SNNs the OLIFBlock significant computational efficiency further enhancing the overall performance of DehazeSNN. It achieves state-of-the-art image dehazing results with considerably smaller model size compared to competing models based on CNNs or Transformers. An intuitive demonstration provided in Fig. 1 shows that both the model size and MACs of DehazeSNN are significantly smaller than those of its competitors. In summary, our contributions are as follows: We introduced DehazeSNN, featuring U-Net-like architecture capable of processing images across different scales, effectively leveraging rich local features and patterns for image dehazing. feature channel We developed the OLIFBlock based on Spiking Neural Networks (SNNs), which facilitates access to long-term dependencies while significantly reducing computational burden, marking the first application of SNNs in the field of image dehazing. Our DehazeSNN performs favorably against stateof-the-art methods on three benchmark datasets, encompassing both photography and remote sensing dehazing. It achieved high evaluation criteria values while maintaining very small model size and less MACs. II. RELATED WORK A. Image Dehazing Image quality can significantly degrade in hazy weather, adversely affecting digital image processing and interpretation. As result, many researchers focus on recovering high-quality, clear scenes from hazy images. Prior to the broad adoption of deep learning in computer vision, image dehazing algorithms primarily relied on physical models and prior assumptions, such as the wellknown Dark Channel Prior (DCP) [1]. However, these traditional methods struggle to compete with deep learning models due to their reliance on limited prior knowledge of image scenes. Early deep learning approaches sought to estimate the parameters of atmospheric scattering models [7, 8], but more recent studies have shown that end-to-end dehazing methodologies [8], which operate independently of physical models, often achieve superior performance. transitioned The rapid evolution of deep learning architecture has profoundly impacted the mainstream models used in image from dehazing. These models have Convolutional Neural Networks (CNNs) [15, 16] to Generative Adversarial Networks (GANs) [17], stable diffusion models, and Transformers [18, 19]. In 2020, the introduction of the Vision Transformers (ViT) marked significant performance boost over previous models, dominating benchmarks across all model sizes [12]. Following this, the Swin Transformers further enhanced image dehazing performance by employing attention mechanisms based on shifted windows [13], enabling it to capture overall feature dependencies at considerably lower computational cost. image Meanwhile, CNN-based models have also adapted to address the challenges of managing long-term dependencies. For instance, researchers developed MixDehazeNet [20], which employs multi-scale parallel large convolution kernels to effectively handle uneven hazy distributions. This model outperformed all other state-of-the-art methods in 2023, including those utilizing more advanced architectures like Transformers. However, the reliance on attention mechanisms or large convolution kernels results in larger models with substantial number of parameters and MACs. decoupling In addition to improving model architecture, many learning recent studies have focused on enhancing methodologies. Orthogonal contrastive regularization has been introduced to facilitate unpaired image dehazing [21], addressing the challenges of obtaining paired hazy and haze-free images. Similarly, Cong et al. proposed semi-supervised approach to tackle this issue [22]. Furthermore, the joint learning of depth estimation and dehazing has been [23], enabling reinforcement learning approach that achieved state-of-theart performance in 2024. While these advancements have boosted image dehazing capabilities, they have also complicated model architectures and learning processes, resulting in increasingly cumbersome designs. implemented Fig. 2: (a) Overall architecture of the proposed DehazeSNN, which learns image features through 5-stage U-net. (b) Architecture of Spiking Neural Network Block (SNNBlock), which contains two core parts: (c) MLPs, and (d) Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock). (e) SKfusion, which serves as connectivity means to link same-latitude feature maps for information exchange. B. Spiking Neural Networks SNNs are an advanced approach in neuromorphic computing and artificial intelligence, designed to mirror the communication methods of biological neurons through discrete spikes or action potentials [24]. Unlike traditional leverage Artificial Neural Networks continuous values and activation functions, SNNs encode information in the timing of these spikes. This enables more efficient processing of temporal data and enhances the representation of dynamic environments. (ANNs) that in Moreover, SNNs offer substantial reductions computational costs compared to conventional ANNs, which often depend on extensive MACs [25]. In an SNN, neuron's internal potential is determined by accumulating the synaptic weights of incoming spikes, thus eliminating the need for multiplication. This approach not only addresses the von Neumann bottleneck but also enables the development on neuromorphic hardware [26]. spiking models large-scale of Due to the challenges posed by the undefined nature of differentiating non-continuous spike generators in SNNs, early models like SpikeProp [27], Tempotron [28], and ReSuMe [29] were difficult to train and had limited applications [29-31]. As result, recent research has focused on enhancing the learning processes of SNNs. Various solutions have been proposed, including ANN-tobackpropagation SNN techniques [32], novel biologically plausible learning rules [33], and continuous spike approximations [34]. Recently, SNNs have found applications in diverse fields such as image classification, segmentation, and robotic control [35]. The architecture and learning mechanisms of SNNs are continually evolving, leading to an expansion of their applicable domains. [30], modified conversion III. METHODOLOGY In this section, we present DehazeSNN. We begin with an overview of its overall architecture, followed by detailed explanation of the Spiking Neural Network Block (SNNBlock) and the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock). Finally, we describe the SKfusionbased skip connection [12] utilized in DehazeSNN. A. DehazeSNN Architecture and deep feature extraction, DehazeSNN is 5-stage U-Net [36], as shown in Fig. 2. Our network consists of three parts: shallow feature extraction, image reconstruction. When hazy image ܫ ଷௐ is input into the network, shallow feature extraction is first performed using 3 3 convolutional layers to generate shallow feature map ܨ ௐ . Subsequently, the feature map ܨ is fed into two-scale encoder-decoder network comprising five SNNblock modules for deep feature extraction, with information transfer between the same scales utilizing SKfusion instead of traditional skip connections. Finally, 3 3 convolutional layer is applied to output the reconstructed image ܬ ଷௐ. B. Spiking Neural Network Block As shown in Fig. 2(b), the SNNBlock consists of two residual sub-blocks: OLIFBlock and Multilayer Perceptron Layers (MLPs). The OLIFBlock adopts an iterative spiking mechanism as replacement for traditional attention mechanisms, enabling the extraction of local information effectively. It can capture texture features efficiently, facilitating perception of both global and local information. The MLPs, consisting of fully connected layers and activation functions, are responsible for further extracting and mapping the feature information from the OLIFBlock. Additionally, drop path is used to connect the two modules, providing regularization to prevent overfitting. C. Orthogonal Leaky-Integrate-and-Fire Module The Leaky-Integrate-and-Fire (LIF) neuron is simplified model of neural activity that captures essential TABLE QUANTITATIVE COMPARISON OF VARIOUS SOTA METHODS ON SYNTHETIC DATASETS Methods DCP [1] DehazeNet [37] GridDehazeNet [38] MSBDN [39] AECR-Net [15] PSD [40] MAXIM-2S [41] Dehamer [42] USID-Net [43] Cycle-SNSPGAN [44] DehazeFormer [12] DDPNet [45] SwinTD-Net [46] UME-Net [47] UVM-Net [48] UCL-Dehaze [49] DehazeSNN-M DehazeSNN-L TPAMI10 TIP16 ICCV19 CVPR20 CVPR21 CVPR21 CVPR22 CVPR22 TMM22 TITS22 TIP23 CVPR23 KBS23 PR24 arXiv24 TIP24 RESIDE-ITS SSIM 0.818 0.821 0.984 0.985 0.990 0.715 0.991 0.988 0.895 0.852 0.996 0.994 0.968 0.704 0.996 0.862 0.995 0.996 PSNR 16.62 19.82 32.16 33.67 37.17 12.50 38.11 36.63 21.41 19.13 40.05 39.31 36.79 20.3 40.17 21.36 40.10 41.26 RESIDE-OTS SSIM PSNR 0.815 19.13 0.927 24.75 0.982 30.86 0.982 33.48 - - 0.749 15.51 0.985 34.19 0.986 35.18 0.919 23.89 0.925 24.28 0.984 34.95 0.989 34.72 0.981 32.07 0.953 27.83 0.984 34.92 0.927 25.21 - - 0.982 33.69 Overhead #Param - 0.009M 0.956M 31.35M 2.61M 33.11M 14.10M 132.50M 3.77M 2.358M 25.44M - 22.24M 52.229M 19.25M 19.451M 2.70M 4.75M MACs - 0.581G 21.49G 41.54G 52.2G 182.5G 108G 24.465G 40.423G 67.145G 279.7G - 1457G 189.62G 173.55G 78.795G 26.28G 37.27G TABLE II QUANTITATIVE COMPARISON ON RESIDE-6K DATASETS. TABLE III QUANTITATIVE COMPARISON ON RS-HAZE DATASETS. Methods DCP [1] DehazeNet [37] MSBDN [39] FFA-Net [50] AECR-Net [15] Dehamer [42] SDA-GAN [51] IR-SDE [52] CCA [53] PNE [53] WAE [54] Bi-Dehazing [55] DehazeSNN-M DehazeSNN-L PSNR SSIM #Params MACs 17.88 21.02 28.56 29.96 28.52 27.52 18.67 28.5 29.06 29.64 25.84 30.71 30.07 30.77 - 0.009M 31.35M 4.46M 2.61M 29.44M 19.96M 5.45M 4.75M 4.75M 12.5M 8.054M 2.70M 4.75M - 0.581G 41.54G 287.5G 52.2G 59.67G 72.19G 41.95G 306.9G 306.9G 0.08G - 26.28G 37.27G 0.816 0.870 0.966 0.973 0.964 0.950 0.795 0.958 0.951 0.964 0.949 0.975 0.973 0.975 features of how real neurons integrate incoming signals and gradually lose potential over time, ultimately firing action potential when certain threshold is reached. The behavior of classic LIF neuron can be modeled as follows: ௨ = ൜ ௧ = ݑ + ܫ, 1 ݑ > th 0 ݑ < th ݑreset ݑ > th ݑ ݑ < th ݑ = ൜ (1) , (2) . (3) Where ݑ represents the membrane potential of the neuron, ܫ is the input from the upper layers acting as external stimulus, is the time decay coefficient, is the final output pulse, and th is the firing threshold for the current neuron. Methods DCP [1] DehazeNet [37] MSBDN [39] AECR-Net [15] Restormer [56] M2SCN [57] Trinity-Net [58] C2PNET [59] DehazeSNN-M DehazeSNN-L PSNR SSIM #Params MACs 17.86 23.16 38.57 35.69 32.94 37.75 32.17 34.78 38.95 39.14 - 0.009M 31.35M 2.61M 26.13M - - 7.17M 2.70M 4.75M - 0.581G 41.54G 52.2G - - - - 26.28G 37.27G 0.734 0.816 0.965 0.959 0.958 0.950 0.919 0.942 0.970 0. When ݑ is below the threshold th, the membrane potential decays over time and receives external stimuli, and at this point, the neuron does not emit pulse signal. When exceeds the threshold th, the neuron emits pulse, and ݑ is then reset to the resting membrane potential ݑreset. Traditionally, LIF neurons accumulate charge potential over time, which poses challenges for large-scale parallel computation in deep neural networks on GPUs. To effectively integrate LIF neurons into deep neural networks, DehazeSNN accumulates charge potential in the spatial domain by an iterative grouping technique for processing feature maps. It divides the feature map into horizontal and vertical branches, independently integrating information within these branches spatially rather than relying on timebased accumulation (i.e., working in group steps instead of time steps). Additionally, DehazeSNN also replaces LIF binary output pulses with continuous outputs, enabling fullprecision outputs that facilitate gradient descent for parameter optimization. The formula is as follows: Fig. 3: Qualitative comparison of synthetic hazy images by different methods. The first two rows of images are from the RESIDEITS dataset, the last two rows are from RESIDEOTS dataset. TABLE IV: MODEL ARCHITECTURE DETAILED. Setting Num. of Blocks Embedding Dims MLP Ratio LIF Init LIF Init th DehazeSNN-M 8, 12, 16, 12, 8 24, 48, 96, 48, 24 4, 4, 4, 4, 4 0.25 0. DehazeSNN-L 8, 16, 32, 16, 8 24, 48, 96, 48, 24 4, 4, 4, 4, 4 0.25 0.25 ݕଵ = ݔ, ݑଵ = ݑ൫1 ൯ + ݕଵ, (4) (5) ଵ = ൜ 1 ݑଵ > th , 0 ݑଵ < th , th). ݎଵ = max(ݑ௧ଵ (6) (7) reduced subsequently concatenation [36], where the branches are directly joined and in dimensionality via convolutions. However, this straightforward concatenation often overlooks differences and biases in the feature map branches. To address this issue, we utilize SKfusion for merging information from the two branches. As illustrated in Fig. 2(e), the feature maps from both branches first undergo element-wise addition. The resulting feature map is then processed using Global Average Pooling (GAP) and passed through MLP module to generate an attention vector for each feature map. These attention vectors act as weights, allowing for selective focus on relevant information from each branch, all while maintaining low computational cost, making SKfusion promising alternative to traditional concatenation fusion. Where represents the group number. , ݔ , and ݕ represent the weight, input, and output, respectively. Different from traditional LIF neurons, OLIFBlock does not directly output variable ଵ . It outputs ݎଵ as the fullinstead serves precision result at the + 1 group step. ௧ଵ as temporary variable to store the output state at the + 1 group step and participates in the next round of iterative calculations. The detailed processing scheme of the OLIFBlock is illustrated in Fig. 2(d). Initially, the feature map undergoes convolution processing using Depth-Wise Convolution (DWConv). To extract information from various directions and establish long-term dependencies, the feature maps are then processed in parallel through two directional groups: horizontal and vertical branches. In each branch, spatial information is accumulated through LIF calculations of internal neural potential. Finally, the outputs from the two LIF groups are combined, resulting in feature map that maintains the same size as the original input. D. Residual Connection with SKfusion In the classic U-Net architecture, the fusion of skip branch and main branch is typically achieved through IV. EXPERIMENTS We implemented DehazeSNN based on PyTorch [60], and all experiments were trained using only one NVIDIA GeForce RTX 4090 GPU. There are two variants for DehazeSNN, namely DehazeSNN-M and DehazeSNN-L, with specific designs listed in TABLE IV. During the training process, we used the AdamW [61] optimizer, with patch size set to 256 256, batch sizes of 4 (DehazeSNNL) and 5 (DehazeSNN-M). The exponential decay rates were set to ߚଵ = 0.9 and ߚଶ = 0.999. The initial learning rates for the parameters th and of the OLIFBlock were set to 5 10ହ and the rest of the network's learning rates were set to 1 10ସ and gradually decreased to 1 10. We used L1 loss and perceptual loss LPIPS [62] as the combined loss functions. A. Datasets and Evaluation Criteria We evaluated our proposed model on the RESIDE [63] and RS-Haze [12] datasets. The RESIDE dataset is widely utilized in the field of image dehazing as benchmark ground. It consists of several sub-datasets, each unique in its characteristics and applications. In our experiments, we used three subsets as training data: ITS Fig. 4: Qualitative comparison of RS hazy images by different methods. TABLE V: ABLATION STUDIES ON COMPONENTS OF DEHAZESNN. Methods Baseline +SKfusion +OLIFBlock PSNR SSIM #Params MACs 17.52G 25.69 0.938 17.38G 25.70 0.939 26.43 29.88 0.971 26.28G DehazeSNN-M 30.07 0.973 1.77M 1.76M 2.70M 2.70M (13,990 pairs of indoor images), OTS (313,950 pairs of outdoor images), and the mixed dataset RESIDE-6k (6,000 pairs of indoor and outdoor images), and evaluated them on SOTS indoor (500 pairs of images) and SOTS outdoor (500 pairs of images). To evaluate the performance of our method on more diverse hazy conditions, we used the RSHaze dataset (51,300 pairs of training images and 2,700 pairs of testing images), as the haze in remote sensing images is highly non-uniform. Performance evaluation was done using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). We also used two metrics to quantify computational costs: the number of parameters and MACs. B. Photography Dehazing We compared DehazeSNN with recent methods in the image dehazing field, and the results are shown in TABLE its to comprehensively capture and TABLE II. The quantitative results in TABLE that our DehazeSNN-L outperforms all demonstrate methods on the RESIDE-ITS dataset, not only improving the PSNR value to above 41 but also having significantly lower parameter and computational costs compared to models with PSNR values above 40. This implies that our model is more suitable for tasks focused on lightweight and performance-oriented applications. Additionally, in the RESIDE-OTS dataset, which is characterized by its substantially larger size, compact model like DehazeSNN struggles features. Consequently, it ranks only among the top performers in the table. Nevertheless, this still highlights the advantage of our model in terms of parameter efficiency and computational performance. Moreover, its results for RESIDE-6K, presented in TABLE II, surpass those of other algorithms, demonstrating that DehazeSNN excels not only on specific features but also exhibits robust feature extraction capabilities across diverse, mixed datasets. Fig. 3 illustrates the visual comparison of DehazeSNN with other dehazing methods. In indoor image dehazing, comparative methods often exhibit artifacts in edge shadow areas, while DehazeSNN does not have these issues and shows better processing results. In outdoor scenes, at the boundary of ground water mist, all comparative methods exhibit varying degrees of residual haze. In comparison, our DehazeSNN generates images that are most faithful to the original, with better recovery of texture details at the boundary. TABLE VI: ABLATION STUDY ON LOSS FUNCTION Loss Function L1 LPIPS 0.5LPIPS+0.5L1 0.3 LPIPS+0.7L1 PSNR SSIM 28.91 0.962 26.57 0.941 30.07 0.973 0.967 29.02 TABLE VII: ABLATION STUDY ON LIF GROUP NUMBER LIF Group number 2 6 4 PSNR SSIM 29.26 0.969 28.95 0.963 30.07 0.973 C. Remote Sensing Dehazing TABLE III displays the comparison results of our method with others on the RS-Haze dataset. DehazeSNN still achieves good results in handling remote sensing images with non-uniform haze. Our approach, as shown in Fig. 4, produces visual results closer to haze-free real images, demonstrating better performance in color and contrast. Such results are demonstrated objectively in TABLE III, where DehazeSNN significantly outperformed advanced dehazing methods in the field with minimal parameter amount and model size. D. Ablation Study In this section, we conduct an in-depth analysis based on DehazeSNN-M, elucidating its core components and the impact of related parameter selections. All models in the analysis were trained on the RESIDE-6K dataset and evaluated on the mixed SOTS indoor-outdoor dataset. Effectiveness of the components. We established Unet-based baseline model, without SKfusion and OLIFBlock. Subsequently, we configured two variants based on the baseline model to verify the effectiveness of these two components of DehazeSNN, as shown in TABLE V. SKfusion does not substantially boost overall performance; however, it offers lower costs than traditional concatenation fusion, positioning it as an effective alternative. The OLIFBlock plays pivotal role in driving the model's overall performance. Following its integration, performance metrics showed significant improvement, strongly confirming its effectiveness in feature extraction. Study of the Loss Function. DehazeSNN utilizes weighted combination of L1 Loss and LPIPS Loss as the Loss function, with the formula defined as follows: ℒ௧௧ = ߙଵℒଵ + (1 ߙଵ)ℒ. (8) To find the optimal value for the hyperparameter ߙଵ, we conducted ablation experiments, with the results shown in TABLE VI, indicating that ߙଵ = 0.5 yields the best performance. Research on the LIF group number. We also explored the optimal hyperparameter , which represents the number of LIF neuron groups. We evaluated the results under various values of , as presented in TABLE VII. The highest accuracy was achieved when = 4. Therefore, our model uniformly adopts = 4 in experiments. V. CONCLUSION In this study, we introduce DehazeSNN, U-Net-like spiking neural network designed for image dehazing. DehazeSNN excels in detailed local feature analysis while effectively managing and significantly reducing computational overhead. These advantages stem from our innovative OLIFBlock, which employs orthogonal leakyintegrate-and-fire neural activity characteristic to extract and generalize image features across various scales. long-term dependencies image grouping and the Experimental results on both photography and remote sensing dehazing datasets demonstrate that DehazeSNN is highly competitive with state-of-the-art methods, achieving first-tier performance with minimal model size and low number of parameters. This indicates the considerable potential of DehazeSNN for application in computationally sensitive dehazing scenarios. Future research will explore the application of DehazeSNN architecture to additional image restoration tasks. ACKNOWLEDGMENT This work was supported by the Wenzhou Major Science and Technology (No. ZG2023011) and partially funded by the Natural Science Foundation of Zhejiang Province (No. LZ25F010007). Innovation Project [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] REFERENCES K. He, J. Sun, and X. Tang, \"Single Image Haze Removal Using Dark Channel Prior,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 12, pp. 23412353, 2011. J. Liu, S. Wang, X. Wang, M. Ju, and D. Zhang, \"A Review of Remote Sensing Image Dehazing,\" Sensors, vol. 21, no. 11, pp. 3926, 2021. R. Vishnoi and P. K. Goswami, \"A Comprehensive Review on Deep Learning based Image Dehazing Techniques,\" in Proceedings of the 11th International Conference on System Modeling & Advancement in Research Trends (SMART), pp. 1392-1397, 2022. Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, \"A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects,\" IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 12, pp. 6999-7019, 2022. S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, \"Transformers in Vision: Survey,\" ACM Computing Surveys, vol. 54, no. 10s, p. Article 200, 2022. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke et al., \"Going deeper with convolutions,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19, 2015. G. Tang, L. Zhao, R. Jiang, and X. Zhang, \"Single Image Dehazing via Lightweight Multi-scale Networks,\" in Proceedings of the IEEE International Conference on Big Data (Big Data), pp. 5062-5069, 2019. D. Zhao, L. Xu, L. Ma, J. Li, and Y. Yan, \"Pyramid Global Context Network for Image Dehazing,\" IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 8, pp. 3037-3050, 2021. H. Zhang, V. Sindagi, and V. M. Patel, \"Multi-scale Single Image Dehazing Using Perceptual Pyramid Deep Network,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 1015101509, 2018. S. Zhang and F. He, \"DRCDN: learning deep residual convolutional dehazing networks,\" The Visual Computer, vol. 36, no. 9, pp. 1797-1808, 2020/09/01 2020. Y. W. Lee, L. K. Wong, and J. See, \"Image Dehazing With Contextualized Attentive U-NET,\" in Proceedings of the [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] IEEE International Conference on Image Processing (ICIP), pp. 1068-1072, 2020. Y. Song, Z. He, H. Qian, and X. Du, \"Vision Transformers for Single Image Dehazing,\" IEEE Transactions on Image Processing, vol. 32, pp. 1927-1941, 2023. J. Liang, J. Cao, G. Sun, K. Zhang, L. V. Gool, and R. Timofte, \"SwinIR: Image Restoration Using Swin Transformer,\" in Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW), pp. 1833-1844, 2021. A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, and A. Maida, \"Deep learning in spiking neural networks,\" Neural Networks, vol. 111, pp. 47-63, 2019. H. Wu, Y. Qu, S. Lin, J. Zhou, R. Qiao, Z. Zhang, Y. Xie, and L. Ma, \"Contrastive learning for compact single image dehazing,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10551-10560, 2021. Y. Cui, W. Ren, X. Cao, and A. Knoll, \"Revitalizing convolutional network for image restoration,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 9423-9438, 2024. X. Zhang, \"Research on remote sensing image dehaze based on GAN,\" Journal of Signal Processing Systems, vol. 94, no. 3, pp. 305-313, 2022. X. Zhang, F. Xie, H. Ding, S. Yan, and Z. Shi, \"Proxy and Cross-Stripes Integration Transformer for Remote Sensing Image Dehazing,\" IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 1-15, 2024. J. Liu, H. Yuan, Z. Yuan, L. Liu, B. Lu, and M. Yu, \"Visual transformer with stable prior and patch-level attention for single image dehazing,\" Neurocomputing, vol. 551, p. 126535, 2023. L. Lu, Q. Xiong, B. Xu, and D. Chu, \"MixDehazeNet: Mix Structure Block For Image Dehazing Network,\" in Proceedings of the International Joint Conference on Neural Networks (IJCNN), pp. 1-10, 2024. Z. Wang, H. Zhao, J. Peng, L. Yao, and K. Zhao, \"ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired Image Dehazing,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 25479-25489, 2024. X. Cong, J. Gui, J. Zhang, J. Hou, and H. Shen, \"A Semisupervised Nighttime Dehazing Baseline with SpatialFrequency Aware and Realistic Brightness Constraint,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2631-2640, 2024. Y. Zhang, S. Zhou, and H. Li, \"Depth Information Assisted Collaborative Mutual Promotion Network for Single Image Dehazing,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 28462855, 2024. J. L. Lobo, J. Del Ser, A. Bifet, and N. Kasabov, \"Spiking Neural Networks and online learning: An overview and perspectives,\" Neural Networks, vol. 121, pp. 88-100, 2020. K. Roy, A. Jaiswal, and P. Panda, \"Towards spike-based machine intelligence with neuromorphic computing,\" Nature, vol. 575, no. 7784, pp. 607-617, 2019. P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, and B. L. Jackson, \"A million spiking-neuron integrated circuit with scalable communication network and interface,\" Science, vol. 345, no. 6197, pp. 668-673, 2014. S. M. Bohte, J. N. Kok, and J. A. La Poutré, \"SpikeProp: backpropagation for networks of spiking neurons,\" in Proceedings of the European Symposium on Artificial Neural Networks (ESANN), pp. 419-424, 2000. R. Gütig and H. Sompolinsky, \"The tempotron: neuron that learns spike timingbased decisions,\" Nature Neuroscience, vol. 9, no. 3, pp. 420-428, 2006. F. Ponulak and A. Kasiński, \"Supervised Learning in Spiking Neural Networks with ReSuMe: Sequence Learning, Classification, and Spike Shifting,\" Neural Computation, vol. 22, no. 2, pp. 467-510, 2010. J. K. Eshraghian, M. Ward, E. O. Neftci, X. Wang, G. Lenz, G. Dwivedi, M. Bennamoun, and D. S. Jeong, \"Training Spiking Neural Networks Using Lessons From Deep [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] Learning,\" Proceedings of the IEEE, vol. 111, no. 9, pp. 1016-1054, 2023. H. Liu, M. Xiang, M. Liu, P. Li, X. Zuo, X, Jiang, and Z. Zuo, \"Random-Coupled Neural Network,\" Electronics, vol. 13, no. 21, p. 4297, 2024. M. Dampfhoffer, T. Mesquida, A. Valentian, and L. Anghel, \"Backpropagation-Based Learning Techniques for Deep Spiking Neural Networks: Survey,\" IEEE Transactions on Neural Networks and Learning Systems, vol. 35, no. 9, pp. 11906-11921, 2024. Z. Yi, J. Lian, Q. Liu, H. Zhu, D. Liang, and J. Liu, \"Learning rules in spiking neural networks: survey,\" Neurocomputing, vol. 531, pp. 163-179, 2023. W. Li, H. Chen, J. Guo, Z. Zhang, and Y. Wang, \"Braininspired Multilayer Perceptron with Spiking Neurons,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 773-783, 2022. K. Yamazaki, V.-K. Vo-Ho, D. Bulsara, and N. Le, \"Spiking Neural Networks and Their Applications: Review,\" Brain Sciences, vol. 12, no. 7, p. 863, 2022. O. Ronneberger, P. Fischer, and T. Brox, \"U-net: Convolutional networks for biomedical image segmentation,\" in Proceedings of the Medical Image Computing and Computer-Assisted Intervention (MICCAI), part III 18, pp. 234-241, 2015. B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, \"Dehazenet: An end-to-end system for single image haze removal,\" IEEE Transactions on Image Processing, vol. 25, no. 11, pp. 51875198, 2016. X. Liu, Y. Ma, Z. Shi, and J. Chen, \"Griddehazenet: Attention-based multi-scale network for image dehazing,\" in Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 7314-7323, 2019. H. Dong, J. Pan, L. Xiang, Z. Hu, X. Zhang, F. Wang, and M. H. Yang, \"Multi-scale boosted dehazing network with dense feature fusion,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 21572167, 2020. Z. Chen, Y. Wang, Y. Yang, and D. Liu, \"PSD: Principled synthetic-to-real dehazing guided by physical priors,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7180-7189, 2021. Z. Tu, H. Talebi, H. Zhang, and Y. Feng, \"Maxim: Multi-axis mlp for image processing,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5769-5780, 2022. C.-L. Guo, Q. Yan, S. Anwar, R. Cong, W. Ren, and C. Li, \"Image dehazing transformer with transmission-aware 3d position embedding,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5812-5820,2022. J. Li, Y. Li, L. Zhuo, L. Kuang, and T. Yu, \"USID-Net: Unsupervised single image dehazing network via disentangled representations,\" IEEE Transactions on Multimedia, vol. 25, pp. 3587-3601, 2022. Y. Wang, X. Yan, D. Guan, M. Wei, Y. Chen, X. P. Zhang, and J. Li, \"Cycle-snspgan: Towards real-world image dehazing via cycle spectral normalized soft likelihood estimation patch gan,\" IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 11, pp. 20368-20382, 2022. G. Kim, J. Park, and J. Kwon, \"Deep Dehazing Powered by Image Processing Network,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1209-1218, 2023. H. Zhou, Z. Chen, Y. Liu, Y. Sheng, W. Ren, and H. Xiong, \"Physical-priors-guided DehazeFormer,\" Knowledge-Based Systems, vol. 266, p. 110410, 2023. H. Sun, Z. Luo, D. Ren, B. Du, L. Chang, and J. Wan, \"Unsupervised multi-branch network with high-frequency enhancement for image dehazing,\" Pattern Recognition, vol. 156, p. 110763, 2024. Z. Zheng and C. Wu, \"U-shaped vision mamba for single image dehazing,\" arXiv preprint, arXiv:2402.04139, 2024. Y. Wang, X. Yan, F. L. Wang, H. Xie, W. Yang, and X. P. Zhang, \"Ucl-dehaze: Towards real-world image dehazing via [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] unsupervised contrastive learning,\" IEEE Transactions on Image Processing, vol. 33, pp. 1361-1374, 2024. X. Qin, Z. Wang, Y. Bai, X. Xie, and H. Jia, \"FFA-Net: Feature fusion attention network for single image dehazing,\" in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), vol. 34, no. 07, pp. 11908-11915, 2020. Y. Dong, Y. Li, Q. Dong, H. Zhang, and S. Chen, \"Semisupervised domain alignment learning for single image dehazing,\" IEEE Transactions on Cybernetics, vol. 53, no. 11, pp. 7238-7250, 2022. Z. Luo, F. K. Gustafsson, Z. Zhao, J. Sjölund, and T. B. Schön, \"Refusion: Enabling large-size realistic image restoration with latent-space diffusion models,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1680-1691, 2023. D. Cheng, Y. Li, D. Zhang, N. Wang, J. Sun, and X. Gao, \"Progressive negative enhancing contrastive learning for image dehazing and beyond,\" IEEE Transactions on Multimedia, vol. 26, pp. 8783-8798, 2024. A. Ali, R. Sarkar, and S. S. Chaudhuri, \"Wavelet-based AutoEncoder for simultaneous haze and rain removal from images,\" Pattern Recognition, vol. 150, p. 110370, 2024. Q. Ma, S. Wang, G. Yang, C. Chen, and T. Yu, \"A novel bistream network for image dehazing,\" Engineering Applications of Artificial Intelligence, vol. 136, p. 108933, 2024. S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang, \"Restormer: Efficient transformer for highresolution image restoration,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5728-5739, 2022. S. Li, Y. Zhou, and W. Xiang, \"M2scn: Multi-model selfcorrecting network for satellite remote sensing single-image dehazing,\" IEEE Geoscience and Remote Sensing Letters, vol. 20, pp. 1-5, 2022. K. Chi, Y. Yuan, and Q. Wang, \"Trinity-Net: Gradientguided Swin transformer-based remote sensing image dehazing and beyond,\" IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 1-14, 2023. Y. Zheng, J. Zhan, S. He, J. Dong, and Y. Du, \"Curricular contrastive regularization for physics-aware single image dehazing,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 57855794, 2023. A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison et al., \"Automatic differentiation in pytorch,\" in Proceedings of the Neural Information Processing Systems Conference (NIPS), 2017. I. Loshchilov, \"Decoupled weight decay regularization,\" arXiv preprint, arXiv:1711.05101, 2017. R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \"The unreasonable effectiveness of deep features as perceptual metric,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 586595, 2018. B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang, \"Benchmarking single-image dehazing and beyond,\" IEEE Transactions on Image Processing, vol. 28, no. 1, pp. 492-505, 2018."
        }
    ],
    "affiliations": [
        "College of Computer Science and Cyber Security, Chengdu University of Technology",
        "College of Nuclear Technology and Automation Engineering, Chengdu University of Technology",
        "Department of Energy, Politecnico di Milano",
        "School of Data Science and Artificial Intelligence, Wenzhou University of Technology",
        "Sigray, Inc."
    ]
}