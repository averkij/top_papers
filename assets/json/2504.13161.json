{
    "paper_title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training",
    "authors": [
        "Shizhe Diao",
        "Yu Yang",
        "Yonggan Fu",
        "Xin Dong",
        "Dan Su",
        "Markus Kliegl",
        "Zijia Chen",
        "Peter Belcak",
        "Yoshi Suhara",
        "Hongxu Yin",
        "Mostofa Patwary",
        "Yingyan",
        "Lin",
        "Jan Kautz",
        "Pavlo Molchanov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains a challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in a pre-training setting. Specifically, CLIMB embeds and clusters large-scale datasets in a semantic space and then iteratively searches for optimal mixtures using a smaller proxy model and a predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific domain (e.g., Social Sciences) yields a 5% improvement over random sampling. Finally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20 clusters as a research playground, and ClimbMix, a compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Our data is available at: https://research.nvidia.com/labs/lpr/climb/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 6 1 3 1 . 4 0 5 2 : r 2025-4-18 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Shizhe Diao, Yu Yang, Yonggan Fu2, Xin Dong, Dan Su, Markus Kliegl, Zijia Chen, Peter Belcak, Yoshi Suhara, Hongxu Yin, Mostofa Patwary, Yingyan (Celine) Lin2, Jan Kautz, Pavlo Molchanov Abstract: Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in pre-training setting. Specifically, CLIMB embeds and clusters large-scale datasets in semantic space and then iteratively searches for optimal mixtures using smaller proxy model and predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for specific domain (e.g., Social Sciences) yields 5% improvement over random sampling. Finally, we introduce ClimbLab, filtered 1.2-trillion-token corpus with 20 clusters as research playground, and ClimbMix, compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Datasets on Hugging Face: ClimbMix ClimbLab Homepage: CLIMB 1. Introduction Pre-training datasets for large language models (LLMs) have scaled to trillions of tokens, typically combining large-scale web crawls with smaller, highquality domain-specific datasets. These corpora enable the development of generalist models capable of addressing diverse tasks. However, their vast scale and heterogeneity pose challenges in balancing general knowledge with domain expertise, often leading to inefficient utilization of high-value data for specialized capabilities. Recent studies emphasize the importance of the final stage of pertaining, commonly referred to as mid-training, where models are refined on targeted, high-quality data to enhance specific capabilities. For example, [1] demonstrated that emphasizing domain-specific datasets during the final pre-training phase significantly improves performance on benchmarks such as GSM8K [2] (math), MMLU [3] (reasoning), and HumanEval [4] (coding). Similarly, OLMo 2 [5] mixes high-quality web data with curated STEM references, synthetic math datasets, and encyclopedic content for mid-training, achieving notable gains in math reasoning tasks. These findings highlight the potential of carefully curated data mixtures in mid-training for improving domain performance. Despite the success of pre-training, optimizing data mixtures for both general and domain-specific tasks remains challenge: (1) Large-scale datasets such as Figure 1 Pre-training 1B model on ClimbMix shows better scaling effects than training on other datasets. We measure the average performance on 12 downstream benchmarks. Common Crawl 1 offer unmatched diversity and scale but lack explicit domain labels, making it difficult to extract domain-relevant content. Filtering data often relies on general-purpose heuristics like perplexity or educational value [6], which do not necessarily capture the most informative or high-quality content for specific domains. (2) Even with curated datasets like The Pile [7] with domain annotations, selecting an optimal data mixture is non-trivial due to the complex, 1https://commoncrawl.org/ Work done while Yu Yang was at NVIDIA; now at OpenAI. 2 with Georgia Institute of Technology, USA; 2025 NVIDIA. All rights reserved. CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Figure 2 Given large-scale pre-training data consisting of web-scale and curated sources, CLIMB identifies the optimal mixture of different topics (A, B, C) to improve performance in target task (e.g., general reasoning). We compare the performance of state-of-the-art language models across different parameter scales on general reasoning benchmarks. CLIMB achieves better tradeoff between model size and performance, demonstrating more efficient scaling trend compared to prior models. nonlinear relationship between dataset composition and model performance. For instance, optimizing model for coding tasks requires not just programmingrelated content but also complementary knowledge from mathematics, reasoning, and security. To these address Iterative (CLIMB)a novel challenges, we propose Data Mixture CLustering-based Bootstrapping framework for automating the search for optimal data mixtures during pre-training. CLIMB consists of three key steps: (1) embedding and clustering large-scale datasets, (2) constructing mixture-performance pairs by sampling and pruning data mixtures and training proxy models, and (3) fitting predictor. By treating the data mixture as input features and performance metrics as target labels, we train regression model as predictor. This approach enables efficient, iterative refinement of data mixtures without relying solely on predefined domain labels. We frame data mixture construction as search problem and solve it using bootstrapping strategy. At each iteration, candidate mixtures are proposed, pruned, and refined to optimize diversity and domain relevance. Unlike static mixing strategies, our method dynamically adjusts data mixtures throughout training using weak predictor approach, integrating multiple predictors iteratively to discover effective configurations for domain adaptation. CLIMB actively Figure 3 Visualization of CLIMBs iterative search process using t-SNE. Each point represents data mixture config in the search space, with different iterations (CLIMB-Iter1, CLIMB-Iter2, CLIMB-Iter3) illustrating how the search space is refined over iterations. Initially, the search explores broad set of configurations (Iter 1), progressively narrowing in subsequent iterations (Iter 2 and Iter 3) as CLIMB selects more optimal mixtures. learns to refine and optimize data mixtures based on real-world feedback from environment verifications, rather than passively relying on predefined heuristics or human-annotated domain labels. This ability to iteratively self-improve makes CLIMB more flexible and adaptive to new data distributions and domainspecific requirements. Additionally, CLIMB prioritizes computational efficiency, demonstrating that iterative data mixture search achieves superior results within fixed training budget. For instance, rather than allocating all resources to larger model searching in one iteration, our approach iteratively refines training data mixtures, balancing verification and generation tasks. Importantly, to reduce the computational overhead, our method leverages lightweight proxy models to evaluate mixture quality and reduce the search space by pruning progressively, significantly reducing the cost of brute-force hyperparameter sweeps. We demonstrate the effectiveness of CLIMB by searching the optimal data mixture in general reasoning tasks first and then extending it to specific domains (e.g., STEM, social sciences, and humanities). Using the optimal data mixture discovered by CLIMB, we train 350M and 1B models on 40B tokens, both of which surpass the previously best data mixing (Doremi and RegMix) methods by large margin. Furthermore, when trained on larger number of tokens (400B) with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. We observe that optimizing for specific domain (e.g., Social Sciences) yields 5% improvement over ran2 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training dom sampling. Finally, based on the insights obtained from our explorations, we further apply CLIMB to two existing datasets, Nemotron-CC [8] and smollmcorpus [9], and produce new dataset with superior performance. Our contributions are threefold: Automated Data Mixture Optimization. We propose an embedding-driven data mixing approach to automatically identify, group, mix high-quality clusters, enabling efficient domain-specific training while removing the reliance on manually predefined domain-specific data. Dynamic and Iterative Search Framework. Our method introduces an iterative search process, dynamically refining data mixtures throughout training to optimize diversity and domain relevance, while addressing scaling challenges in clustering and data filtering. New High-quality Dataset. We contribute filtered 1.2-trillion-token corpus with 20 clusters as new playground for data mixing research and new high-quality 400-billion-token data for efficient pretraining. 2. Related Work Data Mixture for LLM Pre-training. The composition of pre-training datasets are critical in determining the generalization abilities of language models [10, 11, 12]. Typically, data mixtures like those in the Pile [7], GLaM [13], and ROOTS [14] are crafted using manually defined rules, yet these heuristics lack standardization and transferability across different settings. SlimPajama-DC [15] systematically evaluated the influence of various predefined data configurations, yielding valuable insights. More recently, learningbased approaches such as DoReMi [16] and DoGE [17] have introduced optimization techniques for domain proportions by iteratively refining training with reference and proxy models. While [18] investigated data sequencing strategies through the lens of curriculum learning, our work focuses on the simultaneous integration of diverse data domains, emphasizing distinct aspect of pre-training. The aforementioned methods show promise, but they require the dataset to already possess clear and natural domain distinctions. By contrast, we propose novel approach that can automatically identify approximate domains from large amounts of web data and then find the optimal data mixture automatically. In parallel work, WebOrganizer [19] proposes using classifiers to annotate web-scale data with topic and format labels. In contrast, our clustering-based approach is more straightforward, and readily scalable, and we introduce an iterative optimization method to refine the data mixture. Data Selection for Specific Domains. Beyond optimizing the overall pre-training data mixture [20, 21, 22, 23], selecting high-quality domainspecific data [24, 25] is essential for improving model specialization during pre-training. Existing methods approach this challenge differently. DSIR [26] estimates relevance using hashed n-grams and resamples data to better match target domain distributions. CRISP [27] clusters the generalist dataset and samples these clusters according to their frequencies in the smaller specialist dataset. [28] propose to select data that nudges the pre-training distribution closer to the target distribution. Training dynamics-based selection leverages model learning behavior to guide data filtering, including S2L [29], which clusters data based on loss trajectories to prioritize domain-relevant examples, and LESS [30], which selects instruction tuning data with the highest gradient similarity to target task. Embedding-based filtering removes redundant [31] or low-quality data, with SCIP [32] applying synthetic corruptions for filtering and heuristic pruning [33] reducing noise from overrepresented [34] proposes to select the data long-text clusters. on which model losses are predictive of downstream abilities. While these approaches improve data quality for specialized domains, they often rely on predefined domain labels or heuristics, limiting their flexibility for large-scale pre-training. In contrast, our proposed framework, CLIMB, iteratively refines domain-relevant data mixtures without requiring explicit domain labels, making it more applicable to real-world pre-training data and easier to scale. 3. CLIMB: CLustering-based Iterative Data Mixture Bootstrapping Our work focuses on curating training data from massive data source in an automated fashion, specifically tailored to improve the desired tasks or domains. To ensure that the filtered dataset remains relevant to the target domain while maintaining general language modeling and reasoning capabilities, our framework simplifies the data curation process through fully autonomous iterative bootstrapping approach, eliminating the need for manual curation and reducing labor costs. As illustrated in Figure 4, we first cluster documents from the data source in an embedding space to differentiate data across domains. Next, we iteratively optimize the mixture weights using bootstrapping process to progressively enhance the datasets domain relevance. Further details of the two phases are provided in Section 3.1 and 3.2, respectively. 3 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Figure 4 The CLIMB framework overview. Upper section: CLIMB first preprocesses raw data via embedding and clustering it into groups. These clusters serve as the basis for the search space, where mixture is defined as set of weights to combine different clusters. Lower section: CLIMB samples 𝑛𝑘 mixtures in iteration 𝑘, trains proxy models on subset of them, and updates predictor to estimate performance. The predictor prunes mixtures that are likely to perform poorly, so only the most promising mixtures proceed to full proxy training in subsequent iterations. Through progressively refining the search space and eliminating suboptimal candidates, CLIMB converges toward an optimized data mixture and balances general and domain-specific performance without exhaustive manual curation. 3.1. Data Preprocessing To effectively cluster documents belonging to the same domain, we propose clustering them in the embedding space rather than the word space, as this approach promotes deeper semantic alignment among documents within the same cluster. To accomplish this, our framework follows three steps, as shown in Fig. 4 and elaborated below: Text embedding. Given large raw dataset ^𝐷 = {𝐷1, 𝐷2, . . . , 𝐷𝑛} containing 𝑛 documents, we map the documents into an embedding space using an embedding model 𝑀𝑒. The output of the is set of embedding vectors, embedding model 𝐸 = {𝐸1, 𝐸2, . . . , 𝐸𝑛}. Embedding clustering. We then cluster the embeddings using suitable clustering algorithm. For instance, k-means [35] can be used to group them into 𝐾init clusters. To ensure the clusters are as fine-grained as possible for subsequent processing, we prefer to set 𝐾init to relatively large value at this stage, such as 1000. The specific settings are detailed in Section 4.1. Cluster merging. To further improve clustering quality, we perform cluster pruning and merging. Specifically, given 𝐾init clusters, we conduct clusterlevel pruning to remove low-quality clusters, retaining 𝐾pruned high-quality clusters based on modelbased classifiers as the pruning metric. Then we merge the clusters into 𝐾enhanced clusters according to the distance between centroids, where 𝐾enhanced < 𝐾pruned < 𝐾init. The primary goal of merging is to merge similar fine-grained clusters and reduce the number of domains, facilitating the subsequent data mixture process. The entire dataset is reduced to 𝐷 from ^𝐷. The implementation details can be found in Section 4.1. 3.2. Iterative Bootstrapping: Mixture Weight Search Given set of data clusters, the next step is to optimize sampling mixture weights to maximize the desired downstream task performance. We formulate this as bi-level optimization problem and solve it via iterative bootstrapping. Mixture weight search as bi-level opti4 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training mization problem. Given set of data clusters 𝐷 = {𝐷1, 𝐷2, . . . , 𝐷𝑘} and the objective function ℓ(𝛼, 𝜔) with model weights 𝜔 trained with mixture weights 𝛼, which outputs the achievable performance 𝑃 on calibration set, the objective is to identify the optimal mixture weights 𝛼* 𝐴 that maximize the task performance ℓ(𝛼, 𝜔). ℓ𝑣𝑎𝑙(𝛼, 𝜔*(𝛼)) min 𝛼𝐴 s.t. 𝜔*(𝛼) = arg min 𝜔 ℓ𝑡𝑟𝑎𝑖𝑛(𝛼, 𝜔) (1) s.t. 𝑘 𝑖= 𝛼𝑖 = 1, 𝛼𝑖 0 Approximate the objective with task performance predictors. straightforward approach to estimate the objective function ℓ(𝛼, 𝜔) is to train model for each 𝛼 across the entire design space 𝐴. However, this is computationally prohibitive. To address this challenge, we propose using predictor 𝑓𝜃(𝛼) to approximate ℓ(𝛼, 𝜔) based on subset of (mixture weights, performance) pairs, thereby significantly reducing the training cost. In essence, our cluster mixture search can be reformulated as bi-level optimization problem under the above approximation: min 𝛼𝐴 𝑓 (𝛼𝑆) s.t. 𝑓 = arg min 𝑆,𝑓 ℱ 𝑠𝑆 ℒ(𝑓 (𝑠), ℓ(𝑠, 𝑤*)) (2) where ℒ is the loss function for the predictor 𝑓𝜃, ℱ represents the set of all possible approximations to ℓ, and 𝑆 := {𝑆 𝐴 𝑆 𝐶} denotes all configurations that satisfy the sampling budget 𝐶. The value of 𝐶 is directly tied to the total training cost of the proxy models. Iterative bootstrapping to solve Eq. 2. To solve Equation 2, previous methods typically approach this optimization by first uniformly sampling mixture weights from the design space, training model on the corresponding combined datasets, and then learning predictor based on the performance of the trained models. However, we observe that, given fixed training budget, this strategy is limited by the inefficiency of the initial uniform sampling. This inefficiency causes the model to focus excessively on low-quality mixture weights while failing to identify high-quality ones, ultimately leading to suboptimal mixture weights. In light of this, rather than uniformly sampling across the entire space and then fitting the predictor, we propose an iterative approach to evolve both the sampling strategy 𝑆 and the predictor 𝑓𝜃. The rationale behind this method is to guide the predictor to focus more on subspaces with higher-quality weight mixtures, resulting in more accurate predictions under the same training budget. Specifically, this approach can be mathematically formulated as solving the bilevel optimization problem using coordinate descent method that alternates between optimizing the configuration sampling and predictor fitting subroutines, where the iteration 𝑘 can be formulated as: (Sampling) 𝑃 𝑘 = {𝑓𝑘(𝑠)𝑠 𝐴 𝑆𝑘}, 𝑆𝑀 Top 𝑁 ( 𝑃 𝑘), 𝑆𝑘+1 = 𝑆𝑀 𝑆𝑘, (Predictor Fitting) 𝛼* = arg min 𝑓 (𝛼𝑆𝑘+1), 𝛼𝐴 s.t. 𝑓𝑘+1 = arg min 𝑓𝑘 ℱ 𝑠𝑆𝑘+1 ℒ(𝑓 (𝑠), ℓ(𝑠, 𝜔*)) (3) (4) 𝑁 ( 𝑃 𝑘) represents the set of the top 𝑁 conwhere Top figurations, ranked according to the task performance 𝑃 𝑘. In contrast, existing methods [36] can be seen as running the above coordinate descent process for only single iteration, which is special case of our more general framework. Implementation. The above coordinate descent solution is intuitive and straightforward to implement. Suppose that the iterative method consists of 𝐾 iterations. Initialize 𝑆1 by randomly sampling few configurations from 𝐴 and training proxy models to obtain their performance. Then, for iterations 𝑘 = 2, . . . , 𝐾, jointly optimize the sampling set 𝑆𝑘 and the predictor 𝑓 𝑘 𝜃 in an alternating manner: Subroutine 1: Configuration sampling. At iteration 𝑘 + 1, sort all configurations in the weight space 𝐴 (excluding those already in 𝑆𝑘) according to their predicted performance 𝑃 𝑘. Next, randomly sample 𝑀 new configurations from the top 𝑁 ranked configurations based on 𝑃 𝑘 in order to balance exploitation and exploration. These newly sampled configurations, combined with 𝑆𝑘, form 𝑆𝑘+1. Subroutine 2: (Weak) predictor fitting. Train by minimizing the loss ℒ using the predictor 𝑓 𝑘+1 sampled configurations in 𝑆𝑘+1. The learned predictor 𝑓 𝑘+1 is then used to evaluate the configurations 𝜃 and generate the predicted performance 𝑃 𝑘+1. 𝜃 By alternating between these two procedures for predefined number of iterations, one progressively refines the predictors and guides the sampling process toward subspaces with higher-quality mixture weights, thereby increasing the average quality of the searched mixture weights. At the same time, the promising samples in 𝑆𝑘+1 improve the prediction accuracy of 5 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training 𝜃 the updated predictor 𝑓 𝑘+1 for high-performing configurations, allowing for more accurate assessment of the sampled configurations quality. Finally, one selects the best configuration predicted by the final predictor as the final data mixture weight. For implementation, the predictor can be any regression model, such as linear regression, ridge regression, decision tree regression, or multilayer perceptron. In our experiments, we use LightGBM [37], which predicts the target value by learning an ensemble of decision trees. More implementation details could be found in Section 4.1. 4. Experimental Settings Data. For training, we use Nemotron-CC [8] and smollm-corpus [9] as the source dataset. CLIMBclustering yields 21 super-clusters containing 800B tokens. For evaluation, we test on reasoning benchmarks: PIQA [38], ARC_C, ARC_E [39], HellaSwag [40], WinoGrande [41], and SIQA [42]. We optimize using PIQA, ARC_E, and HellaSwag validation data, then evaluate on test sets. LM-Evaluation harness [43] is used, with all datasets in 0-shot setting except MMLU (5-shot) [44, 45]. Model. We first perform phase-1 pre-training to establish solid foundation. Three Transformer decoder-only models (62M, 350M, 1B) are trained with next-token prediction on 10T tokens, similar to [46] (12T tokens). We use the warmup-stable-decay (WSD) learning rate schedule [47], allowing resumption in the stable stage and focusing on data mixing research in the decay stage. For proxy models, we use 62M and 350M for efficiency. For target models, we evaluate all three sizes to assess the approach across scales. For the rest of paper we use the 350M-proxy, ablations with 62M are in the Appendix A.6. Once the optimal data mixture is found, we train the target model on 40B tokens using this mixture and compare performance. Unless stated otherwise, all reported results come from this 40B continuous pre-training. Baselines. We compare our method with (1) Random selection, and state-of-the-art data mixing methods, including (2) DoReMi [16], and (3) RegMix [36]. The details about these baselines are in Appendix A.1.1. 4.1. Implementation Details embedding. Text textstella_en_400M_v5 2, as codes large-scale text with excellent performance. use it efficiently enWe 2https://huggingface.co/NovaSearch/ stella_en_400M_v5 Embedding clustering. We adopt the classic Kmeans clustering algorithm from the FAISS library [48, 49], setting the initial number of clusters 𝐾init to 1000. Cluster merging. We train several fasttext models [50] to evaluate the data quality across four important dimensions - overall quality, educational value, informational value, and advertisement score (1-5) - by annotating 1 million texts with Nemotron-340B [51] with carefully designed prompt template (see Appendix A.8). Then we perform cluster-level pruning based on the fasttext scores, applying relatively loose threshold of 3.0, which results in 240 (i.e., the value of 𝐾pruned) clusters. Finally, we group the clusters according to Euclidean distance threshold of 1.5, resulting in 16 clusters. Iterative bootstrapping. The data mixture search runs for three iterations with 64, 32, and 16 searches in the first, second, and third iterations, respectively. We initialize Dirichlet distribution based on each clusters token count and sample configurations. In each iteration, predictor is trained using both current and past data. For predictor training, we use LightGBM [37] regression model, which fits mix-performance pairs well with limited data [36]. To prevent overfitting, we set L1 and L2 regularization, early stopping, maximum depth of four, and require at least five samples per leaf. The ablation study of the above design choices is in Section 6. Additionally, we employed separate validation set and an early stopping mechanism, halting training after 20 rounds of no improvement. 5. Experimental Results In this section, we will demonstrate the effectiveness of CLIMB. Firstly, we compare the performance of CLIMB with other data mixture methods  (Table 1)  . Then with the optimal data mixture, we train longer and compare the model with stage-of-the-art baseline models. We use general reasoning tasks as the benchmark and 350M proxy model in the main experiment. 5.1. Comparison with Data Mixture Baselines As shown in Table 1, CLIMB outperforms all baseline data mixture methods. For example, with the 350M target model, CLIMB achieves an average accuracy of 54.83%, outperforming Random (52.17%) and the best-performing baseline, Regmix (53.78%). Similarly, for the 1B model, CLIMB achieves an average accuracy of 60.41%, higher than all baselines. Although the optimization objective is confined to the validation sets of PIQA, ARC_E, and HellaSwag, we observe 6 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Table 1 Comparison with data mixture methods. All models are continuously trained on the same number of tokens (40B). The best results are highlighted in bold. Base refers to the model before training and serves as the starting point for all other models. We report perplexity for wiki and lambda, accuracy for arc_e, winogrande, siqa, accuracy_norm for piqa, arc_c, hellaswag. Size Model Proxy wiki lambda piqa arc_c arc_e hellaswag winogrande siqa avg. 350M 1B Base Random Doremi RegMix CLIMB Base Random Doremi RegMix CLIMB - - 22.70 20.92 350M 19.41 350M 20.93 350M 19.67 - - 17.79 17.82 350M 15.78 350M 16.19 350M 15.96 8.87 9.85 10.39 10.32 9.29 6.65 6.53 6.33 6.62 6. 28.11 70.03 30.54 71.16 33.53 70.29 71.92 33.42 72.21 34.87 34.92 73.89 37.12 74.05 40.01 74.91 75.22 40.42 75.78 40.98 56.12 62.50 66.41 66.12 67.25 66.77 70.24 72.34 71.32 72.97 51.16 52.14 52.25 53.69 55.32 62.12 62.90 63.53 64.73 66. 54.48 55.40 55.95 55.27 56.79 59.82 60.77 61.08 62.33 63.32 50.11 40.75 52.17 41.29 53.38 41.86 42.23 53.78 42.54 54.83 56.46 41.26 57.93 42.48 59.16 43.09 42.22 59.37 43.37 60.41 Table 2 Comparison with state-of-the-art language models on general reasoning benchmarks. CLIMB is continuously trained on 400B tokens with the optimal data mixture. The best results are highlighted in bold. Model Size piqa arc_c arc_e hellaswag winogrande siqa mmlu obqa boolq race lambda truthfulqa Avg. 32.42 36.00 35.07 Qwen2.5 SmolLM 490M 69.96 360M 71.49 CLIMB (Ours) 350M 72.52 1.1B 73.29 30.12 1.2B 75.63 33.70 36.26 1.2B 74.59 60.31 65.95 65.49 CLIMB (Ours) 950M 75.46 40.96 73.57 TinyLlama AMD-OLMo Llama-3. 64.60 70.08 67.38 52.14 53.52 56.27 59.19 63.61 63.67 66.90 56.59 56.75 57.93 59.12 61.64 60.69 63.54 44.22 41.20 42. 33.03 32.98 33.28 35.20 37.60 36.60 62.29 55.29 62.29 34.93 34.74 33.39 52.51 45.76 52.62 36.00 40.63 31.60 35.80 44.17 31.92 42.99 37.20 35.40 43.55 36.47 41.20 66.02 36. 58.84 36.46 57.83 60.58 59.31 34.64 63.98 37.80 62.99 59.05 39.74 37.93 36.86 37.60 32.22 37.67 39.06 48.14 47.78 48.93 48.42 49.93 51.56 53.54 that the resulting performance gains carry over to all the benchmark tasks. This clearly demonstrates the robust generalization ability of our approach, indicating that optimizing on limited set of core tasks can effectively capture and transfer essential reasoning capabilities across broader range of problems. 5.2. Comparison with SOTA LMs Using the optimal data mixture identified by our method, we further investigate the effect of scaling up. Specifically, we used the same data mixture to train on 400B tokens and then compared the resulting model against state-of-the-art baselines. As shown in Table 2, CLIMB achieves the best performance among all sub-500M and sub-1.2B models. For example, when comparing models of similar scales (around 1B parameters), CLIMB consistently outperforms other baselines including Llama-3.2 and AMD-OLMo across the majority of the general reasoning benchmarks. In particular, it achieves the highest overall average score, surpassing the next-best method (i.e., Llama-3.2) by noticeable margin (2.0%). Moreover, we introduced additional benchmarks (e.g., mmlu, gpqa, obqa, boolq, and race), and our model is consistently better than baseline models, which demonstrates that our method exhibits excellent generalization performance. Takeaway Iteratively refined data mixtures lead to better pre-training performance. 6. Analysis In this section, we present the analysis and discussion about some important factors and designs behind CLIMB and demonstrate CLIMB is robust data mixing method. Optimizating towards Specific Domains. In addition to optimizing towards general reasoning tasks, one important application of CLIMB is developing domain-specialist model. We explore searching the optimal data for specific domains. Using the MMLU as an example, which has pre-defined three domains: STEM, humanities, and social-sciences and divided tasks into these domains, we conduct experiments on each domain separately. We set the optimization objective as the performance on its validation set. Here, we introduce new baseline: CLIMBBest@N, which directly searches for the best parameters on randomly sampled configs using the target model. Note that to ensure the same search compute in the table, the number of searches is reduced for the 1B model. As shown in Figure 5, the CLIMB-Best@N shows noticeably better accuracy than Random across 7 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Figure 5 Performance of target models on MMLU benchmarks for different subject areas. For both 350M and 1B target models, CLIMB used 350M proxy models, whereas CLIMB-Best@N used proxy models of the same size as the target models. CLIMB consistently improves performance across iterations, outperforming CLIMB-Best@N despite using smaller proxy models. Table 3 Ablation study with 1B target model trained on 40B tokens. Proxy Comp. piqa arc_c arc_e hellaswag winogrande siqa Avg. Setting Abl.comp Abl.allo Abl.proxy Abl.clus Model CLIMB CLIMB CLIMB CLIMB CLIMB CLIMB CLIMB CLIMB CLIMB 48-21cluster 64-21cluster 100-21cluster 1000-21cluster 2000-21cluster 1000-15cluster 1000-30cluster 350M 100% 75.78 350M 150% 76.23 350M 200% 76. 6:1 350M 350M 4:2:1 350M 2:2:1:1 75.32 75.78 75.36 62M 100% 75.41 132M 100% 75.56 350M 100% 75.78 350M 100% 75.89 350M 100% 75.87 350M 100% 76.13 350M 100% 75.78 350M 100% 75.37 350M 100% 75.94 350M 100% 76.03 Abl.init Random Dirichlet 350M 100% 75.42 350M 100% 75.78 40.98 41.28 42.31 40.80 40.98 40.77 40.56 40.93 40.98 39.91 40.34 40.73 40.98 41.33 41.33 40.49 40.12 40. 72.97 73.16 73.41 72.91 72.97 72.88 72.82 72.94 72.97 71.92 72.44 72.57 72.97 72.47 73.34 72.66 72.47 72.97 66.01 66.41 66. 65.51 66.01 65.86 65.76 65.57 66.01 65.87 65.39 66.13 66.01 65.79 66.28 65.78 65.73 66.01 63.32 63.53 63.70 62.84 63.32 62. 63.23 63.09 63.32 63.21 63.14 63.39 63.32 63.46 63.62 63.45 64.27 63.32 43.37 60.41 43.71 60.72 43.99 61.12 42.93 60.05 43.37 60.41 43.02 60.14 42.89 60.11 43.07 60.19 43.37 60. 42.62 59.90 43.55 60.12 43.70 60.44 43.37 60.41 42.99 60.24 43.05 60.59 43.12 60.25 43.22 60.21 43.37 60.41 all three domains, demonstrating the superiority of data searching. This establishes robust baseline for comparison. In contrast, our proposed CLIMB methods consistently improve performance across iterations. For instance, in the 350M model, CLIMB-iter3 achieves accuracies of 28.67%, 29.56%, and 39.36% in STEM, Humanities, and Social Sciences, respectively, significantly outperforming both Random and CLIMB-Best@N. Similarly, in the 1B model, CLIMBiter3 achieves Social Sciences accuracy of 41.79%, surpassing CLIMB-Best@N by 1.13%. These results highlight the broad applicability of our approach to models of varying sizes. In addition, we can see clear improvement over each iteration. For example, from CLIMB-iter1 to CLIMB-iter3, the performance is improved from 40.18% to 41.79% on mmlu-socialsciences. Effects of Search Compute Budget. In the main experiments, we fix our total search budget (total compute) at 100%. Concretely, we perform three iterations of search with 64, 32, and 16 candidates evaluated in iterations 1, 2, and 3, respectively, giving total of 112 searches. To understand how scaling search computes helps, we compare runs with greater total numbers of searches (e.g., 168, 224). Increasing the total number of searches allows the search procedure to more thoroughly explore possible data-mixture candidates each iteration. As shown in Table 3 (rows under Abl.comp), we observe trend that more extensive searches (e.g., 150% or 200%) continue to offer gains. This confirms our intuition that more exhaustive data-mixture optimization can further boost downstream accuracy when sufficient compute is available. Effects of Compute Allocation. By default, we allocate our 100% total compute across three iterations in 4:2:1 ratio (64:32:16). In principle, however, one could allocate compute to create either 8 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training tall search tree (more iterations but fewer searches per iteration) or fat one (fewer iterations but more searches per iteration). Table 3 (rows under Abl.allo) compares several such allocations: 6:1, 4:2:1, and 2:2:1:1. We find that 4:2:1 yields the best overall average performance (60.41%). Having too few iterations (e.g., 6:1) can lead to suboptimal exploration in earlier iterations, while splitting too many iterations (2:2:1:1) spreads compute too thin across each iteration. Thus, balancing depth (number of iterations) and breadth (searches per iteration) proves key to robustly finding good mixture. Takeaway More search iterations improve performance, but compute should be balanced between depth and breadth. 150%-200% compute increase yielded noticeable gains. Effects of Proxy Model. Our method relies on proxy model to rapidly score candidate mixtures. Intuitively, larger proxy models should better approximate the performance of the final (larger) target model. We test three proxy sizes: 62M, 132M, and 350M parameters. From Table 3 (rows under Abl.proxy), as we increase the proxy model from 62M to 350M, the average score improves from 60.11 to 60.41. Although the gains are not dramatic, they consistently favor using the largest feasible proxy model. This shows that stronger proxycloser in capacity to the targetachieves more accurate gradient estimates of mixture quality. Effects of Number of Clusters. In our method, we employ hierarchical clustering procedure. Specifically, we first group all data into 𝐾𝑖𝑛𝑖𝑡 clusters, perform filtering step, and then regroup these clusters into 𝐾𝑒𝑛ℎ𝑎𝑛𝑐𝑒𝑑 super-clusters. In this section, we explore the robustness of our data-mixture method and investigate its sensitivity to the number of clusters. Hence, we experiment with different values of 𝐾𝑖𝑛𝑖𝑡 (48, 64, 100, 1000, 2000) and 𝐾𝑒𝑛ℎ𝑎𝑛𝑐𝑒𝑑 (15, 21, 30). The results in Table 3 (rows under Abl.clus) show that performance improves as 𝐾𝑖𝑛𝑖𝑡 increases from 48 to 100 and declines when 𝐾𝑖𝑛𝑖𝑡 increases from 1000 to 2000. Overall, our method is not particularly sensitive to the number of clusters, demonstrating the robustness of our approach. It is worth mentioning that if 𝐾𝑖𝑛𝑖𝑡 exceeds 2000 given the dataset size, the clustering becomes overly fine-grained and thus too dispersed. Likewise, if 𝐾𝑒𝑛ℎ𝑎𝑛𝑐𝑒𝑑 is set too high, it requires more compute for sampling, increasing the overall cost of the data search process. Effects of Initialization. We compare how different initialization schemes for the mixture weights affect performance. We experiment with simple random initialization versus Dirichlet-based initialization that biases weights to be more evenly spread at the start. Table 3 (rows under Abl.init) shows that Dirichlet initialization achieves slightly higher average score (60.41%) than random (60.21%). The performances are comparable, suggesting the robustness of our data mixing approach, which is largely insensitive to the choice of initialization. Evolution of Cluster Weights. The data mixture weights are important to understand the impact of different clusters, so we closely examine how they evolve across iterations. Figure 8 (a) presents the weights discovered by our search process for the 350M proxy model in the general reasoning domain. As shown, most clusters have minimal or no contribution (weights close to 0.00), while few clusters play significant role, with their weights changing across iterations. Among them, C18, C19, and C21 initially have high weights, but C19 and C21 exhibit decreasing trend, suggesting their diminishing impact. Conversely, C8 and C9 become more relevant in later iterations, with their weights increasing in Iteration 3 (C8: 0.13, C9: 0.18), highlighting an adaptation in feature importance. More discussion about the evoluation of different cluster weights are shown in Appendix A.4. Analysis of Final Weights Furthermore, we analyzed the weights of the final data mixtures. From Figure 8 (a) , for the general reasoning task, C8, C9, C18, and C19 account for the majority of the weight. As shown in A.3, C8, C9, and C19 exhibit high degree of correlation with general reasoning. Moreover, when analyzing the topics of these four clusters (4), we find that they collectively form diverse distribution. In addition, we analyzed the importance of different clusters across domains on MMLU. As shown in Figures 8 (b), (c)), and (d), certain clusters play crucial role in specific domains. For example, C7, C11, and C19 are particularly important for the humanities domain, while C7 and C8 are highly influential in the STEM domain. These findings highlight how different clusters contribute uniquely to various domains, providing deeper insights into domain-specific feature significance. We are also curious about the similarities and differences in the weights discovered by the large proxy model and the small proxy model. To explore this, we compared Figure 8 (a) and (e) and observed that they share similar important features, such as C8, C9, C18, and C19, although the assigned weights vary between the models. This insight suggests that we can leverage smaller 62M proxy model for further experiments, reducing computational costs while retaining key structural patterns. The experimental results are presented in Appendix A.6. Notably, the weights appear sparse because, during the samCLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Figure 6 Weight analysis of ClimbMix across iterations. allowing the model to focus primarily on learning few important domains, whereas pre-training from scratch necessitates more diverse data coverage. Finally, we publicly release these two datasets: the filtered 1.2-trillion-token dataset organized into 20 semantic clusters as research playground for further data-mixture studies, and the optimized 400-billiontoken ClimbMix dataset for efficient pre-training. 8. Conclusion This work introduces CLIMB, novel clustering-based iterative mixture bootstrapping framework for optimizing data mixture for pre-training LLMs. CLIMB automates the discovery, evaluation, and refinement of data mixtures, improving large-scale pre-training with explicit targets. By leveraging unsupervised clustering, proxy model training, and predictor, CLIMB efficiently navigates the vast search space of data compositions, enabling the construction of optimal domain-aware mixtures without reliance on predefined domain labels or extensive manual curation. By training 350M and 1B models with the optimal data mixture searched by CLIMB, we achieve stateof-the-art performance across 12 reasoning tasks. Our experiments demonstrate that intelligently balancing unstructured corpora with targeted domain data leads to significant performance gains under fixed computational budgets. Compared to conventional static mixing strategies, our iterative approach allows for dynamic refinement, preserving general capabilities while excelling in specialized domains. Our findings underscore the potential of data-driven optimization techniques in enhancing LLM efficiency, advancing domain-specialized training. pling process, we intentionally bias towards sparse weights. This approach effectively amplifies important clusters while filtering out less significant ones, enhancing the clarity of key features. In addition, we also investigate the relationship between clusters and downstream task performance in A.3. Takeaway Both the relevance of cluster content to downstream tasks and the diversity among different clusters are crucial for achieving effective data mixtures and robust model performance. 7. ClimbMix: New Pre-training"
        },
        {
            "title": "Data",
            "content": "Based on the insights obtained from our explorations above, we further apply CLIMB to two existing datasets: Nemotron-CC [8] and smollm-corpus [9], with the goal of constructing powerful new pretraining dataset. Specifically, we first combine Nemotron-CC and smollm-corpus, and then employ our proposed CLIMB-clustering method to semantically reorganize and filter this combined dataset into 20 distinct clusters, leading to 1.2-trillion-token high-quality corpus, named ClimbLab. Subsequently, we utilize CLIMB-search to identify an optimal data mixture from these clusters. Using this optimal mixture, we further extract 400-billion-token high-quality dataset named ClimbMix. We train 1B model from scratch with ClimbMix and evaluate its performance relative to models pretrained on other datasets under the same token budget. The results, illustrated in Figure 1, indicate that models trained on ClimbMix significantly outperform those trained on existing datasets. The optimal data mixture weights identified by CLIMB is shown in Figure 6. We note that in the previous continuous pre-training setting, few domains accounted for the majority of the weight. However, since the experiments here are conducted under pre-training-from-scratch setting, more balanced cluster distribution is required compared to continuous pre-training. This difference arises because continuous pre-training provides strong foundation, 10 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training"
        },
        {
            "title": "References",
            "content": "[1] Cody Blakeney, Mansheej Paul, Brett Larsen, Sean Owen, and Jonathan Frankle. Does your data performance gains from domain upspark joy? sampling at the end of training. arXiv preprint arXiv:2406.03476, 2024. [2] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [3] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Evaluating large lanGreg Brockman, et al. guage models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. [6] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. [7] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [8] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. [9] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus, 2024. [10] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [12] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [13] Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 55475569. PMLR, 2022. [14] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:3180931826, 2022. [15] Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, et al. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818, 2023. [16] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2024. [17] Simin Fan, Matteo Pagliardini, and Martin Jaggi. Doge: Domain reweighting with generalization estimation. arXiv preprint arXiv:2310.15393, 2023. [18] Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher Ré. Skill-it! data-driven skills framework for understanding and training language models. Advances in Neural Information Processing Systems, 36, 2024. [19] Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. Organize the web: Constructing domains enhances pre-training arXiv preprint arXiv:2502.10341, data curation. 2025. 11 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training [20] Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ju Fan, et al. Harnessing diversity for important data selection in pretraining large language models. arXiv preprint arXiv:2409.16986, 2024. [21] Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient pretraining with data influence models. Advances in Neural Information Processing Systems, 37:108735108759, 2024. [22] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. Qurating: Selecting high-quality In Forty-first data for training language models. International Conference on Machine Learning. [23] David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan Richard Schwarz, and Sham Kakade. Color-filter: Conditional loss reduction filtering for targeted language model pre-training. Advances in Neural Information Processing Systems, 37:9761897649, 2024. [24] Sebastian Ruder and Barbara Plank. Learning to select data for transfer learning with bayesian optimization. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2017. [25] Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. Efficient continual pre-training for building domain specific large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 1018410201, 2024. [26] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:3420134227, 2023. [27] David Grangier, Simin Fan, Skyler Seto, and Pierre Ablin. Task-adaptive pretrained language models via clustered-importance sampling. In The Thirteenth International Conference on Learning Representations, 2025. [28] Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar Sahu, and Ruoxi Jia. Get more for less: Principled data selection for warming up fine-tuning in LLMs. In The Twelfth International Conference on Learning Representations, 2024. [29] Yu Yang, Siddhartha Mishra, Jeffrey Chiang, and Baharan Mirzasoleiman. Smalltolarge (s2l): Scalable data selection for fine-tuning large language models by summarizing training trajectories of small models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [30] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: Selecting influential data for targeted instruction tuning. In Forty-first International Conference on Machine Learning, 2024. [31] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023. [32] Yu Yang, Aaditya Singh, Mostafa Elhoushi, Anas Mahmoud, Kushal Tirumala, Fabian Gloeckle, Baptiste Rozière, Carole-Jean Wu, Ari Morcos, and Newsha Ardalani. Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data. arXiv preprint arXiv:2312.02418, 2023. [33] Aaditya Singh, Yu Yang, Kushal Tirumala, Mostafa Elhoushi, and Ari Morcos. Brevity is the soul of wit: Pruning long files for code generation. arXiv preprint arXiv:2407.00434, 2024. [34] Kashun Shum, Yuzhen Huang, Hongjian Zou, Ding Qi, Yixuan Liao, Xiaoxin Chen, Qian Liu, and Junxian He. Predictive data selection: The data that predicts is the data that teaches. arXiv preprint arXiv:2503.00808, 2025. [35] John Hartigan and Manchek Wong. Algorithm as 136: k-means clustering algorithm. Journal of the royal statistical society. series (applied statistics), 28(1):100108, 1979. [36] Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024. [37] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017. [38] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [39] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question try arc, the ai2 reasoning challenge. answering? arXiv:1803.05457v1, 2018. [40] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [41] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. 12 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training [42] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [43] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. [44] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. Smollm - blazingly fast and remarkably powerful, 2024. [45] Cosmopedia/evaluation. Cosmopedia/evaluation at main huggingface/cosmopedia. [46] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. [47] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [48] Jeff Johnson, Matthijs Douze, and Hervé Jégou. IEEE Billion-scale similarity search with GPUs. Transactions on Big Data, 7(3):535547, 2019. [49] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. 2024. [50] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016. [51] Bo Adler, Niket Agarwal, Ashwath Aithal, Dong Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. [52] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2021. [53] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training computeoptimal arXiv preprint arXiv:2203.15556, 2022. large language models. [54] Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36:50358 50376, 2023. [55] Sachin Goyal, Pratyush Maini, Zachary Lipton, Aditi Raghunathan, and Zico Kolter. Scaling laws for data filteringdata curation cannot be compute agnostic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2270222711, 2024. [56] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 13 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training A. Appendix A.1. Experimental Settings A.1.1. Baselines We compare our method with state-of-the-art data mixture methods, including DoReMi [16], and RegMix [36]. Random: randomly select data for language model training, where each cluster is assigned an equal and uniform weight. DoReMi [16]: method that trains small proxy model with group distributionally robust optimization (Group DRO) to determine domain weights for pre-training data, which are then used to resample the dataset and train larger model more efficiently. RegMix [36]: an approach that performs single round of data mixture configuration search by sampling configurations and training the model on each configuration to obtain config-performance pairs. We then train regression model to predict the optimal data mixture weights. This method can be regarded as an extension of RegMix [36], using much larger proxy model and larger cluster data labeled by our clustering approach. A.1.2. Data For the source data, we employ Nemotron-CC [8] large dataset filtered from Common Crawl. It divides all the data into 20 buckets based on data quality annotation and we use the subset from the highest-quality bucket that consists of around 500 billion tokens. The hierarchical clustering of this subset results in 16 super-clusters (Cluster 1 to 16). To improve on data realism, we further incorporate five existing high-quality clusters. Three of these are sourced from the smollm-corpus [9]: Cosmopedia (Cluster 18), FineWeb-Edu (Cluster 19), and Python-Edu (Cluster 20). The remaining two clusters (Cluster 17 and Cluster 21) consist of additional academic data. In total, the source dataset consists of approximately 800 billion tokens distributed across 21 clusters. For the downstream evaluation tasks, we conduct experiments on general reasoning benchmarks including PIQA [38], ARC_C [39], ARC_E [39], HellaSwag [40], WinoGrande [41], TruthfulQA [52], and SIQA [42]. In this setting, we optimize the model using the validation data of specific tasks and evaluate it on test data from different tasks. For optimization, we use the validation data of only PIQA, ARC_E, and HellaSwag and evaluate the model on the test sets of all these datasets. We use LM-Evaluation harness [43] for evaluation. Following the setup in [44, 45], except for MMLU, which is evaluated using 5-shot setting, all other datasets are evaluated using 0-shot setting. A.1.3. Model Firstly, we conduct phase-1 pre-training to provide good foundation for all of the following experiments. We train three sizes (62M, 350M, and 1B) of standard Transformer decoder-only models with the next-token language modeling loss. All of them are trained on 10 trillion tokens, similar to [46] that trained for 12T tokens. We acknowledge that this over-training does not strictly align with scaling laws [53, 54, 55]. However, since it does not hurt performance, we chose to train on the same amount of data. This practice has also been adopted in some recent models; for example, Qwen-2 [46] utilized 12 Trillion tokens of data to train their 500M model. We use the warmup-stable-decay (WSD) learning rate schedule [47] because it supports resuming at any time of the stable stage and we could focus on the data mixing research in the decay stage. For the proxy model, we choose 62M and 350M to conduct experiments, as these sizes are computationally efficient for exploring data mixture configurations. For the target model, we conduct experiments on all three sizes (62M, 350M, and 1B) to comprehensively evaluate the impact of our approach across different model scales. After we identify the optimal data mixture, we continue to train the target model on 40B tokens using this new mixture and then compare its performance. Unless otherwise noted, all reported results are obtained from this 40B continuous pre-training. 14 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training A.2. Topics of Clusters To gain deeper understanding of the topics covered in each cluster, we conducted an analysis by extracting the topics with GPT-4o [56]. Specifically, we randomly sampled 100 documents from each cluster and employed GPT-4o to summarize the most representative topics within them. The model was instructed to identify the four to seven most relevant topics for each cluster, ensuring concise yet comprehensive characterization. We also recognize that this approach can only provide auxiliary explanations; our goal is to facilitate the understanding of the internal structure of each cluster rather than to make definitive conclusions through topic analysis. Cluster-ID Topics Table 4 Topics of clusters. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Environment, Public Health, Policy Development, Medical Innovation Technology, Neurophysiology, Health and Safety, Innovative Research, Rehabilitation Restoration Efforts, Climate and Ecosystem, Community Engagement Diagnostics, Diseases, Prevention and Control Vehicles, Ecology, Community, Conservation Efforts Energy, Science, Materials, Nanostrctures, Quantum Computing Physics, Accelerators, Materials, Architecture, System Biology, Genetics, Astronomy, Climate Science Earth Sciences, Space Science, Scientific Collaboration Health, Symptoms, Treatment, Therapy, Disorders, Conditions Communication, Biography, History, Society, Policy Culture, Education, Sustainability, Community, Public Health, Crime, Economy Arts, Literature, Education, History Geography, Government, Organization, Religion, Agriculture, Economy, Civilizations Science, Technology, Education, Engineering, Collaboration Science, Health, Minerals, Population, Agriculture, Vaccination, Welfare, Management Role-Playing, Problem Solving, Mathematics, Algorithms Revolution, Parliament, Efficiency, Communication, Animal Behavior History, Culture, Economy, Energy, Market, Policy Python, Code Government, Law, Scientific Revolution, Music, Literature A.3. Relationship between Clusters and Downstream Tasks In this section, we analyzed the relationship between clusters and downstream task performance. First, we visualized the similarity between each cluster and downstream tasks in Figure 7, where cosine similarity is measured using the average embedding of each cluster. We use arc-e to represent the general reasoning domain. Our key observations are as follows: (1) in-domain data enhances downstream performance. Take the general reasoning as an example, as shown in Figure 7, Clusters C8 and C19 share the most similar distribution with arc-e and indeed they contribute lot to the final mixture weights. (2) out-of-domain data are also useful. From the results of general reasoning, as our search process iterates, we find that while C21 is highly similar, it provides limited benefits to downstream performance, leading to gradual decrease in its importance. Conversely, C8 initially appears out-of-domain but becomes increasingly important with further iterations. (3) domain contribution is complex: While similarity can serve as an indicator of clusters importance, it is not always decisive factor. For instance, in mmlu-stem, the most similar cluster is C7, and as shown in Figure 8 (d), it plays crucial role, contributing 36% of the weight. However, C8, despite having lower similarity score, has an even higher weight contribution (61%). From this analysis, we observe that highly similar data can sometimes enhance downstream task performance. However, using only in-domain data does not necessarily lead to optimal performance. Distribution similarity alone is not sufficient condition for importancethat is, cluster or domain being similar to downstream task does not inherently guarantee performance improvement. This is because data mixture involves complex interactions among different clusters. In some cases, when clusters are highly similar, incorporating only one of them may 15 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training suffice. This highlights the intricate interplay within data mixtures, suggesting that optimal selection requires more than just similarity-based filtering. Figure 7 Similarity between clusters and downstream tasks. Figure 8 Heatmap of weights across iterations. CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training A.4. Evolution of Cluster Weights The data mixture weights are important to understand the impact of different clusters, so we closely examine how they evolve across iterations. Figure 8 (a) presents the weights discovered by our search process for the 350M proxy model in the general reasoning domain. As shown, most clusters have minimal or no contribution (weights close to 0.00), while few clusters play significant role, with their weights changing across iterations. Among them, C18, C19, and C21 initially have high weights, but C19 and C21 exhibit decreasing trend, suggesting their diminishing impact. Conversely, C8 and C9 become more relevant in later iterations, with their weights increasing in Iteration 3 (C8: 0.13, C9: 0.18), highlighting an adaptation in feature importance. A.5. Analysis of Final Weights Furthermore, we analyzed the weights of the final data mixtures. From Figure 8 (a) , for the general reasoning task, C8, C9, C18, and C19 account for the majority of the weight. As shown in A.3, C8, C9, and C19 exhibit high degree of correlation with general reasoning. Moreover, when analyzing the topics of these four clusters (4), we find that they collectively form diverse distribution. In addition, we analyzed the importance of different clusters across domains on MMLU. As shown in Figures 8 (b), (c)), and (d), certain clusters play crucial role in specific domains. For example, C7, C11, and C19 are particularly important for the humanities domain, while C7 and C8 are highly influential in the STEM domain. These findings highlight how different clusters contribute uniquely to various domains, providing deeper insights into domain-specific feature significance. We are also curious about the similarities and differences in the weights discovered by the large proxy model and the small proxy model. To explore this, we compared Figure 8 (a) and (e), and observed that they share similar important features, such as C8, C9, C18, and C19, although the assigned weights vary between the models. This insight suggests that we can leverage smaller 62M proxy model for further experiments, reducing computational costs while retaining key structural patterns. The experimental results are presented in Appendix A.6. Notably, the weights appear sparse because, during the sampling process, we intentionally bias towards sparse weights. This approach effectively amplifies important clusters while filtering out less significant ones, enhancing the clarity of key features. In addition, we also investigate the relationship between clusters and downstream task performance in A.3. A.6. Experiments with 62M proxy model In the main experiment, we used 350M proxy model. To further investigate the effectiveness of smaller proxy models, we conducted additional experiments with reduced model sizes. The results, presented in Tables 5 and 6 , indicate that even when the proxy model size was reduced by factor of five, its performance remained strong. This suggests that smaller proxy models can still be highly effective, providing valuable insights while reducing computational costs. 17 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Table 5 Performance of target models on MMLU-social-sciences task. The main proxy model is 62M. Target Model Proxy Accuracy (%) 62M 350M 1B Random CLIMB-Best@N CLIMB-iter1 CLIMB-iter2 CLIMB-iter Random CLIMB-Best@N CLIMB-iter1 CLIMB-iter2 CLIMB-iter3 Random CLIMB-Best@N CLIMB-iter1 CLIMB-iter2 CLIMB-iter3 - 62M 62M 62M 62M - 350M 62M 62M 62M - 1B 62M 62M 62M 27.40 31.03 29.05 30.71 32. 34.87 38.39 36.09 37.01 37.98 36.69 40.66 40.03 40.46 41.72 Table 6 Performance of target models on general reasoning benchmarks. The main proxy model is 62M. Proxy Comp. piqa arc_c arc_e hellaswag winogrande siqa Avg. Size 62M 350M Model Random - CLIMB-Best@N 62M 62M 62M 62M CLIMB-iter1 CLIMB-iter2 CLIMB-iter 0 61.80 100% 63.16 57% 63.92 85% 64.09 100% 64.54 0 Random 71.16 - CLIMB-Best@N 350M 100% 71.92 57% 71.65 62M 85% 71.54 62M 100% 71.87 62M CLIMB-iter1 CLIMB-iter2 CLIMB-iter Random CLIMB-Best@N CLIMB-iter1 CLIMB-iter2 CLIMB-iter3 - 1B 62M 62M 62M 0 74.05 100% 75.02 57% 74.38 85% 75.26 100% 75.41 1B 24.06 25.51 24.82 26.10 27. 30.54 33.70 33.49 34.01 34.12 37.12 38.39 38.19 39.28 40.56 45.70 51.30 49.83 49.83 53.39 62.50 67.00 65.31 66.43 66.92 70.24 72.34 70.98 72.17 72.82 33.64 35.68 34.76 35.95 35. 52.14 54.55 54.44 54.61 54.81 62.90 64.31 64.21 63.99 65.76 50.19 51.14 49.48 51.06 51.15 55.40 56.59 56.28 56.78 56.11 60.77 61.16 61.58 63.16 63.23 37.51 41.76 38.07 44.14 38.79 43.60 38.68 44.29 39.50 45. 41.29 52.17 41.67 54.24 41.99 53.86 41.37 54.12 42.37 54.37 42.48 57.93 42.52 58.96 43.11 58.74 41.27 59.19 42.89 60.11 A.7. Effects of Predictor In our approach, after training the proxy model on configuration-performance pairs, we use regression model (i.e., predictor) to capture the relationship between configuration and target domain performance. To evaluate prediction accuracy, we hold out portion of the data as the test set and compute the Spearman rank correlation between the predictions and ground truth. As shown in Figure 9, we visualize the predicted and true accuracy pairs for the 350M proxy models and find that the predictor performs exceptionally well, achieving 94% Spearman rank correlation. 18 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Figure 9 The Spearman rank correlation between predicted accuracy made by the predictor model and the groundtruth accuracy. A.8. Prompt Template We present the prompts used for data annotation, as shown in the table below. 19 CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training Evaluation Criteria for Pre-Training Data You are an expert evaluator assessing text for suitability as pre-training data for large language model. For each criterion, start from 0 points. Then add points based on the conditions described. If no conditions are met, the score remains 0. Please evaluate the given text using the rating scale below. Assign score from 0 to 5 for each criterion, and reference the expanded guidelines under each category to determine the appropriate rating: Rating Scale: 0: Does not meet the criterion at all 1: Partially meets the criterion 2: Fairly meets the criterion 3: Mostly meets the criterion 4: Fully meets the criterion 5: Exceeds the criterion Criteria and Expanded Guidelines: 1. Quality: The text is natural, clean, and free from severe grammatical errors, spelling mistakes, syntactical issues, repetitive phrasing, or random symbols. +1: Correct basic spelling and mostly proper grammar, despite minor slips. +1: Coherent sentence structures, no glaring syntactical breakdowns. +1: Natural language, free from repetitive phrasing, easy to read. +1: Polished, no major grammatical errors or spelling mistakes. +1: Professional-level writing quality, free from unnatural phrasing. 2. Advertisement: The text should avoid excessive promotional language or overt advertising. +1: Minimal promotional elements, not distracting. +1: Subtle promotional aspects, not overshadowing content. +1: Mostly neutral with slight marketing-like language. +1: Almost free from advertisements, at most one mild reference. +1: No detectable promotional content. 3. Informational Value: The text provides accurate insights, useful facts, or relevant knowledge. +1: At least one accurate fact or relevant information. +1: Multiple useful pieces of information. +1: Enhances understanding, presents explanations. +1: Substantial, well-structured, reliable information. +1: Exceptional depth, authoritative content. 4. Educational Value: Assess if the text is beneficial for structured learning. +1: Basic educational relevance, even if mixed with non-academic content. +1: Addresses education but lacks strong alignment with standards. +1: Suitable for educational use, introduces key concepts. +1: Highly relevant for structured learning, minimal extraneous content. +1: Outstanding educational value, clear, easy-to-follow insights. Final Output Format: { \" quality \" : < integer 0 -5 >, \" advertisement \" : < integer 0 -5 >, \" informational_value \" : < integer 0 -5 >, \" educational_value \" : < integer 0 -5 >, } Content to evaluate: INPUT_DOC"
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology, USA",
        "NVIDIA",
        "OpenAI"
    ]
}