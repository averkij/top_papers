{
    "paper_title": "From Features to Actions: Explainability in Traditional and Agentic AI Systems",
    "authors": [
        "Sindhuja Chaduvula",
        "Jessee Ho",
        "Kina Kim",
        "Aravind Narayanan",
        "Mahshid Alinoori",
        "Muskan Garg",
        "Dhanesh Ramachandram",
        "Shaina Raza"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\\times$ more prevalent in failed runs and reduces success probability by 49\\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour. Resources: https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 2 1 4 8 6 0 . 2 0 6 2 : r From Features to Actions: Explainability in Traditional and Agentic AI Systems Sindhuja Chaduvula1 , Jessee Ho1 , Kina Kim2 , Aravind Narayanan1 , Mahshid Alinoori1 , Muskan Garg3 , Dhanesh Ramachandram1 , and Shaina Raza1 1 Vector Institute for Artificial Intelligence, Toronto, Canada {sindhuja.chaduvula, jessee.ho, aravind.narayanan, mahshid.alinoori, dhanesh.ramachandram, shaina.raza}@vectorinstitute.ai 2 Independent Researcher 3Mayo Clinic, Rochester, MN, USA Abstract. Over the last decade, XAI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman ρ = 0.86), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7 more prevalent in failed runs and reduces success probability by 49%. These findings motivate shift towards trajectory-level explainability for agentic systems for evaluating and diagnosing autonomous AI behaviour. Resources: Code (cid:128)Project Page Keywords: Explainable AI Agentic AI Large Language Models Trajectory-level Explainability Tool-use Agents"
        },
        {
            "title": "Introduction",
            "content": "Explainability in Artificial Intelligence (AI) has long struggled with fundamental tension: as models grow more capable, their decision-making processes Corresponding author. shaina.raza@vectorinstitute.ai 2 S. Chaduvula et al. become increasingly opaque. The research community has responded with rich toolkit of post-hoc explanation methods such as SHapley Additive exPlanations (SHAP) [26], Local Interpretable Model-agnostic Explanations (LIME) [34], saliency maps, and attention visualizations that infer model behaviour without modifying the underlying system [16,15]. These techniques are well-suited to static prediction settings, where systems behaviour is evaluated with respect to fixed inputoutput mapping. However, the AI landscape has fundamentally changed. The rise of agentic AI systems [19], particularly LLM-based agents, represents departure from the static, single-inference settings for which most traditional explainability methods were developed. Historically, Explainable AI methods (XAI) have focused on providing post-hoc explanations for individual predictions, typically under the assumption of fixed input-output mapping and single decision point, while interpretability has referred more broadly to models or representations that are transparent by design [33]. In contrast, agentic systems operate through sequences of observations, decisions, and tool invocations that unfold over dozens of steps before producing final outcome [13]. This shift from prediction to action, and from single inference to extended reasoning, challenges core assumptions of traditional XAI , rendering them inadequate for explaining agentic systems behaviour. Problem Statement. In static settings, explanations are commonly framed as attributions over input features or tokens for single prediction. In agentic settings, many critical questions are trajectory-level: why an agent chose tool, abandoned strategy, propagated an incorrect state, or failed to recover after an error. In other words, the unit of explanation is no longer single prediction but trajectory: sequence of states, actions, and observations that collectively determines success or failure. We argue that bridging static and agentic explainability requires re-conceptualizing explanation targets from feature-level influence to trajectory-level decision accounts, and coupling explanation artifacts with execution context and faithfulness signals grounded in the run. Contributions. This paper makes three main contributions: We introduce formal distinction between explainability for static predictors and explainability for agentic systems. We propose cross-paradigm taxonomy of explanation targets and artifacts, from feature-level attributions to trajectory-level accounts. We empirically compare attribution-based explanations and trace-based diagnostics across (i) static classification task and (ii) LLM-agent benchmarks (TAU-bench Airline [45], AssistantBench [25]), analyzing tool use, strategy shifts, and error recovery to motivate agent-centric requirements for actionable explanations. Scope. We focus on explainability for agentic task completion systems, where an LLM-based agent achieves high-level goal through multi-step interaction (tool calls, intermediate reasoning, and state updates). Our emphasis is on posthoc, trace-grounded explanations and diagnostics derived from execution logs. We do not aim to provide mechanistic interpretability of model internals, nor Explainability in Traditional and Agentic AI Systems 3 to propose new agent training algorithms; instead, we study what explanation artifacts are informative and actionable under unified static-vs-agentic framing. Empirical Findings. Our experiments on static classification tasks and agentic benchmarks (TAU-bench Airline [45], AssistantBench [25]) reveal clear contrast between attribution-based and trace-based explanations. For static prediction tasks, SHAP and LIME produce stable feature rankings that capture aggregate correlates of model outputs. For agentic benchmarks, these attribution methods can still highlight which high-level behavioural dimensions (e.g., error recovery, tool correctness) correlate with task success in aggregate, but they do not reliably localize which constraint was violated in specific failed execution. In contrast, trace-based rubric evaluations grounded in execution logs consistently yield per-run, diagnostically actionable explanations by identifying violated behavioural constraints such as tool misuse, state inconsistency, or failed recovery. Overall, these findings empirically substantiate the paradigm gap highlighted by our taxonomy and motivate concrete requirements for explainability in agentic AI systems. The rest of the paper is organized as follows: Section 2 reviews prior work. Section 3 formalizes the static versus agentic distinction and motivates trajectorylevel explainability. Section 4 describes the datasets, models, and evaluation setup. Section 5 presents results comparing attribution-based methods and tracegrounded rubric analyses across both paradigms. Section 6 presents the discussion, limitations and future directions, and Section 7 concludes the paper."
        },
        {
            "title": "2 Related Work",
            "content": "We situate our work within three areas: traditional explainability methods for static models, explainability techniques for LLMs and reasoning systems, and evaluation frameworks for agentic AI."
        },
        {
            "title": "2.1 Traditional Explainability Methods",
            "content": "Traditional explainability methods have predominantly focused on extrinsic (posthoc) explanations, which seek to explain model behaviour after inference without modifying the underlying model, in contrast to intrinsic interpretable models (e.g., linear models, decision trees) [11]. Within the post-hoc paradigm, methods vary along two orthogonal dimensions: whether they are model-agnostic, requiring only input-output access, or model-specific, relying on gradients, activations, or internal representations. Broadly, post-hoc methods operate at the level of features, concepts, or internal mechanisms. At the feature level, attribution methods identify which input features or regions most strongly influence predictions. Model-agnostic approaches include LIME [34], which fits local surrogate models to generate instance-level explanations, (Partial Dependence Plots) PDPs [14], which summarize marginal effects globally, and SHAP [26], which provides both model-agnostic (e.g., kernelbased) and model-specific variants. Model-specific attribution methods, include 4 S. Chaduvula et al. gradient-based saliency [38], Integrated Gradients [39], and Gradient-weighted Class Activation Mapping (Grad-CAM) [35] for vision models, as well as Transformer analyses based on attention rollout [1] and relevance propagation [6]. While widely used, feature-level attributions operate on high-dimensional inputs spaces (e.g. thousands of token or pixels) and often yield explanations that are difficult to interpret semantically. Concept-based methods address this limitation by shifting the unit of explanation from individual features to human-interpretable abstractions. Probing classifiers [2] assess the separability of concepts within hidden representations, while Testing with Concept Activation Vectors (TCAV) [23] quantifies the influence of user-defined concepts on predictions. Recent work extends these ideas to vision-language models for concept-level analysis and intervention [37]. Although concept-based methods improve interpretability, they are typically correlational, identifying associations between representations and concepts without establishing how such concepts are causally implemented. Mechanistic interpretability aims to move beyond correlational explainability by reverse-engineering internal computations and identifying causal circuits that produce specific behaviours and predictions [28]. This includes uncovering computational circuits in language models [43], automated causal discovery via Automated Circuit Discovery (ACDC) [9], and extracting interpretable features using sparse autoencoders [10]. These approaches provide deeper insight into model internals but remain focused on explaining behaviour at single inference step. While useful, traditional XAI does not address how decisions unfold over time, how states change, or how agents adapt. This requires new explainability approaches for agentic systems."
        },
        {
            "title": "2.2 Explainability for LLMs and Reasoning",
            "content": "The rise of LLMs has motivated post-hoc approaches that surface intermediate reasoning traces (e.g., rationales and action logs) to improve transparency. Chain-of-thought (CoT) [44] prompting elicits step-by-step rationales that often improve performance on complex tasks and can provide human-readable account of the models solution process. ReAct [46] interleaves reasoning traces with action execution, making tool use and decision steps more explicit. Reflexion [36] adds self-reflection and memory updates to support learning from mistakes over repeated trials. Most LLM explainability techniques in this space are post-hoc and often model-agnostic at the interface level (e.g., prompting for rationales or reasoning traces), whereas attention visualization is model-specific and requires access to internal signals. At the interface level, many trace-based techniques are model-agnostic (prompting-based), whereas attention visualization is model-specific and requires access to internal signals. In Retrieval-Augmented Generation (RAG) settings, explainability further requires tracing how retrieved evidence influences the final output and providing provenance over external sources. Attention visualization tools such as BertViz [41] can reveal attention patterns over context, while recent work on Explainability in Traditional and Agentic AI Systems 5 knowledge-graph-based perturbations provides more structured provenance signals for RAG systems [5]. However, attention is not faithful explanation in general, and provenance mechanisms are often necessary to support reliable attribution to sources. persistent concern is faithfulness. CoT rationales are not guaranteed to reflect the causal factors driving model decisions and may be persuasive but misleading [20]. Counterfactual explanations [24] offer verification strategies, but these are rarely integrated into end-to-end agent workflows. This gap becomes evident in agentic settings, where explanations must account for sequences of actions and tool calls, not only final text output."
        },
        {
            "title": "2.3 Evaluation Frameworks for Agentic AI",
            "content": "Several frameworks have recently emerged for evaluating agentic AI systems, particularly LLM-based agents that operate over multi-step trajectories. These approaches reflect growing recognition that agent behaviour must be assessed beyond final outputs and instead evaluated at the level of planning, tool use, and execution. DeepEval [8] decomposes agent behaviour into reasoning, action, and execution layers, using metrics such as plan quality, plan adherence, tool correctness, step efficiency, and task completion. While this decomposition enables target performance analysis, such predefined metrics may not transfer cleanly across tasks, domains, and agent designs. LangSmith 3 supports final-response grading, trajectory comparison against reference workflows, and component-level testing, though its reliance on \"ideal reference traces can penalize alternative yet valid strategies. Ragas [40] introduces agent-oriented metrics for tool calling and goal fulfillment (e.g., Tool Call Accuracy/F1 and goal accuracy), supporting both reference-based and reference-free evaluation. Phoenix [4] uses LLM judges to assess full tool-calling sequences, enabling end-to-end evaluation but offering limited diagnostic granularity for pinpointing where failures arise within trajectory. Collectively, these frameworks reflect growing recognition that agent evaluation must capture process-level behaviour. Yet they remain fragmented in metric definitions and focus primarily on performance evaluation, rather than explicitly defining or evaluating explainability as first-class diagnostic artifact. Gap and Positioning. Prior work offers strong tools for explaining static predictors (e.g., attribution, concepts, circuits), and separate lines of work surface reasoning traces for LLMs or propose benchmarks and metrics for evaluating agent performance. However, these strands are rarely unified (e.g., prediction vs. trajectory). Our work bridges these areas by organizing xai around paradigm distinction (static vs. agentic) and empirically testing how explanations differ in their ability to diagnose failures in multi-step agents. 3 https://www.langchain.com/ 6 S. Chaduvula et al. Static MEP Agentic MEP Artifact SHAP, LIME and PDP feature scores (token-level attribution) vs. Execution trace + reasoning (tool calls, decisions) Context Input text + predicted label (single instance) Verification Perturbation stability (feature rank correlation) Example: Job category classifier vs. vs. t - u Trajectory: (s0, a0, o0, . . . , sT ) state snapshots, tool logs, retrieved docs, env feedback Rubric flags + replay checks (intent, tool correctness, state consistency) Example: Airline booking agent Key shift: Single inputoutput Trajectory over time Fig. 1: Comparison of Minimal Explanation Packet (MEP) structure across static and agentic paradigms. Static MEPs ground explanations in single input output pair with optional verification. Agentic MEPs require trajectory-level context spanning multiple steps, tool interactions, and state evolution, with verification signals that assess process-level faithfulness."
        },
        {
            "title": "3 Explainability Across Static and Agentic AI Paradigms",
            "content": "Explainability methods have evolved alongside advances in AI, from early rulebased systems and decision trees to post-hoc feature attributions, concept-based analyses, and mechanistic interpretability for deep neural networks. The rise of tool-using, multi-step agentic systems introduces new explainability requirements: explanations must account for action sequences, tool interactions, and state evolution over time. In this section, we formalize the distinction between static and agentic paradigms, map existing XAI families through this lens, and introduce unified framework, the Minimal Explanation Packet (MEP), for packaging explanation artifacts with context and verification when behaviour unfolds over trajectories. Figure 1 conceptually contrasts static and agentic MEPs, highlighting the shift from single inputoutput explanations to trajectory-level accounts."
        },
        {
            "title": "3.1 Paradigm Distinction",
            "content": "We distinguish between two paradigms that place fundamentally different demands on explanation. In the static (prediction-oriented) paradigm, system implements fixed mapping = (x) for given input x, and explanations are defined with respect to single inputoutput decision. This paradigm includes models with recurrence or iterative internal computation, provided that behaviour is evaluated only at the level of single prediction. In agentic systems, behaviour emerges as trajectory τ = (s0, a0, o0, s1, a1, o1, . . . , sT ), where the agent repeatedly observes the environment, reasons, and acts. Here st denotes Explainability in Traditional and Agentic AI Systems 7 Fig. 2: Agent execution loop with explicit stateactionobservation semantics. The architecture separates agent execution from interpretability and verification, showing how traces are reconstructed and analyzed to produce Minimal Explanation Packets (MEPs) for auditing and diagnosis. the internal state at step t, at the action taken (including tool calls), and ot the observation received. Correctness and failure are defined at the level of the full trajectory rather than an individual decision. This shift has two implications for explainability. First, scope expands from explaining single decisions to explaining full trajectories. Second, grounding shifts from input features to tool calls, state updates, retrieved evidence, and environmental feedback. Figure 2 makes explicit the stateactionobservation semantics that motivate trajectory-level explainability."
        },
        {
            "title": "3.2 Mapping of XAI Methods Across Paradigms",
            "content": "Table 1 maps major families of explainability approaches across static prediction and agentic systems. We organize prior work by (i) the primary explanation 8 S. Chaduvula et al. Table 1: Explainability approaches across static models and agentic systems. Role in static models Additional needs in agentic systems Family / target Representative Explanation artifact Attribution saliency and Attention-based analyses methods LIME, PDP saliency, CAM [35] Attention relevance tion [1,6] SHAP, [34,26]; GradFeature heatmaps scores, input Identify gions/features drive single prediction rethat rollout, propagaToken influence paths Characterize token-level influence during generation Concept-based interpretability Probes, TCAV VLM concepts [37] [17,23]; Concept probe accuracy scores, Test whether concepts are encoded in representations Mechanistic interpretability Circuits, sparse coders [43,9,10] ACDC, autoenCausal subgraphs, features Localize internal mechanisms behind predictions Reasoning traces interaction and logs CoT, ReAct, Reflexion [46,36] Stepwise nales; tool logs ratioProvide humana readable rationale for one output Evidence and provenance explanations Attention ization, paths [41,5] visualevidence Citations, provenance graphs Justify outputs using retrieved evidence Counterfactual explanations Recourse, terfactual tion [42,24] counevaluaWhat-if alternatives input Identify minimal changes to alter prediction Verification faithfulness nals and sigSimulatability, bustness checks [7] roFaithfulness signals Optional evaluation layer for explanation quality Interactive interventionbased tions explana- / Causal probing via dialogue [22] Hypotheses with interventions Not applicable Explain action selection per step + connect attributions to tool choice + relate decisions to plan/state evolution Track attention shifts across steps (planning retrieval execution) and across multiple context sources Represent goals/subgoals, constraints, tool intent, and state variables beyond raw feature concepts Analyze causal interactions across memory modules, tool interfaces, retrieval components, and policy updates Require trajectory-level linking: (reasoning action observation) + replayable logs tied to outcomes Attribute which evidence drove which actions and revisions + detect retrieval-induced failure cascades Define counterfactuals over trajectories: alternative plans, tool calls, and decision branches over time Make verification first-class: faithfulness checks over long horizons, consistency under replay, and rubric flags Explain multi-turn behaviour via validated interventions on state, tool access, or observations target (inputs/representations vs. trajectories/interactions) and (ii) the explanation artifact produced (e.g., attributions, traces, counterfactuals, provenance, or verification signals). The table highlights how methods originally developed for single-step prediction must be adapted to support multi-step decision-making in agentic settings. Attributions must become decisionand context-conditional. In static prediction, attribution scores answer well-posed question: which input features influenced single output. In agentic systems, the analogous question is why this action now, relative to available alternatives under the current state (memory, retrieved evidence, tool availability, and past observations). Step-level SHAP/LIME can be informative, but only when paired with (i) the agents action set (tools/actions considered), (ii) the state variables that condition the choice, and (iii) mechanism for composing local explanations into trajectory-level account. Without this context, global summaries (e.g., PDP-style effects across runs) provide population-level correlates but systematically miss where and how failures arise within specific trajectory. Explainability in Traditional and Agentic AI Systems 9 Table 2: Unified criteria for evaluating explanations across static and agentic settings. Criterion Scope Static setting signal Local/global attribution for single output Definition Prediction-level trajectory-level planation Linkage to observable evidence vs. exGrounding Reliability/ faithfulness Auditability Evidence that the explanation is trustworthy Support ging and oversight for debugInput features, coefficients, saliency maps Stability under perturbations; rank consistency Agentic setting signal Trace-level explanation across actions, tools, and state updates Tool-call arguments, retrieved evidence, environment feedback, execution logs Replay-based outcome-conditioned statistics checks; rubric Post-hoc feature inspection Step-level replay, failure localization, tool correctness checks Reasoning traces are necessary but insufficient. CoTand ReAct-style traces expose intermediate reasoning and make multi-step structure legible to humans. However, they are self-reported and can diverge from the actual causal drivers of behaviour, especially when tool results, memory updates, or retrieval events dominate subsequent decisions. Trace-based explanations therefore become more reliable when aligned with interaction logs, linking each stated intent to the corresponding action, tool output, and state update, and when discrepancies between said and done are surfaced explicitly. Verification shifts from optional to first-class. In static XAI, faithfulness checks (e.g., perturbation stability) are often treated as optional add-ons. In agentic settings, failures can emerge from compounding effects across steps (state drift, cascading retrieval errors, or tool mis-specification), making verification essential. Practical signals include replay consistency under fixed seeds/tools, invariants over tracked state, and rubric-style flags that localize violated behavioural constraints. These checks convert explanations from plausible narratives into diagnostically actionable accounts."
        },
        {
            "title": "3.3 Evaluating Agentic Systems",
            "content": "To compare explainability across paradigms, we use four criteria: (i) scope (singlestep vs. trajectory-level), (ii) grounding (linkage to observable evidence such as inputs or traces), (iii) reliability/faithfulness (signals that the explanation reflects the systems actual behaviour), and (iv) auditability (support for debugging, replay, and oversight). Table 2 summarizes how each criterion is operationalized in the two settings. We evaluate agentic explainability using trace-derived behavioural rubrics applied to complete execution trajectories. Our rubric evaluation builds on Docent [27], while our summary statistics follow the conditional analysis approach used in HAL-Harness [21]. For each run, an LLM-based judge assigns binary satisfaction/violation labels per rubric category using only the execution trace (actions, tool calls, observations, and intermediate state), without access to task outcomes or ground truth, thereby avoiding outcome leakage. 10 S. Chaduvula et al. Notation. Let {1, . . . , } index agent runs and index rubric categories (e.g., Intent Alignment, Tool Correctness). We define binary violation indicator fi,r {0, 1}, where fi,r = 1 denotes violation and fi,r = 0 indicates satisfaction. Each run has binary task outcome yi {0, 1}, where yi = 1 denotes success and yi = 0 denotes failure. Failure-mode prevalence. To quantify which violations are over-represented in failed runs, we compute: (fr = 1 = 0), (fr = 1 = 1), (1) and report their difference and ratio: prev(r) = (fr=1 y=0)P (fr=1 y=1), (fr=1 y=0) (fr=1 y=1) (2) where larger prev indicates violations more frequent in failures, and indicates violations exclusive to failed runs. Ratioprev(r) = , Reliability correlates. To assess how strongly violation predicts reduced task success when it occurs, we compute: (y = 1 fr = 1), (y = 1 fr = 0), (3) and report: rel(r) = (y=1 fr=1) (y=1 fr=0), RR(r) = (y=1 fr=1) (y=1 fr=0) , (4) where large negative rel indicates strong association with failure, and RR near 0 indicates highly predictive failure signals. Rubric categories. We use the rubric set in Table 3, including Intent Alignment (goal-consistent actions), Plan Adherence (coherent multi-step planning), Tool Correctness (valid tool invocation), Tool-Choice Accuracy (appropriate tool selection), State Consistency (coherent state maintenance), and Error Recovery (detection and recovery from failures). All rubric prompts and scoring templates are released with our codebase."
        },
        {
            "title": "3.4 Minimal Explanation Packet (MEP)",
            "content": "Agentic systems require explanations that are not standalone artifacts but bundles that connect behaviour to evidence and verification. We therefore introduce the Minimal Explanation Packet (MEP), lightweight, method-agnostic unit that packages: 1. Explanation artifact: The human-interpretable explanation itself (e.g., feature attribution map, reasoning trace, tool-call summary). Explainability in Traditional and Agentic AI Systems 11 2. Linked evidence and execution context: Supporting material that grounds the artifact (e.g., input instance, execution trace, retrieved documents, toolcall logs, state snapshots). 3. Verification signals: Indicators of explanation reliability (e.g., perturbation stability scores, rubric-based behavioural flags, replay-based consistency checks). The MEP is method-agnostic and can be instantiated at different scopes (local instance-level or global model-level), depending on the explanation goal. Static MEP. For classifier predicting job category from posting text: Artifact: SHAP feature attributions or LIME local explanations; PDPs when instantiated at global (model-level) scope. Context: Input text, predicted label, model confidence. Verification: Rank correlation of top features across perturbed inputs. Agentic MEP. For tool-using agent completing an airline booking task: Artifact: Execution trace linking reasoning steps to actions (including tool calls and intermediate outputs). Context: User request, observations at each step, tool arguments and returns, retrieved evidence, and state updates (e.g., memory/plan revisions). Verification: Rubric-based behavioural flags (e.g., intent alignment, tool correctness, state consistency, error recovery) and replay-based consistency checks. By shifting focus from isolated explanation artifacts to artifact-in-context with associated reliability evidence, the MEP supports auditing and replay rather than standalone narrative explanations. In our experiments (Section 4), we use MEP components to structure analysis of attributionand trace-based explanations, emphasizing per-run diagnostic utility alongside aggregate correlations."
        },
        {
            "title": "4 Experimental Setup",
            "content": "We evaluate explanation quality under our unified framework in two complementary settings: (i) static prediction setting and (ii) an agentic setting. Both are assessed using the same MEP-based criteria  (Table 2)  , such as scope, grounding, reliability/faithfulness, and auditability, while the concrete metrics reported in each setting are summarized in Table 3."
        },
        {
            "title": "4.1 Datasets",
            "content": "We use the Online Job Postings dataset for binary text classification (IT vs. non-IT) to study traditional explainability in static prediction settings. For agentic behaviour, we evaluate two established tool-use benchmarks: (i) TAUbench Airline [45], which consists of structured airline customer-service tasks with API-mediated actions (e.g., search, rebook, cancel), and (ii) AssistantBench [25], which comprises web-based assistance tasks requiring multi-step navigation and information gathering. 12 S. Chaduvula et al. Table 3: Evaluation metrics for static and agentic settings. Explanation stability is adopted from prior XAI robustness work. Agentic metrics are custom rubric signals defined in Section 3.3 and operationalized using Docent [27]. Setting Metric Description Static Explanation Stability Avg. Spearman rank correlation (ρ) across perturbed inputs or repeated runs [3,18] Agentic CorIntent Alignment Plan Adherence Tool rectness Tool-Choice Accuracy State Consistency Error covery ReActions align with stated goals and task requirements Maintains coherent multi-step plans throughout execution Invokes appropriate tools with valid parameters Selects optimal tools for given sub-tasks Maintains coherent internal state across steps Reliability Detects and recovers from execution failures Reliability MEP Criteria Reliability Grounding Grounding, Reliability Auditability Grounding, Auditability Custom"
        },
        {
            "title": "4.2 Models",
            "content": "In the static setting, we use two lightweight classifiers: TFIDF with Logistic Regression and Text CNN baseline. In the agentic setting, we study tool-using LLM agents on TAU-bench Airline and AssistantBench, using o4-mini2025-04-16 [30] and GPT-4.1 [29] as the underlying models, respectively. All agent traces are labeled post-hoc with Docent [27], using fixed GPT-5 [31] judge with medium reasoning effect for consistent rubric evaluation. Trace Collection Agent execution traces are collected using the Holistic Agent Leaderboard evaluation harness (HAL-Harness) [21], which standardizes agent execution and logging across benchmarks. HAL-Harness records detailed trajectories for each run, including sequences of actions, tool calls, and intermediate observations. These traces serve as the primary artifact for trajectory-level explainability analysis."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "Metrics for both static and agentic settings are summarized in Table 3. In the static setting, we focus on the stability and consistency of attribution-based explanations under perturbations. In the agentic setting, we report rubric-based behavioural signals (discussed in Section 3.3) and analyze their relationship to task success and failure. All rubric definitions, prompts, and scoring templates are released with our codebase."
        },
        {
            "title": "4.4 Settings and Hyperparameters",
            "content": "All experiments were conducted on CPU-only Linux servers. Agent traces for AssistantBench and TAU-bench Airline were generated using HAL-Harness Explainability in Traditional and Agentic AI Systems 13 Table 4: Summary of experimental settings for both agentic and static explainability evaluations, including hardware, evaluation frameworks, benchmarks, models, training configurations, and explanation parameters used to ensure reproducibility. Category Setting / Value Agentic Explainability Evaluation Setup Hardware Python environment Agent evaluation harness Trace analysis framework LLM judge LLM hyperparameters CPU-only Linux server (no GPU acceleration) Python 3.12; scikit-learn, shap HAL-Harness [21] (default benchmark configurations) Docent [27] (post-hoc, trace-only evaluation) GPT-5 (medium reasoning effort, single-pass, trace-only) temperature = 0.1; default sampling parameters Benchmarks Inference models Data usage TAU-bench Airline [45] (N = 50), AssistantBench [25] (N = 33) GPT-4.1 (AssistantBench), o4-mini-2025-04-16 (TAU-bench Airline) Full benchmark evaluation sets Static Explainability Evaluation Setup Compute Environment TFIDF Logistic Regression Neural Model (CNN) Data Split SHAP LIME Saliency Kaggle CPU-only Linux server (4 vCPUs, 16 GB RAM) N-gram range: (1,2); min_df: 5; max_df: 0.9; stopwords: English max_iter: 500; class_weight: data-driven Embedding: dim=100; mask_zero=True; Conv1D: filters=128, kernel_size=5, activation=ReLU; Pooling: GlobalMaxPooling1D; Dense: 64 units, activation=ReLU; Dropout: 0.5; Output: 1 unit sigmoid 70% train / 15% validation / 15% test Mean absolute SHAP value Number of Features: 10 ; Random_state: 42 Attribution method: Gradient Input; Absolute score for ranking and analyzed offline using Docent. Static explainability experiments were implemented in Python using standard scientific libraries. Table 4 reports the full set of software, model, and explanation settings used across both evaluations for reproducibility."
        },
        {
            "title": "5 Results and Analysis",
            "content": "We first report static attribution results as calibration baseline (Section 5.1), then present trace-based rubric analyses for agentic benchmarks (Section 5.2), and finally run bridging experiment that projects trajectories into rubric features to compare attribution vs. trace-based explanations under shared representation (Section 5.3)."
        },
        {
            "title": "5.1 Static Explainability Results",
            "content": "We evaluate traditional explainability methods in static prediction setting using binary IT vs. non-IT classification on the Online Job Postings dataset. This experiment serves as calibration baseline for explanation reliability in regime 14 S. Chaduvula et al. (a) LIME explanation: LIME plot showing feature importance on single sample. Blue indicates non-IT and orange indicates IT features, showing how the model forms its prediction. (b) SHAP global summary: SHAP beeswarm plot showing global feature importance. Features like software push predictions toward IT, while accounting pushes toward non-IT. Terms such as development appear in multiple contexts, leading to neutral contributions. Fig. 3: Comparison of local (LIME) (a), and global (SHAP) (b) interpretability. where decisions are driven by sparse, semantically meaningful lexical features and explanations are naturally defined over single input-output mapping. Table 5: Static setting: explanation stability (Spearman ρ) under perturbations. The results show that TFIDF + Logistic Regression achieves higher stability score compared to the Text CNN, indicating more consistent explanations under data perturbations. Model Explanation Stability Score TFIDF + Logistic Regression Text CNN 0.8577 0. As shown in Table 5, TFIDF + Logistic Regression yields substantially higher explanation stability (Spearman ρ = 0.8577) than the Text CNN (ρ = 0.6127), indicating more consistent attribution patterns under perturbations. We further illustrate representative local and global attribution behaviours in Figure 3 (a) for single prediction using LIME, while panel (b) summarizes corpus-level feature effects using SHAP. Overall, while post-hoc methods provide useful insight into influential lexical features, they primarily explain final predictions and offer limited visibility into intermediate reasoning steps or decision dynamics. Explainability in Traditional and Agentic AI Systems 15 Table 6: Summary of agent evaluation runs used for rubric-based analysis The table reports benchmark name, agent configuration, underlying model, number of evaluated tasks covering all benchmark-defined tasks, task-level success and failure counts, aggregate task accuracy, and total inference cost. All metrics are computed from HAL-Harness execution traces and aggregated over the full benchmark-defined task sets. Bold indicates the best accuracy across benchmarks. Benchmark Agent Model Tasks (N) Success (N) Failure (N) Acc. Cost ($) AssistantBench Browser Agent GPT-4.1 TAU-bench Airline TAU-bench Tool Calling o4-mini2025-04-16 33 50 2 28 22 17.39% 14.15 56.00% 11.36 Key finding (static). Even under ideal conditions, static XAI methods explain outcomes at the prediction level and do not capture multi-step decision dynamics. This motivates trace-based evaluation in the agentic setting, where failures often arise from state updates, tool choices, and long-horizon trajectories rather than single forward pass."
        },
        {
            "title": "5.2 Agentic Explainability",
            "content": "We analyze agent behaviour using execution traces collected with HAL-Harness on two benchmarks: TAU-bench Airline and AssistantBench. Our goal is to diagnose why agents succeed or fail beyond aggregate accuracy. Each trace is labeled using the behavioural rubrics defined in Section 3.3, with fixed GPT-5 judge 4 to ensure consistency. Overall Performance Table 6 shows that performance differs sharply across benchmarks: TAU-bench Airline achieves 56.0% accuracy (28/50), while AssistantBench achieves 17.39% (2/33). These benchmarks stress different skills, such as structured tool execution vs. open-ended web interaction, so accuracy alone can hide qualitatively different failure modes. This motivates trace-level rubric analysis. Trace-Level Rubric Analysis In trace-level rubric analysis, we label each execution trace with behavioural rubrics provided in Section 3.3. To ensure consistency, we use single fixed judge (GPT-5 [31]) in single pass conditioned only on traces. The results on trace-level rubric analysis are discussed next. Failure-mode prevalence. rubric flag indicates that violation was detected for that run (i.e., the constraint was not satisfied). Table 7 reports how often each rubric violation appears in failed vs. successful runs. In TAU-bench 4 https://openai.com/index/gpt-5-system-card/ S. Chaduvula et al. Airline, failures are most strongly associated with State Tracking Consistency ( = 0.333, Ratio = 2.7), suggesting breakdowns that accumulate over long trajectories. In AssistantBench, Tool Choice Accuracy stands out as sparse but decisive failure mode that appears only in unsuccessful runs (Ratio = ). Table 7: Failure-mode prevalence results aggregated over all benchmark-defined tasks. For each rubric, we report the probability of observing failure flag among failed runs (P (flag failure)) and among successful runs (P (flag success)). () denotes the difference between these probabilities, and the ratio () captures relative over-representation in failed tasks. Within each benchmark, bold and underline indicate the best and second-best rubrics in terms of promoting task success (lowest ), respectively. Cell-level highlighting marks the worstperforming failure mode (highest ). denotes holistic rubrics when they exhibit the strongest failure association. Benchmark Rubric TAU-bench Airline Intent Alignment AssistantBench Error Awareness & Recovery State Tracking Consistency Tool Correctness Tool Choice Accuracy Plan Adherence Intent Alignment Error Awareness & Recovery State Tracking Consistency Tool Correctness Tool Choice Accuracy Plan Adherence (flag failure) (flag success) Ratio 0.212 0.419 1.506 0.007 0.988 0.677 0.333 0.194 2.719 0.136 0.699 0.452 0.027 0.906 0.290 1.632 0.081 0.129 0.632 0.684 0.526 0.316 0.263 0.211 0.774 0.516 0.452 0.548 0.484 0.032 1.000 0.500 0.500 0.500 0.000 0. 0.226 0.77 1.03 0.016 0.90 0.048 0.048 1.10 0.484 0.032 Reliability correlates. Table 8 measures predictive strength by comparing success rates when violation is present vs. absent. In TAU-bench Airline, State Tracking Consistency is the clearest predictor of failure (RR = 0.51), with success dropping by 36 percentage points when violated. This suggests that state drift accumulates silently until it causes irrecoverable errors. Interestingly, Tool Correctness correlates positively with success (RR = 1.24), suggesting it captures minor, recoverable issues rather than fatal errors, as agents that trigger this flag may actually be attempting more complex tool interactions. In AssistantBench, most rubrics have limited predictive value due to the low-success regime, but Tool Choice Accuracy and Plan Adherence violations correspond to zero success (RR = 0.00), indicating these are hard blockers in web-based tasks. Key finding (agentic). Trace-level rubrics reveal how failures unfold: TAUbench Airline failures reflect gradual trajectory degradation (especially state inconsistency), whereas AssistantBench failures are driven by sparse but decisive mistakes. This explains why accuracy alone does not capture agent reliability. Explainability in Traditional and Agentic AI Systems 17 Table 8: Reliability correlates results aggregated over all benchmark-defined tasks. For each rubric, we report the probability of task success when rubric violation is observed (P (success flag)) and when the violation is absent (P (success flag)). denotes the absolute difference, and RR () captures the relative change in success likelihood when violation is present. Within each benchmark, bold and underline indicate the best and second-best rubrics in terms of reliability (highest RR), respectively. Cell-level highlighting marks the worst-performing rubric (lowest RR). denotes holistic rubrics when they exhibit the strongest negative reliability effect. Benchmark Rubric TAU-bench Airline Intent Alignment AssistantBench Error Awareness & Recovery State Tracking Consistency Tool Correctness Tool Choice Accuracy Plan Adherence Intent Alignment Error Awareness & Recovery State Tracking Consistency Tool Correctness Tool Choice Accuracy Plan Adherence (success flag) (success flag) RR 0.20 0.72 0.007 0.99 0.36 0.51 0.133 1.24 0.032 1.05 0.143 0. 0.72 0.625 0.735 0.567 0.611 0.643 0.52 0.618 0.375 0.70 0.643 0.50 0.077 0.059 0.067 0.056 0.000 0.000 0.000 0.062 0.056 0.067 0.111 0.062 0.077 0.004 0.94 0.011 1.20 0.011 0.83 0.111 0.00 0.062 0."
        },
        {
            "title": "5.3 Bridging Static and Agentic Explainability",
            "content": "Traditional XAI methods explain predictions from static input-output mappings, while agentic explainability targets behaviour unfolding over multi-step executions. To compare these paradigms under shared representation, we design controlled bridging experiment on TAU-bench Airline execution traces. Each trajectory is first labelled using Docent rubrics (Section 3.3) and then encoded as compact binary feature vector, where each dimension indicates whether behavioural constraint is satisfied or violated. Using these rubric features, we train logistic regression model to predict task success vs. failure (hyperparameters in Table 4). We compute SHAP values using linear explainer consistent with the model to quantify the influence of each rubric feature on the surrogate outcome predictor. Attribution results. Table 9 reports mean absolute SHAP values, showing that Intent Alignment, State Tracking Consistency, and Tool Correctness are the most influential predictors of outcome under this representation. Figure 4 visualizes the same trend: violations of these rubrics tend to push predictions toward failure, while satisfaction shifts predictions toward success. This experiment also shows that attribution methods can recover sensible global importance rankings when agent executions are compressed into lowdimensional, behaviourally grounded feature space. However, these attributions remain correlative: they explain which rubric features drive the surrogate models outcome predictions, not what caused specific run to fail, consistent with re18 S. Chaduvula et al. Table 9: Global SHAP attribution scores for the rubric-level outcome predictor (logistic regression). Higher values indicate greater overall influence. Bold and underline denote the top two attributes. Rubric Attribute Mean SHAP Intent Alignment State Tracking Consistency Tool Correctness Tool Choice Accuracy Error Awareness & Recovery Plan Adherence 0.473 0.422 0.415 0.122 0.115 0.090 Fig. 4: SHAP summary (beeswarm) plot for rubric-level features. Each point is run; the x-axis shows the SHAP value (contribution to predicted success), and color indicates feature value (low vs. high). Features are ordered by mean absolute SHAP value. cent evaluations of SHAP in reinforcement learning settings that highlight its limitations for causal interpretation [12]. Table 10 summarizes this contrast, highlighting how attribution-based explanations over rubric features differ from trace-based agentic explainability even under shared representation. Key finding (bridge). Projecting trajectories into rubric features makes SHAP useful for aggregate analysis, but it does not provide trace-grounded diagnoses of where or how failures arise. Reliable explainability for long-horizon, tool-using agents therefore requires trajectory-level, trace-based reasoning, with attribution serving as complementary summary tool. Overall takeaway. Static XAI is reliable when the task matches its assumptions, but it cannot explain multi-step failures. Trace-based rubrics diagnose trajectory failures directly, and rubric-to-SHAP bridging shows attribution can summarize global importance only after strong behavioural abstraction. Explainability in Traditional and Agentic AI Systems 19 Table 10: Traditional attribution-based vs. trace-based agentic explainability under shared rubric-derived representation. Aspect Input representation Primary output Unit of explanation Temporal reasoning Tool/state awareness Per-run failure localization Explanation goal Traditional-XAI (SHAP/LIME) Aggregated feature vector Feature importance Outcome prediction Limited (indirect) Correlative (what matters) Agentic-XAI (Docent) Full execution trace Rubric satisfaction/violation Entire trajectory Explicit (direct) Diagnostic (what went wrong)"
        },
        {
            "title": "6.1 Main Findings",
            "content": "Overall, our results show clear paradigm shift in what it means to explain an AI system. In static prediction settings, feature attribution methods remain meaningful and reliable when the models inductive bias aligns with task structure, as reflected by stable SHAP and LIME explanations in our classification experiments. However, when intelligence is expressed through multi-step interaction, such as planning, tool use, state updates, and recovery, explanations tied to single prediction no longer answer the questions practitioners actually ask: what failed, where, and why [32]. Across TAU-bench Airline and AssistantBench, we observe that failures are governed by temporal and state-dependent phenomena, for example, most prominently state inconsistency and compounding errors that attribution methods cannot localize, even when they recover sensible aggregate correlations. In contrast, trace-grounded rubric analysis yields per-run, diagnostically actionable accounts that support auditing, debugging, and reliability assessment. Error Analysis: Where Agentic Failures Arise Agentic failures rarely reduce to single wrong output. Instead, they arise from breakdowns in trajectory integrity, where small deviations compound or where one decisive branching mistake blocks progress. Our rubric analysis surfaces three recurring error classes. In TAU-bench Airline, the strongest failure signal is State Tracking Consistency. Qualitatively, these failures manifest as latent divergence between the agents evolving plan/memory and the environment state (e.g., stale constraints, mis-tracked entities, or inconsistent assumptions), which compounds across steps until final tool call becomes irrecoverable. This produces slow failure pattern: early steps may appear reasonable, but small inconsistencies accumulate over time. In AssistantBench, failures are dominated by sparse but decisive mistakes, most notably Tool Choice Accuracy (and occasionally Plan Adherence). These errors correspond to choosing an incorrect interaction affordance (e.g., wrong navigation path or incorrect tool for subtask), after which recovery becomes S. Chaduvula et al. unlikely under fixed step budget. This yields fast failure pattern: single wrong branching decision can collapse the run. Not all rubric flags are uniformly failure-predictive across benchmarks. For example, Tool Correctness can coincide with successful runs, suggesting that some violations capture minor or recoverable tool issues (e.g., parameter formatting or transient tool errors) rather than fundamental reasoning breakdowns. In these cases, trace-based explanations still improve auditability by revealing what went wrong, even when the issue does not ultimately determine the outcome. Overall, the error modes differ systematically by benchmark: TAU-bench Airline failures reflect gradual trajectory degradation driven by state drift, whereas AssistantBench failures reflect hard blockers driven by incorrect branching/tool choice. These patterns help explain why accuracy alone is insufficient and why trace-grounded explanations are necessary to diagnose where and how failures occur. 6."
        },
        {
            "title": "Impact",
            "content": "Practically, this distinction matters in safety-critical deployments such as healthcare triage, financial operations, and enterprise automation, where explanations must justify actions taken over time rather than merely outputs produced at the end. Theoretically, our findings argue for shift in XAI from explanation as static artifact to explanation as structured account of behaviour, grounded in evidence and coupled with verification. By formalizing this shift through the trajectory-level paradigm and operationalizing it via Minimal Explanation Packets, this work reframes explainability as property of agent execution rather than model inference, providing foundation for evaluating, regulating, and trusting increasingly autonomous AI systems."
        },
        {
            "title": "6.3 Limitations",
            "content": "Our evaluation is limited in scope. We study small set of tool-using LLM agents on TAU-bench Airline and AssistantBench using HAL-Harness. While these benchmarks capture realistic multi-step tool use, they do not cover the full range of agent architectures; our findings may not generalize to embodied agents, multi-agent coordination settings, or systems with online learning or persistent long-term memory. Our trajectory-level explanations are derived post-hoc from execution traces and summarized using predefined behavioural rubrics. This abstraction enables scalable and consistent analysis across runs, but it is necessarily coarse: it can hide fine-grained decision dynamics and supports primarily correlational rather than causal conclusions. Moreover, rubric labels are generated by an LLM judge via Docent, which introduces subjectivity despite fixed prompts and trace-only access, and relies on traces being complete, an assumption that may not hold under incomplete logging or partial observability. Finally, while Docent enables systematic reconstruction and evaluation of behaviour across trajectories, it currently offers limited support for automated Explainability in Traditional and Agentic AI Systems 21 counterfactual interventions, causal validation, and direct integration with internal model representations. Richer evaluations may require exposing full execution scaffolds and audit artifacts, increasing implementation complexity and potentially reducing portability across benchmarks."
        },
        {
            "title": "6.4 Responsible Use",
            "content": "Our goal is to support safer and more accountable deployment of agentic AI by enabling trajectory-level explanations that help diagnose how multi-step behaviours fail, rather than only explaining final outcomes. These explanations can support auditing, debugging, and system improvement, but they must be interpreted with care: rubric violations and attribution scores are not causal guarantees and should not be used in isolation for enforcement or punitive decision-making. Responsible use requires combining trace-based explanations with human oversight, domain expertise, and complementary evaluation methods."
        },
        {
            "title": "7 Conclusion",
            "content": "This work revisits explainability in the context of increasingly prevalent agentic AI systems whose behaviour is defined over extended executions. We address the mismatch between explainability methods designed for single prediction outcomes and agentic settings in which success and failure emerge from sequences of decisions, actions, and observations. Across static models and agentic benchmarks, our results show that attribution-based explanations reveal which factors matter overall, but do not reliably explain why specific agent execution failed, whereas trace-grounded rubric analysis consistently pinpoints executionlevel failures within individual agent runs such as state inconsistency and toolselection errors. Despite these findings, open challenges remain in formalizing explanation faithfulness, aligning explanation artifacts across paradigms, and scaling trajectory-level diagnostics adapted to more diverse tasks and agent architectures. Future work should investigate standardized trajectory-level explanation frameworks, stronger verification mechanisms, and closer integration between explainability and agent evaluation to support reliable and auditable deployment of autonomous AI systems. In particular, developing interventionbased and counterfactual analyses to validate causal hypotheses about agent failures remains an important direction, moving beyond correlational diagnostics derived from post-hoc traces."
        },
        {
            "title": "Acknowledgements",
            "content": "Resources used in preparing this research were provided, in part, by the Province of Ontario and the Government of Canada through CIFAR, as well as companies sponsoring the Vector Institute (http://www.vectorinstitute.ai/#partners). 22 S. Chaduvula et al. This research was funded by the European Unions Horizon Europe research and innovation programme under the AIXPERT project (Grant Agreement No. 101214389), which aims to develop an agentic, multi-layered, GenAI-powered framework for creating explainable, accountable, and transparent AI systems."
        },
        {
            "title": "References",
            "content": "1. Abnar, S., Zuidema, W.: Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928 (2020) 2. Alain, G., Bengio, Y.: Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644 (2016) 3. Alvarez-Melis, D., Jaakkola, T.: Towards robust interpretability with selfexplaining neural networks. Advances in Neural Information Processing Systems (2018) 4. Arize AI: Phoenix: Agent trajectory evaluation for llm systems (2024), https: //arize.com/ai-agents/agent-evaluation/ 5. Balanos, G., Chasanis, E., Skianis, K., Pitoura, E.: Kgrag-ex: Explainable retrievalaugmented generation with knowledge graph-based perturbations. arXiv preprint arXiv:2507.08443 (2025) 6. Chefer, H., Gur, S., Wolf, L.: Transformer interpretability beyond attention visualization. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 782791 (2021). https://doi.org/10.1109/CVPR46437. 2021.00084 7. Chen, Y., Li, R., Tan, C., Wang, Y.T.: Do models explain themselves? language explanations. arXiv preprint counterfactual simulatability of natural arXiv:2307.08678 (2023) 8. Confident AI: Deepeval: framework for evaluating llm-based applications. https: //deepeval.com/guides/guides-ai-agent-evaluation-metrics (2023) 9. Conmy, A., Mavor-Parker, A., Lynch, A., Heimersheim, S., Garriga-Alonso, A.: Towards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems arXiv:2304.14997 (2023) 10. Cunningham, H., Ewart, A., Riggs, L., Huben, R., Sharkey, L.: Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600 (2023) 11. Doshi-Velez, F., Kim, B.: Towards rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608 (2017) 12. Engelhart, M., Wörner, S., Althoff, M.: Evaluation of shap for explainable reinforcement learning. In: Proceedings of the XAI Conference (2024). https: //doi.org/10.1007/978-3-031-63800-8_ 13. Farooq, A., Raza, S., Karim, M.N., Iqbal, H., Vasilakos, A.V., Emmanouilidis, C.: Evaluating and regulating agentic ai: study of benchmarks, metrics, and regulation (2025) 14. Friedman, J.H.: Greedy function approximation: gradient boosting machine. The Annals of Statistics (2001), https://doi.org/10.1214/aos/1013203451 15. Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., Pedreschi, D.: survey of methods for explaining black box models. ACM computing surveys (CSUR) arXiv:1802.01933 (2018) 16. Gunning, D.: Explainable artificial intelligence (xai). Tech. rep., Defense Advanced Research Projects Agency (DARPA) (2017), https://www.darpa.mil/program/ explainable-artificial-intelligence, dARPA XAI Program Overview Explainability in Traditional and Agentic AI Systems 23 17. Hewitt, J., Manning, C.D.: structural probe for finding syntax in word representations. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 41294138 (2019), https://aclanthology. org/N19-1419/ 18. Hooker, S., Erhan, D., Kindermans, P.J., Kim, B.: benchmark for interpretability methods in deep neural networks. Advances in Neural Information Processing Systems arXiv:1806.10758 (2019) 19. Huang, K.: Agentic AI: Theories and Practices. Springer Cham (2025), https: //doi.org/10.1007/978-3-031-90026-6 20. Jain, S., Wallace, B.C.: Attention is not explanation. NACCL arXiv:2201. (2019) 21. Kapoor, S., Stroebl, B., Others: Holistic agent leaderboard: The missing infrastructure for ai agent evaluation (2025) 22. Kim, B., Hewitt, J., Nanda, N., Fiedel, N., Tafjord, O.: Because we have llms, we can and should pursue agentic interpretability. arXiv preprint arXiv:2506.12152 (2025) 23. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viégas, F.B., Sayres, R.: Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In: Proceedings of the 35th International Conference on Machine Learning (ICML) arXiv.1711.11279 (2018) 24. Krause, J., Montavon, G., Samek, W., Müller, K.R.: Explaining black-box models through counterfactuals. arXiv preprint arXiv:2308.07198 (2023) 25. Li, X., et al.: Assistantbench: Benchmarking general-purpose assistants. arXiv preprint arXiv:2407.15711 (2024) 26. Lundberg, S.M., Lee, S.I.: unified approach to interpreting model predictions (2017), advances in Neural Information Processing Systems arXiv 1705.07874 27. Meng, K., Huang, V., Steinhardt, J., Schwettmann, S.: Introducing docent. https: //transluce.org/introducing-docent (March 2025) 28. Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., Carter, S.: Zoom in: An introduction to circuits. Distill https://doi.org/10.23915/distill.00024 (2020) 29. OpenAI: Gpt-4.1 model documentation (2025), https://openai.com/index/ gpt-4-1/ 30. OpenAI: Introducing o4-mini (2025), https://openai.com/index/ introducing-o3-and-o4-mini/ 31. OpenAI: Openai gpt-5 system card (2025), https://openai.com/index/ gpt-5-system-card/ 32. Raza, S., Pour, P.O., Bashir, S.R.: Fairness in Machine Learning meets with Equity in Healthcare. In Proceedings of the AAAI 2023 Spring Symposium: Responsible Medical AI, Design, and Operationalization. arXiv:2305.07041 (2023) 33. Reyes, M., Meier, R., Pereira, S., Silva, C.A., Dahlweid, F.M., Tengg-Kobligk, H.v., Summers, R.M., Wiest, R.: On the interpretability of artificial intelligence in radiology: challenges and opportunities. Radiology: artificial intelligence 2(3), e190043 (2020). https://doi.org/10.1148/ryai.2020190043 34. Ribeiro, M.T., Singh, S., Guestrin, C.: \"why should trust you?\": Explaining the predictions of any classifier (2016), published in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2016) arXiv:1602. 35. Selvaraju, R.R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., Batra, D.: Gradcam: Why did you say that? arXiv preprint arXiv:1611.07450 (2016) 24 S. Chaduvula et al. 36. Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., Yao, S.: Reflexion: An autonomous agent with dynamic memory and self-reflection. In: NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following arXiv:2303.11366 (2023) 37. Shukor, M., Le, H., Requeijo, J., Lavoie, S., Maier, M.J., Titov, I.V.: conceptbased explainability framework for large multimodal models. Advances in Neural Information Processing Systems arXiv:2406.08074 (2024) 38. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013) 39. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In: Proceedings of the 34th International Conference on Machine Learning arXiv:1703.01365 (2017) 40. VibrantLabsAI: Agentic or tool use. https://docs.ragas.io/en/stable/ concepts/metrics/available_metrics/agents/ (2025) 41. Vig, J.: multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1904.02679 (2019) 42. Wachter, S., Mittelstadt, B., Russell, C.: Counterfactual explanations without opening the black box: Automated decisions and the gdpr. Harvard Journal of Law & Technology arXiv:1711.00399 (2017) 43. Wang, K., Variengien, A., Conmy, A., Shlegeris, B., Steinhardt, J.: Interpretability in the wild: circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593 (2022) 44. Wei, J., Wang, X., Schuurmans, D., Bosma, M., et al.: Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022) 45. Yao, S., Shinn, N., Razavi, P., Narasimhan, K.: τ-bench: benchmark for toolagent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045 (2024) 46. Yao, S., et al.: React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022)"
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "Mayo Clinic, Rochester, MN, USA",
        "Vector Institute for Artificial Intelligence, Toronto, Canada"
    ]
}