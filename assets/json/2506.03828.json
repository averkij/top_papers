{
    "paper_title": "AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance",
    "authors": [
        "Dhaval Patel",
        "Shuxin Lin",
        "James Rayfield",
        "Nianjun Zhou",
        "Roman Vaculin",
        "Natalia Martinez",
        "Fearghal O'donncha",
        "Jayant Kalagnanam"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI for Industrial Asset Lifecycle Management aims to automate complex operational workflows -- such as condition monitoring, maintenance planning, and intervention scheduling -- to reduce human workload and minimize system downtime. Traditional AI/ML approaches have primarily tackled these problems in isolation, solving narrow tasks within the broader operational pipeline. In contrast, the emergence of AI agents and large language models (LLMs) introduces a next-generation opportunity: enabling end-to-end automation across the entire asset lifecycle. This paper envisions a future where AI agents autonomously manage tasks that previously required distinct expertise and manual coordination. To this end, we introduce AssetOpsBench -- a unified framework and environment designed to guide the development, orchestration, and evaluation of domain-specific agents tailored for Industry 4.0 applications. We outline the key requirements for such holistic systems and provide actionable insights into building agents that integrate perception, reasoning, and control for real-world industrial operations. The software is available at https://github.com/IBM/AssetOpsBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 8 2 8 3 0 . 6 0 5 2 : r AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance Dhaval Patel1 Shuxin Lin1 James Rayfield1 Nianjun Zhou1 Roman Vaculin1 Natalia Martinez1 Fearghal Odonncha2 Jayant Kalagnanam 1IBM Research - Yorktown 2IBM Research - Ireland pateldha@us.ibm.com, shuxin.lin@ibm.com, jtray@ibm.com, jzhou@us.ibm.com, vaculin@us.ibm.com, Natalia.Martinez.Gil@ibm.com, feardonn@ie.ibm.com, jayant@us.ibm.com *Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "AI for Industrial Asset Lifecycle Management aims to automate complex operational workflowssuch as condition monitoring, maintenance planning, and intervention schedulingto reduce human workload and minimize system downtime. Traditional AI/ML approaches have primarily tackled these problems in isolation, solving narrow tasks within the broader operational pipeline. In contrast, the emergence of AI agents and large language models (LLMs) introduces nextgeneration opportunity: enabling end-to-end automation across the entire asset lifecycle. This paper envisions future where AI agents autonomously manage tasks that previously required distinct expertise and manual coordination. To this end, we introduce AssetOpsBencha unified framework and environment designed to guide the development, orchestration, and evaluation of domain-specific agents tailored for Industry 4.0 applications. We outline the key requirements for such holistic systems and provide actionable insights into building agents that integrate perception, reasoning, and control for real-world industrial operations. The software is available at https://github.com/IBM/AssetOpsBench."
        },
        {
            "title": "Introduction",
            "content": "Industrial assets such as data center chillers [20] and wind farms [18] are complex, multi-component systems that generate vast amounts of multi-modal dataincluding time-series sensor readings, textual inspection and workorder records, operational logs, and imagesthroughout their lifecycle. The ability to monitor and interpret this heterogeneous data from diverse sources such as IoT SCADA (Supervisory Control and Data Acquisition) sensors, operational KPIs, failure mode libraries, maintenance work orders, and technical manuals is key for effective Asset Lifecycle Management (ALM). However, subject matter experts such as maintenance engineers, site operators, and plant managers face considerable challenges in synthesizing insights from these disparate data streams to support timely and condition-aware decisions. The scale, assets semantic diversity, and applicationspecific contexts often render traditional monitoring and management systems inadequate. To address these challenges, the research and industrial communities are increasingly turning to AI agentsautonomous, goal-driven systems capable of integrating data across silos, reasoning over complex conditions, and triggering actions. AI agents are particularly promising in the context of Industry 4.0, where the confluence of real-time IoT telemetry, enterprise asset management (EAM) systems (e.g., IBM Maximo [10]), and reliability engineering frameworks necessitates scalable and Preprint. Figure 1: Complex Industrial Asset - Data Centers managing Chiller and Air Handling Units(AHUs) intelligent automation. These agents promise to support wide range of industrial workflows, from anomaly detection to maintenance scheduling, by bridging the gap between raw sensor data and business-level insights. Despite recent advances in agent-based systemssuch as ReAct[29], HuggingGPT[22], Chameleon[16], and Generalist Agents[5, 17]a gap remains in adapting these innovations for realworld industrial settings. Most recent domain and application specific benchmarks (e.g., ITBench[11], SWE-bench[2], Customer Support Benchmarks) are tailored toward machine learning, IT or customerservice domains and do not address the unique challenges of industrial applications, such as data modality diversity, business object complexity (e.g., failure mode, work orders, asset hierarchies), and task collaboration across multiple operational personas (reliability modeling by expert and time series modeling based on data scientist). This paper introduces AssetOpsBench, the first benchmark framework designed to evaluate AI agents for real-world industrial asset management tasks. AssetOpsBench offers principled, domaingrounded approach to developing, evaluating, and comparing multi-agent systems operating across diverse industrial data environments. It includes: catalog of domain-specific AI agents, including an IoT agent, failure mode to sensor mapping (FMSR) agent, foundation model-driven time series analyst (TSFM) agent, and work order (WO) agent - each targeting specific modalities and tasks curated dataset of 140+ human-authored natural language queries, grounded in real industrial scenarios, covering tasks such as sensor-query mapping, anomaly explanation, and failure diagnosis. simulated industrial environment based on CouchDB-backed IoT telemetry system and multi-source dataset, enabling end-to-end benchmarking of multi-agent workflows. An automated evaluation framework with prescription of six evaluation metrics (e.g., task completness, retrieval accuracy, result verification, correct sequences, clarity and justification comparative analysis of architectural paradigmsTool-As-Agent vs. Plan-Executor highlighting tradeoffs between interleaved decomposition or decomposition-first systematic procedure for the automated discovery of emerging failure modes in multiagent systems, extending beyond fixed taxonomies. key insight from our study is that domain complexity in industrial settings necessitates multiagent approachwhere specialized agents are developed for isolated tasks, then orchestrated to solve composite problems. For example, sensor data may be handled by IoT agent, while fault history is managed by failure strategy library agent. These agents must collaborate intelligently to answer user queries like Why is the chiller efficiency dropping?\"queries that blend physical reasoning, historical correlation, and operational semantics. Furthermore, the design of agent workflows must respect the natural language and intent patterns used by industrial end users. Unlike IT users, operators and engineers often refer to assets in physical or operational terms (e.g., chiller performance,\" oil temperature spike\") rather than database fields or ontologies. Crafting robust benchmarks requires capturing this domain-specific linguistic variance, ensuring agents not only retrieve correct answers but also follow reasoning patterns aligned with domain expectations."
        },
        {
            "title": "2 Related Work",
            "content": "Generalist Agents. The development of generalist agents capable of orchestrating multiple sub-agents to accomplish complex tasks has emerged as prominent research direction. This paradigm is evident across various domains: web-based systems such as Magentic [5] and CUGA [17], multimodal agents like GEA [25], and software engineering platforms including HyperAgent [9], ChatDev [21], and MetaGPT [6]. These agents typically employ predefined sets of sub-agentse.g., terminals, browsers, code editors, file explorerseach assigned specific functional roles to facilitate task decomposition and planning. While this architecture enables targeted integration and task specialization, it often lacks flexibility. Most systems adopt hard-coded reasoning paradigms such as Plan-Execute, ReAct or Reflect, limiting the capacity to support new agent types, adapt to novel task distributions, or experiment with alternative coordination strategies such as AOP [15], Prospector [12]. Domain-Specific Agents. Solving specialized tasks often requires domain-specific capabilities, prompting the development of tailored benchmarks such as MLEBench[2] and MLAgentBench[8] Arena. These frameworks evaluate agents on diverse set of machine learning problemsclassification, regression, and othersacross multiple modalities including tabular and image data. They simulate end-to-end workflows, from resolving GitHub issues to automating model training and evaluation pipelines. In this context, the concept of the AI Research Agent has gained traction, referring to agents built for scientific discovery and iterative experimentation. notable example is MLGym [19], modular benchmark for developing and evaluating research agents in machine learning workflows. However, most current benchmarks lack support for temporal data modalities such as time series, which are critical in domains like forecasting, anomaly detection, and asset health monitoring. Application-Specific Agents. Agent-based automation is also advancing in operational settings, such as IT operations and compliance monitoring. Frameworks developed under initiatives like ITBench [11] and AIOpsLab[4] aim to replicate real-world scenarios involving site reliability engineering, diagnostics, and system auditing. These systems reinforce the importance of application-specific benchmarks, catered toward persona, that not only evaluate agents across structured tasks but also expose capability gaps and drive innovation in reasoning and orchestration strategies. Nevertheless, current benchmarks in this space tend to be narrow in scope, lacking the generality and composability required to assess agent performance across diverse, multi-agent environmentsespecially those involving cross-modal reasoning or domain-specific tool usage. Fine-Tuned and Compact Models. Complementary to architectural advances, recent work has focused on improving agent performance via fine-tuned language models. So-called Large Action Models (LAMs) are designed to execute structured actions within environments, often trained on large-scale datasets to support planning, sequencing, and low-level execution. Systems such as TaskBench[23], xLAM[31], AgentGen [7], AgentBank [24], AgentRM [28], FireAct [3], and ActionStudio [30] exemplify this trend. These models are frequently trained in grounded settingse.g., Windows-based environments [26]and evaluated across diverse task categories including arithmetic, programming, and web-based interaction. While effective, these approaches are typically limited to textual or simulated web environments and have not yet demonstrated broad applicability to more complex or industrial automation tasks involving hybrid agent compositions. Open Challenges. Despite these advances, several gaps remain. First, there is lack of benchmark datasets targeting industrial asset domains, particularly those involving condition-based monitoring, predictive maintenance, and automated diagnostics. Second, time-series datawhich plays central role in industrial and infrastructure-related applicationsremains underrepresented in existing agentic benchmarks. Finally, few systems support orchestration across heterogeneous agents, including those based on text, code, or simulations, nor do they offer modular reasoning strategies adaptable to complex, multi-agent workflows. Addressing these gaps is essential to advance general-purpose agent intelligence in high-stakes, real-world domains."
        },
        {
            "title": "3 Problem Definition: Intelligent Agent-Based Asset Operations",
            "content": "AssetOpsBench aims to establish generalist agent framework for managing the lifecycle of physical assets, integrating multiple domain-specific agents and suite of application-specific tasks for systematic evaluation. It encompasses comprehensive set of operational and analytical tasks that arise 3 Figure 2: Architecture of the Multi-Agent System: Time Series Foundation Model (TSFM) Agent, Failure Mode Sensor Relations (FMSR) Agent, Work Order (WO) Agent . across the asset lifecycle. The initial release of the benchmark focuses on scenarios commonly posed by domain expertssuch as maintenance engineers, reliability specialists, and facility plannerswho translate operational needs into data-driven actions. These scenarios cover key tasks including anomaly detection, root cause analysis, fault explanation, predictive maintenance planning, work order bundling, and service request initiation. For example, user might request: Help configure an anomaly detection model to monitor power consumption of CUXP. Trigger alerts when usage is projected to exceed 8 Watts above the maximum deviation observed over the past 30 days. Such task enables timely corrective actions such as service request creation to mitigate potential issues. Figure 2 illustrates the foundational components of our proposed framework for intelligent agentbased asset operations, drawing inspiration from recent advances in generalist agents as previously discussed in related work. At the core is the AssetOps Agent, which functions as global coordinator. It interprets high-level user queries expressed in natural language, decomposes them into structured subtasks, delegates these to specialized functional sub-agents (IoT, TSFM, etc), and integrates their outputs into coherent responses. The architecture supports on-demand instantiation of agents, dynamic task planning, and reactive executioncapabilities essential for operating in complex, variable industrial environments. Formally, given query or task τ , the objective is to generate valid plan π Π, leverage memory to propagate relevant context, coordinate appropriate agents Ai for task fulfillment, and produce an output that aligns with the intended goal and operational constraints. For brevity, the Appendix A.1 discuss the mathematic formulation of agent oriented planning and A.2 gives framework detail. In this paper, we introduced structured task taxonomy aligned with the stages of the asset management lifecycle. Such taxonomy enables consistent and scalable scenario generation for benchmarking. As illustrated in Figure 3, the taxonomy begins with Asset Configuration, encompassing activities such as retrieving Failure Mode and Effects Analysis (FMEA) documentation and selecting performance KPIstypically carried out by reliability engineers. It progresses to Model Selection and Analysis, where data scientists apply anomaly detection models (e.g., Time Series Foundation Model, ML Models) and use LLM-powered retrieval to surface relevant historical failures. In the Monitoring and Execution phase, operations teams manage live telemetry, refine detection models, and enforce safety guardrails. Finally, the Maintenance and Response phase focuses on actionable outputs: generating work orders, summarizing system health, and prioritizing interventionstasks typically handled by maintenance engineers. Figure 3: Exemplar AssetOps Task Hierarchy: Detailed hierarchy is given in Appendix B."
        },
        {
            "title": "4 AssetOpsBench",
            "content": "AssetOpsBench comprises realistic multi-sources dataset, over 140+ manually constructed task scenarios, and benchmarking environment that includes the design of task specific AI agents and evaluation framework. Each task scenario reflects essential day-to-day capabilities expected of agents operating in realistic industrial settings (see Section 4.2). These tasks typically require accessing domain-or-task-specific data, which we briefly discuss in Section 4.1. Finally, we describe the design of agents and tools that enable seamless communication within an argentic framework. 4.1 Multi-Source Dataset key distinguishing feature of AssetOpsBench is its integration of richly structured, expert-curated multi-source data that reflects the complexity of real-world industrial asset operations. As shown in Table 1, the benchmark includes over 2.3 million sensor data points across 6 assets (4 Chillers and 2 AHUs), capturing time-series signals such as chiller return temperature, load percentage, and condenser water flow. The structured failure models, derived from Failure Mode Effects Analysis (FMEA) records, encompass 53 failure entries across three equipment assets. FMEA provides provide detailed insights into the physical locations of failures, degradation mechanisms (such as wear and erosion), and the influencing factors (including runtime, fluid conditions, and shock loading) that contribute to each failure. Work order histories span 4.2K records across 10+ assets and incorporate ISO-standard failure codes, event timestamps, and linkages to alerts and detected anomalies. Table 1: Key Data Modalities in the AssetOpsBench Dataset with 3 Example Fields: *Sensor Data - dataset-level metadata is included in Croissant file due to limitation of Croissant Data Source Field Description Sensor Data* # Industrial Assets: 6 Quantity: 2.3M points Chiller Return Temp. Chiller % Loaded Condenser Water Flow Measures temperature of water returning to chiller Indicates current load as fraction of the maximum Indicates the current flow rate through the condenser FMEA # Industrial Assets: 3 Quantity: 53 records Failure Location / Comp. Degradation Mechanism Degradation Influences Subsystem/part where failure occurs (e.g., bearings,) Physical process driving failure (e.g., wear, erosion) Stressors like runtime, fluid quality, or shock loading Work Orders # Industrial Assets: 10+ Quantity: 4.2K records ISO Failure Code Event Log Timestamp Linked Anomaly / Alert Standardized classification of the failure category. Time-marked entry recording an operational event References to alerts or anomalies tied to work order Additionally, the operational system generates temporal sequence of alarm logs and leverages domain-specific technical rules, enabling contextual grounding of operational anomalies. This diverse data foundation facilitates comprehensive evaluation of decision-making, tool usage, and multi-hop reasoning in industrial environments. Full schema details are provided in Appendix D. 4.2 Scenario Design and Coverage Each scenario in AssetOpsBench represents structured operational query grounded in the lifecyclealigned task taxonomy (Figure 3) and asset-specific datasets  (Table 1)  . Each scenario is formalized as: = id , type, text, category, form where id is unique identifier; type specifies the task type (e.g., knowledge retrieval, analytical); text is the natural language query; category denotes the operational domain (e.g., anomaly detection); and form defines the expected output (e.g., explanation, API call, action plan). Scenarios are classified into two types: (1) single-agent utterances, which target specific agent (e.g., IoT, TSFM, FMSR), and (2) multi-agent tasks, which span multiple agents and require coordinated reasoning. As shown in Table 2, AssetOpsBench includes total of 141 scenarios with 99 single-agent scenarios and 42 multi-agent scenarios. The goal is to test an agents ability across four capability dimensions: (cid:229) Tool-Centric (e.g., tool and API interaction), Skill-Centric (e.g., analytical reasoning), 5 Table 2: Examples of Scenario with their Subtypes (Aligned with Task Taxonomy - Figure 3) Agent Group Subtype Task Descriptions Capability TSFM Agent # Scenarios: 23 Work Order Agent # Scenarios: Multi-Agent (End-to-End) Tasks # Scenarios: 42 Forecasting Model Tuning Anomaly Detection Hybrid Tasks Model Capabilities Predict future KPI trends over time windows Select or refine time series models for accuracy Identify deviations in operational behavior Combine prediction with anomaly evaluation Query TSFM model limits and configurations Retrieval & Filter Event Summary Scheduling RCA & Alert Review Perform root cause or alert logic review Link alerts or KPI trends to work orders KPI-based Reco. Filter work orders by asset, location, or time Summarize logs or alerts over time windows Recommend or optimize work order sequences Knowledge Query Failure Reasoning Sensor Mapping Sensor Inventory Other Tasks involving anomaly detection or forecasting Uses degradation models and causal logic Maps failure modes to sensors Retrieves installed sensors on an asset Multi-step inference or decision-making (cid:243) (cid:229) (cid:243) (cid:229) (cid:229) (cid:243) (cid:243) (cid:128) (cid:229) (cid:229) (cid:128) (cid:243) Domain-Centric (e.g., context-aware decision-making), and (cid:128) LLM-Centric (e.g., languagebased generalization across tasks). Each scenario is associated with an utterance to complete task. Table 2 summarizes the distribution of scenario subtypes and their alignment with the task taxonomy. Utterance-507 represents an (cid:128) LLM-Centric scenario, where the agent must recognize that forecasting task is redundant in the presence of zero-valued sensor readingindicating that the machine may not be operating. The agent is expected to bypass unnecessary computation and recommend halting diagnostics to address the root issue directly. In contrast, Utterance-511 exemplifies Skill-Centric task, requiring the agent to correlate energy consumption with power input variable and construct corresponding model. This scenario tests the agents analytical reasoning over telemetry data to uncover functional relationships. Detail for other scenario is in Appendix C. 4.3 Agent-Centric Evaluation and Tooling Design AssetOpsBench employs baseline reasoning strategies such as ReAct [29] and CodeReAct [27] to implement domain-specific agents including TSFM, IoT, WO, and FMSR. Each agent is equipped with tailored set of toolsfor example, the IoT agent uses seven specialized tools. global coordinator, AssetOpsAgent, enables collaboration among these agents and can be instantiated using ReAct or Plan-Execute strategy. The framework supports flexible experimentation with alternative reasoning strategies like RAFA to explore both single-agent efficiency and multi-agent coordination. To further enhance performance, we introduce an output augmentation mechanism that generates semantically enriched, self-descriptive outputs. This enables agents to trigger follow-up actions, ask clarifying questions, and make informed decisions. Listing 1 shows an example output for TSFM Forecasting tool. Such standardized enriched outputs promote interoperability, allowing LLM agents to reason over multi-modal data without custom integration. We have also designed similar messaging pattern across agents. Full schema specifications are provided in Appendix E. Listing 1: Structured output from the tsfm_forecasting tool. { } \" type \": \" metadata \" , \" status \": 1 , \" error_message \": \"\" , \" toolname \": \" tsfm_forecasting \" , \" datastructure \": \" dataframe \" , \" datamodel \": \" time series \" , \" results \": \" target_prediction ,...\" , \" results_file \": \"...\""
        },
        {
            "title": "5 Experiments and Leaderboard",
            "content": "We discuss the automatic evaluation protocol used to build an initial leaderboard. Each scenario in AssetOpsBench is paired with characteristic forma structured specification that defines both the expected final output and the intermediate reasoning or procedural steps required to reach it. This form acts as the ground truth for assessing agent behavior and supports rubric-based scoring performed either by an automated evaluation agent or LLM as judge [1, 14, 26]. The evaluation agent takes the trajectory output, the original question, and the characteristic form as input to produce six-dimensional qualitative score. These six qualitative metrics are derived based on experimental observations and common sense principles. We define the evaluation agent as scoring function: : (Q, , C) = (y1, y2, . . . , y6, ytext, yskip) where: is the original task query, is the agents trajectory output (including intermediate reasoning and final output), is the characteristic form (ground-truth specification), (y1, . . . , y6) [0, 1]6 are rubric-based scalar scores corresponding to the following qualitative dimensions: y1: Task Completeness Are all required steps and sub-tasks completed? y2: Data Retrieval Accuracy Was the correct data retrieved and used? y3: Result Verification Is the final result logically and factually correct? y4: Agent Sequence Does the agent follow coherent and logical sequence of actions? y5: Clarity and Justification Is the explanation clear and the reasoning well justified? y6: Hallucinations Does the output avoid fabricated or irrelevant information? ytext is free-form natural language feedback (e.g., suggestions or rationale), yskip {0, 1} is binary flag indicating whether evaluation was skipped due to missing or invalid input. To quantify agent effectiveness for scenario evaluations, we employ the Passk metric. In contrast to the more widely used Pass@kwhich measures the likelihood that at least one of independent attempts is successfulPassk estimates the probability that an agent succeeds on all attempts. This more stringent criterion better reflects the reliability requirements of industrial applications, where retrying failed attempts may be infeasible and consistent behavior is critical for production systems [13]. In our benchmark, we report Pass1 by default, as agents are executed only once per task instance but evaluation agent is executed 5 times to extract out the above performance metrics. We have set the sampling temperature to zero while executing the AgentOpsAgent and 0.3 for executing the evaluation agent. All reported results in this work follow this configuration. 5.1 AssetOpsBench Leaderboard We conducted series of benchmark experiments to evaluate diverse set of language models, including closed-source models (e.g., gpt-4.1), frontier open-source models (e.g., llama-4-maverick, llama-4-scout, mistral-large, llama-3-405b), and medium-to-small open-source models (e.g., llama-3-70b, granite-3-8b). At present, we evaluated two different agentic strategies: Tool-as-Agent and Plan-and-Execute. Tools-As-Agents Result. Figure 4 show that gpt-4.1 leads across nearly all metrics. llama-4-maverick also performs competitively, particularly in result verification (60%) and clarity (78%), with low hallucination rate (11%). In contrast, smaller models such as granite-3-8b and llama-3-3-70b underperform across most dimensions. The comparatively lower baseline of smaller models highlights the potential of targeted fine-tuning approachessuch as those adopted in LAM frameworkto significantly enhance their capabilities. Our default approach is Tool-as-Agent. Plan-and-Execute Result. Figure 5 shows that mistral-large leads overall, achieving the highest scores in task completion (46.5%), action sequencing (58.1%), and competitive performance across 7 Figure 4: AssetOpsBench Leaderboard: Model Performance for Agent-As-Tool Approach Figure 5: AssetOpsBench Leaderboard: Model Performance for Plan-Execute Approach other dimensions. llama-4-maverick demonstrates balanced performance, particularly in clarity and justification (57%) and result verification (44%), while maintaining moderate hallucination rate (47%). In contrast, models like granite-3-8b and llama-3-70b show lower scores across most metrics, especially in action sequence planning and factual consistency, with hallucination rates exceeding 63%. Interestingly, gpt-4.1 achieves consistent mid-range performance across all axes, suggesting reliability but not leadership. These results underscore the challenges of long-horizon reasoning and sequencing in plan-and-execute settings, where even frontier models exhibit significant room for improvement. Additional observation about run-time is included in Appendix F.1. Human Validation. To assess the reliability of using LLMs as automatic evaluators for benchmarking tasks, we compare model-generated judgments against human annotations on sample of 40 tasks. Each task is evaluated along six dimensions by four domain experts, all operating under the same information constraints as the LLMs. Among the candidate models, llama-4-maverick shows the strongest alignment with human annotations, achieving 75% accuracy and Cohens Kappa score of 0.55, outperforming gpt-4.1 (69% accuracy, 0.44 Kappa). Based on this high level of moderate agreement with expert judgments, we adopt llama-4-maverick as the LLM to be used for evaluation agent for our benchmark (see Appendix F.2). 8 Ablation Study. We performed ablation experiments using Tool-as-Agent method: (1) We injected 10 out-of-domain distractors (e.g., SREAgent, EchoAgent) into system and evaluated 99 single-agent scenarios. Surprisingly, model performance improved in the presence of distractorsenhancing task completion, accuracy, and reasoning scoressuggesting that distractors may trigger more deliberate reasoning or robust attention strategies in LLMs. (2) We removed all in-context examples and re-ran 65 single-agent tasks (IoT+FMSR+TSFM) for both the best-performing model (gpt-4.1) and the worst-performing model (granite-3-8b). Performance plummeted from 80% to 34% for gpt-4.1 and from 60% to 3% for granite-3-8b, indicating that in-context examples are critical for enabling ReAct-style agents to coordinate and reason effectively. Detail analysis is in Appendix F.4. 5.2 Failure Modes of AssetOpsAgent Just as industrial assets are prone to failure, recent research has begun to systematically categorize failure modes of multi-agent systems [1]. AssetOpsBench extends this line of work by supporting continuous monitoring and proactive identification of failure modes in LLM-driven agents. Our analysis draws on 881 execution trajectories generated by Agents-as-Tool approach. We adopted the paper recommendation to use gpt-4.1 to discover the distribution of 14 failure modes defined in the framework [1]. Figure 6 shows the system design leads to many failures. To allow for discovery beyond the existing taxonomy, the we adjusted the system prompt to self-discover up to two novel failure modes per trace if present. This enabled the detection of emergent and compound failure behaviors that are not fully captured by fixed taxonomies. Figure 6: Distribution of the 14 failure modes on AssetOpsBench Among the 881 trajectories analyzed, 185 included one novel failure mode beyond the current taxonomy, while 164 exhibited two distinct emergent issues. This reveals the need for extensible evaluation frameworks to capture the nuanced behaviors of real-world LLM-based agents. Frequently observed emergent failures include: Overstatement of Task Completion (122 cases, 23.8%); Extraneous or Ambiguous Output Formatting (110 cases, 21.4%); and Ineffective Error Recovery (160 cases). These observations underscore the importance of adaptive taxonomies that evolve alongside model behaviors. Appendix F.5 provides an algorithmic procedure for new failure mode discovery."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "This paper introduces formalized framework for AI agents for Industrial Assets, encompassing comprehensive taxonomy, over 140 diverse scenarios derived from multi-source data, and standardized evaluation methodology. The Tools-As-Agent paradigm offers promising approach for orchestrating multi-agent interactions. However, our current setup assumes that API access to the environment is cost-free and unconstrained. In future work, we plan to introduce realistic environment constraintssuch as compute limitations and API usage coststo better reflect industrial settings and improve the robustness and efficiency of agent behavior."
        },
        {
            "title": "References",
            "content": "[1] CEMRI, M., PAN, M. Z., YANG, S., AGRAWAL, L. A., CHOPRA, B., TIWARI, R., KEUTZER, K., PARAMESWARAN, A., KLEIN, D., RAMCHANDRAN, K., ET AL. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657 (2025). [2] CHAN, J. S., CHOWDHURY, N., JAFFE, O., AUNG, J., SHERBURN, D., MAYS, E., STARACE, G., LIU, K., MAKSIN, L., PATWARDHAN, T., MADRY, A., AND WENG, L. MLE-bench: In The Thirteenth Evaluating machine learning agents on machine learning engineering. International Conference on Learning Representations (2025). [3] CHEN, B., SHU, C., SHAREGHI, E., COLLIER, N., NARASIMHAN, K. R., AND YAO, S. Fireact: Toward language agent finetuning, 2024. [4] CHEN, Y., SHETTY, M., SOMASHEKAR, G., MA, M., SIMMHAN, Y., MACE, J., BANSAL, C., WANG, R., AND RAJMOHAN, S. Aiopslab: holistic framework to evaluate ai agents for enabling autonomous clouds, 2025. [5] FOURNEY, A., BANSAL, G., MOZANNAR, H., TAN, C., SALINAS, E., ERKANG, ZHU, NIEDTNER, F., PROEBSTING, G., BASSMAN, G., GERRITS, J., ALBER, J., CHANG, P., LOYND, R., WEST, R., DIBIA, V., AWADALLAH, A., KAMAR, E., HOSN, R., AND AMERSHI, S. Magentic-one: generalist multi-agent system for solving complex tasks, 2024. [6] HONG, S., ZHUGE, M., CHEN, J., ZHENG, X., CHENG, Y., WANG, J., ZHANG, C., WANG, Z., YAU, S. K. S., LIN, Z., ZHOU, L., RAN, C., XIAO, L., WU, C., AND SCHMIDHUBER, J. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations (2024). [7] HU, M., ZHAO, P., XU, C., SUN, Q., LOU, J.-G., LIN, Q., LUO, P., AND RAJMOHAN, S. Agentgen: Enhancing planning abilities for large language model based agent via environment and task generation. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (New York, NY, USA, 2025), KDD 25, Association for Computing Machinery, p. 496507. [8] HUANG, Q., VORA, J., LIANG, P., AND LESKOVEC, J. Benchmarking large language models as AI research agents, 2024. [9] HUY, P. N., NGUYEN, T. N., AND BUI, N. D. Q. Hyperagent: Generalist software engineering agents to solve coding tasks at scale, 2025. [10] IBM. IBM Maximo Application Suite. Accessed: May 13, 2025. [11] JHA, S., ARORA, R., WATANABE, Y., YANAGAWA, T., CHEN, Y., CLARK, J., BHAVYA, B., VERMA, M., KUMAR, H., KITAHARA, H., ZHEUTLIN, N., TAKANO, S., PATHAK, D., GEORGE, F., WU, X., TURKKAN, B. O., VANLOO, G., NIDD, M., DAI, T., CHATTERJEE, O., GUPTA, P., SAMANTA, S., AGGARWAL, P., LEE, R., MURALI, P., WOOK AHN, J., KAR, D., RAHANE, A., FONSECA, C., PARADKAR, A., DENG, Y., MOOGI, P., MOHAPATRA, P., ABE, N., NARAYANASWAMI, C., XU, T., VARSHNEY, L. R., MAHINDRU, R., SAILER, A., SHWARTZ, L., SOW, D., FULLER, N. C. M., AND PURI, R. Itbench: Evaluating ai agents across diverse real-world it automation tasks, 2025. [12] KIM, B., JANG, Y., LOGESWARAN, L., KIM, G.-H., KIM, Y. J., LEE, H., AND LEE, M. Prospector: Improving LLM agents with self-asking and trajectory ranking, 2024. [13] LANGCHAIN. Agent evaluation metric, March 2025. Accessed: 2025-05-12. [14] LANGCHAIN. Benchmarking single agent performance, February 2025. Accessed: 2025-05-12. [15] LI, A., XIE, Y., LI, S., TSUNG, F., DING, B., AND LI, Y. Agent-oriented planning in multi-agent systems. In The Thirteenth International Conference on Learning Representations (2025). [16] LU, P., PENG, B., CHENG, H., GALLEY, M., CHANG, K.-W., WU, Y. N., ZHU, S.-C., AND GAO, J. Chameleon: Plug-and-play compositional reasoning with large language models, 2023. 10 [17] MARREED, S., OVED, A., YAELI, A., SHLOMOV, S., LEVY, I., SELA, A., ADI, A., AND MASHKIF, N. Towards enterprise-ready computer using generalist agent, 2025. [18] MONROC, C. B., BUSIC, A., DUBUC, D., AND ZHU, J. WFCRL: multi-agent reinforcement learning benchmark for wind farm control. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track (2024). [19] NATHANI, D., MADAAN, L., ROBERTS, N., BASHLYKOV, N., MENON, A., MOENS, V., BUDHIRAJA, A., MAGKA, D., VOROTILOV, V., CHAURASIA, G., HUPKES, D., CABRAL, R. S., SHAVRINA, T., FOERSTER, J., BACHRACH, Y., WANG, W. Y., AND RAILEANU, R. Mlgym: new framework and benchmark for advancing ai research agents, 2025. [20] NAUG, A., GUILLEN, A., LUNA, R., GUNDECHA, V., BASH, C., GHORBANPOUR, S., MOUSAVI, S., BABU, A. R., MARKOVIKJ, D., KASHYAP, L. D., RENGARAJAN, D., AND SARKAR, S. Sustaindc: Benchmarking for sustainable data center control. In Advances in Neural Information Processing Systems (2024), A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, Eds., vol. 37, Curran Associates, Inc., pp. 100630100669. [21] QIAN, C., LIU, W., LIU, H., CHEN, N., DANG, Y., LI, J., YANG, C., CHEN, W., SU, Y., CONG, X., XU, J., LI, D., LIU, Z., AND SUN, M. ChatDev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Bangkok, Thailand, Aug. 2024), L.-W. Ku, A. Martins, and V. Srikumar, Eds., Association for Computational Linguistics, pp. 1517415186. [22] SHEN, Y., SONG, K., TAN, X., LI, D., LU, W., AND ZHUANG, Y. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023. [23] SHEN, Y., SONG, K., TAN, X., ZHANG, W., REN, K., YUAN, S., LU, W., LI, D., AND ZHUANG, Y. Taskbench: Benchmarking large language models for task automation. In Advances in Neural Information Processing Systems (2024), A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, Eds., vol. 37, Curran Associates, Inc., pp. 45404574. [24] SONG, Y., XIONG, W., ZHAO, X., ZHU, D., WU, W., WANG, K., LI, C., PENG, W., AND LI, S. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction trajectories, 2024. [25] SZOT, A., MAZOURE, B., ATTIA, O., TIMOFEEV, A., AGRAWAL, H., HJELM, D., GAN, Z., KIRA, Z., AND TOSHEV, A. From multimodal llms to generalist embodied agents: Methods and lessons, 2024. [26] WANG, L., YANG, F., ZHANG, C., LU, J., QIAN, J., HE, S., ZHAO, P., QIAO, B., HUANG, R., QIN, S., SU, Q., YE, J., ZHANG, Y., LOU, J.-G., LIN, Q., RAJMOHAN, S., ZHANG, D., AND ZHANG, Q. Large action models: From inception to implementation, 2025. [27] WANG, X., CHEN, Y., YUAN, L., ZHANG, Y., LI, Y., PENG, H., AND HENG, J. Codeact: Your llm agent acts better when generating code. In ICML (2024). [28] XIA, Y., FAN, J., CHEN, W., YAN, S., CONG, X., ZHANG, Z., LU, Y., LIN, Y., LIU, Z., AND SUN, M. Agentrm: Enhancing agent generalization with reward modeling, 2025. [29] YAO, S., ZHAO, J., YU, D., DU, N., SHAFRAN, I., NARASIMHAN, K., AND CAO, Y. React: Synergizing reasoning and acting in language models, 2023. [30] ZHANG, J., HOANG, T., ZHU, M., LIU, Z., WANG, S., AWALGAONKAR, T., PRABHAKAR, A., CHEN, H., YAO, W., LIU, Z., TAN, J., NIEBLES, J. C., HEINECKE, S., WANG, H., SAVARESE, S., AND XIONG, C. Actionstudio: lightweight framework for data and training of large action models, 2025. [31] ZHANG, J., LAN, T., ZHU, M., LIU, Z., HOANG, T. Q., KOKANE, S., YAO, W., TAN, J., PRABHAKAR, A., CHEN, H., LIU, Z., FENG, Y., AWALGAONKAR, T. M., N, R., CHEN, Z., XU, R., NIEBLES, J. C., HEINECKE, S., WANG, H., SAVARESE, S., AND XIONG, C. xLAM: family of large action models to empower AI agent systems. In Proceedings of the"
        },
        {
            "title": "2025 Conference of the Nations of the Americas Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Volume 1: Long Papers) (Albuquerque, New\nMexico, Apr. 2025), L. Chiruzzo, A. Ritter, and L. Wang, Eds., Association for Computational\nLinguistics, pp. 11583–11597.",
            "content": ""
        },
        {
            "title": "A Agents Definition",
            "content": "This appendix provides detailed exposition of the content introduced in Section 3. In particular, we focus on the mathematical formulation of the agent architecture, followed by brief overview of the proposed framework. The goal is to formalize the agents operational components and offer foundational context for readers interested in the underlying design principles. A.1 Agent-Oriented Task Automation Problem - AOP We formalize the Agent-Oriented Problem (AOP) as tuple: AOP = A, , Π, M, where each component defines core capability of modular, agent-based reasoning and action system: = {A1, A2, . . . , An} denotes the set of available agents. Each agent Ai is characterized by its reasoning capabilities, task specialization, internal memory, and communication interfaces, enabling autonomous or cooperative execution of assigned subtasks. = {τ1, τ2, . . . , τk} is the set of tasks. Each task τ is described by triple g, M, C, where denotes the task goal (e.g., fault detection or maintenance planning), specifies the required input modalities (e.g., time-series telemetry, FMEA documents, structured metadata), and captures any domain-specific or operational constraints (e.g., time windows, asset type, or safety requirements). Π is the hierarchical plan space. plan π Π is an ordered sequence of task-agent assignments: π = [τ1, Ai, τ2, Aj, . . .] where each subtask is delegated to an appropriate agent for execution, potentially with dependencies among steps. denotes the memory system, consisting of both agent-local and shared global compoi=1, supporting context nents. It is modeled as dynamic key-value store = {(ki, vi)}m persistence, lookup, and updates throughout the planning and execution process. represents the output space. Each output is the structured or unstructured result of executing plan. Outputs may include diagnostics, action recommendations, summaries, or control triggers, depending on the task and domain. A.2 Framework Introduction AssetOpsBench uses the ReAct framework [29] in an end-to-end agent design that integrates Review Agent to verify the final answer. Figure 7 illustrates the full architecture. Figure 7: ReAct used to build individual agent The ReAct agent executes Think-Act-Observe loop, solving tasks iteratively while detecting and recovering from repetitive or ineffective actions. The Review agent validates whether the ReAct agent has successfully completed the task, ensuring output quality. Subsequent sections present the architecture in detail, highlighting the distinction between two architectural paradigmsAgents-AsTool (See SectionA.3) and Plan-Execute (See SectionA.4). A.3 Agent(s)-As-Tools/Tool-As-Agents For the Agents-As-Tools paradigm as shown in Figure 8, we implemented the following components: Figure 8: Agents-As-Tool standard ReAct (ThinkActObserve) agent loop using open source framework. In the initial setup, the number of reflections was set to oneeffectively disabling reflection. We have extended version of ReActXen, and in future, we will conduct experiments to enable multi-step reflection within ReActXen. curated list of tools, the majority of which are stub interfaces that delegate functionality to specialized sub-agents. The only standalone utility tool in this set was the JSONReader, which reads JSON object from file and returns its contents as the tools direct response. The sub-agent stubs were intentionally designed to be minimal. Each stub accepted single input parametera string called \"request\"and returned structured JSON output. The output JSON object included the following fields: answer the primary answer returned by the sub-agent, represented as plain string. review nested JSON object capturing review of the response, typically including fields such as status, reasoning, and suggestions. summary brief description of the JSON objects structure and semantics, useful for interpretability or chaining with downstream tools. The ReAct agent was initialized with standard prompt that includes: Examples for In-Context Learning small number of sample interactions for each sub-agent were provided to guide behavior. These examples followed the standard ReAct format of ThinkActObserve, illustrating how to invoke tools and interpret their responses. representative example is shown below: Tool Demonstrations These sample calls were concatenated to form comprehensive set of demonstrations for all tools available to the agent, effectively seeding it with usage patterns. 14 Listing 2: Example ReAct Prompt for IoTAgent Question : download asset history for CU02004 at SiteX from 2016 -07 -14 T20 :30:00 -04:00 to 2016 -07 -14 T23 :30:00 -04:00 for \" CHILLED WATER LEAVING TEMP \" and \" CHILLED WATER RETURN TEMP \" Action 1: IoTAgent Action Input 1: request = download asset history for CU02004 at SiteX from 2016 -07 -14 T20 :30:00 -04:00 to 2016 -07 -14 T23 :30:00 -04:00 for \" CHILLED WATER LEAVING TEMP \" and \" CHILLED WATER RETURN TEMP \" Observation 1: { \" site_name \": \" SiteX \" , \" assetnum \": \" CU02004 \" , \" to ta l_observations \": 25 , \" start \": \"2025 -03 -26 T00 :00:00.000000+00:00\" , \" final \": \"2025 -04 -02 T00 :00:00.000000+00:00\" , \" file_path \": \"/ var / folders / fz /.../ cbmdir / c328516a -643 -40 e6 (cid:44) -8701 - e875b1985c38 . json \" , \" message \": \" found 25 observations . file_path contains JSON (cid:44) array of Observation data \" } The sample calls for all the tools are concatenated to form the examples. question - the question input to ReAct tool names - the list of sub-agent tool names (plus JSONReader) tool descriptions - descriptions of the sub-agents Execution Framework. The ReAct engine is reinitialized for each question and executed until either (a) successful completionas determined by the Review component using an LLM-asjudgeor (b) maximum of ten iterations. The framework iterates through list of models (e.g., mistralai/mistral-large) and corresponding list of utterances to execute for each model. The system supports retries for failed executions. After each ReAct run, the complete trajectory and associated evaluation metrics are stored. The recorded metrics include: Question: the input query being processed Total execution time: duration of the entire ReAct loop Number of ReAct steps: count of action-observation cycles Review status: success or failure determined by the LLM-based reviewer A.4 Plan-and-Execute Plan-and-Execute. Plan-and-Execute is widely used architectural paradigm for multi-agent systems. Figure 9 depicts the implementation adopted in our work. The process initiates when user submits query, which is first processed by the Planner. The Planner decomposes the query into discrete, executable tasks. These tasks are then vetted by Reviewer component to ensure quality, completeness, and relevance. Upon approval, the Orchestrator assigns the tasks to the most appropriate agents. Each agent independently executes its assigned task and returns structured response. These responses are then aggregated by the Summarization module, which synthesizes them into coherent final output that is returned to the user. This architecture supports modularity, robustness, and interpretability across the task lifecycle. Figure 9: Plan-Execute Multi-Agent System A.5 Example Demo In the following Figures 10-12, we provide few images to showcase working of Agents-As-Tool approach for single end-to-end utterance. Figure 10: Execution is Initiated with an input query. Figure 11: The Final step of the execution"
        },
        {
            "title": "B Detailed AssetOps Hierarchy",
            "content": "This appendix presents the structured task taxonomy used in AssetOpsBench, which organizes benchmark scenarios based on key stages in the industrial asset lifecycle. The taxonomy is designed to support the creation of realistic, diverse, and role-specific evaluation tasks for intelligent agents operating in complex environments, as shown in Figure 13 for the tasks related to the industrial asset management. To illustrate how the structured task taxonomy guides agent development and evaluation, we highlight four representative agents: the IoT Agent, the FMSR Agent (Failure Mode Sensor Relations Agent), TSFM (Time Series Foundation Model) Agent, and the WO Agent (Work Order Agent). 16 Figure 12: Anomaly Detection : Final Output Figure 13: Representative Routine tasks in Asset Lifecycle Management. Among these, two agentsFMSR Agent and WO Agentare particularly useful for their domain specialization and integration depth within AssetOpsBench. Appendix B.2 presents the rationale for FMSR Agent, emphasizing its role in bridging raw telemetry with diagnostic reasoning through sensorfailure mapping. Appendix B.4 focuses on the WO Agent, which operationalizes maintenance planning and historical analysis by retrieving, filtering, and correlating work order records with asset conditions. Together, these examples demonstrate how high-level task categoriessuch as failure mode alignment, anomaly response, and intervention prioritizationare translated into grounded, data-driven agent behaviors. This alignment reinforces AssetOpsBenchs emphasis on transparency, domain specialization, and end-to-end task automation. 17 Figure 14: Mapping Example internally used by FMSR Agent B.1 Rationale for IoT Agent over Application The IoT Agent plays foundational role in supporting Asset Configuration tasks within the AssetOps framework, as illustrated in Figure 13. It enables structured access to real-time and historical telemetry data, asset metadata, and site configurations. Specifically, it allows users to query available IoTenabled sites, list all assets within given site (e.g., MAIN facility), and retrieve detailed metadata for specific assets such as chillers and air handling units (AHUs). Additionally, it provides access to time-series sensor datasuch as power input, temperature, flow rate, and system tonnageacross customizable time windows. These data queries form the backbone for monitoring tasks, model inputs, and analytics performed by downstream agents like TSFM Agent and WO Agent. Although the IoT Agent does not perform anomaly detection or failure analysis directly, it is critical enabler by delivering high-fidelity, time-aligned telemetry required for advanced applications (such as those using TSFM Agent). For example, users can retrieve the tonnage data for Chiller 6 during specific week, download metadata for Chiller 9, or access sensor values recorded during known operational event. These capabilities align with the early-phase needs of asset lifecycle managementspecifically selecting data sources and configuring metrics of interestensuring all downstream decision-making is grounded in accurate, context-rich operational data. The agents flexible query interface and knowledge and data retrieval support allow it to seamlessly integrate into automated pipelines for asset monitoring, diagnostics, and performance tracking. B.2 Rationale for FMSR Agent over Application The sensorfailure alignment generation (See Figure 14) is critical component of the AssetOpsBench benchmark, serving multiple roles in both dataset understanding and intelligent system design. Its inclusion is motivated by the following key factors: 18 1. Bridging Raw Data and Diagnostic Insight: The table explicitly maps sensor variables to relevant failure modes, establishing direct link between low-level telemetry and high-level maintenance reasoning. This supports tasks such as fault detection, root cause analysis, and feature selection for learning-based systems. 2. Alignment with FMEA Methodology: By structuring failure explanations according to the principles of Failure Modes and Effects Analysis (FMEA), the table offers formalized, interpretable view of asset health. Each sensors diagnostic role is contextualized through failure causes, effects, and detection implications. 3. Supporting Explainability and Safety: In industrial environments, operational decisions require transparency. The alignment table enhances system explainability by clarifying why given signal is relevant, how it relates to equipment health, and what operational risks it may indicate. 4. Improving Dataset Transparency: The AssetOpsBench dataset includes wide range of sensors across multiple devices. This table functions as documentation layer that improves usability, reproducibility, and understanding for researchers and practitioners engaging with the benchmark. 5. Guiding Model and Rule Development: Whether designing rule-based systems, hybrid AI architectures, or physics-informed machine learning models, well-defined mapping of sensors to failure mechanisms is foundational. It informs the construction of robust detection logic and contributes to generalizable reasoning strategies. In sum, the sensorfailure alignment table plays central role in transforming raw operational telemetry into structured, actionable insight. It provides the semantic grounding necessary for developing interpretable, reliable, and effective AI agents for real-world industrial maintenance tasks. Table 3 provides an extensive example for sensor-failure mode relation for chiller system. Table 3: Sensor Interpretation and Failure Mode Relevance in Chiller Systems - Illustrative Sensor Explanation Impact on Chiller Health / Failure Mode Relevance Condenser Leaving Temp Temperature of water leaving the condenser VFD Output Voltage Voltage output from Variable Frequency Drive Chilled water setpoint during free cooling mode Indicates compressor cycling state Indicates if chiller is in ready state leaving Overrides for manual operation Temperature evaporator Water flow through condenser loop Power input to VFD CHWSTSP in Free Mode Cycling Code Ready Status Manual Start/Stop Chilled Water Leaving Temp Condenser Flow VFD Input Power CNW Flow Hi Alarm SP Watt/Ton Indicates heat rejection efficiency; abnormal readings may signal fouling or reduced flow potential heat exchange failure. Instability may affect fan/compressor operation linked to electrical drive failure or load imbalance. Misconfiguration can lead to energy inefficiency related to control logic failure. Frequent cycles may indicate load mismatch, sensor error, or compressor stress. Persistent unavailability may reflect control override, interlock failure, or alarm lockout. May cause unscheduled runtime or safety override conditions. Deviation may suggest capacity loss or improper load conditions. Low flow may cause high pressure shutdown or heat rejection failure. Spikes may indicate motor inefficiency, overload, or harmonic distortion. May indicate bypass valve issues or overpumping. High flow setpoint for condenser loop Cooling efficiency metric Rising ratio suggests energy inefficiency or component degradation. 19 Sensor Explanation Impact on Chiller Health / Failure Mode Relevance May point to pump failure, valve issues, or airlocks. Discrepancies could signal false starts, sensor error, or runtime misreporting. May indicate bearing failure, imbalance, or mechanical looseness. Out-of-range position may imply valve actuator fault or control misbehavior. Suggests clogging, filter fouling, or flow resistance. Triggered by pump overspeed, valve overshoot, or control issues. Important for thermal load calculation and monitoring efficiency. High current may indicate overload, bearing drag, or electrical faults. Improper function may cause flow issues or unmet loads. Indicates scaling, fouling, or pump degradation. High temps may trigger thermal trips or shorten VFD lifespan. Abnormal values may suggest charge issues, expansion valve faults, or heat exchange failure. High temperature may signal bearing wear or insufficient cooling. Used for cooling load and delta-T analysis. Oil sump temperature temp to Mismatch suggests sensor/control error. Return water evaporator Reported motor run confirmation Current through VFD inverter link Setpoint in partial load mode Phase currents from VFD Used to detect imbalances, shorts, or phase High current may indicate overload or VFD stress. Improper configuration can cause energy waste or load mismatch. loss. Elevated temps reduce component life and can cause failure. Low pressure risks lubrication failure and component damage. Used as ground truth label for fault evaluation. Affects energy usage, response time, and cooling capacity. May indicate system control faults or oversized flow components. Instability can reflect power quality issues."
        },
        {
            "title": "Motor Run Status",
            "content": "Water flow through evaporator Compressor motor operational state Vibration Point #1 SP Vibration sensor setpoint"
        },
        {
            "title": "CHW Valve Position",
            "content": "CHW Differential Pressure (D/P) CHW Flow Hi Alarm SP Condenser Temp Average Amps"
        },
        {
            "title": "Return",
            "content": "drop across (location #1) Position of chilled water valve Pressure chilled water loop Alarm setpoint for high CHW flow Water temperature returning to the condenser Average motor current CHW Valve Close Control CNW Differential Pressure (D/P) VFD Internal Ambient Temp Freon Temp Control signal to close CHW valve Pressure drop in condenser loop Internal temperature of VFD Refrigerant temperature Oil Compressor Sump Temp Chilled Water Return Temp Motor Run Status RPT VFD Inverter Link Current CHWSTSP in Part Mode VFD Phase A/B/C Current VFD Converter Heat Sink Temp Compressor Oil Pressure Failure (status flag) VFD Setpoint VFD heat sink temperature Oil pressure in compressor Direct failure indicator torque comSpeed or mand High flow warning flag VFD 20 CHW Flow High Alarm VFD DC Bus Voltage DC voltage level inside Sensor Explanation"
        },
        {
            "title": "CNW Flow High\nAlarm\nCNW Flow Low\nAlarm SP\nWarning Code",
            "content": "Vibration #2/#3 SP"
        },
        {
            "title": "High condenser water\nflow warning\nLow flow alarm threshold",
            "content": "Non-critical warning status Additional vibration setpoints Impact on Chiller Health / Failure Mode Relevance May reflect valve misposition or energy inefficiency. Indicates risk of overheating or shutdown due to poor heat rejection. Helpful for early diagnostics or trend detection. Detect imbalance, wear, or mechanical degradation. B.3 Rationale for TSFM Agent over Application The TSFM Agent is purpose-built to support critical tasks within the AssetOps workflow, as outlined in Figure 2. Within Model Selection and Analysis, TSFM Agent enables forecasting of key performance indicators (KPIs) using lightweight, pre-trained foundation models such as TTM 1. Its adaptive anomaly detection framework, based on post-hoc conformal prediction, supports calibrated and interpretable anomaly scores, providing high utility for both Monitoring and Execution and Maintenance and Response. Specifically, the TSFM Agent can execute and refine models, classify anomalies based on historical deviations, and support operational guardrails by simulating expected trends under normal conditions. In downstream applications, the agents outputs can be used to summarize overall system health by tracking the frequency of anomalies across selected KPIs. These anomalies serve as foundation for maintenance recommendations, enabling preventive and reactive work order generation. TSFM Agent facilitates real-time, data-driven decision-making throughout the asset lifecycle. B.4 Rationale for WO Agent over Application The WO Agent, code based ReAct, in AssetOpsBench is designed to enable intelligent interaction with structured and unstructured maintenance records through modular data model. It operates over set of Business Objects (BOs) that represent work orders, alerts, anomalies, failure codes, and asset metadata. These BOs are categorized into five functional groups that collectively support the WO Agents decision-making capabilities. To reason over these BOs, the WO Agent is equipped with collection of analytic functions that allow it to retrieve, interpret, and act upon historical and real-time data. The agents capabilities are structured as follows: 1. Historical Reasoning via Content Objects and Knowledge Extraction: The WO Agent accesses raw maintenance data such as WorkOrders, Events, including Work orders, alerts, and anomaly Events. Knowledge extraction functions enable the agent to retrieve and filter this data by date, asset, and work order type, allowing targeted analysis and retrospective diagnostics. 2. Standardized Interpretation with Meta/Profile Objects: BOs like ISO Failure Code, AlertRule, and Equipment provide structured classification schemes. These allow the agent to categorize failures, apply semantic filters, and maintain compatibility with domain conventionscritical for aligning alerts and anomalies with actionable categories. 3. Temporal and Causal Reasoning via Statistical Functions: Leveraging relationship BOs such as Alert-Rule Mapping and Anomaly Mapping, the WO Agent applies statistical functions (e.g., Allens Interval Algebra) to detect temporal patternssuch as when alerts consistently precede failures. It also detects repeated work order cycles, helping align maintenance with actual degradation patterns instead of fixed schedules. 4. Predictive and Prescriptive Intelligence through Decision Support Functions: Using the WorkOrderRecommendation BO, the agent forecasts future work orders, recommends 1https://huggingface.co/ibm-granite/granite-timeseries-ttm-r1 21 maintenance based on alerts or KPI anomalies, and identifies opportunities for bundling related tasks. These decision support functions enable proactive scheduling and optimize resource use across the asset lifecycle. 5. Persona-Aligned Interaction and Query Resolution: The WO Agent interfaces naturally with domain personas. Maintenance engineers can explore past interventions for given failure, while planners can query upcoming work order demands or seek opportunities to consolidate tasks. These capabilities are backed by modular functions that support flexible querying and planning logic. In summary, the WO Agent is hybrid reasoning and decision-support agent built atop structured business objects and analytic functions. It connects historical insight with predictive planning, enabling lifecycle-aware maintenance interventions grounded in transparent, data-driven logic. Table 4: WO Agent Summary of Business Objects, Source, Role, and Number of Records Business Object Source Role Content Objects WorkOrder Work Order Manager Event Alert Events Anomaly Events by Aggregated Authors IoT Repository ML Generated Meta/Profile Objects ISO Failure Code ISO Primary Failure_Code AlertRule Developed by Authors Developed by Authors SME Provided Equipment SME Provided Relationship Causality Objects Alert-Rule Mapping Anomaly Mapping Relationship Causality Relationship Causality Recommendation Objects WorkOrder Recommendation Recommendation Tracks scheduled and unscheduled maintenance tasks, categorized as preventive or corrective. Consolidates event decision-making. Logs real-time alerts triggered by IoT sensors based on predefined conditions. Detects KPI deviations using machine learning for predictive maintenance. logs for tracking and Standardizes failure classification for structured maintenance analysis. Defines primary failure categories and links related secondary codes. Specifies conditions for triggering alerts based on system behaviors. Represents industrial assets, including status and specifications. Links alert rules to failure codes for automated diagnostics. Associates anomalies with failure codes for predictive insights. Count 4392 6929 1995 542 68 77 22 46 12 Suggests maintenance actions based on historical patterns. N/A Note: The design and structure of the business objects and corresponding analysis in this section are valid for other industrial asset types, such as standby generators."
        },
        {
            "title": "C Scenario Creation Principles",
            "content": "The scenarios in AssetOpsBench are crafted to evaluate broad spectrum of capabilities expected from autonomous agents in industrial settings. Each scenario is designed to challenge specific dimensions of reasoning, tool use, data interpretation, communication, and decision-making, as outlined below: 22 Reasoning and Tool Use: Assesses domain-specific reasoning such as time and schema operations, appropriate tool invocation, and structured command generation. Common failure modes include premature halts or misuse of tools. Data Handling and Forecasting: Evaluates the agents ability to interpret telemetry, detect anomalies, and configure appropriate forecasting or anomaly detection models. Tasks often require translating domain knowledge into ML configuration steps (e.g., model selection, fine-tuning). Agent Communication and Coordination: Tests multi-agent workflows involving targeted question-asking, summarization, and collaborative decision-making. Scenarios mimic how agents may delegate or escalate tasks in real settings. Workflow Orchestration and Decision-Making: Measures the agents ability to plan and manage dependent subtasks, reason under uncertainty, and terminate appropriately when faced with ambiguity or missing data. C.1 Examples We include two examples (Table 5 and Table 6) that showcase distinct behaviors of agent outputs. Readers can observe that the characteristic form varies even for problems that appear similar on the surface. Table 5: Example Knowledge Query: Energy Prediction for Chiller 9 Field ID Type Text Description 507 Knowledge Query What is the predicted energy consumption for Chiller 9 in the week of 2020-04-27 based on data from the MAIN site? Characteristic Form The expected response should confirm the successful execution of all required actions, ensuring that the correct asset (Chiller 9), location (MAIN), and time range (week of 2020-04-27) were used for data retrieval and analysis. It should specify that the agent identified the sensor name (power input sensor) and retrieved the historical energy consumption data for Chiller 9 during the specified time period. The response must also explain that the agent attempted to analyze the data for energy consumption prediction, but was unable to do so due to insufficient data, as the power input for Chiller 9 was consistently 0.0 from 2020-04-20 to 2020-04-25, indicating that the chiller was not operating."
        },
        {
            "title": "D Datasets for AssetOpsBench and Utilization by Agents",
            "content": "In this part, as extension of Section 4.1, we will zoom into the datasets utilized by the various agents of AssetOpsBench (More details of the roles of the agents in the asset lifetime management can be found at Appendix B). D.1 Sensor Telemetry Dataset for IoT Agent and TSFM Agent Both IoT Agent and TSFM Agent (Figure 2) leverage the Sensor Telemetry Dataset, which comprises sensor telemetry collected from Building Management Systems (BMS) and the SkySpark analytics platform. This dataset captures fifteen-minute interval operational data from industrial HVAC systems, specifically fleet of chillers. Each chiller unit (e.g., Chiller 4, Chiller 14) is instrumented with standardized suite of physical sensors that monitor key operational parameters in real-time. representative subset of these sensors is summarized in Table 7. These sensors record various kinematic, dynamic, thermodynamic, electrical, and operational metrics essential to assessing the performance and health of chiller systems. Measurements include water and refrigerant temperatures, 23 Field ID Type Text Table 6: Example Knowledge Query: Predicting Energy Usage for Chiller 9 Description 511 Knowledge Query Can you predict Chiller 9s energy usage for next week based on data from the week of 2020-04-27 at MAIN? Characteristic Form The expected response should confirm the successful execution of all required actions, ensuring that the correct asset (Chiller 9) and location (MAIN site) were used for data retrieval and analysis. It should specify that the agent first identified the sensors for Chiller 9, then selected the Chiller 9 Power Input sensor, and successfully retrieved the energy usage data for the specified time period. The response should confirm that the agent provided the file path where the data is stored. Additionally, it should mention that although the agent initially encountered errors while analyzing the data and making predictions, it successfully corrected its mistakes and finetuned Time Series Forecasting model using the provided data. The agent should have used the finetuned model to generate predictions for the next week, with the results being stored in the specified file. power consumption, cooling capacity (tonnage), flow rates, and system setpoints. Additionally, computed metrics such as chiller efficiency and load percentage serve as valuable real-time indicators of system performance. Table 7: Representative Sensors in the AssetOpsBench Dataset Sensor Name Description Chiller Return Temperature Supply Temperature Power Input Tonnage Condenser Water Supply to Chiller Temperature Chiller Efficiency Chiller % Loaded Condenser Water Flow Liquid Refrigerant Evaporator Temperature Run Status Setpoint Temperature Temperature of water returning to the chiller Temperature of water exiting the chiller Electrical power consumption Heat extraction rate (cooling capacity) Temperature of water supplied to the condenser Instantaneous performance metric Current load as percentage of the maximum Flow rate through the condenser Temperature of refrigerant in the evaporator Binary indicator of whether the chiller is currently operating Current setpoint for chiller operation Each sensor stream is accompanied by rich metadata, including sensor type, measurement units, physical location, and structured device tags that define device associations. The dataset captures realistic operational variability, encompassing noise, missing data, and seasonal patterns. As such, it provides robust foundation for developing and benchmarking models that require temporal reasoning, fault detection, and decision-making under uncertainty. As illustration, Figure 15 presents layered time series subplots for key chiller sensors over selected snapshot period in June 2020 for Chiller 6. Each subplot corresponds to one sensor variable, enabling clear view of temporal dynamics and inter-variable behavior. This figure provides insight into the operational profile of single chiller unit during real-world usage. 24 Figure 15: Snapshot of time series data from Chiller 6 for June 2020. Each subplot shows an individual sensors trend over time. The IoT Agent interacts with this telemetry data through structured utterances. By leveraging the standardized data provided by AssetOpsBench, the agent enables detailed, query-driven access to operational information across HVAC assets such as chillers and air handling units (AHUs) at IoT-enabled sites like the MAIN facility. Through these utterances, users can request both real-time and historical data, retrieve metadata, and download sensor readings for specific timeframes. This functionality supports knowledge and data queries, facilitating asset-level diagnostics, performance monitoring, and intelligent decision-making, even in noisy or incomplete data. On the other hand, the TSFM Agent operates on sensor telemetry dataeither retrieved via the IoT Agent or accessed directly from the sensor repositoryto perform advanced time series analysis across HVAC systems. It supports range of analytical tasks, including multivariate forecasting, and time series anomaly detection. At its core, the agent utilizes pre-trained time-series foundation models, such as TTM variants (see footnote 2). TTM (Tiny Time-series Mixture) is lightweight, pre-trained time-series foundation model designed for efficient multivariate forecasting. For anomaly detection, the TSFM Agent applies model-agnostic, post-hoc adaptive conformal method that requires no additional fine-tuning data, making it highly practical for real-world, resource-constrained deployments. By learning dynamic weighting strategies from prediction histories, it can detect distributional shifts and maintain calibrated, interpretable anomaly scores aligned with user-defined false alarm rates. Through structured utterances, users can invoke forecasting on specific variables (e.g., Chiller 9 Condenser Water Flow), fine-tune models with minimal data, or detect anomalies in historical trendsall with minimal configuration. This seamless integration of pre-trained models, adaptive analytics, and user-guided queries enables transparent, robust, and immediately deployable monitoring solutions tailored for critical industrial systems. D.2 Failure Mode Datasets for FMSR Agent The failure mode datasets in AssetOpsBench are modeled using the principles of Failure Modes and Effects Analysis (FMEA), structured framework used in reliability engineering to identify failure risks, assess root causes and effects, and inform condition-based maintenance strategies. Each 2https://huggingface.co/ibm-granite/granite-timeseries-ttm-r2 25 failure is defined by its mode, degradation mechanism, detection opportunity, and operational impact, enabling structured reasoning for both rule-based diagnostics and machine learning. Failures in the dataset are annotated at the asset and subsystem levels, with primary focus on centrifugal chillers. These failures reflect realistic degradation pathways and operational stressors derived from field experience. Each record in the failure model includes: Failure Location and Component: The subsystem or part where failure occurs, such as bearings, gearboxes, impellers, or lubrication systems. Degradation Mechanism: The underlying physical process driving the failure, including wear, erosion, oil degradation, vibration-induced fatigue, and misalignment. Degradation Influences: External or internal stressors such as run time, two-phase process fluid, personnel error, or shock loading. Functional Failure Mode: The resulting operational defect, such as decreased oil pressure, audible noise, low head pressure, or capacity loss. Detection Opportunities: Observable precursors or symptoms, including sensor readings (e.g., oil sampling, vibration signals), condition-based alarms, or inspection results. Repair Time and Criticality: Estimated downtime and classification of failure risk, supporting cost-based prioritization and scheduling. Preventive Task Type: Associated maintenance activity, such as oil analysis, vibration analysis, or visual inspection, tagged with effectiveness ratings and intervention intervals. For example, bearing weara recurring failure across chiller subsystemsmay arise from lubrication failure, misalignment, or fluid shock loading. This degradation is detectable via combination of oil analysis and vibration monitoring, with failure symptoms including increased vibration, reduced oil pressure, and audible anomalies. Similarly, impeller erosion is linked to aging and two-phase fluid exposure, typically presenting as reduced capacity and lower head pressure. Each maintenance task in the dataset is mapped to its detection mechanism and action type (e.g., condition monitoring vs. corrective repair), along with documentation on task content and recommended frequency. These structured records not only support early fault detection and diagnostics but also facilitate benchmarking of intelligent agents reasoning over real-world degradation patterns and maintenance decisions. Failures are temporally aligned with telemetry, enabling the study of degradation trajectories and pre-failure conditions. This integrated design makes the dataset suitable for supervised learning, causal inference, and evaluation of digital twins or predictive maintenance agents under realistic operating uncertainty. To utilize the failure modes and their association with the sensors, we design FMSR (Failure Mode Sensor Relations) to interpret failure mode datasets within the AssetOpsBench framework, leveraging structured FMEA (Failure Modes and Effects Analysis) principles to link sensor telemetry with degradation mechanisms and operational failures. Using annotated failure records for assets such as centrifugal chillers, the FMSR Agent builds knowledge graphs and reasoning models that connect specific failure modeslike compressor overheating, evaporator fouling, or refrigerant valve failureto their underlying causes and detectable symptoms. These failure modes are mapped to available sensor measurements (e.g., supply temperature, power input, vibration, flow rate) to identify observable precursors. For example, compressor overheating may be monitored through trends in power input, chiller efficiency, and evaporator temperature, while condenser fouling can manifest in abnormal return temperatures and flow rate deviations. Through structured utterances, users can query which failure modes are associated with specific sensors, which are critical for detecting given failure, or even construct machine learning recipes for predictive modelingsuch as anomaly models for chiller trips or excessive purging. The agent leverages this data to perform rule-based diagnostics, support causal analysis, and assist in condition-based maintenance planning. By aligning temporal sensor patterns with known failure signatures, the FMSR Agent enables explainable fault detection and root cause inference, ultimately enhancing reliability, maintainability, and transparency in HVAC operations. Table 8: Work Order Event Schema Definition Field Name wo_id wo_description collection components primary_code Type"
        },
        {
            "title": "String",
            "content": "primary_code_desc."
        },
        {
            "title": "String",
            "content": "secondary_code"
        },
        {
            "title": "String",
            "content": "secondary_code_desc. String equipment_id equipment_name String String preventive Boolean work_priority Integer actual_finish DateTime duration Duration actual_labor_hours Duration Description Unique identifier for the work order. Example: \"L247402\" Description of the work being done. Example: \"CHILLER COMP OIL ANALYSIS\" Broad group or system the work relates to. Example: \"compressor\" Specific part or component being serviced. Example: \"compressor\" Code representing the main type of work. Example: \"MT010\" Description of the primary work code. Example: \"Oil Analysis\" Sub-code under the primary category. Example: \"MT010b\" Description of the secondary code. Example: \"Routine Oil Analysis\" Unique ID of the equipment. Example: \"CU02013\" Human-readable name of the equipment. Example: \"Chiller 13\" Indicates if this is preventive maintenance. Example: TRUE Priority level of the work (e.g., 15). Example: 5 Date and time when the work was completed. Example: \"4/6/16 14:00\" Total job time. Format: HH:MM. Example: \"0:00\" Actual labor time spent. Format: HH:MM. Example: \"0:00\" Table 9: Alert Event Schema Definition Field Name equipment_id Type"
        },
        {
            "title": "String",
            "content": "equipment_name String rule_id String start_time DateTime end_time DateTime Description Unique identifier for the equipment that triggered the alert. Example: \"CWC04701\" Human-readable name of the equipment. Example: \"Chiller 1\" Identifier for the rule or condition that triggered the alert. Example: \"RUL0021\" Timestamp when the alert or event started. Example: \"11/24/20 19:00\" Timestamp when the alert or event ended. Example: \"11/24/20 23:59\" D.3 Work Order Datasets for WO Agent Table 4 provide the summary of datasets (as business objects) and the size for each dataset. Those work order datasets in AssetOpsBench provide structured view of maintenance activity across industrial assets, encompassing both preventive and corrective interventions using work orders. Each 27 Table 10: Anomaly Event Schema Definition Type Description Field Name timestamp KPI asset_name"
        },
        {
            "title": "String",
            "content": "value"
        },
        {
            "title": "Numeric",
            "content": "upper_bound"
        },
        {
            "title": "Numeric",
            "content": "lower_bound"
        },
        {
            "title": "Numeric",
            "content": "anomaly_score"
        },
        {
            "title": "Float",
            "content": "The date and time when the anomaly event was recorded. Example: \"4/26/20 14:14\" The key performance indicator being monitored (e.g., \"Cooling Load\"). The name of the asset or equipment being measured. Example: \"chiller 9\" The actual measured value of the KPI at the given timestamp. Example: 25978710 The upper threshold for the KPI. Exceeding this may indicate an anomaly. The lower threshold for the KPI. Falling below this may indicate an anomaly. score indicating how likely the data point is an anomaly (typically 0 to 1). Table 11: Mapping Table: KPI Anomalies to Failure Codes Field Name kpi_name Type String Example Description Cooling Load anomaly_type String High category String Operational Failures primary_code String OP pri._code_des. String seco._code String seco._code_des.String Cooling Incorrect Zone Operation OP004c Improperly Controlled or Shut Off Zones Name of the key performance indicator exhibiting anomaly. Indicates the direction or nature of the anomaly (e.g., High, Low, Spike). Broad class of the failure (e.g., Control System, Structural, External, Human). Primary failure code associated with the anomaly. Explanation of the primary failure code. More specific sub-code refining the root cause. Description of the secondary failure code. work order is associated with rich contextual data including equipment metadata, failure classification codes (e.g., ISO Failure Code, ISO Primary Failure Code), event logs, sensor-triggered alerts, and machine-generated anomalies. These records are linked temporally and causally, allowing agents to reason about asset history, detect recurring failure patterns, and recommend actions based on past interventions. The group of datasets distinguishes between core content objects (e.g., WorkOrders, Alerts, Events, Anomalies), metadata profiles, and relational structures that map alerts and anomalies to failure codes. The individual event tables work orders  (Table 8)  , alert events  (Table 9)  , and anomaly events  (Table 10)  capture different but complementary signals related to equipment condition and behavior. To enable integrated analysis and causal reasoning, these events are unified into common event table schema  (Table 12)  , allowing temporal alignment and cross-type relationship discovery between maintenance actions, system warnings, and performance anomalies. Table 12: Unified Event Table Schema Definition Field Name event_id Type"
        },
        {
            "title": "String",
            "content": "event_group"
        },
        {
            "title": "String",
            "content": "event_category"
        },
        {
            "title": "String",
            "content": "event_type description"
        },
        {
            "title": "String",
            "content": "equipment_id String equipment_name String event_time DateTime note String Description the the event Unique identifier for the event (can be work order ID, alert ID, anomaly ID, etc.). Example: \"WO-16170\" High-level event classification of source (e.g., \"WORK_ORDER\", \"ALERT\", \"ANOMALY\"). Sub-classification such as preventive maintenance (\"PM\"), corrective maintenance (\"CM\"), etc. Specific code/type of \"MT001\", \"RUL0021\"). Human-readable description of the event. Example: \"Vibration Analysis\" or \"Refrigerant Leak\". Unique ID of the equipment involved in the event. Example: \"CWC04701\" Name of the equipment. \"Chiller 1\" Timestamp when the event occurred or was logged. Format: YYYY-MM-DD HH:MM:SS Additional description for this event if necessary Example: (e.g., Table 13: Mapping Table: Alert Rule to Failure Code Field Name Type rule_id String Example RUL0012 rule_name String Chiller - Low Supply Temperature primary_code String CS005 primary_code_ String Control System Malfunction Description Identifier for the alert rule triggered by monitoring system. Descriptive name of the alert rule logic or threshold condition. ISO failure code associated with the likely root cause. Human-readable explanation of the failure code. In addition, to support the linkage of failure code over the events, we provide two mapping tables: one that connects alert rules to likely failure codes, and another that maps KPI-based anomalies to structured failure categories (Tables of 13 and 11). These mappings enable agents to infer probable root causes from real-time signals and integrate data-driven insights with expert failure taxonomies. This help us to develop WO agent to support grounded evaluation of diagnostic reasoning, task generation, and repair recommendation. More particularly, the WO agent analyze historical work orders to identify repeated maintenance issues and improve task scheduling. It processed historical work order, alerts (from IoT Agent) and anomalies (from TSFM agent) event, linking them to failure codes to support predictive maintenance recommendations. In the potential industrial applications, WO agent can complete to tasks of automating the interpretation of maintenance data, predicting future work orders, and bundling related tasks to reduce operational downtime. 29 AssetOp Agent Design - Orchestra Appendix A.2 already discussed how we implemented the two approaches for orchestra role: Toolas-Agent and Plan-Executor. In this appendix, we provide additional detail on how we enable communication. Listing 3 outlines how the FMSR agent packages its reasoning output into structured message for downstream agents or evaluators. The custom_json function formats the response to include the final answer, peer review section (comprising status, reasoning, and suggestions), and reflection field. Additionally, natural language message is synthesized to summarize the execution result, enhancing transparency and interpretability in multi-agent settings. This output acts as compact yet comprehensive communication protocol for reasoning agents collaborating in complex task pipeline. Listing 3: Formatted response message from FMSRAgent def custom_json ( obj ) : if isinstance ( obj , FMSRResponse ) : return { \" answer \" : obj . answer , \" review \" : { \" status \" : obj . review [ \" status \" ] , \" reasoning \" : obj . review [ \" reasoning \" ] , \" suggestions \" : obj . review [ \" suggestions \" ] , } , \" reflection \" : obj . reflection , \" message \" : ( \" am FMSR Agent , and have completed my task . \" \" The status of my execution is { obj . review [ (cid:44) status ]} . \" \" also received review from the reflection (cid:44) agent ; \" \" suggestions are included in the review field for (cid:44) further insights . \" ) , } raise TypeError ( \" Cannot serialize object of type { type ( obj ) } \" (cid:44) )"
        },
        {
            "title": "F Additional Experiments",
            "content": "F.1 AssetOpsBench: Execution Efficiency In this section, we analyze AssetOpsBench execution efficiency of 7 LLMs, complementing the Leaderboard results in Section 5.1. Tables 14 and 15 present results from two multi-agent implementations. Metrics include the average number of steps taken per task and the average runtime (in seconds) per task. In the Agents-as-Tools execution mode, most models demonstrate relatively stable planning behavior across both single-agent and multi-agent tasks. Compared to the plan-and-execute setting, models here generally take more steps but operate with greater runtime efficiency. gpt-4.1 again exhibits strong performance, balancing higher number of steps with moderate runtime, indicating precise control over tool invocation. Interestingly, llama-3-70b-instruct shows competitive efficiency, achieving the lowest runtime in both task categories despite slightly fewer steps, suggesting quicker tool usage or lower overhead per step. On the other hand, mistral-large exhibits extreme runtime variability, skewed by pathological case involving prolonged JSONReader calls over large datasets. These results suggest that while tool-based execution benefits from more direct action control, its efficiency is highly sensitive to the invoked tools and data volume. In the Plan-and-Execute setting, the number of steps required for single-agent tasks closely mirrors those of multi-agent tasks, indicating tendency among LLMs to over-plan even for relatively 30 Table 14: Execution Statistics for Agents-As-Tools: Average Steps and Runtime Per Task Model Single-Agent Tasks Multi-Agent Tasks Steps Runtime (sec) Steps Runtime (sec) gpt-4.1 mistral-large llama-3-405b-instruct llama-3-70b-instruct llama-4-maverick-17b-128e-instruct llama-4-scout-17b-16e-instruct granite-3-3-8b 6.0 2.4 4.9 2.6 4.8 2.5 3.9 1.6 4.3 1.5 4.4 2.0 5.3 3.1 104 178 347 19871 250 773 101 107 120 258 101 87 197 240 6.4 2.5 5.2 2.2 5.6 2.2 4.3 2.1 4.5 1.7 5.8 2.9 6.6 3.6 218 371 289 443 255 248 151 220 137 175 178 157 228 1 High standard deviation is due to one outlier task requiring nearly 5 hours. It repeatedly invoked the JSONReader tool to process two years of historical data. simple objectives. This pattern reflects limited sensitivity to task complexity during the planning phase. Among all evaluated models, gpt-4.1 consistently outperforms others, demonstrating both minimal average steps and lowest runtime, particularly in multi-agent tasks. This suggests that gpt-4.1 leverages more effective internal representations and decision strategies, enabling efficient decomposition and execution of plans. In contrast, models like granite-3-3-8b and llama-3-70b-instruct show pronounced inefficiency, often executing significantly more steps and incurring higher computational costs. These results highlight critical trade-off in plan-execute agents: while the architecture enforces task structure, its effectiveness heavily depends on the models reasoning efficiency. Models lacking strong planning priors or execution alignment tend to generate unnecessarily long or suboptimal action sequences, especially in low-complexity settings. Table 15: Execution Statistics of Plan-and-Execute Agents: Average Steps and Runtime per Task Model Single-Agent Tasks Multi-Agent Tasks Steps Runtime (sec) Steps Runtime (sec) gpt-4.1 mistral-large llama-3-405b-instruct llama-3-70b-instruct llama-4-maverick-17b-128e-instruct llama-4-scout-17b-16e-instruct granite-3-3-8b 2.6 1.0 2.7 1.3 3.1 1.9 6.7 1.5 4.0 1.9 3.9 2.0 5.2 1.4 93.3 105.6 186.9 206.9 208.3 176.5 381.8 240.2 384.6 611.6 172.1 114.7 413.3 418.2 2.9 1.5 3.0 1.4 4.0 1.5 6.5 0.9 3.9 1.2 4.4 1.5 5.1 1.3 180.2 122.6 209.7 139.1 224.4 99.7 369.6 151.9 376.8 281.0 218.1 105.4 432.9 294.7 Conclusion. While the Plan-and-Execute architecture demonstrates greater efficiencyrequiring fewer steps and exhibiting lower runtime variability across tasksour evaluation shows that Agentsas-Tools significantly outperform in task performance metrics. For example, gpt-4.1 achieves 65% task completion, 77% data retrieval accuracy, and hallucination rate as low as 6% in the Agents-as-Tools setting, compared to only 3844% on most metrics in Plan-and-Execute. Similarly, llama-4-maverick-17b-128e-instruct excels in both setups but scores notably higher in Agents-as-Tools, achieving 5978% on core performance metrics versus 4557% in Plan-andExecute. This pattern is consistent across most models: Agents-as-Tools incur higher execution costs but deliver better reasoning fidelity, clearer justifications, and lower hallucination rates. Conversely, Plan-and-Execute agentswhile faster and more structuredoften struggle with complex retrieval, verification, and consistency tasks. These findings suggest fundamental trade-off: Plan-and-Execute offers process efficiency, while Agents-as-Tools yield higher end-task qualitya crucial insight for selecting agent architectures based on application goals such as throughput vs. correctness. 31 F.1.1 Deep Investigation of Agents-As-Tool Performance To evaluate the capabilities of various large language models (LLMs) across range of industrialrelevant task categories, we present radar chart (See Figure 16) comparison covering five key dimensions: IoT-focused reasoning, Failure Mode and Sensor Reasoning (FMSR), Time Series and Fault Modeling (TSFM), Work Order (WO) understanding, and End-to-End task integration. The chart illustrates normalized performance scores for each model based on task-specific benchmarks, with higher values indicating stronger task alignment. Among the models compared, gpt-4.1-2025-04-14 demonstrates the most consistent and well-rounded performance, achieving near-saturation in FMSR (100%) and strong results in End-to-End integration. In contrast, granite-3-3-8b-instruct and llama-3-3-70b-instruct perform well in TSFM and FMSR but underperform in WO-related tasks, which are particularly challenging due to their dependence on structured document comprehension and task planning. Notably, the llama-4-maverick model shows promising results in WO and End-to-End integration, indicating potential optimization for cross-domain contextual reasoning. This visualization provides holistic view of model strengths and trade-offs, offering insights for selecting and fine-tuning LLMs in complex, multimodal industrial applications. Figure 16: Task wise distribution of the Accomplished Tasks. F.2 Human Validation We conducted human validation tests using Google Forms. As shown in Screenshots 17, each domain experts are presented with the original task description, the agents reasoning and final answer, and checklist of six evaluation dimensions. Each dimension was assessed using binary judgments (True/False), allowing for structured comparison between human and model evaluations. 32 Figure 17: Google Forms: questionnaire to domain experts for human validation We distributed 4 forms where each form consists of 10 samples and eventually collect 240 data points. We report the results in Section 5.1. 33 You are critical reviewer tasked with evaluating the effectiveness and accuracy of an AI agents response to given task. Your goal is to determine whether the agent has successfully accomplished the task correctly based on the expected or characteristic behavior. Evaluation Criteria: 1. Task Completion: - Verify if the agent executed all necessary actions (e.g., using the correct tools, retrieving data, performing the required analysis). - The agents response should align with the predefined expected behavior for task completion. 2. Data Retrieval & Accuracy: - Ensure that the correct asset, location, time period, and sensor (if applicable) were used. - Verify if the task performed was related to the correct asset and sensor, and ensure the result corresponds to the correct time period. - Check if the agent retrieved the required data and if the forecasting, anomaly detection, or other results are correct. 3. Generalized Result Verification: - Task Type Verification: Based on the task type (forecasting, anomaly detection, classification, etc.), verify if the agent has returned the expected results. - For forecasting tasks: Ensure that the agent generated forecast for the specified future period. - For anomaly detection tasks: Verify that anomalies are detected as expected (if anomalies were anticipated). - For other tasks (e.g., classification), ensure the task result matches the expected format and value. - Comparison with Expected Output: Check if the result matches the expected format, values, or outcomes as outlined in the characteristic answer. - Data Integrity: Ensure that the correct data (e.g., sensor, time period) was used in the task, and that it is consistent with the expected format and structure. 4. Agent Sequence & Order: - Ensure the agents were called in the correct order and that all actions align with the expected behavior for agent interactions. - If the characteristic answer specifies certain agents (e.g., IoTAgent, TSFMAgent), verify that these were used and in the correct sequence. 5. Clarity and Justification: - Ensure the agents response is clear and justified with adequate explanations or evidence to support the claims made. - There should be no contradictions between the agents reasoning and the expected behavior outlined in the characteristic answer. 6. Hallucination Check: - Identify if the agent claims success without performing the necessary actions or without generating meaningful results. - If the agent provides fabricated response or claims success where actions are missing, mark this as hallucination. Question: {question} Characteristic Answer (Expected Behavior): {characteristic_answer} Agents Thinking: {agent_think} Agents Final Response: {agent_response} Output Format: Your review must always be in JSON format. Do not include any additional formatting or Markdown in your response. { \"task_completion\": true/false, \"data_retrieval_accuracy\": true/false, \"generalized_result_verification\": true/false, \"agent_sequence_correct\": true/false, \"clarity_and_justification\": true/false, \"hallucinations\": true/false, \"suggestions\": \"Optional. Actions or improvements for rectifying the response if applicable.\" } (END OF RESPONSE) Please provide your review based on the given criteria. Table 16: Prompt instruction to LLM-as-a-judge evaluation agent 34 F.3 LLM-as-a-judge Evaluation Agent Based on the human validation results shown in Section 5.1, llama-4-maverick is selected to be the LLM of evaluation agent. Table 16 is the prompt instruction to the evaluation agent, which outlines the specific evaluation dimensions, constraints, and response formatting guidelines that the model follows when scoring task outputs. The evaluation criteria is also provided to human judges which ensures consistency across evaluations. F.4 Ablation Experiment In this section, we present the detailed report of the ablation study. We fixed the Tools-As-Agents paradigm, and conduct both the set of experiments. F.4.1 Distractor Agents Detail We have introduced 10 distractor agents to intentionally increase the complexity and ambiguity for global agents. Table 17 categorizes these agents based on their respective domains and functional roles. The set includes both general-purpose agents, such as those for echoing inputs or handling off-topic queries, and domain-specific agents focused on tasks like predictive maintenance, sensor data summarization, and edge ML deployment. This taxonomy enhances the realism of multi-agent environments by supporting modular integration and introducing controlled confusion. Agent Name Domain Description Table 17: Agent Types and Their Roles Echo Agent OffTopic Agent General General Customer SupportAgent Support Operations SRE Agent Frontend DevAgent HRPolicy Agent SensorData Summarizer Historical TrendsAgent EdgeML Agent Site Reliability Software Engineering Human Resources Industrial IoT Analytics Edge Computing RULPredictor Agent Predictive Maintenance Repeats the input verbatim; useful for debugging and testing input-output coherence. Provides unrelated facts or trivia when query is off-topic or not recognized. Handles customer-related issues like password resets, login errors, and service availability. Diagnoses performance degradation, system downtime, and infrastructure issues. Assists with frontend UI/UX concerns, React, JavaScript frameworks, and rendering bugs. Answers HR-related queries like leave policy, benefits, and compliance rules. Summarizes time-series data from sensors, highlighting trends and anomalies. Extracts and interprets historical asset data to identify failure patterns or optimization opportunities. Recommends tools and strategies for deploying ML models on edge hardware with limited resources. Estimates the remaining useful life (RUL) of assets using sensor data and degradation models. F.4.2 Impact of in-context examples Table 18 provides detailed comparison of gpt-4.1 and granite-3-3-8b with and without in-context examples on subset of single-agent benchmark tasks. Consistent with our main findings, in-context examples were critical for enabling effective reasoning and coordination. Table 18: Comparison of gpt-4.1 and granite-3-3-8b With/Without In-Context Examples (# of Tasks = 65) Model Ctx Task Data Verif. Seq. Clar. Hall. gpt-4.1 granite-3-3-8b gpt-4.1 granite-3-3-8b"
        },
        {
            "title": "No\nNo",
            "content": "52 40 22 2 57 44 21 3 55 41 24 41 48 21 2 62 51 29 7 2 9 35 Key Results: Removing in-context examples led to dramatic drop in performance for both models. gpt-4.1 dropped from an average of 80% (with context) to 33% (without), while granite-3-3-8b fell from 60% to just 3% (Section F.4). Hallucinations also increased significantly when context was removed, especially for granite-3-3-8b. Interestingly, gpt-4.1 outperformed granite-3-3-8b across all metrics except agent sequence accuracy under in-context settings, where granite had slight advantage. These results reinforce the conclusion that in-context examples are essential for ReAct-style reasoning in LLM-based agents. We did not select tasks from WO and E2E since their performance is already poor. F.5 Algorithmic Procedure for New Emerging Failure Mode Discovery To support adaptive evaluation of multi-agent LLM systems, this appendix outlines the implementation details behind the failure discovery process referenced in Section 5.2. While the main text presents the empirical distribution of failure typesincluding emergent patternsthis appendix focuses on the structured methodology used to extract and cluster novel failure behaviors beyond the MAST (Multi-Agent System Failure Taxonomy) [1]. The evaluation spanned 881 multi-agent trajectories, drawn from diverse language model configurations. Trajectory distribution by model is as follows: mistral-large: 145 trajectories llama-3-405b-instruct: 145 trajectories llama-3-3-70b-instruct: 145 trajectories llama-4-maverick-17b-128e-instruct-fp8: 125 trajectories llama-4-scout-17b-16e-instruct: 111 trajectories gpt-4.1-2025-04-14: 105 trajectories granite-3-3-8b-instruct: 105 trajectories Among the 881 utterance execution trajectories analyzed using an LLM-as-a-judge framework (selected LLM judge model - openai-azure/gpt-4.1-2025-04-14 as the LLM judge) to identify the causes of multi-agent AI failures, we found thatbeyond the existing MAST categories185 trajectories exhibited one additional failure reason, while 164 trajectories contained two distinct additional failure reasons. This highlights the empirical necessity of taxonomy expansion to capture compound and emergent failure patterns in real-world deployments. To extend the original MAST taxonomy, we conducted structured analysis of novel multi-agent system failures observed in recent interaction traces. This subsection details our identification methodology and explains how the resulting failure modes align with the MAST framework. F.5.1 Algorithm for Emerging Failure Modes Clustering To systematically identify and normalize emerging failure modes observed in multi-agent LLM system interactions, we introduce structured algorithmic framework based on semantic embedding 36 and unsupervised clustering. This process abstracts unanticipated failure patterns into representative categories that either align with or extend the predefined MAST taxonomy. Definitions and Notation. Let: = {t1, . . . , tn}: Set of multi-agent execution trajectories. M: The predefined MAST taxonomy of failure types. = {f1, f2, . . . , fm}: Set of emerging failure mode descriptions not covered by M, extracted from LLM-as-a-judge evaluations. ϕ : Rd: Sentence embedding function (e.g., Sentence-BERT). = [ϕ(f1), . . . , ϕ(fm)] Rmd: Matrix of embedded failure descriptions. = {C1, . . . , Ck}: Partition of into clusters, each with centroid µj. Step 1: Emerging Failure Mode Extraction. Each trajectory ti is evaluated by an LLM-as-ajudge to identify: Labeled failure types from the MAST taxonomy M. Up to two emerging failure descriptions fi1, fi2 / M. The full set of novel descriptions is aggregated as: (cid:91) = {fi1, fi2} NULL i=1 Step 2: Semantic Embedding. Each emerging failure mode fi is transformed into ddimensional vector: ei = ϕ(fi), fi = ϕ(f1) ϕ(f2) ... ϕ(fm) Rmd Step 3: Optimal Clustering via K-Means. To discover latent groups of semantically similar failure descriptions, we apply K-Means clustering over the embeddings E. The silhouette score for given point is: s(i) = b(i) a(i) max{a(i), b(i)} Where: a(i): Mean distance from ei to other points in the same cluster. b(i): Minimum mean distance from ei to points in different cluster. The optimal number of clusters is selected as: = arg max SilhouetteScore(k) Step 4: Cluster Center Selection. For interpretability, we select representative cluster Cj as the most centrally located failure mode: from each = arg min fiCj ϕ(fi) µj2 Figure 18: Silhouette analysis showing optimal number of clusters = 6. Step 5: Taxonomy Alignment. Each representative failure mode one or more MAST categories: is reviewed and mapped to Specification Failures Inter-Agent Failures Task Verification Failures Failures that exhibit characteristics of multiple categories are marked as compound or intersectional, suggesting the need for extensions to the base taxonomy. Outputs. The algorithm yields: clustered taxonomy = {C1, . . . , Ck } of emerging failure modes. Canonical representatives {f 1 , . . . , } for each cluster. Category mappings for taxonomy refinement or extension. Frequency statistics per failure type for prioritization. F.5.2 Methodology: Semantic Clustering of Emergent Failures Building on the formal clustering algorithm outlined above, we implemented practical instantiation of the pipeline to organize the large volume of emerging failure mode descriptions identified by the LLM-as-a-judge. We found lots of new and different behaviors when we first looked. But closer look showed that many of them were either just repeating the same idea or were only slightly different versions of the same core problems. To distill these into interpretable categories, we applied semantic clustering methodology grounded in high-dimensional language representations. Each emerging failure description was manually or programmatically summarized into concise label and explanatory text. These summaries were then embedded into semantic vector space using the all-MiniLM-L6-v2 model from the SentenceTransformer library, yielding set of dense, comparable embeddings suitable for clustering. We applied the KMeans algorithm to group these embeddings into semantically coherent clusters. To determine the optimal number of clusters, we computed silhouette scores for values of ranging from 2 to 7 and selected the value that maximized mean silhouette score (see Figure 18). This analysis yielded an optimal configuration of = 6 clusters. For interpretability, each cluster was assigned canonical label derived from the failure mode description closest to the cluster centroid. This process produced six representative categories of emerging failure modes, summarized below: Cluster 0: Lack of Error Handling for Tool Failure (53 cases, 10.3%) Agents fail to detect or appropriately respond to tool invocation errors. Cluster 1: Failure to Incorporate Feedback (41 cases, 8.0%) Agents ignore or inadequately adjust to feedback from other agents or tools. Cluster 2: Invalid Action Formatting (27 cases, 5.3%) Output includes syntactic or structural errors that prevent execution. Cluster 3: Overstatement of Task Completion (122 cases, 23.8%) Agents claim completion without satisfying task criteria or producing valid outcomes. Cluster 4: Extraneous or Confusing Output Formatting (110 cases, 21.4%) Responses contain unnecessary verbosity, ambiguous structure, or misleading formatting. Cluster 5: Ineffective Error Recovery (160 cases, 31.2%) Agents fail to resolve prior mistakes or restart workflows effectively after failure. These cluster-derived failure modes serve as canonical extensions to the base MAST taxonomy, revealing previously unclassified behaviors that frequently arise in multi-agent LLM interactions. Their emergence underscores the value of inductive, embedding-based clustering for scalable failure mode discovery and taxonomy refinement. F.5.3 Taxonomic Alignment with MAST of Emergent Failures These emergent failure modes reveal both alignment and tension with the original MAST taxonomy. Each cluster can be mapped to one or more of MASTs three core failure categories, but many straddle boundaries or reveal overlapping failure dynamics: Specification Failures: Overstatement of Task Completion and Extraneous Output Formatting reflect unclear success criteria, misunderstood task scopes, or ambiguous output specifications. Inter-Agent Failures: Failure to Incorporate Feedback and Lack of Error Handling for Tool Failure indicate coordination breakdowns or limited adaptivity in dynamic environments. Task Verification Failures: Invalid Action Formatting and Ineffective Error Recovery highlight failures in runtime execution monitoring, verification, and correction procedures. Several emergent failure types cut across multiple categories, underscoring the complexity and interdependence of failure dynamics in real-world multi-agent systems. These findings motivate future refinement of MAST to support cross-category failure representation and compound behavior tracking. This failure mode analysis contributes both methodologically and substantively to multi-agent system evaluation. Methodologically, it introduces scalable pipeline for inductively discovering and structuring new failure behaviors using LLM-judged outputs and semantic clustering. Substantively, it extends the empirical coverage of the MAST taxonomy by surfacing nuanced, real-world failure patterns that reflect the increasing complexity of autonomous agent collaboration. These insights not only validate the need for flexible taxonomic frameworks but also point to the importance of diagnostics that evolve with model behavior. As LLM-based agents continue to scale in capability and deployment scope, the ability to detect emergent, intersectional failures becomes foundational requirement for reliable multi-agent orchestration."
        }
    ],
    "affiliations": [
        "IBM Research - Ireland",
        "IBM Research - Yorktown"
    ]
}