{
    "paper_title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping",
    "authors": [
        "Jingyi Lu",
        "Kai Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: https://visual-ai.github.io/inpaint4drag/"
        },
        {
            "title": "Start",
            "content": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping Jingyi Lu Kai Han Visual AI Lab, The University of Hong Kong lujingyi@connect.hku.hk, kaihanx@hku.hk 5 2 0 2 4 ] . [ 1 2 8 5 4 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Drag-based image editing has emerged as powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining realtime performance. Project page: https://visualai.github.io/inpaint4drag 1. Introduction Image manipulation remains fundamental challenge in computer vision, with increasing demand for intuitive tools that enable users to naturally modify image content. Among various interaction paradigms, drag-based editing has emerged as promising direction [31, 40], offering an intuitive way to directly manipulate image elements through simple mouse operations. Recent advances in this field have demonstrated impressive results [22, 52], allowing users to move, resize, or deform objects within images through simple drag operations. Corresponding author. However, existing drag-based editing methods face several fundamental limitations. These approaches [12, 20, 26, 31] rely on manipulating the latent space of generative models like Stable Diffusion [23, 28, 35], leading to three key challenges: (1) imprecise control latent space manipulations obscure the relationship between user inputs and resulting changes when control points are downscaled to match the lower latent resolution (e.g., from 512512 to 3232 in SD-UNet), (2) poor interactivity users are forced into time-consuming trial-and-error cycles without immediate visual feedback during the generative process, and (3) limited capability (shown in Fig. 1) these approaches often produce unrealistic results when handling large occlusions (e.g., rotating heads or opening mouths) as they rely on general-purpose text-to-image models that are not specifically trained to handle missing or occluded regions. In this paper, we present Inpaint4Drag, novel interactive framework that decomposes drag-based image editing into bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world [38, 39], we treat image regions as deformable materials that maintain natural shape under user manipulation. Given an input image, users specify region mask to define the deformable area and place handle and target control points to guide the transformation. To assist with complex object manipulation, we provide an optional mask refinement module that automatically captures precise object boundaries, ensuring consistent deformation of both edges and interior regions. The core of our approach lies in the bidirectional warping algorithm that transforms the input region based on user control inputs. While forward warping alone moves source pixels to their target positions, it inevitably creates holes and gaps that lead to artifacts. We address this by combining forward warping to define initial contours and rough deformation, with backward warping to fill gaps and establish dense correspondence for the geometric transformation. The warped result is complemented by computed inpainting masks that include dilated revealed regions (areas that emerge after image deformation) and narrow band around warped contours for boundary smoothing. This 1 Figure 1. Comparison of drag-based image editing methods. Our approach (rightmost columns) enables both precise edits and large-scale occlusion handling with real-time preview (0.01 sec) followed by rapid inpainting (0.3 sec). Users select deformable regions and drag from handle points to target positions, with the preview column showing grid overlays in areas requiring inpainting. In contrast, existing methods (leftmost columns) require substantially longer processing times without providing interactive feedback during editing, and often struggle with precise manipulations due to latent-space operations. complete input is then passed to modern inpainting models (e.g., LaMa [43], SD-Inpaint [35]) for final completion. Our decomposition enables clear separation between geometric transformation and content generation while maintaining the familiar drag interaction paradigm. Our physics-inspired formulation and bidirectional warping algorithm enable effective drag-based manipulation for inpainting models. It possesses three key advantages: First, our pixel-space deformation estimation enables precise geometric control while preserving colors of dragged content and maintaining image quality in unedited regions. Second, our method effectively handles large occlusions by leveraging specialized inpainting models, enabling challenging edits like opening lions mouth or rotating persons head. Third, our approach serves as universal adapter for the inpainting field - by transforming drag inputs directly into standard inpainting formats, we enable any inpainting model to function as drag editing method without architecture modification, automatically inheriting all future improvements in inpainting technology. Notably, our system achieves exceptional interaction fluidity with real-time warping previews (0.01s) and efficient inpainting operations (0.3s) (measured on 512512 images). We summarize our technical contributions as follows: physics-inspired deformation framework that treats image regions as elastic materials, enabling natural transformations through user-specified control points and region masks, with optional mask refinement for precise object boundary handling. An efficient bidirectional warping algorithm that establishes initial shape through forward warping and fills gaps via backward mapping, creating dense pixel correspondences while maintaining real-time performance. modular pipeline that clearly separates transformation from generation, computing precise masks for revealed regions and boundary smoothing to enable seamless integration with existing inpainting models. 2. Related Work Generative Models for Image Editing. Generative models have revolutionized image editing, with generative adversarial networks (GANs) marking pivotal breakthrough [11, 1315]. While numerous editing techniques have emerged from the GAN framework [7, 9, 31, 33, 4547], their practical applications remain constrained by training data diversity, model capacity, and challenges in inverting real images into GAN latent spaces [7, 18, 53]. The emergence of large-scale text-to-image diffusion models [35, 37] has expanded image manipulation capabilities through text prompts [2, 5, 6, 10, 16], enabling control over style, motion, and object categories. However, these text-based approaches lack pixel-level precision, focusing primarily on semantic alterations. To address this limitation, drag-based techniques [9, 20, 24, 26, 27, 31, 40] have emerged, allowing fine-grained manipulation of object posture and shape through interactive keypoint control, effectively combining generative modeling with the precision needed for detailed image editing. Drag-based Image Editing. Drag-based methods have transformed image editing by enabling feature manipulation through paired handle and target points. DragGAN [31] pioneered multi-point editing in GAN-generated images using iterative motion supervision and point tracking, while FreeDrag [20] enhanced precision by operating in feature space. Building on these foundations, several diffusion-based approaches have emerged. DragDiffusion [40] adapted the supervise-and-track framework to diffusion models, while DiffEditor [27] and EasyDrag [12] extended feature dragging across the denoising process. SDE2 inpainting models for seamless completion. 3.1. Region Specification and Boundary Refinement Previous drag editing methods [31, 41, 52] typically use sparse control points to guide deformation, with optional masking to restrict editable regions. However, this sparse input format introduces fundamental ambiguity in deformation interpretation the movement of numerous pixels relies on the guidance from only few control points. single drag operation on characters arm could produce vastly different outcomes: full-body rotation, localized arm movement, or isolated point translation (see S4 in supplementary material for examples). Without clear deformation specifications, existing methods often produce unpredictable results that deviate from user intentions. We address this ambiguity by requiring users to explicitly mask regions intended for deformation, similar to how we naturally identify movable parts of objects. Given an input image RHW 3, set of handle points {hi} and their target positions {ti}, our key insight is to treat each masked region as an elastic material, where moving handle point creates ripple effect throughout the connected area much like stretching rubber sheet. Our bidirectional warping approach (Sec. 3.2) computes dense deformation fields that ensure smooth influence propagation from control points to every pixel while maintaining complete pixel coverage throughout the deformed region. For effective deformation propagation, mask boundaries should align with object boundaries pixels belonging to the same object should move coherently. To maintain deformation coherence across object boundaries while simplifying user interactions, we propose an optional mask refinement module based on the Segment Anything Model (SAM) [17]. Given user input mask containing points = {pi}N i=1, directly using all mask points as SAM input would introduce computational bottlenecks. To address this, we sample grid points Ps from the user-drawn mask as SAM input, which maintains interactive performance while preserving critical boundary information. We process these sampled points through SAMs prompt encoder fSAM to obtain point embeddings, which are combined with the image embedding to generate prediction mask Mpred = fSAM(I, Ps). Despite SAMs powerful segmentation capabilities, we found that direct SAM predictions can generate disconnected regions or capture unintended objects far from the users specified area due to SAMs tendency to segment complete objects and semantically similar instances rather than user-intended regions (see Fig. 2 or Fig. 6 for examples). We address this limitation with two-step refinement approach that balances automatic boundary detection while preserving user intent. First, we generate dilated and eroded versions of the input Figure 2. Overview of our optional mask refinement module. Users can achieve precise object boundaries for coherent deformation through SAM [17], constrained by eroded and dilated mask boundaries to preserve user intent. Drag [28] introduced sparse latent copy-paste techniques, and RegionDrag [22] improved efficiency through dense region mapping. InstantDrag [42], motion prediction approach, combines motion guidance with generative models but requires test-time optimization on video clips for each new edit. FastDrag [52], while sharing our intuition of object stretching, operates solely in latent space with sequential processing, lacking our methods vectorized pixel-space manipulation and specialized inpainting capabilities. Image Inpainting. Image inpainting has evolved from classical PDE-based [4] and patch-based [3, 8] methods to modern deep learning approaches. CNN-based methods [34] first learned semantic priors, followed by GANs [21, 48] and transformers [19] that improve the inpainting performance through adversarial training or enhancing longrange consistency. Recent diffusion models like Stable Diffusion [35] and LaMa [43] achieve superior quality in structural coherence and detail synthesis, widely adopted by commercial tools like Adobe Photoshop [1], DALLE3 [30], and RunwayML [36], which inspires us to consider the emerging drag-based image editing problem from the inpainting perspective. 3. Method We present Inpaint4Drag, an interactive approach for dragbased image editing that decomposes the task into bidirectional warping and standard image inpainting. Given an input image, users can draw masks to select objects for deformation, with our mask refinement module to improve boundary precision (Sec. 3.1). After specifying drag point pairs, our bidirectional warping algorithm immediately deforms the selected region (Sec. 3.2), providing real-time preview of the editing result that also serves as input for the inpainting model. Users iteratively refine masks and control points to achieve desired dragging effects before executing the relatively expensive inpainting operation, which fills regions revealed by deformation (Sec. 3.3). Our approach enables real-time interaction while delivering high-quality results through specialized deformation control and integrated 3 Figure 3. Overview of our bidirectional warping pipeline. Given user mask and control points, we first extract region contours and establish point associations. The forward warping step then maps these contours to their target locations and builds initial correspondences. Finally, through backward mapping, we generate the warped image with complete pixel coverage and its corresponding warped mask, which will be used for subsequent inpainting. mask: Mdilated(x, y) = max (i,j)K (x i, j), Meroded(x, y) = min (i,j)K1 (x i, j), (1) (2) where K1 represents dilation / erosion kernel with radius r1. The refined mask is obtained through our boundaryguided formulation: = (Mpred Mdilated) Meroded, (3) where and represent the logical OR and AND operations respectively. This approach uses the dilated and eroded masks to create natural boundary constraints, ensuring the refined result respects the users original intent while improving boundary precision. By exposing the kernel radius r1 as control parameter, users can tailor the refinement strength, achieving their desired balance between automatic boundary detection and manual control. 3.2. Bidirectional Warping for Region Deformation Given user mask (either original or refined) and control point pairs {(hi, ti)}K i=1, our bidirectional warping algorithm aims to translate these inputs into coherent deformation of the masked region and prepares standard inputs for inpainting models: warped image Iwarped containing deformed content alongside an inpainting mask Minpaint identifying areas left vacant by relocated pixels. As shown in Fig. 3 and Fig. 4, our approach consists of four steps: (1) contour extraction to identify independent deformable regions; (2) forward warping to define target region boundaries and establish initial mapping; (3) backward mapping to ensure complete pixel coverage in target regions; and (4) leveraging the established mapping to generate warped content and identify areas requiring inpainting. 3.2.1. Contour Extraction and Control Point Association We first decompose the binary mask into distinct contours representing separate deformable regions (left of pink block, Fig. 3). contour here refers to closed curve that traces the boundary of connected area in the mask: = findContours(M ). (4) To maintain local deformation coherence, we associate each contour point with control points located inside its region: (Hc, Tc) = {(hi, ti) hi inside C}. This ensures that each region responds only to control points within its boundaries, preserving the intuitive behavior where deformation occurs only where user interaction is directly applied. For brevity, we simply use (hi, ti) in subsequent equations to refer to control points associated with contour point c. (5) 3.2.2. Forward Warping For regions with single control point pair, deformation simplifies to translation, where region pixels directly shift from handle to target position. When multiple control points are present, the deformation becomes non-rigid, requiring interpolation-based transformation. We first perform forward warping (green block, Fig. 3) which serves 4 Figure 4. Overview of inpainting mask computation. The final mask (Minpaint) combines dilated unmapped regions (Mtemp) from areas absent after warping and dilated boundaries of the warped mask (Mwarped). This ensures smooth transitions between warped and generated regions in the final result. two critical purposes: i) it transforms the contour itself to define the target region boundary C, and ii) it establishes initial pixel-level mapping to guide subsequent processing. For each point in the source region (including boundary points of contour C), we compute its target position pt through weighted interpolation: pt = + NC(cid:88) i=1 wi(ti hi), (6) where NC denotes the number of control points associated with contour C, and weights wi are computed through inverse distance weighting: wi = 1/(p hi + ϵ) j=1 1/(p hj + ϵ) (cid:80)NC . (7) While this forward approach establishes initial mapping, using it alone for warping creates sampling artifacts (see Fig. 3 or Fig. 7 for examples). Specifically, when nonrigid transformations stretch image regions, the discrete nature of pixels creates gaps where target locations receive no mapped value. This occurs when the transformed source pixel grid becomes stretched and discontinuous in the target space, leaving unmapped gaps. 3.2.3. Backward Mapping To ensure complete pixel coverage within the deformed region (blue block, Fig. 3), we compute source positions for all pixels in the transformed target contour . For each target pixel pt , we determine its corresponding source position ps using: ps = pt + Nn(cid:88) i= wi(psrc ptgt ), (8) 5 where Nn represents the nearest target pixels that were successfully matched during the forward process, ptgt repi resents one of these matched nearest target positions, psrc is its corresponding source position, and wi are the inverse distance weights as previously defined. This local neighborhood approach provides computational efficiency compared to using global pixel references and helps preserve structural coherence by limiting the influence of distant forward warping results on the final mapping. We validate each computed mapping pair by ensuring all coordinates remain within image boundaries: Valid(ps, pt) = ps, pt [0, ) [0, H), (9) where and denote the image width and height, respectively. This prevents sampling from undefined regions outside the image domain. Overall, our backward mapping strategy completes the bidirectional framework by establishing reliable pixel-level mappings that maintain local structural relationships while addressing the limitations of forward transformation. 3.2.4. Computing Warped Result and Inpainting Mask Using the established pixel mappings, we generate the warped image by transferring pixel values: Iwarped(pt) = I(ps) for all valid (ps, pt) pairs. (10) As shown in Fig. 4, we then identify regions requiring inpainting by determining pixels present in the original mask but unmapped in the deformed result: Mtemp = Mwarped. (11) To ensure smooth transitions at both warped content boundaries and vacated regions, we apply dilation to both the unmapped areas Mtemp and the boundaries of the warped mask Mwarped. This creates buffer zone around the boundaries by expanding the inpainting mask, allowing the inpainting model to handle the transition areas and avoid abrupt edges between warped and newly generated content: Minpaint = dilate(Mtemp Mwarped, K2), (12) where K2 represents the dilation kernel with radius r2. To this end, our bidirectional warping algorithm has processed the user mask and control point pairs {(hi, ti)}K i=1 to produce the warped image Iwarped and an inpainting mask Minpaint, which together form the standard input required by image inpainting models. DragBench-D from DragDiffusion [40] containing 205 samples. Each benchmark entry includes source image, text prompt describing the image, binary mask for the editable region, and point pairs showing desired movements. In our framework, masks serve different purpose as deformable regions. While editable regions indicate where editing is allowed, deformable regions define coherent parts that should move together during transformation. We therefore re-annotated the deformable region locations and dragging points while preserving the original user editing intentions and keeping the source images unchanged. 3.3. Integration with Image Inpainting 4.2. Evaluation Metrics The final step in our pipeline is to apply an inpainting model to generate content for areas revealed during deformation: Following [22], we measure editing performance using LPIPS [50] and Mean Distance (MD). Iedit = Inpaint(Iwarped, Minpaint). (13) Before executing inpainting operations, our method provides Iwarped as real-time preview of the final result Iedita capability absent in existing approaches. This preview enables users to achieve desired dragging effects by adjusting mask and control points, resulting in more interactive editing experience that avoids unnecessary expense from repeated inpainting attempts. For our implementation, we selected the Stable Diffusion 1.5 Inpainting Checkpoint [35], which was fine-tuned from the regular Stable Diffusion v1.2 model with additional training for inpainting tasks. The inpainting process follows straightforward pipeline: the mask is resized and concatenated with the image VAE latent representation. During conditional diffusion denoising, we initialize masked regions with pure noise to generate entirely new content in areas revealed by deformation. Finally, the result is transformed back to pixel space through VAE decoding. To optimize performance, we incorporated several efficiency enhancements: TinyAutoencoder SD (TAESD) [29], distilled VAE that reduces memory requirements; LCM (Latent Consistency Model) LoRA [25] to reduce sampling steps; an empty text prompt to eliminate classifier-free guidance computation; and caching of the empty prompt embeddings to avoid repetitive calculations during editing sessions. Its worth noting that while we report experimental results using this representative inpainting model, our framework can accommodate any inpainting model as drop-in replacement (see S2 in supplementary material for examples). 4. Experiments 4.1. Datasets We evaluate drag editing methods using two benchmarks: DragBench-S from SDE-Drag [28] with 100 samples, and LPIPS: Learned Perceptual Image Patch Similarity (LPIPS) v0.1 [50] measures identity preservation between original and edited images. Lower LPIPS indicates better identity preservation. However, limitation of this metric is that it only measures low-level feature similarities between image patches. In drag editing, intentional and correct shape deformations often result in unavoidable LPIPS increases that should not be penalized. Mean Distance (MD): This metric evaluates how accurately handle points are moved to target positions. DIFT [44] is employed to find matched points for the userspecified handle points in the edited image, restricting the search area to regions around user-specified handle and target points to avoid false matches. MD is calculated as the average normalized Euclidean distance between target points and DIFT-matched points. 4.3. Implementation Details Our framework is implemented in Python using PyTorch [32] on single NVIDIA Tesla V100-SXM2 GPU. For mask refinement, we adopt EfficientVIT-SAM-L0 [51] and sample points from user masks, limiting the number of sampled points to Ps 128 for computational efficiency. The dilation/erosion kernel radius r1 is set to 10 pixels to provide sufficient boundary refinement while preserving user intent. In the bidirectional warping module, we set ϵ = 106 for numerical stability in weight computation and use Nn = 4 nearest neighbors for backward mapping. The final inpainting mask is dilated with kernel radius r2 = 5 pixels to ensure smooth transitions. For inpainting, we employ Stable Diffusion 1.5 inpainting checkpoint [35] with TAESD [29] and LCM LoRA [25], processing images resized to 512 pixels on the longer edge while maintaining aspect ratio. We use 8 sampling steps for diffusion without classifier-free guidance. 6 Figure 5. Qualitative comparison with state-of-the-art methods on challenging cases from DragBench-S [28] and DragBench-D [40]. Our method enables users to specify deformable regions and manipulate them by dragging handle points toward target destinations. The grid overlay in the preview column indicates areas to be inpainted. Our approach shows advantages over existing methods by effectively preserving local details in dragged regions through our bidirectional warping algorithm while demonstrating strong capability in generating occluded regions. The real-time preview (10ms) allows users to interactively adjust edits before executing the inpainting process (0.3s). Method DragBench-D Mem(GB) Time(s) MD LPIPS MD LPIPS DragBench-S DragDiffusion [40] DiffEditor [27] SDE-Drag [28] FastDrag [52] Ours 11.6 6.6 6.9 5.0 2.7 177.7 43.1 126.1 4.2 0.3 7.0 23.6 7.5 4.1 3. 18.0 17.6 11.4 24.1 11.4 6.7 22.1 8.1 5.1 3.9 10.2 10.9 14.9 13.5 9.1 Table 1. Comparison of different drag-based image editing methods. MD and LPIPS values are scaled by 100. Time and GPU memory are measured at 512512 resolution. 4.4. Quantitative Evaluation As shown in Tab. 1, we compare Inpaint4Drag with stateof-the-art methods [27, 28, 40, 52]. Our bidirectional warping algorithm enables significant improvements in dragging precision and image consistency on DragBench-S [28] and DragBench-D [40], achieving the lowest MD scores (3.6/3.9) and competitive LPIPS values (11.4/9.1). This advantage stems from our dense pixel-wise deformation calculation that preserves color and geometric relationships during manipulation. Notably, Inpaint4Drag is 14 faster than FastDrag [52] and nearly 600 faster than DragDif-"
        },
        {
            "title": "Bidirectional",
            "content": "Figure 6. Qualitative results of our mask refinement module Sec. 3.1. Left: User-provided initial input mask. Middle: Raw segmentation predictions from Segment Anything Model (SAM). Right: Final refined results where the green dashed boundary, derived from dilating the initial input mask, effectively filters out undesired SAM predictions beyond the users intended scope. fusion [40], with SAM-based refinement taking 0.02s and bidirectional warping only 0.01s. The computational peak occurs during SD inpainting for previews, requiring 0.29s while using the least memory (2.7GB) among all methods. 4.5. Qualitative Results As shown in Fig. 5, Inpaint4Drag outperforms state-ofthe-art methods on challenging cases from DragBench-S and DragBench-D (see S5 in supplementary materials for more results). By leveraging specialized inpainting models, Inpaint4Drag generates realistic content in previously occluded regions (e.g., newly exposed facial features in rows 1, and 4, and the lions open mouth in row 3). Our optional mask refinement module enables users to coherently deform object boundaries (rows 4, 5, and 7) or focus on precise local edits (remaining examples). Unlike latent-space methods that lose precision when downscaling control points to latent resolution, our pixel-space approach enables accurate manipulation of local details (row 2, and 5). Our bidirectional warping generates an informative preview of the dragged content, providing informative context for subsequent inpainting process. 4.6. Method Analysis Mask Refinement. As demonstrated in Fig. 6, our approach transforms rough initial masks (left) into refined results (right) by constraining SAMs predictions (middle) within dilated region of the users input. While our refinement module incorporates both inner and outer boundary constraints, for visualization clarity, we only showcase the outer boundary usage in these examples. The resulting masks effectively capture object boundaries while preserving the users intended editing scope. 8 Figure 7. Unidirectional vs. bidirectional warping (ours). Our method fixes sampling gaps using backward pixel mapping. Unidirectional vs. Bidirectional Warping. We present qualitative comparisons between unidirectional (forwardwarping-only) and bidirectional warping (Sec. 3.2) in Fig. 7. The unidirectional approach struggles with sampling artifacts, creating noticeable gaps in stretched regions during deformation. These artifacts emerge when non-rigid transformations stretch image regions, as the discrete nature of pixels results in unmapped gaps where target locations receive no source values due to discontinuities in the transformed pixel grid. Our bidirectional method effectively addresses these challenges through two-step process: first identifying target contours via forward warping, then employing pixel-level backward mapping to fill the gaps. This approach yields smooth transformations without discontinuities, providing reliable visual context for both user preview and image inpainting. 5. Conclusion In this paper, we introduce Inpaint4Drag, novel approach that repurposes image inpainting for drag-based editing through pixel-space bidirectional warping. Unlike existing solutions that rely on general-purpose text-to-image models unoptimized for drag operations, our specialized separation of warping and inpainting effectively maintains pixel consistency while generating high-quality content in newly revealed areas. Our bidirectional warping algorithm and SAM-based boundary refinement provide realtime feedback for intuitive interaction. Experimental results show that Inpaint4Drag delivers superior performance while reducing processing time from minutes to milliseconds. Moreover, since Inpaint4Drag is compatible with any inpainting model, it can continuously improve alongside advancements in inpainting technology. Acknowledgments. This work is supported by the Hong Kong Research Grants Council - General Research Fund (Grant No.: 17211024)."
        },
        {
            "title": "References",
            "content": "[1] Adobe. Adobe Photoshop Content-Aware Fill. https: //www.adobe.com/products/photoshop.html, 2024. Accessed: 2024-02-14. 3 [2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. TOG, 2023. 2 [3] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan Goldman. Patchmatch: randomized correspondence algorithm for structural image editing. TOG, 2009. 3 [4] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In ACM SIGGRAPH, 2000. 3 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. [6] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465, 2023. 2 [7] Antonia Creswell and Anil Anthony Bharath. Inverting the generator of generative adversarial network. IEEE TNNLS, 2018. 2 [8] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Region filling and object removal by exemplar-based image inpainting. IEEE TIP, 2004. 3 [9] Yuki Endo. User-controllable latent transformer for stylegan image layout editing. In CGF, 2022. 2 [10] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. In NeurIPS, 2024. [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and In NeurIPS, Yoshua Bengio. Generative adversarial nets. 2014. 2 [12] Xingzhong Hou, Boxiao Liu, Yi Zhang, Jihao Liu, Yu Liu, and Haihang You. Easydrag: Efficient point-based manipulation on diffusion models. In CVPR, 2024. 1, 2 [13] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In CVPR, 2023. 2 [14] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020. [15] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In NeurIPS, 2021. 2 [16] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, 2023. 2 [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. [18] Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression: Efficient architectures for interactive conditional gans. In CVPR, 2020. 2 [19] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for large hole image inpainting. In CVPR, 2022. 3 [20] Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, and Yi Jin. Freedrag: Point tracking is not you need for interactive point-based image editing. arXiv preprint arXiv:2307.04684, 2023. 1, 2 [21] Guilin Liu, Fitsum Reda, Kevin Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In ECCV, 2018. 3 [22] Jingyi Lu, Xinghui Li, and Kai Han. Regiondrag: Fast region-based image editing with diffusion models. In ECCV, 2024. 1, 3, 6 [23] Grace Luo, Trevor Darrell, Oliver Wang, Dan Goldman, and Aleksander Holynski. Readout guidance: Learning control from diffusion features. In CVPR, 2024. 1 [24] Minxing Luo, Wentao Cheng, and Jian Yang. Rotationdrag: Point-based image editing with rotated diffusion features. arXiv preprint arXiv:2401.06442, 2024. [25] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. 6 [26] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. 1, 2 [27] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Diffeditor: Boosting accuracy and flexarXiv preprint ibility on diffusion-based image editing. arXiv:2402.02583, 2024. 2, 7 [28] Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, and Chongxuan Li. The blessing of randomness: Sde beats ode in general diffusion-based image editing. arXiv preprint arXiv:2311.01410, 2023. 1, 3, 6, 7 [29] Ollin. Taesd: Tiny autoencoder for stable diffusion. https: //github.com/madebyollin/taesd, 2023. 6 [30] OpenAI. DALL-E 3. https://openai.com/dalle-3, 2024. Accessed: 2024-02-14. [31] Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH, 2023. 1, 2, 3, 14 [32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 6 [33] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In ICCV, 2021. 2 9 [34] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016. 3 [51] Zhuoyang Zhang, Han Cai, and Song Han. Efficientvit-sam: Accelerated segment anything model without performance loss. In CVPR, 2024. 6 [52] Xuanjia Zhao, Jian Guan, Congyi Fan, Dongli Xu, Youtian Lin, Haiwei Pan, and Pengming Feng. Fastdrag: Manipulate anything in one step. In NeurIPS, 2024. 1, 3, 7, [53] Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei Efros. Generative visual manipulation on the natural image manifold. In ECCV, 2016. 2 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 2, 3, 6, 12 [36] Runway. RunwayML. https://runwayml.com, 2024. Accessed: 2024-02-14. 3 [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 2 [38] Scott Schaefer, Travis McPhail, and Joe Warren. Image deformation using moving least squares. In ACM SIGGRAPH, 2006. [39] Thomas Sederberg and Scott Parry. Free-form deformation of solid geometric models. In ACM SIGGRAPH, 1986. 1 [40] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435, 2023. 1, 2, 6, 7, 8 [41] Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent YF Tan, and Jiashi Feng. Instadrag: Lightning fast and accurate dragbased image editing emerging from videos. arXiv preprint arXiv:2405.13722, 2024. 3, 14 [42] Joonghyuk Shin, Daehyeon Choi, and Jaesik Park. Instantdrag: Improving interactivity in drag-based image editing. TOG, 2024. 3 [43] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In WACV, 2022. 2, 3, 12 [44] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. In NeurIPS, 2024. [45] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. High-fidelity gan inversion for image attribute editing. In CVPR, 2022. 2 [46] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: survey. IEEE TPAMI, 2022. [47] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Gan prior embedded network for blind face restoration in the wild. In CVPR, 2021. 2 [48] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Generative image inpainting with contextual attention. In CVPR, 2018. 3 [49] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Free-form image inpainting with gated convolution. In ICCV, 2019. 12 [50] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 10 Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping Supplementary Material Jingyi Lu Kai Han Visual AI Lab, The University of Hong Kong lujingyi@connect.hku.hk, kaihanx@hku.hk"
        },
        {
            "title": "Contents",
            "content": ". . . . . . . . S1. Supplementary Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S2. Integration with More Inpainting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S3. Multi-round Interactive Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S4. Discussion of Input Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S5. More Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S6. Pseudo Code for Inpaint4Drag . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S7. Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 . 12 . 13 . 14 . 15 . 18 . 11 S1. Supplementary Videos Two supplementary videos are available on our project page https://visual-ai.github.io/inpaint4drag: one demonstrating our bidirectional warping algorithm with visualizations, and another showcasing the real-time user interface interaction. S2. Integration with More Inpainting Methods We integrate our framework with diverse image inpainting approaches, from early methods like LaMa [43] and DeepFillv2 [49] to recent generative model-based techniques [35]. While quantitative metrics in Tab. S1 show comparable drag editing performance across methods, qualitative differences emerge in Fig. S1. Early approaches offer computational efficiency, whereas generative methods sometimes produce more realistic resultsa quality distinction not fully captured by existing metrics. Our framework provides users flexibility to select the inpainting method best suited to their specific requirements, balancing computational resources and visual fidelity."
        },
        {
            "title": "Method",
            "content": "DragBench-S DragBench-D Mem(GB) Time(s) MD LPIPS MD LPIPS DeepFillv2 [49] LaMa [43] SD-XL-Inpaint [35] SD-1.5-Inpaint [35] 0.8 1.1 8.1 2.7 0.05 0.07 1.3 0.3 3.2 3.4 3.2 3.6 13.7 13.6 12.5 11. 3.7 3.7 3.8 3.9 9.0 9.0 8.8 9.1 Table S1. Comparison of different inpainting methods. MD and LPIPS values are scaled by 100. Time and GPU memory are measured at 512512 resolution. Figure S1. Qualitative comparison of different inpainting methods. The figure illustrates how various inpainting approaches affect drag editing results, highlighting differences in visual fidelity, artifact handling, and preservation of semantic content across traditional and generative model-based techniques. 12 S3. Multi-round Interactive Editing Our system enables fluid multi-round interactions, allowing users to execute sequential edits with minimal delay. In Fig. S2, we demonstrate this capability through chess sequence where pieces are repositioned in rapid succession to create checkmate scenariohighlighting our systems responsiveness to iterative user inputs. Figure S2. Multi-round interactive drag editing demonstrated through three-move checkmate sequence including five consecutive edits. Users select deformable regions (chess pieces) and drag them from handle points to target positions. Grid overlays in the preview columns indicate areas requiring inpainting. Our method provides real-time preview (10ms) of the warping effect, followed by high-quality inpainting results (0.3s). Existing approaches typically require minutes for inference and fail during the initial interaction. 13 S4. Discussion of Input Ambiguity Previous drag editing methods [31, 41, 52] typically use sparse control points to guide deformation, with optional masking to restrict editable regions. However, this sparse input format (shown on the left of each row in Fig. S3) introduces fundamental ambiguity in deformation interpretation. Through our explicit region-based control (visualized in bottom-right insets), we demonstrate how single ambiguous drag input can be precisely controlled to achieve five distinct editing intentions - from local manipulation to global translation. For instance, the same drag operation on polar bear can be accurately interpreted as body translation, forearm bending, hand raising, upper body stretching, or scene translation. We address this ambiguity by requesting users to specify deformable regions through masking, treating each region as an elastic material where movement smoothly propagates from control points throughout the connected area. Figure S3. Precise control over ambiguous drag operations. Left: Ambiguous sparse input from previous methods can represent at least five different user intentions. Right: Through our explicit deformation-based control interface (bottom-right insets), we precisely implement each distinct user intention, effectively eliminating ambiguity while maintaining intuitive interaction. 14 S5. More Qualitative Results We present extensive qualitative results in Figs. S4 to S6. Our method allows users to specify handle points (red) and target points (blue) with arrows defining deformation regions (highlighted in red). By applying elastic material principles directly in pixel space, we achieve superior performance across diverse editing scenarios. The results demonstrate our methods effectiveness in facial edits, large-scale deformations, and precise local manipulations while maintaining geometric stability. This advantage is particularly evident when handling significant boundary changes and occlusions, where our inpainting models realistically complete both texture and background. Figure S4. Qualitative comparison of Inpaint4Drag with state-of-the-art methods: wildlife, artworks, flowers, birds, and landscapes. 15 Figure S5. Qualitative comparison of Inpaint4Drag with state-of-the-art methods: portraits, interiors, statues, wildlife, still life, and sports. 16 Figure S6. Qualitative comparison of Inpaint4Drag with state-of-the-art methods: urban scenes, landscapes, animals, pets, and reptiles. 17 S6. Pseudo Code for Inpaint4Drag To complement the detailed description of our method of the main paper, we provide concise algorithmic representation of the data flow in Algorithm 1. Algorithm 1: Inpaint4Drag: Drag-based Image Editing via Bidirectional Warping and Inpainting Input: Image I, user-drawn mask , handle points {hi} and target positions {ti} Output: Edited image Iedit Region Specification and Boundary Refinement:; Ps SampleGridPoints(M ) ; Mpred fSAM (I, Ps) ; Mdilated Dilate(M , K1); Meroded Erode(M , K1) ; (Mpred Mdilated) Meroded ; Bidirectional Warping:; ExtractContours(M ) ; foreach contour do // Sample grid points from user mask // SAM prediction // Create boundary constraints // Boundary-guided refinement // Get deformable regions Associate control points: (hi, ti) {(hi, ti) hi inside C}; Forward Warping: ; foreach point in do wi 1/(phi+ϵ) pt + (cid:80) wi(ti hi) ; Store mapping pair (p, pt); 1/(phj +ϵ) ; (cid:80) end transformed contour from forward warping; Backward Mapping: ; foreach pixel pt within boundary of do Find Nn nearest matched pixels {ptgt ps pt + (cid:80)Nn ptgt i=1 wi(psrc if ps [0, ) [0, H) and pt [0, ) [0, H) then ) ; } with source positions {psrc }; // Define target region boundary // Weighted interpolation // Ensure complete pixel coverage // Local neighborhood interpolation // Transfer pixel values // Identify unmapped regions // Create buffer zone // Apply inpainting model Store valid mapping (ps, pt); end end end Compute Warped Image and Inpainting Mask:; foreach valid mapping pair (ps, pt) do Iwarped(pt) I(ps) ; end Mwarped mask of pixels filled in warped image; Mtemp Mwarped; Mwarped boundary of Mwarped ; Minpaint Dilate(Mtemp Mwarped, K2) ; Image Inpainting:; Iedit Inpaint(Iwarped, Minpaint) ; return Iedit 18 S7. Limitations While our method achieves significant improvements in efficiency and precision, it relies on accurate user-specified masks and control points for optimal performance. Imprecise user inputs, such as masks that inadvertently include background elements or poorly positioned control points, can lead to undesired deformation artifacts. Future work could explore understandingenabled models that automatically filter irrelevant background elements or provide intelligent suggestions for mask and control point placement, reducing the burden on users to provide perfectly accurate inputs while maintaining the intuitive nature of drag-based interaction."
        }
    ],
    "affiliations": [
        "Visual AI Lab, The University of Hong Kong"
    ]
}