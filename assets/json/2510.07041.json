{
    "paper_title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking",
    "authors": [
        "Fenghe Tang",
        "Chengqi Dong",
        "Wenxin Ma",
        "Zikang Xu",
        "Heqin Zhu",
        "Zihang Jiang",
        "Rongsheng Wang",
        "Yuhao Wang",
        "Chenxu Wu",
        "Shaohua Kevin Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce a novel metric, U-Score, which jointly captures the performance-efficiency trade-off, offering a deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose a model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes a foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. The project can be accessed at: https://fenghetan9.github.io/ubench. Code is available at: https://github.com/FengheTan9/U-Bench."
        },
        {
            "title": "Start",
            "content": "U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Fenghe Tang1,2, Chengqi Dong1,2, Wenxin Ma1,2, Zikang Xu3, Heqin Zhu1,2, Zihang Jiang1,2, Rongsheng Wang1,2, Yuhao Wang1,2, Chenxu Wu1,2, Shaohua Kevin Zhou1,2 1 1University of Science and Technology of China a, 2MIRACLE Center b, 3HCNS Over the past decade, U-Net has been the dominant architecture in medical image segmentation, leading to the development of thousands of U-shaped variants. Despite its widespread adoption, there is still no comprehensive benchmark to systematically evaluate their performance and utility, largely because of insufficient statistical validation and limited consideration of efficiency and generalization across diverse datasets. To bridge this gap, we present U-Bench, the first large-scale, statistically rigorous benchmark that evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates models along three key dimensions: statistical robustness, zero-shot generalization, and computational efficiency. We introduce novel metric, U-Score, which jointly captures the performanceefficiency trade-off, offering deployment-oriented perspective on model progress. (2) Systematic Analysis and Model Selection Guidance: We summarize key findings from the large-scale evaluation and systematically analyze the impact of dataset characteristics and architectural paradigms on model performance. Based on these insights, we propose model advisor agent to guide researchers in selecting the most suitable models for specific datasets and tasks. (3) Public Availability: We provide all code, models, protocols, and weights, enabling the community to reproduce our results and extend the benchmark with future methods. In summary, U-Bench not only exposes gaps in previous evaluations but also establishes foundation for fair, reproducible, and practically relevant benchmarking in the next decade of U-Net-based segmentation models. Keywords: Benchmark, U-Net, Medical Image Segmentation, U-Score Date: October, 2025 Projects: https://fenghetan9.github.io/ubench Code Repository: https://github.com/FengheTan9/U-Bench Model Weights & Checkpoints: https://huggingface.co/FengheTan9/U-Bench Datasets: https://huggingface.co/FengheTan9/U-Bench Contact: fhtan9@mail.ustc.edu.cn aSchool of Biomedical Engineering, Division of Life Sciences and Medicine, University of Science and Technology of China (USTC), Hefei, Anhui, 230026, P.R. China bCenter for Medical Imaging, Robotics, and Analytic Computing & LEarning (MIRACLE), Suzhou Institute for Advanced Research, USTC, Suzhou 215123, P.R. China, and Jiangsu Provincial Key Laboratory of Multimodal Digital Twin Technology, Suzhou Jiangsu, 215123, China cAnhui Province Key Laboratory of Biomedical Imaging and Intelligent Processing, Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei 230026, China 5 2 0 2 ] . [ 1 1 4 0 7 0 . 0 1 5 2 : r Corresponding author(s): Shaohua Kevin Zhou; Email skevinzhou@ustc.edu.cn U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Figure 1: Overview of U-Bench. (A) The summary of U-Bench, which encompasses the most comprehensive large-scale evaluation of U-shaped architectures. (B) Word cloud of 100 published U-shaped variants in U-Bench Model Zoo. (C) Examples of the 28 datasets in U-Bench Data Zoo. The red / green box: in-domain / zero-shot split for evaluation. (D) Literature analysis. Among 100 recent works, 84% papers neglect zero-shot evaluation and 73% papers lack of statistical significance testing. (E) Significance analysis. Only minority achieve statistically significant gains over U-Net. (F) Overview of new metric, U-score. Top: IoU does not account for efficiency, while U-Score demonstrates strong correlation with both segmentation performance and efficiency metrics. Bottom: while IoU shows trend of saturation, U-Score highlights the yearly trends toward more efficient models. (G) The evaluation and analysis 2 aspects covered in U-Bench. U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking 1. Introduction Medical image segmentation is critical and challenging task that can greatly enhance diagnostic efficiency by offering doctors objective and precise references for regions of interest (Zhou et al., 2017). Over the past decade, U-Net (Ronneberger et al., 2015) has become cornerstone of medical image segmentation, thanks to its encoder-decoder structure with skip connections that effectively combine multi-scale features. Building on its promising segmentation results across diverse modalities, numerous U-shaped variants have been proposed to further improve performance, with lightweight designs (Valanarasu and Patel, 2022, Tang et al., 2024, Chen et al., 2024a, Valanarasu et al., 2021, Cao et al., 2022), attention mechanisms (Oktay et al., 2018, Tang et al., 2023), multi-scale feature fusion (Zhou et al., 2018, Huang et al., 2020), and more recently Mamba- (Liu et al., 2024a, Wu et al., 2025b), RWKV-based (Ye et al., 2025, Jiang et al., 2025), as well as hybrid architectures (Chen et al., 2021, Tang et al., 2025b, Dong et al., 2025, Tang et al., 2025a). Over the past decade, more than ten thousand U-Net variants have been proposed, and by 2025, nearly thousand studies have employed U-shaped networks for medical image segmentation. Among the vast number of U-Net variants, central challenge remains unresolved: How to conduct fair and comprehensive comparison across them? Although several benchmarks and surveys have attempted to organize this proliferation (Tab. 1), they mostly lack large-scale, systematic evaluation. Critical aspects such as robustness of improvements, zero-shot generalization, and computational efficiency are often overlooked, and they also fail to provide complete and in-depth analyses of dataset-specific characteristics and model architectures. Despite reported gains in recent works, many studies report metrics without statistical validation (73% omit it, Fig. 1D), use incomplete baseline comparisons, or rely on limited dataset coverage. Moreover, efficiency, although vital for real-world clinical deployment (Vashist, 2017, Wenderott et al., 2024, Xu et al., 2025), is rarely considered. Compounding this issue, evaluations are typically confined to in-distribution settings (84% of work ignores zero-shot evaluation, Fig. 1D), even though clinical practice inevitably involves domain shifts across institutions and annotation protocols (Yan et al., 2019, Koch et al., 2024). These gaps leave the robustness and practicality of U-Net variants in real-world scenarios largely unverified (Niu et al., 2024). To systematically and comprehensively evaluate U-shaped medical image segmentation models, we introduce U-Bench, the first large-scale, statistically rigorous, and efficiency-oriented benchmark for U-Net and its variants. U-Bench is built upon three key aspects: (1) Broad dataset and model coverage: we implement 100 recent U-Net variants and evaluate them on 28 benchmark datasets covering 10 diverse imaging modalities (ultrasound, dermoscopy, endoscopy, fundus photography, histopathology, nuclear imaging, X-ray, MRI, CT, and OCT; Fig. 1A, C). (2) Rigorous and comprehensive evaluation: all models are implemented to calculate performance gains over the baseline U-Net with statistical significance, ensuring robust and fair comparisons (Fig. 1E). To capture clinical utility, we further assess zero-shot generalization across modalities. Additionally, to address practical considerations in real-world edge deployment, we introduce the U-Score, statistically grounded, large-scale metric that jointly accounts for accuracy, parameter numbers, computational cost, and inference speed (Fig. 1F). (3) Public availability and reproducibility: U-Bench implements models using official code implementations, pre-trained weights, and deep supervision strategies (if available). At the same time, U-Bench is released with all code, models, and protocols, enabling the community to reproduce our results and extend the benchmark with future methods. Building on this large-scale evaluation, we identify key findings that challenge common assumptions. Traditional metrics like IoU show signs of saturation, offering limited discriminative power (Fig. 1F). Additionally, reported improvements are often inconsistent or statistically insignificant (Fig. 1E). At the same time, an increasing focus on storage and computational cost is reflected in the rising trajectory of U-Score 3 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 1: Comparisons between U-Bench and other medical image segmentation benchmarks. Details can be found in the Appendix B. Category Models Datasets Modalities Evaluation Architecture Analysis Dataset Analysis Item Ultrasound Dermoscopy Endoscopy Fundus X-Ray Histopathology CT MRI Nuclei OCT Robustness Generalization Efficiency CNN Transformer Hybrid Mamba RWKV Scale Boundary Shape U-Bench (ours) 100 28 TorchStone (Bassi et al., 2024) 19 3 nnWNet (Zhou et al., 2025) 20 8 MedSegBench (Zhou et al., 2025) 6 35 nnU-Net Revisited (Isensee et al., 2024) 19 6 (Fig. 1F). To explore these dynamics, we conduct systematic analysis of U-Net variants, examining the influence of dataset and architectural factors on model performance across different modalities, architectures, and computational resource limitations (Fig. 1G). Building on these analyses, we introduce model advisor agent that suggests suitable architectures based on dataset and task attributes, turning an actionable guidance for practitioners in clinical and research contexts. Our contribution can be summarized as: We provide comprehensive evaluation benchmark of 100 U-shaped variants across 28 datasets from 10 modalities with rigorous assessment across statistical robustness, zero-shot generalization, and computational efficiency. To better capture the trade-off between accuracy and efficiency, we introduce U-Score, novel metric grounded in large-scale statistical analysis that enables fair and holistic evaluation. We summarize the observations over large-scale evaluation: Most variants show performance gains, but few show in-domain statistical significance over the original U-Net. Zero-shot performances show significant and promising improvements. U-Score shows an increasing trajectory, indicating the shift from purely pursuing accuracy to balancing accuracy with efficiency. We disentangle different aspects, including dataset characteristics and architectural designs, revealing their impact on performance and efficiency, and further build model recommender that helps researchers identify well-suited architectures under diverse data and resource conditions. We open-source U-Bench and all the pretrained weights, providing large-scale benchmark with comprehensive evaluation for medical image segmentation, to foster fair, robust, generalizable, and efficient research in the community. 4 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking 2. U-Bench Construction 2.1. Preliminaries: u-shaped design ùëÅ 2(ùëñ1) ùëñ=1 , ùëìùëñ Rùê∂ùëñ ùêª U-shaped model generally comprises three components: hierarchical encoder, decoder, bottleneck, and skip-connection. Given an input image ùë• R3ùêªùëä , Encoder() extracts multiscale features ùëìùëñ by ùëÅ stages from up-bottom, denoted as ùëä 2(ùëñ1) . Bottleneck() processes the last {ùëìùëñ} output feature, and Decoder() is composed of ùëÅ 1 stages for upsampling decoder features ùëëùëó from bottom-up, each stage comprises Skip-connection() for feature fusion. Final prediction ùë• is produced by the segmentation head after the top decoder stage. The differences across variants are illustrated in Fig. 2: Convolutional Neural Networks (CNNs) and related architectures (Attention, Mamba, RWKV) form the core building blocks, which can be organized in pure CNN / Attention, parallel, or sequential configurations for both encoding and decoding. Detailed categorization can be found in the Appendix and D. Figure 2: Summary of U-shaped networks. The network comprises an encoder, bottleneck, and decoder with skip-connection, each of which can integrate attention gates and multiscale fusion. 2.2. Dataset and Model Zoo Dataset Zoo. As shown in Fig. 1(C), the U-Bench dataset zoo consists of 28 diverse 2D medical image segmentation datasets spanning wide range of imaging modalities, including ultrasound, dermoscopy, endoscopy, fundus photography, histopathology, nuclear imaging, X-ray, MRI, CT, and OCT. We train on 20 datasets and evaluate zero-shot generalization on 8 additional ones. Following prior work (Chen et al., 2021, Tang et al., 2025b, Valanarasu and Patel, 2022, Jiang et al., 2025, Wang et al., 2022a), all datasets are resized to 256256 and augmented by random rotation and flipping; for models with fixed input size, we keep their original resolution (typically 224224). Official splits are used when available; otherwise, 7/3 split is applied. All details on datasets and preprocessing are provided in the Appendix C. Model Zoo. We curate collection of 100 publicly available and widely adopted U-Net variants, covering CNN-, Transformer-, Mamba-, and RWKV-based architectures, as well as their hybrid designs (Fig. 1(B)). To ensure strict reproducibility and fair comparison, we follow the official implementations for all models, adopting their predefined settings, pretrained weights, and deep supervision strategies when available. All model details are provided in the Appendix D. 2.3. Evaluation Protocol Evaluation Metrics. Following previous works (Luo et al., 2025, Jiang et al., 2025, Tang et al., 2025b, Valanarasu and Patel, 2022, Tang et al., 2024), we evaluate segmentation performance using Intersection over Union (IoU). To evaluate the statistical significance of performance differences between models, we conduct paired sample ùë°-tests, comparing each variant to the baseline U-Net. U-Bench also considers computational efficiency metrics, including Parameters (M), FLOPs (G), and FPS. All result details are provided in the Appendix D. Zero-shot Evaluation. To evaluate the generalization capability of each model beyond the training distribu5 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Figure 3: Comparison between IoU and U-Score. Red rectangle indicates the models perform better than U-Net in IoU, and green rectangle indicates the models perform better than U-Net in U-Score. (A) Across 100 variants, few methods show better IoU compared to baseline U-Net, while more than half of the methods show better U-Score. (B) The relationship between performance (IoU) and the increase in computational resources (FLOPs, parameters, FPS) is complex, whereas U-Score offers clear distribution that effectively distinguishes favorable and unfavorable accuracy-efficiency trade-off. tion, we conduct zero-shot inference on unseen datasets within the same modality and task. Specifically, models are trained on source datasets and then directly evaluated on unseen datasets that share the same modality but differ in acquisition domain. Detailed dataset split can be found in Fig. 1(C). This approach aligns with clinical demands, where domain shifts frequently occur in real-world applications due to variations in devices, institutions, and patient populations. U-Score. To assess real-world deployability, we propose U-Score, unified metric that jointly accounts for accuracy and efficiency. For each model ùëñ, we compute segmentation accuracy evaluated by IoU ùê¥ùëñ parameter ùëÉùëñ, FLOPs ùê∫ùëñ, and FPS ùëÜùëñ, which are percentile-normalized into ùëéùëñ, ùëùùëñ, ùëîùëñ, ùë†ùëñ [0, 1] using the 10-th/90-th quantiles across the model zoo. Given harmonic mean function ‚Ñã() with equal weights for each input, an efficiency subscore Effùëñ = ‚Ñã(ùëùùëñ, ùëîùëñ, ùë†ùëñ) is obtained, ensuring that no single factor dominates. The final UScore is defined as U-Scoreùëñ = ‚Ñã(ùëéùëñ, Effùëñ) to incorporate segmentation accuracy and computational efficiency equally. This combination rewards models that achieve favorable accuracy-efficiency trade-offs and provides deployment-oriented alternative to IoU. More details can be found in Appendix E. We report the IoU and U-Score for all models with parameter count, FLOPs, and FPS, as shown in Fig. 3 and Fig 1(F). Models with lower computational costs show wide variation in IoU performance, while heavier models tend to achieve higher accuracy at the expense of greater resource consumption. The U-Net baseline, which falls in the middle in terms of computational demand, delivers reasonably strong performance. Although some models surpass U-Net in segmentation accuracy, they require substantially different levels of computational overhead, making direct comparisons with the baseline difficult. Using U-Score, however, reveals that U-Net has weak accuracy-efficiency trade-off, whereas other models show more distinct and discriminative results, allowing clearer separation between approaches with favorable and unfavorable trade-offs. Notably, the IoU gains of advanced models over U-Net are marginal, suggesting saturation point, while the large gap between the Top-1 model√¢ƒÇ≈πs U-Score and U-Net√¢ƒÇ≈πs highlights that IoU alone is no longer the key bottleneck in medical segmentation tasks. These findings underscore that efficiency is becoming an increasingly critical factor in model development and practical deployment. 6 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking 3. U-Bench Results & Discussion In this section, we present the results of the U-Bench benchmark across multiple dimensions, including accuracy, efficiency, and generalization. We organize the results as follows: In 3.1, we present and discuss retrospective analysis of the develop trends and statistical findings over 100 variants spanning different architectures and publication years. In 3.2, we disentangle influence factors into two aspects: dataset and architecture, and analyze how these factors impact model performance. In 3.3, we propose our ranking-based advisor agent, offering practical guidance for selecting optimal models based on dataset characteristics and resource constraints. 3.1. Retrospective Analysis of the Past Decade Finding 1: In-domain Top-1 performance has marginal gains in segmentation accuracy, while zero-shot improves more pronouncedly. We analyzed 100 variants across different architectures and publication years (the detailed list can be found in the Appendix of Fig. 9), reporting the best-performing variant for each year, as shown in Fig. 4. Over the past decade, 70% of modalities have demonstrated steady progress of segmentation accuracy in both source and target domains, as reflected in IoU. However, IoU gains have been marginal (on average 1%-2%) and inconsistent. Some modalities (i.e. OCT, Nuclei, and Fundus) even show sign of stagnation. In comparison, when considering zero-shot performances, the improvements have been more obvious (more than 3% on average) in 80% of the modalities. Finding 2: Although some in-domain improvements exist on average, few reach statistical significance, whereas the average zero-shot improvements remain consistently significant. To rigorously distinguish modalities with genuine improvements from those with only numerical fluctuations, we perform ùë°-tests between each variant and the U-Net baseline. The results are presented in Fig. 1(E) and Fig. 5. We observe that over 80% of variants fail to achieve statistically significant improvements. Even in the most heavily studied modalities, such as Ultrasound, Endoscopy, Dermoscopy, CT, and MRI, most gains are marginal and lack significance. Only handful of datasets (e.g., BUSI, TNSCUI, Kvasir, ISIC2018, Convidquex) exhibit consistent clusters of superior variants. In contrast, in experiments with zero-shot transfer, for variants that outperform U-Net, more than 50% of the variants are significant across 75% of the modalities. Possible explanations for findings 1 & 2: We provide possible explanation for these interesting observations. Considering in-domain evaluations, the improvements with statistical significance is typically associated with the lesion localization tasks which requires global semantic comparison. Specifically, lesions often exhibit significant differences from surrounding normal tissues, requiring global context to model these distinctions Zhou et al. (2017), Isensee et al. (2021). In recent years, with the growing adoption of long-range modeling techniques (e.g., attention-based Transformers, state-space models such as Mamba, and RNN-inspired hybrids like RWKV), architectural innovations have increasingly focused on capturing long-range dependencies, leading to more pronounced and steady improvements in these lesion segmentation tasks. On the other hand, long-range modeling techniques have been proven to be more generalizable (Jiang et al., 2024a, Harun et al., 2024, Hou et al., 2025, Gu et al., 2024), leading to improvements in zeroshot generalization ability. By contrast, modalities dominated by repetitive local patterns (e.g., Nuclei, Fundus) benefit far less from global modeling, exhibiting only marginal improvements. This underscores the complementary need for localized mechanisms to achieve precise boundary delineation. Finding 3: Increasing attention on efficiency. While IoU shows marginal improvements, U-Score improvements are more pronounced, with an average increase of 33%. This trend supports our argument in Sec. 2.3, where we suggest that IoU has reached saturation point and has limited ability to discriminate favorable 7 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Figure 4: Performance trends of SOTA models over the past decade. The x-axis indicates publication year, with each point marking the yearly best result. The y-axes report two evaluation metrics: IoU (left axis) and U-Score (right axis). The trends summary is shown as arrows at the top of the y-axis, with green ones highlighting improvements and red ones indicating stagnation. Source domain performance is show at the top, and zero-shot performance is shown at the bottom. Figure 5: Statistical significance analysis against U-Net across 28 datasets across 10 modalities. The outer blue pie represents the number of variants surpassing U-Net; the inner pie quantifies the statistical significance of the methods with improvements, annotated by non-significant to highly significant, with the number of works annotated in the middle. In general, in-domain improvements show limited statistical significance, while zero-shot performances show more significant improvements. methods from unfavorable ones, indicating that accuracy alone is no longer the bottleneck of segmentation. The increasing trend of U-Score reflects the growing emphasis and room for improvement on efficient models in the medical community, echoing the practical demand for clinical deployment beyond the lab research. 8 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 2: Top-10 variants ranked by performance (IoU) and efficiency (U-Score) under in-domain and zero-shot settings. Variants cover CNN, Transformer, Mamba, RWKV, and Hybrid architectures. Source (IoU) Target (IoU) Source (U-Score) Target (U-Score) Rank Variants RWKV-UNet AURA-Net UTANet MEGANet # Volume (year) Arxiv (2025) ISBI (2021) AAAI (2025) #4 WACV (2024) #5 Swin-umamba MICCAI (2024) CIBM (2024) #6 Arxiv (2022) #7 TransResUNet ISBI (2025) #8 MIUA (2022) #9 FBB (2024) MICCAI (2025) #10 DA-TransUNet #25 EViT-UNet FCBFormer MFMSNet U-Net Rank # Volume (year) Variants Arxiv (2025) RWKV-UNet G-CASCADE WACV (2024) Swin-umamba MICCAI (2024) WACV (2024) WACV (2023) CIBM (2024) Arxiv (2022) TIM (2022) MICCAI (2025) MICCAI (2020) MICCAI (2025) #4 #5 #6 #7 TransResUNet #8 DS-TransUNet #9 #10 #72 MEGANet CASCADE MFMSNet CENet PraNet U-Net Rank Variants LGMSNet MBSNet CMUNeXt LV-UNet # Volume (year) ECAI (2025) MSSP (2021) ISBI (2024) #4 BIBM (2024) #5 Mobile U-ViT ACM MM (2025) MICCAI (2024) #6 MICCAI (2025) #7 AAAI (2025) #8 CIBM (2023) #9 Arxiv (2025) #10 MICCAI (2025) #64 Tinyunet U-RWKV U-KAN DCSAU-Net RWKV-UNet U-Net Rank Variants LGMSNet LV-UNet U-KAN # Volume (year) ECAI (2025) BIBM (2024) AAAI (2025) #4 Mobile U-ViT ACM MM (2025) RWKV-UNet Arxiv (2025) #5 MBSNet MSSP (2021) #6 SwinUNETR CVPR (2022) #7 ISBI (2024) #8 CMUNeXt WACV (2024) #9 G-CASCADE CIBM (2021) MICCAI (2025) TA-Net U-Net #10 #69 Figure 6: Average performance of different architectures across modalities under in-domain and zero-shot settings. Left: IoU-based comparison; Right: U-Score-based comparison of different architecture strengths in sourcedomain and zero-shot. 3.2. Influencing Factor Analysis: Architectures and Data Characteristics 3.2.1. Architectures To analyze the performance regarding architectural choices, we divide 100 models into five families: CNN, Transformer, Mamba, RWKV, and Hybrid. The detailed descriptions are summarized in Appendix D. We present the top-10 variants across all datasets ranked by IoU and U-Score under in-domain and zero-shot settings, as shown in Tab. 2, and we calculate the average performance of each architecture family, as shown in Fig. 6. Considering segmentation performance (IoU), Hybrid architectures achieve the highest accuracy by combining local priors with global attention. As shown in Tab. 2(Left), 5 of the top 10 models in both in-domain and zero-shot are hybrid, highlighting their high potential. On average, the hybrid family consistently delivers the best in-domain performance and competitive zero-shot generalization (Fig. 6(Left)), particularly excelling on lesion-centric tasks such as Ultrasound and Endoscopy. The newly proposed RWKV family ranks first in IoU for both in-domain and zero-shot evaluations, indicating promising potential despite limited prior research. In contrast, Mamba family shows weaker segmentation performance, which may be attributed to its architectural design, which, despite its strengths in certain tasks, might struggle with capturing fine-grained details or handling complex patterns in segmentation tasks. Once computational demands are taken into account, as shown in Tab. 2(Right), the U-Score-based leaderboard is reshuffled, with the CNN family leading in performance, comprising 7 / 5 out of the top 10 models in in-domain / zero-shot settings, respectively. The newly proposed RWKV family achieves the best average in-domain results and competitive zero-shot performance (Fig. 6(Right)), further supporting its structural 9 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Figure 7: Performance analysis under varying foreground properties. (A) Foreground properties influence segmentation task difficulty. The yellow background indicates the challenge segmentation case. (B) Architectural influence on segmentation difficulty across diverse foreground properties. Dark / Shallow: hard / easy case. superiority and potential. In contrast, inefficient long-range modeling methods, including Transformer, and Hybrid architectures, face higher computational demands, leading to reduced performance when evaluated by U-Score. Although Mamba excels in efficiency, its inconsistent accuracy undermines the U-Score, offsetting its efficiency advantage. 3.2.2. Data Characteristics We further investigate how performances vary with distinct foreground characteristics with three aspects: foreground scale, boundary sharpness, and shape complexity. The Appendix F.1 provides detailed definitions for the different scales of target area, edge, and shape regularity. Figure 7(A) summarizes the characteristics of challenging cases: blurry boundaries are the dominant factor, with often causing substantial drops in segmentation performance, while small object size and irregular shapes further exacerbate the difficulty. When these foreground properties shift across datasets, different models exhibit varying performance patterns. As shown in Fig. 7(B), consistent with our earlier findings, hybrid architectures dominate both in easier and more challenging cases, proving that local and global fusion mechanism enables greater adaptability across diverse foreground properties, particularly for blurry boundaries. RWKV-based models show specific strength in capturing irregular but well-defined shapes, reflecting their ability to model long-range contours. Nonetheless, boundary ambiguity, along with small and irregular targets, remains the central challenge; given its prevalence in medical images, uncertainty-aware designs are needed. Since architectural strengths are dataset-dependent, these observations highlight the importance of task-aware advising mechanisms that can match models to dataset properties. 3.3. Model Advisor Agent Based on our analysis, we introduce ranking-based model advisor agent, designed to guide the community in selecting the most suitable models based on dataset characteristics and task requirements. This tool not only streamlines model selection but also helps users navigate the trade-offs between performance and efficiency, ensuring more informed, task-aware decisions. Figure 8: Our model advisor agent. The system overview is shown in Fig. 8. Our advisor agent system utilizes dataset-level characteristics (e.g., 10 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking modality, boundary sharpness, shape complexity, and foreground scale) along with resource constraints (storage, computation, and speed) to predict the suitability of various U-shaped architectures. Rather than relying on manual trial-and-error approach, our framework leverages XGBoost (Chen and Guestrin, 2016) as the recommended backbone and outputs candidate models and architectures that best satisfy the specified requirements. Crucially, the output is not single best model but prioritized list, offering more flexibility in choices to practitioners. Further details on the recommendation setup, dataset construction, implementation details, and evaluation metrics are provided in the Appendix F.2 and F.3. Table 3: NDCG, MAP, and Spearman correlation of our advisor agent. We design set of experiments to validate the feasibility of automatic model suggestion in medical image segmentation. Our setup uses 18 in-domain datasets for training and holds out 2 datasets for validation. We use Normalized Discounted Cumulative Gain (NDCG), mean average precision (MAP) and Spearman correlation for evaluation (See Appendix F.3). As shown in Tab. 3, our experiments demonstrate that the proposed model advisor agent effectively recovers ranking orders that align with ground-truth IoU and U-Score rank in our benchmark. The results validate that our advisor agent system is able to prioritize suitable models across different task requirements, making it reliable tool for model selection and deployment. NDCG @5 @20 0.75 0.76 0.74 0. IoU U-Score MAP Spearman Ranking Metric 0.24 0.43 0.36 0.52 4. Conclusion key challenge in the field of medical image segmentation remains: How can we conduct fair and comprehensive comparison across the numerous U-shaped variants ? To address this, we introduce U-Bench, framework that fills critical gaps in prior evaluations by offering comprehensive, statistically rigorous, and efficiency-oriented approach. Our results challenge common assumptions in the field, revealing that while many variants show performance gains, few achieve statistical significance in-domain. In contrast, zero-shot generalization demonstrates substantial improvements, highlighting the potential for better model generalization across domains. In addition, the newly proposed U-Score metric, which emphasizes efficiency alongside performance, signals paradigm shift from models focused solely on accuracy to those that balance both performance and efficiency. Leveraging insights from our analysis of model architecture and dataset characteristics, we propose ranking-based model agent that transforms our large-scale evaluation into actionable guidance for selecting models tailored to specific tasks. By releasing U-Bench as an open-source platform, we provide the community with robust, reproducible tool to advance research in segmentation, enabling the development of models that are both accurate and feasible for clinical deployment. 5. Acknowledgment Supported by Natural Science Foundation of China under Grant 62271465, National Key R&D Program of China under Grant 2025YFC3408300, and Suzhou Basic Research Program under Grant SYG202338. Fenghe Tang: Writing review & editing, Methodology, Conceptualization, Visualization, Formal analysis, Validation, Data curation, Writing original draft, Investigation. Chengqi Dong: Writing review & editing, Methodology, Visualization, Data curation, Writing original draft. Wenxin Ma: Writing review & editing, Conceptualization, Formal analysis, Writing original draft. Zikang Xu: Writing review & editing, Formal analysis. Heqin Zhu, Zihang Jiang, Rengsheng Wang, Yuhao Wang, and Chenxu Wu: Writing review & editing. Shaohua Kevin Zhou: Supervision, Writing review & editing, Funding acquisition. 11 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking A. Appendix In this appendix, we provide additional details and results to complement the main paper. The content is organized as follows: Appendix B: Relate Work. Appendix C: Details of U-Bench Data Zoo. Appendix D: Details of U-Bench Model Zoo. Appendix E: Details of U-Score. Appendix F: Implementation and Evaluation Details. Appendix G: Additional Results. Appendix H: Reproducibility Checklist. 12 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking B. Related Work In Appendix B, we present broad view of the variations of U-shape networks, including the architecture of the network and existing medical segmentation benchmarks. B.1. Model Architecture As the core architecture for medical image segmentation, the U-Net has evolved into numerous variants in recent years, driven by advancements in feature representation capabilities, long-range dependency modeling techniques, and the trade-off between efficiency and accuracy. This section categorizes and organizes these U-Net variants based on their core paradigms and design motivations, systematically tracing their evolutionary path from foundational construction to integrated innovation. Fig. 9 summarizes the evolution of U-Net variants over time. Figure 9: Time and architecture distribution of all evaluated models. 1. U-shaped Networks Dominated by Convolutional Neural Networks (CNNs) (2015-2021: Foundational Laying) U-shaped networks, using convolutional neural networks as their sole backbone, extract local features through convolution operations and utilize fixed skip connections to fuse multi-scale information, laying the foundation for the \"encoder-decoder\" paradigm in medical image segmentation. Their advantages lie in their ability to accurately capture local details (such as textures and edges), their relatively lightweight architecture, and their stable training process, providing solid design baseline for subsequent complex variants. However, the local receptive field of convolutional operations limits the ability to model global semantic information, and fixed skip connections easily lead to semantic gap between the encoder and decoder, limiting performance. 2. Transformer-driven U-Networks (2021-2023: Paradigm Shift) This variant introduces the Transformer architecture (including variants such as Vision Transformer (Dosovitskiy et al., 2020) and Swin Transformer (Liu et al., 2021)) to replace or enhance the traditional CNN 13 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking backbone, leveraging the self-attention mechanism to effectively model long-range dependencies. However, the computational complexity of the self-attention mechanism grows quadratically with sequence length, resulting in low inference efficiency. Furthermore, this type of model is poorly adaptable to small-scale medical datasets and is prone to overfitting due to insufficient data, making it difficult to meet the stringent real-time requirements of clinical edge devices. 3. U-Networks Based on State-Space Models (SSMs) and Recurrent Paradigms (2023-2025: EfficiencyOriented) Recent research explores replacing the quadratic-cost self-attention with linear-time alternatives. One line leverages state-space models (e.g., Mamba (Gu and Dao, 2023)) that adopt selective state updates to capture long-range dependencies while achieving linear complexity, markedly improving inference efficiency and adaptability to small sample sizes. Another complementary line introduces RWKV (Peng et al., 2023), recurrent-inspired model that combines Transformer-like expressiveness with RNN-style recurrence, enabling efficient sequential processing and stronger generalization across varying input lengths. Together, these paradigms alleviate the computational and data-dependency limitations of Transformers. 4. Multi-Paradigm Fusion U-Networks (Hybrid Networks) (2020-2025: Fusion and Innovation) This phase aims to integrate the advantages of CNNs in local feature extraction, the global semantic modeling capabilities of Transformers. The goal is to achieve balance between accuracy, efficiency, and generalization by fusing different architectures. This type of network variant can adapt to complex clinical scenarios such as multimodal imaging and cross-center data heterogeneity, significantly improving the practical value of segmentation results. However, the architectural design complexity increases significantly, and the coordination mechanisms between modules of different paradigms (such as the timing of feature interactions and weight distribution) still need further optimization. The development of the four types of U-shaped network variants follows technological evolutionary path of \"local refinement global correlation efficiency considerations multi-paradigm collaboration,\" reflecting the shift in clinical needs from static, single-scenario segmentation toward more efficient, generalized solutions adaptable across diverse conditions. B.2. Medical Segmentation Benchmarks To fill the research gap in the evaluation of U-net systems, we comprehensively compare previous segmentation evaluation benchmarks with the U-Bench proposed in this paper, thereby clarifying the innovative positioning of U-Bench. B.2.1. Related Work Medical image segmentation has seen rapid progress, driven by deep learning architectures and large-scale datasets. However, the validity and reproducibility of many reported advances have been challenged due to inconsistent evaluation protocols, limited dataset diversity, and insufficient consideration of deployment constraints. TorchStone (Bassi et al., 2024) addressed some of these limitations by introducing large-scale collaborative benchmark for abdominal organ segmentation, leveraging diverse CT scans from multiple hospitals worldwide. While it emphasized out-of-distribution generalization and standardized evaluation, its scope was limited to single anatomical region and modality, making it less suitable for assessing broader architectural capabilities. MedSegBench (Zhou et al., 2025) expanded coverage across modalities, incorporating 35 datasets from 14 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking ultrasound, MRI, X-ray, and others. It provided standardized splits and evaluated multiple encoder-decoder variants, aiming to foster universal segmentation models. However, its focus remained on smaller set of architectures and lacked comprehensive analysis of robustness, efficiency, and cross-paradigm comparisons. nnWNet (Zhou et al., 2025) proposed architectural modifications to integrate convolutions and transformers within U-Net framework, addressing the need for continuous transmission of local and global features. Although it benchmarked on multiple 2D and 3D datasets, its evaluation was limited to small number of models and lacked systematic efficiency analysis. nnU-Net Revisited (Isensee et al., 2024) critically examined recent architectural claims, showing that properly configured CNN-based U-Nets could still match or outperform newer transformer and Mamba-based models when trained with sufficient resources. This study highlighted the importance of rigorous baselines and computational reproducibility, yet it did not provide multi-modal, multi-dataset framework for comparing large number of variants. Collectively, these efforts underscore the need for unified, statistically rigorous, and comprehensive benchmark that systematically evaluates broad spectrum of U-Net variants across diverse modalities, datasets, and deployment metrics. B.2.2. Targeted Improvements of U-Bench As summarized in Tab. 1, existing medical image segmentation benchmarks suffer from limited modality coverage, insufficient evaluation diversity, narrow architectural scope, and lack of dataset-specific analysis√¢ƒÇ≈§all of which hinder comprehensive assessment of model generalization. To address these gaps, U-Bench is designed with three targeted innovations, establishing more comprehensive and clinically relevant evaluation framework while aligning with its core goals: evaluating 100 U-Net variants across 28 datasets and 10 modalities, introducing the performance-efficiency balanced U-Score, and enabling fair, reproducible benchmarking. 1. Multimodality and Full Task Coverage U-Bench encompasses 10 major medical imaging modalities (ultrasound, dermoscopy, endoscopy, fundus, histopathology, nuclei, X-Ray, MRI, CT, OCT) and integrates 28 datasets (sample sizes: 20-17,000). It covers tasks from macroscopic organ segmentation (e.g., lung CT, cardiac MRI) to microscopic structure segmentation (e.g., histopathological nuclei, retinal microvasculature), with standardized train/test splits. This design tests cross-modality adaptability of models, matching real-world clinical multimodal diagnostic workflows. 2. Multi-Dimensional Evaluation System Beyond traditional accuracy metrics (IoU, Dice), U-Bench introduces three critical evaluation dimensions and unified U-Score to quantify clinical utility: Computational Efficiency: Standardized reporting of model parameters (M), inference FLOPs (G), and FPS to reflect deployability on resource-constrained devices. Generalization Performance: Zero-shot transfer tests on 8 unseen target datasets (distinct from 20 training source datasets) to assess robustness to domain shifts (e.g., cross-center ultrasound, unseen dermoscopic lesions). Statistical Significance: Paired t-tests between each variant and the original U-Net (p < 0.05 as significant) to validate reliable performance gains. U-Score: comprehensive metric using quantile normalization and weighted harmonic mean to balance accuracy and efficiency, bridging academic performance and clinical deployment value. 3. Large-Scale Reproducible Validation U-Bench includes 100 publicly available U-Net variants, covering mainstream architectures from 2015 to 2025 (CNN, Transformer, Mamba, RWKV, hybrid designs). To ensure reproducibility, all models adopt official implementations, pre-trained weights (if available), and deep supervision strategies (if applicable). 15 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking C. Details of Data Zoo We summarize the dataset statistics used in this paper in Table 4. This table details the datasets used for experimental evaluation, covering 10 core imaging modalities, including ultrasound (e.g., BUSI), dermoscopy (e.g., ISIC2018), endoscopy (e.g., Kvasir-SEG), fundus (e.g., CHASE), histopathology (e.g., Glas), nuclear (e.g., DSB2018), X-ray (e.g., Montgomery), MRI (e.g., ACDC,), CT (e.g., Synapse), and OCT (e.g., Cystoidfluid). For each dataset, we provide key information such as the segmentation class (binary or multiclass), the number of samples, the year of publication, and basic description. All datasets used are publicly available. Therefore, we provide access links in the relevant references and supplementary tables. The details are available in Tab. 4. brief description of the dataset is as follows: BUSI. The Breast Ultrasound Images (BUSI) dataset (Al-Dhabyani et al., 2020), collected from 600 female patients in 2018, contains 133 normal, 487 benign, and 210 malignant cases with corresponding ground truth labels. The data labels are obtained using ultrasound scans to examine breast cancer lesion areas. BUS. The Breast UltraSound (BUS) public dataset (Zhang et al., 2022) includes 562 images (306 benign, 256 malignant) collected via five ultrasound devices, used for generalization experiments. The data labels are obtained using ultrasound scans to examine breast cancer lesion (or non-lesion) areas. BUSBRA. The BUS-BRA dataset (G√≥mez-Flores et al., 2024) comprises 1875 anonymized images from 1064 patients (corresponding to 722 benign and 342 malignant cases) acquired via four ultrasound scanners. The data labels are obtained using ultrasound scans to examine breast cancer lesion (or non-lesion) areas. TNSCUI. The Thyroid Nodule Segmentation and Classification in Ultrasound Images 2020 dataset1 includes 3644 cases from the Chinese Artificial Intelligence Alliance for Thyroid and Breast Ultrasound. The data label is the thyroid nodule area obtained by thyroid ultrasound. TUCC. The Thyroid Ultrasound (TUCC) dataset2 collects data from 167 patients, including 192 biopsyconfirmed nodules. The data label is the thyroid nodule area obtained by thyroid ultrasound. ISIC2018. The ISIC 2018 dataset (Codella et al., 2018) is large-scale dermoscopy dataset for lesion segmentation, containing 2594 skin lesion images. The data label is the melanoma (or non-lesion) area of the skin disease obtained by dermoscopy imaging. PH2. The PH2 database (Mendon√ßa et al., 2013) includes 200 dermoscopic images with manual segmentation and clinical diagnosis. The data label is the melanoma (or non-lesion) area of the skin disease obtained by dermoscopy imaging. SkinCancer. The SkinCancer dataset (Ku≈ü and Aydin, 2024) contains 206 dermoscopic samples extracted from DermIS and DermQuest. The data label is the melanoma (or non-lesion) area of the skin disease obtained by dermoscopy imaging Covidquex. The Covidquex dataset (Ku≈ü and Aydin, 2024) includes 2,913 chest X-ray images (256 256 pixels) for binary segmentation. The dataset is labeled with COVID-infected areas on chest X-rays. Montgomery. The Montgomery dataset (Jaeger et al., 2014) contains 138 chest X-rays (80 normal, 58 with tuberculosis). The data label is the tuberculosis lesion (or non-lesion) area on the lung X-ray. NIH-test. The NIH-test dataset (Tang et al., 2019) is manually annotated chest X-ray dataset with 100 lung masks. The data labels are lung segmentations from chest X-rays. 1Available at: https://tn-scui2020.grand-challenge.org/Home. 2Available at: https://aimi.stanford.edu/datasets/thyroid-ultrasound-cine-clip. 16 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking DCA. The DCA dataset (Ku≈ü and Aydin, 2024) contains 134 fundus images (300 300 pixels). The data label is the blood vessel segmentation of the fundus image. Kvasir. The Kvasir dataset (Jha et al., 2020b) contains 1000 gastrointestinal polyp images and corresponding ground truth. The data labels are pathological areas of gastrointestinal endoscopic imaging. CVC-300. The CVC-300 dataset (V√°zquez et al., 2017) comprises 60 colonoscopy polyp images (500 574 pixels). The data labels are pathological areas of gastrointestinal endoscopic imaging. CVC-ClinicDB. The CVC-ClinicDB dataset (Bernal et al., 2015) includes 612 images from 29 colonoscopy sequences. The data labels are pathological areas of gastrointestinal endoscopic imaging. Robotool. The Robotool dataset (Ku≈ü and Aydin, 2024) consists of 500 images extracted from multiple surgical videos. The data label is the instrument area imaged by the endoscope. Promise. The Promise dataset (Ku≈ü and Aydin, 2024) includes 1,473 prostate MRI samples (512 512 pixels). ACDC. The ACDC dataset (Bernard et al., 2018) contains 100 cardiac MRI scans. The data labels for left ventricle (LV), right ventricle (RV), and myocardium (MYO) in heart segmentation. CHASE. The CHASE dataset (Fraz et al., 2012) includes 28 retinal images (one per eye from 14 children). The data label is the vascular area of the fundus image. Stare. The Stare dataset (Hoover et al., 2000) includes 20 ocular fundus vessel images with manual annotations. The data label is the vascular area of the fundus image. DRIVE. The DRIVE dataset (Staal et al., 2004) is collected from Dutch diabetic retinopathy screening program. The data label is the vascular area of the fundus image. Cell. The Cell dataset (Ku≈ü and Aydin, 2024) consists of 670 nuclei images with resolution of 320256 pixels. The data label is the cell nucleus segmentation area. Glas. The Glas dataset (Sirinukunwattana et al., 2015) contains 165 H&E stained slide images for gland segmentation. The data label is the glandular lesion (or non-lesion) area of the Hematoxylin and Eosin image. Monusac. The Monusac dataset (Ku≈ü and Aydin, 2024) includes 310 H&E stained digital tissue images. The data labels are the nucleus regions of H&E stained histology images. Tnbcnuclei. The Tnbcnuclei dataset (Ku≈ü and Aydin, 2024) contains 50 pathological samples for binary segmentation. The data labels are the cell nucleus regions of Hematoxylin and Eosin stained histology images. Synapse. The Synapse multi-organ dataset includes 30 abdominal CT scans with 8-organ segmentation. The data labels are 8 abdominal organs (aorta, gallbladder, left kidney, right kidney, liver, pancreas, spleen, stomach). Cystoidfluid. The Cystoidfluid dataset (Ku≈ü and Aydin, 2024) contains 1,006 Optical Coherence Tomography images. The dataset is labeled the Cystoid Macular Edema (CME) region of the retina. DSB2018. The DSB2018 dataset (Hamilton, 2018) includes 670 Hematoxylin and Eosin (H&E)-stained nuclear images. The data label is the cell nucleus segmentation area. 17 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 4: Dataset information summary, where in split type represents ourself-split, and represents splitting by data source Category Quantity Year 2020 2022 2024 2020 - Binary Binary Binary Binary Binary Modal Ultrasound Dermoscopy X-Ray Endoscopy MRI Fundus CT OCT Nuclear Histopathology Dataset BUSI BUS BUSBRA TNSCUI TUCC ISIC2018 PH2 SkinCancer Covidquex Montgomery NIH-test DCA Kvasir-SEG CVC-300 CVC-ClinicDB Robotool Promise ACDC CHASE Stare DRIVE Synapse Cystoidfluid DSB2018 Cell Monusac Tnbcnuclei Glas Binary Binary Binary Binary Binary Binary Binary Binary Binary Binary Binary Binary 4-Class Binary Binary Binary 9-Class Binary Binary Binary Binary Binary Binary 0.5k1k 0.5k1k 1k2k 3k4K 10k20k 2k3k <0.5k 206 2k 3k <0.5k <0.5k <0.5k 1k2k <0.5k 0.5k1k 0.5k1k 1k2k <0.5k <0.5k <0.5k <0.5k 3k4k 1k2k 0.5k1k 0.5k1k <0.5k <0.5k <0.5k Split type Source O S S S S S S link link link link link link link link link link link link link link link link link link link link link link link link link link link link 2018 2013 2024 2021 2014 2019 2019 2020 2017 2015 2021 2024 2012 2000 - 2023 2024 2018 2018 2016 2018 2015 D. Details of Model Zoo We conducted comprehensive statistical analysis of the 100 models evaluated by U-Stone, as shown in Tab. 5, 6 and 7. Tab. 5 and Tab. 6 summarize the basic information of the single architecture and hybrid architecture respectively, quantifying critical metrics including deep supervision adoption, pre-training status, zero-shot capability, statistical significance (P-value), parameter count (Params), computational cost 18 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking (FLOPs), and inference speed (FPS); Tab. 7 further clarifies the training foundation of all evaluated models, documenting their publication year, venue, target dataset modality, and open-source repository links for reproducibility. E. Details of U-Score Clinical deployment of segmentation models often requires operation under constrained resources. However, existing evaluations focus predominantly on segmentation performance, while failing to balance key computational factors such as model size, inference cost, and speed. This disconnect makes it difficult to assess real-world deployability. To bridge this gap, we introduce U-Score, unified metric that quantifies the trade-off between performance and efficiency using quantile statistics under large-scale benchmark. Specifically, we report the 10th and 90th percentiles of IoU, Params, FLOPs, and FPS, as summarized in Tab. 8 and 9. The formulation is represented as follow. Given model ùëñ, we compute IoU ùê¥ùëñ across datasets, parameter ùëÉùëñ in millions, FLOPs ùê∫ùëñ in GLOPs, and runtime speed ùëÜùëñ in FPS. We normalize each component using the 10th and 90th percentiles computed over the model zoo. Let ùëÑùëÄ 90 denote the 10th and 90th percentiles of metric ùëÄ . The normalized scores are defined as: 10 and ùëÑùëÄ , 0, 1) , ùëùùëñ = clip ( ùëéùëñ = clip ( ùëîùëñ = clip ( ùê¥ùëñ ùëÑùê¥ 10 ùëÑùê¥ ùëÑùê¥ 90 10 log ùëÑùê∫ log ùëÑùê∫ 90 90 log ùê∫ùëñ log ùëÑùê∫ 10 log ùëÑùëÉ log ùëÑùëÉ 90 log ùëÉùëñ log ùëÑùëÉ 10 ùëÜùëñ ùëÑùëÜ 10 ùëÑùëÜ ùëÑùëÜ 10 90 , 0, 1) , , 0, 1) . (1) , 0, 1) , ùë†ùëñ = clip ( Then, we compute an efficiency subscore via the weighted harmonic mean of ùëùùëñ, ùëîùëñ, and ùë†ùëñ. Since we regard storage, cost, and speed as equally important, we set ùë§ùëÉ = ùë§ùê∫ = ùë§ùëÜ = 1 3 , leading to: Effùëñ = 3 + 1 ùëîùëñ 1 ùëùùëñ . + 1 ùë†ùëñ (2) Finally, we combine accuracy and efficiency via harmonic mean. To balance the two factors equally, we set ùõº = 0.5, yielding: . (3) U-Scoreùëñ = 1 ùëéùëñ F. Implementation and Evaluation Details F.1. Foreground Characterization Metrics 2 + 1 Effùëñ We employ three metrics to characterize dataset-level foreground properties: scale, boundary sharpness, and shape regularity. Foreground scale. Foreground scale is measured as the ratio between the foreground area ùê¥ùëì and the total image area ùê¥ùë°. ùê¥ = ùê¥ùëì ùê¥ùë° . (4) 19 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 5: Single-architecture model comparison. Architecture Model Deep Supervision Pre-training Zero-shot P-value AtU-Net (Oktay et al., 2018) AURA-Net (Cohen and Uhlmann, 2021) CA-Net (Gu et al., 2020) CaraNet (Lou et al., 2022) CENet (Gu et al., 2019b) CE-Net (Gu et al., 2019a) CFPNet-M (Lou et al., 2021) CMU-Net (Tang et al., 2023) CMUNeXt (Tang et al., 2024) CPCANet (Huang et al., 2023a) CSCA U-Net (Shu et al., 2024) DANet (Pramanik et al., 2024) DCSAU-Net (Xu et al., 2023b) DDS-UNet (Li and Niu, 2024) DoubleUNetPlus (Jha et al., 2020a) ERDUnet (Li et al., 2024a) ESKNet (Chen et al., 2023) G-CASCADE (Rahman and Marculescu, 2023b) LFU-Net (Deng et al., 2023) LV-UNet (Jiang et al., 2024b) MALUNet (Ruan et al., 2022) MBSNet (Ye et al., 2021) MCA-Unet (Amer and Ye, 2023) MDSA-UNet (Li et al., 2025c) MEGANet (Bui et al., 2024) MMUNet (Yuan et al., 2024b) MSLAU-Net (Lan et al., 2025) MSRF-Net (Srivastava et al., 2021) MultiResUNet (Ibtehaz and Rahman, 2020) PraNet (Fan et al., 2020) ResNet34UnetPlus (Zhou et al., 2018) ResU-KAN (Wang et al., 2025a) ResUNetPlusPlus (Jha et al., 2019) RollingUnet (Liu et al., 2024c) SimpleUNet (Yu et al., 2025) TA-Net (Wang et al., 2022b) TinyU-Net (Chen et al., 2024a) UACANet (Kim et al., 2021) U-KAN (Li et al., 2025a) ULite (Dinh et al., 2023) U-Net (Ronneberger et al., 2015) UNet3+ (Huang et al., 2020) UNeXt (Valanarasu and Patel, 2022) UTANet (Luo et al., 2025) AC-MambaSeg (Nguyen et al., 2024) CFM-UNet (Niu et al., 2025) H-vmunet (Wu et al., 2025a) Mamba-UNet (Wang et al., 2024) MedVKAN (Zhu et al., 2025) MUCM-Net (Yuan et al., 2024a) Swin-UMamba (Liu et al., 2024a) Swin-UMambaD (Liu et al., 2024b) UltraLight-VM-UNet (Wu et al., 2025b) VM-UNet (Ruan and Xiang, 2024) VM-UNetV2 (Zhang et al., 2024b) RWKV-UNet (Jiang et al., 2025) U-RWKV (Ye et al., 2025) Zig-RiR (Chen et al., 2025) DC-ViT (Zhang et al., 2024a) ColonSegT (Jha et al., 2021) ConvFormer (Gu et al., 2023) CSWin-UNet (Liu et al., 2025) DAE-Former (Azad et al., 2023) MISSFormer (Huang et al., 2023b) Polyp-PVT (Dong et al., 2021) SwinUnet (Cao et al., 2022) UNETR (Hatamizadeh et al., 2022) CNN Mamba RWKV Transformer Params (M) 34.88 52.84 2.79 44.59 33.36 29.00 0.76 49.93 3.15 43.39 35.27 94.51 10.81 43.62 29.29 10.21 26.71 26.63 0.05 0.92 0.18 3.98 8.66 6.58 29.27 17.73 21.88 22.50 7.25 50.01 26.90 18.59 14.48 7.10 0.06 29.57 0.48 67.11 9.38 0.88 34.53 26.97 1.47 45.03 7.42 52.96 6.44 15.48 43.58 0.08 55.06 21.74 0.04 34.62 17.91 17.10 2.82 24.25 6.84 5.01 115.61 23.57 29.69 35.45 25.11 41.34 87.51 FLOPs (G) 5.99 11.50 10. FPS 66.63 126.09 25.15 121.63 31.71 26.82 23.63 8.90 103.22 73.13 3.47 83.28 91.25 7.42 161.14 13.36 16.23 13.74 44.99 33.24 48.18 23.83 56.84 36.87 17.40 53.96 100.99 43.18 10.29 75.38 45.28 5.54 62.77 0.76 167.91 0.21 139.30 0.08 108.64 6.86 115.10 12.26 58.02 77.36 5.65 59.62 11.71 46.93 24.04 35.34 6.27 33.16 109.73 84.31 18.76 27.55 11.96 84.54 37.63 67.56 7.78 91.72 70.99 8.28 31.85 0.74 414.31 94.43 9.32 1.66 150.32 27.79 31.55 6.89 93.47 0.76 323.06 65.52 137.05 50.70 0.57 256.68 85.63 35.64 38.08 16.30 94.47 45.91 0.06 107.24 58.52 65.48 82.45 48.08 62.85 76.14 6.90 107.52 35.59 3.31 20.87 65.34 62.16 135.43 43.05 33.05 55.23 57.68 67.64 63.25 26.41 104.25 121.13 6.14 34.10 7.25 5.30 8.69 87.59 6.27 6.17 0.74 4.60 14.13 43.93 6.20 0.06 7.56 4.40 14.58 199.74 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 6: Hybrid architecture model comparison. Model BEFUNet (Manzari et al., 2024) CASCADE (Rahman and Marculescu, 2023a) CFFormer (Li et al., 2025b) DA-TransUNet (Sun et al., 2024) DS-TransUNet (Lin et al., 2022) D-TrAttUnet (Bougourzi et al., 2024) EMCAD (Rahman et al., 2024) EViT-UNet (Li et al., 2025d) FAT-Net (Wu et al., 2022) FCNFormer (Sanderson and Matuszewski, 2022) GH-UNet (Wang et al., 2025b) H2Former (He et al., 2023) HiFormer (Heidari et al., 2023) LeViT-UNet (Xu et al., 2023a) LGMSNet (Dong et al., 2025) MedFormer (Gao et al., 2023) MedT (Valanarasu et al., 2021) MERIT (Rahman and Marculescu, 2023c) MFMSNet (Wu et al., 2023) MobileUViT (Tang et al., 2025b) MT-UNet (Wang et al., 2022c) Perspective-Unet (Hu et al., 2024) ScribFormer (Li et al., 2024b) SCUNet++ (Chen et al., 2024b) SwinUNETR (Hatamizadeh et al., 2021) TransAttUnet (Chen et al., 2022) TransFuse (Zhang et al., 2021) TransNorm (Azad et al., 2022) TransResUNet (Tomar et al., 2022) TransUNet (Chen et al., 2021) UCTransNet (Wang et al., 2022a) UNetV2 (Peng et al., 2025) UTNet (Gao et al., 2021) Deep Supervision Pre-training Zero-shot P-value Params (M) 42.61 35.27 158.44 2.60 171.34 104.16 26.76 54.79 29.62 52.94 12.81 33.63 34.14 17.53 2.32 28.07 1.37 147.68 31.56 6.21 75.07 111.08 47.91 43.54 6.29 22.65 26.17 105.59 27.07 93.23 66.24 25.13 14.41 FPS FLOPs (G) 69.89 7.95 57.91 8.15 30.28 71.17 67.48 6.92 24.28 51.15 53.85 54.00 56.17 5.60 16.73 8.36 76.01 42.80 25.70 40.88 14.61 21.58 55.26 32.25 17.75 68.12 27.24 102.91 4.89 105.04 59.85 5.15 18.69 13.44 96.80 11.23 41.80 35.25 59.66 84.41 99.76 59.97 42.59 85.84 58.45 35.12 60.33 76.67 21.79 2.41 33.28 10.08 10.43 57.72 124.48 44.63 16.68 4.86 88.78 11.53 39.28 24.06 32.23 43.06 5.40 20.49 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 7: Training modal information of all evaluation models. Modality CT Microscopy ISBI TMI Dermoscopy, MRI SPIE Medical Imaging Colonoscopy, MRI TMI Architecture Model AttU-Net AURA-Net CA-Net CaraNet CENet CE-Net CFPNet-M CMU-Net CMUNeXt CPCANet CSCA U-Net DANet DCSAU-Net DDS-UNet DoubleUNetPlus ERDUnet ESKNet G-CASCADE LFU-Net LV-UNet MALUNet MBSNet MCA-Unet MDSA-UNet MEGANet MMUNet MSLAU-Net MSRF-Net MultiResUNet PraNet ResNet34UnetPlus ResUNetPlusPlus RollingUnet SimpleUNet TA-Net TinyU-Net UACANet ULite U-Net UNet3+ UNeXt UTANet ResU-KAN U-KAN Mamba-UNet MedVKAN Swin-UMambaD UltraLight-VM-UNet VM-UNet VM-UNetV2 AC-MambaSeg CFM-UNet MUCM-Net Swin-UMamba Zig-RiR RWKV-UNet U-RWKV DC-ViT ColonSegT ConvFormer CSWin-UNet DAE-Former MISSFormer Polyp-PVT SwinUnet UNETR BEFUNet CASCADE CFFormer DA-TransUNet DS-TransUNet D-TrAttUnet EMCAD EViT-UNet FAT-Net FCNFormer GH-UNet H2Former HiFormer LeViT-UNet LGMSNet MedFormer MedT MERIT MFMSNet Mobile U-ViT MT-UNet Perspective-Unet ScribFormer SCUNet++ SwinUNETR TransAttUnet TransFuse TransNorm TransResUNet TransUNet UCTransNet UNetV2 UTNet CNN Mamba RWKV Transformer Hybrid JBHI Plos one TMI ISM Publication arXiv (cs.CV) JBHI Year 2018 MIDL 2021 2020 2022 2019 2019 Medical Imaging 2021 Medical Imaging ISBI 2023 2024 ISBI 2023 CMI 2024 AIIM 2024 2023 CBM IET Image Processing 2024 IEEE CBMS 2020 2024 TCSVT 2023 CMPB 2023 WACV 2023 CMI 2024 BIBM 2022 BIBM 2021 MSSP 2023 CMPBU 2025 2024 WACV 2024 BSPC 2025 2021 2020 Neural networks 2020 MICCAI 2018 2019 2024 AAAI 2025 arXiv 2022 WACV 2024 MICCAI 2021 ACM MM 2023 APSIPA 2015 MICCAI 2020 ICASSP 2022 MICCAI 2025 AAAI 2025 Applied Intelligence 2025 AAAI 2024 CoRR arxiv 2025 TMI 2024 2025 Patterns 2024 CoRR ISBRA 2024 ICGTSD 2024 Scientific Reports 2025 2024 CoRR 2024 MICCAI 2025 2025 CoRR 2025 MICCAI 2024 CVPR 2021 2023 MICCAI 2025 2023 2023 2021 2022 2022 WACV 2024 arXiv 2023 WACV ESA 2025 FBB 2024 2022 TIM 2024 CBM 2024 CVPR 2025 ISBI 2022 MIA 2022 MICCAI 2025 Digital Medicine 2023 2023 WACV PRCV 2023 ECAI 2025 2023 arXiv 2021 MICCAI 2023 MIDL 2023 UMB 2025 ACM MM 2022 ICASSP 2024 MICCAI 2024 2024 WACV 2021 MICCAI 2022 2021 MICCAI 2022 2022 CoRR 2021 arXiv 2022 AAAI 2025 ISBI 2021 MICCAI Information Fusion IWPIM TMI arXiv ECCVW IEEE ACCESS IEEE Access TMI TMI TIM TMI Fundus Image, CT, Microscopy, OCT Fundus, CT, Microscopy, OCT Thermography, Microscopy, Colonoscopy, Dermoscopy, Fundus Ultrasound Ultrasound MRI, Dermoscopy Colonoscopy, Pathology, Ultrasound Ultrasound Colonoscopy, Microscopy, Dermoscopy CT Colonoscopy, Dermoscopy, Microscopy Microscopy, Dermoscopy, Colonoscopy, Pathology, MRI Ultrasound CT, MRI, Dermoscopy, Colonoscopy CT, MRI Dermoscopy, Ultrasound, Colonoscopy Dermoscopy Dermoscopy, Ultrasound, Colonoscopy CT Ultrasound, CT, Dermoscopy Colonoscopy Histological image CT, MRI, Colonoscopy Colonoscopy, Microscopy, Dermoscopy Microscopy, Dermoscopy, Colonoscopy, MRI Colonoscopy Microscopy, CT, MRI Colonoscopy Ultrasound, Histological image, Dermoscopy, Fundus Ultrasound, Dermoscopy, Colonoscopy Histological image Dermoscopy, CT Colonoscopy Dermoscopy, Microscopy, Histological image Microscopy, Microscopy CT Dermoscopy, Ultrasound Histology Image, Microscopy, Abdominal CT, Dermoscopy Ultrasound, Histological, Colonoscopy Ultrasound, Histological image, Colonoscopy MRI, CT Microscopy, MRI, Ultrasound, CT MRI, Endoscopy, Microscopy Dermoscopy Dermoscopy, CT Dermoscopy, Colonoscopy Dermoscopy CT, MRI, Colonoscopy, MRI Dermoscopy MRI, Endoscopy, Microscopy Dermoscopy, CT, MRI, Microscopy CT, MRI, Ultrasound, Colonoscopy, Dermoscopy, Histological image Ultrasound, Colonoscopy, Dermoscopy, CT Natural images Colonoscopy Ultrasound, Dermoscopy, CT CT, MRI, Dermoscopy CT, Dermoscopy CT, MRI Colonoscopy CT, MRI CT, MRI CT, Microscopy, Dermoscopy CT, MRI, Colonoscopy Ultrasound, Dermoscopy, Colonoscopy, CT, MRI CT, Colonoscopy, X-ray, Dermoscopy, Endoscopy Colonoscopy, Dermoscopy, Histology, Microscopy CT, Histology Image, Microscopy Colonoscopy, Dermoscopy, Ultrasound, CT, MRI CT, Histology Image, Microscopy Dermoscopy Colonoscopy Dermoscopy, Colonoscopy, Fundus, MRI, CT Fundus, Colonoscopy, Dermoscopy, MRI, CT CT, Dermoscopy, Microscopy CT, MRI Ultrasound, Dermoscopy, Colonoscopy, CT MRI, CT Ultrasound, Histology Image, Microscopy CT, MRI Ultrasound Ultrasound, Dermoscopy, Colonoscopy, CT CT, MRI CT, MRI MRI, CT CT MRI Dermoscopy, X-ray, CT, Biological Image, Histology Image Colonoscopy, Dermoscopy, X-ray, MRI CT, Dermoscopy, Microscopy Colonoscopy Abdominal CT, MRI Histology Image, Microscopy, CT Dermoscopy, Colonoscopy MRI Github [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] link link [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] [link] 22 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 8: In-domain per-dataset 10th and 90th percentiles of IoU, Params, FLOPs, and FPS."
        },
        {
            "title": "Histopathology",
            "content": "X-Ray"
        },
        {
            "title": "Dataset",
            "content": "IoU (%) 90 ùëÑùëÉ 10 ùëÑùê¥ ùëÑùê¥ 0.58 0.71 0.39 BUSI 0.78 0.84 0.39 BUSBRA 0.66 0.78 0.39 TNSCUI 0.81 0.84 0.39 ISIC2018 0.79 0.85 0.39 SkinCancer 0.75 0.84 0.39 Kvasir 0.69 0.85 0.39 Robotool 0.47 0.81 0.39 CHASE 0.15 0.62 0.39 DRIVE 0.85 0.88 0.39 DSB2018 0.78 0.84 0.39 CellNuclear 0.63 0.83 0.39 Glas 0.53 0.67 0.39 Monusac Covidquex 0.63 0.70 0.39 Montgomery 0.92 0.96 0.39 0.51 0.63 0.39 0.73 0.85 0.39 0.78 0.87 0.39 0.55 0.72 0.39 0.63 0.83 0.39 Params (M) 10 ùëÑùëÉ 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4."
        },
        {
            "title": "DCA\nACDC\nPromise\nSynapse\nCystoidfluid",
            "content": "FLOPs (G) 10 ùëÑùê∫ 90 90 ùëÑùê∫"
        },
        {
            "title": "FPS",
            "content": "ùëÑùëÜ 10 ùëÑùëÜ 90 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 Table 9: Zero-shot per-dataset 10th and 90th percentiles of IoU, Params, FLOPs, and FPS."
        },
        {
            "title": "Source",
            "content": "BUSI BUSBRA TNSCUI ISIC2018 Kvasir Kvasir CHASE Monusac Montgomery"
        },
        {
            "title": "Target",
            "content": "BUS BUS TUCC PH2 CVC300 IoU (%) 90 ùëÑùëÉ 10 ùëÑùê¥ ùëÑùê¥ 0.60 0.81 0.39 0.78 0.85 0.39 0.56 0.64 0.39 0.82 0.85 0.39 0.61 0.80 0.39 CVC-ClinicDB 0.60 0.75 0.39 0.30 0.54 0.39 0.25 0.44 0.39 0.58 0.82 0.39 Params (M) 10 ùëÑùëÉ 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 4.32 STARE Tnbcnuclei NIH-test FLOPs (G) 10 ùëÑùê∫ 90 90 ùëÑùê∫"
        },
        {
            "title": "FPS",
            "content": "ùëÑùëÜ 10 ùëÑùëÜ 90 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 0.88 4.20 24.28 121.63 We categorize samples as small-scale if ùê¥ < 0.05 and large-scale otherwise. Shape complexity. We quantify the sharpness of the segmented foreground boundaries using composite U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking score ùëÜ derived from two standard geometric descriptors: circularity and solidity. We categorize samples with ùëÜ < 0.5 as irregular, and those with ùëÜ 0.5 as regular. The boundary sharpness score ùëÜ is defined as: ùëÜ = 0.5 Circularity + 0.5 Solidity. (5) Circularity measures how close the shape is to perfect circle. It is defined as: Circularity = 4ùúãùê¥ùëì ùëÉ 2 , where ùê¥ùëì is the foreground area and ùëÉ is the perimeter of the contour. Solidity evaluates the extent to which shape fills its convex hull. It is given by: Solidity = ùê¥ùëì , where ùê¥ùëì is the foreground area and ùê¥ùëê is the area of its ùê¥ùëê convex hull. Boundary Sharpness. We assess boundary sharpness using two complementary measures: boundary width and boundary contrast. Given binary mask ùëö, we first construct narrow boundary ring by applying morphological dilation and erosion. The boundary width is then computed as the ratio between the area of this ring and the contour perimeter: ùë§ = Area(Ring) , where ùëÉ denotes the sum of contour perimeters. larger ùë§ indicates blurrier boundaries, while smaller ùë§ corresponds to sharper edges. To evaluate intensity separation across the boundary, we form two narrow bands: one inside the mask and one outside, each of width ùë° pixels. Let (ùúáùëñùëõ, ùúéùëñùëõ) and (ùúáùëúùë¢ùë°, ùúéùëúùë¢ùë°) denote the mean and standard deviation of pixel intensities inside and outside the boundary band. The boundary contrast is defined as CNR = ùúáùëñùëõùúáùëúùë¢ùë° . To obtain ùúéùëñùëõ+ùúéùëúùë¢ùë°+ùúñ unified measure of boundary clarity, we normalize both ùë§ and CNR to [0, 1] across the dataset. composite blur score is then computed as: ùëÉ +ùúñ ùêµ = ùë§ùëõùëúùëüùëö ùë§ùëõùëúùëüùëö + ùëêùëõùëúùëüùëö + ùúñ , (6) where ùë§ùëõùëúùëüùëö and ùëêùëõùëúùëüùëö are the normalized boundary width and contrast, respectively. We categorize samples with ùëè < 0.6 as clear, and those with ùëè 0.6 as blur. F.2. Model Advisor Agent settings We construct comprehensive feature space that integrates both continuous and discretized descriptors from models and datasets. For model-level attributes, we discretize storage (parameter) into four scales (Tiny: 0-10M, Small: 10-50M, Medium: 50-200M, Large: >200M), computation cost (FLOPs) into three levels (Low: 0-10 GFLOPs, Medium: 10-100 GFLOPs, High: >100 GFLOPs), and inference speed (FPS) into three categories (Slow: <15 FPS, Medium: 15-60 FPS, Fast: >60 FPS). On the data characteristics side, we discretize foreground-related properties:foreground scale (< 0.05 vs. 0.05, denoting small vs. large targets), shape complexity (< 0.5 vs. 0.5, irregular vs. regular), and boundary sharpness (< 0.6 vs. 0.6, clear vs. blurry). We train an XGBRanker with the rank:pairwise objective on 18 in-domain datasets, reserving 2 datasets (BUSI and SkinCancer) for testing. Scores are normalized into relevance values within each dataset, with higher relevance indicating better relative performance. Dataset-level grouping is used to enforce within-dataset ranking consistency during training. At evaluation, the ranker outputs predicted scores, which are converted into ranked lists for each dataset. Performance is assessed using NDCG@50/20 for ranking quality, MAP for precision under binary relevance, and correlation metrics (Spearman) to quantify alignment between predicted and ground-truth orderings. Finally, the agent exports the recommended models per test dataset, providing practical reference list for downstream selection. F.2.1. Data Preprocessing Experimental Data Split. For all experiments on the unified dataset, we used the same train-test split. For data without clear train-test split in the dataset source, we adopted random split; for data with known 24 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking train-test split in the dataset source, we followed the original split. The division method of each dataset is shown in Tab. 4, where denotes our self-defined division and denotes the division consistent with the referenced data source. F.2.2. Retraining Details The experiments are utilizing eight NVIDIA H20 GPUs. total of 100 models are trained with 6000 hours. The implementation was built on Python 3.9.0 and PyTorch 2.7.0. The model structure files are primarily obtained from the open-source code of the original models, with only minor modifications (e.g. input and output channels) to some input parameters to adapt to our framework. Following prior works (Valanarasu and Patel, 2022, Tang et al., 2023, 2024, Chen et al., 2024a, Tang et al., 2025b, Ye et al., 2025, Jiang et al., 2025, Dong et al., 2025), we rescale all images to resolution of 256 256 by default and apply standard data augmentation. For models that require fixed input size (e.g., Swin Transformer variants designed for 224 224 inputs), we preserve their original settings without rescaling. rotations, random horizontal and vertical flips, and normalization. To Augmentations include random 90 ensure fair comparisons, the same preprocessing pipeline is applied consistently across all experiments. Notably, for models that adopt deep supervision, we retain their original training strategy to enable accurate performance evaluation. Table 10: Hyperparameters in U-Bench Optimizer SGD (Momentum=0.9, Weight Decay=0.0001) Learning Rate Epochs Random Seed Batch Size 0.01 300 41 8 F.2.3. Hyper-Parameters in U-Bench Following previous work (Dong et al., 2025, Tang et al., 2024, 2025b, Chen et al., 2021, Wang et al., 2024, Ye et al., 2025), we unify training settings across all models to ensure fair comparisons, as summarized in Tab. 10. Moreover, we use commonly adopted loss configurations (Dong et al., 2025, Tang et al., 2024, 2025b, Ye et al., 2025) to promote generalizable results and enable more equitable performance evaluation. Specifically, for the ground truth ùë¶ and the predicted output ÀÜùë¶, the loss function is defined as: ‚Ñí = 0.5 ùêµùê∂ùê∏( ùë¶, ùë¶) + ùê∑ùëñùëêùëí( ùë¶, ùë¶), where BCE denotes the binary cross-entropy loss and Dice denotes the Dice loss. Note that for 3D data ACDC and Synapse, we follow CASCADE Rahman and Marculescu (2023a) weights 0.5 ùêµùê∂ùê∏( ùë¶, ùë¶), 0.7 ùê∑ùëñùëêùëí( ùë¶, ùë¶) (7) F.3. Metrics Intersection over Union (IoU) IoU quantifies the overlap between two regions (predicted ùê¥ and ground-truth ùêµ) as: IoU(ùëå , ùëå ) = ùëå ùëå ùëå ùëå (8) , where ùëå ùëå is the area of intersection, and ùëå ùëå is the area of union. U-Score. To address the limitation that existing evaluations primarily focus on segmentation performance while failing to balance key computational factors such as model size, inference cost, and speed√¢ƒÇ≈§making it 25 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking difficult to assess practical deployment capabilities√¢ƒÇ≈§we construct the U-Score based on quantile statistics. detailed description is provided in Appendix E. Normalized Discounted Cumulative Gain (NDCG) NDCG evaluates the \"usefulness\" of ranked list by accounting for two key factors: (1) the relevance of each item, and (2) the position of relevant items (penalizing lower-ranked relevant items via discounting). It is normalized to range of [0, 1] to enable cross-task comparisons. First, the Discounted Cumulative Gain (DCG) is defined to measure the cumulative relevance of ranked list up to position ùëò (denoted as DCG@ùëò): DCG@ùëò = ùëò ùëñ=1 relùëñ log2(ùëñ + 1) (9) where: ùëò: The cutoff position (e.g., ùëò = 10 for NDCG@10, focusing on top-10 results). relùëñ: The relevance score of the ùëñ-th item in the ranked list. For binary relevance (relevant/irrelevant). log2(ùëñ + 1): The discount factor, which reduces the contribution of items ranked later (since users are less likely to inspect lower positions). To normalize DCG across different queries/tasks (where the maximum possible relevance varies), the Ideal DCG (IDCG)√¢ƒÇ≈§the maximum possible DCG@k for given set of items√¢ƒÇ≈§is computed by ranking all relevant items in descending order of relùëñ: IDCG@ùëò = min(ùëò,ùëÖ) ùëñ=1 rel ùëñ log2(ùëñ + 1) (10) where: ùëÖ: The set of all relevant items for the query/task. ùëÖ: The total number of relevant items. rel ùëñ: The ùëñ-th highest relevance score among all items in ùëÖ (i.e., the ideal ranking). NDCG@k is defined as the ratio of DCG@k to IDCG@k. To avoid division by zero (when no relevant items exist, IDCG@ùëò = 0), NDCG@k is set to 0 in this edge case: NDCG@ùëò = { 0 DCG@ùëò IDCG@ùëò if IDCG@ùëò = 0, otherwise. (11) For experiments with multiple queries/tasks (e.g., retrieval dataset with 1k queries), the mean NDCG@k√¢ƒÇ≈§the average of NDCG@k across all queries√¢ƒÇ≈§is reported. In Table 3, NDCG@k values for ùëò = 5 and ùëò = 20 are provided. Mean Average Precision (MAP) MAP quantifies the average precision of relevant items in ranked list, aggregated across all queries/tasks. It is particularly useful for scenarios where \"early relevant items\" (high precision at top positions) are critical (e.g., information retrieval, recommendation systems). U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking First, Average Precision (AP) for single query ùëû is defined as the average of the precision of the ranked list at the position of each relevant item: AP(ùëû) = 1 ùëÖùëû ùëüùëÖùëû Prec(ùëòùëü) (12) where: ùëû: single query (or task instance) from the query set ùëÑ; ùëÖùëû: The set of all relevant items for query ùëû (if ùëÖùëû = 0, AP(ùëû) = 0 by convention); ùëòùëü: The position of relevant item ùëü in the ranked list for ùëû; Prec(ùëòùëü): The precision at position ùëòùëü, defined as Prec(ùëòùëü) = numRel(ùëòùëü) of relevant items in the top-ùëòùëü positions. , where numRel(ùëòùëü) is the number ùëòùëü For set of ùëÑ queries, MAP is the average of AP scores across all queries: MAP = 1 ùëÑ ùëûùëÑ AP(ùëû) (13) Similar to NDCG, MAP ranges from [0, 1]: value of 1 indicates all relevant items are ranked first (perfect precision at every relevant position), while 0 indicates no relevant items are retrieved. Spearmans Rank Correlation Coefficient Spearmans rank correlation coefficient quantifies the monotonic relationship between two ranked variables. It is particularly useful for evaluating how well the order of items (e.g., predicted rankings by model and ground-truth rankings) aligns, making it relevant for tasks where the consistency of relative ordering matters (e.g., comparing ranked recommendations or human judgments). Formally, Spearmans rank correlation coefficient ùúå between two variables ùëã (e.g., model-generated ranks) and ùëå (e.g., ground-truth ranks) (each with ùëõ paired observations) is defined as: where: ùúå = 1 6 ùëõ ùëñ=1 ùëë2 ùëñ ùëõ(ùëõ2 1) ùëëùëñ: The difference between the rank of ùëãùëñ and the rank of ùëåùëñ (i.e., ùëëùëñ = rank(ùëãùëñ) rank(ùëåùëñ)). ùëõ: The total number of paired observations. (14) 27 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 11: Top-10 performing variants across each dataset on source domains. Baseline U-Net is highlighted (gray background), and statistical significance of p-value is highlighted: p<0.0001 , p<0.001 , p<0.05 , p0.05 , and > 0.05 (Not significant) Ultrasound Endoscopy Dermoscopy Rank RWKV-UNet PraNet Mobile U-ViT BUSI 72.32 RWKV-UNet EViT-UNet 71.63 CaraNet 71.59 #4 DA-TransUNet 71.47 MFMSNet #5 #6 TransResUNet #7 #8 #9 #10 TA-Net 71.47 FAT-Net 71.27 UACANet 71.24 70.91 FCBFormer 70.88 MEGANet 70.81 AURA-Net 65.58 U-Net (#41) MFMSNet CFFormer ESKNet CASCADE U-Net (#68) MEGANet BUSBRA 84.76 84.36 84.31 84.27 84.17 84.15 84.07 84.00 83.99 83.95 82.91 RWKV-UNet MEGANet MFMSNet TA-Net UACANet EViT-UNet CaraNet Swin-umamba FAT-Net UTANet U-Net (#58) TNSCUI 80.06 79.01 78.91 78.85 78.83 78.75 78.69 78.64 78.57 78.51 75.99 Swin-umamba VMUNet UACANet CFFormer RWKV-UNet FCBFormer PraNet CASCADE CENet MFMSNet U-Net (#70) MEGANet RWKV-UNet AURA-Net TA-Net EViT-UNet Kvasir 85.56 84.90 84.81 84.57 84.53 84.50 TransResUNet 84.39 84.34 84.32 84.32 80.11 MFMSNet CE-Net PraNet UACANet U-Net (#23) Robotool 86.25 85.95 85.78 85.62 85.39 85.30 85.21 85.11 85.10 84.65 81. ISIC2018 RWKV-UNet CFFormer MEGANet SkinCancer 84.97 87.48 RWKV-UNet 84.89 DA-TransUNet 87.22 86.80 84.50 MSLAU-Net 86.38 86.33 86.20 86.17 86.01 85.86 85.56 80.94 PraNet FCBFormer EMCAD MCA-UNet CaraNet TransNorm AURA-Net U-Net (#77) Swin-umamba 84.49 84.42 84.38 84.34 84.32 84.26 84.25 82.78 PraNet TransResUNet TA-Net CE-Net CaraNet AURA-Net U-Net (#61) Rank AURA-Net RWKV-UNet CaraNet EViT-UNet TA-Net MEGANet PraNet CE-Net #4 #5 #6 #7 #8 #9 TransResUNet #10 MFMSNet U-Net (#31) Covidquex 70.85 70.75 70.61 70.33 70.20 70.19 70.09 70.04 69.81 69.77 68.52 X-Ray RWKV-UNet DA-TransUNet MEGANet TransAttUnet RollingUnet DDANet MT-UNet TransResUNet Mobile U-ViT UNet3plus U-Net (#11) Montgomery 96.21 96.17 96.12 96.03 96.01 95.97 95.90 95.89 95.89 95.88 95.87 DCA DA-TransUNet 64.90 64.23 63.81 63.81 63.77 63.69 63.65 63.61 63.54 63.30 UTANet EViT-UNet MFMSNet MEGANet ESKNet DDANet RWKV-UNet UTNet U-Net MRI Promise 87.56 87.29 87.26 87.05 87.03 87.00 86.95 86.89 86.88 86.87 86. RWKV-UNet FCBFormer MFMSNet EViT-UNet Perspective-Unet MEGANet TransResUNet U-KAN PraNet CMU-Net U-Net (#29) Nuclear Fundus CENet ACDC 85.54 Swin-umambaD 85.45 85.33 85.20 85.11 85.01 84.91 84.90 84.89 84.78 84.32 DoubleUNet RWKV-UNet DDANet AttU-Net EViT-UNet FCBFormer G-CASCADE MSRFNet U-Net (#23) CMU-Net AttU-Net U-Net UNet3plus Perspective-Unet UCTransNet ESKNet ColonSegNet MT-UNet Swin-umamba CHASE 84.33 84.20 84.07 83.69 82.86 82.82 82.69 82.20 82.00 81.65 DRIVE 64.25 63.21 63.19 63.17 63.15 62.85 62.75 62.54 62.49 62.48 61.81 FCBFormer MT-UNet ColonSegNet UTNet ESKNet CMU-Net Swin-umamba UNet3plus RollingUnet D-TrAttUnet U-Net (#14) OCT Rank EMCAD Histopathology Glas 85.85 RWKV-UNet 85.75 85.17 CASCADE 84.38 #4 MSLAU-Net 84.22 #5 83.78 TransAttUnet #6 UTNet 83.77 #7 83.57 #8 EViT-UNet 83.47 D-TrAttUnet #9 83.30 #10 MT-UNet RWKV-UNet UTANet CA-Net DDANet UTANet DDANet MERIT MBSNet CENet U-Net AttU-Net U-Net (#21) Monusac 69.27 68.96 68.39 68.39 68.38 68.25 67.76 67.61 66.96 66.96 66. MT-UNet DoubleUNet TransAttUnet DCSAU-Net UTNet D-TrAttUnet ESKNet AURA-Net LGMSNet DDANet U-Net (#16) DSB2018 88.74 88.61 88.49 88.44 88.39 88.27 88.23 88.23 88.16 88.16 88.05 MT-UNet TransAttUnet AURA-Net CA-Net UTANet ColonSegNet DA-TransUNet RollingUnet RWKV-UNet FCBFormer U-Net (#17) CellNuclear 84.93 84.88 84.87 84.85 84.77 84.70 84.63 84.62 84.56 84.54 84.33 CT CENet Perspective-Unet G-CASCADE CASCADE AURA-Net MEGANet DS-TransUNet DoubleUNet MSLAU-Net RWKV-UNet U-Net (#52) Synapse 74.70 73.69 73.54 73.30 73.25 73.18 72.74 72.63 72.60 72.56 67.90 UNet3plus Swin-umamba UTANet MMUNet H2Former Perspective-Unet FCBFormer D-TrAttUnet MedFormer EViT-UNet U-Net (#23) Cystoidfluid 85.76 85.06 84.89 84.21 83.99 83.90 83.84 83.74 83.58 83.54 82.39 G. Additional Results G.1. Per-Dataset Top-10 and U-Net Comparison We report the top-10 performing methods across each dataset, evaluated on both source and target domains. As shown in Tab. 11 and Tab. 12. For reference, the position of the vanilla U-Net is highlighted with gray background, and we also compute the statistical significance of each variant relative to U-Net. Top10 performance on Source domain. On widely studied datasets and modalities-such as ultrasound, polyp segmentation, ISIC2018 (Dermoscopy), Synapse (CT), Drive (Fundus), ACDC (MRI), and Covidquex (X-ray)-most top-10 variants achieve significant improvements over U-Net. This trend is consistent with the increasing popularity of these datasets and novelity desion for long-range dependency modeling, such as incorporating Transformers, Mamba, RWKV, and hybrid designs. In contrast, on other datasets and modalities the improvements remain marginal. For example, in Montgomery (X-ray lung segmentation), DCA, Chase (Fundus), nuclear segmentation, and Histopathology, the relative gains over U-Net are not significant. This suggests that progress in these modalities has been limited, because they rely on stable local patterns rather than long-range context. These observations highlight an important direction for future research: designing models that are modality-aware, particularly tailored for domains dominated by local 28 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 12: Top-10 performing variants across each dataset on target domains. Baseline U-Net is highlighted (gray background), and statistical significance of p-value is highlighted: p<0.0001 , p<0.001 , p<0.05 , p0.05 , and > 0.05 (Not significant) Rank TNSCUI TUCC Ultrasound (Source Target) BUSBRA BUS BUSI BUS 86.62 MSLAU-Net MEGANet Swin-umamba 82.91 66.15 86.51 DoubleUNet 82.83 MERIT 66.00 86.29 CENet 82.70 EViT-UNet 65.83 86.10 FCBFormer 82.61 Polyp-PVT 65.23 85.96 CASCADE 82.32 LGMSNet 65.16 85.65 Polyp-PVT 82.13 G-CASCADE 65.10 82.11 85.62 TransResUNet CaraNet 64.61 82.11 ResNet34UnetPlus 85.46 H2Former 64.48 64.37 MEGANet 85.28 MCA-UNet 81.88 85.27 Swin-umamba 64.00 G-CASCADE 81.32 60.50 81.37 U-Net (# 65) U-Net (# 79) 72.44 EMCAD CENet #4 G-CASCADE #5 DA-TransUNet #6 #7 #8 #9 #10 PraNet CASCADE TransNorm MCA-UNet CaraNet U-Net (#63) Rank Endoscopy (Source Target) Kvasir CVC300 PraNet RWKV-UNet UACANet MERIT MFMSNet TA-Net EViT-UNet UTANet Kvasir CVC-ClinicDB 77.39 PraNet 83.31 77.38 DS-TransUNet 82.14 77.19 CASCADE 81.72 81.39 Swin-umambaD 76.83 #4 76.56 EMCAD 81.24 #5 #6 76.42 TransResUNet 81.24 76.33 MFMSNet 81.11 #7 76.18 CFFormer #8 80.78 75.72 DoubleUNet #9 DS-TransUNet 80.58 75.64 MEGANet 80.57 69.87 U-Net (#47) 70.33 CASCADE U-Net (#75) #10 Rank Dermoscopy ISIC2018 PH2 Rank MSLAU-Net RWKV-UNet G-CASCADE MERIT #4 MMUNet #5 H2Former #6 #7 CMUNeXt UACANet #8 #9 MFMSNet #10 MCA-UNet 86.52 86.00 85.96 85.94 85.66 85.39 85.32 85.12 85.08 85.08 U-Net (#47) 84.00 and repetitive structures. Fundus CHASE DRIVE MSRFNet 55.60 DS-TransUNet 55.52 54.66 MBSNet 54.27 RWKV-UNet 53.78 CENet 52.80 CSCAUNet 52.69 MCA-UNet 52.49 EViT-UNet 52.26 Tinyunet #10 TransResUNet 52.00 39.64 U-Net (#57) #4 #5 #6 #7 #8 #9 Rank #4 #5 #6 #7 #8 #9 #10 MEGANet TransResUNet CaraNet DA-TransUNet PraNet MFMSNet X-Ray Montgomery NIH-test 88.19 87.69 86.22 85.87 84.58 83.67 Swin-umambaD 83.13 83.03 82.90 82.41 71. TransUnet TransNorm RWKV-UNet U-Net (#51) Rank Histopathology Monusac Tnbcnuclei TA-Net CENet 50.74 48.03 ResNet34UnetPlus 46.44 46.41 46.18 45.32 45.18 45.16 44.53 44.42 26.05 EMCAD G-CASCADE UNetV2 CSWin-UNet DAEFormer DA-TransUNet MedVKAN U-Net (#91) #4 #5 #6 #7 #8 #9 #10 Top10 performance on target domain. On the target-domain datasets, nearly all top-10 methods achieve substantial improvements, highlighting the superior generalization ability of recent variants. These gains are primarily driven by two factors: the adoption of long-range dependency modeling and the increased model complexity. Together, these characteristics enhance the representational capacity and adaptability of the variants, which aligns with the prevailing trend toward more novel and increasingly complex model architectures. In addition, we provide the visualization results of the top 5 models and U-net of the dataset for visualization analysis. The results are shown in Fig. 12 and 13. 29 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking H. Incorporate New Datasets and Algorithms in U-Stone We implement U-Stone using the PyTorch framework. Figure 10 illustrates the comprehensive workflow of U-Stone, system tailored for medical image analysis. It features versatile 2D/3D Dataloader that seamlessly accommodates multiple medical imaging modalities, including MRI, CT, X-Ray, Dermoscopy, and Fundus and so on. rich assortment of models with diverse architectural designs-spanning CNN, Transformer, RWKV, Mamba, and Hybrid-are registered via Model JSON configuration and then leveraged by the Trainer module for 2D/3D slice-wise training. The evaluation pipeline encompasses in-domain testing, zero-shot inference, statistical significance assessments and custom assessments of U-score. Finally, results are systematically logged and visualized using tools such as Weight & Biases (wandb), ensuring thorough tracking of metrics and checkpoints. We demonstrate how to integrate new datasets and algorithms through example pseudocode Figure 11. H.1. Adding New Dataset If the existing Dataset classes cannot meet your processing requirements, you can implement your own dataset with the structure shown as in Figure 11 (a). Additionally, you need to add your dataset name and loading method in the Dataloader file, as shown in the Figure 11 (b). H.2. Adding New Algorithm 1. First, define your model in the models directory, ensuring the first two parameters are input_channel and num_classes to adapt to our project (as shown in Figure 11 (c)). 2. Then, properly import your \"modelname\" in the __init__.py file under the models directory. 3. Finally, register your model in the model_id.json file with the format shown in Figure 11 (d). Note that modelname, id, and deep_supervision are required fields, and modelname serves as the unique identifier for the model. Figure 10: Overall workflow of U-Stone. The Dataloader supports multiple medical imaging modalities (e.g., MRI, CT, X-Ray, Dermoscopy, Fundus). Models (with diverse architectures like CNN, Transformer, RWKV, Mamba, Hybrid) are registered and used by the Trainer for 2D/3D slice training. Evaluation covers In-domain/Zero-shot tasks, with results logged via tools like wandb. 30 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Figure 11: Pseudocode display. (a) Pseudocode for datasets; (b) Pseudocode for data loading; (c) Pseudocode for model definition and input parameters; (d) Example of model registration using JSON file 31 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 13: Average performance of 100 u-shape medical image segmentation networks with IoU. Baseline U-Net is highlighted (gray background), and statistical significance of p-value is highlighted: p<0.0001 , p<0.001 , p<0.05 , p0.05 , and > 0.05 (Not significant) Rank Network RWKV-UNet UTANet AURA-Net Swin-umamba TransResUNet FCBFormer MEGANet DA-TransUNet ESKNet CFFormer EViT-UNet CMU-Net DDANet MFMSNet Perspective-Unet AttU-Net UNet3+ UTNet Mobile U-ViT U-Net MT-UNet RollingUnet UCTransNet MBSNet TransUnet TransAttUnet LGMSNet CENet H2Former CA-Net CMUNeXt D-TrAttUnet MedFormer FAT-Net MCA-UNet MSLAU-Net U-Net++ TA-Net ResU-KAN U-KAN MSRFNet ColonSegNet GH-UNet CE-Net ScribFormer DS-TransUNet DCSAU-Net DDS-UNet TransNorm MedVKAN DoubleUNet AC-MambaSeg MMUNet U-RWKV HiFormer ResUNet++ CASCADE CSCAUNet Tinyunet LV-UNet G-CASCADE DC-UNet ERDUnet ConvFormer ULite SwinUNETR SCUNet++ UNeXt MERIT MDSA-UNet CPCANet LeViT-UNet DAEFormer PraNet CaraNet TransFuse MedT EMCAD UACANet MultiResUNet MUCM-Net LFU-Net MissFormer UNETR CFM-UNet Zig-RiR SimpleUNet CFPNet-M BEFUnet UNetV2 Swin-umambaD VMUNetV2 VMUNet #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 UltraLight-VM-UNet #95 #96 #97 #98 #99 #100 Polyp-PVT H-vmunet CSWin-UNet MALUNet SwinUnet MambaUnet Ultrasound BUSBRA TNSCUI 84.76 83.93 83.95 83.48 83.86 84.00 83.99 83.91 83.77 82.42 84.36 83.39 83.36 84.27 83.53 83.13 82.92 83.09 83.64 82.91 81.80 83.44 82.51 83.60 82.84 83.28 83.22 82.83 83.41 82.87 83.63 82.03 82.43 84.15 83.78 82.69 82.10 84.17 83.01 82.81 83.16 78.47 82.86 83.74 82.02 81.29 83.21 82.74 83.14 83.92 81.79 82.05 83.15 81.95 83.53 80.43 82.42 82.79 82.01 82.08 82.21 80.30 82.01 82.41 80.70 78.55 80.70 80.42 83.18 81.63 81.01 73.72 79.42 83.90 84.31 83.21 79.80 82.37 84.07 82.71 79.47 77.72 79.26 71.40 78.27 75.18 75.77 81.46 77.66 78.20 80.15 80.54 45.08 72.38 82.16 77.23 78.28 74.33 69.94 80. 80.06 78.51 78.32 78.64 78.04 78.10 79.01 77.95 78.29 78.44 78.75 77.28 77.53 78.91 78.20 76.30 75.61 77.42 78.12 75.99 72.66 76.17 75.42 76.78 77.61 77.65 77.71 77.34 77.84 77.34 77.18 75.21 77.81 78.57 78.26 78.18 76.87 78.85 77.39 77.15 77.30 71.03 76.69 78.21 76.55 73.09 78.00 76.19 77.72 77.26 74.95 76.17 77.60 72.70 77.71 73.01 77.49 77.52 74.71 75.39 77.21 73.10 76.04 75.96 70.75 70.31 73.90 71.13 77.46 76.44 72.75 66.10 73.07 78.40 78.69 77.79 70.13 70.95 78.83 75.20 68.59 62.52 72.78 48.34 72.51 66.18 62.93 75.09 65.36 70.15 75.19 74.85 75.31 59.18 76.68 66.49 69.71 68.27 27.35 50.64 BUSI 72.32 69.00 70.63 70.04 71.27 68.24 71.47 71.47 70.88 70.91 70.50 69.25 68.33 71.24 67.00 65.74 65.06 69.60 71.59 65.58 67.01 67.68 68.01 68.82 69.62 68.46 69.45 69.16 69.80 68.96 68.68 65.38 67.08 70.17 69.49 68.69 66.44 69.89 67.38 67.10 65.81 62.77 66.20 69.99 66.37 63.91 67.37 67.63 69.43 66.71 66.56 65.96 67.40 65.72 67.28 62.36 70.81 67.19 61.96 66.35 69.81 61.31 66.72 67.45 63.97 62.50 61.48 62.11 69.24 67.63 66.01 58.38 63.39 71.63 70.61 69.77 60.85 68.56 69.88 65.58 61.25 57.58 62.20 52.28 58.88 54.17 55.08 65.22 61.91 58.12 64.94 61.69 67.38 57.58 70.54 61.83 56.38 59.10 49.81 19.01 Dermoscopy Endoscopy Fundus Histopathology ISIC2018 SkinCancer Kvasir Robotool CHASE DRIVE DSB2018 84.97 83.84 84.25 84.49 84.38 83.60 84.50 83.95 83.34 84.89 84.03 82.74 83.20 84.18 82.78 82.60 82.80 83.56 83.88 82.78 82.84 83.07 83.10 83.09 83.51 83.04 83.68 83.89 83.31 82.87 82.80 82.56 83.09 83.91 83.72 83.56 82.59 84.34 83.36 82.73 82.86 82.06 82.55 84.32 83.13 83.30 83.47 83.67 83.57 83.01 81.07 83.17 83.13 82.68 83.96 82.30 83.53 82.79 82.12 83.53 83.52 80.97 82.48 82.86 82.26 82.70 81.66 82.41 83.85 83.02 82.13 81.50 82.17 84.42 84.26 84.11 82.12 84.01 83.76 82.47 81.99 80.51 81.89 80.87 81.81 81.75 79.52 82.91 79.88 81.49 81.93 82.91 83.01 80.45 83.33 81.66 81.67 80.99 81.18 79.22 87.48 84.48 85.56 83.92 84.79 86.33 85.23 87.22 81.93 83.87 82.90 81.90 80.91 84.67 84.29 81.74 81.85 82.08 83.64 80.94 84.41 81.00 82.77 82.86 84.85 79.11 85.22 84.13 83.02 82.48 85.39 83.82 82.16 84.11 86.17 86.80 83.16 83.32 82.12 84.94 80.87 81.92 84.98 83.64 80.01 83.74 81.47 83.58 85.86 82.87 79.15 83.31 80.89 80.76 84.90 82.58 85.19 82.24 81.44 83.55 84.46 78.43 79.44 82.15 82.79 81.35 82.49 81.00 84.32 81.59 83.38 81.77 80.93 86.38 86.01 82.86 76.65 86.20 83.88 82.21 82.57 75.02 76.54 80.45 80.64 79.06 77.94 81.64 82.59 81.37 70.71 78.35 83.08 80.96 84.71 79.14 81.18 77.25 79.64 42.31 84.53 83.06 83.53 85.56 83.38 84.50 84.29 82.20 82.11 84.57 83.94 83.07 83.06 84.32 82.70 79.23 78.83 80.92 83.21 80.11 80.08 81.50 80.34 81.44 80.93 81.53 81.77 84.32 81.88 80.58 80.21 80.78 82.06 83.39 82.25 83.83 81.83 83.33 82.70 82.11 80.60 79.81 81.36 82.11 79.33 84.02 81.07 81.86 81.29 81.93 84.09 81.48 82.79 78.28 83.36 78.36 84.34 82.25 77.22 81.88 83.56 77.01 76.04 80.25 76.05 80.02 82.91 75.64 83.40 79.28 81.49 75.36 80.99 84.39 83.24 83.01 74.78 84.20 84.81 80.57 74.14 63.84 80.23 72.01 76.53 74.98 69.60 79.22 76.88 79.25 84.27 84.03 84.90 63.17 83.95 74.22 75.09 72.56 67.11 84.28 85.95 84.62 85.78 84.04 85.30 77.58 86.25 83.09 81.36 83.80 85.39 80.25 80.35 85.21 79.77 81.04 81.13 79.66 80.31 81.24 80.82 80.34 81.23 78.25 78.30 80.01 78.45 80.07 78.40 77.61 77.83 78.93 77.61 84.34 79.97 82.93 79.08 85.62 77.54 77.77 78.32 77.87 77.58 85.11 75.38 82.80 77.78 78.22 79.36 78.33 75.65 75.51 76.90 75.33 82.95 75.13 80.35 78.05 75.09 80.21 78.54 76.19 73.28 79.03 73.45 71.71 72.35 75.64 83.91 77.47 73.42 76.04 73.48 85.10 84.53 82.69 70.27 77.52 84.65 77.38 66.91 69.14 70.40 62.20 69.41 71.85 73.54 77.20 69.54 70.62 72.77 61.46 76.42 60.58 80.64 62.31 70.03 56.44 67.26 60.88 70.85 79.81 80.08 81.65 81.01 81.17 73.27 78.65 82.69 76.61 74.06 84.33 78.94 71.18 82.86 84.20 83.69 80.83 77.58 84.07 82.00 78.51 82.82 77.57 79.87 75.63 77.64 70.80 74.94 74.01 78.25 79.19 79.90 75.39 75.56 70.05 79.72 71.68 76.12 75.80 69.36 82.20 70.27 70.23 74.34 72.97 70.08 74.09 76.98 67.95 78.51 78.73 76.15 77.13 66.14 79.93 59.33 70.20 70.74 64.48 55.59 68.10 65.86 62.90 65.37 70.20 75.17 65.51 48.57 53.47 70.88 71.19 69.98 36.43 37.69 49.62 63.13 58.61 36.30 30.58 57.12 59.19 60.91 65.47 47.18 56.12 62.60 67.89 51.04 40.72 50.01 46.22 63.54 50.32 35.87 46.58 27.08 11.87 49.19 53. 59.75 60.86 58.22 62.75 58.85 64.25 52.76 58.05 63.15 60.05 58.95 62.85 61.40 54.35 60.47 62.27 62.54 63.17 60.69 61.81 63.21 62.49 60.38 61.61 59.76 62.45 60.05 58.38 57.93 61.97 60.99 62.48 61.77 50.01 58.90 49.20 58.40 50.29 57.84 57.71 57.27 63.19 57.92 49.23 60.58 55.46 60.31 54.06 57.25 57.50 60.29 54.28 58.40 59.50 47.23 61.55 33.42 28.59 59.07 46.42 36.32 58.03 55.23 37.90 58.10 61.53 34.54 46.52 40.41 37.81 20.77 44.36 28.50 26.97 26.30 18.52 54.62 44.65 25.48 19.22 41.63 55.87 19.32 61.05 14.58 35.55 54.68 58.15 15.83 15.17 14.24 13.02 16.37 14.64 1.28 10.85 14.59 15.08 10.66 11.99 88.10 87.70 88.23 87.81 87.09 88.08 87.33 87.48 88.23 87.57 87.32 87.87 88.16 86.92 87.29 87.94 87.99 88.39 87.67 88.05 88.74 88.14 87.72 87.78 87.26 88.49 88.16 88.06 86.58 88.12 87.62 88.27 87.64 86.07 87.52 85.98 87.11 86.95 86.86 86.68 88.01 88.01 86.01 86.35 87.93 87.82 88.44 86.38 86.53 87.20 88.61 86.25 86.65 87.98 85.69 87.15 86.20 86.87 86.81 86.56 86.90 86.36 87.43 84.32 87.45 87.18 85.46 86.29 82.49 85.63 85.48 85.95 86.82 78.92 79.04 85.15 87.18 86.97 78.16 84.69 84.87 86.71 85.73 87.52 84.85 81.71 85.98 87.03 84.46 84.52 87.55 82.92 85.88 85.99 81.59 85.58 85.60 84.85 85.52 86.53 Glas Monusac 85.75 84.22 82.77 79.06 82.35 79.81 82.92 82.89 82.48 82.17 80.53 81.88 83.78 83.03 81.53 83.06 82.79 80.42 79.05 83.30 80.31 82.54 81.39 83.57 79.33 81.04 81.92 83.47 82.16 81.63 82.12 79.38 77.93 82.81 79.85 84.38 80.43 81.60 79.44 79.75 81.12 80.56 81.91 81.20 77.57 81.46 77.00 79.88 76.72 78.18 77.13 75.39 73.61 78.14 79.74 76.16 85.17 81.58 79.47 78.69 82.70 81.13 72.75 81.02 74.47 70.26 79.68 76.30 83.77 78.23 69.88 72.78 71.62 82.65 83.09 75.03 70.33 85.85 79.86 80.93 73.10 74.73 65.47 61.49 70.27 68.28 77.27 79.05 60.78 67.10 51.22 79.07 52.81 60.37 51.22 67.11 63.12 61.99 57.82 52.52 68.96 68.39 66.56 64.12 66.86 66.29 65.86 64.26 66.92 64.51 67.61 66.48 68.38 64.66 64.66 66.96 66.36 67.76 66.63 66.44 69.27 66.52 66.59 66.81 65.24 68.25 64.88 64.12 65.58 68.39 65.35 66.96 66.52 64.82 65.09 62.82 64.63 64.34 64.16 63.29 66.29 65.74 63.09 62.25 66.84 64.05 64.48 62.80 63.39 65.57 65.59 62.73 61.44 65.90 62.18 63.59 58.27 62.25 60.71 62.87 60.30 64.79 64.59 58.43 61.12 61.59 59.11 60.97 62.11 58.66 57.96 59.01 58.11 52.12 52.01 56.85 59.70 62.40 51.53 62.97 58.08 56.03 54.45 58.35 55.18 46.33 62.98 63.08 49.14 54.72 58.15 51.24 64.00 55.69 46.40 53.34 50.60 54.83 49.81 64.15 Nuclear Cell 84.56 84.77 84.87 84.49 83.78 84.54 84.37 84.63 84.35 84.41 84.21 84.13 84.17 84.14 82.99 84.49 84.19 84.47 83.91 84.33 84.93 84.62 84.18 83.97 84.17 84.88 84.32 83.57 83.51 84.85 84.00 84.15 83.61 82.76 83.87 82.70 83.56 83.60 83.89 83.55 83.82 84.70 83.01 83.19 84.13 83.10 83.86 83.32 83.66 83.47 80.26 82.21 83.09 84.00 82.22 83.75 81.76 82.99 83.40 82.37 81.43 82.86 83.07 81.68 83.27 83.74 81.20 82.83 81.39 81.63 81.07 81.86 81.23 77.50 77.18 81.08 82.62 82.55 76.54 82.38 79.58 81.55 62.55 83.17 80.60 80.35 79.75 83.22 79.57 80.56 34.65 62.69 9.39 78.05 44.22 77.57 79.19 78.91 64.08 0.67 X-Ray Covidquex Montgomery DCA 63.61 64.23 63.25 62.73 63.25 61.70 63.77 64.90 63.69 62.17 63.81 62.64 63.65 63.81 62.35 63.14 63.27 63.54 63.19 63.30 63.23 63.09 62.25 62.95 62.85 63.04 62.01 61.63 62.18 63.01 62.59 62.67 62.43 63.15 62.13 61.94 61.41 62.51 62.05 61.34 63.25 62.56 62.17 62.14 62.26 60.83 62.40 61.94 61.95 61.78 60.54 60.34 60.19 62.82 60.02 59.50 61.34 59.81 59.83 61.21 59.94 57.07 61.13 59.80 58.95 61.16 60.15 59.27 57.42 57.73 57.00 57.64 57.83 50.33 51.74 50.91 60.11 62.49 49.19 52.49 58.29 56.94 57.05 58.15 53.31 53.50 55.08 60.65 53.78 50.68 59.19 47.05 53.84 50.84 0.06 4.64 7.24 51.78 45.24 59. 96.21 95.76 95.67 95.56 95.89 95.05 96.12 96.17 95.65 95.87 95.27 95.54 95.97 95.69 95.20 95.73 95.88 95.73 95.89 95.87 95.90 96.01 95.70 95.79 95.72 96.03 95.56 94.46 95.82 95.62 95.54 95.13 94.87 95.43 95.22 94.83 95.30 94.76 94.83 94.99 95.54 95.65 95.01 95.13 95.63 94.37 94.98 95.14 95.13 95.00 95.53 94.33 93.96 95.18 95.09 94.86 94.38 95.13 95.11 94.42 94.54 95.13 93.56 95.55 94.31 94.25 94.41 94.76 91.78 94.64 93.73 94.80 92.72 94.17 94.44 93.08 93.31 94.69 94.49 92.49 93.63 93.01 91.53 91.61 92.87 92.84 94.08 94.49 90.36 91.43 94.39 90.97 93.87 89.39 94.24 91.57 88.68 90.18 90.07 56.49 70.75 69.29 70.85 69.48 69.81 67.60 70.19 69.61 68.82 68.40 70.33 68.28 68.32 69.77 67.54 68.93 68.69 68.31 68.57 68.52 67.50 68.83 68.34 68.54 68.95 69.34 68.14 67.65 67.45 68.59 68.08 67.07 67.29 69.48 67.63 68.30 66.74 70.20 67.75 66.93 67.64 66.67 67.16 70.04 67.98 66.44 68.51 67.45 67.96 68.43 67.40 68.10 66.84 66.47 68.38 66.70 68.22 68.54 66.80 68.61 67.94 68.79 66.42 69.05 65.05 64.15 66.55 65.74 67.52 66.92 64.93 63.39 65.21 70.09 70.61 69.30 65.25 68.54 69.67 68.28 62.09 64.25 63.38 58.54 65.54 60.00 65.58 66.87 60.60 62.63 66.46 66.01 66.96 60.11 67.43 61.19 62.72 59.74 60.35 65.20 MRI ACDC Promise 87.56 85.20 86.87 84.73 86.66 84.35 86.29 84.51 86.95 83.79 87.29 84.90 87.00 83.99 85.84 84.30 86.34 84.06 86.58 83.99 87.05 84.91 86.87 84.24 86.20 85.11 87.26 84.28 87.03 83.80 86.69 85.01 86.68 84.27 86.04 84.07 85.29 83.79 86.30 84.32 85.63 83.68 85.87 83.88 86.01 84.21 85.98 84.20 86.01 83.40 85.54 83.88 86.02 83.08 83.62 85.54 86.35 84.72 85.36 84.68 85.57 83.13 85.87 83.05 86.53 83.30 86.06 84.78 85.84 84.40 84.84 84.39 86.40 83.25 86.39 82.78 86.35 81.95 86.89 82.44 85.81 84.78 83.98 83.56 86.19 83.21 85.96 82.89 84.89 83.52 83.10 84.68 85.42 82.65 85.78 80.49 86.37 74.11 86.80 81.96 81.01 85.33 86.32 80.47 84.96 80.32 85.64 81.13 85.99 83.25 86.40 81.45 86.42 84.43 86.24 82.19 84.17 81.56 84.82 80.41 83.98 84.89 84.25 83.74 84.38 79.46 85.62 83.28 83.29 75.68 83.96 74.04 80.10 82.98 84.83 77.43 85.24 57.79 84.68 78.88 83.30 81.68 82.16 79.28 80.88 81.81 86.88 84.18 86.66 83.96 86.19 80.97 81.40 73.87 85.61 73.31 86.43 82.52 85.24 81.98 80.99 73.44 78.17 72.68 78.86 77.04 81.25 71.60 83.69 69.65 84.07 66.22 76.57 80.70 0.00 82.65 77.30 75.57 80.77 72.43 47.58 85.45 79.86 82.07 47.79 83.91 77.57 72.64 83.80 84.52 82.58 70.28 78.19 73.93 47.66 63.01 47.52 70.62 47.62 84.62 CT OCT Synapse Cystoidfluid 72.56 69.73 73.25 71.69 69.65 72.06 73.18 70.15 70.78 70.89 67.17 68.42 71.83 72.20 73.69 71.19 70.41 69.16 70.84 67.90 69.81 70.12 69.62 69.66 70.70 70.11 69.35 74.70 70.90 70.14 68.05 70.25 67.21 64.45 58.69 72.60 67.43 63.54 69.60 69.82 70.16 68.07 71.08 64.67 70.96 72.74 66.33 67.80 61.95 66.17 72.63 69.96 65.87 64.59 69.18 62.47 73.30 71.09 67.22 64.14 73.54 66.64 63.86 64.13 63.12 61.42 67.33 60.17 68.78 64.59 67.38 64.37 65.07 70.84 68.97 64.94 58.81 0.00 69.28 64.45 54.66 51.19 63.91 54.86 59.89 50.85 62.85 64.19 57.19 40.58 55.09 65.87 68.52 50.81 68.56 54.32 56.83 51.51 50.08 68.88 72.56 69.73 73.25 71.69 69.65 72.06 73.18 70.15 70.78 70.89 67.17 68.42 71.83 72.20 73.69 71.19 70.41 69.16 70.84 82.39 81.46 81.25 82.29 81.28 82.05 80.83 79.82 82.24 83.99 80.01 80.66 83.74 83.58 80.88 83.07 82.27 82.58 80.79 83.18 82.92 80.74 83.39 82.60 81.17 80.97 79.10 79.78 82.55 82.09 79.99 75.34 83.10 84.21 80.87 77.66 83.17 76.58 81.87 79.82 78.11 75.30 70.55 79.01 76.37 80.62 79.62 75.72 79.27 72.77 74.74 78.87 79.72 75.45 62.03 61.89 69.55 75.55 77.23 63.10 76.02 71.47 72.90 73.03 74.12 70.64 74.49 0.03 0.00 62.79 67.34 67.86 6.44 68.86 68.76 60.33 65.49 64.89 64.54 63.49 61.96 Avg 79.84 79.43 79.37 79.27 79.13 78.95 78.85 78.82 78.79 78.76 78.73 78.70 78.69 78.62 78.58 78.57 78.54 78.52 78.38 78.31 78.26 78.25 78.24 78.23 78.15 78.13 78.02 78.00 77.99 77.95 77.88 77.85 77.74 77.74 77.57 77.55 77.45 77.45 77.38 77.34 77.14 77.11 77.09 77.08 77.02 76.91 76.83 76.78 76.72 76.70 76.57 76.49 76.38 76.34 76.32 76.04 75.65 75.51 75.46 75.31 75.13 74.74 74.64 74.51 74.04 74.01 73.89 73.41 73.27 73.23 72.66 72.47 72.43 72.37 72.26 72.23 72.02 71.84 71.62 71.39 70.19 69.48 68.83 68.74 68.31 67.67 67.63 67.50 66.61 66.39 65.09 64.86 64.55 64.47 64.08 63.70 63.25 61.24 59.34 56.53 32 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 14: Average performance of 100 u-shape medical image segmentation networks with U-Score. Baseline U-Net is highlighted (gray background), and statistical significance (calculated by IoU) of p-value is highlighted: p<0.0001 , p<0.001 , p<0.01 , p0.05 ,and > 0.05 (Not significant) Rank Network LGMSNet LV-UNet CMUNeXt MBSNet Tinyunet Mobile U-ViT U-RWKV U-KAN DCSAU-Net ULite UNeXt CFPNet-M RWKV-UNet MDSA-UNet DDANet ResU-KAN UTNet CE-Net TA-Net SwinUNETR TransResUNet MEGANet G-CASCADE MultiResUNet EMCAD MedFormer HiFormer SimpleUNet CASCADE DC-UNet MMUNet MUCM-Net ERDUnet TransFuse U-Net++ AC-MambaSeg H2Former CSCAUNet Polyp-PVT LFU-Net LeViT-UNet MSLAU-Net FAT-Net ESKNet MedVKAN Swin-umambaD AURA-Net SCUNet++ MambaUnet CA-Net DAEFormer RollingUnet VMUNet VMUNetV2 DDS-UNet MissFormer Swin-umamba DoubleUNet ColonSegNet ResUNet++ TransAttUnet GH-UNet MFMSNet AttU-Net U-Net CENet ScribFormer TransUnet UNet3+ UNetV2 DA-TransUNet UTANet MedT CMU-Net Zig-RiR #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 UltraLight-VM-UNet #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 #95 #96 #97 #98 #99 # TransNorm CPCANet EViT-UNet CFM-UNet MSRFNet BEFUnet UCTransNet CaraNet FCBFormer D-TrAttUnet PraNet MCA-UNet MALUNet UNETR CSWin-UNet Perspective-Unet ConvFormer MERIT DS-TransUNet UACANet CFFormer H-vmunet SwinUnet MT-UNet Ultrasound BUSBRA TNSCUI 85.79 82.62 86.88 85.22 81.90 76.35 72.99 71.29 72.47 65.19 61.09 66.18 61.72 62.93 58.95 59.24 57.02 59.33 58.27 23.25 51.13 50.68 50.86 62.15 49.16 43.96 46.40 9.52 44.00 39.02 43.76 44.13 45.34 51.09 38.71 37.59 37.76 38.27 52.95 9.52 8.87 35.00 36.12 34.51 35.55 41.34 32.24 32.31 49.43 31.09 26.03 30.96 8.49 46.07 28.37 27.98 22.44 23.30 13.13 19.59 21.49 20.84 20.48 20.29 20.24 19.80 19.85 19.32 19.29 14.01 18.83 18.59 19.47 17.35 8.20 9.42 17.12 17.21 16.18 11.76 15.93 8.49 14.83 18.50 14.61 13.82 17.33 13.36 9.50 7.08 12.87 12.30 12.21 12.45 10.39 12.65 9.97 7.26 8.48 9.30 88.20 85.36 84.44 81.57 81.53 77.00 62.42 73.84 74.96 52.52 55.94 72.40 61.72 71.46 59.28 60.59 58.05 59.54 58.27 43.28 50.84 50.68 54.54 59.75 36.93 46.61 46.45 9.52 46.37 43.88 44.42 29.61 47.41 52.06 40.79 39.01 38.03 39.32 56.26 9.52 8.87 36.43 36.12 34.59 34.88 54.28 32.19 34.87 9.21 31.59 33.73 30.27 40.65 55.66 28.38 39.07 22.64 23.60 19.77 20.45 21.60 20.92 20.48 20.14 20.11 20.03 20.35 19.58 19.07 34.44 18.76 18.59 18.84 17.33 8.20 9.42 17.24 17.16 16.18 23.36 15.98 8.49 14.78 18.50 14.58 13.89 17.33 13.37 25.67 7.08 22.18 12.34 12.25 12.49 10.35 12.65 10.13 7.26 8.48 9.12 BUSI 86.60 78.85 81.96 81.21 45.28 77.81 67.91 66.86 66.85 62.66 46.75 63.23 61.72 68.43 56.79 56.11 58.08 58.93 57.20 44.82 51.29 50.68 55.18 54.42 50.79 43.08 43.32 9.52 47.32 28.87 41.80 37.64 45.10 51.82 37.94 36.31 37.89 37.08 59.01 9.52 8.87 35.37 35.87 34.67 32.82 47.87 32.24 24.80 9.21 31.32 30.49 29.90 40.60 34.11 28.01 31.14 22.51 23.37 19.46 18.15 21.27 20.09 20.48 19.28 19.25 19.96 19.66 19.55 18.17 8.87 18.84 18.33 16.28 17.30 8.20 9.42 17.18 17.62 16.17 7.53 15.31 28.54 14.85 18.49 14.36 13.53 17.33 13.30 11.18 7.08 7.91 12.04 12.19 12.46 10.13 12.60 10.13 18.06 8.48 9.34 Dermoscopy Endoscopy Fundus Histopathology ISIC2018 SkinCancer Kvasir Robotool CHASE DRIVE DSB2018 83.21 87.93 64.46 70.00 47.59 74.58 60.99 56.88 68.11 52.91 58.30 64.41 61.72 62.01 52.96 55.84 55.41 59.90 58.27 57.72 51.29 50.68 52.17 46.61 52.71 41.60 46.30 9.52 44.44 8.79 40.31 41.94 37.87 52.55 34.03 37.05 35.65 34.08 52.98 9.52 16.62 34.93 35.50 32.47 31.92 31.67 32.24 19.30 9.21 28.58 27.99 28.73 38.45 51.36 28.50 26.82 22.64 7.39 18.34 18.82 20.40 18.90 20.48 18.49 18.94 20.02 19.66 19.18 18.14 16.29 18.70 18.37 19.00 16.19 20.73 9.42 16.96 15.48 16.11 17.63 15.15 8.49 14.54 18.50 14.37 13.15 17.33 13.23 9.50 7.08 17.55 11.72 11.88 12.46 10.56 12.52 10.13 14.53 8.48 9.17 89.72 83.00 88.85 67.66 54.63 67.76 39.91 75.10 46.94 74.11 46.96 52.24 61.72 48.81 35.00 46.81 44.94 53.74 51.03 47.03 49.95 50.35 53.69 50.21 53.54 37.91 46.25 9.52 46.96 8.79 29.06 69.92 10.27 44.95 37.76 37.17 34.46 33.37 57.09 9.52 40.89 36.58 34.54 28.68 31.82 8.96 32.24 33.52 9.21 28.25 26.64 23.17 38.91 9.02 27.72 8.61 21.89 7.39 20.63 20.53 7.09 21.15 20.21 18.07 16.59 19.74 13.08 19.54 17.47 37.92 18.84 18.30 7.32 15.79 8.20 43.98 17.34 17.84 15.37 18.89 13.42 37.47 14.39 18.50 14.61 13.93 17.33 13.39 9.50 15.58 24.16 12.20 11.69 12.40 10.60 12.40 9.97 7.26 13.72 9. 77.54 84.97 65.67 73.03 37.62 73.98 48.14 69.02 63.17 18.95 11.45 55.62 61.72 52.72 58.05 58.61 50.90 54.81 56.36 59.57 49.89 50.68 54.97 54.76 53.41 43.92 45.95 9.52 47.32 26.53 43.28 9.50 15.87 50.69 39.08 37.52 35.98 37.82 58.47 9.52 8.87 36.23 35.43 32.90 33.51 60.37 31.78 37.75 73.88 28.97 35.25 29.38 43.80 62.66 27.88 39.96 22.64 24.74 21.21 18.60 20.80 20.29 20.48 18.30 19.01 20.23 18.50 18.66 17.20 42.15 18.33 18.33 7.32 17.27 8.20 9.42 16.64 17.88 16.13 15.43 15.29 23.09 14.34 18.28 14.61 13.65 17.33 13.14 9.50 7.08 7.91 12.20 11.87 12.47 10.78 12.65 10.13 7.26 8.48 9.16 69.33 83.02 65.44 66.41 54.37 67.44 51.99 58.72 58.03 41.97 58.13 60.18 61.72 57.40 54.34 49.07 52.14 59.90 58.27 24.69 51.29 50.68 47.41 51.51 43.37 39.29 45.83 42.70 43.32 40.05 36.64 9.50 29.01 50.90 37.55 31.33 33.92 34.70 53.52 9.52 41.26 35.76 35.99 33.10 31.60 29.28 32.24 21.52 9.21 28.24 25.48 29.56 34.87 9.02 26.52 10.76 22.54 21.05 21.55 18.86 20.78 19.51 20.48 19.97 20.05 19.39 17.88 18.43 19.10 13.31 18.64 18.59 8.00 16.90 18.82 9.42 16.58 14.77 16.18 7.53 15.24 8.49 14.83 18.49 13.75 13.67 17.33 13.00 9.50 7.08 7.91 12.01 12.02 12.52 10.72 12.65 10.10 7.26 8.48 9.38 86.33 68.19 85.35 83.10 82.24 74.46 82.99 72.17 64.49 70.39 70.75 66.76 54.56 30.24 59.52 59.25 59.49 52.61 52.49 67.66 51.23 47.14 31.28 9.10 35.65 46.80 39.93 63.30 33.67 45.90 43.51 46.09 42.01 14.12 41.69 40.25 36.87 36.57 8.94 53.43 50.54 33.66 34.85 34.67 32.04 16.16 32.07 37.51 30.68 30.82 35.76 30.97 35.68 9.02 28.27 35.52 22.64 24.55 23.57 22.27 21.32 20.30 19.66 20.60 20.65 19.39 20.23 19.65 19.65 8.87 18.70 18.52 21.22 17.50 24.92 19.12 17.13 17.90 15.85 7.53 15.47 17.70 15.16 6.71 14.61 14.19 6.55 13.22 9.50 19.80 7.91 12.36 11.64 6.20 10.62 5.74 10.05 7.26 11.91 9.52 88.65 79.63 87.62 86.64 96.30 76.66 85.32 74.12 74.72 95.19 79.78 80.90 60.59 54.31 60.36 60.38 59.67 53.68 52.95 79.66 50.23 47.58 41.82 14.90 46.12 47.06 42.43 91.07 34.42 51.91 44.24 70.37 48.23 12.08 41.27 39.21 37.66 26.63 8.94 92.54 47.97 34.15 33.94 34.67 34.84 8.96 31.74 30.51 9.21 32.06 26.17 31.38 8.49 9.02 28.45 13.88 22.64 24.66 23.57 22.31 21.76 21.06 20.06 20.59 20.62 20.04 20.66 19.61 19.65 8.87 18.66 18.53 23.44 17.50 29.79 9.42 17.16 11.06 16.08 7.53 15.96 8.49 15.10 14.25 14.61 14.25 13.77 13.31 9.50 21.51 7.91 12.32 11.67 11.91 10.70 10.33 10.10 7.26 8.48 9.52 90.80 71.97 82.70 83.32 77.54 73.54 86.03 61.24 76.06 89.40 65.60 71.09 61.51 41.12 60.77 53.31 59.67 46.36 50.96 70.15 46.42 47.22 49.10 9.10 47.44 45.49 31.62 57.53 37.14 42.40 39.00 17.77 47.47 23.48 38.83 33.41 33.53 36.08 8.94 75.41 39.30 28.80 29.13 34.67 33.47 57.03 32.24 25.12 56.94 32.08 35.15 31.38 32.09 9.02 25.77 32.19 22.39 24.81 23.47 21.46 21.76 18.48 19.46 20.47 20.59 20.18 20.62 19.12 19.57 8.87 18.45 18.35 23.00 17.37 8.20 54.15 16.21 14.80 15.80 11.51 16.06 8.49 15.01 6.71 14.60 14.25 6.55 13.20 16.69 21.11 23.13 12.12 5.70 5.73 10.74 5.74 10.03 18.26 27.87 9.52 Glas Monusac 88.31 87.64 86.90 87.32 90.03 70.83 76.72 71.74 65.18 72.50 79.51 76.17 61.72 68.49 60.77 58.34 57.04 58.08 56.94 46.66 50.80 50.57 55.99 64.61 53.54 43.63 45.03 82.95 47.32 51.83 37.52 65.37 39.68 44.83 40.76 36.23 38.07 39.33 8.94 73.55 43.31 36.58 36.03 34.48 33.62 8.96 32.16 37.54 9.21 31.71 30.94 31.24 8.49 58.56 28.54 17.05 22.01 23.56 23.18 21.11 21.50 21.16 20.48 20.59 20.65 20.23 19.96 19.28 19.63 26.46 18.82 18.59 19.79 17.40 24.18 9.42 16.66 15.74 15.99 21.35 15.97 8.49 15.05 18.50 14.41 14.03 17.30 13.22 9.50 7.08 7.91 12.29 12.36 12.56 10.75 12.49 10.11 15.94 8.48 9. 84.05 82.50 83.98 87.02 70.35 77.17 84.80 67.77 70.21 72.85 71.93 72.31 61.72 48.22 60.77 57.81 59.67 51.78 54.56 63.75 51.24 49.62 44.42 59.08 47.25 46.88 41.87 83.02 33.45 50.81 39.20 50.99 47.92 30.09 40.36 37.47 37.63 36.18 8.94 33.05 40.44 33.90 34.96 34.67 34.88 38.97 32.10 30.89 67.51 32.11 28.73 31.23 41.33 9.02 27.58 12.93 22.00 24.48 23.31 21.58 21.76 20.45 20.07 20.60 20.57 19.71 20.74 19.45 19.57 16.15 18.42 18.59 21.13 17.44 8.20 28.61 16.83 15.77 16.18 14.33 16.05 8.49 15.13 6.71 14.56 14.25 6.55 13.25 19.76 18.22 7.91 12.21 11.28 12.14 10.64 5.74 10.02 7.26 8.48 9.52 Nuclear Cell 89.78 82.15 85.96 84.37 91.56 75.13 84.96 72.92 73.28 90.39 86.46 77.07 61.72 60.75 59.88 60.63 59.60 56.06 55.90 76.58 49.84 50.44 46.18 58.68 48.53 45.66 42.37 48.75 41.09 49.32 42.76 45.25 47.36 42.19 40.76 37.19 37.25 37.90 8.94 73.33 48.14 34.40 34.09 34.54 34.51 8.96 32.24 33.28 9.21 32.11 33.38 31.38 8.49 9.02 28.50 8.61 22.64 20.84 23.57 22.07 21.76 20.71 20.37 20.60 20.59 19.92 20.63 19.63 19.57 41.48 18.84 18.59 22.99 17.41 29.19 14.28 17.14 17.08 16.12 22.65 15.98 28.87 15.10 6.71 14.61 14.20 6.55 13.30 33.35 21.05 21.19 12.15 11.95 11.96 10.66 5.74 10.12 7.26 8.48 9.52 X-Ray Covidquex Montgomery DCA 86.33 90.90 86.61 86.31 83.80 77.60 86.37 72.14 73.92 78.81 80.66 75.70 61.72 58.63 60.77 60.35 59.67 58.13 57.18 74.15 51.27 50.68 51.00 20.35 52.59 46.42 43.50 50.40 45.33 42.02 42.02 73.16 47.88 8.78 40.56 38.44 37.72 37.09 8.94 65.53 45.56 35.79 36.05 34.67 34.71 52.56 32.23 36.62 63.62 32.00 33.89 31.31 25.66 9.02 28.83 38.24 22.52 23.96 23.40 21.31 21.72 21.08 20.48 20.57 20.65 19.92 20.56 19.65 19.65 8.87 18.84 18.59 23.05 17.41 21.79 9.42 17.16 16.98 16.18 17.05 16.11 25.45 15.05 8.06 14.46 14.20 6.55 13.30 13.00 20.05 7.91 12.29 12.16 11.88 10.66 5.74 10.08 7.26 8.48 9.52 87.70 79.68 85.68 86.60 90.27 77.81 80.96 70.32 69.12 77.77 85.18 70.20 61.72 66.64 60.77 56.75 59.08 56.39 52.86 64.59 51.29 50.68 49.99 30.04 48.62 44.06 44.80 73.58 42.02 50.51 38.25 63.43 38.96 35.46 40.76 36.69 38.34 38.37 50.03 50.12 51.52 34.56 35.39 34.34 34.02 52.11 31.99 35.47 9.21 31.79 25.47 31.38 36.78 9.02 28.48 8.61 22.44 24.54 23.42 21.61 21.76 20.75 20.39 20.53 20.65 19.28 20.62 19.65 19.65 8.87 18.84 18.54 20.38 17.37 25.65 9.42 17.03 17.04 15.97 20.23 16.00 8.49 15.11 17.68 14.37 14.04 16.40 13.23 9.50 7.08 7.91 12.22 12.39 5.73 10.49 12.28 10.13 7.26 8.48 9.52 80.45 91.70 78.45 80.48 73.91 72.49 63.88 61.75 70.64 50.16 60.55 65.61 61.72 61.48 56.60 55.76 55.61 59.90 58.27 31.48 51.29 50.68 51.48 62.00 50.90 42.06 44.76 58.36 44.54 51.35 39.10 9.50 41.26 52.07 36.48 38.56 35.23 38.48 51.84 36.16 15.43 35.01 35.93 33.84 34.24 47.96 32.24 33.68 44.29 31.19 28.97 30.72 38.42 46.74 27.45 14.53 22.57 23.40 21.64 20.63 21.64 20.09 20.48 20.35 20.23 19.43 20.08 19.50 19.34 8.87 18.82 18.49 19.84 17.12 8.20 9.42 16.86 15.51 16.18 22.11 15.60 8.49 14.89 18.50 14.17 13.67 17.33 13.02 9.50 7.08 7.91 12.03 12.38 12.21 10.31 12.65 10.02 7.26 8.48 9.32 MRI ACDC Promise 86.55 84.55 86.67 78.00 82.44 83.05 83.19 85.46 81.65 84.65 71.62 75.20 81.88 73.79 77.30 70.77 70.67 70.36 74.12 39.76 86.75 56.34 9.36 77.34 61.72 61.72 68.16 56.33 59.28 60.77 61.21 56.99 57.85 58.58 57.87 56.75 57.30 55.11 66.82 19.26 51.29 50.16 50.68 49.80 49.38 56.29 62.39 61.06 51.23 9.01 46.77 45.73 45.94 45.61 9.52 79.71 46.72 46.99 47.84 52.03 42.47 39.91 48.34 11.65 45.80 42.08 51.70 47.12 41.58 40.84 40.32 36.72 37.96 38.39 39.36 37.91 50.92 58.75 9.52 9.52 42.28 45.61 34.66 36.36 35.46 36.12 34.28 34.29 35.51 33.74 8.96 60.39 32.11 32.05 23.17 37.76 9.21 73.55 31.07 32.07 27.30 36.77 30.75 30.99 8.49 43.08 27.31 57.95 28.74 27.16 12.69 33.38 22.46 22.58 19.74 24.81 22.27 23.27 22.22 21.45 21.35 21.57 21.11 20.97 20.48 20.39 20.55 20.60 20.50 20.56 19.08 20.23 20.13 20.50 19.51 19.48 19.61 19.57 33.98 8.87 18.60 18.77 18.59 18.58 19.94 11.40 17.50 17.43 34.05 8.20 9.42 9.42 17.25 10.51 17.48 18.03 16.18 16.18 24.54 7.53 15.94 16.11 8.49 25.84 15.03 15.10 18.45 18.37 14.61 14.61 14.12 14.09 17.33 17.26 13.27 13.36 9.50 9.50 18.04 7.08 7.91 13.31 12.36 12.29 12.32 12.34 12.38 5.73 10.37 10.79 12.60 12.47 10.11 10.10 21.04 7.26 8.48 8.48 9.45 9. CT OCT Synapse Cystoidfluid 83.36 69.73 77.97 81.24 83.25 75.33 65.36 72.83 63.66 64.52 46.86 61.88 61.72 59.17 60.37 59.16 56.11 48.69 45.15 48.36 49.12 50.68 56.29 52.87 8.80 43.11 44.93 63.09 47.32 47.38 39.99 9.50 40.87 44.40 38.97 39.68 37.83 39.43 54.83 9.52 45.71 36.58 31.52 34.14 32.49 8.96 32.24 36.28 67.96 31.43 34.41 30.72 41.36 53.51 27.95 38.96 22.56 24.81 22.74 19.56 21.44 21.15 20.48 20.47 19.97 20.23 20.59 19.54 19.44 8.87 18.60 18.31 17.06 17.08 8.20 9.42 15.41 17.97 15.66 19.95 15.94 18.20 14.96 18.11 14.60 14.13 17.21 10.82 9.50 7.08 14.18 12.36 11.81 12.37 10.80 12.49 10.09 7.26 8.48 9.45 82.59 84.68 82.98 83.02 90.12 72.18 82.54 76.59 70.13 92.51 88.44 9.36 61.14 59.39 58.60 62.21 58.52 57.72 55.82 73.38 50.47 49.38 47.19 56.12 47.80 47.22 43.05 9.52 42.12 35.90 45.18 56.28 47.07 32.95 41.71 40.75 38.42 39.32 8.94 64.18 52.94 36.19 35.20 31.96 34.31 29.10 31.44 34.79 9.21 31.09 34.61 30.80 27.48 9.02 29.15 37.71 22.64 22.88 23.57 22.33 21.42 21.21 20.36 20.45 20.53 20.11 20.45 19.58 19.65 25.95 18.45 18.59 22.26 17.42 32.21 39.94 17.23 18.11 16.18 21.37 15.92 8.49 15.09 6.71 14.61 14.25 6.55 13.37 9.92 19.72 10.23 12.36 12.04 11.71 10.64 5.74 10.12 11.43 8.48 9.48 Avg 84.99 81.96 81.37 81.16 74.93 74.25 72.00 69.67 68.15 66.83 64.87 62.40 61.27 57.84 57.73 57.42 56.85 56.01 55.16 54.14 50.52 49.98 49.96 46.99 44.37 44.36 43.82 43.53 43.09 40.76 40.62 40.50 40.37 39.67 39.52 37.54 36.93 36.87 36.41 35.63 35.15 35.05 34.91 33.81 33.72 33.64 32.10 31.83 31.46 30.98 30.86 30.31 30.08 29.21 28.01 24.43 22.47 21.89 21.75 20.75 20.71 20.51 20.31 20.07 20.05 19.85 19.74 19.40 19.15 19.10 18.69 18.50 18.09 17.20 17.16 16.60 16.58 16.57 16.04 15.99 15.67 15.35 14.92 14.74 14.49 13.97 13.86 13.13 12.65 12.64 12.29 12.21 11.71 10.99 10.59 10.39 10.08 10.05 9.89 9.41 33 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 15: Average zeroshot performance of 100 u-shape medical image segmentation networks with IoU. Source Target. Baseline U-Net is highlighted (gray background), and statistical significance of p-value is highlighted: p<0.0001 , p<0.001 , p<0.01 , p0.05 ,and > 0.05 (Not significant) Rank Network BUSI BUS BUSBRA BUS TNSCUI TUCC Kvasir CVC300 Kvasir CVC-ClinicDB ISIC2018 PH2 CHASE Stare Montgomery NIH-test Monusac Tnbcnuclei Ultrasound Endoscopy Dermoscopy Fundus X-Ray Histopathology RWKV-UNet DS-TransUNet TransResUNet CENet MFMSNet G-CASCADE DA-TransUNet TA-Net MEGANet EMCAD CFFormer CASCADE Swin-umamba AC-MambaSeg MCA-UNet MSLAU-Net AURA-Net CSCAUNet VMUNet FAT-Net FCBFormer Perspective-Unet Swin-umambaD LGMSNet CE-Net LV-UNet GH-UNet UTANet U-KAN MMUNet MBSNet CA-Net DDS-UNet HiFormer ResU-KAN PraNet SwinUNETR H2Former EViT-UNet TransFuse CaraNet DAEFormer VMUNetV2 MissFormer Mobile U-ViT UTNet DCSAU-Net CPCANet ESKNet D-TrAttUnet U-Net++ UCTransNet TransAttUnet CMU-Net SCUNet++ UNet3+ RollingUnet MSRFNet UACANet Polyp-PVT MERIT TransNorm DDANet ERDUnet MedFormer U-Net TransUnet MedVKAN Tinyunet CFPNet-M AttU-Net DoubleUNet MDSA-UNet UNeXt MT-UNet UNetV2 ULite CMUNeXt ResUNet++ MUCM-Net H-vmunet U-RWKV BEFUnet ConvFormer MedT CSWin-UNet ScribFormer DC-UNet UNETR MambaUnet Zig-RiR LeViT-UNet CFM-UNet SwinUnet ColonSegNet MultiResUNet #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 #95 #96 #97 UltraLight-VM-UNet #98 #99 #100 SimpleUNet LFU-Net MALUNet 80.73 78.70 81.27 82.70 79.78 82.61 82.32 81.02 81.24 82.83 81.12 82.11 82.91 79.84 81.88 80.96 78.40 78.37 81.18 77.91 79.14 76.17 80.68 78.45 80.34 74.19 77.25 77.05 77.25 76.53 76.37 75.84 77.36 72.21 72.73 82.13 74.70 78.22 79.07 80.18 81.32 72.80 72.72 72.82 77.80 77.64 70.88 79.14 76.99 71.01 76.68 68.11 74.80 75.72 73.77 70.13 72.58 69.01 80.81 81.02 80.46 82.11 71.75 76.56 77.54 72.44 80.27 71.36 67.40 71.42 73.47 73.47 71.81 70.12 67.76 68.88 69.48 19.06 59.29 70.99 74.93 66.30 65.27 71.15 64.12 64.89 60.36 60.45 59.78 37.61 57.05 58.33 63.88 60.42 51.95 62.19 64.99 50.77 51.14 71.04 84.73 84.44 85.62 86.29 84.47 85.27 84.15 84.38 86.62 84.95 79.53 85.96 85.02 82.31 85.28 84.90 84.67 84.65 62.96 84.86 86.10 84.69 84.39 83.34 83.77 82.72 83.26 84.62 83.97 81.53 83.08 83.01 83.49 84.20 83.11 84.19 84.18 83.81 84.95 83.93 84.87 84.02 84.58 84.76 84.42 83.78 84.08 84.65 84.39 79.89 85.46 80.41 83.73 83.52 82.73 80.70 83.70 81.90 84.82 85.65 83.26 84.33 80.45 84.30 85.00 81.37 83.90 82.32 78.61 82.93 81.25 86.51 82.57 82.21 81.09 82.84 78.69 82.31 79.77 82.13 83.30 81.90 77.20 82.91 82.59 83.11 82.41 79.51 76.24 83.92 81.08 70.27 81.77 78.35 68.99 77.67 77.60 72.58 75.95 81. 63.42 61.95 62.36 62.15 61.27 65.10 61.81 62.89 64.37 61.20 62.34 63.48 64.00 63.01 63.78 66.15 61.52 63.40 62.31 60.23 63.52 61.76 62.69 65.16 62.09 63.61 62.49 61.59 62.65 61.69 62.45 62.74 63.09 63.94 62.79 63.59 59.91 64.48 65.83 61.74 64.61 63.72 63.70 61.39 60.34 60.04 61.95 60.52 63.07 59.21 58.14 60.58 61.24 61.57 59.35 60.40 62.34 58.35 62.47 65.23 66.00 63.81 60.11 62.23 63.57 60.50 60.54 59.69 59.50 60.77 58.99 58.48 61.04 58.82 57.52 61.87 56.63 62.22 60.51 58.58 56.18 61.57 57.28 62.96 61.63 57.60 60.43 59.78 46.17 46.43 53.31 53.69 59.27 37.59 50.15 58.39 54.56 51.67 52.15 58.31 82.14 80.58 79.13 78.57 81.24 77.89 71.24 81.24 78.67 79.93 78.56 80.57 79.96 72.82 72.00 78.24 79.18 77.50 76.54 74.22 74.82 77.38 78.38 74.57 76.99 78.86 77.22 80.78 72.87 74.35 78.22 73.61 75.43 78.91 71.83 83.31 63.58 74.03 81.11 79.39 79.12 68.10 78.80 71.99 74.94 72.66 74.78 76.92 77.23 74.85 71.45 75.06 73.98 75.09 75.67 71.98 77.36 72.59 81.72 73.87 81.39 69.78 78.78 72.47 73.91 70.33 72.24 67.30 70.89 71.50 73.40 79.92 69.72 63.13 67.90 69.68 62.46 76.45 68.06 54.35 59.46 58.94 71.27 76.65 57.05 71.46 74.42 66.80 61.36 77.11 65.19 60.61 68.37 45.60 69.58 71.18 33.04 56.36 52.06 58.57 75.50 77.38 76.42 73.14 76.33 74.77 70.64 72.03 75.64 76.56 76.18 77.19 73.57 70.73 69.49 71.49 73.50 70.97 74.15 71.64 73.16 72.13 76.83 75.01 70.87 69.42 70.59 70.99 69.77 68.61 72.68 67.49 71.24 74.53 72.61 77.39 68.44 70.50 71.49 72.54 73.86 66.69 74.66 68.27 69.72 68.39 68.86 68.33 73.85 69.36 66.04 68.57 71.26 70.11 67.82 69.52 69.26 73.48 74.99 75.17 74.70 68.51 70.72 57.91 69.55 69.87 68.20 62.66 59.72 66.16 67.76 75.72 65.42 61.57 65.41 67.73 59.87 67.38 67.79 50.60 63.00 59.56 64.23 67.22 56.94 63.25 66.66 63.24 55.46 75.05 61.02 61.89 59.91 48.58 67.46 65.60 43.42 50.97 48.23 55.02 86.00 84.05 84.20 84.10 85.08 85.96 82.81 84.15 84.99 83.63 85.04 84.63 84.62 84.34 85.08 86.52 83.91 84.54 84.27 84.22 83.85 83.57 83.37 85.00 84.02 84.99 85.01 84.59 83.29 85.66 84.65 82.44 82.18 82.75 84.42 84.30 83.73 85.39 83.79 84.84 84.85 82.81 84.65 83.38 83.87 84.14 82.77 84.30 83.81 84.46 84.62 83.54 83.87 83.15 82.74 83.27 82.97 83.15 85.12 83.90 85.94 83.03 83.43 83.98 84.18 84.00 83.39 85.03 82.99 84.69 80.58 80.70 84.23 82.39 84.35 82.38 83.99 85.32 81.01 83.24 83.55 84.87 79.48 83.44 84.14 83.27 82.68 80.95 81.63 82.08 84.54 81.11 84.09 83.05 82.00 82.43 80.01 82.21 82.00 81.67 61.29 57.65 50.77 53.32 48.94 48.80 56.44 45.67 45.75 49.19 52.38 46.97 54.52 52.78 56.59 46.51 51.22 48.61 52.96 51.38 57.20 45.92 39.76 46.58 43.52 49.02 45.35 45.20 46.24 48.19 45.63 49.38 45.01 50.25 47.60 24.25 57.55 47.19 54.16 35.95 26.94 52.57 36.68 52.27 46.86 45.38 43.94 45.44 40.79 47.43 43.62 45.90 50.12 46.09 47.08 49.76 46.98 55.17 25.10 28.02 38.06 53.31 47.81 52.97 43.98 46.77 52.10 48.48 44.68 48.80 48.93 54.56 37.16 44.89 49.51 31.55 52.23 49.66 44.02 45.83 33.00 44.96 41.29 30.23 49.33 20.29 40.84 46.60 56.52 41.96 33.02 32.46 28.23 39.79 43.98 19.31 38.61 40.12 42.61 3.42 82.41 76.21 87.69 71.06 83.67 78.05 85.87 77.53 88.19 73.37 80.49 77.07 81.96 78.58 76.85 78.69 81.54 73.47 77.96 82.19 74.55 77.21 83.13 76.02 80.83 78.82 64.56 75.38 69.99 70.73 64.98 63.74 74.29 81.65 73.55 84.58 74.66 63.56 43.53 81.78 86.22 65.70 73.96 70.49 72.77 68.83 62.76 62.21 71.68 71.20 61.52 73.57 55.80 73.47 68.66 73.74 64.60 58.97 81.88 77.00 73.80 82.90 61.04 60.27 52.97 71.33 83.03 55.99 67.16 59.24 71.44 77.45 62.69 66.99 60.96 68.47 57.63 67.20 57.08 76.29 75.52 59.35 62.93 56.70 53.30 67.40 65.48 48.64 62.18 48.04 68.03 66.76 70.06 65.60 58.15 60.99 66.75 68.72 62.63 72. 38.96 44.20 35.19 48.03 43.98 46.18 44.53 50.74 38.86 46.41 33.32 37.76 38.39 40.53 32.40 31.46 31.25 36.58 44.40 29.64 42.88 36.12 38.59 30.87 32.30 28.67 43.52 23.55 38.33 37.11 28.80 39.98 38.82 32.41 31.51 38.53 32.98 28.62 30.14 32.58 33.32 45.16 36.84 40.61 31.85 32.14 41.48 37.67 36.36 32.90 46.44 34.93 28.16 38.16 35.38 29.88 22.75 21.74 26.84 32.42 43.43 39.03 22.99 34.34 31.26 26.05 34.36 44.42 34.12 36.32 20.90 26.88 41.23 32.78 26.84 45.32 32.26 43.51 30.00 32.00 37.65 26.51 30.47 36.17 31.27 45.18 13.77 21.43 32.47 34.32 27.10 38.53 32.33 43.81 22.87 24.38 36.21 30.43 30.99 38.51 Avg 70.94 70.07 69.47 69.31 69.08 68.95 68.65 68.58 68.39 68.24 68.09 67.91 67.86 67.61 67.60 67.20 67.10 67.09 66.79 66.60 66.46 66.34 66.33 66.06 65.94 65.83 65.48 65.29 65.28 65.19 65.15 65.01 65.01 64.95 64.82 64.81 64.70 64.68 64.66 64.61 64.39 64.36 64.23 63.99 63.92 63.91 63.81 63.68 63.64 63.62 63.56 63.51 63.47 63.47 63.43 63.33 63.08 62.99 62.84 62.80 62.80 62.72 62.65 62.39 62.37 62.23 61.87 61.79 61.73 61.63 61.43 61.37 60.94 60.78 60.71 60.24 60.23 59.53 59.45 59.37 59.28 59.16 58.64 58.36 58.27 57.88 57.61 57.37 57.02 56.55 56.25 55.53 55.14 54.06 53.73 53.42 53.38 52.90 52.81 52.48 34 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 16: Average zeroshot performance of 100 u-shape medical image segmentation networks with U-Score. Source Target. Baseline U-Net is highlighted (gray background), and statistical significance of p-value is highlighted: p<0.0001 , p<0.001 , p<0.01 , p0.05 ,and > 0.05 (Not significant) Rank Network BUSI BUS BUSBRA BUS TNSCUI TUCC Kvasir CVC300 Kvasir CVC-ClinicDB ISIC2018 PH2 CHASE Stare Montgomery NIH-test Monusac Tnbcnuclei Ultrasound Endoscopy Dermoscopy Fundus X-Ray Histopathology LV-UNet LGMSNet MBSNet CMUNeXt U-KAN Mobile U-ViT SwinUNETR RWKV-UNet CFPNet-M DCSAU-Net TA-Net G-CASCADE VMUNetV2 MUCM-Net Tinyunet ResU-KAN Swin-umambaD MDSA-UNet CE-Net UNeXt EMCAD UTNet TransResUNet MEGANet Polyp-PVT U-RWKV TransFuse CASCADE DDANet ULite MissFormer MambaUnet MMUNet HiFormer VMUNet AC-MambaSeg ERDUnet MedFormer CSCAUNet MALUNet UNetV2 U-Net++ MSLAU-Net DAEFormer H2Former FAT-Net SCUNet++ #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 UltraLight-VM-UNet #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 #95 #96 #97 #98 #99 #100 ESKNet AURA-Net MedVKAN CA-Net RollingUnet DDS-UNet DC-UNet BEFUnet LFU-Net SimpleUNet MultiResUNet Swin-umamba CSWin-UNet SwinUnet GH-UNet MFMSNet CENet LeViT-UNet Zig-RiR DoubleUNet TransAttUnet DA-TransUNet U-Net UNet3+ TransUnet CFM-UNet CPCANet MedT UTANet CaraNet AttU-Net H-vmunet CMU-Net TransNorm ScribFormer ResUNet++ PraNet EViT-UNet UCTransNet FCBFormer MSRFNet D-TrAttUnet MCA-UNet Perspective-Unet UNETR MERIT ColonSegNet DS-TransUNet UACANet ConvFormer CFFormer MT-UNet 79.62 84.79 77.02 9.41 70.78 72.22 67.82 61.22 61.28 55.27 58.07 56.29 51.87 66.09 50.36 51.36 59.87 58.39 59.07 63.62 53.54 56.14 51.29 50.66 58.92 41.67 52.09 47.32 48.46 60.72 40.85 9.21 42.37 39.96 43.76 40.27 46.97 44.92 38.73 66.47 40.05 39.73 36.48 34.51 37.20 34.91 35.20 34.82 33.19 31.44 30.65 30.40 28.23 28.39 8.79 25.62 9.52 9.52 14.92 22.64 20.18 8.48 20.77 20.32 20.23 8.87 8.20 23.10 20.75 18.84 19.20 17.68 19.63 15.96 18.42 15.50 18.16 18.50 19.41 22.30 16.96 17.34 6.98 7.16 17.33 16.03 13.43 14.49 14.46 13.34 13.39 12.12 7.08 12.52 7.28 10.72 12.63 11.76 10.13 8.76 79.18 78.74 74.05 68.19 72.11 74.93 75.92 60.87 70.75 71.60 56.54 56.29 61.86 70.89 9.52 55.48 58.55 62.92 56.04 73.33 53.39 55.88 51.29 50.68 59.13 63.35 50.27 47.32 36.58 9.81 46.97 68.85 36.22 45.65 8.49 35.88 48.94 47.19 39.51 61.83 49.71 42.08 36.46 37.81 36.86 35.96 35.43 9.42 34.06 31.96 31.74 30.03 30.21 28.11 23.53 8.49 9.52 9.52 9.10 22.64 29.87 8.48 20.52 20.29 20.23 8.87 29.22 24.81 21.21 18.58 18.36 16.66 19.34 23.46 18.51 22.46 18.48 18.46 18.17 22.52 17.06 17.17 19.45 15.83 17.12 16.16 12.96 14.61 15.05 11.52 13.39 12.32 7.08 12.28 7.28 10.74 12.62 12.10 8.21 8.92 97.84 90.80 79.08 78.86 71.77 58.24 55.97 60.37 65.26 67.23 55.73 56.29 62.58 46.67 59.89 59.20 57.07 62.60 54.75 50.73 46.72 45.86 48.14 50.68 59.13 73.57 47.84 46.62 46.87 11.01 42.48 9.21 41.36 47.16 41.40 39.76 47.06 46.67 39.38 42.64 51.24 25.92 36.58 38.89 38.42 30.98 30.44 9.42 33.93 30.05 29.26 31.19 30.15 28.82 40.76 18.93 9.52 9.52 36.43 22.64 18.57 8.48 20.79 19.44 19.63 8.87 8.20 19.15 20.57 18.19 19.07 18.16 18.31 22.01 17.34 22.84 17.87 18.50 17.43 7.26 16.85 17.31 19.11 20.54 17.26 16.18 14.33 14.55 13.35 12.83 13.37 12.06 7.08 12.56 7.28 10.60 12.46 12.34 10.00 7.76 96.96 76.60 83.56 80.54 62.44 68.02 20.27 61.72 62.37 66.31 58.27 54.38 61.93 9.50 67.78 50.24 58.75 52.32 56.67 17.38 53.52 50.03 50.68 49.74 51.69 9.40 52.41 47.32 59.54 11.15 40.26 69.26 41.16 46.51 41.74 36.26 43.09 42.40 38.79 9.50 41.89 35.74 35.91 29.09 35.25 33.42 36.89 9.42 33.66 32.01 25.79 29.64 30.60 28.02 32.42 36.88 9.52 9.52 51.70 22.64 28.02 8.48 20.92 20.48 20.07 8.87 21.61 24.80 20.69 17.39 18.58 18.30 18.43 21.67 18.28 7.32 18.59 18.41 19.50 7.26 16.97 15.69 19.88 18.66 17.33 16.18 14.76 14.22 15.31 13.88 12.75 12.23 7.08 12.56 20.52 10.80 12.65 12.29 10.09 8.76 76.12 89.51 79.75 60.39 63.35 63.51 60.58 61.72 52.32 59.60 53.85 55.53 62.12 9.50 9.52 58.34 60.39 45.82 53.28 20.95 53.54 47.93 51.29 50.68 58.76 9.40 49.82 47.32 53.69 9.52 39.50 73.10 38.45 46.45 42.93 37.55 8.71 41.31 37.00 9.50 44.35 31.99 34.44 31.36 35.28 34.12 32.95 9.42 33.98 31.50 20.02 27.55 28.46 27.83 27.62 28.37 9.52 9.52 42.92 22.29 20.46 8.48 20.32 20.48 19.88 20.31 12.05 24.81 20.93 18.08 19.53 18.54 18.18 7.53 17.27 7.32 17.92 18.30 18.74 16.20 16.74 16.22 18.33 20.20 17.33 15.75 14.31 14.43 15.93 13.63 12.86 12.15 7.08 12.51 21.00 10.80 12.62 11.65 10.13 8.78 99.23 90.21 82.13 88.85 50.87 62.62 61.65 61.72 79.91 36.03 52.01 56.29 60.43 57.21 49.23 57.85 44.17 67.55 52.07 22.82 43.46 53.00 46.72 50.49 50.25 85.61 51.90 45.65 45.34 79.09 36.98 9.21 45.18 27.56 40.77 38.54 44.47 43.24 38.48 9.50 19.05 40.71 36.58 25.45 38.42 33.85 24.36 9.42 31.02 29.45 35.54 16.65 23.58 9.00 8.79 8.49 9.52 13.21 22.06 22.23 26.01 30.92 21.28 20.48 19.36 8.87 35.75 7.39 20.38 14.99 19.60 17.29 17.67 24.92 18.07 22.86 18.29 18.39 6.97 21.07 15.29 14.84 15.30 7.16 16.86 15.31 14.12 13.96 14.23 14.02 13.39 11.69 7.08 12.56 7.28 10.53 12.65 11.65 10.13 9. 92.89 80.85 87.32 72.94 73.45 55.00 74.63 61.72 52.40 70.20 54.42 51.75 45.65 72.47 100.00 59.55 43.86 47.04 54.55 86.58 48.97 55.60 51.29 42.87 18.09 81.67 36.28 43.16 59.11 94.81 34.67 57.45 43.44 25.16 43.54 40.64 42.03 42.03 39.97 9.50 8.87 37.85 35.30 35.60 38.17 35.59 35.07 65.59 20.38 30.93 32.04 32.10 30.69 26.41 50.20 35.89 42.39 17.47 9.10 19.13 7.91 36.49 20.69 19.97 20.23 34.99 27.02 7.39 21.75 18.46 19.23 18.96 6.86 7.53 17.12 22.69 18.41 13.86 18.78 13.10 12.65 6.55 15.32 21.87 10.52 16.18 14.78 11.84 16.11 14.00 13.39 12.26 19.79 5.73 7.28 10.80 7.24 5.70 10.13 9.40 92.65 78.81 43.20 52.42 55.98 62.65 68.20 61.72 12.08 31.18 54.56 53.26 54.54 84.26 55.94 53.38 60.39 31.00 58.87 55.21 46.56 44.01 51.29 50.68 54.79 12.90 52.62 44.54 21.08 9.52 39.01 9.21 37.73 46.92 41.90 39.49 16.31 8.61 36.01 72.61 41.64 19.88 35.50 27.95 23.97 36.12 31.51 51.07 30.69 32.10 8.12 21.63 22.48 27.44 8.79 24.46 33.83 62.24 21.57 22.62 25.61 30.06 16.76 20.48 18.67 38.17 29.58 24.09 7.09 18.84 19.08 18.69 19.72 23.40 13.24 7.32 17.95 18.50 19.07 22.46 16.69 17.34 17.00 7.16 17.33 6.38 14.56 14.15 6.73 13.48 13.14 12.17 14.63 12.16 7.28 10.61 12.64 5.70 10.09 7.31 25.00 39.90 25.09 86.90 64.91 42.25 48.32 54.63 63.30 70.95 58.27 56.29 51.76 48.26 61.09 35.93 52.99 71.26 37.90 53.67 53.54 37.29 40.76 45.68 37.99 9.40 35.76 41.72 8.97 50.56 44.66 50.96 39.33 32.64 43.80 38.76 38.61 29.61 34.81 79.25 56.52 42.08 25.44 39.11 17.63 20.71 32.84 66.00 30.54 22.91 35.56 30.55 7.88 27.57 8.79 25.98 42.44 38.50 9.10 21.46 31.76 43.42 21.19 20.43 20.23 49.73 9.13 7.39 11.85 18.84 6.97 14.32 17.62 21.06 17.66 18.43 6.72 16.21 6.97 21.94 16.74 16.74 6.98 15.89 16.66 12.62 14.02 14.52 6.37 12.73 11.88 11.76 17.98 12.51 7.28 10.79 5.74 11.85 9.41 5.00 Avg 82.67 78.41 70.02 67.85 64.96 62.51 61.36 60.74 59.42 58.63 55.13 54.82 54.32 54.06 53.68 53.67 53.55 53.44 53.05 51.85 50.30 49.59 49.19 48.62 45.77 45.45 45.42 45.36 43.40 43.18 41.18 40.81 40.70 40.30 39.16 38.75 38.59 38.55 38.02 37.03 36.22 35.15 34.61 33.83 33.66 33.09 33.05 30.98 30.92 30.38 28.23 28.05 26.15 25.83 25.67 24.72 24.15 23.40 22.60 22.09 21.63 21.50 20.32 20.23 19.87 19.64 19.08 18.77 18.65 18.10 17.93 17.78 17.53 17.51 17.35 17.00 16.99 16.58 16.50 16.28 16.26 15.65 15.64 15.50 15.43 14.70 14.18 14.14 13.36 13.32 13.09 12.07 11.64 11.63 11.40 10.72 10.70 10.08 9.84 8.35 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 17: Per-dataset source ranking of 100 u-shape medical image segmentation networks with IoU Dermoscopy Endoscopy Fundus Rank Rank BUSI RWKV-UNet PraNet MobileUViT DA-TransUNet MEGANet TransResUNet MFMSNet CFFormer ESKNet CASCADE AURA-Net CaraNet Polyp-PVT EViT-UNet FAT-Net Swin-umamba CE-Net TA-Net UACANet G-CASCADE H2Former TransFuse TransUnet UTNet MCA-UNet LGMSNet TransNorm CMU-Net MERIT CENet UTANet CA-Net MBSNet MSLAU-Net CMUNeXt EMCAD TransAttUnet DDANet FCBFormer UCTransNet RollingUnet DDS-UNet MDSA-UNet ConvFormer MMUNet ResU-KAN VMUNet DCSAU-Net HiFormer CSCAUNet U-KAN MedFormer MT-UNet Perspective-Unet ERDUnet MedVKAN DoubleUNet U-Net++ ScribFormer LV-UNet GH-UNet CPCANet AC-MambaSeg MSRFNet AttU-Net U-RWKV MultiResUNet U-Net D-TrAttUnet CFPNet-M UNet3+ Swin-umambaD ULite DS-TransUNet DAEFormer ColonSegNet SwinUNETR ResUNet++ MissFormer UNeXt Tinyunet BEFUnet H-vmunet VMUNetV2 SCUNet++ DC-UNet MUCM-Net MedT MALUNet CFM-UNet LeViT-UNet UNetV2 LFU-Net #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 UltraLight-VM-UNet #95 #96 #97 #98 #99 #100 CSWin-UNet SimpleUNet Zig-RiR UNETR SwinUnet MambaUnet Ultrasound BUSBRA RWKV-UNet EViT-UNet CaraNet MFMSNet TA-Net FAT-Net UACANet FCBFormer MEGANet AURA-Net UTANet MedVKAN DA-TransUNet PraNet TransResUNet MCA-UNet ESKNet CE-Net MobileUViT CMUNeXt MBSNet Perspective-Unet HiFormer Swin-umamba RollingUnet H2Former CMU-Net DDANet TransAttUnet LGMSNet DCSAU-Net TransFuse MERIT MSRFNet MMUNet TransNorm AttU-Net UTNet ResU-KAN UNet3+ U-Net CA-Net GH-UNet TransUnet CENet U-KAN CSCAUNet DDS-UNet MultiResUNet MSLAU-Net UCTransNet MedFormer CFFormer CASCADE ConvFormer EMCAD G-CASCADE Polyp-PVT U-Net++ LV-UNet AC-MambaSeg D-TrAttUnet ScribFormer Tinyunet ERDUnet U-RWKV MT-UNet DoubleUNet MDSA-UNet CFPNet-M DS-TransUNet CPCANet SCUNet++ ULite VMUNetV2 ResUNet++ UNeXt MambaUnet DC-UNet Swin-umambaD MedT MUCM-Net DAEFormer MissFormer SwinUNETR ColonSegNet CSWin-UNet CFM-UNet UNetV2 LFU-Net BEFUnet H-vmunet SimpleUNet Zig-RiR MALUNet LeViT-UNet TNSCUI RWKV-UNet MEGANet MFMSNet TA-Net UACANet EViT-UNet CaraNet Swin-umamba FAT-Net UTANet CFFormer PraNet AURA-Net ESKNet MCA-UNet CE-Net Perspective-Unet MSLAU-Net MobileUViT FCBFormer TransResUNet DCSAU-Net DA-TransUNet H2Former MedFormer TransFuse TransNorm LGMSNet HiFormer TransAttUnet TransUnet MMUNet DDANet CSCAUNet CASCADE MERIT UTNet ResU-KAN CENet CA-Net MSRFNet CMU-Net MedVKAN G-CASCADE CMUNeXt U-KAN U-Net++ MBSNet GH-UNet Polyp-PVT ScribFormer MDSA-UNet AttU-Net DDS-UNet RollingUnet AC-MambaSeg ERDUnet U-Net ConvFormer UNet3+ UCTransNet LV-UNet VMUNet D-TrAttUnet MultiResUNet Swin-umambaD CFPNet-M DoubleUNet VMUNetV2 Tinyunet SCUNet++ DC-UNet DS-TransUNet DAEFormer ResUNet++ MissFormer CPCANet U-RWKV MT-UNet CFM-UNet UNeXt ColonSegNet EMCAD ULite SwinUNETR UNetV2 MedT CSWin-UNet MUCM-Net MALUNet H-vmunet Zig-RiR LeViT-UNet BEFUnet SimpleUNet LFU-Net UltraLight-VM-UNet UltraLight-VM-UNet UltraLight-VM-UNet MambaUnet UNETR SwinUnet ISIC2018 RWKV-UNet CFFormer MEGANet Swin-umamba PraNet TransResUNet TA-Net CE-Net CaraNet AURA-Net MFMSNet TransFuse EViT-UNet EMCAD HiFormer DA-TransUNet FAT-Net CENet MobileUViT MERIT UTANet UACANet MCA-UNet LGMSNet DDS-UNet FCBFormer TransNorm UTNet MSLAU-Net CASCADE LV-UNet G-CASCADE TransUnet DCSAU-Net ResU-KAN ESKNet Polyp-PVT H2Former DS-TransUNet DDANet AC-MambaSeg MMUNet ScribFormer UCTransNet MBSNet MedFormer RollingUnet TransAttUnet MDSA-UNet VMUNet MedVKAN VMUNetV2 CFPNet-M CA-Net ConvFormer MSRFNet MT-UNet UNet3+ CMUNeXt CSCAUNet U-Net Perspective-Unet CMU-Net U-KAN SwinUNETR U-RWKV AttU-Net U-Net++ D-TrAttUnet GH-UNet ERDUnet MultiResUNet UNeXt ResUNet++ ULite DAEFormer CPCANet MedT Tinyunet ColonSegNet MUCM-Net Swin-umambaD MissFormer CFM-UNet Zig-RiR CSWin-UNet H-vmunet SCUNet++ LeViT-UNet UNetV2 SwinUnet DoubleUNet MALUNet DC-UNet UNETR LFU-Net BEFUnet SimpleUNet MambaUnet UNETR SwinUnet VMUNet SkinCancer RWKV-UNet DA-TransUNet MSLAU-Net PraNet FCBFormer EMCAD MCA-UNet CaraNet TransNorm AURA-Net CMUNeXt MEGANet LGMSNet CASCADE GH-UNet U-KAN HiFormer TransUnet TransResUNet Polyp-PVT MFMSNet UTANet G-CASCADE MT-UNet MERIT Perspective-Unet CENet FAT-Net Swin-umamba UACANet CFFormer D-TrAttUnet DS-TransUNet MobileUViT CE-Net DDS-UNet LV-UNet CPCANet TA-Net AC-MambaSeg U-Net++ VMUNet H2Former EViT-UNet MedVKAN TransFuse MBSNet ULite UCTransNet BEFUnet ResUNet++ MUCM-Net SCUNet++ CA-Net CSCAUNet MultiResUNet MedFormer ConvFormer ResU-KAN UTNet ESKNet ColonSegNet CMU-Net UNet3+ LeViT-UNet AttU-Net CFPNet-M MDSA-UNet DCSAU-Net Tinyunet UNetV2 SwinUNETR CSWin-UNet RollingUnet UNeXt UltraLight-VM-UNet U-Net DAEFormer DDANet MMUNet MSRFNet U-RWKV CFM-UNet UNETR ScribFormer SwinUnet ERDUnet DoubleUNet H-vmunet TransAttUnet Zig-RiR DC-UNet VMUNetV2 SimpleUNet MALUNet MedT MissFormer LFU-Net Swin-umambaD MambaUnet Kvasir Swin-umamba VMUNet UACANet CFFormer RWKV-UNet FCBFormer PraNet CASCADE CENet MFMSNet MEGANet MambaUnet Swin-umambaD EMCAD DoubleUNet VMUNetV2 DS-TransUNet Polyp-PVT EViT-UNet MSLAU-Net G-CASCADE AURA-Net MERIT FAT-Net TransResUNet HiFormer TA-Net CaraNet MobileUViT CMU-Net UTANet DDANet TransFuse SCUNet++ MMUNet ResU-KAN Perspective-Unet MCA-UNet CSCAUNet DA-TransUNet ESKNet CE-Net U-KAN MedFormer MedVKAN H2Former LV-UNet DDS-UNet U-Net++ LGMSNet TransAttUnet RollingUnet CPCANet AC-MambaSeg MBSNet GH-UNet TransNorm DCSAU-Net DAEFormer TransUnet UTNet D-TrAttUnet MSRFNet CA-Net MultiResUNet UCTransNet ConvFormer MissFormer CMUNeXt U-Net MT-UNet SwinUNETR ColonSegNet ScribFormer MDSA-UNet UNetV2 AttU-Net CFPNet-M UNet3+ ResUNet++ U-RWKV Tinyunet DC-UNet BEFUnet CFM-UNet ULite ERDUnet UNeXt LeViT-UNet CSWin-UNet Zig-RiR MedT H-vmunet MUCM-Net MALUNet UNETR SimpleUNet SwinUnet LFU-Net UltraLight-VM-UNet Robotool MEGANet RWKV-UNet AURA-Net TA-Net EViT-UNet TransResUNet MFMSNet CE-Net PraNet UACANet UTANet CaraNet FAT-Net Swin-umamba MERIT CFFormer DA-TransUNet HiFormer MSLAU-Net DS-TransUNet TransFuse ESKNet U-Net UCTransNet UNet3+ AttU-Net MT-UNet Polyp-PVT DDANet CASCADE RollingUnet MobileUViT CMU-Net LV-UNet CENet TransAttUnet MCA-UNet Perspective-Unet UTNet TransNorm U-Net++ ConvFormer D-TrAttUnet G-CASCADE LGMSNet H2Former MedVKAN MSRFNet TransUnet MBSNet DDS-UNet CSCAUNet ColonSegNet CMUNeXt DCSAU-Net U-KAN CA-Net MedFormer GH-UNet FCBFormer ResU-KAN EMCAD MDSA-UNet MultiResUNet CFPNet-M MMUNet VMUNet DC-UNet LeViT-UNet DoubleUNet UNeXt AC-MambaSeg ScribFormer U-RWKV ResUNet++ Tinyunet SimpleUNet DAEFormer ULite CPCANet ERDUnet Swin-umambaD SCUNet++ Zig-RiR SwinUNETR UNetV2 MissFormer MedT CSWin-UNet BEFUnet CFM-UNet LFU-Net SwinUnet MUCM-Net H-vmunet UNETR VMUNetV2 MambaUnet UltraLight-VM-UNet MALUNet CHASE CMU-Net AttU-Net U-Net UNet3+ Perspective-Unet UCTransNet ESKNet ColonSegNet MT-UNet Swin-umamba FCBFormer TransResUNet UTNet AURA-Net ResUNet++ MedFormer TransUnet UTANet U-Net++ D-TrAttUnet DDANet AC-MambaSeg DA-TransUNet DoubleUNet RollingUnet CMUNeXt LGMSNet MobileUViT MBSNet U-RWKV TransNorm CFFormer MMUNet ResU-KAN U-KAN TransAttUnet MCA-UNet FAT-Net SCUNet++ H2Former ScribFormer DDS-UNet EViT-UNet CA-Net MEGANet DS-TransUNet TA-Net LeViT-UNet MFMSNet CPCANet RWKV-UNet CENet Tinyunet GH-UNet CE-Net CSCAUNet SwinUNETR DCSAU-Net MSLAU-Net DAEFormer MSRFNet DC-UNet MedVKAN CFPNet-M HiFormer ERDUnet UNeXt UNETR ULite LV-UNet VMUNet MedT ConvFormer SimpleUNet MissFormer CASCADE LFU-Net EMCAD MUCM-Net Zig-RiR G-CASCADE MambaUnet MDSA-UNet BEFUnet UltraLight-VM-UNet Swin-umambaD TransFuse SwinUnet MERIT CFM-UNet H-vmunet VMUNetV2 UNetV2 CaraNet PraNet UACANet Polyp-PVT MultiResUNet CSWin-UNet MALUNet DRIVE FCBFormer MT-UNet ColonSegNet UTNet ESKNet CMU-Net Swin-umamba UNet3+ RollingUnet D-TrAttUnet TransAttUnet AttU-Net CA-Net U-Net MedFormer MBSNet ResUNet++ SwinUNETR DDANet UNETR CMUNeXt UTANet MobileUViT ScribFormer Perspective-Unet UCTransNet DCSAU-Net DoubleUNet CFFormer LGMSNet TransUnet RWKV-UNet U-RWKV Tinyunet EViT-UNet MCA-UNet TransResUNet U-Net++ MMUNet CENet AURA-Net CFPNet-M ULite DA-TransUNet DC-UNet H2Former GH-UNet ResU-KAN U-KAN MedVKAN MSRFNet TransNorm LFU-Net DS-TransUNet ERDUnet SimpleUNet MedT MFMSNet AC-MambaSeg DDS-UNet MEGANet TA-Net FAT-Net CE-Net MSLAU-Net HiFormer UNeXt LV-UNet EMCAD LeViT-UNet MUCM-Net MERIT ConvFormer MDSA-UNet G-CASCADE Zig-RiR SCUNet++ CASCADE CSCAUNet DAEFormer PraNet CaraNet UACANet CPCANet MissFormer MultiResUNet TransFuse VMUNet BEFUnet UNetV2 MALUNet UltraLight-VM-UNet CSWin-UNet CFM-UNet Swin-umambaD VMUNetV2 MambaUnet H-vmunet SwinUnet Polyp-PVT Nuclear Cell MT-UNet TransAttUnet AURA-Net CA-Net UTANet ColonSegNet DA-TransUNet RollingUnet RWKV-UNet FCBFormer AttU-Net Swin-umamba UTNet CFFormer MEGANet ESKNet U-Net LGMSNet EViT-UNet UNet3+ UCTransNet TransUnet DDANet D-TrAttUnet MFMSNet ScribFormer CMU-Net CMUNeXt U-RWKV MBSNet MobileUViT ResU-KAN MCA-UNet DCSAU-Net MSRFNet TransResUNet ResUNet++ SwinUNETR TransNorm MedFormer TA-Net CENet U-Net++ U-KAN H2Former MedVKAN Tinyunet DDS-UNet ULite CFPNet-M CE-Net UNETR DS-TransUNet MMUNet ERDUnet GH-UNet Perspective-Unet CSCAUNet DC-UNet UNeXt FAT-Net MSLAU-Net MedT EMCAD MultiResUNet LV-UNet HiFormer AC-MambaSeg LeViT-UNet CASCADE ConvFormer MDSA-UNet LFU-Net G-CASCADE MERIT DAEFormer SCUNet++ TransFuse CPCANet CFM-UNet UNetV2 Zig-RiR DoubleUNet SimpleUNet MUCM-Net BEFUnet CSWin-UNet MALUNet UltraLight-VM-UNet H-vmunet PraNet CaraNet UACANet SwinUnet VMUNetV2 MissFormer Polyp-PVT Swin-umambaD VMUNet MambaUnet 36 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 18: Per-dataset source ranking of 100 u-shape medical image segmentation networks with IoU Rank DSB2018 MT-UNet DoubleUNet TransAttUnet DCSAU-Net UTNet D-TrAttUnet ESKNet AURA-Net LGMSNet DDANet RollingUnet CA-Net RWKV-UNet FCBFormer CENet U-Net ColonSegNet MSRFNet UNet3+ U-RWKV AttU-Net ScribFormer CMU-Net DS-TransUNet Swin-umamba MBSNet UCTransNet UTANet MobileUViT MedFormer CMUNeXt CFFormer Swin-umambaD UNETR MCA-UNet DA-TransUNet ULite ERDUnet MEGANet EViT-UNet Perspective-Unet TransUnet MedVKAN SwinUNETR MedT ResUNet++ U-Net++ TransResUNet CFPNet-M EMCAD TA-Net MFMSNet G-CASCADE CSCAUNet ResU-KAN DAEFormer Tinyunet LFU-Net U-KAN MMUNet H2Former LV-UNet TransNorm MambaUnet DDS-UNet DC-UNet CE-Net UNeXt AC-MambaSeg CASCADE FAT-Net GH-UNet #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 UltraLight-VM-UNet #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 #95 #96 #97 #98 #99 #100 SimpleUNet MSLAU-Net LeViT-UNet VMUNet MissFormer HiFormer MDSA-UNet CSWin-UNet H-vmunet SwinUnet CPCANet SCUNet++ TransFuse MUCM-Net CFM-UNet MALUNet MultiResUNet UNetV2 BEFUnet ConvFormer VMUNetV2 MERIT Zig-RiR Polyp-PVT CaraNet PraNet UACANet Histopathology Glas EMCAD RWKV-UNet CASCADE MSLAU-Net UTANet DDANet MERIT MBSNet CENet U-Net CaraNet AttU-Net MFMSNet MEGANet DA-TransUNet FAT-Net UNet3+ AURA-Net G-CASCADE PraNet RollingUnet ESKNet TransResUNet CFFormer H2Former CMUNeXt LGMSNet GH-UNet CMU-Net CA-Net TA-Net CSCAUNet Perspective-Unet DS-TransUNet UCTransNet CE-Net DC-UNet MSRFNet TransAttUnet ConvFormer MultiResUNet ColonSegNet EViT-UNet U-Net++ UTNet MT-UNet DDS-UNet UACANet MCA-UNet FCBFormer U-KAN HiFormer SCUNet++ Tinyunet ResU-KAN D-TrAttUnet TransUnet VMUNetV2 Swin-umamba CFPNet-M MobileUViT LV-UNet MDSA-UNet MedVKAN U-RWKV MedFormer ScribFormer SimpleUNet DoubleUNet DCSAU-Net TransNorm UNeXt ResUNet++ AC-MambaSeg TransFuse LFU-Net ULite MMUNet MUCM-Net LeViT-UNet ERDUnet DAEFormer MedT CFM-UNet SwinUNETR CPCANet Zig-RiR H-vmunet UNetV2 MissFormer CSWin-UNet MALUNet UNETR BEFUnet UltraLight-VM-UNet SwinUnet VMUNet MambaUnet Polyp-PVT Swin-umambaD Monusac MT-UNet RWKV-UNet UTANet CA-Net DDANet TransAttUnet UTNet EViT-UNet D-TrAttUnet AttU-Net ESKNet TransResUNet ScribFormer MBSNet MobileUViT UCTransNet AURA-Net RollingUnet MedFormer CMU-Net U-Net UNet3+ MSRFNet FCBFormer U-RWKV MEGANet ColonSegNet DoubleUNet H2Former MedVKAN CMUNeXt TransUnet MCA-UNet LGMSNet FAT-Net DC-UNet MFMSNet Perspective-Unet U-Net++ ERDUnet CFFormer DCSAU-Net TA-Net DA-TransUNet ResU-KAN MambaUnet Swin-umamba CENet DS-TransUNet VMUNet ResUNet++ TransNorm U-KAN GH-UNet CFPNet-M SimpleUNet MultiResUNet LV-UNet MSLAU-Net DDS-UNet AC-MambaSeg EMCAD CE-Net CSCAUNet HiFormer MERIT SwinUNETR MMUNet ULite UNeXt Tinyunet G-CASCADE MedT SCUNet++ LeViT-UNet MDSA-UNet ConvFormer UNETR CASCADE Swin-umambaD DAEFormer MUCM-Net CPCANet TransFuse LFU-Net UltraLight-VM-UNet CFM-UNet MALUNet UNetV2 MissFormer H-vmunet PraNet CaraNet UACANet VMUNetV2 CSWin-UNet SwinUnet BEFUnet Polyp-PVT Zig-RiR Covidquex AURA-Net RWKV-UNet CaraNet EViT-UNet TA-Net MEGANet PraNet CE-Net TransResUNet MFMSNet UACANet DA-TransUNet FAT-Net Swin-umamba TransAttUnet TransFuse UTANet ConvFormer TransUnet AttU-Net RollingUnet ESKNet DC-UNet UNet3+ LV-UNet CA-Net MobileUViT EMCAD CSCAUNet MBSNet U-Net DCSAU-Net MedVKAN CFFormer HiFormer UCTransNet DDANet UTNet MSLAU-Net MultiResUNet CMU-Net CASCADE LGMSNet AC-MambaSeg CMUNeXt ScribFormer TransNorm G-CASCADE ResU-KAN CENet MSRFNet MCA-UNet FCBFormer Perspective-Unet MERIT MT-UNet DDS-UNet H2Former Polyp-PVT DoubleUNet MedFormer GH-UNet D-TrAttUnet VMUNet U-KAN MDSA-UNet CFPNet-M MMUNet Tinyunet U-Net++ ResUNet++ ColonSegNet SCUNet++ U-RWKV Swin-umambaD DS-TransUNet ERDUnet VMUNetV2 UNeXt SimpleUNet CFM-UNet MedT DAEFormer MambaUnet ULite CPCANet LFU-Net SwinUNETR LeViT-UNet MissFormer CSWin-UNet UNetV2 MUCM-Net H-vmunet BEFUnet SwinUnet UltraLight-VM-UNet Zig-RiR MALUNet UNETR X-Ray Montgomery RWKV-UNet DA-TransUNet MEGANet TransAttUnet RollingUnet DDANet MT-UNet TransResUNet MobileUViT UNet3+ U-Net CFFormer H2Former MBSNet UTANet UTNet AttU-Net TransUnet UCTransNet MFMSNet AURA-Net ESKNet ColonSegNet ScribFormer CA-Net LGMSNet Swin-umamba ConvFormer CMUNeXt CMU-Net MSRFNet DoubleUNet FAT-Net U-Net++ EViT-UNet MCA-UNet Perspective-Unet U-RWKV DDS-UNet CE-Net TransNorm D-TrAttUnet CSCAUNet DC-UNet Tinyunet HiFormer FCBFormer GH-UNet MedVKAN U-KAN DCSAU-Net MedFormer ResUNet++ ResU-KAN MSLAU-Net LeViT-UNet UNeXt TA-Net EMCAD MDSA-UNet G-CASCADE UACANet CFPNet-M CENet CaraNet LV-UNet SCUNet++ Swin-umambaD CASCADE DS-TransUNet AC-MambaSeg ULite SwinUNETR Polyp-PVT PraNet SimpleUNet MMUNet VMUNet CPCANet MUCM-Net ERDUnet MedT TransFuse LFU-Net CFM-UNet Zig-RiR DAEFormer MultiResUNet MERIT UNETR H-vmunet MissFormer UNetV2 VMUNetV2 BEFUnet MALUNet SwinUnet UltraLight-VM-UNet CSWin-UNet MambaUnet MRI DCA DA-TransUNet UTANet EViT-UNet MFMSNet MEGANet ESKNet DDANet RWKV-UNet UTNet U-Net UNet3+ TransResUNet MSRFNet AURA-Net MT-UNet MobileUViT FAT-Net AttU-Net RollingUnet TransAttUnet CA-Net MBSNet TransUnet U-RWKV Swin-umamba D-TrAttUnet CMU-Net CMUNeXt ColonSegNet TA-Net EMCAD MedFormer DCSAU-Net Perspective-Unet ScribFormer UCTransNet H2Former CFFormer GH-UNet CE-Net MCA-UNet ResU-KAN LGMSNet TransNorm DDS-UNet MSLAU-Net MedVKAN FCBFormer CENet U-Net++ U-KAN CASCADE LV-UNet SwinUNETR ERDUnet DS-TransUNet CFPNet-M DoubleUNet AC-MambaSeg MMUNet SCUNet++ MedT HiFormer G-CASCADE Tinyunet CSCAUNet ConvFormer MambaUnet ResUNet++ UNeXt Swin-umambaD ULite MUCM-Net UNETR DAEFormer MDSA-UNet LeViT-UNet MERIT DC-UNet MissFormer CPCANet LFU-Net SimpleUNet VMUNet BEFUnet Zig-RiR CFM-UNet MultiResUNet MALUNet CaraNet TransFuse Promise RWKV-UNet FCBFormer MFMSNet EViT-UNet Perspective-Unet MEGANet TransResUNet U-KAN PraNet CMU-Net UTANet MedVKAN AttU-Net UNet3+ AURA-Net CaraNet CFFormer MedFormer UACANet CASCADE U-Net++ ResUNet++ TA-Net TransNorm ResU-KAN H2Former ESKNet AC-MambaSeg U-Net Swin-umamba CSCAUNet DDANet TransFuse GH-UNet FAT-Net UTNet LGMSNet UCTransNet TransUnet HiFormer MBSNet CE-Net D-TrAttUnet RollingUnet MCA-UNet DA-TransUNet MSRFNet DDS-UNet U-RWKV MT-UNet ConvFormer EMCAD CMUNeXt TransAttUnet DCSAU-Net CA-Net MobileUViT MERIT MultiResUNet MMUNet ScribFormer MSLAU-Net UNeXt LV-UNet MDSA-UNet ERDUnet DC-UNet Tinyunet Zig-RiR ColonSegNet G-CASCADE SwinUNETR Polyp-PVT CFM-UNet CENet CPCANet ULite DS-TransUNet H-vmunet LeViT-UNet MedT UNETR DoubleUNet MUCM-Net DAEFormer UNetV2 SCUNet++ VMUNetV2 MissFormer CSWin-UNet LFU-Net ACDC CENet Swin-umambaD DoubleUNet RWKV-UNet DDANet AttU-Net EViT-UNet FCBFormer G-CASCADE MSRFNet FAT-Net UTANet H2Former DS-TransUNet CA-Net MambaUnet Polyp-PVT Swin-umamba CASCADE MCA-UNet MSLAU-Net AURA-Net U-Net DA-TransUNet MFMSNet UNet3+ CMU-Net UCTransNet MBSNet PraNet UTNet ESKNet MEGANet CFFormer CaraNet VMUNet RollingUnet TransAttUnet Perspective-Unet TransResUNet MobileUViT DC-UNet MT-UNet ColonSegNet ScribFormer TransUnet MedFormer ConvFormer U-Net++ HiFormer GH-UNet CMUNeXt LGMSNet D-TrAttUnet SCUNet++ CE-Net TA-Net DCSAU-Net CFPNet-M UACANet U-KAN CSCAUNet VMUNetV2 MultiResUNet MedVKAN ResU-KAN DAEFormer CPCANet Tinyunet ResUNet++ U-RWKV TransFuse SimpleUNet DDS-UNet AC-MambaSeg LV-UNet MMUNet ERDUnet LeViT-UNet MDSA-UNet UNeXt MissFormer ULite BEFUnet TransNorm SwinUNETR CSWin-UNet MedT MUCM-Net EMCAD LFU-Net UltraLight-VM-UNet UltraLight-VM-UNet UltraLight-VM-UNet UNetV2 UNETR SwinUnet H-vmunet CFM-UNet Zig-RiR MALUNet MERIT BEFUnet SimpleUNet VMUNet MALUNet MambaUnet Swin-umambaD SwinUnet CFPNet-M UNetV2 PraNet UACANet VMUNetV2 SwinUnet CSWin-UNet H-vmunet Polyp-PVT CT Synapse CENet Perspective-Unet G-CASCADE CASCADE AURA-Net MEGANet DS-TransUNet DoubleUNet MSLAU-Net RWKV-UNet MFMSNet FCBFormer DDANet Swin-umamba AttU-Net CSCAUNet GH-UNet ScribFormer H2Former CFFormer PraNet MobileUViT ESKNet TransUnet UNet3+ D-TrAttUnet MSRFNet DA-TransUNet CA-Net RollingUnet TransAttUnet AC-MambaSeg U-KAN MT-UNet UTANet MBSNet TransResUNet UCTransNet ResU-KAN LGMSNet UACANet HiFormer UTNet CaraNet MambaUnet MERIT Polyp-PVT VMUNet CMU-Net ColonSegNet CMUNeXt U-Net DDS-UNet U-Net++ CPCANet SCUNet++ Tinyunet MedFormer EViT-UNet DC-UNet DCSAU-Net MedVKAN VMUNetV2 MMUNet DAEFormer TransFuse CE-Net MDSA-UNet U-RWKV MultiResUNet FAT-Net LeViT-UNet CFPNet-M LV-UNet ConvFormer MissFormer ERDUnet TA-Net ULite SimpleUNet ResUNet++ TransNorm SwinUNETR UNeXt CFM-UNet MedT MCA-UNet BEFUnet CSWin-UNet Swin-umambaD UNETR MUCM-Net H-vmunet MALUNet LFU-Net Zig-RiR UltraLight-VM-UNet SwinUnet UNetV2 EMCAD OCT Cystoidfluid UNet3+ Swin-umamba UTANet MMUNet H2Former Perspective-Unet FCBFormer D-TrAttUnet MedFormer EViT-UNet ColonSegNet ResU-KAN ResUNet++ CFFormer AC-MambaSeg MCA-UNet U-KAN RWKV-UNet GH-UNet U-Net++ DDS-UNet CMU-Net U-Net MFMSNet UCTransNet MSLAU-Net CENet TransResUNet UTNet TransNorm AttU-Net TransUnet CSCAUNet MEGANet MT-UNet MBSNet RollingUnet DDANet CE-Net ScribFormer FAT-Net U-RWKV TransAttUnet TA-Net MSRFNet AURA-Net CMUNeXt ULite MobileUViT CA-Net MedVKAN LGMSNet Tinyunet DCSAU-Net DA-TransUNet LeViT-UNet SwinUNETR UNeXt DS-TransUNet ERDUnet CPCANet LV-UNet HiFormer EMCAD ESKNet CASCADE ConvFormer MultiResUNet SCUNet++ MedT DAEFormer DoubleUNet G-CASCADE MDSA-UNet Zig-RiR UNETR MissFormer LFU-Net MERIT MUCM-Net CFM-UNet DC-UNet TransFuse VMUNet UltraLight-VM-UNet Swin-umambaD UNetV2 H-vmunet CSWin-UNet MALUNet SwinUnet UACANet BEFUnet PraNet MambaUnet CaraNet Polyp-PVT VMUNetV2 SimpleUNet CFPNet-M 37 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 19: Per-dataset target ranking of 100 u-shape medical image segmentation networks with IoU. Source Target. Rank BUSI BUS Swin-umamba EMCAD CENet G-CASCADE DA-TransUNet PraNet CASCADE TransNorm MCA-UNet CaraNet TransResUNet MEGANet VMUNet CFFormer Polyp-PVT TA-Net MSLAU-Net UACANet RWKV-UNet Swin-umambaD MERIT CE-Net TransUnet TransFuse AC-MambaSeg MFMSNet FCBFormer CPCANet EViT-UNet DS-TransUNet LGMSNet AURA-Net CSCAUNet H2Former FAT-Net Mobile U-ViT UTNet MedFormer DDS-UNet GH-UNet U-KAN UTANet ESKNet U-Net++ ERDUnet MMUNet MBSNet Perspective-Unet CA-Net CMU-Net H-vmunet TransAttUnet SwinUNETR LV-UNet SCUNet++ AttU-Net DoubleUNet MissFormer DAEFormer ResU-KAN VMUNetV2 RollingUnet U-Net HiFormer MDSA-UNet DDANet CFPNet-M MedVKAN ConvFormer MALUNet D-TrAttUnet MUCM-Net DCSAU-Net UNet3+ UNeXt ULite MSRFNet UNetV2 UCTransNet MT-UNet Tinyunet U-RWKV BEFUnet #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 UltraLight-VM-UNet #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 #95 #96 #97 #98 #99 #100 CSWin-UNet MedT CFM-UNet MultiResUNet DC-UNet SwinUnet ScribFormer UNETR ResUNet++ LeViT-UNet Zig-RiR ColonSegNet LFU-Net SimpleUNet MambaUnet CMUNeXt Ultrasound BUSBRA BUS MEGANet DoubleUNet CENet FCBFormer CASCADE Polyp-PVT TransResUNet U-Net++ MCA-UNet G-CASCADE Swin-umamba MedFormer EMCAD EViT-UNet MSLAU-Net CaraNet FAT-Net UACANet MissFormer RWKV-UNet Perspective-Unet AURA-Net CSCAUNet CPCANet UTANet VMUNetV2 MFMSNet DS-TransUNet Mobile U-ViT ESKNet Swin-umambaD TA-Net TransNorm ERDUnet HiFormer PraNet SwinUNETR DA-TransUNet DCSAU-Net DAEFormer U-KAN TransFuse MambaUnet TransUnet H2Former UTNet CE-Net TransAttUnet RollingUnet CMU-Net DDS-UNet LGMSNet H-vmunet MERIT GH-UNet CSWin-UNet ResU-KAN MBSNet CA-Net CFPNet-M ConvFormer UNetV2 SCUNet++ LV-UNet MedT MDSA-UNet ScribFormer MedVKAN AC-MambaSeg CMUNeXt UNeXt MUCM-Net MSRFNet U-RWKV CFM-UNet MMUNet MALUNet U-Net AttU-Net MT-UNet Zig-RiR UNet3+ DDANet UCTransNet D-TrAttUnet ResUNet++ CFFormer DC-UNet ULite Tinyunet SwinUnet MultiResUNet UltraLight-VM-UNet BEFUnet UNETR LFU-Net SimpleUNet LeViT-UNet ColonSegNet VMUNet TNSCUI TUCC MSLAU-Net MERIT EViT-UNet Polyp-PVT LGMSNet G-CASCADE CaraNet H2Former MEGANet Swin-umamba HiFormer TransNorm MCA-UNet DAEFormer VMUNetV2 LV-UNet PraNet MedFormer FCBFormer CASCADE RWKV-UNet CSCAUNet DDS-UNet ESKNet AC-MambaSeg ConvFormer TA-Net ResU-KAN CA-Net Swin-umambaD U-KAN GH-UNet UACANet MBSNet TransResUNet CFFormer RollingUnet VMUNet ERDUnet CMUNeXt CENet CE-Net DCSAU-Net DS-TransUNet UNetV2 DA-TransUNet Perspective-Unet TransFuse MMUNet MedT UTANet CMU-Net U-RWKV AURA-Net MissFormer MFMSNet TransAttUnet EMCAD MDSA-UNet CFPNet-M UCTransNet TransUnet CPCANet ResUNet++ U-Net ScribFormer UNet3+ Mobile U-ViT FAT-Net DDANet UTNet SwinUNETR DC-UNet MedVKAN Tinyunet SCUNet++ CFM-UNet D-TrAttUnet AttU-Net UNeXt MUCM-Net DoubleUNet MultiResUNet MSRFNet MALUNet U-Net++ CSWin-UNet MT-UNet BEFUnet ULite H-vmunet UltraLight-VM-UNet LeViT-UNet Zig-RiR LFU-Net SimpleUNet ColonSegNet MambaUnet UNETR SwinUnet Endoscopy Kvasir CVC300 PraNet RWKV-UNet UACANet MERIT MFMSNet TA-Net EViT-UNet UTANet DS-TransUNet CASCADE Swin-umamba EMCAD DoubleUNet TransFuse AURA-Net TransResUNet CaraNet HiFormer LV-UNet VMUNetV2 DDANet MEGANet CENet CFFormer Swin-umambaD MSLAU-Net MBSNet G-CASCADE CSCAUNet Perspective-Unet RollingUnet ESKNet GH-UNet MambaUnet CE-Net CPCANet ConvFormer VMUNet CMUNeXt SCUNet++ DDS-UNet CMU-Net UCTransNet Mobile U-ViT D-TrAttUnet FCBFormer DCSAU-Net LGMSNet ScribFormer MMUNet FAT-Net H2Former TransAttUnet MedFormer Polyp-PVT CA-Net AttU-Net U-KAN AC-MambaSeg UTNet MSRFNet ERDUnet TransUnet MCA-UNet MissFormer UNet3+ ResU-KAN CFPNet-M CSWin-UNet U-Net++ BEFUnet DA-TransUNet MultiResUNet Tinyunet U-Net TransNorm MDSA-UNet UNetV2 ColonSegNet CFM-UNet DAEFormer ResUNet++ MT-UNet MedVKAN DC-UNet Zig-RiR SwinUNETR UNeXt ULite UNETR LeViT-UNet H-vmunet U-RWKV MALUNet MedT SimpleUNet MUCM-Net LFU-Net SwinUnet UltraLight-VM-UNet Kvasir CVC-ClinicDB PraNet DS-TransUNet CASCADE Swin-umambaD EMCAD TransResUNet MFMSNet CFFormer DoubleUNet MEGANet RWKV-UNet Polyp-PVT MambaUnet LGMSNet UACANet G-CASCADE MERIT VMUNetV2 HiFormer VMUNet CaraNet ESKNet Swin-umamba AURA-Net MSRFNet FCBFormer CENet MBSNet ResU-KAN TransFuse Perspective-Unet TA-Net FAT-Net EViT-UNet MSLAU-Net TransAttUnet DDS-UNet UTANet CSCAUNet CE-Net AC-MambaSeg DDANet DA-TransUNet GH-UNet H2Former CMU-Net U-Net U-KAN Mobile U-ViT MedFormer UNet3+ MCA-UNet LV-UNet D-TrAttUnet RollingUnet DCSAU-Net MMUNet UCTransNet TransNorm SwinUNETR UTNet CPCANet MissFormer TransUnet SCUNet++ ResUNet++ AttU-Net UNetV2 CA-Net ColonSegNet CMUNeXt ConvFormer DAEFormer ScribFormer CFPNet-M U-Net++ MultiResUNet MDSA-UNet MT-UNet BEFUnet CSWin-UNet DC-UNet H-vmunet MedVKAN LeViT-UNet UNeXt Zig-RiR CFM-UNet ULite Tinyunet U-RWKV ERDUnet MedT UNETR MALUNet SimpleUNet MUCM-Net SwinUnet LFU-Net UltraLight-VM-UNet Dermoscopy ISIC2018 PH2 MSLAU-Net RWKV-UNet G-CASCADE MERIT MMUNet H2Former CMUNeXt UACANet MFMSNet MCA-UNet CFFormer MedVKAN GH-UNet LGMSNet MEGANet LV-UNet U-RWKV CaraNet TransFuse CFPNet-M MBSNet VMUNetV2 CASCADE U-Net++ Swin-umamba UTANet CSCAUNet Zig-RiR D-TrAttUnet ResU-KAN MT-UNet AC-MambaSeg PraNet CPCANet VMUNet MDSA-UNet FAT-Net TransResUNet MedFormer TA-Net MedT UTNet CENet CFM-UNet DS-TransUNet CE-Net U-Net ULite ERDUnet AURA-Net Polyp-PVT TransAttUnet Mobile U-ViT FCBFormer ESKNet EViT-UNet SwinUNETR EMCAD Perspective-Unet H-vmunet UCTransNet ConvFormer DDANet TransUnet MissFormer Swin-umambaD U-KAN UNet3+ CSWin-UNet MUCM-Net CMU-Net MSRFNet SwinUnet TransNorm Tinyunet RollingUnet DA-TransUNet DAEFormer DCSAU-Net HiFormer SCUNet++ ScribFormer CA-Net MultiResUNet UNeXt UNetV2 SimpleUNet DDS-UNet MambaUnet ColonSegNet LFU-Net MALUNet UNETR LeViT-UNet ResUNet++ DC-UNet DoubleUNet AttU-Net UltraLight-VM-UNet BEFUnet Fundus CHASE Stare RWKV-UNet DS-TransUNet SwinUNETR FCBFormer MCA-UNet UNETR DA-TransUNet MSRFNet DoubleUNet Swin-umamba EViT-UNet CENet TransNorm ERDUnet VMUNet AC-MambaSeg DAEFormer CFFormer MissFormer ULite TransUnet FAT-Net AURA-Net TransResUNet HiFormer TransAttUnet UNet3+ CMUNeXt MT-UNet CA-Net MedT EMCAD LV-UNet MFMSNet AttU-Net G-CASCADE CFPNet-M CSCAUNet MedVKAN MMUNet DDANet ResU-KAN D-TrAttUnet H2Former SCUNet++ RollingUnet CASCADE Mobile U-ViT U-Net DC-UNet LGMSNet MSLAU-Net U-KAN CMU-Net Perspective-Unet UCTransNet MUCM-Net MEGANet TA-Net MBSNet CPCANet UTNet GH-UNet UTANet DDS-UNet U-RWKV UNeXt Tinyunet ResUNet++ MedFormer ColonSegNet DCSAU-Net U-Net++ CE-Net LFU-Net MambaUnet BEFUnet ScribFormer ESKNet SimpleUNet SwinUnet Swin-umambaD UltraLight-VM-UNet MERIT MDSA-UNet VMUNetV2 TransFuse Zig-RiR H-vmunet LeViT-UNet UNetV2 ConvFormer CFM-UNet Polyp-PVT CaraNet UACANet PraNet CSWin-UNet MultiResUNet MALUNet X-Ray Histopathology Montgomery NIH-test Monusac Tnbcnuclei MEGANet TransResUNet CaraNet DA-TransUNet PraNet MFMSNet Swin-umambaD TransUnet TransNorm RWKV-UNet FAT-Net Swin-umamba UACANet TransFuse HiFormer AURA-Net CE-Net CFFormer LV-UNet MSLAU-Net AC-MambaSeg G-CASCADE VMUNet TA-Net DoubleUNet Perspective-Unet CASCADE Polyp-PVT MCA-UNet MUCM-Net DS-TransUNet LGMSNet H-vmunet UTANet SwinUNETR FCBFormer DDS-UNet VMUNetV2 MERIT UNet3+ UCTransNet ResU-KAN CSCAUNet CMU-Net EMCAD Mobile U-ViT MALUNet ESKNet AttU-Net U-Net D-TrAttUnet CENet MMUNet MissFormer CFM-UNet U-KAN UTNet SimpleUNet SCUNet++ UNetV2 Zig-RiR CSWin-UNet CMUNeXt Tinyunet UNeXt LeViT-UNet UltraLight-VM-UNet DAEFormer SwinUnet ScribFormer MBSNet RollingUnet GH-UNet CA-Net H2Former BEFUnet DCSAU-Net MDSA-UNet LFU-Net CPCANet UNETR U-Net++ DDANet MultiResUNet MT-UNet ERDUnet U-RWKV CFPNet-M MSRFNet ColonSegNet ULite ResUNet++ ConvFormer MedVKAN TransAttUnet MedT MedFormer DC-UNet MambaUnet EViT-UNet TA-Net CENet U-Net++ EMCAD G-CASCADE UNetV2 CSWin-UNet DAEFormer DA-TransUNet MedVKAN VMUNet DS-TransUNet MFMSNet SwinUnet GH-UNet CMUNeXt MERIT FCBFormer DCSAU-Net MDSA-UNet MissFormer AC-MambaSeg CA-Net TransNorm RWKV-UNet MEGANet DDS-UNet Swin-umambaD LeViT-UNet PraNet MALUNet Swin-umamba U-KAN CMU-Net CASCADE CPCANet H-vmunet MMUNet VMUNetV2 CSCAUNet ESKNet CFPNet-M UltraLight-VM-UNet ConvFormer Perspective-Unet SCUNet++ TransResUNet UCTransNet TransUnet ERDUnet MambaUnet Tinyunet CFFormer CaraNet SwinUNETR D-TrAttUnet UNeXt TransFuse UNETR Polyp-PVT HiFormer MCA-UNet CFM-UNet CE-Net ULite UTNet MUCM-Net Mobile U-ViT ResU-KAN MSLAU-Net MedT MedFormer AURA-Net LFU-Net LGMSNet BEFUnet SimpleUNet EViT-UNet ResUNet++ UNet3+ FAT-Net MBSNet LV-UNet H2Former TransAttUnet Zig-RiR DoubleUNet MT-UNet UACANet U-RWKV U-Net MultiResUNet UTANet DDANet ColonSegNet RollingUnet MSRFNet DC-UNet AttU-Net ScribFormer 38 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 20: Per-dataset source ranking of 100 u-shape medical image segmentation networks with U-Score Rank BUSI LGMSNet CMUNeXt MBSNet LV-UNet Mobile U-ViT MDSA-UNet U-RWKV U-KAN DCSAU-Net CFPNet-M ULite RWKV-UNet Polyp-PVT CE-Net UTNet TA-Net DDANet ResU-KAN G-CASCADE MultiResUNet TransFuse TransResUNet EMCAD MEGANet Swin-umambaD CASCADE UNeXt Tinyunet ERDUnet SwinUNETR HiFormer MedFormer MMUNet VMUNet U-Net++ H2Former MUCM-Net CSCAUNet AC-MambaSeg FAT-Net MSLAU-Net ESKNet VMUNetV2 MedVKAN AURA-Net CA-Net MissFormer DAEFormer RollingUnet DC-UNet BEFUnet DDS-UNet SCUNet++ DoubleUNet Swin-umamba TransAttUnet MFMSNet GH-UNet CENet ScribFormer TransUnet ColonSegNet AttU-Net U-Net DA-TransUNet CaraNet UTANet UNet3+ ResUNet++ H-vmunet CPCANet PraNet CMU-Net TransNorm MedT EViT-UNet MSRFNet UCTransNet FCBFormer D-TrAttUnet MCA-UNet UACANet MERIT ConvFormer Perspective-Unet MALUNet DS-TransUNet CFFormer SimpleUNet LFU-Net #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 UltraLight-VM-UNet #92 #93 #94 #95 #96 #97 #98 #99 #100 MT-UNet MambaUnet UNetV2 LeViT-UNet SwinUnet Zig-RiR CSWin-UNet CFM-UNet UNETR Ultrasound BUSBRA CMUNeXt LGMSNet MBSNet LV-UNet Tinyunet Mobile U-ViT U-RWKV DCSAU-Net U-KAN CFPNet-M ULite MDSA-UNet MultiResUNet RWKV-UNet UNeXt CE-Net ResU-KAN DDANet TA-Net UTNet Polyp-PVT TransResUNet TransFuse G-CASCADE MEGANet MambaUnet EMCAD HiFormer VMUNetV2 ERDUnet MUCM-Net CASCADE MedFormer MMUNet Swin-umambaD DC-UNet U-Net++ CSCAUNet H2Former AC-MambaSeg FAT-Net MedVKAN MSLAU-Net ESKNet SCUNet++ AURA-Net CA-Net RollingUnet DDS-UNet MissFormer DAEFormer DoubleUNet SwinUNETR Swin-umamba TransAttUnet GH-UNet MFMSNet AttU-Net U-Net ScribFormer CENet ResUNet++ MedT TransUnet UNet3+ DA-TransUNet UTANet CaraNet CMU-Net PraNet CPCANet TransNorm EViT-UNet MSRFNet UCTransNet FCBFormer UNetV2 D-TrAttUnet MCA-UNet ColonSegNet CSWin-UNet UACANet MERIT Perspective-Unet ConvFormer CFM-UNet DS-TransUNet CFFormer SimpleUNet LFU-Net MALUNet TNSCUI LGMSNet LV-UNet CMUNeXt MBSNet Tinyunet Mobile U-ViT DCSAU-Net U-KAN CFPNet-M MDSA-UNet U-RWKV RWKV-UNet ResU-KAN MultiResUNet CE-Net DDANet TA-Net UTNet Polyp-PVT UNeXt VMUNetV2 G-CASCADE Swin-umambaD ULite TransFuse TransResUNet MEGANet ERDUnet MedFormer HiFormer CASCADE MMUNet DC-UNet SwinUNETR U-Net++ VMUNet CSCAUNet MissFormer AC-MambaSeg H2Former EMCAD MSLAU-Net FAT-Net MedVKAN SCUNet++ ESKNet UNetV2 DAEFormer AURA-Net CA-Net RollingUnet MUCM-Net DDS-UNet MALUNet DoubleUNet CFM-UNet Swin-umamba CSWin-UNet TransAttUnet GH-UNet MFMSNet ResUNet++ ScribFormer AttU-Net U-Net CENet ColonSegNet TransUnet UNet3+ MedT DA-TransUNet UTANet CaraNet CMU-Net PraNet TransNorm CPCANet EViT-UNet MSRFNet UCTransNet FCBFormer D-TrAttUnet MCA-UNet UACANet MERIT Perspective-Unet ConvFormer DS-TransUNet CFFormer LFU-Net SimpleUNet UltraLight-VM-UNet UltraLight-VM-UNet MT-UNet LeViT-UNet BEFUnet VMUNet SwinUnet Zig-RiR H-vmunet UNETR MambaUnet MT-UNet LeViT-UNet BEFUnet SwinUnet Zig-RiR H-vmunet UNETR Dermoscopy Endoscopy Fundus ISIC2018 LV-UNet LGMSNet Mobile U-ViT MBSNet DCSAU-Net CMUNeXt CFPNet-M MDSA-UNet RWKV-UNet U-RWKV CE-Net UNeXt TA-Net SwinUNETR U-KAN ResU-KAN UTNet Polyp-PVT DDANet ULite EMCAD TransFuse G-CASCADE VMUNetV2 TransResUNet MEGANet Tinyunet MultiResUNet HiFormer CASCADE MUCM-Net MedFormer MMUNet VMUNet ERDUnet AC-MambaSeg H2Former FAT-Net MSLAU-Net CSCAUNet U-Net++ ESKNet AURA-Net MedVKAN Swin-umambaD RollingUnet CA-Net DDS-UNet DAEFormer MissFormer Swin-umamba Zig-RiR MFMSNet TransAttUnet CENet ScribFormer SCUNet++ TransUnet MedT U-Net GH-UNet ResUNet++ DA-TransUNet CaraNet AttU-Net UTANet ColonSegNet UNet3+ CFM-UNet CSWin-UNet PraNet TransNorm LeViT-UNet UNetV2 CMU-Net EViT-UNet CPCANet MSRFNet UCTransNet H-vmunet FCBFormer MCA-UNet D-TrAttUnet UACANet MERIT ConvFormer Perspective-Unet DS-TransUNet CFFormer LFU-Net SimpleUNet MALUNet UltraLight-VM-UNet MambaUnet MT-UNet DC-UNet BEFUnet SwinUnet DoubleUNet UNETR SkinCancer LGMSNet CMUNeXt LV-UNet U-KAN ULite MUCM-Net Mobile U-ViT MBSNet RWKV-UNet Polyp-PVT Tinyunet CE-Net G-CASCADE EMCAD CFPNet-M TA-Net MEGANet MultiResUNet TransResUNet MDSA-UNet SwinUNETR CASCADE UNeXt DCSAU-Net ResU-KAN HiFormer TransFuse UTNet UltraLight-VM-UNet LeViT-UNet U-RWKV VMUNet UNetV2 MedFormer U-Net++ BEFUnet AC-MambaSeg MSLAU-Net DDANet FAT-Net H2Former SCUNet++ CSCAUNet AURA-Net MedVKAN MMUNet ESKNet CA-Net DDS-UNet DAEFormer CSWin-UNet RollingUnet Swin-umamba GH-UNet ColonSegNet ResUNet++ MFMSNet CENet TransUnet CFM-UNet DA-TransUNet CaraNet UTANet AttU-Net CPCANet UNet3+ TransNorm PraNet U-Net CMU-Net UNETR EViT-UNet FCBFormer UCTransNet D-TrAttUnet SwinUnet MSRFNet MCA-UNet ScribFormer UACANet MERIT Perspective-Unet ConvFormer DS-TransUNet ERDUnet CFFormer SimpleUNet LFU-Net MALUNet MT-UNet MambaUnet VMUNetV2 Swin-umambaD DC-UNet MissFormer Zig-RiR DoubleUNet MedT H-vmunet TransAttUnet Kvasir LV-UNet LGMSNet Mobile U-ViT MambaUnet MBSNet U-KAN CMUNeXt DCSAU-Net VMUNetV2 RWKV-UNet Swin-umambaD SwinUNETR ResU-KAN Polyp-PVT DDANet TA-Net CFPNet-M G-CASCADE CE-Net MultiResUNet EMCAD MDSA-UNet UTNet TransFuse MEGANet TransResUNet U-RWKV CASCADE HiFormer MedFormer VMUNet MMUNet UNetV2 MissFormer U-Net++ CSCAUNet SCUNet++ Tinyunet AC-MambaSeg MSLAU-Net H2Former FAT-Net DAEFormer MedVKAN ESKNet AURA-Net RollingUnet CA-Net DDS-UNet DC-UNet DoubleUNet BEFUnet Swin-umamba ColonSegNet TransAttUnet MFMSNet GH-UNet CENet U-Net ULite TransUnet ResUNet++ ScribFormer DA-TransUNet UTANet AttU-Net CaraNet CPCANet PraNet CMU-Net UNet3+ TransNorm EViT-UNet ERDUnet CFM-UNet MSRFNet FCBFormer UCTransNet D-TrAttUnet MCA-UNet UACANet MERIT Perspective-Unet ConvFormer UNeXt DS-TransUNet CFFormer SimpleUNet LFU-Net MALUNet MUCM-Net UltraLight-VM-UNet MT-UNet LeViT-UNet SwinUnet Zig-RiR CSWin-UNet MedT H-vmunet UNETR Robotool LV-UNet LGMSNet Mobile U-ViT MBSNet CMUNeXt RWKV-UNet CFPNet-M CE-Net U-KAN TA-Net UNeXt DCSAU-Net MDSA-UNet Tinyunet DDANet Polyp-PVT UTNet U-RWKV MultiResUNet TransResUNet TransFuse MEGANet ResU-KAN G-CASCADE HiFormer EMCAD CASCADE SimpleUNet ULite LeViT-UNet DC-UNet MedFormer U-Net++ MMUNet FAT-Net MSLAU-Net VMUNet CSCAUNet H2Former ESKNet AURA-Net MedVKAN AC-MambaSeg RollingUnet Swin-umambaD ERDUnet CA-Net DDS-UNet DAEFormer SwinUNETR Swin-umamba ColonSegNet SCUNet++ DoubleUNet TransAttUnet MFMSNet U-Net AttU-Net GH-UNet CENet UNet3+ ResUNet++ Zig-RiR DA-TransUNet UTANet CaraNet TransUnet ScribFormer PraNet CMU-Net TransNorm EViT-UNet MSRFNet UCTransNet CPCANet FCBFormer D-TrAttUnet UNetV2 MCA-UNet UACANet MERIT ConvFormer Perspective-Unet MissFormer DS-TransUNet CFFormer LFU-Net MALUNet MUCM-Net UltraLight-VM-UNet MT-UNet MambaUnet VMUNetV2 BEFUnet SwinUnet MedT CSWin-UNet CFM-UNet H-vmunet UNETR CHASE LGMSNet CMUNeXt MBSNet U-RWKV Tinyunet Mobile U-ViT U-KAN UNeXt ULite LV-UNet SwinUNETR CFPNet-M DCSAU-Net SimpleUNet DDANet UTNet ResU-KAN RWKV-UNet LFU-Net CE-Net TA-Net TransResUNet LeViT-UNet MEGANet MedFormer MUCM-Net DC-UNet MMUNet ERDUnet U-Net++ AC-MambaSeg HiFormer SCUNet++ H2Former CSCAUNet DAEFormer VMUNet EMCAD MissFormer FAT-Net ESKNet CASCADE MSLAU-Net AURA-Net MedVKAN G-CASCADE RollingUnet CA-Net MambaUnet MDSA-UNet DDS-UNet Zig-RiR DoubleUNet ColonSegNet Swin-umamba ResUNet++ TransAttUnet MedT U-Net AttU-Net GH-UNet ScribFormer UNETR MFMSNet UNet3+ TransUnet CENet UltraLight-VM-UNet DA-TransUNet UTANet CPCANet BEFUnet CMU-Net TransNorm Swin-umambaD EViT-UNet MSRFNet UCTransNet FCBFormer D-TrAttUnet TransFuse MCA-UNet Perspective-Unet SwinUnet ConvFormer DS-TransUNet CFFormer MT-UNet MALUNet MultiResUNet VMUNetV2 Polyp-PVT UNetV2 CSWin-UNet CFM-UNet H-vmunet CaraNet PraNet MERIT UACANet DRIVE Tinyunet ULite LFU-Net SimpleUNet LGMSNet CMUNeXt MBSNet U-RWKV CFPNet-M UNeXt SwinUNETR LV-UNet Mobile U-ViT DCSAU-Net U-KAN MUCM-Net RWKV-UNet ResU-KAN DDANet UTNet MDSA-UNet CE-Net TA-Net DC-UNet TransResUNet ERDUnet LeViT-UNet MEGANet MedFormer EMCAD MMUNet HiFormer G-CASCADE U-Net++ AC-MambaSeg H2Former MedVKAN ESKNet CASCADE MSLAU-Net FAT-Net CA-Net AURA-Net RollingUnet SCUNet++ Zig-RiR DDS-UNet CSCAUNet DAEFormer DoubleUNet ColonSegNet MedT Swin-umamba ResUNet++ TransAttUnet UNETR GH-UNet ScribFormer U-Net AttU-Net MFMSNet CENet UNet3+ TransUnet DA-TransUNet UTANet CMU-Net TransNorm EViT-UNet MSRFNet UCTransNet MultiResUNet FCBFormer D-TrAttUnet CaraNet MissFormer PraNet MCA-UNet Perspective-Unet TransFuse MERIT ConvFormer CPCANet DS-TransUNet UACANet CFFormer MT-UNet MALUNet UltraLight-VM-UNet MambaUnet VMUNetV2 Swin-umambaD Polyp-PVT UNetV2 BEFUnet VMUNet SwinUnet CSWin-UNet CFM-UNet H-vmunet Nuclear Cell Tinyunet ULite LGMSNet UNeXt CMUNeXt U-RWKV MBSNet LV-UNet CFPNet-M SwinUNETR Mobile U-ViT LFU-Net DCSAU-Net U-KAN RWKV-UNet MDSA-UNet ResU-KAN DDANet UTNet MultiResUNet CE-Net TA-Net MEGANet TransResUNet DC-UNet SimpleUNet EMCAD LeViT-UNet ERDUnet G-CASCADE MedFormer MUCM-Net MMUNet HiFormer TransFuse UNetV2 CASCADE U-Net++ CSCAUNet H2Former AC-MambaSeg ESKNet MedVKAN MSLAU-Net FAT-Net DAEFormer MALUNet SCUNet++ AURA-Net CA-Net RollingUnet Zig-RiR BEFUnet DDS-UNet ColonSegNet MedT CFM-UNet Swin-umamba ResUNet++ TransAttUnet CSWin-UNet UNETR DoubleUNet GH-UNet ScribFormer AttU-Net U-Net MFMSNet CENet TransUnet UNet3+ DA-TransUNet UTANet CMU-Net TransNorm CPCANet EViT-UNet MSRFNet UCTransNet FCBFormer UltraLight-VM-UNet D-TrAttUnet MCA-UNet Perspective-Unet MERIT ConvFormer DS-TransUNet CFFormer MT-UNet MambaUnet VMUNetV2 Swin-umambaD Polyp-PVT MissFormer VMUNet SwinUnet H-vmunet CaraNet PraNet UACANet 39 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 21: Per-dataset source ranking of 100 u-shape medical image segmentation networks with U-Score MRI Rank DSB2018 LGMSNet ULite U-RWKV MBSNet CMUNeXt Tinyunet DCSAU-Net LFU-Net Mobile U-ViT LV-UNet CFPNet-M SwinUNETR UNeXt RWKV-UNet U-KAN DDANet UTNet SimpleUNet Swin-umambaD MambaUnet #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 UltraLight-VM-UNet #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 #95 #96 #97 #98 #99 #100 ResU-KAN TA-Net G-CASCADE ERDUnet EMCAD MEGANet TransResUNet CE-Net MedFormer DC-UNet MDSA-UNet LeViT-UNet MMUNet U-Net++ CASCADE CSCAUNet DAEFormer ESKNet H2Former MedVKAN AC-MambaSeg AURA-Net MissFormer VMUNet CA-Net HiFormer RollingUnet FAT-Net MSLAU-Net SwinUnet DDS-UNet SCUNet++ DoubleUNet TransFuse ColonSegNet CSWin-UNet MedT Swin-umamba TransAttUnet ResUNet++ UNETR ScribFormer U-Net AttU-Net CENet UNet3+ MFMSNet TransUnet GH-UNet DA-TransUNet UTANet H-vmunet MUCM-Net CMU-Net MALUNet TransNorm MSRFNet EViT-UNet UCTransNet CPCANet FCBFormer D-TrAttUnet MCA-UNet Perspective-Unet CFM-UNet DS-TransUNet CFFormer MT-UNet MultiResUNet VMUNetV2 Polyp-PVT UNetV2 BEFUnet Zig-RiR CaraNet PraNet UACANet MERIT ConvFormer Histopathology Glas Tinyunet LGMSNet LV-UNet MBSNet CMUNeXt SimpleUNet UNeXt U-RWKV CFPNet-M LFU-Net ULite U-KAN Mobile U-ViT MDSA-UNet MUCM-Net DCSAU-Net MultiResUNet RWKV-UNet DDANet VMUNetV2 ResU-KAN CE-Net UTNet TA-Net G-CASCADE EMCAD DC-UNet TransResUNet MEGANet CASCADE SwinUNETR HiFormer TransFuse MedFormer LeViT-UNet U-Net++ ERDUnet CSCAUNet H2Former SCUNet++ MMUNet MSLAU-Net AC-MambaSeg FAT-Net ESKNet MedVKAN AURA-Net CA-Net RollingUnet DAEFormer DDS-UNet UNetV2 Zig-RiR DoubleUNet ColonSegNet Swin-umamba TransAttUnet CFM-UNet GH-UNet ResUNet++ U-Net AttU-Net MFMSNet CENet ScribFormer MedT UNet3+ TransUnet DA-TransUNet UTANet CaraNet CMU-Net PraNet MissFormer TransNorm EViT-UNet MSRFNet H-vmunet CPCANet UCTransNet FCBFormer D-TrAttUnet MCA-UNet MERIT UACANet ConvFormer Perspective-Unet DS-TransUNet CFFormer MALUNet MT-UNet UltraLight-VM-UNet MambaUnet Swin-umambaD Polyp-PVT BEFUnet VMUNet SwinUnet CSWin-UNet UNETR Monusac MBSNet U-RWKV LGMSNet CMUNeXt SimpleUNet LV-UNet Mobile U-ViT ULite CFPNet-M UNeXt Tinyunet DCSAU-Net U-KAN MambaUnet SwinUNETR RWKV-UNet DDANet UTNet MultiResUNet ResU-KAN TA-Net CE-Net TransResUNet MUCM-Net DC-UNet MEGANet MDSA-UNet ERDUnet EMCAD MedFormer G-CASCADE HiFormer VMUNet LeViT-UNet U-Net++ MMUNet Swin-umambaD H2Former AC-MambaSeg CSCAUNet FAT-Net MedVKAN ESKNet MSLAU-Net CASCADE LFU-Net CA-Net AURA-Net RollingUnet SCUNet++ TransFuse DAEFormer UltraLight-VM-UNet DDS-UNet DoubleUNet ColonSegNet Swin-umamba TransAttUnet ResUNet++ MedT ScribFormer AttU-Net U-Net GH-UNet MFMSNet MALUNet CENet UNet3+ TransUnet UTANet DA-TransUNet UNETR CMU-Net TransNorm EViT-UNet UNetV2 MSRFNet CPCANet UCTransNet FCBFormer CFM-UNet D-TrAttUnet MCA-UNet MissFormer Perspective-Unet MERIT ConvFormer DS-TransUNet CFFormer MT-UNet VMUNetV2 Polyp-PVT BEFUnet SwinUnet Zig-RiR CSWin-UNet H-vmunet CaraNet PraNet UACANet Covidquex LV-UNet MBSNet LGMSNet CMUNeXt Tinyunet Mobile U-ViT DCSAU-Net CFPNet-M U-RWKV MultiResUNet U-KAN RWKV-UNet MDSA-UNet UNeXt CE-Net SimpleUNet TA-Net DDANet ResU-KAN UTNet TransFuse Polyp-PVT G-CASCADE DC-UNet TransResUNet EMCAD MEGANet ULite Swin-umambaD VMUNetV2 HiFormer CASCADE MambaUnet MedFormer ERDUnet MMUNet AC-MambaSeg CSCAUNet VMUNet U-Net++ LFU-Net FAT-Net H2Former MSLAU-Net MedVKAN ESKNet SCUNet++ AURA-Net SwinUNETR CA-Net RollingUnet DAEFormer DDS-UNet DoubleUNet Swin-umamba CFM-UNet TransAttUnet ColonSegNet ResUNet++ MFMSNet AttU-Net U-Net GH-UNet ScribFormer MedT TransUnet CENet UNet3+ DA-TransUNet CaraNet UTANet PraNet CMU-Net TransNorm EViT-UNet MSRFNet CPCANet LeViT-UNet UCTransNet MissFormer FCBFormer D-TrAttUnet MCA-UNet UACANet ConvFormer MERIT Perspective-Unet DS-TransUNet CFFormer MALUNet MUCM-Net UltraLight-VM-UNet MT-UNet UNetV2 BEFUnet SwinUnet Zig-RiR CSWin-UNet H-vmunet UNETR X-Ray Montgomery Tinyunet LGMSNet MBSNet CMUNeXt UNeXt U-RWKV LV-UNet Mobile U-ViT ULite SimpleUNet U-KAN CFPNet-M DCSAU-Net MDSA-UNet SwinUNETR MUCM-Net RWKV-UNet DDANet UTNet ResU-KAN CE-Net TA-Net Swin-umambaD LeViT-UNet TransResUNet MEGANet DC-UNet LFU-Net Polyp-PVT G-CASCADE EMCAD HiFormer MedFormer CASCADE U-Net++ ERDUnet CSCAUNet H2Former MMUNet VMUNet AC-MambaSeg SCUNet++ TransFuse FAT-Net MSLAU-Net ESKNet MedVKAN AURA-Net CA-Net RollingUnet MultiResUNet DDS-UNet Zig-RiR DAEFormer DoubleUNet ColonSegNet Swin-umamba TransAttUnet ResUNet++ GH-UNet U-Net ScribFormer AttU-Net MFMSNet MedT CFM-UNet UNet3+ TransUnet CENet DA-TransUNet UTANet CaraNet CMU-Net CPCANet TransNorm PraNet MSRFNet EViT-UNet UCTransNet FCBFormer D-TrAttUnet MCA-UNet ConvFormer UACANet Perspective-Unet DS-TransUNet CFFormer MT-UNet MALUNet DCA LV-UNet CMUNeXt U-RWKV LGMSNet MBSNet Tinyunet UNeXt #7 ULite Mobile U-ViT CFPNet-M SwinUNETR DCSAU-Net MUCM-Net U-KAN LFU-Net MambaUnet RWKV-UNet DDANet ResU-KAN UTNet MDSA-UNet CE-Net TA-Net EMCAD Swin-umambaD TransResUNet G-CASCADE MEGANet SimpleUNet ERDUnet MedFormer LeViT-UNet CASCADE HiFormer DC-UNet MMUNet U-Net++ AC-MambaSeg MissFormer H2Former CSCAUNet SCUNet++ FAT-Net MSLAU-Net MedVKAN ESKNet DAEFormer AURA-Net CA-Net RollingUnet DDS-UNet VMUNet BEFUnet DoubleUNet ColonSegNet MedT Swin-umamba Zig-RiR TransAttUnet ResUNet++ GH-UNet U-Net AttU-Net ScribFormer MFMSNet MultiResUNet UNETR CENet UNet3+ TransUnet DA-TransUNet UTANet CMU-Net TransNorm CFM-UNet CPCANet EViT-UNet MSRFNet UCTransNet FCBFormer D-TrAttUnet MCA-UNet MALUNet Perspective-Unet ConvFormer MERIT DS-TransUNet CFFormer MT-UNet UltraLight-VM-UNet UltraLight-VM-UNet MambaUnet VMUNetV2 UNetV2 MissFormer BEFUnet SwinUnet CSWin-UNet H-vmunet UNETR MERIT VMUNetV2 Polyp-PVT UNetV2 TransFuse SwinUnet CaraNet CSWin-UNet H-vmunet PraNet UACANet ACDC MBSNet Tinyunet LGMSNet CMUNeXt SimpleUNet LV-UNet CFPNet-M Mobile U-ViT U-RWKV MambaUnet U-KAN DCSAU-Net RWKV-UNet MultiResUNet DDANet Swin-umambaD Polyp-PVT UTNet VMUNetV2 ResU-KAN CE-Net UNeXt MDSA-UNet G-CASCADE TA-Net DC-UNet TransResUNet MEGANet TransFuse CASCADE MedFormer LeViT-UNet HiFormer VMUNet ERDUnet U-Net++ MMUNet ULite H2Former CSCAUNet SCUNet++ DAEFormer AC-MambaSeg MSLAU-Net FAT-Net ESKNet MedVKAN MissFormer CA-Net AURA-Net RollingUnet DDS-UNet BEFUnet DoubleUNet ColonSegNet Swin-umamba TransAttUnet ResUNet++ GH-UNet AttU-Net U-Net ScribFormer MFMSNet CENet UNet3+ TransUnet SwinUNETR DA-TransUNet UTANet CaraNet CPCANet CMU-Net PraNet EViT-UNet MSRFNet UCTransNet FCBFormer D-TrAttUnet MCA-UNet CSWin-UNet UACANet ConvFormer Perspective-Unet MUCM-Net MedT DS-TransUNet TransNorm CFFormer LFU-Net MALUNet MT-UNet UltraLight-VM-UNet EMCAD UNetV2 SwinUnet Zig-RiR CFM-UNet H-vmunet UNETR MERIT Promise UNeXt LV-UNet LGMSNet MBSNet CMUNeXt U-RWKV Tinyunet U-KAN ULite Mobile U-ViT DCSAU-Net MDSA-UNet SwinUNETR MultiResUNet RWKV-UNet ResU-KAN DDANet CE-Net UTNet TA-Net TransFuse TransResUNet EMCAD Polyp-PVT MEGANet G-CASCADE MUCM-Net DC-UNet MedFormer CASCADE HiFormer ERDUnet MMUNet LeViT-UNet U-Net++ AC-MambaSeg CSCAUNet H2Former MedVKAN FAT-Net MSLAU-Net ESKNet Zig-RiR UNetV2 AURA-Net CA-Net RollingUnet DDS-UNet VMUNetV2 DAEFormer CFM-UNet SCUNet++ Swin-umamba ColonSegNet ResUNet++ TransAttUnet GH-UNet H-vmunet AttU-Net U-Net MFMSNet ScribFormer MedT DoubleUNet UNet3+ TransUnet CENet DA-TransUNet UTANet CaraNet UNETR CMU-Net CPCANet PraNet TransNorm EViT-UNet MSRFNet UCTransNet FCBFormer D-TrAttUnet MCA-UNet MissFormer UACANet MERIT Perspective-Unet ConvFormer DS-TransUNet CFFormer LFU-Net SimpleUNet MALUNet MT-UNet CT Synapse LGMSNet Tinyunet MBSNet CMUNeXt Mobile U-ViT U-KAN LV-UNet MambaUnet U-RWKV ULite DCSAU-Net SimpleUNet CFPNet-M RWKV-UNet DDANet MDSA-UNet ResU-KAN G-CASCADE UTNet Polyp-PVT VMUNetV2 MultiResUNet MEGANet TransResUNet CE-Net SwinUNETR DC-UNet CASCADE UNeXt LeViT-UNet TA-Net HiFormer TransFuse MedFormer VMUNet ERDUnet MMUNet AC-MambaSeg CSCAUNet U-Net++ MissFormer H2Former MSLAU-Net SCUNet++ DAEFormer ESKNet MedVKAN AURA-Net FAT-Net CA-Net RollingUnet DDS-UNet DoubleUNet ColonSegNet Swin-umamba TransAttUnet GH-UNet ScribFormer MFMSNet AttU-Net CENet U-Net CFM-UNet ResUNet++ TransUnet UNet3+ DA-TransUNet UTANet BEFUnet CaraNet CPCANet PraNet CMU-Net MedT MSRFNet EViT-UNet TransNorm UCTransNet FCBFormer CSWin-UNet D-TrAttUnet UACANet MERIT Perspective-Unet ConvFormer MCA-UNet DS-TransUNet CFFormer LFU-Net MALUNet MUCM-Net MT-UNet UltraLight-VM-UNet UltraLight-VM-UNet CFPNet-M MambaUnet Swin-umambaD BEFUnet VMUNet SwinUnet CSWin-UNet Swin-umambaD UNetV2 EMCAD SwinUnet Zig-RiR H-vmunet UNETR OCT Cystoidfluid ULite Tinyunet UNeXt LV-UNet MBSNet CMUNeXt LGMSNet U-RWKV U-KAN SwinUNETR Mobile U-ViT DCSAU-Net LFU-Net ResU-KAN RWKV-UNet MDSA-UNet DDANet UTNet CE-Net MUCM-Net MultiResUNet TA-Net LeViT-UNet TransResUNet MEGANet EMCAD MedFormer G-CASCADE ERDUnet MMUNet HiFormer CASCADE U-Net++ AC-MambaSeg UltraLight-VM-UNet CSCAUNet H2Former MissFormer MSLAU-Net DC-UNet FAT-Net SCUNet++ DAEFormer MedVKAN TransFuse Zig-RiR ESKNet AURA-Net CA-Net RollingUnet DDS-UNet Swin-umambaD VMUNet UNetV2 ColonSegNet DoubleUNet Swin-umamba ResUNet++ MedT TransAttUnet CFM-UNet GH-UNet U-Net ScribFormer AttU-Net MFMSNet CENet UNETR UNet3+ TransUnet UTANet DA-TransUNet CPCANet CMU-Net TransNorm EViT-UNet MSRFNet UCTransNet FCBFormer D-TrAttUnet MCA-UNet Perspective-Unet ConvFormer MERIT H-vmunet DS-TransUNet CSWin-UNet CFFormer MALUNet SimpleUNet MT-UNet CFPNet-M MambaUnet VMUNetV2 Polyp-PVT BEFUnet SwinUnet CaraNet PraNet UACANet 40 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Table 22: Per-dataset target ranking of 100 u-shape medical image segmentation networks with U-Score. Source Target. Rank BUSI BUS LGMSNet LV-UNet MBSNet Mobile U-ViT U-KAN SwinUNETR MALUNet MUCM-Net UNeXt CFPNet-M RWKV-UNet ULite Swin-umambaD CE-Net Polyp-PVT MDSA-UNet TA-Net G-CASCADE UTNet DCSAU-Net EMCAD TransFuse VMUNetV2 ResU-KAN TransResUNet MEGANet Tinyunet DDANet CASCADE ERDUnet MedFormer VMUNet MMUNet U-RWKV MissFormer AC-MambaSeg UNetV2 HiFormer U-Net++ CSCAUNet H2Former MSLAU-Net SCUNet++ FAT-Net #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 #25 #26 #27 #28 #29 #30 #31 #32 #33 #34 #35 #36 #37 #38 #39 #40 #41 #42 #43 #44 #45 UltraLight-VM-UNet #46 #47 #48 #49 #50 #51 #52 #53 #54 #55 #56 #57 #58 #59 #60 #61 #62 #63 #64 #65 #66 #67 #68 #69 #70 #71 #72 #73 #74 #75 #76 #77 #78 #79 #80 #81 #82 #83 #84 #85 #86 #87 #88 #89 #90 #91 #92 #93 #94 #95 #96 #97 #98 #99 #100 DAEFormer ESKNet AURA-Net MedVKAN CA-Net DDS-UNet RollingUnet BEFUnet DoubleUNet Swin-umamba H-vmunet GH-UNet TransAttUnet MFMSNet CENet CSWin-UNet TransUnet AttU-Net U-Net DA-TransUNet CaraNet CPCANet UTANet UNet3+ TransNorm PraNet CMU-Net EViT-UNet CFM-UNet MedT MultiResUNet FCBFormer MSRFNet UCTransNet MCA-UNet D-TrAttUnet UACANet MERIT Perspective-Unet ConvFormer DS-TransUNet CFFormer SimpleUNet LFU-Net CMUNeXt MambaUnet LeViT-UNet DC-UNet MT-UNet SwinUnet Zig-RiR ColonSegNet ResUNet++ UNETR ScribFormer Ultrasound BUSBRA BUS LV-UNet LGMSNet SwinUNETR Mobile U-ViT MBSNet UNeXt U-KAN DCSAU-Net MUCM-Net CFPNet-M MambaUnet CMUNeXt U-RWKV MDSA-UNet VMUNetV2 MALUNet RWKV-UNet Polyp-PVT Swin-umambaD TA-Net G-CASCADE CE-Net UTNet ResU-KAN EMCAD TransResUNet MEGANet TransFuse UNetV2 ERDUnet CASCADE MedFormer MissFormer HiFormer U-Net++ CSCAUNet DAEFormer H2Former DDANet MSLAU-Net MMUNet FAT-Net AC-MambaSeg SCUNet++ ESKNet AURA-Net MedVKAN RollingUnet CA-Net CSWin-UNet Zig-RiR DDS-UNet DoubleUNet DC-UNet CFM-UNet Swin-umamba H-vmunet MedT TransAttUnet GH-UNet MFMSNet CENet ScribFormer TransUnet DA-TransUNet CPCANet UTANet CaraNet U-Net AttU-Net TransNorm PraNet CMU-Net UNet3+ EViT-UNet ResUNet++ MSRFNet FCBFormer MCA-UNet UCTransNet UACANet Perspective-Unet MERIT ConvFormer D-TrAttUnet DS-TransUNet ULite LFU-Net SimpleUNet Tinyunet UltraLight-VM-UNet MultiResUNet MT-UNet LeViT-UNet BEFUnet VMUNet SwinUnet CFFormer ColonSegNet UNETR TNSCUI TUCC LV-UNet LGMSNet MBSNet CMUNeXt U-RWKV U-KAN DCSAU-Net CFPNet-M MDSA-UNet VMUNetV2 RWKV-UNet Tinyunet ResU-KAN Polyp-PVT Mobile U-ViT Swin-umambaD G-CASCADE SwinUNETR TA-Net CE-Net UNetV2 UNeXt MEGANet TransResUNet TransFuse HiFormer ERDUnet DDANet EMCAD MedFormer MUCM-Net CASCADE UTNet MALUNet MissFormer VMUNet MMUNet DC-UNet AC-MambaSeg CSCAUNet DAEFormer H2Former MSLAU-Net MultiResUNet ESKNet CA-Net FAT-Net SCUNet++ RollingUnet AURA-Net MedVKAN DDS-UNet U-Net++ MedT Swin-umamba CFM-UNet GH-UNet TransAttUnet ResUNet++ CENet MFMSNet DoubleUNet ScribFormer U-Net BEFUnet CSWin-UNet CaraNet TransUnet DA-TransUNet UNet3+ UTANet AttU-Net CPCANet TransNorm PraNet CMU-Net EViT-UNet FCBFormer UCTransNet MCA-UNet MSRFNet D-TrAttUnet MERIT UACANet ConvFormer Perspective-Unet ULite DS-TransUNet CFFormer LFU-Net SimpleUNet UltraLight-VM-UNet MambaUnet LeViT-UNet SwinUnet Zig-RiR MT-UNet ColonSegNet H-vmunet UNETR Endoscopy Kvasir CVC300 LV-UNet MBSNet CMUNeXt LGMSNet MambaUnet Mobile U-ViT Tinyunet DCSAU-Net U-KAN CFPNet-M VMUNetV2 RWKV-UNet DDANet Swin-umambaD TA-Net CE-Net G-CASCADE EMCAD TransFuse MDSA-UNet MultiResUNet Polyp-PVT TransResUNet ResU-KAN UTNet MEGANet CASCADE HiFormer ERDUnet MedFormer UNetV2 VMUNet MMUNet MissFormer CSCAUNet SCUNet++ BEFUnet AC-MambaSeg MSLAU-Net U-Net++ H2Former ESKNet FAT-Net DC-UNet AURA-Net RollingUnet CA-Net DAEFormer DDS-UNet CSWin-UNet MedVKAN DoubleUNet Swin-umamba CFM-UNet Zig-RiR GH-UNet TransAttUnet ColonSegNet MFMSNet SwinUNETR CENet ScribFormer AttU-Net ResUNet++ UTANet U-Net TransUnet CaraNet UNet3+ CPCANet DA-TransUNet UNeXt PraNet CMU-Net EViT-UNet TransNorm MSRFNet UCTransNet FCBFormer D-TrAttUnet MCA-UNet UACANet MERIT ConvFormer Perspective-Unet ULite DS-TransUNet CFFormer SimpleUNet LFU-Net MALUNet MUCM-Net UltraLight-VM-UNet U-RWKV LeViT-UNet MT-UNet SwinUnet MedT H-vmunet UNETR Kvasir CVC-ClinicDB LGMSNet MBSNet LV-UNet MambaUnet Mobile U-ViT U-KAN VMUNetV2 RWKV-UNet SwinUNETR Swin-umambaD CMUNeXt DCSAU-Net Polyp-PVT ResU-KAN G-CASCADE TA-Net DDANet EMCAD CE-Net CFPNet-M TransResUNet MEGANet TransFuse UTNet CASCADE HiFormer MDSA-UNet UNetV2 VMUNet MultiResUNet MedFormer MissFormer MMUNet AC-MambaSeg CSCAUNet H2Former MSLAU-Net FAT-Net ESKNet SCUNet++ U-Net++ AURA-Net DAEFormer RollingUnet BEFUnet DDS-UNet DC-UNet CA-Net DoubleUNet Swin-umamba ColonSegNet UNeXt TransAttUnet MFMSNet CSWin-UNet GH-UNet LeViT-UNet ResUNet++ MedVKAN CENet U-Net AttU-Net UNet3+ ScribFormer CaraNet TransUnet DA-TransUNet UTANet PraNet CPCANet CMU-Net TransNorm H-vmunet MSRFNet EViT-UNet FCBFormer UCTransNet D-TrAttUnet MCA-UNet UACANet MERIT Perspective-Unet Zig-RiR ConvFormer DS-TransUNet CFFormer LFU-Net SimpleUNet Tinyunet ULite MALUNet MUCM-Net UltraLight-VM-UNet U-RWKV MT-UNet ERDUnet SwinUnet CFM-UNet MedT UNETR Dermoscopy ISIC2018 PH2 LV-UNet LGMSNet CMUNeXt U-RWKV MBSNet CFPNet-M ULite MDSA-UNet Mobile U-ViT RWKV-UNet SwinUNETR VMUNetV2 ResU-KAN MUCM-Net G-CASCADE UTNet CE-Net TA-Net TransFuse U-KAN MEGANet Polyp-PVT Tinyunet TransResUNet CASCADE DDANet MMUNet ERDUnet Swin-umambaD EMCAD MedFormer VMUNet U-Net++ AC-MambaSeg CSCAUNet H2Former MissFormer MSLAU-Net DCSAU-Net Zig-RiR MedVKAN FAT-Net ESKNet SwinUnet AURA-Net HiFormer CSWin-UNet DAEFormer CFM-UNet SCUNet++ RollingUnet MedT UNeXt Swin-umamba MultiResUNet GH-UNet H-vmunet MFMSNet TransAttUnet U-Net CENet UNetV2 CaraNet UTANet CPCANet TransUnet UNet3+ PraNet CA-Net EViT-UNet ScribFormer CMU-Net DA-TransUNet TransNorm MSRFNet UCTransNet D-TrAttUnet FCBFormer MCA-UNet SimpleUNet UACANet MERIT Perspective-Unet ConvFormer DS-TransUNet CFFormer LFU-Net MALUNet UltraLight-VM-UNet MT-UNet MambaUnet DDS-UNet LeViT-UNet DC-UNet BEFUnet DoubleUNet ColonSegNet ResUNet++ UNETR AttU-Net Fundus CHASE Stare ULite LV-UNet SwinUNETR CMUNeXt MUCM-Net CFPNet-M UNeXt LGMSNet Tinyunet MBSNet U-RWKV LFU-Net Mobile U-ViT U-KAN RWKV-UNet DCSAU-Net ResU-KAN SimpleUNet DDANet G-CASCADE MambaUnet UTNet EMCAD ERDUnet TA-Net TransResUNet CE-Net DC-UNet MissFormer UltraLight-VM-UNet HiFormer MEGANet VMUNet CASCADE MMUNet AC-MambaSeg MedFormer Swin-umambaD DAEFormer CSCAUNet SCUNet++ U-Net++ MDSA-UNet H2Former FAT-Net BEFUnet MedVKAN MSLAU-Net SwinUnet AURA-Net CA-Net VMUNetV2 RollingUnet ESKNet DDS-UNet TransFuse DoubleUNet MedT Swin-umamba UNETR ColonSegNet TransAttUnet ResUNet++ CENet AttU-Net GH-UNet MFMSNet U-Net TransUnet UNet3+ DA-TransUNet ScribFormer CPCANet UTANet TransNorm CMU-Net EViT-UNet MSRFNet FCBFormer UCTransNet D-TrAttUnet MCA-UNet Perspective-Unet MERIT DS-TransUNet CFFormer Zig-RiR MALUNet MT-UNet MultiResUNet Polyp-PVT UNetV2 LeViT-UNet H-vmunet CSWin-UNet CFM-UNet CaraNet PraNet UACANet ConvFormer X-Ray Histopathology Montgomery NIH-test Monusac Tnbcnuclei LV-UNet MUCM-Net LGMSNet MALUNet SwinUNETR Mobile U-ViT SimpleUNet RWKV-UNet Swin-umambaD CE-Net U-KAN Tinyunet UNeXt Polyp-PVT TA-Net VMUNetV2 ResU-KAN G-CASCADE TransFuse CMUNeXt TransResUNet UltraLight-VM-UNet MEGANet HiFormer EMCAD CASCADE UTNet MBSNet VMUNet UNetV2 AC-MambaSeg MissFormer LeViT-UNet MMUNet FAT-Net CSCAUNet MSLAU-Net LFU-Net AURA-Net SCUNet++ DCSAU-Net MDSA-UNet ESKNet SwinUnet Zig-RiR DAEFormer DDS-UNet CSWin-UNet BEFUnet DoubleUNet H2Former CFM-UNet Swin-umamba RollingUnet H-vmunet CA-Net MultiResUNet DDANet MFMSNet U-Net++ TransUnet U-Net AttU-Net DA-TransUNet UNet3+ CENet CaraNet UTANet TransNorm PraNet ScribFormer GH-UNet CMU-Net ERDUnet UNETR UCTransNet FCBFormer D-TrAttUnet CPCANet MCA-UNet U-RWKV UACANet Perspective-Unet MERIT CFPNet-M DS-TransUNet CFFormer ULite MambaUnet DC-UNet MedFormer MedVKAN MedT MT-UNet ColonSegNet ResUNet++ TransAttUnet MSRFNet EViT-UNet ConvFormer CMUNeXt MALUNet MDSA-UNet DCSAU-Net UltraLight-VM-UNet U-KAN CFPNet-M Tinyunet TA-Net UNetV2 G-CASCADE RWKV-UNet UNeXt EMCAD Swin-umambaD VMUNetV2 MambaUnet ULite LeViT-UNet SwinUNETR MUCM-Net MEGANet MissFormer VMUNet SwinUnet LFU-Net Mobile U-ViT U-Net++ CASCADE TransResUNet LGMSNet MMUNet DAEFormer AC-MambaSeg ERDUnet SimpleUNet Polyp-PVT CE-Net UTNet ResU-KAN TransFuse MedVKAN CSCAUNet SCUNet++ HiFormer CSWin-UNet CA-Net ESKNet MedFormer DDS-UNet BEFUnet MSLAU-Net MBSNet LV-UNet AURA-Net H-vmunet Swin-umamba GH-UNet CFM-UNet FAT-Net MFMSNet CENet DA-TransUNet MedT UNETR CPCANet H2Former TransUnet CMU-Net TransNorm PraNet CaraNet ResUNet++ FCBFormer UNet3+ UCTransNet D-TrAttUnet EViT-UNet MERIT MCA-UNet ConvFormer TransAttUnet Perspective-Unet DS-TransUNet CFFormer U-RWKV Zig-RiR MultiResUNet DDANet DC-UNet RollingUnet DoubleUNet ColonSegNet ScribFormer U-Net AttU-Net UTANet MSRFNet UACANet MT-UNet 41 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Figure 12: Segmentation results of the Top 5 models and U-Net, where the green curve represents the ground truth and the yellow curve represents the model prediction. U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Figure 13: Segmentation results of the Top 5 models and U-Net, where the green curve represents the ground truth and the yellow curve represents the model prediction. 43 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking"
        },
        {
            "title": "References",
            "content": "Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in brief, 28:104863, 2020. Alyaa Amer and Xujiong Ye. Mca-unet: multiscale context aggregation u-net for the segmentation of covid-19 lesions from ct images. Computer Methods and Programs in Biomedicine Update, 4:100114, 2023. Reza Azad, Mohammad Al-Antary, Moein Heidari, and Dorit Merhof. Transnorm: Transformer provides strong spatial normalization mechanism for deep segmentation model. IEEE Access, 10:108205108215, 2022. Reza Azad, Ren√© Arimond, Ehsan Khodapanah Aghdam, Amirhossein Kazerouni, and Dorit Merhof. Daeformer: Dual attention-guided efficient transformer for medical image segmentation. In International workshop on predictive intelligence in medicine, pages 8395. Springer, 2023. Pedro RAS Bassi, Wenxuan Li, Yucheng Tang, Fabian Isensee, Zifu Wang, Jieneng Chen, Yu-Cheng Chou, Yannick Kirchhoff, Maximilian Rokuss, Ziyan Huang, et al. Touchstone benchmark: Are we on the right way for evaluating ai algorithms for medical segmentation? Advances in Neural Information Processing Systems, 37:1518415201, 2024. Jorge Bernal, Javier S√°nchez, Gloria Fern√°ndez-Esparrach, Debora Gil, Cristina Rodr√≠guez, and Fernando Vilari√±o. Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics, 43:99111, 2015. Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? IEEE transactions on medical imaging, 37(11):25142525, 2018. Fares Bougourzi, Fadi Dornaika, Cosimo Distante, and Abdelmalik Taleb-Ahmed. D-trattunet: Toward hybrid cnn-transformer architecture for generic and subtle segmentation in medical images. Computers in Biology and Medicine, 176:108590, 2024. Nhat-Tan Bui, Dinh-Hieu Hoang, Quang-Thuc Nguyen, Minh-Triet Tran, and Ngan Le. Meganet: Multi-scale edge-guided attention network for weak boundary polyp segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 79857994, 2024. Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European conference on computer vision, pages 205218. Springer, 2022. Bingzhi Chen, Yishu Liu, Yingjian Li, Zheng Zhang, Guangming Lu, and Adams Wai Kin Kong. Transattunet: Multi-level attention-guided u-net with transformer for medical image segmentation. IEEE Transactions on Instrumentation & Measurement, 2022. Gongping Chen, Lu Zhou, Jianxun Zhang, Xiaotao Yin, Liang Cui, and Yu Dai. Esknet: An enhanced adaptive selection kernel convolution for ultrasound breast tumors segmentation. Computer Methods and Programs in Biomedicine, 226:107086, 2023. 44 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021. Junren Chen, Rui Chen, Wei Wang, Junlong Cheng, Lei Zhang, and Liangyin Chen. Tinyu-net: Lighter yet better u-net with cascaded multi-receptive fields. In Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer, 2024a. Tianqi Chen and Carlos Guestrin. Xgboost: scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785794, 2016. Tianxiang Chen, Xudong Zhou, Zhentao Tan, Yue Wu, Ziyang Wang, Zi Ye, Tao Gong, Qi Chu, Nenghai Yu, and Le Lu. Zig-rir: Zigzag rwkv-in-rwkv for efficient medical image segmentation. IEEE Transactions on Medical Imaging, 2025. Yifei Chen, Binfeng Zou, Zhaoxin Guo, Yiyu Huang, Yifan Huang, Feiwei Qin, Qinhai Li, and Changmiao Wang. Scunet++: Swin-unet and cnn bottleneck hybrid architecture with multi-fusion dense skip connection for pulmonary embolism ct image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 77597767, 2024b. Noel CF Codella, David Gutman, Emre Celebi, Brian Helba, Michael Marchetti, Stephen Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion analysis toward melanoma detection: challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In ISBI 2018, pages 168172. IEEE, 2018. Ethan Cohen and Virginie Uhlmann. aura-net: Robust segmentation of phase-contrast microscopy images with few annotations. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 640644. IEEE, 2021. Yunjiao Deng, Hui Wang, Yulei Hou, Shunpan Liang, and Daxing Zeng. Lfu-net: lightweight u-net with full skip connections for medical image segmentation. Current Medical Imaging, 19:347360, 2023. Binh-Duong Dinh, Thanh-Thu Nguyen, Thi-Thao Tran, and Van-Truong Pham. 1m parameters are enough? lightweight cnn-based model for medical image segmentation. In 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pages 12791284. IEEE, 2023. Bo Dong, Wenhai Wang, Deng-Ping Fan, Jinpeng Li, Huazhu Fu, and Ling Shao. Polyp-pvt: Polyp segmentation with pyramid vision transformers. arXiv preprint arXiv:2108.06932, 2021. Chengqi Dong, Fenghe Tang, Rongge Mao, Xinpei Gao, and Kevin Zhou. Lgmsnet: Thinning medical image segmentation model via dual-level multiscale fusion. arXiv preprint arXiv:2508.15476, 2025. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverse attention network for polyp segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 263273. Springer, 2020. 45 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Muhammad Moazam Fraz, Paolo Remagnino, Andreas Hoppe, Bunyarit Uyyanonvara, Alicja R. Rudnicka, Christopher G. Owen, and Sarah A. Barman. Chase db1: Retinal vessel reference dataset, 2012. URL https://researchdata.kingston.ac.uk/96/. Yunhe Gao, Mu Zhou, and Dimitris Metaxas. Utnet: hybrid transformer architecture for medical image segmentation. In International conference on medical image computing and computer-assisted intervention, pages 6171. Springer, 2021. Yunhe Gao, Mu Zhou, Di Liu, Zhennan Yan, Shaoting Zhang, and Dimitris N. Metaxas. data-scalable transformer for medical image segmentation: Architecture, model efficiency, and benchmark. arXiv preprint arXiv:2203.00131, 2023. Wilfrido G√≥mez-Flores, Maria Julia Gregorio-Calas, and Wagner Coelho de Albuquerque Pereira. Bus-bra: breast ultrasound dataset for assessing computer-aided diagnosis systems. Medical Physics, 51(4): 31103123, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Pengfei Gu, Yejia Zhang, Chaoli Wang, and Danny Chen. Convformer: Combining cnn and transformer for medical image segmentation. In 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2023. Ran Gu, Guotai Wang, Tao Song, Rui Huang, Michael Aertsen, Jan Deprest, S√©bastien Ourselin, Tom Vercauteren, and Shaoting Zhang. Ca-net: Comprehensive attention convolutional neural networks for explainable medical image segmentation. IEEE transactions on medical imaging, 40(2):699711, 2020. Tiancheng Gu, Kaicheng Yang, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, and Jiankang Deng. Rwkv-clip: robust vision-language representation learner. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 47994812, 2024. Zaiwang Gu, Jun Cheng, Huazhu Fu, Kang Zhou, Huaying Hao, Yitian Zhao, Tianyang Zhang, Shenghua Gao, and Jiang Liu. Ce-net: Context encoder network for 2d medical image segmentation. IEEE transactions on medical imaging, 38(10):22812292, 2019a. Zaiwang Gu, Jun Cheng, Huazhu Fu, Kang Zhou, Huaying Hao, Yitian Zhao, Tianyang Zhang, Shenghua Gao, and Jiang Liu. Ce-net: Context encoder network for 2d medical image segmentation. IEEE transactions on medical imaging, 38(10):22812292, 2019b. Hamilton. Kaggle data science bowl: Find the nuclei in divergent images to advance medical discovery, 2018. Yousuf Harun, Kyungbok Lee, Jhair Gallardo, Giri Krishnan, and Christopher Kanan. What variables affect out-of-distribution generalization in pretrained models? Advances in Neural Information Processing Systems, 37:5647956525, 2024. Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI Brainlesion Workshop, pages 272284, 2021. U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R. Roth, Bennett A. Landman, Daguang Xu, Shashank Nath, Tong Zhao, and Ali Gholipour. Unetr: Transformers for 3d medical image segmentation. In 2022 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 574584. IEEE, 2022. Along He, Kai Wang, Tao Li, Chengkun Du, Shuang Xia, and Huazhu Fu. H2former: An efficient hierarchical hybrid transformer for medical image segmentation. IEEE Transactions on Medical Imaging, 42(9):2763 2776, 2023. Moein Heidari, Amirhossein Kazerouni, Milad Soltany, Reza Azad, Ehsan Khodapanah Aghdam, Julien Cohen-Adad, and Dorit Merhof. Hiformer: Hierarchical multi-scale representations using transformers for medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 62026212, 2023. AD Hoover, Valentina Kouznetsova, and Michael Goldbaum. Locating blood vessels in retinal images by piecewise threshold probing of matched filter response. IEEE Transactions on Medical imaging, 19(3): 203210, 2000. Wenjin Hou, Dingjie Fu, Kun Li, Shiming Chen, Hehe Fan, and Yi Yang. Zeromamba: Exploring visual state space model for zero-shot learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 35273535, 2025. Jintong Hu, Siyan Chen, Zhiyi Pan, Sen Zeng, and Wenming Yang. Perspective+ unet: Enhancing segmentation with bi-path fusion and efficient non-local attention for superior receptive fields. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 499509. Springer, 2024. Hejun Huang, Zuguo Chen, Ying Zou, Ming Lu, and Chaoyang Chen. Channel prior convolutional attention for medical image segmentation. Current Medical Imaging, 19:347360, 2023a. Huimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang, Yutaro Iwamoto, Xianhua Han, Yen-Wei Chen, and Jian Wu. Unet 3+: full-scale connected unet for medical image segmentation. In ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 10551059. Ieee, 2020. Xiaohong Huang, Zhifang Deng, Dandan Li, Xueguang Yuan, and Ying Fu. Missformer: An effective medical image segmentation transformer. IEEE Transactions on Medical Imaging, 42(5):14841494, 2023b. Nabil Ibtehaz and Sohel Rahman. Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation. Neural networks, 121:7487, 2020. Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: selfconfiguring method for deep learning-based biomedical image segmentation. Nature methods, 18(2): 203211, 2021. Fabian Isensee, Tassilo Wald, Constantin Ulrich, Michael Baumgartner, Saikat Roy, Klaus Maier-Hein, and Paul Jaeger. nnu-net revisited: call for rigorous validation in 3d medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 488498. Springer, 2024. Stefan Jaeger, Sema Candemir, Sameer Antani, Y√¨-Xi√°ng W√°ng, Pu-Xuan Lu, and George Thoma. Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery, 4(6):475, 2014. U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Debesh Jha, Pia Smedsrud, Michael Riegler, Dag Johansen, Thomas De Lange, P√•l Halvorsen, and H√•vard Johansen. Resunet++: An advanced architecture for medical image segmentation. In 2019 IEEE international symposium on multimedia (ISM), pages 2252255. IEEE, 2019. Debesh Jha, Michael Riegler, Dag Johansen, P√•l Halvorsen, and H√•vard Johansen. Doubleu-net: deep convolutional neural network for medical image segmentation. In 2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS), pages 558564. IEEE, 2020a. Debesh Jha, Pia Smedsrud, Michael Riegler, P√•l Halvorsen, Thomas De Lange, Dag Johansen, and H√•vard Johansen. Kvasir-seg: segmented polyp dataset. In MultiMedia modeling: 26th international conference, MMM 2020, proceedings, part II 26, pages 451462. Springer, 2020b. Debesh Jha, Sharib Ali, Nikhil Kumar Tomar, H√•vard Johansen, Dag Johansen, Jens Rittscher, Michael Riegler, and P√•l Halvorsen. Real-time polyp detection, localization and segmentation in colonoscopy using deep learning. IEEE Access, 9:4049640510, 2021. Jiarui Jiang, Wei Huang, Miao Zhang, Taiji Suzuki, and Liqiang Nie. Unveil benign overfitting for transformer in vision: Training dynamics, convergence, and generalization. Advances in Neural Information Processing Systems, 37:135464135625, 2024a. Juntao Jiang, Mengmeng Wang, Huizhong Tian, Lingbo Cheng, and Yong Liu. Lv-unet: lightweight and vanilla model for medical image segmentation. In 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 42404246. IEEE, 2024b. Juntao Jiang, Jiangning Zhang, Weixuan Liu, Muxuan Gao, Xiaobin Hu, Xiaoxiao Yan, Feiyue Huang, and Yong Liu. Rwkv-unet: Improving unet with long-range cooperation for effective medical image segmentation. CoRR, 2025. Taehun Kim, Hyemin Lee, and Daijin Kim. Uacanet: Uncertainty augmented context attention for polyp segmentation. In Proceedings of the 29th ACM International Conference on Multimedia (MM 21), pages 19. ACM, 2021. Lisa Koch, Christian Baumgartner, and Philipp Berens. Distribution shift detection for the postmarket surveillance of medical ai algorithms: retrospective simulation study. NPJ Digital Medicine, 7(1):120, 2024. Zeki Ku≈ü and Musa Aydin. Medsegbench: comprehensive benchmark for medical image segmentation in diverse data modalities. Scientific Data, 11(1):1283, 2024. Libin Lan, Yanxin Li, Xiaojuan Liu, Juan Zhou, Jianxun Zhang, Nannan Huang, and Yudong Zhang. Mslaunet: hybrid cnn-transformer network for medical image segmentation. arXiv preprint arXiv:2505.18823, 2025. Chenxin Li, Xinyu Liu, Wuyang Li, Cheng Wang, Hengyu Liu, Yifan Liu, Zhen Chen, and Yixuan Yuan. U-kan makes strong backbone for medical image segmentation and generation. In The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25), 2025a. Hao Li, Di-Hua Zhai, and Yuanqing Xia. Erdunet: An efficient residual double-coding unet for medical image segmentation. IEEE Transactions on Circuits and Systems for Video Technology, 34(4):20832096, 2024a. JianFeng Li and YanMin Niu. Dual encoding dds-unet liver tumour segmentation based on multi-scale deep and shallow feature fusion. IET Image Processing, 18(5):11891199, 2024. U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Jiaxuan Li, Qing Xu, Xiangjian He, Ziyu Liu, Daokun Zhang, Ruili Wang, Rong Qu, and Guoping Qiu. Cfformer: Cross cnn-transformer channel attention and spatial feature fusion for improved segmentation of heterogeneous medical images. Expert Systems with Applications, 2025b. Xiang Li, Chong Fu, Qun Wang, Wenchao Zhang, Chen Ye, Junxin Chen, and Chui-Wing Sham. Multi-scale dynamic sparse attention unet for medical image segmentation. IEEE Journal of Biomedical and Health Informatics, 2025c. Xin Li, Wenhui Zhu, Xuanzhao Dong, Oana Dumitrascu, and Yalin Wang. Evit-unet: U-net like efficient vision transformer for medical image segmentation on mobile and edge devices. In 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2025d. Zihan Li, Yuan Zheng, Dandan Shan, Shuzhou Yang, Qingde Li, Beizhan Wang, Yuanting Zhang, Qingqi Hong, and Dinggang Shen. Scribformer: Transformer makes cnn work better for scribble-based medical image segmentation. IEEE Transactions on Medical Imaging, 2024b. Ailiang Lin, Bingzhi Chen, Jiayu Xu, Zheng Zhang, and Guangming Lu. Ds-transunet: Dual swin transformer u-net for medical image segmentation. IEEE Transactions on Instrumentation & Measurement, 2022. Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Cheng Li, Yong Liang, Guangming Shi, Yizhou Yu, Shaoting Zhang, et al. Swin-umamba: Mamba-based unet with imagenet-based pretraining. In International conference on medical image computing and computer-assisted intervention, pages 615625. Springer, 2024a. Jiarun Liu, Hao Yang, Hong-Yu Zhou, Lequan Yu, Yong Liang, Yizhou Yu, Shaoting Zhang, Hairong Zheng, and Shanshan Wang. Swin-umamba√¢ƒÇƒÉ: Adapting mamba-based vision foundation models for medical image segmentation. IEEE Transactions on Medical Imaging, 2024b. Xiao Liu, Peng Gao, Tao Yu, Fei Wang, and Ru-Yue Yuan. Cswin-unet: Transformer unet with cross-shaped windows for medical image segmentation. Information Fusion, 113(102634), 2025. Yutong Liu, Haijiang Zhu, Mengting Liu, Huaiyuan Yu, Zihan Chen, and Jie Gao. Rolling-unet: Revitalizing mlps ability to efficiently extract long-distance dependencies for medical image segmentation. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24), pages 38193827, 2024c. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin In Proceedings of the IEEE/CVF transformer: Hierarchical vision transformer using shifted windows. international conference on computer vision, pages 1001210022, 2021. Ange Lou, Shuyue Guan, and Murray Loew. Cfpnet-m: light-weight encoder-decoder based network for multimodal biomedical image real-time segmentation. In Medical Imaging 2021: Image Processing, volume 11596, page 115962T. International Society for Optics and Photonics, 2021. Ange Lou, Shuyue Guan, Hanseok Ko, and Murray Loew. Caranet: Context axial reverse attention network for segmentation of small medical objects. In Medical Imaging 2022: Image Processing, volume 12032, pages 8192. SPIE, 2022. Zichen Luo, Xinshan Zhu, Lan Zhang, and Biao Sun. Rethinking u-net: Task-adaptive mixture of skip connections for enhanced medical image segmentation. In The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25), 2025. 49 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Omid Nejati Manzari, Javad Mirzapour Kaleybar, Hooman Saadat, and Shahin Maleki. Befunet: hybrid cnn-transformer architecture for precise medical image segmentation. arXiv preprint arXiv:2402.08793, 2024. Teresa Mendon√ßa, Pedro Ferreira, Jorge Marques, Andr√© RS Marcal, and Jorge Rozeira. Ph 2-a dermoscopic image database for research and benchmarking. In 2013 35th EMBC, pages 54375440. IEEE, 2013. Viet-Thanh Nguyen, Van-Truong Pham, and Thi-Thao Tran. Ac-mambaseg: An adaptive convolution and mamba-based architecture for enhanced skin lesion segmentation. In International Conference on Green Technology and Sustainable Development, pages 1326. Springer, 2024. Ke Niu, Jiacheng Han, and Jiuyun Cai. Cfm-unet: coupling local and global feature extraction networks for medical image segmentation. Scientific Reports, 15(22236), 2025. Ziwei Niu, Shuyi Ouyang, Shiao Xie, Yen-wei Chen, and Lanfen Lin. survey on domain generalization for medical image analysis. arXiv preprint arXiv:2402.05035, 2024. Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Hammerla, and Bernhard et al. Kainz. Attention u-net: Learning where to look for the pancreas. In Medical Imaging with Deep Learning, 2018. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Yaopeng Peng, Danny Chen, and Milan Sonka. U-net v2: Rethinking the skip connections of u-net for medical image segmentation. In 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2025. Payel Pramanik, Ayush Roy, Erik Cuevas, Marco Perez-Cisneros, and Ram Sarkar. Dau-net: Dual attentionaided u-net for segmenting tumor in breast ultrasound images. Plos one, 19(5):e0303670, 2024. Md Mostafijur Rahman and Radu Marculescu. Medical image segmentation via cascaded attention decoding. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 62226231, 2023a. Md Mostafijur Rahman and Radu Marculescu. G-cascade: Efficient cascaded graph convolutional decoding for 2d medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 77287737, 2023b. Md Mostafijur Rahman and Radu Marculescu. Multi-scale hierarchical vision transformer with cascaded attention decoding for medical image segmentation. In Proceedings of Machine Learning Research (MIDL 2023), volume 165, pages 119, 2023c. Md Mostafijur Rahman, Mustafa Munir, and Radu Marculescu. Emcad: Efficient multi-scale convolutional attention decoding for medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1176911779, 2024. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 234241. Springer, 2015. U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Jiacheng Ruan and Suncheng Xiang. Vm-unet: Vision mamba unet for medical image segmentation. CoRR, 2024. Jiacheng Ruan, Suncheng Xiang, Mingye Xie, Ting Liu, and Yuzhuo Fu. Malunet: multi-attention and light-weight unet for skin lesion segmentation. In 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 11501156. IEEE, 2022. Edward Sanderson and Bogdan Matuszewski. Fcn-transformer feature fusion for polyp segmentation. In Medical Image Computing and Computer-Assisted Intervention Workshop, pages 644654. Springer, 2022. Xin Shu, Jiashu Wang, Aoping Zhang, Jinlong Shi, and Xiao-Jun Wu. Csca u-net: channel and space compound attention cnn for medical image segmentation. Artificial Intelligence In Medicine, 150:102800, 2024. Sirinukunwattana, JP Pluim, Chen, Qi, and PAG Heng. Glas@ miccai√¢ƒÇ≈π2015: Gland segmentation challenge contest, 2015. Abhishek Srivastava, Debesh Jha, Sukalpa Chanda, Umapada Pal, H√•vard Johansen, Dag Johansen, Michael Riegler, Sharib Ali, and P√•l Halvorsen. Msrf-net: multi-scale residual fusion network for biomedical image segmentation. IEEE Journal of Biomedical and Health Informatics, 26(5):22522263, 2021. Joes Staal, Michael Abr√†moff, Meindert Niemeijer, Max Viergever, and Bram Van Ginneken. Ridge-based vessel segmentation in color images of the retina. IEEE transactions on medical imaging, 23(4):501509, 2004. Guanqun Sun, Yizhi Pan, Weikun Kong, Zichang Xu, Jianhua Ma, Teeradaj Racharak, Le-Minh Nguyen, and Junyi Xin. Da-transunet: integrating spatial and channel dual attention with transformer u-net for medical image segmentation. Frontiers in Bioengineering and Biotechnology, 12:1398237, 2024. Fenghe Tang, Lingtao Wang, Chunping Ning, Min Xian, and Jianrui Ding. Cmu-net: strong convmixer-based medical ultrasound image segmentation network. In 2023 IEEE 20th international symposium on biomedical imaging (ISBI), pages 15. IEEE, 2023. Fenghe Tang, Jianrui Ding, Quan Quan, Lingtao Wang, Chunping Ning, and Kevin Zhou. Cmunext: An efficient medical image segmentation network based on large kernel and skip fusion. In ISBI, 2024. Fenghe Tang, Wenxin Ma, Zhiyang He, Xiaodong Tao, Zihang Jiang, and Kevin Zhou. Pre-trained llm is semantic-aware and generalizable segmentation booster. arXiv preprint arXiv:2506.18034, 2025a. Fenghe Tang, Bingkun Nian, Jianrui Ding, Wenxin Ma, Quan Quan, Chengqi Dong, Jie Yang, Wei Liu, and Kevin Zhou. Mobile u-vit: Revisiting large kernel and u-shaped vit for efficient medical image segmentation. arXiv preprint arXiv:2508.01064, 2025b. Youbao Tang, Yuxing Tang, Jing Xiao, and Ronald Summers. Xlsor: robust and accurate lung segmentor on chest x-rays using criss-cross attention and customized radiorealistic abnormalities generation. In International Conference on Medical Imaging with Deep Learning (MIDL), 2019. Nikhil Kumar Tomar, Annie Shergill, Brandon Rieders, Ulas Bagci, and Debesh Jha. Transresu-net: Transformer based resu-net for real-time colonoscopy polyp segmentation. CoRR, 2022. 51 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Jeya Maria Jose Valanarasu and Vishal Patel. Unext: Mlp-based rapid medical image segmentation network. In International conference on medical image computing and computer-assisted intervention, pages 2333. Springer, 2022. Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal Patel. Medical transformer: Gated axial-attention for medical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 3646. Springer, 2021. Sandeep Kumar Vashist. Point-of-care diagnostics: Recent advances and trends. Biosensors, 7(4):62, 2017. David V√°zquez, Jorge Bernal, Javier S√°nchez, Gloria Fern√°ndez-Esparrach, Antonio L√≥pez, Adriana Romero, Michal Drozdzal, and Aaron Courville. benchmark for endoluminal scene segmentation of colonoscopy images. Journal of healthcare engineering, 2017(1):4037190, 2017. Haibin Wang, Zhenfeng Zhao, Qi Liu, and Shenwen Wang. Resu-kan: medical image segmentation model integrating residual convolutional attention and atrous spatial pyramid pooling. Applied Intelligence, 55 (568), 2025a. Haonan Wang, Peng Cao, Jiaqi Wang, and Osmar Zaiane. Uctransnet: Rethinking the skip connections in u-net from channel-wise perspective with transformer. In The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22), 2022a. Haotian Wang, Min Xian, and Aleksandar Vakanski. Ta-net: Topology-aware network for gland segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 15561564, 2022b. Hongyi Wang, Shiao Xie, Lanfen Lin, Yutaro Iwamoto, Xian-Hua Han, Yen-Wei Chen, and Ruofeng Tong. In ICASSP 2022-2022 IEEE International Mixed transformer u-net for medical image segmentation. Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 23902394. IEEE, 2022c. Shengxiang Wang, Ge Li, Min Gao, Linlin Zhuo, Mingzhe Liu, Zhizhong Ma, Wei Zhao, and Xiangzheng Fu. Gh-unet: group-wise hybrid convolution-vit for robust medical image segmentation. npj Digital Medicine, 8 (426), 2025b. Ziyang Wang, Jian-Qing Zheng, Yichi Zhang, Ge Cui, and Lei Li. Mamba-unet: Unet-like pure visual mamba for medical image segmentation. CoRR, abs/2402.05079, 2024. URL https://doi.org/10.48550/arXiv. 2402.05079. Katharina Wenderott, Jim Krups, Fiona Zaruchas, and Matthias Weigl. Effects of artificial intelligence implementation on efficiency in medical imaging√¢ƒÇ≈§a systematic literature review and meta-analysis. NPJ Digital Medicine, 7(1):265, 2024. Huisi Wu, Shihuai Chen, Guilian Chen, Wei Wang, Baiying Lei, and Zhenkun Wen. Fat-net: Feature adaptive transformers for automated skin lesion segmentation. Medical Image Analysis, 76:102327, 2022. Renkai Wu, Yinghao Liu, Pengchen Liang, and Qing Chang. H-vmunet: High-order vision mamba unet for medical image segmentation. Neurocomputing, 624:129447, 2025a. Renkai Wu, Yinghao Liu, Guochen Ning, Pengchen Liang, and Qing Chang. Ultralight vm-unet: Parallel vision mamba significantly reduces parameters for skin lesion segmentation. Patterns, 6(101298), 2025b. 52 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Ruichao Wu, Zihuan Yao, Xiangyu Lu, and Yide Ma. Mfmsnet: multi-frequency and multi-scale interactive cnn-transformer hybrid network for breast ultrasound image segmentation. Ultrasound in Medicine & Biology, 2023. Guoping Xu, Xuan Zhang, Xinwei He, and Xinglong Wu. Levit-unet: Make faster encoders with transformer for medical image segmentation. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pages 4253. Springer, 2023a. Qing Xu, Zhicheng Ma, Wenting Duan, et al. Dcsau-net: deeper and more compact split-attention u-net for medical image segmentation. Computers in Biology and Medicine, 154:106626, 2023b. Yiwen Xu, Tariq Khan, Yang Song, and Erik Meijering. Edge deep learning in computer vision and medical diagnostics: comprehensive survey. Artificial Intelligence Review, 58(3):93, 2025. Wenjun Yan, Yuanyuan Wang, Shengjia Gu, Lu Huang, Fuhua Yan, Liming Xia, and Qian Tao. The domain shift problem of medical image segmentation and vendor-adaptation by unet-gan. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 623631. Springer, 2019. Hongbo Ye, Fenghe Tang, Peiang Zhao, Zhen Huang, Dexin Zhao, Minghao Bian, and Kevin Zhou. U-rwkv: Lightweight medical image segmentation with direction-adaptive rwkv. arXiv preprint arXiv:2507.11415, 2025. Yunguang Ye, Ping Huang, Yu Sun, and Dachuan Shi. Mbsnet: deep learning model for multibody dynamics simulation and its application to vehicle-track system. Mechanical Systems and Signal Processing, 157: 107716, 2021. Xiang Yu, Yayan Chen, Guannan He, Qing Zeng, Yue Qin, Meiling Liang, Dandan Luo, Yimei Liao, Zeyu Ren, and Cheng et al. Kang. Simple is what you need for efficient and accurate medical image segmentation. arXiv preprint arXiv:2506.13415, 2025. Chunyu Yuan, Dongfang Zhao, and Sos Agaian. Mucm-net: mamba powered ucm-net for skin lesion segmentation. CoRR, 2024a. Haojun Yuan, Lingna Chen, and Xiaofeng He. Mmunet: Morphological feature enhancement network for colon cancer segmentation in pathological images. Biomedical Signal Processing and Control, 91:105927, 2024b. Hanxiao Zhang, Yifan Zhou, and Guo-Hua Wang. Dense vision transformer compression with few samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1582515834, 2024a. Mingya Zhang, Yue Yu, Sun Jin, Limei Gu, Tingsheng Ling, and Xianping Tao. Vm-unet-v2: rethinking vision mamba unet for medical image segmentation. In International symposium on bioinformatics research and applications, pages 335346. Springer, 2024b. Yingtao Zhang, Min Xian, Heng-Da Cheng, Bryar Shareef, Jianrui Ding, Fei Xu, Kuan Huang, Boyu Zhang, Chunping Ning, and Ying Wang. Busis: benchmark for breast ultrasound image segmentation. In Healthcare, volume 10, page 729. MDPI, 2022. Yundong Zhang, Huiye Liu, and Qiang Hu. Transfuse: Fusing transformers and cnns for medical image segmentation. In International conference on medical image computing and computer-assisted intervention, pages 1424. Springer, 2021. 53 U-Bench: Comprehensive Understanding of U-Net through 100-Variant Benchmarking Kevin Zhou, Hayit Greenspan, and Dinggang Shen. Deep learning for medical image analysis. Academic Press, 2017. Yanfeng Zhou, Lingrui Li, Le Lu, and Minfeng Xu. nnwnet: Rethinking the use of transformers in biomedical image segmentation and calling for unified evaluation benchmark. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2085220862, 2025. Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: nested u-net architecture for medical image segmentation. In International workshop on deep learning in medical image analysis, pages 311. Springer, 2018. Hancan Zhu, Jinhao Chen, and Guanghua He. Medvkan: Efficient feature extraction with mamba and kan for medical image segmentation. arXiv preprint arXiv:2505.11797, 2025."
        }
    ],
    "affiliations": [
        "HCNS",
        "MIRACLE Center",
        "University of Science and Technology of China"
    ]
}