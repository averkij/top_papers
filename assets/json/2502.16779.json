{
    "paper_title": "Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model",
    "authors": [
        "Yaxuan Huang",
        "Xili Dai",
        "Jianan Wang",
        "Xianbiao Qi",
        "Yixing Yuan",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a novel method for multi-view room layout estimation leveraging the 3D foundation model DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on a room layout dataset (Structure3D) with a modified objective to estimate structural planes. By generating uniform and parsimonious results, Plane-DUSt3R enables room layout estimation with only a single post-processing step and 2D detection results. Unlike previous methods that rely on single-perspective or panorama image, Plane-DUSt3R extends the setting to handle multiple-perspective images. Moreover, it offers a streamlined, end-to-end solution that simplifies the process and reduces error accumulation. Experimental results demonstrate that Plane-DUSt3R not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon.Our code is available at: https://github.com/justacar/Plane-DUSt3R"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 2 9 7 7 6 1 . 2 0 5 2 : r Published as conference paper at ICLR UNPOSED SPARSE VIEWS ROOM LAYOUT RECONSTRUCTION IN THE AGE OF PRETRAIN MODEL Yaxuan Huang1 Xili Dai2 Jianan Wang3 Xianbiao Qi4 Yixing Yuan1 Xiangyu Yue5 1Hong Kong Center for Construction Robotics, The Hong Kong University of Science and Technology 2The Hong Kong University of Science and Technology (Guangzhou) 3Astribot 4Intellifusion Inc. 5MMLab, The Chinese University of Hong Kong Figure 1: We present novel method for estimating room layouts from set of unconstrained indoor images. Our approach demonstrates robust generalization capabilities, performing well on both inthe-wild datasets (Zhou et al., 2018) and out-of-domain cartoon (Weber et al., 2024) data."
        },
        {
            "title": "ABSTRACT",
            "content": "Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires image muti-step solutions such as camera intrinsic and extrinsic estimation, matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach. To this end, we introduce Plane-DUSt3R, novel method for multiview room layout estimation leveraging the 3D foundation model DUSt3R. PlaneDUSt3R incorporates the DUSt3R framework and fine-tunes on room layout dataset (Structure3D) with modified objective to estimate structural planes. By generating uniform and parsimonious results, Plane-DUSt3R enables room layout estimation with only single post-processing step and 2D detection results. Unlike previous methods that rely on single-perspective or panorama image, PlaneDUSt3R extends the setting to handle multiple-perspective images. Moreover, it offers streamlined, end-to-end solution that simplifies the process and reduces error accumulation. Experimental results demonstrate that Plane-DUSt3R not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon. Our code is available at: https://github.com/justacar/Plane-DUSt3R"
        },
        {
            "title": "INTRODUCTION",
            "content": "3D room layout estimation aims to predict the overall spatial structure of indoor scenes, playing crucial role in understanding 3D indoor scenes and supporting wide range of applications. For example, room layouts could serve as reference for aligning and connecting other objects in indoor environment reconstruction (Nie et al., 2020). Accurate layout estimation also aids robotic path planning and navigation by identifying passable areas (Mirowski et al., 2016). Additionally, room Equal contribution, Corresponding author 1 Published as conference paper at ICLR 2025 layouts are essential in tasks such as augmented reality (AR) where spatial understanding is critical. Therefore, 3D room layout estimation has attracted considerable research attention with continued development of datasets (Zheng et al., 2020; Wang et al., 2022) and methods (Yang et al., 2022; Stekovic et al., 2020; Wang et al., 2022) over the past few decades. Methods for 3D room layout estimation (Zhang et al., 2015; Hedau et al., 2009; Yang et al., 2019) initially relied on the Manhattan assumption with single perspective or panorama image as input. Over time, advancements (Stekovic et al., 2020) have relaxed the Manhattan assumption to accommodate more complex settings, such as the Atlanta model, or even no geometric assumption at all. Recently, Wang et al. (2022) introduced multi-view approach, capturing single room with two panorama images, marking the first attempt to extend the input from single image to multiple images. Despite this progress, exploration in this direction remains limited, hindered by the lack of well-annotated multi-view 3D room layout estimation dataset. Currently, multi-view datasets with layout annotations are very scarce. Even the few existing datasets, such as Structure3D (Zheng et al., 2020), provide only small number of perspective views (typically ranging from 2 to 5). This scarcity of observable views highlights critical issue: widebaseline sparse-view structure from motion (SfM) remains an open problem. Most contemporary multi-view methods (Wang et al., 2022; Hu et al., 2022) assume known camera poses or start with noisy camera pose estimates. Therefore, solving wide-baseline sparse-view SfM would significantly advance the field of multi-view 3D room layout estimation. The recent development of large-scale training and improved model architecture offers potential solution. While GPT-3 (Brown, 2020) and Sora (Brooks et al., 2024) have revolutionized NLP and video generation, DUSt3R (Wang et al., 2024) brings paradigm shift for multi-view 3D reconstruction, transitioning from multi-step SfM process to an end-to-end approach. DUSt3R demonstrates the ability to reconstruct scenes from unposed images, without camera intrinsic/extrinsic or even view overlap. For example, with two unposed, potentially non-overlapping views, DUSt3R could generate 3D pointmap while inferring reasonable camera intrinsic and extrinsic, providing an ideal solution to the challenges posed by wide-baseline sparse-view SfM in multi-view 3D room layout estimation. In this paper, we employ DUSt3R to tackle the multi-view 3D room layout estimation task. Most single-view layout estimation methods (Yang et al., 2022) follow two-step process: 1) extracting 2D & 3D information, and 2) lifting the results to 3D layout with layout priors. When extending this approach to multi-view settings, an additional step is required: establishing geometric primitive correspondence across multi-view before the 3D lifting step. Given the limited number of views in existing multi-view layout datasets, this correspondence-establishing step essentially becomes sparse-view SfM problem. Hence, incorporating single-view layout estimation method with DUSt3R to handle multi-view layout estimation is natural approach. However, this may introduce challenge: independent plane normal estimation for each image fails to leverage shared information across views, potentially reducing generalizability to unseen data in the wild. To this end, we adopt DUSt3R to solve correspondence establishement and 3D lifting simultaneously, which jointly predict plane normal and lift 2D detection results to 3D. Specifically, we modify DUSt3R to estimate room layouts directly through dense 3D point representation (pointmap), focusing exclusively on structural surfaces while ignoring occlusions. This is achieved by retraining DUSt3R with the objective to predict only structural planes, the resulting model is named Plane-DUSt3R. However, dense pointmap representation is redundant for room layout, as plane can be efficiently represented by its normal and offset rather than large number of 3D points, which may consume significant space. To streamline the process, we leverage well-established off-the-shelf 2D plane detector to guide the extraction of plane parameters from the pointmap. We then apply post-processing to obtain plane correspondences across different images and derive their adjacency relationships. Compared to existing room layout estimation methods, our approach introduces the first pipeline capable of unposed multi-view (perspective images) layout estimation. Our contributions can be summarized as follows: 1. We propose an unposed multi-view (sparse-view) room layout estimation pipeline. To the best of our knowledge, this is the first attempt at addressing this natural yet underexplored setting in room layout estimation. 2. The introduced pipeline consists of three parts: 1) 2D plane detector, 2) 3D information prediction and correspondence establishment method, Plane-DUSt3R, and 3) post-processing algorithm. The 2D detector was retrained with SOTA results on the Structure3D dataset (see TaPublished as conference paper at ICLR 2025 ble 3). The Plane-DUSt3R achieves 5.27% and 5.33% improvement in RRA and mAA metrics, respectively, for the multi-view correspondence task compared to state-of-the-art methods (see Table 2). 3. In this novel setting, we also design several baseline methods for comparison to validate the advantages of our pipeline. Specifically, we outperform the baselines by 4 projection 2D metrics and 1 3D metric respectively (see Table 1). Furthermore, our pipeline not only performs well on the Structure3D dataset (see Figure 6), but also generalizes effectively to in-the-wild datasets (Zhou et al., 2018) and scenarios with different image styles such as cartoon style (see Figure 1)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Layout estimation. Most room layout estimation research focuses on single-perspective image inputs. Stekovic et al. (2020) formulates layout estimation as constrained discrete optimization problem to identify 3D polygons. Yang et al. (2022) introduces line-plane constraints and connectivity relations between planes for layout estimation, while Sun et al. (2019) formulates the task as predicting 1D layouts. Other studies, such as Zou et al. (2018), propose to utilize monocular 360-degree panoramic images for more information. Several works extend the input setting from single panoramic to multi-view panoramic images, e.g. Wang et al. (2022) and Hu et al. (2022). However, there is limited research addressing layout estimation from multi-view RGB perspective images. Howard-Jenkins et al. (2019) detects and regresses 3D piece-wise planar surfaces from series of images and clusters them to obtain the final layout, but this method requires posed images. The most related work is Jin et al. (2021), which focuses on different task: reconstructing indoor scenes with planar surfaces from wide-baseline, unposed images. It is limited to two views and requires an incremental stitching process to incorporate additional views. Holistic scene understanding. Traditional 3D indoor reconstruction methods are widely applicable but often lack explicit semantic information. To address this limitation, recent research has increasingly focused on incorporating holistic scene structure information, enhancing scene understanding by improving reasoning about physical properties, mostly centered on single-perspective images. Several studies have explored the detection of 2D line segments using learning-based detectors (Zhou et al., 2019; Pautrat et al., 2021; Dai et al., 2022). However, these approaches often struggle to differentiate between texture-based lines and structural lines formed by intersecting planes. Some research has focused on planar reconstruction to capture higher-level information (Liu et al., 2018; Yu et al., 2019; Liu et al., 2019). Certain studies (Huang et al., 2018; Nie et al., 2020; Sun et al., 2021) have tackled multiple tasks alongside layout reconstruction, such as depth estimation, object detection, and semantic segmentation. Other works operate on constructed point maps; for instance, Yue et al. (2023) reconstructs floor plans from density maps by predicting sequences of room corners to form polygons. SceneScript (Avetisyan et al., 2024) employs large language models to represent indoor scenes as structured language commands. Multi-view pose estimation and reconstruction. The most widely applied pipeline for pose estimation and reconstruction on series of images involves SfM (Schonberger & Frahm, 2016) and MVS (Schonberger et al., 2016), which typically includes steps such as feature mapping, finding correspondences, solving triangulations and optimizing camera parameters. Most mainstream methods build upon this paradigm with improvements on various aspects of the pipeline. However, recent works such as DUSt3R (Wang et al., 2024) and MASt3R (Leroy et al., 2024) propose reconstruction pipeline capable of producing globally-aligned pointmaps from unconstrained images. This is achieved by casting the reconstruction problem as regression of pointmaps, significantly relaxing input requirements and establishing simpler end-to-end paradigm for 3D reconstruction."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we formulate the layout estimation task, transitioning from single-view to multiview scenario. We then derive our multi-view layout estimation pipeline as shown in Figure 2 (Section 3.1). Our pipeline consists of three parts: 2D plane detector f1, 3D information prediction and correspondence establishment method Plane-DUSt3R f2 (Section 3.2), and post-processing algorithm f3 (Section 3.3). 3 Published as conference paper at ICLR 2025 Figure 2: Our multi-view room layout estimation pipeline. It consists of three parts: 1) 2D plane detector f1, 2) 3D information prediction and correspondence establishment method PlaneDUSt3R f2, and 3) post-processing algorithm f3. 3.1 FORMULATION OF THE MULTI-VIEW LAYOUT ESTIMATION TASK We begin by revisiting the single-view layout estimation task and unifying the formulation of existing methods. Next, we extend the formulation from single-view to multiple-view setting, providing detailed analysis and discussion focusing on the choice of solutions. Before formulating the layout estimation task, we adopt the geometric primitives + relationships representation from Zheng et al. (2020) to model the room layout. Geometric Primitives. - Planes: The scene layout could be represented as set of planes {P1, P2 . . .} in 3D space and their corresponding 2D projections {p1, p2, . . .} in images. Each plane is parameterized by its normal S2 and offset d. For 3D point R3 lying on the plane, we have nT + = 0. - Lines & Junction Points: In 3D space, two planes intersect at 3D line, three planes intersect at 3D junction point. We denote the set of all 3D lines/junction points in the scene as {L1, L2 . . .}/{J1, J2 . . .} and their corresponding 2D projections as {l1, l2, . . .}/{j1, j2, . . .} in images. Relationships. - Plane/Line relationships: An adjacent matrix Wp/Wl {0, 1} is used to model the relationship between planes/lines. Specifically, Wp(i, j) = 1 if and only if Pi and Pj intersect along line; otherwise, Wp(i, j) = 0. Similarly to plane relationship, Wl(i, j) = 1 if and only if Li and Lj intersect at certain junction, otherwise, Wl(i, j) = 0. The pipeline of single-view layout estimation methods (Liu et al., 2019; Yang et al., 2022; Liu et al., 2018; Stekovic et al., 2020) can be formulated as: f1 {2D, 3D} f3 {P , L, , }, (1) where f1 is function that predicts 2D and 3D information from the input single view. Generally speaking, the final layout result {P , L, , } can be directly inferred from the outputs of f1. However, errors arising from f1 usually adversely affect the results. Hence, refinement step that utilizes prior information about room layout is employed to further improve the performance. Therefore, f3 typically encompasses post-processing and refinement steps where the post-processing step generates an initial layout estimation, and the refinement step improves the final results. For instance, Yang et al. (2022) chooses the HRnet network (Wang et al., 2020) as f1 backbone to extract 2D plane p, line l, and predict 3D plane normal and offset from the input single view. After obtaining the initial 3D layout from the outputs of f1, the method reprojects the 3D Published as conference paper at ICLR 2025 line to 2D line ˆl on the image and compares it with the detected line from f1. f3 minimizes the error ˆl l2 2 to optimize the 3D plane normal. In other words, it uses the better-detected 2D line to improve the estimated 3D plane normal. In contrast, Stekovic et al. (2020) uses different approach: its f1 predicts 2.5D depth map instead of 2D line and uses the more accurate depth results to refine the estimated 3D plane normal. Among the works that follow the general framework of 1 (Liu et al., 2019; 2018), Yang et al. (2022) stands out as the best single-view perspective image layout estimation method without relying on the Manhattan assumption. Therefore, we present its formulation in equation (2) and extend it to multi-view scenarios. f1 {p, l, n, d} f3 {P , L, , }, (2) In room layout estimation from unposed multi-view images, two primary challenges aris: 1) camera pose estimation, and 2) 3D information estimation from multi-view inputs. Camera pose estimation is particularly problematic given the scarcity of annotated multi-view layout dataset. Thanks to the recent advancements in 3D vision with pretrain model, this challenge could be effectively bypassed: DUSt3R (Wang et al., 2024) has demonstrated the ability to reconstruct scenes from unposed images without requiring camera intrinsic or extrinsic, and even without overlap between views. Moreover, the 3D pointmap generated from DUSt3R can provide significantly improved 3D information, such as plane normal and offset, compared to single-view methods (Yang et al., 2022) (see Table 1 of experiment section). Therefore, DUSt3R represents critical advancement in extending singleview layout estimation to multi-view scenarios. Before formulating the multi-view solution, we first present the key 3D representation of DUSt3R: the pointmap and the camera pose . The camera pose is obtained through global alignment, as described in the DUSt3R (Wang et al., 2024)). - Pointmap X: Given set of RGB images {I1, . . . , In} RHW 3, captured from distinct viewpoints of the same indoor scene, we associate each image Ii with canonical pointmap Xi RHW 3. The pointmap represents one-to-one mapping from each pixel (u, v) in the image to corresponding 3D point in the world coordinate frame: (u, v) R2 (cid:55) X(u, v) R3. - Camera Pose : Each image Ii is associated with camera-to-world pose Ti SE(3). Now, the sparse-view layout estimation problem can be formulated as shown in equation (3) {I1, I2, . . .} f1,f2 {p, l, X, } f3 {P , L, , }. (3) In this work, we adopt the HRnet backbone from Yang et al. (2022) as f1. In the original DUSt3R (Wang et al., 2024) formulation, the ground truth pointmap obj represents the 3D coordinates of the entire indoor scene. In contrast, we are interested in plane pointmap that represents the 3D coordinates of structural plane surfaces, including walls, floors, and ceilings. This formulation intentionally disregards occlusions caused by non-structural elements, such as furniture within the room. Our objective is to predict the scene layout pointmap without occlusions from objects, even when the input images contain occluding elements. For simplicity, any subsequent reference to in this paper refers to the newly defined plane pointmap p. We introduce Plane-DUSt3R as f2 and directly infer the final layout via f3 without the need for any refinement. 3.2 f2: PLANE-BASED DUST3R The original DUSt3R outputs pointmaps that capture all 3D information in scene, including furniture, wall decorations, and other objects. However, such excessive information introduces interference when extracting geometric primitives for layout prediction, such as planes and lines. To obtain structural plane pointmap X, we modify the data labels from the original depth map (Figure 4 (a)) to the structural plane depth map (Figure 4 (b)), and then retrain the DUSt3R model. This updated objective guides DUSt3R to predict the pointmap of the planes while ignoring other objects. The original DUSt3R does not guarantee output at metric scale, so we also trained modified version of Plane-DUSt3R that produces metric-scale results. Given set of image pairs = {(Ii, Ij) = j, 1 i, n, RHW 3}, for each image pair, the model comprises two parallel branches. As shown in Figure 3, the detail of the architecture can be found in Appendix A. The regression loss function is defined as the 5 Published as conference paper at ICLR 2025 Figure 3: Plane-DUSt3R architecture remains identical to DUSt3R. The transformer decoder and regression head are further fine-tuned on the occlusion-free depth map (see Figure 4). 1 X v scale-invariant Euclidean distance between the normalized predicted and ground-truth pointmaps: lregr(v, i) = 1 2, where view {1, 2} and is the pixel index. The scaling factors and represent the average distance of all corresponding valid points to the origin. In addition, by incorporating the confidence loss, the model implicitly learns to identify regions that are more challenging to predict. As in DUSt3R (Wang et al., 2024), the confidence loss is defined as: Lconf = (cid:80) ℓregr(v, i) α log υ,1 iDv υ,1 , where Dv {1, . . . , H} {1, . . . , } are sets of valid pixels on which the ground truth is defined. v{1,2} (cid:80) Structural plane depth map. The Structure3D dataset provides ground truth plane normal and offset, allowing us to re-render the plane depth map at the same camera pose (as shown in Figure 4). We then transform the structural plane depth map Dp to pointmap in the camera coordinate i,j], where R33 is frame v. This transformation is given by the camera intrinsic matrix. Further details of this transformation can be found in Wang et al. (2024). i,j = K1[iDp i,j, jDp i,j, Dp Metric-scale. In the multi-view setting, scale variance is required, which differs from DUSt3R. To accommodate this, we modify the regression loss to bypass normalization for the predicted pointmaps when the ground-truth pointmaps are metric. Specifically, we set := whenever the ground-truth is metric, resulting in the following loss function lregr(v, i) = i v 2 2/z. (a) The original DUSt3R depth map. (b) The Plane-DUSt3R depth map. Figure 4: The (a) original DUSt3R depth map and (b) occlusion removed depth map. 3. f3: POST-PROCESSING In this section, we introduce how to combine the multi-view plane pointmaps and 2D detection results p, to derive the final layout {P , L, , }. For each single view Ii, we can infer partial layout result { Pi, Li, Ji, Wi} = g1(Xi, pi, li) from the single view pointmaps Xi and 2D detection results pi, li through post-process algorithm g1 in camera coordinate. Then, correspondence-establish and merging algorithm g2 combines all partial results to get the final layout {P , L, , } = g2({ P1, L1, J1, W1}, . . .). Single-view room layout estimation g1. For an image Ii, g1 mainly addresses two tasks: 1) lifting 2D planes to 3D camera coordinate space with 3D normal from pointmap Xi, and 2) inferring the wall adjacency relationship. We follow the post-processing procedure in Yang et al. (2022) but with two improvements. First, the plane normal and offset are inferred from Xi instead of directly regressed by network f1. The points from Xi which belong to same plane are used to calculate 6 Published as conference paper at ICLR 2025 (a) Projected Lines (b) Rotated Lines (c) Aligned Lines (d) Correspondance Figure 5: (a) Planes are projected onto the x-z plane as 2D line segments. (b) The scene is rotated so that line segments are approximately horizontal or vertical. (c) Line segments are classified and aligned to be either horizontal or vertical. (d) Merged planes are shown, with segments belonging to the same plane indicated by the same color and index. and d. Second, with the better 3D information pointmap Xi we can better infer pseudo wall adjacency through the depth consistency of inferred plane intersection (inferred from 2D plane p) and predicted line region (extracted from the region of Xi). In our experiments, the depth consistency tolerance ϵ1 is set to 0.005. Multi-view room layout estimation g2. Based on the results of g1, g2 uses the global alignment of DUSt3R (refer to appendix A) to get the camera pose Ti for each image Ii. Then, we can transform all partial layout results ({ P1, L1, J1, W1}, . . .) to the same coordinate space. In this coordinate space, we establish correspondence for each plane, then merge and assign unique ID to them. Since we allow at most one floor and one ceiling detection per image, we simply average the parameters from all images to obtain the final floor and ceiling parameters. As for walls, we assume all walls are perpendicular to both the floor and ceiling. To simplify the merging process, we project all walls onto the x-z plane defined by the floor and ceiling. This projection reduces the problem to 2D space. making it easier to identify and merge corresponding walls. Figure 5 illustrates the entire process of merging walls. Each wall in an image is denoted as one line segment, as shown in Figure 5a. We then rotate the scene so that all line segments are approximately horizontal or vertical, as depicted in 5b. In Figure 5c, each line segment is classified and further rotated to be either horizontal or vertical, based on the assumption that all adjacent walls are perpendicular to each other. Figure 5d shows the final result after the merging process. The merging process could be regarded as classical Minimum Cut problem. In Figure 5d, all line segments can be regarded as node in graph, two nodes have connection if and only if they satisfy two constraints. 1) they belong to the same categories (vertical or horizontal). 2) they do not appear in the same image. 3) they are not across with the other category of node. Finally, the weight of each connection is settled as the Euclidean distance of their line segment centers. Based on this established graph, the merging results are the optimal solution of the minimum cut on this graph. The detail of the merging process can be found in Algorithm 1 of Appendix."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETTINGS. Dataset. Structured3D (Zheng et al., 2020) is synthetic dataset that provides large collection of photo-realistic images with detailed 3D structural annotations. Similar to Yang et al. (2022), the dataset is divided into training, validation, and test sets at the scene level, comprising 3000, 250, and 250 scenes, respectively. Each scene consists of multiple rooms, with each room containing 1 to 5 images captured from different viewpoints. To construct image pairs that share similar visual content, we retain only rooms with at least two images. Within each room, images are paired to form image sets. Ultimately, we obtained 115,836 image pairs for the training set and 11,030 image pairs for the test set. For validation, we assess all rooms from the validation set. For rooms that only have one image, we duplicate that image to form image pairs for pointmap retrieval. In the subsequent inference process, we retain only one pointmap per room. 7 Published as conference paper at ICLR 2025 Figure 6: Qualitative results on Structure3D testing set. The first 3 columns are input views, the fourth and fifth columns are layout results of Noncuboid+MASt3R and our pipeline respectively. Due to space limitations, we refer reader to appendix for more complete results. Training details. During training, we initialize the model with the original DUSt3R checkpoint. We freeze the encoder parameters and fine-tune only the decoder and DPT heads. Our data augmentation strategy follows the same approach as DUSt3R, using input resolution of 512 512. We employ the AdamW optimizer (Loshchilov & Hutter, 2017) with cosine learning rate decay schedule, starting with base learning rate of 1e-4 and minimum of 1e-6. The model is trained for 20 epochs, including 2 warm-up epochs, with batch size of 16. We train two versions Plane-DUSt3R, one with metric-scale loss and the other one without it. We name the metric-scale one as Plane-DUSt3R (metric) and the other one as Plane-DUSt3R . Evaluation. Following the task formulation in equation (3), our evaluation protocol consists of three parts to assess f1, f2, and the overall performance, respectively. For the 2D information extraction module f1, we use the same metric as Yang et al. (2022) for comparison: Intersection over Union (IoU), Pixel Error (PE), Edge Error (EE), and Root Mean Square Error (RMSE). For the multi-view information extraction module f2, we report the Relative Rotation Accuracy (RRA) and Relative Translation Accuracy (RTA) for each image pair to evaluate the relative pose error. We use threshold of τ = 15 to report RTA@15 and RRA@15 (The comprehensive results of different thresholds can be seen in Table 4 of Appendix C.1). Additionally, we calculate the mean Average Accuracy (mAA30), defined as the area under accuracy curve of the angular differences at min(RRA@30, RTA@30). Finally, for evaluating the overall layout estimation, we employ 3D precision and 3D recall of planes as metrics. predicted plane is considered matched with ground truth plane if and only if the angular difference between them is less than 10and the offset difference is less than 0.15m. Each ground truth plane can be matched only once. Baselines. As this work is the first attempt at 3D room layout estimation from multi-view perspective images, there are no existing baseline methods for direct comparison. Therefore, we design two reasonable baseline methods. We also compare our f1 and f2 with other methods of the same type. Since we use Noncuboid (Yang et al., 2022) as our f1, we not only compare it against the baselines from their paper (Liu et al., 2019; Stekovic et al., 2020), but also retrain it with better hyperparameters obtained through grid search. For f2 (Plane-DUSt3R), we compare it to recent data-driven image matching DUSt3R (Wang et al., 2024) and MASt3R (Leroy et al., 2024). Finally, for the overall multi-view layout baseline, we design two methods: 1) Noncuboid with ground truth camera poses and 2) Noncuboid with MASt3R. In this context, we introduce the fusion of MASt3R (Leroy et al., 2024) and NonCuboid (Yang et al., 2022) as our baseline method. MASt3R further extends DUSt3R, enabling it to estimate camera poses at metric scale from sparse image inputs. We employ the original NonCuboid method to obtain single-view layout reconstruction. Next, we utilize the predicted camera poses to unify all planes from their respective camera poses into common coordinate system. For instance, we designate the coordinate system of the first image as the world coordinate system. We then perform the same operation as described 8 Published as conference paper at ICLR 2025 Figure 7: Birdview of multi-view 3D planes aligned to the same coordinate. The first row shows 5 cases of our pipeline results after post-processing step. The second row is the results of Noncuboid+MASt3R. Line segments of the same color indicate that they belong to the same plane. Table 1: Quantitative results on Structure3D dataset. Method Noncuboid + MASt3R Noncuboid + GT pose Ours (metric) Ours (aligned) re-IoU(%) 74.51 75.93 75.34 76.84 re-PE(%) 8.57 7.97 8.60 7.82 re-EE 12.72 11.37 10.83 9.53 re-RMSE 0.4831 0.4457 0.4388 0.4099 3D-precision(%) 37.00 46.96 48.98 52. 3D-recall(%) 43.39 50.59 45.35 48.37 in Sec 3.3 to achieve the final multi-view reconstruction. The Noncuboid with ground truth camera poses is introduced as an ablation study to eliminate the effects of inaccurate pose estimation. The experimental setup is the same as the Noncuboid with MASt3R pipeline, except for the use of ground truth camera poses instead of poses estimated by MASt3R. 4.2 MULTI-VIEW ROOM LAYOUT ESTIMATION RESULTS In this section, we compare our multi-view layout estimation pipeline with two baseline methods, both qualitatively and quantitatively. Additionally, we conduct experiments to verify the effectiveness of our pipeline components f1 2D detector and f2 Plane-DUSt3R. Layout results comparison. Table 1 and Figure 6 present quantitative and qualitative comparisons of our pipeline with two baseline methods. Ours (metric) and Ours (aligned) in Table 1 refer to the methods from our pipeline using Plane-DUSt3R (metric) and Plane-DUSt3R, respectively. The first 4 metrics (re-IoU, re-PE, re-EE, and re-RMSE) are calculated similarly to their 2D counterparts (IoU, PE, EE, and RMSE), except that the predicted 2D results are reprojected from the estimated multi-view 3D layout. Compared with the baseline methods, Plane-DUSt3R achieves superior 3D plane normal estimations compared to Noncuboids single-view plane normal estimation, even when using ground truth camera pose (Noncuboid + GT pose). Figure 7 further demonstrates that PlaneDUSt3R could predict accurate and robust 3D information with sparse-view input. Table 2: Comparison with data-driven image matching approaches. Methods RealEstate10K Structured3D CAD-estate mAA@30 RRA@15 RTA@15 mAA@30 RRA@15 RTA@15 mAA@ (a) (b) DUSt3R (Wang et al., 2024) MASt3R (Leroy et al., 2024) Plane-DUSt3R (metric) Plane-DUSt3R (aligned) 61.2 76.4 - - 89.44 92.94 98.21 97.95 85.00 89.77 96.66 96.59 76.13 85.34 90.67 91. 99.88 99.94 94.61 94.96 84.82 99.00 70.52 73.74 76.38 95.29 61.48 64. 3D information prediction and correspondence-established method Plane-DUSt3R f2. Table 2 shows the comparison results of our Plane-DUSt3R (part (b) in Table 2), recent popular datadriven image matching approaches (part (a) in Table 2) in RealEstate10K (Zhou et al., 2018), Structure3D (Zheng et al., 2020), and CAD-Estate (Rozumnyi et al., 2023) datasets. Note that in part(b), results on RealEstate10K dataset are not provided since our model is specifically designed to predict 9 Published as conference paper at ICLR 2025 Table 3: 2D detectors comparison on Structure3D dataset. Method Planar R-CNN (Liu et al., 2019) Rac (Stekovic et al., 2020) Noncuboid (Yang et al., 2022) Noncuboid (re-trained) IoU(%) 79.64 76.29 79.94 80. PE(%) EE RMSE 0.4013 6.58 0.3865 7.19 0.2827 6.80 0.2631 6.41 7.04 8.07 6.40 6.13 Figure 8: Qualitative results on in-the-wild data (Zhou et al., 2018). The first three columns are input views, the fourth column is the layout results of Noncuboid+MASt3R. The rightmost column shows the predicted plane pointmap with the extracted wireframe drawn in red. room structural elements, while RealEstate10K contains outdoor scenes that may lead to prediction failures. Instead, we utilize CAD-Estate dataset, which is derived from RealEstate10K with additional room layout annotations, as more suitable benchmark for comparison. The results of parts (a) and (b) on three datasets show the advancements of MASt3R, not only in traditional multi-view datasets (RealEstate10K,CAD-Estate), but also in sparse-view dataset (Structure3D). Plane-DUSt3R could get better performance compared to the previous SOTA MASt3R. One arguable point is that Plane-DUSt3R is obviously better since it is fine-tuned on Strucutre3D. That is the message we want to convey. DUSt3R/MASt3R are the SOTAs in both multi-view and saprse-view camera pose estimation tasks. After our improvements (section 3.2) and fine-tuning, Plane-DUSt3R could get 5.33 points better on the sparse-view layout dataset. Comparison of 2D detectors (f1). We retrain the Noncuboid method with more thorough hyperparameter grid search, resulting in an improved version. Table 3 compares its results with other baseline methods from Yang et al. (2022). Comparison of various input views. We experiment the impact of different number of input views in Appendix C.2. The results in Table 5 show general improvement trend as the number of views increases. 4.3 GENERALIZABILITY TO UNKNOWN AND OUT-OF-DOMAIN DATA Figure 1 and 12 also demonstrate the generalizability of our pipeline. It not only performs well on the testing set of Structure3D (Figure 6), but also generalizes well to new datasets, such as RealEstate10K (Figure 8 shows examples from this dataset). Furthermore, our pipeline proves effective even with data in the wild as shown in appendix in Figure 11,12. We also experiment our pipeline on the dataset CAD-estate (see Appendix for details)."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper introduces the first pipeline for multi-view layout estimation, even in sparse-view settings. The proposed pipeline encompasses three components: 2D plane detector, 3D information prediction and correspondence establishment method, and post-processing algorithm. As the first comprehensive approach to the multi-view layout estimation task, this paper provides detailed analysis and formulates the problem under both single-view and multi-view settings. Additionally, we design several baseline methods for comparison to validate the effectiveness of our pipeline. Our approach consistently outperforms the baselines on both 2D projection and 3D metrics. Furthermore, our pipeline not only performs well on the synthetic Structure3D dataset, but generalizes effectively to in-the-wild datasets and scenarios with different image styles such as the cartoon style. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work is partially supported by the Hong Kong Center for Construction Robotics (Hong Kong ITC-sponsored InnoHK center), the National Natural Science Foundation of China (Grant No. 62306261), CUHK Direct Grants (Grant No. 4055190), and The Shun Hing Institute of Advanced Engineering (SHIAE) No. 8115074."
        },
        {
            "title": "REFERENCES",
            "content": "Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, et al. Scenescript: Reconstructing scenes with an autoregressive structured language model. arXiv preprint arXiv:2403.13064, 2024. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Xili Dai, Haigang Gong, Shuai Wu, Xiaojun Yuan, and Yi Ma. Fully convolutional line parsing. Neurocomputing, 506:111, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Varsha Hedau, Derek Hoiem, and David Forsyth. Recovering the spatial layout of cluttered rooms. In 2009 IEEE 12th international conference on computer vision, pp. 18491856. IEEE, 2009. Henry Howard-Jenkins, Shuda Li, and Victor Prisacariu. Thinking outside the box: Generation of unconstrained 3d room layouts. In Computer VisionACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part 14, pp. 432448. Springer, 2019. Zhihua Hu, Bo Duan, Yanfeng Zhang, Mingwei Sun, and Jingwei Huang. Mvlayoutnet: 3d layout reconstruction with multi-view panoramas. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 12891298, 2022. Siyuan Huang, Siyuan Qi, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, and Song-Chun Zhu. Holistic 3d scene parsing and reconstruction from single rgb image. In Proceedings of the European conference on computer vision (ECCV), pp. 187203, 2018. Linyi Jin, Shengyi Qian, Andrew Owens, and David Fouhey. Planar surface reconstruction from sparse views. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1299113000, 2021. Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. arXiv preprint arXiv:2406.09756, 2024. Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Yasutaka Furukawa. Planenet: Piecewise planar reconstruction from single rgb image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 25792588, 2018. Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, and Jan Kautz. Planercnn: 3d plane detection and reconstruction from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 44504459, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 11 Published as conference paper at ICLR 2025 Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016. Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian Jun Zhang. Total3dunderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5564, 2020. Remi Pautrat, Juan-Ting Lin, Viktor Larsson, Martin Oswald, and Marc Pollefeys. Sold2: Selfsupervised occlusion-aware line description and detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1136811378, 2021. Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1217912188, 2021. Denys Rozumnyi, Stefan Popov, Kevis-Kokitsi Maninis, Matthias Nießner, and Vittorio Ferrari. Estimating generic 3d room structures from 2d annotations, 2023. URL https://arxiv. org/abs/2306.09077. Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016. Sinisa Stekovic, Shreyas Hampali, Mahdi Rad, Sayan Deb Sarkar, Friedrich Fraundorfer, and Vincent Lepetit. General 3d room layout from single view by render-and-compare. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVI 16, pp. 187203. Springer, 2020. Cheng Sun, Chi-Wei Hsiao, Min Sun, and Hwann-Tzong Chen. Horizonnet: Learning room layout In Proceedings of the IEEE/CVF with 1d representation and pano stretch data augmentation. Conference on Computer Vision and Pattern Recognition, pp. 10471056, 2019. Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet: 360 indoor holistic understanding with latent horizontal features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 25732582, 2021. Haiyan Wang, Will Hutchcroft, Yuguang Li, Zhiqiang Wan, Ivaylo Boyadzhiev, Yingli Tian, and Sing Bing Kang. Psmnet: Position-aware stereo merging network for room layout estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 86168625, 2022. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 43 (10):33493364, 2020. Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024. Ethan Weber, Riley Peterlinz, Rohan Mathur, Frederik Warburg, Alexei A. Efros, and Angjoo Kanazawa. Toon3d: Seeing cartoons from new perspective. In arXiv, 2024. Cheng Yang, Jia Zheng, Xili Dai, Rui Tang, Yi Ma, and Xiaojun Yuan. Learning to reconstruct 3d non-cuboid room layout from single rgb image. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 25342543, 2022. 12 Published as conference paper at ICLR 2025 Shang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka, Min Sun, and Hung-Kuo Chu. Dula-net: dual-projection network for estimating room layouts from single rgb panorama. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 33633372, 2019. Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, and Shenghua Gao. Single-image piece-wise planar 3d reconstruction via associative embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10291037, 2019. Yuanwen Yue, Theodora Kontogianni, Konrad Schindler, and Francis Engelmann. Connecting the dots: Floorplan reconstruction using two-level queries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 845854, 2023. Yinda Zhang, Fisher Yu, Shuran Song, Pingmei Xu, Ari Seff, and Jianxiong Xiao. Large-scale scene understanding challenge: Room layout estimation. In CVPR Workshop, volume 3, 2015. Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3d: large photo-realistic dataset for structured 3d modeling. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IX 16, pp. 519535. Springer, 2020. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. Yichao Zhou, Haozhi Qi, and Yi Ma. End-to-end wireframe parsing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 962971, 2019. Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem. Layoutnet: Reconstructing the 3d room layout from single rgb image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 20512059, 2018. 13 Published as conference paper at ICLR DUST3R DETAILS Given set of RGB images {I1, I2, . . . , In} RHW 3, we first pair them to create set of image pairs = {(Ii, Ij) = j, 1 i, n}. For each image pair (Ii, Ij) P, the model estimates two point maps Xi,i, Xj,i, along with their corresponding confidence maps Ci,i, Cj,i. Specifically, both pointmaps are expressed in the camera coordinate system of Ii, which implicitly accomplishes dense 3D reconstruction. The model consists of two parallel branches, as shown in Fig 3, each branch responsible for processing one image. The two images are first encoded in Siamese manner with weight-sharing ViT encoder(Dosovitskiy et al., 2020) to produce two latent features F1, F2: Fi = Encoder(Ii). Next, F1, F2 are fed into two identical decoders that continuously share information through crossattention mechanisms. By leveraging cross-attention mechanisms, the model is able to learn the relative geometric relationships between the two images. Specifically, for each encoder block: G1,i = DecoderBlock1,i(G1,i1, G2,i1), G2,i = DecoderBlock2,i(G1,i1, G2,i1) where G1,0 := F1, G2,0 := F2. Finally, the DPT(Ranftl et al., 2021) head regresses the pointmap and confidence map from the concatenated features of different layers of the decoder tokens: (4) X1,1, C1,1 = Head1(G1,0, G1,1, . . . , G1,B) X2,1, C2,1 = Head2(G2,0, G2,1, . . . , G2,B) where is the number of decoder blocks. The regression loss function is defined as the scaleinvariant Euclidean distance between the normalized predicted and ground-truth pointmaps: (5) lregr(v, i) = 1 v,1 1 X v,1 2 2 (6) where {1, 2} and is the pixel index. The scaling factors and represent the average distance of all corresponding valid points to the origin. The original DUSt3R couldnt guarantee output at metric scale, so we also trained modified version of Plane-DUSt3R that produces metricscale results. The key change we made was setting := z. By introducing the regression loss in confidence loss, the model could implicitly learn how to identify regions that are more challenging to predict compared to others. Same as in DUSt3R (Wang et al., 2024): Lconf = (cid:88) (cid:88) v{1,2} iDv υ,1 ℓregr(v, i) α log υ,1 (7) To obtain the ground-truth pointmaps v,1 , we first transform the ground truth depthmap RHW into pointmap express in the camera coordinate of by i,j = K1[iDi,j, jDi,j, Di,j] with camera intrinsic matrix R33. Then we obtain v,1 by 1 Tvh(X v) with T1, Tv R34 the camera-to-world poses and being the homoX v,1 = 1 geneous transformation. Global Alignment For global alignment, we aim to assign global pointmap and camera pose for each image. First, the average confidence scores of each pair of images are utilized as the similarity scores. higher value of confidence implies stronger visual similarity between the two images. These scores are employed to construct Minimum Spanning Tree, denoted as G(V, E), where each vertex corresponding to an image in the input set and each edge = (n, m) indicates that images In and Im share significant visual content. We aim to find globally aligned point maps{χn RHW 3} and transformation Ti R34 than transform the prediction into the world coordinate frame. To do this, for each image pair = (n, m) we have two point maps n,n, m,n and their confidence maps Cn,n, Cm,n. For simplicity, we use the annotation n,e := n,n, m,e := m,n. Since n,e and m,e are in the same coordinate frame, Te := Tn should align both point maps with the world-coordinate. We then solve the following optimization problem: χ = arg min χ,T,σ (cid:88) (cid:88) HW (cid:88) v,e χv σeTeX v,e 2 2 . (8) i=1 where means can be either or for the pair and σe is positive scaling factor . To avoid the trivial solution where σe = 0, we ensure that (cid:81) eE ve σe = 1 14 Published as conference paper at ICLR f3 ALGORITHM The goal of multi-view layout estimation is similar to that of single-view: we need to estimate 3D parameters for each plane and determine the relationships between adjacent planes. However, in multi-view setting, we must ensure that each plane represents unique physical plane in 3D space. The main challenge in multi-view reconstruction is that the same physical plane may appear in multiple images, causing duplication. Our task is to identify which planes correspond to the same physical plane across different images and merge them, keeping only one representation for each unique plane. Since we allow at most one floor and one ceiling detection per image, we simply average the parameters from all images to obtain the final floor and ceiling parameters. As for walls, we assume all walls are perpendicular to both the floor and ceiling. To simplify the merging process, we project all walls onto the x-z plane defined by the floor and ceiling. This projection reduces the problem to 2D space, making it easier to identify and merge corresponding walls. Figure 5 illustrates the entire process of merging walls. Each wall in an image is denoted as one line segment, as shown in Figure 5a. We then rotate the scene so that all line segments are approximately horizontal or vertical, as depicted in 5b. In Figure 5c, each line segment is classified and further rotated to be either horizontal or vertical, based on the assumption that all adjacent walls are perpendicular to each other. Algorithm 1 Merge Plane if lines.image id in cluster.image id then end if if distance(line, cluster.centroid)< proximity threshold then if overlap(line, cluster.centroid)> overlap threshold then end if if not intersect(line, cluster, horizontalLines, margin) then continue ound False for each cluster in clusters do Insert line into cluster ound True break Require: vertical lines, horizontal lines 1: Sort verticalLines by x-axis value 2: Initialize clusters with the first segment. 3: for each segment in verticalLines[1, :] do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end for 22: if ound == False then 23: 24: 25: end if Ensure: Clusters Create new cluster with line Append the new cluster to clusters Insert line into cluster ound True break end if end for end if"
        },
        {
            "title": "C ADDITIONAL QUANTITATIVE RESULTS",
            "content": "C.1 PERFORMANCE UNDER VARYING THRESHOLDS To give more complete view of the performance of the Plane-DUSt3R method, we present the results under various threshold settings (the threshold of both cameras translation and rotation) in Table 4. 15 Published as conference paper at ICLR 2025 Table 4: Quantitative results with different thresholds on Structure3D dataset. Threshold(Translation & Rotation) 0.1m, 5 0.15m, 10 0.2m, 15 0.4m, 30 3D-precision(%) 34.11 52.63 64.64 82.75 3D-recall(%) 31.66 48.37 59.53 76.13 Table 5: Quantitative results with different input views on Structure3D dataset. Input views 2 3 4 5 re-IoU(%) 75.02 75.29 75.55 75.57 re-PE(%) 8.72 8.53 8.39 8. re-EE 8.70 8.56 8.55 8.59 re-RMSE 0.4148 0.3596 0.3463 0.3422 3D-precision(%) 53.19 54.43 54.91 55.02 3D-recall(%) 42.60 47.97 49.44 49.59 C.2 PERFORMANCE UNDER INPUT VIEWS The impact of varying input views on performance is presented in Table 5. For each input views, we select this number of views from the 5 views as input and only test on rooms that have all 5 views to eliminate potential bias from room complexity variations. The results show general improvement trend as the number of views increases."
        },
        {
            "title": "D ADDITIONAL QUALITATIVE RESULTS",
            "content": "Figure 9 showcases more visualization of our method on the Structured3D dataset, while Figure 10 presents failed cases. To demonstrate real-world applicability, we present results on in-the-wild images in Figure 11 and Figure 12. EVALUATION RESULT ON CAD-ESTATE DATASET We conducted an additional evaluation on the CAD-Estate dataset Rozumnyi et al. (2023). CADEstate is derived from RealEstate10K datasetZhou et al. (2018) and provides generic 3D room layouts from 2D segmentation masks. Due to differences in annotation standards between CAD-Estate and Structured3D, we selected subset of the original data that aligns with our experimental setup. Our method and Structured3D assume single floor, single ceiling, and multiple walls configuration. In contrast, CAD-Estate includes scenarios with multiple ceilings (particularly in attic rooms) and interconnected rooms through open doorways, whereas Structured3D treats doorways as complete walls. To ensure fair comparison, we sampled 100 scenes containing 469 images that closely match Structure3Ds annotation style. Each scene contains 2 to 10 images. Since CAD-Estate only provides 2D segmentation annotations without 3D information, we report performance using 2D metrics: IoU and pixel error. While CAD-Estates label classes include [ignore, wall, floor, ceiling, slanted], we only focus on wall, floor, and ceiling classes. We utilize the datasets provided intrinsic parameters for reprojection during evaluation. Results are reported for both Noncuboid + GT pose and Plane-DUSt3R (metric) in Table 6. We visualize our results in Figure 13 and Figure 14 Table 6: Quantitative results with on CAD-estate dataset. Method Noncuboid + GT pose Ours (metric) re-IoU(%) 55.99 63.14 re-PE(%) 20.33 15.15 16 Published as conference paper at ICLR 2025 Figure 9: Qualitative results on Structure3D testing set. The 5-th column is our result visualized with pointcloud, the last column is the result shown in pure wireframe Figure 10: Failed case on Structure3D testing set. The first 4 columns are input views, the 5-th column is our result visualized with pointcloud, the last column is the result shown in pure wireframe. 17 Published as conference paper at ICLR 2025 Figure 11: We provide qualitative results on in-the-wild data. Figure 12: We provide qualitative results on out of domain cartoon data (Weber et al., 2024). (a) (b) Figure 13: Visualization of results from the CAD-Estate dataset. (a) Input views are shown in the top row, followed by CAD-Estates ground-truth segmentation in the middle row, and our predicted segmentation in the bottom row. (b) Our 3D reconstruction results displayed with point clouds (top row) and wireframe renderings (bottom row). (a) (b) Figure 14: Visualization of results from the CAD-Estate dataset. (a) Input views are shown in the top row, followed by CAD-Estates ground-truth segmentation in the middle row, and our predicted segmentation in the bottom row. (b) Our 3D reconstruction results displayed with point clouds (top row) and wireframe renderings (bottom row)."
        }
    ],
    "affiliations": [
        "Astribot",
        "Hong Kong Center for Construction Robotics, The Hong Kong University of Science and Technology",
        "Intellifusion Inc.",
        "MMLab, The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}