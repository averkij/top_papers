{
    "paper_title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "authors": [
        "Yiming Wang",
        "Qihang Zhang",
        "Shengqu Cai",
        "Tong Wu",
        "Jan Ackermann",
        "Zhengfei Kuang",
        "Yang Zheng",
        "Frano Rajič",
        "Siyu Tang",
        "Gordon Wetzstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 6 7 0 5 0 . 2 1 5 2 : r BulletTime: Decoupled Control of Time and Camera Pose for Video Generation Yiming Wang1,2 Qihang Zhang3,2 Zhengfei Kuang2 Yang Zheng2 Shengqu Cai2 Frano Rajiˇc1 Tong Wu2 Jan Ackermann2 Siyu Tang1 Gordon Wetzstein2 1ETH Zurich 2Stanford University 3CUHK https://19reborn.github.io/Bullet4D/ Figure 1. Timeand camera-controlled 4D video generation. Given single input video where camera motion is entangled with uniform temporal sampling (top row), our method synthesizes new videos that enable decoupled control over world time and camera pose."
        },
        {
            "title": "Abstract",
            "content": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate unique dataset in which temporal and camera variations are independently , Equal contribution parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. 1. Introduction Recent advances in diffusion models have enabled photorealistic video generation [13, 5, 24]. The temporal dynamics in these models are typically represented in video time, derived from frame indices and frame rate. This representation conflates two fundamentally different dimensions: world time, denoting the absolute temporal coordinate that governs the evolution of the scene dynamics, and camera pose, determining the viewpoint from which the scene is observed. Decoupling these quantities would allow users to independently control when and where to observe events within dynamic 4D world, enabling applications ranging from cinematic effects (e.g., bullet time) to gaming and XR scenarios where users freely navigate frozen or sloweddown scene. Domains including free-viewpoint video synthesis [9, 64, 82] and robotics [39, 43, 52, 70] would also benefit from the aforementioned capabilities. Therefore, our goal is to equip video diffusion models with disentangled control over these two dimensions, paving the way toward 4D world modeling [4, 10, 18] and simulation. Recent camera-controlled video diffusion models [9, 25, 26, 68, 82] provide step towards our vision by explicitly conditioning on camera trajectories. While enabling viewpoint control, these methods still entangle video time with world time by treating the frame index as the implicit physical time of the scene. This assumption enforces uniformly advancing world time tied to frame indices, preventing independent manipulation of continuous temporal effects such as slowing down or pausing scene dynamics independently of camera movement. Moreover, naively extending text prompts to control time cannot provide finegrained or accurate control over world time, and two-stage pipeline that first remaps the input video and then applies camera-controlled video diffusion often results in poor 4D consistency (see Sec. 4). Multi-view video diffusion methods [72] further rely on separate reconstruction step built from extensively sampled generated videos (e.g., 4D Gaussian Splatting [71]) to achieve full cameraand time control, making interactive 4D world modeling infeasible. To address these limitations, we introduce 4Dcontrollable video diffusion model that decomposes changes over video time into continuous world time and camera viewpoint, used as explicit conditioning signals for the generation process. To effectively condition on continuous world time, we introduce two complementary mechanisms: time-aware positional encoding (Time-RoPE) that injects world-time information into the attention mechanism, and time-conditioned adaptive layer normalization (Time-AdaLN) module that provides fine-grained temporal modulation. We conduct ablation studies to demonstrate the effectiveness of our design compared to alternative mechanisms for world-time conditioning. To extend temporal control to full 4D disentanglement, we incorporate camera-conditioned adaptive normalization layer together with unified 4D positional encoding (4D-RoPE), augmenting the camera-aware rotary transformations in TimeRoPE. To support disentangled learning and evaluation, we curate 4D-controlled synthetic dataset in which temporal and camera factors vary independently, enabling robust generalization across diverse trajectories, timing patterns, and real-world scenarios. In summary, the main contributions of our paper are as follows: We introduce 4D-controllable video diffusion framework that disentangles visual evolution along video time into world time and camera pose, and conditions the model using unified 4D positional encoding together with adaptive normalization modules. We curate 4D-controlled synthetic dataset with independently varied temporal and camera factors to support both training and evaluation, facilitating the learning of disentangled cameratime control. Ablations demonstrate the superiority of our conditioning design over alternative strategies. Our method generalizes to diverse real-world scenarios and 4D control inputs, achieving state-of-the-art performance in 4D-controllable video-to-video generation. 2. Related Work 2.1. Video Diffusion Models have achieved remarkable Diffusion models success in image synthesis [16, 28, 47, 50, 53, 59, 60] and have recently been extended to video generation [2, 3, 11, 12, 23, 27, 29, 30, 58, 66, 78], enabling realistic synthesis from diverse conditional inputs. Early video diffusion models [11, 12, 23, 27, 29, 30, 58] extended U-Netbased image generators [54] with temporal modules across frames, while recent approaches [35, 66, 78] adopt transformer architectures [17, 47] with 3D full attention for improved spatiotemporal coherence and visual quality. 2.2. Camera-Controlled Video Generation Beyond conditioning on text prompts or images, cameracontrolled video diffusion methods leverage camera poses to enable global viewpoint manipulation over general scene content, offering natural interface for scene exploration [26] and world modeling [10, 73]. Multi-view video diffusion models further extend this capability by synthesizing synchronized videos from multiple camera views [8, 36, 37, 72, 76]. 2.3. Time-Controlled Video Generation Most video diffusion models encode the temporal dimension by applying positional encodings to discrete frame indices, and offer limited support for continuous and precise control of the underlying physical world time. Commercial systems such as Stable Video Diffusion [11] and Wan [66] provide only coarse motion-strength adjustments, modulating the amount of motion but not exposing an explicit, controllable temporal signal. Motion-controlled video diffusion methods guide scene dynamics using motion trajectories [13, 15, 21, 46, 49, 57, 68, 74, 75, 77, 80, 84], pose skeletons reference videos [22, 42, 45, 67, 79, 81]. While these approaches [14, 23, 31, 44, 55], or 2 control object motion for applications such as video editing, they do not render single coherent scene across different world times. Cat4D [72] and 4DiM [69] propose unified 4D formulations that jointly model camera and time within the diffusion framework, enabling training on mixed static and dynamic data. Yet 4DiM, being image-based, cannot produce high-quality 4D-controllable videos. Cat4D and many other multi-view video diffusion models [41, 63, 72, 83] employ an additional 4D reconstruction stage to render this scenes across camera poses and times; however, process typically necessitates extensive multi-view sampling. concurrent work, Lyra [6], distills video diffusion models to decode explicit 3D/4D Gaussian scene representations. However, these approaches inherit the uniformly discretized temporal of the underlying video diffusion model, limiting its ability to interpolate high-quality frames at novel times. Complementary to these directions, we introduce time-aware positional encoding that overcomes this limitation by enabling continuous temporal control in video diffusion models. We further curate 4D synthetic dataset, enabling real-world generalization with explicit and independent control of camera motion and world time. Despite being fine-tuned solely on synthetic data, our method generalizes effectively to real-world scenarios, and produces 4D-controlled videos without requiring explicit post-processing or auxiliary 4D representations. 3. Method Our goal is to enable 4D-controllable video generation, where world time and camera viewpoint can be explicitly manipulated during synthesis. Conventional video diffusion models couple these two dimensions: the apparent motion in generated videos jointly reflects scene dynamics and camera movement along shared video-time axis. We reformulate this by introducing two explicit and orthogonal conditioning signals, world time τworld and camera pose c, which together define controllable 4D coordinate system. Our framework, illustrated in Fig. 2, consists of three components: continuous world-time control mechanism (Sec. 3.2), unified 4D timecamera conditioning module (Sec. 3.3), and 4D-controlled dataset that provides supervision for learning disentangled control (Sec. 3.4). 3.1. Preliminaries: Video Diffusion Models video diffusion model transforms random noise into coherent video sequences through learned denoising process, typically operating in compact latent space encoded by pretrained 3D VAE. Diffusion Transformers (DiTs) [47] have become the standard denoiser architecture in recent video diffusion models. DiTs divide the latent video tensor zt into non-overlapping spatiotemporal patches and processes them using attention [65]: Attn(Q, K, ) = softmax (cid:17) (cid:16) QK V, (1) where Q, K, RN are the query, key, and value matrices, denotes the number of spatiotemporal tokens (patches), and is the feature dimension of each token. Because attention is permutation-invariant, positional encodings are required to provide ordering information and are commonly implemented as rotary positional embeddings (RoPE) [56, 62]. Recent video diffusion models [35, 78] typically adopt 3D extension of RoPE, where independent rotations are applied along time, height, and width. 3.2. Continuous-Time Control We aim to enable explicit temporal control in video diffusion models, allowing users to specify the exact physical time at which each frame should be generated. To achieve this, we condition the model on continuous time sequence i=0 , where each τi R+ denotes the world time as- {τi}F 1 sociated with frame and is the number of frames. Varying the temporal interval τi+1 τi enables arbitrary time reparameterizations such as slow motion, acceleration, temporal pausing, or reversal. We incorporate the continuoustime signal into the Diffusion Transformer (DiT) through (i) time-aware positional encoding (Time-RoPE) and (ii) time-conditioned adaptive normalization module (TimeAdaLN). Time-RoPE. Conventional video diffusion models represent temporal evolution using positional encodings tied to discrete frame indices. This discretization inherently assumes uniform temporal spacing and prevents the model from responding to fine-grained control. To resolve this limitation, we extend RoPE [62] to operate directly on continuous time and refer to this extension as Time-RoPE. Formally, we define the time-aware rotation operator DTime(τ ) Rdd as the following block-diagonal matrix: DTime(τ ) = diag(cid:0)R(τ θ1), R(τ θ2), . . . , R(τ θd/2)(cid:1), (2) where θk = b2(k1)/(d/2) denotes the frequency for each block and R(α) is the standard 2 2 rotation matrix (cos α, sin α; sin α, cos α). Given query Qi and key Kj associated with timestamps τi and τj, their time-modulated forms are: = (cid:0)DTime(τi)(cid:1) QTime = (cid:0)DTime(τj)(cid:1)"
        },
        {
            "title": "K Time",
            "content": "Qi, Kj. (3) The time-aware attention logit is then calculated as follows:"
        },
        {
            "title": "QTime\ni",
            "content": "(K Time ) = = DTime(τi) DTime(τj)Kj DTime(τi τj) Kj, (4) 3 Figure 2. Method Overview. Given conditional input video, our diffusion model generates new videos under 4D control using world time and camera trajectory. These two signals are injected into the Diffusion Transformer through complementary modulation pathways. Time control is enabled by RoPEt (a time-aware positional encoding injected into attention) and MLPt, which predicts the affine scale and shift used to modulate intermediate features. Camera control is introduced analogously through RoPEc (a camera-aware positional encoding) and MLPc. The outputs of RoPEt and RoPEc are fused into unified 4D positional encoding injected into the attention layers. Together, these mechanisms form 4D-controllable DiT block capable of jointly steering temporal evolution and camera motion during generation. We train our model on curated 4D-controlled synthetic dataset that we constructed, where temporal and camera factors vary independently across scenes, providing explicit supervision for disentangling time and camera control. which explicitly encodes the continuous temporal offset. Aggregating tokens gives the time-aware attention equation: AttnTime(Q, K, ) = softmax (cid:18) QTime(K Time) (cid:19) V. (5) This continuous-time variant of RoPE injects world-time awareness directly into attention mechanism while introducing no additional learnable parameters. Time-AdaLN. Because DiT operates on patchified video tokens whose temporal resolution is downsampled relative to the original frame sequence, Time-RoPE alone cannot capture fine-grained frame-level timing. To preserve precise temporal control, we introduce an additional learnable pathway that injects the continuous time signal at the feature level. 1D convolution ftime() encodes the framelevel timestamps τi into embeddings that are then injected through an Adaptive Layer Normalization (AdaLN) module via learned affine transformation: i,n = LN(cid:0)zi,n (cid:1) fγ (cid:0)ftime(τi)(cid:1) + fβ (cid:0)ftime(τi)(cid:1), (6) where fγ() and fβ() are lightweight MLPs producing scale and shift, and denotes element-wise multiplication. AdaLN is well-suited for temporal modulation since the world-time variable is smooth global scalar that influences the dynamics of the entire scene rather than localized spatial regions. Compared to other conditioning mechanisms commonly used in video diffusion models, AdaLN achieves more stable and coherent temporal dynamics than 4 cross-attention [17] and additive schemes [9, 26], because it modulates activations with single smooth global time signal rather than injecting token-level perturbations that can disrupt spatial alignment or produce unstable temporal responses (see ablations in Sec. 4.3). Combined with Time-RoPE, it forms our temporal backbone, providing both feature-wise and attention-level time control for stable and coherent video generation. 3.3. 4D Control To enable full 4D controllability within the diffusion process, we extend the world-timeconditioned model with explicit camera viewpoint control through unified 4D positional encoding (4D-RoPE) and dedicated cameraconditioned normalization branch (Camera-AdaLN). 4D-RoPE. To jointly control time and camera viewpoint within attention, we extend Time-RoPE into 4D positional encoding by incorporating camera-aware geometric component. Following [38], we encode relative camera-ray or camera-pose relationships to construct camera-aware rotary transformation, which is combined with the temporal rotation to form unified 4D operator. This 4D extension injects both continuous time differences and viewpointdependent geometric relations directly into the attention mechanism, enabling disentangled and coherent control of temporal evolution and camera motion during generation. Camera-AdaLN. We introduce parallel AdaLN branch for camera pose modulation. Following existing cameracontrolled video diffusion models [26], we encode per-pixel Table 1. Comparison of Cameraand Time-Controlled Video Generation on the Synthetic Dataset. Baseline methods designed solely for camera control (denoted with *) are extended to 4D control by applying time remapping [33] to the input videos prior to camera-conditioned generation. Our approach attains the highest pixel-level accuracy across all metrics, demonstrating its effectiveness in jointly modeling camera and temporal control. Method PSNR SSIM LPIPS TrajectoryCrafter* 17.72 0.4917 0.3431 ReCamMaster* 21.86 0.5852 0.1846 24.57 0.6905 0.1265 Ours Implementation Details. We build our model on top of CogVideoX [78], using the pretrained CogVideoX-5B-T2V as the base model. For video-to-video generation conditioned on an input video, we follow ReCamMaster [9] and concatenate the source and target video tokens along the frame dimension. We then fine-tune the base model with our 4D cameratime control module. All experiments are conducted at spatial resolution of 384640 with 81 frames per video. We adopt progressive training schedule that first trains the model at half resolution and subsequently fine-tunes it at full resolution. The final model is trained for total of 40K iterations with batch size of 64. Metrics and Baselines. We evaluate 4D-controlled video-to-video generation on both synthetic and real-world videos. For synthetic evaluation, we generate 500 videos using PointOdyssey [85], featuring unseen characters and unseen scenes. For real-world evaluation, we collect 100 videos from ViPE [32]. For the synthetic dataset, we report PSNR, SSIM, and LPIPS to assess reconstruction quality under joint camera and time control. For each real-world video, we randomly sample unseen camera trajectories and temporal patterns to generate the outputs. We evaluate visual quality using VBench [34] metrics as well as FVD and KVD [20]. We follow the evaluation protocol of previous camera-controlled video generation methods [9, 25, 26] by measuring rotation and translation errors. Specifically, we compare the ground-truth camera trajectories against the camera poses estimated using MegaSAM [40] from generated videos. For comparison, we extend state-of-the-art cameracontrolled video-to-video generation models [9, 82] to support joint time and camera control. Specifically, we apply time remapping [33] to the input video to produce the target temporal pattern, and then perform camera-conditioned generation using each baseline. We mark this adaption of the baselines by adding the * symbol. 4.1. Evaluation We conduct comprehensive evaluation of our models ability to perform 4D-controlled video generation, assessFigure 3. Comparison on Synthetic Videos. GT frames compared with predictions from our method and state-of-the-art novelview synthesis models. Our method adheres most closely to the target camera conditions and produces the finest level of detail. camera geometry using Plucker ray [48] embeddings and aggregate them into token-level features via 2D convolutional camera encoder. These features predict affine parameters through lightweight MLPs that modulate intermediate activations according to the desired camera trajectory. 3.4. Dataset for Disentangled 4D Control Learning to control temporal dynamics and camera motion independently requires data that explicitly decouples variations along these two dimensions. However, existing video datasets are not designed for such disentangled learning. Single-camera datasets [7] contain only one fixed or slowly moving viewpoint per sequence, inherently coupling temporal and spatial dimensions, while multi-camera datasets [9] typically employ synchronized captures with uniform temporal sampling, where all cameras observe identical dynamics at the same timestamps. To enable disentangled supervision, we construct dataset that contains diverse temporal and spatial variations within each scene. For each scene, we generate multiple temporal variants by applying time-remapping functions (e.g., slow motion, pausing, random speed change) to moving objects, and render each temporal variant under different camera trajectories. We build the dataset using the PointOdyssey framework [85] within Blender, enabling physically consistent dynamic scenes with controllable cameras, lighting, and character motions. All videos are annotated with corresponding camera parameters and world-time labels for conditional training. Figure 2 includes an example from our 4D-disentangled training dataset. In total, the dataset comprises approximately 2k scenes across 80 environments and 100 characters, each with 3 camera trajectories and 3 temporal patterns, resulting in about 20k videos for learning controllable 4D video diffusion models. Please refer to the Appendix for dataset details and visualizations. 4. Experiments Additional details and video results are provided in the supplementary material and accompanying video. Our dataset, code, and models will be publicly released. Table 2. Comparison of Cameraand Time-Controlled Video Generation on Real-World Videos. Our method achieves the most accurate camera pose control and produces videos with reduced temporal flicker, smoother motion, and higher subjectbackground consistency, indicating stronger 4D controllability while maintaining high visual quality."
        },
        {
            "title": "Method",
            "content": "RotErr TransErr Aesthetic Quality Imaging Quality Temporal Flickering Motion Smoothness Subject Consistency Background Consistency"
        },
        {
            "title": "Video Quality",
            "content": "FVD KVD TrajectoryCrafter* ReCamMaster* Ours 5.44 2.98 1.47 3.31 1.85 1.32 0.4525 0.4470 0.4520 0.5696 0.5256 0. 0.9659 0.9755 0.9780 0.9881 0.9911 0.9923 0.9328 0.9375 0.9428 0.9375 0.9472 0.9506 2399 150.2 2325 146.1 2292 139.1 Figure 4. Qualitative Comparison of Cameraand Time-Controlled Video Generation on Real-World Videos. Qualitative comparison between our method and state-of-the-art novel-view synthesis models extended with time remapping [33]. In the left example, existing methods struggle under extreme view and time changes, producing severe artifacts (ReCamMaster) and showing imprecise camera control (TrajectoryCrafter). The right example similarly illustrates strong artifacts and reduced detail from ReCamMaster, while TrajectoryCrafter again fails to follow the prescribed trajectory. ing how well it generates videos that faithfully follow explicitly specified camera and time controls given an input sequence. Comparison on Synthetic Videos. We first evaluate all methods on the synthetic dataset, where ground-truth videos provide direct reference for assessing 4D-controlled generation. For fair comparison, we fine-tune ReCamMaster on our 4D-controlled dataset, yielding substantially improved performance over its released checkpoint (e.g., more than 2 dB gain in PSNR; see Appendix for details). As shown in Tab. 1, our method achieves the highest pixel-level accuracy across all metrics, reflecting its ability to jointly model camera and time control. Qualitative results in Fig. 3 further show that our approach follows the prescribed camera paths and temporal inputs more faithfully while producing sharper and more detailed generations than the baselines. Comparison on Real-World Videos. We further evaluate our method on real-world videos, assessing both camerapose consistency and overall visual quality following the evaluation protocols of prior work [9, 26]. As shown in Tab. 2, our approach achieves the lowest rotation and translation errors, indicating more accurate camera control when jointly conditioning on time. It also attains higher temporal stability and better subjectbackground consistency according to VBench [34] while maintaining overall visual quality comparable to existing approaches. Although TrajectoryCrafter reports slightly higher image-quality scores on VBench, we find that its camera control is substantially less reliable. For reference, the input conditional videos obtain FVD=2012 and KVD=131.8 on UCF101 [61]. Qualitative comparisons in Fig. 4 corroborate these findings. All models are asked to generate videos under jointly specified camera and time controls. TrajectoryCrafters reliance on monocular-depth-based point-cloud projections often results in geometric distortions and inaccurate cam6 Figure 5. 4D Control: Camera and Time Manipulation. Our model generates videos that faithfully follow independently specified camera and time controls. Each row shows combinations of fixed or moving camera viewpoints ((cid:130)) and fixed or changing world time (7). The model correctly applies each control mode, including challenging settings such as moving camera with fixed time (bullet time effect), while preserving scene dynamics and visual coherence. These results indicate strong disentanglement between camera and world time conditioning as well as robust generalization across diverse real world inputs. control settings, leading to inconsistent 4D generation and, in some cases, loss of input content (e.g., when applying bullet-time effects, later parts of the video are clipped due to temporal pausing). In contrast, our end-to-end 4Dcontrollable video diffusion model conditions on the same input video for all camera and time settings, ensuring stable supervision and consistent content preservation. To quantify this advantage, we measure background consistency on synthetic videos under identical camera controls but varying time inputs. Foreground masks are obtained via SAM2 [51] and ground-truth first-frame mattes, and metrics are computed over background regions to assess visual similarity. As shown in Tab. 3, our method achieves substantially higher scores, reflecting stronger cameratime disentanglement and improved content consistency. Qualitative results in Fig. 7 further illustrate the superior 4D consistency our model achieves on real-world videos. Generalization across Scenes, Cameras, and Timings. We also demonstrate that our method generalizes effectively to diverse real-world scenarios involving unseen camera trajectories and temporal variations, despite being fine-tuned exclusively on synthetic human-centric dataset. As shown in Fig. 5, our approach robustly handles wide range of camera inputs and maintains coherent 4D control across varied environments, extending beyond human subjects to animals and scenes with complex physical dynamics. Moreover, Fig. 6 illustrates that our model accommodates broad set of previously unseen temporal control patterns, further Figure 6. Time Control Generalization. Three generations produced by our method from the same input video under different time conditions. Although the model is trained on only limited subset of time remappings, it generalizes well to complex and previously unseen temporal inputs. era trajectories. Conversely, ReCamMaster [9] exhibits noticeable artifacts in dynamic regions once time control is introduced. Our method, in contrast, produces stable scene dynamics and faithfully follows both the prescribed temporal pattern and camera motion. 4.2. Discussion Advantages over Camera-Only Methods. To extend camera-only video-to-video methods [9, 82] to joint timecamera control, prior approaches rely on time remapping the input video before performing camera-conditioned generation. However, this two-stage pipeline causes the conditioning video itself to change across different time7 Table 3. Evaluation of Disentangled Camera and Time Control. Background consistency is measured between videos generated under identical camera trajectories but different time controls using masked image metrics [19] (mPSNR, mMAE, mSSIM, mLPIPS), where the prefix indicates evaluation within the masked background region. Our method achieves higher consistency than ReCamMaster, indicating more effective disentanglement and improved 4D coherence. Table 4. Ablation on World-Time Conditioning. We compare different temporal conditioning mechanisms for fine-tuning CogVideoX [78] toward world-time-controlled video-to-video generation. AdaLN provides the strongest performance among the learnable baselines (CrossAttention and ChannelAddition). TimeRoPE itself offers strong controllability and consistently improves upon standard RoPE across all configurations, with Time-RoPE + AdaLN achieving the best overall results. Method mMAE mPSNR mSSIM mLPIPS Method ReCamMaster* 0.0362 0.0231 Ours 25.80 28.29 0.8789 0.9096 0.1527 0.1119 RoPE + CrossAttention RoPE + ChannelAddition RoPE + AdaLN PSNR SSIM LPIPS 23.86 0.8274 0.1753 25.31 0.8438 0.1456 29.83 0.8821 0.0742 Time-RoPE 30.45 0.8807 0.0753 30.51 0.8816 0.0753 Time-RoPE + CrossAttention Time-RoPE + ChannelAddition 30.40 0.8813 0.0730 32.15 0.8962 0.0631 Time-RoPE + AdaLN Table 5. Ablation on 4D Conditioning. Our 4D-RoPE and Time/Camera AdaLN enhance 4D conditioning scores."
        },
        {
            "title": "Full Model",
            "content": "w/o 4D-RoPE w/o Camera/Time AdaLN PSNR SSIM LPIPS 23.45 21.98 22.74 0.6283 0.6099 0. 0.1309 0.1785 0.1493 RoPE injects time-control prior directly into the attention logits, replacing RoPE with Time-RoPE consistently improves performance across all learnable conditioning methods. The combination of Time-RoPE with AdaLN yields the best results on all metrics, highlighting the effectiveness of our proposed world-time conditioning design. 4D Conditioning. We analyze the effect of our 4D-RoPE and camera/time AdaLN in adding 4D conditioning. Tab. 5 shows that removing either module degrades performance, with more substantial drop observed when omitting 4DRoPE. All models are trained with batch size of 4 for 20k iterations, using the same dataset configuration as Tab. 1. 5. Conclusion We presented 4D-controllable video diffusion framework that disentangles scene dynamics from camera motion through continuous world-time conditioning and explicit viewpoint control. Our framework injects continuous world-time and camera-trajectory signals via unified 4D positional encoding and adaptive layer normalization modules, which together provide effective conditioning as confirmed by our ablation studies. Supported by synthetic dataset designed to vary temporal and camera factors independently, our approach enables precise manipulation of scene dynamics and flexible camera navigation within dynamic environments. Experimental results demonstrate that Figure 7. Comparison of CameraTime Disentanglement. When varying the time condition while keeping the camera condition fixed, state-of-the-art camera-controlled video generation methods such as ReCamMaster fail to maintain consistent camera control, resulting in geometric inconsistencies within the generated content. highlighting its strong generalization capability. 4.3. Ablation Study World-time Conditoning. We study alternative mechanisms for conditioning continuous world time and compare them with our proposed Time-RoPE and AdaLN. We conduct time-controlled video-to-video generation experiments on the synthetic dataset of [8], where camera poses are fixed and only time conditioning is applied. We compare AdaLN with two commonly used learnable conditioning strategies in video diffusion models: (1) Cross-Attention, where temporal conditions are treated as additional tokens that interact with visual tokens through cross-attention layer, following multimodal diffusion architectures such as MMDiT [17]; and (2) Channel Addition, where temporal embeddings are directly added to the latent feature channels, following prior camera-controlled video generation methods [9, 26]. We also compare our TimeRoPE with the original RoPE to assess the benefit of injecting continuous world time into attention. As shown in Tab. 4 (rows 13 and 57), AdaLN delivers the strongest temporal control among the learnable conditioning baselines, both with the original RoPE and with Time-RoPE. Time-RoPE on its own (row 4) already provides strong temporal control and surpasses all learnable variants that use standard RoPE (rows 13). Because Timethis disentangled control achieves state-of-the-art performance in 4D-controllable video generation while remaining fully compatible with existing video diffusion architectures. Despite these advances, several challenges remain. Our model relies on synthetic dataset for disentangled supervision, which, although effective, may not capture the full complexity of real-world physics, lighting, and longhorizon scene dynamics with large camera baseline. In addition, while our method supports arbitrary time and camera inputs, inference is still performed in parallel (non-autoregressive) diffusion setting, limiting its ability to model extremely long videos or persistent worlds. Future work could explore autoregressive or recurrent formulations of 4D-controllable diffusion, enabling temporally unbounded generation and online trajectory control. Another promising direction is learning disentanglement together with real-world video corpora. Extending the framework to handle physics-aware temporal reasoning may further broaden its applicability. We hope this work inspires continued research toward scalable, controllable, and physically consistent 4D generative video models."
        },
        {
            "title": "Acknowledgements",
            "content": "We gratefully acknowledge the insightful discussions with Qin Han, Yutong Chen, and Haofei Xu. This work was supported as part of the Swiss AI initiative by grant from the Swiss National Supercomputing Centre (CSCS) under project IDs #36 on Alps, enabling large-scale training."
        },
        {
            "title": "References",
            "content": "[1] klingai. https://klingai.com/, 2025. 1 [2] Sora. https://openai.com/sora/, 2025. 2 [3] Veo. https://deepmind.google/models/veo/, 2025. 1, 2 [4] World labs. https://www.worldlabs.ai/, 2025. 2 [5] pika. https://pika.art/, 2025. 1 [6] Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, and Xuanchi Ren. Lyra: Generative 3d scene reconstruction via video diffusion model self-distillation. In arXiv, 2025. 3 [7] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2287522889, 2025. 5 [8] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Syncammaster: Synchronizing multi-camera Di Zhang. video generation from diverse viewpoints. arXiv preprint arXiv:2412.07760, 2024. 2, 8 [9] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 2, 4, 5, 6, 7, 8, 1, 3 [10] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aaron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 3: new frontier for world models. 2025. [11] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [12] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. 2 [13] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, and Ning Yu. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise. In CVPR, 2025. Licensed under Modified Apache 2.0 with special crediting requirement. 2 [14] Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, Linjie Luo, Gordon Wetzstein, and Mohammad Soleymani. X-dyna: Expressive dynamic human image animation, 2025. 2 [15] Pascal Chang, Jingwei Tang, Markus Gross, and Vinicius C. Azevedo. How warped your noise: temporally-correlated noise prior for diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 2 [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 9 [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 4, 8 [18] Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568, 2024. 2 [19] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. Advances in Neural Information Processing Systems, 35:3376833780, 2022. 8 [20] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In ECCV, 2022. 5, 1 [21] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv, 2024. [22] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In ICLR, 2024. 2 [23] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [24] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, pages 393411. Springer, 2024. 1 [25] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation, 2024. 2, 5 [26] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. 2, 4, 5, 6, 8, 1 [27] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 2 [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [29] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Imagen Poole, Mohammad Norouzi, David Fleet, et al. video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [30] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. 2 [31] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 2 [32] Jiahui Huang, Qunjie Zhou, Hesam Rabeti, Aleksandr Korovko, Huan Ling, Xuanchi Ren, Tianchang Shen, Jun Gao, Dmitry Slepichev, Chen-Hsuan Lin, et al. Vipe: Video pose engine for 3d geometric perception. arXiv preprint arXiv:2508.10934, 2025. 5 [33] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In Proceedings of the European Conference on Computer Vision (ECCV), 2022. 5, 6 [34] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 5, 6 [35] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, [36] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2024. 2 [37] Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem. Vivid-zoo: Multi-view video generation with diffusion model. Advances in Neural Information Processing Systems, 37:6218962222, 2024. 2 [38] Ruilong Li, Brent Yi, Junchen Liu, Hang Gao, Yi Ma, and Angjoo Kanazawa. Cameras as relative positional encoding. arXiv preprint arXiv:2507.10496, 2025. 4 [39] Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran arXiv preprint Song. arXiv:2503.00200, 2025. 2 Unified video action model. [40] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1048610496, 2025. 5 [41] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In CVPR, 2024. 3 [42] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv, 2024. 2 10 [43] Zeyi Liu, Shuang Li, Eric Cousineau, Siyuan Feng, Benjamin Burchfiel, and Shuran Song. Geometry-aware 4d arXiv preprint video generation for robot manipulation. arXiv:2507.01099, 2025. 2 [44] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. [45] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. ArXiv, abs/2405.13865, 2024. 2 [46] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David Lindell. Sg-i2v: Self-guided trajectory control in image-to-video generation. arXiv, 2024. 2 [47] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2, 3 [48] Julius Plucker. Xvii. on new geometry of space. Philosophical Transactions of the Royal Society of London, pages 725791, 1865. 5 [49] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv, 2024. 2 [50] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [51] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. [52] Marc Rigter, Tarun Gupta, Agrin Hilmkil, and Chao Ma. Avid: Adapting video diffusion models to world models. arXiv preprint arXiv:2410.12822, 2024. 2 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [54] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 2 [55] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2 [56] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. SelfarXiv attention with relative position representations. preprint arXiv:1803.02155, 2018. [57] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In SIGGRAPH, 2024. 2 [58] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [59] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 and Stefano Ermon. arXiv preprint [60] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [61] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 6, 1 [62] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 3 [63] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. In ICCV, 2025. 3 [64] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: ExIn Eutreme monocular dynamic novel view synthesis. ropean Conference on Computer Vision, pages 313331. Springer, 2024. 2 [65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [66] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2 [67] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. In NeurIPS, 2024. 11 [81] Wenjie Yin, Yi Yu, Hang Yin, Danica Kragic, and Marten Bjorkman. Scalable motion style transfer with constrained diffusion generation. In AAAI, 2024. 2 [82] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 2, 5, 7, 3 [83] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-view video diffusion model for 4d generation. In arXiv, 2024. 3 [84] Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu. Vidcraft3: Camera, object, and lighting control for image-to-video generation. arXiv preprint arXiv:2502.07531, 2025. 2 [85] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J. Guibas. Pointodyssey: large-scale In ICCV, synthetic dataset for long-term point tracking. 2023. 5, [68] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2 [69] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David Fleet. Controlling space and time with diffusion models. arXiv preprint arXiv:2407.07860, 2024. 3 [70] Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, and Xiaodan Liang. Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation. Advances in Neural Information Processing Systems, 37:4105141075, 2024. 2 [71] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2031020320, 2024. 2 [72] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, pages 2605726068, 2025. 2, 3 [73] Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Video world models with long-term spatial memory. arXiv preprint arXiv:2506.05284, 2025. 2 [74] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In ECCV, 2024. [75] Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control. arXiv preprint arXiv:2411.19324, 2024. 2 [76] Dejia Xu, Yifan Jiang, Chen Huang, Liangchen Song, Thorsten Gernoth, Liangliang Cao, Zhangyang Wang, and Hao Tang. Cavia: Camera-controllable multi-view video arXiv preprint diffusion with view-integrated attention. arXiv:2410.10774, 2024. 2 [77] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn SIGdirected camera movement and object motion. GRAPH, 2024. 2 [78] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 5, 8, 1 [79] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In CVPR, 2024. 2 [80] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 12 BulletTime: Decoupled Control of Time and Camera Pose for Video Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Implementation Details We fine-tune the pretrained CogVideoX-5B-T2V [78] model using the AdamW optimizer with learning rate of 2 105 and weight decay of 104. We apply gradient clipping with maximum norm of 1.0. The learning rate follows linear decay schedule with 100 warm-up steps. The model is trained for total of 40K iterations. Since existing latent-space video diffusion models downsample the temporal resolution via the 3D-VAE and the transformers patchify block, directly injecting perframe world-time signals becomes incompatible with the reduced temporal resolution. To address this, we encode the world-time sequence with learnable 1D convolution layers to map it to the downsampled temporal resolution. For our non-learnable Time-RoPE design, we downsample the world-time sequence via average pooling so that it matches the latent temporal resolution. When computing the RoPE positional encoding, we scale the world-time differences by the FPS to convert them into consistent temporal offsets. With this scaling, Time-RoPE becomes exactly equivalent to the standard RoPE formulation under uniform time sampling, making standard RoPE special case of our design to better preserves the models original generative prior. For camera-conditioning signals, we follow prior work [9, 26] and directly subsample the camera trajectories at fixed intervals to match the latent temporal resolution. For video distribution-level evaluation, we report FVD and KVD following the protocol of TATS [20]. We use videos from UCF101 [61] as the real distribution and generate cameratimeconditioned outputs for each baseline as the fake distribution. Following TATS, each video is embedded using pretrained I3D network to obtain single feature vector, and we compute FVD (Frechet distance) and KVD (polynomial-kernel MMD) between the real and generated feature distributions. Using UCF101 as the reference distribution introduces several limitations: many of our time-control operations (e.g., slow motion, temporal pausing) fall outside the temporal statistics of UCF101, potentially inflating divergences unrelated to perceptual quality; and UCF101 primarily contains human-action videos with limited camera motion, whereas our generated videos may include large, user-specified camera trajectories. B. Additional Results We provide full video results in the accompanying supplementary video and recommend watching it for additional qualitative demonstrations. Failure Cases Since our model is fine-tuned from pretrained video diffusion model, it inherits some of their generation limitations. For example, it may struggle with fine-grained hand details under certain viewpoints, where motions may violate physical plausibility or appear lowquality. In addition, background regions that are not visible in the input video may lack high-fidelity details because our training data is limited to synthetic environments. This limitation could potentially be alleviated by jointly incorporating real-world datasets during training. Fig. B.1 shows examples of these failures. Figure B.1. Failure Cases. Our model may struggle with finegrained hand motion and with generating high-quality background details under certain viewpoints. C. 4D Controlled Dataset curated our 4D-controlled dataset using We have includPointOdyssey [85] and its released 3D assets, ing character models, HDR environment textures, and indoor scene layouts. Specifically, we use 100 human-like characters and animate them using real-world motion capture data. The characters are paired with 3D environments that include both outdoor and indoor scenes. For outdoor scenes, we use 60 HDR environment textures to simulate natural backgrounds, while for indoor scenes, we use 20 unique 3D environments provided by PointOdyssey. 1 Figure C.1. 4D Controlled Data Sample. We demonstrate the decoupled curation of spatial control (camera motion) and temporal control (timing). (Left) Three distinct camera views (Cam. A, B, C) show the same synchronized frames (Frame 0, 40, 80), confirming simultaneous camera and action control. (Right) Three videos with different action timings (Time A, B, C) are generated from the same camera trajectory, showing independent control over the temporal dimension. Figure C.2. Examples of Our 4D Controlled Data Samples. We showcase the diversity of our generated dataset, which spans diverse motions, variety of human-like subjects, both singleand multi-character scenes, and wide range of indoor and outdoor environments. We will release the dataset and its generation code. For each characterscene pair, we introduce independent control over camera viewpoints and world-time to generate 4D-controlled sequences. We first define linear world-time as the original motion-capture timeline, where the motion progresses at constant rate. To enable world-time control, we replace this linear progression with remapped timeline that changes how the animation advances over time. The character animation is then generated by retargeting the mocap sequence according to this remapped timeline. By accelerating, decelerating, or holding specific portions of the timeline, these remappings allow effects such as slow motion, pausing, and locally accelerated motion segments. Speicfically, To realize these behaviors, we construct several temporal variants that introduce diverse world-time dynamics. These include slow-motion variant, where consecutive output frames map to closely spaced points on the mocap timeline, effectively reducing the motion speed, and pausing variant, where selected poses are held for extended durations. In addition, we use random time-warping variant that maps the linear timeline to an upsampled one while enforcing local speed constraints, and spline-based variant, where monotonic spline control points are sampled under slope constraints to produce smooth, continuously varying changes in motion speed. To2 Figure D.1. ReCamMaster Artifacts under Time Remapping. In the left three columns we show how ReCamMaster and our method perform under no time remapping. The right three columns show the same generated scene with time remapping. We can observe how ReCamMaster creates significant artifacts after applying time remapping. In comapison our method does not suffer from this. pared to our approach. This highlights the superior effectiveness of our proposed 4D control module. We further observe that visual artifacts in ReCamMaster results stem primarily from the attempt to enforce timecontrol effects, as illustrated in Fig. D.1. When time control is disabled (top row), ReCamMaster exhibits significantly fewer artifacts compared to scenarios where time control is active (bottom row). Crucially, these artifacts persist even after fine-tuning on our time-controlled patterns. Given that time manipulation is prerequisite for full 4D controllability, these results suggest that ReCamMasters two-stage pipeline suffers from intrinsic limitations in handling complex temporal dynamics. Table D.1. Finetuning ReCamMaster on Our Dataset."
        },
        {
            "title": "ReCamMaster",
            "content": "+ finetune on our data"
        },
        {
            "title": "Ours",
            "content": "PSNR SSIM LPIPS 19.67 21.86 24.57 0.5426 0.5852 0.6905 0.2594 0.1846 0.1265 TrajectoryCrafter [82]. We employ the official codebase and the provided checkpoint, trained on approximately 180k videos. TrajectoryCrafter relies on the reconstruction of dynamic point clouds from monocular videos. However, inaccuracies in these point cloudsoften stemming from unreliable depth estimationfrequently lead to geometric artifacts, limitation also noted in the original paper. Our experimental results show similar artifact cases driven by depth inconsistencies. gether, these variants provide diverse and natural temporal augmentations and enable the model to learn wide range of world-time controls. For each scene, we curate data using one linear timeline and two additional variants randomly selected from the above set. We determine each camera position using look-at center, radius, and pair of rotation angles representing azimuth and elevation. static camera keeps these parameters fixed throughout the sequence. For dynamic trajectories, we sample 24 waypoint cameras by perturbing the look-at center, radius, and rotation angles, and then interpolate these waypoints to obtain smooth camera path. To ensure realistic framing, we enforce specific constraints across all trajectories: the radius is sampled within range of 4 12 meters, the total azimuth variation is limited to 75, the elevation variation to 30, and the look-at center offset to maximum of 1 meter from the human characters centroid. For each scene, we generate three distinct views: one trajectories with more than two waypoints, one orbitstyle trajectory with single waypoint, and one static camera. To produce natural, eased motion between waypoints, we randomly select either uniform-speed interpolation or smoothstep function, defined as (t) = 3t2 2t3 for [0, 1]. We use fixed camera intrinsic settings with focal length of 30 mm and sensor width of 50 mm. D. Baselines ReCamMaster [9]. We utilize the official codebase and the released checkpoint, which was pre-trained on 122k synthetic videos. To ensure fair comparison on our synthetic benchmark, we fine-tune ReCamMaster on our dataset of 20k synthetic videos. As demonstrated in Tab. D.1, while fine-tuning yields substantial performance gains, the method still significantly underperforms com-"
        }
    ],
    "affiliations": [
        "CUHK",
        "ETH Zurich",
        "Stanford University"
    ]
}