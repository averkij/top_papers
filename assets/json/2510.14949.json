{
    "paper_title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
    "authors": [
        "Yu Zhou",
        "Sohyun An",
        "Haikang Deng",
        "Da Yin",
        "Clark Peng",
        "Cho-Jui Hsieh",
        "Kai-Wei Chang",
        "Nanyun Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 4 9 4 1 . 0 1 5 2 : r DIALECTGEN: BENCHMARKING AND IMPROVING DIALECT ROBUSTNESS IN MULTIMODAL GENERATION , Da Yin, Clark Peng, Cho-Jui Hsieh, , Haikang Deng Yu Zhou , Sohyun An Kai-Wei Chang, Nanyun Peng University of California, Los Angeles {yuzhou, kwchang, violetpeng}@cs.ucla.edu Figure 1: Multimodal Generative Model Outputs on semantically identical prompts that differ only in one synonymous lexical feature in Standard American English (top) / lower-resource English dialect (bottom)."
        },
        {
            "title": "ABSTRACT",
            "content": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4,200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near-zero cost to SAE performance. Code and data at: dialectgen.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Linguists have defined over 160 dialects (Aeni et al., 2021) within the English language, with three out of four English speakers having dialect background other than Standard American or British English (Crystal, 2003). Despite this rich diversity, current pre-training paradigms employ content filters that can exclude data involving lower-resource English dialects other than Standard American and British English (Gururangan et al., 2022), reducing the effectiveness of pretrained models on * Core contributors. Project lead. 1 inputs from other dialects (Lee et al., 2023). Prior works have shown significant allocational harms toward dialect speakers caused by such dialect performance discrepancies in machine learning applications (Hovy & Spruit, 2016; Bender et al., 2021), making the observation of similar performance trends in multimodal generative models an alarming sign. As shown in Figure 1, while current multimodal generative models can accurately generate high quality image and video content given Standard American English (SAE) prompts (left); they fail in various manners when provided with semantically equivalent prompts containing single synonymous dialect word (right). Stable Diffusion 3.5 Large (Esser et al., 2024) fails to generate \"ang pow\", which is commonly used in Singaporean English to mean \"red packet\", and FLUX.1 [dev] (Black Forest Labs, 2024) fails to generate \"brinjal\", which is synonymous with \"eggplant\" in Indian English. Furthermore, when the dialect lexeme is polysemous, i.e., has an alternative meaning in SAE, models tend to always generate content that align with the SAE meaning, even when the context makes such interpretation highly improbable. For example, DALL-E Mini (Dayma et al., 2021) generations of \"A man driving his whip\" fail to capture the correct meaning of \"whip\" as \"car\" in African American English, given clear context indications. Similar failure modes are observed in text-to-video generative models: Wan 2.1 (Wang et al., 2025) fails to correctly render \"carnal\", which refers to \"brother\" in Chicano English. In this work, we construct DialectGen, new large-scale benchmark evaluating dialect robustness in image and video generation. Our benchmark dataset spans six common English dialects, including Standard American English (SAE), British English (BrE), Chicano English (ChE), Indian English (InE), and Singaporean English (SgE). For each dialect other than SAE, we create SAE Prompt / Dialect Prompt pairs that are semantically identical besides switching single SAE lexeme for synonymous dialect lexeme. We work with dialect speaker annotators to create rigorous feature selection and prompt filtering pipeline that ensures the final dialect prompts are (1) exactly synonymous with the SAE prompt; (2) valid in the dialect context; and (3) non-ambiguous (for polysemous lexemes). These strictly enforced quality guarantees facilitate the development of simple yet effective automatic and human evaluation metrics for evaluating generative model performance. We experiment with 17 widely used image and video generative models on DialectGen, demonstrating up to 38.63% and 48.17% performance drops for SOTA open-weights image and video generative models, respectively. To alleviate such significant dialect performance drops observed in current multimodal generative models, we design general encoder-based learning strategy that enhances dialect robustness for diffusion-based multimodal generative models. Our method teaches the models text encoder to recognize dialect lexemes while retaining its knowledge of SAE polysemous lexemes. We also include an encoder-based KL regularization loss based on image-SAE caption datasets to regulate output distribution shifts. Experiments on five dialects show that our method is able to simultaneously improve Stable Diffusion 1.5 (Rombach et al., 2022) and SDXL (Podell et al., 2023) performance on five dialects to be on par with SAE performance. At the same time, we observe near zero (< 1%) SAE performance drop on the general MSCOCO (Lin et al., 2014) validation set for both models. Our key contributions include: DialectGen, new large-scale multi-dialectal benchmark for evaluating dialect robustness in text-to-image and text-to-video generation. Comprehensive evaluation and analysis of 17 multimodal generative models and five baseline mitigation methods on DialectGen. high-performing method for improving dialect robustness in multimodal generation while maintaining strong SAE performance."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Linguists define dialects as regional variations of language distinguished by unique features in lexicon, phonology, and grammar from each other, together constituting single language (Hudson, 1996; Chambers & Trudgill, 1998; Fromkin et al., 1998; Nerbonne, 2009; Wardhaugh & Fuller, 2021). English, like any other language, is subject to such variations. However, most dataset resources and pre-training paradigms focus only on Standard American and British English (Gururangan et al., 2022), leading to dialect robustness issues and performance gaps in downstream machine 2 Table 1: Example paired textual data entries from the DialectGen dataset, including Lexeme, Concise Prompt, and Detailed Prompt. Dialect name abbreviations: SAE (Standard American English), AAE (African American English), BrE (British English), SgE (Singaporean English). Dialect Lexeme Concise Prompt Detailed Prompt"
        },
        {
            "title": "SAE\nSgE",
            "content": "sneakers kicks brand new sneakers brand new kicks little girl wearing pair of stylish white sneakers little girl wearing pair of stylish white kicks bathroom spacious bathroom clean and tidy bathroom with shiny blue wall tiles loo spacious loo clean and tidy loo with shiny blue wall tiles squid sotong squid on counter sotong on counter large squid in an aquarium with colorful coral large sotong in an aquarium with colorful coral learning applications. Previous works have analyzed and explored such dialectal performance gaps in NLP tasks like QA (Ziems et al., 2023), NLI (Ziems et al., 2022), dependency parsing, and POS tagging (Blodgett et al., 2018; Jørgensen et al., 2015). Recent works have also noticed the impact of dialect variations on text-to-image generation (Lee et al., 2023; Wan et al., 2024). Along this line of research, we create the first large-scale benchmark of dialect robustness in multimodal generation, evaluating both text-to-image and text-to-video generative models on inputs across six different dialects. Moreover, while lexicon, phonology, and grammar are the three key aspects that distinguish each dialect from others, existing works in Dialectal NLP have so far mainly focused on the grammar variations of dialects (Ziems et al., 2022; 2023; Blodgett et al., 2018; Jørgensen et al., 2015). In this work, we provide the first large-scale dataset of dialectal lexical variations, bridging the gap towards holistic dialectal variation evaluation and building dialect-robust machine learning models."
        },
        {
            "title": "3 DIALECTGEN BENCHMARK",
            "content": "3.1 DATASET CONSTRUCTION To select dialect features for our benchmark dataset, we first gather dialect lexemes along with their dictionary definitions and example usages from publicly available regional English dictionaries including The Oxford Regional English Dictionary (Gates et al., 2023), Dictionary of American regional english (Cassidy et al., 1985), dictionary of Singlish and Singapore English (Lee, 2004), Dictionary of Indian English (Subhash, 2020), and The Oxford Dictionary of African American English (Heinmiller, 2023). We collect total of 1126 dialect lexemes for initial processing. Based on the dictionary definitions of the selected lexemes, we manually filter out: (1) potentially derogatory lexemes; (2) culture-unique lexemes without Standard American English (SAE) equivalents. We then carefully read the dictionary definitions of each remaining dialect lexeme and assign it SAE equivalent lexeme with the same meaning, creating list of pair-wise corresponding lexical features for each dialect. Examples of selected pairs can be seen in Table 1 and Figure 1. Next, we use GPT4o (Hurst et al., 2024) to generate prompts for each SAE word in our paired lexical feature set. We specifically instruct the model to generate prompts describing visual scene with the lexeme playing central role, which can be one of the following depending on the semantic role of the lexeme: (1) The central object in the scene; (2) The main action of the central object; (3) prominent descriptive feature of the central object. We also ask the model to create two different sets of Concise and Detailed prompts for each SAE lexeme. Then we simply replace the SAE lexeme in the prompts with the dialect lexeme  (Table 1)  to create our two dialect evaluation settings: Concise prompts generally consist of 6 words, with the goal of providing more challenging evaluation setting where the multimodal generative model is not given too many contextual hints about the lexemes meaning. 3 Detailed prompts generally consist of 9 words, with the goal of providing more relaxed evaluation setting where the multimodal generative model can use more contextual hints to infer the lexemes meaning. These two evaluation settings also intuitively represent two common user input styles for multimodal generative models, where casual users may tend to provide concise prompts and professional users may be more inclined to write detailed prompts. Across Concise and Detailed evaluation settings, we generate total of 6552 prompts. For specific Dialect Prompt / SAE Prompt pairs where the dialect lexeme has an additional polysemous meaning recorded in an SAE dictionary (Webster, 1869), we generate an additional SAE Polysemy Prompt, where the lexeme is used unambiguously in its SAE meaning. This data can be used for regulating model behavior in training scenarios."
        },
        {
            "title": "3.2 DIALECT SPEAKER VALIDATION AND FILTERING",
            "content": "Before admitting the generated prompts to our final evaluation benchmark, we carefully verify their quality and correctness with dialect speaker human annotators. We created specialized Amazon MTurk interface (Figure 4) for prompt annotation and matching potential dialect speaker annotators to their spoken dialect: each human annotator must first self-identify their dialect background and then complete dialect speaker assessment quiz (Ziems et al., 2023) that matches each annotator to at most one dialect (Figure 5). Annotators are only selected if both their self-identified dialect background and their quiz assessment result match to the same dialect. More details on human annotation are available in Section E. After each dialect speaker is selected, they will be presented with Dialect Prompt / SAE Prompt pairs where the only difference is the dialect lexeme being swapped with its SAE equivalent word. For each pair of prompts, the dialect speaker must answer two questions: 1. Does the given Dialect Prompt make sense in said Dialect and correspond exactly in meaning to the given SAE prompt in Standard American English? (Yes / No / dont know) 2. Is the given Dialect Prompt ambiguous? i.e., Does it have reasonable alternative interpretation in the Standard American English (SAE) context? (Yes / No / dont know) Each Dialect Prompt / SAE Prompt pair is presented to two independent dialect speaker annotators. pair is included in the final dataset only if both human annotators answer Yes to the first question and No to the second question. Consistent responses ensure the dialect prompt is: (1) exactly synonymous with the SAE prompt. (2) valid in the dialectal context. (3) non-ambiguous (for polysemous lexemes). In total, dialect speaker filtering further removes 35.9% of all generated prompts, resulting in final dataset containing 4,200 validated prompts."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EVALUATION METRICS Automatic Evaluation To automatically evaluate any multimodal generative model G() on our benchmark, we design scoring functions based on reference-free image-text alignment metrics, including VQAScore (Lin et al., 2024) and CLIPScore (Hessel et al., 2021). For simplicity, we denote any such alignment metric below as A. We further denote the DialectGen prompt subset for any dialect as P, which contains many SAE Prompt / Dialect Prompt pairs = (ps, pd). For each individual text prompt ps or pd, we generate images under different random seeds for text-to-image generative models, or uniformly sample frames in video for text-to-video generative models. Therefore, for each SAE Prompt / Dialect Prompt pair = (ps, pd) P, we can calculate its SAE and Dialect performance as follows: SAE(p, G) = 1 (cid:88) i= 4 A(ps, G(ps)i) (1) Table 2: DialectGen benchmark results for popular text-to-image and text-to-video generative models, including Dialect-wise Performance Drop measured by VQAScore (Lin et al., 2024); and Overall Performance Drop measured by human eval, VQAScore, and CLIPScore (Hessel et al., 2021). Cells are highlighted based on numerical value normalized across the entire table, with darker red indicating higher performance drop in the given metric. Model Overall Performance Drop (%) Human VQAScore CLIPScore Dialect-wise Performance Drop (%) SgE ChE AAE BrE InE d 2 Stable Diffusion 1.4 Stable Diffusion 1.5 Stable Diffusion 2.1 Stable Diffusion XL Stable Diffusion 3 Stable Diffusion 3.5 Large Stable Diffusion 3.5 Large Turbo Flux.1 [dev] DALL-E Mini DALL-E 2 DALL-E 3 DALL-E 3 (w/ Prompt Rewrite) gpt-image-1 (4o Image Gen) Cosmos-1 Open-Sora VideoCrafter-2 CogVideoX Wan 2.1 o 2 e I 2 Stable Diffusion 1.4 Stable Diffusion 1.5 Stable Diffusion 2.1 Stable Diffusion XL Stable Diffusion 3 Stable Diffusion 3.5 Large Stable Diffusion 3.5 Large Turbo Flux.1 [dev] DALL-E Mini DALL-E 2 DALL-E 3 DALL-E 3 (w/ Prompt Rewrite) gpt-image-1 (4o Image Gen) m e n s o e t Cosmos-1 Open-Sora VideoCrafter-2 CogVideoX Wan 2.1 o 2 28.19 29.77 31.46 29.8 31.89 32.31 32.92 36.43 34.29 38.63 26.55 20.19 22.18 25.41 29.98 32.5 40.06 48.17 14.33 16.56 17.39 17.12 17.15 18.42 19.9 23.29 24.71 17.73 12.18 6.55 8. 18.04 17.16 22.59 31.87 32.69 26.7 27.06 28.79 26.69 29.01 29.43 30.28 32.26 31.52 32.79 24.39 18.25 19.18 20.49 26.63 30.24 42.55 47.33 15.93 16.17 18.4 17.09 18.64 19.54 20.63 21.25 21.44 20.2 13.27 10.77 10.97 14.28 14.1 18.31 29.6 31.94 10.35 10.32 11.7 10.88 10.81 11.37 11.34 10.88 11.71 9.97 9.32 6.69 7. 6.66 8.93 10.51 11.04 13.1 5.16 5.51 5.78 5.83 5.86 6.12 6.09 5.46 7.05 5.98 4.29 2.97 3.24 4.3 4.57 5.91 8.08 8.59 20.67 19.51 24.35 23.37 27.89 28.3 30.33 30.61 33.91 35.87 18.97 22.11 26.12 22.15 22.59 25.36 38.33 52.68 11.65 11.18 15.06 14.09 14.74 15.7 15.06 14.84 27.56 18.43 8.85 11.93 13. 11.05 13.49 16.97 21.33 30.23 9.64 8.66 9.31 7.95 8.64 9.74 9.27 10.83 8.18 7.95 3.58 6.48 5.2 9.69 9.19 9.43 23.75 31.27 4.37 5.39 5.51 5.56 6.67 6.99 8.13 9.11 5.29 6.52 4.74 5.28 4.46 9.25 5.13 4.18 14.63 14.97 34.94 36.5 44.82 41.22 42.67 42.66 43.6 44.64 47.11 48.78 41.95 26.86 26. 26.1 43.09 50.36 55.18 43.83 17.35 17.34 23.03 20.57 23.85 23.46 24.94 25.69 27.35 25.5 20.98 10.62 10.56 14.04 19.4 24.16 32.74 42.58 41.27 42.15 41.12 38.74 40.69 41.9 42.49 42.59 42.85 47.21 31.9 23.05 26.51 27.44 31.74 39.95 26.1 53.38 29.23 28.7 29.36 30.12 28.94 31.83 33.42 31.4 31.47 32.8 18.91 17.09 15. 22.49 19.8 27.63 42.88 36.21 Dialect(p, G) = 1 (cid:88) i=1 A(ps, G(pd)i) 26.96 28.48 28.89 22.17 25.12 24.56 25.72 32.62 25.51 24.14 25.56 12.74 11.99 17.09 26.53 26.08 27.44 55.47 17.03 18.22 19.06 15.1 19.02 19.72 21.61 25.23 15.53 17.76 12.85 8.94 10.17 14.58 12.69 18.61 36.4 35.71 (2) Note that when calculating dialect performance, we align the SAE Prompt ps with multimodal output generated from the corresponding Dialect Prompt, i.e., G(pd). This is feasible given that the paired prompts are synonymous, as verified by dialect speaker annotators in Section 3.2. Based on this, we can compute the dialect-induced performance drop of G() for each prompt pair p: Drop(p, G) = SAE(p, G) Dialect(p, G) SAE(p, G) = (cid:88) i=1 A(G(ps)i, ps) A(G(pd)i, ps) A(G(ps)i, ps) (3) To obtain the average model performance drop for specific dialect, i.e., Drop(P, G), we simply average Drop(p, G) for all in P. Human Evaluation We further design human evaluation pipeline to check the empirical alignment between our automatic evaluation metrics and human judgment. For 5% of the model outputs in our benchmark, we ask three independent external human annotators to evaluate: to what extent does the multimodal generations conditioned on the SAE Prompt G(ps) or Dialect Prompt G(pd) match with the scene described by SAE prompt ps. Annotators are asked to rate the alignment between each (image/video, caption) pair with numerical score between 0 and 10. The numerical scores are scaled by 0.1 to match the scoring range of VQAScore and CLIPScore before calculating SAE and 5 Dialect performance. Finally, we use the same formula to calculate the dialect-induced performance drop Drop(p, G). Since we only evaluate the alignment between image/video and the SAE prompt, this task does not require dialect speaker human annotators."
        },
        {
            "title": "4.2 BENCHMARK EXPERIMENTS",
            "content": "Applying the automatic and human evaluation metrics described in Section 4.1, we evaluate popular open-weights and proprietary multimodal generative models on DialectGen. Model performances are separately aggregated for Concise Prompts and Detailed Prompts settings in Table 2. Overall Performances For each model, we record overall dialect-induced performance drop on DialectGen using three different metrics: Human Eval, VQAScore, and CLIPScore. We calculate Pearson correlation coefficients (Pearson, 1895) between each of the two metrics and observe r(Human, VQAScore) = 0.968, r(Human, CLIPScore) = 0.924, and r(VQAScore, CLIPScore) = 0.907. This shows that while both automatic scoring metrics have high correlations to human judgement (the gold standard), VQAScore is better-aligned scoring metric for measuring dialectinduced performance drop. Contrasting the model performance drops across the two evaluation settings Concise Prompts and Detailed Prompts, we can clearly see that all models exhibit significantly larger performance drops for concise prompts compared to detailed prompts. This is in line with our assumption that models can more easily infer the meanings of unknown dialect lexemes from richer prompt contexts, highlighting the need for challenging evaluation via concise prompts to reveal model robustness issues. Looking at individual model performances, we observe that among text-to-video generative models: Wan 2.1 (Wang et al., 2025) and CogVideoX (Yang et al., 2024) exhibit the largest overall performance drops while Cosmos-1 (Agarwal et al., 2025) is the most robust. While for text-to-image generative models, DALL-E 2 (Ramesh et al., 2022) and Flux.1 [dev] (Black Forest Labs, 2024) exhibit the largest overall performance drops while DALL-E 3 (Betker et al., 2023) (w/ Prompt Rewrite) and gpt-image-1 (4o Image Generation) (OpenAI, 2025) are the most robust. Dialect-wise Performance Drop In addition to overall performance, we record each models performance drop on each dialect, measured by VQAScore. Based on the color heatmap in Table 2, we can clearly see that the most severe performance drops occur for ChE and InE for most models, while AAE and SgE also suffer significant performance decreases. On the other hand, models generally do not see very significant performance drop for BrE, which is expected given the relatively higher-resource nature of the dialect."
        },
        {
            "title": "5 MITIGATION METHODS",
            "content": "The significant dialect performance drops of current multimodal generative models shown in Section 4.2 highlight the need for effective mitigation strategies to improve dialect robustness. Here, the goal is to develop method that enhances robustness across multiple dialects while preserving performance on standard SAE prompts. To this end, we first investigate intuitive baseline approaches, including (1) UNet Finetuning; (2) Prompt Revision, and then introduce our new mitigation strategy. 5.1 BASELINE METHODS UNet Finetuning The vast majority of current text-to-image and text-to-video generative models comprise two main components: text encoder and diffusion-based image/video decoder. In current post-training paradigms, typically the text encoder is kept frozen while the diffusion UNet is fine-tuned (Podell et al., 2023; Rombach et al., 2022; Betker et al., 2023; Dai et al., 2023). Existing works in aligning, enhancing, and customizing multimodal generative models also focus heavily on developing reward-based fine-tuning methods for the diffusion UNet while freezing the text encoder (Segalis et al., 2023; Clark et al., 2023; Prabhudesai et al., 2023; Black et al., 2023; Fan et al., 2023; Wallace et al., 2024; Dang et al., 2025). Based on existing works, we apply prominent multimodal generation enhancement methods towards improving dialect robustness, including: 6 Figure 2: Losses used in our mitigation. Text prompts for Dialect Learning and Polysemy Control come from the DialectGen training set, while image-caption pairs for KL Regularization come from the MSCOCO validation set. Diffusion Fine-tune (Rombach et al., 2022) given pair of synonymous Dialect / SAE Prompts, we fine-tune the diffusion UNet with the Dialect Prompt as input, and images generated using the SAE Prompt as target output. Diffusion DPO (Wallace et al., 2024) We similarly use the Dialect Prompt as input, and use images generated with the SAE Prompt / Dialect Prompt as Win / Lose pairs for DPO. Prompt Revision Beyond UNet fine-tuning, another popular family of methods for aligning and enhancing multimodal generative models is prompt revision (Hao et al., 2023; Betker et al., 2023; Wang et al., 2024; Chen et al., 2024). In our experiments, we include both general prompt rewriting method and targeted prompt translation methods using general-purpose LLMs: Prompt Rewrite We apply the general prompt rewriting pipeline in Betker et al. (2023) to all test prompts before passing them to the generative model. Prompt Translate We use general-purpose LLMs (Grattafiori et al., 2024; OpenAI, 2025) to translate all prompts to SAE before passing them to the generative model. 5.2 OUR METHOD Unlike prior approaches, we propose new mitigation strategy that focuses on updating the text encoder(s). natural first step toward improving dialectal robustness is to align the semantic representation of dialect expression with that of its corresponding SAE counterpart. Dialect Learning To operationalize this idea, we introduce Dialect Learning loss that encourages the target text encoder to recognize dialectal lexemes by minimizing the cosine distance between the target encoders embedding of dialect prompt and the frozen encoders embedding of its synonymous SAE prompt: LDL = (cid:0)1 π(pd ), π0(ps )(cid:1) . (4) 1 (cid:88) i=1 Here, , denotes cosine similarity; π() and π0() represent the trainable target text encoder and the frozen reference encoder, respectively; and pd and ps denote the i-th pair of synonymous prompts in dialect and standard English, respectively. Although this may improve dialectal robustness, relying on this loss alone may compromise the models ability to handle dialect lexemes that exhibit polysemy in SAE contexts. Polysemy Control In order to retain the models ability to correctly recognize polysemous lexemes within SAE contexts, we introduce Polysemy Control loss that minimizes the cosine distance between embeddings of the same SAE polysemous prompt generated by the target and frozen encoders: 7 Table 3: Mitigation results for all baseline methods and our best performing method, including Overall Performances on SAE MSCOCO, SAE Polysemy, average Dialect performance, and Dialect Performance for each dialect, all measured using VQAScore Lin et al. (2024). Cell colors reflect column-normalized performance values, with darker green indicating higher VQAScore performance. Mitigation Methods Overall Performances SAE SAE MSCOCO Polysemy Dialect Avg. Dialect Performance AAE BrE ChE InE SgE Base Model (Stable Diffusion 1.5) 75.49 72.84 57.80 60.13 69.39 52. 49.94 56.89 Prompt Revision DALL-E 3 Prompt Rewrite LLaMA 3 Prompt Translate GPT4.1 Prompt Translate UNet Fine-tuning Diffusion Finetune Diffusion DPO Our Encoder Tuning Methods Dialect Learning + Text Cosine Reg. + Image Cosine Reg. + Text KL Reg. + Image KL Reg. + Text KL Reg. + Polysemy Ctrl. + Image KL Reg. + Polysemy Ctrl. 74.25 74.03 74.54 65.01 63.94 67.14 67.06 67.73 72.68 71.69 72.71 74.80 70.85 71.33 71. 52.13 50.32 46.30 46.39 46.48 52.72 53.41 70.15 71.17 60.91 58.48 63.90 60.94 63.52 78.02 77.93 78.00 77.78 78.12 77.74 77.68 57.34 57.73 60. 63.85 66.31 75.21 75.44 74.91 74.40 73.77 72.24 72.61 69.51 70.4 74.39 70.14 68.91 78.33 77.84 78.20 78.27 77.23 75.76 76.74 56.36 53.98 59. 57.3 61.22 79.31 79.31 79.45 78.36 79.06 78.95 77.51 57.54 50.42 60.20 52.84 56.38 78.10 78.22 78.33 78.17 79.25 80.67 80.41 63.81 59.87 64. 60.56 64.79 79.15 78.86 79.11 79.71 81.29 81.07 81.14 LPC = 1 (cid:88) i= (1 π(pm ), π0(pm )) , (5) where each pm examples containing SAE polysemous lexemes. is polysemous SAE prompt sampled from the dataset. This loss is applied only to KL Regularization In addition to the previous two losses, it is also essential to preserve the models performance on general SAE prompts. To this end, one might consider employing the conventional Kullback-Leibler (KL) divergence loss, which promotes alignment between the output distributions of trainable target model and frozen reference model over predefined discrete logit space. However, this approach is not directly applicable in our setting, as text encoders output continuous embeddings rather than discrete logits. To address this challenge, we approximate the output distribution by computing similarity scores between given caption embedding and set of reference image embeddings drawn from joint image-text embedding space. Concretely, we begin , ximg by sampling caption-image pairs from general SAE dataset such as MSCOCO (Lin et al., 2014). For each pair, we compute the caption embedding Ci = π0(xcap ) using frozen text encoder π0, and the corresponding image embedding Ii = ϕ0(ximg ) using frozen image encoder ϕ0, with both encoders operating in the same shared text-image embedding space. The resulting image embeddings {Ii [M ]} serve as reference anchors for computing similarity scores with given caption embedding. These scores act as surrogate logits that approximate the output distributions required for the KL divergence computation. Specifically, for each caption xcap , we define the approximated output distributions for the frozen encoder π0 and the trainable target encoder π as: sπ0 = [I1, Ci, . . . , IM , Ci] , = [I1, sπ i] , i, . . . , IM , ) [M ] (xcap (6) (cid:110) (cid:111) where = π(xcap ). Given these simulated logits, we define the KL divergence loss to encourage the target encoders output distribution to remain close to that of the frozen encoder: LKL = 1 (cid:88) i=1 KL (softmax(sπ ) softmax(sπ )) . (7) This approach is compatible with CLIP-style models (Radford et al., 2021; Zhai et al., 2023), in which image and text embeddings are aligned within shared representation space. When an image encoder is unavailable, we instead use the frozen caption embeddings {Ci [M ]} as proxies for 8 reference anchors. We hereafter refer to the case where image embeddings are used as reference anchors as Image KL Reg. and the one using text embeddings as Text KL Reg. Based on these design choices, the final combined loss function integrates all three components: = LDL + LPC + LKL as illustrated in Figure 2. For more details, please refer to Section B."
        },
        {
            "title": "5.3 MITIGATION RESULTS",
            "content": "Here, we validate all baselines and our method on SD1.5 and SDXL. Due to space limitations, the results for SDXL are reported in Section F."
        },
        {
            "title": "5.3.1 COMPARISON WITH THE BASELINES",
            "content": "As shown in Table 3, prompt rewriting methods that operate solely at the input level do not degrade SAE MSCOCO or polysemy performance, but yield only slight improvements up to 6.1% in average dialect performance. Furthermore, UNet fine-tuning approaches also lead to small gains of up to 5.7% in dialect performance, but at the cost of substantial drops in both general SAE and polysemy scores. In contrast, our method, corresponding to the last row of the table and incorporating all three loss components described in Section 5.2, significantly improves dialect robustness across all five dialects. Its average dialect performance of 77.68% closely approaches the base models SAE score of 77.91%, while causing negligible degradation in SAE MSCOCO and polysemy performance. 5.3.2 ABLATION STUDY To evaluate the contribution of each component in our method, we conduct an ablation study here. Base Model vs. Dialect Learning As shown in Table 3, applying the Dialect Learning loss (LDL) alone yields huge improvements in the base models dialect performance, but also degrades SAE MSCOCO and polysemy performance. Cosine Reg. vs. KL Reg. To solve this issue, simply maximizing cosine similarity between the target text encoders text embeddings and the corresponding text/image embeddings from the frozen text/image encoder (denoted as Text/Image Cosine Reg.), which is computed over the same caption-image pairs used in our KL regularization, does not effectively recover the base models SAE MSCOCO and polysemy performance. In contrast, adding our KL regularization loss (LKL) improves both metrics while preserving dialect gains. Adding Polysemy Ctrl. Finally, incorporating the Polysemy Control loss (LPC) yields substantial gains in polysemy performance, improving it by 17.43% and 17.76% for Text and Image KL Reg. respectively, underscoring the importance of this component in recognizing polysemous lexemes within SAE contexts."
        },
        {
            "title": "6 LIMITATIONS",
            "content": "Our study focuses on the lexical variations that characterize dialects, motivated by the empirical observation that such variations exert much greater influence on multimodal generative model performance than grammatical variations (see Section G). Furthermore, grammatical variation has already been the subject of extensive investigation in text-only contexts (Hudson, 1996; Chambers & Trudgill, 1998; Fromkin et al., 1998; Nerbonne, 2009; Wardhaugh & Fuller, 2021). These considerations jointly motivate our decision to prioritize the evaluation of lexical dialect variation, which appears especially consequential in the multimodal generative setting. Furthermore, our evaluation of text-image alignment utilizes reference-free metrics, namely VQAScore (Lin et al., 2024) and CLIPScore (Hessel et al., 2021). We recognize that these pretrained vision-language models are not perfect. To address this potential weakness, we conducted thorough human evaluation and found very high statistical correlation between our automatic metrics and human judgment (Pearson correlation coefficient = 0.968 for VQAScore and = 0.924 for CLIPScore). Therefore, while acknowledging the imperfections of automated metrics, this high degree of human correlation provides strong evidence for the validity of our evaluation metrics and associated analysis conclusions."
        },
        {
            "title": "7 CONCLUSIONS",
            "content": "In this work, we create DialectGen, large-scale multi-dialectal benchmark evaluating the dialect robustness of multimodal generative models. Our experiments on 17 widely used text-to-image and text-to-video generative models reveal severe performance drops up to 38.63% and 48.17% for image and video generative models, respectively. We further design an encoder-based mitigation strategy to enhance dialect robustness while preserving performance on Standard American English."
        },
        {
            "title": "8 ETHICS STATEMENT",
            "content": "This work makes use of human subjects for annotation and evaluation. All procedures were subject to ethical review and were approved by the IRB from the authors institution. Consent was gathered in accordance with the authors institution guidelines, and annotators had access to data use statement when giving consent. The purpose of DialectGen is to provide tools that enable researchers and practitioners to evaluate and improve dialect robustness in their models. We will release these data responsibly, ensuring that users sign Data Use Agreement that forbids the use of DialectGen for deception, impersonation, mockery, discrimination, hate speech, targeted harassment, and cultural appropriation. In the agreement, researchers and practitioners will also acknowledge the limitations of this work, that DialectGen may not fully or accurately represent the natural usage patterns of all sub-communities of speakers. DialectGen is designed to be easily updatable and configurable, such that it can be extended by and for specific sub-communities and updated as dialects evolve over time. We have carefully checked our data to make sure no personally identifying information or offensive content is included. When utilizing existing artifacts and models, we make sure to follow all relevant regulations and licenses."
        },
        {
            "title": "9 REPRODUCIBILITY STATEMENT",
            "content": "We have taken several steps to ensure the reproducibility of our work. Detailed descriptions of dataset construction, annotation procedures, evaluation protocols, and mitigation methods are provided in the main paper (see Sections 3, 4, etc.), with further implementation details, training configurations, and additional qualitative results included in the appendix (see Sections B, A, etc.). To facilitate independent verification, we also provide as anonymized supplementary material both the DialectGen benchmark dataset and the source code used for data processing, model training, and evaluation. The dataset files include all validated dialectSAE prompt pairs, while the code folder contains scripts for dataset generation, automatic and human evaluation, and reproduction of all tables and figures reported in the paper. Together, these resources enable researchers to replicate our experimental results and extend the benchmark for future work."
        },
        {
            "title": "10 ACKNOWLEDGEMENTS",
            "content": "We would like to thank Connor Couture and Allen Cheung for designing the initial version of our MTurk annotation interface, and Xinrong Du for providing feedback. We also thank Prof. Diyi Yang, Prof. Xiang Chen, Caleb Ziems, Julia Kruk, Hritik Bansal, Jiachen Gu, Zongyu Lin, and Amita Kamath for valuable discussions and pointers; and especially Caleb Ziems for sharing the grammar-based dialect-speaker quiz he created. Finally, we appreciate Wenbo Hu, Lucas Bandarkar, Mohsen Fayyaz, and Tanmay Parekh for their helpful feedback on the paper draft."
        },
        {
            "title": "REFERENCES",
            "content": "Nur Aeni, Like Raskova Octaberlina, Nenni Dwi Aprianti Lubis, et al. literature review of English Language Variation on Sociolinguistics. OSF, 2021. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 610623, 2021. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Su Lin Blodgett, Johnny Wei, and Brendan OConnor. Twitter universal dependency parsing for african-american and mainstream american english. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14151425, 2018. Frederic Cassidy, Joan Houston Hall, and Luanne Von Schneidemesser. Dictionary of American regional english, volume 1. Belknap Press of Harvard University Press Cambridge, Mass., 1985. Jack Chambers and Peter Trudgill. Dialectology. Cambridge University Press, 1998. Zijie Chen, Lichao Zhang, Fangsheng Weng, Lili Pan, and Zhenzhong Lan. Tailored visions: Enhancing text-to-image generation with personalized prompt rewriting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 77277736, 2024. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. David Crystal. English as Global Language. Cambridge University Press, Cambridge, 2 edition, 2003. ISBN 9780521823470. Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. Meihua Dang, Anikait Singh, Linqi Zhou, Stefano Ermon, and Jiaming Song. Personalized preference fine-tuning of diffusion models. arXiv preprint arXiv:2501.06655, 2025. Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc Lê Khac, Luke Melas, and Ritobrata Ghosh. Dalle mini, 7 2021. URL https://github.com/borisdayma/ dalle-mini. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis, 2024. URL https://arxiv. org/abs/2403.03206, 2, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. VA Fromkin, Robert Rodman, and Hyams. An introduction to language 6e. Hartcourt Brace College Publishers: Orlando, FL, USA, 1998. 11 Henry Louis Gates, James Murray, et al. The Oxford Regional English Dictionary. Oxford University Press, Oxford, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer, and Noah Smith. Whose language counts as high quality? measuring language ideologies in text data selection. arXiv preprint arXiv:2201.10474, 2022. Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. Advances in Neural Information Processing Systems, 36:6692366939, 2023. Jennifer KN Heinmiller. Compiling the oxford dictionary of african american english: progress report. Dictionaries: Journal of the Dictionary Society of North America, 44(1):91104, 2023. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Joseph Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Conference on Empirical Methods in Natural Language Processing, 2021. Dirk Hovy and Shannon Spruit. The social impact of natural language processing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 591598, 2016. Richard Hudson. Sociolinguistics. Cambridge university press, 1996. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Anna Jørgensen, Dirk Hovy, and Anders Søgaard. Challenges of studying and processing dialects in social media. In Proceedings of the workshop on noisy user-generated text, pp. 918, 2015. Jack Tsen-Ta Lee. Dictionary of Singlish and Singapore English. Lee, Jack Tsen-Ta, 2004. Accessed 2025-05-16. Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. Advances in Neural Information Processing Systems, 36:6998170011, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), 2014. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pp. 366384. Springer, 2024. John Nerbonne. Data-driven dialectology. Language and linguistics compass, 3(1):175198, 2009. OpenAI. Introducing 4o image generation. https://openai.com/index/ introducing-4o-image-generation/, 2025. Accessed: 2025-05-19. OpenAI. Gpt-4.1. https://openai.com/index/gpt-4-1/, April 2025. Accessed 18 May 2025. Karl Pearson. Notes on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58:240242, 1895. doi: 10.1098/rspl.1895.0041. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 12 Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion models with reward backpropagation (2023). arXiv preprint arXiv:2310.03739, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Dalle 2: new ai system that can create realistic images and art from description in natural language. https: //openai.com/research/dall-e-2, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. picture is worth thousand words: Principled recaptioning improves image generation. ArXiv, abs/2310.16656, 2023. URL https://api.semanticscholar.org/CorpusID:266003242. V. Subhash. Dictionary of Indian English. V. Subhash, Hyderabad, 2020. ISBN 9789354374487. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Yixin Wan, Arjun Subramonian, Anaelia Ovalle, Zongyu Lin, Ashima Suvarna, Christina Chance, Hritik Bansal, Rebecca Pattichis, and Kai-Wei Chang. Survey of bias in text-to-image generation: Definition. Evaluation, and Mitigation, 2024. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Zhijie Wang, Yuheng Huang, Da Song, Lei Ma, and Tianyi Zhang. Promptcharm: Text-to-image generation through multi-modal prompting and refinement. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pp. 121, 2024. Ronald Wardhaugh and Janet Fuller. An introduction to sociolinguistics. John Wiley & Sons, 2021. Noah Webster. An American dictionary of the English language. Merriam, 1869. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023. 13 Yu Zhou, Bingxuan Li, Mohan Tang, Xiaomeng Jin, Te-Lin Wu, Kuan-Hao Huang, Heng Ji, Kai-Wei Chang, and Nanyun Peng. Contrastive visual data augmentation. In ICML 2025, 2025. Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Brooke Anderson, and Diyi Yang. Value: Understanding dialect disparity in nlu. In Annual Meeting of the Association for Computational Linguistics, 2022. Caleb Ziems, William B. Held, Jingfeng Yang, and Diyi Yang. Multi-value: framework for cross-dialectal english nlp. ACL 2023, 2023."
        },
        {
            "title": "A QUALITATIVE COMPARISON",
            "content": "In Figure 3, we provide additional qualitative examples to demonstrate the performances of the baseline mitigation strategy, Diffusion DPO (Wallace et al., 2024), compared with our method. Specifically, we update the Stable Diffusion 1.5 model encoder using Dialect Learning, Polysemy Control, and Image KL. After mitigation, we ask each model to generate images based on the four dialect prompts first mentioned in Figure 1. The Stable Diffusion 1.5 Base model struggles to generate correct images for most of these prompts, including \"Two ang pows on table\", \"A man selling brinjal\", and \"A man hiking with his carnal\". While the model is able to generate moderately reasonable images for the prompt \"A man driving his whip\", it commonly generates physically implausible details such as the mans torso protruding through the car. Fine-tuning the UNet with Diffusion DPO is able to slightly improve generation alignment with the text prompt (e.g., occasionally generating two people for the prompt \"A man hiking with his carnal\"). However, it more often blends visual elements within the desired target images with other irrelevant objects (e.g., generating man selling purple pastries in place of eggplants or man wearing purple shirt holding vegetables). Our method generates higher-quality and better aligned images compared to the base model and Diffusion DPO by accurately learning to generate the target concepts without negatively impacting image quality. significant majority of images in our sampled generations are able to generate images that correctly depict the target prompts, in line with quantitative evaluation results. Figure 3: Qualitative Comparison of Mitigation Strategies using the Stable Diffusion 1.5 model (Rombach et al., 2022) on four different dialect prompts. Specifically, we compare the dialect prompt image generation results of the Stable Diffusion 1.5 Base Model, Stable Diffusion 1.5 fine-tuned with Diffusion DPO (Wallace et al., 2024), and Stable Diffusion 1.5 updated via our best performing method (Dialect Learning + Image KL Regularization + Polysemy Control)."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "Data Preparation We first split the DialectGen dataset into training, validation, and test sets in ratio of 80%, 10%, and 10%, respectively. These training and validation splits of DialectGen are used to compute the Dialect Learning loss and the Polysemy Control loss. For KL Regularization loss, we randomly sample 1,024 and 256 image-caption pairs from the MSCOCO validation set (Lin et al., 2014) for use in training and validation, respectively. The target text encoder is evaluated on 15 the validation set at the end of each epoch, and the checkpoint with the lowest validation loss is selected and saved for final evaluation. We then evaluate SAE polysemy and per-dialect performance using the test split of DialectGen, and assess SAE MSCOCO performance on 50 randomly sampled captions from the MSCOCO validation set. Training We employ the pretrained text encoder and fine-tune it for 30 epochs using the AdamW optimizer with an initial learning rate of 1 104, β1 = 0.9, β2 = 0.999, and ϵ = 1 108. cosine annealing learning rate scheduler is applied across the 30 training epochs. The batch size, i.e., in Equation (4) and Equation (5), is set to 32, and the number of image-caption pairs used for KL regularization, i.e., in Equation (7), is set to 1,024. Training is completed in less than one hour on single NVIDIA RTX A6000 GPU. In the case of SDXL, which includes both Base and Refiner encoders, the number of pairs for the Refiner encoder is set to 512 due to its larger size, and training takes approximately one hour using four NVIDIA RTX A6000 GPUs, with all other configurations kept the same as in the Stable Diffusion 1.5 and SDXL Base encoder settings. About T2Video Models Video-generation models incur substantially higher computational cost than their image counterparts. Since our primary goal is to assess the models ability to interpret and render textual prompts, we generate only small, fixed number of frames per video. This strategy is justified by two observations: (i) the first few frames typically suffice to judge prompt fidelity, and (ii) our prompts do not exhibit extensive motion, so long sequences offer diminishing returns. All models were obtained by cloning their official repositories and following the authors installation instructions. Frame numbers were uniformly reduced frame counts when possible, and in some cases, spatial resolution was also reduced to facilitate efficient evaluationsee Table 5 for the precise settings. Average time per video was measured on single NVIDIA RTX A6000 GPU; the Wan2.1-T2V-14B model, which does not fit in single-GPU memory, was benchmarked using six A6000 GPUs under Fully Sharded Data Parallel (FSDP) supported by the repository under the xdit framework. All models except Wan2.1 fit under single A6000 GPU and use approximately 20-30 GB of VRAM max. Wan2.1 takes at least 3 GPUs, taking an approximate memory usage of 100GB of combined VRAM."
        },
        {
            "title": "C MODEL DETAILS",
            "content": "We provide detailed information on the multimodal generative models and key experimental settings used in our benchmark. Table 4 lists the comprehensive specifications for all models evaluated in our work, including both text-to-image and text-to-video models. For each model, we provide details such as its creator organization, initial release date, hosting platform, availability type (e.g., open source, proprietary), and model size. Table 5 describes in detail the key generation parameters used for the text-to-video models. This includes the specific resolution, number of frames, and inference steps used for each model. Furthermore, we specify the average time required to generate single video and the total time needed to generate our full video dataset to aid in understanding the reproducibility and computational cost of our experiments."
        },
        {
            "title": "D DATASET DETAILS",
            "content": "The final DialectGen Dataset contains total of 4632 prompts, which include 2100 non-SAE dialect prompts, 2100 SAE prompts, and 432 polysemous SAE prompts. The entire dataset is split into three subsets: training, validation, and test. The data split ratio is train : validation : test = 8 : 1 : 1. All benchmarking experiments are performed on the entire dataset, while for mitigation experiments, models are trained on the DialectGen training set while evaluated on the validation set. 16 Table 4: Detailed Model Specifications for all multimodal generative models (text-to-image and textto-video generative models) benchmarked in this work. For reference and reproducibility, we include model name, model type, creator organization, initial release date, hosting platform, availability type, and model size. Model Name Model Type Created by Release Date Hosted by Availability Type Model Size Text to Image CompVis Stable Diffusion 1.4 Text to Image Runway ML Stable Diffusion 1.5 Stability AI Text to Image Stable Diffusion 2.1 Stability AI Text to Image Stable Diffusion XL Stability AI Text to Image Stable Diffusion 3 Medium Stability AI Stable Diffusion 3.5 Large Text to Image Stable Diffusion 3.5 Large Turbo Text to Image Stability AI Flux.1 [dev] DALL-E Mini DALL-E 2 DALL-E 3 gpt-image-1 VideoCrafter-2 Open-Sora CogVideoX Cosmos-1 Wan 2.1 Text to Image Black Forest Labs Text to Image Boris Dayma et al. Text to Image OpenAI Text to Image OpenAI Text to Image OpenAI Text to Video Tencent Text to Video HPC-AI Tech Text to Video THUDM Lab Text to Video Nvidia Text to Video Alibaba 8/22/2022 10/20/2022 12/7/2022 7/26/2023 6/12/2024 10/22/2024 10/22/2024 4/2/2024 7/25/2022 9/28/2022 8/20/2023 4/23/2025 1/26/2024 6/17/2024 8/27/2024 1/6/2025 2/22/2025 Hugging Face Open Source Hugging Face Open Weights Hugging Face Open Weights Hugging Face Open Weights Hugging Face Open Weights Hugging Face Open Weights Hugging Face Open Weights Hugging Face Open Weights Open Weights Github Proprietary OpenAI Proprietary OpenAI OpenAI Proprietary Hugging Face Open Weights Hugging Face Open Weights Hugging Face Open Weights Hugging Face Open Weights Hugging Face Open Weights 1B 1.3 1.3 6.6 2 8.1 8.1 12B 0.4 N/A N/A N/A 1.4 1.2 5 7 14 Table 5: Key Generation Parameters for Text-to-Video Generative Models. For reproducibility and computational cost estimation, we list GPU runtime per video in minutes and GPU runtime for the full video dataset (both concise and detailed = 4110 videos) in hours. All computational costs are estimated for NVIDIA-A6000 GPUs with 48 GB Memory. Model Version Resolution Frames Steps Time / Video (min) Time / Dataset (h) VideoCrafter2 342.5 OpenSora-STDiT-v3 570.8 CogVideoX-5b 416.7 Cosmos-1.0-Diffusion-7B-Text2World 1815.3 329.4 Wan2.1-T2V-14B Note: The dataset-scale timing for Wan2.1-T2V-14B was measured using 6 A6000 GPUs using xdit FSDP. 512 512 405 720 720 480 704 1280 832 480 5.0 8.3 6.1 26.5 4.8 16 51 10 121 10 50 30 10 35"
        },
        {
            "title": "E HUMAN ANNOTATION DETAILS",
            "content": "Figure 4: The Amazon Mechanical Turk Data Annotation Interface for dialect speaker human filtering of generated prompts (prompt generation details in Section 3). Human annotators may use the \"View Instructions\" button to collapse / re-open detailed annotation instructions at any time. The annotation interface places no maximum time limit on each annotation question. Human annotators are allowed to return to previously annotated questions and update their answers at any time. In the creation of the DialectGen Dataset, we recruit total of 17 dialect speaker human annotators from Amazon Mechanical Turk. The demographic involves six annotators from Asia, eight annotators from North America, and 3 annotators from Europe. Each selected annotator is given the option to complete any number of questions as they prefer. We encourage each annotator to take regular breaks during the task and not to work consecutively for more than 2 hours on our task. Our task is relatively simple for dialect speakers as it mainly involves judging the plausibility and meaning of sentence in their native dialect. We estimate each HIT to take around 12 seconds, this corresponds to an hourly wage of $15 USD. Our total annotation time is 21.84 hours, costing total of $327.6. We ran 4 rounds of annotations, with combined total of 6552 prompts. 35.9% of total proposed prompts were rejected by the annotators while 64.1% of prompts were approved."
        },
        {
            "title": "F MITIGATION RESULTS ON STABLE DIFFUSION XL",
            "content": "Stable Diffusion XL consists of two encoders: Base encoder and Refiner encoder. We fine-tuned both components as part of our method. However, since the corresponding CLIP-style image encoder for the Refiner is not publicly accessible, only Text KL Regularization can be applied in this case. Given the Refiners larger size and additional encoding modules, we evaluate our final method against other baselines within this more complex configuration. 18 Figure 5: The English Dialect Speaker Assessment Quiz used for matching dialect speaker annotators to specific dialects for prompt annotation. We adapt the assessment quiz from the existing English Dialect Speaker Survey first created in MultiVALUE (Ziems et al., 2023), which asks the human annotator to select their linguistic acceptability preference for 10 different dialect excerpts. We report the mitigation results on Stable Diffusion XL (Podell et al., 2023) in Table 6, under the experimental setup described above. Similar to the findings on Stable Diffusion 1.5, Prompt Revision methods preserve general SAE performance but yield only marginal improvements in dialect VQAScore, with gains of up to 7.8%. Additionally, UNet fine-tuning methods also result in small gains of up to 5.3% in dialect performance, but at the cost of noticeable degradation in both SAE MSCOCO and SAE polysemy performance. In contrast, our method substantially improves dialect robustness across all five dialects, achieving an average performance of 85.99%, which surpasses the 19 Table 6: Mitigation results on SDXL (Podell et al., 2023) for all methods, including Overall Performances on SAE MSCOCO, SAE Polysemy, average Dialect performance, and Dialect Performance for each dialect, all measured using VQAScore (Lin et al., 2024). Cell colors reflect column-normalized performance values, with darker green indicating higher VQAScore performance. Mitigation Methods Overall Performances SAE SAE MSCOCO Polysemy Dialect Avg. Dialect Performance AAE BrE ChE InE SgE Base Model (Stable Diffusion XL) 86.21 78.21 61.55 61.17 77.58 47. 53.21 68.76 Prompt Revision DALL-E 3 Prompt Rewrite LLaMA 3 Prompt Translate GPT4.1 Prompt Translate UNet Fine-tuning Diffusion Finetune Diffusion DPO Ours 85.36 84.72 85.93 70.49 72.03 78.01 77.60 78.12 52.37 50.29 66.49 64.19 69. 65.22 66.89 59.93 63.74 61.97 77.92 77.93 82.24 60.61 57.40 63.87 63.62 56.09 65.45 70.39 65.80 72. 65.31 65.97 76.69 78.12 60.12 62.88 58.05 60.10 65.91 67.40 Dialect Learning + Text KL Reg.+ Polysemy Reg. 85.45 78.08 85.99 82.43 84.71 85. 89.70 87.14 base models SAE score of 84.43%, while inducing less than 1% drop in both SAE MSCOCO and SAE polysemy performance. Table 7: Quantitative Effects of Grammatical and Lexical Variations on Multimodal Generation, measured in VQAScore. We evaluate three text-to-image generative models under the following dialectal variation types: Grammatical, Lexical, and Grammatical + Lexical. Values in parentheses indicate the percentage performance drop in VQAScore compared to baseline SAE performance. Model SAE Performance (%) Performance under Dialectal Variations (%) Grammatical Lexical Grammatical + Lexical DALL-E Mini FLUX.1 dev Stable Diffusion 3.5 Large 75.63 82.94 85. 74.72 (-1.20) 82.40 (-0.65) 83.91 (-1.49) 51.92 (-31.35) 61.88 (-25.39) 65.37 (-23.26) 51.26 (-32.22) 61.02 (-26.43) 63.80 (-25.10) GRAMMATICAL VS. LEXICAL ROBUSTNESS IN MULTIMODAL MODELS To establish the rationale for our studys focus on lexical variations, we begin with an observation about multimodal generative models. These models often exhibit notable insensitivity to grammatical or syntactic structure, tendency that likely arises from the bag-of-words nature of their CLIP-style encoders. This architectural trait means that variations in sentence construction, such as word order or verb tenses, tend to have minimal effect on the final output. Table 8, adapted from Multi-VALUE (Ziems et al., 2023), showcases several examples of these grammatical variations. Table 8: Examples of Grammatical Dialect Variations between Standard American English (SAE) sentences and African American English (AAE) dialect sentences. The blue texts highlight unique features in SAE while the purple texts (if applicable) highlight corresponding features in AAE. Grammatical Variation Type SAE Prompt AAE Dialect Prompt Clause Structure Negative Concord Word Order Verb Morphology chair that can be folded There is no food on the table There aint no food on the table big and fresh fish Mom brought rice to me fish big and fresh Mom brin rice give me chair can be folded To formally quantify this observation, we conducted small-scale experiment with three representative models in the African American English evaluation setting. We used the Multi-VALUE (Ziems et al., 2023) translation system to apply grammatical variations to 300 SAE prompts from DialectGen and evaluated their generation quality using VQAScore. 20 Table 9: Stable Diffusion 1.5 Mitigation Performance Breakdown by dialect for different mitigation methods on the DialectGen dataset for all baseline methods and ablations of our method. All performance scores are measured using VQAScore (Lin et al., 2024), higher score is better. Mitigation Methods Performance by Dialect (VQAScore) AAE BrE ChE InE SgE Dialect SAE Dialect SAE Dialect SAE Dialect SAE Dialect SAE Base Model (Stable Diffusion 1.5) 57.34 72. 69.51 76.40 56.36 78.66 57.54 81. 63.81 80.50 Prompt Revision DALL-E 3 Prompt Rewrite LLaMA 3 Prompt Translate GPT4.1 Prompt Translate UNet Fine-tuning Diffusion Finetune Diffusion DPO Ours Dialect Learning + Text Cosine Reg. + Image Cosine Reg. + Text KL Reg. + Image KL Reg. + Text KL Reg.+ Polysemy Ctrl. + Image KL Reg.+ Polysemy Ctrl. 57.73 60.87 65.32 63.85 66.31 75.21 75.44 74.91 74.40 73.77 72.24 72.61 73.16 70.36 71. 64.34 63.02 74.31 74.86 74.83 73.97 74.36 72.25 74.30 70.40 74.39 73.52 70.14 68.91 78.33 77.84 78.20 78.27 77.23 75.76 76.74 77.86 76.49 76. 68.35 69.17 78.34 77.52 78.22 79.40 77.60 79.57 76.77 53.98 59.05 58.32 57.30 61.22 79.31 79.31 79.45 78.36 79.06 78.95 77.51 79.99 78.15 78. 69.55 67.83 80.20 79.74 80.32 80.72 80.43 79.27 78.83 50.42 60.22 53.04 52.84 56.38 78.10 78.22 78.00 78.17 79.25 80.67 80.41 81.33 81.09 81. 70.72 70.94 79.90 80.13 80.00 78.24 80.99 79.89 80.85 59.87 64.98 65.12 60.56 64.79 79.15 78.86 79.11 79.71 81.29 81.07 81.14 81.66 79.84 79. 72.42 71.85 78.33 79.21 78.72 78.66 79.54 79.84 78.15 Table 10: Complete DialectGen Benchmark Performance Breakdown by dialect for all text-toimage and text-to-video generative models. All performance scores are measured using VQAScore (Lin et al., 2024), higher score is better. Results complements Table 2 in the main paper. Model d 2 Stable Diffusion 1.4 Stable Diffusion 1.5 Stable Diffusion 2.1 Stable Diffusion XL Stable Diffusion 3 Stable Diffusion 3.5 Large Stable Diffusion 3.5 Large Turbo Flux.1 [dev] DALL-E Mini DALL-E 2 DALL-E 3 DALL-E 3 w/ Rewrite gpt-image-1 Cosmos-1 Open-Sora VideoCrafter-2 CogVideoX 2 Wan 2.1 e I 2 Stable Diffusion 1.4 Stable Diffusion 1.5 Stable Diffusion XL Stable Diffusion 2.1 Stable Diffusion 3 Stable Diffusion 3.5 Large Stable Diffusion 3.5 Large Turbo Flux.1 [dev] DALL-E Mini DALL-E 2 DALL-E 3 DALL-E 3 w/ Rewrite gpt-image-1 Cosmos-1 Open-Sora VideoCrafter-2 CogVideoX Wan 2. d 2 p P c s o e t Performance by Dialect (VQAScore) AAE BrE ChE InE SgE Dialect SAE Dialect SAE Dialect SAE Dialect SAE Dialect SAE 60.66 62.31 60.97 62.97 60.9 60.16 57.27 55.63 50.86 52.07 67.09 63.74 65.47 59.61 65.46 61.3 36.72 29.57 70.07 71.03 72.82 69.41 74.27 73.21 73.24 72.86 53.69 64.72 77.75 76.73 78.26 64.61 74.81 70.88 39.83 55.79 76.47 77.41 80.59 82.17 84.46 83.91 82.2 80.17 76.96 81.19 82.8 81.83 88. 76.57 84.56 82.13 59.54 62.49 79.31 79.97 84.76 81.72 87.11 86.84 86.23 85.56 74.12 79.34 85.3 87.12 90.7 72.64 86.48 85.37 50.63 79.96 71.46 72.59 76.37 80.49 79.22 80.53 79.4 72.7 73.55 79.19 85.68 84.24 88.39 68.87 75.56 76.19 42.55 47.02 74.19 73.5 80.84 77.51 82.58 83.24 81.07 77.43 69.5 80.2 83.82 85.56 86. 67.1 76.69 79.53 46.4 62.14 79.08 79.47 84.21 87.44 86.71 89.22 87.51 81.53 80.1 86.03 88.86 90.08 93.24 76.26 83.21 84.12 55.8 68.41 77.58 77.69 85.6 82.03 88.48 89.5 88.24 85.19 73.38 85.79 87.99 90.33 90.94 73.94 80.84 83 54.35 73.08 51.31 50.4 45.88 49.82 48.32 48.93 47.16 45.85 41.51 42.54 50.43 61.41 65. 53.27 48.49 42.9 27.71 30.37 65.24 65.21 68.19 63.64 66.21 67.05 64.83 61.47 52.39 62.33 68.16 76.36 79.47 57.62 67.65 66.14 38.89 42.39 78.86 79.37 83.15 84.75 84.29 85.33 83.62 82.82 78.48 83.05 86.87 83.96 88.37 72.08 85.21 86.43 61.82 54.07 78.94 78.89 85.85 82.68 86.95 87.6 86.37 82.72 72.11 83.66 86.26 85.43 88. 67.03 83.93 87.21 57.82 73.82 47.5 47.03 50.63 53.66 51.91 51.53 50.07 46.73 44.07 43.11 58.8 68.7 67.77 56.84 59.79 53.3 28.76 30.68 56.99 56.84 61.1 59.39 62.59 60.55 58.46 58.52 50.47 55.51 71.19 75.63 78.04 56.58 69.59 62.58 35.8 48.86 80.88 81.29 85.99 87.6 87.52 88.69 87.06 81.39 77.11 81.66 86.34 89.28 92. 78.34 87.59 88.76 63.23 65.81 80.53 79.72 87.44 84.07 88.08 88.82 87.81 85.31 73.65 82.6 87.79 91.22 92.86 73 86.77 86.47 62.68 76.6 57.64 56.36 58.53 65.56 61.64 63.21 61.72 51.63 54.11 61.65 64.3 74.77 77.67 54.04 59.19 61.73 25.98 30.23 63.87 63.02 70.93 64.71 67.32 67.65 65.05 59.56 58.22 66.07 73.51 78.8 79. 50.39 71.15 68.14 25.51 48.73 78.92 78.8 82.31 84.23 82.32 83.79 83.09 76.62 72.64 81.27 86.38 85.69 88.25 65.18 80.56 83.51 44 67.89 76.98 77.06 83.55 79.95 83.13 84.27 82.98 79.66 68.92 80.34 84.35 86.54 88.38 58.99 81.49 83.72 40.11 75.8 The results, presented in Table 7, provide strong quantitative evidence supporting our initial analysis. While lexical feature variations cause significant performance drops for existing text-to-image generative models, grammatical variations do not incur significant performance drops. This clear distinction validates our decision to focus on the more impactful lexical variations throughout this work. 21 Table 11: Complete DialectGen Benchmark Performance Breakdown by dialect for all textto-image and text-to-video generative models. All performance scores are measured using CLIPScore (Hessel et al., 2021), higher score is better. Results complements Table 2 in the main paper. Model d 2 Stable Diffusion 1.4 Stable Diffusion 1.5 Stable Diffusion 2.1 Stable Diffusion XL Stable Diffusion 3 Stable Diffusion 3.5 Large Stable Diffusion 3.5 Large Turbo Flux.1 [dev] DALL-E Mini DALL-E 2 DALL-E 3 DALL-E 3 w/ Rewrite gpt-images Cosmos-1 Open-Sora VideoCrafter-2 CogVideoX 2 Wan 2.1 e I 2 Stable Diffusion 1.4 Stable Diffusion 1.5 Stable Diffusion 2.1 Stable Diffusion XL Stable Diffusion 3 Stable Diffusion 3.5 Large Stable Diffusion 3.5 Large Turbo Flux.1 [dev] DALL-E Mini DALL-E 2 DALL-E 3 DALL-E 3 w/ Rewrite gpt-image-1 Cosmos-1 Open-Sora VideoCrafter-2 CogVideoX 2 Wan 2.1 p P c C p P i D Performance by Dialect (CLIPScore) AAE BrE ChE InE SgE Dialect SAE Dialect SAE Dialect SAE Dialect SAE Dialect SAE 25.46 25.79 25.72 25.81 25.45 25.62 25.1 24.74 24.77 24.57 25.19 24.92 25. 23.49 25.02 25.88 22.62 22.37 27.98 28.08 28.57 28.46 28.69 28.86 28.61 27.97 27.26 27.66 27.79 27.71 28.65 23.07 27.4 28.4 21.42 25.85 27.83 27.95 28.74 28.69 28.42 28.78 28.4 27.54 28.15 27.4 27.51 26.91 28.33 25.42 27.3 28.83 25.71 25.49 28.84 28.99 29.9 29.68 29.82 30.01 29.58 28.69 29.18 29.02 28.3 28.23 29. 23.79 28.36 29.76 22.55 27.89 28.65 28.66 29.67 29.97 29.89 30.25 29.9 28.52 29.48 29.86 28.95 29.41 30.94 26.17 28.63 29.41 24.14 25.45 29.59 29.54 30.83 30.49 30.76 31.02 30.67 29.54 29.84 30.5 29.55 29.75 31.2 25.98 29.5 30.24 24.38 27.96 29.79 29.91 30.88 31.21 30.97 31.5 31.16 29.88 30.65 30.56 29.75 30.11 31. 27.16 29.73 30.69 25.84 28.27 30.23 30.29 31.69 31.33 31.59 31.84 31.6 30.37 30.56 31.3 30.21 30.46 31.6 26.67 29.93 30.98 25.74 29.2 24.68 24.7 24.7 25.37 25.01 25.22 24.95 24.21 23.57 23.98 24.57 25.15 26.51 22.89 24.34 25.04 22.03 22.14 28.61 28.52 29.68 29.12 29.13 29.48 29.09 28.17 27.75 28.57 28.52 28.88 29. 24.19 28.07 28.95 22.89 26.05 28.34 28.32 29.44 29.57 28.74 29.42 28.9 27.97 27.81 27.3 28.11 27.57 29.48 24.62 27.09 29.04 24.61 24.55 30.1 30.18 31.51 31.14 31.15 31.64 31.23 30.17 30.23 30.54 30.04 29.85 31.29 24.94 29.46 31 24.6 28.83 24.34 24.38 25.31 25.85 25.02 25.67 25.3 24.78 24.4 24.1 25.71 26.87 27. 24.18 25.35 25.88 22.95 22.55 26.79 26.87 28.24 27.95 27.82 28.04 27.8 27.15 26.71 26.48 27.48 28.57 29.41 23.35 27.64 27.83 22.37 25.25 29.38 29.44 30.69 31.15 30.31 31.14 30.78 29.48 29.56 29.3 29.66 29.93 31.21 27.04 29.36 30.56 27.4 27.57 29.83 29.94 31.47 30.96 31 31.61 31.18 30.01 29.93 30.17 29.83 29.98 30. 25.29 30.04 30.88 25.51 28.92 25.97 25.86 26.46 27.45 26.8 27.18 26.96 25.4 25.74 26.44 26.3 27.12 28.57 21.89 25.55 27 19.99 22.75 28.12 27.98 29.54 28.65 29.13 29.29 28.76 27.72 27.42 28.69 28.67 28.61 30.18 19.99 27.64 28.74 17.67 25.45 28.64 28.65 29.54 30.23 29.67 30.22 29.82 28.31 28.6 28.53 29.08 28.47 30. 22.91 28.01 29.69 22.18 26.85 29.78 29.82 31.32 30.52 31.04 31.19 30.78 29.46 29.59 29.88 30.03 29.42 31.27 21.09 29.19 30.61 19.82 27."
        },
        {
            "title": "H PERFORMANCE BY DIALECT",
            "content": "Due to space constraints, we report performance by dialect in Table 9, Table 10, and Table 11. As described in Section 4.1, the scoring functions are based on reference-free image-text alignment metrics, including VQAScore and CLIPScore. We denote the subset of DialectGen prompts corresponding to given dialect as P, which consists of multiple SAE Prompt / Dialect Prompt pairs = (ps, pd). For each individual text prompt ps or pd, we generate images under different random seeds for text-to-image generative models, or uniformly sample frames for text-to-video generative models. Accordingly, for each SAE Prompt / Dialect Prompt pair = (ps, pd) P, we compute its SAE and Dialect performance using Equation (1) and Equation (2), respectively. More concretely, SAE(p, G) in Equation (1) denotes the average VQAScore (as reported in Table 9 and Table 10) or CLIPScore (in Table 11) computed over the images generated from the SAE prompt ps. Similarly, Dialect(p, G) in Equation (2) is computed using the same evaluation pipeline, but with the corresponding dialect prompt pd from the same pair. Each value of SAE(p, G) and Dialect(p, G) is reported as SAE and Dialect, respectively, in the tables."
        },
        {
            "title": "I USE OF AI TOOLS",
            "content": "We employed large language models (LLMs), including OpenAIs GPT-5 and GPT-4o, as auxiliary tools to refine the manuscript and identify grammatical errors. All LLM-assisted content was critically reviewed, fact-checked, and revised by the authors to ensure scientific validity and originality. The authors retain full responsibility for all statements and conclusions presented in this work. Specifically, LLMs were used only to improve wording and clarity of expression."
        },
        {
            "title": "J FUTURE WORK",
            "content": "Our work highlights several promising directions for future research, which we encourage the community to explore. Investigating Cultural and Representational Biases It would be interesting for future works to explore and evaluate the significance of representational and skin tone shifts induced by dialect inputs. For instance, as noted in Figure 1, we observed that FLUX.1 [dev] (Black Forest Labs, 2024) image generations for the prompt man selling eggplant depict more upscale and decorated environments compared to generations for man selling brinjal. Furthermore, individuals depicted in the images for brinjal are darker-skinned. systematic study of these shifts would provide valuable insights into the inherent biases of large-scale multimodal models. Exploring Grammatical and Joint Dialect Variations While this work concentrated on lexical variations, we welcome future works in this line to carefully study the impacts of grammatical dialect variations and their joint effects with lexical variations. Such research could reveal more complex interactions and failure modes in the performance of multimodal generative models. Investigating Downstream Impacts of Dialectal Performance Gaps Many existing studies rely on the accurate semantic understanding and high-fidelity generation capabilities of multimodal text-to-image and text-to-video generative models Zhang et al. (2023); Wallace et al. (2024); Zhou et al. (2025). It would be interesting to investigate the downstream research impacts of dialectal performance gaps on these works as well as downstream societal impacts to dialect speaker user groups. Extending Evaluation to Multi-Lexeme Prompts Another related area for future work is the extension of our evaluation to settings where multiple dialect lexemes are used. This would test the models compositional understanding of dialectal language, and we encourage future works to explore such possibilities. However, it should be noted that creating high-quality, controlled data at scale for such experiments is non-trivial problem that needs to be addressed. Applying the Mitigation Strategy to Text-to-Video Models While our proposed mitigation strategy is designed to be broadly compatible with most multimodal models, it would be interesting to apply our method to text-to-video generative models. Our experiments were limited to text-to-image models due to resource constraints. Therefore, we encourage future researchers with the necessary computing resources to experiment in this domain, as it would serve as strong test of our methods generalizability."
        }
    ],
    "affiliations": [
        "University of California, Los Angeles"
    ]
}