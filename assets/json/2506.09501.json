{
    "paper_title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning",
    "authors": [
        "Jiayi Yuan",
        "Hao Li",
        "Xinheng Ding",
        "Wenya Xie",
        "Yu-Jhe Li",
        "Wentian Zhao",
        "Kun Wan",
        "Jing Shi",
        "Xia Hu",
        "Zirui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 1 0 5 9 0 . 6 0 5 2 : r Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning Jiayi Yuan1 Hao Li2 Xinheng Ding2 Wenya Xie2 Yu-Jhe Li3 Wentian Zhao3 Kun Wan3 Jing Shi3 Xia Hu1 Zirui Liu2 1Rice University 2University of Minnesota Twin Cities 3Adobe Inc. {jy101,xia.hu}@rice.edu {li003703,ding0499,xie00470,zrliu}@umn.edu {jhel,wezhao,kuwan,jingshi}@adobe.com"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precisionwhile critical for reproducibilityis often neglected in evaluation practices. Inspired by this, we develop lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are increasingly being deployed in everyday scenarios, powering applications from chatbots [2] to automated coding tools [23] and personalized healthcare agents [6]. As their impact grows, rigorous benchmarking and evaluation become critical to measure real progress and ensure reliability, safety, and fairness [7, 34]. There are two commonly used evaluation strategies. The first one is to use greedy decoding by setting the temperature to zero, which produces deterministic outputs, and reports the result from single run. The second uses random sampling with non-zero temperature and reports performance using the Pass@K metric [8], i.e., the probability that at least one of independent solution attempts will succeed. In this paper, we highlight commonly overlooked factor in both evaluation settings: numerical precision. First, for the greedy decoding setting, this factor undermines the assumption of determinEqual contribution Preprint. Under review. Figure 1: Left: Under BF16 precision and greedy decoding, the models output can vary significantly depending on factors such as GPU count, evaluation batch size, and GPU hardware version. Right: For example, changes in evaluation batch size alone can lead to noticeable differences in responses, which is often ignored and not standardized by evaluation benchmarks. ismeven with the same prompt and random seed, the generated output can still differ significantly between different hardware and system configurations. Second, for the random sampling setting, the numerical error demands larger number of runs to control the variance. Surprisingly, we found that the results can be significantly different under the greedy decoding when changing the evaluation batch size, number of GPUs, or GPU versions. Through analysis, we found that the root cause of this problem is the non-associative property floating-point arithmetic, meaning (a + b) + = + (b + c) due to finite precision and rounding errors. This issue is particularly problematic for recent reasoningfocused models [13], which generate very long chains of thought. In such cases, small numerical errors accumulate during the token generation process, eventually leading to significant differences in output across different runs. As illustrated in Figure 1, the model produces significantly different outputs when the number of GPUs, evaluation batch size, or hardware versions changeeven if the same random seed and greedy decoding are used. This inconsistency makes it difficult to reproduce results, posing serious problem for measuring the progress. We highlight the significance of this issue for several critical reasons. First, many researchers report benchmark performance based on single inference run with fixed random seed to reduce computational cost. However, as shown in Figure 1. This practice can lead to misleading conclusions about model performance. Second, even when performing random sampling with multiple independent runs, the averaged results can still vary significantly due to hardware and system-level non-determinism. Moreover, when researchers report standard deviations without accounting for this numerical non-determinism, they risk severely overestimating models true uncertainty, since the reported variance reflects mixture of intrinsic model uncertainty and variance introduced by finite numerical precision. When results cannot be exactly reproduced, it becomes difficult to distinguish whether improvements are from better methods or merely random variation. To fill this gap, we conduct comprehensive analysis of how numerical precision and hardware configurations affect the reproducibility of LLMs. Our findings show that inference using the commonly adopted BF16 precision is highly sensitive to variations in hardware and system configurations, such as tensor parallel size, evaluation batch size, and GPU types. These hardware-related factors are often beyond users control and can vary significantly due to resource availability, yet they are often overlooked in current LLM evaluation methods. We observe that increasing the number of mantissa bits in the numerical formatsuch as using FP16 or FP32can significantly mitigate this issue. Based on these findings, we propose an optimized inference pipeline that performs all computations in FP32 while retaining model weights in BF16 precision. This approach effectively balances memory efficiency and reproducibility. Specifically, our contributions and suggestions can be summarized as follows: Extensive analysis of how numerical precision affects reproducibility in both greedy decoding and random sampling scenarios. Our finding suggests that due to the limited precision, the model performance is highly sensitive to variations in hardware and system configurations, such as tensor parallel size, evaluation batch size, and GPU types. Practical suggestions for reproducible reasoning to the community. Based on our findings, we suggest (1) If sufficient computational resources are available, please use random sampling (non-zero temperature) with multiple runs. Report the mean accuracy, average answer length, and 2 Figure 2: Floating-point format of FP32, FP16 and BF16. error bars. Note that with 16-bit precision and small datasets, more trials are needed for stable results in general. (2) If using greedy decoding with single run, please use FP32 precision to improve the reproducibility of your results. An optimized FP32 inference pipeline. We propose an optimized inference pipeline called LayerCast, which performs all computations in FP32 while retaining model weights in BF16 precision. This approach effectively balances memory efficiency and reproducibility. We release it as patch to vLLM and and can be used with just few lines of code change."
        },
        {
            "title": "2.1 Current Practices on LLM Reproducibility",
            "content": "There are two widely adopted evaluation strategies. The first one is greedy decoding, where temperature is set to zero and model always select the token with highest probability. The second strategy employs random sampling with non-zero temperature, and evaluates performance using the Pass@K metric [8]. Below we introduce the commonly adopted experimental setting for the reproducibility. Greedy decoding is deterministic text generation strategy where the model always selects the token with the highest probability at each step. In theory, this approach should produce identical outputs given the same input and model parameters. However, in this paper, we show that even greedy decoding can yield different results across runs due to numerical precision issues. Random sampling selects output tokens based on the models probability distribution with nonzero temperature to introduce randomness. The most commonly used evaluation metric is the mean accuracy across multiple independent trials, which is equivalent to Pass@1 [8]. Deterministic libraries and Random Seeds. Random seeds in LLM generation control the pseudorandom number generator that selects tokens when using non-greedy decoding. It is standard practice to fix random seeds; however, this is not always sufficient for ensuring full determinism in greedy decoding. Even with fixed seed, differences in computation order can alter the sequence of the pseudo-random number generator [15]. Meanwhile, frameworks like PyTorch and TensorFlow provide flags for deterministic behavior (e.g., torch.use_deterministic_algorithms(True) 2). These ensure that certain operations avoid algorithms that could produce different results across runs. However, in practice, it can still produce non-deterministic results even with these flags . Despite these efforts, achieving perfect reproducibility in LLM inference remains challenge. The next subsection delves into crucial factor that often leads to non-deterministic behavior even with greedy decoding and the aforementioned practices in place."
        },
        {
            "title": "2.2 Numerical Precision and Rounding Errors",
            "content": "Numerical precision is critical for reproducibility [24]. Higher precision numbers have less rounding error, which can reduce variability in results. Examples include using FP32 for certain parts of computationlike softmax or attention scoreseven if the model weights are in 16-bit precision. 2 https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html 3 Table 2: Two illustrative cases demonstrate how rounding error arising from the non-associativity of floating-point addition can affect numerical results. Example 1 reveals accumulation error at both precisions; Example 2 exhibits discrepancy only in BF16, while FP32 delivers identical results, illustrating higher-precision numeric types are more tolerant of rounding errors."
        },
        {
            "title": "Sum Order",
            "content": "FP32 BF16 a, b, = 0.1, 0.1, 0.2 a, b, = 0.0016, 0.0027, 1.0 + + 0011111001001101 + + 00111110010011001100110011001110 0011111001001110 + + 0011111110000001 + + 00111111100000001000110011100111 0011111110000000 This practice can mitigate run-to-run differences, but it doesnt provide theoretical guarantees since floating-point arithmetic cannot represent all real numbers exactly. Even with the same precision, different hardware implementations or computation orders can lead to slightly different results. In this paper, we primarily focus on the importance of numerical precision and provide detailed analysis of its impact on LLM inference reproducibility. Table 1: Rounding error when the same true value (1.00012) is represented in three different numeric formats. Precision Decimal Rounding Error FP32 FP16 BF 1.00012004375457761 +4.38e8 1.0 1.0 0.00012 0. Figure 2 shows the format of BF16, FP16 and FP32. Table 1 illustrates the rounding error that occurs when the same true value is represented in three different numeric formats. As expected, lower precision formats like FP16 and BF16 introduce larger rounding errors compared to FP32. Moreover, key aspect of floating-point arithmetic that contributes to non-determinism is its nonassociativity. This means that the order in which numbers are added can affect the final result due to accumulated rounding errors. Table 2 provides illustrative examples of how the order of summation can lead to different results in FP32 and BF16. This becomes particularly relevant in the parallel computations performed in GPUs during LLM inference. In the context of LLMs, even small numerical variations in the logit values can affect the final token selection when the top probabilities are close. Floating-point arithmetic in GPUs exhibits non-associativity, meaning (a + b) + = + (b + c) due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order. Formally, for reduction operation over set of values {v1, v2, ..., vn} (such as summing attention scores or computing softmax denominators), the result depends on the specific ordering of operations: (cid:77) i=1 vi = vπ(1) vπ(2) ... vπ(n), where π is permutation of indices {1, 2, ..., n} determined by thread scheduling."
        },
        {
            "title": "3.1 Experiment Setup",
            "content": "We conduct experiments on four recent LLMs, including two reasoning models: DeepSeek-R1-DistillQwen-7B, DeepSeek-R1-Distill-Llama-8B and two non-reasoning models: Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct [13, 32, 21], across five commonly used LLM evaluation benchmarks: AIME24, MATH500, LiveCodeBench-Easy, LiveCodeBench-Medium, and LiveCodeBench-Hard [1, 14, 17]. For reasoning models, we set the maximum output token length to 32,768 and for nonreasoning models, we set it to 2,048. We use vLLM [19] as the inference backend. In the random sampling experiments, we set temperature to 0.7 and top-p to 0.95. Our Evaluation implementation and prompt setting is adapted from SkyThought-evals [27], more details can be found in Appendix C. 4 For each model-task pair, we evaluate under 12 different runtime configurations, representing all combinations of 2 GPU types (NVIDIA L40S and A100), 2 GPU counts (2 and 4), and 3 batch sizes (8, 16, and 32), i.e., 2 2 3 = 12 different configurations, to simulate the diversity of deployment environments encountered in real-world evaluations. Unlike decoding parameters or random seeds, these hardware-related factors are often beyond users control and can vary significantly due to resource availability, yet they are often overlooked in current LLM evaluation methods. To comprehensively quantify output instability and better analyze the impact of numerical precision on inference variability, we evaluate the reproducibility under both greedy decoding and random sampling settings. Specifically, for greedy decoding, we analyze: Std@Acc (Standard deviation of accuracy): For each numerical precision, we evaluate accuracy under the 12 different runtime configurations and compute their sample standard deviations, which serve as indicators of the stability of LLM inference outputs during greedy inference. Avg_Std@Output_Length (Average standard deviation of output length): We measure the length of output tokens, compute the sample standard deviation per example across 12 runtime configurations, and report the mean of standard deviations over the entire dataset. This provides an alternative perspective on the stability of LLM inference outputs during greedy inference. Div_Index (Divergence index): For the same question, ixf two or more responses produce identical token sequences up to certain position, but generate different tokens after that position, we define the index of that position as the divergence index. higher Div_Index indicates greater consistency across responses under different runtime configurations. Avg_Std@top1_prob (Average standard deviation of top-1 token prediction probability): Before divergence, all responses across different runtime settings produce identical top-1 tokens at every position. However, due to floating-point computation errors, the predicted probabilities of these tokens may vary across settings. To quantify this, we compute the standard deviation of the predicted probability for the top-1 token at each position across settings, then average over all positions from 0 to Div_Index and over all examples in dataset. We define this metric as the Average Standard Deviation of Top-1 Token Prediction Probability, which serves as an indicator of the magnitude of numerical variation introduced by floating-point errors. For random sampling, we analyze: Pass@1: For each numerical precision, we evaluate performance by running the model multiple times and computing the average accuracy accuracy (commonly referred to as Pass@1). Specifically, we use both 16 and 64 independent runs to compute mean accuracy for AIME24, and 4 independent runs for MATH500. To assess the stability of random sampling-based decoding, we also report the standard deviation of Pass@1 across 6 runtime configurations, varying GPU count (2 vs. 4) and numerical precision (BF16, FP16, FP32)."
        },
        {
            "title": "3.2 Greedy Decoding ̸= Deterministic Output",
            "content": "Contrary to common belief, our experiments reveal that greedy decoding does not guarantee deterministic outputs across different hardware and system configurations. However, using FP32 precision helps lot. Table 3 presents the standard deviation of accuracy (Std@Acc) across 12 runtime configurations for BF16, FP16 and FP32 precisions. The results demonstrate clear pattern: FP32 consistently achieves near-perfect reproducibility with negligible variance, FP16 shows moderate variability, while BF16 exhibits substantial instability. This pattern is particularly pronounced for reasoning models like DeepSeek-R1-Distill-Qwen-7B, where BF16 precision introduces up to 9% standard deviation in accuracy on AIME24, compared to virtually zero variance under FP32. This Table 3: Std@Acc of greedy decoding across 12 different settings (GPU types, GPU counts, and batch sizes) under BF16, FP16, and FP32 Numerical Precisions. Reasoning models also exhibit larger variance compared to non-reasoning counterparts. More results can be found in Appendix E. AIME24 MATH500 LiveCodeBench-Easy BF FP16 FP32 BF16 FP16 FP32 BF FP16 FP32 DeepSeek-R1-Distill-Qwen-7B 9.15% DeepSeek-R1-Distill-Llama-8B 4.60% Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct 5.74% 6.00% 1.71% 1.45e-17 1.30% 1.92% 0 5.8e-17 1.45e-17 0 0.12% 1.04% 1.12% 1.59% 0.73% 0.23% 0.83% 0.36% 1.16e-16 0.13% 0.94% 0.34% 0.37% 1.67% 1.28% 2.31% 1.92% 0.29% 0.79% 0.48% 1.16e-16 0.25% 1.00% 0.67% 5 Table 4: Standard deviation of output length of greedy decoding across 12 different settings (GPU types, GPU counts, and batch sizes) under BF16, FP16, and FP32 numerical precisions. The output length of reasoning models exhibit large variance. More results can be found in Appendix E. AIME24 MATH500 LiveCodeBench-Easy BF16 FP16 FP32 BF16 FP16 FP BF16 FP16 FP32 DeepSeek-R1-Distill-Qwen-7B 9189.53 DeepSeek-R1-Distill-Llama-8B 9348.59 Qwen2.5-7B-Instruct 211.47 Llama-3.1-8B-Instruct 119.21 5990.32 7822.43 48.14 49.73 0 0 0 2774.28 4015.00 52.61 124.43 2090.46 2518.38 15.37 40.57 138.75 146.03 0 2.76 5507.52 4732.85 7.79 31.03 4282.78 3652.16 0.71 4.70 262.55 105.85 0 0. Answer 1 Answer"
        },
        {
            "title": "Token",
            "content": "Prob."
        },
        {
            "title": "Token",
            "content": "Prob. know have need ve ... 49.75% have 43.91% know 3.18% need 2.47% 0.49% ve ... ... 46.65% 46.64% 3.39% 2.63% 0.52% ... Figure 3: Left: the top-5 tokens and their predicted probabilities at the divergence index for two different answers to the same question in BF16. Right: The gap between the top-two competing tokens probability. We observe the token probability gap are often minimal in reasoning models. finding is concerning because it suggests that using different GPU types or varying the number of GPUs can prevent you from reproducing others results, even when using greedy decoding. Beyond accuracy variance, our results in Table 4 reveal that BF16 precision also causes response lengths to vary significantly across different settings. This raises significant concerns for recent work in efficient reasoning and long-to-short research [25], which needs to report response length metrics. Specifically, changing the system configuration can lead to difference of up to 9,000 tokens in response length. This variability undermines the effectiveness of many existing efficient reasoning approaches. However, our findings show that using FP16 helps reduce output length variance to some extent, while FP32 offers the most consistent and reliable control over this variability. Thus, we suggest if using greedy decoding, please use FP32 precision to improve the reproducibility of the efficient reasoning research. The performance gap clearly indicates that across different runs, the model generates different tokens. To investigate why this happens, we compared outputs from different runs. Since we are using greedy decoding, the top-1 probability token differs between runs at the point of divergence. Figure 3 shows an example of token logits when two runs diverge, illustrating how numerical precision errors can flip the order of the top-1 and top-2 token probabilities. Figure 3 also shows the histogram of probability differences between top-1 and top-2 tokens under FP32 precision. We observe that for reasoning models, the token probability differences between the top two competing tokens are often minimal. To further understand how this observation interacts with the numerical errors, Figure 4 further illustrates the average standard deviation of the probability of prediction of the top-1 token (Avg_Std@Top1_Prob) at different numerical precision levels. BF16 shows significantly higher variance in top-1 token probabilities compared to FP16 and FP32. This increased variance arises because BF16s limited mantissa bits (7 bits compared to FP16s 10 bits) introduce larger errors in probability computations, increasing the likelihood of token flips when these fluctuations overlap with the small gap between top-1 and top-2 candidate tokens. In contrast, FP32s higher precision (23 mantissa bits) makes runtime variations nearly negligible. Together, these results demonstrate that 6 Figure 4: Avg_Std@top1_prob across 12 different settings for 4 models and 5 tasks, under BF16, FP16 and FP32. FP16 shows significantly lower variance compared to BF16. FP32 yields near-zero variance, demonstrating strong robustness to floating-point rounding errors. token selection during greedy decoding is highly sensitive to even small numerical variations because of the minimal probability differences between competing tokens. The impact of numerical precision on output stability is further evidenced by divergence patterns in greedy decoding. Figure 5 shows the distribution of divergence points (Div_Index) across precision formats for DeepSeek-R1-Distill-Qwen7B on MATH500. As precision increases from BF16 to FP32, we observe both fewer divergent examples overall and significant shift in when divergence occurs. With FP32, almost all examples produce identical outputs across configurations, resulting in only 2.2% of the samples diverged. In contrast, with BF16, divergence frequently occurs early in generation despite the deterministic setting, with over 90% of examples showing some form of divergence. When divergence happens in higher precision formats, it typically occurs much later in the sequence, limiting its impact on the final output and answer accuracy. Figure 5: Distribution of Div_Index for DeepSeekR1-Distill-Qwen-7B on MATH500 under BF16, FP16, and FP32. Higher numerical precision leads to much fewer divergent examples and shift of divergence onset to later token positions. These results conclusively demonstrate that numerical precision is critical factor in achieving truly deterministic outputs with greedy decoding, with higher precision formats providing substantially better reproducibility."
        },
        {
            "title": "3.3 Random Sampling Has Reproducibility Issue, Too",
            "content": "One might argue that while greedy decoding is vulnerable to token-level instability from numerical precision, random sampling might be less sensitive due to its intrinsic stochasticity. However, our experiments reveal that numerical precision significantly affects the stability and reproducibility of sampling-based evaluations as well. When using random sampling with temperature > 0, researchers typically report the mean accuracy averaged over multiple runs (commonly referred to as Pass@1). We conduct Pass@1 evaluations across two benchmarksAIME24 and MATH500using 16 and 64 independent sampling runs for AIME24 and 4 runs for MATH500. Here we emphasize that in Table 5, the reported standard deviation is calculated based on Pass@1 performance across 6 system configurations per model (batch sizes and GPU counts), not across repeated runs of the same configuration. Thus, the reported variance largely reflects the impact from limited numerical precision, not the inherent variance of models. As shown in Table 5, we observe clear trend: numerical precision introduces an additional source of variance beyond the intended randomness, and lower-precision formats such as BF16 tend to produce higher output variance. See Appendix for the complete Pass@1 results that support these variance statistics. 7 Table 5: Standard deviation of Pass@1 performance (%) under different GPU counts and precisions. We emphasize that the reported values reflect the variability of Pass@1 performance across 6 different system configurations (3 batch sizes 2 GPU counts), not across repeated runs of the same configuration. MATH500 (n=4) AIME24 (n=16) AIME24 (n=64) BF16 FP16 FP32 BF16 FP16 FP BF16 FP16 FP32 DeepSeek-R1-Distill-Qwen-7B 0.3158 DeepSeek-R1-Distill-Llama-8B 0.3602 Qwen2.5-7B-Instruct 0.4663 Llama-3.1-8B-Instruct 0.6020 0.1463 0.3371 0.1686 0.1725 0.1021 0.1211 0.0274 0. 1.7151 1.5124 0.7056 0.5992 0.8273 1.8792 0.2523 0.2282 1.1785 0.8606 0 0.7759 0.3749 0.8774 0.1784 0.4216 0.5391 0.8539 0.1382 0.2898 0.7377 0.5034 0 0. This pattern largely holds across the models we evaluate, especially on the MATH500 benchmark. notable exception arises in the AIME24 results for DeepSeek-R1-Distill-Qwen-7B, where at = 64, FP32 exhibits higher variance (0.7377) than BF16 (0.3749). We interpret this as result of dataset size and sampling dynamics rather than contradiction of the overall trend. With only 30 problems in AIME24, single answer can shift Pass@1 by about 3.33%, amplifying statistical noise. Moreover, BF16which exhibits the highest variance at = 16also shows the greatest improvement when increasing to = 64. This suggests that instability from reduced numerical precision can be mitigated with sufficient averaging, but remains dominant factor at typical sample sizes. These findings highlight numerical precision as critical factor in the reproducibility of samplingbased evaluations. Researchers using random sampling with BF16 may need substantially more runs to achieve the same statistical confidence as with higher-precision formats, representing computational overhead that is rarely acknowledged in current evaluation practices."
        },
        {
            "title": "3.4 Ablation: How Runtime Configurations Affect Reproducibility",
            "content": "After establishing that numerical precision plays crucial role in LLM reproducibility, we now investigate how specific runtime configurationsbatch size, number of GPUs, and GPU typeaffect output stability across different precision formats. We conduct experiments of greedy decoding setting on DeepSeek-R1-Distill-Qwen-7B, as shown in Figure 6. Our analysis of runtime configurations reveals three key factors affecting token probability variations. First, in Figure 6 (a), configurations with 4 GPUs tend to exhibit higher probability variation than those with 2 GPUs across 3 tested batch sizes (particularly in BF16 precision), potentially due to increased parallel computation introducing more varied floating-point operation orderings and consequently different rounding errors. Second, Figure 6 (b) suggests that smaller batch sizes counter-intuitively produce higher variance in token probabilities because they may require more sequential processing steps that accumulate rounding errors, while larger batches benefit from (a) (b) (c) Figure 6: Controlled experiments for Avg_Std@top1_prob under different runtime configurations: (a) 2GPU vs 4GPU (varying batch size); (b) BS=8 vs 16 vs 32 (varying A100 GPU Count); (c) L40S vs A100 (BF16, varying GPU Count). Larger GPU counts and smaller batch sizes tend to increase inference instability. 8 Figure 7: Workflow of LayerCast within one transformer block. Here we omit skip connections, position embeddings and activation functions. parallel computation within optimized CUDA kernels that limit error accumulation. Third, GPU architecture matters: Figure 6 (c) shows A100s generally exhibit slightly higher probability variance than L40S under identical configurations, likely due to differences in hardware-level floating-point implementations and memory hierarchies. All these effects are most pronounced under BF16 precision with its limited mantissa bits making it especially susceptible to rounding effects. For more results of runtime configurations and tested tasks, please refer to the Appendix F. In summary, our experiments reveal three critical insights about numerical precision and reproducibility in LLM inference. First, the fundamental cause of nondeterministic outputs is the small gap between competing logits, which makes token selection vulnerable to minute numerical fluctuations. Second, precision format critically impacts stability, with FP32 providing near-perfect determinism, FP16 offering moderate stability, and BF16 exhibiting significant variance despite being commonly used. Third, specific runtime configurationsparticularly GPU count, batch size, and GPU architecturefurther affect reproducibility, with these effects most pronounced in lower precision formats. These findings highlight the urgent need for standardized evaluation practices that account for numerical precision effects, especially as LLMs continue to be deployed in increasingly critical applications where reproducibility is essential."
        },
        {
            "title": "4 Near-Perfect Deterministic Reproduction: LayerCast",
            "content": "Given our findings on when and why reproducibility breaks, we now propose directions to improve reproducibility in LLM inference. The basic solution is using FP32 precision, as weve shown in previous sections. However, this approach incurs significant costs: it doubles the memory usage and inference time compared to BF16, making it impractical for many production environments. We propose more efficient solution: LayerCast, hybrid precision approach that balances computational stability with memory efficiency. LayerCast works by: (1) Loading the model parameters initially in FP32 precision; (2) Explicitly casting all linear layer weights and biases to BF16 for storage before inference; and (3) As inference runs, upcasting each weight back to FP32 just-in-time for matrix multiplication, one at time. Figure 8: Distribution of Div_Index for DeepSeek-R1-Distill-Qwen-7B on MATH500 under BF16, FP32, and LayerCast. As illustrated in Figure 7, this approach ensures all computations occur in full FP32 precision while storing weights in memory-efficient 16-bit formats. Thus, this model benefits from FP32s stability 9 during computation, while the memory footprint remains closer to that of 16-bit models. This provides determinism comparable to full FP32 inference but with substantially reduced memory requirements, particularly beneficial for the KV cache in long-context scenarios. Our experimental results strongly support this approach. When examining the standard deviation of accuracy across runs, Layer Cast achieves stability nearly identical to FP32, while BF16 shows much higher variability. As shown in Figure 8, the divergence index measurements further confirm that LayerCast produces consistent outputs across different batch sizes and GPU configurations, with divergence rates below 3.4%. From resource perspective, LayerCast offers substantial benefits over full FP32: memory usage is reduced by 34% (particularly important for KV cache in long-context scenarios). These improvements make LayerCast practical solution for applications requiring both deterministic outputs and reasonable performance. For full result, please refer to Appendix G."
        },
        {
            "title": "5 Related Works",
            "content": "Since the era of traditional deep learning, the reproducibility of models [26, 36, 31] have remained complex and challenging problems that have yet to be fully resolved. As large language models (LLMs) have risen to prominence, numerous empirical studies [20, 33, 3, 11, 18, 30, 22, 35] have shown that nondeterministic behavior during LLM inference is widely observed. Existing study [5] have found clear negative correlation between output length and inference consistency: As the length of generated text increases, output variation during inference also rises, which explains the phenomenon that reasoning models tend to exhibit greater inference uncertainty. Multiple factors can affect the reproducibility of LLM inference results, including but not limited to prompt formatting, decoding parameters (such as temperature and top-p thresholds), random seed settings, and hardware and software configurations [16, 10, 9]. Existing studies [22, 11, 3, 20, 4] have systematically analyzed the impact of decoding parameters (e.g., temperature, top-p) on the stability of LLM inference outputs. Hochlehnert et al. [15] points out that many reported improvements in LLM performance are, in fact, partially attributable to unfair comparisons and unreported sources of variance in the evaluation process. In practical applications, using FP32 (single-precision floating point) inference [28, 29] is often empirically believed to enhance the robustness of numerical computations. Feng et al. [12] investigates the impact of numerical precision on the mathematical reasoning ability of LLMs. However, there is currently lack of systematic and quantitative studies specifically analyzing the effects of different numerical formats (e.g., FP32, FP16, BF16) on the reproducibility of LLM inference."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we conducted comprehensive investigation into the reproducibility challenges in LLM inference caused by numerical precision issues. Our experiments across multiple models, tasks, and hardware configurations revealed that even under supposedly deterministic greedy decoding, outputs can vary significantly due to floating-point arithmetic non-associativity. We demonstrated that precision format critically impacts stability, with FP32 providing near-perfect determinism while BF16despite being widely usedexhibits substantial variance. To address these challenges without incurring the full overhead of FP32, we proposed LayerCast, practical solution that achieves FP32-level determinism while maintaining reasonable memory efficiency. Our findings highlight the importance of standardizing evaluation practices to account for numerical precision effects, especially as LLMs are increasingly deployed in critical applications where reproducibility is essential."
        },
        {
            "title": "References",
            "content": "[1] Aime 2024 dataset. https://huggingface.co/datasets/Maxwell-Jia/AIME_2024. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Chetan Arora, Ahnaf Ibn Sayeed, Sherlock Licorish, Fanyu Wang, and Christoph Treude. Optimizing large language model hyperparameters for code generation. arXiv preprint arXiv:2408.10577, 2024. [4] Berk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu, Rebecca J. Passonneau, Evan Radcliffe, Guru Rajan Rajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture, Zhe Wu, Lixinyu Xu, and Breck Baldwin. Non-determinism of \"deterministic\" llm settings, 2025. [5] Berk Atil, Alexa Chittams, Liseng Fu, Ferhan Ture, Lixinyu Xu, and Breck Baldwin. Llm stability: detailed analysis with some surprises. arXiv preprint arXiv:2408.04667, 2024. [6] Marco Cascella, Jonathan Montomoli, Valentina Bellini, and Elena Bignami. Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios. Journal of medical systems, 47(1):33, 2023. [7] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [9] Huggingface Docs. Reproducibility. https://huggingface.co/docs/diffusers/v0.14. 0/en/using-diffusers/reproducibility, 2025. Accessed: 2025-04-28. [10] PyTorch Docs. Reproducibility. https://pytorch.org/docs/stable/notes/ randomness.html, 2025. Accessed: 2025-04-28. [11] Benedetta Donato, Leonardo Mariani, Daniela Micucci, and Oliviero Riganelli. Studying the case of chatgpt. arXiv preprint how configurations impact code generation in llms: arXiv:2502.17450, 2025. [12] Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, and Liwei Wang. How numerical precision affects mathematical reasoning capabilities of llms. arXiv preprint arXiv:2410.13857, 2024. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [15] Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. [16] Ingonyama. and llms: ing solving-reproducibility-challenges-in-deep-learning-and-llms-our-journey, 2025. Accessed: 2025-04-28. journey. reproducibility challenges learnin https://www.ingonyama.com/post/ deep"
        },
        {
            "title": "Solving\nOur",
            "content": "[17] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 11 [18] Eugene Klishevich, Yegor Denisov-Blanch, Simon Obstbaum, Igor Ciobanu, and Michal Kosinski. Measuring determinism in large language models for software code review. arXiv preprint arXiv:2502.20747, 2025. [19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [20] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. [21] Meta. Introducing llama 3.1: Our most capable models to date. https://ai.meta.com/ blog/meta-llama-3-1/, 2024. Accessed: 2025-05-13. [22] Shuyin Ouyang, Jie Zhang, Mark Harman, and Meng Wang. An empirical study of the non-determinism of chatgpt in code generation. ACM Transactions on Software Engineering and Methodology, 34(2):128, 2025. [23] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. [24] Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Oscar Hernandez, Mark Coletti, and Ada Sedova. Impacts of floating-point non-associativity on reproducibility for hpc and deep learning applications, 2024. [25] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. [26] Cecilia Summers and Michael Dinneen. Nondeterminism and instability in neural network optimization. In International Conference on Machine Learning, pages 99139922. PMLR, 2021. [27] NovaSky Team. Sky-t1: Train your own o1 preview model within $450. novasky-ai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. https:// [28] vllm Issue 12343. Why are the vllm and hugging face transformers inference results inconhttps://github.com/vllm-project/vllm/issues/12343, 2025. Accessed: sistent? 2025-04-28. [29] vllm Issue 12699. Different logprobs for qwn2-vl when running on transformers and on vllm. https://github.com/vllm-project/vllm/issues/12699, 2025. Accessed: 2025-0428. [30] Julian Junyan Wang and Victor Xiaoqi Wang. Assessing consistency and reproducibility in the outputs of large language models: Evidence across diverse finance and accounting tasks. arXiv preprint arXiv:2503.16974, 2025. [31] Xiangzhe Xu, Hongyu Liu, Guanhong Tao, Zhou Xuan, and Xiangyu Zhang. Checkpointing and deterministic training for deep learning. In Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI, pages 6576, 2022. [32] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [33] Boyang Yu. Benchmarking large language model volatility. arXiv preprint arXiv:2311.15180, 2023. [34] Jiayi Yuan, Jiamu Zhang, Andrew Wen, and Xia Hu. The science of evaluating foundation models. arXiv preprint arXiv:2502.09670, 2025. 12 [35] Qiuhai Zeng, Claire Jin, Xinyue Wang, Yuhan Zheng, and Qunhua Li. An analystinspector framework for evaluating reproducibility of llms in data science. arXiv preprint arXiv:2502.16395, 2025. [36] Donglin Zhuang, Xingyao Zhang, Shuaiwen Song, and Sara Hooker. Randomness in neural network training: Characterizing the impact of tooling. Proceedings of Machine Learning and Systems, 4:316336, 2022."
        },
        {
            "title": "A Limitations",
            "content": "Despite the importance of our work in highlighting and addressing numerical precisions effect on reproducibility, there are limitations that are worth noting. First, our experiments focus on certain set of models and settings. Due to resource constraints, we do not conduct large-scale experiments on much larger models (>70B) or wide range of GPU architectures or accelerators; these settings might exhibit different levels of numerical behaviors. Additionally, our study primarily addresses numerical precision issues in transformer-based LLMs and may not fully generalize to other architectures or modalities."
        },
        {
            "title": "B Broader Impacts",
            "content": "This research has several societal impacts. First, by raising the discussion of reproducibility issues in LLM inference, our work promotes greater scientific rigor in AI research, calling for more reliable comparison of models and techniques. This is particularly important as LLMs are increasingly deployed in critical areas like healthcare, education, and public services, where consistency and reproducibility are essential. On the other hand, our findings reveal that true reproducibility may require additional computational resources, potentially exacerbating the already significant environmental effects of LLM development. We hope our work will encouage the AI community to establity better standards of evaluationultimatelly leading to more trustworthy and reliable AI systems."
        },
        {
            "title": "C Prompt Formats Used in Our Evaluation",
            "content": "Our evaluation implementation and prompt formats are adapted from SkyThought repository. AIME24 and MATH500 share common prompt format across all models. LiveCodeBench-Easy, LiveCodeBench-Medium, and LiveCodeBench-Hard also have the same prompt format across all models, which varies by the test type of the problems (stdin, functional). The ___PROBLEM_TEXT___ is the placeholder that will be replaced with specific problem texts from benchmarks during prompting. C.1 MATH Benchmarks: AIME24 and MATH500 DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B <begin_of_sentence><User>Return your final response within (cid:44) boxed{}. ___PROBLEM_TEXT___<Assistant><think>n Qwen2.5-7B-Instruct <im_start>systemnYou are Qwen, created by Alibaba Cloud. (cid:44) helpful assistant.<im_end>n<im_start>usernReturn your (cid:44) final response within boxed{}. (cid:44) <im_start>assistantn ___PROBLEM_TEXT___<im_end>n"
        },
        {
            "title": "You are",
            "content": "Llama-3.1-8B-Instruct <begin_of_text><start_header_id>system<end_header_id>nnCutting (cid:44) Knowledge Date: December 2023nToday Date: 26 Jul 2024nn (cid:44) <eot_id><start_header_id>user<end_header_id>nnReturn (cid:44) your final response within boxed{}. ___PROBLEM_TEXT___<eot_id> (cid:44) <start_header_id>assistant<end_header_id>nn 14 C.2 Code Generation Benchmarks: LiveCodeBench-Easy, LiveCodeBench-Medium, LiveCodeBench-Hard DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B For problems with \"stdin\" tests: <begin_of_sentence><User>Generate an executable Python function (cid:44) generated from the given prompt. The function should take stdin (cid:44) as input and print the output. Simply call the function after (cid:44) the definition. ___PROBLEM_TEXT___<Assistant><think>n For problems with \"functional\" tests: <begin_of_sentence><User>Generate an executable Python function (cid:44) generated from the given prompt. Return the function body without (cid:44) invoking it at the final solution. ___PROBLEM_TEXT___<Assistant> (cid:44) <think>n Qwen2.5-7B-Instruct For problems with \"stdin\" tests: <im_start>systemnYou are Qwen, created by Alibaba Cloud. (cid:44) helpful assistant.<im_end>n<im_start>usernGenerate an (cid:44) executable Python function generated from the given prompt. The (cid:44) function should take stdin as input and print the output. Simply (cid:44) call the function after the definition. (cid:44) <im_end>n<im_start>assistantn ___PROBLEM_TEXT___"
        },
        {
            "title": "You are",
            "content": "For problems with \"functional\" tests: <im_start>systemnYou are Qwen, created by Alibaba Cloud. (cid:44) helpful assistant.<im_end>n<im_start>usernGenerate an (cid:44) executable Python function generated from the given prompt. (cid:44) Return the function body without invoking it at the final (cid:44) solution. ___PROBLEM_TEXT___<im_end>n<im_start>assistantn"
        },
        {
            "title": "You are",
            "content": "Llama-3.1-8B-Instruct For problems with \"stdin\" tests: <begin_of_text><start_header_id>system<end_header_id>nnCutting (cid:44) Knowledge Date: December 2023nToday Date: (cid:44) <eot_id><start_header_id>user<end_header_id>nnGenerate an (cid:44) executable Python function generated from the given prompt. The (cid:44) function should take stdin as input and print the output. Simply (cid:44) call the function after the definition. (cid:44) <start_header_id>assistant<end_header_id>nn"
        },
        {
            "title": "26 Jul 2024\\n\\n",
            "content": "___PROBLEM_TEXT___<eot_id> For problems with \"functional\" tests: <begin_of_text><start_header_id>system<end_header_id>nnCutting (cid:44) Knowledge Date: December 2023nToday Date: (cid:44) <eot_id><start_header_id>user<end_header_id>nnGenerate an (cid:44) executable Python function generated from the given prompt. (cid:44) Return the function body without invoking it at the final (cid:44) solution. ___PROBLEM_TEXT___<eot_id><start_header_id>assistant (cid:44) <end_header_id>nn"
        },
        {
            "title": "26 Jul 2024\\n\\n",
            "content": "15 Figure 9: Accuracy varies significantly across different settings under BFloat16. Supplementary Results on Accuracy Variance under BFloat16 In Section 1, we demonstrated the variance of accuracy on BF16 with the same seed and greedy decoding but different batch sizes and number of GPUs in Figure 1. To provide comprehensive understanding of reproducibility challenges across different problem domains, Figure 9 extends this analysis to all evaluated datasets. The results reveal clear pattern: while accuracy variance under BF16 precision is relatively modest for MATH500 and LiveCodeBench-Easy, it becomes substantially more pronounced for the more challenging LiveCodeBench-Medium and LiveCodeBench-Hard benchmarks. This dataset-dependent variation suggests that the impact of numerical precision on reproducibility scales with problem complexity."
        },
        {
            "title": "E Supplementary Results on Greedy Decoding",
            "content": "Following up on Figure 5 and Table 3 from Section 3.2, we present comprehensive results across all experimental settings. We report three key metrics: Std@Acc, Div_Percent, and Average Div_Index. Table 6 presents the standard deviation of accuracy (Std@Acc) across 12 runtime configurations under BF16, FP16, and FP32 precisions for LiveCodeBench-Medium and LiveCodeBench-Hard datasets. Table 7 reports the average standard deviation of output lengths (Avg_Std@Output_Length) 16 Table 6: Std@Acc of greedy decoding across 12 different settings (GPU types, GPU counts, and batch sizes) under BF16, FP16, and FP32 numerical precisions on LiveCodeBench datasets LiveCodeBench-Medium LiveCodeBench-Hard BF16 FP16 FP32 BF16 FP16 FP DeepSeek-R1-Distill-Qwen-7B 2.28% 2.21% 0.44% 1.19% DeepSeek-R1-Distill-Llama-8B 2.08% 1.84% 0.78% 2.15% Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct 1.98% 1.90% 1.44% 0.24% 0.40% 1.45e-17 0 0.70% 0.48% 0.44% 0.65% 3.62e-18 0.42% 0.53% 1.45e-17 3.62e-18 Table 7: Avg_Std@Output_Length of greedy decoding across 12 different settings (GPU types, GPU counts, and batch sizes) under BF16, FP16, and FP32 numerical precisions on LiveCodeBench datasets LiveCodeBench-Medium LiveCodeBench-Hard BF16 FP16 FP32 BF16 FP16 FP DeepSeek-R1-Distill-Qwen-7B 8893.02 DeepSeek-R1-Distill-Llama-8B 8240.77 Qwen2.5-7B-Instruct 26.47 Llama-3.1-8B-Instruct 60.25 8026.63 7842.29 6.18 7.49 547.50 646.30 0 0.89 8036.21 7518.49 36.01 78.26 7476.98 6827.19 9.26 15.95 646.30 690.86 0 2. across the same 12 runtime configurations and precision settings for these two datasets. In Table 8, we measure the Div_Index of each example across 12 runtime configurationswithin the 0Div_Index range, the 12 responses corresponding to the same problem have exactly the same token at each positionand report the average Div_Index over the entire dataset. Table 9 shows the percentage of examples in each dataset exhibiting divergent outputs across the 12 runtime settings. These results consistently support our conclusion that during greedy decoding, rounding errors from floating-point operations cause serious reproducibility problems. As numerical precision increases from BF16 to FP32, the divergence phenomenon is substantially reduced, evidenced by lower Std@Acc, lower Div_Percent, and larger Div_Index values."
        },
        {
            "title": "F Supplementary Results on Ablation Study",
            "content": "Continuing the discussion from Section 3.4, we provide additional results that isolate specific runtime configurations to examine their individual effects on reproducibility. For greedy decoding experiments, we focus exclusively on BF16 and FP16 precisions, since FP32 precision effectively mitigates numerical-precision-related errors and thus provides limited insight into reproducibility challenges. Tables 10 and 11 discuss the effect of different numbers of GPUs since, intuitively, the effect has dependency on the GPU version, so we consider the situation on L40S and A100, separately. Table 10 reports the Avg_Std@top1_prob of LLM inference responses for the same question using 2 or 4 L40S GPUs under three different batch sizes (8, 16, and 32), while Table 11 presents the corresponding results using A100 GPUs. As shown in Table 10, increasing the number of GPUs from 2 to 4 generally leads to higher Avg_Std@top1_prob. This observation suggests that increasing the number of GPUs may introduce greater inference nondeterminism. However, this trend becomes less consistent in the A100 setting as shown in Table 11, where in many cases the 2 GPU results are even slightly higher than those of 4 GPUs. One reason behind it is that A100s do have higher instability in our experiments, which may have more influence beyond the number of GPUs (see Figure 6 (c)). Table 12 examines the effect of different batch sizes. We report the Avg_Std@top1_prob of LLM inference responses for identical questions under varying GPU counts and GPU types. The results show that larger batch sizes consistently lead to lower Avg_Std@top1_prob, indicating better 17 Table 8: Average Div_Index across 12 different settings under BF16, FP16, and FP32 numerical precisions. In the table, value of -1 indicates that no divergence occurred for any example in the dataset under the given setting. DeepSeek-R1-DistillQwen-7B DeepSeek-R1-DistillLlama-8B Qwen2.5-7BInstruct Llama-3.1-8BInstruct AIME24 MATH500 LCB-Easy LCB-Medium LCB-Hard BF16 FP16 FP32 BF16 FP16 FP32 BF16 FP16 FP32 BF16 FP16 FP BF16 FP16 FP32 45.67 430.13 -1 65.33 395.46 1825.79 35.11 292.99 1143.30 44.23 267.81 2291.83 43.85 216.16 5807. 67.37 430.47 13520 76.85 463.14 1855.17 47.43 327.53 1036.04 47.59 287.83 2585.83 31.57 329.38 3832.54 82.37 457.41 - 103.58 267.59 -1 46.98 87.53 -1 44.23 107.07 -1 43.64 136.89 -1 22.44 635.43 1976.40 38.07 284.89 1509. 53.99 133.80 108.86 54.33 172.73 125.00 54.95 221.76 352.78 Table 9: Div_Percent across 12 different settings (GPU types, GPU counts, and batch sizes) under BF16, FP16, and FP32 numerical precisions DeepSeek-R1-DistillQwen-7B DeepSeek-R1-DistillLlama-8B Qwen2.5-7BInstruct Llama-3.1-8BInstruct AIME24 MATH500 LCB-Easy LCB-Medium LCB-Hard BF16 FP16 FP32 BF16 FP16 FP32 BF16 FP16 FP32 BF16 FP16 FP32 BF16 FP16 FP 100% 100% 0 99.60% 86.00% 5.80% 100% 100% 10.99% 100% 100% 19.90% 100% 100% 24.39% 100% 100% 6.67% 100% 87.80% 6.00% 100% 97.25% 13.19% 100% 100% 23.30% 100% 100% 30.08% 100% 73.33% 6.67% 90.20% 37.60% 1.80% 72.53% 16.48% 0 89.32% 35.44% 0 95.12% 50.41% 0 53.33% 23.33% 16.67% 77.60% 36% 9.20% 96.15% 41.76% 3.85% 98.06% 48.06% 3.40% 100% 58.54% 7.32% reproducibility in model outputs. This finding aligns with our hypothesis that higher parallelism reduces error accumulation. Finally, we compare the effect of the two GPU versions we used (L40S and A100). Table 13 reveals that under the same experimental settings, the Avg_Std@top1_prob on the A100 GPU is consistently slightly higher than that on the L40S GPU. This conforms to the previous findings that, in our experiments, A100 exhibits slightly higher hardware-induced variability, which may contribute to less stable top-1 token predictions across different runtime configurations. Tables 14, 15, 16 and 17 present the Std@Acc for each ablation study. Unlike Avg_Std@top1_prob, its hard to observe clear trends or patterns from Std@Acc. We argue that Avg_Std@top1_prob Table 10: Instability (Avg_Std@top1_prob (104)) under different numbers of L40S GPUs DeepSeek-R1-DistillQwen-7B DeepSeek-R1-DistillLlama-8B Qwen2.5-7BInstruct Llama-3.1-8BInstruct 2GPU 4GPU 2GPU 4GPU 2GPU 4GPU 2GPU 4GPU AIME24 MATH LCB-Easy LCB-Medium LCB-Hard BF16 FP16 BF16 FP16 BF16 FP BF16 FP16 BF16 FP16 29.66 5.83 32.91 4.48 42.80 6.03 38.44 7. 38.63 6.62 24.52 5.19 32.67 4.72 46.82 6.55 44.52 7.68 48.74 7. 23.02 3.18 23.03 3.19 28.46 4.27 30.25 4.52 29.76 5.04 28.01 3. 27.33 3.55 34.80 4.68 39.24 5.00 36.10 5.58 29.40 2.94 26.51 3. 28.48 4.29 29.68 4.84 34.31 5.03 34.05 3.91 24.91 3.75 30.31 4. 34.56 4.74 29.60 4.85 22.12 2.89 35.92 4.80 28.05 3.62 27.62 3. 28.90 3.17 44.54 7.30 42.31 5.75 33.93 4.45 36.34 4.05 34.60 5. Table 11: Instability (Avg_Std@top1_prob (104)) under different numbers of A100 GPUs DeepSeek-R1-DistillQwen-7B DeepSeek-R1-DistillLlama-8B Qwen2.5-7BInstruct Llama-3.1-8BInstruct 2GPU 4GPU 2GPU 4GPU 2GPU 4GPU 2GPU 4GPU AIME24 MATH LCB-Easy LCB-Medium LCB-Hard BF16 FP16 BF16 FP16 BF16 FP BF16 FP16 BF16 FP16 32.91 7.23 34.59 5.29 48.20 7.55 44.86 8. 46.25 7.95 33.90 5.79 30.88 4.45 43.05 6.38 44.26 7.37 47.95 6. 31.23 4.57 31.69 4.40 36.76 6.14 42.58 6.53 41.58 6.63 25.86 3. 25.63 3.46 32.01 4.52 37.60 5.03 37.75 5.41 30.64 4.23 26.59 4. 28.82 4.75 31.02 5.12 36.88 5.11 41.65 3.75 21.88 3.08 34.64 4. 35.01 4.44 39.98 4.76 53.72 5.66 43.03 7.43 36.01 5.36 39.96 4. 41.82 5.29 35.56 5.02 44.33 6.58 35.56 4.26 33.40 4.07 33.43 5. Table 12: Instability (Avg_Std@top1_prob (104)) under different batch sizes DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct BS= BS=16 BS=32 BS=8 BS=16 BS=32 BS= BS=16 BS=32 BS=8 BS=16 BS=32 AIME MATH500 LCB-Easy LCB-Medium LCB-Hard BF16 FP BF16 FP16 BF16 FP16 BF16 FP16 BF16 FP16 42. 6.66 37.61 5.26 56.00 7.51 53.20 8.55 57.69 8. 37.96 6.17 36.91 5.23 58.99 7.81 51.56 8. 59.04 8.50 39.21 6.16 34.41 4.79 51.39 7.15 48. 8.11 54.60 7.74 30.81 4.40 32.34 4.28 41.93 5. 45.21 6.21 47.51 6.59 36.82 4.14 31.72 4. 41.56 5.90 46.54 6.25 47.89 6.54 34.41 4. 26.93 3.68 36.80 5.13 41.43 5.46 41.43 5.90 35. 4.03 31.16 3.87 39.14 5.11 41.11 5.98 49.21 5. 32.09 4.49 31.84 3.96 38.27 5.52 39.74 5. 47.05 6.01 37.75 3.91 29.22 3.57 36.21 4.70 39. 5.95 46.57 6.15 53.86 3.79 54.17 6.69 44.63 5. 47.32 4.92 46.12 5.54 58.55 4.96 50.32 5. 45.88 4.68 46.33 4.68 44.22 5.75 47.23 3. 47.86 6.14 39.60 4.52 43.92 4.52 39.27 5.30 serves as more informative metric for ablation studies, as it directly reflects the numerical instability introduced by rounding errori.e., fluctuations in the predicted probabilities of the same token. However, such fluctuations do not always lead to token flip, since flip only occurs when the variation at specific position exceeds the original top-1 and top-2 probability gap, which is inherently stochastic event. Moreover, even when token flip happensfor example, replacing the with thatit may not necessarily cause the evaluation outcome to change from correct to incorrect or vice versa, and thus may not affect the final accuracy. 19 Table 13: Instability (Avg_Std@top1_prob (104)) under different GPU versions DeepSeek-R1-DistillQwen-7B DeepSeek-R1-DistillLlama-8B Qwen2.5-7BInstruct Llama-3.1-8BInstruct AIME24 MATH500 LCB-Easy LCB-Medium LCB-Hard 42.43 7.22 39.12 5.50 58.36 7.96 54.76 8.81 60.11 8.38 BF16 FP BF16 FP16 BF16 FP16 BF16 FP16 BF16 FP16 L40S 37.74 6. 36.11 5.09 54.88 7.15 48.67 8.26 54.66 8.10 A100 33.89 4. 33.94 4.51 43.33 6.28 47.79 6.67 49.39 7.04 L40S 31.48 4. 29.32 3.87 36.87 5.32 43.59 5.65 42.40 6.18 A100 L40S L40S 39.08 4.69 30.96 4.22 37.45 5.23 38.53 5.54 47.11 5. 34.86 4.34 30.19 4.00 35.12 4.82 37.90 5.59 44.72 5.91 55.99 6. 53.43 6.86 44.84 5.45 46.17 5.08 45.59 5.59 42.36 6.01 47.06 5. 41.29 4.49 42.22 4.62 40.64 5.13 Table 14: Instability (Std@Acc) under different numbers of L40S GPUs DeepSeek-R1-DistillQwen-7B DeepSeek-R1-DistillLlama-8B Qwen2.5-7BInstruct Llama-3.1-8BInstruct AIME24 MATH500 LCB-Easy LCB-Medium LCB-Hard BF16 FP16 BF16 FP16 BF16 FP16 BF16 FP16 BF16 FP 2GPU 4GPU 3.33% 10.72% 13.47% 5.10% 1.10% 1.79% 2.54% 1.14% 2.92% 3.45% 0.94% 2.81% 1.11% 0.64% 1.14% 0.32% 3.03% 1.56% 1.69% 0.94% 2GPU 5.10% 8.39% 2.53% 0.90% 1.93% 1.10% 1.29% 1.71% 2.40% 2.15% 4GPU 1.92% 1.92% 2.16% 0.46% 0.64% 1.45% 3.12% 1.84% 0.47% 3.08% 2GPU 4GPU 2GPU 4GPU 1.7e-17 1.7e-17 1.92% 1.7e-17 3.33% 1.92% 1.92% 0.90% 0.40% 0.64% 0.32% 0.74% 0.28% 0 0 0.72% 1.70% 0.23% 0.46% 0.46% 0.23% 0.95% 0.83% 1.27% 0.46% 0.23% 0 1.40% 0.57% 0.84% 0.28% 0.84% 0.28% 0.47% 0.94% 0.47% 0 0 Table 15: Instability (Std@Acc) under different numbers of A100 GPUs DeepSeek-R1-DistillQwen-7B DeepSeek-R1-DistillLlama-8B Qwen2.5-7BInstruct Llama-3.1-8BInstruct AIME MATH500 LCB-Easy LCB-Medium LCB-Hard 2GPU 5.78% 5.09% 0.61% 1.44% 1.14% 2.22% 2.39% 2.57% 0.94% 1.88% BF16 FP16 BF16 FP BF16 FP16 BF16 FP16 BF16 FP16 4GPU 5.09% 1.92% 1.60% 0.61% 2.40% 1.14% 1.22% 2.24% 0.47% 0.47% 2GPU 3.85% 5.09% 0.70% 0.64% 2.08% 2.48% 1.84% 2.95% 1.63% 2.05% 20 4GPU 8.39% 6.94% 1.41% 1.11% 3.22% 1.26% 1.75% 1.28% 0.47% 0.82% 2GPU 4GPU 2GPU 4GPU 1.92% 1.7e-17 1.92% 1.7e-17 1.92% 1.92% 0 1.29% 0.35% 0.84% 0.55% 2.12% 0 0.47% 0 0.20% 0.31% 6.8e0.46% 1.00% 0.50% 0.84% 0.64% 1.22% 0.28% 0.47% 0 0.32% 1.14% 0.64% 0.84% 0.84% 0.49% 0.49% 0.28% 0 0 0.47% 0 Table 16: Instability (Std@Acc) under different batch sizes DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct BS=8 BS=16 BS=32 BS=8 BS= BS=32 BS=8 BS=16 BS=32 BS=8 BS= BS=32 AIME24 MATH500 LCB-Easy LCB-Medium LCB-Hard BF16 FP16 BF16 FP16 BF16 FP16 BF16 FP BF16 FP16 7.20% 7.94% 0.76% 0.77% 1.80% 1.38% 1.95% 2.22% 0.67% 2.24% 8.61% 5.00% 1.30% 0.82% 2.12% 1.34% 2.79% 3.23% 1.68% 2.39% 12.58% 4.19% 1.12% 1.30% 0.78% 1.00% 2.38% 0.25% 1.02% 1.67% 4.19% 7.39% 0.50% 0.35% 1.82% 1.45% 1.90% 2.64% 2.05% 2.53% 5.00% 6.38% 2.07% 0.97% 1.45% 2.70% 0.89% 0.79% 1.68% 2.14% 4.19% 3.19% 1.91% 0.50% 3.11% 0.95% 3.13% 0.69% 3.15% 1.41% 1.92% 1.67% 0 0 1.00% 0.43% 0.45% 0.69% 1.65% 0.28% 0.47% 0 0.69% 0.35% 1.30% 0.28% 0.83% 0.24% 0.41% 0 0 0 0.85% 0.33% 0.32% 0.53% 0.89% 0.24% 0.41% 0 0 1.67% 1.44% 0.35% 1.22% 0.94% 0.80% 0.63% 0.78% 0 2.72% 1.67% 0.44% 0.35% 0.52% 0.64% 0.73% 0.40% 0 1.92% 0 0.93% 0.16% 1.29% 0 0.46% 0.46% 0.78% 0 Table 17: Instability (Std@Acc) under different GPU versions DeepSeek-R1-DistillQwen-7B DeepSeek-R1-DistillLlama-8B Qwen2.5-7BInstruct Llama-3.1-8BInstruct AIME24 MATH500 LCB-Easy LCB-Medium LCB-Hard 5.44% 3.50% 1.14% 0.99% 1.69% 1.58% 1.76% 2.16% 6.65% 1.82% BF16 FP BF16 FP16 BF16 FP16 BF16 FP16 BF16 FP16 L40S 11.81% 7.72% 1.01% 1.21% 1.79% 0.96% 2.70% 2.41% 1.51% 2.14% A100 5.87% 5.96% 1.03% 0.83% 2.66% 1.76% 2.20% 2.19% 2.61% 1.52% L40S 3.44% 6.55% 2.11% 0.64% 1.42% 2.14% 2.16% 1.62% 1.58% 2.38% A100 L40S A100 L40S 1.82% 1.52e-17 1.72% 1.52e-17 1.72% 2.11% 1.72% 0.84% 0.33% 0.90% 0.67% 1.83% 0.35% 0.45% 0 0.83% 0.39% 0.75% 0.22% 1.04% 0.25% 0.33% 0 0.72% 1.17% 0.35% 0.34% 9.59% 1.08% 0.73% 0.45% 0.62% 0.67% 0.40% 0.57% 0.33% 0.80%"
        },
        {
            "title": "G Supplementary Results on LayerCast",
            "content": "We evaluate the LayerCast method proposed in Section 4 using the DeepSeek-R1-Distill-Qwen7B model across five benchmarks: AIME24, MATH500, LiveCodeBench-Easy, LiveCodeBenchMedium, and LiveCodeBench-Hard. These evaluations span different batch sizes (8, 16, 32), GPU counts (2 and 4), and numerical precision settings (BF16, FP32, and LayerCast). All LayerCast experiments follow the same experimental configuration as our main studies. Table 18 reports both the accuracy and its standard deviation across different GPU count and batch size configurations. To facilitate comparison, results under FP32 and BF16 are also included. As expected, BF16 exhibits the most instability, with significantly higher standard deviations. LayerCast matches FP32 in terms of stability (often with zero or near-zero std) while preserving memory efficiency. These results corroborate our main findings in Section 4, demonstrating that LayerCast delivers reproducibility on par with FP32 while operating within lower memory footprint."
        },
        {
            "title": "H Supplementary Results on Random Sampling",
            "content": "In this section, we show more results related to the discussion in Section 3.3. In the random sampling setting, we report the complete set of Pass@1 results in AIME24 and MATH500 to further evaluate reproducibility under different precision settings. These experiments are conducted across different batch sizes (8, 16, 32), and GPU counts (2 and 4). The standard deviations are calculated along numeric precision types (BF16, FP16, FP32). Tables 19, 21, 20, and 22 summarize the Pass@1 accuracies and their standard deviation on DeepSeekR1-Distill-Qwen-7B, DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, respectively. In most cases, FP32 yields the lowest standard deviations, reflecting greater stability across configurations. However, it is worth noticing that there are (4 out of 12) cases where FP16 21 results are more stable. This suggests that during random sampling, the two sources of randomness can interleave. We still urge researchers to sample more extensively to obtain more reproducible results. Table 18: Accuracy and standard deviation of DeepSeek-R1-Distill-Qwen-7B under different GPU counts, precisions (including LayerCast), and batch sizes across 5 benchmarks."
        },
        {
            "title": "Benchmark",
            "content": "AIME24 LayerCast FP32 BF16 MATH500 LayerCast FP32 BF16 LCB-Easy LayerCast FP32 BF LCB-Medium LayerCast FP32 BF16 LCB-Hard LayerCast FP32 BF16 2x A100 4x A100 Std@Acc BS= BS=16 BS=32 BS=8 BS=16 BS=32 0.4333 0.4333 0.4667 0.4333 0.4333 0.4667 0.4333 0.4333 0. 0.4333 0.4333 0.4333 0.4333 0.4333 0.5333 0.4333 0.4333 0.4667 0.8680 0.8680 0.8700 0.8680 0.8680 0.8620 0.8680 0.8680 0. 0.8660 0.8680 0.8540 0.8660 0.8660 0.8860 0.8660 0.8700 0.8700 0.7308 0.7363 0.7198 0.7308 0.7363 0.7418 0.7363 0.7363 0. 0.7363 0.7308 0.7418 0.7418 0.7363 0.6978 0.7418 0.7363 0.7363 0.3058 0.3058 0.2816 0.3010 0.3010 0.2767 0.2961 0.3058 0. 0.2961 0.3058 0.2961 0.3010 0.3058 0.2864 0.3058 0.3010 0.2718 0.0488 0.0488 0.0569 0.0488 0.0488 0.0732 0.0569 0.0488 0. 0.0569 0.0569 0.0650 0.0488 0.0569 0.0569 0.0569 0.0569 0.0650 0 0 0.0544 0.0011 0.0013 0.0114 0.0049 0.0022 0. 0.0043 0.0025 0.0176 0.0044 0.0044 0.0066 Table 19: Pass@1 accuracies (%) under 6 system configurations and the standard deviation across them for DeepSeek-R1-Distill-Qwen-7B. 2x A100 4x A100 BS=8 BS=16 BS=32 BS=8 BS=16 BS= 53.33 52.71 53.33 54.38 54.27 54.74 90.60 90.25 90.65 56.25 55.00 56.88 54.12 54.06 54.17 90.80 90.20 91. 55.21 52.92 55.42 53.39 55.52 55.10 90.80 90.20 90.40 53.13 53.33 56.67 54.74 54.38 54.90 90.75 90.45 90. 54.17 53.13 53.54 54.38 54.11 55.16 90.90 90.50 90.85 54.17 53.13 53.13 55.63 54.38 55.10 90.70 90.15 90."
        },
        {
            "title": "Stdev",
            "content": "1.1784 0.8273 1.7151 0.7377 0.5391 0.3749 0.1021 0.1463 0.3158 AIME24, n=16 FP32 FP16 BF16 AIME24, n= FP32 FP16 BF16 MATH500, n=4 FP32 FP16 BF16 22 Table 20: Pass@1 accuracies (%) under 6 system configurations and the standard deviation across them for DeepSeek-R1-Distill-Llama-8B. 2x 4x A100 BS=8 BS=16 BS=32 BS=8 BS=16 BS=32 43.33 43.96 46.46 43.65 43.49 43.13 83.85 84.55 84.50 43.75 45.63 45. 43.39 44.22 42.76 83.95 83.95 84.80 41.88 43.75 42.71 44.22 44.17 43.07 83.75 83.95 84.35 42.92 41.04 43. 44.48 42.87 41.41 83.65 84.15 84.60 43.96 40.83 43.54 43.33 42.60 41.98 83.95 84.25 85.20 42.08 42.08 43. 44.32 42.14 43.85 83.75 83.55 85."
        },
        {
            "title": "Stdev",
            "content": "0.8606 1.8792 1.5124 0.5034 0.8539 0.8774 0.1211 0.3371 0.3602 AIME24, n=16 FP32 FP16 BF16 AIME24, n= FP32 FP16 BF16 MATH500, n=4 FP32 FP16 BF16 Table 21: Pass@1 accuracies (%) under 6 system configurations and the standard deviation across them for Qwen2.5-7B-Instruct. 2x A100 4x BS=8 BS=16 BS=32 BS=8 BS=16 BS=32 AIME24, n=16 FP32 FP16 BF16 AIME24, n=64 FP32 FP16 BF16 MATH500, n= FP32 FP16 BF16 11.46 11.04 11.25 11.25 10.99 11.20 74.85 74.50 90.65 11.46 11.67 10.21 11.25 11.25 11. 74.85 74.90 91.15 11.46 11.25 9.38 11.25 11.20 11.04 74.80 75.00 90.95 11.46 11.67 11.04 11.25 11.41 11. 74.80 74.80 90.85 11.46 11.25 10.42 11.25 11.30 10.94 74.80 74.75 90.35 11.46 11.46 11.04 11.25 11.25 11. 74.85 74.80 90."
        },
        {
            "title": "Stdev",
            "content": "0 0.2523 0.7056 0 0.1382 0.1784 0.0274 0.1686 0.3158 Table 22: Pass@1 accuracies (%) under 6 system configurations and the standard deviation across them for Llama-3.1-8B-Instruct. 2x A100 4x BS=8 BS=16 BS=32 BS=8 BS=16 BS=32 5.00 5.00 3.96 4.01 3.91 4.01 5.42 5.00 4.38 4.01 4.06 3.70 3.75 4.58 4. 4.27 4.17 4.32 5.00 5.21 5.42 4.01 4.22 4.01 5.42 5.00 5.21 4.06 4.58 4.90 3.75 5.21 5. 4.27 4.64 3.91 37.00 37.10 36.25 36.50 36.60 36.95 36.30 37.00 36.65 36.80 36.90 35.35 36.30 36.95 35. 36.15 37.00 36."
        },
        {
            "title": "Stdev",
            "content": "0.7759 0.2282 0.5992 0.1296 0.2898 0.4216 0.3293 0.1725 0.6020 AIME24, n=16 FP32 FP16 BF16 AIME24, n= FP32 FP16 BF16 MATH500, n=4 FP32 FP16 BF"
        }
    ],
    "affiliations": [
        "Adobe Inc.",
        "Rice University",
        "University of Minnesota Twin Cities"
    ]
}