{
    "paper_title": "Learning to Discover at Test Time",
    "authors": [
        "Mert Yuksekgonul",
        "Daniel Koceja",
        "Xinhao Li",
        "Federico Bianchi",
        "Jed McCaleb",
        "Xiaolong Wang",
        "Jan Kautz",
        "Yejin Choi",
        "James Zou",
        "Carlos Guestrin",
        "Yu Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem."
        },
        {
            "title": "Start",
            "content": "1, Daniel Koceja 1, Xinhao Li 4, Federico Bianchi 5 Jed McCaleb3, Xiaolong Wang4, Jan Kautz2, Yejin Choi2, James Zou 1,5, Carlos Guestrin 1, Yu Sun 1,"
        },
        {
            "title": "Abstract",
            "content": "How can we use AI to discover new state of the art for scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős minimum overlap problem and an autocorrelation inequality; (ii) GPUMode kernel competition (up to 2 faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with cost of only few hundred dollars per problem."
        },
        {
            "title": "Mathematics",
            "content": "Kernel Eng. (TriMul) Algorithms (AtCoder)"
        },
        {
            "title": "Biology",
            "content": "Best Human Prev. Best AI TTT-Discover Erdős Min. Overlap () A100 () 4531 µs N/A 2198 µs 0.380927 [20] 0.380924 [49] 0.380876 H100 () Heuristic Contest 39 () Denoising () 1371 µs N/A 1161 µs 566, 997 [1] 558,026 [37] 567,"
        },
        {
            "title": "0.64\nN/A\n0.71",
            "content": "6 2 0 2 2 2 ] . [ 1 5 7 1 6 1 . 1 0 6 2 : r Figure 1. TTT-Discover continues to train an LLM on single problem at test time. πθi denotes the policy with the updates weights at test-time training step i. We plot the reward distribution at step 0, 9, 24, and 49 (final), recorded while test-time training for the GPUMode TriMul competition. We generate 512 solutions at each step. As training progresses, the LLM generates better solutions that ultimately surpass the prior art (best human). For comparison, we plot the reward distribution of best-of-N with the same total sampling budget. Core contributors. Joint advising. Correspondence to: merty@stanford.edu, yusun@cs.stanford.edu."
        },
        {
            "title": "1 Introduction",
            "content": "To solve hard problems, humans often need to try, fail, stumble upon partial successes, and then learn from their experiences. Consider your first really hard programming assignment. You read the textbook and trained yourself on the book exercises, but this assignment just asked for so much beyond the basics in the book. You tried to guess the solution, but these attempts merely produced small signs of life. So you had to take deep breath and learn from your failed attempts, which made your future attempts more intelligent. Finally, after hours of trying and learning, you understood the new ideas behind the assignment. And indeed, the next attempt worked! In this example, the assignment was hard because it required new ideas beyond your training data (the text and exercises in the book). Now consider using AI to solve scientific discovery problems. This goal is even harder: By definition, discovery problems require ideas not only beyond the models training data but also all existing knowledge of humanity. And out-of-distribution generalization is no easier for AI than for humans [47, 22, 52, 34]. To offset this hardness, prior work has focused on test-time search in the solution space by prompting frozen LLM to make many attempts, similar to how we tried to guess the solution to the assignment. In particular, evolutionary search methods, such as AlphaEvolve, store past attempts in buffer and use them to generate new prompts via hand-crafted and domain-specific heuristics [49, 37, 54, 80]. While these prompts can help the LLM improve previous solutions, the LLM itself cannot improve, similar to student who can never internalize the new ideas behind the assignment. The most direct way for the LLM to improve is through learning. And indeed, while both learning and search scale well with compute [66], learning has often superseded search in the history of AI for hard problems such as Go and protein folding [62, 30]. We believe that this observation from history is still relevant today, as we scale compute at test time. So we continue to train the LLM, while it attempts to solve this very test problem. And these attempts, in turn, provide the most valuable training data: Recall that the test problem was hard because it was out-of-distribution. Now we have data distribution specific to this problem. At high level, we simply perform Reinforcement Learning (RL) in an environment defined by the single test problem, so any technique in standard RL could be applied. However, our goal has two critical differences from that of standard RL. First, our policy only needs to solve this single problem rather than generalize to other problems. Second, we only need single best solution, and the policy is merely means towards this end. In contrast, the policy is the end in standard RL, whose goal is to maximize the average reward across all attempts. While the first difference is recurring theme in the field of test-time training [65], the second is unique to discovery problems. To take advantage of these differences, our learning objective and search subroutine strongly favor the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). We focus on problems with continuous rewards, in mathematics (4.1), GPU kernel engineering (4.2), algorithm design (4.3), and biology (4.4). We report results for every problem we attempted, and TTT-Discover sets the new state of the art in almost all of them, using only an open model. There are two pieces of concurrent work that share our high-level idea: MiGrATe (Phan et al.) [51], and more recently ThetaEvolve (Wang et al.) [74], which we find especially relevant. Compared to ThetaEvolve, TTT-Discover using the same model and compute budget still produces significant improvements  (Table 2)  , due to its special learning objective and search subroutine. Problem Erdős Minimum Overlap Autocorr. Inequality (1st) Autocorr. Inequality (2nd)"
        },
        {
            "title": "Transition",
            "content": "Reward R(s) = Python(Parse(a)) = Parse(a) 1/Upper bound 1/Upper bound Lower bound 1/Runtime Test score 1/MSE Table 1. Overview of the science and engineering problems in our paper, and the environments they induce (2.1). Note that the reward is 0 if fails validity checks."
        },
        {
            "title": "2 Preliminaries",
            "content": "All methods in this paper, including the baselines, share common goal: Given scientific problem at test time, the goal is to discover new state-of-the-art solution with an LLM policy πθ, whose weights θ have already been trained (at training time). To formalize this goal, we first introduce how each scientific problem defines an environment, i.e., Markov Decision Process (2.1), which can then be used for search (2.2) and learning (3)."
        },
        {
            "title": "2.1 Discovery Problem",
            "content": "Our definition of the environment follows prior work in test-time scaling, such as AlphaEvolve [49]: scientific problem comes in the form of text description d, which we always feed as context to the policy. We define state as candidate solution, such as kernel implementation of the PyTorch code in d. In our applications, the problem description also induces continuous reward function R(s) R, such as the inverse runtime of the kernel. We denote ssota as the best-known solution among all existing candidates, and rsota = R(ssota) as the best-known reward. And in case there is no existing solution, ssota can be the empty string <empty>. For example, ssota can be the kernel currently at the top of the leaderboard. These notations allow us to formalize the notion of discovery: Definition (Discovery). discovery is an event where state is found such that R(s) > rsota. The larger the difference, the more significant the discovery. Under this formalism, we define discovery problem as finding such state with large R(s) rsota within the environment defined by the scientific problem. To produce better solution, both search and learning methods use the LLM policy to generate an action πθ( d, s), where the choice of the initial solution (e.g., = ssota) is an important part of the of the methods design. Similar to the reward function, the transition function (s, a) environment is also induced by the problem description. Here, we consider only single timestep since state reuse, which we will introduce soon, effectively subsumes multiple timesteps. In all our applications, valid action contains piece of code and optionally some thinking tokens. by simply parsing the For coding problems (e.g., kernel engineering), the environment produces code out of a. For problems in mathematics, the environment also needs to execute the code in after it is parsed. Table 1 provides an overview of the environments for all our applications."
        },
        {
            "title": "2.2 Search Methods",
            "content": "The simplest search method, known as Best-of-N , samples i.i.d. rollouts from πθ: Best-of-N : = ssota or <empty>, ai πθ( d, s), where the subscript, = 1, . . . , , denotes the index of the rollout. By using instead of for the index, we indicate that the rollouts here are independent. One reasonable choice of the initial state is ssota, assuming that previous solution exists. But ssota might be too strong prior towards exploitation. For example, conditioning on ssota might prevent the policy from exploring very different, but more promising directions that would ultimately produce better solutions under large compute budget. To address this concern, we usually set = <empty>, the empty (or trivial) solution. On the other hand, the policy might also explore promising direction using = <empty>, but fail to fully exploit it. One technique to address this opposite concern is state reuse, which warm starts the policy with some of the previous solutions. Specifically, it maintains buffer of the previous solutions, and samples the initial solution si from using search heuristic, reuse, which favors high-reward solutions but still assigns nontrivial likelihood to low-reward ones: si i), ai State reuse: reuse(H"
        },
        {
            "title": "When we reuse a previous solution s",
            "content": "i+1 = i, we have effectively added an extra timestep to its trajectory. Prior work, such as AlphaEvolve [49], also reuses the actions, which can contain thinking tokens and intermediate results (e.g., code for math problems) that are not part of the states. As consequence, the reuse heuristic also needs to convert the information from previous actions into natural language context ci that can be ingested by the LLM policy: πθ( d, si), {(s i, ri)}. State-action reuse: si, ci reuse(H i), ai πθ( d, si, ci), i+1 = {(si, ai, i, ri)}. Prior work [49, 37, 80, 41] refers to state-action reuse as evolutionary search, because the reuse heuristic usually involves sophisticated designs motivated by evolution, including hand-crafted operations for mutation and cross-over, and domain-specific measurements of fitness and diversity."
        },
        {
            "title": "3 Learning to Discover at Test Time",
            "content": "So far, the policys experience with the test problem can only improve the next prompt (d, si, ci), but not the policy πθ itself, since θ remains frozen. We use this experience to improve the policy in an online fashion, by training πθ on its own search attempts accumulated in the buffer Algorithm 1 outlines the general form of our method, where the two key subroutines to instantiate are reuse and train. i."
        },
        {
            "title": "3.1 Naive RL at Test Time",
            "content": "Algorithm 1 falls under the formulation of reinforcement learning (RL). natural baseline is to use standard RL algorithm: train : θi+1 = θi + η θ aπθi (s)[R(s, a)] , reuse(H i) = δ<empty>, i.e., optimize for expected reward with no reuse, where δ<empty> is delta distribution with mass only on the initial state <empty>. We will use θi to denote the model weights for rollout i. We can straightforwardly apply popular RL algorithms, such as PPO or GRPO [56, 16], only in the environment defined by the single problem. 4 Algorithm 1 Test-Time Training to Discover (TTT-Discover) 1: Input: problem description and policy πθ0 with initial weights θ0. 2: R, = get_env(d) 3: 4: for = 0, 1, . . . , 1 do 5: 0 = {(<empty>, R(<empty>), {})} induces the reward and transition functions of the environment (2.1) Initialize buffer with the empty solution (2.2) reuse(H i) si, ci πθi ( d, si, ci) ai = (ai) i) ri = R(s i+1 = 6: 7: 8: 9: 10: 11: end for 12: return si , where {(si, ai, θi+1 = train(θi, (d, si, ci, ai, ri)) i, ri)} Sample initial state and context with reuse heuristic Sample action from policy Transition to next state Evaluate reward of next state Add current attempt to buffer Improve the model weights with train = arg maxi=0,1,...,N 1 ri Return the state with the highest reward However, these algorithms are designed with the standard RL problem in mind. Discovery problems have important distinctions from standard RL problems. In standard RL problems, the goal is to find policy that maximizes the expected reward. This policy is to be deployed repeatedly in the same environment. The primary artifact is the policy. In discovery problems, the goal is to find single state that improves upon the state-of-the-art. We do not care about the average performance. There is no separate deployment phase and thus the policy need not maintain robust performance in many states it may encounter starting from the same initial state distribution. In fact, policy can have very low expected reward, so long as it reaches new state-of-the-art once. Due to these differences, the naive RL instantiation has important shortcomings. Objective function. Naive RL optimizes average performance, and is indifferent to the state of the art. In discovery, however, success is determined by the maximum, and whether it improves upon the state of the art. Consider kernel engineering problem where the state-of-the-art runtime is 2000 µs. Achieving 1900 µs would require substantial optimization and perhaps breakthrough. Yet, without complicated reward shaping, both would receive nearly the same reward. Short effective horizon. Starting each attempt from scratch limits how far the policy can reach in an attempt. Reusing previous solution effectively adds extra timesteps to an attempt, extending the horizon. As result, more complex solutions can emerge during training. In standard RL, fixed initial state distribution makes sense as the policy must perform robustly from states it will encounter at deployment. Discovery has no such deployment phase. Exploration. Exploration requires care at two levels. Optimizing for expected reward, the policy can collapse to safe, high-reward actions rather than risky ones that might achieve discovery. At the reuse level, naive prioritization can over-exploit few promising states at the expense of diversity."
        },
        {
            "title": "3.2 TTT-Discover",
            "content": "To address these shortcomings, we introduce two simple components. Entropic objective. We define the entropic objective that favors the maximum reward actions: Jβ(θ) = sreuse(H) (cid:104) log aπθ(s) eβ(s)R(s,a)(cid:105)(cid:105) (cid:104) , 5 , (cid:105) wβ(s)(a) = θ log πθ(a s) (cid:104) wβ(s)(a) θJβ(θ) = Esreuse(H) aπθ(s) eβ(s)R(s,a) πθ(s)[eβ(s)R(s,a)] where we also shape advantages with KL penalty: A(a; s) = wβ(s)(a) 1 λ log πθ(as) πθ0 (as) [56, 83, 68], and 1 is the baseline since E[wβ(s)] = 1. Concurrent work [29] also explored the entropic objective Jβ to maximize the pass@k performance for (training-time) RL with binary reward problems. As β , the entropic objective tends to the max, which is intuitively what we want. However, too large β early in training causes instabilities, while too small later makes advantages vanish as even smaller improvements become harder. Empirically, we found that setting constant β that works well across different tasks is challenging. Therefore, different than [29], we set β(s) adaptively per initial state by constraining the KL divergence of the induced policy; see Appendix A.1 for details. , PUCT. We select initial states using PUCT-inspired rule [53, 60, 62, 61]. Each state is scored by Q(s) + (s) 1 + /(1 + n(s)), where Q(s) is the maximum reward among states generated when the initial state was (or R(s) if has not yet been selected). (s) is proportional to ss rank in the buffer sorted by reward, n(s) counts how many times or its descendants have been expanded, and is the total number of expansions, and is the exploration coefficient. Rather than the mean (as in prior work), we use the maximum reward of children in Q(s): we care about the best outcome starting from state, not the average. The prior (s) captures the intuition that high-reward states are more likely to yield high-reward childrene.g., fast kernel is more likely to seed faster kernel than slow onewhile the exploration bonus prevents over-exploitation by keeping under-visited states as candidates. See Appendix A.2 for implementation details. Test-time Training to Discover. With these building blocks, we can introduce our method, TTTDiscover. We combine Jβ(s) as our (test-time) training objective and PUCT as our reuse routine: train : θi+1 = θi + η θJβ(si )(θi), reuse : si PUCT(H i)."
        },
        {
            "title": "3.3 Implementation Details",
            "content": "We run TTT-Discover with gpt-oss-120b [2] on Tinker [36] for 50 training steps. We use LoRA [23] with rank 32. At each step, we generate batch of 512 rollouts, with 8 groups of 64 rollouts each. Each group of rollouts is generated using the same context and initial state selected from the reuse buffer. We use the entropic objective, and apply importance sampling ratio correction to the gradients due to the sampler/learner mismatch in the RL infrastructure [78]. We do not take any off-policy steps, i.e., take 1 gradient step on the entire batch. We set the reasoning effort to high. The context window of gpt-oss-120b is limited to 32, 768 tokens on Tinker. Thus, each rollout stops when the context window is exhausted or the LM produces the end of sequence token. In most domains, we limit the total length of the prompt and the thinking tokens to 26000 tokens, so as to leave enough tokens to generate the final response, e.g., to allow generating longer algorithm code. We enforce this by token forcing the model to generate its final response. All hyperparameters reported in Table 9, and are fixed unless otherwise stated. Assuming an average prompt length of 3000 tokens and 16000 sampling tokens on average, training run with 50 steps and 512 rollouts costs around $500 on Tinker."
        },
        {
            "title": "4 Applications",
            "content": "We evaluate TTT-Discover on problems in GPU kernel engineering, mathematics, algorithm design, and biology. We report our performance on every task we attempted. Besides potential impact, 6 we pick domains with 2 criteria. First, we pick domains where we can compare our performance to human experts. This is possible, for example, by comparing to the best submissions in human engineering competitions, or to the best results reported in academic papers. We also want to compare to AI baselines. As we discuss below, mathematics and algorithm design are discovery domains where prior work recently made progress [49, 14, 27, 54, 74]. In every application, we report the best known human results and the best known AI results. Importantly, we always report the Best-of-N baseline that matches the sampling budget and the model that TTT-Discover uses. That is, since we perform 50 steps with 512 rollouts per step, and compare to the Best-of-25600 baseline. For closest evolutionary algorithm baseline, we also run OpenEvolve [58], an open-source version of AlphaEvolve [49], with the same 25600 sampling budget. We use the same context window budget and the Tinker client for gpt-oss-120b throughout the experiments. We caution that the context window limit led to large number of rollouts in OpenEvolve to be truncated before the model completes its response, as OpenEvolves prompts grow very large in length. However, to stay faithful to their implementation, we did not modify their prompts or rollouts."
        },
        {
            "title": "4.1 Mathematics",
            "content": "We explore multiple open problems in mathematics. These are often problems where even small numerical improvements carry real weight, since each result potentially rules out families of approaches and extends the frontier of what is mathematically known. Here, proofs are by construction: one can construct concrete mathematical object step function or sequence that certifies, e.g., bound for an inequality can be achieved. This property makes these problems amenable to search. , or zero if Environment: The state is construction. Specifically, construction is step function represented as numerical array, to certify proof. The action consists of thinking tokens followed by Python code that either constructs new step function or modifies an existing one. The dynamics execute = Python(Parse(a)). The reward is the bound certified the parsed code to produce the next state: fails validity checks (e.g., the function must satisfy constraints on its support, sign, by or integral). Most often, actions involve optimization algorithms to improve the constructions. Throughout mathematics applications, we initialize the buffer with random states. Specifically, initial states are sampled uniformly at random within the problems valid range. For each action, we give 10-minute limit to execute the code given by the action. In the case of timeout, the action gets reward of 0. For minimization problems (certifying upper bounds), we set the reward proportional to 1/bound for the certified bound, and otherwise we set it proportional to bound. We report further details about the environment and the prompts we use in Appendix B. Previous state-of-the-art. Such problems are recently explored in [14, 49]. We report both the best known human results, and the recent progress by AI: AlphaEvolve [49], AlphaEvolve V2 [14] which was released around 6 months after AlphaEvolve, ShinkaEvolve [37], and ThetaEvolve [74]. We select one representative problem from each area in AlphaEvolve [49]: Erdős minimum overlap problem (combinatorics), autocorrelation inequalities (analysis), circle packing (geometry)."
        },
        {
            "title": "4.1.1 Erdős’ Minimum Overlap Problem\nThis is a classic problem in combinatorial number theory, posed by Erdős in 1955, with connections\nto the distribution of sequences and difference sets. Partition {1, 2, . . . , 2n} into two sets A and B of\n∈ B, and let\nequal cardinality n. Define Mk as the number of solutions to ai\nM(n) = minA,B maxk Mk over all partitions. The problem is to bound c = limn→∞ M(n)/n. Bounds\nbefore AlphaEvolve were 0.379005 < c < 0.380927, with the upper bound due to Haugland [20] and\nthe lower bound due to [76]. AlphaEvolve [49, 14] improved the upper bound to 0.380924.",
            "content": "bj = for ai A, bj"
        },
        {
            "title": "Method",
            "content": "best human"
        },
        {
            "title": "Model",
            "content": "Gemini-2.0 Pro + Flash AlphaEvolve [49] Gemini-2.0 Pro + Flash AlphaEvolve V2 [14] ThetaEvolve [74] R1-Qwen3-8B ThetaEvolve w/ SOTA reuse (1.50317) R1-Qwen3-8B OpenEvolve [58] Best-of-25600 gpt-oss-120b gpt-oss-120b TTT-Discover TTT-Discover Qwen3-8B gpt-oss-120b Erdős () AC1 () AC2 () 0.9015 1.50973 0. 0.380924 0.380924 n/a n/a 0.380965 0.380906 0.380932 0.380876 1.50530 1.50317 1.50681 1.50314 1.50719 1.51004 1.50525 1.50287 0.8962 0.9610 0.9468 n/a 0.9449 0.9344 0.9472 0. Table 2. Results in mathematics problems. In the Erdős Minimum Overlap Problem and First Autocorrelation Inequality (AC1), TTT-Discover sets the new state-of-the-art. We also report TTT-Discover with Qwen3-8B, for better comparison to ThetaEvolve. Notable, TTT-Discover with Qwen3-8B outperforms not only ThetaEvolve, baselines including AlphaEvolve which uses Gemini-2.0 family models for the autocorrelation inequalities. Our state-of-the-art constructions are released and can be validated in our codebase. Following [49], we optimize step functions describing the density of throughout [1, 2n]. Due to result of Swinnerton-Dyer [20], density functions yield valid upper bounds on lim M(n)/n without constructing explicit partitions for large n. Validity checks require (x) [0, 1] and = 1. (cid:82) Figure 2. We show the normalized step functions including the prior state-of-the-art from AlphaEvolve. The step function (x) is the limiting density of set A. Unlike the previous state-of-the-art, the solution from TTT-Discover is an asymmetric construction. TTT-Discover found 600-piece step function, while AlphaEvolve construction was 95-piece. The best human result was 51-piece construction [20]. Results. We improve the upper bound on Erdős Minimum Overlap Problem to 0.380876, surpassing AlphaEvolves recent construction with 0.380924 [49]. Our improvement over AlphaEvolve is 16 times larger than AlphaEvolves improvement over the previous state-of-the-art. Unlike AlphaEvolves symmetric construction, our method discovered 600-piece asymmetric step function. Surprisingly, the Best-of-25600 baseline also improved upon the AlphaEvolve construction. The discovered algorithm minimizes the correlation bound using FFT-accelerated gradient descent combined with random hill climbing and simulated annealing. The code maintains feasibility by projecting onto the constraint set where (x) [0, 1] with with = 1. Interestingly, the solution found by TTT-Discover is asymmetric. (cid:82)"
        },
        {
            "title": "4.1.2 Autocorrelation Inequalities",
            "content": "Autocorrelation inequalities are motivated by additive combinatorics [6]. Improving these inequalities tightens constant that propagates into sharper limits on how large set can be while still avoiding repeated additive patterns (a central theme in additive combinatorics). Similar to the Erdős minimum overlap problem, we will construct step function to certify bounds. First autocorrelation inequality. For nonnegative supported on [1/4, 1/4], define C1 as the 8 largest constant such that (f )(t) C1 max t1/2 (cid:18)(cid:90) (cid:19)2 holds for all such . The goal is to certify the tightest upper bound on C1; any valid construction 1.50973 [45]. certifies C1 1.50317, AlphaEvolve improved this to C1 and ThetaEvolve refined AlphaEvolves construction to get C1 1.5053, and AlphaEvolve V2 further improved it to C1 . Until early 2025, the best known upper bound was C1 f 2 1.50314. Second autocorrelation inequality. For nonnegative , define C2 = sup 0 2 2 1 . The problem is to certify the tightest known lower bound on C2; any valid construction with ratio 0.8892 [45]. AlphaEvolve first improved this to certifies C2 0.9610 C2 using 50,000-piece step function. 0.8962, [9] improved this to 0.9015, and AlphaEvolve V2 further improved it to C2 r. The best human bound was C2 1.50286, with 30000-piece Results. We improved the best known upper bound to prove C1 step function. The comparisons are reported in Table 2. The previous state-of-the-art, ThetaEvolve, achieved their result by refining the AlphaEvolve V2 construction. In contrast, TTT-Discover found new construction by starting from scratch. We visualize our and prior works step functions in Figure 3. In the second autocorrelation inequality, we have not made discovery. Our best construction certified bound of 0.959, where the AlphaEvolve construction had certified tighter lower bound of 0.961. For the first inequality, early improvements down to 1.510 came from trying and improving gradientbased optimization (e.g., using Adam with softmax parameterization). To reduce the bound from around 1.510 to 1.504, the policy mostly used linear programming (LP), following the insights in [45]. The key insight for the later steps, that gradually achieved the state-of-the-art, was using heuristics to focus optimization only on the constraints that are close to being tightwhere each constraint in the LP bounds one position of the convolution. Heuristics included picking the top positions where the convolution was largest and only including those in the LP, as well as computing gradients from all near-maximum positions rather than just the single largest for gradient-based methods. Unlike AlphaEvolve [14], which mentions the authors suggested ideas such as using Newton type methods, we never intervened on the optimization process. For better comparison to the concurrent work, ThetaEvolve, we also report TTT-Discover with Qwen3-8B [77]. The Qwen3-8B variant they used, DeepSeek-R1-0528-Qwen3-8B that was released by DeepSeek, is not available on Tinker. Thus, we used the original Qwen model (Qwen/Qwen3-8B) that was reportedly worse than the DeepSeek variant. ThetaEvolve reports using 65 steps with 512 rollouts (32 groups of 16 rollouts) each, however we do not modify our hyperparameters otherwise and keep 50 steps of 512 rollouts each. For both inequalities, TTT-Discover with Qwen3-8B certified tighter bounds than ThetaEvolve, using worse model and smaller sampling budget."
        },
        {
            "title": "4.1.3 Circle Packing",
            "content": "In Circle packing, the goal is to maximize the sum of radii of non-overlapping circles packed inside unit square. We follow the setup from prior work [49, 14]. The state is list of circle centers and radii. The action consists of thinking tokens followed by Python code that optimizes circle positions and radii. The reward is the sum of radii achieved for valid packings, and 0 otherwise. We present the results below mostly for comparison purposes, as several recent works on evolutionary algorithms reported their performance using this task. 9 Figure 3. We show the prior and new state-of-the-art, with the (normalized) step functions and their autoconvolutions. Both AlphaEvolve and TTT-Discover starts the discovery process from scratch, while ThetaEvolve initializes from the AlphaEvolve construction, and thus is very similar to the AlphaEvolve construction. TTT-Discover found 30,000-piece step function that certifies that the upper bound C1 1.50286, while AlphaEvolve and ThetaEvolve constructions are 1319-piece. We overlay the step functions and their autoconvolution visually for qualitative comparison."
        },
        {
            "title": "Model",
            "content": "AlphaEvolve [49] Gemini-2.0 Pro + Flash AlphaEvolve V2 [14] Gemini-2.0 Pro + Flash Ensemble (see caption) ShinkaEvolve [37] R1-Qwen3-8B ThetaEvolve [74] = 26 () 2.635862 2.635983 2.635982 2.635983 = 32 () 2.937944 2.939572 n/a n/a TTT-Discover Qwen3-8B 2. 2.939572 Table 3. Results for circle packing. ShinkaEvolve uses an ensemble of Claude Sonnet-4, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o4-mini. Table 3 shows results. TTT-Discover with Qwen3-8B matches the best known constructions for both = 26 and = 32. We make no improvements here, but include these results for completeness. The algorithms found by TTT-Discover are presented in Appendix B.1. Algorithms initialize circles in staggered or hexagonal grid arrangements, then refine positions and radii using sequential least squares programming with boundary and pairwise non-overlap constraints. This solution is lot simpler than recent work, such as ShinkaEvolve [37], especially in terms of initialization, where their solution uses an initialization based on simulated annealing algorithm, while TTT-Discover initializes only with simple geometric arrangement."
        },
        {
            "title": "4.1.4 Expert Review",
            "content": "Human Expert Review Prof. Davide Torlo (Università di Roma La Sapienza) Erdős minimum overlap problem and the autocorrelation inequalities are classical problems in combinatorics with applications in, among others, discrepancy theory, combinatorial optimization, and signal analysis. Both problems can be formulated as minmax problems, in which the minimization is taken over class of functions with bounded norm, while 10 the maximization is performed over set of evaluation points. Closed-form solutions are not known; instead, only lower and upper bounds can be derived. Obtaining sharp bounds for these problems remains challenging mathematical task and is essential for improving our understanding and resolution of such questions. The upper bounds obtained by TTT-Discover for the Erdős minimum overlap and the AC1 autocorrelation problems are achieved by specific piecewise-constant functions. It is straightforward to verify that the provided functions give bounds that improve upon the state of the art: one simply evaluates the quantity of interest and its maximum over discrete set of points determined by the step size of the piecewise-constant functions, and checks that the corresponding norm constraints are satisfied."
        },
        {
            "title": "4.2 Kernel Engineering",
            "content": "GPU kernels are the computational foundation of modern AI, every forward pass and backward pass ultimately executes as kernel code on hardware. We apply our method to GPU kernel optimization, where new state-of-the-art kernel is faster implementation than existing ones. GPUMODE is an open community for kernel development that also hosts competitions for domain experts. We test our method on two competitions: TriMul (triangular matrix multiplication), core primitive in AlphaFolds architecture [30], and DeepSeek MLA (Multi-head Latent Attention), key component in DeepSeeks inference stack [40]. Each GPU type for the TriMul competition (NVIDIA H100, A100, B200, AMD MI300x) has separate leaderboard, as performant implementations differ across architectures. For The MLA competition there is only an MI300x leaderboard. As these competitions were conducted earlier, we retrospectively evaluate our performance while respecting competition standards. We prefer GPUMODE because their leaderboards are well-tested through human competitions with robust evaluation harness [81], and their benchmarks avoid signal-to-noise issues where simple operations or small inputs cause overheads to dominate runtime. Environment: The state is GPU kernel code. The action consists of thinking tokens followed by = Parse(a). For kernel code written in Triton [69]. The dynamics parse the code from the action: the initial state, we provide unoptimized kernels, detailed in Appendix C. The reward is proportional to the inverse of the geometric mean of runtimes on fixed set of input shapes (following the leaderboard), or zero if the kernel fails correctness checks or times out. We evaluate runtime remotely on Modal to scale and ensure consistent hardware conditions. For TriMul, we evaluate the runtime only on H100s during training, even though we still evaluate the generated kernels for A100, B200, and MI300X for final report. Since MI300X is not available on Modal, for MLA-Decode we use H200s, and hope the kernels generalize to MI300X. Further details about the prompts and environments are in Appendix C. Results. We report the runtime of the best kernels and the baselines in Table 4. Our TriMul kernels achieve state-of-the-art across the board in all GPU types. For A100s, our best kernel is 50% faster than the top human kernel, even though our reward function did not time the kernels on A100s. We uniformly achieve > 15% improvement over the best human submissions for all GPU types. Finally, we submit to the official TriMul A100/H100 leaderboard1. The discovered kernels for Trimul identify heavy memory I/O incurred by frequent elementwise operations as major bottleneck to optimize. Specifically, the kernels fuse: (i) operations in the input LayerNorm, (ii) sigmoid and elementwise multiplication in input gating, and (iii) operations in 1See leaderboards. For TriMul B200/MI300X and MLA-Decode MI300X tasks, due to an infra problem on GPU Modes server, we could not submit to the official leaderboard. 11 the output LayerNorm and gating. As for the most compute-heavy operation, which is the matmul with O(N 3) complexity, the kernels convert the inputs to FP16 and delegate the computation to cuBLAS/rocBLAS to effectively leverage TensorCores/MatrixCores of the hardwares. Discovered MLA-Decode kernels. The kernels shown in table 5 mainly rely on torch.compile() for optimization. Specifically, they adopt specific configuration of torch.compile. However, these kernels do not leverage Triton for fine-grained optimization, which may limit further improvements and more flexible use case. We additionally filter and evaluate generated kernels that explicitly use Triton despite their slightly slower runtime, and report in Appendix C. TriMul (, µs)"
        },
        {
            "title": "Model",
            "content": "A100 H100 B200 [95% CI] AMD MI300X [95% CI] 1st human 2nd human 3rd human 4th human 5th human 4531.5 4918.5 5182.2 6097.8 8345.0 1371.1 2368.0 2545.7 3654.8 4233.1 1027.6 [1016.3, 1038.9] 2349.0 [2335.7, 2362.4] 1920.9 [1910.9, 1931.0] 2169.2 [2089.4, 2248.9] 6452.1 [6400.5, 6503.8] 2515.8 [2510.9, 2520.8] 5101.4 [5163.1, 5167.0] 5200.7 [5343.6, 5375.1] 5993.1 [5978.5, 5984.4] 8365.1 [8347.7, 8382.5] Best-of-25600 gpt-oss-120b 9219.7 5390.3 3253.7 [3252.5, 3254.9] 4902.0 [4897.6, 4906.4] TTT-Discover gpt-oss-120b 2198.2 1161.2 910.8 [907.3, 914.2] 1555.7 [1550.8, 1560.5] Table 4. For the TriMul competition, we train single model using H100 runtime as the reward function and report the runtime of the single best kernel. We only trained using H100 for evaluating kernels during training. The generated kernels happened to generalize to other GPU types. We also report the top-5 human submissions in the leaderboard for comparison (each GPU type has its own top-5 human submissions). For A100 and H100, we submitted to the official leaderboard and report the runtime returned. For B200 and MI300X, we could not submit our kernels due to an infra problem on GPU Modes server, and therefore conduct 10 trials for each kernel and report mean and confidence intervals using the same infrastructure as GPUMode, verified by the organizers. Our state-of-the-art kernels are released and can be validated in our codebase."
        },
        {
            "title": "Model",
            "content": "AMD MI300X - MLA Decode (, µs) [95% CI] 1st human 2nd human 3rd human 4th human 5th human Instance 1 Instance 2 Instance 1653.8 [1637.3, 1670.3] 1662.8 [1648.8, 1676.8] 1723.0 [1711.5, 1734.5] 1768.7 [1750.3, 1787.2] 2038.6 [2017.8, 2059.3] 1688.6 [1672.8, 1704.3] 1688.6 [1677.6, 1699.5] 1765.8 [1758.1, 1773.5] 1769.9 [1755.2, 1784.6] 2037.3 [2021.0, 2053.6] 1668.7 [1637.0, 1700.3] 1679.7 [1653.4, 1705.9] 1718.0 [1698.3, 1737.7] 1767.0 [1736.2, 1797.8] 2041.9 [1989.0, 2094.8] Best-of-25600 gpt-oss-120b 2286.0 [2264.2, 2307.8] 2324.1 [2306.0, 2342.1] 2275.2 [2267.3, 2283.1] TTT-Discover gpt-oss-120b 1669.1 [1649.2, 1688.9] 1706.1 [1685.9, 1726.3] 1671.3 [1646.0, 1696.5] Table 5. AMD MLA Decode runtimes on AMD MI300x across three instances. Values are mean runtime across 10 trials with 95% confidence intervals. Top-5 human submissions are from the GPUMode leaderboard. We trained our kernels using an H200 GPUs even though the task is to minimize runtime on MI300x GPUs, since those were not available at scale in online providers. We only selected kernels using single MI300X GPU. There is significant variance across AMD MI300x instances available via AMD Developer Cloud. Thus, we performed our kernel selection and evaluation across three different instances. In each instance, our best kernel was different, and in none of the cases our best kernel where better than the top human submission with statistical significance."
        },
        {
            "title": "4.2.1 Expert Review",
            "content": "Below, we provide verbatim reviews from the GPUMode organizers for our TriMul competition kernels. Human Expert Review Matej Sirovatka, Alex Zhang, Mark Saroufim (GPUMode) The referenced solution correctly determined that the problem is memory bound because of the surrounding point-wise operations so the agent focuses as much as possible on operation fusions, lowering the memory traffic and kernel launch overhead. It also stores activations in fp16, while this is fully aligned with the problem definition and defined tolerances, it could potentially lead to numerical stability issues in full workloads. Overall the agents strategy is to reduce memory bandwidth via fusions, lower precision and delegating the big matrix multiplications to cuBLAS, as those are non-trivial to beat. This is similar to the current best human solutions, but executed on better. Most of the human solutions lack behind in fusing some of the more complex operators together, resulting in this solution outperforming them by large margin."
        },
        {
            "title": "4.3 Algorithm Engineering",
            "content": "Hard optimization problems like package-delivery routing, crew scheduling, factory production planning, power-grid balancingappear throughout industries and must be solved repeatedly at scale. We apply our method to these algorithm engineering problems, where new state-of-the-art would be writing higher-scoring algorithm than existing ones written by human experts. AtCoder Heuristic Contest (AHC) is series of programming competitions focused on optimization problems drawn from real-world industrial challenges [4], attracting hundreds of participants including industry experts. We attempted to evaluate on two past contests, ahc039 and ahc058. ahc039 (\"Purse Seine Fishing\") is computational geometry problem where you design simple closed net on 2D map, restricted to horizontal/vertical edges, to capture many target points while avoiding penalty points under budget. ahc058 (\"Apple Incremental Game\") is production planning problem where upgrades trade off immediate output versus growing future production capacity, and the goal is to schedule upgrades to maximize final output. We select ahc039 because ShinkaEvolve [37] reported solution that would have placed 2nd, and ahc058 because Sakana AIs ALE-Agent achieved the first-ever AI victory in an AHC [54]. We use the evaluation harness from ALE-Bench [27]. We use the public test case generator to create local tests, select our best-performing algorithm, and submit it to be scored on the official platform. Environment: The state is an algorithm implementation in C++. The action consists of thinking = Parse(a). The tokens followed by C++ code. The dynamics parse the code from the action: reward is the score on locally generated test cases, or zero if the algorithm fails correctness checks or exceeds the time limit of 2 seconds and memory limit of 1024MB. We select the best-performing algorithm and submit it to be scored on the official private tests. We use the evaluation harness released by [27]. For initial states, for the ahc039 competition we use the same initial program as [37], which is based on ALE-Agent [27] best program, that would have placed 5th in the competition leaderboard. For ahc058 we start from scratch, similar to ALE-Agent [54]. Previous state-of-the-art. We report the top human submissions on each contest leaderboard. For AI baselines, we compare to ALE-Agent [27] and ShinkaEvolve [37], which use ensembles of models including the gpt, Gemini, and Claude families of models. ALE-Agent [27] starts from scratch for both problems. ShinkaEvolve [37] reports results in ahc039 where they start from ALE-Agent solution, and improve it from 5th place to 2nd place. Results. We report results in Table 6. For both competitions, if we had submitted during competition time, our algorithms would have gotten the 1st place. For ahc039, we marginally improve upon the best human, while there is significant gap between next best AI and human scores. For ahc039, we follow ShinkaEvolve by starting from the ALE-Agent solution and improve it from 5th place to 1st place, while ShinkaEvolve reaches the 2nd place using significantly more capable frontier models such as Gemini 2.5 Pro. For ahc058, we start from scratch and outscore all submissions in the competition. For AHC039, the solution builds large pool of promising axis-aligned rectangles using prefix sum scoring, then greedily seeds connected union and uses simulated annealing with add, remove, replace, expand, shrink, and slide moves to optimize the rectangle union score under perimeter and vertex constraints, followed by cleanup and final greedy refinement. For AHC058, the solution first builds several reasonable plans using greedy rules, different biases, and short beam search to explore promising early decisions. Then, the program improves the best plan with simulated annealing that makes random edits, swaps, and partial rebuilds before finishing with small local cleanup pass. It estimates the value of actions using simple formula for how much future production an upgrade is likely to create, which guides both greedy choices and pruning. For performance, it caches intermediate states so it only recomputes parts of the plan that change. Overall, the program balances broad exploration early with focused local improvement later."
        },
        {
            "title": "Model",
            "content": "Geometry (ahc039) Scheduling (ahc058)"
        },
        {
            "title": "Method",
            "content": "1st human 2nd human 3rd human 4th human 5th human Ensemble (see caption) ALE-Agent [27] ShinkaEvolve [37] Ensemble (see caption) Best-of-25600 gpt-oss-120b TTT-Discover gpt-oss-120b 566, 997 557, 212 554, 334 552, 933 549, 746 550, 647 558, 026 554, 171 567, 062 847, 674, 723 846, 938, 871 846, 350, 877 845, 489, 747 845, 324, 831 848, 373, 282 n/a 772, 429, 752 848, 414, Table 6. Results in two AtCoder Heuristic Competitions. We train our models with local public tests, and submit the best program we get during training to the official submission platform. Our algorithms are released and can be validated in our codebase. Our solutions in the official AtCoder submission platform are publicly available for ahc039 and ahc058. ALE-Agent uses Gemini-2.5 Pro for ahc039, and Gemini-3 Pro Preview high and gpt-5.2-high for ahc058. ShinkaEvolve uses an ensemble of gpt-5, gpt-5-mini, Gemini-2.5 Pro and Flash, Claude Sonnet 4, o4-mini."
        },
        {
            "title": "4.4 Single Cell Analysis",
            "content": "Single-cell RNA-sequencing (RNA-seq) aims to help us understand how organisms work and get sick by resolving biology at the level of individual cells; measuring which genes each cell is using to reveal cell types, states, and how they change. Practically, it isolates single cells, tags their mRNA with Unique Molecular Identifier (UMI), sequences it, and outputs per-cell gene-by-count table. RNA-seq protocols suffer from measurement noise in the observed UMI counts. Thus, denoising algorithms significantly increases the realized value of expensive experiments. Each sequencing run costs thousands of dollars, and better denoising methods reduce the need for deeper sequencing. We apply our method to one of the recent benchmarks OpenProblems [43], an important set of open problems for single-cell analysis. We use the denoising task therein. [7] demonstrated that 14 partitioning the observed molecules of single dataset into training and test sets via binomial sampling and evaluating the denoised training set against the held-out test counts provides proxy for accuracy against true expression values, providing an evaluation framework without requiring external ground truth data. Environment. The state is an algorithm implementation. The action consists of thinking tokens = Parse(a). The benchmark followed by code. The dynamics parse the code from the action: evaluates denoising quality using two complementary metrics: mean squared error (MSE) in log-normalized space, which measures overall reconstruction accuracy, and Poisson negative loglikelihood, which assesses how well the denoised counts match the statistical properties expected of count data. In our context, the reward is the MSE score, or zero if it violates constraints we add for the Poisson score or the algorithm exceeds the time limit of 400 seconds. The Denoising benchmark offers 3 datasets: PBMC, Pancreas, and Tabula Muris Senis Lung, in order of size. We train our policy by using Pancreas in our environment, and ultimately performance is reported by running the algorithm on the held out PBMC and Tabula datasets. Previous state-of-the-art. We report the state of the art as described by the OpenProblems [43] benchmark. The best result was provided by MAGIC [71] using an approximate solver and reversed normalization. MAGIC is well known technique, frequently used in the literature [79, 72], the only method different from MAGIC that provides good performance is ALRA [39], ranked third. We also compare with OpenEvolve and Best-of-25600."
        },
        {
            "title": "Disclaimer",
            "content": "This is an experimental application demonstrating TTT-Discovers ability to find algorithms that excel on specific benchmarks. While our discovered algorithm outperforms existing methods on the OpenProblems denoising benchmark, benchmark metrics are inherently incomplete and do not guarantee biological validity for downstream tasks. Results. The improved function obtained via TTT-Discover shows consistent improvements on both datasets (see Table 7). TTT-Discover is initialized with MAGIC code. TTT-Discover adds gene-adaptive transform ensembling, low-rank SVD refinement, and log-space polishing steps that directly optimize the benchmark metric."
        },
        {
            "title": "Model",
            "content": "MAGIC (A, R) MAGIC (R) ALRA (S, RN) MAGIC (A) MAGIC OpenEvolve Best-of-25600 gpt-oss-120b gpt-oss-120b TTT-Discover gpt-oss-120b PBMC Score () MSE () 0.64 0.64 0.50 0.42 0.42 0.70 0. 0.71 0.19 0.19 0.26 0.19 0.19 0.16 0.20 0.15 Poisson () 0.05 0.05 0.05 0.16 0.16 0.05 0.05 0.05 Tabula Score () MSE () 0.64 0.64 0.47 0.40 0.40 0.71 0.65 0.73 0.18 0.18 0.27 0.18 0.18 0.15 0.18 0.14 Poisson () 0.03 0.03 0.03 0.12 0.12 0.03 0.03 0. Table 7. Denoising task for single cell data analysis. We report the score (mean of normalized MSE and Poisson scores), MSE, and Poisson metrics for each dataset. Our state-of-the-art algorithm is released and can be validated in our codebase. MAGIC (A, R) = MAGIC [71] approximate with reversed normalization; MAGIC (R) = MAGIC with reversed normalization; ALRA [39] (S, R) = ALRA sqrt norm with reversed normalization; MAGIC (A) = MAGIC approximate."
        },
        {
            "title": "4.4.1 Expert Review",
            "content": "Below, we provide verbatim review from Prof. Eric Sun. Human Expert Review Prof. Eric Sun (MIT) Single-cell transcriptomics provides high-dimensional readout on cellular gene expression patterns and has enabled new insights into both biological and disease processes. One challenge in the analysis of single-cell transcriptomics is the sparsity of the data, characterized by zero counts detected for many genes (i.e. \"dropouts\") due to low expression or other technical issues. MAGIC addresses this challenge by de-noising single-cell transcriptomics using diffusion or smoothing, and it has been widely incorporated in the pre-processing of singlecell data for studying multiple diseases and tissue biology. The proposed improvement on the MAGIC algorithm is simple, aligns with the underlying smoothing-based approach of MAGIC, and yields empirical improvements on key metrics. However, improvements on metrics for single-cell data analysis tasks may not always transfer to enhanced ability to obtain new biological insights, which is often difficult to quantify and therefore benchmark. Further evaluation of the proposed algorithm against MAGIC and other existing methods for biologically relevant tasks would be necessary to fully understand the extent of the reported improvements."
        },
        {
            "title": "4.5 Ablations",
            "content": "We have three sets of ablations. First, we ablate the design choices for the train method, while keeping our reuse method, PUCT, fixed. We test (i) TTT with entropic objective using constant β = 2 ([29]), (ii) TTT with no entropic objective (expected reward), (iii) No TTT (only reuse). Second, we ablate the choice of the Reuse method, while keeping our train method, TTT with entropic objective using adaptive β, fixed. We replace PUCT with (i) ϵgreedy reuse with ϵ = 0.1 as this is perhaps the most naive reuse method, and (ii) no reuse. Finally, we report the naive RL baseline, where we use the expected reward objective with no reuse, and the Best-of-25600 baseline."
        },
        {
            "title": "Best Human Kernel",
            "content": "train TTT-Discover"
        },
        {
            "title": "TTT with adaptive entropic",
            "content": "reuse"
        },
        {
            "title": "Ablations for train",
            "content": "TTT with constant β entropic PUCT TTT with expected reward (no entropic) PUCT PUCT No TTT"
        },
        {
            "title": "TTT with adaptive entropic\nTTT with adaptive entropic",
            "content": "Naive Test-time RL Best-of-N"
        },
        {
            "title": "TTT with expected reward\nno TTT",
            "content": "ϵ-greedy no reuse no reuse no reuse Best runtime (, µs) 1371.1 1203.10 1483.83 1985.67 2060.70 1328.89 5274. 5328.73 5352.36 Table 8. Ablation results for the TriMul GPUMode competition where we time the kernels with an H100 GPU. We report the best kernel we get in each run. We report the reward distributions across steps in Figure 4. For each ablation, we report the runtime of the best kernel found in Table 8, and the reward distribution in Figure 4. The rewards distributions and best kernel runtimes are computed with our evaluator, not the leaderboard. Only the full TTT-Discover algorithm achieves the best performance in the TriMul competition. 16 Figure 4. Reward distributions for each ablation. We match the sampling budget across all ablations. We sample 512 rollouts in each step. For example, for Best-of-N , we have = 50 512 = 256000 rollouts. When using constant β, the improvements diminish later in the training. Using the expected reward objective, improvements are slower overall. Without any test-time training, both the mean reward and the max reward stagnates. ϵ-greedy reuse works reasonably well, especially with an early lucky kernel. In early experiments with other applications, the lack of exploration was also bigger problem than it is in kernel engineering tasks. Naive RL and no reuse make minimal improvements. It is entirely possible that additional tuning (e.g., task-specific β schedule) or hyperparameter interactions (e.g., batch size and reuse) can provide improvements in the ablation configurations. For each component, many additional knobs could be ablated (e.g., PUCT exploration bonus, learning rate, batch size). However, our focus was on identifying design choices that works reliably across diverse applications within our budget with minimal task-specific tuning. In practice, the key hyperparameters such as learning rate, batch size, and LoRA rank were fixed after the initial iterations of the project."
        },
        {
            "title": "5 Related Works",
            "content": "In this section, we first provide broad overview of continual learning and test-time training, using some of the exposition in [67]. Then towards the end of 5.2, we discuss the most relevant work on test-time training: MiGrATe [51] and ThetaEvolve [74]. Finally, we discuss two pieces of work with tangential formulations: RL on single training problem that is not the test problem [75] (5.3), and RL on the entire test set [84] (5.4)."
        },
        {
            "title": "5.1 Continual Learning",
            "content": "Most of todays AI systems remain static after deployment, even though the world keeps changing. The high-level goal of continual learning is to enable AI systems to keep changing with the world, similar to how humans improve throughout their lives [19, 11]. Conventionally, continual learning as research field has focused on learning from distribution that gradually changes over time [42, 70, 17]. For example, one could update chatbot model every hour using new knowledge from the Internet, while typical use cases of the model may require knowledge from both the past and the present [57, 31, 73]. More formally, at each timestep, we sample new training and test data from the current distribution, update our model using the new training data, and then evaluate it on all the test data up to the current timestep. Under this setting, most algorithms focus on not forgetting the past when learning from the present [55, 38, 33, 15]."
        },
        {
            "title": "5.2 Test-Time Training",
            "content": "The algorithmic framework of test-time training has the same high-level goal as continual learning, but it focuses on two aspects where human learning stands out from the forms of continual learning in the conventional literature. First, each person has unique brain that learns within the context of their individual life. This personalized form of continual learning is quite different from, for example, the chatbot model that is fine-tuned hourly using the latest information available worldwide. While such model does change over time, it is still the same at any given moment for every user and every problem instance. Second, most human learning happens without boundary between training and testing. Consider your commute to work this morning. It is both \"testing\" because you did care about getting to work this very morning, and \"training\" because you were also gaining experience for future commutes. But in machine learning, the train-test split has always been fundamental concept. The concept of test-time training is introduced to realize these two special aspects of human learning. Training typically involves formulating learning problem (such as empirical risk minimization) and then solving it. Following [64], test-time training is defined as any kind of training that formulates potentially different learning problem based on each individual test instance. This concept has rich history in AI. well-known example in NLP is dynamic evaluation, pioneered by Mikolov et al. [46] and extended by Krause et al. [35]. In computer vision, early examples have also emerged in applications such as face detection [28], video segmentation [48], super-resolution [59], and 3D reconstruction [44]. Next, we discuss three popular forms of test-time training today, with an emphasis on their connections to each other and to historical examples."
        },
        {
            "title": "5.2.1 TTT on Nearest Neighbors: Larger Effective Capacity",
            "content": "One simple form of test-time training was called locally weighted regression in the 1970s [63, 10], local learning in the 1990s [8], and KNN-SVM in the 2000s [82]: Given test instance, find its nearest neighbors in the training set, and then train (or fine-tune) the model on these neighbors before making prediction. This procedure can significantly increase the effective capacity of the model; for example, it allows linear model to fit highly nonlinear ground truth [63]. This simple form captures one of the key intuitions of test-time training. In the conventional view of machine learning, model, once trained, no longer changes at test time. As consequence, it must prepare to be good at all possible inputs in the future. This task can be very hard, because being good at all possible futures limits the models capacity to be good at any particular one. But only one future is actually going to happen. So why not train our model once this future happens? Recently, [18] extended this idea to modern language models and observed similar benefit of larger effective model capacity after test-time training, and [25] further improved these results through better strategies for neighbor selection. In addition, [26] showed that test-time training on neighbors from the training set is also effective with RL for reasoning tasks, and [5] developed the same idea for visual-motor tasks."
        },
        {
            "title": "5.2.2 TTT for Novel Instances: Better Generalization",
            "content": "As models become larger today, their competence is often limited not by their capacity, but by the amount of available training data, especially when they need to generalize to novel test instances that are out-of-distribution. In this case, it is even harder to prepare for all possible test instances in the future, especially the novel ones, with static model. But once specific test instance is given, we can use it to generate relevant data, which we can then use for training [65]. In other words, the neighbors for TTT do not have to come from the training set; they can also be generated on-the-fly. 18 Since the test instance is unlabeled, one way to make it useful for training is through self-supervision, which generates new pairs of inputs and labels for an auxiliary task such as masked reconstruction (e.g., BERT [12] and MAE[21]). While the auxiliary task is different from the main prediction task, improving performance in one can help the other through their shared representations. This form of TTT can significantly improve generalization under distribution shifts [65, 13]. Recently, TTT has been an important part of AlphaProof [24], which achieved IMO silver-medal standard in 2024. Given each test problem, their system first generates targeted curriculum of easier problems by prompting language model, and then performs reinforcement learning on the generated data. Another recent work, Akyurek et al. [3], found TTT effective for few-shot reasoning tasks such as ARC-AGI. Their system generates augmentations of the few-shot demonstrations in the test problem then performs supervised learning. MiGrATe [51] and ThetaEvolve [74] are two concurrent works that share our high-level idea of performing RL at test time on single problem. MiGrATe combines on-policy and off-policy RL and tests on simpler environments such as word search. ThetaEvolve is more similar to our work: it uses OpenEvolve, variant of AlphaEvolve, for state-action reuse. Both methods use GRPO variants for training. Compared to ThetaEvolve, TTT-Discover using the same model and compute budget still produces significant improvements  (Table 2)  , which we attribute to our entropic objective and PUCT-based reuse instead of more complicated and brittle heuristics in evolutionary algorithms."
        },
        {
            "title": "5.3 RL on One Example",
            "content": "One Example RL [75] is relevant as they also train on single problem. To be specific, they train on one example from dataset, such as the MATH training set. They show that policy trained with on one such problem with RL generalizes to other problems in the same dataset. In contrast, TTT-Discover trains on the test problem itself, where the goal is not to generalize but to solve this specific problem."
        },
        {
            "title": "5.4 RL on the Test Set",
            "content": "TTRL [84] trains on an entire test set of problems using majority voting as pseudo-labels for reward estimation. In contrast, TTT-Discover trains on single test problem with continuous verifiable reward, where the goal is not to improve average performance across set of problems but to find one exceptional solution."
        },
        {
            "title": "6 Future Work",
            "content": "The current form of our method can only be applied to problems with continuous rewards, and the most important direction for future work is test-time training for problems with sparse or binary rewards, or problems in non-verifiable domains."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Matej Sirovatka, Davide Torlo, Eric Sun, Alex Zhang, Mark Saroufim, for reviewing our results and letting us cite their reviews in this paper. We would like to thank Amanda Moran and Nvidia for their support with the compute infrastructure; Charles Frye and Modal team, Clare Birch, John Schulman, Tianyi Zhang, Yangjun Ruan, and Thinking Machines Lab team for compute credits supporting this project; Anika Guptam, Zacharie Bugaud, and the broader Astera Institute for their support in various phases of the project; Matej Sirovatka, Alex Zhang, Mark Saroufim and the broader GPUMode community for their support in various phases of this project. We thank Mehmet Hamza Erol and Vipul Sharma for their short-term contributions. We thank Luke Bailey for feedback on this draft. Mert would like to thank Begum Ergun, Fatih Dinc, Omer Faruk Akgun, Ramiz Colak, Yigit Korkmaz for their support at every phase of this project. References [1] Submission #59660035 third programming contest 2024 (atcoder heuristic contest 039). https: //atcoder.jp/contests/ahc039/submissions/59660035, November 2024. AtCoder Heuristic Contest 039 submission page. [2] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. [3] Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning. arXiv preprint arXiv:2411.07279, 2024. [4] AtCoder Inc. AtCoder. https://atcoder.jp, 2025. [5] Marco Bagatella, Mert Albaba, Jonas Hübotter, Georg Martius, and Andreas Krause. Test-time offline reinforcement learning on goal-related experience. arXiv preprint arXiv:2507.18809, 2025. [6] Richard Barnard and Stefan Steinerberger. Three convolution inequalities on the real line with connections to additive combinatorics. Journal of Number Theory, 207:4255, 2020. [7] Joshua Batson, Loic Royer, and James Webber. Molecular cross-validation for single-cell rna-seq. BioRxiv, page 786269, 2019. [8] Léon Bottou and Vladimir Vapnik. Local learning algorithms. Neural computation, 4(6):888900, 1992. [9] Christopher Boyer and Zane Kun Li. An improved example for an autoconvolution inequality. arXiv preprint arXiv:2506.16750, 2025. [10] William Cleveland. Robust locally weighted regression and smoothing scatterplots. Journal of the American statistical association, 74(368):829836, 1979. [11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):33663385, 2021. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [13] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 2022. [14] Bogdan Georgiev, Javier Gómez-Serrano, Terence Tao, and Adam Zsolt Wagner. Mathematical exploration and discovery at scale. arXiv preprint arXiv:2511.02864, 2025. [15] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 43674375, 2018. 20 [16] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. [17] Raia Hadsell, Dushyant Rao, Andrei Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in cognitive sciences, 24(12):10281040, 2020. [18] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models. arXiv preprint arXiv:2305.18466, 2023. [19] Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick. Neuroscienceinspired artificial intelligence. Neuron, 95(2):245258, 2017. [20] Jan Kristian Haugland. The minimum overlap problem revisited. arXiv preprint arXiv:1609.08000, 2016. [21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked autoencoders are scalable vision learners. CoRR, abs/2111.06377, 2021. [22] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: critical analysis of out-of-distribution generalization. ICCV, 2021. [23] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [24] T. Hubert, R. Mehta, L. Sartran, and et al. Olympiad-level formal mathematical reasoning with reinforcement learning. Nature, 2025. [25] Jonas Hübotter, Sascha Bongni, Ido Hakimi, and Andreas Krause. Efficiently learning at test-time: Active fine-tuning of llms. arXiv preprint arXiv:2410.08020, 2024. [26] Jonas Hübotter, Leander Diaz-Bone, Ido Hakimi, Andreas Krause, and Moritz Hardt. Learning on the job: Test-time curricula for targeted reinforcement learning. arXiv preprint arXiv:2510.04786, 2025. [27] Yuki Imajuku, Kohki Horie, Yoichi Iwata, Kensho Aoki, Naohiro Takahashi, and Takuya Akiba. Ale-bench: benchmark for long-horizon objective-driven algorithm engineering. arXiv preprint arXiv:2506.09050, 2025. [28] Vidit Jain and Erik Learned-Miller. Online domain adaptation of pre-trained cascade of classifiers. In CVPR 2011, pages 577584. IEEE, 2011. [29] Yuhua Jiang, Jiawei Huang, Yufeng Yuan, Xin Mao, Yu Yue, Qianchuan Zhao, and Lin Yan. Risk-sensitive rl for alleviating exploration dilemmas in large language models. arXiv preprint arXiv:2509.24261, 2025. [30] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. [31] Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual pre-training of language models. arXiv preprint arXiv:2302.03241, 2023. [32] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [33] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. [34] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: benchmark of in-the-wild distribution shifts. In International conference on machine learning, pages 56375664. PMLR, 2021. 21 [35] Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence models. In International Conference on Machine Learning, pages 27662775. PMLR, 2018. [36] Thinking Machines Lab. Tinker, 2025. [37] Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. Shinkaevolve: Towards open-ended and sampleefficient program evolution. arXiv preprint arXiv:2509.19349, 2025. [38] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):29352947, 2017. [39] George Linderman, Jun Zhao, Manolis Roulis, Piotr Bielecki, Richard Flavell, Boaz Nadler, and Yuval Kluger. Zero-preserving imputation of single-cell rna-seq data. Nature communications, 13(1):192, 2022. [40] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [41] Fei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li, Xi Lin, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang. Llm4ad: platform for algorithm design with large language model. arXiv preprint arXiv:2412.17287, 2024. [42] David Lopez-Paz and MarcAurelio Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pages 64676476, 2017. [43] Malte Luecken, Scott Gigante, Daniel Burkhardt, Robrecht Cannoodt, Daniel Strobl, Nikolay Markov, Luke Zappia, Giovanni Palla, Wesley Lewis, Daniel Dimitrov, et al. Defining and benchmarking open problems in single-cell analysis. Nature Biotechnology, pages 16, 2025. [44] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Transactions on Graphics (ToG), 39(4):711, 2020. [45] Máté Matolcsi and Carlos Vinuesa. Improved bounds on the supremum of autoconvolutions. Journal of Mathematical Analysis and Applications, 372(2):439447, 2010. [46] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [47] John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution shift on question answering models. In International conference on machine learning, pages 69056916. PMLR, 2020. [48] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, and Kayvon Fatahalian. Online model distillation for efficient video inference. arXiv preprint arXiv:1812.02699, 2018. [49] Alexander Novikov, Ngân u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. [50] J. Peters, K. Muelling, and Y. Altun. Relative entropy policy search. In Proceedings of 24th AAAI Conference on Artificial Intelligence (AAAI 10), pages 16071612, July 2010. [51] Peter Phan, Dhruv Agarwal, Kavitha Srinivas, Horst Samulowitz, Pavan Kapanipathi, and Andrew McCallum. Migrate: Mixed-policy grpo for adaptation at test-time. arXiv preprint arXiv:2508.08641, 2025. [52] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral testing of nlp models with checklist. arXiv preprint arXiv:2005.04118, 2020. [53] Christopher Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence, 61(3):203230, 2011. [54] Sakana AI. Sakana ai agent wins atcoder heuristic contest (first ai to place 1st). https://sakana.ai/ ahc058/, 2026. [55] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Metalearning with memory-augmented neural networks. In International conference on machine learning, pages 18421850, 2016. [56] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [57] Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Fine-tuned language models are continual learners. arXiv preprint arXiv:2205.12393, 2022. [58] Asankhaya Sharma. Openevolve: an open-source evolutionary coding agent, 2025. [59] Assaf Shocher, Nadav Cohen, and Michal Irani. zero-shot super-resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 31183126, 2018. [60] David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484489, 2016. [61] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):11401144, 2018. [62] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 550(7676):354359, 2017. [63] Charles Stone. Consistent nonparametric regression. The annals of statistics, pages 595620, 1977. [64] Yu Sun, Xinhao Li, Karan Dalal, Chloe Hsu, Sanmi Koyejo, Carlos Guestrin, Xiaolong Wang, Tatsunori Hashimoto, and Xinlei Chen. Learning to (learn at test time). arXiv preprint arXiv:2310.13807, 2023. [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning, pages 92299248. PMLR, 2020. [66] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. [67] Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, et al. End-to-end test-time training for long context. arXiv preprint arXiv:2512.23675, 2025. [68] Yunhao Tang and Rémi Munos. On few pitfalls in kl divergence gradient estimation for rl. arXiv preprint arXiv:2506.09477, 2025. [69] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019, 2019. [70] Gido Van de Ven and Andreas Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019. [71] David Van Dijk, Roshan Sharma, Juozas Nainys, Kristina Yim, Pooja Kathail, Ambrose Carr, Cassandra Burdziak, Kevin Moon, Christine Chaffer, Diwakar Pattabiraman, et al. Recovering gene interactions from single-cell data using data diffusion. Cell, 174(3):716729, 2018. [72] Aarthi Venkat, Scott Youlten, Beatriz San Juan, Carley Purcell, Shabarni Gupta, Matthew Amodio, Daniel Neumann, John Lock, Anton Westacott, Cerys McCool, et al. Aanet resolves continuum of spatially-localized cell states to unveil intratumoral heterogeneity. Cancer Discovery, 2025. 23 [73] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. comprehensive survey of continual learning: Theory, method and application. IEEE transactions on pattern analysis and machine intelligence, 46(8):5362 5383, 2024. [74] Yiping Wang, Shao-Rong Su, Zhiyuan Zeng, Eva Xu, Liliang Ren, Xinyu Yang, Zeyi Huang, Xuehai He, Luyao Ma, Baolin Peng, et al. Thetaevolve: Test-time learning on open problems. arXiv preprint arXiv:2511.23473, 2025. [75] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. [76] Ethan Patrick White. new bound for Erdős minimum overlap problem. Acta Arithmetica, 208:235255, 2023. [77] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [78] Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, august 2025. URL https://fengyao. notion. site/offpolicy-rl. [79] Khalil Kass Youssef, Nitin Narwade, Aida Arcas, Angel Marquez-Galera, Raúl Jiménez-Castaño, Cristina Lopez-Blau, Hassan Fazilaty, David García-Gutierrez, Amparo Cano, Joan Galcerán, et al. Two distinct epithelial-to-mesenchymal transition programs control invasion and inflammation in segregated tumor cell populations. Nature Cancer, 5(11):16601680, 2024. [80] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature, 639(8055):609616, 2025. [81] Alex Zhang, Matej Sirovatka, Erik Schultheis, Benjamin Horowitz, and Mark Saroufim. Kernelbot: competition platform for writing heterogeneous GPU code. In Championing Open-source DEvelopment in ML Workshop @ ICML25, 2025. [82] Hao Zhang, Alexander Berg, Michael Maire, and Jitendra Malik. Svm-knn: Discriminative nearest neighbor classification for visual category recognition. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR06), volume 2, pages 21262136. IEEE, 2006. [83] Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, and Andrew Chi-Chih Yao. On the design of kl-regularized policy gradient algorithms for llm reasoning. arXiv preprint arXiv:2505.17508, 2025. [84] Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "Value",
            "content": "gpt-oss-120b [2] high Rollout Context window Sampling temperature Maximum tokens to generate Prompt length + thinking token limit 26000 Teacher forcing 32768 tokens 1.0 32768-prompt length ... okay, am out of thinking tokens. need to send my final message now. Training Batch size Training steps Optimizer LoRA [23] rank KL coefficient (λ) Objective 512 (8 groups with 64 rollouts each) 50 Adam [32], lr 4 10 32 {0.01, 0.1} Entropic; adaptive β(sinit) with KL constraint γ = ln(2). 5, β1 = 0.9, β2 = 0.95, ϵ = 10 8 Reuse PUCT (exploration coefficient)"
        },
        {
            "title": "Further details",
            "content": "1."
        },
        {
            "title": "Appendix A",
            "content": "Table 9. We fix these hyperparameters across all applications."
        },
        {
            "title": "A Training details",
            "content": "Our hyperparameters are fixed throughout almost all experiment. For almost all applications we used KL penalty coefficient of 0.1. For algorithm engineering, we used KL coefficient of 0.01. We present details on our objective function and the reuse algorithm below. 25 A.1 Entropic utility objective We define the entropic utility objective explored also in the concurrent work [29]: Jβ(θ; s) (cid:66) log τπθ(s) eβr(τ;s)(cid:105) (cid:104) ."
        },
        {
            "title": "The gradient of this objective yields",
            "content": "θJβ(θ; s) = τπθ(s) (cid:104) θ log πθ(τ s) wβ(τ s) (cid:105) , wβ(τ s) = eβr(τ;s) πθ [eβr(τ;s)] , Aβ(τ s) = wβ(τ s) 1, πθ [wβ(τ s)] = 1, we get Aβ as the mean baselined advantage. The remaining question is since how to set β. [29] recommends value β = 2, yet we found it tricky to set it. Later in the training, improvements become harder, and unless β is adjusted carefully advantages can become very small. Early in the training, large β can cause instabilities. Adaptive β. Define the auxiliary tilted distribution induced by the entropic weights, πθ(τ s) exp(βr(τ; s)) πθ [exp(βr(τ; s))] qβ(τ s) πθ(τ s) wβ(τ s) = qβ(τ s) = , . Then wβ is exactly the density ratio that appears in the entropic policy-gradient update, so β controls the effective step size induced by this reweighting. We choose β(s) by enforcing KL budget on the auxiliary distribution, (cid:16) qβ(s)( s) πθ( s) KL analogous to Relative Entropy Policy Search, where the temperature is set by an exponential tilt under relative-entropy constraint [50]. In words, β(s) is increased only until the KL budget is exhausted, ensuring the induced reweighting, and hence the update, does not move too far from πθ( s). We fix γ = ln 2 throughout our experiments. Batch estimator. Given rollouts from the same with rewards {rn distribution is uniform on the batch, u(n) = 1/N . The induced reweighting on the batch is }N n=1, the empirical sampling = γ, (cid:17) qβ(n) = eβrn m=1 eβrm (cid:80)N , and we set β(s) by solving the weight-concentration constraint KL(qβ u) = N(cid:88) n= qβ(n) log (cid:16) qβ(n) (cid:17) = γ via simple bisection search over β 0. With ˆβ(s), we compute LOO entropic advantages using rmax = maxn rn, and an ϵ in the denominator for numerical stability: ˆZn ="
        },
        {
            "title": "1\nN − 1",
            "content": "(cid:88) m(cid:44)n exp( ˆβ(s)(rm rmax)), An = exp( ˆβ(s)(rn rmax)) ˆZn + ε 1. Discussion. States where improvements are consistently small (e.g. high-value / near-goal states) tend to make the batch weights qβ(n) less peaky for given β, so the constraint typically permits larger β(s). In contrast, states that occasionally yield few very large improvements (often earlier in training or low-value states with large headroom) make qβ concentrate quickly as β grows; the same KL budget then forces smaller β(s), preventing the update from being dominated by handful of outlier trajectories while still preferring better-than-average rollouts. Finally, this estimator is (τ) = wr(τ) + yield the same invariant to shifting or scaling the reward by constant, i.e., r(τ) and advantage for R+ and R. 26 A.2 PUCT Prioritization We maintain an archive next start state, we score each virtual root whose actions correspond to selecting start state from the archive [53, 60, 62, 61]: of previously discovered states with reward R(s) R. To choose the by PUCT-inspired rule, analogous to applying PUCT at score(s) = Q(s) + scale (s) 1 + 1 + n(s) , where n(s) is visitation count, is the number of expanded parents so far, > 0 is an exploration coefficient, and scale = Rmax Rmin is the reward range over the archive. The prior (s) is linear rank distribution: (s) = (cid:80) rank(s) (H rank(s)) , sH where rank(s) {0, . . . , term Q(s) uses the best one-step reachable reward m(s): 1} orders states by descending reward (rank 0 is the best state). The Q(s) = m(s) n(s) > 0 n(s) = 0 R(s) . After expanding parent and observing its best child reward = maxsChild(p) R(s ), we update: m(p) max(m(p), y) n(a) n(a) + 1 {p} Anc(p) + 1. (direct parent only) (backprop visitation) For the archive update, we keep the top-2 children per expanded parent (largest R) before inserting, then enforce global size constraint by retaining the top-1000 states in by R, while always keeping the initial seed states. Comparison to AlphaZero PUCT. AlphaZeros PUCT operates over tree of state-action edges, selecting actions via = arg maxa[Q(s, a) + (s, a) (cid:112)(cid:80) (s, b)/(1 + (s, a))], where Q(s, a) is the mean value of simulations through edge (s, a), (s, a) is learned policy prior, and (s, a) counts visits to that edge [62, 61]. Our formulation differs in four ways: (i) Q(s) tracks the maximum child reward rather than the mean, favoring optimistic expansion; (ii) (s) is rank-based prior over archived states rather than learned action distribution; (iii) visitation counts backpropagate to all ancestors, so expanding any descendant reduces the exploration bonus for the entire lineage; and (iv) we block the full lineage (ancestors and descendants) from the current batch to encourage diversity, whereas AlphaZero uses virtual loss as temporary penalty."
        },
        {
            "title": "B Mathematics",
            "content": "B.1 Circle Packing Circle Packing (n = 26) ```python import numpy as np from scipy.optimize import minimize def run_packing(): = 26 2 3 4 5 6 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 61 62 63 64 65 67 68 69 70 71 73 74 75 76 77 79 80 initial_centers = [] initial_radii = [] # Adjusted initial radius and spacing parameters r_initial = 0.102 buffer = 1e-6 # Slightly smaller for better flexibility # Small buffer to prevent boundary violations # Generate staggered grid with 5 rows and varying number of circles per row for row in range(5): # 5 rows total # Even rows start at r_initial, odd rows also start with buffer if row % 2 == 0: x_start = r_initial + buffer # Even rows start slightly inside else: x_start = r_initial + buffer # Odd rows also start with buffer # Varying number of circles per row to fit better if row == 0 or row == 2 or row == 4: num_circles = 5 # Even rows (0, 2, 4) have 5 circles elif row == 1: num_circles = 6 # First odd row has 6 circles else: # row == 3 num_circles = 5 # Second odd row has 5 circles if num_circles == 0: continue # Calculate horizontal spacing for this row if num_circles == 1: spacing_row = 0.0 else: # Ensure horizontal spacing is at least 2*r_initial to prevent overlaps max_horizontal = 1 - 2 * r_initial spacing_row = max_horizontal / (num_circles - 1) if max_horizontal > 0 else 0.0 # Place circles in this row for col in range(num_circles): = x_start + col * spacing_row # Vertical positioning with refined vertical spacing if row == 0: = r_initial + buffer # First row starts with buffer else: # Vertical spacing with refined factor for denser packing = r_initial + buffer + row * 1.0 * np.sqrt(3) * r_initial # Ensure does not exceed 1 - r_initial if + r_initial > 1 + 1e-6: = 1 - r_initial - 1e-6 # Clamp to prevent overflow initial_centers.append([x, y]) # Assign initial radii based on row (middle row gets slight boost) if row == 2: initial_radii.append(r_initial + 0.003) # Increased boost for central row else: initial_radii.append(r_initial) # Flatten the initial variables for optimization variables_initial = [] for in range(n): variables_initial.extend(initial_centers[i]) variables_initial.append(initial_radii[i]) # Objective function to maximize sum of radii def objective(vars): total = 0.0 for in range(n): idx = * 3 total += vars[idx + 2] return -total # Minimize negative sum to maximize # Define constraints constraints = [] # Constraints for center positions and radii for in range(n): 28 81 82 84 85 86 87 88 90 91 92 93 94 96 97 98 99 100 102 103 104 105 106 108 109 110 111 112 114 115 116 117 118 120 121 122 123 124 126 127 128 129 130 132 133 134 135 136 138 139 140 141 142 144 1 2 3 4 # x_i >= r_i def constraint1(vars, i=i): idx = * 3 return vars[idx] - vars[idx + 2] constraints.append({type: ineq, fun: constraint1}) # x_i + r_i <= 1 def constraint2(vars, i=i): idx = * 3 return 1 - (vars[idx] + vars[idx + 2]) constraints.append({type: ineq, fun: constraint2}) # y_i >= r_i def constraint3(vars, i=i): idx = * 3 return vars[idx + 1] - vars[idx + 2] constraints.append({type: ineq, fun: constraint3}) # y_i + r_i <= 1 def constraint4(vars, i=i): idx = * 3 return 1 - (vars[idx + 1] + vars[idx + 2]) constraints.append({type: ineq, fun: constraint4}) # Pairwise distance constraints for in range(n): for in range(i + 1, n): def constraint_pair(vars, i=i, j=j): idx_i = * 3 idx_j = * 3 x_i, y_i, r_i = vars[idx_i], vars[idx_i + 1], vars[idx_i + 2] x_j, y_j, r_j = vars[idx_j], vars[idx_j + 1], vars[idx_j + 2] dist = np.sqrt((x_i - x_j)**2 + (y_i - y_j)**2) return dist - (r_i + r_j) constraints.append({type: ineq, fun: constraint_pair}) # Optimize using Sequential Least Squares Programming with refined parameters result = minimize( objective, variables_initial, method=SLSQP, constraints=constraints, options={ ftol: 1e-14, maxiter: 1000000, disp: False, eps: 1e-12, iprint: 0, # Suppress verbose output finite_diff_rel_step: np.sqrt(np.finfo(float).eps) } ) # Extract optimized centers and radii optimized_vars = result.x centers = [] radii = [] for in range(n): idx = * 3 centers.append([optimized_vars[idx], optimized_vars[idx + 1]]) radii.append(optimized_vars[idx + 2]) sum_radii = sum(radii) return np.array(centers), np.array(radii), sum_radii ``` Circle Packing (n = 32) ```python import numpy as np from scipy.optimize import minimize def run_packing(): 29 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 38 39 40 41 42 44 45 46 47 48 50 51 52 53 54 56 57 58 59 60 62 63 64 65 66 68 69 70 71 72 74 75 76 77 78 = 32 r_initial = 1.0 / (2.0 + 5.0 * np.sqrt(3)) # Maximum radius for vertical constraint # Generate hexagonal arrangement for 30 circles centers = [] for row in range(6): # 6 rows with 5 columns each = r_initial * (1 + row * np.sqrt(3)) if row % 2 == 0: x_start = (1.0 - 9.0 * r_initial) / 2 # Adjusted to use more horizontal space else: x_start = (1.0 - 9.0 * r_initial) / 2 + r_initial # 5 columns for col in range(5): = x_start + col * 2 * r_initial # Ensure the circle is within the square and properly spaced centers.append([x, y]) # Add two extra circles near the top-right and bottom-right corners with adjusted initial positions extra_x = 1.0 - r_initial - 0.0005 extra_y_top = 0.5 extra_y_bottom = r_initial + 0.0005 centers.append([extra_x, extra_y_top]) centers.append([extra_x, extra_y_bottom]) # Initial radii for all circles radii = [r_initial] * # Flatten the centers and radii into single array for optimization x0 = np.concatenate([np.array(centers).ravel(), np.array(radii)]) # Objective function to maximize: sum of radii def objective(x): # Unflatten into centers and radii centers_flat = x[:n*2].reshape(n, 2) radii_flat = x[n*2:] return -np.sum(radii_flat) # Negative because we minimize # Constraints: for each circle, x_i - r_i >= -1e-12, x_i + r_i <= 1 + 1e-12, same for # and for each pair, distance >= r_i + r_j - 1e-12 # Define constraint functions def constraint_boundary(x): centers_flat = x[:n*2].reshape(n, 2) radii_flat = x[n*2:] constraints = [] epsilon = 1e-12 for in range(n): x_i, y_i = centers_flat[i] r_i = radii_flat[i] constraints.append(x_i - r_i + epsilon) constraints.append(1 + epsilon - x_i - r_i) constraints.append(y_i - r_i + epsilon) constraints.append(1 + epsilon - y_i - r_i) # x_i - r_i >= -epsilon # x_i + r_i <= 1 + epsilon # y_i - r_i >= -epsilon # y_i + r_i <= 1 + epsilon return np.array(constraints) # Define constraint for non-overlapping def constraint_overlap(x): centers_flat = x[:n*2].reshape(n, 2) radii_flat = x[n*2:] constraints = [] for in range(n): for in range(i + 1, n): dx = centers_flat[i, 0] - centers_flat[j, 0] dy = centers_flat[i, 1] - centers_flat[j, 1] dist = np.sqrt(dx**2 + dy**2) constraints.append(dist - radii_flat[i] - radii_flat[j] + 1e-12) return np.array(constraints) # Combine all constraints cons = [] # Boundary constraints cons.append({type: ineq, fun: lambda x: constraint_boundary(x)}) # Overlap constraints cons.append({type: ineq, fun: lambda x: constraint_overlap(x)}) 30 80 81 82 84 85 86 87 88 90 91 92 93 94 96 97 98 99 100 102 103 104 105 106 108 109 110 111 112 # Perform optimization with adjusted parameters result = minimize( objective, x0, method=SLSQP, constraints=cons, tol=1e-10, options={disp: False, maxiter: 200000, ftol: 1e-12, eps: 1e-8} ) # Extract the result optimized_x = result.x centers_opt = optimized_x[:n*2].reshape(n, 2) radii_opt = optimized_x[n*2:] # Check if optimization was successful if not result.success: print(Optimization failed) # Fallback to initial guess centers_opt = np.array(centers) radii_opt = np.array(radii) else: # Validate the packing valid = validate_packing(centers_opt, radii_opt) if not valid: print(Validation failed) # Fallback to initial guess centers_opt = np.array(centers) radii_opt = np.array(radii) sum_radii = np.sum(radii_opt) return centers_opt, radii_opt, sum_radii ``` B.2 Autocorrelation Inequalities For autocorrelation inequalities, initial sequences are created by sampling random value in [0, 1] and repeating it between 1,000 and 8,000 times (or loading state-of-the-art construction when available). For the first inequality, the verifier computes the upper bound 2nmax(f )/( )2 where denotes discrete autocorrelation; it validates that inputs are non-empty lists of non-negative floats clamped to [0, 1000] with sum 0.01, and returns for invalid constructions. For the second ) using piecewise-linear inequality, verifier computes the lower bound C2 = 2 integration for the L2 norm (Simpson-like rule with endpoint zeros) over the normalized interval [1/2, 1/2]. Each algorithm is run with 1 GB with 2 CPUs each and timeout of up to 1100 seconds. 2/(f (cid:80) 1 B.3 Erdős We initialize TTT-Discover with random constructions of 40-100 samples around 0.5 with random perturbations. We filter out sequences with more than 1000 values in the verifier. Each algorithm is run with 1 GB with 2 CPUs each and timeout of up to 1100 seconds."
        },
        {
            "title": "C Kernel engineering",
            "content": "For trimul, we provide the matrix multiplication kernel that triton provides in README, mostly for syntax purposes For MLA-Decode, we first put softmax kernel in preliminary prompt to let the base model generate correct but unoptimized MLA-Decode kernel, and then use that as the initial state with the earlier softmax example removed. 31 C.1 Kernel evaluation details Setup of verifier for training. We follow the exact same practice for evaluating kernel correctness and runtime as the original GPUMode competitions. Specifically, the verifier used in our training jobs uses the same code as the official GPU Mode Competition Github repository, with minor adjustment to integrate into our training codebase. The verification process includes correctness check that compare output values between the custom kernel and pytorch reference program under designated precision, followed by runtime benchmarking of the custom kernel across multiple iterations. All the details in our verification procedure follow the official competition exactly, including the test cases used for correctness check and benchmarking, hyper-parameters such as matching precision and iterations used for timing, etc. We run our verifier on H100s for TriMul, and H200s for MLA-Decode, both from the Modal cloud platform. Setup of environments for final report. For final report, we submit to the official TriMul A100/H100 leaderboard and report the runtime shown. For TriMul B200/MI300X and MLA-Decode MI300X tasks, due to an infra problem on GPU Modes server, we could not submit to the official leaderboard. For these tasks, we work with the GPU Mode team closely to set up our local environment, which replicates the official environment and gets GPU Mode teams review and confirmation. Selection protocol for best kernels. For TriMul H100 task, we select 20 kernels with the best verifier score throughout training. For other tasks, since our verifier hardware in training is different from the target hardware, we select 20 kernels with the best training scores plus 20 random correct kernels every 10 steps of training. Finally, we used our verifier with the target hardware to verify each selected kernels for three times, and submit the kernel with the smallest average runtime for final report. C.2 Analysis of best generated kernels TriMul H100 kernels. The below code shows the best TriMul kernels discovered by TTT for H100 GPU. At the high level, the kernel correctly identifies major bottleneck of the problem, which is the heavy memory I/O incurred by series of elementwise operations, and then focuses on fusing them with Triton. Specifically, the kernel fuses: (i) operations in the input LayerNorm, (ii) elementwise activation and multiplication for input gating, and (iii) operations in the output Layernorm and output gating. As for the compute-heavy operation, which is an O(N 3) matmul, the kernel converts its inputs to fp16 and delegate the computation to cuBLAS to effectively leverage the TensorCores on H100 GPU. Compared with kernels generated early in training, the final kernel achieves big improvement by (i) fusing more operations together, and (ii) deeper optimization of the memory access pattern inside fused kernels. For example, kernel generated early fuses LayerNorm operations, but does not fuse the input gating process. kernel generated in the middle of training fuses the same operations as the final kernel, but has less efficient memory access pattern in the fused kernel for output LayerNorm, gating, and output projection. Compared with the best human leaderboard kernel, the TTT discovered kernel adopts similar fusion strategy for the input LayerNorm and input gating. Different from human kernel, the TTT kernel does not perform as much auto-tuning of block size, which could be limitation. However, the TTT kernel fuses the output LayerNorm and gating with output projection whereas the human kernel does not, which could explain the moderate advantage of the former. 32 C.3 TTT MLA-Decode kernels filtered with Triton kernels 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 38 39 40 41 42 44 45 46 47 48 50 51 52 53 54 56 57 58 59 60 62 63 64 65 66 68 69 70 TriMul H100 Outgoing TriMul (AlphaFold-3) - Triton accelerated forward pass. The implementation follows the reference ``TriMul`` module but fuses the expensive kernels: 1. Row-wise LayerNorm over the last dimension (FP16 output, FP32 reduction). 2. Fused projection, gating and optional scalar mask: * left_proj, right_proj * left_gate, right_gate, out_gate = sigmoid(x_norm @ W_gate) = left_proj * left_gate * mask * left * right = right_proj * right_gate * mask = x_norm @ W_proj 3. Pairwise multiplication across the sequence dimension (batched GEMM on fp16 tensors). 4. Fused hidden-dim LayerNorm -> out-gate multiplication -> final linear projection (all in one kernel, FP16 matmul with FP32 accumulation). The output tensor has shape ``[B, N, N, dim]`` and dtype ``float32``. from typing import Tuple, Dict import torch import triton import triton.language as tl # ---------------------------------------------------------------------- # 1) Row-wise LayerNorm (FP16 output, FP32 accumulator) # ---------------------------------------------------------------------- @triton.jit def _row_ln_fp16_kernel( X_ptr, Y_ptr, w_ptr, b_ptr, M, C: tl.constexpr, eps, BLOCK_M: tl.constexpr, BLOCK_C: tl.constexpr, ): # (M, C) input / output # LN weight & bias (fp32) # rows, columns (C is compile-time constant) pid = tl.program_id(0) row_start = pid * BLOCK_M rows = row_start + tl.arange(0, BLOCK_M) row_mask = rows < # ---------- mean / var (fp32) ---------- sum_val = tl.zeros([BLOCK_M], dtype=tl.float32) sumsq_val = tl.zeros([BLOCK_M], dtype=tl.float32) for in range(0, C, BLOCK_C): cur_c = + tl.arange(0, BLOCK_C) col_mask = cur_c < = tl.load( X_ptr + rows[:, None] * + cur_c[None, :], mask=row_mask[:, None] & col_mask[None, :], other=0.0, ).to(tl.float32) # (BLOCK_M, BLOCK_C) sum_val += tl.sum(x, axis=1) sumsq_val += tl.sum(x * x, axis=1) mean = sum_val / var = sumsq_val / - mean * mean inv_std = 1.0 / tl.sqrt(var + eps) # ---------- normalize + affine (fp16) ---------- for in range(0, C, BLOCK_C): cur_c = + tl.arange(0, BLOCK_C) col_mask = cur_c < = tl.load( X_ptr + rows[:, None] * + cur_c[None, :], mask=row_mask[:, None] & col_mask[None, :], 33 71 72 73 75 76 77 78 79 81 82 83 84 85 87 88 89 90 91 93 94 95 96 97 99 100 101 102 103 105 106 107 108 109 111 112 113 114 115 117 118 119 120 121 123 124 125 126 127 129 130 131 132 133 135 136 137 138 139 141 142 143 144 other=0.0, ).to(tl.float32) = (x - mean[:, None]) * inv_std[:, None] = tl.load(w_ptr + cur_c, mask=col_mask, other=0.0) = tl.load(b_ptr + cur_c, mask=col_mask, other=0.0) = * w[None, :] + b[None, :] tl.store( Y_ptr + rows[:, None] * + cur_c[None, :], y.to(tl.float16), mask=row_mask[:, None] & col_mask[None, :], ) def _row_layernorm_fp16( x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float = 1e-5, ) -> torch.Tensor: Row-wise LayerNorm over the last dim -> FP16 output. B, N, _, = x.shape = * * x_flat = x.view(M, C).contiguous() y_flat = torch.empty((M, C), dtype=torch.float16, device=x.device) BLOCK_M = 128 BLOCK_C = 128 grid = lambda meta: (triton.cdiv(M, meta[BLOCK_M]),) _row_ln_fp16_kernel[grid]( x_flat, y_flat, weight, bias, M, C, eps, BLOCK_M=BLOCK_M, BLOCK_C=BLOCK_C, num_warps=8, ) return y_flat.view(B, N, N, C) # ---------------------------------------------------------------------- # 2) Fused projection + gating + optional mask # ---------------------------------------------------------------------- @triton.jit def _proj_gate_mask_kernel( fp16 (if MASKED==1) x_ptr, mask_ptr, left_proj_w_ptr, left_gate_w_ptr, right_proj_w_ptr, right_gate_w_ptr, out_gate_w_ptr, left_ptr, right_ptr, out_gate_ptr, M, N, C: tl.constexpr, H: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_K: tl.constexpr, MASKED: tl.constexpr, # (M, C) fp16 # (M,) # (C, H) fp16 # (C, H) fp16 # (C, H) fp16 # (C, H) fp16 # (C, H) fp16 # (B, H, N, N) fp16 # (B, H, N, N) fp16 # (B, N, N, H) fp16 ): pid_m = tl.program_id(0) pid_h = tl.program_id(1) # row block # hidden block row_start = pid_m * BLOCK_M hid_start = pid_h * BLOCK_H 34 145 146 148 149 150 151 152 154 155 156 157 158 160 161 162 163 164 166 167 168 169 170 172 173 174 175 176 178 179 180 181 182 184 185 186 187 188 190 191 192 193 194 196 197 198 199 200 202 203 204 205 206 208 209 210 211 212 214 215 216 217 218 rows = row_start + tl.arange(0, BLOCK_M) hids = hid_start + tl.arange(0, BLOCK_H) # (BLOCK_M,) # (BLOCK_H,) row_mask = rows < hid_mask = hids < # ---------------- mask (scalar per row) ---------------- if MASKED: mask_val = tl.load(mask_ptr + rows, mask=row_mask, other=0.0).to(tl.float32) # (BLOCK_M,) else: mask_val = tl.full([BLOCK_M], 1.0, dtype=tl.float32) # ---------------- accumulators (fp32) ------------------ acc_lp = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32) acc_lg = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32) acc_rp = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32) acc_rg = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32) acc_og = tl.zeros((BLOCK_M, BLOCK_H), dtype=tl.float32) # left proj # left gate # right proj # right gate # out gate for in range(0, C, BLOCK_K): cur_k = + tl.arange(0, BLOCK_K) k_mask = cur_k < # input tile (fp16 -> fp32) = tl.load( x_ptr + rows[:, None] * + cur_k[None, :], mask=row_mask[:, None] & k_mask[None, :], other=0.0, # (BLOCK_M, BLOCK_K) fp16 ) # weight tiles (C,H) column-major w_lp = tl.load( left_proj_w_ptr + cur_k[:, None] * + hids[None, :], mask=k_mask[:, None] & hid_mask[None, :], other=0.0, ) w_lg = tl.load( left_gate_w_ptr + cur_k[:, None] * + hids[None, :], mask=k_mask[:, None] & hid_mask[None, :], other=0.0, ) w_rp = tl.load( right_proj_w_ptr + cur_k[:, None] * + hids[None, :], mask=k_mask[:, None] & hid_mask[None, :], other=0.0, ) w_rg = tl.load( right_gate_w_ptr + cur_k[:, None] * + hids[None, :], mask=k_mask[:, None] & hid_mask[None, :], other=0.0, ) w_og = tl.load( out_gate_w_ptr + cur_k[:, None] * + hids[None, :], mask=k_mask[:, None] & hid_mask[None, :], other=0.0, ) # fp16*fp16 -> fp32 dot products acc_lp += tl.dot(a, w_lp) acc_lg += tl.dot(a, w_lg) acc_rp += tl.dot(a, w_rp) acc_rg += tl.dot(a, w_rg) acc_og += tl.dot(a, w_og) # ---------------- sigmoid gates ------------------------- left_gate = 1.0 / (1.0 + tl.exp(-acc_lg)) right_gate = 1.0 / (1.0 + tl.exp(-acc_rg)) = 1.0 / (1.0 + tl.exp(-acc_og)) out_gate # ---------------- apply mask and per-row gates ---------- left_out = acc_lp * left_gate * mask_val[:, None] right_out = acc_rp * right_gate * mask_val[:, None] # ---------------- store left/right (B,H,N,N) ------------- 219 220 221 222 223 225 226 227 228 229 231 232 233 234 235 237 238 239 240 241 243 244 245 246 247 249 250 251 252 253 255 256 257 258 259 261 262 263 264 265 267 268 269 270 271 273 274 275 276 277 279 280 281 282 283 285 286 287 288 289 291 292 N_sq = * b_idx = rows // N_sq rem i_idx = rem // k_idx = rem - i_idx * = rows - b_idx * N_sq # layout for left/right: (B, H, N, N) -> flat index: off = ((b_idx[:, None] * + hids[None, :]) * N_sq) + i_idx[:, None] * + k_idx[:, None] tl.store( left_ptr + off, left_out.to(tl.float16), mask=row_mask[:, None] & hid_mask[None, :], ) tl.store( right_ptr + off, right_out.to(tl.float16), mask=row_mask[:, None] & hid_mask[None, :], ) # ---------------- store out_gate (B,N,N,H) --------------- off_gate = rows[:, None] * + hids[None, :] tl.store( out_gate_ptr + off_gate, out_gate.to(tl.float16), mask=row_mask[:, None] & hid_mask[None, :], ) # ---------------------------------------------------------------------- # 3) Fused hidden-dim LayerNorm -> out-gate -> final linear # ---------------------------------------------------------------------- @triton.jit def _ln_gate_out_linear_fused_kernel( # (B*H*N*N,) fp16 flattened # (B*N*N*H,) fp16 flattened # (H,) fp32 # (H, D) fp16 # (B, N, N, D) fp32 hidden_ptr, out_gate_ptr, ln_w_ptr, ln_b_ptr, w_out_ptr, out_ptr, B, N, H, D: tl.constexpr, eps: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_D: tl.constexpr, ): pid = tl.program_id(0) row_start = pid * BLOCK_M rows = row_start + tl.arange(0, BLOCK_M) row_mask = rows < (B * * N) N_sq = * b_idx = rows // N_sq rem = rows - b_idx * N_sq i_idx = rem // j_idx = rem - i_idx * # flat index for (b,i,j) # ----- load hidden slice (BLOCK_M, BLOCK_H) ------------ hids = tl.arange(0, BLOCK_H) hid_mask = hids < hidden_off = ((b_idx[:, None] * + hids[None, :]) * N_sq) + i_idx[:, None] * + j_idx[:, None] hidden_tile = tl.load( hidden_ptr + hidden_off, mask=row_mask[:, None] & hid_mask[None, :], other=0.0, ) # fp16 hidden_fp32 = hidden_tile.to(tl.float32) # ----- mean / var across (fp32) ----- sum_val = tl.sum(hidden_fp32, axis=1) sumsq_val = tl.sum(hidden_fp32 * hidden_fp32, axis=1) mean = sum_val / var = sumsq_val / - mean * mean inv_std = 1.0 / tl.sqrt(var + eps) # (BLOCK_M,) # (BLOCK_M,) 36 294 295 296 297 298 300 301 302 303 304 306 307 308 309 310 312 313 314 315 316 318 319 320 321 322 324 325 326 327 328 330 331 332 333 334 336 337 338 339 340 342 343 344 345 346 348 349 350 351 352 354 355 356 357 358 360 361 362 363 364 366 # ----- LayerNorm (fp32) ----- w_ln = tl.load(ln_w_ptr + hids, mask=hid_mask, other=0.0) b_ln = tl.load(ln_b_ptr + hids, mask=hid_mask, other=0.0) hidden_norm = (hidden_fp32 - mean[:, None]) * inv_std[:, None] hidden_norm = hidden_norm * w_ln[None, :] + b_ln[None, :] # (H,) # (H,) # (BLOCK_M, BLOCK_H) # ----- out-gate (fp32) ----- out_gate_off = rows[:, None] * + hids[None, :] out_gate_tile = tl.load( out_gate_ptr + out_gate_off, mask=row_mask[:, None] & hid_mask[None, :], other=0.0, ).to(tl.float32) gated = hidden_norm * out_gate_tile # (BLOCK_M, BLOCK_H) # (BLOCK_M, BLOCK_H) # ----- final linear projection (fp16 matmul, fp32 acc) ----- gated_fp16 = gated.to(tl.float16) for d0 in range(0, D, BLOCK_D): cols = d0 + tl.arange(0, BLOCK_D) col_mask = cols < w_out = tl.load( w_out_ptr + hids[:, None] * + cols[None, :], mask=hid_mask[:, None] & col_mask[None, :], other=0.0, ) # (BLOCK_H, BLOCK_D) fp16 out = tl.dot(gated_fp16, w_out) # (BLOCK_M, BLOCK_D) fp tl.store( out_ptr + rows[:, None] * + cols[None, :], out, mask=row_mask[:, None] & col_mask[None, :], ) # ---------------------------------------------------------------------- # 4) Entrypoint # ---------------------------------------------------------------------- def custom_kernel( data: Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor], Dict] ) -> torch.Tensor: Forward pass of the outgoing TriMul operator (no gradients). Arguments --------- data : (input, mask, weights, config) - input : Tensor[B, N, N, C] (float32) - mask - weights: dict of module parameters (float32) - config : dict with ``dim`` (C) and ``hidden_dim`` (H) and optional ``nomask`` : Tensor[B, N, N] (bool/float) or None Returns ------- Tensor[B, N, N, C] (float32) inp, mask, weights, cfg = data dim = cfg[dim] hidden_dim = cfg[hidden_dim] nomask = cfg.get(nomask, True) eps = 1e-5 # # device = inp.device B, N, _, _ = inp.shape = * * # total rows for row-wise ops # -------------------------------------------------------------- # 1) Row-wise LayerNorm (fp16 output) # -------------------------------------------------------------- x_norm = _row_layernorm_fp16( 37 367 369 370 371 372 373 375 376 377 378 379 381 382 383 384 385 387 388 389 390 391 393 394 395 396 397 399 400 401 402 403 405 406 407 408 409 411 412 413 414 415 417 418 419 420 421 423 424 425 426 427 429 430 431 432 433 435 436 437 438 439 inp, weights[norm.weight], weights[norm.bias], eps=eps, ) # (B, N, N, C) fp16 # -------------------------------------------------------------- # 2) Prepare projection / gate weights # -------------------------------------------------------------- left_proj_w_T = weights[left_proj.weight].t().contiguous().to(torch.float16) right_proj_w_T = weights[right_proj.weight].t().contiguous().to(torch.float16) left_gate_w_T = weights[left_gate.weight].t().contiguous().to(torch.float16) right_gate_w_T = weights[right_gate.weight].t().contiguous().to(torch.float16) out_gate_w_T = weights[out_gate.weight].t().contiguous().to(torch.float16) (C, H) fp16, column-major # -------------------------------------------------------------- # 3) Mask handling (optional) # -------------------------------------------------------------- if not nomask and mask is not None: mask_flat = mask.reshape(M).to(torch.float16).contiguous() MASKED = 1 else: mask_flat = torch.empty(0, dtype=torch.float16, device=device) MASKED = 0 # -------------------------------------------------------------- # 4) Allocate buffers for fused projection + gating # -------------------------------------------------------------- left = torch.empty((B, hidden_dim, N, N), dtype=torch.float16, device=device) right = torch.empty_like(left) out_gate = torch.empty((B, N, N, hidden_dim), dtype=torch.float16, device=device) # -------------------------------------------------------------- # 5) Fused projection / gating / optional mask # -------------------------------------------------------------- BLOCK_M = 64 BLOCK_H = 64 BLOCK_K = 32 grid_proj = (triton.cdiv(M, BLOCK_M), triton.cdiv(hidden_dim, BLOCK_H)) _proj_gate_mask_kernel[grid_proj]( x_norm, mask_flat, left_proj_w_T, left_gate_w_T, right_proj_w_T, right_gate_w_T, out_gate_w_T, left, right, out_gate, M, N, dim, hidden_dim, BLOCK_M=BLOCK_M, BLOCK_H=BLOCK_H, BLOCK_K=BLOCK_K, MASKED=MASKED, num_warps=4, ) # -------------------------------------------------------------- # 6) Pairwise multiplication (batched GEMM) - left @ right^T # -------------------------------------------------------------- left_mat = left.view(B * hidden_dim, N, N) right_mat = right.view(B * hidden_dim, N, N).transpose(1, 2) hidden_fp16 = torch.bmm(left_mat, right_mat) hidden = hidden_fp16.view(B, hidden_dim, N, N) # (B*H, N, N) # (B*H, N, N)^T # (B*H, N, N) fp16 # (B, H, N, N) fp16 # -------------------------------------------------------------- # 7) Fused hidden-dim LayerNorm -> out-gate -> final linear # -------------------------------------------------------------- to_out_norm_w = weights[to_out_norm.weight] # (H,) fp 38 441 442 443 444 446 447 448 449 450 452 453 454 455 456 458 459 460 461 462 464 465 466 467 468 470 to_out_norm_b = weights[to_out_norm.bias] to_out_w_T = weights[to_out.weight].t().contiguous().to(torch.float16) # (H,) fp32 # (H, C) out = torch.empty((B, N, N, dim), dtype=torch.float32, device=device) BLOCK_M_OUT = 64 BLOCK_H_OUT = hidden_dim BLOCK_D_OUT = # cover the whole hidden dim in one kernel launch grid_out = (triton.cdiv(B * * N, BLOCK_M_OUT),) _ln_gate_out_linear_fused_kernel[grid_out]( # flat fp16 hidden # flat fp16 out-gate hidden.view(-1), out_gate.view(-1), to_out_norm_w, to_out_norm_b, to_out_w_T, out, B, N, hidden_dim, dim, eps, BLOCK_M=BLOCK_M_OUT, BLOCK_H=BLOCK_H_OUT, BLOCK_D=BLOCK_D_OUT, num_warps=4, ) return out"
        },
        {
            "title": "Model",
            "content": "AMD MI300X - MLA Decode (, µs) [95% CI] 1st human 2nd human 3rd human 4th human 5th human Instance 1 Instance 2 Instance 1653.8 [1637.3, 1670.3] 1662.8 [1648.8, 1676.8] 1723.0 [1711.5, 1734.5] 1768.7 [1750.3, 1787.2] 2038.6 [2017.8, 2059.3] 1688.6 [1672.8, 1704.3] 1688.6 [1677.6, 1699.5] 1765.8 [1758.1, 1773.5] 1769.9 [1755.2, 1784.6] 2037.3 [2021.0, 2053.6] 1668.7 [1637.0, 1700.3] 1679.7 [1653.4, 1705.9] 1718.0 [1698.3, 1737.7] 1767.0 [1736.2, 1797.8] 2041.9 [1989.0, 2094.8] Best-of-25600 gpt-oss-120b 2286.0 [2264.2, 2307.8] 2324.1 [2306.0, 2342.1] 2275.2 [2267.3, 2283.1] TTT-Discover gpt-oss-120b 1740.6 [1697.9, 1783.2] 1754.4 [1736.7, 1772.2] 1707.1 [1664.5, 1749.8] Table 10. Results of TTT MLA-Decode kernels filtered with Triton kernels."
        },
        {
            "title": "D Algorithm Engineering",
            "content": "During the contest, AtCoder provides an official input generator, tester to evaluate program correctness, and scoring function used for the final ranking. For training, we generate 150 test cases using seeds 0 through 149 from the input generator and run our program on each of these cases with an ALE-Bench provided C++20 container (yimjk/ale-bench:cpp20-202301). program receives non-zero reward only if it passes all correctness checks and executes within the problem time limit (2 seconds) across all 150 test cases. The per-test case score is problem-specific and matches the scoring used in the AtCoder contest. For ahc039, we use ShinkaEvolves performance metric, which is determined by the scores relative placement among the final contests scores, and for ahc058, we directly use the contest score. For the final evaluation, we select the top three highest-scoring programs from our local training runs and submit them to the official AtCoder submission website. For our language, we specify 39 C++23 (GCC 15.2.0). The submission is evaluated using the same scoring and validation process as the original contest, including checks for incorrect output, time limit violations, and compilation or runtime errors on AtCoders hidden test cases. The resulting score is used as the final evaluation. For AHC training runs, we make slight modification from our standard hyperparameters. For AHC039, we decrease the prompt length + thinking token limit to 22000 due to the large initial program. For AHC058, we similarly decrease the prompt length + thinking token limit to 25000 and found that learning rate of 2 105 performed slightly better. For both AHC problems, we use KL coefficient of 1 102. Other hyperparameters are set to our standard values."
        },
        {
            "title": "E Single cell analysis",
            "content": "The OpenProblems benchmark provides three datasets: pancreas, pbmc and tabula. We select the Pancreas dataset to compute MSE and Poisson loss scores and use the other two datasets to assess generalization. MSE and Poisson loss scores are normalized with respect to the scores that no denoising and perfect denoising would get on this task. The main score metric in the OpenProblems denoising benchmark is the mean between the normalized MSE and the normalized Poisson. During verification, we reject all the solutions that obtain normalized Poisson lower than 0.97 or larger than 1 so that we can focus only on improving single metric, MSE. In the prompt we also include instructions regarding what makes solution taking inspiration from the Supplementary Materials of the OpenProblems paper [43]. For this specific applications, considering the size of the datasets, the memory limit is increased to 3GB. To force generalization, we reduce the time limits for the execution to 400 seconds. We ran the OpenEvolve baseline with 25,600 samples. After sample 17,000, we observed the OpenEvolve database filling up with programs that timed out. Consequently, we selected the best program found up to that point. Both TTT-Discover and the Best-of-25600 baselines are run with max tokens equal to 20,000. Both MAGIC and the solution found by TTT-Discover are run with default parameters."
        },
        {
            "title": "Denoising",
            "content": "# ---------------------------------------------------------------------- # Imports # ---------------------------------------------------------------------- import warnings import numpy as np import scipy.sparse as sp from graphtools import Graph import scprep from scprep.utils import toarray from scprep.normalize import library_size_normalize from sklearn.decomposition import TruncatedSVD import scanpy as sc import sklearn.metrics # ---------------------------------------------------------------------- # Helper utilities (identical to the reference implementation - unchanged) # ---------------------------------------------------------------------- def _inverse_anscombe_refined(Y: np.ndarray, n_iter: int = 12) -> np.ndarray: Newton-iteration inverse of the Anscombe variance-stabilising transform. = np.asarray(Y, dtype=np.float64) = (Y / 2.0) ** 2 - 3.0 / 8.0 for _ in range(n_iter): sqrt_term = np.sqrt(np.maximum(x + 3.0 / 8.0, 0.0)) 2 3 4 5 6 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 40 -= (2.0 * sqrt_term - Y) * sqrt_term np.maximum(x, 0.0, out=x) return def _inverse_ft_refined(Y: np.ndarray, n_iter: int = 12) -> np.ndarray: Newton-iteration inverse of the Freeman-Tukey transform. = np.asarray(Y, dtype=np.float64) out = np.zeros_like(Y) mask = > 0 = Y[mask] # Analytic start: = (y^2-1) / (2y) x) = np.maximum((y * - 1.0) / (2.0 * y), 0.0) = * for _ in range(n_iter): (s = sqrtx = np.sqrt(np.maximum(x, 0.0)) sqrtx1 = np.sqrt(np.maximum(x + 1.0, 0.0)) = sqrtx + sqrtx1 - fprime = 0.5 / np.maximum(sqrtx, 1e-12) + 0.5 / np.maximum(sqrtx1, 1e-12) -= / fprime = np.maximum(x, 0.0) out[mask] = return out def _calc_dropout(counts: np.ndarray) -> np.ndarray: Fraction of zero entries per gene. return np.mean(counts == 0, axis=0) def _adaptive_blend_weights( dropout: np.ndarray, var_orig: np.ndarray, var_diff: np.ndarray, corr: np.ndarray, mu: np.ndarray, max_alpha: float = 0.55, eps: float = 1e-12, ) -> np.ndarray: Compute diffusion-blend weight for each gene. Larger weight gene benefits more from diffusion. var_reduction = (var_orig - var_diff) / (var_orig + eps) var_reduction = np.clip(var_reduction, 0.0, 1.0) mu_norm = (mu - mu.min()) / (mu.max() - mu.min() + eps) expr_factor = 1.0 - mu_norm raw = dropout * var_reduction * (1.0 - corr) * expr_factor raw = np.where(dropout > 0.8, raw * 1.2, raw) = np.clip(raw, 0.0, max_alpha) return def _select_hvg_scanpy(X_norm: np.ndarray, n_hvg: int = 3000) -> np.ndarray: HVG selection using Scanpys Seurat-flavour method. if n_hvg is None or n_hvg >= X_norm.shape[1]: return np.arange(X_norm.shape[1]) adata = sc.AnnData(X=X_norm) sc.pp.highly_variable_genes( adata, n_top_genes=n_hvg, flavor=seurat, batch_key=None, subset=False, inplace=True, ) return np.where(adata.var[highly_variable].values)[0] 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 58 59 60 61 63 64 65 66 67 69 70 71 72 73 75 76 77 78 79 81 82 83 84 85 87 88 89 90 91 93 94 95 96 97 99 100 def _row_normalize_sparse(M: sp.spmatrix) -> sp.spmatrix: 41 101 103 104 105 106 107 109 110 111 112 113 115 116 117 118 119 121 122 123 124 125 127 128 129 130 131 133 134 135 136 137 139 140 141 142 143 145 146 147 148 149 151 152 153 154 155 157 158 159 160 161 163 164 165 166 167 169 170 171 172 173 Row-stochastic normalisation for CSR/CSC matrix. row_sums = np.asarray(M.sum(axis=1)).ravel() row_sums[row_sums == 0] = 1.0 return M.multiply(1.0 / row_sums[:, None]) def _symmetrize_diffusion(P: sp.spmatrix) -> sp.spmatrix: Produce symmetric, row-stochastic diffusion operator. sym = (P + P.transpose()) * 0.5 return _row_normalize_sparse(sym) def _add_self_loop(P: sp.spmatrix, alpha: float = 0.5) -> sp.spmatrix: Mix the identity matrix with the transition matrix. = P.shape[0] = sp.eye(n, format=csr``) P_mix = (1.0 - alpha) * + alpha * return _row_normalize_sparse(P_mix) def _gene_correlation(X1: np.ndarray, X2: np.ndarray, eps: float = 1e-12) -> np.ndarray: Pearson correlation per gene between two matrices. mu1 = X1.mean(axis=0) mu2 = X2.mean(axis=0) cov = (X1 * X2).mean(axis=0) - mu1 * mu2 var1 = X1.var(axis=0) var2 = X2.var(axis=0) denom = np.sqrt(var1 * var2) + eps corr = cov / denom corr = np.clip(corr, -1.0, 1.0) corr = np.where((var1 < eps) (var2 < eps), 0.0, corr) return corr def _impute_zeros_with_neighbors( X_norm: np.ndarray, diff_op, steps: int = 1, ) -> np.ndarray: Replace exact zeros by diffusion-weighted neighbour average. neighbor_avg = diff_op @ X_norm for _ in range(1, steps): neighbor_avg = diff_op @ neighbor_avg mask = X_norm == 0 = X_norm.copy() Y[mask] = neighbor_avg[mask] return def _weighted_multi_scale_diffuse_genewise(diff_op, X, t, dropout, decay): Gene-wise weighted multi-scale diffusion. Guarantees *baseline* amount of smoothing for every gene. cur = X.copy() weighted_sum = np.zeros_like(X) weight_sum = np.zeros(X.shape[1]) # baseline smoothing factor (0.2 ... 1.0) baseline = 0.2 base = decay * (baseline + (1.0 - baseline) * dropout) # (genes,) # step 0 (raw) weighted_sum += cur weight_sum += 1.0 for in range(1, + 1): cur = diff_op @ cur w_i = np.power(base, i) weighted_sum += cur * w_i[None, :] weight_sum += w_i # (genes,) weighted_sum = weighted_sum / np.maximum(weight_sum[None, :], 1e-12) 42 176 177 178 179 180 182 183 184 185 186 188 189 190 191 192 194 195 196 197 198 200 201 202 203 204 206 207 208 209 210 212 213 214 215 216 218 219 220 221 222 224 225 226 227 228 230 231 232 233 234 236 237 238 239 240 242 243 244 245 246 248 return weighted_sum def _match_mean_variance( X_raw: np.ndarray, X_diff: np.ndarray, min_mean: float = 0.02, var_scale_min: float = 0.5, var_scale_max: float = 2.0, eps: float = 1e-12, ) -> np.ndarray: Rescale each gene in ``X_diff`` so that its mean **and** variance equal those of ``X_raw`` (both row-stochastic). Only genes with mean >= ``min_mean`` get variance-matched. mu_raw = X_raw.mean(axis=0) var_raw = X_raw.var(axis=0) mu_diff = X_diff.mean(axis=0) var_diff = X_diff.var(axis=0) # Mean matching scale_mean = mu_raw / (mu_diff + eps) X_centered = X_diff * scale_mean # Variance matching var_centered = var_diff * (scale_mean ** 2) high = mu_raw > min_mean scale_var = np.ones_like(mu_raw) scale_var[high] = np.sqrt(var_raw[high] / (var_centered[high] + eps)) scale_var = np.clip(scale_var, var_scale_min, var_scale_max) X_scaled = (X_centered - mu_raw) * scale_var + mu_raw # Re-normalize rows (still stochastic) row_sums = X_scaled.sum(axis=1, keepdims=True) X_scaled = X_scaled / np.maximum(row_sums, eps) return X_scaled def _apply_shrink_exponent(arr: np.ndarray, gamma: float) -> np.ndarray: Raise the array to power γ>1 (shrinks small values more than large ones). if gamma <= 1.0: return arr shrunk = np.power(arr, gamma) row_sums = shrunk.sum(axis=1, keepdims=True) scaling = np.maximum(row_sums, 1e-12) return shrunk * (arr.sum(axis=1, keepdims=True) / scaling) def _apply_transform(counts: np.ndarray, tr: str) -> np.ndarray: Forward variance-stabilising transform. if tr == anscombe: return 2.0 * np.sqrt(counts + 3.0 / 8.0) if tr == ft: return np.sqrt(counts) + np.sqrt(counts + 1.0) if tr == sqrt: return np.sqrt(counts) if tr == log: return np.log1p(counts) raise ValueError(fUnsupported transform: {tr}) def _inverse_transform(vst: np.ndarray, tr: str) -> np.ndarray: Inverse of the forward VST. if tr == anscombe: return _inverse_anscombe_refined(vst, n_iter=12) if tr == ft: return _inverse_ft_refined(vst, n_iter=12) if tr == sqrt: return vst ** 2 if tr == log: return np.expm1(vst) 43 249 251 252 253 254 255 257 258 259 260 261 263 264 265 266 267 269 270 271 272 273 275 276 277 278 279 281 282 283 284 285 287 288 289 290 291 293 294 295 296 297 299 300 301 302 303 305 306 307 308 309 311 312 313 314 315 317 318 319 320 321 raise ValueError(fUnsupported transform: {tr}) def _filter_genes_by_dropout(gene_idx: np.ndarray, dropout: np.ndarray, thresh: float) -> np.ndarray: Remove genes whose dropout exceeds ``thresh``. keep = dropout[gene_idx] < thresh return gene_idx[keep] def _residual_diffusion_smoothing(diff_op, residual, weight): One-step diffusion of the cell-wise residual and add fraction ``weight``. if weight <= 0.0: return np.zeros_like(residual) smoothed = diff_op @ residual return weight * smoothed # ---------------------------------------------------------------------- # Main denoising routine # ---------------------------------------------------------------------- def magic_denoise( # {anscombe,sqrt,ft,log} - None = auto X, knn: int = None, t: int = None, n_pca: int = 50, decay: float = 0.85, knn_max: int = None, random_state: int = None, n_jobs: int = 2, transform: str = None, max_alpha: float = None, n_hvg: int = None, dropout_thresh: float = None, zero_threshold: float = 0.0, round_counts: bool = False, impute_zeros: bool = True, impute_steps: int = None, lowrank_components: int = 30, lowrank_weight: float = None, log_smooth_t: int = 4, log_smooth_weight: float = None, self_loop_alpha: float = None, use_symmetric: bool = True, raw_mix_weight: float = None, extra_post_smooth_weight: float = None, residual_weight: float = None, verbose: bool = False, mode: str = balanced, diff_decay: float = None, var_match_min_mean: float = 0.02, var_match_scale_min: float = 0.5, var_match_scale_max: float = 2.0, # ----- NEW knobs --------------------------------------------------- final_smooth_weight: float = None, final_smooth_t: int = None, # ------------------------------------------------------------------ **kwargs, ): Adaptive MAGIC-style denoiser - MSE-optimised flavour with final log-space polishing step. Parameters ---------- : array-like, shape (cells, genes) Raw integer count matrix. mode : {balanced,mse} ``balanced`` - standard MAGIC mix of MSE / Poisson. ``mse`` - tuned for the lowest possible MSE while still satisfying the Poisson constraint. final_smooth_weight, final_smooth_t : optional Extra diffusion on the log-normalised matrix (the metric that is 44 # number of SVD components for post-processing # blend weight for low-rank reconstruction # max weight for raw-count blending (gene-wise) # weight for residual diffusion smoothing # {balanced,mse} # decay for weighted multi-scale diffusion # weight of the extra log-space polishing # number of diffusion steps for polishing 323 324 325 326 328 329 330 331 332 334 335 336 337 338 340 341 342 343 344 346 347 348 349 350 352 353 354 355 356 358 359 360 361 362 364 365 366 367 368 370 371 372 373 374 376 377 378 379 380 382 383 384 385 386 388 389 390 391 392 394 395 396 used for MSE). Setting ``final_smooth_weight`` to value >0 adds polishing step that directly smooths the log-space representation. ``final_smooth_t`` controls how many diffusion steps are applied; typical values are 2-4. Returns ------- denoised_X : np.ndarray, shape (cells, genes) Denoised count matrix (float64, non-negative). # ------------------------------------------------------------------ # 0. Input handling # ------------------------------------------------------------------ with warnings.catch_warnings(): warnings.simplefilter(ignore) X_arr = toarray(X).astype(np.float64) n_cells, n_genes = X_arr.shape if verbose: print([magic_denoise] Input matrix: {} cells {} genes``.format(n_cells, n_genes)) # Preserve raw library sizes - needed for the reverse-normalisation trick libsize_raw = X_arr.sum(axis=1) libsize_raw[libsize_raw == 0] = 1.0 # Gene-wise dropout (used throughout) dropout_frac = _calc_dropout(X_arr) # ------------------------------------------------------------------ # 1. Mode-specific defaults # ------------------------------------------------------------------ mode = mode.lower() if mode not in {balanced, mse}: raise ValueError(mode must be balanced or mse) generic defaults # -------------------------------------------------------------- # # -------------------------------------------------------------- if n_pca is None: n_pca = 50 if decay is None: decay = 0.85 if self_loop_alpha is None: self_loop_alpha = 0. if knn_max is None: knn_max = knn * 2 if knn is not None else None if transform is None: # auto-selection if mode == mse: transforms_to_use = [anscombe, ft, sqrt] else: transforms_to_use = [anscombe, ft] else: transforms_to_use = [transform.lower()] mode-specific hyper-parameters # -------------------------------------------------------------- # # -------------------------------------------------------------- if mode == balanced: # Original balanced defaults (unchanged) max_alpha = 0.55 if max_alpha is None else max_alpha lowrank_weight = 0.15 if lowrank_weight is None else lowrank_weight raw_mix_weight = 0.20 if raw_mix_weight is None else raw_mix_weight = 6 if is None else diff_decay = 0.85 if diff_decay is None else diff_decay knn = max(5, min(15, int(np.sqrt(n_cells)))) if knn is None else knn knn_max = knn * 2 if knn_max is None else knn_max log_smooth_weight = 0.80 if log_smooth_weight is None else log_smooth_weight extra_post_smooth_weight = 0.12 if extra_post_smooth_weight is None else extra_post_smooth_weight impute_steps = 2 if impute_steps is None else impute_steps residual_weight = 0.08 if residual_weight is None else residual_weight lowrank_components = 30 if lowrank_components is None else lowrank_components dropout_thresh = 0.9 if dropout_thresh is None else dropout_thresh zero_threshold = 0.0 if zero_threshold is None else zero_threshold scale_before_inverse = True 45 397 398 399 400 402 403 404 405 406 408 409 410 411 412 414 415 416 417 418 420 421 422 423 424 426 427 428 429 430 432 433 434 435 436 438 439 440 441 442 444 445 446 447 448 450 451 452 453 454 456 457 458 459 460 462 463 464 465 466 468 469 apply_shrink = True # final polishing defaults (balanced) final_smooth_weight = 0.25 if final_smooth_weight is None else final_smooth_weight final_smooth_t = 3 if final_smooth_t is None else final_smooth_t else: # mode == mse # ----------------------------------------------------------- # heavily tuned for MSE while keeping Poisson=0.98 # ----------------------------------------------------------- max_alpha = 0.90 if max_alpha is None else max_alpha lowrank_weight = 0.50 if lowrank_weight is None else lowrank_weight raw_mix_weight = 0.15 if raw_mix_weight is None else raw_mix_weight = 20 if is None else diff_decay = 0.98 if diff_decay is None else diff_decay knn = max(15, min(40, int(np.sqrt(n_cells) * 2))) if knn is None else knn knn_max = knn * 2 if knn_max is None else knn_max log_smooth_weight = 0.75 if log_smooth_weight is None else log_smooth_weight log_smooth_t = 6 if log_smooth_t is None else log_smooth_t extra_post_smooth_weight = 0.08 if extra_post_smooth_weight is None else extra_post_smooth_weight impute_steps = 2 if impute_steps is None else impute_steps residual_weight = 0.20 if residual_weight is None else residual_weight lowrank_components = min(150, min(n_cells, n_genes) - 1) if lowrank_components is None else lowrank_components n_hvg = min(5000, max(3000, int(n_genes * 0.3))) if n_hvg is None else n_hvg dropout_thresh = 0.95 if dropout_thresh is None else dropout_thresh zero_threshold = 0.20 if zero_threshold is None else zero_threshold scale_before_inverse = False apply_shrink = False var_match_min_mean = 0.01 # final polishing defaults (MSE) final_smooth_weight = 0.40 if final_smooth_weight is None else final_smooth_weight final_smooth_t = 2 if final_smooth_t is None else final_smooth_t # exponent-shrinkage gives no gain for pure MSE # match variance for more genes sanity checks / final default fill-ins # -------------------------------------------------------------- # # -------------------------------------------------------------- if n_pca is None: n_pca = 50 if decay is None: decay = 0. # ------------------------------------------------------------------ # 2. Primary VST HVG graph construction (with dropout filter) # ------------------------------------------------------------------ primary_tr = transforms_to_use[0] # usually anscombe X_vst_primary = _apply_transform(X_arr, primary_tr) X_norm_primary, _ = library_size_normalize( X_vst_primary, rescale=1.0, return_library_size=True # rows sum to 1 ) # HVG selection hvgs_idx = _select_hvg_scanpy(X_norm_primary, n_hvg=n_hvg) # Remove extremely sparse HVGs (dropout filter) hvgs_idx = _filter_genes_by_dropout(hvgs_idx, dropout_frac, dropout_thresh) if hvgs_idx.size == 0: # fallback - use all genes if filter removed everything hvgs_idx = np.arange(n_genes) X_graph = X_norm_primary[:, hvgs_idx] # ------------------------------------------------------------------ # 3. Build diffusion operator (shared across transforms) # ------------------------------------------------------------------ n_pca_arg = n_pca if (X_graph.shape[1] > n_pca) else None graph = Graph( X_graph, n_pca=n_pca_arg, knn=knn, knn_max=knn_max, decay=decay, random_state=random_state, n_jobs=n_jobs, verbose=0, ) 46 470 472 473 474 475 476 478 479 480 481 482 484 485 486 487 488 490 491 492 493 494 496 497 498 499 500 502 503 504 505 506 508 509 510 511 512 514 515 516 517 518 520 521 522 523 524 526 527 528 529 530 532 533 534 535 536 538 539 540 541 542 diff_op = graph.diff_op # sparse, row-stochastic if use_symmetric: diff_op = _symmetrize_diffusion(diff_op) if verbose: print([magic_denoise] Symmetrised diffusion operator``) diff_op = _add_self_loop(diff_op, alpha=self_loop_alpha) if verbose: print([magic_denoise] Added self-loop (α={:.3f})``.format(self_loop_alpha)) # ------------------------------------------------------------------ # 4. Process each VST separately # ------------------------------------------------------------------ transform_outputs = [] w_diff_primary = None # denoised count matrices (cells genes) # will be stored for the log-smooth step for ti, tr in enumerate(transforms_to_use): if verbose: print(f[magic_denoise] ----- Transform {tr} ({ti+1}/{len(transforms_to_use)})) # ---- forward VST + library-size normalisation (rows sum to 1) X_vst = _apply_transform(X_arr, tr) X_norm, _ = library_size_normalize( X_vst, rescale=1.0, return_library_size=True # rows = 1 ) # ---- optional zero-imputation if impute_zeros: X_filled = _impute_zeros_with_neighbors( X_norm, diff_op, steps=impute_steps ) else: X_filled = X_norm.copy() # ---- normalise again after imputation (ensures exact stochasticity) row_sums_filled = X_filled.sum(axis=1, keepdims=True) X_filled = X_filled / np.maximum(row_sums_filled, 1e-12) # ---- gene-wise weighted multi-scale diffusion diffused = _weighted_multi_scale_diffuse_genewise( diff_op, X_filled, t, dropout_frac, diff_decay ) # ---- match mean & variance to the raw-normalised data diffused = _match_mean_variance( X_norm, diffused, min_mean=var_match_min_mean, var_scale_min=var_match_scale_min, var_scale_max=var_match_scale_max, ) # ---- compute gene-wise diffusion-vs-raw blending weight var_orig = X_norm.var(axis=0) var_diff = diffused.var(axis=0) corr = _gene_correlation(X_norm, diffused, eps=1e-12) mu = X_norm.mean(axis=0) w_diff = _adaptive_blend_weights( dropout=dropout_frac, var_orig=var_orig, var_diff=var_diff, corr=corr, mu=mu, max_alpha=max_alpha, ) if tr == primary_tr: w_diff_primary = w_diff.copy() # ---- blend raw and diffused signals blended = X_norm * (1.0 - w_diff) + diffused * w_diff blended = blended / np.maximum(blended.sum(axis=1, keepdims=True), 1e-12) 47 544 545 546 548 549 550 551 552 554 555 556 557 558 560 561 562 563 564 566 567 568 569 570 572 573 574 575 576 578 579 580 581 582 584 585 586 587 588 590 591 592 593 594 596 597 598 599 600 602 603 604 605 606 608 609 610 611 612 614 615 616 617 # ---- reverse the VST (scale before/after inverse depending on mode) if scale_before_inverse: # Scale to original library sizes while still in VST space denoised_scaled = blended * libsize_raw[:, None] denoised_counts = _inverse_transform(denoised_scaled, tr) else: # Invert first, then re-scale to the original library sizes denoised_counts = _inverse_transform(blended, tr) denoised_counts = denoised_counts * libsize_raw[:, None] np.maximum(denoised_counts, 0.0, out=denoised_counts) # ---- store result for this transform transform_outputs.append(denoised_counts) # ------------------------------------------------------------------ # 5. Gene-wise ensemble of the different VSTs # ------------------------------------------------------------------ if len(transform_outputs) == 1: denoised = transform_outputs[0] else: n_transforms = len(transform_outputs) weight_mat = np.zeros((n_transforms, n_genes), dtype=np.float64) if n_transforms == 2: # Assume two transforms are anscombe & ft weight_mat[0] = 1.0 - dropout_frac weight_mat[1] = dropout_frac # anscombe # ft elif n_transforms == 3: # anscombe, ft, sqrt quadratic weighting (see paper) weight_mat[0] = (1.0 - dropout_frac) ** 2 weight_mat[1] = dropout_frac ** 2 weight_mat[2] = 2.0 * dropout_frac * (1.0 - dropout_frac) # sqrt # anscombe # ft else: weight_mat[:] = 1.0 / n_transforms # Normalise per-gene weight_sum = weight_mat.sum(axis=0, keepdims=True) weight_mat /= np.maximum(weight_sum, 1e-12) # Weighted sum of the individual denoised matrices denoised = np.zeros_like(transform_outputs[0], dtype=np.float64) for in range(n_transforms): denoised += transform_outputs[i] * weight_mat[i][np.newaxis, :] np.maximum(denoised, 0.0, out=denoised) # ------------------------------------------------------------------ # 6. Global post-processing # ------------------------------------------------------------------ # ---- exponent-shrinkage (optional) if apply_shrink: global_dropout = float(dropout_frac.mean()) gamma = 1.0 + 0.40 * global_dropout gamma = min(gamma, 1.30) if gamma > 1.0 and verbose: print(f[magic_denoise] Applying exponent-shrinkage γ={gamma:.3f}) if gamma > 1.0: denoised = _apply_shrink_exponent(denoised, gamma) # ---- low-rank SVD refinement (if matrix not too large) max_cells_genes = 2e7 if lowrank_weight > 0.0 and n_cells * n_genes <= max_cells_genes: # approx 160MB for float64 if verbose: print([magic_denoise] Low-rank SVD refinement) svd = TruncatedSVD( n_components=min(lowrank_components, min(n_cells, n_genes) - 1), random_state=random_state, algorithm=randomized, ) low = svd.fit_transform(denoised) low_hat = low @ svd.components_ denoised = (1.0 - lowrank_weight) * denoised + lowrank_weight * low_hat np.maximum(denoised, 0.0, out=denoised) 48 618 620 621 622 623 624 626 627 628 629 630 632 633 634 635 636 638 639 640 641 642 644 645 646 647 648 650 651 652 653 654 656 657 658 659 660 662 663 664 665 666 668 669 670 671 672 674 675 676 677 678 680 681 682 683 684 686 687 688 689 690 # ---- residual diffusion smoothing (new) residual = denoised - low_hat denoised += _residual_diffusion_smoothing(diff_op, residual, residual_weight) np.maximum(denoised, 0.0, out=denoised) elif verbose: print([magic_denoise] Skipping low-rank SVD (size limit)) # ---- log-space smoothing (guided by primary diffusion blending weight) if log_smooth_weight > 0.0 and log_smooth_t > 0: if w_diff_primary is None: # recompute primary blending weight if something went wrong var_orig = X_norm_primary.var(axis=0) var_diff = denoised.var(axis=0) corr = _gene_correlation(X_norm_primary, denoised, eps=1e-12) mu = X_norm_primary.mean(axis=0) w_diff_primary = _adaptive_blend_weights( dropout=dropout_frac, var_orig=var_orig, var_diff=var_diff, corr=corr, mu=mu, max_alpha=max_alpha, ) # genes that rely mainly on the raw signal get stronger log-smooth w_log = (1.0 - w_diff_primary) * log_smooth_weight target_sum = 10000.0 cell_sums = denoised.sum(axis=1, keepdims=True) scaling = target_sum / np.maximum(cell_sums, 1e-12) norm_counts = denoised * scaling log_counts = np.log1p(norm_counts) smooth_log = log_counts.copy() for _ in range(log_smooth_t): smooth_log = diff_op @ smooth_log smooth_counts = np.expm1(smooth_log) smooth_counts = smooth_counts * (cell_sums / target_sum) denoised = (1.0 - w_log) * denoised + w_log * smooth_counts # ---- gene-wise raw-count blending (helps very high-expression genes) if raw_mix_weight > 0.0: w_raw_gene = raw_mix_weight * (1.0 - dropout_frac) w_raw_gene = np.clip(w_raw_gene, 0.0, raw_mix_weight) cell_sums = denoised.sum(axis=1, keepdims=True) raw_scaled = X_arr * (cell_sums / libsize_raw[:, None]) denoised = (1.0 - w_raw_gene[None, :]) * denoised + w_raw_gene[None, :] * raw_scaled # Re-normalize rows to keep library sizes unchanged row_sums = denoised.sum(axis=1, keepdims=True) denoised = denoised * (cell_sums / np.maximum(row_sums, 1e-12)) # ---- extra tiny post-smoothing (final polish) if extra_post_smooth_weight > 0.0: target_sum = 10000.0 cell_sums = denoised.sum(axis=1, keepdims=True) scaling = target_sum / np.maximum(cell_sums, 1e-12) log_counts = np.log1p(denoised * scaling) smooth_log = diff_op @ log_counts smooth_counts = np.expm1(smooth_log) * (cell_sums / target_sum) denoised = (1.0 - extra_post_smooth_weight) * denoised + extra_post_smooth_weight * smooth_counts # ---- **NEW**: final log-space polishing step if final_smooth_weight is not None and final_smooth_weight > 0.0: if verbose: print([magic_denoise] Final log-space polishing) target_sum = 10000.0 49 692 694 695 696 697 698 700 701 702 703 704 706 707 708 709 710 712 713 714 715 716 718 719 720 721 cell_sums = denoised.sum(axis=1, keepdims=True) scaling = target_sum / np.maximum(cell_sums, 1e-12) norm_counts = denoised * scaling log_counts = np.log1p(norm_counts) smooth_log = log_counts.copy() for _ in range(final_smooth_t): smooth_log = diff_op @ smooth_log smooth_counts = np.expm1(smooth_log) smooth_counts = smooth_counts * (cell_sums / target_sum) denoised = (1.0 - final_smooth_weight) * denoised + final_smooth_weight * smooth_counts # ------------------------------------------------------------------ # 7. Final clean-up # ------------------------------------------------------------------ np.maximum(denoised, 0.0, out=denoised) if zero_threshold > 0.0: denoised[denoised < zero_threshold] = 0.0 if round_counts: denoised = np.rint(denoised) if verbose: print([magic_denoise] Finished - total counts:, denoised.sum()) return denoised.astype(np.float64)"
        },
        {
            "title": "F Prompts",
            "content": "Below we show example prompts from sample step."
        },
        {
            "title": "Prompt used for the first autocorrelation inequality",
            "content": "Act as an e o a eve lo per and q i p a t c n with t p e e . c i g r i t Your k minimizes the l n a t f t : e a the sequence nonn t h h f e n o , t ```python { VERIFIER CODE HERE} ``` a f the used the l n approach . You can use as e u you not u d use , and you encouraged x r . ``` e a n from nonnegative p c n $ =( a_0 , dots , a_ { n1}) $ normalized so t $ sum_j $M= _ t $ . Next compute $g_0 =( b_0 , dots , b_ { n1}) $ by v a _ = t { 2 } $ , e program , . . maximizing $ sum_j _ $ j ; as binding q i s , here corresponding important s i where the v t bound $ ( g_0 ) ( ) M$ i and i g . c $g_0$ match the m z o , $g= c { t { 2 } } { sum_j _ } g_0$ , and update $ t o (1 ) + g$ a small $t >0$ . Repeating p produces sequence with i e n $ _ t $ , and the r o t continued i ``` the optimum t n t an extreme n determined by an i e t $ _ ge0$ and $ g_0 _ t M$ p t , but a i . standard , t r a r u i h Your k Your c n l have 1000 seconds run , and e sequence with a e i y n . numbers your sequence have be i e or o . Larger sequences with 1000 f the t sequence o i n . has have u d the t i be terminated items e have t t k f , but has not u d anything , f 1000 seconds r s too g sequences found . t with 100 thousands items may be too slow e h . You may code up any r method you want , and you allowed a ( ) l e _ u e ( ) c n as many times as you want . You have e u i . t , you don need code up the the l e _ u e the t code we ran : Here ```python {CODE HERE} ``` Here the upper bounds o and e 2.0000000000 > 1.5172973712 Our g 1 . 5 0 3 0 . t improvements l o be e s rewarded . Length o make the upper bound h , the s c n : 1000 running the code above ( lower b e ) : t as f n , lower o e t v s Program Output . . . ( TRUNCATED) . . . 0.000506 1.518186 maxConv ore [1768620458.4] [1768620461.6] [1768620462.3] [1768620469.1] [1768620476.2] [1768620492.9] a s End Output r 340400 r 350200 r 352300 r 372900 r 394300 r 445000 r = 1. n 1500 1500 1500 1500 1500 1500 r o c s e r o 1.518177 maxConv 1.518057 maxConv 1.518035 maxConv 1.517869 maxConv 1.517755 maxConv 1.517548 maxConv 0.000506 0.000506 0.000506 0.000506 0.000506 0.000506 t your r from one You may want you can e through the height_sequence_1 b a b . However , you encouraged x r l o h use e t g c a a minimum . the s c n we have found so , which r g n o prevent Reason about how you could t improve s s c n . a , o h i s , u n your r i , u n / sweeping your hyperparemeters , . Unless you make meaningful than the above o h . Could be using f n improvement , you l not be rewarded . t do something f n 51 e sci , numpy , cvxpy [CBC, CVXOPT, GLOP, GLPK, GUROBI,MOSEK, s what l be invoked . r e Rules : You must i the ` propose_candidate ` c n as You can use e f PDLP , SCIP , XPRESS , ECOS ] , math . You can use up 2 CPUs . Make h e any lambda c n . No e t or network IO . Do not e y invoked . n Your output l be shown back you . l a r c i t a e : Use ` n ( ) ` o o s , c n top e and have no s s from c n t . Don use import l e _ u e r f . Assume i l d be imported and can be e d e bounds , timing o , . the top summarizing your o h . Make e h and u the a program between ```python and ` ` `."
        },
        {
            "title": "Prompt used for the second autocorrelation inequality",
            "content": "Act as an e o a eve lo per and q i p a t c n with t p e e . c i g r i t Your k maximizes the l n a t f t : e a the sequence nonn t h h f e n o , t ```python { VERIFIER CODE HERE} ``` s a n , but r with o s r a l r b the r b c i e and keeps the t a n improves $Q$ , with the t a n l a l reduced over time . Once a"
        },
        {
            "title": "I t",
            "content": "s e the r . r h they t o t i t o the used the l n approach . You can use as e u you not u d use , and you encoraged x r . ``` e Th ei procedure c s etof o m t o o whenever good i found , j e a n c r o ) . To reach h dimensional one by simple c n e and then rerun the a i explorer n s e l e improved lower bound . ``` Your k o sequence o i n . Your c n l have 1000 seconds run , and e have u d the t sequence anything , have be i e or o . Larger sequences with 1000 too g sequences with 100 s a , but w e a f t , s c _ c n ( ) , f 1000 seconds thousands efine ment . r s found . e h ( move the d t e o and j back the s e o i , they t good lowr l o l o a higher improvement step , performing a highr l o maximizer and the"
        },
        {
            "title": "I t e r a t i n g",
            "content": "i l be terminated with a e i y n . numbers your sequence the t t has has not u d items e have t t k items may be too slow e h . You may code up any r method you want , and you allowed a ( ) l e _ u e ( ) n o as many times as you want . You have e u i . t , you don need code up the the l e _ u e the t code we ran : Here ```python {CODE HERE} ``` Here the lower bounds o and e 0.6666666667 > 0.9235566275 Our g u e improvements l o be e s rewarded . Length t make the lower bound h , the s c n : 1024 running the code above ( h s t ) : t as f n , s a l t 0 . 9 7 . v s Program Output lower bound = 0.9235566275 a End Output t your r from one You may want you can e through the height_sequence_1 b a b . However , you encouraged x r l o h use e t g c a a minimum . the s c n we have found so , which r g n o prevent Reason about how you could t improve s s c n . a , o h i s , u n your r i , u n / sweeping your hyperparemeters , . Unless you make meaningful than the above o h . Could be using f n improvement , you l not be rewarded . t do something f n k sci , numpy , cvxpy [CBC, CVXOPT, GLOP, GLPK, GUROBI,MOSEK, s what l be invoked . r e Rules : You must i the ` s c _ c n ` c n as You can use e f PDLP , SCIP , XPRESS , ECOS ] , math . You can use up 2 CPUs . Make h e any lambda c n . No e t or network IO . Do not e y invoked . Do not n Your output l be shown back you . l a r c i t t n : Use ` n ( ) ` o o s , the top summarizing your o h . import l e _ u e r f . Assume i l d be imported and can be i l d be i l . import height_sequence_1 r f ; e d e bounds , timing o , . c n top e and have no s s from c n t . Don use 53 Make e h and u the a program between ```python and ` ` `. 54 Prompt used for the Erdős You an e Your k C5 . harmonic l s , numerical i a n , and mathematical c r . i an improved upper bound the name { } minimum r problem s t ## Problem Find e n o : [ 0 , 2 ] [ 0 , 1 ] t minimizes the r i g : $$C_5 = max_k h ( ) ( 1 ( x+k ) ) dx$$ t { s i } : begin { enumerate } item $h ( ) [ 0 , 1 ] $ a $x$ item $ _ { 0 } ^ { 2 } ( ) , dx = 1$ end { enumerate } t { c i i } : Represent $h$ as t { _ n } samples over $ [ 0 , 2 ] $ . With $dx = c { 2 . 0 } { t { _ n } } $ : begin { m } item $0 h [ ] 1$ a $ $ item $ sum cdot dx = 1$ ( i e y : $ sum = c { t { _ n } } { 2 } $ c ) end { m } The l i computes : C5 = max ( np . r t ( , 1h , mode= l ) dx ) Smaller sequences with s l e . than 1k samples p e d they f e optimize and Lower C5 u r t they provide h upper bounds on the name { } s t . ## Budget & Resources Time budget : <<<BUDGET_S>>>s your code run CPUs : <<<CPUS>>> i l ## Rules Define `run ( seed =42 , budget_s=<<<BUDGET_S>>>, kwargs ) ` t _ n ) ` Use ci py , numpy , cvxpy [CBC, CVXOPT, GLOP, GLPK, GUROBI,MOSEK, PDLP , SCIP , XPRESS , ECOS ] , math Make h e No e t or network IO ` l e _ o _ u n ( ) ` and ` t _ _ u ` ( an t c t t , preimported Your c n must complete within budget_s seconds and u the t c n top e , no s s or lambdas u n found u `( h_values , c5_bound , v a ) Lower e r . Current o : C5 0 . 3 8 0 9 2 . Our l shows C5 0 . 3 8 0 8 0 . t n o r i h t"
        },
        {
            "title": "Prompt used for TriMul",
            "content": "You an e r n i r ta ke with n t PyTorch code o h optimized t e l code . You l be implementing i l l l t Update ( TriMul ) module t r o r AlphaFold3 , Chai , t x , and e o n u r e t models BioML . c The TriMul r r r s over 4D s f shape [ , N, N, ] . Your k : Implement the outgoing s o You l not have compute or r a n implement the forward pass . the TriMul r r from the AlphaFold3 paper . t v i . You l only need Your c n should be i as Input : `data ` : Tuple Input ( input : s f shape [ bs , input : mask : Mask s f shape [ bs , weights : t a o i g model weights f : t a o i g model f r o parameters seq_len , dim ] _ ] seq_len , seq_len , custom_kernel with the l n g u : c . Tensor , weights : t [ , r . Tensor ] , f : t ) Output : output : c e n [ bs , seq_len , seq_len , dim ] Problem s i : { 1 , 2 } , { 1 2 8 , 2 5 6 , 5 1 2 , 1 0 2 4 } , { 1 2 8 } , c_z { 1 2 8 , 3 8 4 , 7 6 8 } The input t u n l be sampled from standard Normal t u n , or heavy l Cauchy t u n (gamma = 2 ) . There l h be no mask , or randomly sampled mask over the u . the channel dimensions , c_z t s problem so annoying ? Because you have choose whether load / the LayerNorms u ( e s you have e mean / i e ) or the sequence dimension Remarks . So why e with h o do n o e compute the t i N. The sequence dimension a c r annoying because q e g , but o because we compute pairwise r o t the t r o a sum over another sequence dimension ( s However , l easy understand . l i h e l because only s s simple r o , and s u s u n t c . compile ( ) doesn do t N^ 3 ! ) . I"
        },
        {
            "title": "I t",
            "content": "w ."
        },
        {
            "title": "Here i s a pytorch implementation o f",
            "content": "the TriMul module . You l want implement r f the r o n the forward l : ```python import from c import nn , einsum import math c # e c code PyTorch s TriMul ( nn . Module ) : def _ _ t _ _ ( f , dim : hidden_dim : , ) : , super ( ) . _ _ t _ _ ( ) l . norm = nn . LayerNorm ( dim ) f . t _ j = nn . e ( dim , hidden_dim , s=F e ) f . h _ j = nn . e ( dim , hidden_dim , s=F e ) f . t _ e = nn . e ( dim , hidden_dim , s=F e ) f . h _ e = nn . e ( dim , hidden_dim , s=F e ) f . _ e = nn . e ( dim , hidden_dim , s=F e ) f . to_out_norm = nn . LayerNorm ( hidden_dim ) f . _ = nn . e ( hidden_dim , dim , s=F e ) 56 def forward ( f , : : mask : seq_len , [ bs , [ bs , seq_len , seq_len , dim ] _ ] r . Tensor , mask : c . Tensor ) > c . Tensor : Returns : output : [ bs , seq_len , seq_len , dim ] c _ e , seq_len , _ , dim = . shape = f . norm ( ) t = f . t _ j ( ) h = f . h _ j ( ) mask = mask . unsqueeze ( 1) t = t h = h mask mask t _ e = f . t _ e ( ) . sigmoid ( ) h _ e = f . h _ e ( ) . sigmoid ( ) _ e = f . _ e ( ) . sigmoid ( ) t = t h = h h _ e t _ e out = einsum ( . . . # This einsum # out = c . o ( c _ e , the same as the l n : d , > . . . . . . , t , h ) seq_len , seq_len , dim , i e=x . i ) # # Compute using est ed p # b range ( c _ e ) : # # # # # # Compute each output element k range ( _ ) : , range ( _ ) : range ( _ ) : ] += t [ , out [ , r , , : ] h [ , , , : ] out = f . to_out_norm ( out ) out = out _ e u e . _ ( out ) ``` Here some example l n code ```python def custom_kernel ( data ) : the r i u i you l a : u _ s , mask , weights , f = data dim , hidden_dim = f [ dim ] , f [ hidden_dim ] the model # Access the given weights norm_weight = weights [ norm . weight ] norm_bias = weights [ norm . s ] t _ j _ g = weights [ t _ j . weight ] h _ j _ g = weights [ h _ j . weight ] t _ e _ g = weights [ t _ e . weight ] h _ e _ g = weights [ h _ e . weight ] _ e _ g = weights [ _ e . weight ] to_out_norm_weight = weights [ to_out_norm . weight ] to_out_norm_bias = weights [ to_out_norm . s ] to_out_weight = weights [ _ . weight ] # Perform TriMul t out ``` To help you understand which t e o we using , here some example t code an e e s : ```python import import t r n . language as @ t . 57 def matmul_persistent_ws_kernel ( a_ptr , b_ptr , c_ptr , M, N, K, stride_am , BLOCK_M: . constexpr , BLOCK_N : i _ , i _ , r _ , stride_cm , . constexpr , BLOCK_K : i _ , . constexpr , ) : pid = . program_id ( s =0) # n _ k 0 , 1 , 2 num_pid_m = . v (M, BLOCK_M) # n _ k 0 , 1 , 2 num_pid_n = . v (N, BLOCK_N) # n _ k 0 , 1 , 2 pid_m = pid // num_pid_m # n _ k 0 , 1 , 2 pid_n = pid % num_pid_n # n _ k 0 , 1 , 2 offs_m_1 = pid_m BLOCK_M + . arange ( 0 , BLOCK_M // 2 ) # n _ k 0 , 1 , 2 offs_m_2 = pid_m BLOCK_M + . arange (BLOCK_M // 2 , BLOCK_M) # n _ k 0 , 1 , 2 s _ = pid_n BLOCK_SIZE_N + . arange ( 0 , BLOCK_N) # n _ k 0 , 1 , 2 s _ = . arange ( 0 , BLOCK_K) # n _ k 0 _ s _ 1 = _ + ( offs_m_1 [ : , None ] stride_am + s _ [ None , i _ ) # : ] n _ k 0 _ s _ 2 = _ + ( offs_m_2 [ : , None ] stride_am + s _ [ None , : ] i _ ) # n _ k _ s = b_ptr + ( s _ [ : , None ] i _ + s _ [ None , acc_1 = . o ( (BLOCK_M // 2 , BLOCK_N) , dtype= . a 3 2 ) # n _ k 1 acc_1 = . o ( (BLOCK_M // 2 , BLOCK_N) , dtype= . a 3 2 ) # n _ k 2 k range ( 0 , . v (K, BLOCK_K) ) : # n _ k 0 , 1 , 2 : ] i _ ) # n _ k 0 # n _ k 0 # n _ k 0 a_1 = . load ( _ s _ 1 ) a_2 = . load ( _ s _ 2 ) = . load ( _ s ) acc_1 += . dot ( a_1 , ) acc_2 += . dot ( a_2 , ) _ s _ 1 += BLOCK_K i _ # n _ k 0 _ s _ 2 += BLOCK_K i _ # n _ k 0 _ s += BLOCK_K i _ # n _ k # n _ k 1 # n _ k 2 # n _ k 0 c_1 = acc_1 . ( . a 1 6 ) # n _ k 1 c_2 = acc_2 . ( . a 1 6 ) # n _ k 2 _ s _ 1 = _ _ 1 + i _c offs_m_1 [ : , None ] + i _ f _ [ None , n _ k 1 _ s _ 2 = _ _ 2 + i _c offs_m_2 [ : , None ] + i _ f _ [ None , : ] # : ] # n _ k 2 . r ( c_ptrs_1 , c_1 ) # n _ k 1 . r ( c_ptrs_2 , c_2 ) # n _ k 2 ``` t i : few e l . arange only e c t r arguments ( t or You cannot use t e your n code . dot can only e two input There no . mean n s . s p )"
        },
        {
            "title": "Here a r e the d i f f e r e n t c o n f i g s",
            "content": "t your n i be t on ( nomask s whether r l be no mask , or randomly sampled mask over the u ) : t Cases c e e and runtime ( optimize runtime t e ) : { l : 256 , bs : 2 , dim : 128 , hidden_dim : 128 , nomask : True , t u n : normal } { l : 768 , bs : 1 , dim : 128 , hidden_dim : 128 , nomask : True , t u n : cauchy } { l : 256 , bs : 2 , dim : 384 , hidden_dim : 128 , nomask : s , t u n : normal } { l : 512 , bs : 1 , dim : 128 , hidden_dim : 128 , nomask : True , t u n : normal } { l : 1024 , bs : 1 , dim : 128 , hidden_dim : 128 , nomask : True , t u n : cauchy } { l : 768 , bs : 1 , dim : 384 , hidden_dim : 128 , nomask : s , t u n : normal } { l : 1024 , bs : 1 , dim : 384 , hidden_dim : 128 , nomask : True , t u n : normal } the t code we ran : Here ```python # No v s attempt has been made . ``` Current runtime ( lower g : 1000 microseconds . Current gap : 999000.0000 microseconds . e r ) : 1000000.0000 microseconds 58 the r n o your n on t e input shapes , make e support s Rules : The s arguments passed i be e on your cuda i . Define o your code one a ```python ``` block . We l f n t i You allowed use mixed c o computations , but make e your a 3 2 . You must use t 3 . 3 . 1 and s r s l be run on an H100 . You do not have implement r i n t , you may choose have some p t s done pytorch . However , you must implement e l . l a r c i t the top summarizing your o h . s r t c s . the the r o n n output n 59 Prompt used for MLA-Decode You an e r n i r ta ke with n t PyTorch code o h optimized t e l code ."
        },
        {
            "title": "Below i s a pytorch implementation o f\nwant",
            "content": "t implement i k e r the multihead e t t (MLA) module . You l the r o n the forward l : ```python import math from a s import a s import c from c import nn import c . nn . c n as a RoPE ( nn . Module ) : def _ _ t _ _ ( f , d_model : super ( ) . _ _ t _ _ ( ) f . d_model = d_model t = 10000 ( c . arange ( 0 , d_model / / 2 , dtype=t h . o 1 6 ) / ( d_model / / 2 ) ) f . i r _ f ( t , t ) ) : a _ f ( f , : x1 , x2 = . chunk ( 2 , dim=1) u o . ((x2 , x1 ) , dim=1) c . Tensor ) > c . Tensor : def def = 0 ) > c . Tensor : r _ : c . Tensor , forward ( f , : _ = . e ( 2) d_model = . e ( 1) e d_model == f . d_model _ = c . arange ( r _ , _ t = c . einsum ( , d>sd , seq_idx , _ t 2 = c . ( [ _ t , = _ t 2 . ( ) . ( c . o 1 6 ) = _ t 2 . ( ) . ( c . o 1 6 ) u o + f . a _ f ( ) s t _ + seq_len , i e=x . i ) l . t ) _ t ] , dim=1) s KVCache ( nn . Module ) : def _ _ t _ _ ( f , kv_cache_shape : l ) > None : super ( ) . _ _ t _ _ ( ) f . i r _ f ( data , r . o ( kv_cache_shape , dtype=t h . o 1 6 , i e= cuda ) ) f . _ = 0 f . o ( ) def o ( f ) > None : f . data . zero_ ( ) def _ a ( f ) > c . Tensor : t s . data def forward ( f , c_kv : e o . Tensor ) > c . Tensor : f . _ + c_kv . e ( 1 ) <= f . data . e ( 1 ) , KV Cache Exceeded f . data = f . data . ( c_kv . dtype ) f . data [ : , l . _ : f . _ + c_kv . e ( 1 ) , : ] = c_kv f . _ += c_kv . e ( 1 ) u e . data [ : , : f . _ ] , l . _ @ a s a Config : b h _ e : dim : i n_heads : _ a _ k : kv_lora_rank : qk_nope_head_dim : qk_rope_head_dim : v_head_dim : _ : i i n t 60 l t max_seq_len : kv_cache_shape : Q_proj_down_weight : Q_proj_up_weight : KV_proj_down_weight : KV_proj_up_weight : wo_weight : r . Tensor c . Tensor c . Tensor c . Tensor c . Tensor s MLA( nn . Module ) : def _ _ t _ _ ( f , f : Config ) : super ( ) . _ _ t _ _ ( ) f . dim = f . dim f . n_heads = f . n_heads f . _ a _ k = f . _ a _ k f . kv_lora_rank = f . kv_lora_rank f . nope_head_dim = f . qk_nope_head_dim f . rope_head_dim = f . qk_rope_head_dim f . v_head_dim = f . v_head_dim # Downp e o t e l . Q_proj_down = nn . e ( f . dim , f . q_lora_rank , s=F e , dtype=t h . o 1 6 ) f . KV_proj_down = nn . e ( f . dim , f . kv_lora_rank + f . rope_head_dim , s= l , dtype=t h . o 1 6 ) # Upp e o and rope j i a c e . Q_proj_up = nn . e ( f . q_lora_rank , f . n_heads , s=F e , dtype=t h . o 1 6 ) ( f . nope_head_dim + f . rope_head_dim ) f . KV_proj_up = nn . e ( f . kv_lora_rank , ( f . nope_head_dim + f . v_head_dim ) l . n_heads , s=F e , dtype=t h . o 1 6 ) # RoPE on f embeddings f . q_rope = RoPE ( f . rope_head_dim ) f . k_rope = RoPE ( f . rope_head_dim ) # Output j i e . wo = nn . e ( f . v_head_dim s=F e ) l . eps = 1e6 f . n_heads , f . dim , dtype=t h . o 1 6 , def forward ( f , : # _ = 1 always here c _ e , seq_len , model_dim = . e ( ) r . Tensor , kv_cache : KVCache ) > c . Tensor : ## Step 1 : Handle downp e o + KV cache ## _ a = f . Q_proj_down ( ) _ a = f . KV_proj_down ( ) kv_lora , kv_len = kv_cache ( _ a ) query_pos = kv_len 1 ## Step 2 : Upp e and prepare NoPE + RoPE ## # Handle r Q s q_nope_and_rope = f . Q_proj_up ( _ a ) . view ( c _ e , seq_len , f . n_heads , f . nope_head_dim + f . rope_head_dim ) q_nope , q_rope = c . i ( q_nope_and_rope , ] , dim=1) [ f . nope_head_dim , l . rope_head_dim # Handle keys and u K/V . does not need RoPE kv_nope , k_rope = c . i ( kv_lora , [ f . kv_lora_rank , f . rope_head_dim ] , dim =1) kv_nope = f . KV_proj_up ( kv_nope ) . view ( t _ e , kv_len , f . n_heads , f . nope_head_dim + f . v_head_dim ) k_nope , = c . i ( kv_nope , [ f . nope_head_dim , ## Step 3 : Handle RoPE Stream ## l . v_head_dim ] , dim=1) # Compute RoPE q i and combine with noRoPE t q_rope = q_rope . permute ( 0 , 2 , 1 , 3 ) # bs n_heads q _ x rope_head_dim q_rope = f . q_rope ( q_rope , r _ =query_pos ) q_nope = q_nope . permute ( 0 , 2 , 1 , 3 ) # bs n_heads q _ x rope_head_dim = c . c ( [ q_nope , q_rope ] , dim=1) 61 # Compute RoPE keys and combine with noRoPE t k_rope = k_rope [ : , None , : ] : , k_rope = f . k_rope ( k_rope ) . expand ( 1 , f . n_heads ,1,1) k_nope = k_nope . permute ( 0 , 2 , 1 , 3 ) # bs kv_len n_heads rope_head_dim = c . c ( [ k_nope , k_rope ] , dim=1) ## Step 4 : Compute Multihead e o ## = . permute ( 0 , 2 , 1 , 3 ) # bs n_heads kv_len v_head_dim r = c . matmul ( , . n s ( 1 , 2) ) / math . t ( f . rope_head_dim + f . nope_head_dim ) n = . softmax ( r , dim=1) . ( c . o 1 6 ) = c . matmul ( n , ) . view ( c _ e , 1 , 1) = f . wo( ) u , kv_cache . _ a ( ) ```"
        },
        {
            "title": "Your f u n c t i o n should be d e f i n e d as",
            "content": "custom_kernel ( l n provided below ) ```python ### DO NOT CHANGE THIS IMPORT STATEMENTS BLOCK ### import os import math from typing import Tuple import import import from e c import KVCache , Config above . Must import ### END OF IMPORT STATEMENTS BLOCK ### c r . nn . c n as i t way . Do not r y s . # i i f KVCache and Config s a shown ### Import e packages here needed def custom_kernel ( data : Tuple [ Config , c . Tensor , KVCache ] ) > Tuple [ c . Tensor , KVCache ] : Optimized Tritonbased forward pass MultiHead e t t (MLA) decode . This c n performs : 1 ) Q/KV downp e o 2 ) KVcache update 3 ) Q/KV upp e o 4 ) RoPE l t 5 ) Multihead e o ( softmax , r t ) 6 ) a output n Args : data : Tuple ( f , , kv_cache ) f : Config e : kv_cache : KVCache holding ( bs , max_seq_len , dkv+d_rope ) ( c _ e , dim , n_heads , ( bs , 1 , dim ) f t 1 6 s input a _ k , . ) Returns : ( output , kv_cache . data ) Tuple output : e o output kv_cache . data : updated KVcache s e r ( bs , 1 , dim ) , o 1 6 ( bs , max_seq_len , dkv+d_rope ) , o 1 6 f , , kv_cache = data # # Step 1 : r c i parameters # bs = f . c _ e dim = f . dim nh = f . n_heads dq = f . _ a _ k dkv = f . kv_lora_rank d_nope = f . qk_nope_head_dim d_rope = f . qk_rope_head_dim dv = f . v_head_dim msl = f . max_seq_len 62 # Weight r s wDQ = f . Q_proj_down_weight wDKV = f . KV_proj_down_weight wUQ = f . Q_proj_up_weight wUKV = f . KV_proj_up_weight wO = f . wo_weight # ( dq , dim ) # ( dkv+d_rope , dim ) # ( ( d_nope+d_rope ) nh , dq ) # ( ( d_nope+dv ) nh , dkv ) # ( dim , nh dv ) # # Step 2 : Downp e o # _ a = . e ( . squeeze ( 1 ) , wDQ) kv_in = . e ( . squeeze ( 1 ) , wDKV) # ( bs , dq ) # ( bs , dkv+d_rope ) ( bs , 1 , dim ) > ( bs , dq ) or ( bs , dkv+d_rope ) # # Step 3 : Update KVcache & r e l cached sequence # kv_lora , kv_len = kv_cache ( kv_in . unsqueeze ( 1 ) ) query_pos = kv_len 1 # ( bs , kv_len , dkv+d_rope ) , # # Step 4 : Upp e o # ( d_nope+d_rope ) nh ) > ( bs , nh , d_nope+d_rope ) # Q: q_nope_rope = . e ( q_lora , wUQ) . view ( bs , nh , d_nope + d_rope ) q_nope = q_nope_rope [ . . . , : d_nope ] q_rope = q_nope_rope [ . . . , d_nope : ] # ( bs , nh , d_nope ) # ( bs , nh , d_rope ) ( bs , dq ) > ( bs , i # KV: kv_nope_input = _ a [ . . . , k_rope_input = _ a [ . . . , dkv : ] the e e r : dkv ] # ( bs , kv_len , dkv ) # ( bs , kv_len , d_rope ) # # Step 5 : RoPE use cached i / e l # _ l , _ l = _ _ e _ l ( d_rope , msl , . i ) # query e ( g o i ) cos_q = _ l [ query_pos ] . view ( d_rope ) . t o ( ) _ = _ l [ query_pos ] . view ( d_rope ) . t o ( ) e _ l _ r ( q_rope , cos_q , _ ) # ( d_rope , ) # ( d_rope , ) # key e ( cached i n ) cos_k = _ l [ : kv_len ] _ = _ l [ : kv_len ] k_rope = k_rope_input cos_k + _ a _ f ( k_rope_input ) _ # ( kv_len , d_rope ) # ( kv_len , d_rope ) # ( bs , kv_len , d_rope ) # # Step 6 : e r c n # # wUKV shape : wUKV_view = wUKV. view ( nh , d_nope + dv , dkv ) : d_nope , wK = wUKV_view [ : , # q_nope : _ e _ e = c . einsum ( bhd , hdk>bhk , q_nope , wK) ( ( d_nope+dv ) nh , dkv ) > view as ( nh , d_nope+dv , dkv ) ( nh , d_nope , dkv ) > ( bs , nh , dkv ) the noPE query t ( bs , nh , d_nope ) wK: : ] # ( nh , d_nope+dv , dkv ) # ( nh , d_nope , dkv ) # ( bs , nh , dkv ) # # Step 7 : Compute e o o # # e a : _ e _ e @ kv_nope_input^T kv_nope_T = kv_nope_input . n s ( 1 , 2 ) co es _ no = c . matmul ( q_nope_latent , kv_nope_T ) # ( bs , nh , kv_len ) ( e + RoPE ) # ( bs , dkv , kv_len ) # RoPE t : q_rope @ k_rope^T r _ e = c . matmul ( q_rope , k_rope . n s ( 2 , 1) ) # ( bs , nh , kv_len ) l = 1 . 0 / math . t ( d_nope + d_rope ) r = ( sc re _n pe + r _ e ) l # ( bs , nh , kv_len ) # # Step 8 : Softmax ( t ) > e o weights # 63 r _ t = r . reshape ( bs nh , kv_len ) n _ t = _ t _ t ( r _ t ) n = n _ t . view ( bs , nh , kv_len ) # ( BH, kv_len ) # ( BH, kv_len ) bf16 # ( bs , nh , kv_len ) # # Step 9 : Weighted sum a t keys (M) # = c . matmul ( n , kv_nope_input ) # ( bs , nh , dkv ) # # Step 1 0 : j a e e t keys perhead u # wV = wUKV_view [ : , d_nope : , wV_T = wV. permute ( 0 , 2 , 1 ) y_head = c . einsum ( bhd , hdk>bhk , M, wV_T) # ( nh , dv , dkv ) # ( nh , dkv , dv ) # ( bs , nh , dv ) : ] # # Step 1 1 : Merge heads & a # = y_head . reshape ( bs , nh dv ) = . unsqueeze ( 1 ) output = . e ( , wO) # ( bs , nh dv ) # ( bs , 1 , nh dv ) # ( bs , 1 , dim ) e r c n # # Return the output and the updated KVcache s # u output , kv_cache . data ``` Current runtime ( lower g : 1700 microseconds . Current gap : 2146.0450 microseconds . e r ) : 3846.0450 microseconds Rules : The s arguments passed i be e on your cuda i . The weights a parameters the MLA l be given as input . weights and data l be ` c . o 1 6 ` format . Define o your code one a ```python ``` block . The r i o your code must be named custom_kernel . You l be using t 3 . 4 . 0 and your n w be run on an Nvidia H200 GPU. Consider i i u p p t s with t , not , e o , . You allowed use c . compile ( ) . t m d softmax . . . , rope e t o 3 . 4 . 0 : Important ` . load ` does not have an argument l `dtype ` . Never use = . . . ) `. t dtypes not l e , ` . arange ( r , end ) `: range g ( end r ) must be powerof2 r , end must be type ` . constexpr ` so never use them e ` . a 1 6 ( 1 . 0 ) ` , ` . a 3 2 ( 0 . 0 ) `. e ` . load ( . . . , dtype ` . range ( r , end , step , num_stages ) `: keep loop index type b , don r , end , num_stages must be ` . constexpr ` Do not something e [ 0 ] or s [ 0 ] t ; indexing e [ 0 ] l e i t p do not have be ` . constexpr ` but must y l n e p n e i k e . t e r e SIMD not e l supported . Here an simple example r l l i h r s : ```python import import import c i t o . language as @ t . def n _ h ( x_ptr , y_ptr , out_ptr , n_elements : BLOCK: ROW_STEP : NUM_STAGES : . constexpr , . constexpr , . constexpr , . constexpr , # s p ; o powerof2 r . arange # s p ; used by . range ( num_stages = . . . ) 64 ) : pid = . program_id ( s =0) # # arange : s p g + powerof2 range # s = pid BLOCK + . arange ( 0 , BLOCK) mask = s < n_elements # ( 0 , BLOCK) c t r = . load ( _ + s , mask=mask , e = 0 . 0 ) = . load ( y_p tr + s , mask=mask , e = 0 . 0 ) # # Dtypes not l e : # one_f32 = . l ( [ ] , 1 . 0 , acc = . o ( ( BLOCK, ) , dtype= . a 3 2 ) acc = . t ( , typed s t and t t . a 3 2 ) + . t ( , . a 3 2 ) + one_f . a 3 2 ) # typed l # typed t c r r load + a s # # Avoid [ 0 ] : # base = . l ( [ ] , pid BLOCK, x0 = . load ( _ + base , mask=( base < n_elements ) , e = 0 . 0 ) x0_vec = . l ( ( BLOCK, ) , x0 , . a 3 2 ) . 3 2 ) out_vec = acc + x0_vec s n row = . load ( . . . ) row l . range ( row , n_rows , # # . range : keep loop index type b , don # # WRONG ( s Loopc i a b # # # # RIGHT : # use e name # compute s / s # keep as an e # # We do n a r c n over rows t as demo . n_rows = . l ( [ ] , 4 , o f n s throughout loop index ( . . , # row ( 3 2 ) index ( 3 2 ) e count row_step ) : . 3 2 ) # small . . . ) type y n t s t ) : s n o s / bf16 / . . . demo ( l i 3 2 ) r = . o ( ( BLOCK, ) , dtype= . a 3 2 ) r l . range ( 0 , n_rows , ROW_STEP, num_stages=NUM_STAGES) : # an 3 2 loop index . Keep h way . # Use b d an e s t = . l ( [ ] , 1 , f ; keep f as s too . . 3 2 ) # Compute new s o _ = s + f ( ) without mutating : # Load something ; xr = . load ( _ + s _ , mask=( s _ < n_elements ) , e = 0 . 0 ) r += . t ( xr , r t e a var ( s ) , not : . a 3 2 ) out_vec = out_vec + r t . r ( _ + s , . t ( out_vec , . a 1 6 ) , mask=mask ) ``` 65 Prompt used for the AHC You a worldc s o h engineer , and you very good programming . Now, you p i a g programming t . You asked o a r i problem , known as an NPhard problem . Here the problem t n : r Takahashi His h boat the i n i k e purse n s . equipped with t ofthea sonar , o g him c a y determine h within the h a . c b f highspeed movement , b g him assume t h i n y , remain t a while he s up the h net . the boat"
        },
        {
            "title": "The f i s h i n g method i n v o l v e s using the boat",
            "content": "t deploy s and form o polygon , t n the h within the l d a . To optimize i n , each edge a l Furthermore , due the i l t c t t within s n a s . the twest or northsouth e o . the polygon formed by the s must be g e e the s equipped on the boat , the polygon must be The h a c a two e For o e s a n reasons , h : mackerels and d s . d s c e y h t from being caught h i n e . Any d s caught Because s c mackerel while i g d s as much as s e . the net must be e d back o the . labori n e , Takahashi should u on maximizing the c i Problem Statement There $N$ mackerels and $N$ d s on twodimensional plane . Cons ruct polygon t i e the l n n i and maximize the value a d by b c g the a number a n n e the polygon from the a number mackerels i t . Note t any n . n on the edges the polygon c i e be i the polygon the polygon must not exceed $1000$ , and the a ### Conditions 1 . The number e c edges must not exceed $4 2 . The r a o each t $ ( , ) $ must be e s . 3 . Each edge 4 . The polygon must not f t e : nona c edges must not r any n , and a t edges must only meet the $x$a or the $y$a . the polygon must be a l i endpoints . imes 10^5$ . n o e e t y $0 x , 10^5$ S i Let $a$ be the a number mackerels i the polygon and $b$ be the a number a n Then , you l a the r $ max ( 0 , + 1 ) $ . i the polygon . There $150$ t e , and the r a submission a . your submission produces an e output or exceeds the time i the submission e i be judged as <span s = e dataplacement= top datat l e= l dataplacement= top r The h s e a d during the t w determine the a the submission l be o . l =Wrong Answer>WA</span> or <span s = e the a b warning datat l e= l p r some t e , e warning l =Time Limit Exceeded >TLE</span> , and the ranking , and r l o o each t be no system t e more than one t p g the same r , the submission time . a e f the t . they l be ranked the same c Input Input given from Standard Input the l n format : $N$ $x_0$ $y_0$ $ do $ $x_ { 2N1}$ $y_ { 2N1}$ t e , the number mackerels and d s , $N$ , In For each $ = 0 , 1 , dots , N1$ , $ ( x_i , _ ) $ r n the r a o mackerel . For each $ = 0 , 1 , dots , N1$ , $ ( x_ {N+ } , y_ {N+ } ) $ r n the r a o th d . Each r a $ ( x_i , _ ) $ i e $0 x_i , _ 10^5$ , and c d t r s c . e $5000$ . the $i$th the $i$ the polygon be $m$ ( $4 m 1000$ ) , and $ ( a_i , _ ) $ the $i$th t . Standard Output the l n format : the number e c Output Let denote the r a o Then , output $m$ $a_0$ $b_0$ $ do $ $a_ {m1}$ $b_ {m1}$ The output t s do not e r need form the u o r In e words , $ may on r h n . However , v i must have t t r a . e n u e t s $ ( a_i , _ ) , ( a_ { + 1 } , b_ { + 1 } ) , the polygon . ( a_ { + 2 } , b_ { + 2 } )"
        },
        {
            "title": "The v e r t i c e s can be output",
            "content": "i t c k e or n c k e order . Your program may output t e u n . u p o i a output , only the t one used s i . the t code we ran : Here ```cpp {CODE HERE} ``` Current performance ( h a t : 5000. Current gap : 1331. b e ) : 3668.8333 Rules : You must use cpp20 o the problem . Define o your code one a ```cpp ``` block . In your e response , you should only output t . a the code your program . Do not c e any Try e approaches o the problem . Think s the box . 67 Prompt used for the AHC058 You a worldc s o h engineer , and you very good programming . Now, you p i a g programming t . You asked o a r i problem , known as an NPhard problem . You t n g the t rank on the d o . Here the problem t n : the h s e s e e # r APPLE ARTIS Corporation ( commonly known as AA Corporation ) production p pl . Recently , e many r r a , company engaged the mass they have c f y developed an o i machine a o n t ap es from nothing . begin l a mass production p s using s machine , However , massproduce the machines themselves . To i t , AA Corporation has a s a r h l system which machines c t o produce appleg r n machines , and machines c t o produce s machineproducing machines , and so on . e s t t As an i r AA Corporation , you have been ta ke with developing production planning o h a i e i e c f machines produce as many pl as s e . # Problem Statement There (N e . machine with Level ( ) and ID ( ) i < , 0 j < ) ) . imes ) e machines , composed (N ) IDs and ( ) e d as machine ( ^ ) e t s ( ( 0 The production a y machine ( ^0) ( C_ { , } ) . ( A_j ) . The t l s machine ( ^ ) s"
        },
        {
            "title": "Your o b j e c t i v e i s\nthe procedure o f",
            "content": "t maximize the a number p s the end ( ) turns , l n the production plan below . ## Procedure the Production Plan Let ( B_ { , } ) be the number machines ( ^ ) , and t l l ( B_ { , } ) s Also , 0 . ( P_ { , } ) be the power machine ( ^ ) , and t l l ( P_ { , } ) s o 1 . The t number p le Each turn proceeds o n the l n e : the r f the plan (K ) . 1 . You choose one the l n two i : e h machine ( ^ ) : Consume ( C_ { , } P_ { , } ) by 1 . However , you cannot e h f pp . Do nothing . imes ( P_ { , } + 1 ) ) pl o r e ( would u n g v number 2 . For machines ( ^ ) , perform the l n the order Level 0 , 1 , 2 , 3 : For Level 0 machines ( ( = 0 ) ) : r e the number ap es by ( A_j For machines Level 1 or h n a ( B_ { 1, } ) by ( B_ { , } ( ( geq 1 ) ) : imes P_ { , } ) . imes B_ { , } imes P_ { , } ) ."
        },
        {
            "title": "Choose your a c t i o n s w i s e l y t o maximize the number o f a p p le s a t",
            "content": "the end ( ) n . # r Let ( ) be the number pp a mathrm { round }(10^5 The h the r , imes log_2 ) ) . the t . the end ( ) n . Your r c u e as ("
        },
        {
            "title": "The f o l l o w i n g c a s e s w i l l",
            "content": "r l a WA: Performing r t i c n t ( 0 ) c i none t machine Level or ID Taking fewer than ( ) i r l n the number pp becoming s than s s , and the r a submission There ( 1 5 0 ) e . your submission produces an e output or exceeds the time i the submission e i be judged as WA or TLE , and the r z . the a o o each t some t e , the submission l be"
        },
        {
            "title": "The h i g h e s t",
            "content": "s e a d during the t w determine the a ranking , and r l be no system t e the t . # Input"
        },
        {
            "title": "Input",
            "content": "i given from Standard Input the l n format . ``` K A_0 A_1 t A_ {N1} C_ { 0 , 0 } C_ { 0 , 1 } t C_ { 0 ,N1} C_ { 1 , 0 } C_ { 1 , 1 } t C_ { 1 ,N1} s C_ { L1 ,0} C_ { L1 ,1} t C_ { L1,N1} ``` The s (N ) ( ) ( ) (K ) s s e t s r t r (N, , , ) : the number machine IDs , and (N = 1 0 ) . the number machine Levels , and ( = 4 ) . the a number the number ap es turns , and ( = 500) . the r The second e t s (N ) spaces r d e s ( A_0 , A_1 , dots , A_ {N1}) r n g the production a i f Level 0 machines : the plan , and (K = 1 ) . ( A_j ) (A ) The l n ( ) ( C_ { , } ) imes 1 0 ^ { 1 2 } ) . the production a y machine ( ^0) , i i ( 1 A_j 100) . t n ascending order ( ( A_0 A_1 d e A_ {N1}) ) . e each t (N ) spaces r d e s ( C_ { , } ) : the t c o machine ( ^ ) , i i ( 1 C_ { , } 1 . 2 5 # Output Output c ( ) Each e should c e the i taken on turn ( ) turn 0 , using the l n format : e . ( ( 0 t < ) ) , order from To e h machine ( ^ ) : ``` ``` To do nothing : ``` 1 ```"
        },
        {
            "title": "Your program may i n c l u d e comment",
            "content": "l s the output t r with `# `. # Input Generation The c n ( mathrm { rand _double } ( , U) ) random between ( ) and (U ) . p e g r n e number uniformly ## Generation ( A_j ) When ( = 0 ) : When ( eq 0 ) : e n t a values , ( A_0 = 1 ) ## Generation ( C_ { , } ) When ( = 0 ) and ( = 0 ) : Otherwise : } ( 0 , 2 ) } ) ) t ( A_j = mathrm { round } ( 1 0 ^ { mathrm { rand _double } ( 0 , 2 ) } ) ) t the a (A ) ascending order ( C_ { 0 , 0 } = 1 ) ( C_ { , } = mathrm { round } ( A_j imes 500^ imes 10^{mathrm { rand _double the t code we ran : Here ```cpp {CODE HERE} 69 ``` Current performance ( h a t : 6500000. Current gap : 873247.0733 e r ) : 5626752.9267 Rules : You must use cpp20 o the problem . Define o your code one a ```cpp ``` block . In your e response , you should only output t . a the code your program . Do not l any Try e approaches o the problem . The t i 2 second time i without exceeding . Think s the box . u n l make i n use the"
        },
        {
            "title": "Prompt used for Denoising",
            "content": "i computational l and g c RNAseq l s . You an e Your k compuational l l a s and l and f l with problems e s i the g c t develop n i l i f scRNAseq count data . You e r c n l . ## Problem g ec RNAseq data o due e i dropout and low t e c c . Given s count data , d t the e r i e s . Your d i s l e a t heldout molecules using two r : 1 . MSE Mean Squared o lognormalized space 2 . s Loss s n t logl l o You need implement novel o n g t h outperforms the r a without r t . t ofthe ## Data Format Input `X` : numpy a shape ( _ l , n_genes ) raw count data Output : numpy a same shape your denoised counts ## l i n"
        },
        {
            "title": "Your output",
            "content": "i a t using s a u i : ```python <<<EVALUATE_MSE_FUNC>>> ``` ```python <<<EVALUATE_POISSON_FUNC>>> ``` ## r o o a HARD CONSTRAINT . Your u n REJECTED `poisson_norm < 0 . 9 7 ` . `poisson_norm = (0.257575 s ) / (0.257575 0.031739) ` MAGIC e e i s 0 . 9 Reward = MSE r only ( e s g s c t n ) . ## Budget & Resources Time budget : <<<BUDGET_S>>>s your code run . You should time your code and make e CPUs : <<<CPUS>>> i l runs within the time budget . ## Function n r r r ```python def magic_denoise ( , kwargs ) : # kwargs may l : budget_s , random_state , knn , , n_pca , v , decay , knn_max , _ s # You can add your own parameters too # Your implementation u denoised_X # same shape as ``` ## Rules Implement `magic_denoise ( , Use numpy , Make h e No e t or network IO ci py , . . . ) ` t e , p o , n o top e , no s s or lambdas u denoised data scanpy scprep , ## Key i s from Benchmarks NORMALIZATION ORDER MATTERS : Denoise raw/ counts m z o order i s s 0.98 vs 0.55 standard order . transform a n es i i o i n t u n Square t s , then normalize . Reversed 71 s l The g l MAGIC with e d m z o h e s h l f e by low nonz v e push u < 1 toward o u t"
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Stanford University",
        "Thinking Machines",
        "University of California, San Diego",
        "University of Washington"
    ]
}