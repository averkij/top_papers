{
    "paper_title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?",
    "authors": [
        "Wenzhe Li",
        "Yong Lin",
        "Mengzhou Xia",
        "Chi Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 4 7 6 0 0 . 2 0 5 2 : r Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial? Wenzhe Li*1, Yong Lin*1, Mengzhou Xia1, and Chi Jin 1Princeton University Abstract Ensembling outputs from diverse sources is straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in large number of scenarios: Self-MoA achieves 6.6% improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of 3.8% improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces sequential version of Self-MoA, that is capable of aggregating large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models have made remarkable strides in improving performance across different domains, with notable examples such as GPT [Achiam et al., 2023], Gemini [Team et al., 2023], and Claude [Anthropic, 2023]. Significant efforts have been directed toward increasing model size and training data to boost capabilities. However, scaling at training time comes with steep costs, while scaling computation during inference remains largely underexplored. straightforward way to utilize test-time compute is ensembling, which aims to combine outputs of multiple LLMs [Wang et al., 2024a, Lin et al., 2024, Jiang et al., 2023a, Wang et al., 2024a]. Among various ensembling approaches, Mixture-of-Agents (MoA) [Wang et al., 2024a] has garnered significant interest, achieving superior performance in challenging tasks such as instruction following [Wang et al., 2024a], summarization, data extraction [OpenPipe, 2024], and real-world code issue resolution [Zhang et al., 2024b]. *Equal contribution. Email: {wenzhe.li,yl7690,mengzhou,chij}@princeton.edu. 1 Specifically, MoA first queries multiple LLMs (proposers) to generate responses, and then uses an LLM (aggregator) to synthesize and summarize these responses into high-quality response. Previous research highlights the significance of model diversity within the proposers for optimizing the performance of MoA, primarily focusing on strategies for ensembling diverse set of individual models. We consider cross-model diversity as the variation among different models. However, pursuing cross-model diversity may inadvertently include low-quality models, resulting in quality-diversity trade-off. While previous studies mainly concentrate on achieving high cross-model diversity [Wang et al., 2024a, Zhang et al., 2024b], we adopt holistic perspective on model diversity by considering in-model diversity, which arises from the variability of multiple responses generated by the same model. In-model diversity enables us to aggregate multiple outputs from an individual model. Intuitively, leveraging outputs from the best-performing individual model can more effectively navigate the quality-diversity trade-off by creating higher-quality proposer mixture. Thus, we propose Self-MoA as depicted in Figure 1b, which utilizes the same prompting template as MoA but aggregates outputs that are repeatedly sampled from the same model, rather than from set of different models. To distinguish, we use Mixed-MoA to refer to MoA configurations that combine different individual models when necessary. Surprisingly, we find that Mixed-MoA is usually sub-optimal compared with Self-MoA, especially when there exist significant quality differences among proposers. Specifically, we revisit the same experiment setting of MoA with six open-source instruction fine-tuned models as Wang et al. [2024a]. Compared with Mixed-MoA which aggregates all six models, Self-MoA on the strongest model achieves 6.6 point improvement over its mixed counterpart on the AlpacaEval 2.0 benchmark, showing case of when intramodel diversity is more effective. Moreover, Self-MoA on two best-performed models on AlpacaEval 2.0 consistently achieves 2-3 point gain and secures the top position among non-adversarial methods on the leaderboard, which further confirms the effectiveness of Self-MoA in this task. To explore the limits of model diversity for MoA, we extend our experiments to setting with three specialized models, each excelling in specific task. Specifically, we utilize Qwen2-7B-Instruct [Bai et al., 2023] for common sense QA (MMLU-redux [Gema et al., 2024]), Qwen2-Math-7B-Instruct [Bai et al., 2023] for mathematics (MATH [Hendrycks et al., 2020]), and DeepSeek-Coder-V2-Lite-Instruct [Zhu et al., 2024] for coding (CRUX [Gu et al., 2024]). We compare Self-MoA against range of Mixed-MoA strategies, evaluating 13 combinations of individual models based on their average performance across the three tasks. Our findings indicate that employing task-specific models as proposers for Self-MoA can significantly outperform the best Mixed-MoA. Furthermore, even in constructed mixture task tailored for Mixed-MoA where each individual model excels in specific subtask, only two Mixed-MoA strategies slightly outperform Self-MoA by 0.17% and 0.35%. To better understand the effectiveness of Self-MoA, we conduct comprehensive investigation of the tradeoff between quality and diversity in MoA, involving over 200 experiments. We use the Vendi Score [Dan Friedman and Dieng, 2023] to evaluate the diversity among the outputs of the proposers, while the average performance of the proposers serves as the measure of quality. In Section 4, we confirm that MoA performance has positive correlation with both quality and diversity. Moreover, we clearly show trade-off along the achievable Pareto front of quality and diversity. Interestingly, we find that MoA is quite sensitive to variations in quality, with optimal performance typically occurring in regions characterized by high quality and relatively low diversity. This finding naturally explains the effectiveness of Self-MoA, as it utilizes the strongest model as the proposer, ensuring high quality in its outputs. Finally, we evaluate the performance of Self-MoA under increasing computational budgets. As the number of outputs grows, the scalability of Self-MoA becomes constrained by the context length of the aggregator. To address this issue, we introduce Self-MoA-Seq (Figure 1c), sequential version that processes samples using sliding window, allowing it to handle an arbitrary number of model outputs. Our findings 2 show that Self-MoA-Seq performs at least as effectively as Self-MoA, enabling scalable ensembling for LLMs with shorter context lengths without compromising final performance. Overall, our contributions are three-fold: We introduce Self-MoA, which leverages in-model diversity by synthesizing multiple outputs from the same model. Surprisingly, it demonstrates superior performance compared to existing Mixed-MoA approaches, which emphasize cross-model diversity, across wide range of benchmarks. Through systematic experiments and statistical analysis, we uncover core trade-off between diversity and quality among the proposers, emphasizing that MoA is highly sensitive to proposer quality. This finding also explains the success of Self-MoA, which leverages outputs from the highest-performing model, ensuring superior overall quality. We extend Self-MoA to its sequential version Self-MoA-Seq, which iteratively aggregates small amount of outputs step by step. Self-MoA-Seq unlocks LLMs that are constrained by the context length and enables computation scaling during inference."
        },
        {
            "title": "2 Related Work",
            "content": "Ensembles of LLMs. Model ensembling aims to combine strengths from multiple models. Previous studies have explored various methods to leverage diverse set of models, including but not limited to prompting [Wang et al., 2024a], weight averaging [Lin et al., 2024, Rame et al., 2024], routing [Jiang et al., 2024b, Lu et al., 2023], training generative fusion model [Jiang et al., 2023b], and so on. Zhang et al. [2024a] argues that the fusion of specialized models with certain general abilities could be promising direction toward Artificial General Intelligence. Mixture-of-Agents (MoA, Wang et al. [2024a]) first queries multiple LLMs to generate responses, then iteratively aggregates these samples through several rounds of synthesis. MoA shows promising results on several benchmarks, and its variants achieve superior performance on the AlpacaEval 2.0 leaderboard. Our method is inspired by the prompt pipeline proposed in MoA. However, while existing MoA focuses on unleashing the strength from multiple different models [Wang et al., 2024a, Jiang et al., 2023b, Zhang et al., 2024b], we demonstrate the trade-off between diversity and quality within the proposers, highlighting that focusing solely on diversity may compromise overall quality and final performance. LLM Inference with Repeated Sampling. Previous studies have shown that combining model outputs from repeated sampling can yield better response in various domains. In tasks with automatic verifiers available, such as math [Hendrycks et al., 2021] and code [Chen et al., 2021], simply sampling LLMs multiple times can significantly improve the pass@k metric and hence boost the success rate of solving the tasks [Roziere et al., 2023, Li et al., 2022, Brown et al., 2024]. In more general tasks without verification tools, we can conduct techniques like majority vote, self-consistency, and best-of-n to choose the most promising one from candidate responses [Wang et al., 2022, Chen et al., 2023b, Gui et al., 2024, Li et al., 2024]. Therefore, repeated sampling is recently regarded as one approach of scaling compute during inference time [Brown et al., 2024]. In this work, we identify the surprising effectiveness of repeated sampling in the context of MoA. Unlike majority vote or best-of-N, Self-MoA asks LLMs to synthesize outputs generated from repeated sampling, hence can further improve over each individual output. Collaborative Agents There is surge of interest in building agent systems based on verification, critique, discussion, and refinement. For example, Stechly et al. [2023], Valmeekam et al. [2023], and Madaan et al. 3 Figure 1: Comparison of MoA, Self-MoA, and Self-MoA-Seq. (a) In MoA, multiple models respond to query, followed by an aggregator synthesizing their outputs. (b) Self-MoA simplifies this by repeatedly sampling from single model. (c) Self-MoA-Seq extends Self-MoA by applying sliding window to combine the best output so far with candidate outputs. At each timestep, the synthesized output is repeated to bias the aggregator towards it, reducing the context length requirements and expanding the methods applicability. Note that MoA can extend to multiple rounds of aggregation (Appendix A.1), while Self-MoA and Self-MoASeq can extend to more outputs, but we omit them here for clarity. [2024] use self-critique to iteratively refine outputs through chain structure. Madaan et al. [2024], Chen et al. [2024], and Wang et al. [2024a] explore the incorporation of multiple models to create stronger agent that outperforms each individual model. Du et al. [2023] incorporates multiple LLMs that propose and debate their individual responses over several rounds to reach common final answer. Liang et al. [2023] proposes Multi-Agent Debate, which encourages divergent thinking during LLM debates to arrive at more informative conclusions and avoid rushing to incorrect answers. Chen et al. [2023a] introduces RECONCILE, which adopts confidence-weighted voting mechanism for better consensus among LLM discussions. Interestingly, Wang et al. [2024b] shows that single model with carefully designed prompts can sometimes match the performance of agent discussions. Moreover, agent discussions mainly outperform single LLM when the prompts are insufficient."
        },
        {
            "title": "3 Is Ensembling Different LLMs Beneficial?",
            "content": "As introduced in Section 1, previous research primarily emphasizes cross-model diversity, which can inadvertently include low-quality proposers. In this work, we introduce Self-MoA (Figure 1), which uses single top-performing model to generate multiple outputs and aggregate them to produce the final result. Self-MoA leverages in-model diversity as repeated sampling often produces varied outputs. We propose our research question as follows: Does the benefit of MoA stem from cross-model diversity? Can we build stronger MoA using in-model diversity? 4 Table 1: Comparison of Self-MoA and Mixed-MoA on AlpacaEval 2.0 leaderboard. We use Qwen1.5-110BChat as the aggregator. Model Configuration LC Win Rate Individual WizardLM-2-8x22B Qwen1.5-110B-Chat LLaMA-3-70B-Instruct Qwen1.5-72B-Chat Mixtral-8x22B-Instruct-v0.1 dbrx-instruct Mixed-MoA 2-Layer MoA [Wang et al., 2024a] Self-MoA 2-Layer Self-MoA + WizardLM 53.1 43.9 34.4 36.6 30.2 25. 59.1 65."
        },
        {
            "title": "3.1 Experiments on AlpacaEval 2.0 with General Purpose Models",
            "content": "Evaluation benchmarks. We adopt the same experiment setting as Wang et al. [2024a] in AlpacaEval 2.0 benchmark [Dubois et al., 2024] and compare the performance of Mixed-MoA and Self-MoA1. AlpacaEval 2.0 is widely used benchmark for assessing the instruction-following abilities of LLMs. It offers set of real-world instructions and employs GPT-4-based annotator to compare the models responses against reference answers generated by GPT-4. To address length bias inherent in model-based evaluation, Dubois et al. [2024] introduced the length-controlled (LC) win rate as more robust evaluation metric. Models. Following Wang et al. [2024a], we construct MoA based on six individual models: Qwen1.5110B-Chat [Bai et al., 2023], Qwen1.5-72B-Chat [Bai et al., 2023], WizardLM-8x22B [Xu et al., 2023], LLaMA-3-70B-Instruct [Touvron et al., 2023], Mixtral-8x22B-Instruct-v0.1 [Jiang et al., 2024a], and dbrxinstruct [Team et al., 2024b]. Each model is sampled with temperature of 0.7, following the default in [Wang et al., 2024a]. For Self-MoA, we aggregate six outputs sampled from WizardLM-2-8x22B, as it consistently outperforms the other models. In line with Wang et al. [2024a], we use Qwen1.5-110B-Chat as the aggregator for both Mixed-MoA and Self-MoA. Results. We present the LC win rate for each model configuration in Table 1. For individual models, we report the higher value between the leaderboard results and our reproduction. Notably, Self-MoA demonstrates remarkable effectiveness in this task, outperforming the Mixed-MoA baseline by 6.6 point. This suggests that, while using multiple models intuitively offers greater diversity, ensembling multiple outputs from single model is more effective. Applying Self-MoA on top performing models. To further validate the effectiveness of Self-MoA, we apply it to the two top-performing models on AlpacaEval 2.0: gemma-2-9b-it-WPO-HB [Zhou et al., 2024] and gemma-2-9b-it-SimPO [Meng et al., 2024]. We use each model as both the proposer and the aggregator2, 1We note that this experiment is similar to the single-proposer setting in Wang et al. [2024a], however our reproduced result is different. We conjecture that such major difference is due to different choices of the proposer model, which is not mentioned in Wang et al. [2024a]. As we shall see later in Section 4, ensembling performance is more sensitive to quality rather than diversity. Therefore, worse proposer model will lead to suboptimal performance of Self-MoA. 2Qwen1.5-110B-Chat is not used as the aggregator since the two top models significantly outperform it. 5 Table 2: Self-MoA achieves state-of-the-art performance on the AlpacaEval 2.0 leaderboard when using top-performing models as both proposers and aggregators. We only ensemble 4 outputs due to context window constraints. Model Configuration LC Win Rate Individual gemma-2-9b-it-WPO-HB gemma-2-9b-it-SimPO Self-MoA Self-MoA + gemma-2-9b-it-WPO-HB Self-MoA + gemma-2-9b-it-SimPO 76.7 72.4 78.5 75.0 with temperature of 0.7 for all the generations. Due to the context length constraint of Gemma 2 [Team et al., 2024a], the aggregator can only take four samples as the input. As shown in Table 2, Self-MoA consistently achieves 2-3 point gain and secures the top position on the leaderboard during submission. Results on MT-Bench. Beyond AlpacaEval 2.0, we further evaluate Self-MoA and Mixed-MoA on MTBench [Zheng et al., 2023], another benchmark used in Wang et al. [2024a]. The results align with our findings from AlpacaEval 2.0, reinforcing the effectiveness of Self-MoA. Please refer to Appendix B.1 for more details."
        },
        {
            "title": "3.2 Experiments on Multiple Datasets with Specialized Models",
            "content": "In this section, we compare different ensembling methods on diverse set of benchmarks using specialized models. Evaluation datasets. We conduct evaluations across diverse set of benchmarks: MMLU [Hendrycks et al., 2020] is multiple-choice dataset designed to assess models multitask accuracy. MMLU is widely used to evaluate both the breadth and depth of language understanding capabilities of current LLMs across diverse array of subjects, including mathematics, history, computer science, logic, and law. We adopt MMLU-redux [Gema et al., 2024] for evaluation, which is subset of MMLU with 3,000 samples fixing the errors in the dataset through human re-annotating. CRUX [Gu et al., 2024] consists of 800 Python code functions, each containing 3 to 13 lines along with an input-output pair. Based on this dataset, Gu et al. [2024] constructs two tasks: input prediction and output prediction. To successfully complete these tasks, the LLM must demonstrate code reasoning abilities. MATH [Hendrycks et al., 2021] comprises 12,500 challenging competition-level mathematics problems. For our analysis, we utilize the testing subset of MATH, which consists of 5,000 samples. 3As Qwen2-Math-7B-Instruct only supports context length of 4096, for these two data points, we sample the proposer with reduced token length of 1024, and only aggregates three outputs from the proposer. 6 Table 3: Comparison of Self-MoA and Mixed-MoA in MMLU, CRUX, and MATH. The labels i, m, and refer to Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and Qwen2-Math-7B-Instruct, respectively. The average performance represents the mean accuracy across MMLU, CRUX, and MATH. TaskBest indicates that we use the strongest model for each task as both proposer and aggregator. Aggregator Proposer MMLU CRUX MATH Individual - - - Mixed-MoA m iimmdd imdddd iiiimd immmmd iimmmm iiimmm iiiimm iidddd iiiddd iiiidd mmdddd mmmddd mmmmdd Self-MoA TaskBest 6TaskBest 6TaskBest 66.16 60.91 54.36 67.89 67.42 68.90 66.63 66.23 67.49 68.00 68.21 68.21 68.47 66.34 65.80 65. 69.01 69.01 36.25 49.51 27.88 42.88 44.50 41.25 42.75 39.25 38.25 37.00 45.50 42.88 40.75 46.75 47.00 42.50 50.75 52.62 53.81 53.82 69.573 64.38 63.90 63.00 66.02 66.10 64.16 62.92 62.56 62.38 61.24 66.48 67.32 67. 68.42 69.803 Models. To ensure sufficient diversity, we select three LLMs with specialized strengths: Qwen2-7BInstruct [Yang et al., 2024], DeepSeek-Coder-V2-Lite-Instruct [Zhu et al., 2024], and Qwen2-Math-7BInstruct. We fix the number of proposers to six and sweep various combinations of these three models. For convenience, we denote Qwen2-7B-Instruct as i, DeepSeek-Coder-V2-Lite-Instruct as d, and Qwen2Math-7B-Instruct as m. As shown in Table 3, Qwen2-7B-Instruct, DeepSeek-Coder-V2-Lite-Instruct, and Qwen2-Math-7B-Instruct excel on MMLU, CRUX, and MATH, respectively. We use the short name for the mixture of proposers. For example, iiddmm indicates the inclusion of two samples from each model respectively. When model is represented multiple times in the proposer mixture, we ensure that two samples are generated with different random seeds. We set the temperature of each model to be 0.7 for the individual model, and use temperature 0 for the aggregator. We mainly use Qwen2-7B-Instruct as the aggregator but also try different models as the aggregator. We explore various MoA configurations, including individual models, combinations of two or three models as proposers, and using single top-performing model (TaskBest, for example DeepSeek-Coder-V2-Lite-Instruct for CRUX) as the proposer (Self-MoA). Results. The results are presented in Table 3. When using as the aggregator, Self-MoA with the TaskBest model consistently outperforms all 13 tested Mixed-MoA configurations across all tasks. Furthermore, adopting task-specific aggregator yields an additional performance boost of 1-2 points. Interestingly, increasing model diversity does not always lead to better performance. For instance, while MoA with iimmdd surpasses mmmddd on MMLU, it underperforms on CRUX and MATH. This discrepancy aligns 7 Figure 2: The diversity-quality trade-off: Mixed-MoA incorporates different individual models as proposers, while Self-MoA uses the same individual model for this role. Quality is assessed based on the average performance of each proposer, and diversity is computed with the Vendi Score [Dan Friedman and Dieng, 2023] of outputs generated by proposers on the same prompts. with the relative strengths of the individual modelsi excels on MMLU but lags behind on CRUX and MATH. We postpone more discussion to Section 4.2."
        },
        {
            "title": "4 The Quality-Diversity Trade-off",
            "content": "We investigate factors that contribute to the strong performance of Self-MoA through careful experiments. Previous studies have mainly focused on increasing model diversity within the group [Wang et al., 2024a, Jiang et al., 2023a, Zhang et al., 2024b]. However, searching for diverse models can sometimes lead to including poorly performed models, resulting in trade-off between diversity and quality, where quality refers to how well each individual model performs in the group. Therefore, we aim to identify the existence of general relationship between MoAs performance and quality as well as diversity. Following Section 3, we evaluate MoAs performance on MMLU, CRUX, and MATH, which cover tasks requiring wide range of capabilities. We vary the quality and diversity with two orders of freedom: 1) combinations of individual models in proposers from Section 3.2; and 2) sampling temperature. i.e., 0.5, 0.7, 1.0, 1.1, and 1.2. This results in total of over 70 unique MoA proposer mixtures. We measure the quality and diversity as follows: Diversity: We utilize the Vendi Score [Dan Friedman and Dieng, 2023] to assess the diversity among individual models in the proposer mixture. The Vendi Score represents the effective number of unique elements within collection of samples [Dan Friedman and Dieng, 2023], with further details provided in the Appendix A.2. Specifically, for given prompt x, we obtain responses from each model, denoted as y1, y2, . . . , y6. The diversity of the proposers for prompt x, denoted as d(x), is calculated using the Vendi Score on the set [y1, . . . , y6]. We then compute the overall diversity across the dataset as: = 1 (cid:88) xS d(x). Quality: We first determine the accuracy of each model on the dataset S, yielding values q1, q2, . . . , q6. 6 (q1 + q2 + . . . + q6), serves as our measure of the quality of the proposers. The average accuracy, = 1 We will explore additional quality measurement strategies in later sections. 8 Table 4: Linear regression (Equation 1) of MoAs performance on diversity and quality q. Dataset α Coefficient P-value β Coefficient P-value MMLU 2.558 0.176 < 0.001 1.841 0.176 < 0.001 0.771 CRUX 4.548 0.459 < 0.001 1.421 0.459 < 0. 0.685 MATH 4.719 0.416 < 0.001 2.839 0.416 < 0.001 0.760 Results. We plot MoAs performance with corresponding diversity and quality for each mixture of proposers in Figure 2. We summarize key observations as follows: The trends among MMLU, CRUX, and MATH are consistently aligned. When the quality is fixed, increasing diversity can enhance MoAs performance. When the diversity is fixed, improving quality can also boost MoAs performance. There exists trade-off in the achievable Pareto front between diversity and quality. Notably, the best performance of MoA is typically observed in the bottom right of each subplot, indicating strong sensitivity to quality. Previous work on ensembles [Wang et al., 2024a, Jiang et al., 2023a, Zhang et al., 2024b] primarily focuses on increasing the diversity of models within the proposer mixture. However, as shown in Figure 2, compared to Self-MoA on the best-performing model, simply aiming for greater diversity in the proposer mixture can result in lower overall quality, which may negatively impact MoAs performance. This trade-off between diversity and quality helps to explain why Self-MoA achieves superior performance across various benchmarks."
        },
        {
            "title": "4.1 Statistical Analysis",
            "content": "To further understand the numerical correlation between MoAs performance and diversity as well as quality, we conduct linear regression for MoAs performance on diversity and quality q. Specifically, we fit the following equation for each dataset: = α + β + γ, (1) where α, β, γ are real-valued coefficients to be determined. For each dataset, we collect around 70 data points from Figure 2 to construct the set {qi, di, ti}N i=1. The coefficients α, β, and γ are then derived by solving linear regression on {qi, di, ti}N i=1. To make coefficients α and β comparable, we normalize and by subtracting their means and dividing by their standard deviations (detailed in Appendix A.3), respectively. The results are presented in Table 4. We observe that the p-values for both α and β are less than 0.001, indicating significant correlation between MoAs performance and both quality and diversity [Arnold, 1990]. The R2 values from the linear regression across three datasets are approximately around 0.7, indicating that the linear model based on quality and diversity explains 70% MoAs performance and hence strong correlation between inputs and outputs, according to Appendix A.4. In later parts, we show that using more fine-grained quality calculation can further increase the R2 value. 9 Comparing the effect strength of quality and diversity. From Table 4, we observe that α is greater than β across all three datasets. In particular, for CRUX and MATH, the gap between these two measures is even more pronounced. These results suggest that MoAs performance is particularly sensitive to variations in quality, highlighting the importance of prioritizing quality within the proposer mixture. This finding is also consistent with our observation that MoA achieves its best performance in the bottom right of the plot in Figure 2, further supporting the effectiveness of our proposed Self-MoA approach. Alternative quality measurements. We use the averaged accuracy of each individual model to measure quality in the previous analysis. In this section, we explore alternative methods for assessing the quality of proposers. Recall that q1, . . . , q6 denote the accuracy of each individual model among proposers, and without loss of generality, we assume q1 q2 . . . q6. It is reasonable to assume that the aggregator can select the correct answer from the proposers, particularly when the responses of individual models are inconsistent. In such cases, the aggregator would rely more heavily on models with better individual performance, meaning the weight of q1 would be greater than that of q6. Therefore, we compare the following methods to calculate quality: Average: 1 6 (cid:80)6 i=1 qi. K-Norm: (cid:16) 1 6 (cid:80)6 i=1 qK (cid:17)1/K , where larger places more emphasis on stronger individual models. Centered-1/K-Norm: q1 . In this formulation, we first compute the difference between qi and the best models q1. The 1/K norm emphasizes the weights of models whose performance is closer to q1. (cid:16) 1 6 (cid:80)6 i=1(q1 qi)1/K(cid:17)K All three methods are the same when = 1. For each quality measurement, we fit linear regression to assess the relationship between MoAs performance and the quality and diversity metrics, reporting the R2 values in Table 5. Our analysis shows that in MMLU and CRUX, applying larger weight to betterperforming individual models tends to increase the R2 values. However, this trend is inconsistent for MATH. We conjecture that this inconsistency arises because the aggregator Qwen2-7B-Instruct is relatively weak on MATH compared to the strongest individual model, Qwen2-Math-7B-Instruct. This limitation constrains the performance of MoA, leading to an inconsistent trend in the linear regression results. In contrast, on MMLU, where Qwen2-7B-Instruct is the strongest individual model, we find that the R2 value can exceed 0.9 with = 2 using the Centered-1/K-Norm. This indicates very strong linear relationship between MoA performance and the quality and diversity metrics. Overall, we conclude that employing Centered-1/K-Norm with = 2 (marked in blue) achieves strong performance across all three datasets."
        },
        {
            "title": "4.2 When Mixed-MoA Outperforms Self-MoA?",
            "content": "According to the quality-diversity trade-off illustrated in Figure 2, we conjecture that increasing diversity can enhance MoAs performance when the quality is controlled. Mixed-MoA generally exhibits greater diversity than Self-MoA, which can lead to improved performance when the model quality is similar. This advantage arises when individual models achieve similar overall performance while maintaining significant cross-model diversity. To simulate such scenario, we construct mixture task combining MMLU, CRUX, and MATH as described in Section 3.2. In this setting, test samples are drawn uniformly from the three tasks, and models do not have prior knowledge of samples origin. For given MoA strategy, we evaluate its performance on this mixture task by averaging its performance across the 10 Table 5: The R2 of the linear regression when we use different quality measurement methods. We find using Centered-1/K-Norm with K=2 can achieve good performance among all these three datasets. Dataset Method Avg. (K=1) K=2 K=3 K=4 MMLU CRUX MATH K-Norm Centered-1/K-Norm K-Norm Centered-1/K-Norm K-Norm Centered-1/K-Norm 0.771 0.771 0.685 0.685 0.760 0. 0.809 0.881 0.736 0.753 0.720 0.720 0.832 0.902 0.765 0.758 0.692 0. 0.845 0.903 0.779 0.753 0.672 0.672 Table 6: Comparison of Self-MoA and Mixed-MoA on the mixture task of MMLU, CRUX, and MATH, measured by the average performance of three tasks from Table 3. Mixed-MoA models with top two average performances are highlighted by underline. Aggregator Proposer Average Individual - - - Mixed-MoA m iimmdd imdddd iiiimd immmmd iimmmm iiimmm iiiimm iidddd iiiddd iiiidd mmdddd mmmddd mmmmdd Self-MoA TaskBest dddddd 6TaskBest 6TaskBest 52.07 54.74 50.60 58.38 58.61 57.72 58.47 57.19 56.63 55.97 58.76 57.82 56.82 59.86 60.04 58. 59.69 62.73 63.81 three datasets. The results are reported in Table 6. In this mixture task, each model specializes in different subtasks, with performing best on MMLU, on CRUX, and on MATH. As TaskBest requires additional prior knowledge of the sample origin, we also report Self-MoA with as the proposer, given that it achieves the highest average performance among individual models. From Table 6, we observe that Mixed-MoA indeed outperforms Self-MoA of dddddd. Specifically, Mixed-MoA of mmdddd and mmmddd achieves the average performance of 59.86% and 60.04%, improves 11 Table 7: MoA of Llama-3.1-8B-Instruct and Qwen2-7B-Instruct. is short for Llama-3.1-8B-Instruct and is short for Qwen2-7B-Instruct. Individual Mixed-MoA Self-MoA Aggregator Proposer MMLU - - i iiilll iiiiii llllll 66.16 66.40 70. 69.01 71.27 upon Self-MoA of dddddd by 0.17% and 0.35%. Given the reported small margin, we argue that Self-MoA is still very competitive baseline under this setting, not to mention the dominant performance of Self-MoA over Mixed-MoA when focusing on one single task (Self-MoA with TaskBest models achieve an average of 3.8% improvement from Table 6). In Appendix B.3 we also report normalized results that account for different variances among tasks, which leads to similar conclusion. We further consider another single-task case on MMLU, involving two individual models: Llama3.1-8B-Instruct and Qwen2-7B-Instruct, with Qwen2-7B-Instruct serving as the aggregator. We choose Llama-3.1-8B-Instruct because it performs similarly to Qwen2-7B-Instruct as an individual model. Table 7 demonstrates that even when the performance of two individual models is close, Self-MoAutilizing six Llama-3.1-8B-Instruct proposers (denoted as llllll)still outperforms the Mixed-MoA configuration (denoted as iiilll)."
        },
        {
            "title": "5 Scaling Inference Compute with Self-MoA",
            "content": "In previous sections, we have provided evidence that Self-MoA over one strong model is straightforward but effective. As the community is becoming more aware of scaling inference time computing [Brown et al., 2024, Snell et al., 2024, Wu et al., 2024], one natural question to ask is: Given strong model, does Self-MoAs performance scale with the number of repeated samples? Intuitively, Self-MoA cannot scale indefinitely by simply increasing the computation budget for at least three reasons: As more responses are sampled from single model, the diversity among those samples tends to plateau. Aggregating information from many samples is more challenging for LLMs compared to handling smaller number of samples. Every LLM has context length limit (e.g., 8192 tokens for Gemma 2), which restricts the number of responses an aggregator can process at once. While the first limitation is inherent to repeated sampling, we address the latter two by introducing SelfMoA-Seq, sequential variant designed to manage large numbers of responses without overwhelming the aggregator. Self-MoA-Seq uses sliding window to aggregate fixed number of responses at time, allowing 12 Figure 3: The performance of Self-MoA and Self-MoA-Seq with growing number of samples. Dashed lines indicate the performance of single forward pass with the base model. it to handle an unlimited number of responses, regardless of context length constraints. visual illustration is provided in Figure 1. We evaluate the performance of Self-MoA and Self-MoA-Seq with increasing sample sizes on the MMLU and CRUX benchmarks to study their scaling behavior. For each benchmark, we use the best-performing model as both the proposer and aggregator (Qwen2-7B-Instruct for MMLU and DeepSeek-Coder-V2-LiteInstruct for CRUX), with sampling temperature of 0.7. In Self-MoA-Seq, the window size is set to six, with the first three slots reserved for the current synthesized output. We vary the number of samples from 6 to 30 and plot the accuracy curves from three runs with different seeds in Figure 3. Our key observations are as follows: Both Self-MoA and Self-MoA-Seq significantly improve performance over the individual base model. Adding more samples can have both positive and negative effects, meaning there is no universal compute-optimal solution. Self-MoA-Seq delivers performance that is comparable to, or slightly better than, Self-MoA. These findings suggest that Self-MoA-Seq can extend the effectiveness of Self-MoA to LLMs with shorter context lengths, without sacrificing performance. Following Section 4.2, we explore whether introducing second model can enhance performance in the sequential setting. Given that Llama-3.1-8B-Instruct performs similarly to Qwen2-7B-Instruct on the MMLU task, we compare the impact of adding Llama-3.1-8B-Instruct and DeepSeek-Coder-V2-Lite-Instruct (which underperforms Qwen2-7B-Instruct by 5%) after aggregating 30 samples from Qwen2-7B-Instruct in Self-MoA-Seq. We find that incorporating Llama-3.1-8B-Instruct boosts accuracy by around 2%, whereas adding DeepSeek-Coder-V2-Lite-Instruct reduces accuracy by more than 1.5%. This result provides another example of cross-model diversity benefiting MoA, and shows the potential of Self-MoA-Seq with increasing computation budget."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce Self-MoA, an innovative approach that utilizes in-model diversity to enhance the performance of large language models during inference. Our experiments demonstrate that Self-MoA outperforms traditional Mixed-MoA strategies in many popular benchmarks, particularly when the proposer 13 model quality varies. By aggregating outputs from single high-performing model, Self-MoA effectively addresses the quality-diversity trade-off. We further identify the scenarios where mixing LLM can be potentially beneficial and extend Self-MoA to the constrained context length setting. These findings highlight the potential of in-model diversity in optimizing LLM performance and pave the way for further advancements in ensemble methods."
        },
        {
            "title": "References",
            "content": "J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. A. Anthropic. Introducing claude, 2023. H. J. Arnold. Introduction to the practice of statistics. Technometrics, 32:347348, 1990. URL https: //api.semanticscholar.org/CorpusID:122891525. J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. Re, and A. Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. J. C.-Y. Chen, S. Saha, and M. Bansal. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023a. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. S. Chen, L. Zeng, A. Raghunathan, F. Huang, and T. C. Kim. Moa is all you need: Building llm research team using mixture of agents. arXiv preprint arXiv:2409.07487, 2024. X. Chen, R. Aksitov, U. Alon, J. Ren, K. Xiao, P. Yin, S. Prakash, C. Sutton, X. Wang, and D. Zhou. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023b. D. Dan Friedman and A. B. Dieng. The vendi score: diversity evaluation metric for machine learning. Transactions on machine learning research, 2023. Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023. Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, et al. Are we done with mmlu? arXiv preprint arXiv:2406.04127, 2024. A. Gu, B. Rozi`ere, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. L. Gui, C. Gˆarbacea, and V. Veitch. Bonbon alignment for large language models and the sweetness of best-of-n sampling. arXiv preprint arXiv:2406.00832, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 15 A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a. A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mixtral of experts, 2024b. URL https://arxiv.org/abs/2401.04088. D. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023a. D. Jiang, X. Ren, and B. Y. Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion, 2023b. URL https://arxiv.org/abs/2306.02561. J. Li, Q. Zhang, Y. Yu, Q. Fu, and D. Ye. More agents is all you need, 2024. URL https://arxiv.org/ abs/2402.05120. Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023. Y. Lin, H. Lin, W. Xiong, S. Diao, J. Liu, J. Zhang, R. Pan, H. Wang, W. Hu, H. Zhang, H. Dong, R. Pi, H. Zhao, N. Jiang, H. Ji, Y. Yao, and T. Zhang. Mitigating the alignment tax of rlhf, 2024. URL https://arxiv.org/abs/2309.06256. K. Lu, H. Yuan, R. Lin, J. Lin, Z. Yuan, C. Zhou, and J. Zhou. Routing to the expert: Efficient reward-guided ensemble of large language models, 2023. URL https://arxiv.org/abs/2311.08692. A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. Y. Meng, M. Xia, and D. Chen. SimPO: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. OpenPipe. Openpipe mixture of agents: Outperform gpt-4 at 1/25th the cost, 2024. URL https:// openpipe.ai/blog/mixture-of-agents. A. Rame, J. Ferret, N. Vieillard, R. Dadashi, L. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin, A. Douillard, and O. Bachem. Warp: On the benefits of weight averaged rewarded policies, 2024. URL https: //arxiv.org/abs/2406.16768. B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. K. Sarjana, L. Hayati, and W. Wahidaturrahmi. Mathematical modelling and verbal abilities: How they determine students ability to solve mathematical word problems? Beta: Jurnal Tadris Matematika, 13(2): 117129, 2020. 16 C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314. K. Stechly, M. Marquez, and S. Kambhampati. Gpt-4 doesnt know its wrong: An analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397, 2023. G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Rame, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome, A. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J.-B. Grill, B. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic, A. Carl, A. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal, C. Chen, C. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger, D. Vijaykumar, D. Rogozinska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter, E. Eltyshev, F. Visin, G. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-Plucinska, H. Batra, H. Dhand, I. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P. Zhou, J. Carrasqueira, J. Iljazi, J. Becker, J. Fernandez, J. van Amersfoort, J. Gordon, J. Lipschultz, J. Newlan, J. yeong Ji, K. Mohamed, K. Badola, K. Black, K. Millican, K. McDonell, K. Nguyen, K. Sodhia, K. Greene, L. L. Sjoesund, L. Usui, L. Sifre, L. Heuermann, L. Lago, L. McNealus, L. B. Soares, L. Kilpatrick, L. Dixon, L. Martins, M. Reid, M. Singh, M. Iverson, M. Gorner, M. Velloso, M. Wirth, M. Davidow, M. Miller, M. Rahtz, M. Watson, M. Risdal, M. Kazemi, M. Moynihan, M. Zhang, M. Kahng, M. Park, M. Rahman, M. Khatwani, N. Dao, N. Bardoliwalla, N. Devanathan, N. Dumai, N. Chauhan, O. Wahltinez, P. Botarda, P. Barnes, P. Barham, P. Michel, P. Jin, P. Georgiev, P. Culliton, P. Kuppala, R. Comanescu, R. Merhej, R. Jana, R. A. Rokni, R. Agarwal, R. Mullins, S. Saadat, S. M. Carthy, S. Perrin, S. M. R. Arnold, S. Krause, S. Dai, S. Garg, S. Sheth, S. Ronstrom, S. Chan, T. Jordan, T. Yu, T. Eccles, T. Hennigan, T. Kocisky, T. Doshi, V. Jain, V. Yadav, V. Meshram, V. Dharmadhikari, W. Barkley, W. Wei, W. Ye, W. Han, W. Kwon, X. Xu, Z. Shen, Z. Gong, Z. Wei, V. Cotruta, P. Kirk, A. Rao, M. Giang, L. Peran, T. Warkentin, E. Collins, J. Barral, Z. Ghahramani, R. Hadsell, D. Sculley, J. Banks, A. Dragan, S. Petrov, O. Vinyals, J. Dean, D. Hassabis, K. Kavukcuoglu, C. Farabet, E. Buchatskaya, S. Borgeaud, N. Fiedel, A. Joulin, K. Kenealy, R. Dadashi, and A. Andreev. Gemma 2: Improving open language models at practical size, 2024a. URL https://arxiv.org/abs/2408.00118. M. R. Team et al. Introducing dbrx: new state-of-the-art open llm, 2024. URL https://www. databricks. com/blog/introducing-dbrx-new-state-art-open-llm. Accessed on April, 26, 2024b. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. K. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by selfcritiquing their own plans? arXiv preprint arXiv:2310.08118, 2023. J. Wang, J. Wang, B. Athiwaratkun, C. Zhang, and J. Zou. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692, 2024a. Q. Wang, Z. Wang, Y. Su, H. Tong, and Y. Song. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? arXiv preprint arXiv:2402.18272, 2024b. 17 X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Y. Wu, Z. Sun, S. Li, S. Welleck, and Y. Yang. An empirical analysis of compute-optimal inference for problem-solving with language models, 2024. URL https://arxiv.org/abs/2408.00724. C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Yang, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren, X. Liu, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, Z. Guo, and Z. Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. K. Zhang, B. Qi, and B. Zhou. Towards building specialized generalist ai with system 1 and system 2 fusion. arXiv preprint arXiv:2407.08642, 2024a. K. Zhang, W. Yao, Z. Liu, Y. Feng, Z. Liu, R. Murthy, T. Lan, L. Li, R. Lou, J. Xu, et al. Diversity empowers intelligence: Integrating expertise of software engineering agents. arXiv preprint arXiv:2408.07060, 2024b. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. W. Zhou, R. Agrawal, S. Zhang, S. R. Indurthi, S. Zhao, K. Song, S. Xu, and C. Zhu. Wpo: Enhancing rlhf with weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024. Q. Zhu, D. Guo, Z. Shao, D. Yang, P. Wang, R. Xu, Y. Wu, Y. Li, H. Gao, S. Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024."
        },
        {
            "title": "A Supplements",
            "content": "A.1 Multi-Layer MoA MoA can be extended to multiple layers. For MoA with layers and LLMs {Ai,j}n can formulate it as follows: j=1 in each layer i, we (cid:77) yi = [Ai,j(xi)] + x1, xi+1 = yi, j=1 where each LLM Aj query by the aggregators prompt (cid:76). generates response for the query xi, which is further concatenated with the original Table 8 compares the performance of 3-Layer Mixed-MoA and 2-Layer Self-MoA as well as the total number of forward passes required for each method. Specifically, one forward pass is counted each time proposer model generates an output or an aggregator synthesizes result. Notably, Self-MoA outperforms the 3-Layer Mixed-MoA baseline with only half the forward passes. Table 8: Results of 3-Layer Mixed-MoA. Model Configuration LC Win Rate # Forward Passes Mixed-MoA 3-Layer MoA [Wang et al., 2024a] Self-MoA 2-Layer Self-MoA + WizardLM-2-8x22B 65. 65.7 13 7 A.2 Vendi Score The Vendi Score (VS) is metric designed to evaluate diversity in machine learning. It takes as input collection of samples along with pairwise similarity function, and it outputs single value that represents the effective number of unique elements within the sample set. The score is computed using positive semi-definite similarity matrix Rnn as follows: S(K) = exp tr (cid:18) (cid:18) log (cid:18) (cid:19)(cid:19)(cid:19) (cid:32) = exp (cid:88) i= (cid:33) λi log(λi) Here, λi are the eigenvalues of the normalized matrix , and 0 log 0 = 0. Essentially, the Vendi Score is the exponential of the von Neumann entropy of , which reflects the Shannon entropy of its eigenvalues, also referred to as the effective rank. This metric provides quantitative measure of diversity based on the distribution of similarity scores among the samples. A.3 Normalization of Inputs Given sequence of inputs x1, ..., xn. Let denote the normalized x. We have = xi std(x) , where = 1 (cid:88) i= xi, and std(x) = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) (xi x)2 i= 19 A.4 Implication of R-squre The implications of R2 are presented in Table 9, illustrating the degree of influence between the independent and dependent variables. [Sarjana et al., 2020]. Table 9: The interpretation of R-square R-square Level [0, 0.2) Very weak [0.2, 0.4) Weak [0.4, 0.6) Median [0.6, 0.8) Strong [0.8, 1.0] Very Strong"
        },
        {
            "title": "B Additional Results",
            "content": "B.1 MT-Bench Results We also compare MoA and Self-MoA on the MT-Bench [Zheng et al., 2023] benchmark under the same experiment setting as Wang et al. [2024a]. We copy the numbers from Wang et al. [2024a] for 3-Layer MoA settings, and report our implemented results for the other experiments to ensure that 2-Layer experiments are fair comparisons. Table 10 shows that Self-MoA outperforms its Mixed-MoA counterpart, and using GPT-4o as the aggregator can achieve the best performance even with fewer forward passes compared to 3-Layer MoA with GPT-4o. B.2 Comparison to Universal Self-Consistency We conduct further experiments to compare Self-Consistency [Wang et al., 2022] with MoA and Self-MoA on the AlpacaEval 2.0 benchmark. As this benchmark is an instruction-following task without exact answers, we evaluate on Universal Self-Consistency (USC) [Chen et al., 2023b] which prompts LLMs to generate the most consistent response. We report the result in Table 12, which shows that USC performs worse than its MoA counterpart when proposers and aggregators are controlled. This further suggests that rather than finding the most consistent response, MoA and Self-MoA can encourage LLM to synthesize the references and produce better response. B.3 Normalizing Sub-tasks in Table 6 The results in Table 3 indicate that the variance of models on CRUX is generally higher than that of the other two tasks, which could bias the average performance towards CRUX. To ensure that each task contributes equally to the overall performance metric, we assign weights to the three tasks based on the inverse of their variance. For example, considering MMLU, we report 19 performance metrics (including individual models, Mixed-MoA, and Self-MoA) in Table 3. The standard deviation of performance for MMLU across these 19 20 Table 10: Comparison of Self-MoA and Mixed-MoA on MT-Bench. We use Qwen1.5-110B-Chat and GPT-4o as the aggregator. Model Configuration Avg. 1st turn 2nd turn # Forward Passes Individual Mixed-MoA WizardLM-2-8x22B Qwen1.5-110B-Chat LLaMA-3-70B-Instruct Qwen1.5-72B-Chat Mixtral-8x22B-Instruct-v0.1 dbrx-instruct 2-Layer MoA 2-Layer MoA w/ GPT-4o 3-Layer MoA 3-Layer MoA w/ GPT-4o Self-MoA + WizardLM-2-8x22B 2-Layer Self-MoA 2-Layer Self-MoA w/ GPT-4o 8.99 8.61 8.84 8.62 8.49 7.82 9.06 9.39 9.25 9.40 9.13 9.52 9.05 8.77 9.14 8.66 8.89 8. 9.23 9.40 9.44 9.49 9.36 9.56 8.93 8.45 8.54 8.58 8.09 7.43 8.89 9.37 9.07 9.31 8.89 9.47 1 1 1 1 1 7 7 13 13 7 7 settings is calculated to be 3.50. In comparison, the standard deviation for CRUX and MATH are 5.70 and 4.27, respectively. Consequently, the weight assigned to MMLU when calculating the WeightedAvg is given by: WeightMMLU = 1/3.50 (1/3.50) + (1/5.70) + (1/4.27) . The normalized results are shown in Table 11. 21 Table 11: This table compares Self-MoA and Mixed-MoA using weighted composition of three subtasks. The weights are assigned to each sub-task to prevent high-variance task, such as CRUX, from disproportionately influencing the overall performance metrics. This approach ensures more balanced evaluation, allowing for fairer comparison between the two models. Aggregator Proposer MMLU CRUX MATH Average WeightedAvg Individual Individual Individual Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Mixed-MoA Self-MoA Self-MoA Self-MoA - - - i i i m iimmdd imdddd iiiimd immmmd iimmmm iiimmm iiiimm iidddd iiiddd iiiidd mmdddd mmmddd mmmmdd TaskBest dddddd 6TaskBest TaskBest 66.16 60.91 54.36 67.89 67.42 68.90 66.63 66.23 67.49 68.00 68.21 68.21 68.47 66.34 65.80 65. 65.23 69.01 69.01 36.25 49.51 27.88 42.88 44.50 41.25 42.75 39.25 38.25 37.00 45.50 42.88 40.75 46.75 47.00 42.50 50.75 50.75 52.62 53.81 53.82 69.57 64.38 63.90 63.00 66.02 66.10 64.16 62.92 62.56 62.38 61.24 66.48 67.32 67.62 63.08 68.42 69.80 52.07 54.74 50.60 58.38 58.61 57.72 58.47 57.19 56.63 55.97 58.76 57.82 56.82 59.86 60.04 58. 59.69 62.73 63.81 54.46 55.65 52.80 60.40 60.46 59.94 60.40 59.38 59.00 58.47 60.58 59.86 59.05 61.45 61.57 60.39 60.86 64.21 65.14 Table 12: Comparison of Self-MoA, Mixed-MoA, and Universal Self-Consistency (USC) on AlpacaEval 2.0 leaderboard. We use Qwen1.5-110B-Chat as the aggregator. Model Configuration LC Win Rate # Forward Passes Mixed-MoA Self-MoA Universal Self-Consistency MoA Self-MoA + WizardLM-2-8x22B Mixed-USC Self-USC + WizardLM-2-8x22B 59.1 65.7 53.8 60.2 7 7"
        }
    ],
    "affiliations": [
        "Princeton University"
    ]
}