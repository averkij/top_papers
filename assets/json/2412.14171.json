{
    "paper_title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces",
    "authors": [
        "Jihan Yang",
        "Shusheng Yang",
        "Anjali W. Gupta",
        "Rilyn Han",
        "Li Fei-Fei",
        "Saining Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also ``think in space'' from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance ability."
        },
        {
            "title": "Start",
            "content": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces Jihan Yang1* Shusheng Yang1 Anjali W. Gupta1 Rilyn Han2 Saining Xie1 1New York University 2Yale University Li Fei-Fei3 3Stanford University 4 2 0 2 8 1 ] . [ 1 1 7 1 4 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Evaluation Code",
            "content": "VSI-Bench Figure 1. Whether at home, in the workplace, or elsewhere, the ability to perceive space, remember its layout, and retrieve this spatial information to answer questions on demand is key aspect of visual-spatial intelligence. Recent Multimodal LLMs can understand general videos, but can they think spatially when presented with video recording of an environment? Can they build an accurate, implicit cognitive map that allows them to answer questions about space? What are the strengths and limitations of using MLLMs to enhance spatial intelligence? We dig into these questions by setting up video data for MLLMs to watch, building VQA benchmark to check their recall, and examining what the MLLMs actually remember and understand."
        },
        {
            "title": "Abstract",
            "content": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also think in space from videos? We present novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitivethough subhumanvisual-spatial intel- *Equal contribution. ligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs spatial distance ability. 1 1. Introduction When shopping for furniture, we often try to recall our living room to imagine if desired cabinet will fit. Estimating distances is difficult, yet after even single viewing, humans can mentally reconstruct spaces, recalling objects in room, their positions, and sizes. We live in sensory-rich 3D world where visual signals surround and ground us, allowing us to perceive, understand, and interact with it. Visual-spatial intelligence entails perceiving and mentally manipulating spatial relationships [26]; it requires myriad capabilities, including relational reasoning and the ability to transform between egocentric and allocentric perspectives (Sec. 2). While Large Language Models (LLMs) [3, 6, 9, 35, 59, 65, 66, 75, 79, 80, 85, 100] have advanced linguistic intelligence, visual-spatial intelligence remains under-explored, despite its relevance to robotics [7, 8, 21, 62], autonomous driving [77], and AR/VR [12, 27, 53]. Multimodal Large Language Models (MLLMs) [1, 4, 15, 33, 41, 47, 47, 76], which integrate language and vision, exhibit strong capacities to think and reason in open-ended dialog and practical tasks like web agents [21, 28, 32, 34]. To advance this intelligence in the visual-spatial realm, we introduce VSI-Bench, video-based benchmark featuring over 5,000 question-answer pairs across nearly 290 real indoor-scene videos (Sec. 3). Video data, by capturing continuous, temporal input, both parallels how we observe the world and enables richer spatial understanding and reasoning than static images. Evaluating openand closedsource models on VSI-Bench reveals that even though large performance gap exists between models and humans, MLLMs exhibit emerging visual-spatial intelligence despite the challenges of video understanding, textual understanding, and spatial reasoning (Sec. 4). To analyze model behavior and inspired by dual-coding theory [18], which posits that linguistic and visual processing are distinct yet complementary, we prompt selected models for self-explanations (linguistic) and cognitive maps (visual). Analyzing the self-explanations reveals that spatial reasoning, as compared to visual perception, linguistic intelligence, or temporal processing, is the main factor behind weak performance on VSI-Bench (Sec. 5). Cognitive maps, which represent internal layouts of environments [60, 78], allow us to evaluate MLLMs implicit spatial world models and find that MLLMs build strong local models but weak global ones (Sec. 6). Furthermore, standard linguistic reasoning techniques fail to enhance performance on our benchmark. However, explicitly generating and using cognitive maps improves spatial distance question-answering. Expressing visual-spatial intelligence is difficult (and often piecemeal), even for humans [26]. With this work, we aim to encourage the community to explore grounding frontier models with visual-spatial intelligence and to pave and Figure 2. taxonomy of visual-spatial intelligence capabilities. illuminate this direction. 2. Visual-Spatial Intelligence We discuss preliminaries and scope visual-spatial intelligence to provide context and framework for later analysis. Term Use. We use intelligence rather than cognition as it is broader, and spatial cognition is branch of cognitive psychology [81]. We prefix spatial intelligence in our work with visual, as spatial intelligence exists irrespective of sensory modality (e.g., blind person can perceive space through other senses) [26]. Given our focus on video input, we discuss visual-spatial intelligence. Investigation Scope. While classic spatial intelligence tests also include pen-paper tasks like the Mental Rotation Test [72], our focus is on visual-spatial intelligence as it applies to real-world environments, particularly in common spaces like homes, offices, and factories. Taxonomy. We provide taxonomy of capabilities potentially required for visual-spatial intelligence  (Fig. 2)  , based on cognitive psychology [11, 26, 55, 60] and human experience with our benchmark tasks in Sec. 3. Visual perception, linguistic intelligence, temporal processing, and spatial reasoning are the four areas needed in VSI-Bench. For example, [11] shows that visual object and spatial processing are neurally distinct, which motivates visual perception and spatial reasoning as separate areas. We break spatial reasoning into two broad capabilities: relational reasoning and egocentric-allocentric transformation. Relational reasoning is the ability to identify, via distance and direction, relationships between objects. It also encompasses reasoning about distance between objects by relying on visuospatial common sense about the sizes of other objects. For example, knowing standard beverage can is approximately 12 cm tall, humans can estimate other object sizes by comparing visual proportions. Egocentric-allocentric transformation involves shifting between self-centered (egocentric) view and an environment-centered (allocentric) one. In our setting, each egocentric video frame maps to allocentric object positions and camera trajectory. When humans observe space, they convert egocentric perceptions into an allocentric mental map, enabling perspective-taking from various viewpointsessential for tasks like relative direction or route planning. This transformation relies on visualizing new perspectives and on visuospatial working memory [2], the abil2 Figure 3. Tasks demonstration of VSI-Bench. Note: the questions above are simplified slightly for clarity and brevity. ity to hold and manipulate spatial information, say by updating object positions from new egocentric input [20, 54]. Every task in VSI-Bench requires perceptual, linguistic, and temporal abilities and varying degrees of spatial reasoning. For example, egocentric-allocentric transformation is much more important for task like route planning than object size estimation. These factors provide some context for the complexity of visual-spatial intelligence. 3. VSI-Bench 3.1. Overview We introduce VSI-Bench to quantitatively evaluate the visual-spatial intelligence of MLLMs from egocentric VSI-Bench comprises over 5,000 questionvideo. answer pairs derived from 288 real videos. These videos are sourced from the validation sets of the public indoor 3D scene reconstruction datasets ScanNet [19], ScanNet++ [94], and ARKitScenes [5] and represent diverse environmentsincluding residential spaces, professional settings (e.g., offices, labs), and industrial spaces (e.g., factories)and multiple geographic regions. Repurposing these existing 3D reconstruction and understanding datasets offers accurate object-level annotations which we use in question generation and could enable future study into the connection between MLLMs and 3D reconstruction. VSI-Bench is high-quality, having been iteratively reviewed to minimize question ambiguity and to remove incorrect annotations propagated from the source datasets. VSI-Bench includes eight tasks of three types: configurational, measurement estimation, and spatiotemporal. The configurational tasks (object count, relative distance, relative direction, route plan) test models understanding of the configuration of space and are more intuitive for humans (see Sec. 4 for comparison between MLLM and human performance). Measurement estimation (of object size, room size, and absolute distance) is of value to any embodied agent. While predicting measurement exactly is very difficult, for both humans and models, better sense of distance and other measurements is intuitively correlated with better visual-spatial intelligence and underpins wide range of tasks that require spatial awareness, like interaction with objects and navigation. Spatiotemporal tasks like appearance order test models memory of space as seen in video. See Fig. 3 for an overview of VSI-Bench tasks and Fig. 5 for dataset statistics. 3.2. Benchmark Construction We develop sophisticated benchmark construction pipeline to effectively generate high-quality questionanswer (QA) pairs at scale, as shown in Fig. 4. Data Collection and Unification. We begin our dataset construction by standardizing various datasets into unified meta-information structure, ensuring dataset-agnostic QA pair generation. Our benchmark aggregates existing 3D indoor scene understanding and reconstruction datasets: ScanNet [19], ScanNet++ [94], and ARKitScenes [5]. These datasets provide high-fidelity video scans capable of space reconstruction, ensuring MLLMs can answer spacelevel questions with only video input. Additionally, their object-level 3D annotations facilitated our question generation. We parse the datasets into unified meta-information format including object categories, bounding boxes, video specifications (resolution and frame rate), and more. Question-Answer Generation. QA pairs are primarily auto-annotated using the meta-information and question templates; the route plan task was human-annotated. We sophisticatedly design and refine the question template for each task and provide guidelines for human annotators. For 3 Figure 4. Benchmark curation pipeline. The pipeline first unifies diverse datasets into standardized format and semantic space for consistent processing. QA pairs are then generated through both human annotation and question templates. To ensure quality, human verification is implemented at all key stages for filtering low-quality videos, annotations, and ambiguous QA pairs. 4. Evaluation on VSI-Bench 4.1. Evaluation Setup Benchmark Models. We comprehensively evaluate 15 video-supporting MLLMs across diverse model families, encompassing various parameter scales and training For proprietary models, we consider Geminirecipes. 1.5 [76] and GPT-4o [33]. For open-source models, we evaluate models from InternVL2 [14], ViLA [44], LongViLA [88], LongVA [98], LLaVA-OneVision [39], and LLaVA-NeXT-Video [99]. All evaluations are conducted under zero-shot settings and using each models default prompts. To ensure reproducibility, we use greedy decoding for all models. Metric Design. Based on whether the ground-truth answer is verbal or numerical, our tasks are suited to either Multiple-Choice Answer (MCA) or Numerical Answer (NA) format (see Fig. 3). For MCA tasks, we follow standard practice [24, 30, 96] by using Accuracy (ACC), based on exact matching (with possible fuzzy matching), as the primary metric. For NA tasks, where models predict continuous values, accuracy via exact matching fails to capture the degree of proximity between model predictions and ground-truth answers. Therefore, we introduce new metric, Mean Relative Accuracy (MRA), inspired by previous works [22, 45, 71]. Specifically, for NA question, given models prediction ˆy, ground truth y, and confidence threshold θ, relative accuracy is calculated by considering ˆy correct if the relative error rate, defined as ˆy y/y, is less than 1 θ. As single-confidence-threshold accuracy only considers relative error in narrow scope, MRA averages the relative accuracy across range of confidence thresholds = {0.5, 0.55, . . . , 0.95}: MRA = 1 10 (cid:88) 1 θC (cid:18) ˆy (cid:19) < 1 θ . (1) MRA offers more reliable and discriminative measurement for calculating the similarity between numerical predictions and ground truth values. Chance Level Baselines. We provide two baselines: Figure 5. Benchmark Statistics. Top: The distribution of tasks across three main categories. Bottom: The video length statistic. more detailed design, see Appendix B.1. Human-in-the-loop Quality Review. Despite humanannotated data sources and meticulously designed QA generation methodology, certain ambiguities and errors inevitably persist, primarily due to inherent annotation errors in the source datasets. We implement human-in-theloop verification protocol spanning benchmark construction. This iterative quality assurance is bidirectional: when evaluators flag ambiguous or erroneous questions, we trace the error source and remove the problematic data sample or modify the meta-information, question template, or QA generation rule accordingly to rectify other erroneous questions stemming from the same source. Following each human review cycle, we update and iterate the benchmark until it satisfies our quality standards. 4 o j. t. s. e j. z o t. l. r. l. a t r O r. A Methods Rank Avg. Numerical Answer Multiple-Choice Answer Baseline Chance Level (Random) Chance Level (Frequency) VSI-Bench (tiny) Perf. Human Level Gemini-1.5 Flash Gemini-1.5 Pro Gemini-2.0 Flash Proprietary Models (API) GPT-4o Gemini-1.5 Flash Gemini-1.5 Pro Open-source Models InternVL2-2B InternVL2-8B InternVL2-40B LongVILA-8B VILA-1.5-8B VILA-1.5-40B LongVA-7B LLaVA-NeXT-Video-7B LLaVA-NeXT-Video-72B LLaVA-OneVision-0.5B LLaVA-OneVision-7B LLaVA-OneVision-72B - - - - - - 3 2 11 5 3 12 9 7 8 4 1 10 6 2 - 34.0 79.2 45.7 48.8 45.4 34.0 42.1 45.4 27.4 34.6 36.0 21.6 28.9 31.2 29.2 35.6 40.9 28.0 32.4 40.2 - 62. 94.3 50.8 49.6 52.4 46.2 49.8 56.2 21.8 23.1 34.9 29.1 17.4 22.4 38.0 48.5 48.9 46.1 47.7 43.5 - 32.0 47.0 33.6 28.8 30.6 5.3 30.8 30. 24.9 28.7 26.9 9.1 21.8 24.8 16.6 14.0 22.8 28.4 20.2 23.9 - 29.9 60.4 56.5 58.6 66.7 43.8 53.5 64.1 22.0 48.2 46.5 16.7 50.3 48.7 38.9 47.8 57.4 15.4 47.4 57.6 - 33. 45.9 45.2 49.4 31.8 38.2 54.4 43.6 35.0 39.8 31.8 0.0 18.8 22.7 22.2 24.2 35.3 28.3 12.3 37.5 25.0 25.1 94.7 48.0 46.0 56.0 37.0 37.7 51. 33.8 36.7 42.1 29.6 32.1 40.5 33.1 43.5 42.4 28.9 42.5 42.5 36.1 47.9 95.8 39.8 48.1 46.3 41.3 41.0 46.3 44.2 30.7 32.2 30.7 34.8 25.7 43.3 42.4 36.7 36.9 35.2 39.9 28.3 28. 95.8 32.7 42.0 24.5 31.5 31.5 36.0 30.5 29.9 34.0 32.5 31.0 31.5 25.4 34.0 35.0 34.5 29.4 32.5 25.0 25.2 100.0 59.2 68.0 55.1 28.5 37.8 34. 7.1 39.6 39.6 25.5 24.8 32.9 15.7 30.6 48.6 5.8 24.4 44.6 Table 1. Evaluation on VSI-Bench. Left: Dark gray indicates the best result among all models and light gray indicates the best result among open-source models. indicates results on VSI-Bench (tiny) set. Right: Results including the top-3 open-source models. 4.2. Main Results Tab. 1 shows overall model performance on VSI-Bench. Our key observations are as follows: Human Level Performance. Not surprisingly, human evaluators achieve 79% average accuracy on our benchmark, outperforming the best model by 33%. Notably, human performance on configuration and spatiotemporal tasks is remarkably high, ranging from 94% to 100%, indicating human intuitiveness. In contrast, the performance gap between humans and the best MLLM is much narrower on the three measurement tasks that require precise estimation of absolute distance or size, suggesting that MLLMs may have relative strength in tasks requiring quantitative estimation. Proprietary MLLMs. Despite significant performance gap with humans, the leading proprietary model, Gemini1.5 Pro, delivers competitive results. It surpasses the chance level baselines by substantial margin and manages to approach human level performance in tasks such as absolute Its worth noting that distance and room size estimation. while human evaluators have years of experience in understanding the physical world spatially, MLLMs are only trained on 2D digital data like internet videos. Open-source MLLMs. Top-tier open-source models like LLaVA-NeXT-Video-72B and LLaVA-OneVision-72B demonstrate highly competitive performance to closedsource models, trailing the leading Gemini-1.5 Pro by only 4% to 5%. However, the majority of open-source models (7/12) perform below the chance level baseline, indicating significant limitations in their visual-spatial intelligence. Figure 6. Performance comparisons between Vision Enabled (w/ video), Vision Disabled (w/o video) and Chance Level (Freq.). EnabledDisabled indicates the gap between Vision Enabled and Vision Disabled, and DisabledChance betokens the gap between Vision Disabled and Chance Level (Freq.). Tasks are sorted by EnableDisable for better understanding. Chance Level (Random) is the random selection accuracy for MCA tasks (and is inapplicable for NA tasks). Chance Level (Frequency) represents the highest performance MLLMs would achieve by always selecting the most frequent answer for each task. This identifies performance gains that may result from inherently long-tailed answers or imbalanced multiple-choice distributions. Human Level Performance. We randomly sample subset of 400 questions (50 per task), which we will refer to as VSI-Bench (tiny). Human evaluators independently answer each question, and their performance is evaluated using the above-mentioned metrics. For comparison, we also report Gemini-1.5 Pros performance on VSI-Bench (tiny). See Appendix for details on evaluation setups. 5 Figure 7. Examples of how MLLM thinks as seen in self-explanations. While MLLM exhibits strong video understanding and linguistic reasoning capabilities, its spatial reasoning capabilities are still developing. Blind Evaluation. We compare MLLMs performance against Chance Level (frequency) and Vision Disabled (blind) results, using averages across six of the strongest models (3 open-source and 3 closed-source). As shown in Fig. 6, the consistent improvements in EnabledDisabled and general degradation in DisabledChance demonstrate that video is essential and beneficial for our VSI-Bench, with blind models performing below chance level. Meanwhile, MLLMs struggle to improve beyond chance level in the absolute distance estimation, route plan, and relative direction tasks, whether vision is enabled or not, underscoring the difficulty of these tasks. Note that on object size, Vision Disabled models already significantly outperform chance level, likely due to common-sense knowledge learned during language model training. 5. How MLLMs Think in Space Linguistically To better understand when and why models succeed or fail and to elucidate the facets of visual-spatial intelligence they possess, we examine how MLLMs think in space linguistically here and visually in Sec. 6. We begin by prompting the best-performing MLLM in VSI-Bench, Gemini-1.5 Pro [76], to articulate its internal reasoning in language. 5.1. Probing via Self-Explanations Self-explanations are prevailing approach on par with traditional model explanations like LIME saliency maps [69] for understanding LLM-generated responses [25, 31, 51] and are widely used in analyzing language model behavior [64, 96]. We randomly sample subset of 163 incorrect answers, prompt the MLLM to provide explanations for the predicted answers, and carefully review them by hand. Case Studies. Fig. 7 presents self-explanations in both success and an error case. In both examples, when thinking in space, the MLLM exhibits advanced video understandFigure 8. Human-conducted analysis of errors by type. Over 70% of errors stem from faulty spatial reasoning capabilities. ing, demonstrated by the impressive accuracy of its timestamped descriptions. The model also forms correct stepby-step reasoning processes, outlining steps such as orient yourself, locate the dishwasher and visualize the quadrants for the relative direction task. Furthermore, the construction of global coordinate system (Fig. 7, left) suggests that MLLMs may possess or build an implicit world model. Rather than using isolated frames, short clips, or random guesses, the MLLM used global spatial context and reasoning to infer correctly. In the incorrect example (Fig. 7, right), we can identify faulty visual-spatial capabilities like egocentric-allocentric transformation and relational reasoning, as introduced in Fig. 2. In the video, the camera pans right to shift the view from the edge of the bed to the wall and window. The model obeys this egocentric view, responding that to face the wall where the window is located, you must turn right instead of creating an allocentric view reflecting the reality that the route from door to bed means turning left. Error Analysis. To quantify and identify the main bottleneck for the best-performing MLLM on our benchmark, we analyze its errors on VSI-Bench (tiny), categorizing them 6 into four distinct types which arose from both our outlined visual-spatial capabilities  (Fig. 2)  and clear four-way categorization of errors upon examination: 1. Visual perception error, stemming from unrecognized objects or misclassified object categories; 2. Linguistic intelligence error, caused by logical, mathematical reasoning, or language understanding defects; 3. Relational reasoning error includes errors in spatial relationship reasoning, i.e., distance, direction, and size; 4. Egocentric-allocentric transformation error, resulting from an incorrect allocentric spatial layout or improper perspective-taking. As shown in Fig. 8, around 71% of errors are attributed to spatial reasoning (as ontologically conceived in Fig. 2), which suggests that: Spatial reasoning is the primary bottleneck for MLLM performance on VSI-Bench. Further analysis and case studies are in Appendix E.2. 5.2. Limits of CoT Methods in Visuospatial Tasks Prompting techniques improve the reasoning and problemsolving abilities for large models across diverse tasks [32, 34, 73, 82]. Their successes motivate us to investigate whether these linguistic prompting methods could also improve the visual-spatial capabilities of MLLMs in VSI-Bench. We investigate three prevailing prompting techniques (see Appendix B.3 for more details): Zero-Shot Chain-of-Thought (CoT). Following [37, 86], we add Lets think step by step to the prompts. Self-Consistency w/ CoT. We follow [84] and set the MLLMs temperature to 1.0 to encourage diverse reasoning and then take the majority consensus from five runs (employed w/ Zero-Shot CoT) as the final prediction. Tree-of-Thoughts (ToT). Following the Creative Writting practice in [92], we divide reasoning into plan generation and answer prediction. The MLLM first drafts and selects plan, then generates three candidate answers and selects the most confident one as prediction. As shown in Fig. 9, surprisingly, all three linguistic reasoning techniques lead to performance degradation on VSI-Bench. Zero-Shot CoT and ToT reduce average performance by about 4%, and self-consistency, though slightly better, still falls 1.1% below the no-prompting baseline. The unilateral improvement in the appearance order and absolute distance estimation tasks is easily explained by their significant percentage of linguistic intelligence errors (see Fig. 8). In contrast, the room size and object size tasks suffer large 8% to 21% decrease, showing that encouraging model to think more can be not just unreliable but downright harmful. Meanwhile, as shown in Tab. 2, ZeroShot CoT achieves 1.6% improvement on the general Figure 9. Relative improvements of CoT, self-consistency and Tree-of-Thought compared to the baseline. All three prevailing prompting techniques fail on average on our benchmark, and, in some cases, task performance becomes much worse after applying them. This implies that VSI-Bench cannot be solved by solely improving linguistic capabilities. Case Performance Gemini-1.5 Pro (w/o CoT) Gemini-1.5 Pro (w/ CoT) 77.2 79.8 Table 2. Gemini-1.5 Pro CoT performance on 500-questions subset in VideoMME. video understanding benchmark VideoMME [24]. Therefore, our results suggest that: Linguistic prompting techniques, although effective in language reasoning and general visual tasks, are harmful for spatial reasoning. 6. How MLLMs Think in Space Visually Since humans subconsciously build mental representations [58, 78] of space when reasoning spatially, we explore how MLLMs remember spaces. 6.1. Probing via Cognitive Maps We prompt MLLMs to express their internal representations of the spaces they see using cognitive maps, wellestablished framework for remembering objects in set environment [60, 78]. We prompt the best-performing MLLM, Gemini-1.5 Pro, to predict object center positions within 10 10 grid based on video input (see Fig. 11b for grid size ablation and Appendix B.4 for prompt). We present examples of the resulting cognitive maps in Fig. 10. To quantitatively assess these cognitive maps, we evaluate the Euclidean distance between all pairs of objects within each map. We consider the distance (on the grid) between two objects to be correct if it deviates by no more than one grid unit from the distance in the ground truth cognitive map. As shown in Fig. 11, we divide the map-distances into eight distinct bins for analysis. Interestingly, we find that the MLLM achieves remarkable 64% accuracy in positioning adjacent objects within its cognitive map, indicating robust local spatial awareness. However, this accuracy significantly deteriorates as the distance between two objects Figure 10. Visualization of cognitive maps from MLLM and GT. Figure 11. Locality of the MLLMs predicted cognitive maps. The MLLMs map-distance accuracy decreases dramatically with increasing object distance. valuable pretext task or promising solution for MLLMs to tackle visual-spatial reasoning. increases, which suggests that: 7. Related Works When remembering spaces, MLLM forms series of local world models in its mind from given video, rather than unified global model. This observation aligns with the challenge of forming global space representation from discrete video frames, which is inherently difficult for MLLMs. While this task may not be trivial for humans either, it is likely that they can build such global space representations more accurately. 6.2. Better Distance Reasoning via Cognitive Maps Given the local awareness of MLLMs in remembering spaces (see Fig. 10 and Fig. 11) and the importance of mental imagery to how humans think in space, we investigate whether generating and using cognitive maps can help MLLMs spatial reasoning in terms of VSI-Benchs relative distance task. This tests if the local distance awareness emerged through cognitive maps transfers to improved distance recall and reasoning. Case Rel. Dist Acc. Cog. Map Src. Size Rel. Dist Acc. w/o Cog. map w/ Cog. map w/ Cog. map (GT) 46.0 56.0 66.0 (a) Cognitive map prompting. MLLM MLLM GT GT 10 10 20 20 10 10 20 20 (b) Cognitive map canvas size. 56.0 54.0 66.0 78. Table 3. Relative distance task with cognitive map. We prompt Gemini-1.5 Pro to first generate cognitive map based on the given video and question, and then to use the predicted map to answer the question. As shown in Tab. 3a, we find that using mental imagery improves MLLMs relative distance accuracy by 10%. The 20% to 32% gain over baseline with the ground truth cognitive map underscores the importance of building accurate mental maps of scene, which enforce globally consistent topology, but indicates that such mental imagery is only one part of the puzzle, albeit crucial one. These results point to building mental spatial world model or cognitive map as 8 Apart from visual-spatial intelligence in Sec. 2, we further ground our work in the following two related areas: MLLMs with Visual-Spatial Awareness. Building on the powerful language and reasoning abilities of LLMs [3, 9, 65, 66, 75, 79, 80] and the feature extraction abilities of modern vision encoders [29, 63, 67], MLLMs, especially visual MLLMs, exhibit unprecedented visual understanding capabilities [33, 39, 76, 83, 88, 99], promising direction toward developing world models [48] and embodied agents [17, 21, 36, 57]. However, grounding MLLMs in the real world presents significant challenges for models visual-spatial intelligence, motivating recent efforts [10, 13, 16, 40, 46, 91, 102]. Unlike prior works, which primarily focus on understanding spatial information through 2D images [68, 74, 90] or solely language [56, 70, 87, 87, 89], our work assesses models visual spatial intelligence using real-world videos, which more closely mirrors human understanding of the world and application scenarios for embodied agents. Benchmarking MLLMs on Video. With MLLMs displaying impressive performance on still-images across perception, reasoning, and multi-disciplinary tasks [38, 50, 95, 96], there is increasing interest in evaluating MLLMs video understanding capabilities [23, 24, 42, 43, 49, 52, 53, 61, 93]. For example, Video-MME [24] comprehensively evaluates MLLMs across various video-related tasks, including recognition and perception. EgoSchema [53] and OpenEQA [62] evaluate MLLMs understanding abilities using egocentric videos. Despite their significance, most prior works focus on content-level understanding [24, 42, 53, 61], which primarily serves as temporal extension of 2D image understanding without 3D spatial consideration. Extending beyond prior benchmarks, our work establishes testbed evaluating models 3D video-based visual-spatial intelligence, using video as an interface to understand the real world. 8. Discussion and Future Work We study how models see, remember, and recall spaces by building VSI-Bench and investigating the performance and behavior of MLLMs on it. Our analysis of how MLLMs think in space linguistically and visually identifies existing strengths (e.g., prominent perceptual, temporal, and linguistic abilities) and bottlenecks for visual-spatial intelligence (e.g., egocentric-allocentric transformation and relational reasoning). While prevailing linguistic prompting methods fail to improve spatial reasoning, building explicit cognitive maps does enhance the spatial distance reasoning of MLLMs. Future avenues of improvement include task-specific fine-tuning, developing self-supervised learning objectives for spatial reasoning, or visuospatial-tailored prompting techniques for MLLMs. Acknowledgements. We thank Ellis Brown, Ryan Inkook Chun, Youming Deng, Oscar Michel, Srivats Poddar, Xichen Pan, Austin Wang, Gavin Yang, and Boyang Zheng for their contributions as human annotators and evaluators. We also thank Fred Lu for proofreading our manuscript. We also thank Chen Feng, Richard Tucker, Noah Snavely, Leo Guibas, and Rob Fergus for their helpful discussions and feedback. This work was mainly supported by the Open Path AI Foundation, Google TPU Research Cloud (TRC) program, and the Google Cloud Research Credits program (GCP19980904). S.X. thanks the OpenAI Researcher Access program and an Amazon Research award for their support."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 2 [2] Alan Baddeley. Working memory. Science, 255(5044): 556559, 1992. 2 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 2, 8 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [5] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In NeurIPS, 2021. 3, 9 [6] Gašper Beguš, Maksymilian abkowski, and Ryan Rhodes. Large linguistic models: Analyzing theoretical linguistic abilities of llms. arXiv preprint arXiv:2305.00948, 2023. 2 [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023. 2 [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. In RSS, 2023. 2 [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. NeurIPS, 2020. 2, 8 [10] Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. arXiv preprint arXiv:2406.13642, 2024. [11] Christopher Chabris, Thomas Jerde, Anita Woolley, Margaret Gerbasi, Jonathon Schuldt, Sean Bennett, Richard Hackman, and Stephen Kosslyn. Spatial and object visualization cognitive styles: Validation studies in 3800 individuals. Group brain technical report, 2:120, 2006. 2 [12] Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. In NeurIPS, 2024. 2 [13] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 8 [14] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. arXiv preprint arXiv:2404.16821, 2024. 4 [15] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 2 [16] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. 8 [17] Junmo Cho, Jaesik Yoon, and Sungjin Ahn. Spatially-aware transformers for embodied agents. In ICLR, 2023. [18] James M. Clark and Allan Paivio. Dual coding theory and education. Educational Psychology Review, 3(3):149210, 1991. 2 [19] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: In Richly-annotated 3d reconstructions of indoor scenes. CVPR, 2017. 3, 13 [20] Milton J. Dehn. Working Memory and Academic Learning: Assessment and Intervention. John Wiley & Sons, 2011. 3 [21] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In ICML, 2023. 2, 8 [22] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010. 4 [23] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. In NeurIPS, 2024. 8 [24] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 4, 7, 8 [25] Haoyu Gao, Ting-En Lin, Hangyu Li, Min Yang, Yuchuan Wu, Wentao Ma, Fei Huang, and Yongbin Li. Selfexplanation prompting improves dialogue understanding in large language models. In COLING, 2024. [26] Howard Gardner. Frames of Mind: The Theory of Multiple Intelligences. Basic Books, tenth-anniversary edition, second paperback edition edition, 1983. 2 [27] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, 2022. 2 [28] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. In ICLR, 2024. 2 [29] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 8 [30] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021. 4 [31] Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani Gilpin. Can large language models explain themselves? study of llm-generated selfexplanations. arXiv preprint arXiv:2310.11207, 2023. [32] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In ICML, 2022. 2, 7 [33] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 4, 8 [34] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In ICLR, 2024. 2, 7 [35] Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schütze, and Peter Clark. Language models with rationality. In EMNLP, 2023. 2 [36] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: In CoRL, An open-source vision-language-action model. 2024. 8 [37] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. 7, 15 [38] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: BenchIn CVPR, marking multimodal large language models. 2024. [39] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 4, 8 [40] Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, and Ivan Vulic. Topviewrs: Vision-language arXiv preprint models as top-view spatial reasoners. arXiv:2406.02537, 2024. 8 [41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 2 [42] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. 8 [43] Shicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: diagnostic dataset for temporal concept understanding of videolanguage models. arXiv preprint arXiv:2311.17404, 2023. 8 [44] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. [45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 4 [46] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yansong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondence elicit 3d spacetime understanding in multimodal language model. arXiv preprint arXiv:2408.00754, 2024. 8 [47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2024. 2, 16 10 [48] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. 8 [49] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. TempIn Compass: Do video LLMs really understand videos? Findings of ACL, 2024. [50] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multimodal model an all-around player? In ECCV, 2025. 8 [51] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris CallisonBurch. Faithful chain-of-thought reasoning. In ACL, 2023. 6 [52] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In CVPR, 2024. 8 [53] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. NeurIPS, 2023. 2, 8 [54] Julia McAfoose and Bernhard T. Baune. Exploring visualspatial working memory: critical review of concepts and models. Neuropsychology Review, 2009. 3 [55] Chiara Meneghetti, Laura Miola, Tommaso Feraco, Veronica Muffato, and Tommaso Feraco Miola. Individual differences in navigation: an introductory overview. Prime archives in psychology, 2022. 2 [56] Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi, Robert Ness, and Jonathan Larson. Evaluating cognitive maps and planning in large language models with cogeval. NeurIPS, 2024. [57] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. NeurIPS, 2024. 8 [58] Lynn Nadel. The Hippocampus and Context Revisited. Oxford University Press, 2008. 7 [59] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. 2 [60] Nora S. Newcombe. Spatial Cognition. MIT Press, 2024. https://oecs.mit.edu/pub/or750iar. 2, [61] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: for evaluatA comprehensive benchmark and toolkit arXiv preprint ing video-based large language models. arXiv:2311.16103, 2023. 8 [62] Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. 2, 8 [63] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin ElNouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 8 [64] Letitia Parcalabescu and Anette Frank. On measuring faithfulness or self-consistency of natural language explanations. In ACL, 2024. 6 [65] Alec Radford. Improving language understanding by generative pre-training. OpenAI Blog, 2018. 2, 8 [66] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 2, [67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 8 [68] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cogarXiv preprint nition emerge in frontier models? arXiv:2410.06468, 2024. 8 [69] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should trust you?\" explaining the predictions of any classifier. In KDD, 2016. 6 [70] Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei Cheng, Dell Zhang, and Andre Freitas. Grounding natural language instructions: Can large language models capture spatial information? arXiv preprint arXiv:2109.08634, 2021. 8 [71] Gerard Salton and Michael J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., USA, 1986. 4 [72] Shenna Shepard and Douglas Metzler. Mental rotation: effects of dimensionality of objects and type of task. Journal of experimental psychology: Human perception and performance, 14(1):3, 1988. [73] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In ICCV, 2023. 7 [74] Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, and Jinhua Zhao. Sparkle: Mastering basic spatial capabilities in vision language models elicits generalization to composite spatial reasoning. arXiv preprint arXiv:2410.16162, 2024. 8 [75] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 8 [76] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, 11 extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [91] Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining Xie. V-irl: Grounding virtual intelligence in real life. In ECCV, 2024. 8 [92] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In NeurIPS, 2024. 7, 15 [93] Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, et al. Mm-ego: Towards building egocentric multimodal llms. arXiv preprint arXiv:2410.07177, 2024. 8 [94] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In ICCV, 2023. 3, 13 [95] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. ICML, 2024. 8 [96] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 4, 6, [97] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. 16 [98] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 4 [99] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. 4, 8 [100] Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, and Xuanjing Huang. Unveiling linguistic regions in large language models. In ACL, 2024. 2 [101] Qian-Yi Zhou, and Vladlen Koltun. Open3D: modern library for 3D data processing. arXiv:1801.09847, 2018. Jaesik Park, [102] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024. 8 Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 2, 4, 6, 8, 16 [77] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. In CoRL, 2024. 2 [78] E. C. Tolman. Cognitive maps in rats and men. Psychological Review, 55(4):189208, 1948. 2, 7 [79] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2, [80] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2, 8 [81] David Ed Waller and Lynn Ed Nadel. Handbook of spatial cognition. American Psychological Association, 2013. 2 [82] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. TMLR, 2023. 7 [83] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 8 [84] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023. 7, 15 [85] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. TMLR, 2022. [86] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. 7, 15 [87] Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. Visualization-of-thought elicits spatial reasoning in large language models. NeurIPS, 2024. 8 [88] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. 4, 8 [89] Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen, Jungo Kasai, and Ilker Yildirim. Evaluating spatial understanding of large language models. TMLR, 2024. 8 [90] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes A. Appendix Outline In these supplementary materials, we provide: Technical details about VSI-Bench construction and our linguistic and visual analysis (Appendix B); Evaluation setup and full evaluation results for VSI-Bench sub-experiments (Appendix C); Analysis on input sequencing and repetition (Appendix D); Additional visualization results (Appendix E). B. Technical Details for VSI-Bench Construction and Analysis In this section, we provide more technical details on the construction of VSI-Bench and analyzing MLLM thinking via self-explanations, Chain-of-Thought-based methods, and cognitive maps. B.1. VSI-Bench Construction Pipeline Here, we discuss the concrete setup for each stage in the benchmark construction pipeline. Dataset Collection and Unification. We curate our evaluation dataset by collecting 150 samples from ARKitScenes [5], 50 samples from ScanNet++ [94], and 88 samples from ScanNet [19]. For video processing, we convert ScanNets individual frames into continuous videos at 24 FPS, while subsampling ScanNet++ and ARKitScenes videos to 30 FPS. All videos are standardized to resolution of 640 480 pixels. Given that ARKitScenes contains videos with varying orientations, we normalize their rotation to maintain consistent upward orientation across all samples. Due to varying annotation structures across the three datasets, we unify them into standardized metainformation format for each scene with the following attributes: dataset, video path, room size, room center, object counts, and object bounding boxes. The room size is calculated by the Alpha shape algorithm* with the scenes point cloud. The room center is calculated as the geometric center of the minimal bounding box of the scenes point cloud. Object counts record the number of instances for each category. As for the object bounding boxes, we unify different annotation formats to the format of OrientedBoundingBox in Open3D [101]. For the categories included in the meta-information, we carefully curate subset of categories from the three source datasets. Since our benchmark aims to evaluate the visualspatial intelligence of MLLMs, we exclude both rare categories and those with extremely small object sizes to reduce perceptual challenges. Additionally, we implement category remapping to ensure vocabulary consistency and in- *https://en.wikipedia.org/wiki/Alpha_shape tuitive understanding across the benchmark. This category remapping is also iteratively refined during human review. QA-Pair Generation. Each QA-pair contains the following attributes: question ID, source dataset, task type, video path, question, multiple-choice options w/ letter answer, and verbal or numerical ground truth. Of the eight tasks in VSI-Bench, the QA-pairs for seven tasks are derived from the unified meta-information and the Route Plan QApairs from human-annotated routes. We evaluate the multiple-choice answer (MCA) tasks via accuracy and the numerical-answer (NA) tasks via mean relative accuracy (MRA), but our VQA dataset also includes generated multiple-choice options and letter answers for the NA tasks. The generated multiple-choice options are sampled between lower and upper bound factor of the ground truth numerical answer and are re-sampled if any two options are within given threshold of each other. We sub-sample the number of questions for each scene for each task to prevent over-representation of any scene or task and to create more balanced dataset. For MCA tasks, the letter answers are distributed as uniformly as possible. For the object counting task, objects with counts of one are not included. For the relative distance task, only uniqueinstance objects are used for the primary category; multipleinstance objects are allowed for the object choices. If there are multiple instances of an object category, the minimum absolute distance to the primary object is used. If any of the four option distances are within threshold (30 cm for rooms with size greater than 40 sq m, 15 cm otherwise) of each other, the question is considered ambiguous. For the relative direction task, to make sure the direction is clear, questions are considered ambiguous if they violate lower and upper bounds on the distance between any two objects or threshold for proximity to angle boundaries. For the appearance order task, first appearance is considered to be the timestamp where the number of object pixels cross set threshold, and timestamps too close together are considered ambiguous. For the object size task, the ground truth is taken as the longest dimension of the unique objects bounding box. For the room size task, room size is calculated by the alpha shape algorithm, as specified earlier. For the absolute distance task, we first uniformly sample points within the bounding boxes of the two objects. The distance is the minimum Euclidean distance among pairwise points. For the route planning task, humans construct routes given template and instructions to choose any two unique objects as the start and end position, respectively, such that the route between them can be described in approximately two to five movements. Routes are comprised of two actions: Go forward until [unique object] and Turn [left / right / back]. After collection, filtering and standardization are done. In the question, the \"turn\" directions are replaced with [please fill in]."
        },
        {
            "title": "Task Question Template",
            "content": "Object Counting How many {category}(s) are in this room? Relative Distance Measuring from the closest point of each object, which of these objects ({choice a}, {choice b}, {choice c}, {choice d}) is the closest to the {category}?"
        },
        {
            "title": "Relative Direction",
            "content": "To create comprehensive test of relative direction, three difficulty levels were created: Easy: If am standing by the {positioning object} and facing the {orienting object}, is the {querying object} to the left or the right of the {orienting object}? Medium: If am standing by the {positioning object} and facing the {orienting object}, is the {querying object} to my left, right, or back? An object is to my back if would have to turn at least 135 degrees in order to face it. Hard: If am standing by the {positioning object} and facing the {orienting object}, is the {querying object} to my front-left, front-right, back-left, or back-right? Directions refer to the quadrants of Cartesian plane (assuming am at the origin and facing the positive y-axis). Appearance Order What will be the first-time appearance order of the following categories in the video: {choice a}, {choice b}, {choice c}, {choice d}? Object Size What is the length of the longest dimension (length, width, or height) of the {category}, measured in centimeters? Absolute Distance Measuring from the closest point of each object, what is the direct distance between the {object 1} and the {object 2} (in meters)? Room Size What is the size of this room (in square meters)? If multiple rooms are shown, estimate the size of the combined space."
        },
        {
            "title": "Route Plan",
            "content": "You are robot beginning at {the bed facing the tv}. You want to navigate to {the toilet}. You will perform the following actions (Note: for each [please fill in], choose either turn back, turn left, or turn right.): {1. Go forward until the TV 2. [please fill in] 3. Go forward until the shower 4. [please fill in] 5. Go forward until the toilet.} You have reached the final destination. Table 4. Question Templates for tasks in VSI-Bench. We replace the highlighted part in the question template from scene to scene to construct our benchmark. Note that complete example question is provided for Route Plan."
        },
        {
            "title": "The question templates for the generation of each task",
            "content": "B.2. Probing MLLM via Self-Explanations are listed in Tab. 4. Human-in-the-loop Quality Review. The quality review process occurs throughout two stages of our pipeline. During dataset collection, we manually filter the validation set by removing scenes with high ratio of incomplete 3D mesh reconstruction that could misalign 3D annotations with visible video content. After generating scene metainformation, we manually verify its correctness, with specific focus on ensuring the correctness of object counts. In the QA pairs generation stage, we customize web interface for human quality review. Human evaluators are asked to answer the benchmark questions without prior knowledge of the correct answers. They flag QA pairs where they believe the answers are incorrect. When evaluators identify ambiguous or erroneous questions, we trace the source of the errors and take corrective actions, such as removing problematic data samples or adjusting the metainformation, question templates, or modifying QA generation rules to prevent similar issues in the future. We iterate this procedure multiple times to ensure the quality. Here, we provide more concrete implementations for the self-explanations and error analysis. Self-Explanations. To conduct error analysis on models reasoning chains behind its predictions, we explicitly extract the reasoning chains that support the models questionanswering process. Specifically, after the model predicts an answer to given question, it is further prompted with Please explain your answer step by step. to generate the internal rationale leading to its prediction. It is important to note that this process is fundamentally different from Chain-of-Thought reasoning, where the model is asked to generate reasoning chains first and then predict the answer. Error Analysis. For error analysis, we manually review within VSI-Bench (tiny) all error cases for tasks in multiple-choice answers and the bottom half of the worstperforming cases for tasks in numerical answers, which totals 163 samples. For each error case, human examiners are required to classify its primary error into one of four primary categories: visual perception error, linguistic intel14 ligence error, relational reasoning error, and egocentricallocentric transformation error. If an incorrect prediction is attributed to multiple reasons, it is proportionally assigned as 1 to each applicable category, where is the number of error categories. B.4. Cognitive Map Generation. To generate the cognitive map for each video, we specify the target categories of interest and prompt the MLLM to predict the central position for each of these categories. The following prompt is used: B.3. Implementation Details of CoT Methods"
        },
        {
            "title": "Cognitive Map Prompt",
            "content": "Self-Consistency w/ CoT. As detailed in our paper, we evaluate several advanced linguistic prompting methods on our benchmark, including Chain-of-Thought, Self-Consistency, and Tree-of-Thoughts. In this section, we elaborate on the implementation details of these three methods. Chain-of-Thought prompting. line with Following Zero-shotCoT [37, 86], we append the phrase Lets think step by step. to each question to elicit step-by-step reasoning from the large language model. The temperature, top-p, and top-k parameters are set to 0, 1, and 1, respectively. After the model generates its prediction, we initiate an additional turn of dialogue to prompt the model to extract its answer explicitly (e.g., the letter corresponding to the correct option for multiple-choice questions or numerical value for numerical questions). This approach mitigates errors arising from fuzzy matching. In SelfConsistency [84], we prompt MLLMs to generate multiple answers for given question under Zero-shotCoT [37] prompting. To encourage diversity among runs, we set the temperature to 0.7, top-p to 1, and top-k to 40. Initially, the model is prompted to provide an answer with step-by-step reasoning (using Zero-shot-CoT). As with Zero-shot-CoT, an additional dialogue turn is added to explicitly extract the prediction from the models response. For each question, we perform 5 independent runs and take the majority prediction as the final answer. Inspired by the Creative Writing practice in [92], we divide the problem-solving process into two steps: plan generation and answer prediction. The temperature, top-p, and top-k parameters remain consistent with the Self-Consistency setup. For the plan generation step, we ask the model to generate 3 distinct plans to answer the given question. We then start new dialogue and prompt the model to select the most promising plan based on the video, the question and the generated plans. This voting process is repeated 3 times, with the majority-selected plan chosen for the next step. In the answer prediction step, based on the video and the selected plan, the model is asked to predict the answer. Similar to the previous step, 3 independent predictions are generated, and the model votes 3 times to determine the most confident answer. majority vote determines the final prediction. Tree-of-Thoughts. Fig. 15. Fig. 16, and Fig. 17 illustrate these three prompting techniques and model outputs under the different strategies. 15 [Task] This video captures an indoor scene. Your objective is to identify specific objects within the video, understand the spatial arrangement of the scene, and estimate the center point of each object, assuming the entire scene is represented by 10x10 grid. [Rule] 1. We provide the categories to care about in this scene: {categories_of_interest}. Focus ONLY on these categories. 2. Estimate the center location of each instance within the provided categories, assuming the entire scene is represented by 10x10 grid. 3. If category contains multiple instances, include all of them. 4. Each objects estimated location should acits real position in the scene, curately reflect preserving the relative spatial relationships among all objects. [Output] Present the estimated center locations for each object as list within dictionary. STRICTLY follow this JSON format: {\"category name\": [(x_1, y_1), ...], ...} For the categories of interest, we include all potential categories as shown in Fig. 10 and Fig. 11. Such setup facilitates our focus on assessing the spatial awareness of the MLLM rather than its perceptual capabilities. In contrast, for benchmark tasks such as evaluating relative distance (as shown in Tab. 3), we restrict the provided categories to those explicitly mentioned in each question. This ensures that no additional information apart from the question is included. Distance Locality Calculation. To quantitatively evaluate the cognitive maps, we measure inter-category distances as illustrated in Fig. 11. Specifically, for each category, we compute its Euclidean distance to all other categories. When category contains multiple objects, we define the inter-category distance as the shortest distance between any two objects from the respective categories. We perform these distance calculations on both MLLM-predicted and ground truth cognitive maps and consider an MLLMs predicted distance between two categories to be correct if it differs from the ground truth distance by no more than one grid unit. We apply this evaluation process across all cognitive maps and group the distances into eight bins to calculate the average accuracy on different bins. C. Evaluation Details C.1. General Evaluation Setup Our evaluation processes are primarily conducted using the LMMs-Eval project [97]. To ensure reproducibility, unless otherwise specified, we adopt greedy decoding strategy for all models (i.e., the temperature is set to 0, and both top-p and top-k are set to 1). The input for the models is formatted as follows: [Video Frames][Pre-prompt][Question][Post-prompt], where Question includes the question and any available options. The specific Pre-prompt and Post-prompt for different models and question types are detailed in Tab. 7. C.2. Human Evaluation Setup During the evaluation of human-level performance on VSI-Bench (tiny), human evaluators are allowed unlimited time to answer questions to the best of their ability. They receive both the questions and corresponding videos simultaneously and can review the videos multiple times to gather comprehensive information. We do not restrict the number of times evaluators can review videos for two key reasons. First, MLLMs auto-regressively generate answers, enabling them to analyze videos repeatedly during the response generation process. Second, MLLMs are designed to achieve and exceed typical human-level performance for practical real-world applications. C.3. Number of Frames Setup Typically, MLLMs subsample fixed number of frames for evaluation. For all open-source models and the GPT-4 API, following [97], we manually sample video frames from the entire video at evenly spaced time intervals. For the Gemini API, we follow its instructions, uploading and feeding the entire video to the model. The number of frames used for each model are provided in Tab. 6. C.4. More Evaluation Results Here, we provide more evaluation results on our benchmark, including the full evaluation results of VSI-Bench (tiny), blind evaluation results, and vision-enabled vision-disabled results. VSI-Bench (tiny) Results. As shown in Tab. 8, we provide the evaluation results of all models on VSI-Bench (tiny). The rankings and average accuracy of MLLMs on VSI-Bench (tiny) remain consistent to the results reported in Tab. 1. This consistency suggests that the human evaluation and analysis results conducted on VSI-Bench (tiny) are reliable. Order Video first Question first Avg. 48.8 46.3 (a) Input Sequence # Times Avg. 48.8 1 50.9 2 (b) Video Repetition Times Table 5. Ablations on the video input sequence and repetition. Blind Evaluation. As shown in Tab. 9, we present the evaluation results for all MLLMs on VSI-Bench. Generally, larger variants within the same model family often demonstrate better performance in blind evaluations, as seen in comparisons such as Gemini-1.5 Flash vs. Gemini1.5 Pro and VILA-1.5-8B vs. VILA-1.5-40B. The blind evaluation also highlights LLM biases across tasks. For instance, LongVILA-8B achieves 47.5% accuracy on the object count task, benefiting from bias that frequently leads it to predict 2 as the answer. Vision Enabled Vision Disabled. Tab. 10 presents the improvement of MLLMs from using visual signals to answer VSI-Bench. Almost all MLLMs obtain improvements from visual signals, with notable improvements in tasks such as object count, room size, relative distance and appearance order. D. Input Sequencing and Repetition Analysis Human performance in visual problem-solving improves when they know the question before viewing the visual content, as it helps direct their attention to relevant visual cues. However, current MLLMs typically rely on visualfirst paradigm [47, 76], leading us to examine how the presentation order of video-question pairs impacts model performance. To investigate, we conduct experiments using Gemini-1.5 Pro on VSI-Bench (tiny). MLLMs performance degrades with question-first paradigm. As shown in Tab. 5 (a), switching to video-first approach results in 2.5% decrease in overall performance for Gemini compared to the question-first approach. MLLM benefits from multiple video views. In addition, humans often improve their VQA performance by reviewing visual content multiple times, inspiring us to implement similar setup for MLLMs. As shown in Tab. 5 (b), Gemini achieves notable 2.1% performance gain with two repeated videos as input. This is surprising, as autoregressive MLLMs theoretically have the capability to revisit the video multiple times during answer generation, even if the video is only presented once. This finding suggests that, despite its remarkable capabilities, powerful MLLM like Gemini still has suboptimal reasoning processes for Video QA. 16 ate three potential answers, with the final output determined through majority vote. E.4. Cognitive Map Examples In Fig. 18, we include 10 additional cognitive maps and pair each prediction with its corresponding ground truth map to provide insight into the alignment between predicted and ground truth layouts. Methods # of Frames Proprietary Models (API) GPT-4o Gemini-1.5 Flash Gemini-1.5 Pro Open-source Models InternVL2-2B InternVL2-8B InternVL2-40B LongVILA-8B VILA-1.5-8B VILA-1.5-40B LongVA-7B LLaVA-NeXT-Video-7B LLaVA-NeXT-Video-72B LLaVA-OneVision-0.5B LLaVA-OneVision-7B LLaVA-OneVision-72B 16 - - 8 8 8 32 32 32 32 32 32 32 32 32 Table 6. Number of frames used in evaluation. E. Visualization Results In this section, we present more qualitative results, including more examples of VSI-Bench, further error analysis case studies, examples of Chain-of-Thought promptings, and additional cognitive maps. E.1. VSI-Bench Examples In Fig. 12 and Fig. 13, we provide more examples from VSI-Bench to illustrate the structure and format of tasks, questions, and answers. E.2. Error Analysis Examples In Fig. 14, we present more case studies for our humanconducted error analysis on VSI-Bench. In the error analysis, we identify the categorized error types and highlight the relevant parts of the explanation. E.3. Linguistic Prompting Examples We provide examples for the three CoT prompting methods discussed in Sec. 5.2 to illustrate their concrete reasoning procedure in detail. We include examples of three selected tasks: object count, object size, and room size. For ZeroShot Chain of Thought, as shown in Fig. 15, we highlight each step of the MLLMs reasoning process to offer insights into how it arrives at its final decision. For Self-Consistency w/ CoT, as illustrated in Fig. 16, each example is paired with five independent responses. The final answer is then determined by majority vote. For Tree-of-Thought, Fig. 17 details how each depth of the decision tree is reached. At the first depth, the MLLM generates three potential plans and conducts choice analysis to select the optimal plan. At the second and final depth, the selected plan is used to gener17 Figure 12. VSI-Bench Examples (Part 1). 18 Figure 13. VSI-Bench Examples (Part 2). 19 Figure 14. Additional Error Analysis Examples. 20 Figure 15. Zero-Shot CoT Examples. Figure 16. Self-Consistency w/ CoT Examples. 22 Figure 17. Tree-of-Thought Examples. 23 Figure 18. Additional predicted cognitive map examples. 24 Pre-Prompt Post-Prompt Models - Open-source Models Proprietary Models QA. Type - NA MCA NA MCA Prompt These are frames of video. Please answer the question using single word or phrase. Answer with the options letter from the given choices directly. Do not respond with anything other than single number! Answer with the options letter from the given choices directly. Table 7. Prompts used in evaluation. NA and MAC indicates questions with Numerical Answer and Multiple Choice Answer respectively. u j. t. s. z j. Avg."
        },
        {
            "title": "Numerical Answer",
            "content": "35.6 45.7 48.8 45.4 25.5 32.9 37.6 19.1 31.4 32.3 31.8 35.7 39.3 27.7 33.8 41.6 36.2 50.8 49.6 52.4 30.6 26.4 40.8 23.4 12.2 14.6 41.2 49.0 41.4 44.0 48.2 38.0 4.6 33.6 28.8 30.6 20.4 25.4 23.8 10.8 23.4 21.0 17.4 12.8 26.6 23.0 22.0 31. 47.2 56.5 58.6 66.7 26.0 43.8 48.0 11.4 51.4 48.0 39.6 48.6 55.6 18.8 44.4 54.4 S R 40.4 45.2 49.4 31.8 29.6 41.6 26.0 0.0 18.6 20.6 25.4 21.4 31.6 28.4 14.0 35.2 Methods Proprietary Models (API) GPT-4o Gemini-1.5 Flash Gemini-1.5 Pro Gemini-2.0 Flash Open-source Models InternVL2-2B InternVL2-8B InternVL2-40B LongVILA-8B VILA-1.5-8B VILA-1.5-40B LongVA-7B LLaVA-NeXT-Video-7B LLaVA-NeXT-Video-72B LLaVA-OneVision-0.5B LLaVA-OneVision-7B LLaVA-OneVision-72B t. l. e r. Multiple-Choice Answer P o r. l. 40.0 48.0 46.0 56.0 28.0 30.0 46.0 20.0 36.0 42.0 30.0 40.0 36.0 30.0 44.0 44.0 46.2 39.8 48.1 46.3 39.2 32.2 30.1 33.1 41.5 22.0 52.8 43.5 25.6 33.4 31.9 39.7 32.0 32.7 42.0 24.5 28.0 20.0 42.0 28.0 42.0 40.0 34.0 34.0 42.0 36.0 34.0 32. 38.0 59.2 68.0 55.1 2.0 44.0 44.0 26.0 26.0 50.0 14.0 36.0 56.0 8.0 32.0 58.0 Table 8. Complete VSI-Bench (tiny) evaluation results. 25 Methods Proprietary Models (API) GPT-4o Gemini-1.5 Flash Gemini-1.5 Pro Open-source Models InternVL2-2B InternVL2-8B InternVL2-40B LongVILA-8B VILA-1.5-8B VILA-1.5-40B LongVA-7B LLaVA-NeXT-Video-7B LLaVA-NeXT-Video-72B LLaVA-OneVision-0.5B LLaVA-OneVision-7B LLaVA-OneVision-72B o j. t. s. z j. Avg."
        },
        {
            "title": "Numerical Answer",
            "content": "14.5 19.9 32.3 17.8 27.6 24.4 20.2 21.5 25.5 21.9 25.2 29.1 28.6 25.3 28.9 0.1 25.0 30.6 5.4 31.9 5.4 47.4 7.4 5.3 5.1 14.8 19.0 38.4 13.8 8.2 5.2 30.3 11.5 23.7 26.8 29.1 12.6 7.6 27.6 18.1 14.6 25.4 30.1 8.5 23. 36.7 52.5 51.5 9.2 38.3 39.2 8.7 45.7 46.5 27.4 32.5 46.3 32.0 45.5 54.1 S R 0.0 0.0 33.1 0.0 0.7 0.7 0.6 0.0 0.7 26.1 26.1 26.1 24.3 26.1 26.1 Table 9. Complete blind evaluation results. u j. t. s. z j. Avg."
        },
        {
            "title": "Numerical Answer",
            "content": "19.5 22.2 13.0 9.6 7.0 11.6 1.4 7.3 5.7 7.2 10.5 11.7 -0.5 7.0 11.4 46.1 24.9 25.5 16.4 -8.8 29.6 -18.2 10.0 17.1 32.9 33.8 29.9 7.8 33.9 35.4 0.1 0.5 19.5 1.2 1.9 -2.2 -3.5 14.2 -2.8 -1.5 -0.6 -2.6 -1.7 11.7 0. 7.1 1.0 12.6 12.8 9.9 7.3 7.9 4.6 2.2 11.5 15.2 11.1 -16.6 1.9 3.5 S R 38.2 54.4 10.6 35.0 39.1 31.1 -0.6 18.8 22.0 -3.9 -1.9 9.2 4.0 -13.9 11.4 Methods Proprietary Models (API) GPT-4o Gemini-1.5 Flash Gemini-1.5 Pro Open-source Models InternVL2-2B InternVL2-8B InternVL2-40B LongVILA-8B VILA-1.5-8B VILA-1.5-40B LongVA-7B LLaVA-NeXT-Video-7B LLaVA-NeXT-Video-72B LLaVA-OneVision-0.5B LLaVA-OneVision-7B LLaVA-OneVision-72B t. l. e r. Multiple-Choice Answer P o r. l. 10.8 0.0 33.8 26.9 27.1 30.3 24.3 25.4 30.2 23.4 26.8 29.0 22.0 28.6 30.4 23.2 21.2 44.6 41.2 39.2 37.7 27.0 39.1 37.1 39.8 45.0 38.8 41.8 41.2 38.1 26.9 29.9 33.5 27.9 33.0 27.9 27.4 29.4 31.5 26.9 33.0 33.0 34.5 27.9 33. 13.1 0.2 20.2 7.9 23.6 24.7 13.9 17.6 25.0 8.7 8.5 15.5 5.4 11.1 17.1 t. l. e r. Multiple-Choice Answer P o r. l. 26.2 37.7 17.5 6.9 9.7 11.8 5.3 6.7 10.4 9.7 16.7 13.3 6.9 13.9 12.1 18.0 19.9 1.7 3.0 -8.5 -5.5 3.7 -4.4 -11.4 3.5 -2.7 -2.0 -5.0 -6.0 1.8 4.6 1.5 2.5 2.5 -3.0 6.1 5.1 1.5 0.0 -1.5 1.0 2.0 0.0 1.5 -0. 15.4 37.7 14.4 -0.8 16.0 14.9 11.5 7.2 7.9 7.1 22.1 33.0 0.3 13.3 27.4 Table 10. Results of Vision Enabled Vision Disabled."
        }
    ],
    "affiliations": [
        "New York University",
        "Stanford University",
        "Yale University"
    ]
}