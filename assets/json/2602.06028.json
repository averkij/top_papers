{
    "paper_title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
    "authors": [
        "Shuo Chen",
        "Cong Wei",
        "Sun Sun",
        "Ping Nie",
        "Kai Zhou",
        "Ge Zhang",
        "Ming-Hsuan Yang",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \\textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \\textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \\textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 8 2 0 6 0 . 2 0 6 2 : r Context Forcing: Consistent Autoregressive Video Generation with Long Context Shuo Chen * 1 Cong Wei * 2 Sun Sun 2 Ping Nie 2 Kai Zhou 3 Ge Zhang 4 Ming-Hsuan Yang 1 Wenhu Chen 2 (cid:128) Website: https://chenshuo20.github.io/Context Forcing Code: https://github.com/TIGER-AI-Lab/Context-Forcing Figure 1. Context Forcing mitigates the forgettingdrifting dilemma. (1) State-of-the-art models are limited by short context windows (3.09.2 s), which leads to poor long-term consistency (Forgetting). (2) For streaming long-context tuning baselines (e.g., LongLive), enlarging the context window during inference (3.0 5.25 s) causes error accumulation and distribution shift (Drifting). In contrast, Context Forcing supports 20s+ context while maintaining strong long-term consistency."
        },
        {
            "title": "Abstract",
            "content": "Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train long-context student using short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from teacher limited to short 5-second windows. This structural discrep- *Equal contribution 1Department of EECS, University of California, Merced, USA 2University of Waterloo, Canada 3Netmind.AI to: Ming-Hsuan Yang <mhyang@ucmerced.edu>, Wenhu Chen <wenhuchen@uwaterloo.ca>. 4M-A-P. Correspondence Preprint. February 6, 2026. 1 ancy creates critical student-teacher mismatch: the teachers inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the students context length. To resolve this, we propose Context Forcing, novel framework that trains long-context student via long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce context management system that transforms the linearly growing context into Slow-Fast MemContext Forcing: Consistent Autoregressive Video Generation with Long Context ory architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds210 longer than state-of-theart methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics. 1. Introduction In recent years, video diffusion models based on architectures such as the Denoising Diffusion Transformer(DiT) (Peebles & Xie, 2023) have achieved remarkable success in generating photorealistic videos (Wan et al., 2025). While bidirectional models perform well for short clips, their computational cost limits long-form generation. To address this, the field is moving toward causal video architectures (Yin et al., 2024c; Huang et al., 2025), which, like Large Language Models, can theoretically generate infinite-length videos by predicting future frames from past context. Despite this promise, current causal video models struggle to maintain coherence over long-term contexts. Effective context is often limited to just few seconds (Cui et al., 2025; Yang et al., 2025; Zhang & Agrawala, 2025; Huang et al., 2025; Yesiltepe et al., 2025), beyond which identity shifts and temporal inconsistencies emerge. We identify the root cause as fundamental student-teacher mismatch. As illustrated in Figure 2(b), current methods typically train student to perform long rollouts using supervision from memoryless teacher limited to short windows (e.g., 5 seconds). The teachers inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the students learnable context length. This mismatch results in critical challenge for realtime long-context video generation, which we term the Forgetting-Drifting Dilemma (Figure 1). Existing methods face an unavoidable trade-off: Forgetting: Restricting the model to short memory window minimizes error accumulation but causes the model to lose track of previous subjects and scenes during long rollout. Drifting: Maintaining long context preserves identity but exposes the model to its own accumulated errors. Without teacher capable of correcting these long-term deviations, the video distribution progressively drifts away from the real manifold. To address these challenges, we propose Context Forcing, 2 framework that distills long-context teacher into longcontext student. Our approach resolves the context-drifting dilemma by bridging the capability gap between teacher and student. We first leverage Context Teacher pretrained on video continuation tasks, which is capable of processing long-context inputs. This teacher guides the student via Contextual Distribution Matching Distillation, explicitly transferring the ability to model long-term dependencies and ensuring global consistency. Furthermore, by exposing the student to imperfect, self-generated contexts during training, we enable it to actively recover from accumulated artifacts. The resulting robustness allows for 2 10 longer duration Key-Value (KV) cache management (maintaining 20+ seconds of history) compared to prior SOTA (1.59.2 seconds of history) during inference, effectively addressing the forgetting-drifting trade-off and enabling consistent, long-form video generation. The contributions of this work are: We introduce Context Forcing, novel framework that mitigates the student-teacher mismatch in training real-time long video models. By distilling from longcontext teacher aware of the full generation history, we enable the robust training of long-context student capable of long-term consistency. To support this, we design context management system that transforms the linearly growing context into Slow-Fast Memory architecture, significantly reducing visual redundancy. This mechanism enables effective context lengths exceeding 20 seconds210 longer than state-of-the-art methods. We demonstrate that, equipped with these extended context lengths, our model preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics. 2. Related Work Long Video Generation. The high computational cost of Diffusion Transformers (DiTs) (Kong et al., 2024; Wan et al., 2025; Peebles & Xie, 2023; Yang et al., 2024) has limited video generation to short clips. To extend temporal horizons, many works combine diffusion with autoregressive (AR) prediction (Kim et al., 2024; Lin et al., 2025; Gu et al., 2025), including NOVA (Deng et al., 2024), PyramidFlow (Jin et al., 2024), and MAGI-1 (Teng et al., 2025). Other approaches improve efficiency via causal or windowed attention and KV caching (Yin et al., 2024c; Huang et al., 2025; Kodaira et al., 2025), or extend context through training-free positional encoding modifications (Lu et al., 2024; Lu & Yang, 2025; Zhao et al., 2025). However, most methods still struggle with global consistency beyond 10-20 seconds. key challenge of long video generation is error accumulation (drifting), addressed either during training by Context Forcing: Consistent Autoregressive Video Generation with Long Context Figure 2. Training paradigms for AR video diffusion models. (a) Self-forcing: student matches teacher capable of generating only 5s video using 5s self-rollout. (b) Longlive (Yang et al., 2025): The student performs long rollouts supervised by memoryless 5s teacher on random chunks. The teachers inability to see beyond its 5s window creates student-teacher mismatch. (c) Context Forcing (Ours): The student is supervised by long-context teacher aware of the full generation history, resolving the mismatch in (b). exposing models to drifted inputs (Cui et al., 2025; Chen et al., 2024; 2025) or during inference via recaching, sampling strategies, or feedback (Yang et al., 2025; Zhang & Agrawala, 2025; Li et al., 2025a). To enable real-time generation, recent works distill multi-step diffusion into few-step models (Valevski et al., 2024; Liu et al., 2023; Luo et al., 2023; Sauer et al., 2024), including Distribution Matching Distillation (DMD/DMD2) (Yin et al., 2024b;a; Wang et al., 2023) and Consistency Models (CM) (Song et al., 2023; Wang et al., 2024). mechanisms are key to extending temporal context and maintaining consistency in long-horizon generation. WorldPlay (Sun et al., 2025), Context as Memory (Yu et al., 2025), and WorldMem (Xiao et al., 2025) and Framepack (Zhang & Agrawala, 2025) introduce explicit memory structures to accumulate scene or contextual information over time, while RELIC (Hong et al., 2025) employs recurrent latent states for efficient long-range dependency modeling. PFP (Zhang et al., 2026) compress long videos into short context by training novel compression module. Causal Video Generation. Causal video generation synthesizes video sequences under strict temporal ordering constraints, thereby enabling streaming inference and longhorizon synthesis. Although early autoregressive models (Vondrick et al., 2016; Kalchbrenner et al., 2017) generated frames or tokens sequentially, they often suffered from error accumulation and poor scalability. Recent diffusionbased frameworks have improved visual fidelity by incorporating causal architectural priors, such as the blockwise causal attention introduced in CausVid (Yin et al., 2024c). To mitigate distribution shift, Self-Forcing (Huang et al., 2025), LongLive (Yang et al., 2025) and SelfForcing++ (Cui et al., 2025) align training with inference by conditioning on prior outputs via KV caching and rolloutbased objectives. InfinityRoPE (Yesiltepe et al., 2025) achieve reduction of error accumulation by modifying positional encodings. Further research has addressed efficient long-context inference through windowed attention, as seen in StreamDiT (Kodaira et al., 2025). Memory Mechanism for Video Generation Memory 3. Methodology We operate within the causal autoregressive framework, where the generation of long video X1:N is decomposed into sequence of conditional steps over frames or short chunks Xt. State-of-the-art methods, such as CausVid (Yin et al., 2024c) and Self-Forcing (Huang et al., 2025), enforce strict temporal causality via block-wise attention, modeling the distribution as (cid:81) p(Xt X<t). These approaches typically employ Distribution Matching Distillation (DMD) (Yin et al., 2024b) to distill high-quality bidirectional teacher into causal student. Building on these foundations, we introduce Context Forcing. Our goal is to train causal video diffusion model, parameterized by θ, whose induced distribution over long videos pθ(X1:N ) matches the real data distribution pdata(X1:N ). Here, represents duration spanning tens or hundreds of seconds. The objective is to minimize the global longhorizon KL divergence: 3 Context Forcing: Consistent Autoregressive Video Generation with Long Context Figure 3. Context Forcing and Context Management System. We use KV Cache as the context memory, and we organize it into three parts: sink, slow memory and fast memory. During contextual DMD training, the long teacher provides supervision to the long student by utilizing the same context memory mechanism. Lglobal = min θ KL(cid:0)pθ(X1:N ) pdata(X1:N )(cid:1). (1) Directly optimizing Eq. (1) ensures long-term coherence but is computationally intractable for large . By applying the chain rule of KL divergence, we decompose the global objective into two components: (cid:124) Lglobal = KL(cid:0)pθ(X1:k) pdata(X1:k)(cid:1) (cid:125) KL(cid:0)pθ(Xk+1:N X1:k) pdata(Xk+1:N X1:k)(cid:1)(cid:105) (cid:125) (cid:123)(cid:122) Llocal: Local Dynamics + EX1:kpθ (cid:123)(cid:122) Lcontext: Global Continuation Dynamics (cid:104) (cid:124) (2) This decomposition motivates our two-stage curriculum: Stage 1 (Optimizing Llocal): We match the distribution of short windows (X1:k) to the real data distribution to learn local dynamics. Stage 2 (Optimizing Lcontext): We match the models continuation predictions (Xk+1:N ) with the temporal evolution of real data to learn long-term dependencies. 3.1. Stage 1: Local Distribution Matching The first stage warms up the causal student by minimizing Llocal. Given teacher distribution pT (X1:k) (approximately the real data), we optimize: Llocal = KL(cid:0)pθ(X1:k) pT (X1:k)(cid:1), (3) where corresponds to 15 second window. We estimate the distribution matching gradient follow DMD (Yin et al., 2024b). Let = Gθ(z) for noise z, and let xt be the 4 diffused version of at timestep t. The gradient is given by: (cid:104) wtαt θLlocal Ez,t,xt (cid:0)sθ(xt, t) sT (xt, t)(cid:1) Gθ(z) , (4) where sθ and sT are the student and teacher scores, respectively, and wt is weighting function. This stage ensures pθ(X1:k) pdata(X1:k), providing high-quality contexts for the subsequent stage. θ (cid:105) 3.2. Stage 2: Contextual Distribution Matching Stage 2 targets Lcontext, the second term of Eq. (2). This term requires minimizing the divergence between the students continuation pθ(X1:k) and the true data continuation pdata(X1:k). However, pdata is not directly accessible for arbitrary contexts generated by the student. To solve this, we employ pretrained Context Teacher , which provides reliable proxy distribution pT (Xk+1:N X1:k). We rely on two key assumptions to justify using the teacher as target: Assumption 1 (Teacher reliability near student contexts). Whenever the student context X1:k pθ(X1:k) remains close to the real data manifold, the teachers continuation pT (Xk+1:N X1:k) is accurate. This holds whenever the teacher is well-trained on real video prefixes. Assumption 2 (Approximate real prefixes). Stage 1 successfully aligns pθ(X1:k) with pdata(X1:k). This ensures that student rollouts remain within the teachers reliable region during Stage 2 training. Under these assumptions, we approximate pdata pT in the second term of Eq. (2), yielding the Contextual DMD Context Forcing: Consistent Autoregressive Video Generation with Long Context Figure 4. Comparison on 1-min Video Generation. Our method keeps both the background and subject consistent across 1-min video, while other baselines have different levels drifting or identity shift. (CDMD) objective: LCDMD = EX1:kpθ(X1:k) (cid:104) KL(cid:0)pθ(Xk+1:N X1:k) pT (Xk+1:N X1:k)(cid:1)(cid:105) (5) Crucially, the expectation is over X1:k pθ, ensuring the student is trained on its own rollouts, thereby mitigating exposure bias. Score-based CDMD Gradient. We estimate the gradient of Eq. (5) using conditional variant of the DMD gradient. Let xcont = Gθ(zcont X1:k) be the generated continuation, and xt,cont be its diffused version. Running both fake score and real score models on the same student-generated context produces scores sθ( X1:k) and sT ( X1:k). The gradient is: θLCDMD EX1:kpθ zcont,t (cid:104) wtαt (cid:0)sθ(xt,cont, X1:k) sT (xt,cont, X1:k)(cid:1) Gθ(zcont X1:k) θ (cid:105) . (6) By descending Eq. (6), we align the students long-term autoregressive dynamics with the teachers robust priors. Long Self-Rollout Curriculum. Minimizing Lcontext requires the context horizon to approach the full sequence length . However, sampling X1:k pθ for large early in training causes severe distribution shift due to accumulated drift. To mitigate this, we employ dynamic horizon schedule (t) max that grows linearly with training step t. At each iteration, the rollout length is sampled as U(kmin, (t) max). This curriculum initializes training in the stable Stage 1 regime (k kmin) and progressively exposes the model to long-range dependencies. Clean Context Policy. Self Forcing (Huang et al., 2025) typically generates rollouts using random timestep selection strategy to ensure supervision across all diffusion steps. We retain this random exit policy for the target frames Xk+1:N to preserve gradient coverage, but enforce that the context frames X1:k are fully denoised. We apply complete few-step denoising process to the context. This decoupling ensures the context remains informative and aligned with the teachers training distribution but also maintains supervision for every diffusion step. 3.3. Context Management System Our teacher and student models share an identical architecture; both are autoregressive generative models augmented with memory module for context retention. We utilize KV caches to represent the context X1:k. To maintain efficiency as the sequence length grows, we design KV cache management system inspired by dual-process memory theories. Specifically, the cache is partitioned into three functional components: an Attention Sink, Slow Memory (Context), and Fast Memory (Local). Both the student and teacher are equipped with this system. Cache Partitioning. The total cache is defined as the union of disjoint sets: = Cslow Lfast. 5 Context Forcing: Consistent Autoregressive Video Generation with Long Context Figure 5. Qualitative Results of Context Forcing. Our method enables minute-level video generation with minimal drifting and high consistency across diverse scenarios. Attention Sink (S): Retains initial Nstokens to stabilize attention, following StreamingLLM (Yang et al., 2025; Shin et al., 2025). Slow Memory (Cslow): long-term buffer of up to Nc tokens, storing high-entropy keyframes and updating only with significant new information. Fast Memory (Lfast): rolling FIFO queue of size Nl, capturing immediate local context with short-term persistence. Surprisal-Based Consolidation. Upon generating new token xt and enqueuing it into the Fast Memory Lfast, we evaluate its informational value relative to the immediate temporal context. We postulate that tokens exhibiting high similarity to their predecessors carry redundant information (low surprisal), whereas dissimilar tokens indicate significant state transitions or visual changes (high surprisal). To capture these high-information moments efficiently, we compare the key vector kt of the current token with that of the immediately preceding token kt1. The consolidation policy π(xt) determines whether xt is promoted to Slow Memory Cslow: π(xt) = (cid:40) Consolidate Discard if sim(kt, kt1) < τ, otherwise, (7) where τ is similarity threshold. This criterion ensures that Cslow prioritizes storing temporal gradients and distinctive events rather than static redundancies. As with standard cache management, if Cslow > Nc after consolidation, the oldest entry is evicted to maintain fixed memory complexity. Bounded Positional Encoding. Unlike standard autoregressive video models (Huang et al., 2025; Cui et al., 2025), where positional indices grow unbounded (pt = ), leading to distribution shifts on long sequences, we adopt Bounded Positional Indexing. All tokens temporal RoPE positions are constrained to fixed range Φ = [0, Ns + Nc + Nl 1] regardless of generation step t: ϕ(x) = [0, Ns 1] [Ns, Nc 1] [Nc, Nc + Nl 1] if S, if Cslow, if Lfast. (8) This creates static attention window where recent history (Fast) slides through high indices, while salient history (Slow) is compressed into lower indices, stabilizing attention over long sequences. 3.4. Robust Context Teacher Training Standard training conditions the model on ground-truth context, but inference relies on self-generated history, creating distribution shift known as exposure bias. To ensure our Context Teacher provides robust guidance even when the student drifts, we adopt Error-Recycling Fine-Tuning (ERFT) (Li et al., 2025a). Rather than training on clean history X1:k, we inject realistic 6 Context Forcing: Consistent Autoregressive Video Generation with Long Context accumulated errors into the teachers context. We construct perturbed context X1:k = X1:k + edrift, where edrift is sampled from bank of past model residuals and is Bernoulli indicator. The teacher is optimized to recover the correct velocity vtarget from X1:k. This active correction capability ensures pT ( X1:k) remains reliable proxy for pdata even when the students context X1:k degrades. 4. Experiments Implementation Details. We implement the robust context teacher using Wan2.1-T2V-1.3B (Wan et al., 2025) as the base model. To construct the training dataset, we filter the Sekai (Li et al., 2025b) and Ultravideo (Xue et al., 2025) collections to retain high-quality videos exceeding 10 seconds in duration, yielding total of 40k clips. The robust context teacher is trained for 8k steps with batch size of 8. During training, frames are sampled uniformly from the 520 second interval of the video data to serve as context. The student model also utilizes the Wan2.1-T2V-1.3B model. In Stage 1, we employ 81-frame video clips from the VidProM (Wang & Yang, 2024) dataset and train for 600 iterations with batch size of 64. In Stage 2, which focuses on context distillation, we extend the rollout horizon to video lengths of 1030 seconds to address short-term memory limitations. This phase is similarly trained on the VidProM dataset for 500 iterations using the same batch size. For both teacher and student models, we set the KV cache size to 21 latent frames, and set Ns = 3, Nc = 12, Nl = 6, τ = 0.95. We implement Surprisal-Based Consolidation at 2-chunk intervals. Upon chunk consolidation, we retain only the first latent, effectively extending the context beyond 20s. Baselines. We evaluate our method against three distinct categories of baselines. The first category comprises bidirectional diffusion models, specifically LTX-Video (HaCohen et al., 2024) and Wan2.1 (Wan et al., 2025). The second category includes autoregressive models such as SkyReels-V2 (Chen et al., 2025), MAGI-1 (Teng et al., 2025), CausVid (Yin et al., 2024c), NOVA (Deng et al., 2024), Pyramid-Flow (Jin et al., 2024), and Self Forcing (Huang et al., 2025). The third category consists of recent methods targeting long video generation within autoregressive frameworks. These include LongLive (Yang et al., 2025) with context length of 3 seconds, Self Forcing++ (Cui et al., 2025), Rolling Forcing (Liu et al., 2025) with context length of 6 seconds, and Infinity-RoPE (Yesiltepe et al., 2025) with context length of 1.5 seconds. Finally we include long context baseline Framepack (Zhang & Agrawala, 2025) with context length of 9.2 seconds. Evaluation. We report performance on VBench (Zheng et al., 2025) following (Huang et al., 2025; Yang et al., 2025). Beyond standard benchmarks, we assess fine-grained Figure 6. Video Continuation with Robust Context Teacher. Context teacher can generate next segment videos with context generated by student. consistency using DINOv2 (Oquab et al., 2023) (structural identity), CLIP-F (Radford et al., 2021) (semantic context), and CLIP-T (prompt alignment). To improve robustness against temporal artifacts, we implement window-based sampling: for any timestamp t, we compute the average cosine similarity between the first frame (V0) and frames within [t 0.5s, + 0.5s]. We average results over five random seeds per prompt to ensure statistical reliability. This approach effectively measures long-term subject and background consistency. 4.1. Video Continuation with Robust Context Teacher To evaluate the context teacher, we feed the teacher model with videos generated by the student model after Stage 1 training. We then assess the consistency of the complete sequence, which comprises the initial context with the generated continuation. Evaluation is performed using 100 text prompts randomly sampled from MovieGenBench (Polyak et al., 2024). As illustrated in Figure 6, the context teacher effectively synthesizes the subsequent video segment, providing empirical support for Assumptions 1 and 2. Furthermore, we quantitatively evaluate the performance of the context teacher using student-generated videos as input, reporting subject and background consistency on VBench, as well as DINOv2, CLIP-F, and CLIP-T scores. The consistency metrics for the complete 10-second sequence are presented in Table 1, further demonstrating that the context teacher consistently produces reliable continuations from student-generated contexts. 7 Context Forcing: Consistent Autoregressive Video Generation with Long Context Table 1. Single-prompt 60-second long video consistency evaluation. Model Context Length Dino Score Clip-F Score Clip-T Score Background Subject 10s 20s 30s 40s 50s 60s 10s 20s 30s 40s 50s 60s 10s 20s 30s 40s 50s 60s Consistency Consistency FramePack-F1 LongLive Infinate RoPE Ours, teacher Ours, student 9.2s 3.0s 1.5s 20+s 20+s 81.86 91.25 91.18 87.61 91. 78.84 89.12 85.37 - 89.66 73.10 86.51 79.80 - 87.45 77.95 93.92 89.55 92.80 88.17 91.13 - - 95.05 89.25 Table 2. Comparison of video generation models across architecture families. 33.84 36.58 32.56 - 37.15 33.75 36.17 35.88 - 36.72 34.67 35.80 35.03 - 36. 36.36 36.95 35.26 35.93 37.12 89.36 94.82 88.88 - 95.35 92.47 93.12 86.11 - 94.92 92.44 91.50 89.71 - 93.88 93.42 94.25 91.71 - 94.75 95.41 95.74 94.09 95.52 95. 68.50 86.26 83.72 - 87.89 69.52 87.83 81.10 - 88.33 34.77 35.92 32.29 - 37.08 32.30 37.13 32.28 - 37.66 91.61 94.92 92.42 95.24 95.95 89.15 93.05 90.11 94.87 95. Model #Params Throughput (FPS) Evaluation scores on 5s Evaluation scores on 60s Total Quality Semantic Consistency Consistency Total Quality Semantic Consistency Consistency Background Subject Background Subject Bidirectional models LTX-Video Wan2. Autoregressive models SkyReels-V2 MAGI-1 CausVid NOVA Pyramid Flow Self Forcing, chunk-wise Long autoregressive models LongLive Self Forcing++ Rolling Forcing Infinity-RoPE Ours, student model 1.9B 1.3B 1.3B 4.5B 1.3B 0.6B 2B 1.3B 1.3B 1.3B 1.3B 1.3B 1.3B 8.98 0.78 0.49 0.19 17.0 0.88 6.7 17.0 20.7 17.0 15.8 17.0 17.0 80.00 84. 82.30 85.30 82.67 79.18 81.20 80.12 81.72 84.31 84.87 83.11 81.22 81.79 83.44 84.70 82.04 84.05 80.39 84.74 85.07 86.97 83.79 84.08 83. 84.98 70.79 80.09 74.53 67.74 69.80 79.05 69.62 81.28 76.47 80.37 69.78 75.87 77.29 95.30 96. 96.83 96.83 95.12 95.16 96.09 95.98 96.55 - 96.11 96.34 97.38 95.01 95.99 96.07 95.83 95.96 93.38 96.08 96.29 95.82 - 96.02 95. 96.84 - - - - 70.47 69.87 71.04 65.25 - 71.86 83.64 - 79.31 79.99 82. 75.30 76.12 76.80 70.25 - 77.20 84.53 - 81.87 80.81 83.55 - - 51.15 44.87 48.01 45.24 - 50.51 74.97 - 67.69 74. 76.10 - - 89.95 87.76 89.85 88.06 - 87.84 94.62 - 94.12 94.21 95.34 - - 84.99 79.46 86.75 77.50 - 83.60 93.88 - 93.10 93.05 94.88 4.2. Text-to-Short Video Generation Quantitative Results. We quantitatively compare our method against baselines. We evaluate 5-second video generation on the VBench dataset using its official extended prompts. The results summarized in Table 2 demonstrate that our method achieves performance comparable to the baselines on short video generation. 4.3. Text-to-Long Video Generation Qualitative Results. We evaluate our proposed method against baseline models on 60-second video generation, with qualitative results illustrated in Figure 2. By leveraging slow-fast memory architecture with KV cache size of 21 and context span exceeding 20s, our method achieves superior consistency and effectively mitigates content drifting compared to the baselines. Quantitative Results. We evaluate 60-second video generation performance on the VBench with results summarized in Table 2, using its offical extened prompts. Additionally, we report DINOv2, CLIP-F, and CLIP-T scores in Table 1, using 100 text prompts randomly sampled from MovieGenBench (Polyak et al., 2024), following the same experimental protocol as in Section 4.1. Both tables demonstrate that our method achieves high consistency, particularly during extended video sequences. Notably, while LongLive also achieves competitive scores, qualitative inspection reveals that it frequently exhibits abrupt scene resets and cyclic motion patterns, shown in Figure 8 in Appendix. Table 3. Ablation study on Slow Memory Sampling Strategy, Context DMD, and Bounded Positional Encoding (evaluated on 60s). Model Total Score Quality Score Semantic Score Background Consistency Subject Consistency Dynamic Degree Slow Memory Sampling Strategy Uniform sample, interval 1 Uniform sample, interval 2 Contextual Distillation w/o. Contextual Distillation Bounded Positional Encoding w/o. Bounded Positional Encoding Ours 80.82 81.11 82.20 82.61 75.32 75.12 92.45 93. 92.10 92.85 52.15 55.30 80.36 82.28 72.70 93. 93.20 48.12 73.52 75.44 65.82 82. 83.55 76.10 84.68 95.34 79.24 94. 27.45 58.26 4.4. Ablation Studies Slow Memory Sampling Strategy Our method employs selection strategy based on key-vector similarity to sample context from slow memory. Unlike fixed uniform sampling, this strategy dynamically selects historical chunks that exhibit low similarity to the current generation window, thereby preserving critical semantic information over time. We compare our approach against alternative baselines, specifically uniform sampling with intervals of 1 and 2 chunks. As summarized in Table 3, the results demonstrate the effectiveness of similarity-based selection in maintaining long-term consistency. Context DMD Distillation We evaluate the contribution of Contextual Distribution Matching Distillation by comparing our full model against training-free baseline. In the latter, our context management system is applied directly after Stage 1 training without the DMD process. The results in Table 3 indicate that removing Context DMD leads to degradation in both semantic and temporal conContext Forcing: Consistent Autoregressive Video Generation with Long Context"
        },
        {
            "title": "Impact Statement",
            "content": "This paper contributes to the advancement of generative AI by enhancing temporal consistency in long video generation. Our work enables the creation of more coherent and realistic visual sequences, which has significant positive potential in digital storytelling, filmmaking, world model and professional video editing. However, we acknowledge that the ability to generate highly consistent long-form videos also increases the risk of creating sophisticated synthetic media or deepfakes that could be used for misinformation. To mitigate these ethical concerns, we advocate for the integration of digital watermarking and provenance standards in downstream applications. We believe that fostering transparency and developing robust detection mechanisms are essential as video generation technology continues to mature."
        },
        {
            "title": "References",
            "content": "Chen, B., Martı Monso, D., Du, Y., Simchowitz, M., Tedrake, R., and Sitzmann, V. Diffusion forcing: Nexttoken prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, pp. 24081 24125, 2024. Chen, G., Lin, D., Yang, J., Lin, C., Zhu, J., Fan, M., Zhang, H., Chen, S., Chen, Z., Ma, C., et al. Skyreelsv2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. Cui, J., Wu, J., Li, M., Yang, T., Li, X., Wang, R., Bai, Self-forcing++: ToA., Ban, Y., and Hsieh, C.-J. wards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. Deng, H., Pan, T., Diao, H., Luo, Z., Cui, Y., Lu, H., Shan, S., Qi, Y., and Wang, X. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. Gu, Y., Mao, W., and Shou, M. Z. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Hong, Y., Mei, Y., Ge, C., Xu, Y., Zhou, Y., Bi, S., HoldGeoffroy, Y., Roberts, M., Fisher, M., Shechtman, E., et al. Relic: Interactive video world model with long-horizon memory. arXiv preprint arXiv:2512.04040, 2025. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Figure 7. Ablation on Error-Recycling Fine-Tuning (ERFT). With ERFT, context teacher is more robust to accumulate error. sistency, highlighting its critical role in enabling coherent, long-horizon video generation. Error-Recycling Fine-Tuning (ERFT). We test the context teacher by taking 5s videos from the video dataset as input for autoregressive rollout. As shown in Figure 7, the visualization of 30s generation results indicates that with robust context training, the context teacher produces videos with fewer artifacts. This results in better distribution for further contextual distillation. Bounded Positional Encoding. We investigate the impact of Bounded Positional Encoding by excluding it during inference, with quantitative results presented in Table 3. In the absence of this encoding, we observe significant performance drop in both background stability and subject consistency. This demonstrates its essential role in stabilizing long-range attention and mitigating temporal drift during the generation process. 5. Conclusion In this work, we introduced Context Forcing, framework designed to overcome the fundamental student-teacher mismatch in long-horizon causal video generation. By ensuring the teacher model maintains awareness of long-term history, our approach eliminates the supervision gap that limits existing streaming-tuning methods. To handle the computational demands of extreme durations, we proposed Slow-Fast Memory architecture that effectively reduces visual redundancy. Extensive experiments demonstrate that Context Forcing achieves effective context lengths of 20+ seconds, 210 improvement over current state-of-the-art baselines. While our method significantly mitigates drifting errors and enhances temporal coherence, the current memory compression strategy still leaves room for optimization regarding information density. Future work can focus on learnable context compression and adaptive memory mechanisms to further improve efficiency and semantic retention for even more complex, open-ended video synthesis. 9 Context Forcing: Consistent Autoregressive Video Generation with Long Context Jin, Y., Sun, Z., Li, N., Xu, K., Jiang, H., Zhuang, N., Huang, Q., Song, Y., Mu, Y., and Lin, Z. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. Kalchbrenner, N., Van Den Oord, A., Simonyan, K., Danihelka, I., Vinyals, O., Graves, A., and Kavukcuoglu, K. Video pixel networks. In International Conference on Machine Learning, 2017. Kim, J., Kang, J., Choi, J., and Han, B. Fifo-diffusion: Generating infinite videos from text without training. Advances in Neural Information Processing Systems, pp. 8983489868, 2024. Kodaira, A., Hou, T., Hou, J., Tomizuka, M., and Zhao, Y. Streamdit: Real-time streaming text-to-video generation. arXiv preprint arXiv:2507.03745, 2025. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Li, W., Pan, W., Luan, P.-C., Gao, Y., and Alahi, A. Stable video infinity: Infinite-length video generation with error recycling. arXiv preprint arXiv:2510.09212, 2025a. Li, Z., Li, C., Mao, X., Lin, S., Li, M., Zhao, S., Xu, Z., Li, X., Feng, Y., Sun, J., et al. Sekai: video dataset towards world exploration. arXiv preprint arXiv:2506.15675, 2025b. Lin, S., Yang, C., He, H., Jiang, J., Ren, Y., Xia, X., Zhao, Y., Xiao, X., and Jiang, L. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. Liu, K., Hu, W., Xu, J., Shan, Y., and Lu, S. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025. Liu, X., Zhang, X., Ma, J., Peng, J., et al. Instaflow: One step is enough for high-quality diffusion-based text-toimage generation. In International Conference on Learning Representations, 2023. Lu, Y. and Yang, Y. Freelong++: Training-free long video generation via multi-band spectralfusion. arXiv preprint arXiv:2507.00162, 2025. Lu, Y., Liang, Y., Zhu, L., and Yang, Y. Freelong: Trainingfree long video generation with spectralblend temporal attention. Advances in Neural Information Processing Systems, pp. 131434131455, 2024. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763. PmLR, 2021. Sauer, A., Boesel, F., Dockhorn, T., Blattmann, A., Esser, P., and Rombach, R. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia, pp. 111, 2024. Shin, J., Li, Z., Zhang, R., Zhu, J.-Y., Park, J., Schechtman, E., and Huang, X. Motionstream: Real-time video generation with interactive motion controls. arXiv preprint arXiv:2511.01266, 2025. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. 2023. Sun, W., Zhang, H., Wang, H., Wu, J., Wang, Z., Wang, Z., Wang, Y., Zhang, J., Wang, T., and Guo, C. Worldplay: Towards long-term geometric consistency for real-time interactive world modeling. arXiv preprint arXiv:2512.14614, 2025. Teng, H., Jia, H., Sun, L., Li, L., Li, M., Tang, M., Han, S., Zhang, T., Zhang, W., Luo, W., et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Valevski, D., Leviathan, Y., Arar, M., and Fruchter, S. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Vondrick, C., Pirsiavash, H., and Torralba, A. Generating videos with scene dynamics. Advances in Neural Information Processing Systems, 2016. Luo, S., Tan, Y., Patil, S., Gu, D., Von Platen, P., Passos, A., Huang, L., Li, J., and Zhao, H. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 10 Context Forcing: Consistent Autoregressive Video Generation with Long Context Wang, F.-Y., Huang, Z., Bergman, A., Shen, D., Gao, P., Lingelbach, M., Sun, K., Bian, W., Song, G., Liu, Y., et al. Phased consistency models. Advances in Neural Information Processing Systems, pp. 8395184009, 2024. Yu, J., Bai, J., Qin, Y., Liu, Q., Wang, X., Wan, P., Zhang, D., and Liu, X. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141, 2025. Zhang, L. and Agrawala, M. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, (3):5, 2025. Zhang, L., Cai, S., Li, M., Zeng, C., Lu, B., Rao, A., Han, S., Wetzstein, G., and Agrawala, M. Pretraining frame preservation in autoregressive video memory compression, 2026. URL https://arxiv.org/abs/2512. 23851. Zhao, M., He, G., Chen, Y., Zhu, H., Li, C., and Zhu, J. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025. Zheng, D., Huang, Z., Liu, H., Zou, K., He, Y., Zhang, F., Gu, L., Zhang, Y., He, J., Zheng, W.-S., et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. Wang, W. and Yang, Y. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. Advances in Neural Information Processing Systems, pp. 6561865642, 2024. Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, pp. 84068441, 2023. Xiao, Z., Lan, Y., Zhou, Y., Ouyang, W., Yang, S., Zeng, Y., and Pan, X. Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025. Xue, Z., Zhang, J., Hu, T., He, H., Chen, Y., Cai, Y., Wang, Y., Wang, C., Liu, Y., Li, X., et al. Ultravideo: Highquality uhd video dataset with comprehensive captions. arXiv preprint arXiv:2506.13691, 2025. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., et al. Longlive: Realtime interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Yesiltepe, H., Meral, T. H. S., Akan, A. K., Oktay, K., and Yanardag, P. Infinity-RoPE: Action-controllable infinite video generation emerges from autoregressive self-rollout. arXiv preprint arXiv:2511.20649, 2025. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, B. Improved distribution matching distillation for fast image synthesis. Advances in Neural Information Processing Systems, pp. 4745547487, 2024a. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion In IEEE/CVF with distribution matching distillation. Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024b. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast causal video generators. arXiv e-prints, pp. arXiv 2412, 2024c. 11 Context Forcing: Consistent Autoregressive Video Generation with Long Context Figure 8. Visual artifacts in LongLive. The model exhibits sudden flashback artifact, where the video abruptly resets to the initial frame after 524 frames, disrupting temporal continuity. A. Preliminaries Causal Autoregressive Models. Causal autoregressive models generate videos at the frame or short-chunk level (Xt) while enforcing strict temporal causality. Methods such as CausVid (Yin et al., 2024c) and Self-Forcing (Huang et al., 2025) adopt block-wise causal attention, allowing bidirectional self-attention within each chunk Xt but restricting information flow across chunks. Video generation is formulated as (Xt X<t). In Self-Forcing, the student model is stochastically conditioned on its own generated outputs ˆX<t during training. These models typically employ Distribution Matching Distillation (DMD) (Yin et al., 2024b) to distill knowledge from bidirectional teacher into causal student. B. Visual artifacts in LongLive. While LongLive achieves respectable quantitative scores, we observe that it frequently suffers from abrupt scene resets and repetitive, cyclic motion patterns, as illustrated in Figure 8. C. Algorithm of Context Forcing. Algorithm block of context forcing. 12 Context Forcing: Consistent Autoregressive Video Generation with Long Context Algorithm 1 Contextual DMD Require: Denoise timesteps {t1, .., tT } Require: Pre-trained teacher sreal Require: Checkpoints from stage 1, student score function sf ake, AR diffusion model Gϕ Require: Text prompt dataset D, rollout decay step sd, rollout range (L0, L1), context window c, teacher length l, local attention size (L1 L0) + L0 + 1) Sample prompt Sample rollout length = Uniform(L0, sd Sample random exit = Uniform(1, 2, ..., ) for = 1, ..., do Initialize xi if < then 1: Initialize, step = 0 2: Initialize model output [] 3: Initialize KV cache [] 4: while training do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end if for = 1, ..., do if = then (0, I) = = else Enable gradient computation ˆxi 0 Gϕ(xi , tj, C) tj X.append(ˆxi 0) Disable gradient computation GC 0, 0, C) ϕ (ˆxi Disable gradient computation ˆxi 0 = Gϕ(xi , tj, C) tj Sample ϵ (0, I) Set xi addnoise(ˆxi tj1 0, ϵ, tj1) else end if 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end for 31: 32: end while end for context video vc = X[L : l], target noise vt = addnoise(X[L :], t) Compute Contextual DMD Loss with sf ake(vt, t, vc) and sreal(vt, t, vc)"
        }
    ],
    "affiliations": [
        "Alibaba",
        "UC Merced",
        "University of Waterloo"
    ]
}