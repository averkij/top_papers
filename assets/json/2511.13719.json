{
    "paper_title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
    "authors": [
        "Zhongang Cai",
        "Ruisi Wang",
        "Chenyang Gu",
        "Fanyi Pu",
        "Junxiang Xu",
        "Yubo Wang",
        "Wanqi Yin",
        "Zhitao Yang",
        "Chen Wei",
        "Qingping Sun",
        "Tongxi Zhou",
        "Jiaqi Li",
        "Hui En Pang",
        "Oscar Qian",
        "Yukun Wei",
        "Zhiqian Lin",
        "Xuanke Shi",
        "Kewang Deng",
        "Xiaoyang Han",
        "Zukai Chen",
        "Xiangyu Fan",
        "Hanming Deng",
        "Lewei Lu",
        "Liang Pan",
        "Bo Li",
        "Ziwei Liu",
        "Quan Wang",
        "Dahua Lin",
        "Lei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 9 1 7 3 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
            "content": "Zhongang Cai,1, Ruisi Wang,1, Chenyang Gu,1, Fanyi Pu,1,2, Junxiang Xu,1, Yubo Wang,1, Wanqi Yin,1, Zhitao Yang,1, Chen Wei,1, Qingping Sun,1, Tongxi Zhou,1, Jiaqi Li,1, Hui En Pang,2, Oscar Qian,1,2, Yukun Wei1, Zhiqian Lin1, Xuanke Shi1, Kewang Deng1, Xiaoyang Han1, Zukai Chen1, Xiangyu Fan1, Hanming Deng1, Lewei Lu1, Liang Pan1, Bo Li2, Ziwei Liu(cid:0),2, Quan Wang(cid:0),1, Dahua Lin(cid:0),1, Lei Yang,(cid:0),1 Core Contributors, (cid:0) Corresponding Authors, 1SenseTime Research, 2Nanyang Technological University"
        },
        {
            "title": "Abstract",
            "content": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence. intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction. Date: December 1, 2025 Version: 2.0 Codebase: https://github.com/OpenSenseNova/SenseNova-SI Models: https://huggingface.co/collections/sensenova/sensenova-si"
        },
        {
            "title": "Introduction",
            "content": "In recent years, multimodal foundation models [13, 15, 65] have achieved groundbreaking progress across wide spectrum of tasks. However, it has become evident that even the most advanced models still struggle with spatial intelligence: the ability to understand, reason about, and act within three-dimensional space, which is fundamental to embodied AGI that can perceive, adapt to, and interact with the physical world. Interestingly, such tasks are often considered trivial for humans [6]. One of the key limitations lies in the scarcity and imbalance of spatially grounded data. While recent efforts have introduced surge of large-scale datasets targeting various facets of spatial reasoning, these resources remain fragmented and heterogeneous in scope and quality. Consequently, the community is still in the early stages of understanding how multimodal foundation models acquire and develop spatial intelligence, and what 1 Figure 1 Guided by taxonomy of spatial intelligence [6], we scaled spatial data to construct SenseNova-SI-8M, which we leverage to investigate the impact of data scaling on cultivating spatial capabilities in various MLLMs. The four subfigures at the corners elaborate SenseNova-SIs performance on four core spatial capabilities (i.e., Perspective-taking, Spatial Relations, Metric Measurement, and Comprehensive Reasoning). Through data scaling, SenseNova-SI surpassing open-source models and even outperforms GPT-5 in specific spatial abilities, such as Perspective-taking. The lines denote the average performance across benchmark subtasks within each capability, while the shaded regions (confidence bands) represent 0.5 standard deviation. At center, we show SenseNova-SI achieves state-of-the-art (SoTA) results on five recent spatial intelligence benchmarks (VSI, MMSI, MindCube, ViewSpatial, and SITE) while maintaining strong performance on general multimodal benchmark (MMBench-En). strategies are effective in fostering this capability. In this work, we aim to provide timely insights into cultivating spatial intelligence within state-of-the-art multimodal foundation models by leveraging their powerful generalist backbones and scaling up diverse data collections. Our study investigates the data scaling laws of spatial intelligence through extensive experiments on the widely adopted InternVL3 multimodal foundation model family [65], and further extends the analysis to Qwen3-VL [13] as well as Bagel [15], unified understanding and generation model. We envision the resulting models, denoted with the SenseNova-SI prefix, as open research platforms to advance studies in spatial intelligence. To preserve compatibility with existing research pipelines, we deliberately avoid altering the original architectures of the base models. Instead, we adopt data-centric approach, emphasizing the role of data scaling and training strategies as the primary drivers of spatial understanding capability. Our systematic collection and synthesis of spatial data are guided by principled taxonomy of fundamental spatial intelligence capabilities [6], leading to eight million samples (named SenseNova-SI-8M) spanning five key domains: Metric Measurement (MM), Spatial Relations (SR), Mental Reconstruction (MR), Perspective-taking (PT), and Comprehensive Reasoning (CR). We analyze diverse collection of public datasets for spatial intelligence, followed by strategic further scaling that place special focus on perspective-taking, an underrepresented capability that is critical to spatial intelligence, while isolated from general multimodal capabilities [31]. We evaluate the SenseNova-SI foundation models across broad suite of benchmarks, including VSI-Bench [56], MMSI [60], MindCube [62], ViewSpatial [29], and SITE [50], following continued training on our comprehensive spatial intelligence data collection. The models achieve state-of-the-art performance among open-source models of comparable sizes, with the best performance achieving 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while retaining their original strengths on general multimodal understanding benchmarks such as MMBench-En (84.9%). Our analysis reveals several key findings: (1) Scaling law of spatial intelligence. We systematically investigate how spatial intelligence scales under mixed data regimes. Our analysis reveals distinct scaling behaviors across spatial capabilities and model sizes, and suggests that the observed saturation trends may signal that future advances require paradigm shifts built upon and beyond SenseNova-SI. (2) Emergent generalization through diverse data. We report surprising findings that point to early signs of emergent spatial intelligence: models trained on one set of spatial tasks exhibit nontrivial transfer to seemingly unrelated tasks, and demonstrate extrapolation to longer spatial contexts beyond the training distribution. (3) Robustness against overfitting and shortcuts. 2 Through controlled experiments and circular test designs, we rigorously validate that SenseNova-SI genuinely acquires spatial capabilities rather than exploiting memorization, annotation biases, or unintended shortcuts in the training data. (4) Preliminary study of spatial chain-of-thought (CoT). We construct and evaluate three representative text CoT schemes, but find that none of them reliably improve spatial reasoning beyond what is achieved through simple QA-style data scaling. These results suggest that extending text-based CoT paradigms to spatial intelligence is non-trivial and may require fundamentally different reasoning mechanisms. (5) Downstream task validation. To assess the practical utility of SenseNova-SI, we apply SenseNova-SI to robotic manipulation tasks without any finetuning, and achieve notable performance improvements on EmbodiedBench [57], demonstrating the potential of SenseNova-SI as foundation for embodied AI. In summary, we introduce the SenseNova-SI series of multimodal foundation models, which achieve new state-of-the-art performance across major spatial intelligence benchmarks. Our study further validates that data scaling governs the progression of spatial intelligence. We envision SenseNova-SI as strong, robust baseline that future research can build upon to drive deeper advances in this critical field."
        },
        {
            "title": "2.1 Multimodal Foudational Models",
            "content": "Recent studies [6, 31, 63] reveal that while models like GPT-5 demonstrate strong planar reasoning capabilities, they still lag significantly behind humans in Spatial Intelligence (SI). Furthermore, EASI [6] shows that the performance gap between open-source and closed-source models on SI tasks is relatively small. These findings motivate us to enhance the spatial intelligence of widely used open-source models (e.g., QwenVL series [2, 3, 13, 47] and InternVL series [11, 49, 65]). This not only enables fairer comparisons among models of similar scale but also facilitates the communitys direct use of our models for downstream tasks, (e.g., VLA [26, 57, 66]), with minimal substitution costs."
        },
        {
            "title": "2.2 Multimodal Models for Spatial Intelligence",
            "content": "Efforts to enhance spatial intelligence in multimodal models primarily follow two approaches: leveraging 3D experts or curating spatial-specific datasets. As spatial intelligence is inherently linked to 3D vision, an intuition is to employ 3D expert encoders that infer key 3D attributes from images [10, 44, 52]. Spatial-MLLM [52] incorporates VGGT [45] as an input-level encoder to capture 3D information, while VLM-3R [17] integrates 3D information using combined geometry and camera-view tokens. Recently, 3DThinker [10] aligns model-generated 3D features with VGGT-derived supervision at the output level. Conversely, some studies [8, 12, 53, 55] inject visual-spatial knowledge through dataset curation and training paradigm. SpatialVLM [8] pioneered this direction by synthesizing 2B VQA samples focused on two-object spatial relationships. SpaceR [38] uses RL for spatial reasoning, while MindCube [62] explores SFT and RL using QA and two types of cognitive maps. SpatialLadder [30] constructs dataset with 26K samples and introduces three-stage progressive training strategy. Concurrently, VST [58] adopts two-phase training approach, using 4.1M samples for SFT on spatial perception and 135K samples for RL on spatial reasoning. Cambrian-S [59] develops VSI-590K dataset and employs four-stage training framework to progressively enhance spatial video understanding. In this work, we systematically scale datasets targeting core spatial capabilities [6], addressing key gaps in existing datasets, particularly the previously overlooked perspective-taking tasks."
        },
        {
            "title": "3 Data",
            "content": "The limitations in spatial intelligence mainly stem from high-quality, diverse data scarcity. In this work, we strategically scale data to expand coverage toward holistic spatial intelligence, rather than merely increasing data volume."
        },
        {
            "title": "3.1 Task Taxonomy",
            "content": "We adopt principled approach, following the EASI [6] protocol to decompose spatial intelligence into key fundamental capabilities. We focus on five capabilities that are closely aligned with real-world scenarios. For each, we analyze the core cognitive operation and derive tasks to ensure comprehensive coverage. Fig. 2 illustrates the dataset constructed under this taxonomy. 3 Figure 2 SenseNova-SI-8M reorganizes 4M open-source data and scales 4.5M additional data, according to fundamental spatial capbilities [6]. It covers general visual understanding (Non-SI), 2D grounding, and five core spatial abilities: Metric Measurement (MM), Spatial Relationship (SR), Perspective-Taking (PT), Mental Reconstruction (MR), and Comprehensive Reasoning (CR). Notably, SenseNova-SI-8M addresses the previously overlooked PT tasks. How data from each source is mapped to the core spatial capabilities is illustrated at the top (with scale in the upper-right corner indicating the number of QA pairs), while representative data samples are organized by core capability. The \"Hugging Face\" symbol indicates community datasets. The rest are curated for further scaling. 4 Metric Measurement (MM). MM involves basic understanding of the physical scale and typical object sizes. We include distances estimation between the camera and objects and pairs of objects, and size estimation across scales from individual objects to entire scenes. Spatial Relations (SR). We define SR as the ability to impose and reason within 3D coordinate system. In egocentric, local level of view, it unfolds into frontback, leftright, and updown relations between subjects. In global, scene level, these relations extend to nearfar and relative scale (largesmall) comparisons. Mental Reconstruction (MR). MR focuses on inferring 3D object structure from limited 2D observations. We adopt diagnostic task, which identifies which side of an object is visible. This requires the integration of sparse 2D cues to infer 3D geometry and align views in canonical object-centric frame. Perspective-taking (PT). PT addresses reasoning with changing camera viewpoints. We construct PT tasks in progressively more challenging hierarchy: View Correspondence. Establish correspondences of points or objects across views, recognizing entities under changes in viewpoint, scale, and occlusion. Camera Motion Reasoning. Infer relative camera motion between views, linking appearance changes to 3D transformations. Allocentric Transformation. Simulate viewpoint shifts and express spatial relations across coordinate systems, including camera, object-target, and self-oriented views. This layered design ensures that PT goes beyond pattern matching across images, encouraging the model to build internal representations of how observations transform with viewpoint changes. Comprehensive Reasoning (CR). CR tasks involve coordinating multiple spatial capabilities with extended memory and multi-step reasoning. Such data is scarce and often limited to simple scenarios. As these tasks lie beyond our main goal of scaling spatial QA and core spatial capabilities, we reuse existing datasets as lightweight complement."
        },
        {
            "title": "3.2 Data Sources.",
            "content": "General QA. We collect set of open-source general-purpose QA datasets for 2D image understanding. Specifically, we use VSR [32], SPEC [39], GQA [23], VQA [1], and IconQA [36], resulting in about 0.6M QA pairs. Community Datasets on Spatial Intelligence. Among existing open-source resources, we identify several datasets that focus on spatial reasoning, including Open3D-VQA [64], CLEVR-series [25], REL3D [20], SAT [40], GRiD-3D [28], MultiSpa [55], MindCube [62], ViCA [18], VLM-3R [17], and VSI-590K [59]. We incorporate all of these datasets, yielding in total about 3.3M QA pairs. Further Scaling on Spatial Intelligence. Building on these open-source data, we find gaps in task coverage and data imbalance. MM and SR dominate the data, while PT and MR remain underrepresented. For point, object, scene level correspondence, only MultiSpa provides point level QAs. Camera motion is also mostly limited to MultiSpa. Allocentric viewpoint transformation, especially object-centric and hypothetical views, is largely unexplored, as real-world QA labels are scarce. Tasks such as object reconstruction remain unaddressed. To address these gaps, we leverage richly annotated, scene-diverse 3D datasets, including MessyTable [5], ScanNet [14], ScanNet++ [61], SUN RGB-D [41], CA-1M [27], Ego-Exo4D [21], and Matterport3D [7], to generate large-scale, accurate and task-balanced QA pairs. This scaling process contributes 4.5M data, increasing the overall corpus size to 8.5M QA pairs."
        },
        {
            "title": "4 Training",
            "content": "We adopt three multimodal foundation models in this study. Qwen3-VL [13] is the most capable multimodal model in the Qwen series to date. It adopts strategy to scale from language foundation, that expand strong LLM foundation to handle vision or audio modalities. InternVL-3 [65] is natively multimodal, training vision and language jointly from scratch, thus enables stronger cross-modal alignment, more efficient scaling, and improved visuallanguage reasoning. 5 Models Metric Human Random Choice Proprietary Models Seed-1.6-2025-06-15 [42] Gemini-2.5-Pro-2025-06 [43] Grok-4-2025-07-09 [54] GPT-5-2025-08-07 [37] Gemini-3-Pro-Preview [19] Open-source General Models Bagel-7B-MoT [15] Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] Qwen3-VL-2B-Instruct [13] Qwen3-VL-8B-Instruct [13] InternVL3-2B [65] InternVL3-8B [65] Open-source Spatial Intelligence Models MindCube-3B-RawQA-SFT [62] SpatialLadder-3B [30] Spatial-MLLM-4B [52] SpaceR-7B [38] ViLaSR-7B [53] VST-3B-SFT [58] VST-7B-SFT [58] Cambrian-S-3B [59] Cambrian-S-7B [59] Ours SenseNova-SI Bagel-7B-MoT SenseNova-SI Qwen3-VL-8B SenseNova-SI InternVL3-2B SenseNova-SI InternVL3-8B VSI-Bench [56] MMSI-Bench [60] MindCube* [62] ViewSpatial [29] SITE [50] MMB-EN [33] MRA, Acc 79.2 34.0 49.9 53.5 47.9 55.0 52.5 31.4 27.0 32.3 50.3 57.9 32.9 42.1 17.2 44.8 46.3 41.5 44.6 57.9 60.6 57.3 67.5 Acc 97.2 25.0 38.3 38.0 37.8 41.8 45.2 31.0 28.6 26.8 28.9 31.1 26.5 28.0 1.7 27.4 26.1 27.4 30.2 30.2 32.0 25.2 25.8 Acc 94.5 33. 48.7 57.6 63.5 56.3 70.8 34.7 37.6 36.0 34.5 29.4 37.5 41.5 51.7 43.4 33.4 37.9 35.1 35.9 39.7 32.5 39.6 Acc - 26.3 43.8 46.0 43.2 45.5 50. 41.3 31.9 36.8 36.9 42.2 32.5 38.6 24.1 39.8 34.6 35.8 35.7 52.8 50.5 39.0 40.9 CAA 67.5 0.0 54.6 57.0 47.0 61.8 62.2 37.0 33.1 37.6 35.6 45.8 30.0 41. 6.3 27.9 18.0 34.2 38.7 35.8 39.6 28.3 33.0 Acc - 25.0 87.5 90.1 86.3 85.2 - 82.8 77.4 82.6 75.1 84.6 79.7 81.7 32.3 72.5 64.5 75.4 81.1 80.9 83.3 76.0 80.4 41.6(+32.5%) 62.9(+8.6%) 63.7(+93.6%) 68.7(+63.2%) 36.2(+16.8%) 37.5(+20.6%) 34.2(+29.1%) 43.3(+54.6%) 50.8(+46.4%) 74.8(+154.4%) 41.8(+11.5%) 85.6(+106.3%) 50.3(+21.8%) 48.4(+14.7%) 52.6(+61.8%) 54.6(+41.5%) 41.6(+12.4%) 83.4(+0.72%) 50.1(+9.3%) 83.5(-1.30%) 36.7(+22.3%) 78.9(-1.00%) 47.7(+16.1%) 84.9(+3.92%) Table 1 Evaluation on key spatial intelligence and general benchmarks. For concurrent works, VST [58] and Cambrian-S-7B [59], denotes benchmark results directly quoted from their papers; all other results are verified on EASI [6], using the official protocol. MindCube denotes MindCube-Tiny. Dark purple highlights the best result and light purple indicates the second-best result within Proprietary and Open-source models, respectively. Bagel [15] represents new paradigm of unified understanding and generation. We include it in our study to examine whether such unified architectures can acquire strong spatial understanding capabilities. Training Scheme. Each foundation model is trained for one epoch on the same dataset using 128 GPUs with batch size 2048. Each training takes approximately three days. We employ AdamW [35] with learning rate of 5106 for all model-training runs. Maximum 16 frames are sampled for video data."
        },
        {
            "title": "5.1 Evaluation Benchmarks.",
            "content": "To assess SenseNova-SI under broad range of scenarios, we select five newly released benchmarks for complementary coverage of spatial intelligence. VSI-Bench [56] targets video-based visual-spatial reasoning, evaluating models ability to perceive and understand the 3D layout of real indoor scenes over an extended context. We uniformly sample 32 frames from each video during testing. MMSI-Bench [60] extends spatial reasoning to multi-image settings, requiring models to integrate spatial cues across multiple views. MMSI is notably challenging: each question is manually crafted by researchers rather than mass6 generated through templates. MindCube [62] targets mental modeling of scenes from limited observations, probing the ability to reconstruct occluded spaces and simulate viewpoints. Following the official setup, we train and evaluate on the non-overlapping MindCube-10K and MindCube-Tiny respectively. ViewSpatial-Bench [29] isolates multi-perspective localization, evaluating models perspective-taking ability to reason across egocentric (camera) and allocentric (human or object) viewpoints. SITE [50] provides broad cognitive coverage, unifying over thirty datasets that span diverse facets of spatial intelligence. We adopt SITE to assess the generalization ability of SenseNova-SI, as it consists of highly abstract test cases."
        },
        {
            "title": "5.2.1 Spatial Intelligence Benchmarks.",
            "content": "We compare SenseNova-SI against leading open-source and proprietary multimodal models. As shown in Tab. 1, we observe three key findings: (1) SenseNova-SI outperforms all general open-source models by clear margins, and even surpasses strong proprietary ones such as GPT-5 [37], revealing persistent knowledge gaps in existing foundation models. (2) SenseNova-SI also achieves superior performance over all dedicated spatial-intelligence models, suggesting that algorithmic innovation alone may be premature when the benefits of large-scale spatial data have not yet been fully realized. Notably, SenseNova-SI surpasses two recent strong baselines (VST [58] and Cambrian-S [59]) even when using comparable amounts of training data  (Fig. 1)  . We attribute these gains to the inclusion of extensive perspective-taking data, which is central to spatial intelligence. (3) While InternVL3 [65], Qwen3-VL [13], and Bagel [15] exhibit slightly different behaviors, SenseNova-SI consistently improves upon all three families. This further validates the effectiveness of our scaling strategy across diverse architecture designs and pretraining paradigms."
        },
        {
            "title": "5.2.2 Retention of General Multimodal Capabilities.",
            "content": "As shown in Tab. 1, which includes MMBench-En [33] as representative general multimodal benchmark, large-scale training aimed at cultivating spatial intelligence does not compromise the performance of SenseNova-SI on more general multimodal tasks. Empirically, we find that data diversity is crucial: incorporating wide coverage of multimodal data and varied general knowledge sources effectively mitigates catastrophic forgetting and preserves overall multimodal competence."
        },
        {
            "title": "5.3.1 Effectiveness.",
            "content": "As shown in Fig. 1, scaling spatial intelligence data leads to steady improvements across all key capability dimensions. We highlight three observations: (1) Data mixing is highly effective. By aggregating wide collection of public datasets and further enlarging the spatial intelligence corpus, SenseNova-SI surpasses existing 7B spatial-intelligence baselines with models one size smaller (2B) under comparable data budgets. (2) Model size impacts capability trends. While InternVL3 2B and 8B variants exhibit similar performance trajectories on MM, SR, and CR tasks, their behaviors diverge sharply on PT tasks. We hypothesize that the 2B model lacks sufficient capacity to robustly learn viewpoint transformations: challenging but essential component of spatial intelligence. (3) Capability-wise differences reveal data-driven gains. Proprietary models such as GPT-5 [37] are notably strong on SR, yet show clear deficiencies in PT. In contrast, SenseNova-SI-InternVL3-8B convincingly outperforms GPT-5 on PT, benefiting from the large-scale, comprehensive perspective-taking data introduced during continued scaling. Interestingly, even though we include very limited CR data during training, SenseNova-SI still gradually surpasses GPT-5 in CR performance. This suggests the presence of capability synergy, where advances in fundamental spatial tasks (e.g., PT and SR) transfer to more complex reasoning skills. We discuss this further in Sec. 5.4."
        },
        {
            "title": "5.3.2 Saturation.",
            "content": "As shown in Fig. 1, the performance gains gradually diminish as the amount of training data increases. While it remains unclear whether continued scaling will eventually reach tipping point that triggers stronger emergent capabilities 7 Figure 3 Observations on generalization ability from single data source and single task. The upper example demonstrates how training on ego-exo association task enhance performance on task required imagined first-person perspectives. The lower example demonstrates how camera rotation task, based on cross-view visual correspondence, generalizes to tasks with distinct questions and visual appearances. These findings suggest the potential existence of meta-tasks in PT, which may enable related spatial capabilities. (though we note some early signs discussed in Sec. 5.4), we concur with the broader community that data scaling alone is unlikely to achieve human-level spatial intelligence [59]. Motivated by this, we commit to fully open-sourcing the weights of SenseNova-SI, allowing the community to bypass the costly scaling stage and instead focus on advancing algorithmic innovation on top of strong, spatially capable foundation."
        },
        {
            "title": "5.4 Capability Emergence",
            "content": "We present several interesting cases observed during scaling that may suggest early signs of emerging spatial intelligence."
        },
        {
            "title": "5.4.1 Spill-Over.",
            "content": "64 32 16 128 Model # Frames Benchmark Cambrian-S-7B [59] SenseNova-SI InternVL3-8B 58.6 63.6 66.4 67.5 VSI [56] VSI-Debiased [4] 49.7 55.6 59.1 59.9 Large-scale mixed-domain training inevitably exposes models to broad distribution of scenarios, making it increasingly difficult to determine whether downstream improvements stem from genuine, generalizable spatial reasoning or from incidental overlap with training data. To more rigorously examine spatial capability spill-over, we therefore conduct controlled experiments in which models are trained on single dataset and evaluated on tasks drawn from entirely different domains. As shown in Fig. 3, we observe clear emergence and transfer of spatial understanding across tasks. The viewtransformation dataset, constructed from Ego-Exo4D [21], requires models to translate between egocentric and exocentric viewpointsforcing them to infer cross-view geometric relationships. This ability transfers strongly to downstream tasks such as Maze Pathfinding [50] and Pos-Cam-Cam [60], both of which depend on sequential viewpoint simulation and aggregating information across views. Similarly, the dataset built from MessyTable [5] images requires models to identify shared objects and infer spatial relationships between two viewpoints. This yields notable gains on benchmark sub-categories such as MMSI [60] Pos-Cam-Cam and Attr-Appr, both of which rely on robust spatial correspondence identification between paired images. Table 2 Ablation on inference frames. Our model is trained on maximum 16 frames per sample, while Cambrian-S7B [59] is trained on 64/128 frames. SenseNova-SI demonstrates strong extrapolation capabilities beyond the training number of frames. Interestingly, SenseNova-SI shows clear lead over Cambrian-S-7B [59] on two benchmarks, even with fewer frames at inference. 64.6 68.7 68.8 66.3 VSI [56] VSI-Debiased [4] 58.9 62.8 62.4 59."
        },
        {
            "title": "5.4.2 Extrapolation.",
            "content": "A surprising observation is that although SenseNova-SI is trained with at most 16 frames per sample, it generalizes effectively to sequences of 32 frames or more at inference time, as shown in Tab. 2. This suggests that SenseNova-SI learns to construct coherent spatial structure rather than merely repeating patterns confined to the supervised training window. Interestingly, while SenseNova-SI does not continue to extrapolate beyond 64 frames, unlike CambrianS [59], which is explicitly trained with much longer context windows of 64 or 128 frames, SenseNova-SI nevertheless achieves performance comparable to Cambrian-S while using substantially fewer frames at inference. This indicates that SenseNova-SI possesses stronger spatial understanding capability that enables it to form meaningful connections across larger temporal gaps, without relying on densely sampled frame sequences."
        },
        {
            "title": "5.5 Overfit and Shortcut Analysis",
            "content": "Recent studies [4, 46] have shown that multimodal models can exploit language shortcuts to answer questions without genuine visual reasoning. To ensure that the improvements of SenseNova-SI are not due to overfitting to QA text, we conduct targeted analyses on VSI [56] and MindCube [62]. Models Standard Soft cir. Hard cir. w/o. Vis. MindCube-SFT-RawQA [59] SenseNova-SI InternVL3-8B 51.7 85.6 45.8 84. 23.1 75.6 50.7 52.5 Table 3 Analysis on MindCube [62]. Soft cir. and Hard cir. stands for Soft circular and Hard circular following [6]. w/o. Vis. indicates testing without visual as input, following [46]. The recently proposed VSI-Debiased [4] is specifically designed variant of VSI to eliminate text-only shortcuts by removing questions that can be answered correctly without visual understanding. As shown in Tab. 2, when evaluated on VSI-Debiased, SenseNova-SI exhibits substantially smaller performance drop compared to Cambrian-S-7B [59], indicating that SenseNova-SI relies less on textual heuristics and more on spatially grounded understanding. For MindCube, we follow the protocol in [46] and evaluate models without visual inputs. Surprisingly, as shown in Tab. 3, the previous open-source SoTA on MindCube, MindCube-RawQA-SFT [62] achieves score of 50.7 without any images, which is almost identical to its performance with full visual inputs, revealing heavy dependence on language priors rather than visual reasoning. In contrast, SenseNova-SI drops from 85.6 to 52.5 in the no-vision setting, validating that it genuinely uses visual information rather than relying on language shortcuts. Notably, both models converge to score around 50 in the absence of images, underscoring the importance of debiasing benchmarks, as argued in [4]. To further verify that SenseNova-SI does not overfit to text option ordering, we conduct circular tests [6, 31, 33], which reorders the choices in the questions to eliminate dependency on certain text patterns. As reported in Tab. 3, SenseNovaSI exhibits minimal degradation under the Soft circular test [31]. Even in the Hard circular test [33], which requires robust handling of all rotations of answer choices, SenseNova-SI drops only 10 points, whereas MindCube-RawQA-SFT drops nearly 30 points. This demonstrates that SenseNova-SI is far less sensitive to superficial text patterns and possesses more stable, input-grounded reasoning."
        },
        {
            "title": "5.6 Spatial Chain-of-Thought",
            "content": "Chain-of-Thought (CoT) [51] has become the de facto approach for complex reasoning tasks. However, despite numerous recent attempts [24, 48, 60, 62], incorporating CoT variants typically yields only marginal gains (often 2%), which are consistently overshadowed by improvements derived from large-scale curated spatial datasets. CoT Style Average # Output Token VSI-Bench: Obj. Rel. Direction Overall Easy Medium Hard InternVL3-8B No CoT CoT-GPT-5 CoT-MindCube-Aug-CGMap CoT-SenseNova-SI-CGMap 1 3.4 1070.7 1490.6 2262.8 39.3 54.9 40.0 39.9 47.9 48.8 62.2 41.4 45.9 53.9 47.0 55.8 43.1 42.7 51.3 21.9 46.6 36.1 33.7 41.0 In Tab. 4, we present preliminary evaluation of different CoT styles. We examine three paradigms: (1) CoT-GPT-5, which directly uses large language model (GPT-5 [37]) to annotate CoT given the question and ground-truth answer; (2) CoT-MindCube-Aug-CGMap, which follows MindCube [62] and constructs JSON-style cognition map (CogMap) within the CoT; (3) CoT-SenseNova-SI-CGMap, our extended CogMap that provides step-by-step tracking of objects Table 4 Impact of Chain-of-Thought (CoT) formats on the Object Relative Direction subset of VSI-Bench. 9 across frames, maps them to world coordinate system with precise (rather than coarse-grid) coordinates, and reasons about relative spatial relationships more explicitly. We train each variant on roughly 100K examples, reasonably large compared to typical CoT studies, and evaluate on VSIs Object Relative Direction task, challenging subset known to impede strong baselines such as InternVL3. We find that (1) our elaborated CoT achieves the highest improvement among the three, but (2) all CoT variants yield limited absolute gains, insufficient to justify their computational overhead, especially given the extra tokens required during both training and inference. We acknowledge that these results are preliminary, and it is too early to draw definitive conclusions. Nevertheless, our findings suggest that while carefully engineered CoT can offer modest benefits and should not be dismissed outright, text-based reasoning alone may be neither the most efficient nor the most effective paradigm for spatial intelligence. This may signal the need for broader paradigm shift beyond conventional CoT. GPT-4o InternVL3-8B SenseNova-SI InternVL3-8B OP SIP OP SIP OP SIP 37.5 45.8 10.4 20.8 16.6 (+59.6%) 33.3 (+60.0%) Table 5 Success rate on Spatial subset of EmbodiedBench [57]. OP: Official Prompt; SIP: Spatial-Intelligenceoriented Prompt."
        },
        {
            "title": "5.7 Downstream Task",
            "content": "To evaluate the practical utility of SenseNova-SI enhanced spatial intelligence, we conduct downstream robot manipulation experiments on EmbodiedBench [57], focusing specifically on its spatial subset. In this setting, SenseNova-SI is instantiated as an embodied agent that controls virtual Franka Panda robot to execute user instructions containing rich spatial language such as \"left\", \"on top of\", \"rear\", and \"horizontal\". Importantly, no finetuning of SenseNova-SI is performed for this evaluation. Quantitative results for the spatial subset are shown in Tab. 5. We report success rates under two prompting settings: the official prompt (OP) and spatial-intelligence-oriented prompt (SIP). OP supplies bounding-box coordinates extracted from the input image, whereas SIP enriches OP with additional object-grounding cues to reduce ambiguity from object recognition and better isolate spatial-reasoning performance. Across both OP and SIP, SenseNova-SI delivers substantial improvements, demonstrating that enhanced spatial intelligence directly benefits embodied manipulation. We observe that SenseNova-SI more reliably identifies key spatial cues, enabling more accurate reasoning and more consistent action planning. Representative rollouts are visualized in Fig. 4. These examples illustrate that SenseNova-SI effectively integrates spatial information from both language instructions and visual observations, plans coherent motion trajectories, and generates action sequences that allow the robot to successfully complete the tasks. Figure 4 Visualization of the manipulation task rollout in EmbodiedBench [57], performed by the embodied agent powered by SenseNova-SI."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we validate the effectiveness of scaling spatial intelligence across multiple multimodal foundation models, and achieve significant performance gains across the board. We further validate that the enhanced foundation models retain their general capabilities, and start to develop generalization capabilities that were not possible without training on large-scale, diverse data. We hope our study lays solid foundation for future research on developing spatial intelligence in multimodal foundation models."
        },
        {
            "title": "References",
            "content": "[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, December 2015. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, and Saining Xie. Benchmark designers should\" train on the test set\" to expose exploitable non-visual shortcuts. arXiv preprint arXiv:2511.04655, 2025. [5] Zhongang Cai, Junzhe Zhang, Daxuan Ren, Cunjun Yu, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, and Chen Change Loy. Messytable: Instance association in multiple camera views. In Proceedings of the European Conference on Computer Vision, pages 116. Springer, 2020. [6] Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, et al. Has gpt-5 achieved spatial intelligence? an empirical study. arXiv preprint arXiv:2508.13142, 2025. [7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. [8] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. [10] Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, and Ruqi Huang. Think with 3d: Geometric imagination grounded spatial reasoning from limited views. arXiv preprint arXiv:2510.18632, 2025. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,, pages 2418524198, 2024. [12] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062 135093, 2024. [13] QwenLM Team (Alibaba Cloud). Qwen3-vl: Multimodal large language model series. https://github.com/QwenLM/ Qwen3-VL, 2025. GitHub repository; accessed: 2025-11-14. [14] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie√üner. Scannet: Richlyannotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58285839, 2017. [15] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [16] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, and Rakesh Ranjan. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction, 2025. [17] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. 11 [18] Qi Feng. Towards visuospatial cognition via hierarchical fusion of visual experts. arXiv preprint arXiv:2505.12363, 2025. [19] Gemini. Gemini 3 Pro Model Card. Technical report, Gemini, November 2025. Accessed: 2025-11-18. [20] Ankit Goyal, Kaiyu Yang, Dawei Yang, and Jia Deng. Rel3d: minimally contrastive benchmark for grounding spatial relations in 3d. Advances in Neural Information Processing Systems, 33:1051410525, 2020. [21] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. [22] Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John A. Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Proceedings of the Language Resources and Evaluation, 55(3):661688, December 2020. ISSN 1574-0218. doi: 10.1007/s10579-020-09517-1. URL http://dx.doi.org/10.1007/s10579-020-09517-1. [23] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67006709, 2019. [24] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. [25] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,, pages 29012910, 2017. [26] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Proceedings of the Conference on Robot Learning, volume 270 of Proceedings of Machine Learning Research, pages 26792713. PMLR, 0609 Nov 2025. [27] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify anything: Scaling indoor 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22225 22233, 2025. [28] Jae Hee Lee, Matthias Kerzel, Kyra Ahrens, Cornelius Weber, and Stefan Wermter. What is right for me is not yet right for you: dataset for grounding relative directions via multi-task learning. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, pages 10391045. International Joint Conferences on Artificial Intelligence Organization, 7 2022. [29] Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, et al. Viewspatial-bench: Evaluating multi-perspective spatial localization in vision-language models. arXiv preprint arXiv:2505.21500, 2025. [30] Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, and Yueting Zhuang. Spatialladder: Progressive training for spatial reasoning in vision-language models. arXiv preprint arXiv:2510.08531, 2025. [31] Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Robert Hawkins, Nuno Vasconcelos, Tal Golan, Dezhi Luo, et al. Core knowledge deficits in multi-modal language models. arXiv preprint arXiv:2410.10855, 2024. [32] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. [33] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In Proceedings of the European Conference on Computer Vision, pages 216233. Springer, 2024. [34] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), December 2024. ISSN 1869-1919. doi: 10.1007/s11432-024-4235-6. URL http://dx.doi.org/10. 1007/s11432-024-4235-6. [35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [36] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [37] OpenAI. GPT-5 System Card. Technical report, OpenAI, August 2025. Accessed: 2025-08-10. [38] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. [39] Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. Synthesize diagnose and optimize: Towards fine-grained vision-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1327913288, 2024. [40] Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, et al. Sat: Dynamic spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [41] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 567576, 2015. [42] ByteDance Seed Team. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [43] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [44] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. [45] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. VGGT: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,, pages 52945306, 2025. [46] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. Advances in Neural Information Processing Systems, 37: 7539275421, 2024. [47] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [48] Siting Wang, Luoyang Sun, Cheng Deng, Kun Shao, Minnan Pei, Zheng Tian, Haifeng Zhang, and Jun Wang. Spatialviz-bench: Automatically generated spatial visualization reasoning tasks for mllms. arXiv preprint arXiv:2507.07610, 2025. [49] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [50] Wenqi Wang, Reuben Tan, Pengyue Zhu, Jianwei Yang, Zhengyuan Yang, Lijuan Wang, Andrey Kolobov, Jianfeng Gao, and Boqing Gong. Site: towards spatial intelligence thorough evaluation. arXiv preprint arXiv:2505.05456, 2025. [51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. [52] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. [53] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. [54] xAI. Grok 4, 7 2025. URL https://x.ai/news/grok-4. Model announcement. 13 [55] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models. arXiv preprint arXiv:2505.17015, 2025. [56] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,, pages 1063210643, 2025. [57] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. [58] Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, and Hengshuang Zhao. Visual spatial tuning. arXiv preprint arXiv:2511.05491, 2025. [59] Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, et al. Cambrian-s: Towards spatial supersensing in video. arXiv preprint arXiv:2511.04670, 2025. [60] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. [61] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie√üner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. [62] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop, 2025. [63] Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, et al. How far are vlms from visual spatial intelligence? benchmark-driven perspective. arXiv preprint arXiv:2509.18905, 2025. [64] Weichen Zhang, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, and Xiao-Ping Zhang. Open3dvqa: benchmark for comprehensive spatial reasoning with multimodal large language model in open space. arXiv preprint arXiv:2503.11094, 2025. [65] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [66] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Proceedings of the Conference on Robot Learning, pages 21652183. PMLR, 2023."
        },
        {
            "title": "Appendix",
            "content": "A Details of Fig.1 A.1 Four Subfigures at the Corners The four subfigures at the corners elaborate SenseNova-SIs performance on four core spatial capabilities (i.e., Perspective-taking, Spatial Relations, Metric Measurement, and Comprehensive Reasoning). Through data scaling, SenseNova-SI surpassing open-source models and even outperforms GPT-5 in specific spatial abilities, such as Perspective-taking. The lines denote the average performance across benchmark subtasks within each capability, while the shaded regions (confidence bands) represent 0.5 standard deviation. The detailed benchmark sub-tasks associated with each spatial capability are listed below. Perspective-taking. VSI-Bench: Obj. Rel. Direction; MMSI-Bench: Positional Relationship subtasks (Cam-Cam, Obj-Obj, Reg-Reg, Cam-Obj, Obj-Reg, Cam-Reg), Motion subtasks (Motion-Cam, Motion-Obj); SITE: multi-view & cross-image reasoning. Spatial Relations. VSI-Bench: Obj. Rel. Distance; SITE: 3d information understanding, spatial relationship reasoning. Metric Measurement. VSI-Bench: Obj. Size, Room Size, Obj. Abs. Distance; MMSI-Bench: Attribute Meas.. Comprehensive Reasoning. VSI-Bench: Obj. Cnt., Obj. Appear Order, Route Plan; MMSI-Bench: MSR. A.2 Normalization for Radar Chart Visualization For the radar charts in Fig. 1, we apply normalization to enable fair and intuitive comparison of relative performance. Specifically, for each metric, we first scale all values by dividing them by the maximum value observed across models. We normalize all metrics so that the best score among the models is mapped to 1.0 and the worst score is mapped to 0.2. The radar chart axes have range of 0.0 to 1.0."
        },
        {
            "title": "B Additional Details in Data Curation",
            "content": "Our unified data pipeline collects data from diverse sources and efficiently converts them into reliable, high-quality QA and Chain-of-Thought (CoT) labels. B.1 Data Processing B.1.1 Unified Annotation. We standardize heterogeneous raw data from source datasets into unified set of spatial and multi-view annotations. Specifically, we convert existing formats and augment the data with additional labels to obtain: 3D camera poses, 3D object poses including bounding boxes and orientations, 2D point and object visibility, and rich semantic labels of object and humanobject interaction descriptions. B.1.2 Dataset-specific Processing. ScanNet [14], ScanNet++ [61]. These datasets provide 3D camera poses, 3D object bounding boxes, and 3D point clouds with object IDs. For each camera view, we project the 3D point cloud onto the image plane to establish correspondences between 2D pixels and 3D points, and to derive per-object projected and visible 2D bounding boxes. SUN RGB-D [41], CA-1M [27]. These datasets provide 3D camera poses, 3D object poses, and 2D object bounding boxes. Building on this, we refine and standardize the 3D object orientations, discard object categories whose orientations are inconsistent across scenes, and, using the accurate orientation labels, further annotate possible humanobject interactions with hypothetical 3D poses and rich textual descriptions. MessyTable [5]. This dataset provides 3D camera poses, 2D object bounding boxes, and cross-view instance association labels (the same object instance is assigned the same instance ID in different viewpoints). We further employ visionlanguage model (VLM) to annotate fine-grained textual descriptions of object appearance details. 15 Figure 5 Hard cases in MessyTable [5], where multiple instances of the same object class are present in the same scene. B.2 Object Selection We adopt unified object selection pipeline to retain only recognizable objects with sufficient informative details captured within each frame. The resulting per-frame object sets provide clean and reliable basis for cross-frame association analysis and QA construction. B.2.1 Semantic Filtering. We first filter out object categories with weak geometric structure and ambiguous 3D position, such as floor, ceiling, and wall, as well as objects with unclear or undefined semantic labels. B.2.2 Visibility-based Filtering. Minimum Visible Size. We keep only the objects whose visible 2D bounding box (i.e., the portion not occluded by other objects and lying within the camera view) occupies at least certain fraction of the image area. Visibility Ratio Threshold. We further discard objects whose visible 2D bounding box area falls below fixed ratio of their total projected 2D bounding box area. B."
        },
        {
            "title": "Image Selection",
            "content": "To derive multi-view image sets that are well-posed, visually associated, and sufficiently challenging, we adopt three-stage image selection pipeline. B.3.1 Basic Pose Filtering. We first discard views with extreme camera poses. In particular, we remove images whose camera pitch (severely top-down or bottom-up) or yaw deviates excessively from the typical viewing direction. This step eliminates degenerate or highly uninformative viewpoints. B.3.2 Cross-view Association Control. Connectivity. We select images into sets in manner such that, for any pair of images in the same set, there exists at least one connecting path, along which every pair of adjacent images satisfies minimum association score. The calculation of the association score depends on the available annotation. For datasets with 2D point visibility, we compute the score based on the number of overlapping visible points. Otherwise, we compute the number of shared visually valid objects. Difficulty. To avoid trivial cross-view associations, we enforce that the association score between any pair of images does not exceed specified maximum. We further exploit dataset-specific properties to design richer forms of difficulty control. For example, as shown in Fig. 5, we emphasize hard cases in MessyTable where multiple visually similar or identical objects exist in the same scene. In such cases, establishing cross-view associations requires fully understanding of the 3D spatial layout, while appearance-based shortcuts are impossible. 16 B.3.3 Full-scene Coverage Selection. For scan-based datasets with point-level annotations, we further extend our selection to image sets with broader coverage of the scenes. Leveraging the temporal continuity of the scan videos, we design time-efficient greedy algorithm that iteratively adds views to maximize point coverage, while maintaining the cross-view connectivity and difficulty constraints above. The resulting procedure is summarized in Algorithm 1. Algorithm 1 Frame Selection with Overlap Control Require: Video frames {f1, . . . , fT }, visible points Vfk , minimum frames Nmin = 16 Ensure: Selected frame set œÅk Vfk C/Vfk {Compute overlap with coverage} if 0.03 œÅk 0.20 then 1: {Initialize covered point set} 2: {f1} {Start with first frame} 3: Vf1 4: for = 2 to do 5: 6: 7: 8: end if 9: 10: end for 11: if < Nmin then 12: 13: end if 14: return S {fk} Vfk {Update covered points} Insert additional frames uniformly in temporal gaps until = Nmin B.4 QA Selection We apply quality control and quantity balancing to ensure reliable QA generation. B.4.1 Ambiguity Reduction. To avoid ambiguous references when questions involve object names, we require that only single instance of queried semantic object category is present within the image set. We discard cases in which the angular range of referenced direction cannot be clearly mapped to unique spatial sector (e.g., front/left/right). Such ambiguous geometric configurations are removed to avoid confusion in answer interpretation. B.4.2 Balanced Sampling. We encourage both textual and visual diversity while maintaining balanced sampling. For questions with the same underlying intent, we randomly vary the textual descriptions (e.g., paraphrased phrasings and directional expressions), while capping the total number of samples to avoid redundancy. Within each image set, we select diverse combinations of objects to construct QAs, while limiting the number of QAs per set to maintain balanced distribution across different scenes. B.5 Chain-of-Thought (CoT) Strategies We explore three Chain-of-Thought (CoT) strategies for multi-frame reasoning. VLM-generated CoT. We provide QA pairs and step-wise instructions to GPT-5 for CoT annotations. MindCube Aug-CGMap CoT. MindCube [62] uses discrete 2D cognitive map (CogMap) to describe top-down view of the scenes, projecting objects and cameras onto 2D grid. The designed CoT contains two steps: 17 CGMap Inference. Directly generate JSON-formatted CogMap with discretized grid (e.g., 10 10), encoding approximate positions and four-direction orientations of objects and camera views. Free-Form Reasoning. Perform free-form reasoning on camera changes between consecutive frames, relate observations across views, and finally derive the answer from the aggregated observations. Our Procedural CGMap CoT. We also adopt top-down CogMap representation, but use continuous (non-gridded) coordinates and construct the CogMap in procedural, step-by-step manner interleaved with textual reasoning. This CoT design exhibits improved geometric accuracy and more coherent reasoning, particularly in scenes with complex object layouts and diverse viewpoints. Experimental results can be found in Sec. 5.6. The detailed procedure is as follows: Keyframe Localization. Identify the keyframe set in which the queried objects appear. These frames are emphasized during subsequent reasoning. Incremental Relative Camera Estimation. Traverse frames in temporal order. For each adjacent pair of frames, describe the shared objects and estimate coarse camera pose changes. CogMap Construction. Construct the CogMap along the keyframe set A, following an efficient path inferred from the previous step. We build global CogMap and fix its origin and positive y-axis with the first keyframe as reference. For each new frame, we perform metric 3D grounding of newly observed objects, estimate the camera rotation and translation relative to the reference frame, and then transform the placement of the new camera view and objects into the global CogMap. Answer Derivation. We may flexibly rotate the CogMap coordinate system according to any desired allocentric transformation, and reason about geometric relations (e.g., distances, angles, relative ordering) to produce the final answer."
        },
        {
            "title": "C Full Results of Single Dataset Training",
            "content": "For each dataset, we train model on its training set and evaluate its performance on five key spatial intelligence benchmarks: VSI, MMSI, MindCube, ViewSpatial, and SITE. As shown in Tab. 6, training on single dataset often yields strong performance on few benchmarks while sacrificing performance on others. For example, model trained solely on VSI-590K achieves the best MRA accuracy on VSI (64.0), but its results on tasks like MMSI, MindCube, and SITE drop noticeably. VSI-Bench MMSI-Bench MindCube ViewSpatial SITE Avg. Dataset MRA, Acc Acc Random Choice InternVL3-8B [65] VLM-3R-DATA [16] VSR [32] Rel3D [20] VSI590K [59] SPEC [39] SAT [40] GQA [23] MultiSPA [55] CLEVR [25] VQA [1] 34.0 42.1 53.9 41.1 39.3 64.0 38.7 30.5 26.6 21.8 29.7 30.0 - - 25.0 28.0 28.5 2 28.3 3 26.9 4 29.0 1 27.6 5 26.8 6 27.4 9 10 27.7 25.6 8 26.9 7 - - 2 3 7 1 5 9 6 4 10 7 Acc 33.0 41.5 34.8 37.9 39.8 26.7 33.6 29.1 24.7 22.8 30.0 15.3 - - 3 2 1 7 4 6 8 9 5 10 Acc 26.3 38.6 54.2 55.9 57.7 48.1 54.7 42.8 42.2 34.4 33.1 39.4 - - 4 2 1 5 3 6 7 9 10 8 CAA 0.0 41.1 36.7 40.9 39.5 34.7 34.8 21.3 23.4 32.2 18.6 20.2 - - 23.7 38.3 41.6 3 40.8 1 40.6 2 40.5 5 37.9 4 30.1 8 28.9 7 6 27.8 10 27.4 26.4 Table 6 Evaluation on key spatial intelligence benchmarks using InternVL3-8B trained on each single dataset. : ranking on each benchmark. Dark purple highlights the best result and light purple indicates the second-best result within models trained on different single datasets, respectively. This pattern highlights that no single dataset provides comprehensive spatial intelligence coverage, and therefore mixed-data training is crucial for building more balanced models. As different datasets tend to bias the model toward 18 particular subset of spatial reasoning skills, determining how to effectively balance datasets during training remains an open challenge. This table also provides the community with useful reference for dataset selection, helping researchers understand which datasets contribute to which aspects of spatial reasoning and how to design more complementary training mixtures."
        },
        {
            "title": "D Impact of Scaling on Benchmarks",
            "content": "In Fig. 1, we illustrate the effect of data scaling on model performance, grouped by core spatial capabilities. In contrast, Tab. 7 presents the scaling effects at the benchmark level. Across both views, we observe consistent trend: model performance improves steadily as more data is introduced during training, validating that high-quality, diverse spatial data is effective in addressing key knowledge deficiencies in spatial intelligence. # Data VSI-Bench MMSI-Bench MindCube ViewSpatial SITE Metric MRA, Acc 0M 1M 2M 3M 4M 5M 6M 7M 8M 42.1 56.3 60.3 64.4 62.7 65.9 66.3 67.9 68.7 Acc 28.0 36.0 39.6 41.9 41.9 40.8 41.8 42.3 43.3 Acc 41.5 58.8 76.2 81.1 83.9 81.7 85.0 85.7 85.6 Acc 52.0 55.2 56.7 56.2 55.6 53.7 55.2 54.7 54.6 CAA 42.1 44.5 47.1 46.7 45.2 46.0 47.0 46.5 47.7 Table 7 Impact of scaling on key spatial intelligence benchmarks. 0M indicates base model (InternVL3-8B) whereas 8M indicates SenseNova-SI InternVL3-8B."
        },
        {
            "title": "E Retention of General Capabilities",
            "content": "To evaluate whether SenseNova-SI retains its general understanding capabilities after continued training on spatial intelligence data (i.e., SenseNova-SI-8M), we evaluate its performance on four additional multimodal benchmarks: MMBench-En [33] and MMStar [9] for holistic multimodal understanding, AI2D [22] for scientific diagram reasoning, and OCRBench [34] for character recognition (OCR). As shown in Tab. 8, SenseNova-SI Qwen3-VL-8B and SenseNova-SI InternVL3-2B exhibit minimal performance drops on MMBench-En relative to their respective base models, while SenseNova-SI Bagel-7B-MoT and SenseNova-SI InternVL3-8B even show slight improvements. Across the remaining three benchmarks, only marginal declines are observed. Notably, compared with other open-source spatial-intelligence models, SenseNova-SI maintains competitive general visual understanding. Furthermore, prior studies (such as VST [58] and Cambrian-S [59]) suggest that incorporating additional general visual understanding data can further preserve or enhance this capability, direction we plan to explore in future work."
        },
        {
            "title": "F Downstream Task",
            "content": "F.1 SenseNova-SI as an Embodied Agent for Manipulation. Following EmbodiedBench [57], we implement SenseNova-SI as an embodied agent in the downstream manipulation task to demonstrate its application. In this setting, the SenseNova-SI model controls simulated Franka Panda robot with parallel gripper. Conditioned on language instruction and the visual state of the scene, the agent receives symbolic description of the environment, where each object is represented by discrete 3D position in table-top coordinate frame. The model is required to output sequence of low-level gripper actions in structured action space, where each action specifies the target end-effector position, orientation, and binary gripper state, all expressed in the same discretized coordinate system. This formulation enables direct execution of the predicted actions in the simulator"
        },
        {
            "title": "Models",
            "content": "MMB-EN [33] MMStar [9] AI2D [22] OCRB [34] Open-source General Models Bagel-7B-MoT [15] Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] Qwen3-VL-8B [13] InternVL3-2B [65] InternVL3-8B [65] Open-source SI Models MindCube-3B-SFT-RawQA [62] SpatialLadder-3B [30] SpaceR-7B [38] VST-7B-SFT [58] Cambrian-S-7B [59] Ours SenseNova-SI Bagel-7B-MoT SenseNova-SI Qwen3-VL-8B SenseNova-SI InternVL3-2B SenseNova-SI InternVL3-8B 82.8 77.4 82.6 84.6 79.7 81.7 32.3 72.5 75.4 83.3 80.4 83.4 83.5 78.9 84.9 67.3 55.9 63.9 68.5 56.9 68.2 35.9 48.7 58.6 63.1 43.9 67.8 65.7 57.0 65.4 89.5 81.6 83.9 81.3 77.8 85.2 65.8 76.1 84.3 84.9 76.9 88.8 84.2 76.8 79.0 811 797 864 906 853 880 392 468 873 855 648 797 863 817 815 Table 8 Evaluation on general undestanding benchmarks. denotes benchmark results directly cited from their papers. Dark purple highlights the best result and light purple indicates the second-best result within Open-source and SenseNova-SI models, respectively. MMB-EN: MMBench-En. OCRB: OCRBench. without additional post-processing. This demonstrates the models ability to ground language and perception into coherent, executable manipulation trajectories that require spatial reasoning. F.2 Task Prompt for the Embodied Agent. The official task prompt (OP) includes the role description, the definitions of the input space and output action space, the color space, example task conversations, and the instructions for the output JSON format. This design enables the SenseNova-SI model to perform reasoning and action planning while generating executable actions in the required format. For the spatial-intelligence-oriented prompt (SIP), instead of providing object bounding box coordinates with generic indices such as object 1\" and object 2\" as in OP, we provide the specific name of the objects, such as first cylinder or second container.\" This removes interference from object recognition and allows the model to focus on spatial reasoning. The official task prompt (OP) is shown below. F.3 Case Study. Fig. 6 illustrates how the SenseNova-SI InternVL3-8B model behaves as an embodied agent in the manipulation task under the official task prompt. For each task instance, we show the task instruction, the scene observation provided to the model, the model output, and the resulting execution rollout in the simulator. Cases Fig. 6(a) and Fig. 6(b) demonstrate successful executions, where the model produces correct object recognition and accurate action sequences that lead to successful task completion. In contrast, cases Fig. 6(c) and Fig. 6(d) illustrate typical failure modes, including incorrect object recognition in the visual state description that leads to erroneous reasoning and execution planning, as well as limited manipulation precision that causes task failure even when the executable action plan is correct. Incorrect information in the output is highlighted in red. Please refer to the video demonstration in the Supplementary Materialfor the full rollout, more task instances and performance comparison with the base InternVL3-8B model. 20 Input: \"object 1\": The input space, output action space - Objects are ordered by in ascending order. Human Instruction: Pick up the star and place it into the yellow container. \"visual_state_description\": \"From left to right, can see purple cylinder at [45, 13, 18], You are Franka Panda robot with parallel gripper. You can perform various tasks and output sequence of gripper actions to accomplish given task with images of your status. and color space are defined as follows. Input space - Each input object is represented as 3D discrete position in the following format: [X, Y, Z]. - There is red XYZ coordinate frame located in the top-left corner of the table. The X-Y plane is the table surface. - The allowed range of X, Y, is [0, 100]. Output action space - Each output action is represented as 7D discrete gripper action in the following format: [X, Y, Z, Roll, Pitch, Yaw, Gripper state]. - X, Y, are the 3D discrete position of the gripper in the environment. They follow the same coordinate system as the input object coordinates. - The allowed range of X, Y, is [0, 100]. - Roll, Pitch, Yaw are the 3D discrete orientation of the gripper in the environment, represented as discrete Euler angles. - The allowed range of Roll, Pitch, Yaw is [0, 120] and each unit represents 3 degrees. - Gripper state is 0 for close and 1 for open Color space - Each object can be described using one of the colors below: [\"red\", \"maroon\", \"lime\", \"green\", \"blue\", \"navy\", \"yellow\", \"cyan\", \"magenta\", \"silver\", \"gray\", \"olive\", \"purple\", \"teal\", \"azure\", \"violet\", \"rose\", \"black\", \"white\"]. Below are some examples to guide you in completing the task. Example 0: [45, 13, 18], \"object 2\": [72, 20, 18], \"object 3\": [50, 42, 17], \"object 4\": [36, 42, 18], \"object 5\": [70, 43, 17], \"object 6\": [50, 77, 17] Output: blue moon at [72, 20, 18], green star at [50, 42, 17], purple cube at [36, 42, 18], yellow container at [70, 43, 17] and purple container at [50, 77, 17].\", \"reasoning_and_reflection\": \"The task is to pick up the star located at [50, 42, 17] and place it into the yellow container located at [70, 43, 17]. The plan involves moving the gripper to the stars position, closing the gripper to pick it up, moving to the yellow containers position, and opening the gripper to release the object into the container.\", \"executable_plan\": \"[[50, 42, 17, 6, 61, 36, 1], [50, 42, 17, 6, 61, 36, 0], [47, 41, 26, 6, 61, 36, 0], [69, 39, 24, 88, 32, 77, 0], [69, 39, 24, 87, 32, 77, 1]]\" Example 1: [78, 20, 17], \"object 2\": [28, 30, 17], \"object 3\": [39, 47, 17], \"object 4\": [50, 72, 17], \"object 5\": [39, 75, 18] Output: 20, 17], black star at [28, 30, 17], green container at [39, 47, 17], green triangular prism at [50, 72, 17] and grey cylinder at [39, 75, 18].\", \"reasoning_and_reflection\": located at [28, 30, 17] and place it into the rose container located at [78, 20, 17]. The plan involves moving the gripper to the stars position, closing the gripper to pick it up, moving to the rose containers position, and opening the gripper to release the object into the container.\", \"executable_plan\": \"[[28, 32, 26, 0, 60, 94, 1], [27, 32, 18, 0, 60, 94, 0], [27, 32, 28, 0, 60, 94, 0], [76, 19, 28, 6, 66, 109, 1]]\" Now you are supposed to follow the above examples to generate sequence of discrete gripper actions that completes the human instruction below. Human Instruction: The output JSON format should be \"visual_state_description\": str, \"reasoning_and_reflection\": str, \"language_plan\": The fields in the above JSON follow the purposes below: 1. visual_state_description: Describe the color and shape of each object in the detection box in the numerical order in the image. Then provide the 3D coordinates of the objects chosen from the input. to be taken on the target objects, and reflect on the previous actions taken if available. list of natural language actions to achieve the user instruction. step number and the language action name. 4. executable_plan: the user instruction, with each discrete action being 7-dimensional discrete action. 5. Keep your plan efficient and concise. When generating content for JSON strings, avoid using any contractions or abbreviated forms (like s, re, ve, ll, d, nt) that use apostrophes. Instead, write out full forms (is, are, have, will, would, not) to prevent parsing errors in JSON. Please do not output anything other than the above-mentioned JSON. Reason about the overall plan that needs 3. language_plan: \"visual_state_description\": \"From left to right, can see rose container at [78, Human Instruction: Pick up the star and place it into the rose container. list of discrete actions needed to achieve Each language action is started by the \"The task is to pick up the star 2. reasoning_and_reflection: str, \"executable_plan\": str. <Dict of object positions> <Task instruction> Input: Input: \"object 1\": 21 Figure 6 Demonstration of manipulation with SenseNova-SI InternVL3-8B as an embodied agent. We present the task instruction, scene observation input, model output, and task execution rollout. Cases (a) and (b) are successful executions, while cases (c) and (d) are failure cases. Incorrect information in the output is highlighted in red. Please see the video in the Supplementary Material for the full rollout. 22 Models Human Random Choice(Frequency) Proprietary Models Seed-1.6-2025-06-15 [42] Gemini-2.5-pro-2025-06 [43] Grok-4-2025-07-09 [54] GPT-5-2025-08-07 [37] Open-source General Models Bagel-7B-MoT [15] Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] Qwen3-VL-2B-Instruct [13] Qwen3-VL-8B-Instruct [13] InternVL3-2B [65] InternVL3-8B [65] Avg. 79.2 34.0 49.9 53.5 47.9 55.0 31.4 27.0 32.3 50.3 57.9 32.9 42.1 Open-source Spatial Intelligence Models MindCube-3B-RawQA-SFT [62] 17.2 44.8 SpatialLadder-3B [30] 46.3 Spatial-MLLM-4B [52] 41.5 SpaceR-7B [38] 44.6 ViLaSR-7B [53] VST-3B-SFT [58] 57.9 VST-7B-SFT [58] 60.6 Cambrian-S-3B [59] 57.3 Cambrian-S-7B [59] 67.5 Ours SenseNova-SI Bagel-7B-MoT SenseNova-SI Qwen3-VL-8B SenseNova-SI InternVL3-2B SenseNova-SI InternVL3-8B 41.6 62.9 63.7 68. Numerical Answer Multiple-Choice Answer Obj. Count CR Abs. Dist MM Obj. Size MM Room Size MM Rel. Dis SR,MM Rel. Dir PT Route Plan CR Appr. Order CR 94.3 62.1 43.5 46.0 37.1 53. 30.0 19.1 32.8 62.1 67.5 64.8 66.0 12.8 62.1 66.6 44.5 58.1 69.3 72.0 70.7 73.2 59.4 71.4 74.4 76.7 47.0 32.0 34.3 37.3 32.9 34.4 29.1 21.2 18.1 40.2 47.0 30.8 34. 22.7 35.3 38.0 24.7 33.8 45.4 44.4 40.6 50.5 52.3 46.4 70.1 72.0 60.4 29.9 66.1 68.7 60.8 73.3 35.5 24.2 43.8 71.4 76.3 32.4 43.6 4.3 61.9 63.6 53.5 61.4 71.8 74.3 68.0 74. 25.6 74.5 67.0 72.7 45.9 33.1 52.8 54.3 45.4 47.5 25.7 27.2 31.7 49.7 61.9 22.9 47.5 23.4 41.4 35.4 37.3 28.8 62.4 68.3 46.3 72.2 32.3 66.1 47.1 53. 94.7 25.1 55.0 61.9 53.1 63.7 34.9 33.8 38.0 52.2 58.0 32.2 48.0 20.2 45.6 40.4 41.9 45.0 59.0 59.7 64.8 71.1 44.9 61.8 75.5 76.3 95.8 47. 35.7 43.9 39.6 48.6 41.3 42.0 37.4 42.0 50.9 34.9 39.3 15.7 46.4 48.2 46.1 46.5 46.0 55.8 61.9 76.2 44.7 68.0 73.1 80.7 95.8 28.4 44.3 47.4 47.4 50. 30.4 27.3 28.3 30.4 35.0 32.9 26.2 15.9 27.3 32.9 29.3 29.9 38.7 44.9 27.3 41.8 39.8 43.2 60.9 69.5 100.0 25.2 67.9 68.7 66.8 68.9 24.1 20.8 27.9 54.5 66.3 12.6 31. 22.4 38.5 44.3 54.8 53.2 70.2 65.2 78.8 80.1 33.5 72.0 41.2 48.4 Table 9 Evaluation on VSI-Bench [56]. Numerical Answer uses MRA score, MCA uses Acc score, Avg. is the simple average across these metrics, following the original paper. denotes benchmark results directly cited from their papers."
        },
        {
            "title": "G Detail Results on Key Benchmarks",
            "content": "In this section, we provide detailed per-benchmark results that complement the aggregated scores reported in the main text. The following tables report per-benchmark results for VSI-Bench (Tab. 9), MMSI-Bench (Tab. 10), MindCubeBenchTiny (Tab. 11), ViewSpatial (Tab. 12), and SITE (13), respectively. For each benchmark, we break down performance over all relevant subsets and question types, enabling more fine-grained analysis of model strengths and failure modes than is possible from the single aggregated metrics in the main tables. On most subsets, our model attains the best or near-best accuracy among open-source models, and on several challenging subsets (e.g., Rel.Dir in VSI), its performance is comparable to, or even surpasses, that of proprietary models. 23 Models Avg. Positional Relationship Attribute Motion MSR CC PT OO PT RR PT CO PT OR PT CR PT Meas. MM Appr. MR Cam. PT Obj. PT CR Human Random Choice 97.2 95.7 98.9 97.5 94.2 98.8 96.4 95.3 98.5 98.6 98.7 97.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 Proprietary Models Seed-1.6-2025-06-15 [42] Gemini-2.5-pro-2025-06 [43] Grok-4-2025-07-09 [54] GPT-5-2025-08-07 [37] 38.3 36.5 36.1 32.1 32.5 42.3 46.9 48.4 33.0 31.0 42.1 40.4 38.0 38.7 34.0 40.7 44.1 38.8 40.9 62.5 30.3 39.1 25.0 33.3 37.8 36.5 35.1 39.5 34.8 45.8 50.6 21.8 22.7 40.5 43.4 38.3 41.8 41.9 32.9 35.8 49.8 42.3 68.6 54.6 37.3 28.3 40.7 36.3 Open-source General Models Bagel-7B-MoT [15] Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] Qwen3-VL-2B-Instruct [13] Qwen3-VL-8B-Instruct [13] InternVL3-2B [65] InternVL3-8B [65] Open-source Spatial Intelligence Models MindCube-3B-RawQA-SFT [62] 1.7 0.0 SpatialLadder-3B [30] Spatial-MLLM-4B [52] SpaceR-7B [38] ViLaSR-7B [53] VST-3B-SFT [58] VST-7B-SFT [58] Cambrian-S-3B [59] Cambrian-S-7B [59] 31.0 34.4 35.1 29.6 32.5 42.3 31.3 34.3 21.2 18.9 27.6 30.3 28.6 36.5 30.8 28.3 26.7 28.2 31.3 31.2 16.6 16.2 35.5 28.7 26.8 27.9 26.5 19.7 32.5 38.8 28.9 23.4 21.2 20.2 30.2 24.7 28.9 26.8 29.7 30.8 38.3 35.2 33.7 23.4 28.7 29.7 28.9 21.2 31.1 27.9 37.2 32.0 31.3 35.2 38.5 37.5 15.1 27.0 28.9 29.7 26.5 31.1 22.3 28.3 30.2 28.2 28.9 25.0 22.7 16.2 28.9 26.7 28.0 22.5 22.3 34.5 31.3 42.3 33.7 25.0 19.6 20.2 34.2 24.7 0. 3.1 0.0 0.0 2.1 2.3 3.6 2.4 2. 2.0 27.4 36.5 29.7 29.6 32.5 30.5 24.0 18.7 31.8 22.9 23.6 23.2 26.1 24.7 21.2 28.3 30.2 29.4 28.9 18.7 34.8 10.8 23.6 29.7 27.4 25.8 31.9 29.6 25.5 31.7 22.8 26.5 28.7 16.2 34.2 27.2 30.2 29.0 35.1 28.3 39.5 40.0 44.5 31.2 16.6 17.5 31.5 23.2 28.9 34.4 31.9 32.0 27.9 25.8 33.7 34.3 31.8 27.0 26.3 22.2 32.3 38.7 37.2 34.5 38.3 30.5 33.7 26.5 45.4 37.8 32.8 18.6 25.2 23.6 30.8 25.9 39.5 22.3 28.9 28.1 19.6 17.5 27.6 19.1 25.8 20.4 34.0 24.6 33.7 18.8 25.3 28.1 31.8 14.8 28.9 24.7 Ours SenseNova-SI Bagel-7B-MoT SenseNova-SI Qwen3-VL-8B SenseNova-SI InternVL3-2B SenseNova-SI InternVL3-8B 36.1 41.9 30.8 32.0 45.3 41.1 54.2 29.6 40.9 31.0 31.5 28.2 37.5 36.5 43.6 35.8 60.4 41.1 55.4 43.7 33.3 25.6 31.5 22.7 34.2 39.7 45.7 33.3 46.5 30.5 39.7 31.2 30.3 29.7 32.8 24.7 43.3 50.5 47.8 41.9 62.7 44.7 69.8 40.6 40.9 32.4 32.8 27.7 Table 10 Evaluation on MMSI-Bench [60]. Scores are Acc as in the original paper. Under Positional Relationship, C: Camera; O: Object; R: Region. Since VST [58] does not provide subset scores for MMSI-Bench, the VST scores here use the results reproduced using EASI [6]."
        },
        {
            "title": "Human\nRandom Choice",
            "content": "Proprietary Models Seed-1.6-2025-06-15 [42] Gemini-2.5-pro-2025-06 [43] Grok-4-2025-07-09 [54] GPT-5-2025-08-07 [37] Open-source General Models Bagel-7B-MoT [15] Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] Qwen3-VL-2B-Instruct [13] Qwen3-VL-8B-Instruct [13] InternVL3-2B [65] InternVL3-8B [65] Open-source Spatial Intelligence Models MindCube-3B-RawQA-SFT [62] SpatialLadder-3B [30] Spatial-MLLM-4B [52] SpaceR-7B [38] ViLaSR-7B [53] VST-3B-SFT [58] VST-7B-SFT [58] Cambrian-S-3B [59] Cambrian-S-7B [59] Ours SenseNova-SI Bagel-7B-MoT SenseNova-SI Qwen3-VL-8B SenseNova-SI InternVL3-2B SenseNova-SI InternVL3-8B Avg."
        },
        {
            "title": "Around\nPT",
            "content": "94.5 33.0 - 33.3 48.7 57.6 63.5 56.3 34.7 37.6 36.0 34.5 29.4 37.5 41.5 51.7 43.4 33.4 37.9 35.0 35.9 39.7 32.5 39.6 50.8 74.8 41.2 85. 89.0 88.0 93.0 94.5 34.5 33.5 37.0 32.5 29.5 28.9 36.5 34.0 35.0 39.0 35.0 35.5 32.0 37.0 27.0 35.5 37.5 69.5 33.5 82.0 - 31.8 36.4 44.9 54.4 38. 31.3 35.9 32.3 31.6 28.6 36.9 38.1 51.0 43.2 30.5 34.2 31.0 34.9 35.9 33.2 42.2 57.1 78.4 44.4 84.9 - 35.7 45.6 63.2 61.6 68.4 42.8 44.8 44.0 42.8 31.2 45.6 53. 67.6 50.8 36.0 49.2 44.4 41.6 50.8 35.2 36.8 46.8 70.4 40.0 90.4 Table 11 Evaluation on MindCube-Tiny [62]. All scores are Acc. 25 Models Overall Camera-based Tasks Person-based Tasks Rel. Dir. PT Obj. Ori. PT Obj. Ori. PT Rel. Dir. PT Sec. Sim. PT Random Choice 26.3 25.2 26.1 24. 31.1 26.3 Proprietary Models Seed-1.6-2025-06-15 [42] Gemini-2.5-pro-2025-06 [43] Grok-4-2025-07-09 [54] GPT-5-2025-08-07 [37] Open-source General Models Bagel-7B-MoT [15] Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] Qwen3-VL-2B-Instruct [13] Qwen3-VL-8B-Instruct [13] InternVL3-2B [65] InternVL3-8B [65] 43.8 46.0 43.2 45.5 41.3 31.9 36.8 36.9 42.2 32.5 38. Open-source Spatial Intelligence Models MindCube-3B-RawQA-SFT [62] 24.1 39.8 SpatialLadder-3B [30] 34.6 Spatial-MLLM-4B [52] 35.8 SpaceR-7B [38] 35.7 ViLaSR-7B [53] 52.8 VST-3B-SFT [58] 50.5 VST-7B-SFT [58] 39.0 Cambrian-S-3B [59] 40.8 Cambrian-S-7B [59] Ours SenseNova-SI Bagel-7B-MoT SenseNova-SI Qwen3-VL-8B SenseNova-SI InternVL3-2B SenseNova-SI InternVL3-8B 50.2 48.4 52.6 54.6 55.7 59.1 57.1 60.1 48.2 40.8 46.7 36.9 54.2 42.0 50.3 30.8 46.2 34.9 43.2 46.8 46.8 52.6 41.5 46. 58.2 56.3 65.7 66.2 26.9 33.0 23.8 27.9 38.6 28.7 31.2 23.7 29.7 17.5 27.5 22.0 25.7 23.3 28.9 25.3 35.4 29.6 19.8 20.5 38.1 30.6 19.4 43.1 54.8 51.0 47.5 40. 46.9 30.1 9.9 35.3 47.2 38.8 42.5 23.6 56.2 40.3 37.4 39.0 70.2 51.9 52.7 46.3 58.0 55.3 70.1 38.3 48.4 45.8 51.6 48.4 42.5 29.2 32.4 32.6 40.2 34.5 37.5 2.4 31.9 40.3 34.0 32.6 52.6 50.7 41.2 48. 48.6 37.7 39.4 43.1 26.6 32.5 24.8 40.0 26.5 24.4 26.6 33.3 31.1 23.7 27.3 16.9 33.5 34.5 30.2 26.6 62.8 64.5 38.1 39.2 42.7 53.8 55.9 69.9 Table 12 Evaluation on ViewSpatial-Bench [29]. All scores are Acc. 26 Models Human Random Choice Proprietary Models Seed-1.6-2025-06-15 [42] Gemini-2.5-pro-2025-06 [43] Grok-4-2025-07-09 [54] GPT-5-2025-08-07 [37] Open-source General Models Bagel-7B-MoT [15] Qwen2.5-VL-3B-Instruct [3] Qwen2.5-VL-7B-Instruct [3] Qwen3-VL-2B-Instruct [13] Qwen3-VL-8B-Instruct [13] InternVL3-2B [65] InternVL3-8B [65] Overall Count - Loc - 3D Inf MM,SR MultiV PT Rel SR Mov CR 67.5 0.0 66.0 83.3 54.7 0.0 0.0 0.0 87.5 73.0 52.5 0.0 0.0 0.0 54.6 57.0 47.0 61.8 37.0 33.1 37.6 35.6 45.8 30.0 41.1 61.9 66.4 60.4 61.3 69.1 55.1 50.4 60.3 51.5 63.0 68.6 55. 37.1 70.5 32.4 38.5 71.5 48.6 26.2 61.2 37.4 51.4 72.5 59.2 50.0 63.6 26.4 48.4 42.7 20.5 54.3 44.7 23.3 43.6 44.9 29.9 55.8 60.3 32.2 46.5 39.4 18.3 57.9 51.6 33.9 63.1 11.1 5.2 9.4 56.2 12.7 12.8 63.6 15.9 15.2 56.9 19.1 23.2 67.7 30.1 50.8 10.1 6.6 10.8 61.8 26.3 Open-source Spatial Intelligence Models MindCube-3B-RawQA-SFT [62] SpatialLadder-3B [30] Spatial-MLLM-4B [52] SpaceR-7B [38] ViLaSR-7B [53] VST-3B-SFT [58] VST-7B-SFT [58] Cambrian-S-3B [59] Cambrian-S-7B [59] 6.3 27.9 18.0 34.2 38.7 35.8 39.6 28.3 33.3 20.8 9.2 -1.2 44.9 30.9 21.7 30.3 23.0 11.8 49.6 38.0 20.8 54.9 42.3 23.6 47.4 34.6 31.5 54.6 36.1 36.6 42.4 42.0 17.9 47.9 45.4 28. 15.7 -8.5 -5.7 47.1 11.5 3.8 8.2 19.8 11.2 10.0 60.1 16.3 13.3 64.3 23.2 16.9 57.7 18.9 14.8 63.7 23.5 40.0 23.6 2.5 45.2 27.7 5.6 Ours SenseNova-SI Bagel-7B-MoT SenseNova-SI Qwen3-VL-8B SenseNova-SI InternVL3-2B SenseNova-SI InternVL3-8B 41.6 50.1 36.7 47.7 48.8 61.6 37.8 51.4 63.7 44.1 51.0 44.9 27.1 56.2 48.3 43.7 20.7 59.7 20.7 44.1 63.5 30.3 16.9 48.8 27.0 35.7 57.5 40.7 Table 13 Evaluation on SITE [50]. All scores are CAA. We follow the SITE papers original evaluation protocol, where MCQ are answered by direct QA. 27 Case Study on Text-based CoT As discussed in Sec. 5.6, we observe that textual CoT does not appear to yield genuinely stronger 3D reasoning capabilities. With supporting case studies in Fig. 7 and Fig. 8, we tentatively consider several hypotheses rather than definitive explanations, and we leave more thorough investigation to future work. Note that green indicates correct reasoning steps, while red indicates wrong reasoning steps. Traintest mismatch and error accumulation. Textual CoT enforces long, linear per-token reasoning process. During training, tokens are learned with ground-truth context, whereas at inference time the model conditions on its own generated tokens. Early local errors in the reasoning (as in Fig. 8) may therefore accumulate along the sequence and become increasingly difficult to correct; tasks that require globally coherent 3D structure may be particularly sensitive to this discrepancy. Bias toward human-preferred explanation paths. Compared to direct QA supervision, textual CoT shifts the optimization focus from task accuracy towards reproducing specific intermediate verbal rationalizations, which may not align with the geometry-dominant nature of multi-view 3D reasoning. Supervision along single, human-preferred explanatory path may effectively shrink the models reasoning space. Figure 7 Successful case of trained CoT-SenseNova-SI-CGMap output on VSI-Bench [56] Rel. Dir subset. The model maintains consistent and accurate reasoning through images and concludes to correct final answer. Question: If am standing by the refrigerator and facing the backpack, is the microwave to my front-left, front-right, back-left, or back-right? The directions refer to the quadrants of Cartesian plane (if am standing at the origin and facing along the positive y-axis). Options: A. back-left, B. back-right, C. front-left, D. front-right GT: Prediction(CoT-SenseNova-SI-CGMap ): <think> will reason through this question step by step. For one object in one image coordination system, its location is described as \"category_id at bbox [x1, y1, x2, y2], at direction, distance from camera and x, y\". x-aixs is the right side of the camera view perspective. y-axis is the forwad perspective. The unit of distance is meters, and the unit of angle is degrees. For describing camera motion, the movement from image to image is defined relative to the coordinate system of image a. **Clarify the queried objects in the related image** Step 1: From the question, can infer that the queried objects are the refrigerator, the backpack and the microwave. After looking over the provided images, can get that the refrigerator_6 shows in image 21, specifically at bbox [0.09, 0.0, 0.67, 0.98], at 5 degrees(CCW), dist 1.5, -0.1, 1.4; the backpack_19 shows in image 27, specifically at bbox [0.54, 0.17, 0.76, 0.45], at 10 degrees(CW), dist 2.6, 0.4, 2.3; the microwave_28 shows in image 7, specifically at bbox [0.03, 0.72, 0.37, 1.0], at 15 degrees(CCW), dist 1.6, -0.4, 1.5. Consequently, to solve this question, image [7, 21, 27] need to be paid more attentions to. In this step, have also assigned an id to each queried object. will keep this custom to distinguish objects that belong to the same category but are not the same instance. **Establish connections among the relevant images** Step 2: To analyze the relationships among the images identified in Step 1, will select some images to connect them. Two images are connected only if they share corresponding objects. After looking over all these images, derive the following chain that illustrates how the queried images are connected: From image 1 to image 5 via image 2; image 3; image 4, the corresponding object is: stove_3. Based on the object appearance change, can infer that in this phase, the camera translates right and front, rotates to left and looks down. From image 5 to image 7 via image 6, the corresponding objects are: microwave_28, box_26. appearance change, can infer that in this phase, the camera does not move significantly, rotates to left slightly and looks up. Based on the object 29 From image 7 to image 13 via image 8; image 11, the corresponding object is: appearance change, can infer that in this phase, the camera translates right and back, rotates to left and looks down. From image 13 to image 21 via image 15; image 17, the corresponding object is: object appearance change, can infer that in this phase, the camera translates right and front, rotates to left and looks down. From image 21 to image 27 via image 25, the corresponding object is: refrigerator_6. Based on the object appearance change, can infer that in this phase, the camera translates left, rotates to left and looks down. Based on these analyse, can infer that image 7 and image 21 can be related by image order [7, 8, 11, 13, 15, 17, 21]. Then image 27 can be further connected by image order [21, 25, 27]. (dish rack)_5. Based on the sink_0. Based on the object **Construct Scene Layout** The will analyse the layout in detail. Step 3: The overall scene layout is constructed in the coordinate system of image 7 as follows. How the main images are connected has been described in Step 1. ** From image7 to image13 ** Image 7 contains objects: microwave_28 at bbox [0.03, 0.72, 0.37, 1.0], at 15 degrees(CCW), dist 1.6, -0.4, 1.5; sink_0 at bbox [0.43, 0.52, 0.56, 0.59], at 0 degrees(CW), dist 1.9, 0.1, 1.8; (dish rack)_5 at bbox [0.03, 0.26, 0.33, 0.47], at 15 degrees(CCW), dist 1.7, -0.5, 1.6. Image 13 contains objects: sink_0 at bbox [0.41, 0.48, 0.64, 0.6], at 0 degrees(CW), dist 2.0, 0.1, 1.9. The correspond objects between image 7 and image 13 is sink_0. According to the objects motion changes, from image7 to image13, the camera motion is: translation (x 0.2, -0.1), rotation (left 25 degrees, down 10 degrees). ** From image13 to image21 ** Image 13 contains objects: (dish rack)_5 at bbox [0.03, 0.26, 0.33, 0.47], at 15 degrees(CCW), dist 1.7, -0.5, 1.6. Image 21 contains objects: (dish rack)_5 at bbox [0.85, 0.01, 1.0, 0.37], at 35 degrees(CW), dist 1.5, 0.8, 1.2. The correspond objects between image 13 and image 21 is (dish rack)_5. According to the objects motion changes, from image13 to image21, the camera motion is: translation (x 0.4, 0.4), rotation (left 70 degrees, down 10 degrees). As mentioned above, from image7 to image13, the camera motion is: translation (x 0.2, -0.1), rotation (left 25 degrees, down 10 degrees). Consequently, by accumulating the transformations, from image7 to image21, the camera motion is: translation (x 0.4, 0.4), rotation (left 95 degrees, down 20 degrees). ** From image21 to image27 ** Image 21 contains objects: refrigerator_6 at bbox [0.34, 0.0, 0.79, 0.87], at 5 degrees(CW), dist 1.6, 0.1, 1.5. Image 27 contains objects: backpack_19 at bbox [0.54, 0.17, 0.76, 0.45], at 10 degrees(CW), dist 2.6, 0.4, 2.3; refrigerator_6 at bbox [0.38, 0.0, 0.79, 0.88], at 5 degrees(CW), dist 1.6, 0.1, 1.4. The correspond objects between image 21 and image 27 is refrigerator_6. According to the objects motion changes, from image21 to image27, the camera motion is: translation (x -0.2, 0.0), rotation (left 50 degrees, down 15 degrees). As mentioned above, from image7 to image21, the camera motion is: translation (x 0.4, 0.4), rotation (left 95 degrees, down 20 degrees). Consequently, by accumulating the transformations, from image7 to image27, the camera motion is: translation (x 0.4, 0.2), rotation (left 145 degrees, down 35 degrees). In this phase, image 27 has introduced some new objects in its own coordinate system: backpack_19 at bbox [0.54, 0.17, 0.76, 0.45]; refrigerator_6 at bbox [0.38, 0.0, 0.79, 0.88]. Transfer these objects from image 27 into image 7 coordination system: 2.5, -1.2, -1.5; refrigerator_6 at 70 degrees(CCW), dist 1.4, -1.0, 0.4; backpack_19 at 140 degrees(CCW), dist **Infer object spatial relation and answer the question** Step 4: After localizing the objects from questions in the scene layout from step2, we can know that \"In image 7 coor dination system: refrigerator_6 at 70 degrees(CCW), dist 1.4, -1.0, 0.4; backpack_19 at 140 degrees(CCW), dist 2.5, -1.2, -1.5; microwave_28 at 15 degrees(CCW), dist 1.6, -0.4, 1.5\". Considering the **view perspective** mentioned in the question, the right answer this question is the right answer is A. back-left </think> <answer>A</answer> Figure 8 Failure case of trained CoT-SenseNova-SI-CGMap output on VSI-Bench [56] Rel. Dir subset. Multiple local inconsistencies accumulate through the reasoning trajectory and ultimately lead to an incorrect answer. Question: If am standing by the stove and facing the table, is the refrigerator to my front-left, front-right, back-left, or back-right? The directions refer to the quadrants of Cartesian plane (if am standing at the origin and facing along the positive y-axis). Options: A. front-right, B. front-left, C. back-right, D. back-left GT: Prediction(CoT-SenseNova-SI-CGMap ): <think> will reason through this question step by step. For one object in one image coordination system, its location is described as \"category_id at bbox [x1, y1, x2, y2], at direction, distance from camera and x, y\". x-aixs is the right side of the camera view perspective. y-axis is the forwad perspective. The unit of distance is meters, and the unit of angle is degrees. For describing camera motion, the movement from image to image is defined relative to the coordinate system of image a. **Clarify the queried objects in the related image** Step 1: From the question, can infer that the queried objects are the stove, the table and the refrigerator. After looking over the provided images, can get that the stove_18 shows in image 19, specifically at bbox [0.21, 0.01, 1.0, 1.0], at 5 degrees(CW), dist 1.6, 0.1, 1.4; the table_25 shows in image 21, specifically at bbox [0.02, 0.82, 0.2, 1.0], at 25 degrees(CCW), dist 2.6, -0.9, 2.1; the refrigerator_10 shows in image 25, specifically at bbox [0.41, 0.26, 0.6, 0.98], at 5 degrees(CW), dist 2.4, 0.1, 2.3. Consequently, to solve this question, image [19, 21, 25] need to be paid more attentions to. In this step, have also assigned an id to each queried object. will keep this custom to distinguish objects that belong to the same category but are not the same instance. **Establish connections among the relevant images** Step 2: To analyze the relationships among the images identified in Step 1, will select some images to connect them. Two images are connected only if they share corresponding objects. After looking over all these images, derive the following chain that illustrates how the queried images are connected: From image 1 to image 2 directly, the corresponding object is: doorframe_24. Based on the object appearance change, can infer that in this phase, the camera translates right, rotates to right slightly and looks up. From image 1 to image 4 directly, the corresponding object is: door_21. Based on the object appearance change, can infer that in this phase, the camera translates left and front, rotates to left slightly and looks up. 31 From image 4 to image 16 directly, the objects are mostly not fully visible. But Based on the object appearance change, can infer that in this phase, the camera translates left, rotates to left and looks down. From image 16 to image 19 directly, the corresponding objects are: appearance change, can infer that in this phase, the camera translates right and front, rotates to back-left and looks up. From image 2 to image 21 via image 19, the corresponding object is: (kitchen cabinets)_4. appearance change, can infer that in this phase, the camera translates right and front, rotates to left and looks up. From image 21 to image 25 via image 24, the corresponding object is: doors_0. Based on the object appearance change, can infer that in this phase, the camera translates left and rears, rotates to right and looks up. Based on these analyse, can infer that image 19 and image 21 can be related by image order [19, 16, 4, 1, 2, 19]. Then image 25 can be further connected by image order [19, 21, 24, 25]. towel_16, stove_18. Based on the object Based on the object **Construct Scene Layout** The will analyse the layout in detail. Step 3: The overall scene layout is constructed in the coordinate system of image 19 as follows. How the main images are connected has been described in Step 1. ** From image19 to image4 ** Image 19 contains objects: stove_18 at bbox [0.21, 0.01, 1.0, 1.0], at 5 degrees(CW), dist 1.6, 0.1, 1.4; towel_16 at bbox [0.24, 0.35, 0.41, 0.78], at 10 degrees(CCW), dist 1.4, -0.2, 1.3; towel_15 at bbox [0.31, 0.32, 0.49, 0.77], at 5 degrees(CCW), dist 1.4, -0.1, 1.3. Image 4 contains objects: towel_16 at bbox [0.9, 0.65, 1.0, 0.99], at 40 degrees(CW), dist 1.4, 0.7, 0.9; towel_15 at bbox [0.89, 0.34, 1.0, 0.83], at 35 degrees(CW), dist 1.4, 0.7, 1.0. The correspond objects between image 19 and image 4 are towel_16, towel_15. According to the objects motion changes, from image19 to image4, the camera motion is: translation (x -0.1, 0.1), rotation (right 50 degrees, down 30 degrees). ** From image4 to image16 ** According to the objects motion changes, from image4 to image16, the camera motion is: translation (x -0.2, 0.0), rotation (left 30 degrees, down 15 degrees). As mentioned above, from image19 to image4, the camera motion is: translation (x -0.1, 0.1), rotation (right 50 degrees, down 30 degrees). Consequently, by accumulating the transformations, from image19 to image16, the camera motion is: translation (x -0.2, 0.3), rotation (right 20 degrees, down 45 degrees). ** From image16 to image19 ** Image 16 contains objects: stove_18 at bbox [0.02, 0.0, 0.56, 0.51], at 20 degrees(CCW), dist 1.8, -0.5, 1.6; towel_16 at bbox [0.92, 0.0, 1.0, 0.11], at 30 degrees(CW), dist 1.6, 0.7, 1.3. Image 19 contains objects: stove_18 at bbox [0.21, 0.01, 1.0, 1.0], at 5 degrees(CW), dist 1.6, 0.1, 1.4; towel_16 at bbox [0.24, 0.35, 0.41, 0.78], at 10 degrees(CCW), dist 1.4, -0.2, 1.3. The correspond objects between image 16 and image 19 are stove_18, towel_16. According to the objects motion changes, from image16 to image19, the camera motion is: translation (x 0.7, 1.2), rotation (left 100 degrees, up 30 degrees). As mentioned above, from image19 to image16, the camera motion is: translation (x -0.2, 0.3), rotation (right 20 degrees, down 45 degrees). Consequently, by accumulating the transformations, from image19 to image16, the camera motion is: translation (x 0.8, 1.2), rotation (left 80 degrees, down 15 degrees). ** From image2 to image21 ** Image 2 contains objects: (kitchen cabinets)_4 at bbox [0.6, 0.7, 0.7, 0.96], at 10 degrees(CW), dist 2.9, 0.5, 2.6. Image 21 contains objects: (kitchen cabinets)_4 at bbox [0.19, 0.71, 0.36, 1.0], at 10 degrees(CCW), dist 2.2, -0.4, 1.9; table_25 at bbox [0.02, 0.82, 0.2, 1.0], at 25 degrees(CCW), dist 2.6, -0.9, 2.1. The correspond objects between image 2 and image 21 is (kitchen cabinets)_4. According to the objects motion changes, from image2 to image21, the camera motion is: translation (x 2.0, 1.2), rotation (left 45 degrees, up 25 degrees). As mentioned above, from image19 to image2, the camera motion is: translation (x 0.1, 0.1), rotation (right 50 degrees, down 30 degrees). Consequently, by accumulating the transformations, from image19 to image21, the camera motion is: translation (x 2.4, -0.5), rotation (right 5 degrees, down 0 degrees). In this phase, image 21 has introduced some new objects in its own coordinate system: (kitchen cabinets)_4 at bbox [0.19, 0.71, 0.36, 1.0]; table_25 at bbox [0.02, 0.82, 0.2, 1.0]. Transfer these objects from image 21 into image 19 coordination system: (kitchen cabinets)_4 at 60 degrees(CW), dist 3.1, 2.5, 1.4; table_25 at 50 degrees(CW), dist 3.2, 2.3, 1.9; 32 ** From image21 to image25 ** Image 21 contains objects: doors_0 at bbox [0.01, 0.23, 0.22, 0.85], at 25 degrees(CCW), dist 3.1, -1.3, 2.8. Image 25 contains objects: doors_0 at bbox [0.0, 0.36, 0.32, 0.97], at 20 degrees(CCW), dist 3.7, -1.2, 3.5. The correspond objects between image 21 and image 25 is doors_0. According to the objects motion changes, from image21 to image25, the camera motion is: translation (x -1.0, -0.2), rotation (right 30 degrees, up 10 degrees). As mentioned above, from image19 to image21, the camera motion is: translation (x 2.4, -0.5), rotation (right 5 degrees, down 0 degrees). Consequently, by accumulating the transformations, from image19 to image25, the camera motion is: translation (x 1.4, -0.7), rotation (right 35 degrees, up 10 degrees). In this phase, image 25 has introduced some new objects in its own coordinate system: doors_0 at bbox [0.0, 0.36, 0.32, 0.97]. Transfer these objects from image 25 into image 19 coordination system: doors_0 at 40 degrees(CW), dist 3.6, 2.3, 2.8; **Infer object spatial relation and answer the question** Step 4: After localizing the objects from questions in the scene layout from step2, we can know that \"In image 19 coordination system: the stove_18 at 5 degrees(CW), dist 1.6, 0.1, 1.4; the table_25 at 50 degrees(CW), dist 3.2, 2.3, 1.9; the refrigerator_10 at 5 degrees(CW), dist 2.4, 0.1, 2.3\". Considering the **view perspective** mentioned in the question, the right answer this question is the right answer is C. back-right </think> <answer>C</answer>"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "SenseTime Research"
    ]
}