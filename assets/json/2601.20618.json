{
    "paper_title": "GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection",
    "authors": [
        "Shuguang Zhang",
        "Junhong Lian",
        "Guoxin Yu",
        "Baoxun Xu",
        "Xiang Ao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark."
        },
        {
            "title": "Start",
            "content": "GDCNET: GENERATIVE DISCREPANCY COMPARISON NETWORK FOR MULTIMODAL SARCASM DETECTION Shuguang Zhang1,2, Junhong Lian1,2, Guoxin Yu1,2,3, Baoxun Xu4, Xiang Ao1,2 1 State Key Laboratory of AI Safety, Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS) 2 University of Chinese Academy of Sciences, CAS 3 Pengcheng Laboratory 4 Shenzhen Stock Exchange 6 2 0 J 8 2 ] . [ 1 8 1 6 0 2 . 1 0 6 2 : r ABSTRACT Multimodal sarcasm detection (MSD) aims to identify sarcasm within imagetext pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNets superior accuracy and robustness, establishing new state-of-the-art on the MMSD2.0 benchmark. Index Terms Multimodal sarcasm detection, discrepancy generation, multimodal representation enhancement 1. INTRODUCTION Sarcasm is linguistic phenomenon in which the surface meaning of an utterance diverges significantly from the speakers intended communicative message. It is commonly employed for humor, critique, and subtle social commentary [1], making it potent tool for communication. The surge of multimodal social-media content has subsequently driven the advancement of Multimodal Sarcasm Detection (MSD) as an emerging research frontier. Transitioning from text-only to multimodal contexts considerably complicates this task, as the interplay between images and text often generates irony that transcends the meaning conveyed by either modality in isolation. MSD fundamentally relies on identifying the incongruity between images and text. Prior studies have tackled this challenge by aligning multimodal representations [2], leveraging techniques such as attention mechanisms [3], graph neural networks [4], external knowledge [5], and dynamic routing [6]. Despite these advances, Corresponding author (aoxiang@ict.ac.cn). The research work is supported by National Key R&D Plan No. 2022YFC3303305, the National Natural Science Foundation of China under Grant (No. 62576333, U2436209), the Strategic Priority Research Program of the Chinese Academy of Sciences under Grant No. XDB0680201, Beijing Natural Science Foundation JQ25015, the Innovation Funding of ICT, CAS under Grant No. E461060. existing methods [3, 4, 5, 6] still struggle with out-of-distribution generalization and often rely on superficial cues [7]. Relying solely on cross-modal embedding inconsistencies captures broad misalignments but often misses the subtle ironic cues crucial for accurate sarcasm detection, particularly when imagetext alignment is weak. Recent successes in Large Language Models (LLMs) and their multimodal extensions (MLLMs) [8, 9] have introduced extensive world knowledge and cross-modal reasoning capabilities, providing renewed impetus for the MSD task. These approaches generally exploit prompts to guide MLLMs in generating sarcastic explanations or signals for contextual data augmentation [10]. Such methods help alleviate the limitations of traditional cross-modal embedding mismatches, but they often overlook fundamental challenge in sarcasm detection: the inherent diversity of representation perspectives. As established in cognitive psychology [1], this diversity stems from variations in human cognition and interpretation. The issue becomes even more pronounced in the MSD task, because distinct visual elements can contribute to generating varied ironic texts through culturally-dependent interpretations [11]. As illustrated in Fig. 1, different MLLMs (Fig. 1(a)) or even the same model with different prompts (Fig. 1(b)) generate highly divergent sarcastic expressions for the same image. Motivated by this observation, we propose to utilize MLLMs as objective cross-modal semantic connectors rather than subjective sarcasm generators. In this paper, we propose framework named Generative Discrepancy Comparison Network (GDCNet). Inspired by the success of MLLMs in image captioning [12], we apply the MLLM to generate descriptive and factually grounded image captions, which serve as cross-modality semantic bridge within our framework. These descriptions not only preserve visual semantics but also provide consistent and reliable semantic anchors for effective comparison with the associated textual content, as illustrated in Fig. 1(c). Specifically, within GDCNet, we first employ an MLLM to produce objective image descriptions that serve as stable semantic references. Based on these descriptions and the corresponding original text, we introduce Generative Discrepancy Representation Module (GDRM), which captures discrepancies in semantic and sentiment dimensions between texts, as well as image-text consistency at the representation level. To further enhance sarcasm detection performance, gated fusion module is employed to integrate these multi-dimensional discrepancy representations with the original visual and textual features, thereby improving both the robustness and accuracy of the final classification. Our main contributions are summarized as follows: We present GDCNet, novel framework for MSD that leverages factual-grounded generated image descriptions as the semantic anchor to robustly quantify incongruity across visual = 1 indicates sarcastic and = 0 otherwise. The core challenge lies in identifying cross-modal incongruities, which often manifest as subtle mismatches between the textual semantics of and the visual context of I. 2.2. Cross-modal Feature Alignment Given the i-th sample (Ii, Ti), we utilize modality-specific encoders Rdv and textual features Ev and Et to produce visual features hv Rdt . To align features across modalities, we project both hv ht and ht into shared latent space through learnable linear layers, obtaining zv with the same dimensionality dz. We then adopt contrastive learning to align the projected features. The similarity score sij between the i-th image and j-th text is defined by cosine similarity: and zt sij = (zv )zt 2 zt j2 zv , i, {1, . . . , B}, (1) where is the mini-batch size. The margin-based contrastive loss is calculated as: Lcont ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) (cid:88) i=1 j=1 j=i max(cid:0)0, + sij sii (cid:1), (2) where > 0 is margin hyperparameter encouraging separation between positive and negative pairs. This objective maximizes the similarity of matched pairs while pushing apart mismatched ones, thereby enhancing cross-modal alignment. 2.3. Generative Discrepancy Representation Module The Generative Discrepancy Representation Module (GDRM) captures implicit conflicts between the original text and the image I. The module first employs an MLLM, such as LLaVA-NEXT [9], to generate textual description ˆT from the image, ensuring that the description faithfully reflects visual semantics. To avoid sarcasmrelated biases, the MLLM is restricted to image-only input, excluding any multimodal contextual cues. Consequently, ˆT serves as an unbiased and context-independent representation of the image. To quantify the inconsistency between the generated description ˆT and the original text , we compute three measures: semantic discrepancy, sentiment discrepancy, and visual-textual fidelity. The semantic discrepancy dsem measures the divergence in meaning between the original text and the generated description. We compute the cosine dissimilarity between the CLIP text embeddings of and ˆT . The sentiment discrepancy dsen captures shifts in sentiment. We use RoBERTa-based sentiment classifier [13] to obtain sentiment probability distributions for both texts, calculating the discrepancy via L1 distance. In addition, we measure visual-textual fidelity dfidelity, defined as the alignment between the generated description and the image. This is quantified as the cosine similarity cos(zI , ˆT ) between the CLIP image embedding zI and the CLIP text embedding of ˆT . lower dfidelity value signifies greater deviation between the generated text and the visual content. The three measures are concatenated to form discrepancy feature vector: = dsem dsen dfidelity, (3) Fig. 1. LLMs outputs on the same image: sarcastic explanations diverge across models and prompts, whereas factual descriptions remain stable, highlighting their potential as reliable semantic anchors. and textual modalities. We propose GDRM to extract key representations for MSD by comparing semantic and sentiment differences between generated image descriptions and the original text, as well as assessing image-text fidelity. Extensive experiments on widely-used benchmarks demonstrate that our GDCNet achieves significant improvements in detection accuracy and establishes new state-of-the-art on the MMSD2.0 dataset. 2. METHODOLOGY 2.1. Problem Formulation Given an image-text pair (I, ), the multimodal sarcasm detection task aims to classify it as sarcastic or non-sarcastic. Formally, we seek to learn mapping : (I, ) (cid:55) y, with {0, 1}. Here, where denotes concatenation. The vector is further processed by Multilayer Perceptron (MLP) to obtain the final discrepancy representation FD. Fig. 2. The Architecture of GDCNet. The Gated Multimodal Fusion & Classification module integrates discrepancy (FD), text (FT ), and image (FI ) features to produce the fused representation (Ffused). 2.4. Gated Multimodal Fusion & Classification 3. EXPERIMENTS To effectively integrate textual, visual, and discrepancy-based features, we employ gated fusion mechanism. This assigns learnable importance weights to each modality, enabling the model to adaptively focus on the most informative features. Given feature vectors from the text FT , image FI , and discrepancy FD, we compute modality-specific weights using the following gating functions: gT = σ(WT FT ), gI = σ(WI FI ), gD = σ(WDFD), (4) where WT , WI , WD are trainable parameters and σ denotes the sigmoid activation function. The final fused representation is: Ffused = gT FT + gI FI + gD FD. (5) For classification, we utilize four independent classifiers for each modality-specific feature vector, alongside the fused representation. These logits are concatenated to form combined representation logitsall. Subsequently, the concatenated logits are processed by an MLP to produce the final prediction Pfinal. 2.5. Optimization Objective We train GDCNet to jointly optimize sarcasm classification and multimodal alignment. The objective consists of two components: binary classification loss and contrastive loss. Sarcasm detection is formulated as binary classification problem, computed via the cross-entropy loss between the predicted Pfinal and the ground truth label y. Given batch of training samples, the classification loss is: LBCE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 [yi log Pfinal,i + (1 yi) log(1 Pfinal,i)] . (6) To enforce cross-modal consistency, we incorporate the contrastive loss Lcont (Eq. 2), which aligns paired imagetext embeddings in the shared latent space. The final objective combines both terms, with hyperparameter α controlling the trade-off between classification performance and multimodal alignment: = LBCE + αLcont. (7) 3.1. Experiments Details Dataset: We evaluate our approach on MMSD2.0 [7], refined and reliable benchmark for multimodal sarcasm detection. Constructed upon the original MMSD dataset [2], it eliminates spurious cues and rectifies unreasonable annotations, thereby offering more robust basis for evaluation. Baselines: To evaluate our approach, we compare it against competitive baselines in three categories. Text-only methods include BiLSTM [14], SMSD [15], and BERT [16]; image-only methods consist of ResNet [17] and ViT [18]; and multimodal methods encompass InCrossMGs [19], HKE [5], Multi-view CLIP [7], DIP [20], TFCD [21], MOBA [22], CofiPara [10], and ADs [23]. Implementation Details: We implement GDCNet using the CLIP backbone with feature dimensions of 512 (text) and 768 (image). For image caption generation, we employ LLaVA-Next-7B [9]. Training is conducted on four NVIDIA RTX 4090 GPUs. The model is optimized via the Adam optimizer for 10 epochs with batch size of 32. We employ learning rate of 5 104 for the specific task modules and 1106 for the CLIP backbone. To ensure training stability, we apply weight decay of 0.05 and gradient clipping with maximum norm of 5.0. For the final objective function, the contrastive loss margin is set to = 0.2, weighted by the hyperparameter α = 0.1. 3.2. Main Results Table 1 presents the comparative results, highlighting the efficacy of our proposed framework. GDCNet establishes new state-of-theart on the MMSD2.0 benchmark, consistently outperforming prior baselines in both accuracy and F1-score. This performance advantage stems from our novel modeling paradigm. In contrast to CofiPara, which relies on joint text-image sarcasm rationale generation, GDCNet decouples image description generation from textual input. This design effectively mitigates textual bias by producing neutral visual observations, thereby facilitating the precise quantification of sarcasm through semantic and sentiment divergence analysis. Furthermore, the adaptive gated fusion mechanism explicitly incorTable 1. Performance comparison on MMSD2.0. Multimodal methods outperform unimodal ones, and GDCNet achieves the highest overall performance. Table 3. Comparison results for LLM-based methods and GDCNet on the MMSD2.0 dataset. GDCNet significantly outperforms general-purpose MLLMs, even when they utilize CoT prompting. Method Acc.(%) P(%) R(%) F1(%) Method Acc. (%) (%) (%) F1 (%) Text-Only Methods BiLSTM [14] SMSD [15] BERT [16] 72.48 73.56 76.52 68.02 68.45 74.48 68.08 71.55 73.09 68.05 69.97 73.78 Image-Only Methods ResNet [17] ViT [18] 65.50 72.02 61.17 65.26 54.39 74.83 57.58 69.72 Multi-Modal Methods InCrossMGs [19] HKE [5] Multi-view CLIP [7] DIP [20] TFCD [21] MoBA [22] CofiPara [10] ADs [23] GDCNet (Ours) 79.83 76.50 85.14 84.63 86.54 85.01 85.66 85.60 87.38 75.82 73.48 80.33 84.17 82.46 80.46 85.79 85.28 78.01 71.07 88.24 85.20 87.95 87.67 85.43 85. 76.90 72.25 84.09 84.68 84.31 83.64 85.61 85.41 83.39 89.51 86.34 porates cross-modal divergence features, preventing modality dominance and ensuring balanced multimodal integration. 3.3. Ablation Study To assess the contribution of GDRM and its specific components, we conduct ablations on three configurations: removing the entire GDRM (w/o GDRM), and excluding semantic (w/o SemD) or sentiment (w/o SenD) discrepancies. Table 2 details the results on the MMSD2.0 benchmark. Removing the entire GDRM yields the most significant performance drop, reducing Accuracy by 2.96% and F1score by 4.15%. This underscores the critical role of explicit discrepancy modeling. Specifically, the absence of SemD impairs the detection of literal contradictions in incongruent image-text pairs. Conversely, removing SenD limits the capacity to capture subtle sentiment polarity shifts in text-driven sarcasm. The complementarity of these components enables the full model to maintain balanced performance and superior robustness. 3.4. Comparison with LLM-based Methods To further evaluate the performance of GDCNet, we conduct comparative analysis against various direct LLM-based methods for sarcasm detection on the MMSD2.0 dataset. As presented in Table 3, we evaluate prominent MLLMs such as LLaVA[24], Qwen-VL[25], Table 2. Ablation study on the MMSD2.0 dataset. The significant performance drops across all metrics upon removing GDRM or its sub-components validate their individual and collective importance. Method Acc. (%) (%) (%) F1 (%) Full Model - w/o GDRM - w/o SemD - w/o SenD 87.38 84.42 86.23 85. 83.39 89.51 86.34 78.56 80.27 81.74 86.17 87.09 87.63 82.19 83.54 84. LLaVA (Zero-Shot) LLaVA (CoT) Qwen-VL (Zero-Shot) Qwen-VL (CoT) GPT-4o (Zero-Shot) GPT-4o (CoT) GDCNet(Ours) 51.06 48.69 40.63 58.86 71.07 74.26 87.38 40.09 40.93 32.44 56.82 79.52 65.81 46.40 65.17 35.53 58.67 71.07 72. 83.39 89.51 43.02 50.28 33.63 57.26 70.24 68.92 86.34 Table 4. Performance and cost comparison of BLIP-2 and LLaVANEXT as caption generators in GDCNet. While BLIP-2 is faster, LLaVA-NEXT provides the detailed semantic grounding necessary for higher accuracy. MLLM Time (s) Tokens CLIP-S Acc. (%) F1 (%) BLIP-2 LLaVA-NEXT 0.23 1.70 21.53 67.29 31.3 49.2 86.73 87. 85.66 86.34 and GPT-4o[26], under both Zero-Shot and Chain-of-Thought (CoT) prompting strategies. GDCNet consistently outperforms all LLMbased baselines in all metrics. While CoT improves reasoning for some models, MLLMs still struggle with sarcasm detection, underscoring the advantage of GDCNets explicit discrepancy modeling for this challenging task. 3.5. MLLM Contribution Analysis To investigate the impact of the MLLM on GDCNets performance, we conducted an ablation study on the MMSD2.0 benchmark by substituting our image caption generator with BLIP-2[27] and LLaVA-NEXT[9]. For each MLLM, we generate descriptions for all samples using standardized prompt and train GDCNet with fixed hyperparameters for both configurations. As shown in Table 4, while BLIP-2 offers superior inference speed, LLaVA-NEXT generates significantly richer descriptions with higher semantic consistency. Crucially, richer semantic captions lead to consistently better downstream performance, highlighting the importance of caption quality in multimodal sarcasm detection. 4. CONCLUSION In this paper, we propose GDCNet, novel framework for multimodal sarcasm detection that effectively mitigates noise from LLM-generated sarcastic cues and explicitly models cross-modal incongruity. By leveraging LLM-generated image-grounded captions as cross-modal anchors, GDCNet captures cross-modal incongruity through fine-grained semantic and sentiment discrepancy modeling. Furthermore, an adaptive gated fusion module integrates visual, textual, and discrepancy signals, effectively alleviating modality dominance and reducing spurious correlations. Extensive experiments on the MMSD2.0 dataset demonstrate that GDCNet establishes new state-of-the-art, consistently outperforming competitive baselines. These results underscore the potential of leveraging LLMs not merely as data generators but as structural guides for capturing subtle cross-modal incongruities, advancing the frontier of complex multimodal understanding tasks such as sarcasm detection. 5. REFERENCES [1] Raymond Gibbs and Herbert Colston, Irony in language and thought: cognitive science reader, Psychology Press, 2007. [2] Yitao Cai, Huiyu Cai, and Xiaojun Wan, Multi-modal sarcasm detection in twitter with hierarchical fusion model, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 25062515. [3] Xinyu Wang, Xiaowen Sun, Tan Yang, and Hongbo Wang, Building bridge: method for image-text sarcasm detection without pretraining on image-text data, in Proceedings of the first international workshop on natural language processing beyond text, 2020, pp. 1929. [4] Bin Liang, Chenwei Lou, Xiang Li, Min Yang, Lin Gui, Yulan He, Wenjie Pei, and Ruifeng Xu, Multi-modal sarcasm detection via cross-modal graph convolutional network, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 2022, pp. 17671777. [5] Hui Liu, Wenya Wang, and Haoliang Li, Towards multimodal sarcasm detection via hierarchical congruity modeling with knowledge enhancement, in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 49955006. [6] Yuan Tian, Nan Xu, Ruike Zhang, and Wenji Mao, Dynamic routing transformer network for multimodal sarcasm detection, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 24682480. [7] Libo Qin, Shijue Huang, Qiguang Chen, Chenran Cai, Yudi Zhang, Bin Liang, Wanxiang Che, and Ruifeng Xu, Mmsd2.0: Towards reliable multimodal sarcasm detection system, Findings of the Association for Computational Linguistics: ACL 2023, pp. 1083410845, 2023. [8] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny, Minigpt-4: Enhancing vision-language understanding with advanced large language models, in ICLR, 2024. [9] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [10] Zixin Chen, Hongzhan Lin, Ziyang Luo, Mingfei Cheng, Jing Ma, and Guang Chen, Cofipara: coarse-to-fine paradigm for multimodal sarcasm target identification with large multimodal models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 96639687. [11] Shafkat Farabi, Tharindu Ranasinghe, Diptesh Kanojia, Yu Kong, and Marcos Zampieri, survey of multimodal sarcasm detection, in Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, 2024. [12] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee, Improved baselines with visual instruction tuning, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 2629626306. [13] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli, fairseq: fast, extensible toolkit for sequence modeling, in Proceedings of NAACL-HLT 2019: Demonstrations, 2019. [14] J. Zhou, B. Xu, X. Xie, and Q. Xu, Attention-based bidirectional lstm for text classification, in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016, pp. 14681477. [15] Tao Xiong, Peiran Zhang, Hongbo Zhu, and Yihui Yang, Sarcasm detection with self-matching networks and low-rank bilinear pooling, in The World Wide Web Conference, 2019, pp. 21152124. [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of NAACLHLT, 2019, pp. 41714186. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Deep residual learning for image recognition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770778. [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, and Sylvain Gelly, An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2020. [19] Bin Liang, Chenwei Lou, Xiang Li, Lin Gui, Min Yang, and Ruifeng Xu, Multi-modal sarcasm detection with interactive in-modal and cross-modal graphs, in Proceedings of the 29th ACM international conference on multimedia, 2021, pp. 4707 4715. [20] Changsong Wen, Guoli Jia, and Jufeng Yang, Dip: Dual incongruity perceiving network for sarcasm detection, in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 25402550. [21] Zhihong Zhu, Xianwei Zhuang, Yunyan Zhang, Derong Xu, Guimin Hu, Xian Wu, and Yefeng Zheng, Tfcd: Towards multi-modal sarcasm detection via training-free counterfactual debiasing, in Proceedings of IJCAI, 2024. [22] Yifeng Xie, Zhihong Zhu, Xin Chen, Zhanpeng Chen, and Zhiqi Huang, Moba: Mixture of bi-directional adapter for multi-modal sarcasm detection, in Proceedings of the 32nd ACM International Conference on Multimedia, New York, NY, USA, 2024, MM 24, p. 42644272, Association for Computing Machinery. [23] Soumyadeep Jana, Sahil Danayak, and Sanasam Ranbir Singh, Ads: Adapter-state sharing framework for multimodal sarcasm detection, arXiv preprint arXiv:2507.04508, 2025. [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee, Visual instruction tuning, Advances in neural information processing systems, vol. 36, pp. 3489234916, 2023. [25] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou, Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. [26] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi, Blip2: Bootstrapping language-image pretraining with frozen imarXiv preprint age encoders and large language models, arXiv:2301.12597, 2023."
        }
    ],
    "affiliations": [
        "Pengcheng Laboratory",
        "Shenzhen Stock Exchange",
        "State Key Laboratory of AI Safety, Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS)",
        "University of Chinese Academy of Sciences, CAS"
    ]
}