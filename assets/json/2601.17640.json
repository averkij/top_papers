{
    "paper_title": "End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions",
    "authors": [
        "Anfeng Xu",
        "Tiantian Feng",
        "Somer Bishop",
        "Catherine Lord",
        "Shrikanth Narayanan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate transcription and speaker diarization of child-adult spoken interactions are crucial for developmental and clinical research. However, manual annotation is time-consuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and speech recognition pipelines, which can lead to error propagation. This paper presents a unified end-to-end framework that extends the Whisper encoder-decoder architecture to jointly model ASR and child-adult speaker role diarization. The proposed approach integrates: (i) a serialized output training scheme that emits speaker tags and start/end timestamps, (ii) a lightweight frame-level diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) a state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whisper-small and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of child-adult interactions at scale. The code and model weights are publicly available"
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 End-to-End Joint ASR and Speaker Role Diarization with Child-Adult Interactions Anfeng Xu, Graduate Student Member, IEEE, Tiantian Feng, Member, IEEE, Somer Bishop, Catherine Lord, Shrikanth Narayanan, Fellow, IEEE 6 2 0 J 5 2 ] . e [ 1 0 4 6 7 1 . 1 0 6 2 : r AbstractAccurate transcription and speaker diarization of childadult spoken interactions are crucial for developmental and clinical research. However, manual annotation is timeconsuming and challenging to scale. Existing automated systems typically rely on cascaded speaker diarization and automatic speech recognition pipelines, which can lead to error propagation. This paper presents unified end-to-end framework that extends the Whisper encoderdecoder architecture to jointly model ASR and childadult speaker role diarization. The proposed approach integrates: (i) serialized output training scheme that emits speaker tags and start/end timestamps, (ii) lightweight framelevel diarization head that enhances speaker-discriminative encoder representations, (iii) diarization-guided silence suppression for improved temporal precision, and (iv) state-machine-based forced decoding procedure that guarantees structurally valid outputs. Comprehensive evaluations on two datasets demonstrate consistent and substantial improvements over two cascaded baselines, achieving lower multi-talker word error rates and demonstrating competitive diarization accuracy across both Whispersmall and Whisper-large models. These findings highlight the effectiveness and practical utility of the proposed joint modeling framework for generating reliable, speaker-attributed transcripts of childadult interactions at scale. The code and model weights are publicly available1. Index TermsASR, Speaker Diarization, Child Speech, Serialized Output Training I. INTRODUCTION interest, meaningful Child-adult spoken interactions play central role in developmental research, clinical assessment, and behavioral analysis. These conversational exchanges provide critical information about childrens expressive language abilities, social communication patterns, and turn-taking behavior [1]. Unlike monologic speech settings where only single speakers content may be of interpretation of childadult interactions fundamentally depends on knowing what both participants say and how they respond to one another. Extracting reliable speaker-attributed transcripts from these interactions enables the computation of conversational metrics (e.g., words per minute, utterance duration, conversational latency) that serve as quantitative indicators of spoken language development and severity of social communication symptoms [2], [3]. However, manual transcription and speaker labeling are extremely labor-intensive and difficult to scale for large cohorts. This motivates the development of frameworks Anfeng Xu, Tiantian Feng, and Shrikanth Narayanan are affiliated with Viterbi School of Engineering, University of Southern California, US. Somer Bishop is affiliated with Weill Institute for Neurosciences, University of California, San Francisco, US. Catherine Lord is affiliated with David Geffen School of Medicine, University of California, Los Angeles, US. 1https://github.com/usc-sail/joint-asr-diarization-child-adult for joint automatic speech recognition (ASR) and speaker diarization that can accurately generate transcripts with precise speaker attribution and timing. Despite substantial advancements in end-to-end ASR [4] and speaker diarization [5] systems, automated transcription of child-adult dyadic interactions remains challenging. Child speech differs significantly from adult speech in terms of pitch, formant structure, articulation, and linguistic complexity [6] [8]. These challenges are further exacerbated in naturalistic settings where recordings often contain significant ambient noise and speakers present widely varying expressive abilities [9]. While such acoustic and linguistic heterogeneity poses fundamental modeling challenges for ASR systems, recent studies show that Whisper models can achieve strong performance when appropriately fine-tuned, likely due to their large-scale and diverse pretraining data [10], [11]. Crucially, many downstream behavioral analyses require accurate utterance-level timestamps, in addition to transcripts and speaker labels [12]. Prior work has approached speakerand, in some cases, timestamp-attributed transcription using cascaded pipelines, typically performing speaker diarization followed by ASR, or applying ASR first and attributing speakers afterward [13][15]. Diarization-first pipelines can produce reasonable speaker boundaries, but segmentation errors naturally propagate to the ASR stage. Conversely, ASR-first pipelines can degrade the performance of forced alignment and speaker attribution through transcription errors. These cascaded designs require substantial domain tuning and often produce suboptimal timestamp or speaker-role accuracy. To overcome these challenges, we propose unified endto-end framework for joint ASR and speaker-role diarization tailored to child-adult interactions. Our approach extends the Whisper encoder-decoder architecture with two key components: (1) serialized output training scheme that generates speaker tags and start/end timestamp tokens within single output sequence, and (2) lightweight diarization head attached to the final encoder layer that produces frame-level speaker activity labels. Timestamp supervision, speaker-role supervision, and lexical prediction are learned jointly, enabling the encoder to develop speaker-discriminative and temporally aligned representations. To ensure structurally valid outputs, we introduce state-machine-based forced decoding mechanism that enforces the correct ordering of speaker and timestamp tokens. Additionally, we incorporate diarization-guided silence suppression during decoding to reduce timestamp drift and improve boundary precision. Our work makes the following contributions: Unified Whisper ASR and diarization modeling: single JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Whisper-based system that predicts transcripts, speaker roles, and utterance-level timestamps without external segmentation or forced alignment. Diarization-guided supervision and decoding: framelevel diarization head shapes speaker-discriminative encoder representations, while silence-aware timestamp suppression stabilizes temporal boundaries and improves speaker-attribution consistency. Reliable structured decoding: forced decoding state machine that eliminates missing-token and malformedoutput errors commonly observed in SOT generation under low-resource fine-tuning. Comprehensive evaluation on childadult speech: Experiments on two child-adult interaction datasets demonstrate consistent improvements in mtWER, WER, AER, and DER over two cascaded baselines. Overall, this work introduces practical and effective solution for generating rich, speaker-attributed transcripts with accurate temporal boundaries in child-adult conversation settings. By integrating ASR, speaker role identification, and timestamp prediction into single model, our approach enables scalable and reliable automatic extraction of behavioral speech measures for research and clinical applications. II. RELATED WORKS A. Speaker Role Diarization for Child-Adult Interactions [18] directly predict Early speaker diarization systems rely on modular pipelines that combine speech activity detection and speaker embedding extraction, such as x-vectors [16], followed by clustering [5]. More recent end-to-end neural speaker diarizaframetion (EEND) methods [17], level speaker activities, achieving strong results in meeting transcription and multi-party conversations. However, little work has explored role-specific diarization in settings such as interviewerinterviewee or doctorpatient interactions. In childadult conversations, the two speakers often exhibit highly distinctive vocal characteristics (e.g., pitch, formants, spectral profiles), making the role labels acoustically well separated [6][8]. These innate differences render end-to-end models particularly suitable for direct speaker-role assignment, as they can learn stable, discriminative cues without relying on semantic contexts. Building on this intuition, recent work has begun to directly target childadult role diarization by leveraging large speech foundation models within end-to-end frameworks that predict silence, child, and adult speaker labels frame by frame [19], [20]. The studies found that pre-trained models, such as Whisper [21], WavLM [22], and MMS [23], substantially enhance diarization accuracy compared to traditional clustering-based systems on clinical datasets. Notably, Whisper models consistently outperform other evaluated speech foundation models [19]. Further extending this direction, follow-up work [24] proposed data-efficient approach that trains Whisper encoders on simulated childadult conversations, achieving strong zeroshot performance and further gains after lightweight finetuning on limited real data. Together, these advances highlight both the effectiveness of large pretrained speech models and the importance of role-specific modeling tailored to dyadic childadult interactions. B. Multi-Speaker ASR and Serialized Output Training Traditional speaker-attributed ASR systems are commonly built as cascaded pipelines in which diarization, separation, and ASR are handled by separate, independently trained modules [25], [26]. Similarly, in the childadult domain, existing systems predominantly follow modular approach, performing ASR child-adult speaker prediction independently [13]. These pipelines suffer from error propagation between stages and incur additional computational costs due to the need for separate training for each component. Earlier work began exploring the idea of emitting speaker tokens jointly with lexical transcriptions [27]. Building on this direction, Serialized Output Training (SOT) [28] has emerged as prominent framework for unified multi-speaker ASR. Originally developed for end-to-end overlapped speech recognition, SOT represents multi-talker interactions as single serialized sequence containing both lexical tokens and explicit speaker or speaker-change markers. Leveraging multiheaded attention mechanisms [29], SOT-based architectures can attend to different speakers within one model rather than duplicating encoders or decoder heads. This formulation enables encoderdecoder models to jointly learn content and speaker attribution without relying on independent modular systems. Subsequent studies have extended SOT for multitalker ASR objectives [30][32], demonstrating its effectiveness in modeling multi-talker conversations within unified sequence-to-sequence framework. C. Joint ASR and Diarization Modeling Following the development of the SOT framework, several works have investigated training for both ASR and speaker diarization objectives. Transcribe-to-Diarize [33] builds on speaker-attributed ASR trained with SOT by learning tokenlevel start and end times from decoderencoder attention, enabling joint transcription and diarization, though its timestamp estimates rely on attention patterns without any supervision and are therefore less robust. Sortformer [34] and JEDIS-LLM [35] integrate speaker supervision into the ASR encoder, improving multi-speaker recognition with standard cross-entropy training, but they do not provide frameor wordlevel diarization timestamps. Notably, our work is motivated by SLIDAR [36] and SOMSRED [37], which jointly predict transcripts, timestamp tokens, and local speaker embeddings within single decoding pass, followed by clustering to obtain global speaker identities. Our setting differs in that we target speaker-role diarization rather than general speaker diarization, and we work in far more low-resource conditions, which necessitate fine-tuning an existing speech foundation model (e.g., Whisper) to achieve reliable performance. III. CASCADED BASELINES We characterize the performance of non-end-to-end frameworks by evaluating cascaded systems that follow the more traditional pipeline for speaker-attributed ASR. Prior approaches JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 speech segments and speaker roles. Each resulting segment is then passed to Whisper ASR model to generate the corresponding transcript. Frame-level speaker activities from the diarization model are first converted into contiguous segments by merging consecutive frames with the same speaker label. To produce more natural ASR input segments, predicted speech regions from the same speaker that are separated by less than 0.3s are further merged. Then, extremely short speaker segments (under 0.2s) are discarded. Each resulting segment is then decoded independently by Whisper ASR to generate the transcript for that speakers turn. The ASR and diarization modules are fine-tuned independently but share the same Whisper backbone for initialization. The ASR model is trained on utterance-level inputs, while the diarization model is fine-tuned on longer speech regions, up to 30s. The speaker diarization modeling is illustrated in Figure 1, following the prior work [24]. We model child-adult speaker diarization as frame-level classification task using the Whisper encoder. Given an input waveform = [x1, . . . , xT ], the objective is to assign label yt {s, c, a} to each frame xt, corresponding to silence/noise, child speech, and adult speech, respectively. Similar to [24], rather than unfreezing the Whisper encoder, we keep it frozen and apply Low-Rank Adaptation (LoRA) [40] within its feed-forward transformer blocks to mitigate overfitting when fine-tuned only for the speaker diarization objective. The input waveform is first encoded into hidden embeddings from the Whisper encoder. learnable weighted pooling then combines the hidden embeddings from all encoder layers into single fused embedding. The pooled features are then fed into stack of three 1D CNN layers, each with 256 channels, kernel size of 1, ReLU activation, and dropout rate of 0.1 during training. final 1D CNN with kernel size of 1 and channel width of 3 produces the frame-level class probabilities. C. ASR-First Baseline: SOT-ASR Forced Alignment The second fine-tuned cascaded baseline seeks to obtain speaker-attributed transcripts without training dedicated diarization model. We fine-tune Whisper ASR model with SOT to generate unified multi-speaker transcript in the form: <speaker> transcript, where <speaker> can be <child> or <adult>, which mark child and adult speech, respectively. The model focuses solely on lexical and speaker-role prediction without emitting timestamp tokens. The ASR model is trained and applied to audio chunks up to 30 seconds long. To recover temporal boundaries, we apply forced alignment between each audio chunk and its decoded transcript. Specifically, we use the NVIDIA NeMo forced aligner [41] on the transcript after removing the speaker tokens, which yields word-level timestamps. The earliest and latest aligned word boundaries are then taken as the utterance-level start and end times. IV. PROPOSED METHOD Fig. 1. Baseline speaker diarization pipeline with Whisper Encoder. typically decompose the task into modular stages, allowing speaker diarization and lexical transcription to be handled by separate systems. In this framework, we first consider zero-shot baseline using WhisperX [38]. In addition, we evaluate two fine-tuned baselines, considering using either (i) diarization model that first predicts who is speaking and when, followed by ASR on each segmented region, or (ii) an ASR model that directly outputs both words and speakerrole tags, with segmented boundaries generated afterward via forced alignment. These two variants serve as complementary cascaded baselines against which we compare our unified end-to-end model. We choose Whisper models for both the cascaded baselines and our proposed method because they demonstrate state-of-the-art performance in both ASR and speaker diarization in child-adult contexts, while also being suitable for SOT-based modelings [11], [19]. A. Zero-Shot Baseline: Using WhisperX WhisperX [38] is an enhanced transcription pipeline that integrates Voice Activity Detection (VAD), Whisper ASR, and forced alignment module based on wav2vec 2.0 [39] to generate accurate word-level timestamps. In our setup, we use WhisperX solely for transcription and timestamp prediction. Given an input audio segment, WhisperX first applies VAD to identify speech regions and then produces an utterancelevel transcript using the Whisper model. Its alignment module subsequently refines these outputs by estimating start and end times for each word. This yields sequence of (word, start time, end time) tuples, which we use as the foundation for downstream speaker-role attribution. To assign speaker label to each word, we apply publicly available childadult diarization model [24] that outputs frame-level probabilities for child, adult, silence, and overlap classes. For each word, we identify all frames corresponding to its timestamp interval and compute the average child and adult log probabilities over the duration. The speaker role with the higher average log probability is then assigned to the word. Then, we merge words within 0.3s apart from the same speaker as single utterance. This procedure generates utterance-level transcripts with speaker and timestamp labels, eliminating the need for additional fine-tuning, forming strong zero-shot cascaded baseline. B. Diarization-First Baseline: Speaker Diarization ASR A. Model Architecture For the first fine-tuned baseline, we implement cascaded system in which dedicated diarization model first predicts Our framework extends the Whisper encoder-decoder architecture with both diarization head and serialized output JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 2. Proposed joint ASR and speaker diarization training architecture. The last hidden layer from the encoder is used for the decoder and diarization head. training. The encoder processes input log-Mel features through convolutional layer and stack of transformer layers, producing contextualized embeddings that capture long-range temporal dependencies. These encoder representations are used for two complementary objectives: ASR and speaker diarization through frame-level speaker classification. 1) Serialized Output Training Setup: We adopt serialized output training (SOT) framework to handle multi-speaker conversation transcription and speaker diarization within single decoding stream. Specifically, we train the decoder to autoregressively produce unified token sequence for the full conversation, including explicit speaker and timestamp tokens. Each utterance segment is represented as: <t_start> <speaker> transcript <t_end>, where <speaker> indicates the active speaker (<child> for child, <adult> for adult), and <t_start> / <t_end> denote the starting and ending timestamps for the utterance. This ordering places temporal boundaries first, followed by the speaker role and then the lexical content, providing the decoder with clear sequential structure that aligns with how utterances unfold in the audio. The sequencebased representation enables the model to jointly predict lexical, timestamp, and speaker-role information within single decoding process, eliminating the need for external alignment or cascaded systems. 2) Diarization Head.: In addition to the serialized output, we incorporate lightweight diarization head attached to the final encoder layer. The head consists of stack of 1D convolutional layers that outputs frame-level probabilities across three mutually exclusive classes: child, adult, and silence, similar to the Diarization-First Baseline diarization method as detailed in III-B. This auxiliary supervision provides explicit information about speaker-role and speech activity boundaries, enhancing the encoders speaker and acoustic representations to aid the decoder with speaker and timestamp token predictions. The shared encoder thus learns representations that are jointly informative for frame-level diarization and token-level ASR generation. B. Loss Function The model is optimized using weighted multi-task objective that combines ASR and diarization losses: Ltotal = LASR + λdiar Ldiar, (1) where λdiar controls the weight of the diarization supervision. ASR Loss.: The ASR objective is the standard sequence cross-entropy loss over serialized token sequences: LASR ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 log (yt y<t, h), (2) where yt denotes the ground-truth token (including text, timestamp, or speaker tokens) at step from total tokens, and represents the encoder output. This formulation enables the model to learn consistent decoding of both lexical and structural tokens, aligning the ASR process with diarization and timing cues. Diarization Loss.: The diarization head outputs 3dimensional probability vector ˆsn = [ˆs(child) ] for each encoder frame n. We supervise these predictions using the multi-class cross-entropy loss: , ˆs(adult) , ˆs(sil) Ldiar ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:88) n=1 c{child, adult, sil} log (ˆs(c) s(c) hn), (3) where s(c) is one-hot label for the active class at frame n. This term explicitly aligns encoder features with speaker roles and speaking activity states, promoting stronger temporal and role-specific representations that benefit both decoding and downstream diarization metrics. C. Diarization-guided Silence Suppression To further leverage diarization information at inference time, we introduce decoder guidance based on predicted speech activity. Timestamp tokens corresponding to diarizationpredicted silence regions are suppressed during beam search decoding. Specifically, we identify silence regions as contiguous spans where ˆs(sil) < 0.7. To allow more flexible timestamp JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST"
        },
        {
            "title": "State Machine for Forced Serialized Output Decoding",
            "content": "S2 Start Time S3 Speaker S4 Text S5 End Time S1 Transcription Start S6 Transcription End Fig. 3. State-machine diagram for forced decoding during inference. placement near the boundaries, we shrink each predicted silence region by removing 0.2 from both its start and end before applying suppression. This guidance discourages the decoder from emitting redundant timestank tokens for silence segments, thereby reducing utterance boundary mistakes and improving utterance segmentation reliability. D. State-Machine-Based Forced Decoding To ensure consistent generation of structured, serialized outputs, we employ forced-decoding strategy based on finitestate machine. While standard beam search decoding allows flexible token ordering, unconstrained generation may lead to malformed or misordered tokens, such as missing timestamp boundaries or incorrect speakertext order. To mitigate this, we define decoding state machine that enforces the expected token order within each conversational segment. The state machine, as illustrated in Fig. 3, defines valid transition graph. At each state, the decoder is expected to output the following tokens: S1 :< startof transcript >< en >< transcribe > S2 :< timestamp > S3 :< child > or < adult > S4 : text token S5 :< timestamp > S6 :< endof transcribe > . During beam search, only tokens allowed by the current state are assigned nonzero probabilities, masking out invalid transitions. For example, once timestamp token < timestamp > is emitted at state S2, only text tokens can follow immediately. Then, both text and ending timestamp tokens become permissible. Similarly, the next valid token after an ending timestamp must be either another speaker token or the final transcription end token. The forced decoding framework ensures the syntactic correctness of the serialized output, guaranteeing that every speaker segment includes both start and end timestamps. When combined with the diarization-guided silence suppression described earlier, we expect to achieve more temporally and structurally coherent outputs without sacrificing speech recognition accuracy. TABLE SUMMARY OF PLAYLOGUE AND ADOS DATASETS. Category Playlogue ADOS Age (Min-Max, in months) Gender (M/F/Unknown) Clinical Diagnosis 36-66 96/61/1 TD 43-158 126/45/9 Mostly Non-TD Segment Count (train/dev/test) Total Hours (train/dev/test) 2326/722/987 16.20/5.07/6.75 836/267/1082 5.36/1.66/7. Recording Condition Naturalistic Interview Publically Available Yes No E. Diarization Head Initialization and Fine-tuning Prior to joint optimization, the diarization head is pretrained on frozen encoder representations using scalar-mix of all encoder layers. This aggregated representation exposes the diarization head to lower-level acoustic cues, combining lowlevel speaker and energy information from early layers with higher-level phonetic context from deeper layers. The encoder remains frozen, and no LoRA is applied. After convergence, we fine-tune the joint ASRdiarization model (Section IV-B), where only the final encoder layer is used as input. Although scalar mixing is no longer employed, this initialization transfers the lower-level information into the final-layer representation, yielding richer speaker embeddings. After joint training, the speaker head is further fine-tuned while all other parameters remain frozen. This step improves the accuracy of silence prediction, which is crucial for suppressing silent timestamp tokens during decoding. V. EXPERIMENTS A. Datasets We consider two child-adult conversational datasets: the publicly available Playlogue dataset [42] and the in-house ADOS-Mod3 corpus of the Autism Diagnostic Observation Schedule [43]. Our research adheres to all Institutional Review Board (IRB) protocols and complies with the Data Use Agreements (DUAs) by the ADOS-Mod3 data providers. The summary of the dataset statistics is summarized in Table I. 1) Playlogue: The Playlogue dataset comprises over 33 hours of naturalistic, long-form adultchild interactions sourced from the TalkBank system. It spans three playbased corpora and one narrative corpus, all of which involve typically developed (TD) preschool-aged children (35 years old). These recordings capture spontaneous, free-play dyadic conversations between children and adults, including parents, clinicians, and researchers. Each recording includes wordaligned transcripts with speaker labels derived using NVIDIA NeMo forced alignment [41]. The corpus does not contain overlapping speech, as the forced aligner only supports nonoverlapping segments. Since we have observed that words with abnormally long predicted durations were typically misaligned, we discard any word whose duration exceeds 2 seconds from both the audio and transcript. We merge consecutive words from the same speaker into single utterance whenever the gaps between them are shorter than 0.3 seconds. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 Playlogue presents challenging benchmark due to its realworld recording conditions, including higher background noise and less accurate ASR annotations than our in-house ADOSMod3 dataset. 2) ADOS-Mod3: The ADOS [44] consists of multiple semi-structured activities and typically takes 4060 minutes to administer. Module 3 (Mod3) is intended for verbally fluent children and includes 14 activities, several of which involve interview-style questions on various topics. For our analyses, we focus on two of these interview components, Social Difficulties and Annoyance and Emotions, totalling 352 components from 180 children. The children are generally older than those in the Playlogue dataset, with an average age of 8.53 years and range of 3 to 13 years. Approximately half received diagnosis of ASD, while most of the remaining participants were diagnosed with other neurodevelopmental or psychiatric conditions, including ADHD and mental or language disorders. Recordings were obtained from two medical centers: 96 from Cincinnati Childrens Hospital (CHIC) and 84 from the University of Michigan Autism and Communication Disorders Center (MICH). We also note that the majority of CHIC cases were not diagnosed with ASD, whereas the majority of MICH cases were diagnosed with ASD. Since the annotations for overlapping speech were highly unreliable, and to maintain consistency with the Playlogue dataset, we only consider non-overlapping speech segments in both training and testing. We refer to this dataset by ADOS for the rest of the paper for conciseness. Overall, the ADOS dataset represents substantially different data domain from Playlogue, particularly in terms of participant age, clinical conditions, and recording setup. 3) Data Preprocessing: We preprocess the recordings for both datasets by forming audio segments that maximize the number of complete utterances within 30-second window, the maximum duration allowed for Whisper input. Each boundary is placed at the midpoint of ground-truth silence segment. As result, the Playlogue and ADOS datasets have average segment durations of 25.0 and 23.2 seconds, respectively. We use the official train-dev-test split for the Playlogue dataset. In total, Playlogue has 16.20 hours, 5.07 hours, and 6.75 hours of training, development, and test data, respectively. For the ADOS dataset, the CHIC recordings are used exclusively for testing. The MICH recordings are partitioned at the session level into an 80:20 train/dev split. This partitioning enforces strict speaker disjointness, ensuring that no speaker in the train or development sets appears in the test set. In total, ADOS is much smaller with 5.36 hours, 1.66 hours, and 7.05 hours of training, development, and test data, respectively. B. Metrics We employ the multi-talker Word Error Rate (mtWER), originally introduced in the CHiME-8 MMCSG Challenge [45]. This metric extends conventional WER by jointly evaluating transcription accuracy and speaker-attribution consistency in multi-speaker conditions. In addition to standard word errors (substitutions, insertions, deletions), mtWER explicitly counts speaker-attribution mistakes, making it more REF HYP how how are were you you oh ERROR INS - SUB ATTR - am am good great thanks - SUB ATTR DEL Fig. 4. Error components for mtWER. The yellow and blue colors highlight two separate speaker roles. REF is the ground-truth transcript, while HYP is the inferred hypothesis. informative metric for joint speaker-attributed ASR. An illustrative example of all error components is shown in Figure 4. We report mtWER averaged over the child and adult speaker roles. The metric for each role is computed as mtW ERs = IN Ss + DELs + SU Bs + AT Rs REFs , where denotes specific speaker role (e.g., child), and IN Ss, DELs, SU Bs, and AT Rs are the insertion, deletion, substitution, and speaker-attribution errors, respectively. Here, REFs is the total number of reference words from speaker s. Speaker-attribution errors occur when word spoken by is incorrectly assigned to another speaker role, including cases where the word itself is substituted. In addition to mtWER, we also report the conventional WER and Attribution Error Rate (AER), each computed per speaker role: ERs = IN Ss + DELs + SU Bs REFs , AERs ="
        },
        {
            "title": "AT T Rs\nN REFs",
            "content": ", where denotes the specific speaker role. We also report the Diarization Error Rate (DER), which evaluates speaker diarization accuracy by accounting for missed speech, false alarms, and speaker confusion errors. Formally, DER = + + SC OT AL , where D, A, and SC denote Missed Detection, False Alarm, and Speaker Confusion, respectively. We note that Playlogue [42] reports DER using the total audio duration (including silence) as OT AL, whereas we follow the standard definition using only reference speech. We use Pyannote metrics [46] to compute the DERs. For the final results, we report the mean scores of each of the above metrics. C. Experimental Setup We use single NVIDIA RTX A6000 48GB GPU for all the experiments. We choose Whisper-small (English only) and Whisper-large (v3) models from HuggingFace [47] for our experiments. We perform all analyses and ablation experiments using Whisper-small to enable broad comparisons across training variants. For downstream applications, we use Whisper-large to obtain higher absolute performance. When training for ASR objectives, we apply the Whisper English text normalizer to the texts as pre-processing step. For decoding, we use greedy decoding with repetition penalty of 1.1. Infinite decoding loops are detected by the decoder reaching the maximum token length of 256, in which case the final utterance is discarded. We specify other experimental details for the proposed method and the baselines below. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 7 TABLE II MAIN RESULTS AGAINST BASELINES. THE RESULTS ARE REPORTED IN PERCENTAGES, WITH THE LOWER AND UPPER BOUNDS OF THE CONFIDENCE INTERVAL (CI) BETWEEN 2.75% AND 97.5% FOR EACH RESULT (LOWER BOUND, UPPER BOUND). Dataset Model Method E2E mtWER WER AER DER Playlogue ADOS Whisper-small Whisper-large Whisper-small Whisper-large Baseline (Zero-Shot) Baseline (Diarization-First) Baseline (ASR-First) Proposed Baseline (Zero-Shot) Baseline (Diarization-First) Baseline (ASR-First) Proposed Baseline (Zero-Shot) Baseline (Diarization-First) Baseline (ASR-First) Proposed Baseline (Zero-Shot) Baseline (Diarization-First) Baseline (ASR-First) Proposed 94.4(83.2, 105.7) 45.4(40.0, 50.7) 41.4(36.4, 46.4) 37.4(32.7, 42.1) 91.7(80.3, 103.1) 38.8(34.3, 43.3) 47.5(42.7, 52.3) 34.3(30.0, 38.7) 93.9(83.9, 104.0) 36.1(33.2, 39.0) 33.6(30.1, 37.1) 28.8(25.9, 31.6) 95.8(84.6, 107.0) 29.7(27.0, 32.4) 27.0(24.6, 29.5) 21.7(19.4, 23.9) 89.9(78.7, 101.2) 43.5(38.3, 48.7) 38.0(32.8, 43.1) 35.5(30.8, 40.1) 87.0(75.7, 98.3) 37.2(32.9, 41.6) 41.8(37.1, 46.5) 32.2(28.1, 36.3) 85.0(75.0, 95.0) 35.1(32.2, 37.9) 29.6(26.1, 33.1) 27.8(25.0, 30.6) 86.1(75.0, 97.3) 28.6(26.0, 31.3) 22.4(20.1, 24.7) 20.6(18.3, 22.8) 4.5(3.6, 5.4) 1.9(1.4, 2.3) 3.4(2.5, 4.3) 1.9(1.5, 2.4) 4.7(3.8, 5.6) 1.6(1.2, 1.9) 5.7(4.4, 7.0) 2.1(1.5, 2.7) 8.9(7.5, 10.3) 1.0(0.8, 1.3) 4.1(3.2, 4.9) 1.0(0.7, 1.3) 9.7(8.2, 11.1) 1.0(0.7, 1.3) 4.6(3.6, 5.6) 1.1(0.7, 1.6) 65.4(63.4, 67.4) 35.7(33.2, 38.1) 56.1(51.3, 60.9) 40.6(37.5, 43.8) 64.4(62.4, 66.5) 36.4(34.0, 38.9) 67.4(62.9, 71.9) 42.6(39.7, 45.6) 74.9(71.4, 78.4) 20.4, (19.2, 21.6) 37.7(36.1, 39.3) 21.8(20.4, 23.1) 73.4(70.0, 76.8) 19.2(18.0, 20.5) 36.7(35.1, 38.4) 18.4(17.1, 19.6) 1) Proposed Method: For the diarization head pre-training and fine-tuning, we train for up to 10 epochs with batch size of 16. We use the Adam optimizer with learning rate of 2e4. The best model is chosen based on the validation loss, evaluated at the end of every epoch. We use weight decay of 1e5 during fine-tuning and larger value of 0.01 during pre-training to mitigate overfitting. For the joint-training stage, we also use batch size of 16 and the Adam optimizer, but with learning rates of 5e6 and 1e6 for Whisper-small and Whisper-large models, respectively. We set λdiar = 1 for all experiments. All the parameters from the encoder, decoder, and diarization head are unfrozen. The model checkpoints with the lowest validation loss are selected, evaluated at the end of every epoch. Validation loss is used instead of validation WER, as computing WER is substantially more expensive. 2) Diarization-First Baseline: For the speaker diarization model, we apply LoRA with rank 16 to the feed-forward layers of the transformer, and use the same remaining hyperparameters as in the diarization-head fine-tuning of the proposed joint method. For the ASR models, since utterance-level sampling yields significantly larger number of training instances, we use batch size of 32 and evaluate every 200 steps, rather than at the end of each epoch. Training runs for up to 5 epochs, which we find sufficient for convergence. All other hyperparameters match those used during joint training in the proposed method. 3) ASR-First Baseline: For training ASR with SOT, we use the same hyperparameters as those used for the joint training stage for the proposed method, with both the encoder and decoder unfrozen. For the forced alignment with Nvidia Nemo, we use the default hyperparameters with the stt en fastconformer hybrid large pc model. VI. MAIN RESULTS Table II summarizes the ASR and Diarization performances of all systems across the Playlogue and ADOS datasets, along with the 95% confidence intervals to show the statistical reliability and variability of the reported metrics. A. Baseline Results The zero-shot results show very high ASR and diarization error rates, underscoring the need for domain-specific finetuning. Comparing the Diarization-First and ASR-First finetuned baselines reveals clear trade-off in the cascaded systems. Diarization-First Baseline segments audio using dedicated diarization model before transcription, yielding more accurate speaker boundaries. However, its ASR performance is limited by error propagation, as segmentation mistakes degrade the subsequent transcription. In contrast, ASR-First Baseline benefits from applying ASR first, achieving lower WER in most setups. Yet its AER is noticeably higher, reflecting the difficulty of naively relying on SOT-based ASR outputs for speaker-role detection. In addition, the forced alignment step is limited by the quality of the ASR hypotheses. Transcription inaccuracies propagate into the alignment process, resulting in less reliable timestamp predictions and much higher DER. B. Proposed Method mtWER Overall, the proposed joint ASR and Speaker Role Diarization framework shows consistent mtWER reductions over the cascaded baselines across both Whisper-small and Whisperlarge configurations, achieving approximately 9.7 19.6% relative reductions in mtWER. Compared to the DiarizationFirst Baseline, the joint approach avoids the error propagation introduced by external segmentation, where boundary mistakes directly disrupt downstream transcription. Relative to ASRFirst Baseline, the model benefits from explicit frame-level speaker-role supervision, which mitigates the speaker-token errors in SOT-ASR outputs, as observed by AER. Notably, the joint model also improves WER compared to the ASRFirst Baseline. We attribute this to the timestamp-prediction mechanism and joint optimization, which together produce JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8 TABLE III INDIVIDUAL MTWER RESULTS FOR CHILD AND ADULT. THE PARENTHESIS () SHOWS THE ABSOLUTE IMPROVEMENTS OVER THE BEST BASELINE RESULT. RESULTS ARE SHOWN IN PERCENTAGE(%). Dataset Model Child Adult Playlogue Whisper-small Whisper-large 53.1( 4.1) 48.4( 5.3) 21.7( 3.9) 19.8( 4.1) ADOS Whisper-small Whisper-large 45.0( 3.9) 35.0( 5.2) 12.5( 5.9) 8.4( 5.5) TABLE IV CHILD MTWER FROM ADOS BASED ON THE AGE AND ADOS CSS SCORES. RESULTS ARE SHOWN IN PERCENTAGE(%). Demographics Session Count mtWER low CSS & 8 years low CSS & > 8 years high CSS & 8 years high CSS & > 8 years 40 22 20 43.7 43.1 48.8 49.4 more temporally aligned and coherent encoder representations, ultimately leading to more robust decoder performance. Overall, jointly modeling ASR and speaker roles provides stronger inductive bias for producing coherent speaker-labeled transcripts, thereby improving both WER and the speakerattribution component within mtWER. C. Proposed Method DER The proposed joint method consistently outperforms ASRFirst Baseline in DER, as it avoids the forced-alignment stage where transcription errors propagate into boundary predictions. However, the DER trend becomes more nuanced when compared to the Diarization-First Baseline. On the Playlogue dataset, the joint model yields higher DER than the cascaded Diarization-First Baseline for both Whispersmall and Whisper-large. This outcome is expected given the challenging recording conditions, where environmental noise and annotation inconsistencies make timestamp token decoding particularly sensitive to boundary errors, whereas the dedicated diarization model in Diarization-First Baseline remains more robust. In contrast, on the ADOS dataset, the DER gap between the joint model and Diarization-First Baseline is substantially smaller for Whisper-small, and the joint model even surpasses Diarization-First Baseline when using Whisperlarge. ADOS recordings are cleaner and more structured, reducing the ambiguity inherent in timestamp prediction. Under the shared encoder representation in the these conditions, joint framework becomes advantageous, enabling the model to match the diarization performance of separate, specialized diarization system. D. How difficult is child ASR compared to adult? Table III presents comparison of the mtWER between children and adults. Child speech shows markedly higher mtWER on both Playlogue and ADOS, highlighting the persistent modeling challenges posed by childrens less stable articulation patterns and the fact that Whisper is likely primarily trained on adult speech. Adult speech, by contrast, is recognized with much lower error rates. For both children and adults, Whisperlarge consistently outperforms Whisper-small, indicating that the larger model is better at transcribing both child and adult speech. Overall, we see that our method consistently improves mtWERs by around 4 6% absolute reductions over the best baseline results for both child and adult speech across different models and different datasets. Table IV reports child mtWER broken down by demographic characteristics. Children are divided into four groups based on age (8 years and younger/older than 8 years) and the ADOS Calibrated Severity Score (CSS), standardized 10-point continuous metric that reflects the severity of autismrelated behaviors observed during the ADOS assessment. CSS scores of 6 or higher are typically associated with severe autism behaviors, whereas scores of 3 or lower indicate few atypical behaviors. Accordingly, we partition CSS into low (13) and high (610) severity groups. Across both age groups, children with high CSS consistently exhibit higher mtWER, whereas mtWER remains comparable within each age group at the same severity level. These results suggest that increased autism symptom severity is primary factor contributing to recognition difficulty, reflecting more challenging speech characteristics for the model. E. Summary of Main Findings The joint model provides the best overall performance across transcription and speaker-role metrics. It consistently improves mtWER relative to both cascaded baselines and delivers strong diarization accuracysubstantially outperforming the ASR-First Baseline and approaching the DiarizationFirst Baseline, depending on the dataset conditions. These results highlight the advantage of integrating ASR and diarization within single framework, reducing error propagation and producing more coherent, speaker-attributed transcripts. VII. ANALYSIS To understand how timestamp supervision and diarization supervision reshape the acoustic representation space, we compare three configurations in the following analyses: (1) Without Timestamp (Without T), (2) With Timestamp (With T), and (3) With Timestamp and Diarization Head (With and DH). Note that the Without configuration is identical to the ASR-SOT used for the ASR-First Baseline. A. k-NN Classification Accuracy To examine how the different training configurations shape the encoders speaker-related representations, we evaluate childadult separability using standard k-nearest neighbor (k-NN) classification probe on the encoder embeddings from all test files, averaged for each utterance. Fig. 5 summarizes the k-NN accuracies for the three configurations with = 5 and cosine distance. Overall, the results reveal consistent trend across both datasets. Introducing timestamp prediction slightly reduces k-NN accuracy in both datasets, suggesting JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 TABLE VI ABLATION ON DIARIZATION HEAD WITH WHISPER-SMALL USING PLAYLOGUE DATA. NONE MEANS NO DIARIZATION HEAD IS ATTACHED, AND SIL-SUPPR MEANS SILENCE SUPPRESSION. Diarization Head mtWER WER AER None Random Initialization Pretrained Pretrained + Sil-suppr 40.7 37.9 38.3 37.8 37.4 36.3 35.8 35.5 2.8 2.0 2.0 1.9 DER 43.3 42.3 41.4 40.6 p<.05, p<.01: significantly lower than None by one-sided paird t-test TABLE VII ABLATION ON DIARIZATION HEAD WITH WHISPER-SMALL USING ADOS DATA. NONE MEANS NO DIARIZATION HEAD IS ATTACHED, AND SIL-SUPPR MEANS SILENCE SUPPRESSION. Diarization Head mtWER WER AER None 30.0 28.2 1. Random Initialization Pretrained Pretrained + Sil-suppr 29.9 29.3 28.8 28.1 28.3 27.8 1.8 1.1 1.0 DER 23. 25.5 23.6 21.8 p<.05, p<.01: significantly lower than None by one-sided paird t-test VIII. ABLATION A. Ablation on Diarization Head Fig. 5. Child vs adult kNN classification on utterance-level encoder outputs. and DH denote Timestamp and Diarization Head, respectively. TABLE SPEAKER CLASSIFICATION LINEAR PROBING FOR EACH 0.1S ON 0.3S AROUND THE SPEECH BOUNDARIES. AND DH DENOTE TIMESTAMP AND DIARIZATION HEAD. RESULTS SHOWN IN PERCENTAGE (%) Dataset Config Playlogue ADOS w/o w/ w/ & DH w/o w/ w/ & DH F1 Child / Adult / Silence / Macro 70.68 / 60.97 / 72.68 / 68.11 70.83 / 60.98 / 73.19 / 68.33 72.74 / 63.46 / 74.20 / 70.13 81.93 / 76.36 / 76.64 / 79.27 82.35 / 76.74 / 79.60 / 79.56 85.13 / 78.94 / 80.09 / 81.38 Acc 69.10 69.37 70.98 79.40 79.66 81. that the added temporal supervision introduces variability in the encoder space that is not directly aligned with speaker identity. In contrast, adding the diarization head restores speaker-discriminative information, yielding the highest kNN accuracy on both datasets and leading to representation space that is both temporally organized and speakerinformative. Together, these results highlight complementary relationship between timestamp-guided temporal alignment and diarization-driven speaker discrimination. B. Linear Probing for Encoder Temporal Alignment To assess how timestamp and diarization supervision influence encoder representations, particularly their temporal alignment with the input, we perform linear probing with logistic regression on 0.3-second windows centered on annotated speech boundaries for child, adult, and silence labels. We compute the average embeddings over 5 frames (0.1 seconds) at each boundary, using 200 randomly sampled audio segments (up to 30 seconds) for both the training and test sets. As shown in Table V, adding timestamp prediction consistently boosts the silence F1 score, indicating that the encoder becomes more aligned to the actual speech activity present in the audio at the corresponding temporal locations. Incorporating the diarization head yields further improvements across all classes, with especially notable gains for child and adult F1 scores. These results suggest that timestamp supervision enhances boundary-level temporal alignment in the encoder, while diarization supervision further strengthens speaker-role discrimination at those same transitions. Tables and II report the effect of the diarization head on Playlogue and ADOS. On Playlogue, jointly training diarization head from scratch slightly improves MtWER, WER, and AER significantly relative to the ASR-only baseline, showing that adding the diarization objective does not harm recognition performance. However, DER is not significantly reduced, indicating that the non-pretrained diarization head provides limited diarization benefit. In contrast, the pretrained head improves all metrics, lowering mtWER and DER. With silence suppression, the improvements are largest, achieving the best mtWER, WER, and DER, and the lowest AER. On ADOS, the diarization head without pre-training worsens DER. We found that when the diarization head is randomly initialized and trained on limited childadult speech data, its optimization lags behind that of the ASR component, leading to incomplete convergence. As diarization head optimization is slower and incomplete without pretraining, noisy gradients from the randomly initialized head can lead to worse encoder speaker representations. In contrast, pretraining the diarization head yields significant reduction in AER. The best results again come from pretraining the diarization head and using the silence suppression, yielding the lowest mtWER, AER, and DER. Overall, these results demonstrate that joint ASR-diarization training, when combined with pretrained diarization head initialization, maintains the WER comparable to or better than the ASR-only baseline, while substantially reducing AER. Decoder-level silence suppression further enhances this effect by producing cleaner speaker boundaries, resulting in the best mtWER and DER across both datasets. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10 TABLE VIII ERROR ANALYSIS WITH AND WITHOUT FORCED DECODING (F.D.), WITH ERROR PERCENTAGE PER SINGLE DECODING. TABLE IX COMPARISON BETWEEN GROUND-TRUTH (G.T.) AND PREDICTED (PRED) SPEECH METRICS ON THE PLAYLOGUE TEST DATASET (N=34). Error Type Playlogue Ados w/o F.D. w/ F.D. w/o F.D. w/ F.D. Miss Speaker Miss Timestamp Miss Both 0.5% 13.1% 4.2% 0% 0% 0% Infinite Loop 2.5% 4.2% 0.4% 16.2% 30.4% 12.1% 0% 0% 0% 0.6% B. Ablation on State-Machine-Based Forced Decoding To better understand the effect of the proposed forced decoding mechanism, we analyze the major decoding failures observed in SOT-style generation: (1) missing speaker tokens, (2) missing timestamp tokens, (3) missing both speaker and timestamp tokens, and (4) infinite decoding loops. Table VIII summarizes the percentage of utterances affected by each error type on both Playlogue and ADOS datasets, with and without forced decoding (F.D.). Without forced decoding, missing tokens are the dominant error source, especially on ADOS, where more than 46% of utterances lack timestamps or speaker tags, leading to severe alignment failures and unusable outputs. Similar trends appear on Playlogue, where 17.8% of utterances lose required structural tokens (speaker and timestamp), indicating that unconstrained generation struggles to reliably emit the required SOT format. In contrast, forced decoding eliminates all missing-token errors on both datasets. The model is explicitly constrained to output valid structural tokens, and every utterance remains properly segmented and attributed to its corresponding speaker. As result, forced decoding converts previously invalid transcripts into fully decodable SOT outputs, directly improving mtWER and timestamp-based diarization alignment. The only remaining failure mode with forced decoding is the occasional occurrence of infinite loops. On Playlogue, the loop rate increases slightly from 2.5% to 4.2%, likely because strict token enforcement may cause the decoder to enter repetitive states when the confidence is low. However, on ADOS, loop errors almost disappear, decreasing from 12.1% to 0.6%, suggesting that forced decoding largely stabilizes speech decoding under more controlled and cleaner conditions. Overall, this analysis demonstrates that the proposed forced decoding mechanism significantly enhances structural reliability, ensuring that the output always contains speaker and timestamp tokens. This reliability is crucial for downstream segmentation and role attribution, and it directly leads to more stable diarization-aware ASR performance across datasets. IX. APPLICATIONS A. Automatically Derived Conversational Speech Metrics We analyze automatically predicted speech metrics that can be directly derived from the model-generated transcripts. For each child, we compute five speech measures grouped into three functional categories: (1) words per minute and (2) utterances per minute, which together capture overall Metric G.T. Mean Pred Mean PCC Speech Quantity Words per minute Utterances per minute Utterance Length Mean words per utterance Mean utterance duration (s) Fluency Speaking rate (words/s) 42.64 14.51 2.89 0.80 41.44 11. 3.52 1.15 .975 .923 .857 .531 217.0 184.11 . TABLE COMPARISON BETWEEN GROUND-TRUTH (G.T.) AND PREDICTED (PRED) SPEECH METRICS ON THE ADOS TEST DATASET (N=84). Metric G.T. Mean Pred Mean PCC Speech Quantity Words per minute Utterances per minute Utterance Length Mean words per utterance Mean utterance duration (s) Fluency Speaking rate (words/s) 48.68 9.44 5.10 1.84 50.63 9.31 5.37 1.95 .971 .831 .890 . 163.58 163.08 .859 speech speech quantity; (3) mean duration per utterance and (4) mean words per utterance, reflecting overall utterance length; and (5) speaking rate, capturing speech fluency. For the ADOS data, we merge the two sessions (Social Difficulties and Annoyance and Emotional) for the analysis. The test set consists of 84 children from MICH, all of whom completed both sessions. For the Playlogue dataset, we also evaluate on the test split at the child level, which includes 34 children. Tables IX and summarize the agreement between predicted and ground-truth speech metrics for the Playlogue and ADOS datasets, respectively. For each measure, we report the mean value across children for ground-truth (G.T.) and predicted (Pred) sources, along with the Pearson correlation (PCC) computed across children. The predicted metrics closely approximate the ground-truth values across all measures, with PCC ranging from 0.53 to 0.98 for the Playlogue dataset and 0.70 to 0.97 for the ADOS dataset. Strong agreements with the ground truth are observed in the words per minute and mean words per utterance metrics across both datasets, indicating that the model accurately captures the number of words produced by each child. Mean utterance duration and speaking rate metrics show lower correlations, likely because the errors are further compounded by diarization timestamp errors. Nevertheless, the overall results confirm that the system robustly recovers child spoken language characteristics, which can be helpful for downstream clinical analysis. B. Discussion on Clinical Applicability The proposed joint ASR and speaker-role diarization framework enables scalable analysis of childadult interactions by directly producing structured, speaker-attributed transcripts with temporal boundaries, eliminating the need for manual JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11 segmentation. This substantially reduces the annotation burden and facilitates large-scale, longitudinal analyses of childrens expressive language and conversational behavior. In the context of child ASD research, prior studies [48][51] have demonstrated that speechand transcript-derived features are informative for modeling ASD-related behavioral variability. The proposed framework provides practical foundation for extracting such features at scale. Future work can leverage the automatically generated speakerand timestamp-tagged transcripts to develop clinically meaningful interactional and linguistic measures, and systematically connect these measures to clinician-annotated outcomes. While not substitute for clinician-administered assessments, the proposed end-to-end joint modeling offers promising pathway toward embedding automated speech and language analytics into clinical and research workflows involving childadult interactions. X. CONCLUSION This paper has introduced unified Whisper-based framework that jointly performs ASR and childadult speaker-role diarization within single end-to-end model. By combining serialized output training, timestamp prediction, pretrained diarization head, silence-guided decoding, and state-machine constraint, the system produces structurally reliable, speakerattributed transcripts with accurate temporal boundaries. Experiments on Playlogue and ADOS-Mod3 show consistent improvements in mtWER and strong diarization accuracy compared with cascaded baselines. The model-derived conversational metrics closely align with those derived from human annotations. These results demonstrate the practicality and scalability of the proposed framework for automated analysis of childadult interactions and related clinical research."
        },
        {
            "title": "This work was",
            "content": "supported by SIMONS FOUNDATION (SFI-AR-HUMAN-00004115-03, 655054). We also thank Huang-Cheng Chou for insightful discussions and feedback."
        },
        {
            "title": "REFERENCES",
            "content": "[1] M. Barokova and H. Tager-Flusberg, Commentary: Measuring language language samples, Journal of autism and change through natural developmental disorders, vol. 50, no. 7, pp. 22872306, 2020. [2] A. Xu, R. Hebbar, R. Lahiri, T. Feng, L. Butler, L. Shen, H. TagerFlusberg, and S. Narayanan, Understanding spoken language development of children with asd using pre-trained speech embeddings, Interspeech, 2023. [3] H. Tager-Flusberg, S. Rogers, J. Cooper, R. Landa, C. Lord, R. Paul, M. Rice, C. Stoel-Gammon, A. Wetherby, and P. Yoder, Defining spoken language benchmarks and selecting measures of expressive language development for young children with autism spectrum disorders, Journal of Speech, Language, and Hearing Research, vol. 52, no. 3, pp. 643652, 2009. [4] R. Prabhavalkar, T. Hori, T. N. Sainath, R. Schluter, and S. Watanabe, End-to-end speech recognition: survey, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 325351, 2023. [5] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan, review of speaker diarization: Recent advances with deep learning, Computer Speech & Language, vol. 72, p. 101317, 2022. [6] S. Lee, A. Potamianos, and S. Narayanan, Acoustics of childrens speech: Developmental changes of temporal and spectral parameters, The Journal of the Acoustical Society of America, vol. 105, no. 3, pp. 14551468, 1999. [7] A. Potamianos and S. Narayanan, Robust recognition of childrens speech, IEEE Transactions on speech and audio processing, vol. 11, no. 6, pp. 603616, 2004. [8] S. Lee, A. Potamianos, and S. Narayanan, Developmental acoustic study of american english diphthongs, The Journal of the Acoustical Society of America, vol. 136, no. 4, pp. 18801894, 2014. [9] J. Li, M. Lavechin, X. Fan, N. L. McElwain, A. Cristia, P. Garcia-Perera, and M. Hasegawa-Johnson, Automated analysis of naturalistic recordings in early childhood: Applications, challenges, and opportunities, arXiv preprint arXiv:2509.18235, 2025. [10] R. Fan, N. B. Shankar, and A. Alwan, Benchmarking childrens asr with supervised and self-supervised speech foundation models, Interspeech, 2024. [11] A. Xu, T. Feng, S. H. Kim, S. Bishop, C. Lord, and S. Narayanan, Large language models based asr error correction for child conversations, Interspeech, 2025. [12] L. Shen, A. Xu, L. K. Butler, K. Chenausky, M. Maffei, S. Narayanan, and H. Tager-Flusberg, Conversational latency in autistic children with heterogeneous spoken language abilities, Journal of Speech, Language, and Hearing Research, vol. 68, no. 5, pp. 23862398, 2025. [13] A. Sun, T. Feng, G. Gutierrez, J. J. Londono, A. Xu, B. Elbaum, S. Narayanan, L. K. Perry, and D. S. Messinger, Who said what wsw 2.0? enhanced automated analysis of preschool classroom speech, arXiv preprint arXiv:2505.09972, 2025. [14] B. Long, R. Z. Sparks, V. Xiang, S. Stojanov, Z. Yin, G. E. Keene, A. W. Tan, S. Y. Feng, C. Zhuang, V. A. Marchman et al., The babyview dataset: High-resolution egocentric videos of infants and young childrens everyday experiences, arXiv preprint arXiv:2406.10447, 2024. [15] B. Kim, A. Ghosh, M. Fuhs, A. Chowdhury, D. Bagchi, and M. Woszczyna, hybrid approach to combining role diarization with asr for professional conversations, in Interspeech, 2025, pp. 52435247. [16] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, Xvectors: Robust dnn embeddings for speaker recognition, in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 53295333. [17] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, End-to-end neural speaker diarization with self-attention, in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 296303. [18] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Nagamatsu, Endto-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors, arXiv preprint arXiv:2005.09921, 2020. [19] A. Xu, K. Huang, T. Feng, L. Shen, H. Tager-Flusberg, and S. Narayanan, Exploring speech foundation models for speaker diarization in child-adult dyadic interactions, Interspeech, 2024. [20] J. Li, M. Hasegawa-Johnson, and N. L. McElwain, Towards robust family-infant audio analysis based on unsupervised pretraining of wav2vec 2.0 on large-scale unlabeled family audio, arXiv preprint arXiv:2305.12530, 2023. [21] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, in International conference on machine learning. PMLR, 2023, pp. 28 49228 518. [22] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., Wavlm: Large-scale self-supervised pretraining for full stack speech processing, IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 15051518, 2022. [23] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu, A. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi et al., Scaling speech technology to 1,000+ languages, Journal of Machine Learning Research, vol. 25, no. 97, pp. 152, 2024. [24] A. Xu, T. Feng, H. Tager-Flusberg, C. Lord, and S. Narayanan, Data efficient child-adult speaker diarization with simulated conversations, in ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025, pp. 15. [25] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj et al., Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings, arXiv preprint arXiv:2004.09249, 2020. [26] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, J. Du, T. Yoshioka, Y. Luo et al., Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis, in 2021 IEEE spoken language technology workshop (SLT). IEEE, 2021, pp. 897904. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12 [48] M. Kumar, R. Gupta, D. Bone, N. Malandrakis, S. Bishop, and S. S. Narayanan, Objective language feature analysis in children with neurodevelopmental disorders during autism assessment. in Interspeech, 2016, pp. 27212725. [49] M. Eni, I. Dinstein, M. Ilan, I. Menashe, G. Meiri, and Y. Zigel, Estimating autism severity in young children from speech signals using deep neural network, IEEE Access, vol. 8, pp. 139 489139 500, 2020. [50] A. Mohanta and V. K. Mittal, Analysis and classification of speech sounds of children with autism spectrum disorder using acoustic features, Computer Speech & Language, vol. 72, p. 101287, 2022. [51] R. Assaf, Z. Shehabeddine, and V. Ramesh, Screening autism spectrum disorder in children using machine learning on speech transcripts, Scientific Reports, vol. 15, no. 1, p. 34134, 2025. [27] L. E. Shafey, H. Soltau, and I. Shafran, Joint speech recognition and speaker diarization via sequence transduction, arXiv preprint arXiv:1907.05337, 2019. [28] N. Kanda, Y. Gaur, X. Wang, Z. Meng, and T. Yoshioka, Serialized output training for end-to-end overlapped speech recognition, arXiv preprint arXiv:2003.12687, 2020. [29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [30] N. Kanda, J. Wu, Y. Wu, X. Xiao, Z. Meng, X. Wang, Y. Gaur, Z. Chen, J. Li, and T. Yoshioka, Streaming multi-talker asr with token-level serialized output training, arXiv preprint arXiv:2202.00842, 2022. [31] Y. Liang, F. Yu, Y. Li, P. Guo, S. Zhang, Q. Chen, and L. Xie, Batraining for multi-talker asr, sot: Boundary-aware serialized output Interspeech, 2023. [32] Z. Fan, L. Dong, J. Zhang, L. Lu, and Z. Ma, Sa-sot: Speakeraware serialized output training for multi-talker asr, in ICASSP 20242024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 99869990. [33] N. Kanda, X. Xiao, Y. Gaur, X. Wang, Z. Meng, Z. Chen, and T. Yoshioka, Transcribe-to-diarize: Neural speaker diarization for unlimited number of speakers using end-to-end speaker-attributed asr, in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 80828086. [34] T. Park, I. Medennikov, K. Dhawan, W. Wang, H. Huang, N. R. Koluguri, K. C. Puvvada, J. Balam, and B. Ginsburg, Sortformer: Seamless integration of speaker diarization and asr by bridging timestamps and tokens, arXiv preprint arXiv:2409.06656, 2024. [35] M. Shi, X. Xiao, R. Fan, S. Ling, and J. Li, Train short, infer long: Speech-llm enables zero-shot streamable joint asr and diarization on long audio, arXiv preprint arXiv:2511.16046, 2025. [36] S. Cornell, J.-w. Jung, S. Watanabe, and S. Squartini, One model to rule them all? towards end-to-end joint speaker diarization and speech recognition, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 11 85611 860. [37] N. Makishima, N. Kawata, M. Ihori, T. Tanaka, S. Orihashi, A. Ando, and R. Masumura, Somsred: Sequential output modeling for joint multi-talker overlapped speech recognition and speaker diarization, in Interspeech, vol. 2024, 2024, pp. 16601664. [38] M. Bain, J. Huh, T. Han, and A. Zisserman, Whisperx: Time-accurate speech transcription of long-form audio, Interspeech, 2023. [39] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, wav2vec 2.0: framework for self-supervised learning of speech representations, Advances in neural information processing systems, vol. 33, pp. 12 449 12 460, 2020. [40] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., Lora: Low-rank adaptation of large language models. ICLR, vol. 1, no. 2, p. 3, 2022. [41] E. Rastorgueva, V. Lavrukhin, and B. Ginsburg, Nemo forced aligner and its application to word alignment for subtitle generation, in Interspeech, 2023. [42] M. Kalanadhabhatta, M. M. Rastikerdar, T. Rahman, A. S. Grabell, and D. Ganesan, Playlogue: Dataset and benchmarks for analyzing adultchild conversations during play, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 8, no. 4, pp. 134, 2024. [43] S. L. Bishop, K. A. Havdahl, M. Huerta, and C. Lord, Subdimensions of social-communication impairment in autism spectrum disorder, Journal of Child Psychology and Psychiatry, vol. 57, no. 8, pp. 909916, 2016. [44] C. Lord, S. Risi, L. Lambrecht, E. H. Cook Jr, B. L. Leventhal, P. C. DiLavore, A. Pickles, and M. Rutter, The autism diagnostic observation schedulegeneric: standard measure of social and communication deficits associated with the spectrum of autism, Journal of autism and developmental disorders, vol. 30, no. 3, pp. 205223, 2000. [45] K. Zmolikova, S. Merello, K. Kalgaonkar, J. Lin, N. Moritz, P. Ma, M. Sun, H. Chen, A. Saliou, S. Petridis et al., The chime-8 mmcsg challenge: Multi-modal conversations in smart glasses, in 8th International Workshop on Speech Processing in Everyday Environments (CHiME), 2024, pp. 712. [46] H. Bredin, pyannote. metrics: toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems. in Interspeech, 2017, pp. 35873591. [47] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., Huggingfaces transformers: State-of-the-art natural language processing, arXiv preprint arXiv:1910.03771, 2019."
        }
    ],
    "affiliations": [
        "David Geffen School of Medicine, University of California, Los Angeles, US",
        "Viterbi School of Engineering, University of Southern California, US",
        "Weill Institute for Neurosciences, University of California, San Francisco, US"
    ]
}