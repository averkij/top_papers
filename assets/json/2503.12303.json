{
    "paper_title": "Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs",
    "authors": [
        "Xiaoying Zhang",
        "Da Peng",
        "Yipeng Zhang",
        "Zonghao Guo",
        "Chengyue Wu",
        "Chi Chen",
        "Wei Ke",
        "Helen Meng",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) face challenges with fine-grained perception and complex reasoning. Prevalent multimodal pre-training approaches focus on enhancing perception by training on high-quality image captions due to the extremely high cost of collecting chain-of-thought (CoT) reasoning data for improving reasoning. While leveraging advanced MLLMs for caption generation enhances scalability, the outputs often lack comprehensiveness and accuracy. In this paper, we introduce Self-Improving cognition (SIcog), a self-learning framework designed to construct next-generation foundation MLLMs by enhancing their systematic cognitive capabilities through multimodal pre-training with self-generated data. Specifically, we propose Chain-of-Description, an approach that improves an MLLM's systematic perception by enabling step-by-step visual understanding, ensuring greater comprehensiveness and accuracy. Additionally, we adopt a structured CoT reasoning technique to enable MLLMs to integrate in-depth multimodal reasoning. To construct a next-generation foundation MLLM with self-improved cognition, SIcog first equips an MLLM with systematic perception and reasoning abilities using minimal external annotations. The enhanced models then generate detailed captions and CoT reasoning data, which are further curated through self-consistency. This curated data is ultimately used for multimodal pre-training to develop next-generation foundation models. Extensive experiments on both low- and high-resolution MLLMs across diverse benchmarks demonstrate that, with merely 213K self-generated pre-training samples, SIcog produces next-generation foundation MLLMs with significantly improved cognition, achieving benchmark-leading performance compared to prevalent pre-training approaches."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 2 3 0 3 2 1 . 3 0 5 2 : r Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs Xiaoying Zhang1, Da Peng2, Yipeng Zhang3, Zonghao Guo3, Chengyue Wu4 Chi Chen3, Wei Ke2, Helen Meng1, Maosong Sun3 1The Chinese University of Hong Kong 2Xian Jiaotong University 3Tsinghua University 4The University of Hong Kong {zhangxy, hmmeng}@se.cuhk.edu.hk, guozonghao96@outlook.com https://github.com/thunlp/SICOG"
        },
        {
            "title": "Abstract",
            "content": "elf-I mproving Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) face challenges with fine-grained perception and complex reasoning. Prevalent multimodal pre-training approaches focus on enhancing perception by training on high-quality image captions due to the extremely high cost of collecting chain-of-thought (CoT) reasoning data for improving reasoning. While leveraging advanced MLLMs for caption generation enhances scalability, the outputs often lack comprehensiveness and accuracy. In this paper, we introduce nition (SICOG), self-learning framework designed to construct next-generation foundation MLLMs by enhancing their systematic cognitive capabilities through multimodal pre-training with self-generated data. Specifically, we propose Chain-of-Description, an approach that improves an MLLMs systematic perception by enabling step-by-step visual understanding, ensuring greater comprehensiveness and accuracy. Additionally, we adopt structured CoT reasoning technique to enable MLLMs to integrate in-depth multimodal reasoning. To construct next-generation foundation MLLM with self-improved cognition, SICOG first equips an MLLM with systematic perception and reasoning abilities using minimal external annotations. The enhanced models then generate detailed captions and CoT reasoning data, which are further curated through selfconsistency. This curated data is ultimately used for multimodal pre-training to develop next-generation foundation models. Extensive experiments on both lowand high-resolution MLLMs across diverse benchmarks demonstrate that, with merely 213K self-generated pre-training samples, SICOG produces next-generation foundation MLLMs with significantly improved cognition, achieving benchmarkleading performance compared to prevalent pre-training approaches. 1 introduction Recent advances in Multimodal Large Language Models (MLLMs) [1, 2, 3, 4, 5] have achieved remarkable progress in both multimodal perception [6, 7] and reasoning [8, 9], marking significant step towards real-world understanding [10]. However, two critical limitations persist: (i) inadequate fine-grained perception, particularly in detecting detailed object properties and spatial relationships [7, 11], and (ii) limited complex reasoning capabilities, especially in tasks requiring advanced logical analysis and problem-solving [8, 9, 12]. Addressing these limitations is essential for broadening MLLMs practical applications [10, 13]. Project lead, core contributor. Corresponding authors. Figure 1: (a) The SICOG framework enhances an MLLMs systematic cognition during multimodal pre-training using self-generated data, enabling the construction of next-generation foundation MLLM. Specifically, SICOG first improves perception and reasoning through Chain-of-Description and structured chain-of-thought with minimal external annotations. The enhanced model then generates captions and VQA responses, refined via self-consistency for multimodal pre-training to construct foundation MLLM from scratch. (b) With up to 213K self-generated pre-training samples, SICOG produces foundation MLLMs with superior cognitive capabilities, demonstrating benchmark-leading performance compared to prevalent pre-training approaches. Introducing multimodal pre-training into foundation MLLM construction [3, 7, 14, 15] has significantly advanced MLLMs by addressing these limitations, thereby establishing stronger foundation for developing advanced multimodal capabilities in subsequent instruction-tuning. Specifically: (i) Pre-training on high-quality, detailed image captions [2, 3, 16, 17, 18] enhances MLLMs perception by integrating fine-grained visual details and contextual world knowledge. (ii) Pre-training with high-quality multimodal chain-of-thought (CoT) reasoning data [19, 20] enables MLLMs to generate intermediate reasoning steps before final answers, thereby improving multimodal reasoning [21]. Despite their effectiveness, these approaches face significant scalability challenges. Collecting high-quality image-captioning data is labor-intensive and costly [7, 14, 16], while CoT reasoning data incurs even higher expenses [21]. Consequently, prevalent pre-training approaches [7, 14, 15, 16, 22] prioritize extensive pre-training on image-captioning data. To improve scalability, they typically leverage advanced proprietary or open-source multimodal MLLMs [6, 23, 24] to generate detailed captions [9, 16]. However, their outputs often lack comprehensiveness and accuracy. For example, they frequently omit crucial peripheral details and exhibit hallucinations (i.e., inaccurate descriptions), as shown in Table 7 in Section 5. We attribute these shortcomings to the absence of systematic framework for generating detailed and comprehensive captions. While enriching captions with diverse fine-grained attributes (e.g., size, depth, OCR, etc.) annotated by multiple off-the-shelf experts [7, 15, 22] improves coverage, these captions often lack fluency due to the absence of clear logical structure. In this paper, we introduce SICOG, self-learning framework designed to enhance MLLMs systematic cognitive abilitiesboth perception and reasoningduring multimodal pre-training with self-generated data for next-generation foundation MLLM construction. Specifically, we propose Chain-of-Description, which enables an MLLM to systematically acquire and interpret visual information through logical, step-by-step analysis, organizing these observations into cohesive, detailed description to ensure comprehensiveness and mitigate hallucinations. Additionally, we incorporate structured CoT reasoning [9] to reinforce systematic reasoning by effectively integrating multimodal information and facilitating in-depth analysis. As illustrated in Figure 1, SICOG first develops MLLMs systematic perception and reasoning abilities with minimal external annotations. This is achieved by fine-tuning the model on small set of high-quality caption data featuring our proposed Chain-of-Description and limited amount of structured CoT reasoning data [9]. Subsequently, the fine-tuned models generate candidate captions and responses, which are curated using selfconsistency [25, 26]. Higher-quality candidates are identified based on semantic coherence. Finally, these self-generated data are used for multimodal pre-training, enabling cognitive self-improvement. This process allows an MLLM to acquire knowledge and understanding through both perception and reasoning, mirroring human cognitive processes [27], ultimately resulting in stronger foundation MLLM. We validate the efficacy of SICOG on both low-resolution and high-resolution models across eleven well-studied benchmarks. Extensive experimental results show that SICOG significantly enhances MLLMs multimodal cognition, leading to stronger foundation MLLM. For instance, it achieves an approximate 24% accuracy gain on MMStar over the base MLLM, notably surpassing prevalent pretraining approaches [14, 22]. In addition, in-depth quantitative and qualitative analyses confirm that Chain-of-Description effectively strengthens systematic perception, producing more comprehensive and accurate image captions. In summary, our contributions are three-fold: We propose SICOG, self-learning framework that enhances MLLMs systematic cognition for constructing next-generation foundation MLLMs through multimodal pre-training with self-generated data. We design Chain-of-Description to enhance an MLLMs perception by enabling systematic, step-by-step interpretation of visual information. We demonstrate SICOGs effectiveness across eleven benchmarks on both lowand highresolution MLLMs, significantly surpassing prevalent pre-training approaches. We release all related data, code, and models to support the research community."
        },
        {
            "title": "2 Related Work",
            "content": "Method w/o External Annotation Caption Type VQA Type Detailed DD-FGA CoD Direct CoT Detailed (Re-)Captioning (Perception) ALLaVA [16], LLaVA-NeXT [14] DCE [7], MMGiC [15] () VILA2 [22] Detailed Re-Captioning & Visual Instruction Tuning (Perception & Reasoning) SICOG (Ours) Table 1: Comparison of multimodal (vision-language) pre-training methods for enhancing multimodal capabilities. For VILA2, () indicates hybrid approach combining bootstrapped captions with fine-grained attributes from expert models. Detailed (Detailed Description), DD-FGA (Detailed Description with Fine-Grained Attributes, e.g., size, depth, OCR, ...), CoD (Chain-of-Description), Direct (Direct Answer), CoT (Chain-of-Thought). Improving multimodal perception abilities of MLLMs. Although MLLMs demonstrate strong multimodal perception capabilities [1, 3], they often struggle with fine-grained tasks such as OCR [4, 17, 28, 29, 30, 31]. These challenges arise primarily from the reliance on popular large-scale caption datasets (i.e., image-text pairs) [32, 33, 34] for modality alignment, which often contain short, coarsegrained captions, restricting their ability to extract detailed visual information [16, 35, 36]. One common solution is additional pre-training with high-quality, detailed captions [2, 3, 16, 17, 18] or captions enriched with fine-grained attributes [7, 15, 22], improving their ability to capture visual details. In contrast, we propose Chain-of-Description, which explicitly models the perception process. This approach trains models to systematically acquire and interpret visual information through stepby-step analysis and decomposition of complex scenes, enabling deeper understanding of fine-grained details. Improving multimodal reasoning abilities of MLLMs. Complex multimodal reasoning tasks that require integrating visual information into reasoning processes, such as mathematical computation, present significant challenges for MLLMs [9, 12, 19, 37]. Recent studies [19, 20, 38] enhance reasoning capabilities by incorporating chain-of-thought (CoT) reasoning [39, 40], prompting or finetuning models to generate intermediate reasoning steps before producing final answers. Structured and systematic extensions of CoT [8, 9, 38, 41] further improve performance through step-by-step logical processes. While these approaches prove effective during post-training, we investigate 3 Figure 2: The SICOG framework comprises four steps: (i) Developing multimodal cognitive capabilities by finetuning an MLLM with minimal annotated image-captioning data (with Chainof-Description) and visual instruction-tuning data (with structured Chain-of-Thought), enhancing systematic perception and reasoning (upper left). (ii) Generating candidate captions and responses for pre-training by sampling from the improved models (upper right). (iii) Curating self-generated pre-training data through self-consistency-guided quality evaluation, selecting the most consistent candidates for refinement (lower right). (iv) Constructing next-generation foundation MLLM by performing self-refinement via multimodal pre-training with the curated data (lower left). For brevity, language ability preservation is omitted; see Figure 11 for the complete version. incorporating CoT reasoning during pre-training, recognizing this stage as foundational to MLLMs overall capabilities. Multimodal pre-training. Multimodal (Vision-language) pre-training has shown significant effectiveness in enhancing the multimodal perception abilities of MLMMs by training on diverse, high-quality image-captioning datasets [2, 3, 6]. However, collecting and annotating such data often relies on proprietary or open-source models to generate detailed captions [16, 17] or visual experts to extract fine-grained attributes [31], which are then incorporated into descriptive captions [7, 15, 22]. To reduce reliance on external annotations, we leverage the models inherent visual instructionfollowing and generalization capabilities to generate detailed caption data for self-refinement, similar to [11, 22]. Beyond perception, we further enhance the models multimodal reasoning abilities by incorporating self-generated visual instruction-tuning data, including both direct answers and chainof-thought formats  (Table 1)  . This enables shift from focusing solely on perception to advancing cognitive capabilities."
        },
        {
            "title": "3 SICOG",
            "content": "In this section, we introduce SICOG, self-learning framework designed to enhance MLLMs multimodal cognition for next-generation foundation MLLM construction by integrating systematic perception and reasoning during pre-training with self-generated multimodal data. 4 3.1 Overview The goal of SICOG is to advance MLLMs beyond basic visual recognition (what to see) and question answering (what to answer), enabling systematic visual understanding (how to observe) through our proposed Chain-of-Description and in-depth reasoning (how to think) via structured chain-ofthought reasoning during multimodal pre-training, with minimal external annotations. As illustrated in Figure 2, SICOG consists of four key steps. Step 1: Developing systematic multimodal cognition with minimal annotations. We enhance the MLLM, (parameterized by θ), by fine-tuning it to systematically interpret and integrate multimodal information using minimal annotated data. This includes two main components: Systematic multimodal perception. To improve the MLLMs ability to systematically observe and interpret images, we fine-tune using minimal high-quality image-captioning datasets, resulting in an enhanced perception model, MPerception . These datasets include images v, prompts x, step-by-step analyses s, and descriptions y, structured in two formats: Detailed Description (DD) and Chain-of-Description (CoD). Details of the Chain-of-Description strategy and data collection are provided in Section 3.2. 0 DPerception = DPerception DD + DPerception CoD = {(vi, xi, yi)}N i=1 + {(vi, xi, si, yi)}N i=1, (1) where is the number of samples. The fine-tuning objective for improving perception, formulated as maximizing the log-likelihood over the perception training dataset DPerception, is given by: MPerception 0 Jθ(DPerception) = (cid:88) i=1 (cid:2) log pθ(y v, x) + log pθ(s, v, x)(cid:3). (2) Systematic multimodal reasoning. To improve reasoning, we fine-tune with minimal visual instruction-tuning datasets, resulting in MReasoning . These datasets include images v, questions q, intermediate step-by-step rationales r, and answers a, structured as Direct Answer (DA) and Chain-of-Thought (CoT). Details of data curation are provided in Section 3.3. DReasoning = DReasoning + DReasoning CoT where is the number of samples. The fine-tuning objective for reasoning is: = {(vi, qi, ai)}M i=1 + {(vi, qi, ri, ai)}M DA i=1, MReasoning 0 Jθ(DReasoning) = (cid:88) i=1 (cid:2) log pθ(a v, q) + log pθ(r, v, q)(cid:3). (3) (4) Step 2: Generating candidate captions and responses for pre-training data collection. We and MReasoning construct multimodal pre-training data by leveraging the improved models, MPerception , 0 to generate candidate image captions and visual instruction responses. This step involves the following: 0 Image caption candidate generation. Given set of unlabeled images {vk}K k=1, we prompt ) using two types of instructions to generate detailed descrip0 MPerception tions and induce Chain-of-Description perception: (with policy pMPerception 0 1. Please generate detailed caption of this image. Be as descriptive as possible. (xDD). 2. Please generate detailed caption of this image. Describe the image step by step. (xCoD). For each image vk, MPerception {ˆyk} pMPerception 0 generates multiple candidate captions via sampling: ( vk, xDD), {(ˆsk, ˆyk)} pMPerception ( vk, xCoD), 0 (5) where {ˆyk} is the set of detailed descriptions, and {(ˆsk, ˆyk)} is the set of step-by-step analyses with descriptions. The resulting dataset includes captions in two formats. 5 Visual instruction candidate response generation. For set of unlabeled images {vz}Z corresponding questions qz, we prompt MReasoning instructions to generate direct answers and induce Chain-of-Thought reasoning: (with policy pMReasoning 0 z=1 with ) using two types of 1. <original question>. (qDA). 2. <original question> Answer the question step by step. (qCoT ). For each image vz and question qz, MReasoning {ˆaz} pMReasoning 0 0 produces multiple candidate responses: ( vz, qDA), {(ˆrz, ˆaz)} pMReasoning ( vz, qCoT ), (6) 0 where {ˆaz} is the set of direct answers, and {(ˆrz, ˆaz)} is the set of step-by-step rationales with answers. The resulting dataset includes responses in two formats. Step 3: Curating self-generated pre-training data via self-consistency-guided quality evaluation. To ensure the quality of self-generated pre-training data, we employ self-consistency [26, 42] to evaluate candidate samples without external supervision. This method is based on the principle that higher-quality candidates exhibit greater semantic consistency. The most consistent candidates are selected for further self-refinement during multimodal pre-training. Specifically, we apply self-consistency-guided evaluation function, (). For each instance (e.g., an unlabeled image), it assesses the quality of candidates (e.g., candidate captions) by comparing each candidate against all others based on semantic similarity and selects the candidate with the highest consistency score, provided it exceeds predefined threshold τ (i.e., otherwise, the instance and its candidates are skipped): ({c}) = arg max c{c} 1 Ncand Ncand(cid:88) j=1 sim(c, c(j)), s.t. max c{c} 1 Ncand Ncand(cid:88) j=1 sim(c, c(j)) τ, (7) where Ncand is the number of candidate samples being compared, and {c} represents the candidate set. As illustrated in the lower-right part of Figure 2, we apply this method as follows: Image caption curation. For each image vk, we apply () to evaluate the quality of candidate captions by comparing each generated description ˆyk against all others. The caption with the highest semantic consistency is selected as the final self-generated caption, resulting in curated dataset of selected captions DPerception : Selected DPerception Selected ˆyselected (ˆsselected, ˆyselected) = ({ˆyk} {(ˆsk, ˆyk)}). (8) Visual instruction response curation. For each image vz and question qz, () evaluates candidate responses by comparing each generated answer ˆaz against all others. The most consistent response is selected, resulting in curated dataset DReasoning Selected : DReasoning Selected ˆaselected (ˆrselected, ˆaselected) = ({ˆaz} {(ˆrz, ˆaz)}). (9) In addition, to preserve language capabilities, we prompt the backbone LLM, MLLM , to generate candidate responses for unlabeled text-only instructions. These responses are then curated using similar process, resulting in DLanguage Selected . The final curated self-generated pretraining dataset is: DPre-training = DPerception Selected + DReasoning Selected + DLanguage Selected . (10) Step 4: Constructing the next-generation MLLM through multimodal pre-training. To build the next-generation foundation MLLM, we introduce an intermediate multimodal pre-training stage, Stage 1.5, within the standard two-stage training strategy, following [6, 14]. This stage refines the MLLM using curated self-generated pre-training data. The complete training strategy consists of three stages, as shown in the lower left part in Figure 2: Stage 1: Modality alignment. Align image features with the text embedding space. Following [14], the vision-language (VL) connector is trained on image-text pairs for alignment, while both the vision encoder (e.g., vision transformer) and the LLM remain frozen. 6 Stage 1.5: Multimodal pre-training for self-refinement. Train the model on curated pretraining data DPre-training to acquire multimodal knowledge and integrate systematic perception and reasoning. During this stage, all model components are fully trainable. Stage 2: Visual instruction-tuning. Fine-tune the model on instruction-tuning data to develop robust visual instruction-following capabilities. All model components remain fully trainable. This three-stage training process yields next-generation foundation MLLM with enhanced selfimproving cognition. comprehensive overview is given in Appendix A, while Algorithm 1 summarizes the complete training process, and implementation details are in Section 4.1 and Appendix B. 3.2 Enhancing Systematic perception with Chain-of-Description We introduce Chain-of-Description (CoD) to enable systematic and comprehensive perception, equipping the MLLM with the ability to logically analyze and describe vistructured approach ensual hances the MLLMs efficiency in thoroughly interpreting visual content. Specifically, Chain-of-Description perception is organized into the following five steps (Figure 3): information step by step (how to observe)."
        },
        {
            "title": "This",
            "content": "Step 1: Extract salient content. Identify the key elements that define the overall context and meaning of the image, laying the foundation for basic visual recognition. Step 2: Analyze detailed information. Focus on instance-level attributes, such as lowlevel and fine-grained details, e.g., the guitar is classic wooden brown with lightcolored frets. This step ensures precise and detailed interpretation of the image. Step 3: Consider relational-level attributes. Analyze interactions between elements and their spatial organization, e.g., the person is seated on the bed, leading to richer and more comprehensive understanding of visual relationships. Step 4: Examine marginal or peripheral content. Pay attention to less prominent or background details, e.g., the dresser in the background, to ensure no important information is overlooked. Step 5: Organize all observations. Synthesize all findings into cohesive, detailed description, enabling comprehensive coverage and holistic image understanding. Data preparation. To enable systematic perception in MLLMs, we utilize GPT-4o [24] with manually curated prompts  (Table 13)  to generate detailed, step-by-step analyses of visual features. Specifically, we prompt GPT-4o to recaption 35k images from the Vision-Flan dataset [43], which provides diverse visual content. detailed example is presented in Table 15. Figure 3: Illustration of Chain-of-Description for enhancing systematic perception. 3.3 Improving Systematic Reasoning with Structured Chain-of-Thought We adopt structured Chain-of-Thought (CoT) approach [9] to enhance systematic and in-depth reasoning. For completeness, we briefly summarize this approach. It enables the MLLM to decompose problem-solving into logical steps: analyzing multimodal questions, gathering relevant 7 visual information, and answering progressively. Specifically, the structured CoT process (Figure 12) follows four logical steps: (i) Step 1: Clarify the task objective. Identify the problems requirements and constraints, establishing foundational understanding. (ii) Step 2: Extract crucial visual information. Identify and extract relevant visual elements to enhance multimodal comprehension. (iii) Step 3: Generate detailed reasoning. Construct logical sequence of intermediate steps based on the extracted information to derive an answer systematically. (iv) Step 4: Conclude with an answer. Synthesize the reasoning steps into coherent and accurate final response. Data preparation. To enable systematic reasoning in the MLLM, we revise the dataset curated by [9]. Specifically, we randomly select 35k training examples from LLaVA-CoT, covering ten well-studied QA tasks, and replace the special tags (e.g., <SUMMARY> and </SUMMARY>) with curated step-by-step instructions. detailed example is shown in Table 16."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we conduct extensive experiments to validate the efficacy of SICOG in enhancing MLLMs cognitive capabilities, i.e., , perception and reasoning, for constructing next-generation foundation MLLM with minimal external annotation. 4.1 Setup Datasets and Evaluation Metrics. We evaluate the efficacy of SICOG on the following wellestablished benchmarks, using the open-source evaluation toolkit VLMEvalKit [52]: Multimodal Comprehensive Understanding: MMStar [53], MMBench [29], MMVet [54] (report accuracy). Hallucination: POPE [55] (report F1 score). Chart/Table Understanding: OCRBench [56], DocVQA [57], ChartQA [58] (report accuracy). Knowledge-Oriented Tasks: MathVista [59], ScienceQA [60], AI2D [61] (report accuracy). Real-World Understanding: RealWorldQA [62] (report accuracy). Compared Methods. We compare SICOG against the following representative MLLM pre-training approaches (as discussed in Section 2). Differences are considered significant at < 0.01: Strong-to-Weak Distillation (Perception) [14]: Pre-training with re-caption data containing detailed descriptions (DD) generated by stronger models. Multi-Agent Collaboration (Perception) [22]: Pre-training with re-caption data containing detailed descriptions and fine-grained attributes (DD-FGA) generated by base and expert models. Self-Improving Cognition (Perception & Reasoning Ours): Pre-training with self-generated data, including re-caption data containing detailed descriptions (DD) and Chain-of-Description (CoD), visual instruction-tuning data with direct answers (DA) and structured chain-of-thought (CoT), as well as text-only instruction-tuning data. Implementation Details. (i) Models: We utilize both low-resolution (LLaVA-Qwen2-7B [63]) and high-resolution (LLaVA-Qwen2-7B-UHD [64] and LLaVA-Llama3.1-8B-UHD) models for our investigation. Specifically, we employ CLIP-ViT-L/14-336 [65] as the visual encoder, Qwen2-7BInstruct [66] and LLama-3.1-8B-Instruct [67] as the backbone LLMs. (ii) Self-Generated Data Source: We recaption the images used for modality alignment and self-annotate 63k image-question pairs randomly selected from LLaVA-CoT, ensuring zero overlap with the curated training data in Section 3. Additionally, we self-annotate 100k text-only prompts randomly sampled from OpenHermes2.5 [68]. (iii) Data Utilized in the Three-Stage Training Strategy: During the self-refinement step (Step 4 in Figure 2), we use BLIP-558k caption data [65] for modality alignment, following [6]. For multimodal pre-training, we use self-generated data along with 858k instruction-tuning samples organized by [69], which include the widely adopted LLaVA-Mix665k [1] and 160k samples from UReader [70]. Further details are provided in Appendix B. 8 Method Train Data Stage 1.5 Comprehensive Hallu. Chart/Table Knowledge MMBen. MMStar POPE DocV. Chart. Math. Science. AI2D Open-Sourced Models (For holistic analysis, not for comparison) VITA-1.0-Mixtral-8x7B [44] LLaVA-v1.5-13B [1] ShareGPT4V-13B [45] Molmo-7B-O [46] Eagle-X5-13B [47] CogVLM2-19B-Chat [48] LLaVA-NeXT-8B [14] Cambrian-1-8B [49] XGen-MM-Instruct-Interleave-v1.5 [50] Janus-Pro-7B [48] DeepSeek-VL-7B [3] VILA1.5-13B [51] Base Model LLaVA-Qwen2-7B Strong-to-Weak Distillation (Perception) LLaVA-Qwen2-7B-LLaVA-NeXT-34B LLaVA-Qwen2-7B-Qwen2-VL-72B-Instruct Multi-Agent Collaboration (Perception) LLaVA-Qwen2-7B-Multi-Experts Self-Improving Cognition (Perception & Reasoning) SICOG-LLaVA-Qwen2-7B (Perception) SICOG-LLaVA-Qwen2-7B (Perception, Reasoning) SICOG-LLaVA-Qwen2-7B (Perception, Reasoning, Language) Base Model LLaVA-Qwen2-7B-UHD - - - - - - - - - Sufficient high-quality multimodal data Sufficient high-quality multimodal data Sufficient high-quality multimodal data - 69.20 69.80 72.20 72.60 73.90 74.80 74.60 78.30 62. 46.60 34.30 38.30 50.10 43.70 50.50 43.90 50.70 48.40 46.50 - 88.40 87.50 86.70 89.80 83.40 87.10 86.40 87.20 78.90 73.80 40. 85.60 74.40 44.20 85.00 - - - - - - - - - - - - - 44.50 18.20 27.70 24.60 29.40 36.50 43.90 69.60 40.80 33.00 38.70 68.70 37.70 72.60 48.10 40.60 - - 72.60 72.60 88.80 71.80 90.20 73.10 81.00 88. 72.80 61.10 61.40 75.70 77.00 73.40 72.80 74.60 74.20 - 42.50 83.20 68.10 59.10 37. 80.90 65.30 - 42.30 79.10 69. Low-Resolution - 74.44 46.67 84.55 50.62 52.72 38.00 74. 73.77 118k Caption w/ DD by LLaVA-NeXT-34B 118k Caption w/ DD by Qwen2-VL-72B-Instruct 76.18 46.73 83.72 51.26 52.68 36.60 75. 74.38 75.84 48.20 83.84 50.85 52.56 36.10 76.15 74. 118k Caption w/ DD-FGA by base and expert models Self-generated 118k caption w/ DD&CoD Self-generated 118k caption w/ DD&CoD, 45k VQA w/ DA&CoT Self-generated 118k caption w/ DD&CoD, 45k VQA w/ DA&CoT, 50k textual QA 76.01 47.60 84.12 51.06 53.36 38.90 75. 74.97 75.34 48.27 83.89 50.83 54.88 38.50 74.71 75. 76.01 48.67 84.10 52.70 55.20 38.10 78.88 76.78 75. 48.60 84.35 52.52 54.48 38.80 77.44 76.20 High-Resolution - 77.63 48.93 87.31 70.18 69.96 38.90 77.29 74.94 Strong-to-Weak Distillation (Perception) LLaVA-Qwen2-7B-UHD-LLaVA-NeXT-34B LLaVA-Qwen2-7B-UHD-Qwen2-VL-72B-Instruct Multi-Agent Collaboration (Perception) 118k Caption w/ DD by LLaVA-NeXT-34B 118k Caption w/ DD by Qwen2-VL-72B-Instruct LLaVA-Qwen2-7B-UHD-Multi-Experts Self-Improving Cognition (Perception & Reasoning) SICOG-LLaVA-Qwen2-7B-UHD (Perception) 118k Caption w/ DD-FGA by base and expert models Self-generated 118k caption w/ DD&CoD Self-generated 118k caption w/ DD&CoD, 45k VQA w/ DA&CoT Self-generated 118k caption w/ DD&CoD, 45k VQA w/ DA&CoT, 50k textual QA SICOG-LLaVA-Qwen2-7B-UHD (Perception, Reasoning) SICOG-LLaVA-Qwen2-7B-UHD (Perception, Reasoning, Language) Base Model LLaVA-Llama3.1-8B-UHD 77.75 50.60 86.46 71.20 71.56 36. 78.38 76.00 77.75 51.87 86.43 71.05 72.40 38.30 79. 76.52 77.97 49.87 86.48 71.27 71.80 37.90 77.79 76. 78.08 51.60 87.03 72.42 73.04 39.50 77.34 77.59 77. 50.13 87.32 73.70 73.12 39.50 79.23 77.91 77.80 52. 87.84 73.05 72.24 41.40 79.42 78.40 - 72.14 43. 87.85 64.32 64.64 33.90 74.96 71.70 Self-Improving Cognition (Perception & Reasoning) SICOG-LLaVA-Llama3.1-8B-UHD (Perception) SICOG-LLaVA-Llama3.1-8B-UHD (Perception, Reasoning) SICOG-LLaVA-Llama3.1-8B-UHD (Perception, Reasoning, Language) Self-generated 118k caption w/ DD&CoD Self-generated 118k caption w/ DD&CoD, 45k VQA w/ DA&CoT Self-generated 118k caption w/ DD&CoD, 45k VQA w/ DA&CoT, 50k textual QA 71.92 44.80 87.37 65.09 64.96 35.70 74.52 71. 72.03 43.20 87.38 65.78 65.56 35.90 76.15 72.05 72. 43.07 87.21 64.95 65.00 33.30 75.56 70.76 Table 2: Evaluation results of the compared methods on eight benchmarks (direct answer inference). The only difference between the compared methods is the pre-training data utilized in Stage 1.5 (see details in Step 4 of Section 3). Results marked with an asterisk () are cited from the OpenVLM Leaderboard [52]. Abbreviations: MMBen. (MMBench), Hallu. (Hallucination), DocV. (DocVQA), Chart. (ChartQA), Math. (MathVista), Science. (ScienceQA). 4.2 Can SICOG Yield Next-Generation Foundation MLLMs with Self-Improved Systematic Cognition? Table 2 presents the comprehensive evaluation results. We summarize the following key observations: SICOG yields next-generation foundation MLLMs with self-improved cognitive capabilities. SICOG consistently improves both high-resolution and low-resolution foundation MLLMs across diverse tasks, achieving gains of 2%3.5% on MMStar for comprehensive tasks, 2%3% on perception tasks (e.g., DocVQA and ChartQA), and 2%4% on reasoning tasks (e.g., ScienceQA and AI2D). Systematic perception through self-learning strengthens foundation MLLMs. SICOG (Perception), leveraging self-generated captions with detailed descriptions and chain-ofdescription, delivers comparable or superior accuracy across benchmarks compared to strongto-weak distillation and multi-modal collaboration methods. Unlike these alternatives, which depend heavily on fine-grained captions requiring extensive external annotations, SICOG reduces this reliance by utilizing self-learning. Integrating systematic reasoning into pretraining proves more effective than prioritizing perception alone. SICOG (Perception + Reasoning) boosts multimodal reasoning, surpassing perception-only methods by 2.5%4% on ScienceQA while preserving strong perception capabilities. Notably, perception-only pretraining degrades performance on hallucination tasks (a 0.5%1% drop on POPE), whereas systematic reasoning mitigates this issue and maintains robustness. self-generated text-only Incorporating instruction-tuning data during pre-training further enhances performance, especially for high-resolution MLLMs. This observation aligns with findings in [3]. Figure 4: Fine-grained evaluation of six core capabilities on LLaVA-Qwen2-7B-UHD using the MMStar benchmark (direct answer). Stronger foundation MLLMs enable more effective self-improvement. SICOG achieves greater performance gains when applied to LLaVA-Qwen2-7B-UHD (higher baseline capabilities) compared to LLaVA-Llama3.1-8B-UHD (lower baseline capabilities), demonstrating that base model performance significantly influences self-improvement potential, with stronger MLLMs yielding better results. Additionally, the fine-grained evaluation of six core capabilities in Figure 4 highlights the effectiveness of SICOG in advancing multimodal cognitive abilities in MLLMs. 4.3 How Does SICOG Enhance the Reasoning Capabilities of Foundation MLLMs? Quantitative Analysis. We validate the efficacy of SICOG (perception, reasoning, language) in enhancing the reasoning capabilities of MLLMs under two inference settings: direct answer and CoT. The absence of CoT reasoning annotations in the instruction-tuning data [1, 69] used in stage 2 limits the models ability to generate CoT reasoning. To address this limitation, we incorporate 45k self-generated visual instruction-tuning examplesoriginally used during the pre-training stage (stage 1.5)into the instruction-tuning stage (stage 2) (see details in step 4 of section 3). Incorporating self-generated visual instruction-tuning data for instruction-tuning further improves multimodal reasoning. As shown in Table 3, incorporating self-generated visual instructiontuning data in stage 2 enhances SICOGs performance on most reasoning-intensive tasks across both inference settings. For instance, it provides an additional accuracy gain of approximately 1-4% on 10 Method Infer. Train Data Comprehensive Hallu. Chart/Table Knowledge Stage 2 MMBen. MMStar MMVet POPE DocV. Chart. Math. Science. AI2D Base Model LLaVA-Qwen2-7B-UHD Self-Improving Cognition SICOG-LLaVA-Qwen2-7B-UHD (Perception, Reasoning, Language) Direct - 77. 48.93 38.26 87.31 70.18 69.96 38.90 77.29 74.94 - Direct Direct + Self-generated 45k VQA w/ DA&CoT CoT + Self-generated 45k VQA w/ DA&CoT 77.80 76. 52.47 51.00 40.14 87.84 73.05 72.24 41.40 79.42 78.40 39.82 88.02 74.15 73.12 42.90 80.61 78.21 65.19 44.60 40.92 87.36 72.48 76.20 36.90 72.93 73. Table 3: Evaluation results of SICOG variants on LLaVA-Qwen2-7B-UHD in two inference settings: direct answer and CoT for reasoning abilities. ChartQA. On benchmarks such as POPE and ScienceQA, direct answer inference outperforms CoT inference, likely due to the overwhelming prevalence of direct answer annotations compared to CoT annotations in the instruction-tuning data. In addition, we observe performance drop on perception-heavy benchmarks like MMBench and MMStar. We suspect this decline stems from data distribution shift introduced by the additional reasoning-focused data. Figure 5: Qualitative comparison of responses generated by LLaVA-Qwen2-7B-UHD and SICOGLLaVA-Qwen2-7B-UHD for the visual instruction-following task. Qualitative Analysis. We compare the responses generated by the base LLaVA-Qwen2-7B-UHD and SICOG-LLaVA-Qwen2-7B-UHD (enhanced with self-generated VQA data in Stage 2) on an image-question pair from ChartVQA. Figure 5 illustrates that, unlike LLaVA-Qwen2-7B-UHD, which produces an incorrect answer, SICOG-LLaVA-Qwen2-7B-UHD effectively integrates multimodal information into systematic reasoning process, yielding an accurate and coherent response. Specifically, SICOG-LLaVA-Qwen27B-UHD first clarifies the task requirements and extracts key visual information, such as United Arab Emirates (94%) and Mauritania (91%). It then systematically leverages this information, identifying the two largest barsUnited Arab Emirates (94%) and Mauritania (91%)and the three smallest barsArmenia (73%), South Korea (72%), and Spain (88%). By performing precise reasoning and calculations, SICOG-LLaVA-Qwen2-7B-UHD ultimately derives the correct answer. 11 These findings confirm the efficacy of SICOG in enhancing the systematic reasoning capabilities of MLLMs, aligning with the results observed in the quantitative analysis. 4.4 Can SICOG Facilitate Stronger Reasoning Foundation for Prototyping Chain-of-Thought Reasoners During Post-Training? Method Inference Comprehensive Hallu. Chart/Table Knowledge MMBen. MMStar MMVet POPE DocV. Chart. Math. Science. AI2D Base Model LLaVA-Qwen2-7B-UHD + Finetune w/ 35k VQA (CoT) Self-Improving Cognition SICOG-LLaVA-Qwen2-7B-UHD (P-R-L) + Finetune w/ 35k VQA (CoT) Direct CoT 77.63 72.09 48.93 49. 38.26 87.31 70.18 69.96 38.90 77.29 74.94 41.06 85.32 69.08 77.48 44.90 84.88 72.12 CoT 71.97 51.00 47.29 86.34 70.76 79.24 45.70 85.77 74. Table 4: Evaluation results of fine-tuning foundation MLLMs to build CoT reasoner via supervised fine-tuning on 35k CoT reasoning examples (curated in Section 3). We validate the efficacy of SICOG in strengthening the reasoning foundation for constructing CoT reasoners during post-training. Specifically, we adopt supervised fine-tuning approach, refining both the base model, LLaVA-Qwen2-7B-UHD, and SICOG-LLaVA-Qwen2-7B-UHD on the 35k CoT reasoning dataset curated in Section 3. SICOG establishes stronger foundation for prototyping CoT reasoner. Table 4 demonstrates that post-training the SICOG-LLaVA-Qwen2-7B-UHD outperforms the post-trained baseline across most benchmarks, with up to 6% higher accuracy on MMVet. Solely enhancing CoT reasoning may compromise perception abilities. We observe significant performance drop on MMBench, which assesses diverse range of perception tasks. This suggests that prioritizing CoT reasoning in MLLMs can inadvertently impair perception capabilities, underscoring the trade-off between reasoning and perception and the need for balanced optimization. 4.5 How Does Scaling Self-Generated Pre-Training Data Affect the Performance of SICOG? The primary objective of multimodal pre-training is to refine and enhance knowledge acquisition from image captioning datasets [14]. In this context, we analyze the effect of scaling self-generated pre-training data on SICOG-LLaVA-Qwen2-7B-UHD (perception, reasoning, language) by prioritizing an increase in the number of self-generated caption data. Specifically, we assess the performance of SICOG across four dimensions on ten benchmarks: comprehensive understanding (MMBench, MMStar, MMVet), hallucination (POPE), OCR and chart/table understanding (OCRBench, DocVQA, ChartQA), and knowledgeintensive tasks (MathVista, ScienceQA, AI2D) (Figure 6). Scaling up self-generated captions improves the performance of SICOG. Increasing the quantity of self-generated caption data results in consistent performance improvements across three dimensions: comprehensive understanding (up to approximately 2%), OCR and chart/table understanding (up to around 2.5%), and knowledge-intensive tasks (up to around 3%), while maintaining stable performance on hallucination tasks. These improvements underscore the importance of scaling caption data in enhancing SICOGs ability to improve MLLMs multimodal cognition. 12 Figure 6: Impact of scaling self-generated captions on SICOG-LLaVA-Qwen2-7B-UHD during multimodal pre-training, evaluated across four dimensions on ten benchmarks. However, slight performance decline occurs when the amount of caption data is increased without proportionally adjusting the quantities of visual and text-only instruction tuning data. We hypothesize that this decline arises from the overwhelming dominance of caption data, which creates an imbalanced data ratio and hinders effective model optimization. 4.6 Does SICOG Remain Effective When Varying Recaptioned Images? Method Base Model LLaVA-Qwen2-7B-UHD Self-Improving Cognition SICOG-LLaVA-Qwen2-7B-UHD (P-R-L) (Recap. w/ BLIP 118k [71]) SICOG-LLaVA-Qwen2-7B-UHD (P-R-L) (Recap. w/ V-FLAN 148k [43]) Comprehensive Hallu. Chart & Table Knowledge Vision MMBen. MMStar MMVet POPE OCR. DocV. Chart. Math. Science. AI2D Realworld. 77.63 48.93 38.26 87.31 55.20 70.18 69.96 38.90 77.29 74.94 63.53 77. 52.47 40.14 87.84 57.70 73.05 72.24 41.40 79.42 78.40 63.92 77.07 51.93 38.67 87.50 56.40 73.45 73.60 40.20 79.38 77. 67.19 Table 5: Evaluation results of varying unlabeled image datasets for recaptioning on SICOG-LLaVAQwen2-7B-UHD across eleven benchmarks. We validate the effectiveness of SICOG across varying corpora by employing different unlabeled image datasets for recaptioning. Specifically, we randomly sample 148k images from the Vision-Flan (V-Flan) dataset [43], which provides diverse range of images and ensures zero overlap with the curated data described in Section 3. SICOG is robust to variations in recaptioned images. As shown in Table 5, SICOG-LLaVAQwen2-7B-UHD (Perception, Reasoning, Language) consistently outperforms the base model, LLaVA-Qwen2-7B-UHD, achieving an approximate 4% accuracy gain on RealworldQA. This result underscores the role of image diversity in enhancing real-world understanding and demonstrates the robust generalizability of SICOG across different corpora. 4.7 Can SICOG Contribute to the Construction of Next-Generation Foundation MLLMs through Continuous Cognitive Self-Improvement? Figure 7: Evaluation results for next-generation foundation MLLM construction through continuous self-improvement using SICOG. Accuracy gains are reported as absolute improvements over the base model LLaVA-Qwen2-7B-UHD, with the base models performance shown in parentheses. We investigate the potential of SICOG in advancing next-generation foundation MLLM construction through continuous cognitive self-improvement. Specifically, we consider SICOG-LLaVA-Qwen27B-UHD (perception, cognition, language) from Table 2 as the foundation MLLM obtained in the first iteration. In the second iteration, 148K images from the V-Flan dataset are newly recaptioned, generating self-curated dataset comprising caption data (118K from BLIP and 148K from V-Flan), VQA data (45K), and textual QA data (50K). SICOG drives next-generation foundation MLLM construction via continuous cognitive selfimprovement. Figure 7 presents absolute accuracy gains over the initial base MLLM, LLaVA-Qwen27B-UHD. The results indicate that SICOG-LLaVA-Qwen2-7B-UHD enhances MLLM cognition across most benchmarks in the second iteration, achieving an additional 1.5% accuracy gain on 13 MMVet. However, performance regressions on certain benchmarks may result from an overrepresentation of caption data. 4.8 Can Preference Learning Support SICOGs Systematic Perception and Reasoning Development? Method Base Model LLaVA-Qwen2-7B Self-Improving Cognition SICOG-LLaVA-Qwen2-7B (Per., Rea., Lan.) Capability Development Comprehensive Hallu. Chart & Table Knowledge MMBen. MMStar MMVet POPE DocV. Chart. Math. Science. AI2D - 74. 46.67 38.85 84.55 50.62 52.72 38.00 74.91 73.77 SFT (Per., Rea.) 75.45 DPO (Per.), SFT (Rea.) 76.18 74.83 DPO (Per., Rea.) 48.60 48.40 49.00 37.84 84.35 52.52 54.48 38.72 83.53 52.20 54.80 38.90 84.85 52.54 55.64 38.80 77.44 76.20 39.20 77.49 75.78 41.00 76.20 76.33 Table 6: Evaluation results of different training methods for developing perception and reasoning in LLaVA-Qwen2-7B during Step 1 of SICOG (Section 3). Motivated by the great success of preference learning in adapting MLLMs to follow instructions during the post-training stage [72, 73], we explore its application to enhance MLLMs multimodal perception and reasoning capabilities during Step 1 of SICOG (Section 3). Specifically, we construct preference caption pairs by using high-quality captions from the annotated caption dataset (Section 3) as preferred captions and pairing them with corresponding low-quality (dispreferred) captions. The low-quality captions are generated by corrupting the associated images through the following methods (Figure 13): (i) introducing random noise to hinder key information capture, (ii) altering object colors to disrupt fine-grained detail perception, (iii) mirroring and rotating images to distort relation-level attributes, and (iv) masking peripheral objects to obscure peripheral content. We fine-tune the MLLM on these caption preference pairs using the Direct Preference Optimization (DPO) algorithm [72] to initialize systematic perception capabilities. Similarly, we extend preference learning to develop systematic reasoning capabilities. Preference learning supports SICOGs systematic perception and reasoning development. As shown in Table 6, preference learning with DPO significantly enhances MLLMs systematic perception and reasoning, enabling their self-improvement via SICOG, e.g., achieving 2.5% accuracy gain on MMstar. Preference learning is more effective than supervised fine-tuning for systematic perception and reasoning development. Preference learning with DPO consistently surpasses standard supervised fine-tuning across all benchmarks for initializing systematic perception and reasoning in SICOG. For example, on MathVista, preference learning improves accuracy by approximately 2% on the low-resolution model LLaVA-Qwen2-7B, which is particularly challenging to enhance due to inherent visual perception limitations. These results underscore the importance of learning not only from correct examples but also from avoiding mistakes, thereby fostering more robust skill development."
        },
        {
            "title": "5 Detailed Analysis of Chain-of-Description and Chain-of-Thought",
            "content": "In this section, we examine two key factors driving SICOGs efficacy: (i) how Chain-of-Description facilitates multimodal perception, and (ii) how structured chain-of-thought enhances multimodal reasoning. 5.1 How Does Chain-of-Description Facilitate Multimodal Perception? Quantitative Analysis. We analyze captions for 100 images randomly sampled from BLIP-558k [74], which is used as unlabeled image captioning data in Section 4. These captions are generated by perception-enhanced models fine-tuned on annotated caption data in three formats: detailed description (Detailed D), Chain-of-Description (CoD), and their combination (as implemented in SICOG, described in Section 3). Using GPT-4 with the prompt shown in Table 14, we evaluate six key dimensions: salient content, fine-grained details, relational attributes, peripheral content, faithfulness, and world knowledge. For holistic analysis, we also include LLaVA-NeXT-34B, leading open-source MLLM known for its strong captioning capabilities [14]. Table 7 shows that 14 Method # Avg. Tokens Systematic Perception General Performance Sali. Fine-Grain. Rela. Peri. Faith. Know. Low-Resolution LLaVA-Qwen2-7B + Finetune w/ Detailed + Finetune w/ CoD (Caption) + Finetune w/ CoD (Multi.) + Finetune w/ Detailed & CoD (Detailed D) + Finetune w/ Detailed & CoD (CoD Multi.) 129.36 129.68 130.55 458.09 130.13 436.53 4.51 4.59 4.73 4.71 4.75 4.89 High-Resolution LLaVA-Qwen2-7B-UHD + Finetune w/ Detailed + Finetune w/ CoD (Caption) + Finetune w/ CoD (Multi.) + Finetune w/ Detailed & CoD (Detailed D) + Finetune w/ Detailed & CoD (CoD Multi.) 135.08 140.73 126.93 453.13 136.5 453.26 4.77 4.71 4.78 4.82 4.76 4. LLaVA-NeXT-34B [6] 206.50 4.77 4.21 4.49 4.52 4.69 4.54 4.81 4.30 4.52 4.58 4.80 4.67 4.87 4. 3.82 3.88 4.06 4.62 4.13 4.76 3.99 3.92 4.11 4.74 4.01 4.78 4.04 3.67 3.88 3.92 4.22 3.97 4.26 3.81 3.91 3.90 4.29 3.82 4.32 3. 4.07 4.13 4.36 4.32 4.49 4.67 4.41 4.20 4.57 4.57 4.51 4.71 4.59 3.63 3.87 3.90 4.01 3.95 4.05 3.84 3.77 3.93 4.01 3.88 4.05 4. Table 7: Evaluation of re-captioning quality comparing the perception-enhanced models fine-tuned on curated caption data in three formats: detailed description (Detailed D), Chain-of-Description (CoD), and their combination (Section 3). Metrics (rated 1-5): salient content, fine-grained details, relational attributes, peripheral content, faithfulness, and world knowledge. Caption: standard format; Multi.: CoD step-by-step format (see Table 9 for details). Figure 8: Distribution of caption token lengths: perception-enhanced LLaVA-Qwen2-7B-UHD (fine-tuned on curated captions using the standard detailed description format and the mixture of detailed description and Chain-of-Description formats) vs. LLaVA-NeXT-34B. the base model, regardless of resolution, consistently underperforms in salient content, fine-grained details, relational attributes, and peripheral content. These results highlight the importance of the four-step perception analysis design used in Chain-of-Description. Chain-of-Description shows strong efficacy in facilitating systematic perception across six key dimensions. Perception-enhanced models fine-tuned with Chain-of-Description outperform those trained on detailed descriptions in both single-step (caption-only) and multi-step formats. Notably, their combination achieves the highest evaluation scores, surpassing LLaVA-NeXT-34B in five of the six dimensions. Additionally, Chain-of-Description generates captions with the longest token lengths (i.e., approximately 430450 tokens). Figure 8 compares token length distributions, further showing that Chainof-Description consistently produces longer captions. Qualitative Analysis. We compare two caption examples generated by LLaVA-Qwen2-7B-UHD and perception-enhanced LLaVA-Qwen2-7B-UHD (adopted in SICOG) on an image from V-FLAN 148k, as referenced in Table 5. In Figure 9, the results reveal that Chain-of-Description enables MLLMs to capture richer and more detailed visual information across all six dimensions, whereas captions generated by the base LLaVA-Qwen2-7B-UHD often include inaccuracies, e.g., hallucinations [75]. For instance, Chain15 Figure 9: Qualitative comparison of captions generated by LLaVA-Qwen2-7B-UHD and perceptionenhanced LLaVA-Qwen2-7B-UHD across six key dimensions in the image captioning task. of-Description allows MLLMs to identify nuanced details such as the road appears wet and the lighting conditions are subdued in step 4, suggesting recent rain and an overcast day. In contrast, the base LLaVA-Qwen2-7B-UHD fails to capture these details, resulting in an inaccurate description of clear sky. This observation aligns with quantitative findings, confirming Chain-of-Descriptions effectiveness in enhancing systematic perception. Chain-of-Description avoids redundancy through systematic step-wise analysis. While Chain-ofDescription generates comprehensive and detailed descriptions, it maintains efficiency by eliminating information overlap in its step-by-step analysis. For instance, peripheral elements such as red car and road sign reading do not enter are described with precise spatial relations in step 3. Subsequently, step 4 focuses exclusively on new observations, such as the road appears wet, ensuring non-redundant content progression and avoiding verbosity. 5.2 How Does Structured Chain-of-Thought Enhance Multimodal Reasoning? Quantitative Analysis. We evaluate the accuracy of answers for 1k image-question pairs randomly sampled from the 63k LLaVA-CoT split, which is used as unlabeled pre-training VQA data in Section 4. These answers are generated by reasoning-enhanced models fine-tuned on curated reasoning data in three formats: direct answer, structured CoT, and their combination (as implemented in SICOG). As shown in Table 8, models fine-tuned with structured CoT improve the base models performance by 9% in EM. The combination of CoT and direct answer achieves the best results, outperforming other methods by 2% to 5% EM, demonstrating the effectiveness of structured CoT in enhancing 16 Low-Resolution High-Resolution Method Correct. (EM) Method Correct. (EM) LLaVA-Qwen2-7B + Finetune w/ Direct Answer + Finetune w/ Chain-of-Thought (CoT) + Finetune w/ Direct Ans. & CoT (Direct) + Finetune w/ Direct Ans. & CoT (CoT) 26 35 35 37 37 LLaVA-Qwen2-7B-UHD + Finetune w/ Direct Answer + Finetune w/ Chain-of-Thought (CoT) + Finetune w/ Direct Ans. & CoT (Direct) + Finetune w/ Direct Ans. & CoT (CoT) 33 43 42 47 47 Table 8: Evaluation of self-generated reasoning quality, comparing reasoning-improved models fine-tuned on curated reasoning data in three formats: direct answer (Direct), chain-of-thought (CoT), and their combination (Section 3). Exact Match (EM) scores are used to assess the correctness of final answers. multimodal reasoning in MLLMs. However, models fine-tuned solely with CoT perform comparably or slightly worse than those fine-tuned with direct answers, likely due to insufficient CoT reasoning data in the base models training set. Qualitative Analysis. We compare the responses generated by the base LLaVA-Qwen2-7B-UHD Figure 10: Qualitative comparison of responses generated by LLaVA-Qwen2-7B-UHD and reasoningenhanced LLaVA-Qwen2-7B-UHD in the visual question-answering task. and the reasoning-enhanced LLaVA-Qwen2-7B-UHD (used in SICOG) on an image-question pair from the 63k LLaVA-CoT split. In Figure 10, our analysis demonstrates that the structured CoT enables the MLLM to generate systematic, logical, and in-depth reasoning step-by-step, resulting in accurate answers. Specifically, CoT first helps clarify the task requirements and captures critical visual information. Then, CoT enables the MLLM to utilize key visual details, such as 212.22 million U.S. dollars and 354 million U.S. dollars, to perform accurate reasoning and calculations."
        },
        {
            "title": "6 Conclusion",
            "content": "We present SICOG, self-learning framework designed to enhance an MLLMs systematic cognition for constructing next-generation foundation MLLMs through multimodal pre-training with selfgenerated data. Specifically, we introduce Chain-of-Description for step-by-step visual analysis 17 and adopt structured CoT for integrated multimodal reasoning. With minimal external annotations, SICOG first develops these capabilities, then generates and curates training data via self-consistency for further pre-training refinement, ultimately yielding stronger foundation MLLM. Extensive experiments on both lowand high-resolution models across eleven benchmarks demonstrate that SICOG produces next-generation foundation MLLM with significantly self-improved cognitive capabilities, outperforming prevalent pre-training approaches on nearly all benchmarks. These findings suggest that SICOG provides promising foundation for exploring MLLMs cognitive self-improvement. Moreover, our analyses confirm Chain-of-Descriptions effectiveness in fostering systematic perception, highlighting its potential for perception-intensive tasks such as embodied navigation."
        },
        {
            "title": "References",
            "content": "[1] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: versatile vision-language model for understanding, localization, text reading, and beyond, 2024. [3] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseekvl: Towards real-world vision-language understanding, 2024. [4] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12), November 2024. [5] Meta AI. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https: //ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/, 2024. [6] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [7] Yanpeng Sun, Jing Hao, Ke Zhu, Jiang-Jiang Liu, Yuxiang Zhao, Xiaofan Li, Gang Zhang, Zechao Li, and Jingdong Wang. Descriptive caption enhancement with visual specialists for multimodal perception, 2025. [8] Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Jianhua Han, Lanqing Hong, Hang Xu, and Xiaodan Liang. Atomthink: slow thinking framework for multimodal mathematical reasoning, 2024. [9] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2025. [10] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, and Vikas Chandra. An introduction to vision-language modeling, 2024. [11] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, Quanquan Gu, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension, 2024. [12] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark, 2025. [13] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. Found. Trends Comput. Graph. Vis., 16(1-2):1214, 2024. [14] Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li. Llava-next: What else influences visual instruction tuning beyond data?, May 2024. [15] Xiao Xu, Tianhao Niu, Yuxi Xie, Libo Qin, Wanxiang Che, and Min-Yen Kan. Exploring multi-grained concept annotations for multimodal large language models, 2024. [16] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language models, 2024. [17] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie. What if we recaption billions of web images with llama-3?, 2024. [18] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale, 2024. 19 [19] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3CoT: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 81998221, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [20] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models, 2024. [21] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. [22] Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jan Kautz, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. VILA^2: VLM augmented VLM with self-improvement, 2024. [23] OpenAI. Gpt-4v(ision) system card. OpenAI blog, 2023. [24] OpenAI. Hello gpt-4o. OpenAI blog, 2024. [25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. [26] Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Pakazad, and Graham Neubig. Better instruction-following through minimum bayes risk, 2024. [27] Nasima Khatun, Prosanta Paul, Sridip Chatterjee, and Sudip Sundar Das. Physical activity and neurocognitive health: narrative review. INTERNATIONAL JOURNAL OF RESEARCH PEDAGOGY AND TECHNOLOGY IN EDUCATION AND MOVEMENT SCIENCES, 12(02):94101, May 2023. [28] Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxin Huang, Zhang Li, Guozhi Tang, Bin Shan, Chunhui Lin, Qi Liu, Binghong Wu, Hao Feng, Hao Liu, Can Huang, Jingqun Tang, Wei Chen, Lianwen Jin, Yuliang Liu, and Xiang Bai. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning, 2024. [29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. [30] Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, and Meng Cao. From scarcity to efficiency: Improving CLIP training via visual-enriched captions, 2024. [31] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world, 2023. [32] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 25562565. Association for Computational Linguistics, 2018. [33] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. [34] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts, 2021. [35] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions, 2023. [36] Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, and Meng Cao. Veclip: Improving clip training via visual-enriched captions, 2024. 20 [37] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. [38] Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. Vision-language models can self-improve reasoning via reflection, 2024. [39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [40] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023. [41] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models, 2024. [42] Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, and Wai Lam. Large language models can self-improve in long-context reasoning, 2024. [43] Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and Lifu Huang. Vision-flan: Scaling human-labeled tasks in visual instruction tuning, 2024. [44] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. [45] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. In European Conference on Sharegpt4v: Improving large multi-modal models with better captions. Computer Vision, pages 370387. Springer, 2024. [46] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [47] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, and Guilin Liu. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv:2408.15998, 2024. [48] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [49] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [50] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. [51] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2668926699, June 2024. [52] Haodong Duan, Xinyu Fang, Junming Yang, Xiangyu Zhao, Yuxuan Qiao, Mo Li, Amit Agarwal, Zhe Chen, Lin Chen, Yuan Liu, Yubo Ma, Hailong Sun, Yifan Zhang, Shiyin Lu, Tack Hwa Wong, Weiyun Wang, Peiheng Zhou, Xiaozhe Li, Chaoyou Fu, Junbo Cui, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2025. [53] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models?, 2024. 21 [54] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2024. [55] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023. [56] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), December 2024. [57] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. Document collection visual question answering. In ICDAR 2021, 2021. [58] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv:2203.10244, 2022. [59] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. [60] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022. [61] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. [62] x.ai. Grok-1.5 vision preview. Grok-1.5v blog, 2024. [63] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. CoRR, abs/2304.08485, 2023. [64] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images, 2024. [65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 2021. [66] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. [67] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal 22 Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo 23 Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. [68] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants. 2023. [69] Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Llava-uhd v2: an mllm integrating highresolution feature pyramid via hierarchical window transformer, 2024. [70] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, and Fei Huang. UReader: Universal OCR-free visuallysituated language understanding with multimodal large language model. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 28412858, Singapore, December 2023. Association for Computational Linguistics. [71] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1288812900. PMLR, 2022. [72] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [73] Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. Self-alignment for factuality: Mitigating hallucinations in LLMs via self-evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19461965, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [74] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022. [75] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey, 2024. [76] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. CoRR, abs/2405.17428, 2024. 24 Figure 11: The SICOG framework comprises four steps: (i) Developing multimodal cognitive capabilities by finetuning an MLLM with minimal annotated image-captioning data (with Chainof-Description) and visual instruction-tuning data (with structured Chain-of-Thought), enhancing systematic perception and reasoning (upper left). (ii) Generating candidate captions and responses for pre-training by sampling from the improved models (upper right). (iii) Curating self-generated pre-training data through self-consistency-guided quality evaluation, selecting the most consistent candidates for refinement (lower right). (iv) Constructing next-generation foundation MLLM by performing self-refinement via multimodal pre-training with the curated data (lower left)."
        },
        {
            "title": "A Comprehensive Overview of SICOG",
            "content": "At its core, SICOG aims to develop MLLMs that go beyond simple visual recognition (what to see) and visual question answering (what to answer) to achieve systematic visual understanding (how to observe) and in-depth reasoning (how to think) through multimodal pre-training. To accomplish this, SICOG: (i) enhances systematic visual understanding by enabling the active acquisition of visual information from an image through our proposed Chain-of-Description (CoD), and (ii) strengthens in-depth reasoning with structured Chain-of-Thought (CoT) approach to effectively process and utilize acquired multimodal information for deriving accurate answers through self-learning. As illustrated in Figure 2, SICOG operates through four key steps. Step 1: Developing systematic multimodal cognitive capabilities with minimal annotated data. We first equip given MLLM M, parameterized by θ, with systematic perception and reasoning abilities while using minimal annotated multimodal data. This involves fine-tuning the model to 25 develop structured, multi-step perception and reasoning chains, enabling it to systematically process and integrate multimodal information. Specifically, this step consists of the following two core components: Systematic multimodal perception. To enhance the MLLMs ability to systematically observe and interpret images, we fine-tune using combination of image-captioning datasets, yielding model with improved perception, MPerception . Specifically, these datasets consist of images v, prompts x, intermediate step-by-step analyses s, and descriptions y, structured in two formats of captions: Detailed Description (DD) and Chain-of-Description (CoD) (see Section 3.2 for details on the Chain-of-Description strategy and data collection.) 0 DPerception = DPerception DD + DPerception CoD = {(vi, xi, yi)}N i=1 + {(vi, xi, si, yi)}N i=1, (11) where is the number of samples in each dataset. The model is fine-tuned using the following objective, improving its multimodal perception:"
        },
        {
            "title": "MPerception",
            "content": "0 Jθ(DPerception) = (cid:88) [log pθ(y v, x) + log pθ(s, v, x)] (12) i= Systematic multimodal reasoning. Similarly, to strengthen the models systematic and in-depth reasoning capabilities, we fine-tune using mix of visual instruction-tuning datasets, yielding model with enhanced reasoning, MReasoning . These datasets consist of images v, questions q, intermediate step-by-step rationales r, and answers a, structured in two formats of responses: Direct Answer (DA) and Chain-of-Thought (CoT) (see Section 3.3 for details on data curation). 0 DReasoning = DReasoning DA + DReasoning CoT = {(vi, qi, ai)}M i=1 + {(vi, qi, ri, ai)}M i=1, (13) where is the number of samples in each dataset. The model is fine-tuned using the following objective, fostering its multimodal reasoning: MReasoning Jθ(DReasoning) = (cid:88) i=1 [log pθ(a v, q) + log pθ(r, v, q)] (14) Step 2: Generating candidate captions and responses for pre-training data collection. Next, we construct multimodal pre-training data by leveraging the improved models, MPerception and MReasoning , 0 to generate candidate image captions and visual instruction responses. Additionally, to mitigate potential degradation of the MLLMs language capabilities during multimodal pre-training, we prompt the backbone large language model (LLM), MLLM , to generate candidate responses for text-only instructions. 0 This step consists of three key components: Image caption candidate generation. Given collection of unlabeled images {vk}K k=1, we ) using two types of prompts to generate detailed prompt MPerception (with policy pMPerception descriptions and induce Chain-of-Description perception: 0 1. Please generate detailed caption of this image. Be as descriptive as possible. (xDD). 2. Please generate detailed caption of this image. Describe the image step by step. (xCoD). Specifically, for given image vk, the model MPerception captions via sampling: 0 generates multiple candidate {ˆyk} pMPerception {(ˆsk, ˆyk)} pMPerception 0 ( vk, xDD), ( vk, xCoD), (15) 26 where {ˆyk} represents the set of detailed descriptions, and {(ˆsk, ˆyk)} represents the set of step-by-step analyses along with corresponding detailed descriptions. This results in collection of candidate image captions in two formats:"
        },
        {
            "title": "DPerception\nCand",
            "content": "= {(vk, xDD, {ˆyk})}K k=1 + {(vk, xCoD, {(ˆsk, ˆyk)})}K k=1. (16) Visual instruction candidate response generation. Similarly, given collection of unz=1 with corresponding questions qz, we prompt MReasoning (with ) using two types of prompts to generate direct answers (DA) and induce labeled images {vz}Z policy pMReasoning Chain-of-Thought (CoT) reasoning: 0 0 1. <original question>. (qDA). 2. <original question> Answer the question step by step. (qCoT ). Specifically, for given image vz and corresponding question qz, the model MReasoning generates multiple candidate responses: 0 {ˆaz} pMReasoning {(ˆrz, ˆaz)} pMReasoning 0 ( vz, qDA), ( vz, qCoT ), (17) where {ˆaz} represents the set of direct answers, and {(ˆrz, ˆaz)} represents the set of step-bystep rationales along with corresponding answers. This results in collection of candidate visual instruction responses in two formats: DReasoning Cand = {(vz, qDA, {ˆaz})}Z z=1 + {(vz, qCoT , {(ˆrz, ˆaz)})}Z z=1. (18) Text-only instruction candidate response generation. To maintain language capabilities, we generate textual instruction responses using the backbone LLM, MLLM (with policy pMLLM ), based on collection of unlabeled text prompts {xt}T Specifically, for given prompt xt, the model MLLM generates set of candidate responses: t=1. resulting in collection of candidate textual instruction responses: {ˆyt} pMLLM ( xt), DLanguage Cand = {(xt, {ˆyt})}T t=1. (19) (20) Step 3: Curating self-generated pre-training data via self-consistency-guided quality evaluation. To ensure the quality of self-generated pre-training data, we employ self-consistency to evaluate candidate samples without external supervision. This method is based on the principle that higherquality candidates exhibit greater semantic consistency [26]. The most consistent candidates are selected for further self-refinement during multimodal pre-training. Specifically, we apply self-consistency-guided evaluation function, (). For each instance (e.g., an unlabeled image), it assesses the quality of candidates (e.g., candidate captions) by comparing each candidate against all others based on semantic similarity and selects the candidate with the highest consistency score, provided it exceeds predefined threshold τ (i.e., otherwise, the instance and its candidates are skipped): ({c}) = arg max c{c} 1 Ncand Ncand(cid:88) j=1 sim(c, c(j)), s.t. max c{c} 1 Ncand Ncand(cid:88) j= sim(c, c(j)) τ, (21) where Ncand is the number of candidate samples being compared, and {c} is the candidate set. We apply this method as follows: Self-generated image caption curation. For each image vk, we apply () to assess the quality of candidate captions by comparing each generated description in the caption against all others. The most consistent caption is selected as the final self-generated caption: ˆyselected (ˆsselected, ˆyselected) = ({ˆyk} {(ˆsk, ˆyk)}) = arg max y{ˆyk}{(ˆsk,ˆyk)}"
        },
        {
            "title": "1\nNcand",
            "content": "Ncand(cid:88) j=1 sim(y, y(j)), s.t. max y{ˆyk}{(ˆsk,ˆyk)}"
        },
        {
            "title": "1\nNcand",
            "content": "Ncand(cid:88) j=1 sim(y, y(j)) τ Perception. (22) where Ncand = {ˆyk} {(ˆsk, ˆyk)} is the total number of candidate captions for each image. The curated dataset of self-generated image captions is:"
        },
        {
            "title": "DPerception",
            "content": "Selected = {(v, xDD, ˆyselected) (v, xCoD, ˆsselected, ˆyselected)}K Self-generated visual instruction response curation. Similarly, for each image vz and corresponding question qz, we apply () to evaluate candidate responses, selecting the most consistent response: k=1. (23) ˆaselected (ˆrselected, ˆaselected) = ({ˆaz} {(ˆrz, ˆaz)}) = arg max a{ˆaz}{(ˆrz,ˆaz)} 1 Ncand Ncand(cid:88) j=1 sim(a, a(j)), s.t. max a{ˆaz}{(ˆrz,ˆaz)} 1 Ncand Ncand(cid:88) j=1 sim(a, a(j)) τ Reasoning. (24) where Ncand = {ˆaz} {(ˆrz, ˆaz)} is the total number of candidate responses for each question. The curated set of self-generated visual instruction responses is: DReasoning Selected = {(vz, qDA, ˆaselected) (vz, qCoT , ˆrselected, ˆaselected)}Z Self-generated text-only instruction response curation. Similarly, for each prompt xt, we z=1. (25) apply () to evaluate candidate responses, selecting the most consistent response: ˆyt-selected = ({ˆyt}) = arg max yt{ˆyt} 1 Ncand Ncand(cid:88) j=1 sim(yt, y(j) ) s.t. max yt{ˆyt} 1 Ncand Ncand(cid:88) j=1 sim(yt, y(j) ) τ Language. (26) where Ncand = {ˆyt} is the total number of candidate responses for each question. The curated set of self-generated textual instruction responses is: DLanguage Selected = {(xt, ˆyt-selected)}T t=1. Finally, we obtain the curated self-generated multimodal pre-training dataset: DPre-training = DPerception Selected + DReasoning Selected + DLanguage Selected . (27) (28) Step 4: Constructing the next-generation MLLM through multimodal pre-training. To build the next-generation foundation MLLM, we introduce an intermediate multimodal pre-training stage, Stage 1.5, within the standard two-stage training strategy, following [6, 14]. This stage refines the MLLM using curated self-generated pre-training data. The complete training strategy consists of three stages, as shown in the lower left part in Figure 2: 28 Figure 12: Illustration of Chain-of-Description (left) for enhancing systematic perception and structured Chain-of-Thought (right) for strengthening reasoning capabilities. Stage 1: Modality alignment. In this stage, image features are aligned with the text embedding space. Following [14], we train only the vision-language (VL) connector using image-text pairs from DAlignment, while keeping the vision encoder (e.g., vision transformer) and large language model (LLM) frozen. Stage 1.5: Multimodal pre-training for self-refinement. The model undergoes training with curated self-generated pre-training data DPre-Training to acquire multimodal knowledge from these self-generated samples and internalize its systematic multimodal perception and reasoning abilities. During this stage, all model components are fully trainable. Stage 2: Visual instruction-tuning. In the final stage, the model is fine-tuned using instruction-tuning data DInstruction-Tuning to develop robust visual instruction-following capabilities, with all model components fully trainable. This three-stage training process is formulated as follows, resulting in the next-generation foundation MLLM with self-improved cognition MNext: (DAlignment) ϕ M1 LStage 1 M1.5 LStage 1.5 MNext LStage 2 ϕ ϕ (DInstruction-Tuning). (DPre-training) (29) The complete training process is summarized in Algorithm 1, with implementation details in Section 4.1 and Appendix B. 29 Algorithm 1 SICOG: Self-Learning Framework for Systematic Multimodal Cognition 1: Input: Pretrained MLLM M, parameterized by θ 2: Systematic perception data: DPerception = DPerception Systematic reasoning data: DReasoning = DReasoning Default alignment data: DAlignment, instruction-tuning data: DInstruction-Tuning Unlabeled data: (1) unlabeled image sets {vk} with prompts x, (2) unlabeled image sets {vz} + DPerception CoD + DReasoning CoT DD DA 3: 4: 5: with questions q, (3) unlabeled text-only prompts xt 6: Goal: Enable systematic visual understanding and reasoning via self-learning 7: M0 8: for = 1, . . . , do 9: 10: Step 1: Systematic Multimodal Cognitive Training Fine-tune perception, reasoning models: # Initialize model # Iterative foundation MLLM update, when applicable"
        },
        {
            "title": "MPerception",
            "content": "n1 Jθ(DPerception), MReasoning n1 Jθ(DReasoning) 11: 12: Step 2: Generating Candidate Captions and Responses Generate image captions: # Foster multimodal perception {ˆyk}, {(ˆsk, ˆyk)} pMPerception n1 ( vk, x) 13: Generate visual instruction responses: # Enhance multimodal reasoning {ˆaz}, {(ˆrz, ˆaz)} pMReasoning n1 ( vz, q) Generate text-only responses: # Maintain language {ˆyt} pMLLM ( xt) Step 3: Self-Consistency Selection Select the optimal candidates based on self-consistency, using the predefined threshold τ : 14: 15: 16: DPerception Selected arg max DReasoning Selected arg max sim(y, y(j)) s.t. max sim(a, a(j)) s.t. max DLanguage Selected arg max yt sim(yt, y(j) ) s.t. max yt (cid:88) (cid:88) (cid:88) 1 1 1 (cid:88) (cid:88) (cid:88) sim(y, y(j)) τ Perception sim(a, a(j)) τ Reasoning sim(yt, y(j) ) τ Language 17: Construct refined pre-training dataset: Selected + DLanguage Step 4: Constructing the Next-Generation Foundation MLLM Selected + DReasoning DPre-training = DPerception Selected Stage 1: Modality Alignment 18: 19: M1 LStage 1 ϕ (DAlignment) 20: Stage 1.5: Multimodal Pre-Training # Pre-train on curated self-generated data M1.5 LStage 1. ϕ (DPre-training) 21: Stage 2: Visual Instruction-Tuning MNext LStage ϕ (DInstruction-Tuning) 22: end for 23: Output: Next-generation foundation MLLM with self-improved cognition MNext"
        },
        {
            "title": "B Implementation Details",
            "content": "Three Caption Formats for Chain-of-Description Conversion in Multimodal Pre-Training Format 1: Standard caption-only. Human: <image>nPlease provide thorough description of this image. Model: The image showcases modern spiral staircase with unique design. The staircase features curved, wooden handrail that is dark brown in color, contrasting with the lighter wooden steps. The railings are made of the same dark wood and have horizontal slat design, giving sense of movement along the staircases curve. The staircase itself appears to be constructed from polished wood, giving it sleek and contemporary look. Below the staircase, the flooring is light beige, complementing the warm tones of the wooden staircase. The room surrounding the staircase is spacious, with large windows that allow natural light to enter, enhancing the brightness of the space. There are no people or objects in the immediate vicinity of the staircase, emphasizing the architectural features. The overall color scheme is warm, with the wood tones being the dominant feature. Format 2: Step-by-step elicitation. Human: <image>nPlease provide thorough description of this image. Explain your description step by step. Model: Step 1: Extract salient content by identifying the key elements that define the image. The image depicts modern spiral staircase with striking design. The staircase features curved, wooden handrail that is dark brown in color, contrasting with the lighter wooden steps. The railings are made of the same dark wood and have horizontal slat design, providing sense of movement along the staircases curve. Step 2: Analyze detailed information, focusing on instance-level attributes such as low-level and fine-grained details. The staircase is constructed from polished wood, which gives it sleek and contemporary appearance. The flooring below the staircase is light beige, complementing the warm tones of the wooden staircase. The room surrounding the staircase is spacious, with large windows that allow natural light to enter, enhancing the brightness of the space. Step 3: Consider relational-level attributes, analyzing interactions between elements and their spatial organization. The staircases design is both functional and aesthetic, with its spiral form creating dynamic visual element within the room. The natural light from the windows highlights the warm tones of the wood and adds depth to the space. The absence of people or objects in the room emphasizes the architectural features of the staircase. Step 4: Examine marginal or peripheral content to ensure no important information is missed. The rooms architecture is modern, with clean lines and minimalistic design elements. The combination of the staircases curved form and the natural light creates an inviting and open atmosphere in the space. Step 5: Organize all observations into detailed, cohesive description. The image showcases modern spiral staircase with unique design. The staircase features curved, wooden handrail that is dark brown in color, contrasting with the lighter wooden steps. The railings are made of the same dark wood and have horizontal slat design, giving sense of movement along the staircases curve. The staircase itself appears to be constructed from polished wood, giving it sleek and contemporary look... Table 9: Three different caption formats used for Chain-of-Description conversion during the multimodal pre-training stage (1/2). (i) Implementing Step 2 of SICOG: To collect pre-training data, we prompt the model multiple times to sample candidate outputs using temperature of 0.7 and top-p of 0.95: For image captioning, we sample three candidate captions (two in Chain-of-Description format and one as detailed description). For visual instruction tuning, we sample three candidate responses (two in chain-of-thought (CoT) format and one as direct answer). For text-only instruction tuning, we sample three candidate responses. (ii) Implementing Step 3 of SICOG: To ensure the quality of self-generated pre-training data, we use NV-Embed-v2 [76] to generate candidate embeddings and calculate their semantic similarity. 31 Three Caption Formats for Chain-of-Description Conversion in Multimodal Pre-Training Format 3: Multi-turn conversation. Human: <image>nWhat are the crucial details that define the image? Model: The image depicts modern spiral staircase with striking design. The staircase features curved, wooden handrail that is dark brown in color, contrasting with the lighter wooden steps. The railings are made of the same dark wood and have horizontal slat design, providing sense of movement along the staircases curve. Human: Can you analyze the image for instance-level attributes and low-level details? Model: The staircase is constructed from polished wood, which gives it sleek and contemporary appearance. The flooring below the staircase is light beige, complementing the warm tones of the wooden staircase. The room surrounding the staircase is spacious, with large windows that allow natural light to enter, enhancing the brightness of the space. Human: What is the relationship between the components, and how are they arranged? Model: The staircases design is both functional and aesthetic, with its spiral form creating dynamic visual element within the room. The natural light from the windows highlights the warm tones of the wood and adds depth to the space. The absence of people or objects in the room emphasizes the architectural features of the staircase. Human: Is there anything in the margins or borders of the image worth noting? Model: The rooms architecture is modern, with clean lines and minimalistic design elements. The combination of the staircases curved form and the natural light creates an inviting and open atmosphere in the space. Human: How would you describe the image in well-organized and cohesive manner? Model: The image showcases modern spiral staircase with unique design. The staircase features curved, wooden handrail that is dark brown in color, contrasting with the lighter wooden steps. The railings are made of the same dark wood and have horizontal slat design, giving sense of movement along the staircases curve. The staircase itself appears to be constructed from polished wood, giving it sleek and contemporary look... Table 10: Three different caption formats used for Chain-of-Description conversion during the multimodal pre-training stage (2/2). Model LLaVA-Qwen2-7B LLaVA-Qwen2-7B-UHD LLaVA-Llama3.1-8B-UHD Training Stage Stage 1 Stage 1.5 Stage 2 Stage 1 Stage 1.5 Stage 2 Stage 1 Stage 1.5 Stage 2 Learning Rate Batch Size Sequence Length Number of Epochs 2e-4 256 4096 1 2e-5 128 4096 1 2e-5 128 4096 1 2e-4 256 4096 1 2e-5 128 4096 1 2e-5 128 4096 2e-4 256 4096 1 2e-5 128 4096 1 2e-5 128 4096 1 Table 11: Hyperparameters for implementing SICOG in Step 4 (Section 3). The data is curated based on similarity scores and predefined thresholds. Specifically, we apply the following curation strategies: Curation of Self-Generated Image Captioning Data: We set the similarity threshold τ Perception = 0 and retain all top-1 ranked self-generated captions in mixture of two formats: detailed descriptions and Chain-of-Description. To preserve the MLLMs multiturn conversational ability, we convert the top-1 ranked Chain-of-Description captions with consistency score higher than 0.85 into multi-turn conversational format. Detailed examples are provided in Tables 9 and 10. In addition, Table 12 presents comparison of two perception-enhancement approaches for SICOG: (i) fine-tuning with 35 detailed descriptions and (ii) Chain-of-Description in three formats (standard caption-only, step-by-step elicitation, and multi-turn conversation, as shown in Tables 9 and 10). The results suggest: (1) the efficacy of the proposed Chain-ofDescription approach in enhancing richer visual understanding. (2) the impact of the three different Chain-of-Description formats on the overall performance of SICOG. Curation of Self-Generated Visual Instruction-Tuning Data: We set the similarity threshold τ Perception = 0.95 and retain all top-1 ranked self-generated responses in two formats: direct answers and chain-of-thought (CoT). 32 Curation of Self-Generated Text-Only Instruction-Tuning Data: We set the similarity threshold τ Perception = 0.8 and retain only the first 50k text-only prompt-response pairs based on their similarity score rankings. Table 11 provides the detailed hyperparameters used for implementing SICOG in Step 4 (Section 3). The same hyperparameters applied in Stage 2 are also used to develop the MLLMs capabilities in Step 1. Method Base Model LLaVA-Qwen2-7B-UHD SICOG-LLaVA-Qwen2-7B-UHD (Perception) Perception Enhancement Train Data Comprehensive Hallu. Chart/Table Knowledge Stage 1. MMBen. MMStar POPE DocV. Chart. Math. Science. AI2D - Fintune w/ 35k DD Fintune w/ 35k CoD - 77. 48.93 87.31 70.18 69.96 38.90 77.29 74.94 Self-generated 118k caption w/ DD Self-generated 118k caption w/ CoD (caption) Self-generated 118k caption w/ CoD (multi.) Self-generated 118k caption w/ CoD (conv.) Self-generated 118k caption w/ CoD (caption, multi., conv.) 77.19 50.67 85.74 71.43 71.96 37.70 78.14 75. 78.25 50.93 86.80 71.48 72.12 40.30 77.84 76.81 78.48 50.20 86.22 71.74 72.52 41.20 76.95 76. 78.53 50.87 87.19 71.67 72.16 40.00 78.24 76.62 77.52 50.60 86.86 71.84 72.32 41.60 77.44 76. Table 12: Comparison of two perception-enhancement approaches for SICOG: (i) fine-tuning with 35 detailed descriptions and (ii) Chain-of-Description in three formats (as shown in Tables 9 and 10). Refer to Table 7 for detailed discussion. The self-generated caption data used are sampled only once, without filtering. Figure 13: Illustration of (a) the original image and (b) the four types of image corruption. 33 Prompt Used by GPT-4o for eliciting Chain-of-Description You are an expert AI assistant tasked with analyzing an image and generating detailed, stepby-step description. You are provided with an original description as reference. Your goal is to ensure accuracy, clarity, and logical progression in your response. Follow these guidelines: Guidelines: 1. Ensure Comprehensive Coverage: Identify and include all relevant details visible in the image. Avoid unnecessary repetition or irrelevant information. 2. Avoid Adding Imaginary Details: Base your reasoning strictly on what is visible in the image or provided in the description. Do not include fabricated or unverifiable details. 3. Incorporate Relevant Context: Add factual, relevant context to enhance understanding where appropriate, but ensure it aligns strictly with the visible or provided content. 4. Prevent Inaccuracies: Stick to the given data. Avoid assumptions or deviations from the available evidence. Step-by-Step Process: Step 1: Extract salient content by identifying the key elements that define the image. Example: The image is monochrome photocopy of document that appears to be page of meeting or project notes. It contains both typed and handwritten text, with focus on tasks and progress updates related to paper-related issues. The document includes reference number at the bottom and source URL. Step 2: Analyze detailed information, focusing on instance-level attributes such as low-level and fine-grained details. Example: The document lists several tasks, such as checking with \"KC\" on the possibility of putting bands \"long-ways,\" which is marked as \"In progress.\" Other tasks include checking on \"shrinking\" paper, which is also \"In progress,\" and checking the commercial viability of banded papers, marked as \"Okay.\" There are handwritten notes and checks next to some points, indicating their status. Step 3: Consider relational-level attributes, analyzing interactions between elements and their spatial organization. Example: The tasks are organized in list format, with some items having associated handwritten notes that indicate completion or ongoing status. The name \"Jimmy Wu\" is associated with an action item regarding DC work request with KC banded papers, awaiting approval for banded additives. The document also mentions running \"GPC KS and KOOL KS on RIP-4 (LCC)\" and notes that KC is running \"cross-hatch\" papers. Step 4: Examine marginal or peripheral content to ensure no important information is missed. Example: The document specifies that the next meeting is scheduled for Monday, February 7, at 9:00 a.m. in the International Conference Room. The reference number \"584100571\" is located at the bottom of the page, and the source URL is included at the bottom. Step 5: Organize all observations into detailed, cohesive description. Example: The image is monochrome photocopy of document that appears to be page of meeting or project notes, containing both typed and handwritten text. The document lists several tasks related to paper-related issues, such as checking with \"KC\" on the possibility of putting bands \"long-ways,\" which is marked as \"In progress,\" and checking the commercial viability of banded papers, marked as \"Okay.\" Handwritten notes and checks next to some points indicate their status. The name \"Jimmy Wu\" is associated with an action item regarding DC work request with KC banded papers, awaiting approval for banded additives. Other items include running \"GPC KS and KOOL KS on RIP-4 (LCC)\" and KC running \"cross-hatch\" papers. The next meeting is scheduled for Monday, February 7, at 9:00 a.m. in the International Conference Room. The document is marked with reference number \"584100571\" at the bottom, and source URL is included. Important Notes: - **Steps 14**: Write concise observations in one or two sentences each. - **Step 5**: Summarize all observations into detailed paragraph or two, as descriptive as necessary. Input: <image> Question: Could you please transcribe the image into descriptive paragraph? Explain your description step-by-step. Original description: <caption> Output: Table 13: The prompt utilized by GPT-4o for eliciting Chain-of-Description for image-captioning training datasets. 34 Prompt Used by GPT-4o to Evaluate Image Caption Quality You are an expert AI assistant tasked with evaluating the quality of captions for images. Your job is to assess the captions quality based on specific criteria and provide clear, concise critique followed by structured evaluation scores. Ensure your response follows the exact format below and adheres to the evaluation criteria. Evaluation Process: 1. Critique First: Begin by generating concise critique of the caption. Highlight both its strengths and weaknesses in plain language. Focus on how well the caption describes the image and aligns with the criteria. 2. Score Each Criterion: After the critique, provide score for each evaluation criterion on scale from 1 to 5. Ensure the scores are consistent with the critique and avoid contradictions. Evaluation Criteria: Evaluate the caption based on the following eight dimensions: 1. Salient Content: Does the caption highlight the key elements and most important details of the image? 2. Fine-Grained Details: Does the caption include specific attributes, such as textures, colors, or text found in the image? 3. Relational Attributes: Does the caption describe interactions or spatial relationships between elements in the image? 4. Peripheral Content: Does the caption include additional relevant details that enhance completeness without being redundant? 5. Faithfulness: Does the caption accurately describe what is visible in the image without adding imaginary or false information? 6. World Knowledge: Does the caption incorporate relevant world knowledge, such as context or implied meaning, to enhance its coherence? Scoring Rubric: - **Poor (1)**: Fails to meet the criterion. - **Fair (2)**: Partially meets the criterion but has noticeable shortcomings. - **Average (3)**: Adequately meets the criterion but lacks depth or sophistication. - **Good (4)**: Strongly aligns with the criterion and demonstrates nuanced understanding. - **Excellent (5)**: Perfectly aligns with the criterion with high-quality descriptions. Output Format: Follow this structured format exactly: 1. **Critique**: Write concise critique (2-4 sentences), summarizing the strengths and weaknesses. 2. **Scores**: Provide score for each dimension using the following format: - Salient Content: Score = [[Your Score]] - Fine-Grained Details: Score = [[Your Score]] - Relational Attributes: Score = [[Your Score]] - Peripheral Content: Score = [[Your Score]] - Faithfulness: Score = [[Your Score]] - World Knowledge: Score = [[Your Score]] Important Notes: - Focus solely on the captions alignment with the criteria and the image content. - Be objective, impartial, and avoid introducing biases. - Do not let the captions length, style, or structure influence your evaluation unless it directly affects quality. - Ensure your response is clear, concise, and follows the exact output format. Input: <image> Caption: <caption> Output: Table 14: The prompt utilized by GPT-4o for evaluating the quality of re-captioned data. 35 Image Captioning Data for Enhancing Systematic Multimodal Perception Example with Detailed Description Caption Format Prompt: Please provide vivid description of the image. Caption: The image features person with long, red hair sitting cross-legged on bed, playing an acoustic guitar. They are wearing dark blue shirt with rolled-up sleeves and black pants, appearing focused on the instrument. The guitar is classic wooden brown with light-colored frets. In front of them, an open laptop on small table or stand suggests they might be following music or tutorial. The room exudes warm ambiance with soft lighting, and the colorful bedspread adds vibrancy. Decorations like string lights and wall hangings contribute to bohemian aesthetic, while dresser with various personal items in the background adds personal touch to the space. Example with our proposed Chain-of-Description Caption Format Prompt: Please provide vivid description of the image. Describe the image step-by-step. Caption: Step 1: Extract salient content by identifying the key elements that define the image. The image depicts person with long, red hair sitting cross-legged on bed, playing an acoustic guitar. The person is dressed in dark blue shirt with rolled-up sleeves and black pants. An open laptop is positioned on small table or stand in front of them, suggesting they might be using it for guidance, such as following music sheets or tutorial. Step 2: Analyze detailed information, focusing on instance-level attributes such as low-level and fine-grained details. The guitar is classic wooden brown with light-colored frets. The room has warm ambiance, enhanced by soft lighting. The bedspread is colorful, adding vibrancy to the scene. Decorations such as string lights and wall hangings contribute to bohemian aesthetic. In the background, there is dresser with various personal items, adding to the lived-in feel of the space. Step 3: Consider relational-level attributes, analyzing interactions between elements and their spatial organization. The person is seated on the bed, with the guitar resting on their lap, indicating engagement with the instrument. The laptops placement suggests it is being actively used, possibly for learning or playing along with music. The string lights and wall hangings are positioned to create cozy and artistic environment. Step 4: Examine marginal or peripheral content to ensure no important information is missed. The rooms decor, including the string lights and wall hangings, adds personal touch and suggests creative or artistic personality. The dresser in the background, with its assortment of items, further personalizes the space. Step 5: Organize all observations into detailed, cohesive description. The image features person with long, red hair sitting cross-legged on bed, playing an acoustic guitar. They are wearing dark blue shirt with rolled-up sleeves and black pants, appearing focused on the instrument. The guitar is classic wooden brown with light-colored frets. In front of them, an open laptop on small table or stand suggests they might be following music or tutorial. The room exudes warm ambiance with soft lighting, and the colorful bedspread adds vibrancy. Decorations like string lights and wall hangings contribute to bohemian aesthetic, while dresser with various personal items in the background adds personal touch to the space. Table 15: An example from the image captioning training dataset curated to elicit systematic multimodal perception. 36 Visual Instruction-Following Data for Enhancing Systematic Multimodal Reasoning Example 1: Open-Ended Problem Example with Direct Answer Response Format Question: In the right triangle ABC, = 90, point is point on AC, DE AB at point E, AB = 10, BC = 6, DE = 2.4, then the length of AD is ( ) A. 1.2 B. 3 C. 4 D. 5 Response: Example with Chain-of-Thought Response Format Question: In the right triangle ABC, = 90, point is point on AC, DE AB at point E, AB = 10, BC = 6, DE = 2.4, then the length of AD is ( ) A. 1.2 B. 3 C. 4 D. 5 Answer the question step by step. Response: Step 1: Clarify the task objective. To solve this problem, will use the properties of similar triangles and the known measurements provided. By identifying similar triangles, can set up proportion to find the length of AD. Step 2: Extract the crucial visual information from the image. The image shows right triangle ABC with = 90. Point is on segment AC, and DE is perpendicular to AB at point E. Measurements are given as AB = 10, BC = 6, and DE = 2.4. Step 3: Generate detailed reasoning to solve the task. In the right triangle ABC, = 90. DE is perpendicular to AB, making AED also equal to 90. Because both triangles ADE and ABC share angle A, triangle ADE is similar to triangle ABC by AA similarity (Angle-Angle). Using the similarity of triangles ADE and ABC, we can set up the proportion: AD DE = AB BC Substituting the known values: Simplifying the right side: Thus, the proportion becomes: Solving for AD: AD 2.4 = 10 6 10 6 = 5 3 AD 2.4 = 5 3 AD = 5 2.4 = 4 Step 4: Conclude the task with an answer. Table 16: An example from the visual instruction-following training dataset designed to enhance systematic multimodal reasoning. 37 Visual Instruction-Following Data for Enhancing Systematic Multimodal Reasoning Example 2: Multiple-Choice Problem Example with Direct Answer Response Format Question: Why is the person in the air? A. Grabbing frisbee B. Falling C. Angry D. Bouncing Response: Example with Chain-of-Thought Response Format Question: Why is he in the air? A. grab frisbee B. falling C. angry D. bouncing Answer the question step by step. Response: Step 1: Clarify the task objective. will analyze the image to determine why the person is in the air by examining relevant visual clues. Ill then follow reasoning process to arrive at the correct answer. Step 2: Extract the crucial visual information from the image. The image shows an indoor sports setting with person in mid-air holding an object that resembles frisbee. The individual is wearing athletic gear, and there is goal visible in the background, suggesting sports activity. Step 3: Generate detailed reasoning to solve the task. Considering the context and the object in hand, it appears the person is actively participating in sport involving frisbee. The position of the person suggests they might be jumping to grab or throw the frisbee. The presence of goal could imply that the activity requires throwing the frisbee accurately, which aligns with the action of jumping. Step 4: Conclude the task with an answer. Table 17: An example from the visual instruction-following training dataset designed to enhance systematic multimodal reasoning."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong",
        "The University of Hong Kong",
        "Tsinghua University",
        "Xian Jiaotong University"
    ]
}