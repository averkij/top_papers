{
    "paper_title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
    "authors": [
        "Pablo Ruiz-Ponce",
        "German Barquero",
        "Cristina Palmero",
        "Sergio Escalera",
        "José García-Rodríguez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 9 1 0 1 0 . 4 0 5 2 : r MixerMDM: Learnable Composition of Human Motion Diffusion Models Pablo Ruiz-Ponce1, German Barquero2, Cristina Palmero3, Sergio Escalera2, Jose Garcıa-Rodrıguez1 1Universidad de Alicante, Spain 2Universitat de Barcelona and Computer Vision Center, Spain 3Kings College London, UK pruiz@dtic.ua.es https://pabloruizponce.com/papers/MixerMDM Figure 1. We introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. MixerMDM has demonstrated consistent ability to generate highly controllable human interactions by combining model that generates individual motions from textual descriptions with model that creates human-human interactions."
        },
        {
            "title": "Abstract",
            "content": "Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pretrained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine singleand multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix. 1. Introduction Creating synthetic human motion that closely mimics real individuals is important for applications such as animation [44], virtual reality [32, 34], and robotics [45]. Human motion generation techniques have experienced remarkable progress in recent years [70]. However, unlike other generative fields such as image or text where data is abundant, high-quality human motion data remains scarce and costly to obtain and annotate. Currently, there exists variety of small, specialized datasets that focus on specific ranges of motion over the entire human motion manifold (e.g., 1 [15, 37]). However, there is no consensus on the data representation format (positions/rotations, relative/global, redundancy, etc.), and the conditions to generate the motions vary widely (text, audio, other motions, scene, etc.). This heterogeneity makes it difficult to integrate such datasets to expand the distribution of possible human motions. While recent attempts have been made to create generalist models for human motion generation [65], the general trend has been the development of highly specialized models tailored to the characteristics of individual datasets. To exploit the specific capabilities of these models into new motions, we introduce MixerMDM, model composition technique that learns how to mix motions generated by specialized motion models without re-training. Mixing the distinct motions generated by these models can result in the generation of unique new motion preserving some of their specialized generative capabilities, thereby expanding the coverage of possible motions. MixerMDM is, to the best of our knowledge, the first learnable approach that dynamically mixes text-conditioned human motion diffusion models, leveraging each models unique strengths to enhance task-specific performance. Unlike previous methods [39, 41], MixerMDM performs unique mixing strategy using the mixing weights predicted by the Mixer depending on the motions generated by two pre-trained models, the conditions used to generate them, and the step in the denoising chain. Furthermore, we propose different modalities of mixing, giving MixerMDM the capability of blending motions at global, motion duration, body joint, or spatio-temporal (durationjoints) levels. As an application of our approach, in this paper we focus on the blending of single-person (i.e. individual) motions and human-human interactions generated by text-conditioned human motion diffusion models. This results in the consistent generation of unique new interactions leveraging intrapersonal capabilities of the individual model -such as diverse and precise conditioning of individual motionswith interpersonal capabilities of the interaction model -such as global positioning and orientation in multi-human scenarios. To train MixerMDM, we introduce pipeline based on adversarial losses as in Generative Adversarial Networks (GANs) [13] to take advantage of the prior knowledge embedded in the pre-trained models. The only components that undergo training are the Mixer module, which determines how to best blend the motions, and discriminator per model, that captures the particularities of each model. Since no ground truth exists for the combination of different motions, we utilize the predictions generated by the pretrained models as positive samples for their respective discriminators. In addition, the Mixer uses directly the final predictions from the pre-trained models to mix the motions, allowing it to adapt directly to the data distribution without relying on the particularities of each model architecture. This flexibility allows seamless swapping of models trained with the same dataset without additional training. Finally, we introduce an evaluation procedure to assess the mixing quality. To do so, we evaluate the alignment of the generated mixed motion and the condition used to generate it with respect to each of the distributions of the pretrained models used in the combination. We further measure the capabilities of our proposal to adapt the mixing depending on the specific motions, conditions, and denoising timestep to blend. The main contributions of this paper are as follows: We propose MixerMDM, method for dynamically mixing distinct text-conditioned human motion diffusion models while keeping their specific capabilities. As result, our approach consistently mixes individual motions and human-human interactions, achieving more finegrained individual controllability than previous methods. We introduce an adversarial training strategy using multiple discriminators. This approach improves the ability of our method to learn robust blendings as the pre-trained model outputs are used as ground truth in the absence of real one. Additionally, as the motions predicted are directly used as inputs to the Mixer, it allows to swap the pre-trained models without additional training. Given the lack of quantitative metrics for the task of mixing motions generated by different models, we propose new evaluation pipeline to assess the quality of the mixing as well as the capabilities of the model to dynamically adapt the mixing strategy during the denoising process. 2. Related Work Text-Conditioned Human Motion Generation. recent review [70] has highlighted significant advances in human motion generation. Early works focused on aligning text and motion latent spaces using Kullback-Leibler divergence loss [1, 16, 35, 47], but these methods often suffer from latent space misalignments and semantic mismatches due to limited data. Recent successes in autoregressive models, such as Large Language Models [10, 49, 67] powered by Transformers [50], have inspired new motion generation methods [17, 23, 51, 61, 68]. These methods tokenize motions into discrete codes and use Transformers to convert text tokens into motion tokens. However, tokenizing motion is complex, and autoregressive models cannot capture bidirectional dependencies. Methods like MMM [36] and MoMask [18] address this using masked attention. Diffusion models [19, 42] have become popular for generative tasks [9, 58, 69], achieving state-of-the-art results in textto-motion generation. FLAME [26] and MotionDiffusion [64] use traditional diffusion models with Transformers as noise predictors. MDM [48] employs x0 reparametrization [5, 57], allowing kinematic loss functions to improve motion quality. Other approaches incorporate physical con2 straints [59], use latent diffusion for faster sampling [8], or apply retrieval techniques [12, 63]. Despite slow inference, diffusion models produce realistic and diverse samples [14]. Text-Conditioned Human-Human Interaction Generation. In addition to single human motion generation, several recent works focus on the generation of human-human interactions using textual descriptions. ComMDM [41] extends MDM to multi-human interactions using crossattention module. Similarly, [46] uses shared crossattention module to connect two models for asymmetric interactions. InterGen [27] proposes the use of cooperative denoisers in the recently introduced InterHuman dataset. MoMat-MoGen [7] extends the retrieval diffusion model from [63] for human interactions. More recently, methods such as in2IN [39] and InterMask [22] propose further improvements to enhance the generation of human interactions, achieving state-of-the-art performance. Human Motion Composition. Mixing data of the same modality has long history in fields like image synthesis [31, 55, 60, 62], where new data can be generated that retains characteristics from multiple sources. The iterative nature of diffusion models allows for smoother mixing of such data [4, 20, 28, 29, 66]. In the context of human motion, composition can be broadly categorized into temporal and spatial approaches. Temporal composition focuses on combining individual motion sequences into larger cohesive sequence [2, 6, 41], enabling smooth and realistic transitions between different actions. By contrast, spatial composition merges multiple motions to create new motion of the same length, incorporating specific elements such as actions, trajectories, or joint movements from the original motions [3, 47]. More generally, [41] introduced model composition technique to combine the sampling processes of two different diffusion models, generating blended motion through fixed-weight mixing during the denoising process. Building on this, in2IN [39] proposed weight scheduler to adjust the mixing throughout the denoising process, resulting in more versatile and refined outputs. However, both methods require the mixing weight or scheduler to be predefined manually prior to inference. In contrast, and inspired by [20], we introduce the Mixer module capable of learning unique mixing weight for each motion at each timestep of the denoising process. While this approach incurs minor additional computational cost, it provides an adaptable mixing that effectively preserves features from both motions being combined. Adversarial Training. GANs [13] revolutionized the generative field when first introduced, employing generator and discriminator in competitive framework where the generator aims to fool the discriminator, while the discriminator is trained to distinguish real from generated data. This training paradigm deviates from typical loss functions, which directly predict the ground truth. Despite numerous advancements on this initial idea [53], GAN-based architectures have been largely supplanted by diffusion models [11]. However, the adversarial training paradigm is still applied to enhance diffusion models [24, 25, 40, 54, 56], providing guidance and improving generation quality. In our approach, we leverage this paradigm to train the module responsible for determining how two motions are mixed. Specifically, we introduce discriminator for each pretrained model, compelling the generator to deceive both discriminators and thus achieving mixing strategy that preserves the core characteristics from each model. 3. Method The goal of MixerMDM is to generate new motion sequence by combining, at each step of the denoising process, the motions generated by two models previously trained in specialized dataset (i.e., pre-trained models). MixerMDM englobes the whole pipeline (Sec. 3.1, Fig. 2) of generating the motions from the pre-trained models, determining how these particular motions are going to be mixed for this specific step of the denoising chain (Sec. 3.2), and performing the mixing (Sec. 3.3). We train the whole procedure using adversarial losses to use the specific knowledge of each pretrained model as pseudo-ground truth (Sec. 3.4). Building on the work of [39], this paper focuses on mixing an interaction model with specialized individual model. We consider interaction and individual models as text-conditioned diffusion models that generate humanhuman interactions and single-human motions from textual descriptions, respectively. The dynamics of human-human interactions can be divided into two distinct levels: intrapersonal and interpersonal. The former refers to the specific movements of an individual in isolation, whereas the latter encompasses not only the motion but also the overall trajectory and orientation of the body with respect to the other individual involved in the interaction. The goal is to combine the interpersonal capabilities of the interaction model with the intrapersonal capabilities of the individual model to be able to generate new diverse interactions with fine-grained control of the specific motions of the individuals. Nevertheless, given the characteristics of the method, it could be applied to mix any type of motion diffusion model. 3.1. Pipeline The mixed motion xm at timestep of the denoising chain is the result of combining two motion sequences {xa , xb t} generated by two distinct pre-trained models {Ma, Mb}. These two motions will be used by the Mixer (Sec. 3.2) to predict the mixing weight wt used by the Mixing procedure to blend both motions. Fig. 2 depicts this process for the specific case of mixing interaction and individual motions. The first step to adapt this general pipeline to the specific case of combining human-human interactions gener3 Figure 2. MixerMDM pipeline. At each timestep of the denoising process, mixed motion is generated by first obtaining motions from separate text-conditioned pre-trained motion diffusion models. Using these motions and their conditions, the Mixer predicts unique mixing weights that are subsequently used in the Mixing procedure to blend the generated motions and obtain the mixed motion xm . ated by Ma with individual motions generated by Mb is the duplication of Mb to generate two individual motions instead of one. Additionally, as stated in [39], whereas individual models focus on intrapersonal dynamics, interaction models place greater emphasis on interpersonal dynamics. The pre-trained models expect as input the denoised motion from the previous step. However, this motion has to be somewhere within the learned motion manifold of this model. Since in MixerMDM the inputs of the pre-trained models are the mixed motion from the previous steps, some transformations are required to match the input format of In particular, the centering function each of the models. canonicalizes motions initial global translation and orientation. Conversely, the alignment function undoes the centering by harmonizing the global positions and trajectories of two motions. It is applied to the the individual model with the trajectory and orientation of its respective individual generated by the interaction model (see sup. material). 3.2. Mixer The Mixer (see Fig. 3) is lightweight and specialized module capable of learning to combine two motions from different models. This module receives as input the two motions from the pre-trained models, the actual timestep of the denoising process, and the conditions employed by the two models to generate the motions {ca, cb}. These inputs will be processed by Transformer [50] encoder that will transform them into high-dimensional representation that then will be processed by Multi-Layer Perceptron (MLP) to output mixing weight wt. This wt will be used to blend xa and xb following the process described in Sec. 3.3. wt is vector with scalar values in the range [0, 1] with its shape determined by the distinct variations that we propose: Global [G]: One global value. Temporal [T]: One value per frame in the sequence. Spatial [S]: One value per body joint of an individual. Spatio-Temporal [ST]: One value per joint and frame. These variations allow the Mixer to have more flexibility in determining how to mix the motions. Unlike previous approaches [39, 41], with minor additional computation cost, the Mixer can predict distinct mixing weights for disparate motions allowing for dynamic and unique mixing conditioned on the particularities of each model output. 3.3. Mixing We base our mixing formula on DualMDM [39]. This model composition technique is based on DiffusionBlending [41], where two different motions can be combined using constant weight that specifies the importance of each model in the blending. DualMDM proposes to use weight scheduler that defines weight at every step of the denoising process wt to have more fine-grained control over the influence of each model at each denoising step, based on the findings of [21, 52]. This mixing formula is defined as: = xa xm + wt (xb xa ). (1) However, the DualMDM approach involves the specification of experimentally found manual and constant weight schedulers. Although this approach yields superior outcomes compared to previous methods, it lacks the capability 4 Figure 3. Mixer architecture. The Mixer is composed of Transformer encoder that takes as input both generated motions by the pre-trained models, their respective conditions, and the actual timestep of the denoising process. This encoder generates latent representation, which is decoded by an MLP that outputs the mixing weights. : number of frames of the motion sequence. to adapt the mixing weights for different motions and conditions. This is crucial aspect, as different motions generated by distinct models may necessitate unique weights depending on their specific characteristics. Instead, our Mixer module can predict dynamic mixing weights, such that: = xa xm + ixer(xa , ca, xb t, cb, t) (xb xa ). (2) 3.4. Adversarial Training The resulting motion from MixerMDM is unique mixing of two generated motions, and thus no ground truth can be employed to train the Mixer in supervised way. Inspired by [13, 21], we propose an adversarial training of the Mixer with GANs (see Fig. 4). In this particular case, we have one discriminator per each pre-trained model. The training goal for the Mixer (generator) is to be able to generate mixed motion that can fool both discriminators. These discriminators take the motions generated by the pre-trained models as positive samples (real), and the mixed motions generated by MixerMDM as negative ones (fake). In accordance with [40], the hinge loss is employed as the adversarial objective function. The generator is trained to minimize: LG adv = Da(xm ) Db(xm ) + L1, whereas the discriminator is trained to minimize: (3) 5 Figure 4. Adversarial training. Each pre-trained model has specific discriminator that is trained with hinge loss. We use the outputs of the pre-trained model as positive samples, and the mixed predictions generated by MixerMDM as negative samples. LD adv =min(0, 1 Da(xm min(0, 1 + Da(xa L1, )) + min(0, 1 Db(xm )) + min(0, 1 + Db(xb ))+ t))+ (4) where are the predictions of given discriminator and L1 is regularization loss that is applied to penalize big differences between the losses of the distinct discriminators. The generator and discriminators are trained interleaved. Such adversarial training procedure allows training MixerMDM without having explicit ground truth motion, and preserving the core characteristics of each of the pre-trained models by fooling their respective discriminators. 4. Experiments 4.1. Datasets Our experiments employ the InterHuman [27] and HuInterHuman stands as one of manML3D [15] datasets. the most extensive annotated human-human interaction datasets available, containing 7,779 interactions labeled with textual descriptions. While including diverse range of interactions, the intrapersonal diversity is limited. Conversely, HumanML3D is dataset containing 14,616 individual motions. Although no interpersonal dynamics are included in this dataset, the intrapersonal diversity is far superior compared to InterHuman. The individual pretrained models employed by MixerMDM are trained on HumanML3D, whereas the interaction counterparts are trained on InterHuman, both using textual descriptions. We further leverage InterHuman to train the Mixer, using the individual textual descriptions generated by [39]. These individual descriptions are used as conditioning for generating the motions from the individual models. For consistency across experiments, we adapted the HumanML3D motion representation to align with the one used in the InterHuman dataset. 4.2. Evaluation Metrics The metrics proposed in [15] are usually employed to evaluate Human Motion Generation. Among them, we can find R-Precision, Multimodal-Dist, FID, Diversity, and Multimodality. Each of them is used to evaluate certain aspects of the motion generation. Specifically, R-Precision and Multimodal-Dist assess the semantic similarity between the generated motions and the input prompts. However, for the specific challenge addressed in this paper, these metrics fall short in reflecting the quality of the mixing process, which we define as the capability of method to blend the predictions made by an interaction and individual model, maintaining the overall interaction while being precise in motions from the different individuals. To address this limitation, [39] introduced the EID metric, which captures intrapersonal diversity within human-human interactions by randomly replacing individual motions. However, it does not consider the interaction and individual quality of the mixing. Therefore, we introduce: Alignment. This metric uses the R-Precision (Top-3) to represent the alignment between the condition and the generated motion. However, instead of calculating this metric directly using the dataset used to train the Mixer, we calculate the alignment of the mixed motions with respect to each dataset used by the pre-trained models in the pipeline. With this procedure, we quantify how much the blended motion preserves the characteristics of their base pre-trained models. In the particular case that we present, the Interaction and Individual Alignment are calculated by computing the R-Precision of MixerMDM over the InterHuman and HumanML3D datasets, respectively. Two distinct feature extractors are used to calculate this metric on each dataset. Ultimately, an Overall Alignment score is derived by computing the harmonic mean of these two alignment metrics, providing more interpretable measure for identifying the optimal mixing model. Adaptability. Unlike previous methods, MixerMDM can predict distinct mixing weights for different motions and conditions. Adaptability indicates the models capability to adapt the mixing of two motions depending on their particularities. It is computed as the average standard deviation of the mixing weights over the test set of InterHuman. 4.3. Implementation Details The Mixer consists of four consecutive multi-head attention layers, each with latent dimension of 512 and eight heads, resulting in model with 21M parameters significantly fewer than the over 300M parameters of the pre-trained models utilized. We employ frozen CLIP-ViTL/14 model [38] as the text encoder, shared across all models in the pipeline. The number of diffusion timesteps is set to 1K, with cosine noise schedule [33]. During inference, we apply DDIM sampling [43] with η = 0 and 50 timesteps. All models are trained using the AdamW optimizer [30], with betas of (0.9, 0.999), weight decay of 105, and learning rate of 105. Training incorporates adversarial losses as detailed in Sec. 3.4 with an L1 weight of 0.1. Each model undergoes 300 training epochs with batch size of 128, utilizing gradient accumulation and 16-bit mixed precision. Training took 36 hours on single Nvidia 4090 GPU. 4.4. Comparison to state-of-the-art Approaches We compare MixerMDM against DiffusionBlending [41] and DualMDM [39] as previous methods for mixing human motion models. Additionally, model trained with HumalML3D and finetuned with InterHuman is used as baseline for the comparisons (Finetuned). Different versions of MixerMDM are evaluated by incorporating the different mixing weights variations explained in Sec. 3.2 with different combinations of pre-trained models. We select InterGen [27] and in2IN [39] models for human-human motion generation (Ma), and MDM [48] and the individual version of in2IN for individual motion generation(Mb). Quantitative Comparisons. Tab. 1 presents the quantitative evaluations conducted across all MixerMDM variations. The results indicate that the version using SpatioTemporal (ST) mixing weights, alongside the interaction and individual versions of in2IN, achieves the highest performance in terms of Overall Alignment. Concerning Adaptability, this model achieves the third-best score. However, the best models in this metric have much lower Alignment, suggesting that they sacrifice the mixing quality to achieve more adaptability. As the main goal of this method is to achieve the best blending possible, we consider the ST model with Ma = in2IN and Mb = in2IN as the best trade-off. With respect to the previous stateof-the-art approaches, almost all versions of MixerMDM outperform previous approaches. Additionally, the Adapatability component is new indicator that is not present in previous approaches. While higher Adaptability does not mean higher quality, this value alongside better quantitative performance, reinforces the claim that MixerMDM benefits from our new learnable dynamic mixing strategy, compared to existing static ones. 6 Figure 5. Mean mixing weights. The mean mixing weights of the best models for each variation of the Mixer output. Previous model composition techniques appear in the Global plot with dotted line (DiffusionBlending [41]) and dashed line (DualMDM [39]). Method Finetuned Diff.Blending [41] DualMDM [39] Ma Mb Alignment Interaction individual Overall .184.01 .289.00 .675.01 .137.02 .221.01 in2IN in2IN .577.00 .134.01 .217.00 in2IN in2IN .574.00 - - Ma Mb Type in2IN [39] InterGen [27] in2IN [39] MDM [48] in2IN [39] MDM [48] S ST ST ST ST Adp. Alignment Interaction individual Overall .317.00 .228.01 .521.00 .004 .245.01 .150.02 .672.02 .002 .257.01 .391.01 .310.00 .015 .286.01 .335.01 .112 .406.01 .172.01 .458.02 .000 .203.01 .391.01 .059 .161.01 .505.02 .222 .230.01 .385.01 .107 .272.01 .380.01 .004 .297.01 .328.01 .020 .281.01 .266.01 .004 .313.01 .276.01 .013 .180.01 .416.01 .001 .181.01 .438.01 .022 .196.01 .490.02 .125 .172.02 .489.02 .030 .250.01 .267.00 .244.01 .288.00 .317.00 .312.00 .273.01 .293.00 .251.00 .256.00 .280.01 .254.02 Table 1. Quantitative evaluation. Top: state-of-the-art comparison. Bottom: ablation of all the variations tested with MixerMDM. Type: type of mixing weights predicted by the Mixer. Adp.: Adaptability metric. All evaluations are executed 10 times to elude the randomness of the generation. indicates the 95% confidence interval. Best results are highlighted. Fig. 5 illustrates the mean mixing weights for the topperforming models. The higher the curve, the more importance is given to the individual model with respect to the interaction. These plots reveal patterns that, while bearing some resemblance to those of DualMDM, diverge considerably, particularly in the ST version, where the overall curve differs markedly. The common factor of all the learned curves is that the individual model has more importance at the beginning of the denoising process, while the interaction takes all the importance on the last steps of the diffusion process, validating the hypothesis stated in [39]. Qualitative Comparisons. Beyond quantitative evaluation, qualitative comparisons demonstrate subtle differences between MixerMDM and previous methods. Fig. 6 shows how MixerMDM is much more consistent in generating mixed motions aligned with their conditioning than previous approaches. Additionally, in Fig. 7 we can observe that MixerMDM can generate more fine-grained individual variations to interaction from the InterHuman test set. For additional examples and interactive visualizations, please refer to the supplementary video. Figure 6. Consistency. When an individual variation (underline) is performed in one of the interactions, MixerMDM achieves greater level of consistency and individual assignment generating the mixed motion. Finally, we have conducted user study to provide more comprehensive evaluation of the methods performance. 35 users ranked the evaluated methods based on the alignment of 20 randomly selected generated motions with 1) their interaction and 2) their individual textual descriptions. Tab. 2 shows that MixerMDM significantly outperforms previous methods. Interaction individual Method Avg. Diff.Blending [41] 2.531.584 2.286.641 DualMDM [39] 1st 10.57% 24.57% MixerMDM (ours) 1.182.467 85.14% 1.309.573 74.86% 1st 4.57% 2.446.678 10.29% 2.051.736 Avg. Table 2. User study. Average rank (Avg), First ranked (1st). indicates standard deviation. 7 Figure 7. Controllability. While all methods can properly generate an interaction (top), when variation in one of the individual conditions is applied (bottom, underline), MixerMDM generates the most aligned motion to the overall interaction and individual textual descriptions. 4.5. Modularity 5. Conclusion One advantage of MixerMDM is its ability to mix motions by directly using the outputs of pre-trained models. This allows the Mixer to learn how to mix motion distributions rather than focusing on model-specific features. This approach enables the reuse of weights of trained Mixer with new specialized models that can generate similar distribution of motions to the ones originally used to train the Mixer. In other words, pre-trained models can be substituted by others that have been trained with the same specific dataset. This provides modular capability to the method, which is especially useful in fast-paced field such as human motion generation. We evaluate this property in Tab. 3, where models with initially lower performance are combined with Mixer weights from the top-performing models. This leads to notable performance boost without requiring any additional training. Ma Mb Type Alignment Interaction individual Overall Adp. Impr. InterGen [27] MDM [48] .385.02 .313.01 .345.00 .004 37% .589.01 .232.01 .333.00 .001 30% .380.01 .315.01 .344.00 .014 23% ST .417.01 .292.01 .344.00 .126 35% Table 3. Modularity Evaluation. The worst combination of pretrained models is evaluated by using the weights of the best Mixer from Tab. 1. Impr.: relative Overall Alignment improvement (%) with respect to the original evaluation. We introduced MixerMDM, learnable approach for combining motions from distinct text-conditioned human motion diffusion models. Unlike previous methods, which rely on manually set parameters for mixing, MixerMDM learns unique mixing weights depending on the specific motions to combine through adversarial losses. Mixing models that generate individual motions with models that generate human-human interaction has resulted in MixerMDM consistently producing blended motions with higher degree of individual controllability with respect to previous methods. Additionally, as MixerMDM learns to blend motions by directly using the outputs of the pre-trained models, it has modular property that allows the swapping of pre-trained models without the need for re-training. Finally, we introduced an evaluation pipeline to quantitatively asses all these capabilities. Limitations and Future Work. Although MixerMDM is approximately ten times smaller than the pre-trained models it integrates, it still incurs additional computational costs and extends inference time, as mixing weights must be calculated at each denoising step. Moreover, due to the nature of adversarial training, achieving stability and tuning hyperparameters can be challenging. further limitation and direction for future work lies in data representation discrepancies. For effective model mixing, models must predict unified data representation, which is not always achievable directly and sometimes necessitates retraining models to ensure representation compatibility."
        },
        {
            "title": "Acknowledgments This publication is part of",
            "content": "the TSI-100927-2023-6 Project, funded by the Recovery, Transformation and Resilience Plan from the European Union Next Generation through the Ministry for Digital Transformation and the Civil Service. This work has been partially supported by the Spanish project PID2022136436NB-I00, the Spanish national grant for PhD studies, FPU22/04200, by ICREA under the ICREA Academia programme, and CIAICO/2022/132 Consolidated group project AI4Health funded by Valencian government."
        },
        {
            "title": "References",
            "content": "[1] Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. In 2019 International Conference on 3D Vision (3DV), pages 719728. IEEE, 2019. 2 [2] Nikos Athanasiou, Mathis Petrovich, Michael Black, and Gul Varol. Teach: Temporal action composition for 3d humans. In 2022 International Conference on 3D Vision (3DV), pages 414423. IEEE, 2022. 3 [3] Nikos Athanasiou, Mathis Petrovich, Michael Black, and Gul Varol. Sinc: Spatial composition of 3d human motions In Proceedings of the for simultaneous action generation. IEEE/CVF International Conference on Computer Vision, pages 99849995, 2023. 3 [4] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In International Conference on Machine Learning, pages 17371752. PMLR, 2023. 3 [5] German Barquero, Sergio Escalera, and Cristina Palmero. Belfusion: Latent diffusion for behavior-driven human moIn Proceedings of the IEEE/CVF Internation prediction. tional Conference on Computer Vision, pages 23172327, 2023. [6] German Barquero, Sergio Escalera, and Cristina Palmero. Seamless human motion composition with blended posiIn Proceedings of the IEEE/CVF Contional encodings. ference on Computer Vision and Pattern Recognition, pages 457469, 2024. 3 [7] Zhongang Cai, Jianping Jiang, Zhongfei Qing, Xinying Guo, Mingyuan Zhang, Zhengyu Lin, Haiyi Mei, Chen Wei, Ruisi Wang, Wanqi Yin, et al. Digital life project: Autonomous In Proceedings of 3d characters with social intelligence. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 582592, 2024. 3 [8] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. 3 [9] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In European Conference on Computer Vision, pages 390408. Springer, 2024. 2 [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transIn Proceedings of formers for language understanding. NAACL-HLT, pages 41714186, 2019. 2 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [12] Kent Fujiwara, Mikihiro Tanaka, and Qing Yu. Chronologically accurate retrieval for temporal grounding of motionlanguage models. In European Conference on Computer Vision, pages 323339. Springer, 2024. 3 [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2, 3, 5 [14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1069610706, 2022. 3 [15] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 51525161, 2022. 2, 5, 6, 1 [16] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 51525161, 2022. 2 [17] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts. In European Conference on Computer Vision, pages 580597. Springer, 2022. 2 [18] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. 2 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [20] Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation and editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 3 [21] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation In Proceedings of the IEEE/CVF Conference and editing. on Computer Vision and Pattern Recognition, pages 6080 6090, 2023. 4, 5 [22] Muhammad Gohar Javed, Chuan Guo, Li Cheng, and Xingyu Li. Intermask: 3d human interaction generation via collaborative masked modelling, 2024. 3 [23] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Ad9 vances in Neural Information Processing Systems, 36, 2024. [24] Filip Ekstrom Kelvinius and Fredrik Lindsten. Discriminator guidance for autoregressive diffusion models. In International Conference on Artificial Intelligence and Statistics, pages 34033411. PMLR, 2024. 3 [25] Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, and Il-Chul Moon. Refining generative process with discriminator guidance in score-based diffusion models. arXiv preprint arXiv:2211.17091, 2022. 3 [26] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: FreeIn Proform language-based motion synthesis & editing. ceedings of the AAAI Conference on Artificial Intelligence, pages 82558263, 2023. 2 [27] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based multi-human motion generaInternational Journal of tion under complex interactions. Computer Vision, 132(9):34633483, 2024. 3, 5, 6, 7, 8, 1 [28] Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. Magicmix: Semantic mixing with diffusion models. arXiv preprint arXiv:2210.16056, 2022. 3 [29] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pages 423439. Springer, 2022. [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 6, 1 [31] Chia-Ni Lu, Ya-Chu Chang, and Wei-Chen Chiu. Bridging the visual gap: Wide-range image blending. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 843851, 2021. 3 [32] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, and Yaser Sheikh. Pixel codec avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6473, 2021. 1 [33] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 6 [34] Sang-Min Park and Young-Gab Kim. metaverse: Taxonomy, components, applications, and open challenges. IEEE access, 10:42094251, 2022. 1 [35] Mathis Petrovich, Michael Black, and Gul Varol. TEMOS: Generating diverse human motions from textual descriptions. In European Conference on Computer Vision, pages 480 497. Springer, 2022. [36] Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen In ProChen. Mmm: Generative masked motion model. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15461555, 2024. 2 [37] Abhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael J. Black. BABEL: Bodies, action and behavior with english labels. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pages 722731, 2021. 2 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual In International conference on machine learning, models. pages 87488763. PMLR, 2021. 6, 1 [39] Pablo Ruiz-Ponce, German Barquero, Cristina Palmero, Sergio Escalera, and Jose Garcıa-Rodrıguez. in2in: Leveraging individual information to generate human interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 19411951, 2024. 2, 3, 4, 6, 7, 1 [40] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2025. 3, 5 [41] Yoni Shafir, Guy Tevet, Roy Kapon, and Amit Haim Bermano. Human motion diffusion as generative prior. In The Twelfth International Conference on Learning Representations, 2024. 2, 3, 4, 6, [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 2 [43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. 6 [44] Sebastian Starke, Paul Starke, Nicky He, Taku Komura, and Yuting Ye. Categorical codebook matching for embodied character controllers. ACM Transactions on Graphics (TOG), 43(4):114, 2024. 1 [45] Ryo Suzuki, Adnan Karim, Tian Xia, Hooman Hedayati, and Nicolai Marquardt. Augmented reality and robotics: survey and taxonomy for ar-enhanced human-robot interaction and robotic interfaces. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1 33, 2022. 1 [46] Mikihiro Tanaka and Kent Fujiwara. Role-Aware InteracIn Proceedings tion Generation from Textual Description. of the IEEE/CVF International Conference on Computer Vision, pages 1599916009, 2023. 3 [47] Guy Tevet, Brian Gordon, Amir Hertz, Amit Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision, pages 358374. Springer, 2022. 2, [48] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffuIn The Eleventh International Conference on sion model. Learning Representations, 2023. 2, 6, 7, 8, 1 [49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia 10 Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2, 4 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 364373, 2023. [64] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [65] Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. arXiv preprint arXiv:2404.01284, 2024. 2 [66] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming-Yu Liu. Diffcollage: Parallel generation of large content with diffusion models. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1018810198. IEEE, 2023. 3 [67] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 2 [68] Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multiIn Proceedings of the perspective attention mechanism. IEEE/CVF International Conference on Computer Vision, pages 509519, 2023. 2 [69] Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu. Emdm: Efficient motion diffusion model for fast and high-quality motion generation. In European Conference on Computer Vision, pages 1838. Springer, 2024. [70] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. IEEE Transactions Human motion generation: survey. on Pattern Analysis and Machine Intelligence, pages 120, 2023. 1, 2 [51] Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, and Lingjie Liu. Tlcontrol: Trajectory and language control for human motion synthesis. In European Conference on Computer Vision, pages 3754. Springer, 2024. 2 [52] Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernandez Abrevaya, David Picard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers, 2024. 4 [53] Zhengwei Wang, Qi She, and Tomas Ward. Generative adversarial networks in computer vision: survey and taxonomy. ACM Computing Surveys (CSUR), 54(2):138, 2021. 3 [54] Min Wei, Jingkai Zhou, Junyao Sun, and Xuesong Zhang. Adversarial score distillation: When score distillation meets gan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81318141, 2024. 3 [55] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. Gp-gan: Towards realistic high-resolution image blending. In Proceedings of the 27th ACM international conference on multimedia, pages 24872495, 2019. [56] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In International Conference on Learning Representations (ICLR), 2022. 3 [57] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In International Conference on Learning Representations (ICLR), 2022. 2 [58] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and MingHsuan Yang. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56(4): 139, 2023. 2 [59] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1601016021, 2023. 3 [60] Hao Zhang, Han Xu, Xin Tian, Junjun Jiang, and Jiayi Ma. Image fusion meets deep learning: survey and perspective. Information Fusion, 76:323336, 2021. 3 [61] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descripIn Proceedings of tions with discrete representations. the IEEE/CVF conference on computer vision and pattern recognition, pages 1473014740, 2023. [62] Lingzhi Zhang, Tarmily Wen, and Jianbo Shi. Deep image blending. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 231240, 2020. 3 [63] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In 11 MixerMDM: Learnable Composition of Human Motion Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material aims to enhance the reproducibility and understanding of the previously presented contributions. In Sec. A, we detail the datasets used and describe the modifications made to integrate them into the MixerMDM pipeline. In Sec. B, we outline the implementation specifics of the state-of-the-art models used for comparison, as well as the evaluators employed in the proposed evaluation pipeline. In Sec. C, we complement the quantitative evaluation with additional experiments and ablations. Lastly, in Sec. D, we include additional visual examples illustrating the MixerMDM capabilities. A. Datasets A.1. InterHuman InterHuman [27] is one of the most extensive annotated datasets for human-human interactions, containing 7,779 interactions labeled with textual descriptions. Each individuals motion within an interaction is represented as set of poses xi= (cid:2)jp, jv, jr, cf (cid:3), where xi denotes the i-th motion timestep. This representation includes joint positions and velocities jp, jv R3Nj in the world frame, 6D representation of local rotations jr R6Nj in the root frame, and binary foot-ground contact features cf R4. The number of joints in the InterHuman dataset is Nj = 22. Each interaction in the dataset is paired with three textual descriptions summarizing the overall interaction. Additionally, the in2IN [39] framework introduced more detailed textual descriptions, generated by Large Language Models, for the motions performed by each individual in the interaction. We utilized these detailed descriptions to condition the generation of individual models employed in the mixing process. A.2. HumanML3D textual The HumanML3D [15] dataset contains 14,616 individual motions descripannotated with tions. Each motion is represented as set of poses xi= (cid:2) ra, rx, rz, ry, jp, jv, jr, cf (cid:3), where xi denotes the ra is the root i-th motion timestep. In this format, rx, rz are the root angular velocity in the Y-axis, linear velocities on the XZ-plane, ry is the root height, jp, jv R3Nj and jr R6Nj are the joint positions, velocities, and rotations in the root frame, and cf R4 are binary foot-ground contact features. The number of joints in the HumanML3D dataset is Nj = 22. Since this representation differs from the format used in InterHuman and is not optimal for capturing the relative positions of interactants, we have converted it to the InterHuman format. This conversion involves processing the raw SMPL motions from HumanML3D to extract the global joint positions and velocities as well as the relative rotations as it is done in the InterHuman pre-processing. B. Further Implementation Details B.1. Motion Transformations Motion transformations are applied to maintain the pretrained models within their learned distribution. The centering function translates the motion to the origin of coordinates in the XZ plane, simultaneously orienting the trajectory initially in the Z+ direction. The alignment is global transformation applied to motion (xa) with respect to another (xb). Firstly, xa is translated to the initial position of xb. Secondly, xa is rotated to match the orientation of the vector of the initial and end positions of xb. The same transformation is applied to the whole motion, thus not introducing foot sliding, and standardizes global positioning and orientation to the individual models. B.2. State-of-the-art Implementations The methods employed in Sec. 4 were implemented using their respective official codebases. We leveraged the original checkpoints of InterGen [27] and the interaction and individual versions of in2IN [39], as they were trained with the same motion representation we use. For MDM [48], originally trained on HumanML3D, we kept the architecture as is, and adapted the output shape of the denoiser to match the size of our motion representation. B.3. Evaluators The evaluation metrics for human motion generation require feature extractor that produces aligned latent representations of both the generated motions and the corresponding conditions (text, in this case). The feature extractor architecture is based on the one used in the InterHuman dataset: MotionEncoder and TextEncoder. The MotionEncoder is Transformer encoder with 8 layers of 8 heads each, which transforms the motion into 2048-dimensional latent vector. This vector is then compressed to dimension of 512 using Multi-Layer Perceptron. The TextEncoder is frozen CLIP-ViTL/14 model [38], supplemented with Transformer encoder with 8 layers of 8 heads each to adapt the CLIP latent space to better match the dataset distribution. We trained feature extractor for each dataset employed. These models were trained for 500 epochs with batch size of 64, using the AdamW optimizer [30] with β parameters set to (0.9, 0.999), weight decay of 105, and learning rate of 104. 1 Method Type Ground Truth Diff.Blending [41] DualMDM [39] MixerMDM (ours) - - - ST Diversity MM Dist FID R-Precision Interaction Individual Interaction Individual Interaction Individual Interaction Individual 16.3.05 7.95.06 3.44.00 3.76.01 1.04.14 .273.01 .563.00 .701.01 11.9.22 6.14.14 5.18.01 3.89.00 36016 33.8.29 .137.02 .577.00 12.5.28 7.04.17 5.13.01 3.85.01 330.02 22.9.19 .134.01 .574.00 19919 .228.01 .521.00 14.0.53 6.57.19 4.70.14 3.92.00 44.5.99 2455.1 3.85.00 5.05.00 7.57.07 13.6.16 .150.02 21.2.70 .672.02 .257.01 .391.01 14.4.09 4.69.03 1925.9 52.41.8 .286.01 47.6.88 142.75 .406.01 - 2.74.01 2.74.09 2.99.12 3.04.21 3.94.00 2.96.07 3.93.01 4.60.02 6.57.09 15.1.18 1.23.02 3.26.07 - .779.12 .935.12 1.08.19 1.14.25 1.11. MModality Interaction Individual 6.40.20 Table A. Quantitative evaluation with conventional metrics. Ours {G,T,S,TS} uses Ma=Mb= in2IN . Mean of 10 evaluations, shows the 95% confidence interval. Best in bold. C. Further Quantitative Examples D. Further Qualitative Examples In this section, we complement the quantitative study from Sec. 4.4 with additional experiments and ablations. In addition to the proposed metrics, we have evaluated MixerMDM with conventional metrics. Tab. shows that our method surpasses previous methods on the proposed and conventional metrics. C.1. Motion Transformations The centering and alignment transformation (Sec. 3.1, Sec. B.1) help to maintain the mixed motion within the distribution of the pre-trained models. Tab. shows the effect of not using the alignment transformation. While producing performance drop in the interaction evaluation, results still outperform previous methods. In this section, we complement the qualitative study from Sec. 4.4 with additional examples. Fig. show the superior individual controllability of MixerMDM when compared to previous approaches. With MixerMDM, we can achieve detailed control of the individual motions while still preserving the dynamics of the interaction. This is achieved thanks to the adversarial training that promotes preserving the main interaction dynamics as well as the individual ones. Fig. shows another example of consistency on this dual control. We refer the reader to the attached video for more detailed visualization of all the examples that we showed and discussed in this section. Method ST Top-3 R-Prec. Interaction .391.01 .578.00 .375.01 .380. FID MM Dist Diversity MModality Interaction Interaction Interaction 45.7.31 6.51.08 3.92.01 20.2.04 3.84.00 7.73.09 6.43.01 3.93.01 50.61.0 6.62.05 3.93.01 41.1.00 Interaction 1.15.02 .990.01 1.01.15 1.01.01 Table B. MixerMDM without alignment. Compare with Tab. A. C.2. Usabilitiy While using more prompts allows higher controllability, it can hinder usability with tedious descriptions in cases where such controllability is not priority. Using an LLM (gpt4o-mini) at inference allows using just the interaction prompt and inferring the individual ones. Tab. shows that this strategy does not affect the interaction motion quality and text-alignment. Figure A. Consistency. When an individual variation (underlined) is performed in one of the interactions, MixerMDM achieves greater level of consistency generating the mixed motion. Method ST Top-3 R-Prec. Interaction .451.00 .651.02 .412.03 .341.00 FID MM Dist Diversity MModality Interaction Interaction Interaction 46.7.18 6.61.12 3.92.01 21.2.77 3.83.01 7.69.17 6.40.09 3.93.01 49.31.2 6.39.04 3.94.00 49.11. Interaction 1.03.18 .998.00 1.11.00 1.03.05 Table C. MixerMDM LLM aided. Compare with Tab. A. 2 (a) Boxing interaction Figure B. Controllability. While all methods can properly generate an interaction (top), when variation in one of the individual conditions is applied (bottom, underlined), MixerMDM generates the most aligned motion to the overall interaction and individual textual descriptions. (b) Walking in circles interaction"
        }
    ],
    "affiliations": [
        "Kings College London, UK",
        "Universidad de Alicante, Spain",
        "Universitat de Barcelona and Computer Vision Center, Spain"
    ]
}