{
    "paper_title": "LongCat-Flash-Thinking-2601 Technical Report",
    "authors": [
        "Meituan LongCat Team",
        "Anchun Gui",
        "Bei Li",
        "Bingyang Tao",
        "Bole Zhou",
        "Borun Chen",
        "Chao Zhang",
        "Chao Zhang",
        "Chen Gao",
        "Chen Zhang",
        "Chengcheng Han",
        "Chenhui Yang",
        "Chuyu Zhang",
        "Cong Chen",
        "Cunguang Wang",
        "Daoru Pan",
        "Defei Bu",
        "Dengchang Zhao",
        "Di Xiu",
        "Dishan Liu",
        "Dongyu Ru",
        "Dunwei Tu",
        "Fan Wu",
        "Fengcheng Yuan",
        "Fengcun Li",
        "Gang Xu",
        "Guanyu Wu",
        "Guoyuan Lin",
        "Haibin Wang",
        "Hansi Yang",
        "Hao Yang",
        "Haonan Yan",
        "Haoxiang Ma",
        "Haoxing Wen",
        "Hongyan Hao",
        "Hongyin Tang",
        "Hongyu Zang",
        "Hongzhi Ni",
        "Hui Su",
        "Jiacheng Zhang",
        "Jiahong Zhou",
        "Jiahuan Li",
        "Jiaming Wang",
        "Jian Yang",
        "Jianfei Zhang",
        "Jianhao Xu",
        "Jianing Wang",
        "Jiapeng Zhu",
        "Jiaqi Sun",
        "Jiarong Shi",
        "Jiarui Zhao",
        "Jingang Wang",
        "Jinluan Yang",
        "Jinrui Ding",
        "Jinwei Xiao",
        "Jiyuan He",
        "Juncan Xu",
        "Kefeng Zhang",
        "Keheng Wang",
        "Li Wei",
        "Lianhui Ma",
        "Lin Qiu",
        "Lingbing Kong",
        "Lingchuan Liu",
        "Linsen Guo",
        "Mengshen Zhu",
        "Mengxia Shen",
        "Mingyang Zhu",
        "Peiguang Li",
        "Peng Pei",
        "Pengcheng Jia",
        "Pengtao Zhang",
        "Peng Zhao",
        "Qi Gu",
        "Qiong Huang",
        "Qiyuan Duan",
        "Quanchi Weng",
        "Rongxiang Weng",
        "Rongzhi Zhang",
        "Rumei Li",
        "Shanglin Lei",
        "Shengnan An",
        "Shijun Dai",
        "Shuaikang Liu",
        "Shuang Zhou",
        "Shuo Wang",
        "Songyuan Zhao",
        "Tao Liang",
        "Tianhao Hu",
        "Tianze Chen",
        "Wei Liu",
        "Wei Shi",
        "Wei Wang",
        "Weifeng Tang",
        "Wenjie Shi",
        "Wenlong Zhu",
        "Wentao Chen",
        "Wentao Shi",
        "Xi Su",
        "Xiangcheng Liu",
        "Xiandi Ma",
        "Xiangyu Xi",
        "Xiangyuan Liu",
        "Xiangzhou Huang",
        "Xiao Liu",
        "Xiaodong Cai",
        "Xiaolong Chen",
        "Xiaowei Shi",
        "Xiaoyu Li",
        "Xin Chen",
        "Xingchen Liu",
        "Xuan Huang",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Yan Chen",
        "Yang Bai",
        "Yang Liu",
        "Yang Yang",
        "Yang Zheng",
        "Yaoming Wang",
        "Yaoming Zhu",
        "Yaqi Huo",
        "Yanyu Chen",
        "Yaorui Shi",
        "Yerui Sun",
        "Yi Zhang",
        "Yihao Chen",
        "Yi-Kai Zhang",
        "Yifan Lu",
        "Yifan Zhao",
        "Yitao Zhai",
        "Yongjing Yin",
        "Yongwei Zhou",
        "Youshao Xiao",
        "Yuchuan Dai",
        "Yuchen Xie",
        "Yuchen Yu",
        "Yufei Zhang",
        "Yuhuai Wei",
        "Yulei Qian",
        "Yunfan Liang",
        "Yunke Zhao",
        "Yuwei Jiang",
        "Yuxin Bian",
        "Yuxin Chen",
        "Yuxin Liu",
        "Yue Xu",
        "Yueqing Sun",
        "Zeyang Yu",
        "Zhao Yang",
        "Zhengsheng Huang",
        "Zhengyu Chen",
        "Zhijian Liu",
        "Zhikang Xia",
        "Zhimin Lin",
        "Zhiyuan Yao",
        "Zhuofan Chen",
        "Zhuowen Han",
        "Zijian Zhang",
        "Ziran Li",
        "Ziwen Wang",
        "Ziyuan Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking."
        },
        {
            "title": "Start",
            "content": "LongCat-Flash-Thinking-2601 Technical Report Meituan LongCat Team longcat-team@meituan.com"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce LongCat-Flash-Thinking-2601, 560-billion-parameter open-source Mixture-ofExperts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking2601 achieves state-of-the-art performance among open-source models on wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the models strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking. We release our checkpoints to facilitate future research and real-world applications of agentic systems. LongCat Chat: https://longcat.ai Huggingface: https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking-2601 Github: https://github.com/meituan-longcat/LongCat-Flash-Thinking-2601 6 2 0 2 3 2 ] A . [ 1 5 2 7 6 1 . 1 0 6 2 : r Figure 1: Benchmark performance of LongCat-Flash-Thinking-2601. LongCat-Flash-Thinking-2601 Technical Report"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in reasoning models have led to rapid progress on complex tasks such as mathematics and programming, in some cases even surpassing top human experts [DeepSeek-AI, 2025, Shao et al., 2025, Anthropic, 2025, Google, 2025]. The natural next question then arises: how can such complex problem-solving capabilities be applied to solve complex real-life tasks, and how can complex problem-solving capabilities be extended beyond this point? As intrinsic reasoning ability approaches its limits, we identify that interaction with external environments emerges as key mechanism for further progress [MoonshotAI, 2025, DeepSeek-AI et al., 2025]. From this perspective, agentic reasoning can be understood as the ability to solve complex problems through adaptive interaction with external environments. Beyond internal deliberation, advanced agentic reasoning capability requires models to determine when and how to interact with the environment, and to effectively integrate environmental feedback to sustain and advance the reasoning process. In this way, reasoning and interaction naturally interleave and reinforce each other, jointly enabling more general and powerful problem-solving behavior [MiniMax, 2025]. However, enabling such agentic reasoning capability poses substantial challenges for existing models and training pipelines. Agentic tasks typically involve long-horizon trajectories, heterogeneous environments, and long-tailed interaction dynamics, which place new demands on data curation, environment construction, reinforcement learning strategies, and system-level infrastructure spanning from pre-training to post-training stage. In this work, we introduce LongCat-Flash-Thinking-2601, powerful and efficient Mixture-of-Experts (MoE) reasoning model with 560B total parameters and 27B activated parameters on average per token, featuring strong agentic reasoning capability. The pre-training of LongCat-Flash-Thinking-2601 largely follows the recipe of LongCat-Flash-Chat [Team et al., 2025a], retaining the original data distribution to preserve competitive general reasoning performance. Building on this foundation, we further extend the model toward large-scale agentic reasoning through carefully designed mid-training stage. Compared to traditional reasoning, agentic behaviors typically involve long-horizon trajectories with proactive tool invocations. However, such interaction patterns are extremely scarce in real-world corpora, where most data consist primarily of natural language. As result, raw model is largely unfamiliar with agentic interaction dynamics, leading to inefficient exploration during the reinforcement learning stage. To address this challenge, we expose the model to moderate-scale synthesized structured agentic trajectories during mid-training, providing strong initialization for agentic reasoning behaviors. In post-training, we focus on efficiently scaling RL compute to improve agentic reasoning capabilities. Beyond specific agentic scenarios such as search and code, we further identify generalization across tasks and environments as core characteristic of advanced agentic behavior. Such capability is primarily acquired through interaction with diverse environments, which therefore serve as the learning playground. To this end, we design an automated environment scaling pipeline to construct complex and diverse environments covering over 20 domains, while strictly preserving executability and verifiability. As the resulting environments exhibit substantial heterogeneity in both domain and difficulty, training over them requires careful co-design of training strategies and infrastructure support to ensure stable and effective multi-domain environment learning. To support large-scale agentic reinforcement learning under this setting, we extend our multi-version asynchronous training system, Dynamic ORchestration for Asynchronous Rollout (DORA), to enable scalable and stable training with up to 32,000 environments executing concurrently. However, generalization to real-world settings remains highly challenging, as real-world environments are inherently imperfect. To bridge the gap between idealized training settings and real-world deployment, we systematically analyze realworld noise and design an automated pipeline to progressively incorporate multi-type and multi-level environmental imperfections into the multi-domain environment training process. To further push reasoning capability beyond existing limits, LongCat-Flash-Thinking-2601 incorporates Heavy Thinking Mode that enables effective test-time scaling of reasoning. This mode decomposes challenging problem solving into complementary stages that jointly expand both reasoning width and depth, allowing the model to explore diverse solution paths while progressively refining its reasoning. An additional reinforcement learning stage is introduced to strengthen the models ability to aggregate and refine intermediate reasoning outcomes, further enhancing the effectiveness. While retaining strong competitiveness on general reasoning benchmarks, LongCat-Flash-Thinking-2601 achieves stateof-the-art performance among open-source models across wide range of agentic benchmarks, while demonstrating strong generalization to out-of-distribution real-world agentic scenarios. Notably, LongCat-Flash-Thinking-2601 attains 73.1% on BrowseComp, 77.7% on RWSearch, 88.2% on τ 2-Bench, and 29.3% on VitaBench, establishing it as the leading open-source model for agentic search and agentic tool-use tasks. Our work presents three core contributions: Environment Scaling and Multi-Domain Environment Training. We develop scalable environment construction and task generation framework that produces large collection of high-quality, executable, and verifiable agentic 2 LongCat-Flash-Thinking-2601 Technical Report environments. Building on this, we extend our asynchronous reinforcement learning infrastructure to support stable and efficient multi-domain environment training, enabling the acquisition of generalizable agentic skills across diverse domains. Robust Agentic Training under Noisy Environments. To address the inherent imperfection of real-world environments, we systematically analyze major sources of environmental noise and design an automated pipeline to inject multi-type and multi-level noise into training environments. curriculum-based reinforcement learning strategy is adopted to progressively increase noise complexity, substantially improving robustness and performance under imperfect conditions. Heavy Thinking Mode for Test-Time Scaling. We introduce Heavy Thinking Mode that enables effective test-time scaling of reasoning by jointly expanding reasoning width and depth. Through parallel trajectory exploration and iterative reasoning refinement, this mode further enhances performance on challenging reasoning and agentic tasks."
        },
        {
            "title": "2 Pre-Training",
            "content": "Our model builds upon the pretraining recipe of LongCat-Flash-Chat [Team et al., 2025a], inheriting its data distribution to preserve strong general-purpose language and reasoning capabilities. Beyond traditional reasoning, agentic behaviors typically involve long-horizon trajectories with proactive tool invocations. This brings two additional challenges: (i) substantially increased demands on long-context modeling efficiency, and (ii) the scarcity of large-scale agentic trajectories in the real world. To address the long-context requirements of agentic reasoning, we adopt staged mid-training procedure with progressively increasing context lengths, allocating 500B tokens to the 32K/128K stages and an additional 40B tokens to the 256K stage. Since large-scale reinforcement learning alone is often inefficient and unstable without model that has already been primed with basic agentic behaviors, we choose to expose the model to moderate-scale agentic data at this stage. To mitigate the scarcity of real-world agentic trajectories, we construct hybrid data synthesis pipeline to collect and construct agentic training data. Furthermore, we predict optimal hyperparameters in the mid-training stage based on any pre-trained checkpoint, specifically designed to minimize the computational cost of finding the best configuration (see Appendix A). Following the original mid-training recipe to preserve general reasoning capability [Zhang et al., 2025a, Tu et al., 2025], we further augment the data distribution with structured agentic trajectories [Xu et al., 2026]. Since large-scale, highquality agentic dataespecially long-horizon trajectories involving reasoning, planning, and interactionis extremely scarce, we construct hybrid data synthesis framework to fill this gap. Specifically, our framework draws from two complementary sources: unstructured text and executable environments, corresponding to text-driven synthesis and environment-grounded synthesis, respectively. The former provides broad semantic and task diversity, while the latter ensures logical consistency and executability. In addition, to explicitly strengthen planning abilitya core component of agentic reasoning that is difficult to acquire from existing datawe further design dedicated planning-centric data construction strategy. As shown in Figure 2, the model trained on this enhanced mid-training recipe demonstrates superior agentic capability boundaries as demonstrated by larger pass@k on the τ 2-Bench. Text-driven synthesis Large-scale text corpora contain abundant implicit procedural knowledge, such as tutorials, instructions, and multi-step problem-solving workflows. We leverage this property to mine and restructure such latent procedures into explicit agentic interaction trajectories using the following pipeline: Text Filtering & Tool Extraction: We strategically identify text segments that exhibit rich, multi-step workflows. From these segments, we define potential functions and extract corresponding function-call lists, converting implicit procedures into explicit tool schemas. Synthesis & Refinement: We translate abstract workflows and raw text into concrete, multi-turn user-agent interactions. To ensure the robustness and breadth of the dataset, we apply extensive agentic pattern diversity enhancement and rigorous quality filtering, ensuring the final trajectories cover diverse array of interaction scenarios and task domains. To further increase the structural complexity of the synthesized trajectories, we apply two decomposition-based augmentations: Tool Decomposition: Starting from simple tool invocation trajectories, we iteratively hide parts of the tool parameters into the environment. Correspondingly, we synthesize model interactions to extract these parameters, progressively building more complex trajectories. Reasoning Decomposition: For each action step in the models output, we generate multiple alternative candidates and replace the original action with these alternatives. We then synthesize the model reasoning step to select the most 3 LongCat-Flash-Thinking-2601 Technical Report Figure 2: The comparison of agentic capability (pass@k. Baseline represents the model undergoing the old midtraining recipe, while Enhanced represents the model undergoing the agentic-enhanced mid-training recipe. appropriate candidate, transforming the trajectory into decision-making process that reflect the agents reasoning across multiple steps. Environment-grounded synthesis In addition to text-derived trajectories, we also construct agentic data directly from executable environments to guarantee logical correctness and execution consistency. We implement lightweight Python environments for collected toolsets and generate trajectories through controlled tool-chain sampling and execution verification: Environment Construction & Dependency Modeling: Based on existing tool definitions, we implement lightweight, verifiable Python environments. We explicitly model the logical dependencies between tools, constructing directed graph where nodes represent tools and edges represent parameter dependencies. This structure allows us to sample tool invocation chains of varying complexity and depth systematically. Reverse-Synthesis & Execution Verification: We sample valid tool execution paths from the dependency graph and employ reverse-engineering approach to synthesize corresponding system prompt for user simulators that align with the chosen tool chain. Crucially, the correctness of each trajectory is validated by executing the code and verifying the final state of the environment database. This ensures that the synthesized data is grounded in actual execution logic. Planning-Oriented Data Augmentation We observe that agentic reasoning critically depends on planning ability, which governs how the model decomposes complex goals, explores alternatives, and commits to intermediate decisions. However, such planning-centric behaviors are poorly represented in existing data and are difficult to acquire at scale. To explicitly strengthen this capability during mid-training, we design targeted data construction strategy that transforms existing trajectories into planning-centric decision-making processes. The first type of data focuses on synthesizing effective problem decomposition trajectories paired with correct initial action selections, providing supervision for coarse-to-fine planning and early-stage decision making. The second type starts from complete interaction trajectories and further enriches them by generating multiple alternative candidates at each decision step. The model is then trained to reason over and select among these candidates, transforming originally linear trajectories into structured multi-step decision-making processes."
        },
        {
            "title": "3 Scaling Reinforcement Learning",
            "content": "Post-training with reinforcement learning has become primary approach to elicit stronger reasoning capabilities. Following LongCat-Flash-Thinking [Team et al., 2025b], we adopt unified multi-domain post-training pipeline, in which domain-specialized expert models are first trained under shared framework and then consolidated into single general model through both model-level and data-level merging. Reinforcement learning with agentic reasoning requires: (i) carefully prepared training setups, including scalable environment construction, high-quality cold-start data, and well-calibrated RL task sets; (ii) dedicated infrastructure that can sustain high-throughput, asynchronous, and long-tailed multi-turn rollouts; and (iii) specialized training strategies that remain stable and effective across heterogeneous domains and varying difficulty levels. In this section, we present systematic pipeline that addresses these challenges and enables scalable agentic reinforcement learning. 4 LongCat-Flash-Thinking-2601 Technical Report Figure 3: Automated construction of executable domain graph. Starting from high-level domain specification, the pipeline synthesizes domain-specific tools, generates corresponding databases schema and tool schema implementations. Building on these we construct verified tool dependency graph that serves as the foundation for environment scaling. 3.1 RL Preparation Before conducting large-scale reinforcement learning, several essential preparation steps are required. Standard reinforcement learning relies on two core components: (i) well-initialized policy that exhibits basic task-relevant behaviors, and (ii) principled training task set that enables stable and effective learning. In addition to these shared components, agentic reinforcement learning further requires reliable and scalable environment foundation to support long-horizon, interaction-driven training. 3.1.1 Environment Traditional reasoning is largely environment-free, operating purely within internally linguistic space. In contrast, environments constitute defining component of agentic systems, as they directly determine what an agent can perceive, how it can interact with external systems, and what actions are executable in the world. We specifically introduce two particularly challenging types of environments in agentic tasks: agentic coding and agentic tool-use. For agentic coding tasks, the environment takes the form of an executable code sandbox, which must simultaneously support complex real-world toolchains, ensure strict execution correctness, and remain stable and reproducible under large-scale parallel usage. For general agentic tool-use tasks, the environment must model complex and diverse interactions across heterogeneous tools and databases in consistent manner. These requirements call for dedicated and domain-specific environment designs, which we describe in detail below. Code Sandbox For each coding task, the agent must operate within an executable code sandbox and interact with various terminal tools in real time. This setting introduces two major challenges: the need for efficient large-scale resource scheduling and the heterogeneity of tools and execution environments. In large-scale training, the sandbox system must simultaneously ensure flexibility and robustness. To address these challenges, we design scalable execution sandbox system that provides unified tool interfaces with high-throughput interaction. To handle the heterogeneity of real-world code execution settings, we consolidate commonly used toolsincluding search, file reading and writing, code editing, and shell executioninto standardized environment interface. To further support large-scale training, we implement high-concurrency sandbox scheduler that asynchronously provisions and recycles sandbox instances, distributes tasks across workers, and executes thousands of sandboxes in parallel, thereby removing environment startup and blocking overhead from the critical training path. Agentic Tool-Use Environment The core characteristics of advanced agentic reasoning capability is generalization, which is the ability to acquire effective behaviors in known environments and reliably transfer them to previously unseen settings. To train such generalized agents, we posit that transferable agentic reasoning capability should emerge from exposure to diverse tool sets and interaction patterns. While it is infeasible to design single meta-environment that fully captures the complexity of real-world tool use, we find that sufficiently diverse set of environments can provide basis, enabling transferable and generalizable tool-use behaviors. In practice, however, constructing such environments presents non-trivial challenges, as different domains impose distinct requirements on interaction interfaces, execution mechanisms, and evaluation criteria. LongCat-Flash-Thinking-2601 Technical Report Figure 4: Verifiability-preserving environment expansion. Environment is progressively expanded from seed executable tool chain through controlled graph growth. We design fully automated pipeline that converts high-level domain specification into an executable graph. As illustrated in Figure 3, the pipeline synthesizes domain-specific tool set from the domain definition and abstracts their functionalities into unified database schemas with explicit tooldatabase mappings. Based on this schema-level specification, it then automatically generates the database implementation and the corresponding tool code. To ensure correctness and robustness, the generated code is validated through unit testing and an auxiliary debugging agent. In practice, this pipeline achieves success rate exceeding 95% in transforming schema-level designs into fully executable tool implementations. Finally, we construct the tool dependency graph from the verified tool set, which serves as the basis for subsequent environment construction and expansion. Using our automated pipeline, we construct collection of domain-specific tool graphs covering over 20 domains. Each tool graph contains more than 60 tools organized in dense dependency graph, paired with corresponding executable tool-specific database schema implementations, providing sufficient structural complexity for large-scale exploration and diverse environment construction. We next introduce our pipeline for constructing executable environments based on domain graphs. Given domainspecific tool graph G, we first sample moderate-size tool chain s1 G. The sampling probability of previously selected tools is progressively reduced to promote task diversity. For each tool in s1, we sequentially instantiate the corresponding database states, ensuring that all required tool dependencies are fully satisfied. After the tool-associated databases are constructed, we automatically generate task grounded on this tool chain. The sampled s1 serves as an executable tool chain for the generated task. To mitigate potential human prior bias, the task generator was restricted to utilizing the full tool chain exclusively, without any deliberate task design. Specifically, each generated task consists of three components: (i) task description, (ii) user profile, and (iii) evaluation rubrics. To ensure reliable supervision, the evaluation rubrics are validated through multiple rounds of consistency checking, guaranteeing that any executable tool chain can be accepted as correct solution and that incorrect or incomplete trajectories are consistently rejected. We note that task difficulty arises from two complementary sources: interaction complexity and environment complexity. Interaction complexity is ensured by synthesizing diverse user prompts that require varying degrees of clarification, planning, and multi-step interaction, encouraging the model to engage in rich and adaptive agentic behaviors. Environment complexity is quantified using structural properties of the underlying tool graph, including the number of involved tool nodes and the connectivity density among them. natural way to increase environment complexity is to expand the initial tool chain s1 into larger subgraph R(s1) by introducing additional tool nodes from G, together with maintaining their database instances. Notably, after such expansion, the resulting environment may additionally contain multiple valid executable tool chains. However, as the subgraph grows, maintaining cross-tool database consistency becomes increasingly challenging. In particular, due to the constraints imposed by the tool dependency graph, the uncontrolled introduction of additional tool nodes may trigger cascade of previously unmet dependencies, requiring extensive database augmentation and the maintenance of large number of auxiliary tool nodes. Once the database state becomes inconsistent under such expansion, tool invocations may yield unpredictable outcomes, causing even valid tool trajectories to fail execution. This can lead to correct solutions being incorrectly judged as failures, thereby introducing biased negative rewards during training. To mitigate this issue, instead of blindly injecting random tools, we expand environments by extending executable tool chains. Specifically, starting from s1, we perform BFS-style expansion on the tool graph and only add new tool node if all of its dependencies are already satisfied by previously instantiated tools, whose database states have been 6 LongCat-Flash-Thinking-2601 Technical Report fully constructed. This controlled expansion strategy allows us to progressively increase environment complexity while preserving executability and reliable supervision. Through this controlled graph expansion, we extend s1 into more complex subgraph R(s1). We denote the resulting extended environment as E1 = R(s1). Let D1 = E1 denote the remaining unused tool nodes for E1. further design choice is whether to initialize another seed executable tool chain s2 D1 and extend it into R(s2) to construct the environment: E2 = R(s1) R(s2), (cid:91) which to further increase environment complexity. We make this decision based on (i) the structural complexity of the current environment, c(E1), (ii) the difficulty of identifying new valid tool chain from the remaining graph, g(D1), and (iii) the number of remaining unused nodes D1. Concretely, given the current environment: En = (cid:91) i= R(si), we define the remaining unused tool set as Dn = En, and sample new seed chain sn+1 from Dn with probability: = (cid:0)c(En), g(Dn), Dn(cid:1), where g(Dn) is measured by the number of attempts required by strong solver to discover an alternative valid tool chain from Dn, and () is monotonic decision function controlling the environment growth process. In practice, when > τ , where τ is predefined threshold, we initialize new seed golden tool chain sn+1 from Dn and extend it into R(sn+1). To ensure minimum level of environment complexity, we further introduce fallback mechanism. If the resulting expansion En contains only small number of tool nodes, we randomly sample an additional moderate-size tool chain from and incorporate it into the environment, while ensuring the database consistency with current tool chains. This guarantees that each constructed environment contains at least 20 tools, providing sufficient structural complexity for meaningful agentic interaction and exploration. This automatic environment construction strategy allows us to progressively scale environment complexity while maintaining reliable supervision signals, enabling stable and effective large-scale agentic reinforcement learning. 3.1.2 Initial Policy The cold-start stage serves critical role in initializing the policy for subsequent reinforcement learning stage. Instead of prioritizing immediate gains on standard benchmarks, our primary goal is to prime the model for effective largescale exploration. high-quality cold-start policy must exhibit diverse reasoning patterns and stable interaction formats, ensuring that the subsequent RL stage can be conducted efficiently while retaining general thinking capability. Consequently, we evaluate the cold-start model from more fundamental perspective, focusing on: (1) its proficiency on the specific tasks designated for the RL stage, and (2) the diversity of its reasoning paths, as assessed through qualitative human inspection. While we report pass@k performance on standard benchmarks to gauge underlying reasoning competence, it serves as reference rather than the primary optimization target. major challenge in cold-start training lies in constructing high-quality data that can effectively prime the model for agentic behaviors. For some domains, such as mathematics, general coding, and agentic code tasks, large-scale data sources are available in real-world. In these cases, we curate and assemble trajectories from existing sources, and the core challenge lies in strict quality control and executability verification. However, for most of the agentic tasks, such as search and tool-use, high-quality real-world trajectories are largely unavailable. As result, we rely on carefully designed data synthesis pipelines to construct cold-start trajectories. We next describe how we instantiate these two complementary strategies for different agentic capabilities in detail. General Thinking Agentic capability require strong thinking capability as foundation. We therefore design strict data filtering pipeline to construct high-quality general thinking data. Specifically, we adopt K-Center-Greedy (KCG) selection algorithm guided by perplexity (PPL) [Sener and Savarese, 2018, Du et al., 2023, Wu et al., 2023]. Existing PPL-based filtering methods either ignore sequence length or average PPL over the full sequence [Jiang et al., 2025, Zhang et al., 2025b], which masks localized hard tokens and discards informative samples. To address this, we introduce sliding-window PPL, which computes the maximum average PPL over all 512-token windows in sequence, capturing peak model uncertainty without dilution from global averaging. During KCG selection, we weight the distance between candidate sample and the current selected set by its sliding-window PPL score. This design jointly captures two complementary objectives: KCG preserves coverage over the original data distribution, while sliding-window PPL emphasizes samples that expose gaps in the models current reasoning capability. Using this pipeline, we downsample 210K general thinking samples from large-scale corpus. Models trained on this subset outperform those trained on the full corpus on multiple reasoning benchmarks. 7 LongCat-Flash-Thinking-2601 Technical Report Agentic Coding For agentic coding tasks, large-scale software development platforms provide rich source of real-world trajectories. However, raw trajectories collected from such sources are often noisy, partially incorrect, or irreproducible, and ensuring their reliability and executability becomes critical challenge. To address this, we construct strict curation pipeline for code-interaction trajectories. We require that all retained trajectories are fully executable and verifiable within reproducible environments, and we keep only those that correctly resolve the target issue while preserving existing functionality. To further avoid learning spurious behaviors, we apply fine-grained action-level filtering, removing erroneous, redundant, or speculative operations that do not contribute to correct problem solving. In addition, to preserve long-horizon reasoning patterns commonly observed in real debugging processes, we retain trajectories involving long and iterative debugging by compressing earlier steps, allowing long-horizon code reasoning to be maintained without length constraints. Agentic Search For agentic search capability, constructing high-quality trajectories requires explicitly modeling multi-step evidence gathering and complete condition verification, while avoiding spurious shortcuts based on partial information. Such structured reasoning traces are rarely available in real-world search logs. We therefore construct synthetic reasoning trajectories that prioritize correctness, reasoning completeness, and robustness against shortcut behaviors. We therefore construct synthetic reasoning trajectories that prioritize correctness, reasoning completeness, and robustness against spurious shortcuts. We apply strict filtering to ensure trajectory quality, removing trivial cases and enforcing consistent reasoning and tool-use format. To avoid shortcut learning such as lucky guesses based on partial evidence, we require trajectories contains explicitly verification of all conditions specified in the query. We further retain trajectories that involve long and iterative exploration by compressing earlier steps, allowing long-horizon reasoning to be preserved without length constraints. Finally, we scale data collection and increase behavioral diversity by reusing rollout trajectories from subsequent reinforcement learning stages. Agentic Tool-Use For agentic general tool-use capability, the main challenge lies in modeling complex, stateful interactions across heterogeneous tools and databases with diverse schemas and dependency structures. Such environments and interaction traces are also difficult to obtain or standardize from real-world sources. We construct scalable data synthesis pipeline built on top of our environment scaling pipeline that models realistic tool-use environments covering 33 representative domains by jointly defining domains, tool schemas, database states, and task objectives, and generating multi-step tasks grounded in structured tool dependencies. To ensure diversity, we explicitly promote variability in three aspects: domain coverage, trajectory structure, and interaction length. Each task admits multiple distinct correct tool-call trajectories, and interaction horizons range from short dialogues to long, multi-turn executions, capturing wide spectrum of task difficulty and behavioral patterns. To ensure data quality, all synthesized trajectories are strictly filtered using rubric-based outcome validation and turn-level quality control. Only trajectories that correctly reach the target final state are retained. Within these trajectories, we apply turn-level loss masking to exclude low-quality turnssuch as failed tool calls or format violationsfrom loss computation, ensuring the model learns only from correct actions while preserving the full interaction context. 3.1.3 RL Task Set In reinforcement learning, the environment defines what the agent can do, while the task set determines what the agent is trained to do. Together, they shape how the agent explores the environment, allocates computation, and refines its policy through repeated rollouts. well-designed task set should be both informative and of appropriate complexity, so that it can provide effective learning signals without being either trivial or intractable. common practice is to first construct diverse collection of environments paired with tasks instructions spanning multiple domains and difficulty levels, and then assess task suitability by evaluating the models pass rate. Based on this signal, tasks that are neither trivially solvable nor prohibitively hard are selected for reinforcement learning. However, the availability of training tasks varies significantly across domains. For some domains, such as coding, large number of complex and high-quality tasks already exist and can be directly collected and curated. In contrast, for domains like agentic search and tool-use, suitable tasks are scarce or do not exist in readily usable form, making direct collection insufficient. We therefore introduce principled synthesis pipelines to construct task sets spanning diverse complexity levels for agentic search and tool-use scenarios. Agentic Search We identify two fundamental difficulty factors that characterize search problems: (i) multi-hop reasoning over relational entity chains, and (ii) reasoning over single entity under multiple ambiguous constraints. Most complex search tasks can be viewed as compositions or iterative refinements of these two factors. Graph-based QA Synthesis: To model multi-hop reasoning difficulty, we construct graph-based question-answer tasks through systematic pipeline that builds relational graphs from Wikipedia entities and generates challenging reasoning problems. Our approach begins by extracting low-frequency entities from Wikipedia as initial seed nodes, 8 LongCat-Flash-Thinking-2601 Technical Report Figure 5: The Execution Workflow of Our Scalable Asynchronous Agentic RL Framework. then iteratively expands the graph by sampling from the existing entity set, retrieving their Wikipedia pages, and incorporating related entities along with their corresponding relations until reaching predefined size threshold. Once the graph is constructed, we sample multiple fixed-size connected subgraphs and use them to generate question-answer pairs. During question generation, we leverage large language models to create questions that correspond to the subgraph information, then deliberately obfuscate explicit details such as numerical values, entity names, geographical locations, and temporal markers to maximize reasoning complexity. To ensure quality and correctness throughout the pipeline, we employ an LLM-as-a-judge approach at critical steps including entity relation extraction, question generation, and obfuscation to maintain baseline accuracy. Finally, for each generated question-answer pair, we utilize an agent-based methodology to identify other potential correct answers and assess their validity, retaining only those pairs where the original answer is correct and all other identified potential answers are incorrect. Agent-based QA Synthesis: To model ambiguity-driven difficulty, we present scalable, efficient data synthesis pipeline, where multi-agent collaborative interactions are orchestrated by Finite State Machine (FSM). Within this framework, an Entity Extraction Agent identifies representative long-tail entities and extracts their salient attributes, which serve as the foundational ground truth for synthesis question. Subsequently, Question Synthesis Agent utilizes random sampling of these attributes to formulate tailored questions. To ensure precision, Verification Agent leverages search and browse tools to rigorously validate whether the ground truth satisfies all constraints specified in the question, thereby mitigating the risk of entity-question mismatches. For each validated question, the Answer Generation Agent utilizes search and browse tools to produce candidate answers. The Judgment Agent evaluates the alignment between these candidates and the predefined ground truth. When the Judgment Agent identifies non-ground-truth answer that still satisfies the verification criteria, it signifies multi-answer conflict. To resolve this, the system randomly incorporates additional attributes of the ground truth entity and triggers re-synthesis of the question to ensure its uniqueness. This pipeline facilitates the high-throughput generation of diverse, high-quality QA pairs across multiple domains. Furthermore, it enables an automated difficulty grading mechanism based on the accuracy metrics of the Answer Generation Agent. Agentic Tool-Use For agentic tool-use, our task set is directly constructed through the environment scaling pipeline described in Section 3.1.1. Each synthesized environment naturally defines standalone task, and collection of such environments constitutes the final task set. 3.2 Scalable Asynchronous Agentic RL Framework Compared with single-turn scenarios such as standard reasoning tasks, agentic training involves multi-turn interactions with variable environments (ENV) or tools, which pose new challenges to RL Infrastructure. During typical multiturn rollout stage, it repeatedly interleaves the LLM generation, environment execution, and reward assessment to construct the final trajectory for RL training. In this setting, trajectories are not only long-tailed skewed but also involve unpredictable and latency-skewed environment interactions, which lead to device underutilization under batch setting [Fu et al., 2025, Zhang et al., 2025c, Lu et al., 2025a]. Additionally, our production cluster consists of mid-range accelerators, especially with only around 60GB of available device memory. The hardware constraints, together with its software ecosystem, pose significant challenges to achieving stable and scalable agentic RL training. To address these problems, we extend our multi-version asynchronous training system, DORA (Dynamic ORchestration for Asynchronous rollout) [Team et al., 2025c], to fully support large-scale RL training in multi-turn agentic scenarios. LongCat-Flash-Thinking-2601 Technical Report Figure 6: Workflow of Prefill-Decode Disaggregation with KV-cache Swapping. The rollout manager coordinates prefill (P) and decode (D) instances and collects final results from instances. Upon receiving new request, and perform an initial handshake and launch asynchronous chunked KV-cache transfers. When the device KV-cache usage reaches given watermark, swapping is triggered and can be executed concurrently across both and instances. As illustrated in Figure 5, our controllers adopt producer-consumer architecture consisting of RolloutManager (which manages the rollout stage), SampleQueue (which controls the sample staleness), and Trainer (which manages the Experience-Maker and training stage). These components run on different nodes and are primarily responsible for logic control and coordination via Remote Procedure Call (RPC), while workers running on CPUs or accelerators execute the actual tasks. We propose several key techniques to enable efficient, scalable, and stable agentic RL training. Fully Streaming Asynchronous Pipeline To minimize device idleness in the agentic setting, we introduce fully streaming asynchronous pipeline both within the rollout process and between rollout and training, based on streaming RPC [Team et al., 2025c]. Within the rollout loop in the RolloutManager, we remove batch barriers, enabling LLM generation, environment execution, and reward computation to be executed on remote workers at the granularity of individual samples. This prevents accelerators from idling while waiting for batched ENV calls to complete. To further address the long-tailed generation problem with training stability, our DORA system enables multi-version asynchronous training, where trajectories generated by different model versions are immediately enqueued upon completion. Within step, our multi-version generation instances continue rollout using multiple previous model versions, while the Trainer can initiate training as soon as its conditions are met, or elastically scale up additional generation instances for free extra throughput when training devices are idle. Scaling to Large-scale Agentic Training Our algorithm setting requires large number of environments, e.g., up to 32,000 environments running across roughly 400 physical machines with thousands of accelerators. However, the intensive interactions between these environments lead to single-machine bottlenecks for our RolloutManager when scaling out, as each interaction typically involves few CPU operations. To address this issue, we first decompose the original design into Lightweight-RolloutManager, which manages global control metadata, and multiple RolloutController, each of which manages the lifecycle of virtual rollout group in data-parallel manner. virtual rollout group consists of multiple trajectories and associated physical machines (including generation instances and ENV instances). Secondly, to flexibly schedule massive environments in our production jobs, we extend the PyTorch RPC framework [Damania et al., 2023] to provide CPU-idleness-aware remote function invocation and object instantiation. This extension enables instantiating and executing remote environments on arbitrary or idle machines, which enables efficient deployment of massive environments. PD Disaggregation with CPU Swapping For efficient generation of LongCat-Flash-Thinking-2601, 560B MoE model, on our accelerators, we employ high degree of expert parallelism together with graph-level compilation for decode. However, in multi-turn agentic training, frequent incoming requests with long contexts lead to workload imbalance within an expert parallelism group: ranks assigned longer contexts consume disproportionately more computation and communication bandwidth, becoming the performance bottleneck. To address this issue, we introduce PrefillDecode (PD) Disaggregation in RL training [Zhong et al., 2024, Qin et al., 2024] as shown in Fig. 6. For generation instances, we deploy prefill nodes and decode nodes on separate device groups, allowing the decode execution graph to run without being interrupted by the prefill workloads of newly arriving requests. This prevents degradation in generation efficiency and maintains high throughput during multi-turn rollouts. PD Disaggregation, however, introduces additional challenges, including KV-cache transfer overhead and expensive recomputation overhead when the on-device KV-cache on the decode nodes is insufficient, especially on our accelerators. To mitigate KV-cache transfer cost, we aggregate KV-cache blocks in chunks level and enable asynchronous transmission between PD nodes. 10 LongCat-Flash-Thinking-2601 Technical Report We enable overlapping between previous chunks with the computation of subsequent chunks to minimize the overhead of KV-cache transfer. To avoid recomputation caused by limited KV-cache memory on devices, we further introduce CPU-resident KV-cache, which dynamically swaps KV-cache blocks in and out as needed. This design eliminates recomputation overhead due to insufficient on-device KV-cache capacity and helps sustain high throughput on our accelerators. Overall, our agentic RL training framework achieves strong performance and stability at industrial scale on our 560B models, supporting tens of thousands of accelerators and environments. According to runtime statistics from our production jobs, our request load ratio1 is approximately 63% throughout the entire rollout procedure. This metric quantifies the impact of long-tailed requests on generation throughput, with higher values indicating better utilization. We do not achieve full request load ratio during training because we limit new requests to the older model version to control average staleness at the expense of efficiency. Additionally, within single step that may include multiple load-balancing operations, we employ two-phase strategy: in the initial rollout phase before the first load-balancing when there are no long-tailed generation, we allow higher number of requests per device (e.g., 8), and subsequently limit the requests per device to an optimal level (e.g., 4) to avoid recomputation and improve generation efficiency. In the future, we plan to adopt more optimistic staleness control strategies and explore staleness-aware stability techniques to enable more efficient asynchronous training. In conclusion, our multi-version asynchronous training system, DORA, is 2 to 4 times faster than synchronous training across our production jobs spanning different scenarios. 3.3 RL Training Strategy Reinforcement learning has become central mechanism for continuously improving models reasoning capability. Traditionally, RL training strategies mainly focus on stabilizing policy optimization, improving sample efficiency, and managing the explorationexploitation trade-off under relatively homogeneous task distributions and single-step rollouts. In these settings, tasks can exhibit widely varying difficulty levels, leading to highly imbalanced learning values across training samples. Aside from curriculum learning, we dynamically allocate rollout budgets within each training step to make effective use of limited computational and training-time budgets, concentrating learning resources on high-value tasks. To further improve training effectiveness, we additionally model verification as an auxiliary task that supports generation and accelerates optimization. When extended to the agentic setting, reinforcement learning faces new challenges arising from multi-turn interactions and unpredictable environment feedback, which place stringent demands on the models effective context length. To address this issue, we introduce context management strategy that enables the model to support long-horizon trajectories under limited context windows, while preserving the most informative context. When further extended to agentic tool-use scenarios, the training problem becomes substantially more challenging. As mentioned earlier, generalization and robustness are particularly important in this setting. To promote generalization, agentic tool-use environments are drawn from our environment scaling pipeline. Tasks can span heterogeneous environments across diverse domains in single batch, amplifying training instability and imbalance. Under careful co-design of training strategies and infrastructure support, we perform efficient and stable multi-domain environment training at scale. Generalization to real-world settings remains challenging, as real-world environments are inherently imperfect. To improve robustness, we explicitly incorporate environmental imperfections into the training process, enabling the model to learn resilient behaviors under non-ideal conditions. Together, these designs form unified training strategy that enables stable, efficient, and scalable agentic reinforcement learning. 3.3.1 General Training Strategy In large-scale reinforcement learning, the training set spans tasks with highly diverse difficulty levels. As result, naive uniform treatment of all tasks often leads to inefficient learning. To this end, we design set of training strategies that are applied throughout all our reinforcement learning recipes. Specifically, we introduce curriculum learning to progressively increase task difficulty, apply dynamic budget allocation to focus computation on the most informative tasks under the current model state. We incorporate self-verification as an auxiliary task to further improve optimization efficiency and effectiveness. Training Objective We adopt Group Sequence Policy Optimization (GSPO) as our training objective, as its empirical effectiveness on MoE models and provides more stable sequence-level optimization for long-horizon agentic trajectories. 1Request load ratio is continuous value ranging from 0 to 1, aggregated over all generation devices across the entire rollout period. For example, our accelerator can accommodate approximately 4 requests when the maximum response length is set to 64K to avoid recomputation when considering only the available on-device KV-Cache blocks. The request load ratio will be 25% if there is 1 request and 0% when there are no requests on that accelerator, particularly in long-tailed generation scenarios. 11 LongCat-Flash-Thinking-2601 Technical Report Given an input x, we sample group of trajectories {yi}G i=1 from the old policy πθold and optimize: JGSPO(θ) = xD, {yi}G i=1πθold (x) (cid:34) 1 (cid:88) i=1 (cid:16) min si(θ) ˆAi, clip(si(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:35) (cid:17) , (1) where ϵ is the clipping threshold. Following Zheng et al. [2025], we adopt group-based advantage estimation and define the importance ratio si(θ) at the sequence level based on normalized likelihoods. We rely primarily on outcome-oriented supervision and relax penalties on long trajectories, allowing effective strategies to emerge naturally during training. Curriculum Learning To improve learning effectiveness, we adopt curriculum learning strategy that progressively structures the training process. Concretely, our curriculum is organized along two complementary axes: task difficulty and capability requirement. Task difficulty is quantified using the models pass rate estimated prior to optimization, where lower pass rates indicate more challenging tasks. In parallel, we characterize tasks by the agentic capabilities they primarily exercise, such as basic tool invocation, multi-step planning, or autonomous decision making. During early training stages, we prioritize tasks that are either easier to learn or expose capabilities that are expected to be autonomously reused by the agent when solving harder tasks. As training progresses, we gradually shift toward tasks that are both more difficult and require more advanced combinations of agentic capabilities. This two-dimensional curriculum allows the model to first acquire reusable agentic skills and then compose them to solve increasingly complex problems. Empirically, this curriculum strategy improves the overall task pass rate and yields particularly significant gains on the most challenging tasks. Our analysis in agentic settings reveals that these improvements stem from three main factors: Tool-use generalization: Skills acquired from simpler taskssuch as tool selection and constraint handlingtransfer effectively to more complex scenarios, substantially reducing tool-call failures. Interaction efficiency: Improved understanding of task instructions enables more thorough internal reasoning and reduces redundant clarifications or unnecessary tool invocations, thereby decreasing the number of interaction rounds. Planning capability: Enhanced joint reasoning over multiple constraints such as time, location, and entities enables more direct task completion with fewer corrective iterations. Dynamic Budget Allocation We further apply dynamic budget allocation within training batch to prioritize tasks that provide higher learning value under the current model state. We observe that tasks whose difficulty is well matched to the models current capability yield substantially higher learning gains under fixed rollout budget. Existing large-scale RL pipelines typically assign uniform rollout budget to all tasks. Recent research starts to explore adaptive rollout allocation [Li et al., 2025], but most approaches rely on predefined value functions. However, this assumption breaks down as the models capabilities evolve continuously during training, causing the set of tasks that provide the most informative learning signals changes accordingly. To address this, we propose dynamic rollout budget allocation strategy that adapts to the models real-time training state. Specifically, we quantify the capability of the current policy πθt by monitoring real-time training metrics mt (e.g., pass rate). We map both the specific task τi and mt via dynamic value function: vi,t = (τi πθt, mt) where vi,t indicates the estimated value of task τi, characterizing the models evolving preference distribution over the task space. Based on this value estimation, we employ heap-based greedy algorithm to compute the rollout allocation that maximizes the aggregate learning value of the current training batch. (2) Self-Verification Beyond using the model solely as an actor policy for generation, we additionally leverage the model as verifier to assess the quality of its own on-policy trajectories. We observe notable asymmetry: even advanced reasoning models that are capable of generating high-quality trajectories often struggle to reliably assess the correctness of those trajectories in the absence of explicit ground-truth signals. This motivates us to explicitly enhance the models self-verification capability and use it as an auxiliary signal to improve its reasoning ability in specific domains. Concretely, we introduce on-policy self-verification as dynamically activated training phase during reinforcement learning. When the generator exhibits signs of stagnation or convergence to local optima, we trigger verification stage in which the model evaluates its own rollout trajectories. Compared to generation, verification is comparatively easier task and typically yields higher rewards. To further enhance the effectiveness of self-verification, we adopt tailored training recipe. Specifically, we ensure that verification is emphasized on challenging cases, and the influence of verification is coupled with the quality of the corresponding generated trajectories, so that the auxiliary signal encourages faithful improvement of generation rather than degenerate shortcut behaviors. Empirically, introducing onpolicy self-verification as an auxiliary task accelerates model convergence, leading to improved generation performance. 12 LongCat-Flash-Thinking-2601 Technical Report Figure 7: Pass@1 accuracy of BrowseComp with different context management strategies. 3.3.2 Agentic Specific Strategy In agentic scenarios, the interaction pattern naturally shifts from single-turn generation to multi-turn trajectories that interleave model reasoning and tool invocations. As task complexity increases, both the number of interaction turns and the length of tool responses grow, making the total context length highly variable and often difficult to control. In practice, this frequently leads to context window overflow, truncated reasoning chains, and incomplete task execution. Therefore, effective context management becomes necessary component for reinforcement learning with tool use under limited context windows. Context Management We design hybrid context management strategy to support long-horizon trajectories under limited context windows. Existing agentic models mainly adopt two strategies for context management: Summary-based Management: When the cumulative context length exceeds predefined token threshold, historical tool call results are distilled into concise summary that replaces the original context for contextual continuity. Based on the framework of ReSum [Wu et al., 2025], we employ the model itself as the summary tool and conduct series of comparative experiments with different token threshold values, ultimately identifying the optimal threshold of 80K tokens. More details can be found in Appendix B. Furthermore, to enhance the models summarization performance, we synthesize high-quality dataset consisting of 15K samples specifically for the cold-start phase. Empirical results demonstrate that this yields approximately 3% gains in accuracy. Discard-based Management: When the context length exceeds predefined threshold, the model will discard the entire or partial historical context, and then resume or restart the generation process based on the truncated context. Following DeepSeek-V3.2 [DeepSeek-AI et al., 2025], we adopt the discard-all strategy in our work. Combining the above two strategies, we design hybrid context management method tailored for agentic reasoning.Specifically, we first apply summary-based compression whenever the context window exceeds our predefined limit of 80K tokens.When the interaction exceeds the maximum number of turns, we trigger discard-all reset and restart generation with an initialized system and user prompt derived from the original question. We evaluate these three strategies on the BrowseComp benchmark [Wei et al., 2025]. As illustrated in Figure 7, under heterogeneous computational budgets, context management yields substantial performance improvements by enabling the model to scale test-time computation resources. Notably, the hybrid strategy outperforms the other two in most cases and demonstrates the highest efficiency throughout the experiment, starting from 55.8% and reaching peak of 73.1%. This superior performance stems from dynamic switching between compression and reset, governed by context window and interaction turn constraints, which achieves favorable trade-off between critical reasoning context retention and computational overhead control. Besides, we adopt progressive discard schedule that gradually increases the discard threshold, allowing gradual increase in the number of reasoning steps for more challenging samples. 3.3.3 Training Strategy with Scaled Environment Introducing scaled environments further elevates the complexity of reinforcement learning beyond tool-augmented settings. In this stage, we conduct large-scale training over environments that span multiple domains and exhibit high heterogeneity. We expect that training over such diverse environments encourages the model to acquire transferable 13 LongCat-Flash-Thinking-2601 Technical Report Figure 8: Training reward of LongCat-Flash-Thinking-2601 during large-scale multi-environment agentic RL. agentic behaviors that generalize beyond domain-specific patterns. This heterogeneity introduces new challenges at both the algorithmic and system levels: the training process must simultaneously maintain cross-domain generalization, ensure stable optimization under highly diverse task distributions, and remain efficient under long-tailed and imbalanced environment workloads. To address these challenges, we adopt multi-domain environment training paradigm that jointly optimizes across diverse environments within each training batch. To ensure both stability and scalability, we need co-design of training strategies and our asynchronous infrastructure. Inspired by the environment scaling construction procedure, we notice that real-world environments are inherently imperfect, Accordingly, we explicitly introduce environmental noise during training to improve robustness to heterogeneous and unreliable environment feedback. Multi-Domain Environment Training To improve cross-environment generalization and training stability, we adopt multi-domain environment training strategy that jointly optimizes across diverse environments within each training batch. Through environment scaling, we construct tens of thousands of environments spanning more than 20 domains, providing broad coverage of heterogeneous interaction patterns. Training under this setting introduces non-trivial system challenges. Specifically, to maintain training stability, it is necessary to ensure that all domains contribute comparably to the overall training process, while preventing any single training batch from being dominated by small subset of domains. This algorithm constraint significantly degrades the efficiency of our DORA system, as it breaks the design principle of asynchrony: the trainer may be forced to wait for slow or rare long-tailed domains to produce enough samples, while faster domains accumulate excessive rollout trajectories, leading to scheduling bubbles and device underutilization. To mitigate this issue, we support configuring separate oversampling ratios for different data types and domains. In practice, we increase the rollout quota for more challenging or low-throughput domains, allowing them to contribute sufficient samples without blocking the overall pipeline, while faster domains are sampled at lower effective rate. This design relaxes the strict per-batch balancing constraint and maintains the high-throughput property of asynchronous training using DORA, while still maintaining roughly balanced data mixture at the training stage with training convergence guarantee. To ensure the compatibility of dynamic budget allocation in this training setting with our asynchronous infrastructure, we introduce an oversampling coefficient for each task based on its historical pass rate. Tasks with lower success rates are assigned higher oversampling coefficients, effectively allocating more rollout budget to more challenging tasks. Concretely, each task is duplicated into multiple groups according to its oversampling coefficient, and each group independently computes advantages during training. This design approximates dynamic budget allocation while preserving the simple and fully asynchronous scheduling behavior of DORA, achieving dynamic token budget control with minimal system complexity. We present the training reward curves of LongCat-Flash-Thinking-2601 in Figure 8 and the corresponding performance on agentic benchmarks at different training steps in Figure 9. The training reward in Figure 8 exhibits stable and 14 LongCat-Flash-Thinking-2601 Technical Report Figure 9: Agentic benchmark performance of LongCat-Flash-Thinking-2601 during RL training using exclusively synthetic general agentic data. Table 1: Performance comparison across different training strategies. ColdStart Training w/o Noise Training w/ Noise Dataset VitaBench (Avg@4) VitaBench-Noise (Avg@4) Tau2Bench (Avg@4) Tau2Bench-Noise (Avg@4) 10.0 6.3 78.8 58.8 28.6 13.3 87.1 62. 29.3 20.5 88.2 67.1 consistent increasing trend throughout training, indicating that our algorithminfrastructure co-design effectively ensures training stability at scale. Moreover, the performance of agentic benchmarks in Figure 9 demonstrates strong generalization across multiple benchmarks, which validates the effectiveness of our environment synthesis pipeline. Empirically, multi-domain environment training achieves higher average task completion rate across environments, with especially significant improvements on the most challenging environments. Moreover, the model attains strong performance in randomly generated environments, demonstrating strong generalization. Robust RL We explicitly incorporate environmental imperfections into the training process to improve robustness. Existing agentic models suffer from significant performance degradation when deployed in previously unseen or imperfect environments. This issue largely stems from common assumption in current agentic training paradigms: agents are typically trained with carefully curated instructions and interact with stable, well-controlled environments. In contrast, real-world environments are inherently imperfect. Users exhibit diverse interaction styles and unpredictable behaviors, while tools may fail, return noisy outputs, or produce incomplete results due to various external factors. Rather than assuming idealized environments during training and relying on agents to adapt post hoc, we systematically 15 LongCat-Flash-Thinking-2601 Technical Report Figure 10: Framework of heavy thinking mode. analyze real-world noise and design automated pipeline to explicitly incorporate environmental imperfections into the training process. To avoid introducing unreliable or misleading reward signals, we ensure that the injected imperfections do not invalidate task solvability, but instead increase the difficulty and stochasticity of the interaction process. Concretely, we model two major sources of interaction noise in real-world agentic scenarios: instruction noise, which captures ambiguity and variability in user interaction patterns, and tool noise, which simulates execution failures, inconsistent responses, and partial results from external tools. During agentic reinforcement learning, we introduce these noises progressively using curriculum-based strategy. We measure the robustness of an agentic model as the performance gap between perfect and imperfect environments on the same task. Starting from mild perturbations, we gradually increase noise difficulty and diversity as the model demonstrates sufficient robustness at the current level. This adaptive process ensures that training remains informative rather than overwhelming, and avoids inefficient exploration of excessively noisy regimes. Table 1 presents an ablation study of our robust training strategy under imperfect environments. The results show that training with noise achieves comparable or even slightly better performance on standard agentic benchmarks, while yielding substantial improvements under noisy and imperfect conditions."
        },
        {
            "title": "4 Test-Time Scaling Through Heavy Thinking",
            "content": "Test-time scaling (TTS) has emerged as an effective paradigm for improving model performance on complex reasoning tasks by expanding computation during inference. Recent advances demonstrate that increasing reasoning depththrough long chains of thought with self-reflection allows models to iteratively refine their reasoning process [OpenAI, 2024, DeepSeek-AI, 2025, DeepMind, 2025]. In parallel, approaches such as self-consistency and Monte Carlo Tree Search (MCTS) expand computation along the width dimension, exploring multiple reasoning trajectories to better approximate the models reasoning boundary. More recently, several frontier models have introduced heavy thinking [Team, 2025, OpenAI, 2025a, MoonshotAI, 2025, Hu et al., 2025], which aim to jointly scale both reasoning depth and width at test time. Empirically, these modes outperform strategies that scale only depth or width. However, the concrete implementation details of such heavy thinking remain largely undisclosed, limiting their reproducibility and systematic study. To further unlock reasoning capability and push beyond existing performance ceilings, we propose simple and effective heavy thinking framework that decomposes test-time computation into two complementary stages: parallel reasoning and heavy thinking. As shown in Figure 10, in the first stage, we allow thinking model to perform generation in parallel, producing multiple candidate reasoning trajectories to expand the breadth of exploration. In the second stage, we utilize summary model to conduct reflective reasoning over these trajectories, synthesizing their intermediate reasoning and outcomes to arrive at final decision. To support tool use and multi-turn conversation scenarios, we also introduce context memory module to store the history of messages. As shown in Figure 11, in each turn, the summary model will receive the history messages from the parallel reasoning stage to perceive context. We design specific prompt template to organize the permutations of the parallel trajectories (only retain answer content) at the current turn, and elicit the summary model to generate the final response, which aims to aggregate or refine the answers derived 16 LongCat-Flash-Thinking-2601 Technical Report Figure 11: The context message management for parallel reasoning and heavy thinking. from the parallel reasoning stage. We also constrained the final output response of the summary model to maintain consistency with the style and format of the parallel reasoning stage, enabling direct concatenation of the response from the summary model with the message history. Notably, the thinking and the summary module can either share the same model parameters or be instantiated as distinct models. To further enhance performance, we also introduce an additional RL stage specifically tailored to the summary phase. Empirically, we find that heavy thinking is effective across wide range of settings, including long chain-of-thought reasoning, tool-integrated reasoning, and fully agentic tool-use scenarios. By enabling test-time computation to scale adaptively along both reasoning depth and width, heavy thinking consistently outperforms self-consistency, with its performance advantage becoming increasingly pronounced as the test-time computational budget grows."
        },
        {
            "title": "5 Evaluation",
            "content": "5.1 Benchmarks and Configurations Our evaluation covers five aspects of model capability: mathematical reasoning, agentic search, agentic tool use, general reasoning, and coding. Mathematical Reasoning We evaluate mathematical reasoning using standard Olympiad-level benchmarks, including AIME 2025 [Zhang and Math-AI, 2025], HMMT 2025 (February) [HMMT, 2025], and IMO-AnswerBench [Luong et al., 2025]. Aside from these standard benchmarks, we introduce AMO-Bench [An et al., 2025], the most challenging dataset among existing Olympiad-level benchmarks. It contains 50 problems designed by human experts with both English and Chinese versions, enabling analysis of cross-lingual mathematical reasoning. We release the English version of AMO-Bench and the evaluation scripts2. Due to the limited size of these datasets, we report Avg@k metrics, using k=4 for IMO-AnswerBench and k=16 for all other math benchmarks. We primarily focus on tool-integrated reasoning performance, where the tool refers to code execution. For external models with code execution support, we enable this functionality during evaluation. For models without code support, we report either official results (if available) or their performance without tools. Agentic Search We evaluate agentic search capability on BrowseComp [Wei et al., 2025] and BrowseComp-ZH [Zhou et al., 2025], and report results under both settings with and without context management. For BrowseComp-zh, we notice some errors in the original annotations and manually revised the answers for 24 cases3. We attempted to reproduce the reported results of both open-source and closed-source models on these benchmarks under our agentic search framework. However, we observed consistently lower performance than the reported numbers. As result, for external models, we use results from their official reports. To enable fair and controlled comparison of search capability, we 2https://github.com/meituan-longcat/AMO-Bench/ 3https://github.com/AGI-Eval-Official/BrowseComp-ZH-revised LongCat-Flash-Thinking-2601 Technical Report Table 2: Performance (%) comparison across multiple benchmarks (Best in bold, best of open-source in underlined). indicates the score is from external reports. indicates the score using our heavy mode. indicates the w/ tools result is unavailable, and thus the corresponding w/o tools result is reported instead. Regarding BrowseComp, the performance is reported both without and with the context management technique. Open-Weights Reasoning Models Closed-Weights Reasoning Models Ours DeepSeek-V3.2Kimi-K2Qwen3-235B-A22BGLM-4.7Claude-Opus-4.5Gemini-3-Pro Thinking Thinking Thinking-2507 Thinking Thinking GPT-5.2LongCat-FlashThinking-xhigh Thinking-2601 Benchmark Architecture # Total Params # Activated Params AIME-25(Avg@16) HMMT-25(Avg@16) IMO-AnswerBench(Avg@4) AMO-Bench EN(Avg@16) AMO-Bench CH(Avg@16) MoE 671B 37B 93.5 93.5 77.7 51.9 52.0 MoE 1T 32B 99.1 95.1 78.7 56.0 51.8 BrowseComp(Pass@1) BrowseComp-zh(Pass@1) RW Search(Pass@1) 51.4 / 67.6 65.0 / - 74.0 - / 60.2 - / 62.3 63.0 τ 2-Retail(Avg@4) τ 2-Airline(Avg@4) τ 2-Telecom(Avg@4) τ 2-Avg(Avg@4) τ 2-Noise(Avg@4) VitaBench(Avg@4) VitaBench-Noise(Avg@4) Random Complex Tasks(Avg@4) HLE text-only (w/o tools) GPQA-Diamond(Avg@16) LCB (24.08-25.05)(Avg@4) OJBench(Pass@1) OIBench EN(Pass@1) SWE-bench Verified(Avg@5) 81.8 63.8 96.2 80.6 64.1 24.0 14.0 32.5 24.1 86.9 82.4 41.8 43.3 73.1 - - - 74.3 63.1 12.8 9.2 29. 24.4 85.4 75.1 42.3 39.0 71.3 MoE 235B 22B MoE 355B 32B Mathematical Reasoning w/ Tools 92.6 83.9 73.0 47.8 28.8 - - 20.5 95.3 98.1 84.0 62.4 35.1 Agentic Search 52.0 / 67.5 66.6 / - 69.0 Agentic Tool Using 71.9 58.6 47.3 59.3 44.3 14.5 6.5 28. 17.8 80.5 76.2 35.6 36.8 - - - - 87.4 66.0 18.3 10.8 25.3 General QA Coding 26.9 84. 84.8 44.6 30.8 73.8 - - - 100.0 98.6 82.8 66.0 67.7 - - 75.5 88.9 - 98.2 82.4 59.4 28.5 20.3 32.6 32.0 86. 82.8 46.7 50.0 80.9 - - - 99.8 99.8 86.7 72.5 74.9 - - 74.5 - - - 90.7 57.3 31.5 20.8 32.5 40.3 91. 88.1 61.2 58.2 76.2 - - - 100.0 99.6 - - - MoE 560B 27B 99.6 / 100.0 93.4 / 97.5 78.6 / 86.8 61.6 / 66.0 56.8 / 67.5 65.8 / - - 82.0 / - 56.6 / 73.1 69.0 / 77.7 79.5 82.0 - 98.7 80.6 65.0 24.3 19.0 17.2 34.5 92.9 - - - 80.0 88.6 76.5 99.3 88.2 67.1 29.3 20.5 35.8 25.2 80.5 / 85.2 82.8 42.2 47.7 70.0 additionally construct RWSearch4, challenging agentic search benchmark which consists of 200 real-world search queries that require complex reasoning and multi-step information retrieval. All models are evaluated on RWSearch without context management, ensuring fair comparison. Agentic Tool-Use We evaluate agentic tool-use capability on τ 2-Bench [Barres et al., 2025], VitaBench [He et al., 2025], τ 2-Noise, Vita-Noise, and Random Complex Tasks. For τ 2-Bench, we observe that the default user simulator occasionally exhibits abnormal behaviors that introduce uncontrolled noise into the evaluation. To address this issue, we replace the original simulator with GPT-4.1 and adjust the prompting strategy accordingly [Anthropic, 2025, OpenAI, 2025b, Google, 2025]. For the airline subset of τ 2-Bench, we further identify several annotation and environment issues that can lead to spurious failures and unreliable evaluation [Anthropic, 2025, OpenAI, 2025b, Google, 2025]. We therefore evaluate all models on fixed and cleaned version of the airline subset, in which 19 problematic cases are corrected. All these modifications have been publicly released for reproducibility5. For VitaBench, we update the evaluation setup by upgrading the verifier model to the strongest publicly available version and adopting stricter evaluation criterion, thereby further improving the reliability of the benchmark. We have publicly released this updated version of VitaBench6. We next introduce the construction and evaluation protocols for τ 2-Noise, Vita-Noise, and Random Complex Tasks. τ 2-Noise and Vita-Noise. To assess the robustness of agentic reasoning capability, we systematically analyze bias observed in real-world environments and design an automated noise injection pipeline that can inject realistic noise into arbitrary benchmarks. Based on this pipeline, we repeatedly and randomly inject noise into both the τ 2 and Vita benchmarks to construct τ 2-Noise and Vita-Noise, and report the averaged performance over multiple noisy instantiations. 4https://github.com/AGI-Eval-Official/RW-Search 5https://github.com/AGI-Eval-Official/tau2-bench-revised 6https://github.com/meituan-longcat/vitabench LongCat-Flash-Thinking-2601 Technical Report Random Complex Tasks. To evaluate the generalization of agentic reasoning capability, we introduce new evaluation protocol, Random Complex Tasks, built upon an automated task synthesis process that randomly generates complex, executable, and verifiable agentic tasks across diverse scenarios inspired by our environment scaling pipeline. For each evaluation run, we randomly sample 100 diverse and complex tasks spanning more than four domains, and calculate the Avg@4 score. To ensure reliability, we repeat the evaluation for three independent runs and report the average results across runs. We will release the noise injection pipeline, as well as integrate Random Complex Tasks into an open evaluation platform, to support future reproducibility and benchmarking. General QA We evaluate general reasoning on GPQA-Diamond [Rein et al., 2023] and HLE [Phan et al., 2025], which cover broad range of knowledge-intensive and reasoning-focused tasks. For GPQA-Diamond, we report Avg@16 to reduce variance introduced by sampling stochasticity. For HLE, since our model is text-only, we report results on the text-only subset and ensure that all compared models follow the same setting. We observe that the results of HLE are sensitive to prompt templates and scoring models. To ensure fair comparison, all models are evaluated using the official HLE-recommended prompt template and are scored with the official o3-mini scoring model and template. Coding We evaluate coding capability in two settings: code reasoning and agentic coding. For code reasoning, we use LiveCodeBench [Jain et al., 2025], OJBench [Wang et al., 2025], and OIBench [Zhu et al., 2025]. For LiveCodeBench, we evaluate on the 24082505 subset, covering the two most recent releases and comprising 454 problems. We report Avg@4 for LiveCodeBench. For OJBench and OIBench, due to evaluation cost and the long reasoning traces required by thinking models, we report Pass@1. For agentic coding, we use SWE-bench Verified, which is standard benchmark for software engineering agents. We adopt the third-party agent framework R2E-Gym7 as the execution backbone. To ensure correctness, we manually clean and fix small number of Docker images in the original benchmark, primarily due to dependency mismatches introduced by library upgrades, guaranteeing that all gold patches are executable. 5.2 Main Results As illustrated in Table 2, we compare LongCat-Flash-Thinking-2601 with several advanced open-weight and closedweight reasoning models. Specifically, the open-weight models include DeepSeek-V3.2-Thinking [DeepSeek-AI et al., 2025], Kimi-K2-Thinking [MoonshotAI, 2025], Qwen3-235B-A22B-Thinking-2507 [Yang et al., 2025], and GLM-4.7-Thinking [Z.AI, 2025], while the closed-weight models include Claude-Opus-4.5-Thinking [Anthropic, 2025], Gemini-3-Pro [Google, 2025], and GPT-5.2-Thinking-xhigh [OpenAI, 2025b]. Across comprehensive suite of benchmarks, LongCat-Flash-Thinking-2601 achieves highly competitive performance on traditional reasoning tasks and demonstrates strong advantages in agentic reasoning capabilities. Unless otherwise specified, inference is conducted with temperature= 1.0, top-k = 1, and top-p = 1.0. detailed breakdown of these capabilities is provided in the subsequent analysis. Mathematical Reasoning: On challenging mathematical reasoning benchmarks, LongCat-Flash-Thinking-2601 exhibits strong tool-integrated reasoning capability and consistently achieves first-tier performance. In particular, when equipped with heavy mode, LongCat-Flash-Thinking-2601 reaches performance comparable to leading closedsource models. Specifically, LongCat-Flash-Thinking-2601 with heavy mode attains perfect score on AIME-2025, achieves state-of-the-art score of 86.8 on IMO-AnswerBench, and delivers open-source state-of-the-art results on AMO-Bench. Notably, although slightly behind the strongest closed-source models on AMO-Bench (EN), LongCatFlash-Thinking-2601 remains the best-performing open-source model. Moreover, LongCat-Flash-Thinking-2601 exhibits comparable performances on both the English and Chinese version of AMO-Bench, indicating advanced mathematical reasoning and tool-use capability in non-English settings. Agentic Search: LongCat-Flash-Thinking-2601 achieves state-of-the-art performance on both BrowseComp and BrowseComp-ZH. With context management enabled, it attains 73.1 on BrowseComp and 77.7 on BrowseComp-ZH, surpassing all evaluated models. On RWSearch, private benchmark designed to evaluate real-world complex search scenarios, LongCat-Flash-Thinking-2601 achieves score of 79.5, second only to GPT-5.2-Thinking. Agentic Tool Using: LongCat-Flash-Thinking-2601 demonstrates state-of-the-art agentic tool-use capability among open-source models. It achieves strong performance on τ 2-Bench and VitaBench, including competitive results on their noise-augmented variants. In particular, our model achieves state-of-the-art results on random complex tasks with arbitrarily generated tools. These results indicate strong robustness to real-world environmental noise and excellent generalization to previously unseen task distributions. 7https://github.com/R2E-Gym/R2E-Gym LongCat-Flash-Thinking-2601 Technical Report General QA: LongCat-Flash-Thinking-2601 maintains strong performance on general QA benchmarks while scaling agentic and tool-integrated reasoning. It achieves score of 25.2 on the text-only subset of HLE and score of 85.2 on GPQA-Diamond under heavy mode, approaching open-source state-of-the-art results. Coding: LongCat-Flash-Thinking-2601 demonstrates competitive performance across both code reasoning and agentic coding benchmarks. On algorithmic problem-solving tasks from the LiveCodeBench series, it ranks among the top open-source models. On more difficult benchmarks such as OJBench and OIBench, our model achieves opensource second-best and best performance, respectively. Notably, compared to GLM-4.7, LongCat-Flash-Thinking2601 achieves similar performance with substantially lower inference cost, requiring approximately 45k tokens per problem versus 57k tokens. On SWE-bench Verified, LongCat-Flash-Thinking-2601 performs competitively within the top tier of open-source models, further validating its capability in real-world software engineering tasks."
        },
        {
            "title": "6 One More Thing: Zig-Zag Attention Design",
            "content": "Long-context efficiency has become an increasingly critical challenge for modern large language models. The steadily growing trend of longer reasoning traces poses fundamental limitation to standard full attention, whose quadratic complexity quickly becomes prohibitively expensive for long-context agentic training and inference. Moreover, in heavy-thinking mode, inference latency is further amplified due to the simultaneous decoding of multiple parallel reasoning traces, making efficient attention mechanisms even more indispensable. Existing approaches, including sparse and linear attention methods [Yuan et al., 2025, Lu et al., 2025b, Du et al., 2025, Sun et al., 2025], attempt to mitigate this issue by reducing the computational complexity of attention. However, these methods typically require substantial retraining to adapt the model to new attention architecture, which introduces considerable additional compute overhead and engineering cost. To address this limitation, we explore an experimental efficient attention design and concurrently release an open-source model, LongCat-Flash-Thinking-ZigZag8. Specifically, we propose Zigzag Attention, sparse attention mechanism that enables existing full-attention models to be efficiently converted into sparse variants during mid-training. This conversion incurs only negligible overhead, while allowing the model to efficiently scale to ultra-long contexts, supporting sequence lengths of up to 1M tokens. Concretely, Zigzag Attention [Zhang et al., 2026] combines Multi-head Latent Attention (MLA) with Streaming Sparse Attention (SSA) [Xiao et al., 2024] to achieve computation that scales sub-quadratically with the full context length. For each query token ht, attention is restricted to fixed set of key-value tokens consisting of (i) local window of recent tokens and (ii) small set of initial tokens at the beginning of the sequence. Formally, the attention output is computed as: ut = Attn(ht, {hs [t W, t] [0, B)}) , (3) where denotes the local context window size and denotes the number of preserved prefix tokens. Compared to full attention, this design significantly reduces computational and memory complexity while retaining both short-term context and global anchors. (a) Prefill. (b) Decode. Figure 12: Inference efficiency comparison between LongCat-Flash-Thinking and LongCat-Flash-Thinking-ZigZag with Zigzag Attention. Speedups are measured on production inference clusters. Zigzag Conectivity Zigzag Attention adopts layer-wise interleaved sparsification strategy. Specifically, approximately 50% of full-attention layers are replaced with SSA layers, while the remaining layers retain MLA-based 8https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking-ZigZag 20 LongCat-Flash-Thinking-2601 Technical Report full attention. This layer-level sparsity avoids the computational imbalance and GPU thread divergence commonly introduced by head-level sparsification, leading to more efficient hardware utilization. Although attention within each SSA layer is sparse and local, global information is preserved through cross-layer composition. By alternating sparse and full-attention layers, information propagates across distant positions over multiple layers, forming zigzag-shaped connectivity path along the sequence. As result, long-range dependencies remain accessible despite per-layer sparsity. Zigzag Integration Zigzag Attention is introduced during mid-training through structured sparsification procedure. First, we use calibrated dataset to estimate the relative importance of attention layers in the pretrained model. Second, the subset of layers with the lowest importance scores is replaced with SSA layers. After sparsification, the model undergoes continued long-context mid-training, together with YaRN-based positional encoding extension, enabling context lengths of up to 1M tokens. In practice, we adopt block size of 128, with one sink block and seven local blocks, resulting in an effective attention span of 1,024 tokens per layer. Replacing approximately half of the full-attention layers with Zigzag Attention yields about 1.5 end-to-end inference speedup, as shown in Figure 12, while preserving reasoning performance and agentic capabilities across benchmarks. To unlock the ability of handling longer context, we equip these recipes with YaRN [Peng et al., 2024] so that LongCat-Flash-Thinking-ZigZag can extrapolate itself to processing up to 1M tokens. In addition to that, we provide few crucial parameters involved in the model. The block size is 128, the number of sink blocks is 1, and the number of local blocks is 7, summing to 1,024 tokens. LongCat-Flash-Thinking-ZigZag yields good trade-off between performance and speed. quick glimpse on how it improves efficiency yet maintaining competitive performance is illustrated in Figure 13. Figure 13: The performance versus relative cost. The percentages attached to arrows indicate the reduced cost when benchmarking the performance on the concerned datasets."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce LongCat-Flash-Thinking-2601, an open-weight 560B MoE reasoning model with superior agentic reasoning capability. This model is built upon unified, end-to-end training pipeline that co-designs architecture, pre-training, post-training, data construction, and scalable RL infrastructure, all explicitly tailored to support longhorizon, interaction-driven agentic reasoning. The core innovations underpining LongCat-Flash-Thinking-2601 are as follows: (i) scalable environment construction and multi-domain reinforcement learning framework that enables stable acquisition of generalizable agentic skills. (ii) robust agentic training pipeline that systematically incorporates real-world environmental noise through curriculum-based reinforcement learning. (iii) Heavy Thinking Mode that enables effective test-time scaling of reasoning by jointly expanding reasoning width and depth. Through this holistic design, LongCat-Flash-Thinking-2601 sets new state-of-the-art performance among open-source models across wide range of agentic benchmark, narrowing the performance gap with leading closed-source models. 21 LongCat-Flash-Thinking-2601 Technical Report"
        },
        {
            "title": "Contributions",
            "content": "The listing of authors is in alphabetical order. Entries with identical English names will be sorted based on the order of Chinese stroke count and pronunciation. An asterisk (*) indicates members who have departed from the team. Anchun Gui Bei Li Bingyang Tao Bole Zhou Borun Chen Chao Zhang(张超) Chao Zhang(张朝) Chen Gao Chen Zhang Chengcheng Han Chenhui Yang Chuyu Zhang Cong Chen Cunguang Wang Daoru Pan Defei Bu Dengchang Zhao Di Xiu Dishan Liu Dongyu Ru Dunwei Tu Fan Wu Fengcheng Yuan Fengcun Li Gang Xu Guanyu Wu Guoyuan Lin Haibin Wang Hansi Yang Hao Yang Haonan Yan Haoxiang Ma Haoxing Wen"
        },
        {
            "title": "References",
            "content": "Hongyan Hao Hongyin Tang Hongyu Zang* Hongzhi Ni Hui Su Jiacheng Zhang Jiahong Zhou Jiahuan Li Jiaming Wang Jian Yang Jianfei Zhang Jianhao Xu Jianing Wang Jiapeng Zhu Jiaqi Sun Jiarong Shi Jiarui Zhao Jingang Wang Jinluan Yang Jinrui Ding Jinwei Xiao Jiyuan He Juncan Xu Kefeng Zhang Keheng Wang Li Wei Lianhui Ma Lin Qiu Lingbing Kong Lingchuan Liu Linsen Guo Mengshen Zhu Mengxia Shen Mingyang Zhu Peiguang Li Peng Pei Peng Zhao Pengcheng Jia Pengtao Zhang Ping Liu Qi Gu Qiong Huang Qiyuan Duan Quanchi Weng Rongxiang Weng Rongzhi Zhang Rumei Li Shanglin Lei Shengnan An Shijun Dai Shuaikang Liu Shuang Zhou Shuo Wang Songyuan Zhao Tao Liang Tianhao Hu Tianze Chen Wei Liu Wei Shi Wei Wang Weifeng Tang Wenjie Shi Wenlong Zhu Wentao Chen Wentao Shi* Xi Su Xiandi Ma Xiangcheng Liu Xiangyu Xi Xiangyuan Liu Xiangzhou Huang Xiao Liu Xiaodong Cai Xiaolong Chen Xiaowei Shi Xiaoyu Li Xin Chen Xingchen Liu Xuan Huang Xuezhi Cao Xunliang Cai Yan Chen Yang Bai Yang Liu Yang Yang Yang Zheng Yanyu Chen Yaoming Wang Yaoming Zhu Yaorui Shi Yaqi Huo Yerui Sun Yi Zhang Yifan Lu Yifan Zhao Yihao Chen Yi-Kai Zhang Yitao Zhai Yongjing Yin Yongwei Zhou Youshao Xiao Yu Wang Yu Yang Yuchen Xie Yuchen Yu Yuchuan Dai Yue Xu Yueqing Sun Yufei Zhang Yuhuai Wei Yulei Qian Yunfan Liang Yunke Zhao Yuwei Jiang Yuxin Bian Yuxin Chen Yuxin Liu Zeyang Yu Zhao Yang Zhengsheng Huang Zhengyu Chen Zhijian Liu Zhikang Xia Zhimin Lin Zhiyuan Yao Zhuofan Chen Zhuowen Han Zijian Zhang Ziran Li Ziwen Wang Ziyuan Zhuang LongCat-Flash DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. Zhihong Shao, Yuxiang Luo, Chengda Lu, Z. Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, and Xiaokang Zhang. Deepseekmath-v2: Towards self-verifiable mathematical reasoning. CoRR, abs/2511.22570, 2025. Anthropic. Introducing claude opus 4.5, 2025. URL https://www.anthropic.com/news/claude-opus-4-5. Google. new era of intelligence with gemini 3, 2025. URL https://blog.google/products-and-platforms/ products/gemini/gemini-3/#note-from-ceo. MoonshotAI. Kimi-k2 documentation, 2025. URL https://moonshotai.github.io/Kimi-K2/. DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenhao Xu, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Erhang Li, Fangqi Zhou, Fangyun Lin, Fucong Dai, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Li, Haofen Liang, Haoran Wei, Haowei Zhang, Haowen Luo, Haozhe Ji, Honghui Ding, Hongxuan Tang, Huanqi Cao, Huazuo Gao, Hui Qu, Hui Zeng, Jialiang Huang, Jiashi Li, Jiaxin Xu, Jiewen Hu, Jingchang Chen, Jingting Xiang, Jingyang Yuan, Jingyuan Cheng, Jinhua Zhu, Jun Ran, Junguang Jiang, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Kexin Huang, Kexing Zhou, Kezhao 22 LongCat-Flash-Thinking-2601 Technical Report Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Wang, Liang Zhao, Liangsheng Yin, Lihua Guo, Lingxiao Luo, Linwang Ma, Litong Wang, Liyue Zhang, M. S. Di, M. Xu, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Panpan Huang, Peixin Cong, Peiyi Wang, Qiancheng Wang, Qihao Zhu, Qingyang Li, Qinyu Chen, Qiushi Du, Ruiling Xu, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runqiu Yin, Runxin Xu, Ruomeng Shen, Ruoyu Zhang, S. H. Liu, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaofei Cai, Shaoyuan Chen, Shengding Hu, Shengyu Liu, Shiqiang Hu, Shirong Ma, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, Songyang Zhou, Tao Ni, Tao Yun, Tian Pei, Tian Ye, Tianyuan Yue, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjie Pang, Wenjing Luo, Wenjun Gao, Wentao Zhang, Xi Gao, Xiangwen Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaokang Zhang, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xingyou Li, Xinyu Yang, Xinyuan Li, Xu Chen, Xuecheng Su, Xuehai Pan, Xuheng Lin, Xuwei Fu, Y. Q. Wang, Yang Zhang, Yanhong Xu, Yanru Ma, Yao Li, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Qian, Yi Yu, Yichao Zhang, Yifan Ding, Yifan Shi, Yiliang Xiong, Ying He, Ying Zhou, Yinmin Zhong, Yishi Piao, Yisong Wang, Yixiao Chen, Yixuan Tan, Yixuan Wei, Yiyang Ma, Yiyuan Liu, Yonglun Yang, Yongqiang Guo, Yongtong Wu, Yu Wu, Yuan Cheng, Yuan Ou, Yuanfan Xu, Yuduan Wang, Yue Gong, Yuhan Wu, Yuheng Zou, Yukun Li, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehua Zhao, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhixian Huang, Zhiyu Wu, Zhuoshu Li, Zhuping Zhang, Zian Xu, Zihao Wang, Zihui Gu, Zijia Zhu, Zilin Li, Zipeng Zhang, Ziwei Xie, Ziyi Gao, Zizheng Pan, Zongqing Yao, Bei Feng, Hui Li, J. L. Cai, Jiaqi Ni, Lei Xu, Meng Li, Ning Tian, R. J. Chen, R. L. Jin, S. S. Li, Shuang Zhou, Tianyu Sun, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xinnan Song, Xinyi Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Dongjie Ji, Jian Liang, Jianzhong Guo, Jin Chen, Leyi Xia, Miaojun Wang, Mingming Li, Peng Zhang, Ruyi Chen, Shangmian Sun, Shaoqing Wu, Shengfeng Ye, T. Wang, W. L. Xiao, Wei An, Xianzu Wang, Xiaowen Sun, Xiaoxiang Wang, Ying Tang, Yukun Zha, Zekai Zhang, Zhe Ju, Zhen Zhang, and Zihua Qu. Deepseek-v3.2: Pushing the frontier of open large language models, 2025. URL https://arxiv.org/abs/2512.02556. MiniMax. Minimax m2 and agent: Ingenious in simplicity, 2025. URL https://www.minimax.io/news/ minimax-m2. Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, and Shuai Liang. Longcat-flash technical report. CoRR, abs/2509.01322, 2025a. Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Shuo Wang, Hongfei Yan, Jingang Wang, and Xunliang Cai. Expanding reasoning potential in foundation model by learning diverse chains of thought patterns, 2025a. URL https://arxiv.org/abs/2509.21124. Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, and Xunliang Cai. survey on llm mid-training, 2025. URL https://arxiv.org/abs/2510.23081. Zhihao Xu, Rumei Li, Jiahuan Li, Rongxiang Weng, Jingang Wang, Xunliang Cai, and Xiting Wang. Unlocking implicit experience: Synthesizing tool-use trajectories from text, 2026. URL https://arxiv.org/abs/2601.10355. Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chengcheng Han, Chenhui Yang, Chi Zhang, Chong Peng, Chuyu Zhang, Cong Chen, Fengcun Li, Gang Xu, Guoyuan Lin, Hao Jiang, Hao Liang, Haomin Fu, Haoxiang Ma, Hong Liu, Hongyan Hao, Hongyin Tang, Hongyu Zang, Hongzhi Ni, Hui Su, Jiahao Liu, Jiahuan Li, Jialin Liu, Jianfei Zhang, Jianhao Xu, Jianing Wang, Jiaqi Sun, Jiaqi Zhang, Jiarong Shi, Jiawei Yang, Jingang Wang, Jinrui Ding, Jun Kuang, Jun Xu, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Li Wei, Liang Shi, Lin Qiu, Lingbin Kong, Lingchuan Liu, Linsen Guo, Longfei An, Mai Xia, Meng Zhou, Mengshen Zhu, Peng Pei, Pengcheng Jia, Qi Gu, Qi Guo, Qiong Huang, Quan Chen, Quanchi Weng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shanglin Lei, Shuai Du, Shuaikang Liu, Shuang Zhou, Shuhao Hu, Siyu Xu, Songshan Gong, Tao Liang, Tianhao Hu, Wei He, Wei Shi, Wei Wang, Wei Wu, Wei Zhuo, Weifeng Tang, Wenjie Shi, Wenlong Zhu, Xi Su, Xiangcheng Liu, Xiangyu Xi, Xiangzhou Huang, Xiao Liu, Xiaochen Jiang, Xiaowei Shi, Xiaowen Shi, 23 LongCat-Flash-Thinking-2601 Technical Report Xiaoyu Li, Xin Chen, Xinyue Zhao, Xuan Huang, Xuemiao Zhang, Xuezhi Cao, Xunliang Cai, Yajie Zhang, Yang Chen, and Yang Liu. Longcat-flash-thinking technical report. CoRR, abs/2509.18883, 2025b. Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: core-set approach. In ICLR (Poster). OpenReview.net, 2018. Qianlong Du, Chengqing Zong, and Jiajun Zhang. Mods: Model-oriented data selection for instruction tuning. CoRR, abs/2311.15653, 2023. Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, and Chang Zhou. Self-evolved diverse data sampling for efficient instruction tuning. CoRR, abs/2311.08182, 2023. Tingyu Jiang, Shen Li, Yiyao Song, Lan Zhang, Hualei Zhu, Yuan Zhao, Xiaohang Xu, Kenjiro Taura, and Hao Henry Wang. Importance-aware data selection for efficient LLM instruction tuning. CoRR, abs/2511.07074, 2025. Jia Zhang, Chen-Xi Zhang, Yao Liu, Yi-Xuan Jin, Xiao-Wen Yang, Bo Zheng, Yi Liu, and Lan-Zhe Guo. D3: diversity, difficulty, and dependability-aware data selection for sample-efficient LLM instruction tuning. In IJCAI, pages 83488356. ijcai.org, 2025b. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, et al. Agentrl: Scaling agentic reinforcement learning with multi-turn, multi-task framework. arXiv preprint arXiv:2510.04206, 2025c. Han Lu, Zichen Liu, Shaopan Xiong, Yancheng He, Wei Gao, Yanan Wu, Weixun Wang, Jiashun Liu, Yang Li, Haizhou Zhao, et al. Part ii: Roll flashaccelerating rlvr and agentic training with asynchrony. arXiv preprint arXiv:2510.11345, 2025a. Meituan LongCat Team, Anchun Gui, Bei Li, Bingyang Tao, Bole Zhou, Borun Chen, Chao Zhang, Chengcheng Han, Chenhui Yang, Chi Zhang, et al. Introducing longcat-flash-thinking: technical report. arXiv preprint arXiv:2509.18883, 2025c. Pritam Damania, Shen Li, Alban Desmaison, Alisson Azzolini, Brian Vaughan, Edward Yang, Gregory Chanan, Guoqiang Jerry Chen, Hongyi Jia, Howard Huang, et al. Pytorch rpc: Distributed deep learning built on tensoroptimized remote procedure calls. Proceedings of Machine Learning and Systems, 5:219231, 2023. Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193210, 2024. Ruoyu Qin, Zheming Li, Weiran He, Jialei Cui, Heyi Tang, Feng Ren, Teng Ma, Shangming Cai, Yineng Zhang, Mingxing Zhang, et al. Mooncake: kvcache-centric disaggregated architecture for llm serving. ACM Transactions on Storage, 2024. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. CoRR, abs/2507.18071, 2025. Ziniu Li, Congliang Chen, Tianyun Yang, Tian Ding, Ruoyu Sun, Ge Zhang, Wenhao Huang, and Zhi-Quan Luo. Knapsack RL: unlocking exploration of llms via optimizing budget allocation. CoRR, abs/2509.25849, 2025. Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Xinmiao Yu, Dingchu Zhang, Yong Jiang, et al. Resum: Unlocking long-horizon search intelligence via context summarization. arXiv preprint arXiv:2509.13313, 2025. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. CoRR, abs/2504.12516, 2025. OpenAI. Introducing openai o1, 2024. URL https://openai.com/o1/. DeepMind. Gemini 2.5 pro, 2025. URL https://deepmind.google/models/gemini/pro/. Gemma Team. Gemma 3 technical report. CoRR, abs/2503.19786, 2025. OpenAI. Introducing gpt-5, 2025a. URL https://openai.com/index/introducing-gpt-5/. Jingcheng Hu, Yinmin Zhang, Shijie Shang, Xiaobo Yang, Yue Peng, Zhewei Huang, Hebin Zhou, Xin Wu, Jie Cheng, Fanqi Wan, Xiangwen Kong, Chengyuan Yao, Ailin Huang, Hongyu Zhou, Qi Han, Zheng Ge, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Pacore: Learning to scale test-time compute with parallel coordinated reasoning, 2025. URL [https://github.com/stepfun-ai/PaCoRe/blob/main/pacore_report. pdf](https://github.com/stepfun-ai/PaCoRe/blob/main/pacore_report.pdf). 24 LongCat-Flash-Thinking-2601 Technical Report Yifan Zhang and Team Math-AI. American invitational mathematics examination (aime) 2025, 2025. HMMT. Hmmt 2025, 2025. URL https://www.hmmt.org/. Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, and Junehyuk Jung. Towards robust mathematical reasoning. CoRR, abs/2511.01846, 2025. Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, and Shuang Zhou. Amo-bench: Large language models still struggle in high school math competitions. CoRR, abs/2510.26768, 2025. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, and Yining Hua. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. CoRR, abs/2504.19314, 2025. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. τ 2-bench: Evaluating conversational agents in dual-control environment. CoRR, abs/2506.07982, 2025. Wei He, Yueqing Sun, Hongyan Hao, Xueyuan Hao, Zhikang Xia, Qi Gu, Chengcheng Han, Dengchang Zhao, Hui Su, Kefeng Zhang, Man Gao, Xi Su, Xiaodong Cai, Xunliang Cai, Yu Yang, and Yunke Zhao. Vitabench: Benchmarking LLM agents with versatile interactive tasks in real-world applications. CoRR, abs/2509.26490, 2025. OpenAI. Introducing gpt-5.2, 2025b. URL https://openai.com/index/introducing-gpt-5-2/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P. Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin, Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume, Abdallah Galal, Hangrui Cao, Alexis C. Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schröder de Witt, Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu, Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua Vendrow, Vladyslav Kuchkin, and Ng Ze-An. Humanitys last exam. CoRR, abs/2501.14249, 2025. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In ICLR. OpenReview.net, 2025. Zhexu Wang, Yiping Liu, Yejie Wang, Wenyang He, Bofei Gao, Muxi Diao, Yanxu Chen, Kelin Fu, Flood Sung, Zhilin Yang, Tianyu Liu, and Weiran Xu. Ojbench: competition level code benchmark for large language models. CoRR, abs/2506.16395, 2025. Yaoming Zhu, Junxin Wang, Yiyang Li, Lin Qiu, ZongYu Wang, Jun Xu, Xuezhi Cao, Yuhuai Wei, Mingshi Wang, Xunliang Cai, and Rong Ma. Oibench: Benchmarking strong reasoning models with olympiad in informatics. CoRR, abs/2506.10481, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. Z.AI. Glm-4.7: Advancing the coding capability, 2025. URL https://z.ai/blog/glm-4.7. 25 LongCat-Flash-Thinking-2601 Technical Report Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In ACL (1), pages 2307823097. Association for Computational Linguistics, 2025. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, and Jiezhong Qiu. Moba: Mixture of block attention for long-context llms. CoRR, abs/2502.13189, 2025b. Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. Mom: Linear sequence modeling with mixture-of-memories. CoRR, abs/2502.13685, 2025. Weigao Sun, Disen Lan, Tong Zhu, Xiaoye Qu, and Yu Cheng. Linear-moe: Linear sequence modeling meets mixture-of-experts. CoRR, abs/2503.05447, 2025. Chen Zhang, Yang Bai, Jiahuan Li, Anchun Gui, Keheng Wang, Feifan Liu, Guanyu Wu, Yuwei Jiang, Defei Bu, Li Wei, Haihang Jing, Hongyin Tang, Xin Chen, Xiangzhou Huang, Fengcun Li, Rongxiang Weng, Yulei Qian, Yifan Lu, Yerui Sun, Jingang Wang, Yuchen Xie, and Xunliang Cai. Efficient context scaling with longcat zigzag attention, 2026. URL https://arxiv.org/abs/2512.23966. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context LLM inference with retrieval and streaming heads. CoRR, abs/2410.10819, 2024. doi:10.48550/ARXIV.2410.10819. URL https://doi.org/10.48550/arXiv.2410.10819. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=wHBfxhZu1u. 26 LongCat-Flash-Thinking-2601 Technical Report Figure 14: Scaling curves of optimal batch size and learning rate. The circles and squares represent MoE-1.8B and MoE-100M models respectively. The dashed lines show the power-law fitting with confidence intervals. The scaling laws are first validated on MoE-3B and MoE-6B models, then extrapolated to predict optimal configurations for MoE-26B. Figure 15: Pass@1 accuracy of BrowseComp with varying summary context token lengths."
        },
        {
            "title": "A Optimal Hyperparameter Prediction",
            "content": "Efficiently identifying the optimal hyperparameters is one of the core challenges in large-scale mid-training, given the vast search space and high computational costs. To address this challenge, we propose novel method for predicting optimal hyperparameters, specifically designed to minimize the computational cost of finding the best configuration. Our approach consists of two key steps: Hyperparameter Mapping: We train small models with different hyperparameters, using validation loss and FLOPS to map the optimal hyperparameters to their computational cost (see Figure 14), providing insights into how configurations affect training efficiency and performance. Hyperparameter Prediction: For given continual training checkpoint, we estimate the equivalent compute cost using validation loss, which means the amount of compute required to achieve the same loss when training from scratch on the continual training data. We then predict the optimal hyperparameters based on this estimate and the actual computational load. Through this approach, we can predict optimal hyperparameters that enable efficient continual training, improving model performance with minimal computational overhead."
        },
        {
            "title": "B Token Threshold Context Management Performance Evaluation",
            "content": "As shown in Figure 15, we evaluate BrowseComps Pass@1 accuracy across varying summary context token lengths, with maximum context turn limit of 500. Accuracy rises steadily from 63.86% at 20K tokens to peak of 66.58% at 80K tokens, then drops to 65.9% at 100K tokens. This identifies 80K as the optimal context length for summary-based context management, so we fix 80K context length as the summarization trigger threshold in all subsequent experiments."
        }
    ],
    "affiliations": [
        "Meituan"
    ]
}