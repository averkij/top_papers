{
    "paper_title": "LiveVQA: Live Visual Knowledge Seeking",
    "authors": [
        "Mingyang Fu",
        "Yuyang Peng",
        "Benlin Liu",
        "Yao Wan",
        "Dongping Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research."
        },
        {
            "title": "Start",
            "content": "LIVEVQA: Live Visual Knowledge Seeking Mingyang Fu1*, Yuyang Peng1*, Benlin Liu2, Yao Wan1, Dongping Chen1 2 1Huazhong University of Science and Technology, 2University of Washington {u202112313,wanyao}@hust.edu.cn 5 2 0 2 7 ] . [ 1 8 8 2 5 0 . 4 0 5 2 : r Figure 1. LIVEVQA comprises 14 different News categories, containing 1233 News and 3602 question-answer pairs. Each instance includes representative image, QA pair for basic image for understanding, and two multimodal multi-hop QA pairs for deeper reasoning."
        },
        {
            "title": "Abstract",
            "content": "We introduce LIVEVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LIVEVQA consists of 3,602 singleand multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image- * Equal Contribution. Corresponding Author. Project Leader. text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research. 1 1. Introduction In todays rapidly evolving information landscape, the ability to understand and reason about live content has become increasingly crucial. As news and events, continuously update across the globe, AI systems that can effectively process, comprehend, and respond to this dynamic information flow are essential for applications ranging from personalized experience [31] to real-time decision support [37]. Large language models (LLMs) have made remarkable progress in understanding and reasoning about live textual content when integrated with search engines [13, 18]. However, while live textual knowledge understanding has advanced significantly, critical question remains unanswered: has other modality knowledge in live contexts such as visual knowledgebeen similarly solved? To address this research gap, we introduce LIVEVQA, automatically collected benchmark dataset specifically designed to evaluate current AI system on their ability to answer questions requiring live visual knowledge. LIVEVQA is constructed with three key design principles: (1) strict temporal filtering to prevent dataset contamination, ensuring evaluation of true retrieval capabilities rather than memorized knowledge, (2) automated ground truth with humanin-the-loop annotations, and (3) high-quality and authentic image-question pair to ensure meaningful visual knowledge challenges. Finally, our dataset comprises 1,233 authentic news articles with 3,602 latest VQA problems sourced from six major global news platforms and categorized across 14 domains. Each instance features representative image paired with three types of questions: basic visual understanding question and two difficult multi-hop questions requiring deeper reasoning. Extensive experiments conducted on 15 state-of-the-art MLLMs (e.g., Gemini-2.0-Flash [22], Qwen-2.5-VL [24, 25, 35], and the Gemma-3 family [23]) using two types of visual search engines [12] reveal that, while larger models generally perform better, significant challenges remain in addressing complex multi-hop visual questions that require up-to-date real-world knowledge. For instance, Gemini2.0-Flash [22] achieves the highest accuracy of 24.93%. Additionally, models with stronger reasoning capabilities, such as QvQ-72B-Preview [24], demonstrate advantages in tackling multi-hop reasoning tasks, achieving 7.41% accuracy on Level 3 problems, compared to just 1.35% for base models. Notably, integrating GUI-based MM-search substantial improves performance, boosting Gemini-2.0Flashs average accuracy to 29.00%, with particularly notable gains on the more challenging Level 2 and Level 3 questions (reaching 22.75% and 13.66%, respectively). We would like to clarify that our work is still in progress. We hope that LIVEVQA offers valuable insights into the current state of live visual knowledge and highlights promising directions for future research. Table 1. The distribution of 1,232 news instances across 14 categories and 6 major sources, containing 3,602 VQA. Overall By News Source (%) Count % VRTY BBC CNN APNWS FORB YHO Category Sports Other Movies TV Science Economy Health Media Music G.Business Tech Opinion Art/Design Theater 305 24.8 219 17.8 8.3 102 7.2 89 6.5 80 5.8 72 5.4 67 4.7 58 3.8 47 3.7 45 3.7 45 3.7 45 3.5 43 1.2 Total Source % 1,232 100 100 1.0 1.0 36.7 31.0 0.0 0.0 1.0 7.6 11.9 1.9 2.4 1.0 0.0 4.8 210 17.1 48.8 17.3 0.7 1.8 5.5 4.4 6.6 3.1 2.0 1.8 2.6 2.4 2.4 0. 457 37.1 20.3 25.3 1.7 2.1 7.1 7.9 3.3 7.5 0.8 7.5 4.2 8.3 4.2 0.0 241 19.6 7.5 28.4 6.0 2.5 16.9 8.0 12.4 3.5 3.0 2.5 3.0 2.5 4.0 0.0 201 16.3 15.5 13.6 5.8 4.9 0.0 14.6 1.0 1.9 4.9 6.8 10.7 5.8 13.6 1. 103 8.4 0.0 30.0 0.0 5.0 20.0 10.0 5.0 5.0 0.0 15.0 5.0 5.0 0.0 0.0 20 1.6 2. LIVEVQA: The Dataset We introduce LIVEVQA, benchmark dataset for live visual knowledge, as shown in Figure 1. This dataset incorporates multi-hop question-answer pairs that establish explicit dependencies between textual news facts and visual content, enabling the evaluation of both reasoning capabilities and generalization potential in multimodal models. Each instance in LIVEVQA consists of: (1) representative image, (2) basic question-answer pair that establishes the fundamental image-text correspondence, and (3) two detailed question-answer pairs that require deeper reasoning. Following the classification system of [30], we select 14 major news categories, such as Sports, Movies, Television, Science, Economy, etc.. Each news item is classified using GPT-4o-mini [17]. The statistics are presented in Table 1. 2.1. Dataset Construction Data Collection. We select six global news platforms: CNN, BBC, Yahoo, Forbes, AP News, and Variety. These sources provide comprehensive geographic coverage and content diversity, with an average of 23 images per article to ensure visual richness. Our collection pipeline consists of three steps: URL Normalization: Predefined patterns and regular expressions are used to identify actual news pages while filtering out indexes, advertisements, and pure-textual content, ensuring data relevance and authenticity. Structured Content Extraction: multi-level parsing strategy is employed, starting with site-specific CSS selectors (e.g., h1.pg-headline for CNN, h1.story-headline for BBC) to extract key content. Generic selectors serve as fallback, and metadata extraction is used as last resort to enhance robustness. Image Filtering: The pipeline prioritizes contentrelevant images over decorative elements, particularly 2 Figure 2. Pipline of LIVEVQA data engine. Our pipeline consists of three modules: news collector, data filter, and Q&A pairs builder. It collects illustrated news from mainstream media, performs multi-level data filtering, and generates foundational and detailed Q&A pairs for training multimodal question-answering models. those marked with og:image meta tags. All images undergo standardized processing, resized to 1024 680 pixels while maintaining aspect ratio, assigned unique identifiers, and stored in dedicated repository to ensure consistency and traceability. Raw Data Filtering. To ensure dataset quality, we design multi-level filtering mechanism covering URL validation, image screening, and duplicate removal. For URL filtering, our pipeline judge the current URL according to corresponding news platforms (e.g. Format compliance ensures CNN URLs contain /year/month/day/, while BBC domains end with .bbc.co.uk). Content-type filtering excludes non-news pages using regular expressions. Path depth analysis enforces platform-specific URL structures (e.g. Forbes five-level paths). For image selection, website-specific CSS selectors (e.g., .image container for CNN) extract candidate images. Basic filtering removes images under 100100 pixels and excludes icon/logo URLs. Quality prioritization selects lead images above 800600 pixels, ensuring editorial relevance. To eliminate duplicates, we implement hierarchical deduplication for both historical and intra-session duplicates. Historical duplicates are identified by matching URLs, title fragments, or textual similarity (Levenshteinbased score>0.8). Intra-session duplicates are blocked through real-time title hashing, ensuring each article remains unique within session. Question-Answer Generation. We employ GPT-4o [16] to generate QA pairs based on raw news document, with each sample comprises three components: (1) an image reflecting the news topic accurately, (2) basic question requiring to understand the image content, and (3) two muti-hop detailed questions requiring cross-modality reasoning. To ensure valuable images for generating, we prioritize relevance. Notably, some news articles feature header images that bear little connection to their content. For instance, may some news about harmful foods just show close-up of the food without conveying substantive information. To address this, we use GPT-4o [16] to evaluate the relevance of images by analyzing the news title, content, and associated visuals. Basic Questions focus on substantive elements such as people, objects, or locations, while avoiding queries without visual knowledge that solely reliant on visual content within the image like color or shape. Answers are constrained to factual phrases of 27 words. For example, Who is the person speaking in the image? is valid, whereas What color is the persons tie? is filtered out. Mutli-hop Questions require deeper contextual reasoning. The two muti-hop questions that must be answerable only through the news text, covering events, person, time, etc. Questions must remain distinct and must not reveal or reference answers from prior questions. 3 2.2. Data Statistics Finally, we collect 1,232 carefully curated news spanning 14 categories and 6 global news platforms, amounting to total of 3,602 QA pairs. As illustrated in Figure 1, the dataset covers diverse range of news topics with representative examples, showcasing its breadth and richness in both content and modality. LIVEVQA demonstrates distinct domain specificity. As shown in Table 1, sports news is the most prevalent category, with significant portion sourced from BBC, highlighting its strength in sports reporting. News sources also exhibit clear domain preferencesVariety primarily covers film and music, Forbes focuses on business, and AP News emphasizes science and health. Additionally, we categorize the final generated questions into 8 distinct types, as shown in Table 2. 3. Experiments and Analysis 3.1. Experiment Setups Models. We conduct series of zero-shot testing for diverse range of state-of-the-art MLLMs, including Gemini 2.0 Flash [22], Qwen2.5-VL-3/7/32/72B [35], Gemma-34/12/27B-it [23], QVQ-72B-Preview [24], QVQ-Max [25], GPT-4o-mini [17], and GPT-4o [16]. Our experiments are conducted on Linux server equipped with 8 A800 GPUs. Models with Search Engine. We enable the built-in search functionalities and MM-Search [12] with Gemini-2.0-Flash [22], GPT-4o-mini [17], and GPT-4o [16]. Metric. We instruct GPT-4o-mini [17] as an impartial judge, which strictly answer only yes or no and marks the final answer with <answer> tags. 3.2. Experiment Results Larger-scale models demonstrate improved performance across difficulty levels, though proprietary models retain clear advantage. For models within the same family (e.g., Gemma [23] or Qwen [35]), we observe that increasing model size leads to consistently better accuracy across all question difficulty levels. For instance, Gemma-3-4b-it [23] achieves only 2.46% on L3-level questions, whereas Gemma-3-27b-it [23] reaches 4.92%. Despite these improvements, open-source models still lag behind proprietary models in overall performance. Notably, Gemini-2.0-flash [22] achieves the best results across nearly all dimensions with an overall accuracy of 24.93%, which may be attributed to its more recent data coverage, extending up to 2024 June. Strong visual reasoning ability plays critical role in boosting cross-modality multi-hop question. Models equipped with stronger reasoning capabilities, such as QvQ72B-Preview [24] and QvQ-Max [25], outperform their Question: Based on the provided image, what character did the man on the right play on Parks & Recreation? Ground Truth: Jerry Gergich Gemma-3-4b-it: Ron Swanson Qwen2.5-VL-7B-Instruct: Tom Haverford GPT-4o-mini: The man on the right played the character Andy Dwyer on Parks and Recreation. Gemma-3-27b-it: Ron Swanson Qwen2.5-VL-3B-Instruct: Tom Haverford Qwen2.5-VL-72B-Instruct: Leslie Knopes father Qwen2.5-VL-32B-Instruct: Leslie Knopes father GPT-4o: dont know who the man on the right is based on the image provided. QvQ-72B-Preview: Jerry Gergich. Gemini-2.0-Flash(MM Search): this question. cant answer Figure 3. Large visual reasoning model QvQ-72B-Preview perform best on cross-modality multi-hop pproblems. base model Qwen2.5-VL-72B [35], highlighting the effectiveness of enhancing visual reasoning abilities in live knowledge. We also try different prompt template for QvQ72b-preview as shown in Table 3 and find out that more reasoning tokens result in better performance in multi-hop L2 and L3 problems. Current models excel in entity-centric categories but struggle with abstract knowledge. Across different question categories, models perform better on concrete entity recognition tasks such as Person, Organization, and Object. In contrast, their performance drops significantly on more abstract knowledge like Time and Reason, indicat4 Table 2. Overall performance on LIVEVQA. See Table 5 for performance on another categorizing taxonomy for live visual knowledge. Model Avg. L1 L3 Per. Loc. Tim. Eve. Org. Obj. Rea. Oth Gemma-3-4b-it Gemma-3-12b-it Gemma-3-27b-it Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B GPT-4o GPT-4o-mini Gemini-2.0-Flash QVQ-72B-Preview QVQ-Max GPT-4o GPT-4o-mini Gemini-2.0-Flash GPT-4o GPT-4o-mini Gemini-2.0-flash 14.65 17.10 20.43 15.63 18.74 18.96 21.07 16.38 17.30 24.93 19.94 17.80 13.38 22.27 29.46 20.20 21.80 29.00 38.42 44.19 48.50 39.98 41.28 47.93 55.93 41.02 43.71 58.81 39.90 38.10 28.43 32.58 59.63 34.88 41.28 49. 3.10 3.47 7.93 4.58 7.44 5.12 5.94 4.54 4.95 8.75 11.62 9.50 5.78 19.49 16.43 15.57 14.97 22.75 w.o. Search 19.20 23.96 29.19 25.65 29.43 27.19 32.87 2.61 5.84 43.01 21.81 24.88 11.96 15.78 17.77 13.29 17.61 17.61 20.60 21.43 21.93 20.93 19.44 17. w. Search 2.46 12.14 44.85 13.79 21.26 25.91 w. MM Search [12] 8.18 24.55 44.55 20.75 24.53 26. 2.46 3.71 4.92 2.38 3.63 3.88 1.35 3.62 3.19 5.86 7.41 4.91 5.34 14.22 11.03 9.32 8.07 13.66 2.82 5.08 2.82 3.11 3.07 2.82 4.52 5.08 3.67 4.24 2.25 3.67 3.39 11.58 11.58 16.98 7.55 20. 14.51 15.95 20.50 12.98 16.89 17.54 19.59 18.68 20.05 19.36 19.95 15.95 18.45 26.42 24.60 25.97 16.88 20.78 26.75 29.40 34.46 28.67 30.23 33.49 35.66 28.67 32.53 35.66 34.46 33.01 22.17 34.22 44.10 34.48 31.03 29. 28.37 29.58 35.21 27.89 33.82 35.21 32.96 41.97 41.13 43.10 36.52 26.76 34.93 42.25 45.63 52.38 47.62 42.86 2.89 2.69 5.17 2.89 2.87 4.75 3.51 6.20 6.20 6.61 10.33 4.34 6.61 15.50 9.92 8.33 10.00 11. 10.26 12.25 15.23 5.30 10.67 8.61 12.25 15.23 13.58 19.54 13.58 11.59 13.91 23.51 23.51 10.64 17.02 27.66 Table 3. Performance comparison between Qwen2.5-VL-72B [35] and QvQ-72B-Preview [24] with different thinking pattern via prompt engineering. Model Avg. L1 L2 L3 Qwen2.5-VL-72B 21.07 Short 21.52 19.94 Midium 21.71 Long 55.93 40.78 39.90 41.75 5.94 12.05 11.62 12. 1.35 8.96 7.41 10.09 Table 4. Performance of GPT-4o-mini on LIVEVQA using image and image description as input. We find that model perform better in L2 and L3 problems when provided image caption. Provided Info. Avg. L1 L2 Image Image Caption 17.30 14.83 43.71 34.77 4.95 5.28 3.19 3.62 ing that current models are still limited in their ability to conduct causal reasoning and temporal understanding. As shown in Table 5, models achieve higher performance in domains with rich visual and textual cues are present. However, performance is notably lower in more ambiguous or opinionated domains such as Opinion and Other, reflecting the difficulty of handling multi-intent or subjective content in current multimodal models. Incorporating the MMSearch and integrated search engine significantly improves the performance. Gemini2.0-Flash sees its average accuracy rise from 24.93%  (Table 2)  to 29.00%, with substantial gains on harder questionsachieving 22.75% and 13.66% on L2 and L3 respectively. These results demonstrate that integrating retrievalbased evidence is particularly helpful for addressing questions that go beyond the internal knowledge scope of the models. Notably, GPT-4o-mini [17] exhibits more pronounced improvement than GPT-4o, highlighting its strong synergy with retrieval pipelines and its potential as lightweight yet effective reasoning agent. Figure 4 illustrates this by comparing GPT-4o [16]s correct performance with MMSearch to its failure without it. Unexpected performance gap between GPT-4o-mini [17] and GPT-4o [16]. We find that GPT-4o-mini [17] tends to consume more tokens in image processing, which may account for its superior visual understanding and reasoning. Furthermore, GPT-4o [16] performs better without search tool, which attributes to it believes it can answer the query without invoking external resources, even though search functionality is permitted. In addition, both GPT4o [16] and GPT-4o-mini [17] exhibit poor performance on face recognition, likely due to its privacy protections and policy constraints. 3.3. Error Analysis To analyze the limitations of current LLMs on the LiveVQA benchmark, we conduct comprehensive analysis of their failure cases. By categorizing the incorrect responses, we identify the following primary sources of error: Recognition Error: Failure to correctly identify visual elements such as people, objects, locations, or text. Reasoning Error: Inability to perform reasoning beyond recognition, including inference, causality, or temporal understanding. Ambiguous Answer: Response is too generic or indirect to address the question accurately. 5 Figure 4. MMSearch [12] enables GPT-4o [16] to answer knowledge-intensive visual question correctly by retrieving external evidence, whereas it fails without retrieval. Privacy Restriction: Refusal to answer due to privacy or safety constraints, despite the question being valid. Judge Error: Correct answers are mistakenly marked as incorrect due to evaluation issues. Others: Errors not covered by the above categories, including system or network failures. In Figure 6, we showcase the statistics of different error types occurring in GPT-4o [16]. To gain deeper understanding of these error types, we present case study for each category. Specifically, we illustrate representative failure cases covering four common error types: Recognition Error, Reasoning Error, Vague Answer, and Privacy 6 Question: Based on the provided image, which music group is performing? Ground Truth: Semisonic Question: Based on the provided image, what creatures inspired the design of these entities? Ground Truth: Creepers Model Response: cant identify the music group from the image, but if you provide more context or details, might be able to help further. Error Type: Recognition Error Model Response: The design of the creatures in the image is likely inspired by tardigrades, also known as water bears. Tardigrades are microscopic, waterdwelling animals known for their resilience to extreme conditions. These fictional creatures likely take inspiration from their segmented bodies and small, bulbous appendages. Error Type: Reasoning Error (a) Recognition Error (b) Reasoning Error Question: Based on the provided image, what type of animal is stranded? Ground Truth: Long-finned pilot whale Question: Based on the provided image, can you identify the individual? Ground Truth: James Taylor Model Response: The animal in the image appears to be whale that is stranded on the beach. Error Type: Vague Answer Model Response: Im sorry, cant identify individuals in images. Error Type: Privacy Restriction (c) Vague Answer (d) Privacy Restriction Figure 5. Case studies of four representative error types made by GPT-4o [16] on LIVEVQA. Each subfigure illustrates different failure mode: Recognition Error, Reasoning Error, Vague Answer, and Privacy Restriction. Restriction. Each example highlights how the models response deviates from the ground truth and sheds light on the underlying limitations that lead to the error. 4. Related Works Live Knowledge Seeking. Recent advances in LLMs as search engines have demonstrated significant progress [13, ongoing situations and events. News QA. News Question Answering (News QA) aims to enable systems to comprehend and respond to news-related questions, requiring efficient information retrieval capabilities to handle rapidly updating news data. Early research [1, 26] in News QA primarily focused on news text, where answers were composed of multiple textual fragments extracted from original articles. In recent years, News Visual Question Answering (NewsVQA) has emerged as novel field that extends news information retrieval by enabling models to answer questions related to news images or videos through the integration of textual and visual information. However, NewsVQA faces several challenges, including multimodal fusion, temporal information processing, and scene text understanding. Current datasets [7, 10, 27] are used to evaluate model performance, with research efforts focusing on optimizing visual question answering models, integrating OCR for video text recognition, and developing more accurate evaluation methods. 5. Conclusion This paper introduces LIVEVQA, comprehensive benchmark designed to evaluate MLLMs on live visual knowledge across 15 models (ranging from 3B to 72B parameters). Equipping these models with online search tools or GUI-based image search [12] leads to significant performance boost on such queries, highlighting promising directions for future research."
        },
        {
            "title": "Ackownledgement",
            "content": "We thank Dongzhi Jiang from MM-Search Team [12] for his valuable insights and discussion."
        },
        {
            "title": "Limitations",
            "content": "While our study provides comprehensive evaluation of state-of-the-art MLLMs on latest visual questions, several (1) Currently, our dataset primarily limitations remain: structures latest information into visual question answering formats. Additional synthetic data approaches such as image captioning or Chain-of-Thought reasoning could further enhance MLLMs understanding and reasoning capabilities on Live Visual Content. (2) Our research predominantly derives Live Visual Content from mainstream news websites such as CNN and BBC, which may lead to imbalance and incomplete representation of current Internet content. Incorporating data from social media platforms such as (formerly Twitter) and Reddit1 could provide more diverse and comprehensive dataset. (3) More robust visual search tools or engines need to be developed to enhance model performance on latest visual knowledge queries. 1reddit.com 8 Figure 6. Distribution of Error types of GPT-4o [16] in LIVEVQA. 18, 33]. Traditional search engines rely on keyword matching, while LLMs use natural language understanding for precise, context-aware answers. Retrieval-augmented generation (RAG) enhances LLMs knowledge retrieval capabilities, making them more suitable for search engines. Early RAG methods [8] integrated retrieval and generation, and later work [34] optimized interactive search. The latest work, Search GPT [18], combines RAG with online search to create more efficient search engine architecture. Multimodal search engines have also shown great potential, integrating text, images or other types of information to provide richer search experience. Recent research [12] improves both search accuracy and interactivity. Additionally, an enhanced version of multimodal RAG-LLM [34] has been proposed for accurate visual question answering, showcasing the application of multimodal RAG-LLM in cross-modal information retrieval. Live Visual Knowledge. Visual knowledge, also known as world knowledge, refers to the ability to connect visual information with broader facts, concepts, and relationships about the real world [14, 20]. This knowledge encompasses both factual information from external sources [4, 12] and commonsense understanding about objects, their interactions, and contextual relationships in visual scenes [11, 21, 28, 29, 32]. The acquisition of visual knowledge involves leveraging external knowledge bases and structured repositories that ground visual elements in their broader conceptual context [2, 3, 38]. Recent developments are expanding visual knowledge from static data to live visual content, addressing challenges in being more helpful as real-time and real-life multimodal assistants [12]. This live visual knowledge specifically pertains to understanding and reasoning about current news [6], emerging events [36], cultural phenomena [15, 19], and temporally relevant information that constantly evolves [5, 9]. Such live knowledge allows multimodal assistants to provide timely, relevant, and contextually appropriate responses to visual queries about"
        },
        {
            "title": "References",
            "content": "[1] Sandipan Basu, Aravind Gaddala, Pooja Chetan, Garima Tiwari, Narayana Darapaneni, Sadwik Parvathaneni, and Anwesh Reddy Paduri. Building question and answer system for news domain. arXiv preprint arXiv:2105.05744, 2021. 8 [2] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18181826, 2024. 8 [3] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. arXiv preprint arXiv:2210.02928, 2022. 8 [4] Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. arXiv preprint arXiv:2502.13059, 2025. 8 [5] Yuntao Du., Kailin Jiang, Zhi Gao, Chenrui Shi, Zilong Zheng, Siyuan Qi, and Qing Li. MMKE-bench: multimodal editing benchmark for diverse visual knowledge. In The Thirteenth International Conference on Learning Representations, 2025. 8 [6] Xingyu Fu, Ben Zhou, Ishaan Preetam Chandratreya, Carl Vondrick, and Dan Roth. There is time and place for reasoning beyond the image. arXiv preprint arXiv:2203.00758, 2022. 8 [7] Pranay Gupta and Manish Gupta. Newskvqa: Knowledgeaware news video question answering. In Pacific-asia conference on knowledge discovery and data mining, pages 3 15. Springer, 2022. [8] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pretraining. In International conference on machine learning, pages 39293938. PMLR, 2020. 8 [9] Han Huang, Haitian Zhong, Tao Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Vlkeb: large visionarXiv language model knowledge editing benchmark. preprint arXiv:2403.07350, 2024. 8 [10] Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Watching the news: Towards videoqa modIn Proceedings of the IEEE/CVF Winels that can read. ter Conference on Applications of Computer Vision, pages 44414450, 2023. 8 [11] Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh Ramakrishnan, and Soumen Chakrabarti. Select, substitute, search: new benchmark for knowledgeaugmented visual question answering. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 24912498, 2021. 8 [12] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959, 2024. 2, 4, 5, 6, [13] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. 2, 7 [14] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering In Proceedings benchmark requiring external knowledge. of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 8 [15] Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd Van Steenkiste, Lisa Anne Hendricks, Aishwarya Agrawal, et al. Benchmarking vision language models for cultural understanding. arXiv preprint arXiv:2407.10920, 2024. 8 [16] OpenAI. Gpt-4o, 2024. Accessed: 2024-06-01. 3, 4, 5, 6, 7, 8 [17] OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence. https://openai.com, 2024. 2, 4, 5, 1 [18] OpenAI. Search gpt. OpenAI, 2024. Accessed: 2025-03-29. 2, 8 [19] David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. Cvqa: Culturally-diverse multilingual visual question answering benchmark. arXiv preprint arXiv:2406.05967, 2024. 8 [20] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowlIn European conference on computer vision, pages edge. 146162. Springer, 2022. 8 [21] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware visual question answering. In Proceedings of the AAAI conference on artificial intelligence, pages 88768884, 2019. 8 [22] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 4, 1 [23] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, arXiv preprint et al. arXiv:2503.19786, 2025. 2, 4 Gemma 3 technical report. [24] Qwen Team. Qvq-72b-preview: large multimodal model by qwen. https://qwenlm.github.io/zh/blog/ qvq-72b-preview/, 2024. 2, 4, [25] Qwen Team. Qvq-max: Think with evidence. https:// qwenlm.github.io/zh/blog/qvq-max/, 2024. 2, 4 [26] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: machine comprehension dataset. arXiv preprint arXiv:1611.09830, 2016. 8 9 [27] Jiexin Wang, Adam Jatowt, and Masatoshi Yoshikawa. Archivalqa: large-scale benchmark dataset for opendomain question answering over historical news collections. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 30253035, 2022. 8 [28] Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony Dick. Explicit knowledge-based reaarXiv preprint soning for visual question answering. arXiv:1511.02570, 2015. 8 [29] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):24132427, 2017. [30] Zhen Wang, Xu Shan, Xiangxie Zhang, and Jie Yang. N24news: new dataset for multimodal news classification. arXiv preprint arXiv:2108.13327, 2021. 2 [31] Jiale Wei, Xiang Ying, Tao Gao, Felix Tao, and Jingbo Shang. Ai-native memory 2.0: Second me. arXiv preprint arXiv:2503.08102, 2025. 2 [32] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: novel task for fine-grained image understanding. arXiv preprint arXiv:1901.06706, 2019. 8 [33] Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. Search-in-the-chain: Interactively enhancing large language models with search for knowledge-intensive In Proceedings of the ACM Web Conference 2024, tasks. pages 13621373, 2024. 8 [34] Junxiao Xue, Quan Deng, Fei Yu, Yanhao Wang, Jun Enhanced multimodal rag-llm arXiv preprint Wang, and Yuehua Li. for accurate visual question answering. arXiv:2412.20927, 2024. 8 [35] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2, 4, 5, 1 [36] Zhenguo Yang, Jiale Xiang, Jiuxiang You, Qing Li, and Wenyin Liu. Event-oriented visual question answering: The e-vqa dataset and benchmark. IEEE Transactions on Knowledge and Data Engineering, 35(10):1021010223, 2023. 8 [37] Bhada Yun, Dana Feng, Ace Chen, Afshin Nikzad, and Niloufar Salehi. Generative ai in knowledge work: Design implications for data navigation and decision-making. arXiv preprint arXiv:2503.18419, 2025. 2 [38] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. Retrieving multimodal arXiv information for augmented generation: survey. preprint arXiv:2303.10868, 2023. 8 10 LIVEVQA: Live Visual Knowledge Seeking"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Additional Experiment Results Implementation Challenges and Engine Improvements. During the reproduction and deployment of the MMSearch engine, we encountered number of practical challenges and implemented several targeted improvements. First, in terms of environment configuration, we observed that multiple multimodal models (e.g., Qwen and LLaVA) have incompatible dependencies and must be installed in separate virtual environments to avoid conflicts. Second, while implementing the web search module, we faced issues with frequent access being flagged as bot activity, which triggered CAPTCHA verification. This blocked page retrieval and interfered with both requery and rerank stages. Moreover, prompt design proved critical in the multimodal reasoning chain. If the model in Stage 1 fails to extract valid information from the input image, it generates an uninformative requery, which propagates errors downstream. We also observed cases where, despite having relevant screenshots, the model selected irrelevant web pages during rerank (Stage 2), degrading performance in the summarization stage (Stage 3). To mitigate these issues, we implemented the following strategies: (1) If Stage 1 yields no valid information from the image, the requery defaults to the original query, avoiding error amplification; (2) If the retrieved screenshot is CAPTCHA page, the system skips it directly to ensure robustness; (3) If Stage 3 still fails to produce valid searchbased content, we fallback to directly querying the model with the image and original question. These improvements significantly enhance the systems stability and overall answer quality, particularly in complex visual-language scenarios. Accuracy variate across different category. We show the accuracy percentages of different models across various question types, difficulty levels, and domains, evaluated both with and without search functionality. In Table 5, Models like Gemini-2.0-Flash [22] stand out across several domains, achieving the highest overall accuracy of 24.93%. It also performs exceptionally well in domains such as Music, Sports, Global, and Science, with accuracy reaching 24.63%, 27.59%, 29.85%, and 25.59%, respectively. Other models like Qwen2.5-VL-72B and Qwen2.5VL-32B [35] also perform well, but Gemini-2.0-Flash [22] generally outperforms them, particularly in more challenging domains. Gemini-2.0-Flash [22] again demonstrates superior performance with an overall accuracy of 29.00%, excelling in domains like Movies, Technology, and Other, where it achieves 33.80%, 33.33%, and 40.91%, respectively. The GPT-4o-mini [17] model also shows competitive results, especially in the domains of Music and Art, with scores of 17.65% and 46.15%, respectively. Models with search functionality, especially Gemini-2.0-Flash [22], consistently outperform those without, indicating the positive impact of integrating search capabilities into these models. These results suggest that while all models have strengths in specific domains, Gemini-2.0-Flash [22] is particularly effective across broader range of topics. 7. Prompt We design series of prompts to help the model better handle the following key process of our pipeline: (1) content classification for domain-specific organization, as shown in Table 6; (2) basic image-based QA generation with relevance verification, as shown in Table 7; (3) contextual QA generation requiring news background knowledge, as shown in Table 8; (4) question type diversification to ensure coverage across different information categories, as shown in Table 9; (5) binary correctness evaluation for answer validation, Table 10. 8. Taxonomy Our news content is categorized into the following areas: Health: Content related to health, medicine, wellness, diseases, or healthcare systems, including advancements treatments, preventive measures, in medical research, It also healthcare policies, and trends in public health. covers topics like mental health, fitness routines, nutrition, and global health challenges. Science: Content about scientific research, discoveries, space exploration, or natural phenomena. This includes the latest breakthroughs in fields like biology, chemistry, physics, and environmental science, as well as updates on space missions, astronomical research, and the exploration of the universe. Television: Content about TV shows, streaming series, or television industry news. This encompasses reviews, ratings, cast interviews, behind-the-scenes content, and industry trends related to television networks, cable, and streaming platforms such as Netflix, Amazon Prime, or Disney+. Movies: Content related to films, movie reviews, film industry, or cinema releases. It covers the latest boxoffice hits, film reviews, director interviews, actor spotlights, and discussions on the impact of movies in popular 1 Table 5. Accuracy (%) of different models across question types and difficulty levels Model Avg. Mus. Mov. The. Tel. Med. Spo. Glo. Sci. Eco. Tec. Hel. Opi. Art. Oth. w.o. Search Gemma-3-4b-it Gemma-3-12b-it Gemma-3-27b-it Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B GPT-4o GPT-4o-mini Gemini-2.0-Flash QvQ-72B-Preview QvQ-max 14.65 17.10 20.43 15.63 18.74 18.96 21.07 16.38 17.30 24.93 19.94 17.80 8.82 8.21 16.42 10.45 13.46 11.94 15.67 5.97 7.46 24.63 16.18 11.19 14.77 13.13 17.51 12.79 18.52 14.48 17.85 14.81 15.15 24.24 17.39 13.80 13.64 18.18 18.18 9.09 16.00 11.36 20.45 20.45 13.64 20.45 20.45 13.64 11.41 14.07 15.97 9.89 14.51 13.69 16.73 11.03 11.41 25.86 15.21 15. 19.16 22.75 26.35 22.75 21.53 23.35 23.95 11.98 14.37 31.14 23.95 15.57 16.02 18.46 23.14 18.69 21.52 21.80 25.25 18.91 20.69 27.59 22.80 21.91 20.15 21.64 23.88 18.66 19.76 23.13 26.87 22.39 20.90 29.85 23.13 19.40 14.66 15.52 18.53 12.93 17.32 18.53 19.40 21.12 24.57 18.97 20.26 16.81 16.59 19.43 21.80 19.91 24.26 23.70 23.70 14.69 15.64 25.59 20.85 21.80 16.15 15.38 23.85 15.38 20.78 21.54 21.54 23.08 20.00 25.38 22.31 15. 11.73 17.35 18.88 14.80 16.10 18.37 16.33 15.31 14.80 20.41 17.86 16.84 12.98 17.56 19.08 12.98 16.78 17.56 19.85 11.45 12.21 24.43 19.85 17.56 13.11 13.93 14.75 13.93 16.18 13.93 15.57 15.57 16.39 20.49 15.57 12.30 13.55 18.22 19.94 14.80 16.78 18.69 20.09 16.51 17.60 23.05 18.69 17.76 w. Search GPT-4o GPT-4o-mini Gemini-2.0-Flash 13.38 22.27 29.46 2.99 14.93 32.84 12.12 16.16 27.27 15.91 11.36 25.00 8.75 18.25 32.70 10.18 19.76 32. 15.46 27.03 32.26 12.69 23.88 38.06 19.83 23.28 21.98 13.74 22.75 29.86 16.15 29.23 30.00 14.29 25.51 23. 10.69 16.03 25.95 11.48 16.39 22.13 13.55 22.12 28.50 w. MM-Search [12] GPT-4o GPT-4o-mini Gemini-2.0-Flash 20.20 21.80 29. 17.65 17.65 23.53 19.72 18.31 33.80 16.67 33.33 16.67 13.33 8.89 33.33 17.65 38.24 29.41 22.64 20.13 22. 11.11 3.70 22.22 50.00 56.25 31.25 15.38 7.69 23.08 25.00 33.33 25.00 22.22 22.22 44.44 8.33 0.00 16. 15.38 46.15 38.46 21.21 28.79 40.91 Table 6. Prompts for question category classification. Task Prompt System Prompt for Classification You are professional content classification assistant. Your task is to categorize the provided content into one of the specified categories, returning only the category name. User Prompt for Content Classification Please classify the following content into the most appropriate single category from the list provided. Title: {topic} Content Description: {topic description} Image Path: {image path} Available categories with descriptions: - Health: Content related to health, medicine, wellness, diseases, or healthcare systems - Science: Content about scientific research, discoveries, space exploration, or natural phenomena - Television: Content about TV shows, streaming series, or television industry news - Movies: Content related to films, movie reviews, film industry, or cinema releases - Economy: Content about finance, markets, economic policies, or business trends - Sports: Content about athletic competitions, sports teams, athletes, or sporting events - Theater: Content about stage performances, plays, musicals, or theatrical productions - Music: Content about songs, musicians, concerts, albums, or the music industry - Opinion: Content expressing viewpoints, editorials, or commentary on current events - Art & Design: Content about visual arts, exhibitions, design, fashion, or architecture - Media: Content about journalism, publishing, social media, or news organizations - Technology: Content about tech innovations, gadgets, software, or digital trends - Global Business: Content about international trade, multinational corporations, or global economics - Other: Content that doesnt clearly fit into any of the above categories Analyze the title and content description carefully to determine the most suitable category. Please respond with only the category name, without any additional text or explanation. For example, if its sports news, just reply with Sports. culture. It also includes information about film festivals, award shows, and cinematic trends. Economy: Content about finance, markets, economic policies, or business trends, including the analysis of macroeconomic indicators like GDP, inflation, and unemployment rates. It also focuses on financial markets, investments, global trade, corporate strategies, and the economic implications of policy changes or technological disruptions. Sports: Content about athletic competitions, sports teams, athletes, or sporting events. This includes news on professional and amateur sports, tournament results, profiles of famous athletes, match highlights, and coverage of major sporting events like the Olympics, FIFA World 2 Task Prompt Table 7. Prompts for Level 1 QA generation. Topic-Image Relevance Check Basic QA Generation You are an assistant that determines if topic and an image are directly related. Your task is to analyze the image and topic carefully, and decide if the image clearly depicts or is directly relevant to the topic. Guidelines: 1. The image must clearly depict the topic or be directly relevant to it. 2. If the image is only loosely related, indirectly related, or unrelated to the topic, it should be marked as irrelevant. 3. Be strict in your assessment. Only mark an image as relevant if there is clear and direct connection to the topic. 4. Respond ONLY with <relevant> if the image is directly related to the topic, or <irrelevant> if it is not. You are an educational assistant specialized in creating simple, image-based Q&A pairs related to current topics and news. - The question Generate ONE simple question-answer pair by following these requirements: MUST begin with Based on the provided image, ... - The question should be SIMPLE and DIRECT, focusing on identifying people, objects, events, places, or dates visible in the image. - The answer must be SHORT PHRASE (2-7 words), NOT complete sentence. - The answer must be FACTUAL, UNIQUE, and VERIFIABLE. Table 8. Prompts for Level 2 QA generation. Task Prompt System Prompt for News Context QA User Prompt for News Context QA You are specialized AI assistant for creating image-based questions that require NEWS CONTEXT to answer, not just whats visibly obvious. Your task is to analyze the provided image and generate ONE question-answer pair that: 1. Begins with Based on the provided image, ... but requires understanding of news context to answer 2. Cannot be answered by simply describing whats visible in the image 3. Asks about SPECIFIC FACTS, EVENTS, FIGURES, DATES, or NAMES related to the news context 4. Is DIFFERENT from existing questions Guidelines: - Questions MUST begin with Based on the provided image, ... - Focus on FACTUAL, VERIFIABLE information from the news context - Questions should have SINGLE, DEFINITIVE answers based on the news article - Answers must be SHORT (2-7 words), direct phrases, not complete sentences - NEVER reference or reveal answers from existing questions in your new question - Use generic references like this person, this building, etc. even if you know their names from previous QA pairs TOPIC: {topic} CATEGORY: {category} NEWS DESCRIPTION (VERY IMPORTANT): {description} IMAGE DESCRIPTION: {image description} EXISTING QUESTIONS AND ANSWERS: {json.dumps(existing qa, indent=2)} Generate ONE NEW question-answer pair that: 1. Begins with Based on the provided image, ... but requires NEWS CONTEXT to answer 2. Cannot be answered by simply describing whats visible in the image 3. Relates to underlying events, significance, impacts, or context shown in the image 4. Has direct, SHORT answer (2-7 words) 5. Is different from the existing questions Focus on the IMPLICATIONS, CONTEXT, SIGNIFICANCE or BACKGROUND of whats shown, not on obvious visual elements. Use the NEWS DESCRIPTION provided above, which is very important to generate meaningful question that requires context. Cup, Super Bowl, or the NBA Finals. Theater: Content about stage performances, plays, musicals, or theatrical productions. It includes reviews of live performances, interviews with theater professionals, trends in stage design, acting, and direction, as well as coverage of Broadway, West End, and off-Broadway productions. Music: Content about songs, musicians, concerts, albums, or the music industry. It covers the latest album releases, chart-topping songs, artist interviews, concert reviews, music awards, and trends in various genres such as pop, rock, hip-hop, classical, and electronic music. Opinion: Content expressing viewpoints, editorials, or commentary on current events. It includes opinion pieces, thought-provoking essays, and editorials on topics such 3 Task Prompt Table 9. Prompts for Level 3 QA generation. Diverse Question Type Generation You are specialized AI assistant for creating image-based questions that require NEWS CONTEXT to answer, not just whats visibly obvious. It is forbidden to give ambiguous answers. Your task is to analyze the provided image and generate ONE question-answer pair that: 1. Begins with Based on the provided image, ... but requires understanding of news context to answer 2. Cannot be answered by simply describing whats visible in the image 3. Asks about SPECIFIC FACTS, EVENTS, FIGURES, DATES, or NAMES related to the news context 4. Is DIFFERENT in TYPE from the existing questions CRITICAL RULE: - NEVER use ANY person names, organization names, product names, or event names that appear in ANY existing answer. - Instead of specific names, always use generic terms like this person, the organization, etc. QUESTION TYPES to consider (prioritize types NOT already used): - LOCATION questions (where an event happened) - TIME questions (when something occurred) - QUANTITY questions (how many, how much) - CAUSE questions (why something happened) - EFFECT questions (what resulted from an event) - COMPARISON questions (how things differ) - METHOD questions (how something was accomplished) - PURPOSE questions (the goal of an action) Table 10. Prompts for LLM-as-a-Judge. Task Prompt Answer Correctness Evaluation You are an impartial judge evaluating if models response correctly answers question. Ground Truth Answer: {gt answer} Model Response: {model answer} Does the model response correctly answer the question based on the ground truth? Answer with ONLY yes or no. Include your final answer within <answer> tags. as politics, culture, society, technology, and the environment, offering diverse perspectives and insights. Art & Design: Content about visual arts, exhibitions, design, fashion, or architecture. This includes news on art exhibitions, gallery openings, artist profiles, design trends, architecture innovations, and fashion movements. It also covers topics like graphic design, industrial design, and the influence of art on culture. Media: Content about journalism, publishing, social media, or news organizations. It covers trends in digital media, the role of social media platforms in news distribution, journalistic integrity, and the challenges faced by traditional media in the digital age. It also includes stories about the changing landscape of journalism, fake news, and media ethics. Technology: Content about tech innovations, gadgets, software, or digital trends. This includes updates on the latest technological advancements in fields like artificial intelligence, cybersecurity, mobile technology, and software development. It also covers the impact of emerging technologies on various industries and society as whole. trade, multinational corporations, or global economics. This inGlobal Business: Content about international cludes insights into the global business landscape, supply chain issues, international trade agreements, and the strategies of large multinational companies. It also focuses on the economic and political challenges facing global markets. Other: Content that doesnt clearly fit into any of the above categories. This category includes miscellaneous news and topics that are relevant or important but dont fall under traditional categories. It could include lifestyle trends, regional news, niche interests, or emerging topics that dont yet have broad classification. 9. Case Study We select the QvQ-72B-Preview [24] and Gemini-2.0Flash [22] models, which exhibit strong overall capabilities, for demonstration. The former showcases robust reasoning abilities , while the latter, being the most recently released model, demonstrates the strongest performance when combined with MM-Search [12]. The results compared with other models are shown in Figures 3 and 7. We provide examples of all 8 types of questions, as shown in Figures 8, 9, 10, 11, 12, 13, 14 and 15. The search functionality enhances Gemini-2.0-Flashs performance. Question: Based on the provided image, when will the stamp collection be available? Ground Truth: March 27 Gemma-3-4b-it: November 21, 2023 Qwen2.5-VL-7B-Instruct: Not specified in the image GPT-4o-mini: The stamp collection will be available on Friday, May 23. Gemma-3-27b-it: Now/Soon Qwen2.5-VL-3B-Instruct: 1ST Qwen2.5-VL-72B-Instruct: 1st Qwen2.5-VL-32B-Instruct: 24 May 2019 QvQ-72B-Preview: cant answer this question Gemini-2.0-Flash(MM Search): March 27 Figure 7. The search functionality enhances Gemini-2.0-Flashs performance. 5 Question: Based on the provided image, what event involving these characters occurred in specific episode? Ground Truth: Incest kiss on yacht GPT-4o: cant identify the characters. Gemini-2.0-Flash: spending time with his son, Albie. In that episode, Dominic expresses his gratitude to Jack for QvQ-72B-Preview: heartwarming reunion Figure 8. An example of Event type of question-answer. 6 Question: Based on the provided image, who is the person wearing the colorful dress? Ground Truth: Selena Gomez GPT-4o: Im sorry, but cant tell who the person in the image is. Gemini-2.0-Flash: Based on the image, the person wearing the colorful dress appears to be Selena Gomez. QvQ-72B-Preview: Selena Gomez Figure 9. An example of Person type of question-answer. 7 Question: Based on the provided image, what is the name of the building? Ground Truth: United States Institute of Peac GPT-4o: The building is the United States Institute of Peace. Gemini-2.0-Flash: United States Institute of Peace. QvQ-72B-Preview: United States Institute of Peace Figure 10. An example of Location type of question-answer. 8 Question: In this event, when will the individual start giving reports for the occasion? Ground Truth: July 26, 2024 GPT-4o: Im unable to determine. Gemini-2.0-Flash: Based on the image, it is impossible to determine. QvQ-72B-Preview: At the beginning of the event. Figure 11. An example of Time type of question-answer. 9 Question: In this image, which football team are they playing for? Ground Truth: Hearts GPT-4o: Hearts Gemini-2.0-Flash: Hearts QvQ-72B-Preview: Hearts Figure 12. An example of Organization type of question-answer. 10 Question: Based on the provided image, what type of structure is depicted? Ground Truth: Underwater habitat GPT-4o: Underwater habitat Gemini-2.0-Flash: Underwater habitat or research station QvQ-72B-Preview: Underwater habitat or research facility Figure 13. An example of Object type of question-answer. 11 Question: In this event, why was the Justice Department lawyer reportedly fired? Ground Truth: Refusing gun rights restoration GPT-4o: Im sorry, but cant identify or provide information on individuals in images. Gemini-2.0-Flash: The person on the right in the image is Mel Gibson. He is associated with controversies involving antisemitic and racist remarks. QvQ-72B-Preview: anti-Semitic remarks Figure 14. An example of Reason type of question-answer. 12 Question: In this event, what type of assessment was not executed? Ground Truth: Vulnerability assessment GPT-4o: The image suggests that risk assessment was not executed properly, as it appears ship collision caused significant damage to bridge. Proper risk assessments could have identified potential hazards and helped prevent this incident. Gemini-2.0-Flash: ship colliding with and collapsing major bridge structure QvQ-72B-Preview: Structural integrity assessment Figure 15. An example of Other type of question-answer."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "University of Washington"
    ]
}