{
    "paper_title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains",
    "authors": [
        "Chun Wang",
        "Xiaoran Pan",
        "Zihao Pan",
        "Haofan Wang",
        "Yiren Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, a high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 0 0 7 8 1 . 5 0 5 2 : r GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains Chun Wang1, Xiaoran Pan1, Zihao Pan2, Haofan Wang3, Yiren Song4 1Zhejiang University, 2Sun Yat-sen University, 3LibLib.ai, 4NUS zjuheadmaster@zju.edu.cn, {pixe1ran9e,haofanwang.ai,songyiren725}@gmail.com, panzh33@mail2.sysu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE."
        },
        {
            "title": "1\nWorldwide image geo-localization [29, 46] aims to predict the geographical coordinates of the\nshooting location based on any given photo taken anywhere on Earth. Unlike geo-localization within\nspecific regions [18, 25, 41], global geo-localization, unrestricted to any specific region but covering\nthe entire Earth, greatly unleashes the potential of geo-localization, which has significant applications\nacross multiple domains, such as autonomous driving system positioning, social media image geo-\ntagging, and cultural heritage preservation. However, precise global-scale image geo-localization still\nfaces substantial technical challenges due to the vast diversity of global geographical environments,\nvisual ambiguity between similar locations, and the variability of shooting conditions including\nweather patterns, seasonal changes, and lighting conditions.",
            "content": "Corresponding author : songyiren725@gmail.com. Preprint. Under review. Figure 1: Performance comparison of our reasoning-based GRE versus traditional alignment-based approaches and MLLM baselines on image geo-localization. Geo-localization requires predicting the geographic coordinates of photograph solely from the ground-view image. Extracting general geographical visual semantics is insufficient for the task, as two distant locations could potentially share similar image-level features. Instead, models need to identify and reason with geographically relevant visual elements from complex visual information. As illustrated in Fig. 1, when inferring the target location - San Diego Convention Center, the model is expected to jointly leverage explicit indicators such as the white sail roof design and implicit indicators such as flat terrain. However, existing approaches [15, 44] rely on data-driven cross-modal alignment strategies, which establish correspondences through large-scale annotated image-GPS pairs while neglecting the inherent logical relationships among fine-grained geographical indicators within images. In addition, models need to predict geographic coordinates for images captured at any location in the world. However, existing methods based on closed-domain assumptions either maintain candidate database of GPS coordinates [15, 67] or images [21, 43, 49, 51, 70], or divide the entire geographical space into fixed grids for classification purposes [6, 13, 24, 29, 45], compromising the continuity of coordinate prediction. Thus, it is essential for image geo-localization models to possess the ability to predict open-ended coordinates without relying on candidate information, feature that current methods inadequately address. Recently, DeepSeek-R1 [8] has successfully applied Reinforcement Learning (RL) to induce the self-emergence of complex cognitive reasoning ability in LLMs. Image geolocalization is inherently multi-step cognitive process that requires progressive reasoning - from identifying visual cues in images, to inferring geographical correlations among these cues, and ultimately determining specific locations. This progressive reasoning process aligns naturally with the sequential decision-making characteristics of RL. Through RL, models can learn to formulate optimal reasoning strategies based on identified visual features, gradually narrowing down potential geographical regions, and ultimately arriving at accurate location predictions, rather than simply relying on pre-established image-GPS correspondences. Unfortunately, this direct RL training is challenged, as it struggles to effectively guide MLLMs generating complex CoT reasoning in absence of large-scale, high-quality multimodal data and prolonged training [11]. To address the aforementioned challenges, we propose Geo Reason Enhancement (GRE), novel reasoning solution that integrates cold-start supervised fine-tuning and two-stage reinforcement learning training for worldwide image geolocalization. To facilitate the training process, we establish geography reasoning dataset GRE30k by leveraging o3 to generate chain-of-thought demonstrations for geography seed questions. Our curated GRE30K consists of two sub-datasets: GRE30K-CoT, which contains format-standardized CoT content and answers refined through annotator filtering, and GRE30K-Judge, which comprises reasoning chain judgment tasks constructed through regular expression matching. GRE30k-CoT serves as cold start dataset to establish basic reasoning capabilities of the base model. Then, we need to apply two-stage Group Relative Policy Optimization (GRPO) [8, 35] on GRE30K-Judge and seed questions to enhance the models reasoning capability. 2 Figure 2: Summary of current image geo-localization model architectures. Furthermore, to rigorously assess models ability to leverage geographical visual cues for geolocalization and evaluate the quality of their reasoning chains, we develop benchmark named Geo Reason Evaluation Benchmark (GREval-Bench). Specifically, we design an automated pipeline to filter images containing geographical indicators and provide each image with corpus of explicit and implicit geographical identifiers along with high-quality CoT annotations. We summarize the key contributions of our work as follows: We present GRE, novel reasoning solution for the worldwide image geo-localization task. Our proposed methodology integrates cold-start initialization with two-stage reinforcement learning training paradigm to effectively leverage geographical indicators within images and enable openended geolocalization. We introduce GER30K, comprising high-quality CoT dataset and judgement task dataset. We anticipate the dataset will benefit more future work for location-aware visual reasoning. Furthermore, to comprehensively evaluate the image geo-localization capability of the models, we develop GREval-Bench, consisting of higher quality images, CoT quality assessments, and corpus of geographic indicators."
        },
        {
            "title": "2 Related Work\nImage Geo-localization. Image Geo-localization is an important task in computer vision [71–\n73], spatial data mining [61], and GeoAI [62]. As shown in Fig. 2, previous work in image\ngeo-localization can be divided into four main modes: classification mode, retrieval mode, extra\ninformation mode and RAG mode. (1) Retrieval mode treat the image geo-localization task as a\nretrieval problem, typically maintaining a database of images [21, 43, 49, 51, 69, 70] or a gallery of\nGPS coordinates [44]. They take the most similar images and GPS coordinates to the query image as\nthe predicted values. However, maintaining a global-level image database or GPS gallery is infeasible.\n(2) Classification mode [6, 24, 29, 33, 46, 48] divide the entire earth into multiple grid cells and\nassign the center coordinates as predicted values. Models are then trained to classify the input image\ninto the correct cell. However, if the actual location of the image is far from the center of the predicted\ncell, there can still be significant errors, even if the cell prediction is correct. (3) Extra information\nmode approaches [45] incorporate higher-level geographical information, such as continental-scale\npriors, to enhance performance. Nevertheless, this approach essentially provides partial solutions,\ncontradicting the fundamental purpose of the task. (4) RAG mode [15, 67] leverage large language\nmodels by retrieving relevant image-GPS pairs as references to optimize predictions. However, these\napproaches rely on establishing large-scale aligned databases. In contrast to existing global image\ngeo-localization approaches, we propose a reasoning-based methodology that leverages both explicit\nand implicit geographical indicators within images to predict open-ended coordinate prediction.",
            "content": "Vision Language Models (VLMs). Models in the vein of GPT-4o [26] achieve excellent visual understanding ability by integrating both visual and textual data. This integration enhances the models ability to understand complex multi-modal inputs and enables more advanced AI systems [19, 20, 47, 58] capable of processing and responding to both images and text. Generally, the training of LVLMs involves two steps: (a) pre-training and (b) post-training which contains supervised finetuning and reinforcement learning. Post-training is crucial in improving the models response quality, instruction following, and reasoning abilities. While there has been significant research on using reinforcement learning to enhance LLMs during post-training [1, 4, 28, 31, 36, 37, 40, 52, 56, 64, 74], the progress for LVLMs has been slower. In this paper, we propose GRE-RL, which used GRPO3 Figure 3: Overview of our GRE framework. The geographical reasoning pipeline begins with data preparation, incorporating automated CoT generation, regular expression matching, and manual filtering. Based on our constructed GRE30K dataset, we employ post-training procedure that consists of supervised fine-tuning to learn reasoning patterns, followed by two-stage rule-based reinforcement learning to enhance image geo-localization reasoning capabilities. based reinforcement algorithms and verifiable reward during the post-training phase to enhance the models visual perception and reasoning capabilities. Reinforcement Learning. Recently, with the emergence of reasoning models like OpenAIs o1 [14] and Deepseek-R1 [7], the research focus in Large Language Models (LLMs) has increasingly shifted towards enhancing the models reasoning capabilities through reinforcement learning (RL) techniques. Studies have explored improving LLMs performance in reasoning tasks such as solving mathematical problems [3, 23, 34, 50, 53] and coding [12, 16, 57, 60]. notable breakthrough in this area is Deepseek-R1-Zero [7], which introduced new approach to achieving robust reasoning capabilities using RL merely, eliminating the supervised fine-tuning (SFT) stage. However, current research on RL-based reasoning has largely been confined to the language domain, with limited exploration of its application in multi-modal settings. For LVLMs, RL has primarily been used for tasks like mitigating hallucinations and aligning models with human preference [22, 38, 39, 54, 55, 63, 65, 66], but there remains significant gap in research focusing on enhancing reasoning and visual perception of Large Vision Language Models. To address this gap, our work uses novel reinforcement fine-tuning strategy , applying verifiable rewards with GRPO-based [34] RL to visual geo-localization tasks. Our approach aims to improve the performance of LVLMs in processing various geo-localization tasks, especially when the high-quality fine-tuning data is limited."
        },
        {
            "title": "3 Methodology",
            "content": "Fig. 3 illustrates the comprehensive reasoning pipeline of GRE. This method begins with coldstart using high-quality geo-localization Chain-of-Thought dataset, which initially teaches the base model to reason step-by-step following human-like patterns. Subsequently, we apply twostage reinforcement learning training to the cold-start initialized model GRE-CI to guide it towards adopting the correct geographical reasoning process, thereby enhancing the geo-localization reasoning capability in the final model GRE. In the following sections, we first describe our approach to create high-quality geo-localization reasoning dataset GRE30K in Section 3.1. Then we introduce our proposed Post-Training Strategy, comprising cold-start supervised fine-tuning ( Section 3.2.1) and two-stage reinforcement learning training ( Section 3.2.2). Correspondingly, our GRPO-based training strategy and two-stage reward function design will be described in Section 3.3. 4 3.1 GRE30K Construction In this section, we present GRE30K, geo-localization reasoning dataset designed to enhance the visual reasoning capability of MLLMs. Specifically, GRE30K consists of GRE30K-CoT for cold-start Initialization and GRE30K-Judge for reinforcement learning. Examples of the generated data are provided in Appendix A.1. Reasoning Process Generation. We make full use of the publicly available dataset MP16-Pro [15] with GPS coordinates. However, the source dataset only contains images, coordinates, and discrete geographical information including the corresponding county and state for each image, which are insufficient to train an MLLM. Our goal is to construct CoT dataset that encompasses complex cognitive processes to facilitate our training strategy, enabling GRE to reason in manner that closely resembles human cognitive patterns. Furthermore, GPT-o3 has demonstrated the capabilities in generating CoT reasoning that mirrors natural cognitive processes and has proven to have strong reasoning capability. Leveraging these insights, we employ GPT-o3 to generate image-CoT-coordinate triples through meticulously designed prompt templates. Please refer to Appendix A.2 for the detailed prompts for GPT-o3. GRE30K-CoT. To address potential errors and mismatches in source CoT data, we combine automated filtering and manual verification to ensure the quality and reliability of the test data. Please refer to Appendix A.3 for more details. Finally, we collect 20k high-quality CoT samples. By acquiring CoT data in this manner, which closely mimics human cognitive behavior, reasoning processes exhibit natural and logical thinking. GRE30K-Judge. In addition to standardizing the models reasoning process through high-quality CoT data, we develop GRE30K-Judge, judgment task dataset. This dataset is created by comparing extracted predictions with ground truth using threshold θ, labeling images as \"Truth\" or \"False\" accordingly. The resulting dataset is incorporated into reinforcement learning training, enabling the model to learn from both correct and incorrect reasoning patterns and thereby enhancing its geographical reasoning abilities. In total, we obtain 10k judgment samples. 3.2 Post-Training Strategy To enhance visual reasoning capabilities, we introduce three-stage post-training strategy consisting of cold-start initialization and two-stage rule-based reinforcement learning (RL). SFT stabilizes the models reasoning process and standardizes its output format, while RL further improves generalization across various geo-localization tasks. 3.2.1 Cold-start Initialization Leveraging the GRE30K-CoT dataset, we conduct SFT on pretrained MLLM as the base MLLM for cold-start initialization. The MLLM after cold start initialization is named as GRE-CI. At this stage, the base MLLM had learned the complex reasoning mode from o3 [27]. Through SFT with the GRE30K-CoT dataset, the model standardize output format and establish systematic reasoning framework. This critical phase facilitates the models acquisition of high-quality structured reasoning patterns, thereby constructing solid foundation for subsequent RL procedures. 3.2.2 Reinforcement Learning on the GRE-CI Building upon the SFT-trained model, we employ rule-based reinforcement learning (RL) to optimize structured reasoning and ensure output validity. Specifically, we define two kinds of reward rules inspired by R1 and update the model using Group Relative Policy Optimization (GRPO). The RL stage further encourages the model to generate reliable outputs and enhances its generalization capabilities in geographical reasoning tasks. Please refer to Appendix C.1 for more details about the two-stage RL training pipeline. Rule-Based Rewards. We define two kinds of reward rules that evaluate the generated answers from two perspectives: Accuracy Reward: The accuracy reward rule evaluates the correctness of the final answer by extracting final answer via regular expressions and verifying them against the ground truth. For 5 image geo-localization task, the final answer must be provided in specified format to enable reliable rule-based verification. In RL stage I, given an input image along with its CoT and predicted answer, the model evaluates the correctness of both the reasoning process and the final answer. The model receives reward score of ri = 1 only if the generated final result aligns with the ground truth; otherwise, it receives score of ri = 0. In RL stage II, where the model directly predicts coordinates based on the input image, the reward is determined by the threshold metric θ. Format Reward: In order to ensure the existence of the reasoning process, the format reward rule requires that the response must follow strict format where the models reasoning is enclosed between <think> and </think>. regular expression ensures the presence and correct ordering of these reasoning markers. Whats more, <answer> and </answer> are used to ensure model have given answer. 3.3 Group Relative Policy Optimization We employ GRPO to achieve balanced integration of consistent policy updates and robust reward signals in controlled manner. For each token in the generated output, GRPO first compute the log probabilities under both the new policy (πθ) and reference policy (πref). It then calculates the probability ratio and clips it to the range [1 ϵ, 1 + ϵ] to constrain policy updates and avoid divergence. The normalized reward (treated as an advantage estimate) is subsequently used in PPO-style loss function, combining policy optimization with KL-divergence (weighted by β) regularization: (cid:104) Lclip = min (cid:16) ratiot Advt, clipped_ratiot Advt (cid:17)(cid:105) . (cid:104) LGRPO(θ) = (cid:16) min ratiot Advt, clipped_ratiot Advt (cid:17) (cid:16) β KL πθ(y x), πref(y x) (cid:17)(cid:105) . (1) (2) Here, Advt denotes the advantage function, capturing how much better (or worse) particular action is compared to baseline policy value. Compared to other methods, the GRPO clipping mechanism prevents extreme policy shifts, while the KL regularization keeps the updated policy aligned with the baseline. This combination ensures that our model integrates rule-based rewards efficiently without compromising training stability. Subsequently, we will introduce the reward function adopted for second-stage( Eq. (4)) and third-stage( Eq. (5)). = geodesic(cid:0)(ϕpred, λpred), (ϕtrue, λtrue)(cid:1) (cid:26)1.0 0. if E(ypred) = E(ytrue) otherwise Ryes/no(ypred, ytrue) = Rgeo(ypred, ytrue) = (cid:40) 2 1+exp(d/θ) 0 if V(ypred, ytrue) = True otherwise (3) (4) (5) Here, θ denotes the threshold, it is used as factor to control the range of reward in this reward function Eq. (5). mean the boolean value of the prediction and mean the values of prediction and ground truth are valid."
        },
        {
            "title": "4 GREval-Bench\nTo comprehensively evaluate the image geo-localization capability of the models, we develop\na geographical reasoning benchmark named GREval-Bench. Existing benchmarks [9, 42] are\ndirectly constructed from geotagged Flickr images without appropriate filtering. Specifically, these\nbenchmarks contain numerous images that lack geographical relevance cues, such as portraits\nand object-focused photographs. The inclusion of such geographically uninformative samples\ncompromises the validity of evaluation results. Moreover, these benchmarks primarily focus on\nfinal predictions while neglecting the evaluation of the entire CoT process. The CoT process\nreflects multiple aspects of geographical reasoning capabilities and serves as a critical medium for\nunderstanding models’ reasoning patterns and limitations.",
            "content": "6 To address these challenges, we propose an semi-automated pipeline for geo-localization image filtering and CoT annotation generation in our GREval-Bench. Fig. 4 and Table 1 provide data statistics, respectively. Please refer to Appendix B.1 for more details of the GREval-Bench construction and evaluation pipeline. GREval-Bench comprises 3K triplets, each containing: (1) geographical inference images filtered through our pipeline, (2) corresponding corpus of geographical indicators categorized into explicit and implicit types, with detailed subcategories presented in Appendix B.2, and (3) annotated key Chain-of-Thought steps and reference GPS coordinates. Through our construction pipeline, we have enhanced both the image quality and complexity of the benchmark by eliminating noisy images lacking geographical indicators while increasing the proportion of samples that require reasoning based on implicit indicators. This improvement facilitates more accurate assessment of models geo-localization capabilities. Statistic Outdoor - natural scene - artificial landscape - agricultural scene - industrial scene - road traffic Indoor - commercial premises - offices - transportation place - cultural sites - medical place - entertainment venues Number 2400 811 1138 58 66 327 600 147 131 54 148 4 116 Figure 4: Indicators distribution of GREval-Bench. Table 1: Statistics of GREval-Bench. As illustrated in Fig. 5, we instruct GPT-4o [26] to categorize each reasoning step into three categories: background information, image caption, and logical inference. We calculate the recall between background information and the corresponding geography corpus. Then, we employ RefCLIPScore [10] to evaluate the semantic alignment between image captions and visual content, and utilize BertScore [59] to assess the similarity between predicted and ground-truth logical inference steps. As these components are crucial for visual reasoning, we calculate CoT-quality by the follow equation ( Eq. (6)). CoT-quality = Recall + RefCLIPS + BertS (6)"
        },
        {
            "title": "5 Experiment",
            "content": "Datasets and Evaluation details: We randomly sample 5% of MP-16 [17], dataset containing 4.72 million geotagged images from Flickr 2, as geography seed datasets to construct our GRE30K. This dataset is strategically utilized across our three-stage training process: GRE30K-CoT, comprising 20k high-quality Chain-of-Thought examples curated by geography experts and standardized in format, serves for cold-start initialization; GRE30K-Judge, consisting of 10k CoT judgment tasks, is employed for Stage reinforcement learning training and the remaining 170k seed datasets are utilized for Stage II reinforcement learning training. We test our trained model on Im2GPS3k [9] and Google World Streets 15k (GWS15k) [6]. To ensure fair comparison with existing methods in the evaluation of Im2GPS3k, both our proposed model and transformer-based models are trained using only 5% of the MP-16 dataset. Follow the protocol followed in previous works [15, 45], we report our results using threshold metric. Given the predicted coordinates and the ground truths, this metric quantifies the percentage of predictions where the distance to the ground truth falls within specified thresholds (1km, 25km, 200km, 750km, and 2500km). Implementation details: We adopt Qwen2.5-VL-7B as base model, the SFT experiments are conducted with batch size of 128, learning rate of 1e-5, and training over 1 epochs. Then, we perform RL on our dataset and experiment with training subsets of 10k for single epoch each. All experiments are conducted with Pytorch and 16 NVIDIA H20(96G) GPUs. 2https://www.flickr.com/ 7 Figure 5: detailed illustration of the evaluation pipeline. 5.1 Comparison with State-of-the-art methods We perform comparative analysis of GRE against worldwide Geo-Localization benchmarks, Im2GPS3k and GWS15k. The results on Im2GPS3k [9] and GWS15k [6] are shown in Table 2. In all metrics, our method surpasses the previous state-of-the-art (SOTA) model on Im2GPS3k, achieving improvements of +0.5%, +4.2%, +3.0%, +1.7% and +2.5% in the 1km, 25km, 200km, 750km, and 2500km thresholds respectively. The results on additional geographical benchmarks are put in Appendix C.2, where we also observe similar trend. Moreover, our approach exhibits large gain on the more challenging GWS15k dataset, surpassing the previous SOTA model with significant accuracy improvements of +0.2%, +1.0%, +2.0%, +9.1%, and +4.2% in the 1km, 25km, 200km, 750km, and 2500km thresholds respectively. Our model achieves superior performance over previous state-of-the-art approaches while utilizing merely 5% of the data, compared to their use of the complete MP-16 dataset. The GWS15k contains samples that are uniformly sampled across the Earth and are not biased towards any specific geographic location. Moreover, the images in this dataset have large distribution shift compared to the training set, making the geo-localization task tough and challenging for brute-force alignment approaches. Our substantial improvement can be attributed to effective reasoning that leverages both explicit and implicit geographical indicators within images. Table 2: We compare the performance of GRE with the state-of-the-art methods on (a) Im2GPS3k [9] and (b) GWS15k [6] datasets. Our method yields consistent gains across datasets and different distance thresholds. denotes transformer-based models. (a) Results on the Im2GPS3k [9] dataset (b) Results on the recent GWS15k [6] dataset Method [L]kNN, σ = 4 [46] PlaNet [48] CPlaNet [33] ISNs [24] Translocator [29] GeoDecoder [6] GeoCLIP [45] Ours Street City Region Country Continent 1 km 25 km 200 km 750 km 2500 km 7.2 8.5 10.2 3.2 7.6 5.7 10.8 11.3 38.9 48.4 48.6 25.1 40.7 28.9 67.6 69.3 55.9 64.6 64.6 43.9 63.3 38.6 83.2 85.7 19.4 24.8 26.5 9.6 20.3 10.3 31.1 35.3 26.9 34.3 34.6 14.3 27.1 21.4 48.7 51.7 Method ISNs [24] Translocator [29] GeoDecoder [6] GeoCLIP [45] Ours 8 Street City Region Country Continent 1 km 25 km 200 km 750 km 2500 km 0.05 0.5 0.7 0.6 0.9 15.5 25.5 26.9 45.7 54.8 38.5 48.3 50.5 74.1 78.3 4.2 8.0 8.7 16.9 18. 0.6 1.1 1.5 3.1 4.1 5.2 Performance on GREval-Bench We compare our approach on GREval-Bench with the previous generalist models, including InternVL2.5 series [5], InternVL3 series [68], Qwen2.5-VL series [2]. We conduct comprehensive evaluations of models, analyzing the above metric across different distance thresholds and scenarios, while also assessing the quality of its reasoning chains. Table 3 presents the comparison results. Our approach achieves the leading average performance in various evaluation metrics while demonstrating more coherent reasoning processes that avoid local cognitive traps. Models with smaller parameter sizes like Qwen2.5VL-3B and InternVL3-2B exhibit significantly greater difficulty in extracting implicit cues compared to their larger counterparts. These models frequently commit errors in the early stages of CoT reasoning, compromising subsequent logical coherence. Fig. 6 illustrates typical visual comparison. Method Street City Region Country Continent CoT 1 km 25 km 200 km 750 km 2500 km quality 1.76 ISNs 2.45 GeoCLIP 0.05 InternVL2.5-4B 0.33 InternVL2.5-8B 0.19 InternVL3-2B 1.32 InternVL3-8B 0.19 Qwen2.5VL-3B Qwen2.5VL-7B 0.33 Qwen2.5VL-32B 5.45 6.14 23.08 64.85 12.08 14.62 3.82 25.90 3.40 9.39 54.33 66.56 26.4 76.61 18.96 22.64 6.18 35.38 5.14 10.90 65.00 83. 16.94 34.08 5.09 6.75 1.56 14.34 2.03 6.84 37.41 44.67 11.23 15.71 2.74 3.44 0.75 7.50 0.61 4.34 23.12 26.15 - - 31.22 34.29 23.41 36.48 37.93 50.36 55.56 59.54 Ours Table 3: Performance comparisons among traditional leading models, open-source MLLMs, and our GRE on GREval-Bench. Figure 6: Visual demonstration of the performance of models. 5.3 Ablation Study To evaluate the effectiveness of our training data and training strategies, we compare the models performance under four distinct training strategies: (1) applying Cold-start Initialization on our dataset, (2) further optimizing the GRE-CI with RL stage I, (3) further optimizing the GRE-CI with RL stage II, and (4) further optimizing the GRE-CI with RL stage and stage II. As illustrated in Table 4, the application of CI on our dataset significantly enhances the models performance on both the coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. For (2) and (3) , (3) reach comparable performance and (2) dropped at some levels of granularity, attributed to the misalignment between training and test task (reward) types in Stage I. Overall, (4) demonstrates superior performance to (3) due to its more robust reasoning capabilities. Table 4: Ablation study on (a) Im2GPS3k [9] and (b) GWS15k [6] datasets. (a) Results on the Im2GPS3k [9] dataset (b) Results on the recent GWS15k [6] dataset Method Street City Region Country Continent 1 km 25 km 200 km 750 km 2500 km Method Street City Region Country Continent 1 km 25 km 200 km 750 km 2500 km 16.62 Qwen2.5-VL-7B 3.20 29.30 7.77 7.16 28.13 10.96 36.11 11.33 35.28 CI CI + CI + II CI + + II 28.03 44.78 42.41 52.17 51.72 42.14 62.43 63.29 67.26 69. 52.99 78.81 78.61 83.32 85.67 Qwen2.5-VL-7B 0.05 0.45 0.35 0.88 0.91 CI CI + CI + II CI + + II 0.29 2.17 2.03 3.91 4.13 1.39 12.91 12.82 18.69 18.86 4.43 37.58 37.88 55.61 54. 8.66 61.83 62.16 78.03 78."
        },
        {
            "title": "6 Conclusion\nIn this paper, we introduce a comprehensive framework for visual geo-localization reasoning, built\nupon a formalization approach that unifies data construction, model training, and evaluation. Our\nframework is designed to address the limitations of the current methods, enabling model to reason\nin geo-localization task. The ability of extracting of multigranular visual cues from images and\nintegrating with external world knowledge will also inspire us in other domains of VLMs. This\nframework has led to the creation of the GRE dataset, a rich resource featuring detailed step-by-step\nreasoning annotations designed to enhance model training and evaluation on geo-localization task.\nThe GRE model, trained using this framework, demonstrates strong geo-localization reasoning\ncapabilities and exhibits robust generalization across a diverse range of scenes, from implicit scenes",
            "content": "9 to explicit scenes. To further support the evaluation of geo-localization, we introduce GREval-Bench, comprehensive benchmark that rigorously assesses model performance across various geospatial scenario. Our extensive experiments validate the effectiveness of our approach, showing significant improvements over state-of-the-art open-source models."
        },
        {
            "title": "References",
            "content": "[1] Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232, 2023. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. [4] Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In ICLR, 2023. [5] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [6] Brandon Clark, Alec Kerrigan, Parth Parag Kulkarni, Vicente Vivanco Cepeda, and Mubarak Shah. Where we are and what were looking at: Query based worldwide image geo-localization using hierarchies and scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2318223190, 2023. [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] James Hays and Alexei A. Efros. Im2gps: estimating geographic information from single image. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 18, 2008. doi: 10.1109/CVPR. 2008.4587784. [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [11] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [12] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [13] Mike Izbicki, Evangelos Papalexakis, and Vassilis Tsotras. Exploiting the earths spherical geometry to geolocate images. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, Würzburg, Germany, September 1620, 2019, Proceedings, Part II, pages 319. Springer, 2020. [14] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv:2412.16720, 2024. [15] Pengyue Jia, Yiding Liu, Xiaopeng Li, Xiangyu Zhao, Yuhao Wang, Yantong Du, Xiao Han, Xuetao Wei, Shuaiqiang Wang, and Dawei Yin. G3: an effective and adaptive framework for worldwide geolocalization using large multi-modality models. Advances in Neural Information Processing Systems, 37:5319853221, 2024. [16] Fangkai Jiao, Geyang Guo, Xingxing Zhang, Nancy Chen, Shafiq Joty, and Furu Wei. Preference optimization for reasoning with pseudo feedback. arXiv preprint arXiv:2411.16345, 2024. 10 [17] Martha Larson, Mohammad Soleymani, Guillaume Gravier, Bogdan Ionescu, and Gareth JF Jones. The benchmarking initiative for multimedia evaluation: Mediaeval 2016. IEEE MultiMedia, 24(1):9396, 2017. [18] Seongwon Lee, Hongje Seong, Suhyeon Lee, and Euntai Kim. Correlation verification for image retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5374 5384, 2022. [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [20] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [21] Liu Liu and Hongdong Li. Lending orientation to neural networks for cross-view geo-localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 56245633, 2019. [22] Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Mia-dpo: Multi-image augmented direct preference optimization for large vision-language models. arXiv preprint arXiv:2410.17637, 2024. [23] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning, 2024. URL https://arxiv.org/abs/2401.08967. [24] Eric Muller-Budack, Kader Pustu-Iren, and Ralph Ewerth. Geolocation estimation of photos using hierarchical model and scene classification. In Proceedings of the European conference on computer vision (ECCV), pages 563579, 2018. [25] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large-scale image retrieval with attentive deep local features. In Proceedings of the IEEE international conference on computer vision, pages 34563465, 2017. [26] OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. [27] OpenAI. Openai o3-mini system card, 2025. URL https://openai.com/index/ o3-mini-system-card/. [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [29] Shraman Pramanick, Ewa Nowara, Joshua Gleason, Carlos Castillo, and Rama Chellappa. Where in the world is this image? transformer-based geo-localization in the wild. In European Conference on Computer Vision, pages 196215. Springer, 2022. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [31] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In ICLR, 2023. [32] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face In Proceedings of the IEEE conference on computer vision and pattern recognition and clustering. recognition, pages 815823, 2015. [33] Paul Hongsuck Seo, Tobias Weyand, Jack Sim, and Bohyung Han. Cplanet: Enhancing image geolocalization by combinatorial partitioning of maps. In Proceedings of the European Conference on Computer Vision (ECCV), pages 536551, 2018. [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [35] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [36] Charlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. Offline RL for natural language generation with implicit language learning. In ICLR, 2023. [37] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. In NeurIPS, 2022. [38] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [39] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [40] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. In ACL, 2024. [41] Fuwen Tan, Jiangbo Yuan, and Vicente Ordonez. Instance-level image retrieval using reranking transformers. In proceedings of the IEEE/CVF international conference on computer vision, pages 1210512115, 2021. [42] Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59 (2):6473, 2016. [43] Yicong Tian, Chen Chen, and Mubarak Shah. Cross-view image matching for geo-localization in urban In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, environments. pages 36083616, 2017. and Mubarak Shah. Clip- [44] Vicente Vivanco Cepeda, Gaurav Kumar Nayak, for effective worldwide geo-localization. inspired alignment between locations and images In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, editors, Advances in Neural Information Processing Systems, volume 36, pages 86908701. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 1b57aaddf85ab01a2445a79c9edc1f4b-Paper-Conference.pdf. and S. Levine, Geoclip: [45] Vicente Vivanco Cepeda, Gaurav Kumar Nayak, and Mubarak Shah. Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization. Advances in Neural Information Processing Systems, 36:86908701, 2023. [46] Nam Vo, Nathan Jacobs, and James Hays. Revisiting im2gps in the deep learning era. In Proceedings of the IEEE international conference on computer vision, pages 26212630, 2017. [47] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [48] Tobias Weyand, Ilya Kostrikov, and James Philbin. Planet-photo geolocation with convolutional neural networks. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 3755. Springer, 2016. [49] Scott Workman, Richard Souvenir, and Nathan Jacobs. Wide-area image geolocalization with aerial In Proceedings of the IEEE International Conference on Computer Vision, pages reference imagery. 39613969, 2015. [50] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [51] Hongji Yang, Xiufan Lu, and Yingying Zhu. Cross-view geo-localization with layer-to-layer transformer. Advances in Neural Information Processing Systems, 34:2900929020, 2021. [52] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In ICLR, 2023. 12 [53] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. [54] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. RlHF-V: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, 2024. [55] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. RLAIF-V: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. [56] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. IJCV, 2024. [57] Kechi Zhang, Ge Li, Yihong Dong, Jingjing Xu, Jun Zhang, Jing Su, Yongfei Liu, and Zhi Jin. Codedpo: Aligning code models with self generated and verified source code. arXiv preprint arXiv:2410.05605, 2024. [58] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Internlm-xcomposer-2.5: versatile large vision language model Bin Wang, Linke Ouyang, et al. supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. [59] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. [60] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. [61] Zijian Zhang, Xiangyu Zhao, Qidong Liu, Chunxu Zhang, Qian Ma, Wanyu Wang, Hongwei Zhao, Yiqi Wang, and Zitao Liu. Promptst: Prompt-enhanced spatio-temporal multi-attribute prediction. In CIKM. [62] Sen Zhao, Wei Wei, Ding Zou, and Xianling Mao. Multi-view intent disentangle graph networks for bundle recommendation. In Proceedings of the 36th AAAI Conference on Artificial Intelligence, volume 36, pages 43794387, 2022. [63] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023. [64] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. In ICML, 2024. [65] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024. [66] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024. [67] Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, and Gengchen Mai. Img2loc: Revisiting image geolocalization using multi-modality foundation models and image-based retrieval-augmented generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 27492754, 2024. [68] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479. [69] Sijie Zhu, Taojiannan Yang, and Chen Chen. Vigor: Cross-view image geo-localization beyond one-to-one In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, retrieval. pages 36403649, 2021. 13 [70] Sijie Zhu, Mubarak Shah, and Chen Chen. Transgeo: Transformer is all you need for cross-view image geolocalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11621171, 2022. [71] Yuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Qidong Liu, Yongchao Ye, Wei Chen, Zijian Zhang, Xuetao Wei, and Yuxuan Liang. Controltraj: Controllable trajectory generation with topology-constrained diffusion model. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, page 46764687. [72] Yuanshao Zhu et al. Difftraj: Generating gps trajectory with diffusion probabilistic model. In Proceedings of the 37th Annual Conference on Neural Information Processing Systems, 2023. [73] Yuanshao Zhu et al. Synmob: Creating high-fidelity synthetic gps trajectory dataset for urban mobility In Proceedings of the 37th Annual Conference on Neural Information Processing Systems, analysis. NeurIPS23 (Dataset and Benchmarks Track), 2023. [74] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv:1909.08593, 2019. 14 More Details of GRE30K A.1 Example Illustrations of GRE30K We provide several examples to illustrate the reasoning data in our GRE30K dataset, including the high-quality CoT data in Fig. 7 and the judgment data in Fig. 8. Figure 7: Three examples to show CoT data in GRE30K-CoT. Figure 8: Two examples to show Judgment data in GRE30K-Judge. Red option indicates the wrong reasoning steps. 15 A.2 Detailed prompt for GPT-o Please refer to Fig. 9 for more details. Figure 9: One example to illustrate the prompt for GPT-o3 to generate CoT data. The top block indicates the contexts including the image and instruction used to prompt o3, and the bottom block shows the response. A.3 Review and Refinement Pipeline for GRE30K-CoT Review and Refinement Pipeline for GRE30K-CoT. After the data generation process, we employ regular expression matching to filter out samples where the predicted coordinates deviate from the ground truth beyond threshold θ. Notably, these filtered samples are not discarded but rather incorporated into GRE30K-Judge. To ensure the high quality of the generated samples, we apply manual verification after automated filtering. The process is conducted by three trained annotators with geographic-relevant professional backgrounds. The annotators examine and correct hallucinated image descriptions and inconsistent geographical reasoning in the CoT, ensuring that o3s output adheres to <think> </think><answer> </answer> format. Additionally, they maintain alignment between the reasoning process and the instruction structure. Examples of Manual Filtering. As illustrated in Fig. 10, through combination of regular expression matching and manual filtering, we enhance the quality of o3 generated Chain-of-Thought outputs, ultimately constructing high-quality CoT dataset, GRE30K-CoT. Figure 10: An illustrative example of Chain-of-Thought refinement and format normalization. The red strikethrough text denotes hallucinated content where the instructor model (o3) generated descriptions that are not actually present in the image. More Details of GREval-Bench B.1 Detail of GREval-Bench Construction and Evaluation Pipeline For image filtering, we construct geographical reasoning corpus based on GRE30K-CoT, utilizing Named Entity Recognition (NER) to identify locations and architectural entities, and Semantic Role Labeling (SRL) to extract geographical reasoning patterns (e.g., spire style European church). The geographical indicators in the corpus are then categorized into explicit and implicit types. Explicit indicators encompass artificial landmarks, natural geographical features, and textual symbols, while implicit indicators include architectural styles, urban planning patterns, social characteristics, and environmental characteristics. Please refer to Appendix B.2 for detailed sub-categories. We employ CLIP [30] to compute similarity scores between images and geography-relevant textual prompts from our geographical corpus (e.g., \"base of Eiffel Tower\", \"Arabic text\", \"redwood forest\"), retaining samples with high relevance scores. Subsequently, images with single facial regions occupying more than 50% of the area are removed through face detection [32]. The rule-filtered images then undergo manual verification, where annotators answer the question: Can the approximate geographical location (country/city level) be inferred solely from this image? Images are excluded if two or more out of three annotators respond negatively. To facilitate CoT evaluation, we provide key steps annotation and reference GPS coordinate for all samples. Key steps are defined as the essential reasoning components required to reach the correct answer. We initially leverage o3 to generate the answer rationale. For the rationale, we provide both instructions and ground truth coordinates to o3, which yields more accurate rationales compared to instruction-only prompting. Subsequently, three geography domain annotators review and annotate key intermediate steps, utilizing o3s responses as reference. For cases where o3 fails to generate reasonable rationales, annotators develop solutions independently. B.2 Detailed Subcategories of Geographical Indicators In the image geolocation task, geolocation indicators refer to the visual elements in the image that can directly or indirectly infer the geographic location. Table 5 shows the classification and specific examples of geolocation clues. 17 Type Subcategory Scenario Table 5: Detailed subcategories of geographical indicators. Explicit nature landmark Global/National Landmarks: Eiffel Tower (Paris); Statue of Liberty (New York); Great Wall (Beijing) Regional Architecture: Neuschwanstein Castle (Bavaria, Germany); Kiyomizu-dera Temple (Kyoto, Japan); Prague Astronomical Clock (Czech Republic) Unique Structures: Bridges (Golden Gate Bridge); Ferris Wheel (London Eye); Religious Buildings (Mosque Domes, Gothic Church Spires) Explicit iconic buildings Global/National Landmarks: Eiffel Tower (Paris); Statue of Liberty (New York); Great Wall (Beijing) Regional Architecture: Neuschwanstein Castle (Bavaria, Germany); Kiyomizu-dera Temple (Kyoto, Japan); Prague Astronomical Clock (Czech Republic) Unique Structures: Bridges (Golden Gate Bridge); Ferris Wheel (London Eye); Religious Buildings (Mosque Domes, Gothic Church Spires) Explicit language Language signs: Language on road signs and store signs (Arabic Middle Eastern; Cyrillic Eastern European). Explicit symbolic Administrative signs: License plates (German license plates \"D\") Currency and flags: Euro coins (European countries); Canadian maple leaf flag Unique landforms: Uyuni Salt Flats (Bolivia); Grand Canyon (USA); Guilin Karst landforms Implicit geographical features Vegetation types: Cactus (desert areas); coconut trees (tropical coastal areas); birch trees (northern temperate zones) Water features: Victoria Falls (Africa); Dead Sea (high salinity water bodies) Architectural style: Spanish colonial style (Mexico); neoclassicism (Washington, DC); earthen building (Fujian) Implicit architectural style Street characteristics: Narrow cobblestone roads (European ancient towns); grid layout (Manhattan, New York); tricycles (Southeast Asian cities) Implicit social characteristics Clothing and customs: Kimono (Japan); Scottish plaid skirt; Indian sari Transportation: Tunisian carriage; Venetian gondola; London red bus Implicit climate Seasons and Weather: Aurora (high latitudes); monsoon rainforest (rainy season in Southeast Asia); sandstorms (deserts in the Middle East)"
        },
        {
            "title": "C More Experiments",
            "content": "C.1 More Details on Training Please refer to Fig. 11 and Fig. 12 for more details. C.2 Additional Main Results We also conduct evaluations on the Google StreetView dataset( Table 6), where we observe similar performance trends. Additionally, we demonstrate the performance of our base model, Qwen-2.5VL series, on Im2GPS3k and GWS15k datasets( Table 7). The results align with our conclusions from the main results, further validating the effectiveness of our proposed training strategy. C.3 Qualitative Results In the supplementary materials, we provide additional visual examples illustrating the reasoning performance on the image geo-localization task. These examples demonstrate GREs capability to generate remarkable chains of thought for accurate coordinate prediction in challenging scenarios. 18 Figure 11: RL stage training pipeline and Judgment Prompt. Figure 12: RL stage II training pipeline and Inference Prompt."
        },
        {
            "title": "D Limitations and Future Work",
            "content": "D.1 Limitations The primary limitations of GRE include (1) substantial computational resource requirements, specifically utilizing 8 NVIDIA H20 GPUs for model training, and (2) the associated API costs for dataset generation. D.2 Future Work Leveraging geo-localization reasoning capabilities, we can implement geographic information privacy identification and protection mechanisms. Furthermore, this approach can be extended through agentbased architectures that integrate reasoning capacities with tool invocation functionalities. D.3 Broader Impacts The reasoning capacity improvement in geo-localization facilitates the extraction of multi-granularity geographic indicators from imagery, offering dual benefits for geospatial data mining applications and location privacy preservation frameworks."
        },
        {
            "title": "E More Qualitative Results",
            "content": "We present additional visual examples to highlight the geographic reasoning performance. Fig. 13 displays more visual cases involving diverse locations. GRE is capable to generate explainable predictions with robust capabilities in these challenging scenarios. Furthermore, Fig. 14 and Fig. 15 provides comparisons with previous alignment-based methods and existing MLLMs with reasoning capabilities. Our approach exhibits superior image geo-localization results with implicit geographic indicators. 19 Table 6: Results on the Google StreetView dataset. Method Street City Region Country Continent 1 km 25 km 200 km 750 km 2500 km 46.92 Qwen2.5VL-3B 4.47 61.00 7.99 Qwen2.5VL-7B Qwen2.5VL-32B 14.62 67.50 15.53 64.25 13.59 63.75 18.15 71. CI CI + Ours 68.22 70.42 69.04 74.46 75.19 75.36 78.26 83.20 88.42 94.20 92.30 91.30 83.89 85.56 92.59 96.14 96.02 92.75 Table 7: We test the Qwen2.5VL series on (a) Im2GPS3k [9] and (b) GWS15k [6] datasets for reference here. (a) Results on the Im2GPS3k [9] dataset (b) Results on the recent GWS15k [6] dataset Method Street City Region Country Continent 1 km 25 km 200 km 750 km 2500 km 0.33 Qwen2.5VL-3B Qwen2.5VL-7B 3.20 Qwen2.5VL-32B 6.47 7.31 52.99 75.32 5.37 42.14 59.87 1.20 16.62 25. 3.57 28.03 40.96 Method Street City Region Country Continent 1 km 25 km 200 km 750 km 2500 km Qwen2.5VL-3B 0.02 Qwen2.5VL-7B 0.05 Qwen2.5VL-32B 0.06 0.17 0.29 0.36 0.41 1.39 7. 2.14 4.43 28.46 6.70 8.66 52.39 Figure 13: Visual examples of GRE. 20 Figure 14: Qualitative comparisons with previous alignment-based methods and existing MLLMs with reasoning capabilities. (Lat, Lon) denotes the ground truth coordinates, (Lat, Lon) denotes the models predicted answer, Indicator denotes the explicit indicator and Indicator denotes the implicit indicator. Notably, GeoCLIP generate five candidates coordinates and select the candidate with the maximum probability score as the answer. 21 Figure 15: Qualitative comparisons."
        }
    ],
    "affiliations": [
        "LibLib.ai",
        "NUS",
        "Sun Yat-sen University",
        "Zhejiang University"
    ]
}