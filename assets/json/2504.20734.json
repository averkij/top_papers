{
    "paper_title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities",
    "authors": [
        "Woongyeong Yeo",
        "Kangsan Kim",
        "Soyeong Jeong",
        "Jinheon Baek",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 4 3 7 0 2 . 4 0 5 2 : r UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities Woongyeong Yeo1 Kangsan Kim1 Soyeong Jeong1 Jinheon Baek1 Sung Ju Hwang1,2 KAIST1 DeepAuto.ai2 {wgcyeo, kksan07, starsuzi, jinheon.baek, sungju.hwang}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which single type of knowledge source cannot address. To address this, we introduce UniversalRAG, novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into unified representation space derived from single combined corpus causes modality gap, where the retrieval tends to favor items from the same modality as the query, we propose modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines. Our project page is at https://universalrag.github.io."
        },
        {
            "title": "Introduction",
            "content": "Recently, we have witnessed the remarkable performance of Large Language Models (LLMs) in various tasks, such as question answering (OpenAI et al., 2024; Anil et al., 2023), and their widespread adoption in various services, such as ChatGPT, to empower users in everyday life. Yet, LLMs often *Equal contribution 1 generate factually incorrect or misleading information, especially on topics they were less or not exposed to during training (e.g., recent events) (Zhang et al., 2023; Huang et al., 2025). To address this issue, Retrieval-Augmented Generation (RAG) has emerged as promising approach, which allows the model responses to be grounded in the queryrelevant knowledge retrieved from external knowledge sources, enhancing factual accuracy (Lewis et al., 2020; Gao et al., 2024; Chen et al., 2024a). However, despite its effectiveness, existing RAG approaches are typically designed for single corpus and modality, limiting their ability to address user queries that demand different types of knowledge sources. In practice, as illustrated in Figure 1, user queries vary widely in the type of knowledge that they require: some are best answered using text (e.g., surface-level facts and definitions), others demand visual understanding from images (e.g., spatial relations of objects), and yet others require temporal reasoning supported by videos (e.g., stepby-step instructions with dynamic scenes). On the contrary, the field of RAG primarily originates with focus on the textual corpus (Lewis et al., 2020; Jiang et al., 2023; Yan et al., 2024), and although recent efforts have expanded it to modalities beyond text (such as images and videos) (Abootorabi et al., 2025; Riedler and Langer, 2024; Jeong et al., 2025), existing RAG methods individually are typically modalityand corpus-specific; therefore, they may be suboptimal to serve as universal, one-for-all framework that can flexibly handle the wide range of queries, whose knowledge requirements vary. In this work, we present UniversalRAG, novel RAG framework that brings together knowledge distributed across multiple modality-specific corpora, including text, image, and video sources, and leverages them to generate grounded responses to queries in universal workflow. To operationalize this, the most straightforward approach might be to aggregate all entries from the collected, hetFigure 1: Illustration of (a, b) limitations of existing RAG methods and (c) the proposed RAG framework, UniversalRAG. Figure 2: t-SNE visualization of unified embedding space. Figure 3: Average scores of the baselines and UniversalRAG. erogeneous knowledge corpora, and embed them into unified representation space using multimodal encoder (which is typically trained to align inputs from different modalities if they are semantically similar). However, despite such alignment efforts, we find that this strategy suffers from modality gaps, which is the tendency to cluster inputs based on their modality rather than their semantic meaning (visualized in Figure 2), phenomenon observed similarly in prior work under different settings (Zhang et al., 2025; Wei et al., 2024). As result, retrieval becomes biased toward knowledge sources that share the same modality as the query, overlooking relevant content from other modalities. To address this challenge, instead of relying on unified embedding space that forces all modalities into the shared representation, we take different direction: introducing modality-aware routing strategy. Specifically, UniversalRAG dynamically determines the most suitable knowledge source to retrieve from, based on the modality requirement of the given query, then routes the retrieval process to the corresponding modality-specific corpus. It is worth noting that this strategy not only sidesteps modality gaps by avoiding direct cross-modal comparisons, but also enables seamless integration of new modalities by extending the routing logic without modifying existing modality-specific retrievers. Beyond modality, another important dimension is data granularity (the size or unit of each entry in the corpus), which plays critical role in both retrieval precision and generation quality (Chen et al., 2024b; Zhong et al., 2025), since different queries benefit from different levels of granularity even within the same modality. This is because overly fine-grained entries can dilute context, while overly coarse entries may bundle unrelated information. For example, complex analytical question may require long-form documents or full-length videos to capture sufficient context, while simple factoid question might be best served with single paragraph or short video clip. To accommodate this aspect, we further break down each modality into multiple granularity levels, organizing them into distinct corpora: textual documents are additionally segmented into paragraphs and stored in paragraph-level corpus, and similarly, full-length videos are divided into short clips and stored, while images are kept intact since they are inherently piecemeal. Overall, with these modalityand granularity-aware corpora (including paragraphs, documents, images, clips, and videos) in place, as well as an additional no-retrieval option to efficiently handle straightforward queries (that require no external knowledge), our UniversalRAG dynamically routes each query to the most relevant knowledge source, ultimately supporting the diverse information needs of real-world users. We validate UniversalRAG on 8 benchmarks with different modalities (Hendrycks et al., 2021; Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Yang et al., 2018; Chang et al., 2022; Wang et al., 2024a; Jeong et al., 2025). UniversalRAG outperforms all baselines in an average score, indicating robust performance across diverse queries. We also investigate the efficacy of multi-modal and multigranular corpora with experimental results."
        },
        {
            "title": "2 Method",
            "content": "In this section, we present UniversalRAG, novel RAG framework that retrieves knowledge from diverse corpora spanning multiple modalities and granularities, conditioned on the given query."
        },
        {
            "title": "2.1 Preliminaries",
            "content": "We begin with preliminaries, introducing LVLMs and RAG with formal descriptions. Large Vision Language Models In order to extend the powerful capabilities of Large Language Models (LLMs) beyond text and support the understanding of visual inputs such as images and videos, Large Vision-Language Models (LVLMs) have recently been introduced by incorporating visual encoders into LLMs, enabling them to process both textual and visual inputs such as images and videos. Formally, an LVLM takes as input sequence = [x1, x2, . . . , xn], which may include both text and visual tokens, and generates sequence of output tokens = [y1, y2, . . . , ym], denoted as: = LVLM(x). Nevertheless, despite their multimodal capacity, LVLMs are still limited to parametric knowledge and often struggle with queries requiring detailed or grounded information beyond what was encoded during pretraining. Retrieval-Augmented Generation To address the aforementioned limitations of parametric-only models, Retrieval-Augmented Generation (RAG) retrieves query-relevant information from large external corpus and incorporates it into the generation process. Specifically, in the retrieval step, Retriever selects the relevant context from corpus C, formalized as = Retriever(q; C), where C. In the subsequent generation step, an LVLM generates response conditioned on both the input query and the retrieved context, denoted as = LVLM(q, c). However, most existing RAG approaches are restricted to retrieving from single corpus within single modality (e.g., image-only), limiting their ability to handle diverse real-world queries that often require multimodal information. Modality Gap in Unified Retrieval Given that external knowledge in real-world scenarios often spans multiple modalitiessuch as text, images, and videoswe define three modality-specific corpora: the text corpus Ctext = {t1, . . . , tn}, the image corpus Cimage = {i1, . . . , im}, and the video corpus Cvideo = {v1, . . . , vk}. common approach to handling such heterogeneous data is to unify all items into shared embedding space using multimodal encoder, resulting in unified corpus Cunified = Ctext Cimage Cvideo, where each item is represented as vector in the shared space (Zhang et al., 2025; Wei et al., 2024), and retrieval is then performed as = Retriever(q; Cunified). However, our experiments reveal clear modality gap in such unified spacesas shown in Figure 2where queries, being inherently textual, tend to align more closely with text corpus items regardless of the actual modality required. As result, even when query demands visual or temporal understanding, the retriever returns text-based content, leading to suboptimal or irrelevant responses. This observation highlights fundamental limitation of unified retrieval strategies and motivates the need to maintain separate feature spaces for different modalities."
        },
        {
            "title": "2.2 UniversalRAG",
            "content": "We now turn to introduce UniversalRAG, novel framework that dynamically identifies and routes queries to the most appropriate modality and granularity of knowledge for retrieval. Modality-Aware Retrieval To address the modality gap in retrieval, we maintain separate embedding spaces for each modality, organizing the overall corpus into three distinct sub-corpora: Ctext, Cimage, and Cvideo, where each consists of modality-specific vector representations. We then introduce routing module, Router, which dynamically selects the most appropriate modality for each query. Specifically, given query q, Router predicts the query-relevant modality {Text, Image, Video}, formalized as = Router(q). Once the modality is determined, modalityspecific Retriever selects the relevant item from the corresponding corpus Cr, and the LVLM generates the final response based on the query and the retrieved content. However, while this design mitigates the modality gap, separating corpora solely by modality may still fall short, as different queries can require varying levels of granularityeven within the same modality. Granularity-Aware Retrieval To flexibly accommodate the varying information needs of different queries, we extend UniversalRAG to operate across multiple levels of granularity within each modality, constructing two corpus levelsfinegrained and coarse-grainedfor both the text and video modalities. Specifically, while the text corpus is initially organized at the paragraph level, where each item typically contains knowledge about single entity, some complex queries require reasoning across multiple paragraphs. To address this, we construct document-level corpus Cdocument = {d1, . . . , dl}, where each is the 3 vector representation of document obtained by concatenating multiple paragraphs and encoding the resulting text. On the other hand, the original video corpus consists of full-length videos, which can often exceed an hour in duration, making it inefficient to retrieve an entire video when certain questions can be answered with only short clip. Therefore, we segment each full-length video into multiple fixed-duration clips, constructing cliplevel corpus Cclip = {k1, . . . , kp}, where each denotes the representation of trimmed video clip extracted from the original full-length videos. Note that since images are inherently fine-grained, we do not perform additional segmentation for the image corpus and maintain it as is. To this end, the routing decision made by Router falls into one of six categories: {None, Paragraph, Document, Image, Clip, Video}, and the retrieval process is formalized as follows: = None Retriever(q; Ctext) Retriever(q; Cdocument) Retriever(q; Cimage) Retriever(q; Cclip) Retriever(q; Cvideo) if = None if = Paragraph if = Document if = Image if = Clip if = Video Finally, the LVLM generates the final response conditioned on the retrieved content c, which reflects the most suitable modality and granularity determined for the given query q. Furthermore, if no retrieval is required (i.e., = None), the LVLM directly generates the response based solely on without any additional context."
        },
        {
            "title": "2.3 Router Design within UniversalRAG",
            "content": "Here, we explore two designs for the router, which is responsible for dynamically selecting the retrieval modality and granularity based on the query. Training-free Router The training-free router utilizes the inherent knowledge and reasoning abilities of pretrained LLM to classify queries into appropriate retrieval types without requiring additional training. Specifically, given query q, the LLM is prompted with detailed instruction describing the routing task, accompanied by several in-context examples, and predicts the most suitable retrieval type from set of six predefined options. Trained Router We further explore training the router module to enable more accurate routing decisions. However, one key challenge in this strategy 4 is the absence of ground-truth query-label pairs for optimal corpus selection. To address this, we construct training dataset for the router by leveraging the modality-specific inductive biases of existing benchmarksthat is, we assume that each benchmark is primarily associated with particular modality and retrieval granularity. Specifically, for text QA benchmarks, queries from datasets intended to be answered solely based on the models parametric knowledge are labeled as None, queries from single-hop RAG benchmarks as Paragraph, and those from multi-hop RAG benchmarks as Document. Similarly, queries from image-based RAG benchmarks are labeled as Image. For video QA benchmarks, queries that focus on localized events or specific moments within videosuch as identifying an action at particular timestampare labeled as Clip, while those requiring comprehension of the full storyline or broader temporal context are labeled as Video. Using this constructed dataset, we train the router to predict the appropriate retrieval type for given query at inference time."
        },
        {
            "title": "3 Experimental Setup",
            "content": "In this section, we explain the experimental setup, including datasets, models, evaluation metrics, and implementation details."
        },
        {
            "title": "3.1 Datasets",
            "content": "To evaluate the performance of our framework across diverse modalities, we compile comprehensive QA benchmark covering six distinct retrieval settings: no-retrieval, paragraph, document, image, clip, and video. QA Datasets For the no-retrieval setting, we utilize MMLU (Hendrycks et al., 2021), which evaluates models knowledge without requiring external sources. For the text retrieval settings, we incorporate three benchmarks: SQuAD (Rajpurkar et al., 2016) and Natural Questions (NQ) (Kwiatkowski et al., 2019) serve as single-hop RAG benchmarks, where the retrieval units are paragraphs, while HotpotQA (Yang et al., 2018) serves as multi-hop RAG benchmark, where the retrieval units are documents. For the image retrieval setting, we use subset of WebQA (Chang et al., 2022), consisting of queries that require grounding in external images. Finally, for the video retrieval setting, we use queries from LVBench (Wang et al., 2024a), VideoRAG-Wiki (Jeong et al., 2025), and Table 1: Results of diverse RAG variants, including UniversalRAG and baselines, on modality-specific benchmarks. Our methodology, UniversalRAG, represented by the colored cells, includes trained approaches for DistilBERT and T5-Large, while GPT-4o operates in train-free manner. Bold indicates the best performance for each metric; underline indicates the second-best among UniversalRAG approaches. R-L and BERT refer to ROUGE-L and BERTScore, respectively. MMLU SQuAD NQ HotpotQA Text Image WebQA LVBench VideoRAG-Wiki VideoRAG-Synth Avg. Video Models Naïve Paragraph Document Image Clip Video Unified Random GPT-4o DistilBERT T5-Large Oracle Naïve Paragraph Document Image Clip Video Unified Random GPT-4o DistilBERT T5-Large Oracle Naïve Paragraph Document Image Clip Video Unified Random GPT-4o DistilBERT T5-Large Oracle 8 - 5 . 2 - r I 7 - - 5 . 2 Q r I - s - 5 . 3 - Acc 64.50 64.50 51.50 54.50 53.50 59.50 59.00 55.50 61.50 62.00 63.00 64.50 73.00 72.00 66.50 68.50 68.50 70.00 71.50 72.00 71.50 73.50 72.50 73.00 61.00 58.50 52.50 55.50 54.00 53.00 55.00 55.50 57.50 57.00 58.50 61. EM F1 EM F1 EM R-L BERT 7.82 20.62 6.33 7.41 4.58 3.77 4.72 7.68 18.33 19.14 20.49 20.62 10.78 23.58 8.76 11.19 10.65 11.05 7.95 12.67 21.70 21.83 23.58 23.58 9.30 22.24 6.47 8.36 7.68 8.09 6.47 9.57 20.35 20.62 22.37 22.24 16.86 30.97 13.72 15.74 12.52 11.55 12.81 16.20 28.09 29.71 30.87 30. 19.85 34.25 15.13 18.30 17.66 18.07 15.06 19.49 30.62 32.52 34.12 34.25 18.32 33.38 12.95 15.20 13.38 14.05 14.63 16.67 30.18 31.90 33.36 33.38 24.71 35.14 23.57 23.57 13.86 14.43 17.00 22.71 33.43 33.57 35.00 35.14 17.29 38.43 23.14 16.14 15.14 14.00 12.29 20.86 36.57 37.57 38.29 38.43 10.43 34.86 16.43 9.86 11.43 9.29 5.86 14.00 32.86 33.71 34.71 34.86 38.11 47.89 32.66 32.96 21.82 22.98 27.87 32.79 46.28 46.45 47.78 47. 25.71 49.37 31.02 23.14 22.69 21.42 19.81 29.23 48.11 48.27 49.22 49.37 18.49 46.07 24.80 15.73 16.48 15.09 13.48 21.72 44.19 44.87 45.94 46.07 12.92 14.45 19.71 13.11 9.38 9.95 9.67 12.82 17.80 19.43 18.09 19.71 18.47 19.04 20.96 16.94 16.46 17.42 14.35 18.47 20.19 20.96 19.52 20.96 14.26 17.03 17.80 13.68 13.40 13.11 11.87 15.12 16.84 18.18 17.61 17.80 20.87 23.05 28.49 20.18 16.51 16.95 17.08 20.37 26.10 28.35 26.90 28. 25.47 26.54 28.78 23.01 22.86 23.74 21.11 25.09 28.00 28.87 27.32 28.78 21.01 24.82 25.86 18.70 18.73 17.91 18.46 21.15 25.09 26.30 25.98 25.86 40.63 35.72 28.92 46.50 39.53 40.08 41.71 38.37 45.39 46.40 45.47 46.50 61.26 53.42 54.37 64.39 62.78 63.89 55.64 59.67 63.58 64.20 63.53 64.39 54.01 59.90 57.46 63.25 60.22 59.90 51.05 58.84 62.88 63.39 62.69 63.25 90.30 89.13 87.45 91.32 90.27 90.51 90.27 89.71 91.10 91.29 91.09 91. 94.39 92.65 92.71 94.73 94.38 94.54 93.07 93.85 94.58 94.70 94.55 94.73 93.01 93.65 93.18 94.13 93.60 93.50 92.67 93.48 94.11 94.14 94.04 94.13 Acc 28.60 29.19 28.80 31.64 35.36 33.59 27.23 31.15 33.01 35.16 34.28 35.65 29.38 27.13 27.23 30.17 33.50 32.81 30.14 28.80 32.42 33.01 33.01 33.20 29.58 28.21 29.09 31.15 35.06 32.13 28.50 29.77 32.62 34.87 34.97 34. R-L BERT R-L BERT 15.74 14.82 13.28 17.26 18.76 19.23 15.87 16.55 14.65 19.23 19.18 18.79 14.26 14.88 14.78 16.17 18.39 19.34 15.00 15.96 14.87 19.34 19.34 18. 15.94 17.31 14.05 15.16 19.50 19.33 18.09 16.94 16.79 19.33 19.33 19.53 84.20 84.08 83.75 83.79 86.38 86.35 83.96 84.79 84.11 86.35 86.32 86.38 83.04 83.30 83.33 83.62 85.04 85.64 82.74 83.91 83.29 85.64 85.62 85.05 83.64 85.02 84.18 85.02 86.04 86.14 84.76 85.02 84.95 86.14 86.10 86.04 14.93 19.15 18.51 20.72 27.37 28.23 19.03 21.02 19.68 28.15 27.71 27.45 10.52 12.62 11.39 13.35 20.53 23.31 11.38 15.63 12.69 23.18 22.85 20. 34.58 32.11 33.27 34.18 36.34 36.71 35.78 33.88 32.01 36.48 36.31 36.20 85.73 86.53 86.12 87.02 89.34 89.45 86.46 87.37 86.83 89.44 89.33 89.35 84.34 84.93 84.50 85.10 87.75 88.52 84.16 86.01 85.01 88.46 88.38 87.80 90.66 89.94 90.18 90.32 90.97 90.95 90.82 90.31 89.93 90.91 90.87 90.97 33.76 35.59 30.24 33.43 31.55 32.47 31.15 32.27 37.56 39.60 39.36 40.34 38.48 39.94 36.39 37.97 38.81 39.36 35.87 38.50 42.61 44.34 44.01 44. 35.16 39.61 34.95 35.24 35.62 34.56 32.47 35.28 40.48 41.78 42.08 42.46 VideoRAG-Synth (Jeong et al., 2025). Among them, queries targeting short or localized segments are categorized as clip-level queries, whereas those requiring an understanding of the long or entire video are treated as video-level queries. Retrieval Corpus To support retrieval across modalities and granularities, we construct retrieval corpus specific to each modality and granularity. For paragraph-level retrieval, we use Wikipedia paragraph corpus derived from SQuAD and Natural Questions (Karpukhin et al., 2020). In the case of document-level retrieval, we follow the construction method from LongRAG (Jiang et al., 2024) to build corpus of aggregated Wikipedia articles. Regarding image retrieval, we use retrieval corpus consisting of images from the WebQA dataset. For video-related retrieval, we define two separate corpora: the video retrieval corpus consists of full-length YouTube videos from LVBench and VideoRAG, whereas the clip-level retrieval corpus comprises trimmed segments extracted from the same videos. Further details on dataset construction are provided in Appendix A."
        },
        {
            "title": "3.2 Models",
            "content": "We compare UniversalRAG against eight different baselines as follows: 1) Naïve answers queries without retrieving external knowledge. 2) Paragraph, 3) Document, 4) Image, 5) Clip, and 6) Video retrieve information only from their respective modality-specific corpora. 7) Unified retrieves the information over single unified embedding space of multimodal encoder, InternVideo2 (Wang et al., 2024b), for all data in different corpora, similar to (Zhang et al., 2025; Wei et al., 2024). 8) Random randomly selects one modality-specific corpus for retrieval. We also implement three variants of UniversalRAG, varying in their retriever components. 9) GPT-4o adopts GPT-4o (OpenAI et al., 2024) as training-free router. 10) DistilBERT and 11) T5-Large use DistilBERT (Sanh et al., 2019) and T5-Large (Raffel et al., 2020), respectively, trained on the routing dataset. 12) Oracle is our ideal setting, in which each query is routed to the most appropriate modality-specific corpus, simulating perfect routing."
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "We evaluate the performance of UniversalRAG and the baselines with the following metrics. For benchmarks with multiple choice questions, we use Top1 Accuracy (Acc), which shows how many questions get correct answers. For benchmarks whose answers are shorter than few words, we use Exact Match (EM), which checks whether the predicted response exactly matches the ground truth, 5 Table 2: Router accuracy and generation performance across retrieval methods on inand out-of-domain dataset. Table 3: Effect of granularity on the performance of three models across two benchmarks. Gn denotes Granularity. In-Domain Out-Domain"
        },
        {
            "title": "Router Acc Avg Score Router Acc Avg Score",
            "content": "Random Unified GPT-4o DistilBERT T5-Large Ensemble 16.67 16.67 57.23 66.42 59.99 63.99 32.27 31.15 37.56 39.60 39.36 39.43 16.67 16.67 69.49 39.62 47.47 61.55 29.99 28.92 36.85 32.58 35.27 35.22 Figure 4: Confusion matrices of router predictions using different models on inand out-of-domain queries. and F1 Score (F1), which measures the word-level overlap between the response and the reference answer. Lastly, for benchmarks whose answers are longer than sentence, we use ROUGE-L, which captures the longest matching sequences between predicted and ground truth answers (Lin, 2004), and BERTScore, which measures the semantic similarity between response and annotation using contextual embeddings (Zhang et al., 2020). 3."
        },
        {
            "title": "Implementation Details",
            "content": "To effectively retrieve information from different modalities, we leverage modality-specific encoders: bge-large-en-v1.5 (Xiao et al., 2024) as the text encoder, and InternVideo2 (Wang et al., 2024b) as the vision encoder. For response generation, we use variety of LVLMs, including InternVL2.5-8B (Chen et al., 2025), Qwen2.5-VL7B-Instruct (Bai et al., 2025), and Phi-3.5-VisionInstruct (Abdin et al., 2024). For the router module, the trainable routers are trained for 5 epochs with learning rate of 2e-5, selecting the best checkpoint based on validation accuracy. In the training-free setting, GPT-4o (OpenAI et al., 2024) is instantiated through prompt as shown in Figure 6. Further details are provided in Appendix B."
        },
        {
            "title": "4 Experimental Results and Analyses",
            "content": "We now present our results and in-depth analyses."
        },
        {
            "title": "4.1 Main Results",
            "content": "Here, we present the overall results across diverse retrieval scenarios spanning multiple modalities"
        },
        {
            "title": "Models",
            "content": "Gn EM F1 GPT-4o"
        },
        {
            "title": "DistilBERT",
            "content": "T5-Large 14.26 17.80 14.55 19.43 14.35 18.09 22.95 26.10 23.08 28.35 23.03 26."
        },
        {
            "title": "Acc",
            "content": "32.32 33.01 33.20 35.16 33.20 34.28 and levels of granularity. Overall Results First of all, Figure 3 illustrates the average scores of UniversalRAG and baseline models across eight multimodal benchmarks, and detailed breakdown of the results is provided in Table 1. UniversalRAG consistently outperforms all baselines in terms of average score, demonstrating the effectiveness of leveraging multiple modalities through adaptive corpus selection. In contrast to single-modality corpora that provide limited information, UniversalRAG dynamically selects the most relevant modality for each query, enabling more accurate retrieval and generation. Interestingly, UniversalRAG significantly outperforms the Unified baseline, highlighting the effectiveness of our routing strategy in realistic, multi-modal setting. Specifically, the Unified baseline struggles due to modality gap in its unified embedding space, often defaulting to retrieving only textual data and consequently suffering in performance. UniversalRAG mitigates this issue by using router to select single modality-specific corpus for retrieval, effectively addressing the modality gap. Given the inherent challenge of constructing unified embedding space across modalities without modality gap, our router-based strategy offers promising direction to tackle this issue. Effectiveness of Router Among the UniversalRAG models, trained router models achieve better results compared to the training-free router model across all experiments with different LVLMs. This improvement is due to the trained routers being explicitly optimized for the routing task during training, leading to superior routing performance. As result, UniversalRAG models with trained routers are better at identifying the most optimal data source and generating more accurate answers. Nevertheless, the training-free router still outperforms other baseline methods, including the random router, demonstrating that zero-shot routing remains effective within our framework. Table 4: Router accuracy with varying router model size."
        },
        {
            "title": "Models",
            "content": "T5-Small T5-Base T5-Large T5-XL # params"
        },
        {
            "title": "Router Acc",
            "content": "60M 220M 770M 3B 51.16 63.65 59.99 67.50 To further understand the impact of routing on overall system performance, we analyze the accuracy of each router model and the corresponding overall score. Figure 4 illustrates the confusion matrices for zero-shot and trained router models. While both routers generally succeed in directing inputs to the appropriate modality, the trained router demonstrates superior accuracy compared to the training-free model. Notably, for the clip and video modalities, there are few misrouted queries, primarily due to ambiguity in separating two different granularities. Nevertheless, the inputs are still correctly routed to the video modality, highlighting the robustness of the routing mechanism. As seen in Table 2, our routing methods significantly outperform both random and unified baselines in terms of routing accuracy. This improvement in accuracy directly translates to better overall performance, demonstrating strong correlation between accurate routing and end-to-end effectiveness. These results underscore the importance of correctly routing queries to the appropriate modality corpus, demonstrating the necessity of reliable router for multimodal RAG scenario. Effectiveness of Multigranularity To further investigate the effectiveness of incorporating multiple levels of granularity, we evaluate UniversalRAG under both coarseand fine-grained retrieval settings. In the no-granularity (coarse) setting, router classifies queries into four broad modalities: none, text, image, or video. In the granular (finegrained) setting, we further subdivide modalities for more precise retrieval: text is split into paragraph and document levels, while video is divided into clip and full video. For benchmarking, we use HotpotQA to evaluate document-level reasoning across multiple entities, and LVBench for clip-level tasks, as its questions are typically answerable using short video segments. As shown in Table 3, UniversalRAG with granularity consistently outperforms the model without granularity on both benchmarks across all router models. This highlights that supporting different levels of granularity in text and video corpora improves the performance of UniversalRAG by enabling the model to retrieve an appropriate amount of information tailored to each query. In contrast, models without granularity control apply the same level of granularity to all queries, which may result in either insufficient or excessive information retrieval. Therefore, supporting multiple levels of granularity is crucial for adaptively handling wide range of user queries."
        },
        {
            "title": "4.2 Analyses and Discussion",
            "content": "Here, we provide detailed analysis of the performance improvements. Results on Out-of-Domain Datasets To investigate the generalizability of our approach, we evaluate UniversalRAG on five unseen datasets, with detailed descriptions of each benchmark provided in Appendix A.2. As shown in Table 2, GPT-4o achieves the highest routing accuracy, even surpassing its in-domain performancedemonstrating strong generalization capabilities. However, trained routers underperform on out-of-domain data, demonstrating routers are overfitted to the training data, mainly due to the insufficient diversity of queries in training data. Figure 4 further highlights the performance trade-off between in-domain and out-domain datasets. Benefiting from its robust routing, GPT-4o also achieves the highest average QA score, outperforming both trained routers and baseline models. As solution to the performance trade-off between two settings, we introduce an ensemble router using both trained and train-free routers. Specifically, routing result from the trained router is selected if its confidence score is high enough; otherwise, response from the train-free router is leveraged. This strategy enables exploiting the trained router for queries that have characteristics similar to in-domain dataset, while relying on generalized routing ability of the train-free router for unfamiliar or out-of-domain queries. As shown in Table 2, UniversalRAG with the ensemble router demonstrates better performance in both the inand out-of-domain benchmarks. Analysis on Router Size To assess the impact of router size on routing accuracy, we evaluate UniversalRAG with trained routers of varying model sizes. Specifically, we train four variants of the T5 model with different parameter counts and measure router accuracy using InternVL2.5 as the generator. As shown in Table 4, router accuracy varies substantially with model size, indicating that larger models are more effective at making accurate routing decisions across modalities and granularities. 7 Retrieval-Augmented Generation RetrievalAugmented Generation (RAG) can address the aforementioned challenges by incorporating external knowledge when generating answers; however, conventional RAG approaches rely solely on text data, while recent studies have begun to explore RAG over diverse multimodal corpora, highlighting its significant potential beyond text-only settings. Specifically, image-based RAG (Chen et al., 2022; Riedler and Langer, 2024) was the first attempt at multimodal RAG, which retrieves and uses visual information to answer queries. Furthermore, Jeong et al. (2025) recently extends RAG to video, capturing both visual and temporal elements for process-related questions. Despite these advances, most existing methods only consider single modality corpus, which is impractical given that real-world queries could require information from any modality. Therefore, it is crucial to leverage all available data to generate the best possible answer, rather than restricting the model to limited modality. More recent approaches (Cui et al., 2024; Liu et al., 2025a) support retrieval from multimodal corpora, but typically retrieve from all available modalities and decide what to use only after retrievalor even after generationwhich is inefficient and fails to adapt retrieval to the specific needs of the query. Handling diverse queries requires an RAG approach that adapts to the specific context and query, instead of using single fixed method. One promising approach is to route queries according to their predefined complexity levels (Jeong et al., 2024; Tang et al., 2025; Islam et al., 2024), categorizing them as requiring no retrieval, single-step retrieval, or multi-step retrieval, to balance performance and latency. Another strategy leverages model confidence (Ding et al., 2024; Yao et al., 2024), retrieving external information only when the model confidence is low, therefore efficiently allocating resources to challenging queries. Although adaptive retrieval has become central to RAG, existing benchmarks (Zhang et al., 2024; Li et al., 2024c) primarily evaluate text-only systems, leaving open the question of how to adapt retrieval across multiple modalities. In real-world scenarios, queries benefit from different data types, making it essential to identify the most suitable modality for retrieval in mixed-modality corpus. Retrieval Granularity The size of indexing corpus, retrieval granularity, is key design choice Figure 5: Generation performance with varying generation model (InternVL2.5) size. Analysis with Different Model Sizes To see how the performance of UniversalRAG scales with LVLM size, we evaluate our models and baselines with different sizes of InternVL2.5 models, as reported in Figure 5. Across all model sizes, UniversalRAG scores consistently increase and outperform other baselines. This indicates the scalability of UniversalRAG and implies that its performance could be enhanced by employing larger LVLMs. Case Study We present the case studies of UniversalRAG in Appendix D."
        },
        {
            "title": "5 Related Work",
            "content": "Large Vision Language Models Building on the powerful performance of Large Language Models (LLMs), researchers have made efforts to enable LLMs to understand visual information. Liu et al. (2023) pioneered Large Vision Language Models (LVLMs) by employing CLIP-based (Radford et al., 2021) image encoder that allows the language model to understand the input image within its textual feature space. Following this, various image understanding language models have been introduced, each using different vision encoders over LLMs (Bai et al., 2023; Chen et al., 2024c; Liu et al., 2024). As image understanding performance has become robust, several studies have extended these methods to video data, which can be viewed as sequence of image frames (Li et al., 2024a; Chen et al., 2025; Bai et al., 2025). Thanks to larger training datasets and improved model structures, current LVLMs show strong image and video understanding abilities, as demonstrated by multiple benchmark evaluations (Yue et al., 2024; Mathew et al., 2021; Li et al., 2024b; Fu et al., 2024). However, standalone LVLMs often suffer from hallucination mainly due to the limited knowledge boundary inherited from their base language models. 8 in retrieval, as it significantly impacts both the performance and efficiency of RAG. Chen et al. (2024b) discovered that retrieval from corpus indexed in propositions outperforms sentenceor passage-level retrieval performance. Recent studies (Liu et al., 2025b; Zhong et al., 2025) also showed that considering multiple granularities achieves better retrieval performance. Likewise, granularity-aware text-to-video retrieval was studied to find not just full video but specific clip related to the query from video corpus (Chen et al., 2023). Therefore, in multimodal corpora, it is not sufficient to select the appropriate modality alone; the system should also identify the optimal level of granularity for retrieval."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose UniversalRAG, novel RAG framework designed to retrieve from corpora with diverse modalities and granularities. Through modalityand granularity-aware routing mechanism, UniversalRAG dynamically selects the most suitable knowledge source for each query, effectively addressing the limitations posed by modality gaps and fixed-granularity retrieval. Extensive evaluations across 8 benchmarks demonstrate that UniversalRAG consistently outperforms both modalityspecific and unified baselines, showcasing robust performance across diverse modalities. Furthermore, our analyses highlight the importance of finegrained retrieval and the complementary strengths of train-free and trained routers. These findings demonstrate the potential of UniversalRAG as an adaptive solution for grounding LVLMs with heterogeneous external knowledge, opening new directions for more reliable multimodal reasoning and modality-aware information integration."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. 2025. Ask in any modality: comprehensive survey on multimodal retrieval-augmented generation. Preprint, arXiv:2502.08826. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, and 33 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. Preprint, arXiv:2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. Preprint, arXiv:2502.13923. Valeriia Bolotova-Baranova, Vladislav Blinov, Sofya Filippova, Falk Scholer, and Mark Sanderson. 2023. WikiHowQA: comprehensive benchmark for multidocument non-factoid question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 52915314, Toronto, Canada. Association for Computational Linguistics. Yingshan Chang, Guihong Cao, Mridu Narang, Jianfeng Gao, Hisami Suzuki, and Yonatan Bisk. 2022. Webqa: Multihop and multimodal QA. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1647416483. IEEE. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024a. Benchmarking large language models in In Thirty-Eighth retrieval-augmented generation. AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1775417762. AAAI Press. Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. 2024b. Dense retrieval: What retrieval granularity should we use? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1515915177. Association for Computational Linguistics. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. 2022. Murag: Multimodal retrieval-augmented generator for open question anIn Proceedings of swering over images and text. the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 55585570. Association for Computational Linguistics. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, and 23 others. 2025. Expanding performance boundaries of opensource multimodal models with model, data, and test-time scaling. Preprint, arXiv:2412.05271. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, and 1 others. 2024c. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198. Zhiguo Chen, Xun Jiang, Xing Xu, Zuo Cao, Yijun Mo, and Heng Tao Shen. 2023. Joint searching and grounding: Multi-granularity video content retrieval. In Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 20233 November 2023, pages 975983. ACM. Wanqing Cui, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. MORE: Multi-mOdal REtrieval augmented generative commonsense reasoning. In Findings of the Association for Computational Linguistics: ACL 2024, pages 11781192, Bangkok, Thailand. Association for Computational Linguistics. Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024. Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models. Preprint, arXiv:2402.10612. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. 2024. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. Preprint, arXiv:2405.21075. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: survey. Preprint, arXiv:2312.10997. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Trans. Inf. Syst., 43(2). Shayekh Bin Islam, Md Asib Rahman, K. S. M. Tozammel Hossain, Enamul Hoque, Shafiq Joty, and Md. Rizwan Parvez. 2024. Open-rag: Enhanced retrieval augmented reasoning with open-source large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 14231 14244. Association for Computational Linguistics. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 70367050. Association for Computational Linguistics. Soyeong Jeong, Kangsan Kim, Jinheon Baek, and Sung Ju Hwang. 2025. Videorag: Retrievalaugmented generation over video corpus. Preprint, arXiv:2501.05874. Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 79697992. Association for Computational Linguistics. Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024. Longrag: Enhancing retrieval-augmented generation with long-context llms. Preprint, arXiv:2406.15319. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada. Association for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, 10 Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeIn Advances in Neural Inforintensive nlp tasks. mation Processing Systems, volume 33, pages 9459 9474. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava-onevision: Easy visual task transfer. Preprint, arXiv:2408.03326. Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, and Minhao Cheng. 2025. LaRA: Benchmarking retrieval-augmented generation and long-context llms no silver bullet for lc or rag routing. Preprint, arXiv:2502.09977. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. 2024b. Mvbench: comprehensive multi-modal video understanding benchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2219522206. IEEE. Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Pengjun Xie, Philip S. Yu, Fei Huang, and Jingren Zhou. 2024c. Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. Preprint, arXiv:2411.02937. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland. Association for Computational Linguistics. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2628626296. IEEE. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Pei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan Meng, Ding Wang, and Jun Ma. 2025a. Hm-rag: Hierarchical multi-agent multimodal retrieval augmented generation. Preprint, arXiv:2504.12330. Zuhong Liu, Charles-Elie Simon, and Fabien Caspani. 2025b. Passage segmentation of documents for extractive question answering. Preprint, arXiv:2501.09940. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2021. Docvqa: dataset for VQA on document images. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021, pages 21992208. IEEE. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, and 401 others. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Ruchit Rawal, Khalid Saifullah, Miquel Farré, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. 2024. CinePile: long video question answering dataset and benchmark. Preprint, arXiv:2405.08813. 11 Monica Riedler and Stefan Langer. 2024. Beyond text: Optimizing rag with multimodal inputs for industrial applications. Preprint, arXiv:2410.21943. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, distilled version of BERT: smaller, faster, cheaper and lighter. In NeurIPS 2019 EMC2 Workshop. Xiaqiang Tang, Qiang Gao, Jian Li, Nan Du, Qi Li, and Sihong Xie. 2025. MBA-RAG: bandit approach for adaptive retrieval-augmented generation through question complexity. In Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pages 32483254. Association for Computational Linguistics. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. 2024a. Lvbench: An extreme long video understanding benchmark. Preprint, arXiv:2406.08035. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Jilan Xu, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. 2024b. Internvideo2: Scaling foundation models for multimodal video understanding. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part LXXXV, volume 15143 of Lecture Notes in Computer Science, pages 396416. Springer. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024. Uniir: Training and benchmarking universal multimodal information retrievers. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXXXVII, volume 15145 of Lecture Notes in Computer Science, pages 387404. Springer. Yin Wu, Quanyu Long, Jing Li, Jianfei Yu, and Wenya Wang. 2025. Visual-rag: Benchmarking text-to-image retrieval augmented generation for visual knowledge intensive queries. Preprint, arXiv:2502.16636. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, page 641649, New York, NY, USA. Association for Computing Machinery. Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. Preprint, arXiv:2401.15884. 12 Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium. Association for Computational Linguistics. Zijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, and Juanzi Li. 2024. Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation. Preprint, arXiv:2406.19215. Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, and 3 others. 2024. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 9556 9567. IEEE. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. 2025. GME: Improving universal multimodal retrieval by multimodal llms. Preprint, arXiv:2412.16855. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Sirens song in the ai ocean: survey on hallucination in large language models. Preprint, arXiv:2309.01219. Zihan Zhang, Meng Fang, and Ling Chen. 2024. Retrievalqa: Assessing adaptive retrieval-augmented generation for short-form open-domain question answering. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 6963 6975. Association for Computational Linguistics. Zijie Zhong, Hanwen Liu, Xiaoya Cui, Xiaofan Zhang, and Zengchang Qin. 2025. Mix-of-granularity: Optimize the chunking granularity for retrievalaugmented generation. In Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pages 57565774. Association for Computational Linguistics."
        },
        {
            "title": "A Additional Details on Dataset",
            "content": "Table 5 provides an overview of all datasets and their corresponding data corpora used in our experiments, including the target modality type as well as the size of the queries and corpora. We divide each dataset into 3:7 ratio for training and testing. detailed explanation of each dataset is provided below. A.1 In-Domain Dataset MMLU As dataset comprising queries that can be answered without the need for retrieval, we use MMLU (Hendrycks et al., 2021), benchmark that spans wide range of tasks, including problem-solving abilities (e.g., elementary mathematics, computer science) and world knowledge (e.g., law, world religions). Specifically, we use questions from all tasks in the development split. SQuAD SQuAD v1.1 (Rajpurkar et al., 2016) is benchmark dataset consisting of questions generated by crowdworkers based on set of Wikipedia articles. Each question is answerable given the appropriate context paragraph. From the datasets 100,000+ QA pairs, we randomly sample 1,060 pairs of dev split. For context retrieval, we utilize the full provided Wikipedia corpus, segmenting each article into paragraphs of at most 100 words. Natural Questions (NQ) We also use Natural Questions (Kwiatkowski et al., 2019), question answering dataset consisting of real user queries issued to the Google search engine, with answers annotated based on supporting Wikipedia articles. We randomly sample 1,000 QA pairs of dev split, and formulate the text corpus in same setting as SQuAD, segmenting the Wikipedia corpus into paragraphs of at most 100 words. HotpotQA HotpotQA (Yang et al., 2018) is Wikipedia-based QA benchmark, but contains complex queries that are annotated to reason over multiple articles. We utilize 1,492 randomly sampled QA pairs of test split. As it requires multi-hop reasoning over multiple documents, we formulate the text corpus by grouping multiple related documents following LongRAG (Jiang et al., 2024), that can be longer than 4K tokens. WebQA WebQA (Chang et al., 2022) is benchmark designed to evaluate the ability of LLMs to reason over multiple sources of information, including both text and images, in an open-domain setting. As the dataset is originally constructed with question-specific retrieval sources that combine text and images, we extract subset of questions that require only single image for retrieval. We then further filter these using GPT-4o with the prompt shown in Figure 7 to make sure questions are not grounded to certain image, resulting in final set of 2,000 QA pairs. LVBench LVBench (Wang et al., 2024a) is benchmark developed for long video understanding, featuring questions generated by annotators based on YouTube videos with an average duration of over one hour. Since the benchmark was originally designed for non-RAG tasks, we rephrased the original text-video interleaved queries into text-only format to align with our experimental setup using GPT-4o, with video metadata and prompt (Figure 8). Each query is associated with specific video and corresponding time range. Notably, the majority of queries are annotated with timestamps spanning less than five minutes, thereby focusing on short segments within the longer videos. For training, we use these shorttimestamp queries as clip-level dataset. VideoRAG We also utilize VideoRAG-Wiki and VideoRAG-Synth benchmarks, introduced in VideoRAG (Jeong et al., 2025), which are designed to evaluate RAG over video corpus. These benchmarks are built on the HowTo100M (Miech et al., 2019) corpusa large-scale collection of instructional YouTube videoswith queries sourced from WikiHowQA (Bolotova-Baranova et al., 2023) and synthetically generated QA pairs based on the videos. Since they lack timestamp annotations, we employ GPT-4o to identify video-level queries that are better answered through full video retrieval rather than short segments from the ground-truth video, which are then used as video-level dataset for training the router. A.2 Out-of-Domain Dataset Unlike the in-domain datasets, the out-of-domain datasets are used solely for evaluation to assess the generalizability of our routing approach, and consist only of test splits. TruthfulQA TruthfulQA (Lin et al., 2022) includes general knowledge questions designed to test whether LLMs can avoid common false belief or misconception, on diverse categories, including health, law, and politics. We use the multiple13 Table 5: Dataset Summary. Avg. corpus length is the mean token count for text corpora and the mean duration for video corpora."
        },
        {
            "title": "Gold Retrieval",
            "content": "# Queries"
        },
        {
            "title": "Corpus Size",
            "content": "Avg. Corpus Length MMLU SQuAD Natural Questions HotpotQA WebQA LVBench VideoRAG-Wiki VideoRAG-Synth TruthfulQA TriviaQA LaRA Visual-RAG CinePile None Paragraph Paragraph Document Image Clip/Video Clip/Video Clip/Video In-Domain Datasets 285 1,060 1,000 1,492 2,000 1,376 374 374 - 1.19M 850k 509k 20k 9k None Paragraph Document Image Clip/Video Out-of-Domain Datasets 790 661 112 374 1,440 - 661k 34 2k 144 - 100 tokens 100 tokens 693 tokens - 3,941s 378s - 100 tokens 28k tokens - 158s choice version of the dataset, which includes only single correct answer per question. TriviaQA TriviaQA (Joshi et al., 2017) is reading comprehension dataset consisting of trivia questions paired with evidence texts sourced from Wikipedia and the web. To distinguish between queries that require text retrieval and those that do not, we categorize each query based on whether GPT-4o can produce an exact-match answer without access to external text. We randomly sample QA pairs from the dev split. Following the preprocessing strategies used in SQuAD and NQ, all supporting evidence documents are segmented into paragraphs of no more than 100 words. LaRA We also utilize LaRA (Li et al., 2025), which is designed for understanding long-context documents such as academic papers and novels. For our use case, we focus on subset of these documents, specifically excluding queries on the comparison task, as our goal is RAG, not reading comprehension. Additionally, we slightly reformat the remaining queries to align with general QA format. Given the length of the source material, each document is treated as single entry in the document-level corpus. Visual-RAG Visual-RAG (Wu et al., 2025) is question-answering benchmark designed for visual knowledge-intensive questions, specifically tailored for text-to-image retrieval tasks. We utilize the full set of provided queries but sample five images per category to construct the image retrieval pool, ensuring efficient text-to-image retrieval. CinePile CinePile (Rawal et al., 2024) is longvideo question-answering benchmark that features questions based on movie clips from YouTube. Since the benchmark was originally designed for video understanding tasks rather than RAG, we reformulate each query using the same procedure as LVBench. For each of the 144 available videos, we randomly select 10 questions from the test split. Since CinePile does not provide granularity annotations, we classify the questions into two categoriesclip-level and full-video-level granularityusing GPT-4o, following the same approach used in VideoRAG."
        },
        {
            "title": "B Additional Implementation Details",
            "content": "To effectively leverage both visual and textual information for visual element retrieval, we employ an ensemble approach that combines visual and textual similarity scores with weighting ratio of 0.8 for visual information. The textual information consists of image captions for images and scripts for videos. During the generation stage, we use only the top-1 retrieved result, selected based on the cosine similarity of the corresponding embeddings. Moreover, we uniformly sample 32 frames per video for both the retrieval and generation stages. Trainable routers are trained for 5 epochs with learning rate of 2e-5, with the best-performing state selected based on validation accuracy."
        },
        {
            "title": "C Additional Experimental Results",
            "content": "C.1 Routing Results per Dataset We present routing results of three routers for each dataset in Table 6. On in-domain datasets, GPT-4o often struggles to distinguish between Paragraph and Document RAG queries, and misroutes VideoRAG queries to textual corpus. Meanwhile, two 14 Table 6: Routing results across in-domain and out-domain dataset. Text Image Video Text Image Video MMLU SQuAD NQ HotpotQA WebQA LVBench VRAG-Wiki VRAG-Synth TruthfulQA TriviaQA LaRA Vis-RAG CinePile In-domain Dataset Out-domain Dataset 200 117 39 44 0 0 0 120 49 11 0 5 15 110 79 2 0 0 742 44 512 185 1 0 0 2 679 32 6 0 23 4 731 3 1 0 3 700 57 588 39 5 6 1 669 12 11 1 6 1 698 0 0 1 0 1045 38 505 502 0 0 0 1 150 866 17 8 3 0 461 571 9 2 1392 25 102 44 1210 0 11 0 30 12 1351 5 2 1 145 6 1234 13 1 829 3 17 34 44 622 0 1 3 7 818 0 0 13 15 15 784 2 374 8 304 52 1 0 9 0 0 0 0 0 374 1 5 0 0 0 374 27 271 53 9 0 14 0 6 0 0 2 366 1 15 0 0 3 355 790 374 205 206 2 3 2 629 53 12 3 91 21 709 35 1 2 22 661 121 509 27 4 0 0 1 274 338 32 7 9 4 558 94 2 1 112 0 4 108 0 0 0 42 21 2 4 34 9 30 74 0 0 1 7 374 0 18 0 356 0 0 0 2 371 0 1 0 0 0 374 0 0 1440 0 0 6 1 1354 79 0 1 1 1 1436 1 0 13 2 4 1420 Models None Paragraph Document Image Clip Video 4 - T None Paragraph Document Image Clip Video s e L - 5 None Paragraph Document Image Clip Video RAG, on queries from the WebQA dataset. Traditional approaches such as TextRAG and VideoRAG fail to generate accurate answersTextRAG retrieves passages lacking relevant visual details, while VideoRAG is better suited for temporal reasoning tasks. In contrast, UniversalRAG correctly routes the query to the image modality, recognizing that visual information about color is necessary, and successfully generates the correct response. This highlights the advantage of modality-aware routing in leveraging the appropriate data from the correct modality corpus, demonstrating UniversalRAGs ability to adaptively select the most informative modality for accurate answer generation. In addition to modality routing, we observe that UniversalRAG also benefits from retrieving information at the appropriate granularity. Table 9 shows results from HotpotQA, where the query requires complex reasoning over multiple text sources. While paragraph-level granularity fails to provide sufficient context for reasoning, UniversalRAG routes the query to the documentlevel corpus, to retrieve all the textual information necessary for accurate reasoning. Similarly, for video queries, Table 10 shows results from LVBench, on the query that requires only short segment of the full long video to answer. While full-video-level retrieval includes irrelevant content and uniformly sampled 32 frames fail to capture the necessary information, clip-level retrieval focuses on smaller, more relevant segments of the video to ensure that only the most pertinent visual details are considered, leading to more accurate answer. Table 7: Detailed results of UniversalRAG and baselines on out-domain dataset. Text Image Video TruthfulQA TriviaQA LaRA Vis-RAG Cinepile Avg. Models Naïve Paragraph Document Image Clip Video Unified Random GPT-4o DistilBERT T5-Large Oracle Acc 64.68 58.73 28.73 57.85 51.01 47.34 52.15 51.27 55.19 57.85 57.85 64.68 EM R-L BERT R-L BERT Acc 49.47 54.61 39.94 45.23 31.62 33.59 35.70 42. 54.01 42.51 50.08 57.92 65.14 44.73 52.50 42.40 43.82 45.01 51.51 64.05 51.01 60.16 23.15 20.23 25.18 21.40 19.64 19.89 21.28 21.81 24.93 20.96 20.63 87.62 86.48 86.83 87.09 87.50 87.19 86.83 87. 88.42 87.26 86.73 55.52 64.85 25.18 86.83 6.24 4.74 4.34 7.31 6.92 70.4 4.31 5. 7.17 7.34 7.31 7.31 80.98 80.77 81.14 82.32 81.32 81.42 80.47 81.09 82.26 82.32 82.32 82.32 30.76 30.07 32.64 34.03 35.63 37.43 30.76 32. 35.56 35.63 35.49 37.71 33.88 33.88 26.68 33.35 29.59 29.47 28.92 29.99 36.85 32.58 35.27 38.26 trained routers show strong classification perforIn out-ofmance across all in-domain datasets. domain datasets, GPT-4o generalizes well for most datasets, except for image-based RAG queries. In contrast, trained routers fail to classify the appropriate granularity needed for each query. This is mainly due to the limited diversity of training data, which causes overfitting to seen examples. C.2 Detailed Results on Out-domain Dataset QA evaluation results of UniversalRAG models and the baselines for each out-of-domain dataset are shown in Table 7. UniversalRAG models outperform the baselines in general. GPT-4o demonstrates robust performance across all datasets, primarily due to the outstanding generalization capability of the router on unseen queries, as discussed in Section C.1. However, trained routers show degraded performance compared to the results on in-domain datasets, since their routers often misclassify unseen queries."
        },
        {
            "title": "D Qualitative Results",
            "content": "We present case studies to demonstrate the effectiveness of UniversalRAG. Table 8 compares the results of various RAG approaches, including traditional single-modality methods and Universal15 Comparison of UniversalRAG to Single-modality RAG"
        },
        {
            "title": "TextRAG",
            "content": "What color balloons were displayed on the USNS Carl Brashear during its ship launching ceremony in San Diego? The balloons were red, white, and blue that were displayed on the USNS Carl Brashear during the ship launching ceremony in San Diego. The USNS Carl Brashears ship launching ceremony in San Diego likely featured diverse range of balloons, but the specific color or colors of the balloons during the ceremony are not universally documented. They may not have been focus of the event, and such details can vary or not be available in the public record. Therefore, the exact answer to the colors of the balloons on the USNS Carl Brashear may not be specified or may be unknown. Retrieved: Randall Dark Randall P. Dark is Canadian writer, director, and producer recognized as an early proponent of HDTV. In 1995, Dark and his company, HD Vision, were involved with the United Nations 50th Anniversary where they shot, in HDTV, President Bill Clintons speech in San Francisco. In 1999, Dark and HD Vision produced documentary for PBS about the Albuquerque Balloon Festival. This program aired in HDTV in June 1999. Much of Darks work involves sweeping nature cinematography including Trinity River Raft Ride featuring Eddie Bracken. One of the 3D projects Dark directed in 2010 was \"3 Cities in Response: The provided documents do not contain any information about the color of the balloons displayed on the USNS Carl Brashear during its ship launching ceremony in San Diego."
        },
        {
            "title": "VideoRAG",
            "content": "Retrieved: Response: Green, blue, and yellow. UniversalRAG (Ours) Routed to: Image Retrieved: Response: The USNS Carl Brashear was adorned with red, white, and blue balloons during its launch ceremony in San Diego. Table 8: Case study comparing RAG on single modality vs. UniversalRAG (Ours). Traditional RAG models are limited by modality-specific retrieval, often missing relevant knowledge from other modalities. UniversalRAG (Ours) dynamically routes each query to the most appropriate modality, enabling more accurate retrieval and response generation."
        },
        {
            "title": "Question",
            "content": "George Reed was, along with which Drake University player, one of the greatest running backs in CFL history?"
        },
        {
            "title": "Ground Truth",
            "content": "Johnny D. Bright"
        },
        {
            "title": "ParagraphRAG",
            "content": "Retrieved: Blue Bombers. He remained in Winnipeg for two seasons (1986-87), but only played in nine games. He finished his career with 4,586 rushing yards, 275 receptions for 2,861 yards and 48 total touchdowns. His son Corey was sprinter on Clemsons mens track & field team. Lester Brown (Canadian football) Lester Brown (born January 5, 1957) is former Canadian Football League running back for the Saskatchewan Roughriders, Montreal Concordes, Toronto Argonauts, Ottawa Rough Riders and Winnipeg Blue Bombers. He played college football at Clemson University. Brown attended Myrtle Beach High School, before accepting football scholarship from Clemson University. UniversalRAG (Ours) Routed to: Document Response: Lester Brown Retrieved: George Reed (Canadian football) George Robert Reed, CM, SOM (born October 2, 1939), is former American college football and Canadian Football League player. Reed, along with Mike Pringle and Johnny Bright, is one of the players most often mentioned as being the greatest running back in CFL history. In November 2006, Reed was voted one of the CFLs Top 50 players (#2) of the leagues modern era by Canadian sports network. ... Johnny Bright Johnny D. Bright (June 11, 1930 December 14, 1983) was professional Canadian football player in the Canadian Football League. He played college football at Drake University. He is member of the Canadian Football Hall of Fame, the National Football Foundations College Football Hall of Fame, the Missouri Valley Conference Hall of Fame, the Edmonton Eskimos Wall of Honour, the Alberta Sports Hall of Fame, and the \"Des Moines Registers\" Iowa Sports Hall of Fame. Response: Johnny Bright Table 9: Case study comparing different levels of text granularity. The user query requires complex retrieval involving multiple entities. ParagraphRAG retrieves limited context centered around single entity, leading to an incorrect answer. UniversalRAG (Ours) routes the query to the document corpus and retrieves richer document-level information, allowing it to capture both relevant entities and generate the correct response."
        },
        {
            "title": "Question",
            "content": "Who finishes first in the Mens 100M Round 1 Heat 5 during the London 2012 Olympics, featuring Usain Bolt and Yohan Blake? (A) Su BingTian (B) Usain Bolt (C) Asafa Powell (D) Tyson Gay"
        },
        {
            "title": "VideoRAG",
            "content": "C Retrieved: (Timestamp Range: 00:0038:26) Response: UniversalRAG (Ours) Routed to: Clip Retrieved: (Timestamp Range: 25:5729:22) Response: Table 10: Case study comparing different levels of video granularity. The user query requires only segment of the video to determine the answer. VideoRAG retrieves broad range of frames across the video, which includes irrelevant content and leads to an incorrect answer. UniversalRAG (Ours) routes the query to the clip-level granularity, retrieving more focused and relevant visual information, enabling it to generate the correct response. 18 Classify the following query into one of six categories: [No, Paragraph, Document, Image, Clip, Video], based on whether it requires retrieval-augmented generation (RAG) and the most appropriate modality. Consider: No: The query can be answered directly with common knowledge, reasoning, or computation without external data. Paragraph: The query requires retrieving factual descriptions, straightforward explanations, or concise summaries from single source. Document: The query requires multi-hop reasoning, combining information from multiple sources or documents to form complete answer. Image: The query focuses on visual aspects like appearances, structures, or spatial relationships. Clip: The query targets short, specific moment or event within video, without needing full context. Video: The query requires understanding dynamic events, motion, or sequences over time in video. Examples: \"What is the capital of France?\" No \"What is the birth date of Alan Turing?\" Paragraph \"Which academic discipline do computer scientist Alan Turing and mathematician John von Neumann have in common?\" Document \"Describe the appearance of blue whale.\" Image \"Describe the moment Messi scored his goal in the 2022 World Cup final.\" Clip \"Explain how Messi scored his goal in the 2022 World Cup final.\" Video \"Solve 12 8.\" No \"Who played key role in the development of the iPhone?\" Paragraph \"Which Harvard University graduate played key role in the development of the iPhone?\""
        },
        {
            "title": "Document",
            "content": "\"Describe the structure of the Eiffel Tower.\" Image \"Describe the moment Darth Vader reveals he is Lukes father in Star Wars.\" Clip \"Analyze the sequence of events leading to the fall of the Empire in Star Wars.\" Video Classify the following query: {query} Provide only the category. Figure 6: Prompt for query routing in train-free manner 19 Evaluate whether the query can be answered using general knowledge about the images subject rather than relying solely on details unique to the provided image, and verify that the answer is obtainable from the image and the query. Respond \"yes\" if: 1. The query can be fully answered using general knowledge about the subject. 2. The answer can be derived solely from the image and the query, without needing imagespecific details. Respond \"no\" if either condition is not met. Example 1: Image: portrait of Donald Trump Query: What is the color of Trumps hair? Answer: White Response: \"yes\" Example 2: Image: close-up photo of light bulb Query: What is the color of the light bulb in this image? Answer: Yellow Response: \"no\" Figure 7: Prompt to filter queries for WebQA You will receive query from video QA dataset and the title of the corresponding video on YouTube. want you to paraphrase the query by replacing \"in the video?\", \"of the video\", or similar phrases with references to the video content naturally. The output should sound as if human is asking ChatGPT, and should not explicitly mention the exact name of the video or even parts of the title. However, the rephrased query should contain enough implicit information about the video to allow the model to identify it. Try to reduce the chance of the model getting confused between multiple possible video candidates. If there could be multiple video matches for given query, try to include more information in the rephrased query. Example 1: Query: What year appears in the opening caption of the video? Video Title: Blue Eye Samurai Hammerscale Full Episode Netflix Upload Date: 2023-11-05 Channel Name: Netflix Rephrased Output: What year appears in the opening caption of the Blue Eye Samurai episode on Netflix? Example 2: Query: After the vlogger sees dog with an advertisement from the company named Smitten, camera changes to the scene with ___. Video Title: My ICELAND Experience Ultimate Travel Vlog Upload Date: 2022-10-26 Channel Name: Kallmekris Rephrased Output: After spotting dog with Smitten advertisement, what scene does the camera transition to in Kallmekriss Iceland travel vlog from 2022? Figure 8: Prompt to rephrase queries using video metadata for LVBench and CinePile"
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST"
    ]
}