{
    "paper_title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
    "authors": [
        "Minki Kang",
        "Jongwon Jeong",
        "Seanie Lee",
        "Jaewoong Cho",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation."
        },
        {
            "title": "Start",
            "content": "Minki Kang1,2 Jongwon Jeong2 Seanie Lee1 Jaewoong Cho2 Sung Ju Hwang1,3 1KAIST, 2KRAFTON, 3DeepAuto.ai {minkikang, sjhwang82}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with nexttier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation. 5 2 0 2 3 2 ] . [ 1 2 1 6 7 1 . 5 0 5 2 : r Figure 1: Performance comparison of different sizes of Qwen2.5-Instruct models [1] on the average accuracy of four factual reasoning tasks (HotpotQA [2], Bamboogle [3], MuSiQue [4], 2WikiMultiHopQA [5]) and four mathematical reasoning tasks (MATH [6], GSM-Hard [7], AIME [8], OlymMATH [9]). Distillation is done using the 32B model as the teacher and models ranging from 0.5B to 7B as students. Agent distillation consistently improves the performance of smaller models across both domains by enabling them to perform code execution and retrieve information for tasks adaptively. Full results are provided in Table 2. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable performance across complex real-world tasks, surpassing average human accuracy on college-level mathematics and demonstrating competence in high-stakes domains [1012]. However, as LLM usage grows, their high inference cost becomes increasingly burdensome. While these considerations have motivated growing interest in smaller language models (sLMs) [13, 14], preserving the problem-solving capabilities of larger models in sLMs remains challenging. Therefore, core research question emerges: how can we preserve LLM-level problem-solving ability in much smaller models? Although recent advancements in preand post-training methods have steadily increased the capabilities of sLMs [15], sLMs still struggle to solve complex tasks at the level of LLMs. To address this, recent works have explored reasoning distillation, where sLMs are trained to mimic CoT reasoning traces generated by teacher LLMs through next-token prediction [10, 11, 1, 16, 17]. However, distilled small models are prone to hallucination and often fail to perform accurate calculations [18]. For example, answering the real-world question, What would $100 invested in Apple stock in 2010 be worth by 2020?, requires both factual knowledge about stock history and arithmetic reasoning. As illustrated in Figure 2, LLMs can correctly answer this question using CoT by leveraging memorized knowledge and numerical skills. However, simply distilling such reasoning trace into an sLM does not guarantee generalization, especially those involving new knowledge or calculation not observed during distillation due to their limited capability [19]. In this work, we propose Agent Distillation, framework that moves beyond static reasoning to distill the ability to take action with tools, from LLM agents (e.g., ReAct [20], CodeAct [21]) into sLMs through reason-act-observe trajectories. Our goal is to equip sLMs with agentic capabilities: reasoning through problems, taking actions to use code and retrieval tools, observing outcomes, and refining their approachcloning the behavior of LLM agents. This approach offers two key advantages: (1) sLMs focus on learning how to reason and act to solve problems using tools, rather than memorizing knowledge and calculations, and (2) they generalize better to new queries requiring previously unseen facts or calculations. remaining challenge is whether such complex agentic behavior can be distilled from large teacher model (>30B) into much smaller student (0.53B) [1]. To this end, we introduce two simple but effective methods to aid effective distillation. First, we propose first-thought prefix method that aligns agentic reasoning with the teacher models instructiontuned behavior, improving trajectory quality of teacher agent without additional fine-tuning. These improved trajectories offer better supervision for sLM distillation. Second, we improve student robustness at test-time through self-consistent action generation, which samples multiple trajectories and selects the one yielding valid and consistent outcome leveraging code interpreter. We evaluate our agent distillation on four factual (e.g., HotPotQA [2]) and four mathematical (e.g., MATH [6]) reasoning benchmarks. For each reasoning type, we consider one in-domain task and three out-of-domain tasks to test generalization. As in Figure 1, our results show that agent distillation consistently enhances the problem-solving capabilities of small models of 0.5B to 7B. To summarize, our work makes the following key contributions: We propose Agent Distillation, framework for training sLMs to imitate trajectories from LLM agents, enabling agentic behavior without memorizing factual knowledge and calculations. We introduce two methods to overcome limitations of naive distillation: (1) first-thought prefix for improving teacher trajectory, and (2) self-consistent action generation to boost test-time robustness. We validate our method across 8 factual and mathematical reasoning benchmarks, showing strong performance across domains and student model scales (0.5B-7B) compared to CoT distillation. Remarkably, we demonstrate that even 0.5B, 1.5B, and 3B models distilled with our method can achieve comparable performance to next-tier larger models distilled with CoT on average."
        },
        {
            "title": "2 Related works",
            "content": "2.1 Reasoning distillation of language models Large language models (LLMs) have shown strong performance on complex reasoning tasks using methods like chain-of-thought (CoT) prompting [22, 23]. To transfer these capabilities to smaller mod2 Figure 2: Concept. Chain-of-Thought (CoT) distillation trains student models to mimic static reasoning traces from LLMs, but often fails when new knowledge or precise computation is needed at test time. Our proposed agent distillation instead teaches student models to think and acte.g., retrieve facts or execute codeoffering stronger generalization and better robustness to hallucination. els (sLMs), CoT distillation methods [16, 17, 2427] train sLMs to reproduce step-by-step reasoning traces from stronger LLMs. This has proven effectiveparticularly in mathematical reasoningand is now common component of post-training pipelines [11, 1]. To improve generalization, recent methods incorporate external tools such as retrieval [19, 28, 29] or code execution [3032], helping sLMs focus on transferable reasoning strategies rather than memorization of others. Still, most existing approaches rely on static demonstrations and lack interaction with the environment. In contrast, we distill agentic behaviors where models learn the reasoning and tool use during interactions with environments. This enables sLMs to learn how to act for solving tasks. 2.2 Langauge agents and agentic reasoning An agent can be broadly defined as an entity that autonomously pursues goals by observing the world and acting upon it. Powered by LLMs, early works like ReAct [20, 33] introduced the concept of language agentswhich observe the world, think in natural language, and act to complete the diverse range of tasks interactively. Since most LLMs are not natively trained for such interaction, prior works have relied on carefully designed prompts (e.g., few-shot examples) for stronger LLMs, and fine-tuned weaker LLMs on trajectories from stronger ones [20, 3342]. Building on foundations, recent works have pushed language agents toward more advanced agentic capabilities. In particular, agentic retrieval systems have emerged to support multi-hop reasoning over realworld knowledge [4345], while tool-augmented reasoning leverages external capabilities like code execution to tackle challenging math problems [4650]. Other approaches promote the notion of agentic reasoning, enhancing the decision-making and planning capabilities of LLMs for solving complex tasks with tools through prompting or reinforcement learning [5153]. Unlike prior work, which primarily only focused on fine-tuning LLMs ( 7B) on trajectories from stronger close-sourced LLMs (e.g., GPT-4 [12] in FireAct [34]), our work aims to distill the agentic capabilities of LLMs into much smaller models (sLMs, 3B), enabling them to operate as capable agents. We address key challenges such as improving the quality of teacher trajectories and optimizing student behavior at test time, building on improved agent framework [21]. We show its effectiveness across range of small models (e.g., 0.5B-3B) and tasks requiring strong knowledge and reasoning capabilitesan under-explored yet important setting for practical, small language agents."
        },
        {
            "title": "3 Preliminary",
            "content": "Knowledge Distillation. Knowledge distillation [54] transfers the capabilities of large teacher model pT to smaller student model pS. Modern language models follow the auto-regressive transformer architecture [55], where token-level policy predicts the next token given previous tokens. Given source and target sequences (x, y), distillation optimizes the following objective: min θ E(x,y)Dtrain"
        },
        {
            "title": "1\nLy",
            "content": "Ly (cid:88) n=1 D(pT ( y<n, x) pS( y<n, x; θ)), (1) where is divergence metric (e.g., KullbackLeibler or JensenShannon divergence), and Ly denotes the length of the target sequence y. In reasoning tasks, the target sequence can be rationale that solves Reasoning distillation. the problem step-by-step. Since collecting human-annotated reasoning is expensive, recent approaches [16, 24, 25, 27] use chain-of-thought (CoT) prompting [23] to generate rationales with large teacher models and train the student to imitate them: min θ ExDtrain,ypT (x,ICoT) Ly (cid:88) n=1 log pS(yn x, y<n; θ), (2) where ICoT denotes CoT-style prompt such as Lets think step by step. [23]."
        },
        {
            "title": "4 Agent Distillation",
            "content": "While reasoning distillation is effective and has become standard post-training technique [11, 1], it does not equip models with the ability to interact with external environments through actions. Recent work [20, 21] shows that large models can generate actions grounded in intermediate reasoning, observe feedback from the environment, and adapt accordingly. We refer to such interactive sequences as agent trajectories, consisting of repeated cycles of thought (r), action (a), and observation (o). Given an input x, the teacher model generates trajectory: τ = ((r1, a1, o1), . . . , (rLτ , aLτ , oLτ )) pT ( x, Iagent), (3) where Iagent is an instruction prompt for the agent (e.g., To solve the task, you must plan forward to proceed in series of steps, in cycle of Thought:, Code:, and Observation: sequences [21, 56]). Each observation comes from the environment in response to action a, not generated by the model. Following prior works [34, 47], we fine-tune the student model on generated trajectories, excluding observations from the loss: min θ ExDtrain,τ πT (x,Iagent) Lτ(cid:88) t=1 log pS(rt, at x, τ<t; θ), (4) where τ<t = ((r1, a1, o1), . . . , (rt1, at1, ot1)). This distillation enables student models to function as interactive agents. For instance, model distilled from CodeAct [21] can reason about which code snippet to generate, generate actions as codes (e.g., API calls, loops), and respond to execution feedback. If the interpreter returns an error, the model can revise the code accordingly; if the output is valid but insufficient (e.g., suboptimal search results), it can rephrase the query and continue the task adaptively. Despite its promise, agent distillation presents two key challenges, particularly when applied to small language models (sLMs). First, agentic behavior often lies out-of-distribution relative to the pretraining and instruction-tuning distribution of both teacher and student models. As result, distilling such behavior may degrade performance on domains where the student is already well-optimized for CoT-style reasoning. Second, although sLMs are pretrained on large code corpora [57], they may struggle to produce functional code during inference. Typical failure cases include misformatted code outputs or incorrect usage of library functions, which hinder the ability of agents to interact. 4 Figure 3: (a) First-thought Prefix: We prompt teacher with CoT prompt to induce step-by-step reasoning. The first reasoning step is used as prefix to generate an agentic trajectory, which is then distilled to student agent to teach CoT-style reasoning initialization. (b) Self-consistent Action Generation: The agent generates multiple candidate actions and selects the one with consistent outcomes. Thoughts are omitted for brevity. First-thought prefix. We observe that instruction-tuned LLMs (e.g., Qwen2.5-32B-Instruct [1]), when employed as agents, demonstrate reduced performance on challenging problems from MATH500 benchmarks compared to their performance with CoT prompting [23] (see Appendix D.1 for experimental results). This degradation can further propagate during distillation, negatively impacting student models where they have also been instruction-tuned on CoT-style data. To better understand this observation, we analyze the initial reasoning behavior (the first thought) produced by the LLM, as prior works show that the first reasoning step is important [5860]. Our analysis reveals substantial differences depending on whether the model is prompted to reason stepby-step (CoT) or think to generate action as an agent. CoT responses typically start with structured, reflective planning statements such as To solve this problem, we need to identify... explicitly outlining intermediate reasoning steps. In contrast, thoughts of agents frequently begin with direct or declarative statements, such as will calculate the sum..., prioritizing immediate execution. Based on this observation, we propose the first-thought prefix (ftp). Motivated by the prefix-attack in LLM jail-breaking works [6163], this method integrates the initial reasoning step from CoT as prefix to the agents first thought as in Figure 3(a). Formally, we modify the trajectory sampling described in Equation 3 as follows: y1 pT ( x, ICoT), τ = {(r 1, a1, o1), . . . , (rLτ , aLτ , oLτ )} pT ( x, y1, Iagent), (5) where y1 is the first-step of CoT reasoning and 1 denotes the completed first thought of the agent following the prefixed first-step y1. Note that this method is only used to generate trajectories from the teacher agent; the student agent does not explicitly require first-thought prefix during inference. Self-consistent action generation. We observe that small distilled agents often produce invalid actions, particularly in the context of CodeAct [21], where invalid actions refer to code that either fails to execute or throws errors. To improve robustness in action generation, we introduce self-consistent action generation (sag). Instead of using greedy decoding, we sample multiple thought-action sequences for each step through nucleus sampling [64] with high temperature to encourage diversity. We then filter out any sequences that result in parsing or execution errors using lightweight code interpreter. To further ensure correctness, we perform majority voting over the resulting observations [65], selecting the action whose output is most consistent across samples. For example, in Figure 3(b), the agent generates four candidate sequences. One result in an interpreter error is filtered out. Among the remaining three, two produce the same outputso we select one of these two consistent actions as final action."
        },
        {
            "title": "5 Experimental setup",
            "content": "We evaluate our proposed Agent Distillation across benchmarks to test whether small language models (sLMs) can acquire agentic abilities from large language model (LLM) agent teacher. Tasks and datasets. We evaluate two categories of reasoning tasks: factual and mathematical. For each, we assess both in-domain and out-of-domain generalization. We use 1000 HotPotQA [2] and 2000 MATH [6] examples for training. For test benchmarks, we summarize them in Table 1. To 5 Table 1: Task categorization with domain and sampled test data size we used. Task Type Domain Dataset Name Description Test Data Size Factual Reasoning Math Reasoning In-domain HotPotQA [2] Out-of-domain Bamboogle [3] Out-of-domain MuSiQue [4] Out-of-domain 2WikiMultiHopQA [5] 2-hop question-answering 2-hop question-answering 3-hop question-answering 2-hop question-answering MATH [6] In-domain Out-of-domain GSM-Hard [7] Out-of-domain AIME [8] Out-of-domain OlymMath [9] College-level math Large number arithmetics Olympiad-level problems Olympiad-level problems 500 125 500 500 500 500 90 reduce evaluation cost, we limit each test set to 500 examples, following Wang et al. [66]. As metric, we use exact match for math and llm-as-a-judge [67] using gpt-4o-mini for factual reasoning. Models. The teacher model is Qwen2.5-32B-Instruct, 32B parameter instruction-tuned model. For student models, we use the Qwen2.5-Instruct series with four sizes: 0.5B, 1.5B, 3B, and 7B parameters. All student models are instruction-tuned prior to distillation [1]. Baselines. We compare two main distillation paradigms: (1) CoT distillation [16], which transfers static reasoning traces generated using Chain-of-Thought prompting, and (2) our proposed Agent Distillation, which transfers interactive reason-act-observe trajectories. For CoT distillation, we add the baseline that uses retrieval-augmented generation [68] in both distillation and inference for fair comparison with external knowledge [19, 28, 29]. For ours, we adopt the formulation from CodeAct [21, 56], where each step consists of Thought, Action (e.g., Python code), and Observation. Additionally, we incorporate two proposed methods distillation using trajectories through first-thought prefix ftp and self-consistent action generation sag. Training & inference details. For reproducibility of experiments, we use Wikipedia 2018 as knowledge base for both agents and RAG instead of search engine. We use e5-base-v2 [69] as both document and query embeddings as in Jin et al. [44]. For both CoT and agent, we sample one trajectory per question from the teacher model and filter out wrong trajectories. We fine-tune student models using parameter-efficient tuning with LoRA (rank 64) [70]. All models are fine-tuned for 2 epochs using batch size of 8 and learning rate of 2 104. All experiments are conducted using four NVIDIA A100 80GB GPUs. For inference, we use greedy decoding. For all agents, we set max steps to 5. For sag in main experiments, we set = 8 with temperature to 0.4. More details in Appendix C."
        },
        {
            "title": "6 Results",
            "content": "Overall results. In Table 2, we find that agent distillation consistently improves performance across all model sizes. Before distillation, most sizes of models except 7B fail to produce effective agentic outputs via prompting alone, often generating incorrect or unparseable code action. In contrast, our distilled agents outperform CoT-distilled counterparts, particularly on out-of-domain tasks across both factual and mathematical domains. These results highlight the effectiveness of agent distillation in improving generalization of sLMs. Notably, the gains are further amplified by our two methodsFirst-thought Prefix (ftp) and Self-consistent Action Generation (sag). Our findings also demonstrate that agent distillation enables small models to match or exceed the performance of CoT-distilled models that are 24 larger, offering promising path toward efficient and capable language agents. Specifically, the 0.5B agent matches the performance of 1.5B CoT-distilled model, the 1.5B agent reaches its 3B counterpart, the 3B agent surpasses the 7B CoT model, and the 7B agent even outperforms the 32B CoT model. Factual reasoning results. We find that retrieval improves the performance of CoT-distilled models on factual reasoning benchmarks. However, due to its static nature, it can degrade performance on tasks requiring dynamic or adaptive information use, such as mathematical reasoning. In contrast, our distilled agents outperform even RAG-enhanced CoT models. This is because agent distillation 6 Table 2: Main results. Distilled agents show the strong performance on most of tasks, especially on out-ofdomain tasks, compared to baselines. ftp = First-Thought Prefix, sag = Self-consistent Action Generation. Highlighting best among same-sized models. Avg. denotes the average score across all tasks. In-domain Out-of-domain Params Method Hotpot QA MATH 500 MuSiQue Bamboogle 2Wiki QA GSMHard AIME OlymMATH Avg. 32B CoT Prompting Agent Prompting 7B 3B 1.5B 0.5B CoT Agent CoT Agent CoT Agent CoT Agent Prompting Distill Distill + RAG Prompting Distill + ftp + ftpsag Prompting Distill Distill + RAG Prompting Distill (Ours) + ftp + ftpsag Prompting Distill Distill + RAG Prompting Distill (Ours) + ftp + ftpsag Prompting Distill Distill + RAG Prompting Distill (Ours) + ftp + ftpsag Teacher: Qwen-2.5-Instruct 79.2 69.2 12.2 25. 60.8 58.4 Student: Qwen-2.5-Instruct 71.8 72.6 68.0 56.0 62.2 66.6 67.8 62.8 61.8 59. 30.5 54.0 54.4 60.2 47.6 46.4 48.6 22.2 46.8 46.4 50.6 28.4 28.6 28.0 3.0 30.4 28.8 34.4 5.8 9.0 6. 16.8 19.6 17.6 19.4 6.2 6.4 4.6 8.8 13.0 13.0 15.8 3.0 2.0 4.2 1.6 9.0 8.0 9.2 0.2 1.4 1. 0.0 7.0 3.4 5.6 43.2 44.8 40.0 41.6 52.0 56.0 55.2 33.6 34.4 32.0 29.6 37.6 43.2 38.4 21.6 21.6 26. 10.4 27.2 30.4 33.6 7.2 10.4 13.6 0.8 17.6 24.0 24.0 36.8 56.4 29.2 31.0 42. 46.8 51.2 55.0 54.4 38.6 26.8 40.6 38.6 48.4 47.6 49.4 17.8 23.8 37.6 8.6 43.0 43.6 45.6 9.2 13.2 29. 2.4 34.6 32.4 33.4 33.4 49.8 29.2 26.8 27.6 45.6 45.2 44.6 45.2 21.6 25.0 28. 28.8 37.4 41.4 41.0 19.0 18.4 27.0 10.6 35.6 32.6 33.6 12.8 23.8 25.4 2.8 28.8 30.8 31.2 74. 76.4 66.6 67.6 60.6 62.2 72.0 70.8 72.4 60.2 56.8 53.2 25.8 64.2 63.0 65.4 49.0 51.0 48. 9.0 54.8 60.6 60.6 25.6 28.6 27.4 5.4 31.2 36.4 40.8 13.3 21.1 12.2 10.0 6. 13.3 11.1 14.4 15.6 6.7 5.6 5.6 4.4 6.7 7.8 15.6 1.1 5.6 2.2 1.1 1.1 7.8 6.7 1.1 1.1 0. 0.0 3.3 1.1 3.3 6.0 39.54 11.5 46.00 7.5 6.5 5. 10.0 5.5 13.0 11.5 4.5 5.0 4.5 3.0 7.5 5.5 7.0 3.5 1.5 2.5 0.0 7.0 3.5 4.5 4.0 2.0 2. 0.0 1.0 3.0 2.5 33.19 33.54 32.16 36.54 39.85 42.26 42.68 29.27 27.72 28.53 21.20 33.60 34.49 36.60 20.33 21.28 24. 7.94 28.06 29.11 30.55 11.06 13.64 15.90 1.8 19.24 19.99 21.90 Table 3: Comparison of performance across general and code-specific models. 32B/1.5B denote general models and 32B-Coder/1.5B-Coder denote code-specific models. For all models, we apply sag with = 8. MuSiQue MATH 500 OlymMATH Hotpot QA Bamboogle GSMHard 2Wiki QA Teacher Student AIME Avg. 32B 32B-Coder 32B 32B-Coder 1.5B 1.5B 1.5B-Coder 1.5B-Coder 45.6 42.6 37.8 41.4 50.6 51.4 52.6 49. 9.2 10.0 8.2 9.4 33.6 36.8 30.4 30.4 33.6 36.8 38.0 37.4 60.6 60.0 59.8 63.6 6.7 6.7 3.3 4.4 4.5 3.0 6.0 5. 30.55 30.91 29.52 30.17 equips the model to actively retrieve and integrate knowledge during reasoning, rather than relying solely on pre-fetched documents that may be insufficient or misaligned with the task. Math reasoning results. On mathematical reasoning tasks, our distilled agents demonstrate strong overall performance. The 1.5B, 3B, and 7B models show improvements on the AIME and OlymMATH benchmarks, benefiting from code tool use for complex calculations acquired through distillation. On GSM-hard, agent distillation improves robustness in reasoning over rare number combinations, such as multi-digit arithmetic. While performance on MATH500 lags behind CoTdistilled models, we attribute this to the Qwen2.5 series being heavily instruction-tuned on collegelevel math, which may align better with CoT. Despite this exception, agent distillation delivers substantial gains across wide range of math tasks. We provide detailed breakdown in Section 7. 7 Figure 4: Performance comparison on the MATH subcategories and levels between CoT and Agent distillation of 3B models. Left: Accuracy by problem category. Right: Accuracy by problem difficulty level. The results highlight that ftp improves the performance of small agents in harder problems."
        },
        {
            "title": "7 Analysis",
            "content": "Code-specific teacher yields better studentsmarginally. We primarily study general instructiontuned models for both the teacher and student agents, as shown in Table 2. Given that CodeAct [21] requires generating code to perform actions, natural question arises: Can we obtain better agents by using code-specific models for the teacher or student in the agent distillation process? To explore this, we conduct the same set of experiments using Qwen2.5-Coder-32B-Instruct as the teacher and Qwen2.5-Coder-1.5B-Instruct as the student [71]. The results, presented in Table 3, suggest that the use of code-specific student model does not significantly impact performance. Instead, the choice of code-specific model as the teacher appears to be more influential in generating effective trajectories for distillation. Nevertheless, the overall improvements are marginal on average, indicating that code-specific post-training has limited impact, which suggests the code knowledge is not critical bottleneck of the student. First-thought prefix improves the agents on more complex reasoning problems. In Table 2, we observe that agent distillation does not improve performance on MATH500 compared to CoT distillation, particularly for the 3B model. To investigate further, we break down MATH500 performance by both problem category and difficulty level. Interestingly, naive distillation degrades the performance of distilled 3B agent on most of levels. However, when using teacher trajectories with first-thought prefix, distilled 3B agent shows improved performance on level 4 and 5 problemswith especially significant gains at level 5. These results suggest that trajectories from ftp help student agents become more robust on complex reasoning tasks, trend also observed in the challenging AIME benchmark in Table 2. However, remaining concern is the performance drop in certain categoriesmost notably, decline in precalculus. Our analysis suggests that this degradation is primarily due to the nature of certain problem types that require an analytic approach rather than straightforward calculations (e.g., applying properties of trigonometric functions). Such problems are harder to solve using code tools. We explore this issue in detail in Appendix D. Self-consistency improves CoT, but the agent with SAG still performs better. Self-consistent action generation (sag) enhances small agents by filtering out invalid code actions and retaining only those that are consistent with observations. Similarly, self-consistency [65] can be applied at test time in Chain-of-Thought (CoT) reasoning to improve performance without relying on an external verifier. natural question is whether CoT with self-consistency, using the same computational budget, can outperform an agent with sag. To investigate this, we conduct experiments using self-consistency [65] on CoT-distilled small language models (sLMs), applying majority voting over multiple samples. As shown in Figure 5, in the MATH benchmarkwhere CoT already surpasses the agent with sag self-consistency further improves the performance of the CoT-distilled model. However, in the more challenging AIME benchmark, the small agent with sag still outperforms the CoT-distilled model under the same generation budget. Moreover, in factual reasoning tasks such as HotpotQA and MuSiQue, self-consistency yields only marginal gains, suggesting limited effectiveness in these settings. 8 Figure 5: Comparison of sag in agents and self-consistency [65] in CoT for 3B models: self-consistency in CoT is helpful in math tasks but not in factual tasks. Figure 6: Generated token counts comparisons in 3B models. For factual reasoning tasks (HotpotQA, MuSiQue), agent generates more tokens than CoT. In contrast, for math reasoning tasks (MATH, AIME), CoT generates slightly more tokens than agent. Figure 7: Impact of Self-consistent Action Generation (sag) on code generation errors across models and 3 math datasets. sag consistently reduces code parse (dark) and code execution (light) errors, especially for smaller models (0.5B) and the AIME dataset. Figure 8: Average retrieval tool calls across three model sizes and datasets. Harder tasks and larger sizes make agents use more retrieval calls. How many tokens should agents generate? natural question is whether distilled agent should generate significantly more tokens than CoT-distilled model, potentially affecting the efficiency and practicality of small models. To investigate this, we analyze token counts on two factual and two math reasoning tasks using 3B distilled models. As shown in Figure 6, there is no significant difference in total token generation between the two approaches across both domains. In factual reasoning, the agent tends to generate more tokens due to making multiple retrieval calls across several steps to gather accurate information. In contrast, in math reasoning, the agent generates fewer tokens than CoT models by delegating repetitive calculations to code execution, often leveraging logical structures like for-loops. SAG significantly reduces invalid code actions. In Figure 7, we show the effect of self-consistent action generation (sag). sag reduces the generation of codes with both parsing and code execution errors. This result indicates that the small distilled agent is capable of generating valid code but the likelihood of generating valid code tends to decrease with smaller model sizes. Larger agents make more retrieval calls, FTP reduces them We analyze how frequently agents use the retrieval tool across different model sizes and factual reasoning benchmarks. As shown in Figure 8, larger models tend to make more retrieval calls than smaller ones, likely because they are better distilled from teacher trajectories and more effective at formulating queries and deciding when to retrieve information. In contrast, smaller models may underuse retrieval due to weaker judgment or limited capacity. For instance, they often over-rely on an initially retrieved document, even when it lacks the necessary information, rather than attempting new retrieval. Interestingly, we find that the first-token prefix (ftp) leads agents to make fewer retrieval calls. As shown in Table 2, ftp improves performance in Bamboogle, but results are mixed in HotpotQA and MuSiQue, possibly due to reduced retrieval. One explanation is that ftp encourages generating factual statements in thought process, which can lead agentsespecially smaller onesto utilize their internal knowledge instead of retrieving them, increasing the risk of hallucination. These findings 9 suggest that the composition of teacher trajectories plays crucial role in helping student models learn effective tool use, especially for solving complex tasks. We include more analysis in Appendix D."
        },
        {
            "title": "8 Conclusion",
            "content": "We proposed Agent Distillation, framework for transferring agentic behavior and tool use from LLMs to small language models (sLMs). By introducing first-thought prefix and self-consistent action generation, we improve both the quality of teacher trajectories and student robustness at test time. Our experiments show that distilled small agents can match or outperform next-tier larger models trained via CoT distillation, especially on out-of-domain tasks. These results highlight agent distillation as practical path for building capable, tool-using small models for real-world problems. Limitations & Future Works. While our method shows strong overall performance, it also highlights several open challenges. The first-thought prefix (ftp) improves agent distillation on average, underscoring the importance of high-quality teacher trajectory generation for effective distillation. However, ftp can sometimes degrade performanceespecially when the model generates facts during reasoning instead of leveraging tools (Figure 8). This highlights the need for improved agentic trajectory generation strategies that align with the behavior and limitations of small models. The success of self-consistent action generation (sag) (Figure 7) suggests the potential of test-time compute scaling and opens up opportunities for incorporating process-level reward models [72, 73]. Finally, while agent distillation enhances the sLMs through agentic behavior, it does not directly improve their core reasoning abilities. Reinforcement learning in tool-augmented environments [43, 49, 74] could further refine these models post-distillation across diverse domains."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)) and the KRAFTON AI Research Center."
        },
        {
            "title": "References",
            "content": "[1] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv, 2412.15115, 2024. URL https://doi.org/10.48550/arXiv.2412.15115. [2] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259/. [3] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 56875711. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/2023.findings-emnlp.378. [4] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539554, 2022. URL https://doi.org/10.1162/tacl_a_00475. 10 [5] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Núria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 66096625. International Committee on Computational Linguistics, 2020. URL https://doi.org/10.18653/v1/2020.coling-main.580. [6] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving In Joaquin Vanschoren and Sai-Kit Yeung, editors, Prowith the MATH dataset. ceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. [7] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: program-aided language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1076410799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.html. [8] AI-MO. Aime. https://huggingface.co/datasets/AI-MO/aimo-validation-aime, 2024. [9] Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, and Ji-Rong Wen. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models. arXiv, 2503.21380, 2025. URL https://doi.org/10. 48550/arXiv.2503.21380. [10] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. 11 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. arXiv, 2407.21783, 2024. URL https: //doi.org/10.48550/arXiv.2407.21783. [12] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https: //doi.org/10.48550/arXiv.2303.08774. [13] Zechun Liu, Changsheng Zhao, Forrest N. Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, and Vikas Chandra. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=EIGbXbxcUQ. [14] Jinheng Wang, Hansong Zhou, Ting Song, Shaoguang Mao, Shuming Ma, Hongyu Wang, Yan Xia, and Furu Wei. 1-bit AI infra: Part 1.1, fast and lossless bitnet b1.58 inference on cpus. arXiv, 2410.16144, 2024. URL https://doi.org/10.48550/arXiv.2410.16144. [15] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, and Mengwei Xu. Small language models: Survey, measurements, and insights. arXiv, 2409.15790, 2024. URL https://doi.org/10.48550/arXiv.2409.15790. [16] Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1485214882. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/2023.acl-long.830. [17] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of GPT-4. arXiv, 2306.02707, 2023. URL https://doi.org/10.48550/arXiv.2306.02707. [18] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1569615707. PMLR, 2023. URL https://proceedings.mlr.press/v202/kandpal23a.html. [19] Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, and Sung Ju Hwang. Knowledgeaugmented reasoning distillation for small language models in knowledge-intensive tasks. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 97faedc90260eae5c400f92d5831c3d7-Abstract-Conference.html. [20] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. [21] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better LLM agents. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=jJ9BoXAfFa. [22] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. [24] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023. URL https: //doi.org/10.48550/arXiv.2301.12726. [25] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Findings of the Association for Computational Linguistics: ACL 2023, pages 80038017, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/2023.findings-acl.507. [26] Hojae Lee, Junho Kim, and SangKeun Lee. Mentor-kd: Making small language models better multi-step reasoners. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1764317658. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.emnlp-main. 977. [27] Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 17731781, Toronto, Canada, July 2023. Association for Computational Linguistics. [28] Xiang Li, Shizhu He, Fangyu Lei, JunYang JunYang, Tianhuang Su, Kang Liu, and Jun Zhao. Teaching small language models to reason for knowledge-intensive multi-hop question answering. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 78047816. Association for Computational Linguistics, August 2024. URL https://aclanthology.org/2024.findings-acl.464/. [29] Shengmin Piao and Sanghyun Park. TinyThinker: Distilling reasoning through coarse-to-fine knowledge internalization with self-reflection. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 60696087. Association for Computational Linguistics, April 2025. URL https://aclanthology.org/2025.naacl-long.309/. [30] Minki Kang, Jongwon Jeong, and Jaewoong Cho. T1: Tool-integrated self-verification for test-time compute scaling in small language models, 2025. URL https://arxiv.org/abs/ 2504.04718. [31] Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xinwei Long, Zhouhan Lin, and Bowen Zhou. Pad: Program-aided distillation can teach small models reasoning better than chain-of-thought finetuning. In Kevin Duh, Helena Gómez-Adorno, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational 13 Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 25712597. Association for Computational Linguistics, 2024. [32] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. Distilling mathematical reasoning capabilities into small language models. Neural Networks, 179:106594, 2024. URL https: //doi.org/10.1016/j.neunet.2024.106594. language agents with verbal reinforcement learning. [33] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 1b44b878bb782e6954cd888628510e90-Abstract-Conference.html. [34] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv, 2310.05915, 2023. URL https: //doi.org/10.48550/arXiv.2310.05915. [35] Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 93549366. Association for Computational Linguistics, 2024. URL https://doi.org/10.18653/v1/2024.findings-acl.557. [36] Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andrés Codas, Yadong Lu, Weige Chen, Olga Vrousgos, Corby Rosset, Fillipe Silva, Hamed Khanpour, Yash Lara, and Ahmed Awadallah. Agentinstruct: Toward generative teaching with agentic flows. arXiv, 2407.03502, 2024. URL https://doi.org/10.48550/arXiv.2407.03502. [37] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning from scratch for QA via self-planning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 30033021. Association for Computational Linguistics, 2024. URL https://doi.org/10.18653/v1/2024.acl-long. 165. [38] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Raghavi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1238012403. Association for Computational Linguistics, 2024. URL https://doi.org/10.18653/v1/2024.acl-long.670. [39] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 30533077. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.181. URL https://doi.org/10.18653/v1/2024.findings-acl.181. [40] Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Manoj Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, and Caiming Xiong. Agentohana: Design unified data and training pipeline for effective agent learning. arXiv, 2402.15506, 2024. URL https://doi.org/10.48550/arXiv.2402.15506. [41] Qinhao Zhou, Zihan Zhang, Xiang Xiang, Ke Wang, Yuchuan Wu, and Yongbin Li. Enhancing the general agent capabilities of low-paramter LLMs through tuning and multi-branch 14 reasoning. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 29222931. Association for Computational Linguistics, June 2024. doi: 10.18653/v1/2024.findings-naacl.184. URL https://aclanthology.org/2024.findings-naacl.184/. [42] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. Agentbank: Towards generalized LLM agents via fine-tuning on 50000+ interaction trajectories. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 21242141. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.findings-emnlp.116. [43] Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher D. Manning. Synthetic data generation & multi-step rl for reasoning & tool use, 2025. URL https://arxiv.org/ abs/2504.04736. [44] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv, 2503.09516, 2025. URL https://doi.org/10.48550/arXiv.2503.09516. [45] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv, 2501.05366, 2025. URL https://doi.org/10.48550/arXiv.2501.05366. [46] Edward Beeching, Shengyi Costa Huang, Albert Jiang, Jia Li, Benjamin Lipkin, Zihan Qina, Kashif Rasul, Ziju Shen, Roman Soletskyi, and Lewis Tunstall. Numinamath 7b tir. https: //huggingface.co/AI-MO/NuminaMath-7B-TIR, 2024. [47] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=Ep0TtjVoap. [48] Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. START: self-taught reasoner with tools. arXiv, 2503.04625, 2025. URL https://doi.org/10.48550/arXiv.2503.04625. [49] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs, 2025. URL https: //arxiv.org/abs/2504.13958. [50] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024. URL https://doi.org/ 10.48550/arXiv.2409.12122. [51] Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, Himanshu Tyagi, and Pramod Viswanath. Open deep search: Democratizing search with open-source reasoning agents. arXiv, 2503.20201, 2025. doi: 10.48550/ARXIV.2503.20201. URL https://doi. org/10.48550/arXiv.2503.20201. [52] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning, 2025. URL https://arxiv.org/abs/ 2505.01441. [53] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research. arXiv, 2502.04644, 2025. URL https://doi.org/10.48550/arXiv.2502. 04644. 15 [54] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in neural network. arXiv, 1503.02531, 2015. URL http://arxiv.org/abs/1503.02531. [55] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [56] Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. smolagents: smol library to build great agentic systems. https://github. com/huggingface/smolagents, 2025. [57] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, and Chengxiang Zhai. If LLM is the wizard, then code is the wand: survey on how code empowers large language models to serve as intelligent agents. CoRR, abs/2401.00812, 2024. URL https://doi.org/10.48550/arXiv.2401.00812. [58] Kushal Jain, Moritz Miller, Niket Tandon, and Kumar Shridhar. First-step advantage: Importance of starting right in multi-step math reasoning, 2024. URL https://arxiv.org/abs/2311. 07945. [59] Ke Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie Wang, Junying Chen, Benyou Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. The first few tokens are all you need: An efficient and effective unsupervised prefix fine-tuning method for reasoning models. arXiv, 2503.02875, 2025. URL https://doi.org/10.48550/arXiv.2503.02875. [60] Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. Critical tokens matter: Token-level contrastive estimation enhances llms reasoning capability. CoRR, 2411.19943, 2024. URL https://doi.org/10.48550/ arXiv.2411.19943. [61] Seanie Lee, Haebin Seong, Dong Bok Lee, Minki Kang, Xiaoyin Chen, Dominik Wagner, Yoshua Bengio, Juho Lee, and Sung Ju Hwang. Harmaug: Effective data augmentation for knowledge distillation of safety guard models. CoRR, abs/2410.01524, 2024. URL https: //doi.org/10.48550/arXiv.2410.01524. [62] Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and Peter Henderson. Safety alignment should be made more than just few tokens deep. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=6Mxhg9PtDE. [63] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html. [64] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=rygGQyrFvH. [65] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. [66] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. MINT: evaluating llms in multi-turn interaction with tools and language feedback. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=jp3gWrMuIZ. [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, In Alice Oh, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. 16 Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html. [68] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive In Advances in Neural Information Processing Systems 33: Annual ConNLP tasks. ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html. [69] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv, 2212.03533, 2022. URL https://doi.org/10.48550/arXiv.2212.03533. [70] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, In Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9. [71] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report. arXiv, 2409.12186, 2024. URL https://doi.org/10.48550/arXiv.2409.12186. [72] Sanjiban Choudhury. Process reward models for LLM agents: Practical framework and directions. arXiv, 2502.10325, 2025. URL https://doi.org/10.48550/arXiv.2502.10325. [73] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 94269439. Association for Computational Linguistics, 2024. URL https://doi.org/10.18653/v1/2024.acl-long.510. [74] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv, 2402.03300, 2024. URL https://doi.org/10.48550/arXiv. 2402.03300. [75] Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-Bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Róbert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, and Ivan Nardini. Gemma 3 technical report. arXiv, 2503.19786, 2025. URL https://doi.org/10.48550/arXiv.2503.19786. 17 [76] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/ forum?id=0IOX0YcCdTn. [77] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html. [78] Anthropic. Introducing the model context protocol. https://www.anthropic.com/news/ model-context-protocol, 2024."
        },
        {
            "title": "A Limitations",
            "content": "In addition to the discussion in Section 8, we outline here several additional limitations of our study. First, our experiments are limited to the Qwen2.5 model series [1]. While we expect our proposed approach to generalize across model families, we have not validated its effectiveness on other widelyused language models such as LLaMA [11] or Gemma [75]. Extending our study to these models would strengthen the generality of our findings and remains an important direction for future work. Second, we only distill from single teacher model (Qwen2.5-32B). Using stronger or larger teacher modelsparticularly proprietary closed-source models like GPT-4 [12]may lead to further performance gains in student agents. However, such experiments were not feasible due to computational and budget constraints. Third, we do not investigate the effect of the number of teacher trajectories per question on student performance, which has been shown to be an important factor in prior CoT distillation research [16, 19]. Exploring this variable could offer further insights into how to optimize agent distillation. Lastly, our current work focuses exclusively on agents that utilize retrieval and code execution tools to solve real-world problems that the general LLM can solve without tools. Other agent applications such as embodied agents [76] or web-based agents [77]remain unexplored. Future research could extend agent distillation to these broader settings, leveraging tool-augmented environments such as web browsers, simulators, or desktop interfaces. In particular, integration with frameworks like the Model Context Protocol (MCP) [78] could further enhance the capabilities of small agents across diverse real-world tasks."
        },
        {
            "title": "B Broader impacts",
            "content": "This work contributes toward the development of small language agents capable of running on local devices, enabling functional on-device AI that can retrieve information from external knowledge sources (including the web) and perform code-based action to complete complex tasks. On the positive side, this advancement promotes more accessible and inclusive AI by lowering the hardware and computational barriers for deployment. It opens opportunities for broader adoption of AI agents in resource-constrained or privacy-sensitive domains, such as healthcare, where data locality and privacy are critical. However, there are potential risks. Since our distilled agents are capable of retrieving web information and executing code, they could be misused to automate malicious behaviors, such as generating harmful scripts or launching unauthorized attacks. Addressing these concerns will require the integration of robust safeguards, including behavior monitoring, tool-use restrictions, and secure deployment practices. We highlight this as an important avenue for future research and responsible development."
        },
        {
            "title": "C Implementation details",
            "content": "Prompts and agent framework. For CoT prompt, we use the prompt in Prompt D.1 for both math and factual reasoning. For agent prompt, we use the prompt from smolagents library [56]. We present the part of prompt in Prompt D.2. As an agent framework, we use the CodeAct [21] implemented in smolagents. We only include the retriever for wikipedia as tool with the name of web_search. For student model, we use the same prompt for CoT reasoning. For agent, we only remove few-shot demonstrations as it is no longer needed after fine-tuning. Training dataset details. We use 1000 HotPotQA [2] and 2000 MATH [6] examples for training. Specifically, we only use 1000 hard examples from HotPotQA and 1000 level 2-3 examples, 1000 level 4-5 examples from MATH dataset. We prompt LLM to generate trajectories for both CoT and agent and filter out wrong trajectories based on the correctness of predicted answer. After filtering, we use approximately 2,000 trajectories to train the small models. The exact number varies 19 Table 4: Comparison of CoT and Agent approaches on Qwen2.5-32B-Instruct across training dataset. ftp denotes the first-thought prefix. Hard denotes level 4-5 and medium denotes level 2-3 questions. Model HotpotQA MATH (hard) MATH (medium) CoT Qwen2.5-32B-Instruct Agent Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct + ftp 40.9 59.3 60.8 71.1 58.4 67.1 89.8 78.4 83. depending on the performance of the teacher models on the training dataset, which we present details in Appendix D.1."
        },
        {
            "title": "D Additional analysis",
            "content": "D.1 Teacher model performance on training dataset In Section 4, we propose that the first-thought prefix improves teacher performance on hard math problems. To support this, we present teacher model results on the training set in Table 4. We observe that the LLM agent outperforms chain-of-thought (CoT) prompted LLM in factual reasoning, as the LLM relies heavily on prompting to use tools effectivelyand proper tool use contributes significantly to performance. However, the performance of the LLM agent on math tasks drops considerably, especially on harder (level 45) problems. In such cases, adding the first-thought prefix helps recover some of the lost performance, as discussed in Section 4. These results suggest that simply prepending the first CoT step to the agents reasoning improves its capabilities, which in turn benefits distillation, as shown in Table 2. D.2 Failure case analysis of agent on the math subcategory In example D.1, we present failure case of the distilled 3B agent on level 2 precalculus problem. In this instance, the generated code produces decimal result, which is not the correct form for an answer expected in radians. Although the agent attempts conversion in its reasoning, it ultimately produces an incorrect radian value. Examples D.2 and D.3 involve more challenging level 4 precalculus problems. In Example D.2, for instance, the agent makes conceptual error in its reasoning by misidentifying the appropriate range for the angle θ. These examples suggest that the agent struggles particularly with problems requiring analytic reasoningsuch as understanding the properties of trigonometric functionsrather than straightforward computation. D.3 Deeper analysis on the first-thought prefix Effects on mathematical reasoning. As discussed in Section 4, the inclusion of first-thought prefix (ftp) influences the initial reasoning patterns of the agent. In this section, we analyze how this prefix affects student agents distilled from trajectories both with and without the ftp, using representative examples. In examples D.4 and D.5, drawn from the MATH500 dataset, we compare the reasoning approaches of distilled 3B agents with and without the ftp. Without the prefix (Example D.4), the agents initial reasoning begins with descriptive analysis, e.g., The problem is asking..., focusing on understanding the question. In contrast, with the prefix (Example D.5), the agent begins with goaloriented plan, e.g., To find the smallest positive real number..., which mirrors chain-of-thought (CoT) strategy. This shift illustrates that the ftp nudges the agent toward more proactive and structured reasoning style, which might be beneficial in domains requiring multi-step reasoning (e.g., challenging math problems). 20 Effects in factual reasoning. As shown in Figure 8, the use of the first-thought prefix (ftp) consistently reduces the number of retrieval tool calls made by distilled agents. To better understand this phenomenon, we include illustrative examples from the Bamboogle dataset. Examples D.6 and D.7 demonstrate cases where the ftp causes the distilled agent to generate factual knowledge internally rather than retrieving it. This question requires identifying the founder of geometry, the city associated with that individual, and the founder of that city. In Example D.6, the agent (with ftp) directly generates the statement The founder of geometry, Euclid without making retrieval call. In contrast, in Example D.7, the agent (without ftp) uses the retrieval tool to search for the founder of geometry, which reduces the risk of hallucination. This pattern helps explain the behavior observed in Figure 8: while ftp can reduce the number of tool calls, it may also increase the likelihood of factual errors due to hallucination, as the agent relies more on internally generated knowledge. 21 Prompt D.1: Prompt for Chain-of-Thought Reasoning You are an expert assistant who can answer the given question accurately and provide clear reasoning. When answering questions, follow these guidelines: 1. Provide clear and structured reasoning first 2. Follow up with final answer, must in the <answer> </answer> tag. For example, <answer> xxx </answer>. 3. The answer must be succinct and final. For math problems, return the answer using LaTeX in the boxed format. 4. If the question requires multiple steps or facts, break down your reasoning accordingly 5. Be precise and factual in your responses 6. If youre unsure about something, acknowledge the uncertainty Now, please answer the following question: Prompt D.2: Prompt for Agent You are an expert assistant who can solve any task using code blobs. You will be given task to solve as best you can. To do so, you have been given access to list of tools: these tools are basically Python functions which you can call with code. To solve the task, you must plan forward to proceed in series of steps, in cycle of Thought:, Code:, and Observation: sequences. At each step, in the Thought: sequence, you should first explain your reasoning towards solving the task and the tools that you want to use. Then in the Code: sequence, you should write the code in simple Python. The code sequence must end with <end_code> sequence. During each intermediate step, you can use print() to save whatever important information you will then need. These print outputs will then appear in the Observation: field, which will be available as input for the next step. In the end you have to return final answer using the final_answer tool. For math problems, if not specified, always return LaTex format as the final answer. ... (ellipsis) ... Here are the rules you should always follow to solve your task: 1. Always provide Thought: sequence, and Code:py sequence ending with <end_code> sequence, else you will fail. 2. Use only variables that you have defined! 3. Always use the right arguments for the tools. DO NOT pass the arguments as dict as in answer = wiki(query: What is the place where James Bond lives?\"), but use the arguments directly as in answer = wiki(query=What is the place where James Bond lives?\"). 4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block. 5. Call tool only when needed, and never re-do tool call that you previously did with the exact same parameters. 6. Dont name any new variable with the same name as tool: for instance dont name variable final_answer. 7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables. ... (ellipsis) ... 22 Example D.1: Failure case 1 of Agent in Precalculus (level 2) Problem: Compute arcsin (cid:0) 1 (cid:1) . Express your answer in radians. 2 Agent: Thought: To compute arcsin (cid:0) 1 within the range of the arcsine function, which is [ π Code: 2 2 , π 2 ]. (cid:1), we need to find the angle θ such that sin(θ) = 1 2 and θ lies 1 2 3 4 5 import math # Calculate the arcsine of -1/2 theta = math . asin ( -1/2) print ( theta ) Observation: -0.5235987755982989 Thought: The output from the code snippet is the value of θ in radians. However, the task requires the answer to be in LaTeX format and as an exact value, not decimal approximation. The value of θ is 5π 2 within the range of the arcsine function. Code: 6 , which is the exact value of the arcsine of 1 1 final_answer ( \" boxed { frac {5 pi }{6}} \" ) Observation: 5π 6 (wrong) (cid:1), we need to find the angle θ such that sin(θ) = 1 2 , π 2 ]. CoT: To compute arcsin (cid:0) 1 2 the range of the arcsine function, which is [ π 1. Recall that sin(θ) = 1 y-coordinate is 1 2 . 2. The sine function is negative in the third and fourth quadrants. However, since the range of the arcsine function is [ π 3. The reference angle for sin(θ) = 1 sine value of 1 2 is π 6 . Thus, the angle θ such that sin(θ) = 1 2 corresponds to the sine of an angle in the unit circle where the 2 ], we are only interested in the fourth quadrant for this problem. 6 . Therefore, the angle in the fourth quadrant that has 2 and θ lies within 2 is π 2 , π 2 and θ [ π 2 , π 2 ] is π 6 . π 6 (correct) 23 Example D.2: Failure case 1 of Agent in Precalculus (level 4) part Problem: Let (x) = (arccos x)2 + (arcsin x)2. Find the range of (x). All functions are in radians. Agent: Thought: To find the range of the function (x) = (arccos x)2 + (arcsin x)2, we start by using the identity that relates the arccosine and arcsine functions: π 2 arccos + arcsin = . Let θ = arccos x. Then arcsin = π 2 θ. Substituting these into the function, we get: (x) = θ2 + (cid:17)2 . θ (cid:16) π 2 Expanding the expression, we have: (x) = θ2 + (cid:17)2 (cid:16) π 2 πθ + θ2 = 2θ2 πθ + (cid:17)2 . (cid:16) π 2 This is quadratic function in terms of θ. To find the range of (x), we need to determine the range of θ and then find the minimum value of the quadratic function over this range. The range of θ is [0, π 2θ2 πθ + (cid:0) π (cid:1)2 2 ax2 + bx + c, which is = Code: 2 ]. We can find the minimum value of the quadratic function (θ) = by completing the square or using the vertex formula for quadratic function 2a . Lets find the minimum value of (θ) using the vertex formula. 1 2 4 5 6 7 8 10 11 12 from sympy import symbols , diff , solve , pi # Define the variable and function theta = symbols ( theta ) f_theta = 2* theta **2 - pi * theta + ( pi /2) **2 # Find the derivative of the function f_prime_theta = diff ( f_theta , theta ) # Solve for the critical points critical_points = solve ( f_prime_theta , theta ) print ( critical_points ) Observation: [π/4] Thought: The critical point is θ = π 4 . We can now evaluate the function (θ) at this point to find the minimum value of the function. Lets evaluate (θ) at θ = π 4 . Code: 1 2 3 # Evaluate the function at the critical point theta_critical = pi / 4 f_theta_critical = f_theta . subs ( theta , theta_critical ) print ( f_theta_critical ) Observation: π2/8 Thought: The minimum value of the function (θ) at θ = π 8 . Since the function is quadratic function that opens upwards (the coefficient of θ2 is positive), the minimum value is the only value in the range of (x). Therefore, the range of (x) is Lets use the final_answer tool to provide the final answer in LaTeX format. Code: 4 is π2 (cid:104) π2 (cid:17) 8 , . final_answer ( \" boxed { left [ frac { pi ^2}{8} , infty right ) } \" ) Observation: (cid:19) , (wrong) (cid:20) π2 24 Example D.3: Failure case 1 of Agent in Precalculus (level 4) part 2 Problem: Let (x) = (arccos x)2 + (arcsin x)2. Find the range of (x). All functions are in radians. CoT: To find the range of the function (x) = (arccos x)2 + (arcsin x)2, we start by using the relationship between arccos and arcsin x. Recall that for any [1, 1], arccos + arcsin = π . Let θ = arccos x. Then arcsin = π 2 θ. Substituting these into the function (x), we get Expanding the expression, we have (x) = θ2 + (cid:17) . θ (cid:16) π 2 (cid:17)2 (cid:17)2 θ (cid:16) π 2 = θ2 + (x) = θ2 + πθ + θ2 = 2θ2 πθ + (cid:16) π 2 This is quadratic function in terms of θ. To find the range of (x), we need to determine the range of θ as varies from 1 to 1. Since θ = arccos x, θ ranges from 0 to π. We need to find the minimum and maximum values of the quadratic function g(θ) = 2θ2 πθ + π2 4 over the interval [0, π]. The vertex of the parabola g(θ) = 2θ2 πθ + π2 the vertex and at the endpoints of the interval [0, π]: 1. At θ = 0: 4 occurs at θ = 4 . Evaluating g(θ) at 2a = π . π2 4 g(0) = 2(0)2 π(0) + π2 = π2 4 . g(π) = 2(π)2 π(π) + π2 4 = 2π2 π2 + π2 4 = π2 + π2 4 = 5π2 4 . 2. At θ = π: 3. At θ = π 4 : (cid:17) (cid:16) π 4 = (cid:16) π 4 (cid:17) (cid:17)2 π (cid:16) π 4 π2 4 8 and the maximum value is 5π π2 8 4 . Therefore, the range of (x) is π2 16 π2 4 π2 4 = 2 = + + . The minimum value of g(θ) is π2 (cid:20) π2 , 5π2 4 (cid:21) . Thus, the range of (x) is (cid:21) (cid:20) π2 8 , 5π2 4 . (correct) 25 Example D.4: Comparison of first-thought prefix Problem: Find the smallest positive real number for which (cid:19) (cid:13) (cid:18)2 (cid:13) (cid:13) (cid:13) 3 0 2 (cid:13) (cid:13) (cid:13) (cid:13) Cv for all two-dimensional vectors v. Note that for two-dimensional vector a, is the magnitude of a. Small Agent distilled from trajectories without ftp: Thought: The problem is asking for the smallest positive real number for which the inequality holds for all two-dimensional vectors v. This is problem of finding the spectral radius of the matrix. The spectral radius of matrix is the largest absolute value of its eigenvalues. will first find the eigenvalues of the matrix, then find the largest absolute value of the eigenvalues, and that will be the value of C. Code: 1 2 4 5 6 7 8 10 11 12 import numpy as np # Define the matrix = np . array ([[2 , 3] , [0 , -2]]) # Find the eigenvalues eigenvalues = np . linalg . eigvals ( ) # Find the largest absolute value of the eigenvalues = max ( abs ( eigenvalues ) ) print ( ) Observation: 2.0 Thought: The code has executed and the value of has been found. Now will return the final answer using the final_answer tool. Code: 1 final_answer ( \" boxed {2} \" ) Observation: 2 (wrong) 26 Example D.5: Comparison of first-thought prefix (cont.) Problem: Find the smallest positive real number for which (cid:19) (cid:13) (cid:18)2 (cid:13) (cid:13) (cid:13) 3 0 (cid:13) (cid:13) (cid:13) (cid:13) Cv for all two-dimensional vectors v. Note that for two-dimensional vector a, is the magnitude of a. Small Agent distilled from trajectories with ftp: Thought: To find the smallest positive real number for which the inequality (cid:13) (cid:18)2 (cid:13) (cid:13) (cid:13) 3 0 Cv (cid:13) (cid:13) (cid:13) (cid:13) (cid:19) holds for all two-dimensional vectors v, we need to analyze the norm of the matrix-vector product. The norm of the matrix-vector product can be computed as the maximum singular value of the matrix. The singular values of matrix are the square roots of the eigenvalues of the matrix , the singular values are the square multiplied by its transpose. For 2x2 matrix = roots of the eigenvalues of AT A. Lets compute the singular values of the matrix = Code: (cid:19) (cid:18)a c (cid:18)2 3 0 2 (cid:19) . 1 3 4 5 6 7 import numpy as np # Define the matrix = np . array ([[2 , 3] , [0 , -2]]) # Compute the singular values of the matrix singular_values = np . linalg . svd (A , compute_uv = False ) print ( singular_values ) Observation: [4. 1.] Thought: The singular values of the matrix = (cid:19) (cid:18)2 3 0 2 are 4 and 1. The maximum singular value is 4. Therefore, the smallest positive real number for which the inequality holds for all two-dimensional vectors is the maximum singular value. Code: 1 3 # The maximum singular value is the smallest = max ( singular_values ) final_answer ( ) Observation: 4 (correct) 27 Example D.6: Illustrative case of first-thought prefix in factual reasoning Problem: Who founded the city where the founder of geometry lived? Small Agent distilled from trajectories with ftp: Thought: To answer this question, we need to identify the city where the founder of geometry, Euclid, lived and then determine who founded that city. Code: 1 2 eucl id_birthplace = web_search ( query = \" where did Euclid live \" ) print ( \" Euclid lived in { eucl id_birth place }. \" ) Observation: Euclid lived in (...) \"Euclid, Ohio\" Euclid, Ohio Euclid is city in Cuyahoga County, Ohio, United States. It is an inner ring suburb of Cleveland. As of the 2010 census, the city had total population of 48,920. In 2009, Euclid celebrated its bicentennial. The city was named after Euclid, the Greek mathematician. The city was settled in 1796 and became township in 1809. Euclid then became village in 1903 and city in 1930, during the Great Depression. The city (then village) was the primary appealing party in the landmark case \"\"Village of Euclid v. Ambler Realty Co.\"\". Euclid is located (...) Thought: From the search results, we can see that Euclid, Ohio was named after the Greek mathematician Euclid. Now, we need to find out who founded the city of Euclid, Ohio. Code: 2 foun der_of_euclid = web_search ( query = \" who founded the city of Euclid , Ohio \" ) print ( \" The city of Euclid , Ohio was founded by { founder_of_eu clid }. \" ) Observation: The city of Euclid, Ohio was founded by (...) \"South Euclid, Ohio\" treaty with the Iroquois confederation in 1796 by the Connecticut Land Company. In 1797, Moses Cleaveland named the area east of the Cuyahoga River Euclid, after the Greek mathematician and \"\"patron saint\"\" of surveyors. Euclid Township was officially formed in 1809. In 1828, Euclid Township was divided into nine districts, with South Euclid becoming district two. (...) Thought: The search results indicate that the city of Euclid, Ohio was settled in 1796 by Moses Cleaveland, who was trained in mathematics and named the township after the Greek mathematician Euclid. Therefore, Moses Cleaveland founded the city where the founder of geometry, Euclid, lived. Code: 1 final_answer ( \" Moses Cleaveland \" ) Observation: Moses Cleaveland (wrong) 28 Example D.7: Illustrative case of first-thought prefix in factual reasoning (cont.) Problem: Who founded the city where the founder of geometry lived? Small Agent distilled from trajectories without ftp: Thought: need to find out who founded the city where the founder of geometry lived. will first find out who the founder of geometry is, then find out where he lived, and finally find out who founded the city where he lived. will use the tool web_search to get this information. Code: 1 2 foun de r_ of _geo me tr = web_search ( query = \" founder of geometry \" ) print ( \" Founder of geometry : { founder_of_geometry } \" ) Observation: Founder of geometry: (...) Euclid Euclid Euclid (; \"\"Eukleídes\"\", ; fl. 300 BC), sometimes given the name Euclid of Alexandria to distinguish him from Euclides of Megara, was Greek mathematician, often referred to as the \"\"founder of geometry\"\" or the \"\"father of geometry\"\". He was active in Alexandria during the reign of Ptolemy (323283 BC). His \"\"Elements\"\" is (...) Thought: From the search results, can see that the founder of geometry is Euclid. Now need to find out where he lived and who founded the city where he lived. will use the tool web_search again to get this information. Code: 1 2 euclid_lived = web_search ( query = \" where did Euclid live \" ) print ( \" Where did Euclid live : { euclid_lived } \" ) Observation: Where did Euclid live: (...) Euclid detailed biography of Euclid is given by Arabian authors, mentioning, for example, birth town of Tyre. This biography is generally believed to be fictitious. If he came from Alexandria, he would have known the Serapeum of Alexandria, and the Library of Alexandria, and may have worked there during his time. (...) Thought: From the search results, can see that Euclid lived in Alexandria, Egypt. Now need to find out who founded the city where Euclid lived. will use the tool web_search again to get this information. Code: 2 fo nde r_ f_a le an dr = web_search ( query = \" who founded Alexandria \" ) print ( \" Who founded Alexandria : { oun der_of _alex andria } \" ) Observation: Who founded Alexandria: (...) Alexandria Alexandria is believed to have been founded by Alexander the Great in April 331 BC as (\"\"Alexandreia\"\"). Alexanders chief architect for the project was Dinocrates. Alexandria was intended to supersede Naucratis as Hellenistic center in Egypt, (...) Thought: From the search results, can see that Alexandria was founded by Alexander the Great. Therefore, the founder of the city where the founder of geometry lived is Alexander the Great. will now provide the final answer using the final_answer tool. Code: 1 final_answer ( \" Alexander the Great \" ) Observation: Alexander the Great (correct)"
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST",
        "KRAFTON"
    ]
}