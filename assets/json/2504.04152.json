{
    "paper_title": "Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources",
    "authors": [
        "Zihao Li",
        "Shaoxiong Ji",
        "Hengyu Luo",
        "JÃ¶rg Tiedemann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 2 5 1 4 0 . 4 0 5 2 : r Preprint. Under review. Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources Zihao Li1, Shaoxiong Ji2,1, Hengyu Luo1, org Tiedemann1 1University of Helsinki {zihao.li, hengyu.luo, jorg.tiedemann}@helsinki.fi; shaoxiong.ji@tu-darmstadt.de 2Technical University of Darmstadt"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs), built upon the Transformer architecture (Vaswani et al., 2017), have achieved remarkable progress in tasks such as machine translation, text classification, and generative dialogue. Despite these advances, their performance remains highly uneven across languages, favoring high-resource languages and marginalizing underrepresented ones (Li et al., 2024). This imbalance deepens the digital language divide and limits the inclusivity of NLP technologies. Recent work on Continual Pretraining (CPT) has shown promise for adapting pretrained models to new languages through additional training on targeted data (Zheng et al., 2024a). EMMA-500 employed CPT with extensive monolingual datasets across more than 500 languages, significantly improving multilingual performance, particularly for low-resource languages (Ji et al., 2024a). LLaMAX achieved notable translation improvements through CPT on over 100 languages involving data augmentation with bilingual translation data (Lu et al., 2024). Similar effects could be demonstrated on translation tasks with the CPT-based TOWER model (Alves et al., 2024). However, the relative effectiveness of monolingual and bilingual translation data for CPT remains unclear, particularly in terms of their impact on continual language learning, language interference, and performance consistency across different resource levels of languages. Corresponding author 1 Preprint. Under review. In addition to textual data in natural languages, growing practice in LLM training is to incorporate programming code as an additional source of information. Previous research indicates that incorporating code enhances reasoning capabilities and improves the ability to handle structured information (Petty et al., 2024; Aryabumi et al., 2024), but its role in multilingual context remains underexplored. There is critical gap in understanding how language characteristics interact with CPT strategies. recent classification proposed by Yuan et al. (2024) categorizes languages as altruistic, selfish, and stagnant based on their cross-lingual transfer patterns. However, this classification has only been validated in narrow experimental settings using English-centric bilingual data, leaving open questions about its generalizability to: (1) non-English language pairs, (2) code-augmented training regimes, and (3) models with varying pretraining corpora and architectures. To systematically assess the impact of different CPT strategies, we conduct extensive experiments with 36 configurations, evaluating monolingual, bilingual, and code-augmented CPT on multilingual adaptation.1 Our setup includes three multilingual base modelsLlama3.1-8B (Dubey et al., 2024), Llama-2-7B (Touvron et al., 2023), and Viking-7B (Luukkonen et al., 2025)continual-pretrained languages spanning high-, medium-, and low-resource categories. We evaluate the model performance on 14 training languages and assess crosslingual transfer on 25 related languages, with particular focus on assessing how different CPT configurations perform across altruistic, selfish, and stagnant language categories by Yuan et al. (2024). Our systematic evaluation of 36 CPT configurations across three base models and 30+ languages yields three core insights: Bilingual CPT improves classification performance but introduces generation challenges: Compared to monolingual CPT, bilingual CPT generally improves multilingual classification accuracy for mediumand low-resource languages. However, it frequently results in problematic language mixing during generation tasks, limiting its overall utility. Code data enhances classification but introduces trade-offs in generation: Adding code data during CPT significantly boosts multilingual classification performance across resource levels, especially for low-resource languages, acting as an effective scaffold for representation learning. Nevertheless, code inclusion may lead to trade-off, slightly degrading generation quality in certain scenarios. The categorization of languages according to their cross-lingual transfer abilities does not generalize under varying conditions: Our experiments reveal substantial deviations from language classifications proposed in previous work (Yuan et al., 2024): so-called altruistic languages are not always helpful and often negatively impact related languages, selfish languages exhibit highly configuration-dependent cross-lingual effects, and languages classified as stagnant demonstrate unexpected adaptability under specific training settings. These findings highlight the complexity of multilingual interactions in CPT and emphasize the need for more adaptive classification framework for cross-lingual learning."
        },
        {
            "title": "2 Materials and Methods",
            "content": "2.1 Language Selection We systematically evaluate the effects of CPT on multilingual models by selecting languages according to the altruistic, selfish, and stagnant categories defined in Yuan et al. (2024), which classify languages based on their behavior in multilingual training and evaluation. 1Monolingual data consists of texts in single language, though it may include code-switching. Bilingual translation data contains sentence pairs in two languages that convey the same meaning. When monolingual data from different languages is combined, it forms multilingual continual pertaining, and similar principle applies to bilingual translation data. However, for clarity, we refer to these setups as monolingual CPT and bilingual CPT, respectively. 2 Preprint. Under review. For each category, we select 1 high-resource language (except for the stagnant category for which no high-resource language is available in the dataset we use), 2 medium-resource languages, and 2 low-resource languages to ensure balanced representation across different resource levels. The classification of languages into high-, medium-, and low-resource categories is determined by analyzing the data distribution of the Lego-MT dataset (Yuan et al., 2023), which serves as the basis for our setup. Specifically, we calculate the total token count for each language. Languages are then categorized as follows: high-resource languages exceed 1 billion tokens, medium-resource languages range between 10 million and 1 billion tokens, and low-resource languages fall below 10 million tokens. These languages serve as the training languages in our CPT experiments. The selected training languages, along with their corresponding category and resource level, are summarized in the first three columns of Table 1. To further validate the findings in Yuan et al. (2024), we select 1-2 linguistically related languages for each training language based on the language evolutionary tree23. These related languages are not included in the CPT phase but are used for cross-lingual evaluation to determine whether the effects observed in training languages extend to unseen but related languages. The fourth and fifth columns of Table 1 list the selected related languages. For some languages, this includes one, and for others, two related languages that are available in the evaluation benchmarks. Category Resources Training Language Related Language 1 Related Language 2 Altruistic Selfish Stagnant High Medium Medium Low Low High Medium Medium Low Low Medium Medium Low Low zho Hani ceb Latn mar Deva zul Latn khm Khmr deu Latn bel Cyrl mri Latn kir Cyrl nya Latn tha Thai yor Latn sna Latn wol Latn yue Hant tgl Latn hin Deva xho Latn vie Latn nld Latn rus Cyrl smo Latn kaz Cyrl bem Latn lao Laoo ibo Latn nya Latn bam Latn - ilo Latn npi Deva ssw Latn - dan Latn ukr Cyrl fij Latn bak Cyrl sna Latn shn Mymr hau Latn zul Latn - Table 1: Selected languages for CPT along with their corresponding related languages for evaluation. - indicates the second related language cannot be found in the benchmark."
        },
        {
            "title": "2.2 Pretraining Data",
            "content": "Bilingual Translation Data We utilize subsets of the Lego-MT (Yuan et al., 2023) and NLLB (Schwenk et al., 2021; Heffernan et al., 2022; Costa-juss`a et al., 2022) datasets as our sources of parallel bilingual data. The Lego-MT dataset, derived from OPUS4, provides translations across 433 languages. The NLLB dataset consists of 148 English-centric and 1,465 non-English-centric bitext pairs mined from different parallel sources. To construct our parallel training data, we select specific language pairs from these datasets and apply OpusFilter (Aulamo et al., 2020) to remove duplicate data points. The resulting dataset comprises approximately 292 million tokens across 22 language pairs, distributed over three language categories: altruistic (10 pairs, 92M tokens), selfish (8 pairs, 100M tokens), and stagnant (4 pairs, 100M tokens). For training, we format parallel data using the following structure: 2http://www.elinguistics.net/Language Evolutionary Tree.html 3Using the language evolutionary tree to identify related languages, we assess whether CPT effects transfer to unseen but linguistically similar languages, thus evaluating cross-lingual robustness. 4https://opus.nlpl.eu 3 Preprint. Under review. [source language]: [source] [target language]: [target] Monolingual Data We extract subset of MADLAD-400 (Kudugunta et al., 2024), large-scale multilingual dataset derived from Common Crawl5, covering 419 languages. Since web-crawled text does not inherently guarantee monolingual integrity, we employ GlotLID (Kargaran et al., 2023), language identification model, to analyze the language composition of each text segment and ensure strict monolingual consistency. Specifically, for each document in the dataset, we first segment the text into sentences using the NLTK (Bird & Loper, 2004) sentence splitter. Then, GlotLID predicts the language of each sentence independently. We retain only those documents where all sentences are identified as belonging to the same language, discarding any text segment that exhibits code-switching or multilingual content. We finally select data for 15 languages across our three categories: altruistic (6 languages, 92M tokens), selfish (6 languages, 100M tokens), and stagnant (5 languages including English, 87M tokens), resulting in total of approximately 279 million tokens. Code Data We incorporate code data from The Stack (Kocetkov et al., 2022), following the pre-processing strategy used in EMMA-500 (Ji et al., 2024a). The dataset is first filtered to retain high-quality source files, with focus on data science-related code and the 32 most commonly used general-purpose programming languages. Additionally, we include LLVM code due to its importance in multilingual code generation (Paul et al., 2024; Szafraniec et al., 2022). For training configurations that include code, we maintain 2:1 ratio between textual (monolingual/bilingual) and code data, with code comprising about 33% of the total tokens. This aligns with prior work (Aryabumi et al., 2024), which recommends 25% code proportion (text:code 3:1) for balancing language and code performance, noting that 33% remains reasonable for enhancing reasoning tasks. We sample the code dataset down to 50 million tokens, matching the 100 million tokens of textual data. 2.3 Base Models We evaluate across three open-source multilingual LLMs with diverse training recipes: Llama-3.1-8B (Dubey et al., 2024) is pretrained on approximately 15 trillion tokens from diverse, multilingual sources. Its extensive multilingual pretraining and high capacity make it ideal for analyzing CPT effects on well-trained models. Llama-2-7B (Touvron et al., 2023) is pretrained on 2 trillion tokens, covering broad yet less multilingual data distribution. It provides baseline to evaluate CPT effectiveness on English-centric models commonly used in multilingual adaptation research. Viking-7B (Luukkonen et al., 2025) is pretrained mainly on Nordic languages, English, and code, offering insights into how CPT impacts models initially trained on narrower, region-specific data. 2.4 CPT Configurations We train models under 4 CPT configurations across 3 base models and 3 language categories, resulting in total of 36 models. Each model is named using the format: Model-Data[+Code]-LangCat where: Model {L3 (Llama-3.1-8B), L2 (Llama-2-7B), V7 (Viking-7B)} Data {Mono (Monolingual), Bi (Bilingual)} 5https://commoncrawl.org/ 4 Preprint. Under review. Code (optional) is added if code data is included LangCat {Alt (Altruistic), Sel (Selfish), Stag (Stagnant)} For example, L3-Mono-Alt refers to Llama-3.1-8B trained on monolingual data for altruistic languages, while L2-Bi+Code-Sel denotes Llama-2-7B trained on bilingual parallel texts in selfish languages and code data. Base Model Category Mono Bi Mono+Code Bi+Code Training Data Llama-3.1-8B Llama-2-7B Viking-7B L3-Bi-Alt L3-Bi-Sel L3-Mono+Code-Alt L3-Mono+Code-Sel L3-Bi+Code-Alt Altruistic L3-Mono-Alt Selfish L3-Bi+Code-Sel L3-Mono-Sel Stagnant L3-Mono-Stag L3-Bi-Stag L3-Mono+Code-Stag L3-Bi+Code-Stag L2-Bi+Code-Alt Altruistic L2-Mono-Alt Selfish L2-Bi+Code-Sel L2-Mono-Sel Stagnant L2-Mono-Stag L2-Bi-Stag L2-Mono+Code-Stag L2-Bi+Code-Stag Altruistic V7-Mono-Alt V7-Bi-Alt V7-Mono+Code-Alt V7-Bi+Code-Alt Selfish V7-Bi+Code-Sel Stagnant V7-Mono-Stag V7-Bi-Stag V7-Mono+Code-Stag V7-Bi+Code-Stag L2-Mono+Code-Alt L2-Mono+Code-Sel L2-Bi-Alt L2-Bi-Sel V7-Mono+Code-Sel V7-Mono-Sel V7-Bi-Sel Table 2: Continual pretraining configurations with structured naming. Each model is trained for 2 epochs on cluster with 4 AMD MI250X GPUs (8 Graphics Compute Dies) on each node. Training data is organized by language category (altruistic, selfish, stagnant), with all languages within category (e.g., altruistic: zho Hani, ceb Latn, etc.) mixed into single dataset per configuration (e.g., monolingual, bilingual+code). As for software, we use the LLaMA-Factory (Zheng et al., 2024b) framework with DeepSpeed (Rajbhandari et al., 2020) ZeRO-3 config. The hyperparameter setup includes per-device batch size of 8 with gradient accumulation steps of 2. We use cosine learning rate scheduler with an initial learning rate of 4.0 105 and warmup ratio of 0.03."
        },
        {
            "title": "3 Evaluation and Discussion",
            "content": "3.1 Benchmarks and Setup We evaluate our models on two highly multilingual benchmarks covering classification and generation task: SIB-200 (Adelani et al., 2024) for topic classification and FLORES200 (Costa-juss`a et al., 2022; Goyal et al., 2022; Guzman et al., 2019) for machine translation. Classification focuses on whether CPT improves the multilingual models understanding within single language, while translation studies the alignment between languages that emerges with multilingual CPT. All experiments use consistent 3-shot prompting setup. SIB-200 SIB-200 is multilingual news topic classification benchmark covering 200 languages. The task involves classifying news headlines into one of the following predefined categories: science/technology, travel, politics, sports, health, entertainment, and geography. The model predicts by ranking logits for each category, and accuracy measures performance across languages. FLORES-200 FLORES-200 evaluates multilingual translation performance across diverse language pairs. Translations are generated using the vLLM (Kwon et al., 2023) inference engine. BLEU (Papineni et al., 2002) scores, computed via SacreBLEU (Post, 2018) with the flores200 tokenizer, quantify translation quality. 6BLEU signature: nrefs:1case:mixedeff:notok:flores200smooth:expversion:2.4.2 5 Preprint. Under review. 3.2 Effect of Monolingual and Bilingual Continual Pretraining This section shows that bilingual CPT hampers generation due to language mixing but excels in classification for mediumand low-resource languages over monolingual CPT."
        },
        {
            "title": "3.2.1 Language Mixing in Generation Tasks",
            "content": "The FLORES-200 translation task revealed significant language mixing issues in models trained with bilingual translation data. Specifically, when generating translations between language pairs, models frequently appended unintended language tokens to the output. For example, when translating from English (eng Latn) to Chinese (zho Hani), models trained on bilingual data produced outputs like: The text with green background represents the desired Chinese translation, while the text with an orange background contains nonsensical multilingual fragments. This phenomenon occurred consistently across bilingual CPT configurations, suggesting that the parallel data format ([Lang1]: xxx [Lang2]: yyy) encourages cross-lingual interference. More examples are in Figure 6 in Appendix A.5. This language inconsistency leads to significant translation quality degradation, as shown in Figure 1. Bilingual CPT configurations underperform monolingual CPT across all resource levels and base models. For high-resource languages, Llama-3.1-8B achieves only 7.47 BLEU with bilingual CPT versus 25.52 with monolingual CPT (-71% relative), while Llama-2-7B shows similar disparities (14.12 vs 24.60, -43%). The pattern persists for midand low-resource languages, with bilingual CPT consistently lagging behind monolingual CPT. Notably, monolingual CPT often matches or exceeds baseline performance, whereas bilingual CPT only exceeds baseline in specific cases, such as Llama-2-7B on midand low-resource languages. Appendix A.4 presents the detailed results on each language. Figure 1: FLORES-200 X-Eng BLEU score comparing bilingual and monolingual CPT across high-, mid-, and low-resource languages. 3.2.2 Comparative Analysis in Classification Tasks To isolate the effects of CPT strategies without interference from language mixing, we evaluate SIB-200 classification accuracy. Figure 2 shows the average accuracy aggregated across models trained separately on altruistic, selfish, and stagnant languages, grouped by resource level. High-Resource Languages For high-resource languages, both monolingual and bilingual CPT degrade performance across all base models compared to their respective baselines. Llama-3.1-8B, despite its strong baseline (76.63%), exhibits drops with bilingual CPT (71.41%, -6.8% relative) and monolingual CPT (64.21%, -16.2%). Llama-2-7B shows significant declines with both strategies: bilingual CPT reduces accuracy to 31.54% (vs baseline 37.75%, -16.5%), 6 Preprint. Under review. Figure 2: SIB-200 classification accuracy comparing monolingual and bilingual CPT across high-, mid-, and low-resource languages. while monolingual CPT performs similarly (31.86%, -15.6%). Viking-7B partially escapes this trend, with bilingual CPT achieving marginal gains (36.60% vs baseline 34.80%, +5.2%), though monolingual CPT underperforms (25.98%, -25.3%). This suggests that high-resource languages generally do not benefit from CPT, likely due to interference with existing strong representations in pretrained models. However, model-specific factors, such as whether the models pretraining data aligns well with the target languages in CPT, may enable limited improvements in certain cases. For example, Viking-7B, which was pretrained primarily on Nordic languages and English, may benefit more from bilingual CPT due to its ability to leverage cross-lingual transfer between related languages. Mid-Resource Languages Mid-resource languages show mixed trends. Llama-3.1-8B maintains near-baseline performance with bilingual CPT (64.05% vs baseline 65.85%, -2.7%), but monolingual CPT degrades significantly (52.53%, -20.2%). Llama-2-7B struggles across both configurations, with bilingual CPT reducing accuracy to 24.26% (vs baseline 20.59%, +17.8%) and monolingual CPT performing slightly better (26.80%, +30.2%). Viking-7B uniquely benefits from bilingual CPT (29.74% vs baseline 23.94%, +24.2%), while monolingual CPT underperforms (24.02%, +0.3%). This indicates that bilingual CPT can stabilize mid-resource language performance for certain models (e.g., Viking-7B and Llama-2-7B). However, monolingual CPT risks overfitting to limited in-language data, particularly for models with weaker pretraining (e.g., Llama-3.1-8B). Low-Resource Languages Low-resource languages exhibit divergent patterns. Llama3.1-8B improves with bilingual CPT (62.91% vs baseline 56.86%, +10.6%) but declines with monolingual CPT (52.04%, -8.5%). Llama-2-7B degrades significantly with bilingual CPT (20.84%, +19.8%) and shows minimal gains with monolingual CPT (24.51%, +40.9%). Viking7B benefits substantially from bilingual CPT (29.25%, +66.5%), while monolingual CPT slightly underperforms (21.33%, +21.4%). This highlights that bilingual CPT can enhance low-resource language performance for models with compatible pretraining (e.g., Viking-7B and Llama-3.1-8B). 3.3 Effect of Including Code Data The integration of code data during monolingual CPT shows task-dependent effects, enhancing classification performance while introducing tradeoffs in generation quality. Figure 3 and Figure 4 compare monolingual CPT with and without code data across resource levels and tasks, revealing key patterns in how code data influences multilingual adaptation. Code integration consistently improves classification accuracy across all resource levels and models. For high-resource languages, Llama-3.1-8B shows marginal gains (64.21% to 68.47%, +6.7% relative to baseline 76.63%), while Llama-2-7B and Viking-7B exhibit more substantial improvements (42.48% vs 31.86%, +33.3%; 30.88% vs 25.98%, +18.8%). Mid-resource languages benefit even more, with Llama-3.1-8B recovering near-baseline performance (52.53% to 62.83%, -4.6% vs baseline 65.85%) and Llama-2-7B achieving significant gains (34.40% vs 26.80%, +67.0%). Low-resource languages see the most pronounced improvements, partic7 Preprint. Under review. ularly for Viking-7B (28.68% vs 21.33%, +63.2% relative to baseline 17.57%). This pattern extends to bilingual CPT configurations (see Appendix A.2). In contrast, code integration often degrades translation quality, particularly for high-resource languages. Llama-3.1-8B shows slight degradation (25.52 BLEU to 25.35, -0.7% vs baseline 27.97), while Llama-2-7B and Viking-7B exhibit gains (25.05 vs 24.60, +8.6%; 11.69 vs 9.18, +27.3%). Mid-resource languages show mixed trends, with Llama-3.1-8B experiencing slight drop (17.62 vs 18.59, -5.2%) and Viking-7B improving significantly (4.21 vs 3.58, +52.0%). Low-resource languages partially escape this trend, with Viking-7B showing substantial gains (2.84 vs 2.31, +37.4%). The benefits of code integration are most pronounced for low-resource languages, where it acts as scaffold to improve classification accuracy (avg. +25.1%) and partially mitigate generation deficits. Mid-resource languages also benefit, though to lesser extent, while high-resource languages see diminishing returns, with classification gains (e.g., Llama-3.18B: +6.7%) offset by generation losses. Figure 3: SIB-200 classification accuracy comparing monolingual and monolingual+code CPT across high-, mid-, and low-resource languages. Figure 4: FLORES-200 X-Eng BLEU score comparing monolingual and monolingual+code CPT across high-, mid-, and low-resource languages. 3.4 Validation of Language Category Hypotheses This section evaluates the validity of the altruistic, selfish, and stagnant language classifications proposed in prior work (Yuan et al., 2024). We evaluate each model (e.g., L3-Mono-Alt) trained on language category (e.g., altruistic languages: zho Hani, ceb Latn, etc.) and measure SIB-200 classification accuracy changes on both the trained languages and their related languages (e.g., yue Hant, tgl Latn, etc.), as defined in Table 1. We analyze whether CPT strategies align with these hypothesized behaviors. Table 3 reports accuracy changes (%) relative to base models. Altruistic languages can also be selfish or mutually harmful The altruistic hypothesis indicates that training in altruistic languages enhances multilingual performance (related languages) with minimal impact on their own performance (trained languages). Our results 8 Preprint. Under review. Table 3: SIB-200 classification accuracy changes (%) for training and related languages across altruistic, selfish, and stagnant categories. Results are reported relative to base models, with Met column to indicate whether the hypothesis is met or contradicted. Model L2-BiL2-Bi+CodeL2-MonoL2-Mono+CodeL3-BiL3-Bi+CodeL3-MonoL3-Mono+CodeV7-BiV7-Bi+CodeV7-MonoV7-Mono+CodeAltruistic Languages Selfish Languages Stagnant Languages Training Related Met? Training Related Met? Training Related Met? +7.08 +62.37 -14.60 +50.43 +4.46 +1.64 -24.37 -1.78 -11.41 +22.82 -8.22 +5.93 -22.55 +28.31 -31.32 +19.04 -4.46 -7.70 -31.26 -11.13 -31.95 -9.35 -19.74 -11. No No No No No No No No No No No No +12.33 +52.32 +53.18 +52.32 -7.90 -5.85 -9.07 +2.49 +19.24 +17.57 +5.86 +53.96 +2.90 +31.67 +21.94 +26.29 -19.54 -15.66 -19.84 -10.85 -10.22 -16.35 -33.45 +8.18 Yes No No No No No No No No No No Yes +5.88 +26.13 +31.36 +64.02 +14.76 +21.81 -7.71 0.00 +78.18 +11.16 0.00 +21.31 -9.99 +6.25 -8.33 +14.57 -28.43 -28.04 -43.54 -37.01 +26.32 -19.36 -0.83 +17. No No No No No No No No No No Yes No reveal three critical contradictions: (1) 83% of configurations (10/12) degraded related language performance, with code-free CPT causing up to -31.32% accuracy (L2-Mono-Alt); (2) Contrary to minimal self-impact, trained language accuracy fluctuated wildly (+62.37% in L2-Bi+Code-Alt vs. -24.37% in L3-Mono-Alt); These bidirectional effects challenge the unidirectional altruism assumption. Selfish languages exhibit conditional isolation only in certain cases While the selfish hypothesis suggests trained languages primarily improve their own performance (trained languages) while minimally affecting others (related languages), we find this only holds in specific configurations: (1) Non-code bilingual training (L2-Bi-Sel) showed minimal impact on related languages (+2.90%); (2) Code-augmented monolingual training (V7Mono+Code-Sel) achieved strong self-improvement (+53.96%) with moderate spillover (+8.18%). However, 83% of cases (10/12) violated the hypothesis through either negative spillover (V7-Mono-Sel: -33.45%) or excessive cross-lingual transfer (L2-Bi+Code-Sel: +31.67%). Stagnant languages demonstrate more adaptability than expected Stagnant languages neither improve their own performance (trained languages) nor influence others Contrary to their purported stagnation, 92% of configurations (11/12) induced significant performance shifts: (1) Bilingual training boosted trained languages by +78.18% (V7-Bi-Stag) while improving related languages (+26.13%); (2) Monolingual+code CPT (L2-Mono+Code-Stag) achieved +64% self-improvement with +14.76% cross-lingual gains. Only V7-Mono-Stag showed true stagnation (+0.00% trained, -0.83% related). This reveals that most stagnant languages possess untapped adaptation potential under proper CPT strategies."
        },
        {
            "title": "4 Conclusion",
            "content": "In this study, we systematically evaluated the effects of multilingual CPT strategies, including monolingual, bilingual, and code-augmented configurations, across diverse resource levels and language categories. Through experiments with 36 configurations involving three multilingual base models and over 30 languages, we identified several critical insights: First, while bilingual CPT enhances classification accuracy for midand low-resource languages, it introduces language mixing during generation, limiting its utility for translation tasks. Second, code integration during CPT acts as scaffold for low-resource language understanding but introduces task-dependent trade-offs, improving classification while slightly degrading generation quality. Third, we demonstrate that language classifications based on cross-lingual transfer patterns (altruistic, selfish, stagnant) fail to generalize under varying CPT strategies. 9 Preprint. Under review. Overall, our work underscores the complexity of multilingual representation learning and highlights the need for flexible frameworks for language categorization and training strategy selection. Future research should focus on developing more adaptive CPT methods that balance classification improvements and generation quality, further bridging language disparities in large language models."
        },
        {
            "title": "Ethics Statement",
            "content": "This research focuses on reducing the digital language divide and improving inclusivity for underrepresented languages. We acknowledge potential biases due to uneven data distribution and strive to mitigate them by including diverse languages across resource levels. All datasets used are publicly available and preprocessed to ensure integrity and monolingual consistency. All the models trained in this paper are strictly for research purposes and are not intended to be deployed in real-world applications. We encourage further work to address ethical challenges in multilingual NLP, especially for underrepresented languages."
        },
        {
            "title": "Reproducibility Statement",
            "content": "To ensure reproducibility, we release: Model Checkpoints: All models trained under various configurations (monolingual, bilingual, code-augmented) across base models (Llama-3.1-8B, Llama-2-7B, Viking-7B) and language categories (altruistic, selfish, stagnant). Processed Dataset: Filtered subsets of Lego-MT, NLLB, MADLAD-400, and code data. Scripts: Data cleaning, training, and evaluation scripts, including LLaMA-Factory with DeepSpeed ZeRO-3 configuration. All resources are available at https://mala-lm.github.io/MixCPT.html."
        },
        {
            "title": "References",
            "content": "David Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba Alabi, Yanke Mao, Haonan Gao, and En-Shiun Lee. SIB-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 226245, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. eacl-long.14/. Duarte Alves, Jose Pombal, Nuno Guerreiro, Pedro Martins, Joao Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, et al. Tower: An open multilingual large language model for translation-related tasks. arXiv preprint arXiv:2402.17733, 2024. Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Ust un, and Sara Hooker. To code, or not to code? exploring impact of code in pre-training. arXiv preprint arXiv:2408.10914, 2024. Mikko Aulamo, Sami Virpioja, and org Tiedemann. OpusFilter: configurable parIn Proceedings of the 58th Annual Meeting of the Assoallel corpus filtering toolbox. ciation for Computational Linguistics: System Demonstrations, pp. 150156. Association for Computational Linguistics, July 2020. doi: 10.18653/v1/2020.acl-demos.20. URL https://www.aclweb.org/anthology/2020.acl-demos.20. Steven Bird and Edward Loper. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pp. 214217, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/P04-3031/. Preprint. Under review. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Marta Costa-juss`a, James Cross, Onur elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, and Naoaki Okazaki. Continual pre-training for cross-lingual llm adaptation: Enhancing japanese language capabilities. arXiv preprint arXiv:2404.17790, 2024. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzman, and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538, 2022. doi: 10.1162/ tacl 00474. URL https://aclanthology.org/2022.tacl-1.30/. Francisco Guzman, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, and MarcAurelio Ranzato. The FLORES evaluation datasets for lowresource machine translation: NepaliEnglish and SinhalaEnglish. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 60986111, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1632. URL https://aclanthology.org/D19-1632/. Kevin Heffernan, Onur elebi, and Holger Schwenk. Bitext mining using distilled sentence representations for low-resource languages. arXiv preprint arXiv:2205.12654, 2022. Masanori Hirano and Kentaro Imajo. The construction of instruction-tuned llms for finance without instruction data using continual pretraining and model merging. arXiv preprint arXiv:2409.19854, 2024. Shaoxiong Ji, Zihao Li, Indraneil Paul, Jaakko Paavola, Peiqin Lin, Pinzhen Chen, Dayyan OBrien, Hengyu Luo, Hinrich Sch utze, org Tiedemann, et al. Emma-500: Enhancing massively multilingual adaptation of large language models. arXiv preprint arXiv:2409.17892, 2024a. Shaoxiong Ji, Timothee Mickus, Vincent Segonne, and org Tiedemann. Can machine translation bridge multilingual pretraining and cross-lingual transfer learning? In Proceedings of LREC-COLING, 2024b. Mihir Kale, Aditya Siddhant, Rami Al-Rfou, Linting Xue, Noah Constant, and Melvin Johnson. nmT5 - is parallel data still relevant for pre-training massively multilingual language models? In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 683691, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.87. URL https://aclanthology.org/2021.acl-short.87/. Amir Hossein Kargaran, Ayyoob Imani, Francois Yvon, and Hinrich Schuetze. GlotLID: In Houda Bouamor, Juan Pino, Language identification for low-resource languages. and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 61556218, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.410. URL https://aclanthology.org/2023. findings-emnlp.410/. 11 Preprint. Under review. Najoung Kim, Sebastian Schuster, and Shubham Toshniwal. Code pretraining improves entity tracking abilities of language models. arXiv preprint arXiv:2405.21068, 2024. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu noz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. Madlad-400: multilingual and documentlevel large audited dataset. Advances in Neural Information Processing Systems, 36, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Zihao Li, Yucheng Shi, Zirui Liu, Fan Yang, Ali Payani, Ninghao Liu, and Mengnan Du. Quantifying multilingual performance of large language models across languages. arXiv preprint arXiv:2404.11553, 2024. Peiqin Lin, Andre FT Martins, and Hinrich Sch utze. recipe of parallel corpora exploitation for multilingual large language models. arXiv preprint arXiv:2407.00436, 2024. Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei Yuan. LLaMAX: Scaling linguistic horizons of LLM by enhancing translation capabilities beyond 100 languages. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1074810772, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp. 631. URL https://aclanthology.org/2024.findings-emnlp.631/. Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Ville Komulainen, Peter Sarlin, and Sampo Pyysalo. Viking: family of nordic llms, 2025. URL https://huggingface.co/LumiOpen/ Viking-33B. Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. At which training stage does code data help llms reasoning? arXiv preprint arXiv:2309.16298, 2023. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128, 2022. Mitodru Niyogi and Arnab Bhattacharya. Paramanu-ayn: Pretrain from scratch or continual pretraining of llms for legal domain adaptation? arXiv preprint arXiv:2403.13681, 2024. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Indraneil Paul, Goran GlavaËs, and Iryna Gurevych. IRCoder: Intermediate representations make language models robust multilingual code generators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1502315041, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 802. URL https://aclanthology.org/2024.acl-long.802/. Jackson Petty, Sjoerd van Steenkiste, and Tal Linzen. How does code pretraining affect language model task performance? arXiv preprint arXiv:2409.04556, 2024. Matt Post. call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018. Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Alec Radford, OpenAI, Sutskever. Language models are unsupervised multitask learners. 2019. URL https://cdn.openai.com/better-language-models/language models are unsupervised multitask learners.pdf. Preprint. Under review. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Leonardo Ranaldi, Giulia Pucci, and Andre Freitas. Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 79617973, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.473. URL https://aclanthology.org/2024.findings-acl.473/. Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 64906500, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long. 507. URL https://aclanthology.org/2021.acl-long.507/. Marc Szafraniec, Baptiste Roziere, Hugh Leather, Francois Charton, Patrick Labatut, and arXiv preprint Gabriel Synnaeve. Code translation with compiler representations. arXiv:2207.03578, 2022. Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. Multilingual translation from denoising pre-training. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 34503466, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.304. URL https://aclanthology.org/2021.findings-acl.304/. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Anh-Dung Vo, Minseong Jung, Wonbeen Lee, and Daewoo Choi. Redwhale: An adapted korean llm through efficient continual pretraining. arXiv preprint arXiv:2408.11294, 2024. gatay YÄ±ldÄ±z, Nishaanth Kanna Ravichandran, Nitin Sharma, Matthias Bethge, and Beyza Ermis. Investigating continual pretraining in large language models: Insights and implications. arXiv preprint arXiv:2402.17400, 2024. Yao-Ching Yu, Tsun-Han Chiang, Cheng-Wei Tsai, Chien-Ming Huang, and Wen-Kwang Tsao. Primus: pioneering collection of open-source datasets for cybersecurity llm training. arXiv preprint arXiv:2502.11191, 2025. Fei Yuan, Yinquan Lu, Wenhao Zhu, Lingpeng Kong, Lei Li, Yu Qiao, and Jingjing Xu. Lego-MT: Learning detachable models for massively multilingual machine translation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1151811533, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.731. URL https://aclanthology.org/2023.findings-acl.731/. Fei Yuan, Shuai Yuan, Zhiyong Wu, and Lei Li. How vocabulary sharing facilitates multilingualism in LLaMA? In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1211112130, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.721. URL https://aclanthology.org/2024.findings-acl.721/. 13 Preprint. Under review. Wenzhen Zheng, Wenbo Pan, Xu Xu, Libo Qin, Li Yue, and Ming Zhou. Breaking language barriers: Cross-lingual continual pre-training at scale. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 77257738, 2024a. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024b. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Related Work Continual Pretraining Continual pretraining has emerged as pivotal technique for adapting LLMs to new domains or languages while retaining previously acquired knowledge (YÄ±ldÄ±z et al., 2024). This approach has demonstrated significant benefits across diverse domains, including cybersecurity (Yu et al., 2025), finance(Hirano & Imajo, 2024), and law (Niyogi & Bhattacharya, 2024). In the context of language adaptation, researchers have successfully leveraged continual pretraining to enhance performance on lowand mediumresource languages. For instance, Ji et al. (2024a); Lu et al. (2024) extended the capabilities of open-weight LLMs by pretraining them on multilingual datasets encompassing hundreds of languages. Similarly, Fujii et al. (2024) significantly improved Japanese language proficiency by continually pretraining LLama-2 (Touvron et al., 2023) on large-scale Japanese web corpus. In another study, Vo et al. (2024) achieved notable advancements in Korean language processing by utilizing 9.7 billion tokens for continual pretraining. Bilingual Translation Data Incorporating bilingual translation data into pretraining has been shown to enhance multilingual performance, although the benefits tend to diminish as model size increases (Kale et al., 2021). Even relatively small parallel corpora, such as 10,000 sentence pairs, can be as effective as much larger datasets when carefully filtered for quality (Lin et al., 2024). Recent efforts further highlight how strategically leveraging bilingual data can enhance multilingual capabilities. For example, Ranaldi et al. (2024) introduced Translation-following demonstrations to improve semantic alignment between English and other languages during instruction tuning. Their CrossAlpaca models, trained with both instruction and translation data, significantly outperformed monolingual baselines on multilingual QA tasks. Similarly, Alves et al. (2024) showed that including high-quality parallel data during continual pretraining, alongside monolingual data, leads to substantial improvements in translation and related tasks. In contrast to the improvement from training with bilingual translation data, Ji et al. (2024b) found that utilizing bilingual translation to enforce sentence-level alignment during continual pretraining actually hinders cross-lingual transfer based on the study on mBART (Tang et al., 2021). Code Data in Language Model Training Including code in pretraining data has become common practice, even for models not specifically designed for code generation (Chen et al., 2021). Recent studies show that code data not only improves performance on programming tasks but also enhances general capabilities such as natural language reasoning, entity tracking, and commonsense understanding (Aryabumi et al., 2024). For instance, models trained with code exhibit stronger performance in structured reasoning tasks (Madaan et al., 2022) and demonstrate better entity tracking compared to purely text-trained counterparts (Kim et al., 2024). Furthermore, adding high-quality or synthetic code during pretraining or cooldown leads to consistent gains across wide range of benchmarks (Aryabumi et al., 2024). Systematic experiments also suggest that mixing code data during both pretraining and instruction tuning stages leads to better reasoning abilities without harming performance on non-code tasks (Ma et al., 2023). 14 Preprint. Under review. A.2 Additional Results on Bilingual CPT with Code Data Figure 5 shows the impact of adding code data to bilingual CPT configurations for the SIB-200 classification task. While Section 3.3 in the main text focuses on monolingual CPT comparisons, the results in this section demonstrate that code integration also benefits bilingual CPT across most models and language resource levels for natural language understanding. For high-resource languages, the improvements are modest but consistent: Llama-3.1-8B increases from 71.41% to 72.39% (+1.4% relative), Viking-7B from 36.60% to 39.21% (+7.1%), and Llama-2-7B shows the largest gain (31.54% to 38.56%, +22.3%). Midresource languages exhibit similar patterns, with Llama-2-7B improving from 24.26% to 33.50% (+38.0%) and Llama-3.1-8B from 64.05% to 65.77% (+2.7%). Notably, Viking-7B shows slight degradation (29.74% to 26.96%, -9.3%), suggesting model-specific sensitivities to code interference in this configuration. The most significant benefits emerge for low-resource languages, Llama-2-7B improves from 20.84% to 28.51% (+36.8% relative), outperforming its baseline of 17.40%. Llama-3.1-8B sees moderate gain (62.91% to 64.54%, +2.6%), while Viking-7B experiences slight decline (29.25% to 24.84%, -15.1%). For detailed per-language results on the SIB-200 benchmark, refer to Appendix A.3 We intentionally omit FLORES-200 comparisons between bilingual and bilingual+code configurations because the fundamental language mixing issue identified in generation tasks as described in Section 3.2.1 makes this comparison nonsensical. As reference, per-language BLEU scores are available in Appendix A.4. Figure 5: SIB-200 classification accuracy comparing bilingual and bilingual+code CPT across high-, mid-, and low-resource languages. A.3 SIB-200 Accuracy The SIB-200 accuracy results are detailed across language categories: Table 4 presents scores for altruistic languages, Table 5 for selfish languages, and Table 6 for stagnant languages, covering various models and languages within each category. Model L2-Bi-Alt L2-Bi+Code-Alt L2-Mono-Alt L2-Mono+Code-Alt Llama-2-7B (Base) L3-Bi-Alt L3-Bi+Code-Alt L3-Mono-Alt L3-Mono+Code-Alt Llama-3.1-8B (Base) V7-Bi-Alt V7-Bi+Code-Alt V7-Mono-Alt V7-Mono+Code-Alt Viking-7B (Base) zho Hans mar Deva 0.2598 0.3529 0.1765 0.3529 0.3382 0.7500 0.7157 0.6176 0.6814 0.7549 0.2206 0.3578 0.2157 0.2157 0.3725 0.2108 0.3284 0.1765 0.3235 0.1765 0.6324 0.6324 0.4510 0.6324 0.6667 0.1814 0.2206 0.1814 0.1814 0.1814 ceb Latn 0.2402 0.4412 0.2108 0.3922 0.2598 0.7010 0.6765 0.4902 0.6814 0.6912 0.2500 0.2843 0.2353 0.2990 0.2206 zul Latn 0.2794 0.3627 0.1814 0.3382 0.1569 0.6569 0.6275 0.5196 0.6765 0.5441 0.1618 0.2451 0.1814 0.2500 0. khm Khmr 0.1961 0.3137 0.2010 0.2598 0.1765 0.7059 0.7010 0.4167 0.5686 0.6422 0.1373 0.2108 0.1716 0.1912 0.1520 eng Latn 0.4216 0.5049 0.2304 0.4167 0.4020 0.7157 0.7353 0.6176 0.7843 0.7843 0.2500 0.3137 0.2843 0.2941 0.3235 hin Deva 0.1667 0.2402 0.1471 0.2598 0.2353 0.6127 0.6422 0.3971 0.5245 0.7010 0.1127 0.1569 0.1618 0.1569 0.1765 tgl Latn 0.1912 0.3873 0.1765 0.3235 0.2647 0.6814 0.6520 0.4265 0.6127 0.7255 0.2353 0.2451 0.1618 0.2353 0.2010 xho Latn 0.2010 0.3088 0.1667 0.2843 0.1716 0.5637 0.5049 0.4069 0.5147 0.5392 0.1814 0.2206 0.1618 0.2108 0.1814 vie Latn 0.1961 0.3382 0.2010 0.3284 0.3088 0.6225 0.6814 0.5343 0.6961 0.7500 0.1422 0.1569 0.2402 0.1863 0. ilo Latn 0.1618 0.3039 0.1618 0.2990 0.2402 0.6471 0.5931 0.3775 0.5637 0.6765 0.1814 0.2402 0.2157 0.2255 0.2500 npi Deva 0.1716 0.2451 0.1471 0.2108 0.2402 0.5686 0.5441 0.4020 0.4804 0.6520 0.0931 0.1716 0.1716 0.1569 0.1961 yue Hant 0.2255 0.3725 0.1863 0.3186 0.3333 0.7647 0.6912 0.6422 0.7255 0.7647 0.1814 0.3186 0.2206 0.2451 0.4118 ssw Latn 0.2010 0.3137 0.1569 0.3039 0.1618 0.5882 0.5686 0.4461 0.5784 0.4755 0.1569 0.2010 0.1814 0.2500 0.1520 Table 4: SIB-200 task accuracy for Altruistic languages across all models. Training language columns have shaded background. Preprint. Under review. Model L2-Bi-Sel L2-Bi+Code-Sel L2-Mono-Sel L2-Mono+Code-Sel Llama-2-7B (Base) L3-Bi-Sel L3-Bi+Code-Sel L3-Mono-Sel L3-Mono+Code-Sel Llama-3.1-8B (Base) V7-Bi-Sel V7-Bi+Code-Sel V7-Mono-Sel V7-Mono+Code-Sel Viking-7B (Base) deu Latn 0.3088 0.4265 0.4412 0.4412 0.3922 0.7206 0.6863 0.7059 0.7451 0.7598 0.3088 0.3039 0.2549 0.3431 0.3480 bel Cyrl mri Latn 0.3186 0.3922 0.4020 0.3775 0.2157 0.6078 0.6127 0.5833 0.6863 0.7206 0.2745 0.3529 0.3627 0.3676 0.2843 0.2500 0.3186 0.2647 0.3186 0.1912 0.5784 0.6078 0.5980 0.6471 0.6029 0.2255 0.2206 0.1814 0.3578 0. kir Cyrl 0.1814 0.2843 0.3775 0.3186 0.1765 0.5735 0.6127 0.5833 0.7108 0.7157 0.2549 0.2451 0.2059 0.3578 0.2059 nya Latn 0.2353 0.3333 0.2794 0.2990 0.1765 0.6078 0.6373 0.5784 0.6471 0.5539 0.3333 0.2549 0.2353 0.3775 0.1667 eng Latn 0.4167 0.5098 0.4461 0.4412 0.4020 0.7451 0.6716 0.6520 0.7108 0.7843 0.3578 0.3676 0.2745 0.3922 0.3235 fij Latn 0.1618 0.2206 0.1765 0.2206 0.1765 0.3824 0.4118 0.3971 0.3775 0.4559 0.2255 0.1716 0.1520 0.2402 0.1961 bak Cyrl 0.1569 0.2500 0.2549 0.2843 0.1912 0.5294 0.5294 0.5147 0.5784 0.6961 0.2157 0.1961 0.1814 0.2500 0.2451 dan Latn 0.3578 0.4461 0.4167 0.3873 0.3627 0.6569 0.6618 0.6618 0.7402 0.7451 0.2990 0.3039 0.2108 0.3873 0. rus Cyrl 0.3578 0.4069 0.3725 0.3676 0.2892 0.6373 0.6569 0.5637 0.7010 0.7157 0.2598 0.3039 0.2059 0.3382 0.3529 smo Latn 0.1765 0.2206 0.1765 0.2059 0.1765 0.3627 0.3824 0.4363 0.3627 0.5931 0.1961 0.1716 0.1667 0.2206 0.1961 bem Latn 0.1618 0.2206 0.1912 0.2451 0.1716 0.3627 0.4461 0.4167 0.4069 0.4559 0.2108 0.1814 0.1520 0.2598 0.1569 kaz Cyrl 0.1569 0.2794 0.2696 0.2696 0.1667 0.5343 0.6029 0.5686 0.6618 0.7304 0.2157 0.2010 0.1618 0.2745 0.2108 sna Latn 0.1618 0.2402 0.2010 0.2255 0.1618 0.3627 0.3676 0.3775 0.3725 0.4706 0.2010 0.2010 0.1520 0.2451 0.1618 ukr Cyrl 0.3235 0.3922 0.4069 0.3627 0.2941 0.6225 0.6029 0.5196 0.7304 0.7402 0.2451 0.2549 0.1912 0.3186 0. nld Latn 0.4216 0.4412 0.4216 0.4216 0.3775 0.6373 0.6716 0.6127 0.7059 0.7206 0.2990 0.2206 0.1814 0.3186 0.4020 Table 5: SIB-200 task accuracy for Selfish languages across all models. Training language columns have shaded background. Model L2-Bi-Stag L2-Bi+Code-Stag L2-Mono-Stag L2-Mono+Code-Stag Llama-2-7B (Base) L3-Bi-Stag L3-Bi+Code-Stag L3-Mono-Stag L3-Mono+Code-Stag Llama-3.1-8B (Base) V7-Bi-Stag V7-Bi+Code-Stag V7-Mono-Stag V7-Mono+Code-Stag Viking-7B (Base) tha Thai 0.2598 0.3186 0.3137 0.3480 0.2353 0.7157 0.7696 0.5784 0.5637 0.7451 0.4412 0.2647 0.2549 0.3480 0.3725 yor Latn sna Latn wol Latn nya Latn zul Latn shn Mymr bam Latn hau Latn ibo Latn lao Laoo 0.1765 0.2108 0.2402 0.3039 0.1569 0.6078 0.6471 0.4510 0.5588 0.5245 0.4118 0.2745 0.2255 0.2794 0. 0.1961 0.2255 0.2549 0.3529 0.1618 0.6667 0.6520 0.5539 0.5882 0.4706 0.4461 0.2745 0.2598 0.3284 0.1618 0.1618 0.1912 0.1765 0.2255 0.1961 0.5637 0.6422 0.4706 0.5147 0.4853 0.4216 0.2598 0.2255 0.2157 0.2206 0.1569 0.1912 0.1618 0.2255 0.1765 0.5441 0.5490 0.3431 0.4167 0.5539 0.2647 0.1569 0.2010 0.2206 0.1667 0.1471 0.1814 0.1618 0.1961 0.1569 0.3971 0.4069 0.3480 0.3922 0.5441 0.2206 0.1471 0.1912 0.1814 0.1471 0.1569 0.1863 0.1471 0.1814 0.1863 0.3382 0.3284 0.3137 0.2990 0.4657 0.1569 0.1225 0.1422 0.2059 0.1863 0.1471 0.1618 0.1618 0.1912 0.1667 0.3431 0.3529 0.3039 0.3333 0.3971 0.2647 0.1667 0.1912 0.2304 0. 0.1471 0.1618 0.1520 0.1961 0.1667 0.3382 0.4118 0.3235 0.3480 0.6716 0.2255 0.1422 0.1814 0.1814 0.1569 0.1520 0.2010 0.1569 0.2010 0.1667 0.4020 0.4118 0.2696 0.3676 0.6520 0.2059 0.1176 0.1765 0.1912 0.1667 0.1520 0.1667 0.1373 0.1569 0.1569 0.3775 0.2941 0.2598 0.2549 0.5441 0.1667 0.1078 0.0980 0.1863 0.2010 Table 6: SIB-200 task accuracy for Stagnant languages across all models. Training language columns have shaded background. A.4 FLORES-200 BLEU Scores The BLEU scores for the FLORES-200 benchmark are detailed across language categories and translation directions: Tables 7 and 8 present scores for altruistic languages (Eng-X and X-Eng, respectively), Tables 9 and 10 for selfish languages (Eng-X and X-Eng), and Tables 11 and 12 for stagnant languages (Eng-X and X-Eng) Language Pair eng Latn-zho Hans eng Latn-ceb Latn eng Latn-mar Deva eng Latn-zul Latn eng Latn-khm Khmr eng Latn-npi Deva eng Latn-vie Latn eng Latn-tgl Latn eng Latn-ssw Latn eng Latn-xho Latn eng Latn-yue Hant eng Latn-ilo Latn eng Latn-hin Deva L2-Bi-Alt 9.62 19.37 8.44 6.56 3.03 1.40 6.15 5.81 3.34 3.86 6.81 3.55 2.09 L2-Bi+Code-Alt 4.10 3.59 14.81 8.31 2.84 2.01 0.71 1.62 3.72 3.71 1.39 1.30 3.14 L2-Mono-Alt 10.23 19.46 8.93 6.54 3.27 1.41 6.83 5.79 3.27 3.44 8.51 3.58 1.96 L2-Mono+Code-Alt 10.13 19.63 8.63 6.54 3.38 1.49 6.47 6.25 3.61 4.02 7.59 3.65 1.79 Llama-2-7B 10.47 5.35 1.39 1.64 0.09 1.53 15.44 7.32 1.54 1.91 8.15 2.97 5. L3-Bi-Alt 2.87 0.75 4.22 6.22 4.69 0.66 0.70 1.23 2.78 2.83 1.26 0.79 1.05 L3-Bi+Code-Alt 5.53 1.51 5.45 6.77 5.02 0.93 0.79 1.43 2.72 2.96 2.80 1.01 1.42 L3-Mono-Alt 17.14 20.81 9.20 9.59 8.46 1.35 13.23 5.67 3.93 4.39 14.58 3.48 3.17 L3-Mono+Code-Alt 17.34 20.37 8.33 9.70 8.30 1.29 16.16 5.67 4.08 4.64 14.54 3.48 2.80 Llama-3.1-8B 24.27 22.72 6.83 26.17 1.76 6.13 26.63 15.14 3.04 3.55 4.63 25.82 24.30 V7-Bi-Alt 0.80 1.95 6.21 12.55 4.13 0.80 0.55 0.98 4.29 5.63 0.37 0.82 1. V7-Bi+Code-Alt 2.07 3.50 7.24 12.62 4.19 0.99 1.03 1.83 4.41 5.44 1.40 1.34 1.48 V7-Mono-Alt 1.18 6.27 0.86 1.63 1.59 0.07 0.09 1.32 0.90 1.14 0.51 0.77 0.14 V7-Mono+Code-Alt 2.10 6.88 1.05 2.07 1.54 0.08 0.29 2.06 0.79 1.01 0.88 0.78 0.18 Viking-7B 9.72 3.66 0.21 0.94 0.07 0.28 5.30 4.23 0.82 1.13 6.50 2.34 1.29 Table 7: FLORES-200 BLEU scores for Altruistic languages (Eng-X). Training language rows have shaded background. A.5 Language Mixing Examples This section supplements the main text with examples of language mixing in bilingual CPT (L3-Bi-), where translations contain unintended multilingual fragments. For comparison, outputs from monolingual CPT (L3-Mono-) are provided, showing cleaner, target-languageonly results. Individual BLEU scores are included to quantify quality. Language mixing reduces BLEU scores by introducing irrelevant tokens that disrupt n-gram precision, as these fragments fail to match the reference translations target-language sequences, lowering overlap, especially for higher-order n-grams like the default 4-grams in SacreBLEU (Post, 2018), where single irrelevant token disrupts multiple overlapping sequences. Figure 6 illustrates the four examples and the translation generated by monolingual and bilingual CPT models. 16 Preprint. Under review. Language Pair zho Hans-eng Latn ceb Latn-eng Latn mar Deva-eng Latn zul Latn-eng Latn vie Latn-eng Latn khm Khmr-eng Latn ssw Latn-eng Latn npi Deva-eng Latn yue Hant-eng Latn tgl Latn-eng Latn hin Deva-eng Latn ilo Latn-eng Latn xho Latn-eng Latn L2-Bi-Alt 18.35 29.85 17.12 19.04 20.99 13.49 8.47 3.25 17.55 13.83 6.62 5.54 9.56 L2-Bi+Code-Alt 13.03 11.16 5.35 8.26 10.85 1.64 3.82 0.94 8.00 7.81 2.13 2.24 4. L2-Mono-Alt 16.85 29.36 16.63 18.28 19.78 12.77 8.04 2.70 16.45 13.95 6.77 5.34 9.00 L2-Mono+Code-Alt 17.97 29.81 17.59 18.81 19.97 13.10 8.80 3.29 17.63 14.52 7.72 5.28 9.70 Llama-2-7B 18.28 9.58 4.09 3.05 20.61 2.06 3.16 4.69 18.66 16.29 12.10 5.67 3.35 L3-Bi-Alt 6.62 8.12 1.79 2.67 8.57 0.76 1.87 1.18 4.52 4.67 2.33 1.23 1.98 L3-Bi+Code-Alt 9.40 10.83 3.67 4.39 9.11 2.41 1.94 1.70 6.94 6.07 3.80 1.77 2.83 L3-Mono-Alt 18.99 29.98 19.52 20.72 21.97 16.49 8.78 6.40 18.94 15.10 16.14 6.06 8. L3-Mono+Code-Alt 19.68 28.10 19.69 20.77 22.70 17.67 8.88 7.10 19.45 14.91 16.38 5.16 9.75 Llama-3.1-8B 22.43 22.67 22.38 8.73 26.12 15.51 6.29 22.81 23.26 28.92 27.20 15.19 8.83 V7-Bi-Alt 0.86 8.52 0.14 0.12 0.08 0.22 0.13 0.04 0.31 0.38 0.05 0.19 0.17 V7-Bi+Code-Alt 4.94 14.92 0.24 0.78 0.19 0.67 0.55 0.20 2.63 2.33 0.11 0.62 0.87 V7-Mono-Alt 2.47 6.74 0.99 3.10 0.13 0.79 1.31 0.18 1.78 1.57 0.31 0.60 1.65 V7-Mono+Code-Alt 3.02 9.15 0.91 4.07 0.40 1.01 1.99 0.26 2.91 2.15 0.21 0.95 2. Viking-7B 16.06 6.03 0.55 2.33 10.32 0.81 2.18 0.75 14.27 6.74 1.04 4.23 2.62 Table 8: FLORES-200 BLEU scores for Altruistic languages (X-Eng). Training language rows have shaded background. Language Pair eng Latn-deu Latn eng Latn-bel Cyrl eng Latn-mri Latn eng Latn-kir Cyrl eng Latn-nya Latn eng Latn-sna Latn eng Latn-bak Cyrl eng Latn-nld Latn eng Latn-kaz Cyrl eng Latn-fij Latn eng Latn-smo Latn eng Latn-rus Cyrl eng Latn-dan Latn eng Latn-ukr Cyrl eng Latn-bem Latn L2-Bi+Code-Sel 18.51 2.63 7.13 4.60 4.40 0.92 1.29 9.08 1.36 1.05 1.66 2.13 7.51 0.59 1.48 L2-Bi-Sel 8.50 1.65 3.60 2.73 3.34 0.61 0.63 2.24 0.63 0.63 0.88 0.76 2.55 0.45 0.91 L2-Mono+Code-Sel 23.16 12.27 4.88 4.01 6.76 1.11 1.48 15.86 1.70 0.91 1.04 13.87 18.75 2.23 1. L2-Mono-Sel 22.85 11.81 3.94 3.76 6.30 1.03 1.31 13.76 1.52 0.65 0.66 12.88 16.45 2.19 0.86 Llama-2-7B 23.96 1.95 2.50 1.71 1.65 1.73 1.67 18.00 1.54 1.75 1.76 21.99 21.74 18.59 1.34 L3-Bi+Code-Sel 11.00 3.26 3.88 3.60 4.65 0.92 1.09 3.37 1.18 0.90 1.00 2.56 4.31 0.61 1.25 L3-Bi-Sel 8.43 0.82 2.88 2.72 3.22 0.65 0.78 1.07 0.79 0.64 0.85 1.01 1.24 0.35 0.95 L3-Mono+Code-Sel 22.19 11.98 5.07 6.51 6.44 1.45 2.57 14.38 2.90 0.83 1.01 16.71 16.89 3.45 1.31 L3-Mono-Sel 24.78 14.12 6.15 7.09 8.06 1.56 2.55 11.25 3.02 0.96 0.90 16.58 15.19 3.23 1. Llama-3.1-8B 27.08 11.23 4.55 0.90 2.98 3.67 7.11 20.31 6.93 3.32 11.34 4.01 1.37 7.14 14.91 V7-Bi+Code-Sel 16.69 0.59 6.43 3.18 8.59 1.24 0.98 1.87 1.08 1.31 1.16 1.24 3.05 0.41 1.13 V7-Bi-Sel 11.15 0.24 4.92 1.53 7.84 1.15 0.49 0.99 0.62 0.89 0.98 0.38 0.80 0.15 1.26 V7-Mono+Code-Sel 12.22 4.00 1.05 1.26 1.59 0.25 0.45 1.87 0.65 0.23 0.18 1.94 2.85 0.40 0.53 V7-Mono-Sel 6.09 0.82 0.50 0.39 0.51 0.11 0.31 0.68 0.36 0.09 0.07 0.44 1.05 0.09 0.19 Viking-7B 20.45 0.98 0.83 0.78 0.86 0.94 0.60 16.44 0.79 1.32 1.09 11.78 38.18 8.87 0. Table 9: FLORES-200 BLEU scores for Selfish languages (Eng-X). Training language rows have shaded background. Language Pair deu Latn-eng Latn bel Cyrl-eng Latn mri Latn-eng Latn kir Cyrl-eng Latn nya Latn-eng Latn ukr Cyrl-eng Latn nld Latn-eng Latn dan Latn-eng Latn rus Cyrl-eng Latn smo Latn-eng Latn bak Cyrl-eng Latn fij Latn-eng Latn kaz Cyrl-eng Latn sna Latn-eng Latn bem Latn-eng Latn L2-Bi+Code-Sel 29.31 15.59 2.69 4.15 1.31 23.49 21.81 31.12 23.10 0.95 1.55 0.74 1.87 0.68 0.81 L2-Bi-Sel 9.88 5.61 0.72 0.99 0.20 7.43 6.73 10.03 7.64 0.38 0.53 0.21 0.62 0.32 0.26 L2-Mono+Code-Sel 32.13 18.95 10.66 10.43 15.84 26.05 24.14 34.53 26.38 3.02 3.66 2.02 4.61 3.77 3.74 L2-Mono-Sel 32.34 18.44 10.40 10.52 16.07 26.35 24.52 34.89 26.13 3.29 3.86 1.94 4.34 3.40 3. Llama-2-7B 27.87 8.78 4.21 3.29 2.66 26.16 20.21 29.78 23.66 2.92 4.07 2.53 3.64 2.90 2.73 L3-Bi+Code-Sel 12.85 7.54 1.55 2.81 2.11 8.75 7.64 10.41 9.04 0.70 1.38 0.38 1.89 0.83 0.60 L3-Bi-Sel 8.32 5.36 0.08 0.19 0.12 7.09 4.94 6.87 7.11 0.06 0.11 0.03 0.19 0.09 0.10 L3-Mono+Code-Sel 31.02 16.67 10.88 13.02 15.48 24.64 21.27 30.69 23.96 2.88 7.26 2.20 8.53 3.27 3.04 L3-Mono-Sel 32.05 18.10 12.22 13.63 17.25 25.77 22.88 31.68 25.05 3.10 6.97 1.90 9.46 3.47 2.84 Llama-3.1-8B 33.51 19.36 11.15 14.98 6.54 30.98 24.35 35.30 27.08 9.34 18.59 4.52 20.01 7.09 4. V7-Bi+Code-Sel 10.51 5.43 0.33 0.86 0.39 2.54 1.47 12.17 5.98 0.10 0.56 0.12 0.56 0.31 0.12 V7-Bi-Sel 2.28 4.89 0.06 0.23 0.10 0.72 0.35 3.50 1.26 0.05 0.18 0.09 0.16 0.07 0.04 V7-Mono+Code-Sel 20.36 7.73 3.20 2.88 4.92 4.72 5.53 24.14 9.05 0.97 1.03 0.71 1.44 1.40 1.30 V7-Mono-Sel 15.89 7.02 1.71 2.32 3.03 4.09 3.37 18.36 7.87 0.43 0.71 0.55 0.88 0.73 0.93 Viking-7B 31.29 3.49 1.86 1.93 2.43 24.78 22.61 39.68 23.83 1.78 1.69 2.07 2.24 2.50 2.42 Table 10: FLORES-200 BLEU scores for Selfish languages (X-Eng). Training language rows have shaded background. Language Pair eng Latn-tha Thai eng Latn-yor Latn eng Latn-sna Latn eng Latn-wol Latn eng Latn-hau Latn eng Latn-shn Mymr eng Latn-nya Latn eng Latn-zul Latn eng Latn-lao Laoo eng Latn-ibo Latn eng Latn-bam Latn L2-Bi+Code-Stag 23.11 1.29 4.07 0.29 0.44 0.25 0.71 0.75 0.25 0.77 0.13 L2-Bi-Stag 21.99 1.15 3.27 0.30 0.52 0.20 0.59 0.69 0.32 0.64 0.12 L2-Mono+Code-Stag 18.06 1.84 5.07 1.05 0.68 0.06 1.30 1.53 0.16 0.71 0.61 L2-Mono-Stag 16.84 1.96 4.83 0.96 0.66 0.11 1.52 1.54 0.24 0.80 0.55 Llama-2-7B 3.60 0.55 1.73 0.97 0.73 0.00 1.65 1.64 0.05 0.56 0. L3-Bi+Code-Stag 10.11 1.08 4.62 0.38 0.72 0.26 0.65 0.79 0.18 0.64 0.31 L3-Bi-Stag 8.48 0.90 3.55 0.25 0.54 0.15 0.59 0.69 0.13 0.54 0.08 L3-Mono+Code-Stag 20.76 2.59 6.74 1.12 1.31 0.12 1.67 1.89 0.18 1.14 0.59 L3-Mono-Stag 21.85 2.57 7.21 1.25 1.40 0.01 1.88 2.13 0.31 1.26 0.63 Llama-3.1-8B 19.44 2.69 3.67 2.24 6.63 0.28 2.98 26.17 3.68 5.45 22.51 V7-Bi+Code-Stag 15.05 2.37 8.67 0.58 0.26 0.11 0.83 0.97 0.22 0.56 0. V7-Bi-Stag 16.23 2.29 10.10 0.55 0.43 0.26 0.94 1.35 0.37 0.63 0.48 V7-Mono+Code-Stag 4.02 0.69 1.37 0.24 0.21 0.07 0.78 0.45 0.19 0.12 0.21 V7-Mono-Stag 3.27 0.67 1.52 0.25 0.32 0.08 0.56 0.42 0.06 0.18 0.17 Viking-7B 2.98 0.60 0.94 0.85 0.87 0.03 0.86 0.94 0.09 0.59 0.20 Table 11: FLORES-200 BLEU scores for Stagnant languages (Eng-X). Training language rows have shaded background. Language Pair tha Thai-eng Latn yor Latn-eng Latn sna Latn-eng Latn wol Latn-eng Latn hau Latn-eng Latn bam Latn-eng Latn shn Mymr-eng Latn nya Latn-eng Latn zul Latn-eng Latn lao Laoo-eng Latn ibo Latn-eng Latn L2-Bi+Code-Stag 1.744 0.181 0.245 0.091 0.128 0.077 0.127 0.223 0.180 0.104 0.218 L2-Bi-Stag 0.491 0.049 0.016 0.040 0.037 0.034 0.073 0.071 0.065 0.061 0.034 L2-Mono+Code-Stag 17.486 8.495 13.943 4.723 2.024 2.425 2.494 3.048 2.913 2.142 2.158 L2-Mono-Stag 16.364 8.500 13.119 4.372 1.949 2.314 2.072 3.454 2.980 1.834 2.192 Llama-2-7B 5.85 2.08 2.9 2.91 2.25 2.11 1.96 2.66 3.05 2.06 2.23 L3-Bi+Code-Stag 1.944 0.359 1.268 0.514 0.281 0.215 0.433 0.535 0.381 0.394 0. L3-Bi-Stag 0.062 0.026 0.307 0.191 0.177 0.078 0.053 0.269 0.201 0.053 0.126 L3-Mono+Code-Stag 21.167 9.224 15.935 6.461 2.324 2.145 2.433 3.470 3.165 2.080 2.402 L3-Mono-Stag 21.349 10.366 17.034 6.521 2.083 2.105 1.753 3.332 2.960 1.993 2.066 Llama-3.1-8B 22.72 6.48 7.09 4.69 14.55 3.38 5.35 6.54 8.73 9.88 12.3 V7-Bi+Code-Stag 0.112 0.065 0.061 0.041 0.026 0.054 0.073 0.049 0.046 0.050 0.037 V7-Bi-Stag 0.061 0.042 0.064 0.039 0.030 0.028 0.076 0.042 0.041 0.045 0. V7-Mono+Code-Stag 2.501 1.761 3.345 0.832 0.441 0.255 0.238 0.614 0.337 0.405 0.346 V7-Mono-Stag 3.396 1.647 3.766 0.842 0.366 0.401 0.143 0.585 0.480 0.258 0.263 Viking-7B 3.15 1.54 2.5 2.4 1.75 1.97 0.88 2.43 2.33 1.37 1.48 Table 12: FLORES-200 BLEU scores for Stagnant languages (X-Eng). Training language rows have shaded background. 17 Preprint. Under review. Figure 6: Examples of language mixing in bilingual CPT (L3-Bi-) compared to monolingual CPT (L3-Mono-). A.6 Prompt Templates For the SIB-200 classification task, we adopt the following template: Classification: Topic sports, health, entertainment, geography. {examples} The topic of the news \"{text}\" is science/technology, travel, politics, For the FLORES-200 translation task, we employ the following 3-shot prompt: Translate the following sentence from {src lang} to {tgt lang} {examples} [{src lang}]: {src sent} [{tgt lang}]: A.7 Data Statistics The data statistics presented in Tables 13 and 14 summarize the bilingual translation and monolingual training datasets used in this study. Token counts in the two tables are calculated by splitting text on whitespace, method chosen for its computational efficiency given the large volume of data. For code data, we provide raw token counts from The Stack dataset across 32 programming languages in Table 15, totaling 51,253,373,176 tokens. We then downsample this to 49,999,171 tokens as counted by using the GPT-2 tokenizer (Radford et al., 2019), selected for its speed, to match the training dataset setup in Subsection 2.2. 18 Preprint. Under review. Category Altruistic Selfish Stagnant Language Pair eng Latn-zul Latn zho Hani-zul Latn ceb Latn-zul Latn zho Hani-ceb Latn eng Latn-mar Deva zho Hani-mar Deva ceb Latn-mar Deva ceb Latn-eng Latn zho Hani-khm Khmr eng Latn-khm Khmr Total bel Cyrl-deu Latn bel Cyrl-eng Latn deu Latn-mri Latn eng Latn-mri Latn deu Latn-kir Cyrl eng Latn-kir Cyrl deu Latn-nya Latn eng Latn-nya Latn Total eng Latn-tha Thai eng Latn-yor Latn eng Latn-sna Latn eng Latn-wol Latn Total Source Tokens 12,672,195 341,665 190,637 696,789 7,736,633 2,244,545 835,219 12,355,815 1,157,707 11,364,386 49,595,591 27,012,850 1,598,358 1,682,621 717,914 1,682,749 2,262,374 1,155,433 19,714,307 55,826,606 5,619,794 14,334,000 9,813,703 13,600,133 43,367,630 Target Tokens Total Tokens 9,196,313 208,653 94,910 863,637 7,248,634 1,825,067 634,881 11,719,494 577,403 10,147,868 42,516,860 18,085,602 1,920,079 2,250,042 913,809 1,583,623 1,515,087 1,077,300 16,830,192 44,175,734 18,138,086 16,887,000 7,608,164 13,636,959 56,270,209 21,868,509 550,318 285,547 1,560,426 14,985,267 4,069,612 1,470,100 24,075,309 1,735,110 21,512,254 92,112,452 45,098,452 3,518,437 3,932,663 1,631,723 3,266,372 3,777,462 2,232,733 36,544,499 100,002,341 23,757,879 31,221,000 17,421,867 27,237,092 99,637, Table 13: Bilingual translation data statistics: source, target, and total token counts across language pairs for each language category, with totals for each group. Category Altruistic Selfish Stagnant Language eng Latn zho Hani ceb Latn mar Deva zul Latn khm Khmr Total eng Latn deu Latn bel Cyrl mri Latn kir Cyrl nya Latn Total eng Latn tha Thai yor Latn sna Latn wol Latn Total Total Tokens 43,492,709 4,440,706 14,245,308 9,708,582 9,499,876 10,725,271 92,112,452 24,614,674 22,606,405 28,611,208 3,163,851 3,098,710 17,907,492 100,002,341 43,367,629 18,138,086 16,887,000 7,608,164 554,809 86,555,688 Table 14: Monolingual training data statistics: total token counts for each language across the three language categories. 19 Preprint. Under review. Language assembly cpp c-sharp clojure common-lisp dart erlang f-sharp fortran glsl go haskell java javascript julia kotlin llvm markdown pascal perl php powershell python ruby rust scala shell solidity sql typescript Total Total Tokens 331,667,471 8,741,971,474 7,816,404,624 2,378,224,612 82,101,240 392,951,006 596,729,087 145,648,910 67,025,280 442,165,240 116,320,040 3,566,871,370 401,113,392 3,659,465,643 3,027,933,059 221,192,206 851,638,489 383,439,623 1,795,961,949 424,339,418 473,210,127 2,315,544,678 74,390,317 5,199,071,526 49,449,207 1,107,302,714 1,572,906,932 568,062,821 510,858,653 151,560,961 1,179,866,764 2,607,984,343 51,253,373, Table 15: Raw code data statistics from subset of The Stack dataset processed by Ji et al. (2024a), showing total token counts for each programming language before downsampling."
        }
    ],
    "affiliations": [
        "Technical University of Darmstadt",
        "University of Helsinki"
    ]
}