{
    "paper_title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy",
    "authors": [
        "Yuhao Liu",
        "Tengfei Wang",
        "Fang Liu",
        "Zhenwei Wang",
        "Rynson W. H. Lau"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, however, users often seek tools to faithfully realize their creative editing intentions with precise and consistent control. Despite the progress achieved by existing methods, ensuring fine-grained alignment with user intentions remains an open and challenging problem. In this work, we present Shape-for-Motion, a novel framework that incorporates a 3D proxy for precise and consistent video editing. Shape-for-Motion achieves this by converting the target object in the input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be performed directly on the proxy and then inferred back to the video frames. To simplify the editing process, we design a novel Dual-Propagation Strategy that allows users to perform edits on the 3D mesh of a single frame, and the edits are then automatically propagated to the 3D meshes of the other frames. The 3D meshes for different frames are further projected onto the 2D space to produce the edited geometry and texture renderings, which serve as inputs to a decoupled video diffusion model for generating edited results. Our framework supports various precise and physically-consistent manipulations across the video frames, including pose editing, rotation, scaling, translation, texture modification, and object composition. Our approach marks a key step toward high-quality, controllable video editing workflows. Extensive experiments demonstrate the superiority and effectiveness of our approach. Project page: https://shapeformotion.github.io/"
        },
        {
            "title": "Start",
            "content": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy YUHAO LIU, City University of Hong Kong, Hong Kong SAR, China TENGFEI WANG, Tencent, China FANG LIU, City University of Hong Kong, Hong Kong SAR, China ZHENWEI WANG, City University of Hong Kong, Hong Kong SAR, China RYNSON W.H. LAU, City University of Hong Kong, Hong Kong SAR, China 5 2 0 2 7 2 ] . [ 1 2 3 4 2 2 . 6 0 5 2 : r Fig. 1. The proposed 3D-aware framework, Shape-for-Motion, supports precise and consistent video editing by reconstructing an editable 3D mesh to serve as control signals for video generation. The first two examples demonstrate pose editing (by rotating the back to the right by 50 degrees and the head to the left by 10 degrees) and object composition (by composing tree from the reference image onto the top of the car). In each example, the first row shows the input video frames, followed by the editing in 3D space at the right end; the bottom row of images shows the corresponding edited frames. In addition, our approach also supports diverse applications, such as Image-to-Video Animation, as shown in the third example. Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, however, users often seek tools to faithfully realize their creative editing intentions with precise and consistent control. Despite the progress achieved by existing methods, ensuring fine-grained alignment with user intentions remains an open and challenging problem. In this work, we present Shape-for-Motion, novel framework that incorporates 3D proxy for precise and consistent video editing. Shape-for-Motion achieves this by converting the target object in the input video to time-consistent mesh, i.e., 3D proxy, allowing edits to be performed directly on the proxy and then inferred back to the video frames. To simplify the editing process, we design novel Dual-Propagation Strategy that allows users to perform edits on the 3D mesh of single frame, and the edits are then automatically propagated to the 3D meshes of the other frames. The 3D meshes for different frames are further projected onto the 2D space to produce the edited geometry and texture renderings, which serve as Joint corresponding authors. inputs to decoupled video diffusion model for generating edited results. Our framework supports various precise and physically-consistent manipulations across the video frames, including pose editing, rotation, scaling, translation, texture modification, and object composition. Our approach marks key step toward high-quality, controllable video editing workflows. Extensive experiments demonstrate the superiority and effectiveness of our approach. Our code and model will be released at https://shapeformotion.github.io. Additional Key Words and Phrases: 3D-Aware Video Editing, Generative Model"
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid proliferation of video content on online platforms, combined with the explosive emergence of video generation models, has fueled the surge in video content generation. Recently, controllable video editing, which focuses on modifying the source video content to align with user intentions, has gained increasing attention. 2 Yuhao Liu, Tengfei Wang, Fang Liu et al. Early approaches [Bar-Tal et al. 2022; Wu et al. 2023] use text as the interaction signal, but text often lacks precision and flexibility for editing spatial attributes. Besides text, image-based methods [Ku et al. 2024; Ouyang et al. 2024] introduce an edited image as guidance and propagate edits across frames, while drag-based methods [Deng et al. 2024; Teng et al. 2023] propose to manually drag anchor points for localized adjustments. However, they struggle in handling complex editing and ensuring frame-to-frame consistency. Unlike these existing works, our objective in this work is to develop video editing system with two key features. 1) Precision. Accurate controllability gives users precise control over various aspects of the video elements, including object pose, shape, location, and spatial layout. Such fine-grained control often extends to object attributes with quantifiable precision. For example, rotating the panda in the third example of Fig. 1 by 20 degrees to the left requires precise manipulation. 2) Consistency. Consistent alignment demands edits to remain coherent across frames. For example, placing tree on the moving car in the second example of Fig. 1 requires alignment that simultaneously accounts for the cars motion, rotation, and changing perspectives. While these two key features are essential for video editing, it is non-trivial to achieve this with 2D framework, due to the absence of underlying 3D representations. To address the above challenges, in this paper, we introduce Shape-for-Motion, novel video editing framework that incorporates 3D proxy (i.e., mesh) to enable precise editing while maintaining temporal and spatial consistency. Our framework follows 3D-aware workflow by first reconstructing 3D proxy of the target object from the video, followed by interactive manipulation in the 3D space, and finally, producing video with the help of the edited 3D proxy. We leverage three key designs to ensure (1) temporalconsistent 3D proxy reconstruction, (2) consistent 3D editing across frames, and (3) generative rendering from edited 3D to video. The first is consistent mesh reconstruction of the target object. We note that reconstructing separate mesh for each frame individually [Tang et al. 2025] leads to poor consistency due to the lack of inter-frame correspondences. To address this problem, we propose to reconstruct consistent mesh representation for the object across all frames using canonical mesh with time-varying deformation field. However, the limited viewpoint information in monocular input video often results in unsatisfactory reconstruction. To mitigate this, we leverage novel views generated from an existing multi-view generator [Voleti et al. 2025] to enhance mesh reconstruction and propose balanced view sampling strategy to alleviate inconsistencies caused by the generated novel views. The second design is automatic editing propagation on 3D mesh. While editing the mesh in each frame individually suffices for simple global operations, e.g., global rotation or scaling, it becomes impractical for more complex tasks, like localized pose editing as shown in the first example of Fig. 1. To enable user-friendly editing process (in which users only need to edit the 3D mesh of single frame, and the editing is then automatically propagated to other frames), we propose novel Dual-Propagation Strategy, which utilizes learned correspondences between canonical and deformed meshes to propagate geometry and texture edits across frames, enabling the generation of consistent editing. The final key design is generative rendering from the edited 3D proxy. Once we have edited the object in 3D space, we then need to convert the edited 3D meshes to high-quality edited video. However, it is difficult to train such generation model, due to the absence of paired training data (3D mesh and corresponding video). To address this challenge, we adopt decoupled control strategy in our video diffusion model, treating geometry and texture information from the edited proxy as two separate conditioning signals. self-supervised mixed training strategy is proposed to alternate between geometry and texture controls, enabling the model to preserve its generative capabilities while achieving temporal and spatial consistency across frames. In summary, Shape-for-Motion allows users to perform finegrained geometry control (e.g., pose editing, rotation, scaling, translation, and object composition) and texture modification in 3D space. It also supports appearance editing in 2D space by incorporating 2D editing tools. Extensive experiments and user studies on the new V3DBench dataset demonstrate that our approach generates temporally consistent video edits, and outperforms existing methods qualitatively and quantitatively. Our main contributions are: We propose Shape-for-Motion, novel video editing framework that incorporates 3D proxy to allow diverse and precise video object manipulation while ensuring temporal consistency across frames. We propose video object reconstruction method, which produces meshes with correspondences across frames, and dual propagation strategy. Together, these components enable user-friendly editing process: users can perform edits directly on the canonical 3D mesh only once, with the edits are automatically propagated to subsequent frames. We propose self-supervised mixed training strategy to train decoupled video diffusion model that leverages geometry and texture information from the edited 3D proxy as control signals, achieving more consistent editing results."
        },
        {
            "title": "2 RELATED WORKS\n2.1 Controllable Video Generation",
            "content": "Recent success in diffusion models [Ho et al. 2020; Song et al. 2021] have fueled significant progress in image [Rombach et al. 2022] and video generation [Brooks et al. 2024]. Earlier Text-to-Video (T2V) models [Blattmann et al. 2023; Ho et al. 2022] are mostly evolved from Text-to-Image (T2I) models by incorporating additional temporal layers. Later, Image-to-Video (I2V) methods [Guo et al. 2024b,a] are proposed to animate an image to produce videos. Methods like [Chen et al. 2023; Esser et al. 2023; Hu and Xu 2023; Yang et al. 2024b] are also proposed to generate videos by conditioning on sequence of control frames. Several works [Cai et al. 2024; Lv et al. 2024; Shi et al. 2024] also explore the application of 3D priors for video generation, either as direct inputs or as intermediate outputs to enhance the generation quality. Recently, incorporating video priors for controllable 4D generation [Bahmani et al. 2024; Jiang et al. 2024] has also attracted huge research interests. Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy 3 Fig. 2. Overview of Shape-for-Motion. Our approach is an interactive video editing framework that utilizes an editable 3D proxy (e.g., mesh) to enable users to perform precise and consistent video editing. Given an input video, our approach first converts the target object into 3D mesh with frame-by-frame correspondences. Users can then perform editing on this 3D proxy once only (i.e., on single frame), and the edits will be automatically propagated to the 3D meshes of all other frames. The edited 3D meshes are then converted back to 2D geometry and texture maps, which are used as control signals in decoupled video diffusion model to generate the final edited result. Note that, for better visualization, the colors of the propagated meshes are disabled in Step-II."
        },
        {
            "title": "2.2 Controllable Video Editing",
            "content": "Apart from text-based generation, Control-based methods [Gu et al. 2025; Ma et al. 2023; Wang et al. 2022; Zhang et al. 2023b] are proposed to utilize additional reference maps (e.g., depth or edge maps) to modify object motion and shape. However, creating references for each frame by the user is impractical. First-frame-based methods [Ceylan et al. 2023; Fan et al. 2024; Ku et al. 2024; Mou et al. 2024; ?] reduce this burden by editing on the first frame only and propagating [Kagaya et al. 2010] the changes to subsequent frames. Recently, drag-based methods [Deng et al. 2024; Pan et al. 2023; Teng et al. 2023] allow users to modify the shape of an object by dragging its local region from start point to an endpoint. However, they struggle with fine-grained editing, such as rotating an object by specific degree or adjusting its pose while preserving motion. 2.3 Image Editing Using 3D Proxy Recently, growing body of methods has sought to integrate 3D priors into image editing. 3DIT [Michel et al. 2023] introduces an object-centric image editing framework that edits objects based on language instructions. Diffusion Handles [Pandey et al. 2024] edits selected objects by leveraging depth to lift diffusion activations into 3D space. The most related works to ours are Image Sculpting [Yenphraphai et al. 2024] and 3D-Fixup [Cheng et al. 2025], which use 3D mesh to support precise image editing. In contrast, our approach enables precise and consistent video editing with 3d proxy."
        },
        {
            "title": "2.4 Dynamic 3D Reconstruction",
            "content": "Compared to NeRF [Mildenhall et al. 2021; Pumarola et al. 2021], 3D Gaussian Splatting (3DGS) [Kerbl et al. 2023] supports both high-quality rendering and real-time efficiency. When extended to dynamic scenes, deformable 3DGS incorporates explicit temporal changes of the canonical points [Luiten et al. 2023; Wu et al. 2024b; Yang et al. 2024a]. However, the point-based representation is inherently less suited for precise editing. Most recently, DG-Mesh [Liu et al. 2024] integrates deformable 3DGS for mesh reconstruction, and can be used to handle dynamic scenes. However, it requires 360 capturing of the target object, which is not applicable for general videos. In contrast, our method aims at reconstructing consistent 3D mesh from general monocular video, which typically has limited views of the target object. Inspired by multi-view generation [Shi et al. 2023; Wang et al. 2025], several methods [Ren et al. 2024; Wu et al. 2024a; Xie et al. 2024] leverage diffusion models to produce novel views for video, followed by 4D reconstruction. However, these methods lack explicit mesh correspondences across frames, limiting their applicability for video editing."
        },
        {
            "title": "3 OUR APPROACH",
            "content": "Given an input video, we aim to enable precise manipulation of the target object. To this end, we propose novel video editing pipeline (see Fig. 2) consisting of three key steps. First, we convert the target object into its 3D mesh to provide consistent geometry structure for editing (Sec. 3.1). Second, we propose new Dual-Propagation Strategy to enable precise and controllable edits in 3D space, which are then consistently propagated across frames (Sec. 3.2). Third, we introduce self-supervised mixed training strategy for decoupled video diffusion model, which uses geometry and texture renderings as control signals to produce more consistent results. (Sec. 3.3)."
        },
        {
            "title": "3.1 Consistent 3D Proxy Reconstruction",
            "content": "One naive way to obtain the 3D proxy is to reconstruct each frame individually. However, this separate modeling lacks frame-to-frame correspondences, resulting in poor temporal consistency. To address this, we adopt the deformable Gaussian Splatting [Yang et al. 2024a] to reconstruct the target object in the input video. Furthermore, manipulating Gaussian points is not user-friendly and only supports limited editing types. To enable 3D-guided video editing, we aim to obtain consistent 3D meshes as the editing proxy. Overall Pipeline. The workflow is illustrated in Fig. 3. At time ğ‘¡, we first obtain the deformation of the canonical Gaussians from the deformation field encoded with learnable MLP ğœƒ () to form the deformed Gaussians. We then utilize DPSR [Peng et al. 2021] combined with Marching Cubes (MC) to convert the deformed Gaussians into 4 Yuhao Liu, Tengfei Wang, Fang Liu et al. Fig. 3. Pipeline of our consistent object reconstruction. We use deformable3DGS to reconstruct the 3D mesh of the target object in video by maintaining canonical Gaussians and time-varying deformation field. ğ‘– Gaussian-propagated deformed mesh. To obtain the vertex colors of the deformed mesh, we follow [Liu et al. 2024] to store the texture in canonical color MLP, denoted as ğ‘€ğ¿ğ‘ƒğ‘ (). By training backward deformation field ğœƒ 1 (), we map the deformed mesh back to the canonical space at each time ğ‘¡, and query the vertex colors via ğ‘€ğ¿ğ‘ƒğ‘ (). Finally, we perform differentiable rasterization [Laine et al. 2020] to render the depth map, mesh image, and mask of the deformed mesh. Meanwhile, Gaussian image is rendered from the deformed Gaussians. Dynamic Object Reconstruction with Novel-View Augmentation. Although deformable 3DGS can provide frame-to-frame correspondences, it relies heavily on accurate multi-view observations of dynamic scene for reconstruction. However, most video clips are typically captured from certain view, lacking contents from other viewpoints across frames, which in turn leads to unsatisfactory reconstruction [Liu et al. 2024; Yang et al. 2024a]. To address this, we propose to leverage multi-view diffusion models [Voleti et al. 2025] to assist in dynamic object reconstruction. Given video ğ‘‹ ğ‘ ğ‘Ÿğ‘ = {ğ‘‹ ğ‘ ğ‘Ÿğ‘ ğ‘– = 1,2, . . . ,ğ‘‡ }, where is the number of frames, we utilize multi-view generator Mdiff to generate ğ‘ (imperfect) novel views = {ğ‘‰ğ‘–,ğ‘— ğ‘– = 1, 2, . . . ,ğ‘‡ ; ğ‘— = 1, 2, . . . , ğ‘ } with associated camera pose. We can then combine these novel views with input frames to optimize the reconstruction of the target object. Balanced View Sampling. Since 3D serves merely as proxy during the video editing process, users primarily focus on editing the observed view ğ‘‹ src, while the generated novel views are employed mainly to enhance the completeness of the reconstruction. However, trivially combining them inevitably amplifies inter-frame and intra-frame inconsistencies arising from the target object variations and generated novel views, leading to incomplete or irregular geometry (See row-2 in Fig. 7). To this end, we introduce Balanced View Sampling to alleviate the inconsistency between observed and generated views. We define two sampling probabilities: ğ›½ğ‘– for each observed view ğ‘‹ ğ‘ ğ‘Ÿğ‘ , and ğœğ‘–,ğ‘— for each novel view ğ‘‰ğ‘–,ğ‘— in V, representing their respective selection probabilities during sampling. Since the number of novel views is ğ‘ times larger than that of the observed view at each frame ğ‘–, an equal sampling strategy would disproportionately favor the novel views, amplifying inconsistencies due to their synthetic nature. Thus, we enforce the constraint that the total sampling probability of all views in Vğ‘– equals to that of the observed view ğ‘‹ ğ‘ ğ‘Ÿğ‘ ğœğ‘–,ğ‘— . This inherently assigns , i.e., ğ›½ğ‘– = (cid:205)ğ‘ ğ‘—=1 ğ‘– ğ‘– Fig. 4. Workflow of our consistent editing propagation. Solid orange arrows denote texture propagation from the canonical mesh, and solid blue arrows indicate geometry propagation from canonical Gaussians. lower sampling frequency to each novel view in ğ‘‰ğ‘– compared to ğ‘‹ ğ‘ ğ‘Ÿğ‘ , thereby reducing inconsistencies caused by the novel views. ğ‘– Loss function. The overall training objective is: = Lğ‘”ğ‘  + Lğ‘šğ‘ğ‘ ğ‘˜ + Lğ‘Ÿğ‘”ğ‘ + Lğ‘‘ğ‘’ğ‘ğ‘¡â„. (1) We employ combination (i.e., Lğ‘”ğ‘  ) of L1 loss and SSIM loss on the Gaussian images, using the input video frames and generated novel views as supervision. To constrain the shape of the reconstructed mesh from different views, we apply an L1 loss (i.e., Lğ‘šğ‘ğ‘ ğ‘˜ ). Additionally, we adopt the same rendering loss as Lğ‘”ğ‘  to supervise the mesh image (i.e., Lğ‘Ÿğ‘”ğ‘ ). To mitigate sunken surfaces and floating artifacts in reconstructed meshes, we further enforce scale-invariant depth constraint Lğ‘‘ğ‘’ğ‘ğ‘¡â„ between mesh depth and GT image depth. Refer to Supp. A.3 for more details."
        },
        {
            "title": "3.2 Editing 3D Proxy with Automatic Propagation",
            "content": "To enable user-friendly editing process, we aim to transfer crossframe correspondences in deformable Gaussian splatting to the reconstructed mesh, such that users can edit the canonical mesh once, with the edits propagated to all frames in 3D space automatically. Formally, given canonical Gaussian ğºğ‘ , we first convert it to the canonical mesh ğ‘€ğ‘ via Marching Cube (MC). The user then performs edits directly on ğ‘€ğ‘ , yielding the edited canonical mesh ğ‘€ ğ‘ and thus the editing offset in the canonical space is Î”ğ‘š = ğ‘€ ğ‘ ğ‘€ğ‘ . naive approach to propagate this editing offset is to directly apply the deformation field 1 ğœƒ () to the ğ‘€ğ‘ to obtain the meshpropagated deformed mesh ğ‘€ğ‘‘ ğ‘¡ = ğ‘€ğ‘ + ğœƒ (ğ‘€ğ‘ ) at each frame ğ‘¡, and then integrate the ğ‘€ğ‘‘ ğ‘¡ with the editing offset to produce the meshpropagated edited mesh ğ‘€ğ‘’ ğ‘¡ + Î”ğ‘š (see flow indicated by the orange arrow in Fig. 4). However, since the deformation field is optimized for Gaussian points rather than mesh vertices directly, certain vertices may incur positional shifts, leading to inaccurate geometry (see Fig. 8, Variant-1). To this end, we develop Dual Propagation Strategy that propagates geometry and texture edits using the canonical Gaussians and the canonical mesh, respectively. ğ‘¡ = ğ‘€ğ‘‘ 1For brevity, we omit the ğ‘¡ symbol in time-dependent MLPs, e.g., deformation field ğœƒ () and color MLP network ğ‘€ğ¿ğ‘ƒğ‘ (). Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy 5 Geometry Propagation. As the users editing is performed in the canonical mesh, we first build vertex-point mapping to transfer the editing offset Î”ğ‘š for the canonical mesh to Î”ğ‘” for the canonical Gaussian. We introduce nearest-neighbor mapping from ğºğ‘ to ğ‘€ğ‘ . For any gaussian point in ğºğ‘ , we seek the closest vertex in ğ‘€ğ‘ by: (2) v2. ğœ™ (x) = arg min ğ‘€ğ‘ Thus, the editing offset for canonical Gaussian is Î”ğ‘” = Î”ğ‘š (ğœ™ (ğºğ‘ )). Then, we leverage the correspondence in the deformation field ğœƒ (), to obtain the deformed Gaussian ğºğ‘¡ = ğºğ‘ + ğœƒ (ğºğ‘ ). The Gaussianpropagated edited mesh at frame ğ‘¡ can thus be produced by integrating ğºğ‘¡ with the editing offset Î”ğ‘” by Ë†ğ‘€ğ‘’ Texture Propagation. With Ë†ğ‘€ğ‘’ ğ‘¡ , one naive method of obtaining the edited texture is to query the color directly by using it as input to the color network ğ‘€ğ¿ğ‘ƒğ‘ (). However, due to vertex misalignment before and after the editing, this method often results in spatially shifted colors (see Fig. 8, Variant-2). ğ‘¡ = MC(ğºğ‘¡ + Î”ğ‘”). ğ‘€ğ‘’ ğ‘¡ and Ë†ğ‘€ğ‘’ Instead, we note that although the mesh-propagated edited mesh ğ‘€ğ‘’ ğ‘¡ , may contain geometric errors due to outliers in vertices and faces, the majority of its color remains correct. Thus, we establish an additional mapping ğœ‰ () (similar to Eq. 2) between the two edited ğ‘¡ , which are propagated from the canonical mesh meshes, and canonical Gaussians, respectively. We can then retrieve the color information from ğ‘€ğ‘’ ğ‘¡ . Specifically, we first obtain the color ğ¶ğ‘‘ ğ‘¡ before editing. As the number of vertex is the same for ğ‘€ğ‘‘ ğ‘¡ before and after geometry editing, the color of the mesh-propagated edited mesh can be queried by: ğ‘¡ of deformed mesh ğ‘€ğ‘‘ ğ‘¡ and ğ‘€ğ‘’ ğ‘¡ and map it to Ë†ğ‘€ğ‘’ ğ‘¡ )(cid:1), ğ‘¡ = ğ¶ğ‘‘ ğ¶ğ‘’ ğ‘¡ = ğ‘€ğ¿ğ‘ƒğ‘ (cid:0) ğ‘€ğ‘‘ ğ‘¡ ğœƒ 1 ( ğ‘€ğ‘‘ (3) where ğœƒ 1 () is the backward deformation field. Then, we can obtain the propagated texture color Ë†ğ¶ğ‘’ ğ‘¡ )(cid:1) for the Gaussianpropagated edited mesh Ë†ğ‘€ğ‘’ ğ‘¡ , and form the propagated mesh ğ‘€ğ‘¡ = { Ë†ğ‘€ğ‘’ ğ‘¡ } at frame ğ‘¡. Finally, we render it into its 2D representations: normal map for geometry and mesh image for texture. ğ‘¡ = ğ¶ğ‘’ ğ‘¡ (cid:0)ğœ‰ ( Ë†ğ‘€ğ‘’ ğ‘¡ ; Ë†ğ¶ğ‘’"
        },
        {
            "title": "3.3 Generative Rendering from the 3D Proxy",
            "content": "With the edited geometry and texture in place, the next step is to synthesize the final refined video renderings by leveraging these signals along with the input frames. To this end, we train decoupled video diffusion model using self-supervised mixed training strategy. The overall training pipeline is illustrated in Fig. 5. Data Construction. Since no such 3D-video paired video editing datasets are available, we generate training data by augmenting the input video to simulate preand post-editing states. (1) Preediting: The reference object is constructed by the reference generator, which first segments the target object from the original video and then applies random augmentations, including scaling, shifting, and rotation. (2) Post-editing: The original video frames serve as Ground Truth (GT), with masked normal maps extracted as geometry control. Then, the texture simulator generates coarse texture by randomly degrading the GT with SLIC-based segmentation [Achanta et al. 2012], median blurring, and down-up sampling. Overall Pipeline. We use the I2V release of the stable video diffusion (SVD) [Blattmann et al. 2023] as the base model, and follow the Fig. 5. Training pipeline of the generative rendering from the 3D proxy. Using rendered geometry (i.e., normal map) and texture as separate control signals, we adopt self-supervised mixed training strategy in which the geometry controller and the texture enhancer (i.e., denoising U-Net) are alternately trained in two stages. The contents enclosed by the purple dashed box indicates inputs shared by both stages, while those in the orange and blue dashed boxes are specific to the first and second stages, respectively. Reference objects and coarse textures are constructed via the reference and texture simulators to facilitate self-supervised training. ControlNet [Zhang et al. 2023a] paradigm. To handle the domain gap between texture and geometry, we employ the base model to refine coarse texture, and utilize the control branch to guide the structure via geometry control. Refer to Supp. C.2 for more details. Mixed Training. straightforward approach is to train the entire model jointly using two control signals. Nonetheless, the coarse texture is pixel-aligned to the GT, while the distribution of the geometry is very different. It is easy to discard the geometry branch and downgrade the generative model to an enhancement model again. Motivated by the animation production that first draws structure and then coloring, we propose to train the geometry controller and texture enhancer (i.e., base model) in two-stage manner. Specifically, in the first stage, we fix the base model and train only the geometry controller. In the second stage, we freeze the trained geometry controller and fine-tune the base model. In both stages, the model receives the same set of shared inputs: random noise, normal map for geometry control, target mask, and reference object 2. The primary difference between the two stages lies in the handling of texture control. In the first stage, texture control is replaced by background image, where random noise is inserted into the object region. In this way, although the generated appearance may appear coarse (see Fig. 9, Case-2) due to the spatial misalignment between the reference object and the target geometry, the model is encouraged to learn geometry-following generation. In the second stage, the model focuses on texture enhancement by simultaneously refining details within the edited object and inpainting missing regions (e.g., areas filled with white in Fig. 5). Unlike the first stage, this phase introduces an additional set of stagespecific inputs, i.e., an inpainting mask and coarse texture rendered from the edited 3D proxy. To preserve the models ability to follow geometric guidance, the texture control is provided as input with probability of 20%, while it is replaced by background image with probability of 80%. This mixed-training strategy encourages the 2For simplicity, we omit the process of transforming pixels to latents through the VAE. Yuhao Liu, Tengfei Wang, Fang Liu et al. Fig. 6. Qualitative comparison with four video editing baselines (DragVideo [Deng et al. 2024], Image Sculpting [Yenphraphai et al. 2024] + I2V-Edit [Ouyang et al. 2024], Pix2Video [Guo et al. 2024b], and Tune-A-Video [Wu et al. 2023]). Due to their lack of 3D awareness, the baseline methods only achieve limited modifications to the object geometry. Instead, our approach enables direct editing in the 3D space only once, ensuring precise and consistent results for all frames. The first row shows inputs, and the second highlights text instructions (provided solely for illustrating the performed edits) and 3D space edits. model to balance appearance preservation when texture information is available, while retaining generative flexibility when it is not."
        },
        {
            "title": "4 EXPERIMENTS\n4.1 Comparisons with State of the Art Methods",
            "content": "4.1.1 Qualitative Results. To the best of our knowledge, ours is the first work to leverage 3D proxy for video editing. As existing methods lack such precise editing capabilities, we compare our approach against four baseline methods (i.e., Tune-A-Video [Wu et al. 2023], Pix2Video [Guo et al. 2024b], Image-Sculting+I2V-Edit [Ouyang et al. 2024]), and DragVideo [Deng et al. 2024]. Fig. 6 shows the visual comparison. It clearly demonstrates that when dealing with fine-grained video editing scenarios, such as locally rotating the dog in case-1 and stretching the cars roof in case-2, all compared methods exhibit varying degrees of failure. Among the compared methods, I2V-Edit produces acceptable geometry editing results, due to reliable 3D editing on the first frame 3. However, as it heavily relies on the first frame for propagation, it often fails when there is constant geometry/shape variation across frames. For example, the dogs leg hangs across all frames in case-1. Moreover, since DragVideo only supports point-to-point editing in the 2D space, its limited point coverage is insufficient for fine-grained local editing. As result, it can extend the cars roof but fails to account for the entire upper part of the car in case-2, and generates multiple legs for the dog in case-1. As for the prompt-based methods, neither the training-free Pix2Video nor the optimization-based Tune-a-Video can handle fine-grained editing with text prompts. In contrast, our approach achieves high-quality results due to precise and consistent editing in the 3D space and reliable enhancement in the 2D space. 4.1.2 Quantitative Results. We collect V3DBench to evaluate our method. V3DBench consists of 22 videos, covering six categories: pose editing, rotation, scaling, translation, texture modification, 3For fair comparison, we use the 3D editing-capable Image-Sculpting to edit the first frame. Table 1. Quantitative comparison of video editing methods. The user study reports averaged rank of five methods in editing quality (EQ) and semantic consistency (SC). Best results are marked in bold. Methods Tune-A-Video Pix2Video I2V-Edit DragVideo Ours User Study Fram-Acc Tem-Con CLAP Score EQ SC Metrics 0.559 0.851 0.887 0.918 0.970 0.981 0.988 0.987 0.985 0. 0.474 0.690 0.829 0.856 0.917 4.77 4.61 3.90 3.71 2.23 2.55 2.97 2.87 1.16 1.23 Table 2. Ablation study on consistent 3D proxy reconstruction. All ablations are trained and tested on 21 novel views. BVS: balanced view sampling. Novel Views BVS ğ¿ğ‘‘ PSNR SSIM LPIPS DINO Score 20.35 22.87 22.91 23.00 0.903 0.922 0.923 0. 0.060 0.057 0.055 0.055 0.375 0.396 0.396 0.397 and object composition. For evaluation, we employ widely used CLIP [Radford et al. 2021]-based metrics, Fram-Acc [Ma et al. 2023] and Tem-Con [Esser et al. 2023]. We also introduce new metric, CLAP Score (CLIP-APpearance Score), to measure the cumulative error of the Fram-Acc and the DINO similarity accuracy: CLAP Score = Fram-Acc DINO(ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡, ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡). In addition, we also conduct user study to evaluate the perceptual quality in editing quality (EQ) and semantic consistency (SC). Tab. 1 shows the results. Our method achieves the highest performance on all metrics, demonstrating the superior editing quality and temporal consistency compared to the baselines. Refer to Supp. C.4 for more details."
        },
        {
            "title": "4.2 Ablation Study",
            "content": "4.2.1 Consistent 3D Proxy Reconstruction. We validate each component by individually removing them, with visual results shown in Fig. 7. It shows that the absence of novel views as constraints leads to wrong reconstructions (row 1). Disabling the balanced sampling strategy increases the sampling frequencies for novel views, which, in turn, results in greater inconsistencies and incomplete reconstructions (row 2). When the depth loss is removed, the mesh images remain relatively unaffected due to their depth-invariance, but the normal map exhibits numerous sunken holes (row 3). The quantitative results are presented in Tab. 2 in terms of PSNR, SSIM, LPIPS, and DINO score. We can conclude that (1) Using only the monocular video results in poor reconstruction quality, while introducing novel views significantly improves all metrics (row 1 vs. row 2), highlighting the critical role of generated novel views as constraints; (2) Further incorporating balanced view sampling and depth constraints leads to additional performance gains (row 3 & 4). 4.2.2 Editing 3D Proxy with Automatic Propagation. We compare two variants for propagating edits. (1) We directly utilize the meshpropagated edited mesh ğ‘€ğ‘’ ğ‘¡ as the propagated 3D mesh { ğ‘€ğ‘’ ğ‘¡ } at frame ğ‘¡. (2) After obtaining the Gaussian-propagated edited mesh Ë†ğ‘€ğ‘’ ğ‘¡ , we send it to the color network ğ‘€ğ¿ğ‘ƒğ‘ () to query its vertex colors and get { Ë†ğ‘€ğ‘’ ğ‘¡ )}. ğ‘¡ combined with its vertex color ğ¶ğ‘’ ğ‘¡ ; ğ¶ğ‘’ ğ‘¡ ; ğ‘€ğ¿ğ‘ƒğ‘ ( Ë†ğ‘€ğ‘’ Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy 7 Table 3. Ablation study on mixed training of decoupled video diffusion. Stage-1 Stage-2 Fram-Acc Temp-Con CLAP Score 0.786 0. 0.988 0.988 0.746 0.917 We present the comparisons of geometry and texture renderings in Fig. 8. Although the deformation field originally designed for GS points can be applied to mesh vertices, discrepancies in quantity and position between GS points and mesh vertices lead to several inaccuracies in the reconstructed geometry and texture. In contrast, the second variant naively queries colors, leading to noticeable color shifts. Our method, instead, establishes dual propagation for both geometry and texture, thus achieving higher editing consistency. 4.2.3 Generative Rendering from 3D Proxy. We evaluate the effectiveness of the mixed training strategy by disabling Stage 2 and present the visual comparisons in Fig. 9. Training exclusively on augmented data causes the model to struggle with view-changing edits (e.g., rotating the penguin 20 degrees to the left) when relying solely on geometry as control. This is because such paired data cannot be accurately simulated during the augmentation process. Additionally, spatial misalignment between the target geometry and the reference object leads to failures in maintaining the correct appearance (e.g., Patrick Stars face and pants are distorted or lost). Quantitative comparisons in Tab. 3 on the V3DBench dataset also demonstrate that our mixed training strategy significantly improves the Frame-Acc and CLAP Score metrics."
        },
        {
            "title": "5 APPLICATIONS",
            "content": "Image-to-Video Animation. Given an input image, we first reconstruct 3D model of the target object. The reconstructed 3D model can then be rigged and animated to create various motions. Finally, the edited 3D meshes are further employed by our decoupled video diffusion model to generate the animated video (see Fig. 10). Appearance Editing. As the geometry and texture controls are decoupled in our stage 3, we can easily integrate various image editing tools into our framework to support flexible object appearance editing. Visual results are presented in Fig. 11."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce Shape-for-Motion, 3D-aware video editing method that enables users to perform precise and consistent edits on target objects in video. Our method leverages generated novel views as constraints to reconstruct the target object into time-consistent mesh, allowing users to perform edits directly in 3D space with dual-propagation strategy. With the help of 3D proxy, the proposed method demonstrates compelling performance in various video editing applications. Our approach does have limitations. It struggles to handle object associations (e.g., shadow and reflection). In the future, we aim to introduce Physically Based Rendering (PBR) technique in reconstruction and editing to solve this problem. Additionally, extending the current object-centric framework to scene-level video editing is also promising direction. 8 Yuhao Liu, Tengfei Wang, Fang Liu et al. Fig. 7. Comparison of the key components in consistent 3D proxy reconstruction. Each row starts with an input video on the left, followed by results of our full model in the middle and ablated baselines on the right, each consisting of rendered image and its corresponding normal map. Fig. 8. Comparison of editing propagation strategies. The edited 3D mesh is rendered into the texture (i.e., mesh image) and geometry (i.e., normal map). The first column presents the input image at frame ğ‘¡ , followed by the editing in 3D space at this frame in the second column, with results from ours and two variants shown in subsequent columns. Variant-1: mesh-propagated edited mesh with its color. Variant-2: Gaussian-propagated edited mesh with its color. Fig. 9. Comparison of mixed training in generative rendering from the 3D proxy. For each case, the input video is shown on the left, followed by 3D space editing in the second column, with results from our method and the baseline in the subsequent columns. Two frames are displayed for each case. Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy 9 Fig. 10. Our approach supports Image-to-Video animation. Starting with an input image (first column), we generate its 3D mesh and apply skeletal rigging (second column). The rigged model is then animated with driven motion sequence to produce edited meshes (third column), whose geometry and texture renderings are fed into our decoupled diffusion model to generate high-quality video. Fig. 11. Our approach supports integration with existing 2D image editing tools (e.g., Flux [Labs 2024]) to enable diverse object appearance editing. The two input frames are shown on the left, the editing processincluding geometry editing in 3D space and appearance editing in 2D spaceis displayed in the middle, and the final edited results are presented on the right. 10 Yuhao Liu, Tengfei Wang, Fang Liu et al. This supplemental material provides additional details on our methods and experiments. Sec. covers 3D proxy reconstruction, including data preparation and implementation. Sec. focuses on editing 3D proxies with automatic propagation. Sec. discusses generative rendering from 3D proxies, including inference workflow and experiments. Sec. includes discussion on pipeline design, efficiency, mesh topology, reconstruction performance, and 3D proxy quality. Finally, Sec. presents additional visual results. CONSISTENT 3D PROXY RECONSTRUCTION A.1 Preliminaries 3D Gaussian Splatting (GS) [Kerbl et al. 2023] adopts novel approach based on explicit point clouds to efficiently model 3D scenes. Each 3D Gaussian is parameterized by its mean position ğœ‡, covariance matrix Î£ and other attributes. Formally, 3D Gaussian ğº is defined as: ğº (ğ‘¥) = ğ‘’ 1 2 (ğ‘¥ )ğ‘‡ Î£1 (ğ‘¥ ) (4) Each Gaussian is multiplied by opacity ğ›¼ during the blending process. When projecting 3D Gaussian to 2D space, the covariance matrix is updated to Î£ using Jacobian matrix ğ½ and viewing transformation ğ‘Š via Î£ = ğ½ğ‘Š Î£ğ‘Š ğ‘‡ ğ½ğ‘‡ . To handle the differentiable optimization, the covariance matrix Î£ is divided into two learnable elements ğ‘Ÿ and ğ‘  to represent the rotation and scaling, which is then transformed into the corresponding matrices ğ‘† and ğ‘… to form the Î£ via Î£ = ğ‘…ğ‘†ğ‘†ğ‘‡ ğ‘…ğ‘‡ . Each 3D Gaussian is represented as ğº (ğ‘¥;ğœ‡,ğ‘Ÿ ,ğ‘ ,ğ›¼). Deformable 3DGS [Yang et al. 2024a] extend the 3D GS to dynamic scene by learning set of canonical Gaussians along with timevarying deformation field ğœƒ parametrized as an MLP. At each time ğ‘¡, the position ğ›¾ (ğ‘¥) of 3D Gaussians and time ğ›¾ (ğ‘¡) with positional encoding are used as input to the deformation MPL to obtain the offset ğœƒ (ğ‘¥), ğœƒ (ğ‘Ÿ ) and ğœƒ (ğ‘ ) of the dynamic 3D Gaussians in canonical space. ğ›¾ () denotes the positional embedding function. The new 3D Gaussian at the deformed space can then be represented as ğº (ğ‘¥ + ğœƒ (ğ‘¥);ğœ‡,ğ‘Ÿ + ğœƒ (ğ‘Ÿ ),ğ‘  + ğœƒ (ğ‘ ),ğ›¼). Directly performing editing on the Gaussian Splatting has been explored [Shin et al. 2024; Sun et al. 2024] recently. However, manipulating irregular 3D points is not user-friendly and only supports limited editing types. A.2 Data Preparation The workflow of data preparation during the target object reconstruction is depicted in Fig. A12. Given an input video, we first crop and segment the target object using SAM2 [Ravi et al. 2024]. Each frame of the cropped video is processed by multi-view generator [Voleti et al. 2025] to produce novel views. During the novel views generation, we assume that the camera in the input video is fixed and set the same camera pose for all input frames, where the field of view (FOV) = 33.8, elevation angle = 0, and azimuth angle = 0. For the newly generated views, we keep the field-of-view and elevation angles the same as in the input frame and sample different azimuths from the 360-degree sphere. The six azimuths are: {51.43, 102.86, 154.29, 205.71, 257.14, 308.57}. Then, we send both the original input video frames and the generated novel views to depth estimation method [Yang et al. 2024c] to produce depth maps, which will be used as the Ground-Truth depth supervision. We also employ SAM2 to obtain the Ground-Truth mask from the input frames and novel views. By default, we process 21 frames. If the motion between consecutive frames in the input video is minimal, the number of frames is increased to 42."
        },
        {
            "title": "Implementation Details",
            "content": "A.3 Formally, given the input video frames Xsrc and generated novel views V, during the reconstruction process, we render Gaussiansplatted image ğ¼ğ‘”ğ‘  Rğ» ğ‘Š 3 from the deformed gaussian, and render three mesh outputs from the Gaussian-propagated deformed mesh, i.e., mesh mask ğ‘€ğ‘ğ‘Ÿğ‘’ğ‘‘ Rğ» ğ‘Š 1, mesh depth ğ·ğ‘ğ‘Ÿğ‘’ğ‘‘ Rğ» ğ‘Š 1 and mesh image ğ¼ğ‘šğ‘’ğ‘ â„ Rğ» ğ‘Š 3. The loss functions are illustrated as follows. GS Loss. We follow [Kerbl et al. 2023] to adopt combination of L1 loss and SSIM loss to supervise the gaussian-splatted image: ,ğ¼ğ‘”ğ‘¡ ), Lğ‘”ğ‘  = (1 ğœ†ğ‘ ğ‘ ğ‘–ğ‘š) ğ¼ğ‘”ğ‘  ğ¼ğ‘”ğ‘¡ + ğœ†ğ‘ ğ‘ ğ‘–ğ‘š Lğ‘ ğ‘ ğ‘–ğ‘š (ğ¼ğ‘”ğ‘  (5) where ğœ†ğ‘ ğ‘ ğ‘–ğ‘š = 0.2. We use the input frames and generated views as the ğ¼ğ‘”ğ‘¡ for the observed (input) and generated (novel) views, respectively. Mesh Mask Loss. We apply an L1 loss to the rendered mesh mask to help constrain the shape of the mesh: Lğ‘šğ‘ğ‘ ğ‘˜ = ğ‘€ğ‘ğ‘Ÿğ‘’ğ‘‘ ğ‘€ğ‘”ğ‘¡ Mesh Image Loss. We employ the same loss as Lğ‘”ğ‘  to supervise the rendered mesh image ğ¼ğ‘šğ‘’ğ‘ â„: Lğ‘Ÿğ‘”ğ‘ = (1 ğœ†ğ‘ ğ‘ ğ‘–ğ‘š) ğ¼ğ‘šğ‘’ğ‘ â„ ğ¼ğ‘”ğ‘¡ + ğœ†ğ‘ ğ‘ ğ‘–ğ‘š Lğ‘ ğ‘ ğ‘–ğ‘š (ğ¼ğ‘šğ‘’ğ‘ â„ ,ğ¼ğ‘”ğ‘¡ ). (7) (6) Mesh Depth Loss. Although the balanced view sampling strategy can alleviate inconsistencies and improve the overall quality of the 3D mesh, the surface geometry still suffers from depth ambiguities, resulting in issues such as sunken surfaces or floating artifacts. To address this, we further introduce scale-invariant depth constraint: ğ‘€ Ldepth = 1 ğ‘€ğ‘”ğ‘¡ ğ‘–=1 ğ·pred ğ‘– ğ·gt ğ‘– , (8) where ğ‘€ğ‘”ğ‘¡ denotes the Ground-Truth mask. Dgt are the disparity maps predicted by Depth-Anything [Yang et al. 2024c] and rendered from the reconstructed mesh (i.e., ğ·ğ‘ğ‘Ÿğ‘’ğ‘‘ ), respectively. ğ‘– and Dpred ğ‘– During optimization, gradients can propagate back to the Gaussians and the deformation MLP through both the Gaussian splatted image and the mesh outputs, enabling updates to these components jointly. In this way, the correspondences across frames in the deformable GS are transferred to the mesh, resulting in correspondence-enabled mesh for the target object. Implementation. We use the implementation from 3D Gaussian Splatting [Kerbl et al. 2023] for differentiable Gaussian rasterization. Instead of using SfM for initialization, we initialize the 3DGS using points uniformly sampled from sphere of radius 1. The model is trained for 25k iterations, with the first 3k optimizing only the 3D Gaussians for the first frame. Joint training of 3D Gaussians and the deformation field follows, and from iteration 12k, DPSR with Marching Cubes is introduced to optimize the mesh geometry. By default, ğ‘ = 6 novel views are uniformly sampled along circular trajectory using SV3D [Voleti et al. 2025]. All reconstructions were Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy 11 Fig. A12. Workflow of data pre-processing in consistent 3D proxy reconstruction. conducted against white background at resolution of 576 576 on an NVIDIA A100 GPU. For the optimized networks, such as the deformation field ğœƒ and the color network ğ‘€ğ¿ğ‘ƒğ‘ (), we follow the same implementations as in [Yang et al. 2024a]. Additionally, the backward deformation field ğœƒ 1 adopts the same network as ğœƒ . Considering the inconsistency caused by the novel views, for all losses, we apply smaller weight (1/5) to novel views than the observed view. Querying the color during training. We adopt the approach of [Liu et al. 2024], storing vertex colors in canonical color MLP, denoted as ğ‘€ğ¿ğ‘ƒğ‘ (). The vertex color query process for the deformed mesh proceeds as follows: Deformation Query: Given the canonical Gaussians ğºğ‘ , we query their deformation of ğºğ‘ at time via the deformation field ğœƒ (ğºğ‘ ). Gaussian Wraping: The deformed Gaussians ğºğ‘¡ are computed as ğºğ‘¡ = ğºğ‘ + ğœƒ (ğºğ‘ ). Mesh Extraction: Using DPSR combined with Marching Cubes (MC), we convert ğºğ‘¡ into its corresponding deformed mesh. Canonical Mapping: The deformed mesh is projected back to the canonical space via the inverse deformation field ğœƒ 1 (). Color Assignment: Vertex colors are queried from ğ‘€ğ¿ğ‘ƒğ‘ () in the canonical space and transferred to the deformed mesh, leveraging their one-to-one vertex correspondence. A.4 Additional Experiments We also conduct experiments to evaluate the use of novel views: (1) determining which time slots (i.e., frames) utilize novel views, with the number of novel views ğ‘ fixed; and (2) evaluating the impact of the number of novel views ğ‘ , where novel views are applied to all frames. The results in Tab. A4 demonstrate that (1) providing novel views for only key frames is insufficient and (2) due to the inconsistencies introduced by novel views, using more novel views does not necessarily yield better results. Thus, by default, we utilize six generated novel views for each frame to balance the reconstruction completeness and novel-views inconsistency. Table A4. Ablation study on the use of novel views. [f, m, l] represent the first, middle, and last frames, respectively. Note that all the metrics in this experiment are evaluated on six views. Metrics Time slots uses novel views All [f] [f,m,l] [f,m] Number of novel views 6 3 PSNR SSIM LPIPS DINO Sim. 22.53 0.919 0.054 0.428 22.40 0.919 0.054 0.445 22.60 0.921 0.054 0.410 23.45 0.925 0.054 0. 23.59 0.926 0.053 0.405 24.04 0.928 0.054 0.419 23.30 0.925 0.054 0.389 EDITING 3D PROXY WITH AUTOMATIC"
        },
        {
            "title": "PROPAGATION",
            "content": "B."
        },
        {
            "title": "Implementation Details",
            "content": "Our approach supports diverse video editing tasks, including but not limited to pose editing, rotation, scaling, translation, texture modification, and object composition. While the key idea and implementation rely on our dual-propagation strategy, specific task implementation may involve task-dependent settings and variations. General Editing. For four types of editing, pose editing, rotation, translation, and scaling, users only need to apply their desired modifications to the canonical mesh once, after which the edits are automatically propagated across all frames. Object Composition. To composite static object into dynamic object, we first generate the 3D mesh of the new object. The generated mesh is then imported into Blender, where it is manually positioned to align with the intended location, forming new edited mesh. Next, an anchor vertex is selected from the canonical mesh of the original dynamic object by finding the closest vertex to the new object. The motion of this anchor vertex is then transferred to all vertex of the new object, ensuring consistent movement within the scene. Additionally, users can achieve more diverse motion effects by applying complex binding strategies to the new object. Texture modification. Texture editing in this context refers to directly modifying the vertex color of the mesh (e.g., editing the flower in Fig. E16). Since the canonical mesh does not store color information, we replace it with the mesh from the first frame (i.e., ğ‘¡ = 0). Users can modify the texture color using Blenders vertex 12 Yuhao Liu, Tengfei Wang, Fang Liu et al. paint function or employ advanced UV mapping techniques for more precise adjustments. GENERATIVE RENDERING FROM 3D PROXY C.1 Data Preparation As our decoupled video diffusion model operates in self-supervised manner, we provide more details on the data construction process. Reference Object. For target object in video, we first extract the object from the background using the ground-truth mask provided in the VOS dataset. Next, we apply random scaling, shifting, and rotation augmentations to all frames simultaneously, ensuring that the augmentation parameters remain consistent across all frames (including both the image and the corresponding mask). After augmentation, the background of the target object is filled with uniform gray color. In addition, during the training process, we also randomly re-order the input video frames to simulate the reference object. In this way, we simulate the generation of pre-editing video data. Texture Control. Since our dual-propagation strategy relies on nearest-neighbor mapping for texture propagation, the texture colors exhibit locally similar pattern. To simulate this characteristic, we first apply Simple Linear Iterative Clustering (SLIC) to segment each frame into multiple small patches, with the number of patches randomly selected from the range [800, 1200]. Next, we use median blur filtering to smooth details within the patches and reduce boundary artifacts, with the kernel size randomly chosen from the range [3, 11]. Finally, consecutive downsampling and upsampling operations are applied, with the scaling factor randomly selected from the range [0.25, 0.5]. In this way, we simulate the coarse texture after editing. Geometry control.We utilize Depth-Anything [Yang et al. 2024c], to generate depth maps of the Ground-Truth object. These depth maps are then converted into normal maps, serving as the geometry control input for the geometry controller. C."
        },
        {
            "title": "Inference Workflow",
            "content": "The inference pipeline is shown in Fig. C13. Formally, we denote the input video as Vğ‘–ğ‘›ğ‘ . The object mask for editing, obtained from SAM, is represented as Mğ‘–ğ‘›ğ‘ . The edited color is denoted by Vğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘’ğ‘‘ , and the edited normal map is represented as Vğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘’ğ‘‘ ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ . The target object mask is extracted by SAM and denoted by Mğ‘¡ğ‘”ğ‘¡ . We then obtain the inpainting mask Mğ‘–ğ‘›ğ‘ğ‘ğ‘–ğ‘›ğ‘¡ by subtracting the overlapping regions of Mğ‘–ğ‘›ğ‘ and Mğ‘¡ğ‘”ğ‘¡ from Mğ‘–ğ‘›ğ‘ : ğ‘Ÿğ‘”ğ‘ Mğ‘–ğ‘›ğ‘ğ‘ğ‘–ğ‘›ğ‘¡ = Mğ‘–ğ‘›ğ‘ (Mğ‘–ğ‘›ğ‘ Mğ‘¡ğ‘”ğ‘¡ ) (9) To reduce the distraction from the backgrounds of the input video Vğ‘–ğ‘›ğ‘ , we mask its background and only retain its foreground object, to obtain the reference object video Vğ‘Ÿğ‘’ ğ‘“ : Vğ‘Ÿğ‘’ ğ‘“ ,Vğ‘ğ‘” = Split(Vğ‘–ğ‘›ğ‘ ,Mğ‘–ğ‘›ğ‘ ) (10) where Split() denotes the binary separating function and Vğ‘ğ‘” represents the background information, in which the foreground regions in Vğ‘ğ‘” are filled with white color, and the background regions in Vğ‘Ÿğ‘’ ğ‘“ are filled with gray color, respectively. Then, the texture control map can be obtained by: = ğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ ğ‘¡ğ‘’ğ‘¥ ğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘’ğ‘‘ ğ‘Ÿğ‘”ğ‘ We then obtain the geometry control map Vğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ background region in Vğ‘’ğ‘‘ğ‘–ğ‘¡ğ‘’ğ‘‘ ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ with gray color. ğ‘”ğ‘’ğ‘œ Mğ‘¡ğ‘”ğ‘¡ + Vğ‘ğ‘” (1 Mğ‘¡ğ‘”ğ‘¡ ) (11) by replacing the ğ‘¡ğ‘’ğ‘¥ With all inputs ready, we first convert the reference object Vğ‘Ÿğ‘’ ğ‘“ and the texture control Vğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ from pixel-space to latent space using VAEs encoder. We then apply nearest-neighbor down-sampling to the target mask Mğ‘¡ğ‘”ğ‘¡ and the inpainting mask Mğ‘–ğ‘›ğ‘ğ‘ğ‘–ğ‘›ğ‘¡ to resize them. For geometry control, we first extract its features using several consecutive convolutional blocks with down-sampling [Zhang et al. 2023a]. These extracted features are then concatenated with the noisy latents obtained from the base model and passed into the control branch. The CLIP image embeddings extracted from the reference object Vğ‘Ÿğ‘’ ğ‘“ are also used as Key and Value in the cross-attention of the base model and the control branch. Finally, the denoised latents are sent to the VAE decoder to generate the output editing results. C."
        },
        {
            "title": "Implementation Details",
            "content": "We use the I2V release of the stable video diffusion (SVD) [Blattmann et al. 2023] as the base model. Our model is trained on the VOS [Xu et al. 2018] dataset 4, filtering out extremely small-sized objects from the original 7800 unique objects to retain 5968 samples. As we do not have real paired data (source/target videos), we generate training dataset via on-the-fly augmentation (Supp. C.1). We use two-stage mixed-training method, first training the geometry controller and then fixing it to train the denoising UNet. In both stages, we adopt the same denoising loss as in SVD. The two training stages are optimized for 120K and 60K iterations, respectively, using the Adam [Loshchilov 2017] optimizer on 8 NVIDIA A100 GPUs for 7 days. Each GPU processes batch size of 1 with resolution of 384256. During testing, the default resolution is 768 512, but the model supports higher resolutions. During training, classifier-free guidance is applied to the reference object with probability of 0.1. During the inference, we use the Euler [Karras et al. 2022] sampler with 25 sampling steps and classifier-free guidance scale of 3. C.4 Experimental Details Metrics. We adopt the widely-used video editing evaluation metrics: Fram-Acc and Tem-Con. Fram-Acc measures the frame-wise editing accuracy, defined as the percentage of frames where the edited image achieves higher CLIP similarity to the target prompt than to the source prompt. Tem-Con evaluates temporal consistency by calculating the cosine similarity between consecutive frame pairs. However, for fine-grained edit types, CLIP cannot measure the consistency of objects before and after editing. To this end, we introduce new metric, CLAPScore (CLIP-APpearance Score), to jointly consider textual and semantic consistency. CLAPScore measures the accumulative error of the Fram-Acc and the DINO similarity score: CLAP Score = Fram-Acc DINO(ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡, ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡), where ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ and ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ represent the input and edited frames. 4Note that all examples used during testing are entirely unseen during training. Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy 13 Fig. C13. Inference pipeline of our decoupled video diffusion. V3DBench . To evaluate the fine-grained video editing capabilities, we collect new benchmark dataset. The videos in this dataset are sourced from the Internet [Pexels [n. d.]], the DAVIS dataset [Perazzi et al. 2016], and generated videos [Brooks et al. 2024; Chai et al. 2023]. This dataset includes diverse video content across categories such as animals, humans, vehicles, and etc., covering six types of video editing tasks: pose editing, rotation, scaling, translation, texture modification, and object composition. As many video editing methods rely on text prompts, we leverage GPT-4o to generate source, target, and instruction-based prompts by providing the input keyframe along with editing details. Specifically, we first manually edit keyframe, typically the first frame, using ImageSculpturing [Yenphraphai et al. 2024], which serves as the edited reference. The original and edited frames are then input to GPT-4o, accompanied by specific editing instructions to generate appropriate prompts. The prompt to GPT-4o is: Describe the difference between the two images before and after the object is edited. Note that you should describe the object and its state. You do not need to mention phrases like object xx does not exist or no operation was done in the source prompt; simply describe the state of the edited object in the target prompt. Avoid using vague or redundant phrases, such as in simple and unmodified state. The editing for this case is [xxx]. We will release this benchmark dataset. User Study. In addition to measuring high-level input-output similarity, we conduct user study to evaluate the perceptual quality of our approach in two aspects: editing quality (EQ) and semantic consistency (SC). We collect results from four methods compared and from our own on the V3DBench benchmark. The videos were randomized and presented to 45 participants. For editing quality, participants ranked the results based on their alignment with the edited 3D mesh, with higher alignment receiving higher rank. For semantic consistency, rankings were based on the degree of variation in the object before and after editing, where greater variation resulted in lower rank. The results in Table 1 of the main paper indicate that our method consistently outperforms others and is the most preferred. DISCUSSION D.1 Pipeline Design While one might initially perceive our framework as an intricate integration of multiple components, it is designed explicitly to address several crucial challenges in 3D-aware video editing through modular structure. Specifically, our method introduces: (1) novel-view augmentation strategy combined with balanced-view sampling to significantly improve 3D proxy reconstruction consistency; (2) dual-propagation strategy utilizing canonical Gaussian and mesh representations to efficiently propagate geometry and texture edits without the need for frame-by-frame manual adjustments; and (3) decoupled video diffusion model trained with self-supervised mixed-training strategy to ensure appearance consistency, conditioned directly on the geometry and texture from the manipulated 3D proxies. This clearly structured pipelineconsisting of distinct stages for reconstruction, interactive manipulation, and generative renderingfacilitates ease of use, maintainability, and future extensibility. Therefore, the perceived complexity is actually strategic, enhancing the robustness and flexibility of our approach (e.g., swapping in improved reconstruction or diffusion modules), and the 14 Yuhao Liu, Tengfei Wang, Fang Liu et al. Table D5. Inference time comparison between our method and state-of-theart (SOTA) methods. To ensure fair comparison with I2V-Edit, we first apply Image-Sculpting to edit the initial frame and then use the resulting edited frame as input for the subsequent process. Methods Tune-a-video Pix2Video I2V-Edit Drag-Video Ours Time (mins) 10.7 2.8 44.3 12.2 91. integration of these innovative elements is what enables our novel and effective solution. D.2 Efficiency Comparisons We conducted the inference speed comparisons at fixed resolution (512512) for 14-frame video using single A100 GPU. The result is shown in Tab. D5. Note that we exclude manual editing time, which naturally varies based on the complexity and user interaction. Although our framework requires relatively more computational time, with approximately 91 minutes for reconstruction and 43 seconds for the video diffusion model stage, this trade-off is justified by significantly improved geometric fidelity and temporally coherent appearances, as demonstrated in our experiments (Tab. 1 & Fig. 6 of the main paper). Moreover, the design of our pipeline emphasizes practical usability: once the initial 3D proxy reconstruction is completed, multiple subsequent edits can be efficiently performed without re-optimization (e.g., rotate the object or adjust its pose), unlike most alternatives, which require full recomputation for each new edit. Thus, despite its higher initial reconstruction time, our approach provides substantial long-term efficiency benefits and enhanced editing quality, aligning closely with practical usage scenarios where precision and consistency are paramount. We also believe that future developments in faster 4D reconstruction methods for video will further reduce the computational overhead, further improving the practicality of our approach. D.3 Topology of Extracted Mesh Topology and Quality of Extracted Meshes. potential concern in the propagation of geometry is that extracted meshes might exhibit topological inconsistencies, such as disconnected components or internal holes. To address this, we enhance mesh quality by reducing geometric inconsistencies through balanced-view-sampling (BVS) and scale-invariant depth constraint. These strategies help mitigate artifacts such as sunken surfaces (holes) and floating outliers. The effectiveness of these measures is validated through ablation studies presented in Fig.7 of the main paper. Reconciling Topological Differences. Meshes extracted at different temporal instances may differ in topology. Given that the number of Gaussian points remains fixed throughout propagation, we utilize Gaussian points as intermediaries to reconcile these topological discrepancies. Specifically, we impose mask, RGB, and depth constraints during reconstruction to align mesh vertices closely with Gaussian points, thereby ensuring consistent geometry representation. D.4 Baseline Reconstruction Performance In Fig.7 (row 1) of the main paper, the baseline configuration (without novel views) utilizes only the input frames directly for dynamic 3D reconstruction, optimizing solely the observed view across time steps. Consequently, this configuration yields inferior reconstruction quality for unseen viewpoints due to its inherent lack of multi-view constraints. While state-of-the-art image-to-3D methods achieve impressive single-image reconstructions, their performance primarily focuses on static objects. These methods typically lack explicit inter-frame correspondence modeling, making them unsuitable for maintaining the temporal consistency necessary for coherent dynamic video editing tasks. D.5 Influence of the 3D Proxy Quality The reconstructed geometry and texture from the rendered 3D proxy serve as critical control signals guiding the video generation process. The quality of this proxy can impact the effectiveness and coherence of subsequent editing tasks. For instance, as illustrated in Fig.9 and quantified in Tab.3 of the main paper, removing texture controls (i.e., relying only on stage-1 geometry control) results in notably diminished visual quality. This degradation is primarily due to limited data augmentation during model training, as well as spatial misalignment between the target object before and after editing. Thus, inaccuracies or lower fidelity in the proxys geometry and texture inherently propagate through the pipeline, leading to compromised appearance and coherence in the final generated video. Incorporating more advanced or robust video reconstruction methods capable of providing higher-quality proxies could potentially enhance the overall coherence and controllability of the generated video, opening avenues for further improvements in dynamic editing tasks. D.6 User APIs Our framework uses Blenders intuitive and widely-used API along with automatic rigging tools (e.g., Mixamo5 and Anything World6) to streamline the mesh manipulation. For instance, once the canonical mesh is reconstructed, we can first apply auto-rigging to generate skeletal structure. Subsequently, the geometry of the object can be easily controlled by manipulating skeletal keypoints. D.7 Long Video Processing In the video diffusion stage, to handle sequences exceeding the default 14-frame limitation of the SVD model, we divide the video into several overlapping windows. The overlapping regions from previous windows are utilized to initialize the noise of subsequent windows via an SDEdit [Meng et al. 2021]-based initialization strategy. Then, overlapping segments across windows are seamlessly merged using progressive alpha-blending mechanism, where the weight of the previous window gradually decreases from 1 to 0, while the weight of the next window correspondingly increases from 0 to 1 across overlapping frames, ensuring smooth temporal transitions. In Fig. D14, we present an example of editing results on 9.3-second video. The result exhibits consistent geometry and 5https://www.mixamo.com/ 6https://anything.world/ Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy 15 Fig. D14. 9.3-second edited video example demonstrating our methods ability to handle long video sequences. appearance throughout, highlighting temporal coherence over extended durations. D.8 Limitations We consider several limitations of our method. (1) Our approach struggles to generate realistic visual effects around the object (see Fig. D15), such as shadows and reflections, due to data scarcity. This limitation could be addressed by constructing scene-level 3D models [Bruce et al. 2024] with Physically Based Rendering for editing or by collecting and training on large-scale paired datasets [Winter et al. 2024]. (2) Our method struggles with the faithful reconstruction of details (e.g., human faces) on the target object. This is due to inconsistencies in novel views that cause an averaging gray effect during reconstruction and the VAE to lose further detail in these regions. Leveraging more consistent novel views [Wu et al. 2024a] and better VAE models may mitigate this issue. Fig. D15. Limitation. Our approach struggles to generate the corresponding visual effects of the target object (e.g., reflections, as highlighted by the red arrow on the water for the white swan). Edited mesh is shown in the middle."
        },
        {
            "title": "E ADDITIONAL VISUAL RESULTS",
            "content": "We present additional visual results of our approach in Fig. E16, showcasing various editing types, including pose editing, rotation, translation, texture modification, object composition, and mixed edits (i.e., combinations of different editing types)."
        },
        {
            "title": "REFERENCES",
            "content": "Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine SÃ¼sstrunk. 2012. SLIC superpixels compared to state-of-the-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence 34, 11 (2012), 22742282. Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. 2024. Tc4d: Trajectory-conditioned text-to-4d generation. In European Conference on Computer Vision. Springer, 5372. Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022. Text2live: Text-driven layered image and video editing. In European conference on computer vision. Springer, 707723. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. 2024. Video generation models as world simulators. https: //openai.com/research/video-generation-models-as-world-simulators (2024). Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. 2024. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning. Shengqu Cai, Duygu Ceylan, Matheus Gadelha, Chun-Hao Paul Huang, Tuanfeng Yang Wang, and Gordon Wetzstein. 2024. Generative rendering: Controllable 4d-guided video generation with 2d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 76117620. Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. 2023. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2320623217. Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. 2023. Stablevideo: Text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2304023050. Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. 2023. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840 (2023). Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liangyan Gui, Matheus Gadelha, Paul Guerrero, and Nanxuan Zhao. 2025. 3D-Fixup: Advancing Photo Editing with 3D Priors. In Proceedings of the SIGGRAPH Conference Papers. ACM. https://doi.org/10.1145/3721238. Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. 2024. Dragvideo: Interactive drag-style video editing. In European Conference on Computer Vision. Springer, 183199. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. 2023. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Xiang Fan, Anand Bhattad, and Ranjay Krishna. 2024. Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion. arXiv preprint arXiv:2403.14617 (2024). Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. 2025. Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control. arXiv preprint arXiv:2501.03847 (2025). Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, et al. 2024b. I2v-adapter: general imageto-video adapter for diffusion models. In ACM SIGGRAPH 2024 Conference Papers. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2024a. AnimateDiff: Animate Your Personalized 16 Yuhao Liu, Tengfei Wang, Fang Liu et al. Fig. E16. Additional visual results of our method. For each case, three or four input frames are shown on the left, and the edited results are displayed on the right. For each case, we highlight the editing using the red arrow in the first frame, except the texture modification. Text-to-Image Diffusion Models without Specific Tuning. International Conference on Learning Representations (2024). Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Zhihao Hu and Dong Xu. 2023. Videocontrolnet: motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073 (2023). Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. 2024. Animate3d: Animating any 3d model with multi-view video diffusion. arXiv preprint arXiv:2407.11398 (2024). Mizuki Kagaya, William Brendel, Qingqing Deng, Todd Kesterson, Sinisa Todorovic, Patrick Neill, and Eugene Zhang. 2010. Video painting with space-time-varying style parameters. IEEE transactions on visualization and computer graphics 17, 1 (2010), 7487. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems 35 (2022), 2656526577. Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 2023. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. 42, 4 (2023), 1391. Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. 2024. AnyV2V: Tuning-Free Framework For Any Video-to-Video Editing Tasks. Transactions on Machine Learning Research (2024). Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux. Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. 2020. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics (ToG) 39, 6 (2020), 114. Isabella Liu, Hao Su, and Xiaolong Wang. 2024. Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular Videos. arXiv preprint arXiv:2404.12379 (2024). Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. 2023. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 (2023). Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, and Shifeng Chen. 2024. GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14301440. Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. 2023. Magicstick: Controllable video editing via control handle transformations. arXiv preprint arXiv:2312.03047 (2023). Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 (2021). Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. 2023. Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing Systems 36 (2023), 34973516. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1 (2021), 99106. Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. 2024. ReVideo: Remake Video with Motion and Content Control. arXiv preprint arXiv:2405.13865 (2024). Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. 2024. I2VEdit: FirstFrame-Guided Video Editing via Image-to-Video Diffusion Models. In SIGGRAPH Asia 2024 Conference Papers. Xingang Pan, Ayush Tewari, Thomas LeimkÃ¼hler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. 2023. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 Conference Proceedings. Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy Mitra. 2024. Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 76957704. Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger. 2021. Shape as points: differentiable poisson solver. Advances in Neural Information Processing Systems 34 (2021), 1303213044. Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. 2016. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition. 724732. Pexels. [n. d.]. PEXELS. https://www.pexels.com. Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy 17 Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1031810327. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In ICML. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al. 2024. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024). Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al. 2024. L4gm: Large 4d gaussian reconstruction model. Advances in Neural Information Processing Systems 37 (2024), 5682856858. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In CVPR. Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. 2024. Motioni2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers. 111. Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. 2023. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023). Inkyu Shin, Qihang Yu, Xiaohui Shen, In So Kweon, Kuk-Jin Yoon, and Liang-Chieh Chen. 2024. Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting. arXiv preprint arXiv:2406.02541 (2024). Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising Diffusion Implicit Models. In ICLR. Yang-Tian Sun, Yi-Hua Huang, Lin Ma, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. 2024. Splatter Video: Video Gaussian Representation for Versatile Processing. arXiv preprint arXiv:2406.13870 (2024). Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. 2025. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision. Springer, 118. Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, and Xihui Liu. 2023. Drag-a-video: Non-rigid video editing with point-based interaction. arXiv (2023). Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. 2025. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision. Springer, 439457. Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. 2022. Pretraining is All You Need for Image-to-Image Translation. In arXiv. Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Hancke, Ziwei Liu, and Rynson WH Lau. 2025. Phidias: generative model for creating 3d content from text, image, and 3d conditions with reference-augmented diffusion. ICLR (2025). Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. 2024. ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion. arXiv preprint arXiv:2403.18818 (2024). Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 2024b. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2031020320. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023. Tune-a-video: Oneshot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 76237633. Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. 2024a. Cat4d: Create anything in 4d with multi-view video diffusion models. arXiv preprint arXiv:2411.18613 (2024). Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. 2024. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470 (2024). Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. 2018. Youtube-vos: large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327 (2018). Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. 2024c. Depth Anything V2. arXiv preprint (2024). Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. 2024b. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers. 112. Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. 2024a. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, and Saining Xie. 2024. Image sculpting: Precise object editing with 3d geometry control. In Proceedings of 18 Yuhao Liu, Tengfei Wang, Fang Liu et al. the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 42414251. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023a. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. 2023b. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077 (2023)."
        }
    ],
    "affiliations": [
        "City University of Hong Kong, Hong Kong SAR, China",
        "Tencent, China"
    ]
}