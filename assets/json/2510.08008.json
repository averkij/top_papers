{
    "paper_title": "Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training",
    "authors": [
        "Ruizhe Wang",
        "Yucheng Ding",
        "Xiao Liu",
        "Yaoxiang Wang",
        "Peng Cheng",
        "Baining Guo",
        "Zhengjun Zha",
        "Yeyun Gong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerous computational costs have been invested in existing well-trained checkpoints, but many of them remain underutilized due to engineering constraints or limited model capacity. To efficiently reuse this \"sunk\" cost, we propose to recycle pretrained checkpoints by expanding their parameter counts and continuing training. We propose orthogonal growth method well-suited for converged Mixture-of-Experts model: interpositional layer copying for depth growth and expert duplication with injected noise for width growth. To determine the optimal timing for such growth across checkpoints sequences, we perform comprehensive scaling experiments revealing that the final accuracy has a strong positive correlation with the amount of sunk cost, indicating that greater prior investment leads to better performance. We scale our approach to models with 70B parameters and over 1T training tokens, achieving 10.66% accuracy gain over training from scratch under the same additional compute budget. Our checkpoint recycling approach establishes a foundation for economically efficient large language model pretraining."
        },
        {
            "title": "Start",
            "content": "RECYCLING PRETRAINED CHECKPOINTS: ORTHOGONAL GROWTH OF MIXTURE-OF-EXPERTS FOR EFFICIENT LARGE LANGUAGE MODEL PRE-TRAINING Ruizhe Wang 1,2 Yucheng Ding 2,3 Xiao Liu 2 Yaoxiang Wang 2,4 Peng Cheng 2 Baining Guo 2 Zhengjun Zha 1 Yeyun Gong 2 1University of Science and Technology of China 3Shanghai Jiao Tong University 2Microsoft Research Asia 4Xiamen University"
        },
        {
            "title": "ABSTRACT",
            "content": "The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerous computational costs have been invested in existing well-trained checkpoints, but many of them remain underutilized due to engineering constraints or limited model capacity. To efficiently reuse this sunk cost, we propose to recycle pretrained checkpoints by expanding their parameter counts and continuing training. We propose orthogonal growth method well-suited for converged Mixture-of-Experts model: interpositional layer copying for depth growth and expert duplication with injected noise for width growth. To determine the optimal timing for such growth across checkpoints sequences, we perform comprehensive scaling experiments revealing that the final accuracy has strong positive correlation with the amount of sunk cost, indicating that greater prior investment leads to better performance. We scale our approach to models with 70B parameters and over 1T training tokens, achieving 10.66% accuracy gain over training from scratch under the same additional compute budget. Our checkpoint recycling approach establishes foundation for economically efficient large language model pretraining. 5 2 0 2 9 ] . [ 1 8 0 0 8 0 . 0 1 5 2 : r Figure 1: Main effect and method of our model growth framework"
        },
        {
            "title": "INTRODUCTION",
            "content": "The unprecedented success of large language models (LLMs) has been largely attributed to scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022), which suggest that increasing model size Work done during internship in Microsoft Research Asia Correspondence to yegong@microsoft.com"
        },
        {
            "title": "Preprint",
            "content": "and training data consistently improves performance. However, training these models from scratch demands enormous computational resources, and the exponential growth of this cost poses fundamental barrier to further progress. Consequently, developing methods to scale models efficiently under constrained computational budgets has become critical research challenge. Modern LLM development pipelines routinely produce smaller pre-trained model checkpoints and numerous intermediate artifacts from processes like hyperparameter tuning or preliminary evaluations. These models are often discarded once training concludes, leaving much of their potential unrealized due to inherent size constraints. We propose that these checkpoints represent massive sunk costa significant computational investment that can be systematically leveraged. Model growth offers new perspective on scaling: rather than starting from scratch, larger models can be created by recycling smaller pre-trained models, thereby inheriting their learned knowledge and optimized parameters. However, recent studies on model growth seldom investigate its application to fully converged models. Existing works (Shen et al., 2022; Du et al., 2024) typically grow models after only brief initial training period, scenario that fails to leverage significant sunk costs. This work addresses more pressing question: what is the optimal method for growing well-trained model to maximize the return on its substantial sunk cost? Besides, with the increasing adoption of Mixture-of-Experts (MoE) architectures, it is crucial to investigate the effect of model growth on such structures, but to the best of our knowledge, this topic has not been systematically studied until now. To address this gap, we develop framework specifically for well-converged MoE models, proposing two orthogonal growth strategies: depth-wise expansion (adding layers) and width-wise expansion (increasing the number of experts), as illustrated in fig. 1 (right). We challenge the widely adopted stacking method for layer copying (Du et al., 2024; Wu et al., 2024), hypothesizing that it is suboptimal for converged models. Instead, we propose an interpositional method that better preserves the learned structural properties of the model, such as the characteristic trend in layer-wise weight norms. Moreover, we discover that adding small amount of noise to newly copied experts is crucial as it facilitates better expert specialization. We also provide comprehensive study on the optimal timing for growth to best utilize the sunk cost. Our findings reveal strong positive correlation between the amount of pre-training (measured in sunk FLOPs) and the final performance of the grown model. This confirms that greater initial investment leads to better final model, highlighting the efficacy of our framework in recycling prior computation. We further demonstrate that under fixed total training budget (sunk + additional FLOPs), model growth is comparable or even slightly superior to training large model from scratch. Finally, we conduct extensive experiments to demonstrate the scalability and robustness of our orthogonal growth framework. As shown in fig. 1 (left), our method effectively scales an MoE model from 17 billion to 70 billion parameters using 1-trillion-token dataset. The resulting model achieves 10.66% average accuracy improvement on downstream tasks compared to model trained from scratch with the same additional FLOPs budget. In summary, our primary contributions are: We identify the interposition method as superior to stacking method for depth-growing converged models, as it better preserves the models learned internal structure. We also introduce an optimized strategy for MoE width growth, showing that injecting Gaussian noise into new experts is critical for promoting effective specialization. We provide comprehensive study on the optimal timing for model growth. We establish strong positive correlation between the sunk cost (prior computation) of base model and the final performance of the grown model. We validate the scalability of our framework by growing 17B MoE model into highperforming 70B model, which achieves 10.66% accuracy gain over scratch-trained baseline under the same extra FLOPs budget."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Efficient Pretraining. One direct approach for efficient model pretraining cost usage is to reduce computational costs, like model quantization (Jacob et al., 2018; Micikevicius et al., 2017; Peng et al., 2023; Wang et al., 2025), model pruning (Zhu & Gupta, 2017; Xia et al., 2022; Ma et al., 2023), and distillation (Gou et al., 2021; Loureiro et al., 2021; Sreenivas et al., 2024). An alternative approach focuses on reusing sunk cost to reduce the final training cost of the large model, like model growth (Shen et al., 2022) and upcycling (Komatsuzaki et al., 2023). Model Growth for Pretraining. Model growth, or model expansion, is technique to increase the number of parameters of pre-trained models or within the training process. Previous works such as Net2Net (Chen et al., 2015) focus on CNN models, while Bert2Bert (Chen et al., 2022), StackedBert (Gong et al., 2019), and MSG (Yao et al., 2024) have explored model growth techniques for BERT models. LEMON (Wang et al., 2024) and LiGO (Wang et al., 2023) further extend these approaches to other architectures such as vision transformers and DeiT. For Transformer-based architectures, studies such as Shen et al. (2022), Du et al. (2024), and Wang et al. (2024) investigate optimal growth strategies and initialization techniques, but these works are limited to relatively small models that are not trained on large-scale datasets. In the context of Large Language Models (LLMs), LLaMA Pro (Wu et al., 2024) proposes expanding the pre-trained LLaMA2-7B model to 8.3B parameters and fine-tuning it on new corpora, thereby improving knowledge coverage while mitigating catastrophic forgetting. Technical reports on Solar 10.7B (Kim et al., 2024) and FLM-101B (Li et al., 2023) also describe the adoption of model growth in large-scale pretraining, though details of the techniques and analyses are limited. Mixture-of-Experts Model Upcycling. Mixture-of-Experts (MoE) (Shazeer et al., 2017; Zhou et al., 2022; Mu & Lin, 2025) is classic model architecture widely adopted in large-scale models such as DeepSeek, Qwen-3, and LLaMA-4. Unlike the traditional Transformer architecture, MoE expands the Multi-Layer Perceptron (MLP) layers into multiple experts but activates only subset during training. This design increases the overall model capacity while keeping the computational cost manageable. In contrast, traditional Transformer models without such sparsity are referred to as dense models. Recent works propose to initialize MoE models with existing dense checkpoints such as Sparse Upcycling (Komatsuzaki et al., 2023), thus reusing the sunk cost. Nakamura et al. (2025) and He et al. (2024) further explore this approach by introducing randomness or modifying expert granularity when transforming dense MLP layers into expert layers. Several technical reports, including Qwen-2 (Team, 2024) and Skywork-MoE (Wei et al., 2024), adopt this strategy to train MoE models from dense checkpoints. We extend this line of work by expanding existing MoE models into larger ones."
        },
        {
            "title": "3 GROWTH METHOD",
            "content": "This section introduces orthogonal growth strategies for Mixture-of-Experts (MoE) models. In section 3.1, we introduce Depth Growth, method for expanding model by duplicating its layers. In section 3.2, we present Width Growth, which involves expanding the number of experts. Finally, in section 3.3, we compare these two strategies and outline their respective advantages. 3.1 DEPTH GROWTH Large Language Models (LLMs) are typically constructed from multiple transformer layers. Given model with layers l1, l2, . . . , ln, common method for layer-wise growth is called is stacking, which involves concatenating the original models layers sequentially times: = stack(m) = l1, l2, . . . , ln, l1, l2, . . . , ln, , l1, l2, . . . , ln (cid:125) (cid:124) (cid:123)(cid:122) times Alternatively, the interposition method duplicates each layer times in place: = interposition(m) = l1, l1, . . . , l1 , l2, l2, . . . , l2 (cid:123)(cid:122) (cid:125) (cid:125) times (cid:123)(cid:122) times (cid:124) (cid:124) , , ln, ln, . . . , ln (cid:125) (cid:124) (cid:123)(cid:122) times 3 (1) (2)"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Characteristic layer-wise weight norm distribution in pre-trained LLMs, including pretrained models in this work and from open-source community. Figure 3: Performance comparison of interposition and stack depth growth strategies. Left: training loss; Right: average downstream task accuracy. Previous studies, such as Wu et al. (2024) and Du et al. (2024), empirically advocate for the stack method. However, their work primarily focuses on the early stages of model training, before the parameter distributions of different layers have significantly diverged. We will demonstrate that for well-converged checkpoints, where layers have specialized roles, the stack method can be harmful to final performance. As shown in fig. 2, the layer-wise weight norms of pre-trained models exhibit distinct pattern: the norms of the initial layers are small and variable, followed by gradual increase across the middle layers, and slight decrease in the final layers. This trend is observable across several popular open-source models (see fig. 2) and we hypothesize that it is signature of healthy, stable pretrained LLM. When grown from such converged checkpoints, clearly we should strive to maintain this upward trend in the norm as much as possible. So we hypothesize that the stack method disrupts this learned, position-dependent functional structure, whereas the interposition method preserves it, leading to better performance post-growth. We provide more examples in section to further validate this observation. To conduct an end-to-end study, we trained 3B-parameter MoE model with 20 layers and 64 experts from scratch. This model was then grown to 6B parameters to evaluate the effects of each growth strategy. In our experiments, the growth factor in eq. (2) is fixed to 2. Further details regarding model pre-training are available in section D. Figure 3 shows the results. To ensure fair comparison given the increased size of the grown models, the x-axis represents the total training Floating Point Operations (FLOPs). Based on both training loss and average downstream task accuracy1, the interposition method outperforms the stack method. We also obtain similar results on the larger 17B model in our experiments (see fig. 9 in section 5). In conclusion, for converged models rather than those in the early stages of training, the interpositional method is better choice than the widely adopted stacking method. 3.2 WIDTH GROWTH For MoE models, an alternative to increasing depth is to expand the parameter count by increasing the number of experts. To preserve the capabilities of converged MoE model during such growth, it 1The accuracy metric is computed as the average score across multiple downstream evaluation tasks, such as MMLU, ARC-C, HellaSwag, BoolQ and OpenbookQA. The computation method is detailed in section E.1, and full results are in section E.2."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The impact of noise injection scale on width growth performance. Left: training loss; Right: average downstream task accuracy. is crucial to proportionally increase the number of activated experts (the top-k parameter) as tokens must be routed to the newly added capacity. In our experiments, we simultaneously double both the total number of experts and the number of activated experts (the value of k). For an original MoE layer with experts and top-k routing scheme, the output is given by: MoE(x) = (cid:88) gi(x) fi(x), Topk (cid:0)g(x)(cid:1), g(x) RE (3) Where fi is the i-th expert and g(x) is the vector of gating weights from the router. When the number of experts is doubled to 2E and the number of activated experts to 2k, the formulation becomes: MoEgrowth(x) = (cid:88) i(x) (x), Top2k (cid:0)g(x)(cid:1), g(x) R2E (4) critical aspect of MoE training is achieving both load balancing and expert specialization, which ensures that tokens are distributed evenly across experts and that different experts learn distinct functions. In our growth scenario, we first duplicate each expert to preserve the models learned capabilities. To encourage the new experts to diverge and learn new knowledge, we propose adding Gaussian noise to the weights of the newly created experts and to the corresponding logits in the router. Specifically, we add noise with mean of 0 and standard deviation of α σorig, where σorig is the standard deviation of the original weights. The new expert and router weights are then concatenated with the original ones. To promote divergence without destabilizing the well-trained original experts, we use small value for α number such as α = 0.01. As the experimental results in fig. 4 reveal, while the language modeling loss is similar for both direct expert copying (α = 0) and the noise-addition method, the latter demonstrates better performance on downstream tasks, yielding an accuracy improvement of approximately 1%. The results also indicate that excessive noise may be harmful. These findings validate the importance of adding small magnitude of noise to stimulate expert specialization during width growth. 3.3 DISCUSSION ON DEPTH AND WIDTH GROWTH Increasing models depth and width are two orthogonal growth strategies for MoE models. In our study, we investigate the distinct characteristics of these two methods. For general downstream task performance, fig. 5 (left) shows that depth growth generally yields better results than width growth. Width growth requires more continued training for the expanded set of experts to achieve balanced load distribution and specialize effectively. Thus, its benefits are less immediate than those of depth growth. However, width growth holds significant advantage in preserving model stability. Interestingly, in fig. 5 (right) we find that evaluating checkpoint immediately after width growth (before any further training) results in only minor decrease in downstream task accuracy, or in some cases even slight improvement due to the inherent randomness of evaluation. In contrast, depth growth can disrupt the functional role of layers, and in older post-layer normalization (Post-LN) architectures like BERT and original Transformer, this would cause significant performance degradation immediately after"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Comparative analysis of performance and stability between depth and width growth. expansion. Width growth, however, effectively preserves performance post-expansion in both PreLN and Post-LN architectures. This observation suggests that width growth aligns well with the principle of Function-Preserving Transformations (Evci et al., 2022; Wang et al., 2023; Yao et al., 2024), which stipulate that models output should remain unchanged immediately after expansion. Further discussion and proof regarding this property are provided in section C."
        },
        {
            "title": "4 ANALYSIS OF GROWTH TIMING AND SUNK COST",
            "content": "Having established the efficacy of our growth methods, we now turn to critical practical question: when is the optimal time to apply them? In this section, we investigate the optimal point during the pre-training process to apply the growth strategy and compare its efficacy against training larger model from scratch. We demonstrate that even for already converged trained checkpoints, model growth can still effectively leverage the computational investment (i.e. sunk FLOPs cost). 4.1 IMPACT OF SUNK COST WITH FIXED ADDITIONAL BUDGET This analysis addresses primary questions: given series of checkpoints with varying amounts of sunk cost, which serves as the optimal base for growth? Specifically, does greater sunk cost lead to superior performance post-growth? Expanding on the experiments in section 3, we trained the 3B MoE model to full convergence using standard learning rate schedule, which included warmup, constant learning rate phase, and an annealing phase (see fig. 6). We saved series of checkpoints throughout this process, each representing different level of sunk cost. To evaluate the benefit of this investment, we conducted experiments with fixed budget for additional training FLOPs. Since depth growth ultimately yields better results than width growth, we focus exclusively on depth growth in these experiments. We selected 12 checkpoints, sampled between 8k and 96k training steps, and grew each to 6B parameters. We also include baseline where 6B model is trained from scratch, which is equivalent to growing model with zero sunk FLOPs. Figure 6: Full training curve and learning rate scheduler of 3B model pretraining. The results of growing models from different checkpoints, each with the same budget for additional FLOPs, are shown in fig. 7. Both the final training loss and the average downstream accuracy exhibit strong positive correlation with the sunk cost invested prior to growth. This indicates that"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Investigation of growth time according to amount of sunk cost. Left: loss curve. Right: downstream tasks average accuracy. Table 1: Quantitative accuracy results growth time investigation for amount of sunk cost. Start steps 0k 8k 16k 24k 32k 40k 48k 56k 64k 72k 80k 88k 96k Start acc End acc Average acc 30.59 38.79 36. 35.67 42.07 39.09 37.66 44.65 41.19 39.39 46.00 42.69 39.05 46.75 43.34 41.11 47.00 44.51 42.26 47.20 45. 43.86 47.37 46.15 43.32 47.81 46.49 43.66 47.90 47.13 44.95 48.76 47.43 46.43 48.99 47.88 46.19 48.52 47. larger initial training investment leads to better final model, confirming that the growth method effectively recycles prior computational work, and provide suggestion that latter checkpoints with more sunk cost can be leveraged to get better growth performance. We further present quantitative results in table 1 to support this positive correlation. The table reports the starting, ending, and average accuracy across the entire continued training process with an additional 3 1020 FLOPs. Notably, while the positive correlation persists when the base model enters the learning rate annealing stage (beyond 72k steps), the marginal performance gains diminish. This is likely because all grown models in this experiment are trained with the same new constant learning rate for fair comparison, which may not be optimal for checkpoint from late annealing phase. This suggests that one should either carefully tune the learning rate for the continued training phase or, preferably, select checkpoint from the constant learning rate phase for growth. 4.2 COMPARISON TO SCRATCH TRAINING WITH FIXED TOTAL BUDGET The results in fig. 7 also demonstrate that, for fixed additional training budget, model growth is clearly superior to training from scratch. We next investigate whether this advantage holds when the total FLOPs budget is fixed. The results of this experiment are presented in fig. 8. Here, models grown from later checkpoints are allocated correspondingly smaller budget for continued training. The results show that for most growth timings, the final accuracy of the grown model is comparable or slightly superior to the scratch-trained model. Specifically, models grown from earlier checkpoints, which thus allocated larger proportion of the total budget for post-growth training, tend to perform best. This suggests that the pre-trained smaller model serves as highly effective initialization for the larger models training process. The growth method underperforms only when initiated from very late checkpoint, where the budget for continued training is insufficient. This provides valuable heuristic: one should allocate additional FLOPs at least on the same order of magnitude as the sunk cost in order to achieve performance comparable to pre-training under the same total FLOPs."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Investigation of growth time according to total amount of training FLOPs. Left: loss curve. Right: downstream tasks average accuracy. Table 2: Quantitative accuracy results growth time investigation for total FLOPs. Start steps 0k 8k 16k 24k 32k 40k 48k 56k 64k End acc Average acc 45.03 45.82 47.66 46.99 48.53 47.38 47.80 47.29 48.15 47. 47.70 46.96 47.20 46.47 47.03 45.74 46.12 45.37 Quantitative results are provided in table 2 to support this finding, showing the average accuracy over the final six accuracy measurements (with the exception of line 64k, which contains only four data points). Notably, although the training loss of later checkpoints remains relatively high, the final accuracy quickly recovers during continued training. In conclusion, model growth is an effective strategy for leveraging the sunk cost of pre-trained models, with final performance positively correlating with the initial training investment. Furthermore, its effectiveness is comparable and sometimes superior to training from scratch, even when evaluated under fixed total-FLOPs budget."
        },
        {
            "title": "5 SCALABILITY EXPERIMENTS",
            "content": "The practical value of model growth depends on its scalability, since training larger models comes with proportionally higher sunk costs. To this end, we scale our experiments to 17B-parameter MoE model, which we progressively grow to 70B model over one trillion training tokens. This large-scale experiment demonstrates the robustness and effectiveness of our proposed methods. We employ the same growth techniques introduced in section 3. As preliminary step, we re-validate our findings on depth growth at the 17B scale. The 17B base models architecture is scaled-up version of the 3B model, while the complete architectural and training details are available in Appendix D. The results, shown in fig. 9, confirm Figure 9: Performance comparison of interposition and stack depth growth strategies for 17B model. Left: training loss; Right: average downstream task accuracy."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Full training loss for 17B model pretraining and growth training. Left: original loss curves. Right: zoom in for better visualization. that the interposition method remains superior to the stack method, further substantiating our central insight regarding the growth of converged checkpoints. In this scalability experiment, we first expand the models depth to increase its functional capacity, then broaden its width to enhance expert specialization. This is also good example to validate the independence and orthogonality of our proposed two growth method. First, we train the initial 17B model (with 4 activated experts) for approximately 600B tokens. At this point, we perform Depth Growth, increasing the number of layers from 28 to 54, which results in 35B model. After training this intermediate model for an additional 300B tokens, we perform Width Growth, doubling the number of experts from 96 to 192. This yields the final 70B model, which is then trained for another 100B tokens. The complete training loss curve and downstream evaluation results are presented in fig. 10 and fig. 11, respectively. The experimental results reveal critical finding: our growth method can unlock substantial performance gains even after the base models improvement has saturated following extensive training. Furthermore, the sequential application of depth and width growth creates well-proportioned final architecture, which leads to superior overall performance compared to the intermediate models. As shown in fig. 11, the final 70B model achieves an average accuracy of 64.17, representing notable improvement of 2.21 points (61.96 64.17) over the 35B checkpoint and 5.62 points (58.55 64.17) over the initial 17B model. Even under the same training FLOPs, the 70B model outperforms the 17B model by 2.96 points (approximately 4.0% relative to 61.71). From the perspective of sunk cost utilization, the growth model also demonstrates superior performance, surpassing the model trained from scratch by 6.18 points (approximately 10.6% relative to 57.99) under the same extra FLOPs budget, as shown in fig. 1. Figure 11: Downstream task evaluation result for 17B model pretraining and growth training. These large-scale results reaffirm that model growth is powerful and efficient strategy for leveraging the computational investment in existing checkpoints while pushing the performance boundaries of the resulting model."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose systematic framework for model growth, addressing the computational cost problem in large language model pretraining. We demonstrate that pre-trained checkpoints, often considered disposable assets, can be effectively recycled to create larger and more capa-"
        },
        {
            "title": "Preprint",
            "content": "ble models, thus preserving their significant sunk cost. We identify optimal strategies for two orthogonal growth dimensions in Mixture-of-Experts (MoE) models, establish scaling principle that growing from more converged checkpoint yields superior final performance, and demonstrate that our framework is highly scalable. By redefining pre-trained checkpoints as valuable foundations for future growth, our methods contribute to more sustainable and accessible path for pre-training Large Language Models."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research focuses on developing methods for efficiently scaling large language models through model growth, with the primary goal of reducing computational costs and reusing previously trained checkpoints. The study does not involve human subjects, personal data, or sensitive demographic information. Potential societal impacts of large language models are acknowledged, such as misuse for generating harmful or biased content, but this work does not introduce new risks beyond those already inherent in the use of such models. Instead, by improving training efficiency, the proposed methods may lower the environmental footprint of model development."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have taken several steps to ensure the reproducibility of our results. The main paper provides detailed descriptions of the proposed model growth framework and experimental settings, while additional implementation details, dataset composition, hyperparameters, and training configurations are included in the section D. To further facilitate reproducibility, we provide anonymized source code fragment in the supplementary materials, and we will release our training framework to facilitate further research in this area. Figures and tables referenced in the main text are generated directly from logged experimental outputs listed in section E.2, ensuring that reported results can be consistently verified."
        },
        {
            "title": "REFERENCES",
            "content": "Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 21342148, 2022. Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457, 2018. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, and Jie Fu. Stacking your transformers: closer look at model growth for efficient llm pre-training. In Proceedings of the 38th International Conference on Neural Information Processing Systems, pp. 1049110540, 2024."
        },
        {
            "title": "Preprint",
            "content": "Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Max Vladymyrov, and Fabian PearXiv preprint dregosa. Gradmax: Growing neural networks using gradient information. arXiv:2201.05125, 2022. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In International conference on machine learning, pp. 23372346. PMLR, 2019. Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. International journal of computer vision, 129(6):17891819, 2021. Ethan He, Abhinav Khattar, Ryan Prenger, Vijay Korthikanti, Zijie Yan, Tong Liu, Shiqing Fan, Ashwath Aithal, Mohammad Shoeybi, and Bryan Catanzaro. Upcycling large language models into mixture of experts. arXiv preprint arXiv:2410.07524, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Bi Huo, Bin Tu, Cheng Qin, Da Zheng, Debing Zhang, Dongjie Zhang, En Li, Fu Guo, Jian Yao, Jie Lou, Junfeng Tian, Li Hu, Ran Zhu, Shengdong Chen, Shuo Liu, Su Guang, Te Wo, Weijun Zhang, Xiaoming Shi, Xinxin Peng, Xing Wu, Yawen Liu, Yuqiu Ji, Ze Wen, Zhenhai Liu, Zichao Li, and Zilong Liao. dots.llm1 technical report, 2025. URL https://arxiv.org/abs/ 2506.05767. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 27042713, 2018. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Sanghoon Kim, Dahyun Kim, Chanjun Park, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pp. 2335, 2024. Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. In The Eleventh International Conference on Learning Representations, 2023. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024."
        },
        {
            "title": "Preprint",
            "content": "Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, et al. Flm-101b: An open llm and how to train it with $100 budget. arXiv preprint arXiv:2309.03852, 2023. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. LogiQA: Challenge Dataset for Machine Reading Comprehension with Logical Reasoning. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 36223628, 2021. Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5(5):5, 2017. Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Learning curves of generic features maps for realistic datasets with teacherstudent model. Advances in Neural Information Processing Systems, 34:1813718151, 2021. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can Suit of Armor Conduct Electricity? New Dataset for Open Book Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, 2018. Siyuan Mu and Sen Lin. comprehensive survey of mixture-of-experts: Algorithms, theory, and applications. arXiv preprint arXiv:2503.07137, 2025. Taishi Nakamura, Takuya Akiba, Kazuki Fujii, Yusuke Oda, Rio Yokota, and Jun Suzuki. DropIn The Thirteenth upcycling: Training sparse mixture of experts with partial re-initialization. International Conference on Learning Representations, 2025. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer language models. In International Conference on Machine Learning, pp. 19893 19908. PMLR, 2022. Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil Mahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, et al. Llm pruning and distillation in practice: The minitron approach. arXiv preprint arXiv:2408.11796, 2024. Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024."
        },
        {
            "title": "Preprint",
            "content": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024. Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. In The Eleventh International Conference on Learning Representations, 2023. Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, and Peng Cheng. Optimizing large language model training using fp4 quantization. arXiv preprint arXiv:2501.17116, 2025. Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, and Hongxia Yang. Lemon: Lossless model expansion. In The Twelfth International Conference on Learning Representations, 2024. Tianwen Wei, Bo Zhu, Liang Zhao, Cheng Cheng, Biye Li, Weiwei Lu, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Liang Zeng, et al. Skywork-moe: deep dive into training techniques for mixtureof-experts language models. arXiv preprint arXiv:2406.06563, 2024. Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, and Ping Luo. Llama pro: Progressive llama with block expansion. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 65186537, 2024. Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Junbo Zhao, Lin Liu, Zenan Huang, Zhenzhong Lan, Bei Yu, and Jianguo Li. Grovemoe: Towards efficient and superior moe llms with adjugate experts. arXiv preprint arXiv:2508.07785, 2025. Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate models. arXiv preprint arXiv:2204.00408, 2022. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language model pre-training. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=rL7xsg1aRn. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can Machine Really Finish Your Sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114, 2022. Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017."
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "Large Language Models (LLMs) were used only to polish the writing (e.g., grammar, style, and readability). All research ideas, methods, experiments, and analyses were fully developed and conducted by the authors. MORE RESULTS ON LAYER-WISE NORM DISTRIBUTION We further extend our analysis by examining broader range of open-source MoE models. Specifically, we compute and visualize the layer-wise average weight norm distributions for Deepseekv2-Lite-16B-A2.4B (DeepSeek-AI, 2024), Qwen1.5-MoE-14.3B-A2.7B-Chat (Yang et al., 2025), Mixtral-8x7B (Jiang et al., 2024), Hunyuan-A13B-Instruct (Sun et al., 2024), Dots-LLM1-142BA14B (Huo et al., 2025), and GroveMoE-Inst-33B-A3.2B (Wu et al., 2025). Figure 12: Characteristic layer-wise weight norm distribution in pre-trained LLMs from several open-source models. As shown in fig. 12, consistent pattern emerges across well-converged MoE models: the layerwise weight norms tend to increase with depth. This trend provides further empirical support for our proposed interpositional growth method (section 3.1), highlighting its ability to align with the intrinsic training dynamics of large MoE architectures."
        },
        {
            "title": "C DISCUSSION ON FUNCTION PRESERVING",
            "content": "We observed that, under our model architecture, directly growing smaller model into larger one does not lead to severe accuracy degradation on downstream evaluations, even though the outputs for identical inputs may differ due to manual alterations of model weights. Furthermore, the accuracy drop tends to be smaller for width growth compared to depth growth. This phenomenon relates to principle in model growth known as Function Preserving (FP) (Evci et al., 2022; Wang et al., 2023; Yao et al., 2024). FP stipulates that, for any given input, the output before and after model growth should remain identical, thereby guaranteeing that performance is not immediately harmed: yoriginal(x) = ygrowth(x). In practice, however, we find that even when FP rules are not strictly"
        },
        {
            "title": "Preprint",
            "content": "enforced, performance degradation is minor. This robustness can be attributed to the pre-norm structure widely adopted in modern transformers. In pre-norm layers, the normalization is applied before the residual connection, i.e., h(l+1) = h(l) + F(cid:0)LN(h(l))(cid:1), (5) where h(l) is the input to layer l, LN denotes layer normalization, and represents the sublayer transformation (e.g., attention or feedforward block). By contrast, in the original Transformer and BERT, the post-norm structure was used, where normalization is applied after the residual connection: h(l+1) = LN(cid:0)h(l) + F(h(l))(cid:1). (6) Although post-norm structures can better exploit model capacity, they are known to be harder to optimize and less stable during training. Pre-norm designs, in contrast, are easier to train but may reduce the models effective depth. This structural distinction explains our empirical findings. Under the pre-norm structure, when layers are duplicated during depth growth, the residual-normalization combination in eq. (5) ensures that the difference between the output of single layer and that of duplicated pair of layers is small. As result, the overall model output remains similar, and performance degradation is limited. In contrast, with post-norm (eq. (6)), duplicating layers alters the scale of normalized outputs more substantially, leading to larger deviations and thus greater performance drops immediately after growth. Experimental evidence supporting this claim is presented in fig. 5. From another perspective, width growth is naturally more function-preserving. When adding experts in MoE layers, both expert weights and router weights are copied. This implies that, at inference time, the widened MoE produces outputs identical to the original configuration, fully consistent with the FP principle. In practice, we add small Gaussian noise to the new experts to encourage specialization during continued training, but such noise only causes negligible shifts in model outputs. Importantly, since width growth operates solely within the MoE module and does not alter the layer structure, it maintains performance under both pre-norm and post-norm settings. This explains why width growth yields better immediate performance retention than depth growth, as shown in fig. 5."
        },
        {
            "title": "D DETAILED TRAINING SETTINGS",
            "content": "This appendix provides details of our pretraining pipeline, including model architecture (section D.1), dataset composition (section D.2), training hyperparameters (section D.3), and infrastructure configurations (section D.4). D.1 MODEL STRUCTURE We adopt standard decoder-only LLM architecture, with each layer containing Grouped Query Attention (GQA) and Mixture-of-Experts (MoE) modules in both the 3B and 17B models. RMSNorm is used for layer normalization, and rotary position embeddings are applied. For the 3B model, we set the number of layers to 20 with hidden size of 1024. The GQA module uses 16 attention heads grouped into 4 query groups. The MoE module consists of 64 experts, of which 4 are activated during computation. The hidden size of each expert is 768. For the 17B model, we use 28 layers with hidden size of 2048. GQA again uses 16 attention heads with 4 query groups. The MoE module includes 96 experts, with 6 activated during computation. Each expert has hidden size of 1024. For MoE models specifically, we apply sigmoid function to compute router scores instead of the softmax function. In the router, expert bias is disabled for the 3B model but enabled for the 17B model. For load balancing, we use sequence-level auxiliary loss in the 3B model and global-batch auxiliary loss in the 17B model."
        },
        {
            "title": "Preprint",
            "content": "D.2 DATASET COMPOSITION Our pretraining corpus is constructed from diverse and high-quality dataset comprising mixture of public and proprietary sources, including: DCLM: dataset released by Apple (Li et al., 2024) with de-duplication (1T tokens) FineWeb-Edu: dataset released by Hugging Face (Penedo et al., 2024) with de-duplication (280B tokens) Nemotron-CC-HQ: high-quality Common Crawlbased dataset (4.67T tokens) released by NVIDIA (Su et al., 2024) Filtered Code Data: curated code dataset (640B tokens) Synthetic Data: high-quality, instruction-oriented synthetic corpora (1.8T tokens) We randomly shuffle these corpora and uniformly sample approximately 1T tokens for training. We preprocess the raw dataset using the GPT-4o tokenizer, which has vocabulary size of 200,019. The maximum sequence length is fixed at 4096 tokens. During training, the batch size is set to 1024 for the 3B model and 4096 for the 17B model. D.3 TRAINING HYPERPARAMETERS Both for 3B model and 17B model, all learnable parameters are randomly initialized with standard deviation of 0.02. We employ the AdamW optimizer (Loshchilov et al., 2017) with hyper-parameters set to β1 = 0.9, β2 = 0.95, and weight-decay = 0.1. Max learning rate is 3 104 for 3B model and 2.6 104 for 17B model. As for the learning rate scheduling, we first linearly increase it from 0 to max learning rate during the first 3K steps. Then, we keep constant learning rate. For 3B model, we decay it into the minimum leaning rate, which is 1/10 of max learning rate during the annealing process. We donot do annealing for 17B model yet. D.4 INFRASTRUCTURE DETAILS We train our model with mixed precision framework (BF16 + FP32). We use Flash Attention(Dao et al., 2022) for training acceleration. distributed optimizer is employed to partition optimizer states across data-parallel GPUs, thereby reducing memory consumption. MoE layer recomputation is enabled to further decrease memory usage, and Grouped GeMM (General Matrix Multiplication) is used to accelerate MoE computations. For the 17B model, we use an expert parallel size of 8 to distribute expert weights across GPUs, which allows us to fit within the memory constraints of each device. For infrastructural reasons, we occasionally enable pipeline parallelism (size = 2) to free memory for larger microbatch sizes, improving GeMM efficiency. For the smaller 3B model, we use an expert parallel size of 2 without pipeline parallelism, since the cost of all-to-all expert communication is lower than the overhead introduced by pipeline scheduling and idle bubbles."
        },
        {
            "title": "E EVALUATION DETAILS",
            "content": "E.1 METHOD FOR COMPUTING AVERAGE ACCURACY We conduct our evaluation through the widely used lm-evaluation-harness library2 (Gao et al., 2024). All reported average accuracy values in the main text are derived from the average accuracy of following two categories: (1) comprehensive knowledge and reasoning ability, and (2) basic multiplechoice QA performance. For comprehensive knowledge and reasoning ability, we use the MMLU benchmark (Massive Multitask Language Understanding) (Hendrycks et al., 2020), which consists of 57 tasks spanning STEM, humanities, social sciences, and professional domains. MMLU is widely recognized for assessing models ability to apply world knowledge, solve problems, and perform reasoning beyond surfacelevel pattern recognition. We evaluate using few-shot setting with 5 in-context examples. 2https://github.com/EleutherAI/lm-evaluation-harness"
        },
        {
            "title": "Preprint",
            "content": "In addition, we assess performance on multiple-choice QA benchmarks including ARC (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), LogiQA (Liu et al., 2021), OpenBookQA (ObQA) (Mihaylov et al., 2018), and Winogrande (Sakaguchi et al., 2021). These tasks are evaluated in the zero-shot setting, with accuracy (percentage of correctly chosen options) as the evaluation metric. Collectively, they complement MMLU by emphasizing commonsense and scientific reasoning in narrower but challenging domains. E.2 DETAILED EVALUATION RESULTS We provide the complete accuracy tables from table 3 to table 26. The results presented in the main text as averaged figures or tables are derived directly from these original tables. For clarity, we indicate the corresponding appearances of each result in the table headers. Table 3: Full evaluation results of 3B model pretraining, shown in fig. 3 and fig. 5 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 2k 4k 6k 8k 10k 12k 14k 16k 18k 20k 22k 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 46k 48k 50k 52k 54k 56k 58k 60k 62k 64k 66k 68k 70k 72k 74k 76k 78k 80k 82k 84k 86k 88k 90k 92k 94k 96k 98k 25.54 27.69 28.94 30.70 29.08 31.75 32.46 33.34 32.93 33.46 32.90 35.83 33.94 36.26 35.98 35.94 36.88 37.73 37.49 37.57 38.13 39.58 38.29 40.44 40.38 40.01 40.23 41.21 40.17 41.31 40.45 41.85 42.28 41.08 43.06 42.06 40.81 43.17 42.57 42.91 43.83 44.09 45.25 45.34 45.93 45.78 45.93 46.05 45. 34.75 37.91 38.76 41.26 42.02 42.95 42.42 44.10 43.87 43.04 43.26 44.42 43.74 43.70 44.26 43.73 45.43 46.88 44.70 45.19 47.47 44.26 45.78 46.17 45.11 46.52 45.49 44.79 46.72 47.57 46.34 47.69 47.72 47.23 48.20 47.81 47.76 47.92 48.53 49.05 48.79 49.68 49.12 50.47 50.46 50.18 50.01 50.07 50.12 22.87 28.75 32.85 34.56 35.84 34.39 37.12 37.03 39.85 39.33 38.48 36.69 39.68 41.04 40.70 40.61 43.09 42.32 41.30 41.04 41.72 41.13 41.98 41.04 41.72 42.58 41.47 42.06 42.66 40.96 42.83 41.89 42.41 40.87 42.06 42.66 43.86 42.58 43.60 42.92 42.92 42.83 43.00 44.20 44.62 43.94 43.34 43.94 44.20 54.34 49.79 46.30 53.36 55.32 57.49 48.93 56.79 52.54 49.08 51.01 57.95 47.25 45.50 47.80 44.13 52.69 63.15 53.91 51.59 62.51 48.35 55.05 55.60 49.20 54.28 50.40 44.10 56.02 59.11 54.80 59.27 58.38 60.15 62.81 61.01 59.14 61.28 61.07 63.70 61.62 64.25 60.40 65.17 65.50 64.07 64.07 62.54 65.32 28.88 28.88 29.19 29.65 28.42 29.49 29.19 29.65 30.57 28.73 27.04 29.19 28.73 29.49 28.42 31.18 29.34 28.26 27.65 26.42 28.57 27.96 27.96 29.19 27.65 29.49 28.88 29.03 28.42 29.34 28.42 28.73 29.19 28.73 29.95 28.88 30.26 28.57 29.34 28.88 29.65 30.26 30.41 30.88 29.65 29.95 28.88 30.26 28.88 24.20 30.40 31.60 32.60 33.40 33.40 34.20 34.40 34.00 34.80 34.80 33.80 35.20 35.00 35.00 33.80 34.80 34.80 33.40 35.40 35.00 33.20 34.00 34.80 35.20 35.80 35.20 34.80 34.80 37.60 35.20 34.80 36.40 35.20 35.00 34.80 34.60 33.40 36.20 36.20 35.60 36.60 36.60 37.40 38.20 37.60 38.00 37.80 36.60 27.60 35.41 39.95 43.62 45.24 47.51 48.85 50.15 50.69 51.66 52.21 52.90 53.34 53.73 54.99 54.87 55.18 55.44 55.50 56.24 56.30 56.65 56.67 56.47 57.23 57.48 57.06 58.06 57.40 57.63 57.84 58.36 58.11 58.15 58.75 58.33 58.92 59.63 59.58 60.17 60.46 60.65 61.18 62.04 62.14 62.23 62.33 62.66 62. 17 50.59 54.22 52.64 53.75 53.91 55.41 56.20 56.59 55.56 54.62 56.04 55.96 58.25 57.46 58.64 57.77 57.46 57.30 56.43 60.46 60.69 58.25 59.04 59.91 59.67 59.51 59.91 60.69 61.01 60.77 58.96 63.06 61.80 60.30 60.62 61.17 59.75 62.04 61.40 62.43 62.51 63.46 63.14 63.14 62.67 63.30 63.46 63.22 63.14 30.14 32.80 33.85 35.98 35.55 37.35 37.44 38.72 38.40 38.25 38.08 40.12 38.84 39.98 40.12 39.83 41.15 42.30 41.09 41.38 42.80 41.92 42.04 43.30 42.75 43.27 42.86 43.00 43.44 44.44 43.40 44.77 45.00 44.16 45.63 44.93 44.28 45.54 45.55 45.98 46.31 46.88 47.19 47.91 48.20 47.98 47.97 48.06 47."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Full evaluation results of 6B model pretraining, shown in fig. 5, fig. 7 and fig. 8 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 2k 4k 6k 8k 10k 12k 14k 16k 18k 20k 22k 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 46k 24.65 27.03 30.12 31.76 30.73 33.63 33.07 32.82 36.58 35.78 38.06 39.20 38.11 40.93 42.24 42.28 42.10 43.63 42.81 44.02 44.30 44.59 44.51 36.53 38.87 39.68 42.20 44.68 44.83 45.23 44.77 46.53 43.18 45.40 46.05 45.80 45.01 45.79 47.92 46.44 46.43 47.64 47.77 49.44 47.44 47.27 23.55 29.95 32.85 36.52 38.65 38.91 41.30 40.78 40.96 41.30 42.24 42.32 42.41 40.53 43.60 44.37 44.20 44.88 45.73 45.65 45.56 45.31 46.50 60.06 59.79 50.24 52.35 61.47 59.08 56.73 51.62 59.42 40.86 48.47 54.19 49.91 46.27 43.21 56.39 48.84 44.74 48.41 51.56 62.35 48.41 46. 28.27 37.24 42.74 47.67 50.44 51.98 53.21 55.11 55.21 56.51 57.30 57.59 58.19 59.00 59.41 59.59 59.96 60.47 61.11 60.83 61.41 61.66 61.81 27.50 27.96 27.65 29.49 28.73 28.11 27.96 26.42 28.88 27.80 29.34 26.42 28.42 28.42 28.42 29.49 28.42 29.34 29.03 28.88 29.03 29.19 28.26 28.00 27.60 32.80 32.40 32.60 34.80 34.80 35.80 36.40 35.80 35.20 37.00 35.80 36.40 38.20 37.20 38.60 38.00 38.00 39.40 37.80 38.40 37.40 51.78 50.67 51.78 54.78 56.20 56.12 57.38 58.88 58.33 56.83 59.83 58.80 60.06 59.43 61.88 60.46 58.64 61.17 63.54 60.30 60.46 61.64 63.14 30.59 32.95 34.90 36.98 37.71 39.23 39.15 38.79 41.56 39.48 41.73 42.63 41.95 42.97 44.01 45.10 44.27 45.03 45.22 45.90 46.87 46.01 45.89 Table 5: Full evaluation results of 6B model interpositional growth at 24k, shown in fig. 3, fig. 7 and fig. Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 46k 48k 50k 52k 54k 34.22 35.86 36.53 39.30 40.50 41.50 42.38 43.46 42.61 44.30 44.30 45.11 46.03 44.54 46.52 46.99 44.57 45.03 45.21 45.40 45.73 46.61 48.17 48.53 48.75 49.22 48.73 48.48 49.36 49.38 49.47 48.61 37.80 39.76 40.53 41.04 42.24 43.34 42.24 44.28 43.17 45.22 46.08 44.28 44.88 43.43 45.05 46.16 62.45 52.59 52.02 51.62 51.62 53.76 62.11 61.56 63.79 65.60 58.93 56.24 62.66 61.87 57.92 51. 50.90 54.62 55.76 56.94 57.67 57.56 58.75 58.85 59.31 59.79 60.06 60.71 60.33 61.11 61.23 62.19 27.80 30.57 29.19 28.42 29.19 29.34 27.96 28.73 28.73 28.11 30.57 30.72 28.42 30.88 30.88 30.26 33.20 35.00 34.80 35.60 35.20 35.80 36.40 37.00 35.80 36.00 37.20 36.20 36.80 37.20 38.20 39.20 55.25 57.62 58.96 58.80 58.48 59.83 61.56 60.77 61.72 60.62 59.51 62.75 63.06 61.80 63.54 62.75 39.39 40.44 40.87 42.35 43.12 44.05 45.28 46.00 45.68 46.76 46.51 46.80 47.69 46.96 48.00 47.80 Table 6: Full evaluation results of 6B model stack growth at 24k, shown in fig. 3 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 24k 26k 28k 30k 32k 34k 36k 38k 40k 34.14 33.69 34.77 36.41 37.54 36.68 39.20 39.70 36.92 44.94 44.83 45.31 46.14 44.92 46.71 47.42 47.61 47.39 37.20 41.21 41.38 41.30 42.92 43.09 43.26 43.34 42.75 60.24 50.73 54.65 57.55 49.17 54.62 58.13 60.55 55.96 28.57 29.95 28.88 29.03 28.57 28.57 29.19 29.65 30. 35.40 35.00 33.40 35.00 34.20 36.20 36.40 34.00 36.00 55.88 57.46 57.93 57.30 57.77 59.91 59.19 59.19 59.51 39.54 39.26 40.04 41.28 41.23 41.69 43.31 43.65 42.16 52.35 54.61 55.60 56.67 56.88 57.86 58.34 58.92 59."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Full evaluation results of 6B model interpositional growth at 8k, shown in fig. 7 and fig. 8 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 8k 10k 12k 14k 16k 18k 20k 22k 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 46k 48k 29.75 30.40 32.64 33.35 33.95 36.41 38.53 38.33 38.72 39.78 41.53 41.40 43.13 42.12 44.05 44.70 43.66 44.75 45.46 45.39 46.15 41.58 41.91 41.68 44.27 45.13 46.74 44.96 45.80 47.38 48.56 46.35 46.74 48.44 46.95 48.12 48.44 48.69 49.35 49.23 48.94 49.18 34.30 34.98 35.67 37.29 40.61 40.02 40.02 42.66 42.92 42.92 43.86 42.06 42.83 42.49 43.60 44.03 46.08 45.48 44.71 45.82 44.45 59.72 52.48 48.69 56.45 54.10 61.80 54.07 54.10 59.57 64.19 50.95 52.72 61.16 51.22 58.69 58.47 56.97 59.76 60.76 57.46 57. 43.29 46.70 49.62 51.87 53.55 54.59 55.06 56.27 57.15 57.44 58.15 58.75 58.79 59.62 60.23 60.69 60.59 60.59 60.88 61.25 61.51 29.65 29.19 27.34 27.34 30.57 29.95 27.65 28.26 29.34 29.49 28.73 29.19 29.65 29.80 30.57 27.96 29.65 29.65 29.95 29.49 30.88 29.80 33.20 32.40 34.60 35.60 36.00 34.80 35.60 35.00 36.40 36.00 35.60 36.80 35.40 34.80 37.20 36.80 38.00 36.40 36.80 37.80 52.72 54.93 56.35 58.09 56.35 58.09 58.17 57.93 60.30 60.93 60.38 62.12 61.40 63.14 60.85 62.27 62.04 62.59 62.67 62.80 63.06 35.67 36.16 37.16 38.81 39.54 41.58 41.75 42.07 43.05 44.17 43.94 44.07 45.78 44.53 46.09 46.57 46.17 47.05 47.34 47.16 47.66 Table 8: Full evaluation results of 6B model interpositional growth at 16k, shown in fig. 7 and fig. 8 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 16k 18k 20k 22k 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 46k 48k 50k 52k 31.23 34.09 33.76 37.69 38.04 39.81 41.23 41.65 41.41 42.89 43.57 44.87 45.24 44.01 45.34 45.22 45.70 46.21 46.05 44.10 43.29 44.06 46.01 44.89 44.63 46.99 47.65 48.63 48.06 47.32 49.06 49.02 48.44 46.91 49.51 49.53 50.64 51.01 36.35 37.12 38.74 40.96 40.61 41.30 40.78 43.34 43.00 43.69 43.77 43.09 43.52 43.34 42.92 45.39 44.97 45.73 46.25 60.73 50.76 50.06 54.13 47.52 47.80 60.37 57.92 62.51 59.88 56.12 62.54 63.64 57.19 49.63 57.68 60.43 63.27 67.65 48.80 51.92 53.70 55.20 55.93 56.48 57.35 57.92 58.41 58.63 58.66 59.41 59.86 60.52 60.62 61.23 61.34 61.37 61. 30.72 30.41 29.19 31.18 30.57 27.80 28.73 29.49 29.95 29.65 28.11 30.26 27.34 29.49 29.49 32.41 28.88 30.88 30.41 32.80 33.40 34.80 35.40 36.20 35.20 36.20 36.40 37.60 38.20 37.80 36.80 36.80 38.00 37.00 38.40 37.60 38.00 37.80 55.17 56.12 57.85 59.19 58.48 59.19 58.48 60.85 60.30 58.33 59.43 62.27 62.98 62.12 61.80 61.96 63.93 64.56 62.19 37.66 38.69 38.91 41.85 41.46 42.22 44.11 44.65 45.02 45.48 45.44 46.97 47.13 46.23 46.13 47.37 47.61 48.42 48.53 Table 9: Full evaluation results of 6B model interpositional growth at 32k, shown in fig. 7 and fig. 8 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 32k 34k 36k 38k 40k 42k 44k 46k 48k 50k 52k 54k 56k 34.45 36.84 38.76 39.95 41.29 41.21 43.70 44.30 43.01 45.24 45.68 45.15 46.18 43.64 44.41 46.45 47.68 46.62 47.39 47.55 49.20 48.67 48.64 48.67 49.75 50.12 40.61 40.96 41.13 43.26 42.49 40.87 43.17 45.05 44.28 44.37 45.82 44.03 44.80 49.02 50.80 54.46 61.44 55.05 61.31 57.03 64.65 61.65 62.23 56.61 65.32 67.71 28.88 27.65 30.57 30.11 28.88 28.42 29.19 29.03 27.96 29.49 30.72 28.73 26.88 33.20 33.60 35.20 34.80 35.00 35.80 36.00 36.20 36.80 35.00 37.60 36.40 37. 56.67 57.06 60.22 58.33 59.98 58.96 60.01 60.54 61.25 60.38 60.77 62.98 63.06 39.05 40.63 42.60 43.82 43.95 44.30 45.63 46.75 45.84 46.94 47.18 47.45 48.15 53.48 56.41 57.11 58.15 58.30 58.95 59.91 59.75 60.09 60.39 60.52 61.01 60."
        },
        {
            "title": "Preprint",
            "content": "Table 10: Full evaluation results of 6B model interpositional growth at 40k, shown in fig. 7 and fig. 8 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 40k 42k 44k 46k 48k 50k 52k 54k 56k 58k 60k 36.87 38.41 40.68 41.70 43.22 42.74 44.47 45.73 44.91 46.55 46.34 45.36 46.43 46.91 47.79 47.28 47.91 48.39 48.27 49.11 50.09 49.05 39.59 41.72 42.06 43.00 43.77 43.26 43.60 43.17 43.86 45.31 44.88 59.08 53.94 56.70 62.26 54.50 56.21 57.28 61.53 63.46 63.61 58. 54.18 57.27 58.33 58.92 59.24 59.66 60.60 60.49 60.88 61.25 61.50 26.88 30.57 28.73 28.42 26.88 30.11 29.80 28.57 28.57 29.03 31.64 34.00 35.80 35.20 34.40 36.40 36.40 36.60 34.00 35.40 38.80 36.40 58.41 59.27 60.46 59.75 62.90 61.80 62.43 61.88 62.51 62.51 61.17 41.11 42.42 43.80 44.75 45.25 45.32 46.43 47.00 47.01 48.32 47.70 Table 11: Full evaluation results of 6B model interpositional growth at 48k, shown in fig. 7 and fig. 8 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 48k 50k 52k 54k 56k 58k 60k 62k 38.14 40.88 41.65 43.24 44.23 43.23 44.95 44.98 46.38 47.66 48.64 48.76 49.00 48.87 50.67 49.41 41.81 42.56 43.00 43.86 43.77 43.60 45.73 44.20 61.01 61.56 63.85 61.47 63.12 59.36 65.57 64.43 54.74 57.46 58.71 59.53 59.71 60.35 60.89 61. 27.80 30.15 31.03 30.11 28.88 29.95 30.72 28.26 35.80 34.61 34.40 36.60 36.40 36.60 38.00 36.40 57.14 59.64 60.85 61.01 62.12 63.38 63.12 61.96 42.26 44.27 45.15 46.00 46.62 46.05 47.81 47.20 Table 12: Full evaluation results of 6B model interpositional growth at 56k, shown in fig. 7 and fig. 8 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 56k 58k 60k 62k 64k 66k 68k 70k 40.41 42.44 43.17 42.81 45.24 44.77 45.11 45.36 47.31 46.71 48.28 49.61 48.86 49.29 49.68 49.38 41.47 43.60 44.37 44.28 44.03 46.76 42.75 42.68 63.49 54.16 61.28 65.75 59.54 58.62 66.15 65.38 55.81 58.95 59.30 60.23 60.37 61.14 61.41 60.45 28.11 29.65 28.73 29.03 31.03 29.95 27.80 29. 35.40 35.00 34.80 36.80 36.40 37.60 36.80 36.20 59.59 58.88 61.17 61.56 61.80 61.64 63.14 61.89 43.86 44.57 45.72 46.21 47.05 47.03 47.39 47.37 Table 13: Full evaluation results of 6B model interpositional growth at 64k, shown in fig. 7 and fig. 8 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 64k 66k 68k 70k 72k 74k 76k 78k 39.47 43.46 43.93 43.36 45.09 44.92 46.00 45. 47.17 47.74 48.95 48.89 49.44 49.44 50.34 49.95 42.41 42.92 44.80 44.03 45.48 45.05 44.80 43.68 59.97 57.03 61.93 62.26 62.29 58.72 67.06 66.48 55.83 59.50 59.61 60.37 60.43 61.35 61.67 60.59 29.19 29.19 29.34 28.88 29.80 31.34 29.03 30.50 35.00 36.60 35.80 34.40 35.60 37.60 36.80 36. 60.62 61.17 62.19 63.38 63.06 62.59 62.67 61.74 43.32 45.60 46.44 46.12 47.27 47.18 48.17 47.81 Table 14: Full evaluation results of 6B model interpositional growth at 72k, shown in fig. 7 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 72k 74k 76k 78k 80k 82k 84k 86k 40.44 44.35 44.97 44.45 45.33 45.27 46.61 46.32 46.88 49.47 49.50 48.94 50.27 50.97 50.76 49. 42.24 45.56 45.31 44.45 46.08 45.05 45.14 45.14 60.52 65.17 67.28 61.68 65.05 66.79 67.92 61.16 29.03 29.95 30.11 28.57 29.80 31.34 29.49 29.03 34.40 35.40 34.00 34.80 36.20 37.60 35.80 35.40 59.27 61.01 60.30 63.22 63.61 63.38 64.40 64.25 43.66 46.91 47.24 46.69 47.80 48.12 48.69 47. 55.79 59.75 60.02 60.89 60.88 61.67 61.83 61."
        },
        {
            "title": "Preprint",
            "content": "Table 15: Full evaluation results of 6B model interpositional growth at 80k, shown in fig. 7 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 80k 82k 84k 86k 88k 90k 92k 94k 41.46 44.68 45.46 43.46 46.30 45.86 46.86 47.12 48.44 49.58 49.17 49.56 49.27 50.71 50.46 50.39 41.64 44.03 44.28 45.65 44.97 44.88 44.03 45.39 65.54 62.11 63.98 64.53 61.13 67.09 65.47 65. 57.39 60.33 60.76 61.17 61.03 61.78 62.63 62.43 28.73 30.57 29.34 28.26 27.50 29.95 29.49 28.57 36.00 37.60 36.20 35.80 36.40 37.20 37.60 36.00 61.33 62.83 60.46 61.96 64.56 63.38 63.54 64.40 44.95 47.13 47.32 46.51 47.78 48.29 48.66 48.76 Table 16: Full evaluation results of 6B model interpositional growth at 88k, shown in fig. 7 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 88k 90k 92k 94k 96k 98k 100k 102k 43.78 44.52 45.76 43.42 45.95 45.59 47.80 47.52 49.09 49.75 48.43 50.46 50.73 51.92 50.93 50.45 44.45 43.26 45.22 45.22 44.88 46.33 44.20 45.05 63.85 64.01 58.90 67.82 68.59 68.81 66.09 66.91 58.70 60.38 60.69 61.11 61.10 62.13 62.52 62. 29.49 30.11 29.95 29.19 29.80 31.95 30.88 27.96 35.60 37.60 35.20 35.80 37.20 38.60 36.80 37.20 62.43 63.14 60.62 63.61 62.83 63.69 65.11 63.14 46.43 47.14 47.10 46.94 48.34 48.75 49.37 48.99 Table 17: Full evaluation results of 6B model interpositional growth at 96k, shown in fig. 7 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 96k 98k 100k 102k 104k 106k 108k 110k 43.80 45.05 45.28 44.49 45.94 45.88 47.41 46.96 48.58 49.03 49.44 50.30 50.56 51.12 51.11 50.08 43.52 43.77 44.54 45.56 45.56 45.56 44.80 44.71 63.58 57.49 64.59 66.57 64.68 65.93 67.89 65.66 58.94 60.76 60.84 60.69 61.45 61.55 62.31 62.25 28.57 30.72 30.57 29.95 29.65 30.57 29.19 28. 35.60 37.60 35.00 36.60 37.60 37.60 38.00 36.00 61.25 63.85 61.09 62.43 64.40 65.51 64.48 63.30 46.19 47.04 47.36 47.40 48.25 48.50 49.26 48.52 Table 18: Full evaluation results of 6B model width growth with no noise, shown in fig. 4 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 35.85 33.89 35.22 36.35 37.60 37.56 38.38 39.83 38.78 41.30 41. 44.65 44.02 43.14 45.00 45.61 45.98 44.37 46.66 48.59 47.69 47.65 36.86 39.51 40.61 41.04 42.49 42.41 40.78 43.26 44.62 45.48 44.88 58.10 48.10 45.78 52.39 55.02 55.26 43.84 54.89 63.12 57.40 56.73 52.79 54.52 54.67 55.99 56.11 56.61 58.02 58.00 58.79 59.16 59.37 29.34 29.49 28.26 30.57 28.26 28.42 27.80 29.34 27.96 27.96 29.80 33.80 35.60 33.40 33.40 35.60 36.80 36.40 35.20 36.40 36.60 36. 56.99 56.87 56.12 56.59 56.20 56.35 59.35 59.27 60.62 59.51 58.72 40.25 38.95 39.18 40.67 41.61 41.77 41.37 43.25 43.68 44.49 44.50 Table 19: Full evaluation results of 6B model width growth with noise std=0.01, shown in fig. 4 and fig. 5 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 35.91 33.43 33.58 36.55 37.64 38.33 39.99 41.46 39.11 41.36 42. 44.52 43.80 44.12 46.73 46.76 46.72 45.28 48.20 47.81 48.27 48.56 36.86 39.59 40.96 41.47 42.92 42.75 39.33 43.52 43.52 43.86 44.28 58.23 47.25 48.38 61.77 57.95 57.86 51.31 63.09 59.85 61.74 61.53 28.42 28.88 29.03 30.41 28.57 28.42 29.19 27.50 29.34 28.57 29.34 33.80 35.40 33.20 33.60 35.40 35.80 36.40 38.20 36.60 37.00 36.20 56.99 57.38 57.77 56.99 58.88 58.09 57.30 58.41 58.88 59.19 60. 40.21 38.62 38.85 41.64 42.20 42.52 42.63 44.83 43.46 44.81 45.52 52.79 54.30 55.36 56.13 56.81 57.39 58.12 58.48 58.64 59.24 59."
        },
        {
            "title": "Preprint",
            "content": "Table 20: Full evaluation results of 6B model width growth with noise std=0.05, shown in fig. 4 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 36.03 33.94 35.60 36.87 37.89 38.30 39.65 40.02 39.72 41.85 42.41 44.81 43.63 45.12 46.48 46.45 47.89 46.35 47.57 47.93 48.21 48.42 37.03 39.08 41.72 41.72 42.75 42.41 40.27 43.00 43.43 44.28 45.31 58.47 46.94 55.57 61.35 57.37 62.54 56.57 59.85 61.10 60.34 60. 52.72 54.30 54.94 56.09 56.84 57.00 58.26 58.62 59.00 59.16 59.39 29.34 29.95 29.03 29.65 29.49 29.03 28.42 30.41 26.88 30.11 29.65 34.00 34.20 32.80 34.20 35.00 38.60 36.00 35.00 37.60 36.80 36.20 57.30 57.30 56.67 55.88 57.22 57.77 58.56 58.56 59.59 58.56 59.91 40.42 38.78 40.36 41.68 42.17 43.10 43.00 43.80 43.83 45.03 45.42 Table 21: Full evaluation results of 6B model width growth with noise std=0.1, shown in fig. 4 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 24k 26k 28k 30k 32k 34k 36k 38k 40k 42k 44k 35.84 34.38 35.90 37.10 38.24 38.26 40.09 41.33 39.58 41.98 41.69 44.67 43.77 44.10 46.17 47.00 47.23 45.85 46.48 48.60 48.11 48.18 36.86 38.99 41.98 41.30 43.94 43.26 42.06 43.17 44.28 43.77 44.71 58.72 46.97 47.03 60.24 61.56 61.93 54.16 54.62 62.87 61.38 57.16 52.80 54.33 55.28 56.30 56.39 57.14 58.16 58.43 59.07 59.02 59. 28.88 29.19 29.19 29.80 28.42 28.11 26.73 29.95 28.88 28.88 29.65 34.40 34.80 33.60 33.00 34.40 36.00 36.00 35.00 37.00 36.20 37.20 56.35 58.33 57.54 56.35 57.30 56.91 58.01 57.70 59.51 59.43 60.93 40.25 39.07 40.00 41.63 42.62 42.74 42.97 43.90 44.09 45.05 44.93 Table 22: Full evaluation results of 6B models direct growth under different model structure, shown in fig. 5 Model Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 3B Pre-norm 6B Pre-norm Depth 6B Pre-norm Width 3B Post-norm 6B Post-norm Depth 6B Post-norm Width 8k 16k 24k 32k 8k 16k 24k 32k 8k 16k 24k 32k 8k 16k 24k 32k 8k 16k 24k 32k 8k 16k 24k 32k 29.19 32.07 35.83 36.21 29.75 31.23 34.22 34.45 30.71 33.23 35.99 35.84 28.85 30.17 32.64 31.71 27.45 25.17 23.01 23.12 28.98 30.00 32.51 31. 41.26 44.10 44.42 43.73 41.58 44.10 44.57 43.64 41.53 44.04 44.68 43.61 40.06 42.27 42.18 42.73 37.55 37.23 35.13 35.36 40.16 42.14 42.13 42. 34.56 37.03 36.69 40.61 34.30 36.35 37.80 40.61 34.81 37.03 37.29 40.53 29.69 33.02 33.87 36.52 30.63 28.84 28.41 27.73 29.44 33.02 34.04 36. 53.36 56.79 57.95 44.13 59.70 60.73 62.45 49.02 54.04 56.79 58.07 44.22 59.48 56.33 48.50 45.90 55.99 56.57 48.38 51.59 59.39 56.09 48.81 45. 22 43.62 50.15 52.90 54.87 43.29 48.80 50.90 53.48 43.58 50.13 52.73 54.88 41.59 47.01 50.37 52.80 36.95 36.25 30.78 31. 41.54 46.98 50.44 52.78 29.65 29.65 29.19 31.18 29.65 30.72 27.80 28.88 30.26 30.26 29.34 30.88 26.88 29.95 30.26 29.34 22.12 21.66 22.43 21. 28.11 29.19 30.57 29.65 32.60 34.40 33.80 33.80 29.80 32.80 33.20 33.20 32.40 34.00 33.80 33.40 30.00 32.60 34.20 35.20 28.20 27.80 30.20 29. 29.60 32.60 34.00 35.00 53.75 56.59 55.96 57.77 52.72 55.17 55.25 56.67 54.06 56.04 56.83 57.77 52.72 54.70 55.88 56.59 51.38 52.25 50.59 50. 52.88 54.93 54.93 56.20 35.22 38.09 40.12 39.97 35.66 37.66 39.39 39.05 36.12 38.64 40.33 39.73 34.46 36.22 37.41 37.22 32.50 31.20 29.07 29. 34.57 36.07 37.32 37."
        },
        {
            "title": "Preprint",
            "content": "Table 23: Full evaluation results of 17B model pre-training, shown in fig. 1, fig. 9 and fig. 11 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 0k 4k 8k 12k 16k 20k 24k 28k 32k 36k 40k 44k 48k 52k 53k 54k 55k 56k 57k 58k 59k 60k 61k 62k 63k 64k 65k 66k 67k 68k 69k 70k 71k 72k 73k 74k 75k 76k 77k 78k 79k 80k 81k 82k 83k 84k 85k 86k 87k 88k 89k 90k 91k 92k 93k 94k 95k 96k 97k 98k 99k 100k 101k 102k 103k 104k 25.00 29.90 34.23 40.15 43.33 46.40 48.58 49.17 51.11 51.71 53.64 55.70 56.02 57.61 57.46 58.33 57.27 58.03 57.93 58.09 58.40 58.57 58.37 58.75 59.27 59.17 59.74 58.16 59.07 59.65 59.86 60.06 59.61 59.64 60.20 59.42 60.04 59.71 60.21 60.75 60.60 59.93 60.24 60.49 61.61 61.74 61.74 62.47 62.05 62.14 61.76 62.29 62.09 62.10 62.32 62.14 61.94 62.93 62.70 62.95 62.86 62.77 62.63 62.40 62.45 62.63 33.33 40.95 45.25 48.97 50.77 51.72 53.57 54.10 54.35 55.02 55.55 56.64 56.90 57.98 58.51 58.45 57.58 58.53 58.05 58.24 58.09 58.53 58.44 58.55 58.54 58.10 57.82 58.47 59.29 58.51 58.70 59.12 59.02 59.21 59.16 59.53 58.71 59.38 58.97 59.56 59.14 59.07 59.35 59.24 60.50 60.20 60.45 59.55 60.23 60.68 59.69 60.53 59.75 60.06 60.25 59.89 59.90 60.14 60.13 60.06 60.34 59.87 60.11 60.21 59.96 60.78 25.00 35.67 41.64 46.08 47.87 49.49 51.62 52.13 52.22 55.80 56.14 55.29 56.91 57.51 57.76 59.73 57.94 57.68 58.45 57.68 56.66 58.87 57.94 58.70 58.62 57.25 57.68 57.42 58.11 58.45 58.19 58.02 59.13 58.70 59.13 57.76 58.11 59.22 58.19 58.28 58.70 59.13 58.19 57.85 59.64 59.39 59.81 58.02 58.45 60.15 60.41 60.07 58.79 59.30 59.64 59.39 60.15 59.04 60.32 59.81 59.81 60.84 59.90 59.98 59.98 60.41 50.00 53.76 50.21 54.71 61.99 59.85 64.10 65.87 66.30 67.43 66.18 69.24 64.53 70.70 71.90 68.65 68.26 72.29 68.41 70.21 69.51 70.34 71.41 70.00 71.38 70.34 67.83 71.04 75.26 72.45 72.78 73.12 72.97 74.04 71.04 73.58 69.94 72.91 73.82 75.99 71.41 71.47 75.60 73.24 77.71 75.02 75.41 72.94 74.71 75.41 73.79 73.94 74.25 74.80 74.86 74.31 72.29 78.59 72.42 74.50 74.83 72.29 75.11 73.73 72.72 74. 25.00 32.60 37.20 38.20 39.40 41.20 41.80 40.80 40.80 41.20 41.40 42.20 43.20 43.80 44.80 45.60 44.60 44.00 43.60 43.00 43.80 43.20 44.80 45.40 43.80 44.40 43.20 43.60 44.20 42.60 43.80 44.40 44.40 43.40 45.30 44.00 44.20 44.40 43.00 44.00 44.00 44.20 43.80 44.40 44.00 44.80 44.80 44.80 45.00 45.40 43.80 45.80 43.60 44.60 45.60 44.40 44.80 42.60 45.60 44.80 44.20 45.00 44.80 45.00 44.40 46.40 25.00 25.96 27.04 28.26 28.11 28.88 30.72 29.80 28.11 26.73 28.88 29.80 29.65 29.49 32.26 30.41 29.34 29.49 30.72 31.49 31.18 31.49 29.19 29.80 29.03 29.34 29.49 30.72 30.72 29.80 30.11 29.80 29.03 30.26 31.03 32.10 32.41 31.18 30.72 30.26 31.64 30.57 29.80 31.03 30.41 30.26 31.03 30.88 31.80 32.10 29.19 31.49 31.49 30.57 30.26 31.03 31.49 29.95 30.72 31.80 31.03 29.34 29.03 30.57 29.65 31.95 25.00 47.10 57.81 65.56 64.98 66.57 67.69 68.20 69.46 69.91 71.66 73.08 73.78 73.35 73.33 73.83 72.72 74.15 74.52 73.97 74.91 74.56 74.91 74.54 74.67 75.04 75.07 74.25 74.46 74.84 74.78 75.00 74.69 75.11 75.08 75.31 75.04 74.99 75.18 75.12 75.30 75.91 75.12 75.45 76.17 76.11 76.37 75.97 76.45 75.59 76.29 76.12 75.94 76.13 76.50 76.16 76.66 76.53 76.65 76.35 76.15 76.08 76.85 77.00 76.60 76.64 23 50.00 50.59 57.62 61.01 62.27 64.33 65.51 67.80 69.22 69.06 69.06 70.24 73.32 73.01 71.03 72.45 72.61 73.56 72.61 73.09 72.45 72.69 72.38 72.85 73.72 72.22 73.64 73.80 73.01 72.93 72.53 74.35 73.88 73.72 73.40 74.43 72.53 73.56 72.93 73.72 73.80 73.16 73.56 73.48 75.06 75.61 75.30 74.66 74.98 75.45 74.66 75.77 74.43 74.98 74.66 74.03 74.03 74.11 75.06 73.09 76.01 75.69 74.98 74.98 76.40 75.14 29.17 35.42 39.74 44.56 47.05 49.06 51.08 51.64 52.73 53.37 54.60 56.17 56.46 57.79 57.99 58.39 57.42 58.28 57.99 58.17 58.24 58.55 58.40 58.65 58.90 58.63 58.78 58.32 59.18 59.08 59.28 59.59 59.31 59.42 59.68 59.48 59.37 59.54 59.59 60.16 59.87 59.50 59.79 59.87 61.05 60.97 61.10 61.01 61.14 61.41 60.73 61.41 60.92 61.08 61.29 61.01 60.92 61.53 61.41 61.50 61.60 61.32 61.37 61.31 61.20 61."
        },
        {
            "title": "Preprint",
            "content": "Table 24: Full evaluation results of 34B model interleaved growth, shown in fig. 1, fig. 9 and fig. 11 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 60k 61k 62k 63k 64k 65k 66k 67k 68k 69k 70k 71k 72k 73k 74k 75k 76k 77k 78k 79k 57.96 59.61 59.54 60.25 60.51 60.57 60.19 60.54 61.50 61.42 61.67 62.08 61.97 62.21 62.74 62.85 63.13 62.92 63.52 62.65 58.05 58.64 58.60 60.27 59.66 59.29 58.90 60.04 59.40 59.53 59.66 60.36 60.34 60.29 60.99 60.67 61.19 60.41 60.05 61.27 58.10 59.04 57.85 59.47 58.45 57.85 59.30 59.40 59.47 60.67 60.49 59.98 61.01 59.90 60.67 60.15 60.49 59.81 59.90 60.84 69.02 68.47 69.48 73.24 73.15 71.22 67.80 74.25 69.82 67.49 69.11 72.57 72.05 70.80 73.15 71.41 75.60 72.05 69.88 76. 73.93 75.86 75.46 75.84 76.18 75.92 76.06 76.18 76.19 76.65 76.74 77.09 77.03 77.06 76.97 77.33 77.42 77.09 77.47 77.11 30.72 29.95 30.11 31.80 30.88 31.18 32.41 32.41 32.72 31.95 32.26 31.64 31.95 32.26 31.18 32.10 31.03 30.57 30.11 31.49 42.60 43.40 42.80 45.60 45.60 43.80 43.40 44.20 45.20 45.60 44.60 44.40 45.00 45.00 46.40 46.00 46.80 46.80 46.20 45.20 73.93 75.14 75.88 75.69 73.72 75.77 74.43 73.80 73.01 74.82 74.74 76.48 74.98 76.72 77.58 77.03 75.77 76.16 76.72 76.80 58.01 59.13 59.07 60.26 60.09 59.93 59.55 60.29 60.45 60.48 60.66 61.22 61.15 61.25 61.87 61.76 62.16 61.67 61.78 61.96 Table 25: Full evaluation results of 34B model stack growth, shown in fig. 9 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 60k 61k 62k 63k 64k 57.86 58.41 58.52 58.63 58.79 57.55 58.45 59.24 59.49 59.47 57.17 58.36 59.56 59.81 60.04 70.24 71.74 73.85 71.41 73.31 73.99 74.84 75.14 75.22 74. 29.34 28.73 29.95 30.41 29.98 44.40 43.20 43.60 45.20 44.20 70.17 73.80 73.32 74.90 74.65 57.71 58.43 58.88 59.06 59.13 Table 26: Full evaluation results of 70B model growth, shown in fig. 1 and fig. 11 Steps MMLU QA average Arc BoolQ Hellaswag Logiqa Openbookqa Winogrande Average 79k 79.5k 80k 80.5k 81k 81.5k 82k 82.5k 83k 83.5k 84k 62.63 63.59 64.49 64.44 65.08 64.70 64.93 65.55 66.06 65.85 65.87 61.24 61.86 62.29 62.07 62.20 62.43 62.14 62.10 62.52 61.62 62.48 61.09 61.09 60.58 62.03 62.17 62.46 62.46 62.54 62.71 61.69 63.05 76.09 76.42 78.41 74.95 78.23 77.03 76.76 75.41 78.47 75.29 77.86 32.10 31.64 32.41 32.10 31.64 31.80 32.72 33.49 31.64 30.41 31.64 44.80 45.40 46.20 47.20 46.80 46.40 44.60 45.40 45.20 45.40 45. 76.32 78.45 78.37 78.22 76.64 78.69 77.98 77.82 79.01 78.93 78.53 61.94 62.72 63.39 63.25 63.64 63.57 63.53 63.83 64.29 63.74 64.17 77.04 78.15 77.77 77.89 77.74 78.21 78.31 77.96 78.08 78.01 78."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "Shanghai Jiao Tong University",
        "University of Science and Technology of China",
        "Xiamen University"
    ]
}