{
    "paper_title": "RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services",
    "authors": [
        "Fei Zhao",
        "Chonggang Lu",
        "Yue Wang",
        "Zheyong Xie",
        "Ziyan Liu",
        "Haofu Qian",
        "JianZhao Huang",
        "Fangcheng Shi",
        "Zijie Meng",
        "Hongcheng Guo",
        "Mingqian He",
        "Xinze Lyu",
        "Yiming Lu",
        "Ziyang Xiang",
        "Zheyu Ye",
        "Chengqiang Lu",
        "Zhe Xu",
        "Yi Wu",
        "Yao Hu",
        "Yan Gao",
        "Jun Fan",
        "Xiaolong Jiang",
        "Weiting Liu",
        "Boyang Wang",
        "Shaosheng Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios."
        },
        {
            "title": "Start",
            "content": "RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services Fei Zhao, Chonggang Lu, Yue Wang, Zheyong Xie, Ziyan Liu, Haofu Qian, JianZhao Huang, Fangcheng Shi, Zijie Meng, Hongcheng Guo, Mingqian He, Xinze Lyu, Yiming Lu, Ziyang Xiang, Zheyu Ye, Chengqiang Lu, Zhe Xu, Yi Wu, Yao Hu, Yan Gao, Jun Fan, Xiaolong Jiang, Weiting Liu, Boyang Wang, Shaosheng Cao NLP Team, Xiaohongshu Inc., China caoshaosheng@xiaohongshu.com 5 2 0 2 3 1 ] . [ 1 5 0 6 0 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, domainspecific LLM designed to break the performance bottleneck of single-task baselines and establish comprehensive foundation for the SNS. RedOne was developed through threestage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using large-scale realworld dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios."
        },
        {
            "title": "Introduction",
            "content": "With the widespread adoption of online platforms and mobile applications, social networking services (SNS) have emerged as central medium for modern information dissemination, such as communication, knowledge sharing, and emotional expression (Elahimanesh et al., 2025). Unlike the general textual corpora, SNS data is highly informal, context-sensitive, and often emotionally charged. These characteristics present unique Figure 1: Performance comparison of different models in the SNS domain, where all models are instructiontuned and the evaluation score is the average of all tasks on SNS-Bench. challenges including linguistic variability, frequent roleswitching, and subtle conversational norms, which complicate applications (e.g. platform content management and interaction quality improvement) for traditional natural language processing (NLP) systems (Jin et al., 2024). Given these complexities, numerous studies have explored recent advanced large language models (LLMs) based adaptation for SNS-related tasks (Zeng et al., 2024; Jiang and Ferrara, 2023). However, these solutions primarily focus on isolated tasks, which not only experience diminishing benefits as data scales within individual scenarios but also struggle to adapt flexibly to diverse real-world contexts. This highlights fundamental limitation in current SNS domain-specific models, where performance plateaus due to the inability to incorporate more diverse domain knowledge corpus during training (Yue et al., 2025). To address these deficiencies, we introduce RedOne, demain-specific LLM with meticulous three-stage post-training strategy using large-scale dataset from real-world, which consists of continued pretraining (CPT), supervised fine-tuning (SFT), and preference optimization (PO). In the CPT stage, the model acquire extensive foundational knowledge in the SNS domain by processing large-scale corpora. Building on this foundation, the SFT stage refines the models capability to tackle specific SNS tasks by leveraging carefully defined domain-specific problem formulations. Finally, in the PO stage, we further optimize the models behavior to ensure seamless alignment with human preferences and maximize its practical utility in real-wold deployments. Through extensive experiments, RedOne not only maintains strong general capabilities, but also excels across multiple SNS-specific evaluation benchmarks, significantly outperforming leading proprietary or opensource models as shown in Figure 1. Further online testing in harmful content detection and post-view search, indicates its broad and promising potential application in real-world scenarios. Our contributions can be summarized as follows: We introduce RedOne, domain-specific LLM, engineered to break the performance bottleneck of single-task models, providing comprehensive improvements for SNS. three-stage training strategy is designed, using large-scale real-world dataset, which maintains strong general capabilities while delivering exceptional generalization across diverse SNS tasks. Through extensive experiments and online testing to demonstrate RedOnes effectiveness across wide range of tasks, and establish comprehensive and robust baseline for SNS application."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 NLP tasks in Social Networking Services Due to the inherent characteristics of SNS platforms, namely their informality and rapid linguistic evolution (Carr and Hayes, 2015), these platforms present numerous complex NLP challenges that have garnered sustained academic attention. In the early stages of development, researchers primarily focused on fundamental capability assessments, particularly prevalent tasks such as sentiment analysis (Mohammad et al., 2018; Rosenthal et al., 2019), harmful content detection (i Orts, 2019; Lu et al., 2024), and meme detection (Xie et al., 2023; Lin et al., 2024). Following the emergence of LLMs and building upon previous research foundations, various techniques have evolved in multiple domains, including content understanding (Kumar et al., 2024; Kmainasi et al., 2024), information extraction (Islam and Goldwasser, 2025; Li et al., 2024b; Peng et al., 2024), and dialogue systems (Yi et al., 2024; Zhang et al., 2024). These technological advances have significantly enhanced problem-solving capabilities within the SNS domain, but have primarily focused on single tasks. In contrast to these works, RedOne demonstrates superior performance across diverse SNS tasks, providing foundational model for improved services. 2.2 Domain-specific Post-training To better serve specialized domains, recent efforts have focused on developing vertical domain LLMs across various fields, including finance (Wu et al., 2023; Konstantinidis et al., 2024), law (Colombo et al., 2024), home renovation (Wen et al., 2023), medicine (Xiong et al., 2023; Chen et al., 2023; Yang et al., 2024c; Wu et al., 2024; Zakka et al., 2024), and scientific research (Azerbayev et al., 2023; Bi et al., 2023; Yang et al., 2024d). Despite these advancements, these vertical domain LLMs have not addressed the unique challenges posed by SNS. While (Liu et al., 2024b) and (Yang et al., 2024b) explore the application of LLMs to limited set of NLP tasks within SNS, their coverage remains constrained. Therefore, significant gap exists in this area, which RedOne aims to address."
        },
        {
            "title": "3 RedOne Model",
            "content": "As illustrated in Figure 2, the training strategy of RedOne contains three stages. First, in Section 3.1, we conduct continue pretraining to enrich the models grasp of nuanced SNS field knowledge. Subsequently, in Section 3.2, we sharpen the models instruction-following capabilities through supervised fine-tuning across various tasks. Finally, we leverage preference information from the training data to perform preference optimization, ultimately yielding the RedOne with superior performance in the SNS domain. 3.1 Continue Pretraining To enhance the large models fundamental domain knowledge we conducted continue pretraining at this stage, which can be divided into three sub-stages: data collection and data construction, filtering and mixture, along with domain-aware continue pretraining. 3.1.1 Data Collection and Data Construction We specifically collected continue pretraining data from the following two data sources: (1) General High-quality Data. We selected several high-quality open-source pretraining corpora (Qiu et al., 2024; Weber et al., 2024; Penedo et al., 2024) to preserve the models fundamental generalization capabilities. To improve training efficiency, we uniformly construct all general data into single-sentence text format and perform segmentation and concatenation processing based on predefined text length thresholds. (2) SNS-specific Domain Data. We collect the largescale training data from SNS platforms and the open web, capturing diverse social communication patterns including informal discussions, short-form comments, sarcasm, emotionally charged content, and so on. For better reveal the underlying information in the pretraining data, we incorporate user interaction data to guide the training process. Specifically, we group contexts and comments with their corresponding user interaction data, which naturally clusters semantically related SNS content without additional processing. Through these steps, we collected and constructed large-scale dataset comprising various tasks with over 100B tokens for downstream processing. Figure 2: Overview of our training pipeline. 3.1.2 Filtering and Mixture Considering data quality is crucial for model training (Zhou et al., 2023a), we constructed data-filtering pipeline inspired by (Yuan et al., 2024), which comprises task-oriented rule filtering and small language model filtering (Wang et al., 2025). The former identifies specific error content such as HTML tags and repetitive sentences, while the latter focuses on global assessment aspects including coherence and tone appropriateness. Based on this data-filtering pipeline, we further applied the RegMix method (Liu et al., 2024a) to identify an optimal data mixture distribution and filter out unnecessary data. Through this comprehensive filtering and mixture process, we ultimately constructed high-quality dataset of 20B tokens for training. 3.1.3 Domain-aware Continue Pretraining After data construction, we conduct continue pretraining on the complete dataset. Specifically, RedOne is trained from the Qwen2.5 (Qwen et al., 2025a) checkpoint using the same configurations, leveraging its strong linguistic capabilities across multiple domains. Through this domain-aware continue pretraining process, we ultimately obtain model that effectively captures SNSspecific linguistic patterns while maintaining minimal degradation in general language modeling capabilities. 3.2 Supervised Fine-Tuning To bridge the gap between pretraining objectives and the specific requirements of real-world SNS applications, we further conduct supervised fine-tuning on our model through carefully designed data construction and multistage training strategies. 3.2.1 Task Definition and Data Construction As SFT training data is significat affect the final instruction following ability in domain tasks (Dong et al., 2023), we extensively collected large-scale userTask Name Note Taxonomy Query Classification Query Intent Recognition Hashtag Prediction Machine Reading Comprehension Highlight Word Detection Query-Note Relevance Query-Note Retrieval Post-View Search Emotional Companion Dialogue Role-playing Dialogue SNS Domain Translation Capability Content Understanding Content Understanding Content Understanding Information Extraction Information Extraction Information Extraction Semantic Matching Semantic Matching User Behavior Modeling Dialogue Dialogue Translation Table 1: Overview of SNS Tasks and Their Capabilities generated content from public platforms, including notes, comments, queries, and interaction logs, which provide real enviorment signal for us to improve model actions. Notably, we focused on preserving the linguistic style which exhibit typical SNS characteristics such as informal language, sarcasm, sentiment, and topical shifts while collecting data (Eisenstein, 2013), aim for representative and practical coverage for SNS scenarios. After data collection, we ultimatly consolidate six kinds of core capabilities essential for SNS applications: content understanding, information extraction, semantic matching, user behavior modeling, dialogue and persona simulation, and translation, as show in Table 1. Each is supported by well-defined tasks reflecting real-world challenges and the overview is shown in Appendix A.2. Additionally, during SFT, we also incorporated open source instruction data covering general tasks such as instruction following (Li et al., 2025; Zhou et al., 2023a), multiturn dialogue (Zhao et al., 2024), and long chainof-thought (CoT) reasoning (Guha et al., 2025; Ye et al., 2025), to mitigate catastrophic forgetting (McCloskey and Cohen, 1989) and retain generalization ability of RedOne model. 3.2.2 Two-Step Training In domain SFT, two-step mixed fine-tuning has been demonstrated to effectively enhance domain-specific capabilities (Dong et al., 2024). For RedOnes SFT, we implement this strategy by mixing SNS-specific data with general data across two steps. In the first step, we train the model on the complete SNS dataset combined with large volume of general data. This approach enables the model to learn diverse task formats within the SNS domain while preserving its generalization capabilities. In the second step, we fine-tune the model using higher proportion of SNS domain data, thereby further enhancing performance on domain-critical tasks. 3.3.2 Direct Preference Optimization To effectively leverage the rich preference signals in our SNS dataset, we adopt DPO (Rafailov et al., 2023) as our preference-based fine-tuning algorithm. This approach enables the model to better align with human preferences while simultaneously exploiting the latent information embedded in ground-truth labels. Finally, through this comprehensive three-stage training pipeline encompassing CPT, SFT and PO, we ultimately obtain domain-specific large language model RedOne that demonstrates superior performance in the target domain while maintaining reasonable general capabilities. 3.3 Preference Optimization SNS tasks like query-note relevance modeling often produce multiple plausible but quality-diverse outputs. While SFT improves instruction-following, it fails to exploit implicit preference signals among these candidates, causing overfitting and poor generalization (Chu et al., 2025). To address these limitations, in this section, we carefully craft preference data and perform PO to obtain better domain-specific model. 3.3.1 Preference Data Construction To enhance alignment with human preferences and utilize the information embedded in data labels, we integrate different preference pair construction strategies according to the nature of different task types. Specifically, we categorize our data into two types and adopt corresponding strategies: For subjective tasks, such as emotional dialogue and role-playing, our primary objective is to achieve better alignment with human preferences. Therefore, the first step begins with domain experts creating preference annotations on model-generated responses (Ouyang et al., 2022). Furthermore, to scale up the preference dataset, we evaluate the consistency between trained judge models (Cao et al., 2024a) and human preference, then leverage these models with high performance to expand specific data. In contrast, for objective tasks with definitive correct answers, our strategy shifts toward extracting and utilizing the implicit structural information within the data labels. Here, we employ two approaches: First, we leverage the inherent structure of questions that contain both correct answers and incorrect options, constructing preference pairs that exploit the ordinal relationships within data. Complementarily, to actively address model limitations, we construct preference pairs from model errors, using ground truth as positive examples and incorrect predictions as negative to target specific weaknesses. By integrating these tailored approaches, we systematically process all SNS-domain data according to their inherent characteristics, ultimately constructing preference optimization datasets that effectively capture both human preferences and implicit data information for comprehensive model enhancement."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation details During the CPT stage, we follow hyperparameter recommendations from the Qwen2.5 (Yang et al., 2024a) and set the sequence length to 4096 and the learning rate to 1 105. Training is performed for one epoch over mixed corpus of general and SNS-specific data. Following pretraining, SFT is conducted for three epochs in step one and two epochs in step two, with maximum sequence length of 16 384, batch size of 128, linear warm-up ratio of 0.1, and learning rate of 3 106. Optimization is performed using AdamW (Loshchilov and Hutter, 2017) (β1=0.9, β2=0.95, ϵ=108). In the final PO stage, we employ learning rate of 1 107, batch size of 64, sequence length of 4096, training for two epochs, with sft loss coefficient set to 0.3. 4.2 Benchmarks et al., (Rein For general capabilities evaluation, we use datasets includsimilar to those employed in community, ing general natural language comprehension (i.e. MMLU (Hendrycks et al., 2021a), CMMLU (Li et al., 2023a), CEVAL (Huang et al., 2023), 2023), NewsGPQA-Diamond Bench (Li et al., 2024a)), reasoning (i.e. MMLUPro (Wang et al., 2024), BBH (Suzgun et al., 2023), GaokaoBench (Zhang et al., 2023)), mathematics (i.e. AIME2025 (MAA, 2025), GSM8K (Cobbe et al., 2021) and MATH500 (Hendrycks et al., 2021b)), coding (i.e. HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and LiveCodeBench(24072502)(Jain et al., 2024)), translation (i.e. WMT-22/23/24 and Flores(Goyal et al., 2022)), instruction following (i.e. IFEval (Zhou et al., 2023b)), hallucination and human preference alignment (i.e. HaluEval (Li et al., 2023b) and CompassBench (Cao et al., 2024b)). To further evaluate RedOnes performance in SNS domain, we selected specialized SNS benchmarks including SNS-Bench (Guo et al.) and SNS-TransBench (Guo et al., 2025). 4.3 Main Results As shown in Tables 2 and 3, we conducted comparison between RedOne with baseline models across varModels Avg. Taxonomy Hashtag QueryCorr MRC NER Gender CHLW QueryGen Avg. General-Bench SNS-Bench Llama-3.1-8B (Grattafiori et al., 2024) Ministral-8B (Mistral-AI, 2024) InternLM3-8B (Cai et al., 2024) GLM-4-9B-0414 (GLM et al., 2024) 51.24 49.93 58.55 63. Qwen2.5-7B (Qwen et al., 2025b) RedOne-7B (Ours) 63.01 63.83 (+0.82%) 37.74 42.62 51.83 56.03 49.50 72.18 66.62 70.58 76.98 77.67 73.80 88. 33.32 36.24 38.65 38.03 42.37 65.09 31.27 30.71 25.25 45.29 45.32 63.98 47.10 37.79 39.41 47.01 45.41 51. 74.61 82.38 66.84 51.30 88.08 70.47 26.88 28.04 44.71 27.51 33.76 74.73 38.60 46.27 43.46 45.52 44.65 48. 44.52 46.83 48.39 48.55 52.86 66.88 (+14.02%) SNS-TransBench ZHEN ENZH BLEU chrF++ BLEU chrF++ 23.07 25.67 24.85 32.20 31.43 38.06 48.15 50.91 50.44 56.90 55.91 62.66 29.32 32.02 35.58 39.73 38.36 46. 29.13 31.18 34.04 37.40 36.48 44.82 Avg. 32.42 34.95 36.23 41.57 40.55 48.11 (+7.56%) Table 2: Results of 7B-scale models. Bold entries indicate the best model, while underlined entries denote the second one. Percentage improvements relative to the baseline Qwen2.5 foundation model are also shown. Models General-Bench SNS-Bench Avg. Taxonomy Hashtag QueryCorr MRC NER Gender CHLW QueryGen Avg. Phi-4-14B (Abdin et al., 2024) Mistral-Small-24B (Mistral-AI, 2025) Llama-3.3-70B (Grattafiori et al., 2024) GLM-4-32B-0414 (GLM et al., 2024) Deepseek-V3-0324 (DeepSeek-AI et al., 2025) Doubao-1.5-Pro-32k (Doubao-Team, 2025) GLM-4-Plus (GLM et al., 2024) GPT-4o-1120 (OpenAI) Claude-3.7-Sonnet (Anthropic) Gemini-2.0-Flash (DeepMind, 2024) Qwen-Max (Qwen et al., 2025b) 63.00 65.63 67.64 74.39 75.22 76.13 70.25 70.72 75.10 74.42 71.86 Qwen2.5-32B (Qwen et al., 2025b) RedOne-32B (Ours) 71.68 73.72 (+2.04%) 57.62 64.88 62.94 63.36 67.27 30.00 65.46 65.79 72.03 68.76 65.68 59.90 81.45 79.56 83.89 83.28 85.50 86.59 83.21 84.31 84.98 88.83 87.36 84.47 80.51 90. Open-Source Large Language Models 46.32 48.77 50.76 47.33 47.71 53.39 46.51 27.38 53.72 60.97 44.99 52.09 56.09 50.41 56.00 89.12 91.19 91.19 80.31 90.16 Closed-Source Large Language Models 58.25 52.13 51.79 54.10 48.41 54.36 46.00 67.07 61.32 55.81 58.89 54.86 52.21 61.34 55.04 59.24 56.60 53.16 54.99 56.13 53.58 55.78 54.51 51. 90.67 86.53 88.08 92.23 89.64 91.19 90.67 81.87 29.23 32.10 33.58 33.19 40.45 30.61 30.09 38.96 31.11 37.39 37.97 38.84 70.40 44.76 46.01 46.41 46.90 46. 46.55 44.68 47.33 45.49 46.27 46.64 45.66 50.37 55.62 58.18 56.45 57.59 61.90 57.15 59.02 61.35 61.85 60.45 62.18 58.89 69.03 (+10.14%) SNS-TransBench ZHEN ENZH BLEU chrF++ BLEU chrF++ 31.28 31.29 34.00 36.32 35.65 33.71 41.57 40.32 35.63 32.72 35.55 32.56 40. 57.23 56.72 59.18 61.31 61.58 61.85 65.95 63.91 61.66 58.84 60.92 58.14 64.54 37.58 39.28 41.25 42.53 46.86 45.54 48.79 49.15 45.79 41.80 46.08 42.34 48. 36.68 37.32 39.56 40.77 44.58 44.35 47.06 47.28 44.23 40.16 44.14 40.71 46.05 Avg. 40.69 41.15 43.50 45.23 47.17 46.36 50.84 50.17 46.83 43.38 46. 43.44 49.84 (+6.40%) Table 3: Results of 32B-scale models. Bold entries indicate the best model, while underlined entries denote the second one. Percentage improvements relative to the baseline Qwen2.5 foundation model are also shown. Models HashTag QueryCorr MRC Qwen2.5-Finetuned RedOne RedOne-Finetuned 88.93 88.02 (-0.91%) 90.51(+1.78%) 57.76 65.09 (+12.63%) 65.77(+13.87%) 62.26 63.98 (+2.76%) 64.47(+3.55%) Models CHLW QueryGen SNS-Trans Qwen2.5-Finetuned RedOne RedOne-Finetuned 78.41 74.73 (-4.72%) 79.11(+0.89%) 48.25 48.69 (+0.91%) 49.21(+1.99%) 48.01 48.11 (+0.21%) 48.32(+0.65%) Table 4: Performance comparison of task-specific Finetuned on Qwen-2.5-Instruct and RedOne (all models are 7B scale). 4.4 Task-specific SFT Comparison To further explore the impact of base model selection on task-specific fine-tuning and validate our domain LLMs effectiveness, we conducted experiments on two 7Bscale models: the original Qwen-2.5-Instruct (\"Qwen\") and our SNS-adapted model (\"RedOne\"). We evaluated three variants: (1) Qwen-Finetuned, involving taskspecific fine-tuning on Qwen; (2) RedOne-Finetuned, involving task-specific fine-tuning on RedOne; and (3) RedOne, representing zero-shot inference without further fine-tuning. As shown in Table 4, RedOne-Finetuned consistently outperforms Qwen2.5-Finetuned across most datasets, demonstrating that domain-aligned post-training (i.e., RedOne) provides stronger foundation for downstream SFT. Meanwhile, even RedOne in the zero-shot setting exhibits strong performance, further corroborating the benefits of domain adaptation. Overall, these results indicate that initializing SFT from domainadapted base model is more effective than starting from general-purpose large model. This finding suggests that domain-specific post-training can serve as powerFigure 3: Model capability radar diagram across different task categories. ious tasks. Meanwhile, as illustrated in Figure 3, we compared RedOne with its base model across seven dimensions (six for general and one for SNS). Both results indicate that RedOne not only maintains strong general capabilities, even surpassing its base model on general tasks, but also exhibit exceptional effectiveness in the SNS domain. Additionally, RedOne achieves performance comparable to significantly larger models across most tasks, with limited improvement opportunities observed only in few areas. The results also demonstrate that scaling up RedOne consistently enhances performance over smaller versions, aligning with established model scaling laws. These findings underscore RedOnes strong potential for further advances through continued increases in model size. CPT SFT PO General SNS SNS-Trans 62.28 61.95 63.83 62.65 64.36 53.28 65.12 66.88 64.57 64.98 41.39 47.70 48.11 47.47 47.64 Table 5: Ablation study results of RedOne-7B. Task Metric Change (%) Harmful Content Detection Post-View Search Exposure Rate () Click Page Rate () 11.23 +14.95 Table 6: Effectiveness in online scenarios. ful approach for improving both zero-shot capabilities and task-specific performance after fine-tuning. 4.5 Ablation Study In this section, we investigate the contributions of each stage in our training pipeline through comprehensive ablation studies, with results summarized in Table 5. The results demonstrate that introducing the CPT stage leads to slight decrease in performance on generaldomain tasks, while significantly boosting performance on SNS related tasks. This observation confirms that continue pretraining with domain-specific data effectively adapts the model to target domains, with the tradeoff in general performance being minimal. Moreover, the sequential application of SFT and PO yields additional improvements, which indicates the effectiveness of our comprehensive training strategy. 4.6 Online Results To further validate RedOnes practical effectiveness, we deployed the model across multiple internal SNS scenarios and witnessed remarkable performance gains in real-world applications compared with previous singletask models as shown in Table 6. In harmful content detection, RedOne exhibited exceptional safety capabilities by slashing the exposure rate of harmful notes by 11.23%, effectively filtering out non-compliant content and strengthening platform security. Moreover, for post-view search recommendation, the model delivered 14.95% increase in click page rate, indicating improved content discovery and enhanced user engagement following note interactions. These online results demonstrate the strong practical utility of RedOne in real-world scenarios. 4.7 Out-of-Domain Ability Analysis In this subsection, we examine the impact of preserving general-domain capabilities during domain adaptation by evaluating out-of-domain (OOD) robustness. Specifically, we select one task without corresponding supervised training data (Note Taxonomy) and two tasks with available data (Note Hashtag, Note MRC) from the SNS bench. For the latter two, we remove their related training data during supervised fine-tuning (SFT), effectively Figure 4: Performance on OOD tasks for models of varying parameter size. making all three tasks OOD. We then compare models trained with both general and SNS data against those trained with SNS data only, across various model sizes. Our results (Figure 4) suggest that including general-domain data helps models generalize better to OOD tasks, and this trend is more visible for larger models. This highlights that maintaining general capabilities can be beneficial for domain adaptation, though further study is needed to fully understand this effect. 4.8 Case Study Input Title: Found it! Soft-Sole Commuter Loafer You Can Walk In All Day. Tags: Height Increasing Thick Sole Shoes, Loafers, Beautiful Loafers. Popular Comments: How to buy; link. Qwen How to choose height-increasing platform shoes. RedOne height-increasing thick-soled loafers. Table 7: Post-view search case: Input SNS context and model-generated queries. To further demonstrate RedOnes effectiveness in capturing user search intent within SNS scenarios, we present case study on the post-view search task. As shown in Table 7, we analyze sample post featuring height-increasing loafers that generated significant purchase intent among users. Qwen produces general shopping query, whereas RedOne directly identifies the core product keywords, better reflecting users intent to search for and purchase the featured item. This demonstrates RedOnes superior capability for generating actionable queries aligned with real user needs."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce RedOne, domain-specific LLM trained through three-stage strategy that enhances SNS-specific capabilities while preserving general performance. We believe our approach can inspire future research in developing specialized LLMs and advancing practical applications in social media."
        },
        {
            "title": "Limitations",
            "content": "Although our proposed method demonstrates high effectiveness, it requires extensive data processing, resulting in considerable resource costs. Additionally, while our approach shows promising results on existing SNS benchmarks, the limited availability of SNS-related benchmarks restricts our comprehensive understanding of the methods performance across domain tasks."
        },
        {
            "title": "Ethical Considerations",
            "content": "When integrating large language models as essential components within application services, it is crucial to rigorously consider potential model hallucinations and security risks. To ensure reliable service delivery, leveraging RedOne for model services requires careful implementation to mitigate adverse user impacts. Furthermore, we emphasize the critical importance of adhering to stringent user privacy protection standards throughout data collection and processing workflows, ensuring comprehensive personal information security."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, and 8 others. 2024. Phi-4 technical report. Preprint, arXiv:2412.08905. Anthropic. Claude 3.7 sonnet and claude code. https: //www.anthropic.com/news/claude-3-7-sonne t. Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. CoRR, abs/2108.07732. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631. Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. 2023. Oceangpt: large language model for ocean science tasks. arXiv preprint arXiv:2310.02031. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, and 81 others. 2024. Internlm2 technical report. Preprint, arXiv:2403.17297. Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. 2024a. Compassjudger-1: All-in-one judge model helps arXiv preprint model evaluation and evolution. arXiv:2410.16256. Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. 2024b. Compassjudger-1: All-in-one judge model helps model evaluation and evolution. abs/2410.16256. Caleb Carr and Rebecca Hayes. 2015. Social media: Defining, developing, and divining. Atlantic journal of communication, 23(1):4665. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374. Yirong Chen, Zhenyu Wang, Xiaofen Xing, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu, and 1 others. 2023. Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt. arXiv preprint arXiv:2310.15896. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre FT Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia Morgado, and 1 others. 2024. Saullm-7b: pioneering large language model for law. arXiv preprint arXiv:2403.03883. Google DeepMind. 2024. Introducing gemini 2.0: our new ai model for the agentic era. https://blog.g oogle/technology/google-deepmind/google-g emini-ai-update-december-2024/. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. How abilities in large language models are affected by supervised fine-tuning data composition. CoRR, abs/2310.05492. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. How abilities in large language models are affected by supervised fine-tuning data composition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 177198, Bangkok, Thailand. Association for Computational Linguistics. Doubao-Team. 2025. Doubao-1.5-pro: Model release. https://team.doubao.com/en/special/douba o_1_5_pro. Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of the 2013 conference of the North American Chapter of the association for computational linguistics: Human language technologies, pages 359369. Sina Elahimanesh, Mohammadali Mohammadkhani, and Shohreh Kasaei. 2025. Emotion alignment: Discovering the gap between social media and real-world arXiv sentiments in persian tweets and images. preprint arXiv:2504.10662. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, and 37 others. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:522538. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, and 1 others. 2025. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178. Hongcheng Guo, Shaosheng Cao, Boyang Wang, Lei Li, Liang Chen, Xinze Lyu, Zhe Xu, Yao Hu, Zhoujun Li, and 1 others. Sns-bench: Defining, building, and assessing capabilities of large language models in social networking services. In Forty-second International Conference on Machine Learning. Hongcheng Guo, Fei Zhao, Shaosheng Cao, Xinze Lyu, Ziyan Liu, Yue Wang, Boyang Wang, Zhoujun Li, Chonggang Lu, Zhe Xu, and 1 others. 2025. Redefining machine translation on social network services with large language models. arXiv preprint arXiv:2504.07901. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. In ICLR. OpenReview.net. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-Eval: multi-level multi-discipline chinese evaluation suite for foundation models. In NeurIPS. Òscar Garibo Orts. 2019. Multilingual detection of hate speech against immigrants and women in twitter at semeval-2019 task 5: Frequency analysis interpolation for hate in speech detection. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 460463. Tunazzina Islam and Dan Goldwasser. 2025. Uncovering latent arguments in social media messaging by employing LLMs-in-the-loop strategy. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 73977429, Albuquerque, New Mexico. Association for Computational Linguistics. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974. Julie Jiang and Emilio Ferrara. 2023. Social-llm: Modeling user behavior at scale using language models and social network data. arXiv preprint arXiv:2401.00893. Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, and Srijan Kumar. 2024. Mm-soc: Benchmarking multimodal large language models in social media platforms. arXiv preprint arXiv:2402.14154. Mohamed Bayan Kmainasi, Ali Ezzat Shahroor, Maram Hasanain, Sahinur Rahman Laskar, Naeemul Hassan, and Firoj Alam. 2024. Llamalens: Specialized multilingual llm for analyzing news and social media content. arXiv preprint arXiv:2410.15308. Thanos Konstantinidis, Giorgos Iacovides, Mingxue Xu, Tony Constantinides, and Danilo Mandic. 2024. Finllama: Financial sentiment classification for algorithmic trading applications. arXiv preprint arXiv:2403.12285. Deepak Kumar, Yousef Anees AbuHashem, and Zakir Durumeric. 2024. Watch your language: Investigating content moderation with large language models. In Proceedings of the International AAAI Conference on Web and Social Media, volume 18, pages 865 878. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023a. CMMLU: Measuring massive multitask language understanding in Chinese. CoRR, abs/2306.09212. Jijie Li, Li Du, Hanyu Zhao, Bo-wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, and Yonghua Lin. 2025. Infinity instruct: Scaling instruction selection and synthesis to enhance language models. arXiv preprint arXiv:2506.11116. Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023b. HaluEval: large-scale hallucination evaluation benchmark for large language models. pages 64496464. Miao Li, Ming-Bin Chen, Bo Tang, ShengbinHou ShengbinHou, Pengyu Wang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Keming Mao, Cheng Peng, and Yi Luo. 2024a. NewsBench: systematic evaluation framework for assessing editorial capabilities of large language models in Chinese journalism. pages 999310014. Wanhua Li, Zibin Meng, Jiawei Zhou, Donglai Wei, Chuang Gan, and Hanspeter Pfister. 2024b. Socialgpt: Prompting llms for social relation reasoning via greedy segment optimization. arXiv preprint arXiv:2410.21411. Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, and Ruichao Yang. 2024. Towards explainable harmful meme detection through multimodal debate between large language models. In Proceedings of the ACM Web Conference 2024, pages 23592370. Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. 2024a. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492. Qiang Liu, Xiang Tao, Junfei Wu, Shu Wu, and Liang Wang. 2024b. Can large language models arXiv preprint detect rumors on social media? arXiv:2402.03916. Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. CoRR, abs/1711.05101. Junyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, and Hongfei Lin. 2024. Towards comprehensive detection of chinese harmful memes. Advances in Neural Information Processing Systems, 37:1330213320. MAA. 2025. American invitational mathematics examination - aime. American Invitational Mathematics Examination - AIME 2025. Michael McCloskey and Neal Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109165. Elsevier. Mistral-AI. 2024. Un ministral, des ministraux. http s://mistral.ai/news/ministraux. Accessed: 2024-10-16. Mistral-AI. 2025. Mistral small 3.1. https://mist ral.ai/news/mistral-small-3-1. Accessed: 2025-03-17. Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. 2018. Semeval2018 task 1: Affect in tweets. In Proceedings of the 12th international workshop on semantic evaluation, pages 117. OpenAI. Gpt-4o: Openais newest multimodal model. https://openai.com/index/gpt-4o. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, and 1 others. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849. Letian Peng, Zilong Wang, Feng Yao, Zihan Wang, and Jingbo Shang. 2024. Metaie: Distilling meta model from llm for all kinds of information extraction tasks. arXiv preprint arXiv:2404.00457. Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia Yu, ChaoBin Zhang, Zhenxiang Li, Pei Chu, Yuan Qu, and 1 others. 2024. Wanjuan-cc: safe and high-quality open-sourced english webtext dataset. arXiv preprint arXiv:2402.19282. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. Preprint, Qwen2.5 technical report. 2025a. arXiv:2412.15115. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025b. Qwen2.5 technical report. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In NeurIPS. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. GPQA: graduate-level Google-proof Q&A benchmark. CoRR, abs/2311.12022. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, and Dinggang Shen. 2023. Doctorglm: Fine-tuning your chinese doctor is not herculean task. arXiv preprint arXiv:2304.01097. Sara Rosenthal, Noura Farra, and Preslav Nakov. 2019. Semeval-2017 task 4: Sentiment analysis in twitter. arXiv preprint arXiv:1912.00741. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2023. Challenging BIGBench tasks and whether chain-of-thought can solve them. In ACL (Findings), pages 1300313051. Association for Computational Linguistics. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. MMLU-Pro: more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574. Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng, Chaojun Xiao, and 1 others. 2025. Ultrafineweb: Efficient data filtering and verification for high-quality llm training data. arXiv preprint arXiv:2505.05427. Maurice Weber, Dan Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, and 1 others. 2024. Redpajama: an open dataset for training large language models. Advances in neural information processing systems, 37:116462116492. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and 43 others. 2024a. Qwen2 technical report. CoRR, abs/2407.10671. Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024b. Mentallama: interpretable mental health analysis on social media with large language models. In Proceedings of the ACM Web Conference 2024, pages 44894500. Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, and Hongying Zan. 2024c. Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 1936819376. Xianjun Yang, Junfeng Gao, Wenxin Xue, and Erik Alexandersson. 2024d. Pllama: An open-source large language model for plant science. arXiv preprint arXiv:2401.01600. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and Ying Shen. 2024. survey on recent advances in llm-based multi-turn dialogue systems. arXiv preprint arXiv:2402.18013. Cheng Wen, Xianghui Sun, Shuaijiang Zhao, Xiaoquan Fang, Liangyu Chen, and Wei Zou. 2023. Chathome: Development and evaluation of domain-specific language model for home renovation. arXiv preprint arXiv:2307.15290. Dong Yuan, Eti Rastogi, Gautam Naik, Sree Prasanna Rajagopal, Sagar Goyal, Fen Zhao, Bharath Chintagunta, and Jeff Ward. 2024. continued pretrained llm approach for automatic medical note generation. In NAACL (Short Papers). Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. 2024. Pmc-llama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9):18331843. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. 2025. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv e-prints, pages arXiv2504. Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex Dalal, Jennifer Kim, Michael Moor, Robyn Fong, Curran Phillips, Kevin Alexander, Euan Ashley, and 1 others. 2024. Almanacretrieval-augmented language models for clinical medicine. Nejm ai, 1(2):AIoa2300068. Zheyong Xie, Weidong He, Tong Xu, Shiwei Wu, Chen Zhu, Ping Yang, and Enhong Chen. 2023. Comprehending the gossips: Meme explanation in time-sync video comment via multimodal cues. ACM Trans. Asian Low-Resour. Lang. Inf. Process., 22(8). Jingying Zeng, Richard Huang, Waleed Malik, Langxuan Yin, Bojan Babic, Danny Shacham, Xiao Yan, Jaewon Yang, and Qi He. 2024. Large language models for social networks: Applications, challenges, and solutions. arXiv preprint arXiv:2401.02575. Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. 2024. Self-emotion blended dialogue generation in social simulation agents. In Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 228247, Kyoto, Japan. Association for Computational Linguistics. User Behavior Modeling: This capability involves modeling and simulating user actions, such as generating follow-up queries based on previous browsing or posting activities (post-view search). It reflects how users might interact with content in dynamic SNS environment. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. 2023. Evaluating the performance of large language models on GAOKAO benchmark. CoRR, abs/2305.12474. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, and 1 others. 2023a. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023b. Instruction-following evaluation for large language models. CoRR, abs/2311.07911. Dialogue and Persona Simulation: To enhance natural interaction and personalization, dialogue tasks ask the model to engage in emotional companion conversations or role-play as different personas in group chats, capturing both the style and richness of real SNS dialogues. Translation: Given the prevalence of multilingual content, the model is also trained to translate notes between languages, with attention to preserving the original tone, sentiment, and informal expressions common across SNS platforms. Each task adopts the most suitable instruction-tuning format: multiple choice supports classification and selection, extraction is used for entity and span prediction, and generation handles open-ended responses such as dialogue or translation. This format-driven design ensures consistent prompting and facilitates efficient multi-task training."
        },
        {
            "title": "A Appendices",
            "content": "A.1 SNS Tasks In this section, we provide an overview of the key tasks defined for SFT in SNS scenarios. These tasks are designed to reflect real-world user behavior, content patterns throughout social platforms. The SFT task suite covers six core capability areas, each capturing an important aspect of SNS applications: Content Understanding: This category focuses on the models ability to comprehend and categorize usergenerated content as well as user queries. Example tasks include classifying notes into categories (note taxonomy), determining the topic or domain of user queries (query classification), and identifying fine-grained query intent (query intent recognition). Information Extraction: Tasks in this category address the identification and extraction of structured information from informal SNS posts. This includes predicting appropriate hashtags for post, answering questions about note content, and detecting highlight or anchor words that represent user focus. Figure 5: Token length distribution in the dataset. The histogram uses logarithmic y-axis with dashed lines indicating the median (345 tokens) and the 95-th percentile (2,342 tokens). A.2 SFT Data Statistical Analysis Token Length Statistics Figure 5 presents the token length distribution across all samples in our dataset, displayed on logarithmic scale to accommodate the wide range of sequence lengths up to 16,384 tokens. The distribution exhibits the characteristic heavy-tailed pattern typical of natural language corpora. Semantic Matching: Here, the model is required to judge the semantic relationship and relevance between items such as user queries and social notes. Typical tasks include evaluating whether note is relevant to given query (query-note relevance) and retrieving the most pertinent or high-quality notes for search scenarios (query-note retrieval). Task Category Distribution We adopted the labeling taxonomy from Infinity Instruct (Li et al., 2025), organizing it into primary and secondary categories. Specifically, we used subset of Infinity Instruct data, consisting of instructions paired with their corresponding labels, as training data to fine-tune labeling model based on Qwen2.5-7B-Instruct. This trained labeling (a) Primary task categories. (b) Secondary task categories. Figure 6: Top 10 distributions of primary and secondary task categories in the SFT dataset. model was then applied to annotate all instructions in our complete SFT dataset. The comprehensive distribution of primary and secondary label categories is illustrated in Figures 6a and 6b, respectively. The distribution analysis reveals several key characteristics of our SFT dataset. At the primary level, natural language processing and understanding dominates with over 3.4 million instances, followed closely by information processing and integration (3.1 million) and problem solving and support (2.2 million). This indicates strong emphasis on core language comprehension and analytical capabilities. Mathematical ability and programming-related tasks also constitute significant portions, with over 1.2 million instances each, reflecting the datasets comprehensive coverage of technical skills. At the secondary level, information extraction leads with 2.1 million instances, while text understanding and problem-solving tasks follow with 1.7 million and 1.4 million instances respectively. The relatively balanced distribution across different cognitive abilities suggests that our dataset provides diverse training scenarios for developing well-rounded AI capabilities."
        }
    ],
    "affiliations": [
        "NLP Team, Xiaohongshu Inc., China"
    ]
}