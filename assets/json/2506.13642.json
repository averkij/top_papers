{
    "paper_title": "Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model",
    "authors": [
        "Shaolei Zhang",
        "Shoutao Guo",
        "Qingkai Fang",
        "Yan Zhou",
        "Yang Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 4 6 3 1 . 6 0 5 2 : r Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model Shaolei Zhang1,3, Shoutao Guo1,3, Qingkai Fang1,3, Yan Zhou1,3, Yang Feng1,2,3 1Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) 2Key Laboratory of AI Safety, Chinese Academy of Sciences 3University of Chinese Academy of Sciences, Beijing, China zhangshaolei20z@ict.ac.cn, fengyang@ict.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, large languagevision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces CTCbased layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users comprehensive multimodal experience1."
        },
        {
            "title": "Introduction",
            "content": "Large multimodal models (LMMs) such as GPT-4o [1] exhibit omni-capabilities across text, vision, and speech modalities, unlocking broad potential across applications. Compared to vision-oriented LMMs [2, 3], omni-modal LMMs can support speech interaction based on visual information. Furthermore, advanced online services like GPT-4o can offer seamless see-while-hear interaction for users by simultaneously providing intermediate text (i.e., transcription of user inputs and model responses) during speech interaction, which highlights the importance of building LMMs that can simultaneously support interactions through various modality combinations. However, building LMMs that support text, vision, and speech remains substantial challenge due to the intrinsic representational discrepancies across modalities. Most existing LMMs specialize in Corresponding author: Yang Feng. 1Code: https://github.com/ictnlp/Stream-Omni, Model: https://huggingface.co/ICTNLP/stream-omni-8b. Preprint. Under review. (a) Sequence-dimension concatenation for modality alignments in previous works (b) Sequence-dimension concatenation for vision-text alignment, and layer-dimension mapping for speech-text alignment Figure 1: Comparison of modality alignments in Stream-Omni and previous works. either vision [4, 3, 57] or speech [811], feeding the extracted modality representations into the context of large language model (LLM) backbone. Recently, some omni-modal LMMs [1214] aim to integrate text, vision, and speech within unified framework. Such models typically concatenate representations from individual modality encoders along the sequence dimension before feeding them into the LLM backbone, as shown in Figure 1(a). These concatenation-based approaches simplify modality integration, but they heavily rely on large-scale data to learn modality alignments in datadriven manner [10, 11, 13, 14], which is not friendly to limited public tri-modal data. Moreover, such concatenation-dimension alignments are not flexible enough to simultaneously produce intermediate text results during speech interactions, as GPT-4o does. To this end, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. In multimodal interaction, text, vision, and speech modalities serve different roles, where vision primarily conveys visual information [3], while text and speech focus on language information [9]. As such, directly concatenating all three modalities in sequence-dimension is suboptimal for modality alignments. Ideally, the speech and text should exhibit high semantic consistency, while the vision is semantically complementary to the text. Therefore, vision and speech should be separately aligned to text in different ways. Along with this idea, we introduce Stream-Omni, language-vision-speech LMM based on efficient text-centric modality alignments, which can flexibly support interactions under various modality combinations. As shown in Figure 1(b), Stream-Omni is built upon the LLM backbone and aligns the vision and speech modalities to text using different mechanisms. For vision, which is semantically complementary to text, Stream-Omni employs sequence-dimension concatenation for vision-text alignment. For speech, which shares higher semantic consistency with text, Stream-Omni introduces layer-dimension speech-text mapping for speech-text alignment. Specifically, Stream-Omni takes LLM as the core and introduces bottom and top speech layers to model speech-to-text mapping via Connectionist Temporal Classification (CTC) [15], thereby enabling external interaction through the speech modality and simultaneous internal generation via the text modality. With speechtext mapping, Stream-Omni can transfer the text capability of LLM backbone to the speech modality with less speech data. As byproduct, Stream-Omni can simultaneously produce intermediate text results (i.e., transcription of instruction and response) during speech interaction, offering more comprehensive multimodal experience. We evaluate Stream-Omni on various benchmarks covering visual understanding, speech interaction, and vision-grounded speech interaction, and the results demonstrate that Stream-Omni achieves strong performance using only 23,000 hours of speech data."
        },
        {
            "title": "2 Related Work",
            "content": "Existing large multimodal models can be categorized into three types: vision-oriented, speechoriented, and omni-modal. For vision-oriented LMMs, LLaVA [3] is the most widely adopted architecture. In LLaVA, vision encoder (CLIP [16]) is used to extract visual features from visual inputs, which are then concatenated with the text inputs and fed into LLM to generate text responses. 2 Figure 2: Architecture of Stream-Omni. Right: Interactions under various modality combinations. Based on LLaVA, the following works improve the vision-oriented LMMs through improved training data [5, 17, 18], enhanced image encoding [19, 4, 20], and extended video understanding [2125]. For speech-oriented LMMs, existing methods rely on either continuous or discrete speech units. Methods based on continuous representations, such as Mini-Omni [8], LLaMA-Omni [9], and FreezeOmni [26], use speech encoder (e.g., Whisper [27]) to extract speech features, which are then projected into the LLMs embedding space to facilitate speech understanding. These approaches often incorporate speech decoder to generate speech responses based on LLMs text outputs. Methods based on discrete units, such as SpeechGPT [28], Moshi [10] and GLM-4-Voice [11], employ speech tokenizer [2931] to convert speech into discrete units, allowing the LLM to directly understand and generate speech units, which are finally synthesized into speech using unit-based speech decoder [32, 31]. Compared to continuous representations, discrete units can be jointly modeled with text in LLMs context, but they often rely on more speech data for speech pre-training [33, 11]. Existing omni-modal LMMs, such as VITA-1.5 [12], MiniCPM2.6-o [7], Baichuan-Omni [13], Qwen2.5-Omni [14], use various encoders to extract the modality representations, which are then concatenated and fed into the LLM to facilitate multimodal understanding, and finally speech decoder is employed to synthesize speech from the generated text. Such methods typically model modality alignments in data-driven manner. In contrast, Stream-Omni models the relationships between modalities more purposefully, thereby achieving efficient and flexible modality alignments."
        },
        {
            "title": "3 Stream-Omni",
            "content": "We introduce Stream-Omni, languagevisionspeech LMM based on text-centric modality alignments. Stream-Omni aligns vision and speech to the text modality via sequence-dimension concatenation and layer-dimension mapping, respectively, thereby achieving efficient and flexible modality alignments. The architecture, training, and inference of Stream-Omni are introduced as follows."
        },
        {
            "title": "3.1 Architecture",
            "content": "The architecture of Stream-Omni is illustrated in Figure 2. Stream-Omni adopts the LLM as its backbone and progressively aligns the vision and speech to the text, efficiently developing LMM that supports text, vision, and speech. For vision-text alignment, Stream-Omni applies vision encoder and projection to extract visual representations, which are then concatenated with the text tokens. For speech-text alignment, Stream-Omni introduces several speech layers at the bottom and top of LLM backbone to respectively map speech to the text and generate speech based on the text."
        },
        {
            "title": "3.1.1 Vision Modality",
            "content": "Given the semantic complementarity between the vision and text modalities, Stream-Omni adopts sequence-dimension concatenation for vision-text alignment, which is commonly employed in 3 vision-oriented LMMs [3, 5, 34]. Specifically, Stream-Omni introduces the vision encoder and projection to convert visual inputs into visual representations, which are then concatenated with text representations and jointly fed into the LLM to facilitate visual understanding."
        },
        {
            "title": "3.1.2 Speech Modality",
            "content": "Compared to vision, aligning speech and text is more challenging due to the greater variability of speech representations and the relative scarcity of speech data. To address this, Stream-Omni leverages the higher semantic consistency between speech and text, employing speech-text mapping to facilitate alignment through more direct supervision. speech + + top speech speech layers added to the bottom for speech-to-text mapping and top To achieve this, Stream-Omni incorporates an -layer LLM backbone as the inner core, with bottom speech speech layers added to the top for text-to-speech mapping. Overall, Stream-Omni extends an -layer LLM into (N bottom speech)-layer decoder-only architecture, and leverages multi-task learning to separate different layers into different functions of speech-to-text mapping, text-to-text generation, and text-to-speech mapping. During inference, Stream-Omni autoregressively generates speech at the outermost layer, while relying on the LLM backbone at the inner layers for response generation. In this way, Stream-Omni preserves the generative capabilities and knowledge within the LLM core, while effectively broadening its interaction modalities, avoiding the high cost of using large-scale speech data to relearn textual knowledge. The speech interaction process in Stream-Omni includes speech tokenizer, speech-text mapping, text generation, and streaming speech generation. Speech Tokenizer To enable the mapping with text token, Stream-Omni employs the pre-trained CosyVoice speech tokenizer [31] to discretize the raw speech into sequence of discrete speech units = (u1, , uU ): = SpeechTokenizer(S), (1) where SpeechTokenizer() denotes speech tokenizer, with the speech units vocabulary U. To joint modeling speech and text, we extend the vocabulary by merging the speech unit vocabulary with the LLMs text vocabulary T, and introduce special blank token blank, yielding the multimodal vocabulary of Stream-Omni omni = U {blank}. Speech-Text Mapping To take advantage of LLMs capabilities, Stream-Omni introduces the bottom and top speech layers to learn the speech-text mapping, thereby transferring the text capabilities within LLM to the speech modality. Specifically, the bottom and top speech layers consist of bottom speech and top speech Transformer layers, which share the same configuration as the LLM backbone. The bottom speech layers bottom speech () maps the speech units to the text: = bottom speech (U ), (2) where denotes the representation of the speech units. Then, to achieve speech-to-text mapping, Stream-Omni introduces Connectionist Temporal Classification (CTC) [15] decoder CTCDec() to decode the text sequence from U: DU = CTCDec(H U), (3) where DU RU omni represents the probability distribution over the multimodal vocabulary for each speech unit, which can be decoded into CTC sequence that includes repeated and blank tokens. During training, this module is optimized using the CTC loss: LCT = log (cid:88) p(Z DU), ZΠ1(X) (4) where Π1(X) denotes the set of all possible CTC sequences that map to the text sequence by removing repeated and blank tokens, and p(Z DU) is the decoding probability of sequence from DU. At inference time, Stream-Omni can decode the CTC sequence from DU to produce streaming speech recognition results as an intermediate output for user. More potentially, the CTC decoder holds promise for real-time speech interaction by detecting when the user has stopped speaking based on the consecutive blank tokens in the CTC sequence [35]. Text Generation Through CTC modeling, the bottom speech layers map the speech units into the text representation, achieving speech-text alignment at the representational level. To further bridge the structural gap between speech and text, Stream-Omni removes blank tokens blank from to produce the refined sequence ˆH U. To preserve the models understanding of the speech inputs, this blank token removal is only performed during the generation phase (i.e., generated speech). The processed speech representation ˆH is then concatenated with the visual representation (if has visual inputs) and fed into the LLM backbone Fllm() to generate the text representation T: = Fllm([H : ˆH U]), (5) where [ : ] is sequence concatenation. Owing to the semantic alignment via CTC modeling, StreamOmni can transfer text intelligence to the speech modality while preserving the text capabilities. Streaming Speech Generation While autoregressively generating the text outputs, Stream-Omni uses top speech layers to generate the corresponding speech units in streaming manner. To ensure consistency between the generated speech and text, we introduce an alignment-based fusion to use text information to guide speech unit generation. As illustrated in Figure 3, the top speech layers take the speech representations from bottom speech layers and text representations from the LLM backbone as the inputs, where each layer comprises self-attention, alignmentbased fusion, and FFN. The alignment-based fusion module fuses the text representations into the speech representations U, thereby achieving text-to-speech mapping. However, to enable streaming generation, the key challenge lies in accurately identifying which text corresponds to each speech unit, thereby generating the speech units once the related text token is generated. Figure 3: Diagram of top speech layers. Fortunately, the CTC decoder introduced in Stream-Omni can naturally capture the positional alignment between speech and text [35], which can be used to guide the alignment-based fusion. Formally, based on the CTC sequence DU, Stream-Omni computes the number of aligned text tokens (excluding duplicate and blank tokens) corresponding to the speech sequence up to unit ui, denoted as Ni. That is, within the first speech units Ui, Stream-Omni identifies the first Ni text tokens XNi . Accordingly, when autoregressively generating the next speech unit ui+1, Stream-Omni should use the next text token xNi+1 to guide the generation of speech unit ui+1. In practice, to involve richer text context, Stream-Omni extends the fusion window from the aligned text token xNi+1 to its preceding 1 tokens, where is the hyperparameter of window size. The alignment-based fusion is implemented via cross-attention, with the speech representations attending to the text representations, so the fused representation hf usion hf usion (6) where Ni+2W :Ni+1 are text representations within the local window (W = 5 in Stream-Omni). To reduce generation latency, similar to the widely used wait-k policy in simultaneous translation [3640], Stream-Omni begins streaming speech generation after lagging text tokens (K = 3 in Stream-Omni). Therefore, the first speech unit will be generated immediately after text tokens have been produced. Using the top speech layers top speech(), Stream-Omni can simultaneously generate both text and the corresponding speech units: of speech unit ui is calculated as: = CrossAttn (cid:0)ui, Ni+2W : Ni+1 (cid:1) , (7) where ˆU denotes the generated speech unit sequence. Finally, CosyVoice speech decoder [31] is used to synthesize the speech waveform from the generated speech units. speech ˆU = top (cid:0)H U, T(cid:1) ,"
        },
        {
            "title": "3.2 Training",
            "content": "Stream-Omni achieves efficient alignment across text, visual, and speech modalities, thus requiring only small amount of tri-modal training data. Given the scarcity of existing datasets that jointly incorporate all three modalities, we first construct tri-modal corpus consisting of text, images, and speech through an automated pipeline. Then, Stream-Omni adopts three-stage training strategy to progressively align the text, visual, and speech modalities."
        },
        {
            "title": "Stages",
            "content": "Stage1: Vision-Text Table 1: Training stages and data of Stream-Omni. Training Tasks"
        },
        {
            "title": "Trainable Modules",
            "content": "Vision+TextText"
        },
        {
            "title": "Projection\nLLM Backbone",
            "content": "Stage2: Speech-Text ASR (CTC Loss in Eq.(6)) SpeechSpeech"
        },
        {
            "title": "Bottom Speech Layers\nTop Speech Layers",
            "content": "Stage3: Text-Vision-Speech Vision+TextText Vision+SpeechText Vision+SpeechSpeech"
        },
        {
            "title": "Datasets",
            "content": "LLaVA LLaVA-OV LLaVA-zh LibriSpeech (960h) WenetSpeech (1240h) UltraChattts (6500h) Wikitts (4000h) LLaVAtts (8700h) LLaVA-zhtts (1200h) LLaVAtts (8700h) LLaVA-zhtts (1200h) The training of Stream-Omni involves text-vision, text-speech, and text-vision-speech multimodal datasets to support interactions across various modality combinations. For text-vision data, we adopt the LLaVA [3] and the LLaVA-OV dataset [6], while filtering out samples involving maths, code, and other content unsuitable for speech interaction. For text-speech data, we use automatic speech recognition (ASR) corpora from LibriSpeech [41] and WenetSpeech [42] to train bottom speech layers. Given the scarcity of public speech interaction data, we construct speech interaction dataset by converting existing text-only and vision-language instruction datasets into speech interactions datasets using open-source text-to-speech synthesis (TTS) [31], named InstructOmni2. The construction details are introduced in Appendix A. Table 1 summarizes the used training data (only 23K hours of speech), where those marked with superscript tts indicate synthesized speech interaction dataset. 3.2.2 3-Stage Training Stream-Omni is initialized using LLM and adopts three-stage training strategy, which aligns vision and speech with the text in succession, and then models alignments across three modalities. In this stage, Stream-Omni uses the standard training method Stage 1: Vision-Text Alignment used in vision-oriented LMMs such as LLaVA [3]. Stage 2: Speech-Text Alignment In this stage, the speech-text alignment is achieved by training the bottom and top speech layers using combination of CTC loss after the bottom speech layers (refer to Eq.( 4)) and cross-entropy loss after the top speech layers. Note that, the text representations fed into the top speech layers during training (i.e., in Eq.(6)) are drawn from ground-truth transcriptions rather than LLM generated text, which aim to avoid text-speech dismatching caused by generating incorrect text, thereby enhancing the consistency of text-to-speech generation. Stage 3: Text-Vision-Speech Alignment Finally, we train the LLM backbone of Stream-Omni using constructed tri-modal data through multi-task learning. Specifically, we formulate multiple tasks by combining different modalities, including Vision+TextText, Vision+SpeechText, and Vision+SpeechSpeech, which are all optimized using the cross-entropy loss. In this way, StreamOmni is able to flexibly support interactions under various modality combinations. 3."
        },
        {
            "title": "Inference",
            "content": "Algorithm 1 gives the inference process of Stream-Omni when performing vision-grounded speech interaction. Given vision input and speech input S, Stream-Omni generates the text token in an autoregressive manner, and simultaneously synthesizes the corresponding speech of y. During speech synthesis, Stream-Omni autoregressively generates speech units based on y, until the entire speech corresponding to is generated. To determine whether the generated speech units for are complete, Stream-Omni leverages alignment in the CTC decoder (in Eq.(3)). If the CTC decoder identifies new text token from the generated (i.e., the semantics of the generated speech are complete), the model proceeds to generate the next text token. Otherwise, the model continues to generate speech units for the current y. Stream-Omni repeats the above process until eos is generated. 2https://huggingface.co/datasets/ICTNLP/InstructOmni 6 speech (U ); simultaneously produce ASR results of speech inputs Algorithm 1 Inference of Stream-Omni Input: Speech input S, Vision input , Fusion window size , Lagging text tokens Output: Generated speech output (cid:98)S Init: ASR results (CTC sequence) (cid:98)A = [ ]; Generated text tokens (cid:98)Y = [ ]; Generated speech units (cid:98)U = [ ] 1: Extract visual representation from using the vision encoder and projection; 2: Extract speech units from using the speech tokenizer; 3: bottom 4: while (cid:98)Y [1] = eos do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: return (cid:98)S Fllm([H : ˆH : (cid:98)Y ]) (cid:98)Y .append(y); if (cid:98)Y < then continue; // Generate speech units corresponding to until the text token is recognized in the generated speech while (cid:98)A[1] == blank or (cid:98)A[1] == (cid:98)A[2] do Generate speech unit based on and (cid:98)Y [W :] based on Eq.(6); (cid:98)U .append(u); argmax (cid:0)CTCDec(F bottom (cid:98)A.append(a); Synthesize speech from (cid:98)U using the speech decoder; (cid:98)S.append(s); simultaneously produce text outputs lagging text tokens speech (U ))(cid:1); recognize text from generated speech generate speech for text Besides vision-grounded speech interaction, Stream-Omni also supports interaction of various modality combinations. As shown in Figure 2(right), by flexibly integrating the vision encoder, bottom speech layers, LLM, and top speech layers, Stream-Omni can support various multimodal scenarios."
        },
        {
            "title": "4.1 Benchmarks",
            "content": "We evaluate the multimodal capabilities of Stream-Omni across vision and speech benchmarks. For vision evaluation, we conduct experiments on 11 benchmarks used by LLaVA, including VQA-v2 (VQAv2) [43], GQA [44], VizWiz [45], ScienceQA-IMG (SciQA) [46], TextVQA (VQAT) [47], POPE [48], MME [49], MMBench (MMB) [50], SEED-Bench (SEED) [51], LLaVA-Bench-in-theWild (LLaVAW) [52], and MM-Vet [53]. All evaluations follow LLaVA [3] to ensure comparability. For speech evaluation, we assess the models knowledge-grounded speech interaction on spoken question answering benchmarks, Llama Questions (Llama Q.) [54] and Web Questions (Web Q.) [55], where the metric is the accuracy that whether the models response matches the ground-truth answer. To further assess Stream-Omnis vision-grounded speech interaction capabilities, we construct real-world visual-speech interaction benchmark based on the real-world VQA benchmark VisIT [56], named SpokenVisIT 3. Following Fang et al. [9], the evaluation for SpokenVisIT employs the GPT model (gpt-4o version) to assign score ranging from 1 to 5 for response. Appendix gives the details of SpokenVisIT benchmark. Following previous works [9, 11], all speech evaluations are further divided into speech-to-text (ST) and speech-to-speech (SS) settings. For generated speech responses, we use Whisper-large-v3 [27] to transcribe the speech into text for evaluation."
        },
        {
            "title": "4.2 Baselines",
            "content": "We compare Stream-Omni with vision-oriented, speech-oriented, and omni-modal LMMs of similar model scale and training data size. Vision-oriented LMM baselines include models comparable in scale to LLaVA-v1.5 [3], such as BLIP-2 [57], InstructBLIP [58], IDEFICS [59], Qwen-VL [17], Qwen-VL-Chat [17], SPHINX [19], and mPLUG-Owl2 [20]. Speech-oriented LMM baselines include TWIST [60], SpeechGPT [28], Spectron [54], Moshi [10], Freeze-Omni [26], LLaMA-Omni [9], and GLM-4-Voice [11]. Most existing omni-modal LMMs are trained on large-scale proprietary datasets. For fair comparison, we compare Stream-Omni with VITA-1.5 [12], text-vision-speech LMM trained on comparable amount of data, primarily based on LLaVA [3] and LLaVA-OV [6]. 3https://huggingface.co/datasets/ICTNLP/SpokenVisIT 7 Methods LLM Table 2: Results on visual understanding benchmarks. Sci QA VQAT POPE MME MMB SEED LLaVAW MMVet VQAv2 GQA Vis Wiz BLIP-2 InstructBLIP IDEFICS-9B Qwen-VL Qwen-VL-Chat SPHINX SPHINX-2k mPLUG-Owl2 LLaVA-1.5 LLaVA-NeXT LLaVA-OV VITA-1.5 Vicuna-13B Vicuna-7B LLaMA-7B Qwen-7B Qwen-7B LLaMA-13B LLaMA-13B LLaMA-7B Vicuna-7B Vicuna-7B Qwen2-7B Qwen2-7B Stream-Omni LLaMA-3.1-8B 65.0 50.9 78.8 78.2 78.1 80.7 79.4 78.5 81.8 78.8 79.7 41.0 49.2 38.4 59.3 57.5 62.6 63.1 56.1 62.0 64.2 60.6 68.3 19.6 34.5 35.5 35.2 38.9 39.9 44.9 54.5 50.0 57.6 54. 45.5 61.0 60.5 67.1 68.2 69.3 70.6 68.7 66.8 70.1 96.0 90.9 93.4 42.5 50.1 25.9 63.8 61.5 51.6 61.2 54.3 58.2 64.9 65.0 62.7 85.3 80.7 87.2 85.9 86.5 85. 86.0 1293.8 1487.5 1476.1 1470.6 1450.2 1510.7 1519.0 1580.0 1687.7 36.0 48.2 38.2 60.6 66.9 65.9 64.5 64.3 67.4 80.8 76.7 1752.7 82.4 46.4 53.4 56.3 58.2 56.2 57.9 57.8 58.6 70.2 75.4 70. 76.3 38.1 60.9 73.5 76.9 - 63.4 81.6 71.0 71.2 22.4 26.2 36.0 40.2 36.2 30.5 43.9 49.6 44.7 Avg. (%) 56.0 59.0 56.3 62.6 64.0 64."
        },
        {
            "title": "4.3 Configuration",
            "content": "Stream-Omni is built upon the LLaMA-3.1-8B-Instruct4 [61], which consists of 32 Transformer layers. For vision, Stream-Omni employs the SigLIP-so400m-patch14-3845 [62] as the vision encoder. For speech, Stream-Omni incorporates the bottom speech layers with 3 Transformer layers and top speech layers with 5 Transformer layers, where all Transformer layers share the same architecture and parameter configuration as those in LLM. The speech tokenizer and flow-matching-based speech decoder are adopted from CosyVoice-300M-25Hz [31]. The vocabulary of Stream-Omni comprises 128K text tokens from LLaMA-3.1-8B-Instruct, 4096 speech units from the CosyVoice tokenizer, and blank token blank. Stream-Omni is trained using 8 H800 GPUs and tested on 1 A100 GPU."
        },
        {
            "title": "5.1 Visual Understanding",
            "content": "We evaluate the visual understanding capabilities of Stream-Omni in Table 2. Compared to advanced vision-oriented LMMs and VITA-1.5 [12], Stream-Omni demonstrates strong visual capabilities on various visual tasks. More importantly, despite being unified model that simultaneously supports vision, speech, and text, Stream-Omni achieves performance comparable to vision-oriented LMMs, indicating its effectiveness in mitigating modality interference."
        },
        {
            "title": "5.2 Speech Interaction",
            "content": "Avg. Web Q."
        },
        {
            "title": "Methods",
            "content": "Llama Q. ST SS ST SS Table 3: Results on spokenQA benchmarks. To verify whether Stream-Omni can acquire speech capabilities and knowledge with small amount of speech data, we conduct experiments on knowledge-based LLaMA Question and Web Question, covering both speech-to-text (ST) and speech-to-speech (SS) tasks. As shown in Table 3, Stream-Omni demonstrates strong knowledge-based speech interaction performance. Speech-oriented LMMs based on discrete speech units, such as SpeechGPT, Moshi, and GLM-4Voice, typically rely on speech pretraining to acquire knowledge from large-scale speech data [28, 10, 11], Stream-Omni achieves superior knowledge-based speech interaction with significantly less speech data of 23K hours, particularly in the speech-to-text setting. This advantage primarily stems from the CTC-based speech-to-text mapping in Stream-Omni, which effectively transfers the text knowledge within LLM to the speech modality and thereby supports knowledge-based speech interaction in more efficient manner. TWIST SpeechGPT Spectron Moshi GLM-4-Voice Freeze-Omni LLaMA-Omni VITA-1. 4.0 - - 21.0 50.7 - 49.0 - 1.5 - - 9.2 15.9 - 23.7 - 2.8 - - 15.1 33.3 - 36.4 - - 14.1 14.0 44.5 48.5 58.4 50.6 59.7 - 6.5 6.1 26.6 32.2 44.7 33.4 42.7 - 21.6 21.9 62.3 64.7 72.0 67.7 76. Stream-Omni ST SS 76.3 44.2 27.5 65. 60.3 46.3 4https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct 5https://huggingface.co/google/siglip-so400m-patch14-"
        },
        {
            "title": "5.3 Vision-grounded Speech Interaction",
            "content": "We evaluate the vision-grounded speech interaction of Stream-Omni on the SpokenVisIT benchmark in Table 4. As the omni-modal LMMs with similar training data, StreamOmni demonstrates superior real-world visual understanding capabilities compared to VITA-1.5. In addition, StreamOmni supports speech generation, extending its potential for multimodal interaction. Appendix gives specific case studies, demonstrating the advantages of Stream-Omnis speech-text mapping in cross-modal consistency. Table 4: Results on SpokenVisIT (V: vision, T: text, S: speech). Methods SpokenVisIT V+TT V+ST V+SS GPT-4V VITA-1.5 Stream-Omni 4.81 3.63 3.93 - 3.45 3.68 - - 2."
        },
        {
            "title": "5.4 Quality of Speech-Text Mapping",
            "content": "Methods test-clean Stream -ing Table 5: Results on LibriSpeech benchmarks. Stream-Omni introduces the auxiliary ASR task to train the bottom speech layers and CTC decoder, thereby learning effective speech-to-text mapping. To evaluate the quality of mapping, we evaluate the ASR performance of Stream-Omni on the LibriSpeech benchmark [41]. As shown in Table 5, Stream-Omni achieves advantages in both accuracy and inference time. SpeechGPT [28], Freeze-Omni [26], and GLM-4-Voice [11] need to forward full LMM to autoregressively generating the ASR results. In contrast, StreamOmni generates the ASR results using its bottom speech layers in non-autoregressive manner, resulting in lower inference time for ASR task. More importantly, this layer-dimension allows Stream-Omni to simultaneously present intermediate ASR results during speech interaction, providing users with more comprehensive interaction experience. Whisper SpeechGPT Moshi Mini-Omni Freeze-Omni GLM-4-Voice VITA-1. 2.5 18.9 5.7 4.7 3.2 2.8 3.4 Stream-Omni 3.0 4.5 29.1 - 9.4 7.7 7.7 7.5 616 755 - 148 965 701 - 692 794 - 196 984 756 - Inference Time (ms) Inference Time (ms) test-other WER WER 125 7."
        },
        {
            "title": "5.5 Effect of Alignment-based Fusion",
            "content": "Stream-Omni generates speech from text in streaming manner using alignment-based fusion. To evaluate its effectiveness, we conduct the ablation study of alignment-based fusion on Llama Questions and Web Questions benchmarks (SS) in Table 6, focusing on the fusion type and the fusion window. Table 6: Analysis on alignment-based fusion."
        },
        {
            "title": "Attention",
            "content": "Add (input) Add (per layer)"
        },
        {
            "title": "Fusion\nWindow",
            "content": "Llama Q. SS Web Q. SS 5 1 1 65.0 40.3 45. 27.5 19.2 21.5 Fusion Type For the fusion type, we compare the current cross-attention (named Attention) with adding aligned text representations to the input (named Add (input)) or each layer (named Add (per layer)) of the top speech layers. Results show that the attention-based approach outperforms the others, mainly due to its ability to attend to broader context rather than merely adding single text token."
        },
        {
            "title": "Attention\nAttention\nAttention",
            "content": "22.1 25.7 24.3 54.3 62.3 60.0 2 10 Fusion Window For the fusion window, we find that attending to either very few or all text tokens during speech generation is less effective than focusing on moderate window of tokens, which is attributed to the inherent monotonicity and locality in text-to-speech generation. This is also in line with the widely used speech-text interleaved generation methods [33, 11, 63]. The difference lies in that previous methods achieve consistency between generated speech and the current text through interleaving along the sequence dimension, while alignment-based fusion ensures consistency by guiding the speech to attend to the current text along the layer dimension."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose Stream-Omni, LMM that simultaneously supports various multimodal interactions. Stream-Omni achieves efficient modality alignments via the sequence-dimension concatenation for vision and layer-dimension mapping for speech. Furthermore, Stream-Omni can enhance the multimodal experience by simultaneously providing intermediate text results during speech interaction."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. [2] OpenAI. Gpt-4v(ision) system card, 2024. URL https://cdn.openai.com/papers/ GPTV_System_Card.pdf. [3] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3489234916. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf. [4] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=1tZbq88f27. [5] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. [6] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. URL https://arxiv.org/abs/2408.03326. [7] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [8] Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming, 2024. URL https://arxiv.org/abs/2408.16725. [9] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. LLaMAomni: Seamless speech interaction with large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=PYmrUQmMEw. [10] Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue, 2024. URL https://arxiv.org/abs/2410.00037. [11] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot, 2024. URL https://arxiv.org/abs/2412.02612. [12] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, and Ran He. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction, 2025. URL https://arxiv.org/abs/2501.01957. [13] Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Da Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Tao Zhang, Keer Lu, Yaqi Zhao, Yanjun Shen, Fan Yang, Kaicheng Yu, Tao Lin, Jianhua Xu, Zenan Zhou, and Weipeng Chen. Baichuan-omni technical report, 2024. URL https://arxiv.org/abs/2410.08565. [14] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report, 2025. URL https://arxiv.org/abs/2503.20215. 10 [15] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, ICML 06, page 369376, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933832. doi: 10.1145/1143844.1143891. URL https://doi.org/10.1145/1143844.1143891. [16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/ radford21a.html. [17] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308. 12966. [18] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2418524198, June 2024. [19] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, 2023. URL https://arxiv.org/abs/2311.07575. [20] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1304013051, June 2024. [21] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning, 2022. URL https://arxiv.org/abs/2212.03191. [22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding, 2024. URL https: //arxiv.org/abs/2305.06355. [23] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Video-ChatGPT: Towards detailed video understanding via large vision and language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1258512602, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/ v1/2024.acl-long.679. URL https://aclanthology.org/2024.acl-long.679/. [24] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models, 2023. URL https://arxiv.org/abs/2311.17043. [25] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-LLaVA: Learning united visual representation by alignment before projection. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.342. URL https://aclanthology.org/2024.emnlp-main.342/. [26] Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, and Long Ma. Freeze-omni: smart and low latency speech-to-speech dialogue model with frozen llm, 2024. URL https://arxiv.org/abs/2411.00774. [27] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. [28] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1575715773, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.1055. URL https://aclanthology.org/2023.findings-emnlp.1055/. [29] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:34513460, 2021. doi: 10.1109/TASLP.2021.3122291. [30] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= AF9Q8Vip84. [31] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhifu Gao, and Zhijie Yan. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens, 2024. URL https: //arxiv.org/abs/2407.05407. [32] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1702217033. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ c5d736809766d46260d816d8dbc9eb44-Paper.pdf. [33] Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Mary Williamson, Gabriel Synnaeve, Juan Pino, Benoît Sagot, and Emmanuel Dupoux. SpiRit-LM: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052, 2025. doi: 10.1162/tacl_a_00728. URL https: //aclanthology.org/2025.tacl-1.2/. [34] Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. LLaVA-mini: Efficient image and video large multimodal models with one vision token. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=UQJ7CDW8nb. [35] Shaolei Zhang, Qingkai Fang, Shoutao Guo, Zhengrui Ma, Min Zhang, and Yang Feng. StreamSpeech: Simultaneous speech-to-speech translation with multi-task learning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 89648986, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. acl-long.485. URL https://aclanthology.org/2024.acl-long.485/. [36] Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and Haifeng Wang. STACL: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 30253036, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1289. URL https://www.aclweb.org/anthology/P19-1289. 12 [37] Shaolei Zhang, Yang Feng, and Liangyou Li. Future-guided incremental transformer for simultaneous translation. Proceedings of the AAAI Conference on Artificial Intelligence, 35 (16):1442814436, May 2021. URL https://ojs.aaai.org/index.php/AAAI/ article/view/17696. [38] Shaolei Zhang and Yang Feng. Universal simultaneous machine translation with mixture-ofexperts wait-k policy. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 73067317, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.581. URL https://aclanthology.org/2021.emnlp-main.581. [39] Shaolei Zhang and Yang Feng. Information-transport-based policy for simultaneous translation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9921013, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.65. URL https://aclanthology.org/2022. emnlp-main.65/. [40] Shaolei Zhang and Yang Feng. End-to-end simultaneous speech translation with differentiable segmentation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 76597680, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.485. URL https://aclanthology.org/2023.findings-acl.485/. [41] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210, 2015. doi: 10.1109/ ICASSP.2015.7178964. [42] Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, Di Wu, and Zhendong Peng. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition, 2022. URL https://arxiv.org/ abs/2110.03370. [43] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. [44] Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. [45] Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [46] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reaIn S. Koyejo, S. Mohamed, soning via thought chains for science question answering. A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 25072521. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf. [47] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. [48] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id= xozJw0kZXF. 13 [49] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https://arxiv. org/abs/2306.13394. [50] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. URL https://arxiv.org/abs/2307.06281. [51] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13299 13308, June 2024. [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3489234916. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf. [53] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023. URL https://arxiv.org/abs/2308.02490. [54] Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken question answering and speech continuation using spectrogram-powered LLM. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=izrOLJov5y. [55] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 15331544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/ D13-1160/. [56] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schmidt. Visit-bench: dynamic benchmark for evaluating instruction-following vision-and-language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 2689826922. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 5503389dbe070cdae9b48086c4996a59-Paper-Datasets_and_Benchmarks. pdf. [57] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/li23q.html. [58] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2629626306, June 2024. [59] Laurençon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, et al. Introducing idefics: An open reproduction of state-of-the-art visual language model, 2023. URL https://huggingface. co/blog/idefics. Accessed, pages 0918, 2023. 14 [60] Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, and Yossi Adi. Textually pretrained speech language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= UlHueVjAKr. [61] Meta. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [62] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1197511986, October 2023. [63] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, and Jie Tang. Scaling speech-text pre-training with synthetic interleaved data, 2024. URL https: //arxiv.org/abs/2411.17607. [64] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 30293051, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.183. URL https://aclanthology.org/2023.emnlp-main.183/. [65] Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, and Yang Feng. Llama-omni2: Llmbased real-time spoken chatbot with autoregressive streaming speech synthesis, 2025. URL https://arxiv.org/abs/2505.02625. [66] Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li. Aishell-3: multi-speaker mandarin tts corpus. In Interspeech 2021, pages 27562760, 2021. doi: 10.21437/Interspeech.2021-755."
        },
        {
            "title": "A Construction of InstructOmni",
            "content": "Existing publicly available text and vision instruction data are readily accessible, while speech instruction data and tri-modal instruction data involving text, vision, and speech remain relatively scarce. To address this, we propose InstructOmni, an omni-modal dataset automatically constructed using text-to-speech (TTS) synthesis. InstructOmni builds upon existing publicly available textonly and vision-language instruction datasets by generating corresponding speech based on textual instructions and responses, thereby producing both speech-based instruction data and tri-modal instruction data for training. Specifically, we synthesize speech instruction data from the LLaVA visual instruction tuning dataset [3], the UltraChat text instruction tuning dataset [64] (used in LLaMA-Omni [9] and LLaMA-Omni2 [65]), and subset of Wikipedia entries. The text instructions and responses from these sources are converted into speech using the CosyVoice TTS model [31]. To better simulate the variability of speech input in real-world scenarios, we randomly sample speaker embeddings from LibriSpeech [41] and AISHELL [66], and apply voice cloning techniques to generate speech with diverse speaker characteristics, thereby enhancing the realism and diversity of the speech. Table 1 summarizes all datasets used during training, where those marked with superscript tts indicate samples with synthesized speech. Overall, Stream-Omni is trained on only 23K hours of speech data, which is significantly less than the large-scale datasets used in previous methods, such as TWIST (150K hours) [60], SpeechGPT (60K hours) [28], Moshi (7M hours) [10], GLM-4Voice (700K hours) [11], and VITA-1.5 (110K hours) [12], highlighting its clear advantage in data efficiency."
        },
        {
            "title": "B Construction of SpokenVisIT",
            "content": "In Sec.5.3, we construct SpokenVisIT benchmark to evaluate the vision-grounded speech interaction capability. Here, we give detailed introduction to SpokenVisIT. To better reflect real-world scenarios of vision-based speech interaction, we adopt the VisIT benchmark [56] as the source dataset. VisIT is real-world visual question answering benchmark comprising 574 images and 70 types of instructions covering object recognition, visual reasoning, creative writing, and more. Unlike existing vision evaluation benchmarks that mainly use multiple-choice format, all text instructions in the VisIT benchmark are written in colloquial style, making it particularly well-suited for speech interaction. To adapt VisIT for speech interaction, we employ text-to-speech synthesis [31] to convert each text instruction into corresponding speech utterance, resulting in derived benchmark named SpokenVisIT. During construction, eight math-related instructions that were unsuitable for speech interaction were removed. For the evaluation metric, following the open-ended spoken interaction evaluation protocol proposed by Fang et al. [9], we use ChatGPT (gpt-4o version) to assess the quality of model-generated responses on 1-5 scale. The evaluation prompt includes the image caption as reference, along with the question and the models answer."
        },
        {
            "title": "Prompt of SpokenVisIT Evaluation",
            "content": "I need your assistance in evaluating the performance of several models in vision-based speech interaction scenario. These models process the users spoken input and generate spoken responses. For evaluation purposes, both the users speech input and the models speech output have been transcribed into text using Automatic Speech Recognition (ASR). Additionally, brief image caption is provided to help you understand the visual context of the conversation. Your task is to assess the models responses based on the given visual context [Image Caption], the transcribed user input [Instruction], and the transcribed model output [Response]. Please evaluate the responses considering factors such as helpfulness, responsiveness, empathy, and suitability for real-world multimodal interaction, and assign single score on scale from 1 to 5. Below are the image caption and the transcriptions of the users instruction and the models response: ### [Image Caption]: {caption} ### [Instruction]: {question} ### [Response]: {answer} After evaluating, please output the scores in JSON format: {score: ...}. You dont need to provide any explanations. 16 Figure 4: Case Study of Stream-Omni (detail understanding)."
        },
        {
            "title": "C Case Study",
            "content": "To provide more intuitive demonstration of Stream-Omnis multimodal interaction capabilities, we conduct two case studies in Figure 4 and 5, where both the visual and speech inputs are sourced from the constructed SpokenVisIT benchmark. The case in Figure 4 focuses on visual detail understanding, while the case in Figure 5 highlights the models ability to generate long speech responses. The red-marked text indicates the incorrect part of the response. In both cases, Stream-Omni demonstrates good performance across different modalities. Specifically, in vision-based text interaction, StreamOmni accurately interprets visual inputs and generates output sequences that closely resemble those produced by GPT-4V [2]. When conditioned on both visual and speech inputs, Stream-Omni outperforms VITA-1.5. In the example shown in Figure 4, when the instruction is delivered via text and speech respectively, VITA-1.5 produces two contradictory responses of \"does not allow traveling to the second floor\" and \"leads directly to the second floor\". This contradictory response when facing different modal instructions stems from VITA-1.5s sequence-dimension concatenation of visual, speech, and text representations to achieve multimodal alignment [12], without modeling rigorous semantic alignment In contrast, Stream-Omni employs the speech-to-text between the speech and text modalities. mapping that enables precise semantic alignment between speech and text representations. As result, Stream-Omni achieves more consistent performance across modalities and can generate similar responses regardless of whether the instruction is delivered via text or speech. 17 Figure 5: Case Study of Stream-Omni (long response). In the example shown in Figure 5, Stream-Omni exhibits strong speech generation capabilities, producing high-quality speech outputs lasting up to 30 seconds. Notably, the generated speech is highly consistent with the corresponding text outputs, underscoring the effectiveness of the proposed alignment-based fusion module. Overall, Stream-Omni enables high-quality, vision-grounded speech interactions, fulfilling the diverse requirements of multimodal interaction."
        },
        {
            "title": "D Limitations",
            "content": "In this paper, we present Stream-Omni, large multimodal model that supports text, vision, and speech. To address the scarcity of public tri-modal data, we focus on how to model the modality alignment more purposely to achieve efficient and flexible modality alignments. However, beyond the modeling way of modality alignments, high-quality multimodal interaction also rely on other factors, such as speech expressiveness and the degree of human-likeness. These aspects are important but are not the primary focus of Stream-Omni, so we leave them for future work."
        }
    ],
    "affiliations": [
        "Key Laboratory of AI Safety, Chinese Academy of Sciences",
        "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
        "University of Chinese Academy of Sciences, Beijing, China"
    ]
}