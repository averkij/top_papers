{
    "paper_title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis",
    "authors": [
        "Jaehyun Park",
        "Minyoung Ahn",
        "Minkyu Kim",
        "Jonghyun Lee",
        "Jae-Gil Lee",
        "Dongmin Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 1 5 9 0 2 . 2 0 6 2 : r See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis Jaehyun Park1, Minyoung Ahn2,3, Minkyu Kim3 Jonghyun Lee3 Jae-Gil Lee1 Dongmin Park3, 1KAIST 2Seoul National University 3KRAFTON {jhpark813,jaegil}@kaist.ac.kr,{minkyu.kim,jonghyunlee,dongmin.park}@krafton.com,michellahn02@snu.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation highly crucial area of study. Previous artifactaware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifactIt comprises three agents: perception injected images. agent that recognizes and grounds entities and subentities from real images, synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within diffusion transformer, and curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link. 1. Introduction Recent advances in diffusion models have led to remarkable success in generating highly photorealistic images [45, 53]. While these models have demonstrated striking achievements in text-to-image alignment and aesthetic quality, they still suffer from producing visual artifacts, unintended distortions or anomalies in generated outputs. For example, in Figure 1(a), even state-of-the-art models like NanoBanana [13] produce artifacts, e.g., six-fingered hands and fused entities, reducing user satisfaction, e.g., uncanny valley. Furthermore, mitigating such artifacts is especially critical in high-stakes diffusion applications where reliability is paramount, including image generation in medicine [23], robotics [17], autonomous driving [19], and so on. * Equal contribution; work done during an internship at KRAFTON. Corresponding author. Meanwhile, in image understanding, vision-language models (VLMs) have achieved substantial progress, showing advanced capabilities in visual question answering and scene description [54]. These advances have allowed VLMs to serve as automatic decision-making systems for many vision-centric tasks, including medical analysis [16] and robotics control [24]. However, we find that VLMs face significant challenges when confronted with visual artifacts. As shown in Figure 1(a), even state-of-the-art VLMs, such as GPT-5 [34] and Gemini-2.5-pro [8], exhibit limited ability to detect, localize, or explain artifacts in AI-generated images, nearly indistinguishable from random guessing (see 6 for details). Consequently, VLMs cannot yet serve as reliable automated systems for artifact understanding, which in turn constrains their utility. Recently, several approaches have been proposed to tackle visual artifacts in VLMs and diffusions[20, 48, 55]. However, existing efforts exhibit two major limitations: 1) they mainly target simple artifact types (e.g., Gaussian noise or blur), which were prevalent in early diffusion models such as SD1.0 [43], but are rarely observed in modern models, where more plausible physical distortions are predominant; and 2) they rely heavily on human-annotated artifacts (e.g., as many as 10K labels), which is costly and fundamentally limited in scalability to capture the full diversity of diffusion-generated artifacts. These limitations highlight the need for scalable, annotator-free methods to address plausible and extensive artifacts across diverse visual contexts produced by modern diffusion models. To fill in this gap, we introduce ArtiAgent, novel agentic framework that synthesizes artifacts without human intervention, as shown in Figure 1(b). The key idea builds on our observation that plausible artifacts can be deliberately injected during image inversion-restoration [9] by perturbing the patch attention of diffusion transformer (DiT) (see 4.2.2 for details). ArtiAgent is equipped with the perturbation modules as callable tools, orchestrated by three agents: (1) perception agent, which identifies the most suitable objects or entities in real image to perturb, (2) synthesis agent, which selects and applies the tools to generate plausible artifacts, and (3) curation agent, which filters low1 Figure 1. Overview of our challenges and approach. The red boxes indicate the regions with visual artifacts. (a) Examples of structural visual artifacts in state-of-the-art diffusion models and the inability of VLMs to recognize or explain them. (b) Overview of ArtiAgent, novel agentic framework that synthesizes artifacts for arbitrary visual contexts without human intervention. (c) Example of VLMbased artifact comprehension via detection, explanation, and localization. (d) Application to reward-guided text-to-image generation. (e) Application to image correction, where artifact-aware VLM-guided inpainting removes the flawed regions. quality results and refines their annotations. Through this agentic pipeline, ArtiAgent produces high-quality artifactinjected images with rich annotations, including binary artifact labels, their locations in images, and textual explanations, enabling comprehensive studies on detection, localization, and reasoning for artifact understanding. For thorough validation, we present ArtiBench, new benchmark of 1K AI-generated images produced by modern generative models such as FLUX-dev and Nano-Banana, where each image is carefully annotated by humans with binary artifact labels, their bounding box locations, and explanations. We evaluate ArtiAgent on ArtiBench as well as three existing benchmarks, showing that open-source VLMs (e.g., Qwen2.5-VL) fine-tuned on 100K training samples generated by ArtiAgent consistently outperform proprietary VLMs (e.g., GPT-5) and prior baselines across major artifact-perception tasks, including detection, localization, and reasoning. Furthermore, with these artifactaware VLMs, we benefit two downstream applications: (1) guiding diffusion sampling toward artifact-free generation with VLM-based artifact reward and (2) automatically editing diffusion outputs that contain artifacts, substantially improving the image generation pipelines. Our key contributions are summarized as follows: Framework. We introduce ArtiAgent, an agentic data synthesis framework that produces diverse plausible artifacts at scale, enriched with high-quality annotations. Tools. We develop agentic tools that inject artifacts via our novel inversion-injection method during image reconstruction, which can be used by any DiT models. Datasets. We present large-scale training set with 100K examples synthesized by ArtiAgent, along with ArtiBench1, challenging benchmark of 1K images generated by modern generative models with human labels. Experiments. Extensive experiments demonstrate the superiority of ArtiAgent by scaling up VLM performance on core artifact-perception tasks. Moreover, we show its utility in downstream diffusion-based applications: artifact-free image generation and editing. 2. Related Work Visual Artifact Datasets. Several training datasets have been introduced to supervise the understanding of visual artifacts in generative models. PAL [55] provides 10K images with pixel-level annotations of perceptual defects for segmentation-based training. SynthScars [20] provides 12K images with pixel-level masks and textual explanations. DiffDoctor [48] scales human labeling through semisupervised expansion from 25K seed set. Although these datasets offer essential supervision for artifact detection and correction, their reliance on human annotation makes them costly and difficult to scale. 1https://huggingface.co/datasets/KRAFTON/ArtiBench 2 Moreover, several benchmark datasets have been released to evaluate models ability to understand visual artifacts. RichHF-18K [27] provides annotated artifact regions, while LOKI [51] adds natural-language explanations with bounding box labels. SynthScars also releases evaluation splits to test perception, localization, and explanation capabilities. These benchmarks serve as reference points for measuring models artifact awareness; however, most of them rely on artifacts from earlier diffusion models and tend to focus on degenerate artifacts (e.g., Gaussian noise), limiting their relevance to the richer and more diverse failure modes of modern generative systems. Handling Visual Artifacts. Using these datasets, different modeling strategies have been explored. PAL [55] trains segmentation models to localize artifact regions and enables automated correction via inpainting. RichHF-18K [27] is used to train multimodal models that predict humanlike feedback heatmaps, which can then refine diffusion models through preference learning. LEGION [20] trains GLaMM [39] with the SynthScars dataset that allows detection, localization, and explanation into unified model. DiffDoctor [48] employs its dataset to train an artifact segmentation model, which is used for diffusion fine-tuning to alleviate artifact generation. While these studies emphasize the value of artifact detection and reasoning, they also reveal the necessity of reducing reliance on manual annotation to enable the scalable and reliable dataset construction. 3. Understanding Visual Artifacts 3.1. Problem Scope As generative models have matured, the types of visual artifacts that frequently appear have also changed. Early GANs [15] and U-Netbased diffusions [42] predominantly exhibited naive distortions such as Gaussian-like noise or low-level pixel corruption [55]. Modern diffusion models, equipped with DiT-based architectures and trained on higher-quality datasets, have largely overcome these naive distortions. Yet, they still generate plausible structural visual artifacts, which are the main focus of this work. Definition 3.1. (Informal) Structural visual artifacts refer to defects in which the inherent physical structures of objects are distorted in the generated image. That is, while the contents specified in the prompt are represented, their form violates common-sense plausibility (e.g., generated dog with two noses). This definition excludes text-to-image misalignments where the prompt content itself is misrepresented (e.g., generating cat given the prompt of dog). Table 1. Artifact frequency of modern diffusion models. Generative Model Freq. SD3.5-Large FLUX-schnell Qwen-Image FLUX-dev Nano-Banana 36% 28% 17% 12% 5% Figure 2. Artifact type distribution of diffusion models. Figure 1(a) provides illustrative examples of each case, showing how diffusion models can compromise reliability while still producing visually high-quality pixels. To systematically analyze these artifacts, we randomly sampled 100 captions from the MS-COCO dataset [7] and used them as text-to-image prompts across five state-or-the-art diffusion models, including Stable Diffusion 3.5 [11], FLUX [5], Qwen-Image [49], and Nano-Banana [13]. For each generated image, human annotator manually inspected the outputs and identified whether they contained any artifacts, further categorizing them into the four structural types. Based on this evaluation, Table 1 summarizes the frequency of artifacts for each model, while Figure 2 presents the relative occurrence of different artifact types. These findings underscore the importance of addressing structural artifacts in modern generative modeling, highlighting the need for scalable methods to detect, analyze, and mitigate them. 4. Agentic Pipeline for Artifact Synthesis We propose ArtiAgent, an agentic pipeline designed to synthesize visual artifacts for arbitrary visual contexts in fully automated manner. Figure 3 illustrates the overall workflow of ArtiAgent, which consists of three specialized agents: 1) perception agent that identifies entities and subentities in an image and determines suitable candidates for artifact injection, 2) synthesis agent that injects artifacts into the image, and 3) curation agent that filters out low-quality results and enriches the supervision of injected artifacts with local and global explanations. Using the powerful contextunderstanding capabilities of recent LLMs and our novel artifact injection tools, ArtiAgent determines proper artifact types and locations from given image and synthesizes them with high-quality annotations. Visualizations of the dataset and the details of the agents are in Appendix A. 4.1. Perception Agent (Figure 3a) The perception agent aims to analyze real image and decompose it into meaningful semantic units that can serve as reliable candidates for the synthesis agent. 3.2. Artifact Analysis in Modern Generative Models We have categorized structural artifacts into four representative types: duplication, omission, distortion, and fusion. 4.1.1. Entity-Subentity Vocabulary Generation The agent leverages out-of-the-box VLMs to decompose the input image into hierarchical vocabulary of entities 3 Figure 3. ArtiAgent consists of three coordinated agents: (1) the perception agent detects entities and subentities using Grounded-SAM; (2) the synthesis agent injects artifacts through patch mapping tool and the inversion-injection paradigm; and (3) the curation agent filters low-quality results and generates localized and global textual explanations. (e.g., dog) and subentities (e.g., nose). These components are categorized into two semantic levels: peripheral subentities, such as fingers or legs, and intermediate subentities, such as body or face. The detailed prompt template of this vocabulary generation module is provided in Appendix B.1. 4.1.2. Entity-Subentity Grounding The agent then employs Grounded-SAM [41] to ground entities and subentities visible in the image. Once the segmentation masks for both entities (e.g., dog) and subentities (e.g., leg) are obtained, the agent performs containment analysis to associate each subentity with its parent entity (e.g., leg of dog) by computing the overlap ratio. 4.2. Synthesis Agent (Figure 3b) The synthesis agent uses the perception agents grounding information to inject artifacts into an image. It consists of two components: toolbox that generates targetreference patch mappings, and an inversion-injection module that uses the patch mapping to inject artifacts into the real image. 4.2.1. Target-Reference Patch Mapping Toolbox The toolbox contains four artifact injection tools: add, remove, distort, and fuse, each of which is responsible for Figure 4. Visualization of each target-reference patch mapping and its resulting artifact-injected image. producing the corresponding target-reference patch mapping. The toolbox executes the add and remove tools for peripheral subentities, the distort tool for intermediate subentities, and the fuse tool for two overlapping entities. Figure 4 demonstrates an example of each tool. The detailed algorithm of each tool is provided in Appendix C. Add. Reference patches are given as the original subentity region, while the tool selects the best candidate for 4 target patches by searching through surrounding patches. Nearby patches without overlaps or subentity class conflicts are preferred. Remove. Target patches are given as the original subentity region, while the tool produces reference patch mappings that will replace the target patches with surrounding context. Distort. Target patches are given as the original subentity region, while the tool applies transformation kernels on the target patches to generate distorted mapping relations to reference patches. We employ three types of kernels: the jitter kernel for random displacement, the strip kernel for circular shifts over band-like partitions, and random permutation kernel. Fuse. The fuse tool generates mappings that blend content across multiple entities. The target patches contain the overlapping region between two entity instances, and the chunks of reference patches from one entity are assigned to the target patches of the other entity. 4.2.2. Inversion-Injection Method for Artifact Synthesis We propose an inversion-injection module, which extends the inversion-restoration paradigm from image editing [9, 29, 31, 47]. Specifically, the inversion-injection module employs the target-reference patch mapping to manipulate the positional information in the self-attention layers of DiT, allowing realistic structural artifact synthesis. RN denote the input to transNotations. Let (ℓ) former layer ℓ, where is the number of image patches and is the embedding dimensionality. We denote the set = . During artifact inof all patch indices as } = jection, we accept the target-reference patch mapping provided by the toolbox, where pt is (pt, pr) { target patch to be modified and pr its reference patch. The = , the set of all target patches is set of all reference patches is , M} . and the set of background patches becomes (pt, pr) pr { (pt, pr) = pt { = P} 1, . . . , M} P { Inversion Stage. The inversion stage maps the input image into its corresponding noisy latent representation. Queries, keys, and values for the self-attention layer are Q(ℓ) = (ℓ)W (ℓ) , (ℓ) = (ℓ)W (ℓ) , (ℓ) = (ℓ)W (ℓ) , (1) Rdd are learnable. We encode where (ℓ) positions with rotary embeddings (RoPE) [46] such that , (ℓ) , (ℓ) Q(ℓ) = RoPE(Q(ℓ), p), (ℓ) = RoPE(K (ℓ), p), . (2) Then, the layers attention output is Attn(ℓ)(X (ℓ)) = Softmax (cid:33) (cid:32) Q(ℓ) (ℓ) (ℓ). (3) 5 Figure 5. Inversion-injection module. In this example, the right arm from the reference patches is added to the target patches directly below it. While executing Equation (3), we cache the per-layer value embeddings (ℓ) (ℓ) for use in the injection stage. inv P Injection Stage. As in Figure 5, during the denoising process, the injection stage injects structural artifacts into by borrowing spatial semantics from R, while keeping the B. original semantics of Target Region. For pt mapped to pr, we replace the positional embedding (PE) and the value embedding pt = RoPE(Q(ℓ) of pt with those of pr: Q(ℓ) pt = (ℓ) RoPE(K (ℓ) pr,inv. pt pt , pr), and (ℓ) pt , pr), (ℓ) pb = RoPE(K (ℓ) Background Region. For non-target patch pb pb = RoPE(Q(ℓ) (ℓ) pb,inv. pb B, we keep their original positional information and reuse the (ℓ) inv values to maintain the context of the original image: Q(ℓ) pb , pb), (ℓ) pb , pb), and (ℓ) Then, self-attention is performed as in Equation (3). Overall, PE injection controls where the model believes denoising is occurring, while value injection provides what semantic content fills that position. Their combination allows local injections of realistic artifacts, while the background remains consistent with the original image. Although value injection [9, 47] is well-established approach in image editing research, our method introduces the novel idea of manipulating their positions during injection. Furthermore, PE injection itself has not been used in prior image editing works. Combining PE and value injections offers particularly effective methodology for generating structural artifacts, as it enables direct manipulation of spatial information during the reconstruction process. 4.3. Curation Agent (Figure 3c) The curation agent functions as the pipelines quality assurance and enrichment stage, refining the synthesis agents raw outputs into training-ready datasets for wide range of downstream tasks. With the real image paired with the artifact-injected image provided, the curation agent performs data filtering followed by explanation generation. The pair-wise input enables reliable filtering and explanation by contrasting the artifact-injected regions with their real counterparts. 4.3.1. Data Filtering This stage applies one of two filtering methods, depending on which tool was used for the artifact injection. LPIPS-Based Filtering. Distortion artifacts are validated using the LPIPS [56] metric, which measures the perceptual difference between two images by comparing their feature representations, producing metric that aligns much more closely with human judgments. For each cropped original-injected pair, we compute the LPIPS score and retain the pair if it satisfies τ1 τ2,with τ2 filtering out pairs too similar with unidentifiable changes, while τ1 filters out severely corrupted samples with implausible damage in the region. dLPIPS(xoriginal, xartifact) 1 VLM-Based Filtering. For duplication, omission, and fusion artifacts, we employ VLM to validate whether the injected change is perceptible and localized within the designated region. The VLM receives (i) the original image with the target region masked out (to provide the global scene context), (ii) the original image cropped to the target region (to show the unaltered content), and (iii) the artifact-injected image cropped to the same region (to focus on the modification). From this triplet, the VLM makes binary judgment, confirming whether new instance has appeared (duplication), an expected object has become missing (omission), or two objects have been unnaturally merged (fusion). For the detailed prompt template used for VLM-based filtering, see Appendix B.2. 4.3.2. Explanation Generation Local Explanation. For each candidate region, the VLM receives the same triplet used in filtering. We prompt the VLM to synthesize short local description by guiding the VLM to describe what is different in the artifact region compared to the real counterpart. For the detailed prompt template used, see Appendix B.3. Global Explanation. After generating local explanations for all artifacts, the curation agent generates global explanation for the whole image. The VLM accepts the artifact image and list of artifact-injected bounding boxes and its corresponding local explanations. The VLM prompted to explain why this artifact injected image is indeed an artifact. For the detailed prompt template used, see Appendix B.3. 4.4. Data Collection With our novel three-stage agentic pipeline, we collect 50K pairs of artifact-injected images and the corresponding original images, alongside the metadata consisting of artifact explanation shown in the final output of Figure 3. Here, we reconstruct the source images using the inversionrestoration method to alleviate pairwise differences originating from diffusion-generated image traits. The source If Table 2. Comparison of artifact benchmark datasets. benchmark reused another dataset as source, we describe the generative models used in that source. The table with the full citations is provided in Appendix D. Benchmark Source Models Oldest Newest RichHF SD2.1 Dreamlike Photoreal LOKI pix2pix FLUX SynthScars Midjourney DALLE3 ArtiBench SD3. Nano-Banana Sample Bin. Loc. Exp. 955 229 1K 1K images are composed of four different datasets, COCO [7], Caltech-101 [12], 11K Hands [1], and Celeba HQ [21], broadening the distribution from diverse real world scenes to specific single entity images. We employ GPT-4o [33] as the VLM contributing to our agentic flow. Also, the inversion-injection method in Section 4.2.2 uses FLUX.1dev [5] for the DiT and FireFlow [9] for the inversioninjection module. 5. ArtiBench: Artifact Detection Benchmark such artifact detection benchmarks Existing as RichHF [25], LOKI [51], and SynthScars [20] were primarily built using earlier diffusion models, including Stable Diffusion 1 or 2 and Midjourney. These datasets, though valuable at the time, no longer capture the characteristics of modern artifacts produced by current diffusion the evaluaand multimodal transformers. As result, tion on these benchmarks may not accurately reflect the artifact-handling capabilities of todays models. To address this issue, we introduce ArtiBench, new benchmark that reflects the current state of artifact phenomena in recent generative models. As summarized in Table 2, previous benchmarks were limited either in sample diversity, recency of generative sources, or task coverage. Our benchmark is designed to overcome these limitations by including data from recent models and providing comprehensive annotations for multiple artifact-related tasks. We construct ArtiBench with 1K images generated by five state-of-the-art diffusion models, Stable Diffusion3.5 [11], FLUX-schnell/dev [5], Qwen-Image [49], and Nano-Banana [13], with the prompts sampled from three datasets, MS-COCO [7], PartiPrompts [52], and FuseCap [44]. For annotation, we involve 12 human annotators and label each image with: 1) binary indicator denoting the presence or absence of artifacts, 2) bounding boxes for all artifact regions, and 3) concise descriptions of the observed abnormalities. The dataset is balanced with an equal ratio of artifact-free and artifact-containing samples. Further details on the construction and annotation pipeline are provided in Appendix E. Table 3. Artifact understanding performance across (a) binary detection, (b) localization, and (c) explanation. The benchmarks are listed in the order in which they were published. RichHF LOKI (b) Localization Methods PAL DiffDoctor LEGION (a) Binary detection ArtiBench Acc F1 - - - - - - GPT-4o Gemini-2.5-Pro GPT0.619 0.582 0.599 0.601 0.575 0.577 mIoU 0.079 0.077 0.067 0.086 0.061 0.126 0.028 0.139 0.112 0.040 0.091 0.146 mIoU 0.021 0.175 0.100 0.037 0.109 0.089 + ArtiAgent Qwen2.5-VL-7B 0.501 0.627 InternVL3.5-8B 0.498 0.630 0.336 0.052 0.129 0.627 0.357 0.015 0.620 0.126 LEGION was trained on the SynthScars training dataset split. 0.028 0.198 0.022 0.170 0.075 0.119 0.013 0.100 + ArtiAgent 0.037 0.274 0.158 0.056 0.169 0.141 0.068 0.198 0.025 0.196 SynthScars F1 mIoU 0.035 0.083 0.106 0.032 0.064 0.117 0.013 0.137 0.019 0.140 0.053 0.136 0.152 0.052 0.101 0.185 0.018 0.214 0.033 0.217 ArtiBench LOKI (c) Explanation SynthScars ArtiBench mIoU F1 ROUGE CSS ROUGE CSS ROUGE CSS 0.040 0.081 0. 0.049 0.095 0.061 0.010 0.111 0.010 0.119 0.066 0.137 0.099 0.084 0.147 0.099 0.014 0.168 0.015 0.176 - - 0. 0.107 0.097 0.121 0.106 0.169 0.081 0.137 - - 0.314 0.266 0.358 0.382 0.267 0.454 0.189 0.401 - - 0.247 0.125 0.103 0.120 0.115 0.196 0.050 0.179 - - 0.589 0.404 0.474 0.461 0.362 0.578 0.180 0.513 - - 0. 0.143 0.159 0.145 0.117 0.233 0.126 0.226 - - 0.332 0.433 0.420 0.434 0.263 0.643 0.256 0.625 6. Experiments 6.1. Understanding Artifacts with VLMs We assess the efficacy of ArtiAgent by training VLMs with visual question-answering (VQA) samples generated from ArtiAgent and measuring its performance on artifact-aware benchmarks, including ArtiBench. 6.1.1. Setup We consider three tasks, artifact detection, localization, and explanation. For evaluation metrics, we use accuracy and F1 score for detection, mIoU and F1 score for localization, and ROUGE and CSS for explanation. More details of the evaluation protocol are provided in Appendix F. For baselines, we use three artifact segmentation algorithms, PAL [55], DiffDoctor [48], and LEGION [20]; and three proprietary VLMs, GPT-4o [33], Gemini-2.5-Pro [14], and GPT-5 [34]. PAL and DiffDoctor are evaluated only on the localization task as they output only segmentations, LEGION is evaluated on the localization and explanation tasks, and the three VLMs are evaluated on all three tasks. We fine-tune Qwen2.5-VL-7B [4] and InternVL3.5-8B [57] on 100K training set generated by ArtiAgent. The detailed procedure for VQA generation is provided in Appendix G. 6.1.2. Main Results Overall, fine-tuning open-source VLMs with synthetic data generated by ArtiAgent consistently enhances their ability to detect, explain, and localize visual artifacts. Across all three tasks, ArtiAgent-trained models not only outperform their vanilla counterparts but also match or exceed the performance of proprietary systems such as GPT-5 and Gemini-2.5-Pro. These gains highlight the quality and scalability of ArtiAgent-generated supervision, demonstrating that artifact synthesis through agentic data generation provides rich, transferable signals for both spatial grounding and semantic reasoning. Artifact Binary Detection. Table 3(a) shows that VLMs trained with ArtiAgent achieve clearly superior artifact detection performance. Specifically, ArtiAgent improves the accuracy of InternVL3.5-8B by 26.5%. At the same time, the overall low accuracy on ArtiBench highlights the difficulty and importance of understanding visual artifacts in AI-generated images, especially because modern diffusion models increasingly exhibit subtle, structured failures that current VLMs easily miss. Artifact Localization. Table 3(b) shows that ArtiAgent consistently enhances the spatial grounding capability of open-source VLMs. Although DiffDoctor exhibits high accuracy in LOKI, it fails to generalize to more recent benchmarks, including ArtiBench. This outcome means that ArtiBench further extends the artifact detection field one step deeper, by capturing artifacts that prior artifact-detection models struggle to identify. Artifact Explanation. Table 3(c) shows that training with the ArtiAgents generated dataset greatly strengthens the reasoning and description capabilities of VLMs. The finetuned models exhibit clear improvements in the ROUGE and CSS metrics across all benchmarks. 6.1.3. Data Scaling Effect of ArtiAgent Figure 6 shows how model performance grows as we increase the training data size with ArtiAgent. In all three tasks, we observe clear upward trend, which means that more synthesized data consistently leads to better artifact understanding. Notably, for localization and explanation, the performance with even small subsets as little as 1K samples already surpasses that of GPT-5, showing that ArIn tiAgent provides highly sample-efficient supervision. contrast, the binary detection task continues to improve up to the 100K scale, suggesting that detection benefits from larger and more diverse artifacts. These results highlight the rich supervision and strong scaling potential of ArtiAgent. 6.2. Mitigating Artifacts in Diffusion Models 6.2.1. Reward-Guided Artifact-Free Generation key strength of ArtiAgent is its pairwise data design: for every instance, it provides two tightly matched images with the same contentone clean and one with artifact. This Figure 6. Scaling effect of data generated by ArtiAgent with Qwen2.5-VL-7B. We average the results of all the benchmarks for each task. Figure 8. Image correction. The ArtiAgent-trained VLM can effectively guide image inpainting models to correct artifact regions. Setup. We use Qwen2.5-VL-7B trained with ArtiAgent to determine whether an artifact is present in given image and to localize the artifact region. This region is then processed by the FLUX inpainting pipeline [3], which synthesizes corrected version of the localized area. Next, the corrected image is re-evaluated by the VLM to verify whether the artifact in the region has been fully resolved in the specified region. If the VLM continues to detect an artifact, the inpainting procedure is repeated. This iterative loop continues until the VLM confirms the absence of artifacts. The detailed procedure is provided in Appendix I. Results. Figure 8 presents qualitative results of our image correction pipeline. The VLM accurately identifies the artifact region, and the inpainting model corrects the region with natural and structurally consistent content. These results demonstrate that the proposed pipeline can reliably locate and correct artifacts, depicting the usefulness of our artifact understanding VLM. 7. Conclusion In this work, we introduce ArtiAgent, scalable agentic framework that automatically synthesizes visual artifacts through positional embedding manipulation in diffusion transformers. By integrating the perception, synthesis, and curation agents, our pipeline generates large-scale, richly annotated artifact datasets without human supervision. Experiments showed that VLMs fine-tuned on ArtiAgent data achieved substantial gains in artifact detection, localization, and explanation, and that the resulting models can guide diffusion sampling toward artifact-free generations and perform automated artifact correction. Together, these results demonstrate that agentic data synthesis provides an effective and general pathway for perceiving and mitigating visual artifacts in modern generative models. Figure 7. Reward-guided generation. ArtiAgent can train reward model that guides diffusion to generate artifact-free images. structure gives extremely rich supervision for learning artifact preferences. Using these pairs, we adopt the rewardguided test-time scaling framework [28] that steers diffusion models toward producing artifact-free images. Setup. We use CLIP as the backbone of the BradleyTerry [6] reward model. The model learns to assign higher score to the real image over the artifact image. With this artifact-aware reward model, we apply test-time scaling to FLUX-schnell. We run six search rounds each for 100 prompts sampled from MS-COCO and measure how much the reward increases as the search progresses. The detailed training schema and the test-time scaling procedure is provided in Appendix H. Results. As can be seen in Figure 7, the reward steadily improves throughout the search rounds, indicating that the diffusion model continues to generate images with fewer artifacts. Qualitatively, the examples in Figure 7 show clearer structures and reduced artifact patterns in later rounds, demonstrating that the reward model has successfully learned the real-artifact preference and enables guidance toward artifact-free images. 6.2.2. VLM-Guided Artifact Correction Since our artifact-trained VLM can reliably detect and localize visual artifacts, we leverage this capability to guide an image inpainting model to correct artifact regions of AIgenerated image."
        },
        {
            "title": "References",
            "content": "[1] Mahmoud Afifi. 11k hands: gender recognition and biometric identification using large dataset of hand images. Multimedia Tools and Applications, 78:2083520854, 2019. 6 [2] Vatsal Agarwal, Matthew Gwilliam, Gefen Kohavi, Eshan Verma, Daniel Ulbricht, and Abhinav Shrivastava. Towards multimodal understanding via stable diffusion as taskaware feature extractor. arXiv preprint arXiv:2507.07106, 2025. 1 [3] AlimamaCreative. Flux.1-dev controlnet inpainting (beta). https://huggingface.co/alimama-creative/ FLUX . 1 - dev - Controlnet - Inpainting - Beta, 2024. Model weights released under the FLUX.1 [dev] NonCommercial License. 8, 17 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and Others. Qwen2. 5-vl technical report. Arxiv Preprint Arxiv:2502.13923, 2025. 7, 16 [5] Blackforestlabs. Flux: powerful tool for text generation. https://blackforestlabs.ai/, 2024. 3, 6, 15 [6] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 8, 16 [7] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: data collection and evaluation server. Arxiv Preprint Arxiv:1504.00325, 2015. 3, 6 [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and Others. Gemini 2.5: pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Arxiv Preprint Arxiv:2507.06261, 2025. [9] Yingying Deng, Xiangyu He, Changwang Mei, Peisong Wang, and Fan Tang. Fireflow: fast inversion of rectified flow for image semantic editing. In Proceedings of the International Conference on Machine Learning (ICML), 2025. 1, 5, 6 [10] Dreamlike Art. Dreamlike photoreal 2.0. https:// huggingface.co/dreamlikeart/dreamlikephotoreal-2.0, 2023. 15 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024. 3, 6, 15 [12] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Image Understanding, 106(1):5970, 2007. 6 [13] Alisa Fortin, Guillaume Vernade, Kat Kampf, and AmIntroducing gemini 2.5 flash image. maar Reshi. https : / / developers . googleblog . com / en / introducing-gemini-2-5-flash-image/, 2025. Google AI / DeepMind blog post, August 26 2025. 1, 3, 6, [14] Gemini Team, Google DeepMind. Gemini 2.5: pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Technical Report, 2025. 7 [15] Ian Goodfellow, Jean Pouget-abadie, Mehdi Mirza, Bing Xu, David Warde-farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 3 [16] Iryna Hartsock and Ghulam Rasool. Vision-language models for medical report generation and visual question answering: review. Frontiers in Artificial Intelligence, 7:1430984, 2024. 1 [17] Xinyu Huang. survey of domain adaptation in robotics using diffusion models. Applied and Computational Engineering, 179:18, 2025. 1 [18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11251134, 2017. 15 [19] Max Jiang, Yijing Bai, Andre Cornman, Christopher Davis, Xiukun Huang, Hong Jeon, Sakshum Kulshrestha, John Lambert, Shuangyu Li, Xuanyu Zhou, and Others. Scenediffuser: efficient and controllable driving simulation initialization and rollout. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), pages 55729 55760, 2024. [20] Hengrui Kang, Siwei Wen, Zichen Wen, Junyan Ye, Weijia Li, Peilin Feng, Baichuan Zhou, Bin Wang, Dahua Lin, Linfeng Zhang, and Others. Legion: learning to ground and explain for synthetic image detection. Arxiv Preprint Arxiv:2503.15264, 2025. 1, 2, 3, 6, 7, 15 [21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, In Proceedings of the International Conferand variation. ence on Learning Representations (ICLR), 2018. 6 [22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of the ing the image quality of stylegan. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 81108119, 2020. 15 [23] Firas Khader, Gustav Muller-Franzes, Soroosh Tayebi Arasteh, Tianyu Han, Christoph Haarburger, Maximilian Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina Baeßler, Sebastian Foersch, et al. Denoising diffusion probabilistic models image for 3d medical generation. Scientific Reports, 13(1):7303, 2023. 1 [24] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, and Others. Openvla: an open-source vision-language-action model. Arxiv Preprint Arxiv:2406.09246, 2024. 1 [25] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. 6 9 [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1 [27] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-tuset, Sarah Young, Feng Yang, and Others. Rich human feedIn Proceedings of the back for text-to-image generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1940119411, 2024. 3, 15 [28] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Others. Inference-time scaling for diffusion models beyond scaling denoising steps. Arxiv Preprint Arxiv:2501.09732, 2025. 8, [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equaIn Proceedings of the International Conference on tions. Learning Representations (ICLR), 2022. 5 [30] Midjourney, Inc. midjourney.com/, 2022. 15 Midjourney. https : / / www . [31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 60386047, 2023. [32] Openai. Dalle 3. https://openai.com/research/ dall-e-3, 2023. 15 [33] OpenAI. Gpt-4o system card. Technical Report arXiv:2410.21276, OpenAI, 2024. 6, 7, [34] Openai. Gpt-5: large language model. https : / / openai.com/research/gpt-5, 2025. 1, 7 [35] Ji-Hoon Park, Yeong-Joon Ju, and Seong-Whan Lee. Explaining generative diffusion models via visual analysis for interpretable decision-making process. Expert Systems with Applications, 248:123231, 2024. 1 [36] Taesung Park, Alexei Efros, Richard Zhang, and JunYan Zhu. Contrastive learning for unpaired image-to-image translation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 319345. Springer, 2020. [37] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. Advances in Neural Information Processing Systems, 36: 2412924142, 2023. 1 [38] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion modIn Proceedings of els for high-resolution image synthesis. the International Conference on Learning Representations (ICLR), 2024. 15 [39] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. 10 Glamm: pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1300913018, 2024. 3 [40] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2019. [41] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, and Others. Grounded sam: assembling open-world models for diverse visual tasks. Arxiv Preprint Arxiv:2401.14159, 2024. 4 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 3, 15 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 1 [44] Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz, and Ron Kimmel. Fusecap: leveraging large language models for enriched fused image captions. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 56895700, 2024. 6 [45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo lopes, Burcu Karagol ayan, Tim Salimans, Photorealistic text-to-image diffusion modand Others. In Proceedings of els with deep language understanding. the Conference on Neural Information Processing Systems (NeurIPS), pages 3647936494, 2022. 1 [46] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [47] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming In Proceedings of rectified flow for inversion and editing. the International Conference on Machine Learning (ICML), 2025. [48] Yiyang Wang, Xi Chen, Xiaogang Xu, Sihui Ji, Yu Liu, Yujun Shen, and Hengshuang Zhao. Diffdoctor: diagnosing image diffusion models before treating. Arxiv Preprint Arxiv:2501.12382, 2025. 1, 2, 3, 7 [49] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-Ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, and Others. Qwen-image technical report. Arxiv Preprint Arxiv:2508.02324, 2025. 3, 6, 15 [50] Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, and Weidi Xie. sanity check for AIIn Proceedings of the Intergenerated image detection. national Conference on Learning Representations (ICLR), 2025. 15 [51] Junyan Ye, Baichuan Zhou, Zilong Huang, Junan Zhang, Tianyi Bai, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, and Others. Loki: comprehensive synthetic data detection benchmark using large multimodal models. Arxiv Preprint Arxiv:2410.09732, 2024. 3, 6, 15 [52] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. Featured Certification. 6 [53] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion models in generative ai: survey. Arxiv Preprint Arxiv:2303.07909, 2023. 1 [54] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. IEEE Vision-language models for vision tasks: survey. Transactions on Pattern Analysis and Machine Intelligence, 46(8):56255644, 2024. [55] Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian Zhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, and Jianbo Shi. Perceptual artifacts localization for image synthesis tasks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 75797590, 2023. 1, 2, 3, 7 [56] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In Proceedings of deep features as perceptual metric. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 586595, 2018. 6 [57] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: exploring advanced training and test-time recipes for open-source multimodal models. Arxiv Preprint Arxiv:2504.10479, 2025. 7, 16 11 See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Further Studies of ArtiAgent ( 4) A.3. Ablations A.1. Visualization Figure 9 and 10 illustrates the sampled images generated by the ArtiAgent pipeline and its synthesized annotations. The bounding boxes highlight the target patch area where artifacts are injected by the synthesis agent. A.2. Implementation Details Injection Stage. The details of the injection process in 4.2.2 was designed to introduce natural and coherent structural artifacts to the image. We accept and build on the coarse-to-fine manner of denoising processes, adapting the understanding that earlier denoising time steps contribute to overall structural context, while later steps focus on regions of fine-grained details [2, 35, 37]. PE Injection. Among the 25 denoising steps, we disable PE injection for selected final steps and apply PE injection only in the earlier steps. Duplication, distortion, and fusion artifacts disable five final steps, focusing on introducing coarse context of structural modifications while maintaining natural connection with the original scene. In contrast, the omission artifact is generated by disabling PE injection only in the final timestep, since it requires stronger perturbation on positional context to remove an existing feature from the image and distinguish the newly injected background from its original entity. Value Injection. In contrast, value injection is performed only for the first 15 denoising steps. Reducing the injection steps for value injection ensures artifact injection while maintaining image quality. For the architecture consisting of sequential double stream blocks and single stream blocks of FLUX.1-dev, only the deeper single stream blocks (20-38) carry out the value injection process. Filtering. The thresholds used for distortion-type artifact filtering, τ1 and τ2, were chosen heuristically to align with human perception, set to 0.5 and 0.9. For each artifact region cropped from the original image and the artifactinjected image, LPIPS distance was measured to ensure certain quality among the artifacts introduced. While malformed regions that are not acceptable as plausible structural artifacts are filtered out by high LPIPS distance over 0.5, unsuccessful artifact injections with similar features are discarded by the low LPIPS distance, indicating high similarity. To justify the selected configuration of the synthesis agent in A.2, we conducted ablations on the PE injection steps along with the value injection blocks and visualized their results. Figure 11 shows how the choice of injection steps and value-injecting blocks effects artifact injection quality. B. Prompt Templates for ArtiAgent B.1. Entity-Subentity Vocabulary ( 4.1.1) We employ the capability of GPT-4o [33] to identify and recognize relationships between entities in images. To ensure that the extracted entity-subentity sets are clearly segmentable and valid for generating plausible artifacts, we set strict rules and guidances as in Figure 12 and instruct the VLM model to respond with qualified sets of vocabulary in an explicit format. Figure 13 shows the precise prompt used to generate the entity-subentity vocabulary sets and an example response from the perception agent. B.2. Data Filtering ( 4.3.1) The curation agent uses set of deliberately described artifact types referring to the injection methods and detailed instructions to detect or explain the artifacts, shown in Figure 14. Utilizing the descriptions and criteria according to the type of artifact injected, we query the VLM model with the entity name and triplet of images consisting of 1) the original image with the target part masked out, 2) the original image with only the target part cropped, and 3) the generated image with only the target part cropped. Although modern out-of-the-box VLMs demonstrate short understanding of structural artifacts, providing the models with the rich image context through the image triplet and detailed descriptions of the type of artifact enhances their reliability for this task. The prompt template is elaborated in Figure 15. B.3. Explanation Generation ( 4.3.2) Similarly to Appendix B.2, we employ artifact type description in Figure 14 to generate rich explanations. The detailed prompt and an example of the local and global explanation generation process is shown in Figure 16 and Figure 17. By providing multiple image that contains both the global and local view, we maximize VLM capacity of understanding structural artifacts for reliable quality in explanations. Moreover, we employ BLIP2 [26] to generate concise caption describing the real image. C. Synthesis Agent Tools ( 4.2.1) The artifact injection tools in the synthesis agent produce targetreference patch mappings that are subsequently consumed by the inversioninjection module in Section 4.2.2. Each tool follows common interface but implements different geometric prior tailored to specific artifact type (duplication, omission, distortion, and fusion). Notation. Let the image be discretized into patch grid of size (hp, wp), and let Pall be the set of all patch coordinates on this grid. For given tool call, the tool outputs targetreference mapping = (pt, pr) { , } where pt Pall denotes target patch to be modified, and pr Pall is its reference patch whose semantics will be injected at pt during the diffusion inversioninjection stage. We freely switch between linear indices and (y, x) grid coordinates using simple indexcoordinate conversion routines. For tools that operate on entityor subentity-specific regions, we additionally assume access to patch sets such as Pent and same-subentity forethe same-entity foreground ground Pent contains all patches that bePsub. Concretely, long to single object instance (e.g., one person or one dog), Psub Pall marks patches of other instances of the while same semantic part (e.g., hands of other people, paws of other dogs) that we want to avoid colliding with. These sets are provided by the synthesis agents perception stage (e.g., derived from instance/part segmentation) and are treated as fixed inputs when running the tools. Whenever we say that we clip to the valid patch grid, we mean that any candidate coordinate (i, j) whose row or colwp is umn index falls outside the range 0 either discarded or projected back into the rectangular domain [0, hp 1] by truncating it to the nearest boundary index, so that all patches used by the tools lie on valid positions of the patch grid. hp or 0 [0, wp 1] Add Tool (duplication). The Add Tool (Alg. 1) realizes duplication-type artifacts by creating an extra, plausibly placed copy of subentity (e.g., an extra hand or paw adjacent to the original one). Given set of reference patches for the original subentity, the tool first computes the subentity centroid (ci, cj) in patch space and constructs perimeter band Pring of candidate locations around this centroid with Manhattan distance in [1, α]. For each candidate (i, j) Pring, it evaluates the score S(i, j) = (cid:0) rself where rself, rent, and rsub measure the overlap ratio of the shifted subentity with (i) the original subentity region itself, (ii) other foreground patches of the same entity, and rent rsub (cid:1) gdist, (iii) foreground patches belonging to other instances of the same subentity, respectively, and gdist is distance-based decay term that penalizes large offsets. Intuitively, this prefers candidate locations that (i) stay close to the source entity, (ii) minimally overlap the original instance, and (iii) avoid collisions with other same-subentity regions. The tool then selects the best-scoring perimeter patch (i, j), computes , the corresponding offset ( ), and builds the mapping = { ((ri + , rj + ), (ri, rj)) : (ri, rj) , } which duplicates the entire subentity at the chosen location. Remove Tool (omission). The Remove Tool (Alg. 2) implements omission-type artifacts by erasing subentity and filling the region with nearby background context. The target set is the set of patch coordinates belonging to the subentity being removed. The tool constructs local neighborhood Pnbr = discarding any coordinates outside the valid patch grid. Pall : , / R, pt 1 pt { , } From this neighborhood it derives: Pnbr Psub, Pnbr Pent. Pnbr-no-sub = If sufficiently many true background patches exist, i.e. Pnbr-non-ent = Pnbr-non-ent 1 2 Pnbr-no-sub , pool = the tool prioritizes them as the reference pool Pnbr-no-sub, which Pnbr-non-ent; otherwise it uses avoids collisions with other same-subentity patches but may include same-entity foreground. Finally, each target patch pt reference patch under L1 distance: selects the nearest pool = pr = arg min pP pool pt 1, . = (pt, pr) { and the Remove Tool outputs the mapping } Distort Tool (distortion). The Distort Tool (Alg. 3) performs structural perturbations within subentity while keeping its global placement intact. Here, the target and reference sets are drawn from the same foreground region: is obP tained by applying distortion kernel. The tool supports three kernel types: Shuffle kernel. The simplest kernel copies indexes the original subentity patches, and and applies random permutation. Each target patch is thus reassigned to randomly chosen patch of the same subentity, breaking local structure while preserving appearance statistics. into P 2 Gaussian jitter kernel. For each (py, px) , the kernel repeatedly samples discrete offset from Gaussian (0, σ2I). If distribution in patch space, (δy, δx) sampled location falls within the same-entity foreground Pent, it is accepted as the reference; otherwise, the kernel resamples up to fixed budget and falls back to the nearest foreground (or self) patch if necessary. This yields small, local displacements that bend the entitys internal geometry. Strip kernel. This kernel first computes the bounding box of and chooses dominant direction (vertical or horizontal) by aspect ratio. It then partitions the subentity into strips along that direction and, within each strip, imposes an ordering of patches. Each strip is circularly shifted by an integer offset (with alternating signs and magnitudes), and the shifted positions define the references. This produces band-like shearing or sliding artifacts within the entity. the tool After applying the chosen kernel to form R[i]) ( returns the one-to-one mapping , } which causes the inversioninjection module to reconstruct structurally distorted yet context-consistent object. R, [i], = { Poverlap = Fuse Tool (fusion). The Fuse Tool (Algs. 45) introduces fusion artifacts along the interface of two overlapping entities with patch sets B. It first identifies the overlap region and B A Poverlap = and the union foreground non-overlapping parts Poverlap. If Pfg = B, as well as the PBA = PAB = Poverlap and , no fusion is applied. Given the overlap, the tool constructs thin fusion band around Poverlap by dilating each overlap patch within an L1 radius and intersecting with Pfg. To avoid treating the band as single global region, the tool selects up to seeds on via farthest-point sampling and assigns each band patch to its nearest seed in L1 dis- . For each region, it then tance, forming local regions } chooses an opposite-side pool Popp: if the regions seed is closer (in L1) to PBA, and vice versa; if PAB, the pool is distances tie or side is empty, the pool defaults to . Over discrete set of small integer offsets Pfg {R j , Roff} Ω = (i, j) : 1 + { the tool then selects the offset which, when applied to patches in the region, maps the largest number of them onto Popp. Each band patch is finally valid, non-band patches in paired either with its offset-shifted opposite patch (if valid) Popp in L1 distance. The reor with the nearest patch in sulting mapping injects appearance from one entity into the boundary band of the other, producing visually plausible yet structurally implausible fusion along their interface. In practice, we optionally add subset of reversed pairs (pr, pt) while ensuring that each target is unique, which yields more symmetric blending as visualized in Figure 4. 3 Figure 9. Visualizations of artifacts injected with ArtiAgent. The first and third row shows the original images, whereas the second and fourth row shows the output images with artifacts injected. Figure 10. An instance of ArtiAgent with the annotation. 4 Figure 11. Hyperparameter study on PE injection and value injection steps (1). The image in the red box shows our selected configuration. Figure 12. Guidance and rules for generating entity-subentity vocabulary sets. 5 Figure 13. Full prompt for generating entity-subentity vocabulary sets. 6 Algorithm 1 Add Tool Require: Reference patches PR, patch grid (hp, wp), same-entity foreground Pent, same-subentity foreground Psub, ring thickness α, distance weight λdist Ensure: target-reference patch mapping = {(pt, pr)} 1: (ci, cj ) CENTROID(PR) /* subentity center in patch space */ 2: Pring PERIMETERBAND(PR, (ci, cj ), α, hp, wp) /* candidate centroid patches band around the subentity */ 3: 4: for each (i, j) Pring do 5: 6: 7: 8: ci, cj Pshift {(ri + i, rj + ) : (ri, rj ) PR} rself PshiftPR Pshift rent Pshift(PentPR) Pshift rsub PshiftPsub Pshift dL1 ci + cj 1 gdist 1+λdistdL1 S(i, j) (3 rself rent rsub) gdist 9: 10: 11: 12: 13: (i, j) arg max(i,j)Pring cj 14: ci, 15: {((ri + , rj + 16: return S(i, j) ), (ri, rj )) : (ri, rj ) PR} 17: function CENTROID(PR) 18: 19: (ri,rj )PR PR (cid:80) ci 1 cj 1 ci round(ci), return (ci, cj ) (cid:80) 20: 21: 22: (ri,rj )PR ri rj cj round(cj ) Pring for 0 to hp 1 do 23: function PERIMETERBAND(PR, (ci, cj ), α, hp, wp) 24: 25: 26: 27: 28: 29: dL1 ci + cj if 1 dL1 α then for 0 to wp 1 do Pring Pring {(i, j)} 30: return Pring 7 Algorithm 2 Remove Tool Require: Subentity patch set PT , patch grid (hp, wp), same-entity foreground Pent, same-subentity foreground Psub, neighborhood radius Ensure: Targetreference mapping 1: Pnbr LOCALNEIGHBORHOOD(PT , R, hp, wp) /* collect nearby non-target patches in the grid */ 2: Pnbr-no-sub Pnbr Psub 3: Pnbr-non-ent Pnbr Pent 4: if Pnbr-non-ent > 1 pool 5: 6: else pool 7: 8: 9: for each pt PT do 10: 2 Pnbr-no-sub then Pnbr-non-ent Pnbr-no-sub pr arg min pt q1 qP pool 11: {(pt, pr)} 12: return Pnbr for each (ti, tj ) PT do for dy = to do 13: function LOCALNEIGHBORHOOD(PT , R, hp, wp) 14: 15: 16: 17: 18: 19: 20: 21: 22: Pnbr Pnbr {(pi, pj )} for dx = to do return Pnbr if dy + dx and (dy, dx) = (0, 0) then pi ti + dy, pj tj + dx if 0 pi < hp and 0 pj < wp and (pi, pj ) / PT then PR SHUFFLEKERNEL(PT ) /* randomly permute subentity patches */ Algorithm 3 Distort Tool Require: Subentity patches PT , patch grid (hp, wp), same-entity patches Pent, kernel type {shuffle, jitter, strip} Ensure: target-reference patch mapping = {(pt, pr)} 1: if = shuffle then 2: 3: else if = jitter then 4: 5: else if = strip then 6: 7: {(PT [i], PR[i]) : = 1, . . . , PT } 8: return PR GAUSSIANJITTERKERNEL(PT , σ, hp, wp, Pent) /* locally jitter patches within the entity */ PR STRIPSHIFTINGKERNEL(PT , S, hp, wp) /* shift patches along strips of the subentity */ 9: function SHUFFLEKERNEL(PT ) 10: 11: 12: PR PT RANDOMSHUFFLE(PR) return PR δx (0, σ2) PR for each (py, px) PT do found False for 1 to Amax do δy (0, σ2), ny clip(round(py + δy), 0, hp 1) nx clip(round(px + δx), 0, wp 1) if Pent = or (ny, nx) Pent then 13: function GAUSSIANJITTERKERNEL(PT , σ, hp, wp, Pent) 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: PR PR {(ny, nx)} found True break if not found then return PR (ny, nx) NEARESTFOREGROUNDORSELF(py, px, Pent) PR PR {(ny, nx)} Compute bounding box of PT in patch space Determine direction {vertical, horizontal} from aspect ratio Partition PT into strips {S1, . . . , SS } along For each strip Ss, sort patches to obtain an order (p(s) Choose integer strip shifts {s}S PR list of length PT for 1 to do 29: function STRIPSHIFTINGKERNEL(PT , S, hp, wp) 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: converted to patch units (circular shift) for 1 to ns do 1 + ((u + 1) mod ns) Set reference of p(s) in PR to p(s) return PR 1 , . . . , p(s) ns ) s=1 (alternating signs, increasing magnitudes) 9 return Algorithm 4 Fuse Tool (Main) Require: Entity patches PA, entity patches PB, patch grid (hp, wp), band radius R, max offset Roff, number of seeds Ensure: target-reference patch mapping = {(pt, pr)} 1: Poverlap PA PB 2: if Poverlap = then return 3: 4: Pfg PA PB 5: PAB PA Poverlap, PBA PB Poverlap 6: PT OVERLAPFUSIONBAND(Poverlap, Pfg, R, hp, wp) /* build foreground band around the overlap */ 7: if PT = then 8: 9: FARTHESTPOINTSAMPLING(PT , K) /* choose seeds that cover the band */ 10: Initialize {Rs}sS as empty sets 11: for each PT do 12: 13: 14: Ω {(i, ) : 1 + Roff} 15: 16: for each seed do Rs 17: if = then 18: 19: continue 20: 21: 22: 23: 24: 25: return Popp OPPOSITEREGION(s, PAB, PBA, Pfg, PT ) /* decide which side to fuse from */ BESTOFFSET(R, Popp, Ω, hp, wp, PT ) /* find best shared shift for this region */ for each do pr OFFSETORNEAREST(p, , Popp, hp, wp, PT ) /* apply offset or nearest opposite patch */ {(p, pr)} arg minsS s1 Rs Rs {p} 10 PT for each (oi, oj ) Poverlap do for dy to do Algorithm 5 Fuse Tool (Helpers) 1: function OVERLAPFUSIONBAND(Poverlap, Pfg, R, hp, wp) 2: 3: 4: 5: 6: 7: 8: 9: 10: if dy + dx then PT PT {(vi, vj )} for dx to do return PT vi oi + dy, vj oj + dx if 0 vi < hp and 0 vj < wp and (vi, vj ) Pfg then if = then return min(K, P) Choose initial seed s1 (e.g., closest to centroid) {s1}; for 2 to do 11: function FARTHESTPOINTSAMPLING(P, K) 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: sm arg maxpP d(p) {sm} for each do d(p) min{d(p), sm1} d(p) s11 for all return 23: function OPPOSITEREGION(s, PAB, PBA, Pfg, PT ) 24: 25: 26: dA +, dB + if PAB = then dA minaPAB a1 27: 28: 29: 30: 31: 32: 33: 34: if PBA = then dB minbPBA b1 if dA < dB then return PBA else if dB < dA then return PAB else return Pfg PT None, 0 for each (i, ) Ω do 35: function BESTOFFSET(R, Popp, Ω, hp, wp, PT ) 36: 37: 38: 39: 40: 41: 42: 43: 44: 45: count, (i, ) if count > then count count + 1 rj vj + return count 0 for each (vi, vj ) do ri vi + i, if 0 ri < hp and 0 rj < wp and (ri, rj ) Popp and (ri, rj ) / PT then (vi, vj ) if = None then (i, ) ri vi + i, if 0 ri < hp and 0 rj < wp and (ri, rj ) Popp and (ri, rj ) / PT then 46: function OFFSETORNEAREST(p, , Popp, hp, wp, PT ) 47: 48: 49: 50: 51: 52: 53: return arg minuPopp (vi, vj ) u1 return (ri, rj ) rj vj + 11 Figure 15. Full prompt and example for data filtering. Figure 14. Type descriptions and instructions by artifact type. 12 Figure 16. Full prompt and example for regional explanation generation. Figure 17. Full prompt and example for global explanation generation. 13 D. Benchmark Datasets ( 5) Table 4 shows the fully cited artifact benchmark datasets and the sources used for generation. Details on the datasets metadata formats for artifact region representations and image explanations are elaborated below. RichHF. The RichHF dataset, with 995 images sampled and annotated from the Pick-a-Pic dataset, provides the artifact map in heatmap format, highlighting the probability of abnormal regions in the image. Annotations and scores are generated by the trained multimodal transformer to predict human perception-aligned feedback. LOKI. The LOKI benchmark is multimodal syntheticdata detection dataset, covering image, video, text, audio, and 3D content, and comprises roughly 18,000 curated questions across 26 subcategories. It includes coarse real vs. synthetic judgments, multiple-choice detection, anomaly / artifact region selection, and explanation tasks. We use the image artifact subset of the dataset, which is consisted of 229 images. SynthScars. The SynthScars dataset consists of 12,236 fully synthetic images across four distinct content types (Human, Object, Scene, Animal) and three artifact categories (Physics, Distortion, Structure). Each image is annotated with pixel-level segmentation masks delimiting artifact regions, detailed textual explanations of the artifact(s), and artifact-category labels. Visualizations. Visualizations of the evaluated benchmarks, including ArtiBench, are shown in Figure 18. It is clearly visible that even though our benchmark dataset shows overall better quality of image generation compared to the previous benchmarks, image artifacts are still visible in the generated images. We emphasize the vitality of our timely benchmark that successfully represents structural artifacts remaining in the most recent diffusion models. E. ArtiBench Generation ( 5) Image Generation. ArtiBench is built on set of images generated by state-of-the-art diffusion models, reflecting the most timely artifacts appearing in current image generation models. We use five different models for image generation and three reliable prompt sources. After generating set of images with the five models from randomly sampled prompts, the images were annotated according to the strictly guided annotation process. Annotation Pipeline. With the set of diffusion-generated images, we construct ArtiBench following guided 4-step annotation process: 1) classification, 2) drawing and labeling bounding boxes, 3) VLM-based explanation generation, and 4) expert curation for quality and balance guarantees. Classification. Annotators are asked to meticulously observe the images and determine whether there are any arFigure 18. Artifact examples of the four benchmarks, listed in the order of publishment. tifacts visible in the image. The caption used for T2I generation is provided with the image for better understanding of generative intentions and respecting the models capability to follow instructions on generating abnormalities. All images classified as having artifacts proceed to the next step, where they are shuffled and redistributed to the annotators to mitigate human bias in the examination process. Bounding Box Labeling. For the artifact-containing images from the classification step, annotators are expected to generate bounding boxes, identify the type of artifact, and write short description about what abnormalities exist in the specified region. Explanation Generation. The final annotations are used as the input of the VLM query to generate comprehensive and polished explanation for the full image. With the prompt used for generation and the list of pairs of bounding box and captions as the input, the VLM is required to output global explanation of the image regarding all artifacts mentioned in the captions. Curation. With the set of images and the metadata completed, we curate the images to build the final version of our 1k benchmark. Images are carefully selected to ensure balanced set that provides clear and straightforward view of plausible artifacts. F. Evaluation Protocol ( 6.1) F.1. Binary Classification Accuracy. The rate of true predictions are measured to show straightforward performance of accurate classification. Macro F1. In addition, we use the macro F1 score, to 14 Table 4. Comparison of artifact benchmark datasets, their generative sources, and evaluation tasks. Highlighted entries denote dataset sources that were reused by subsequent benchmarks. Benchmark Generative Sources RichHF [27] SD2.1 [42], SDXL [38], Dreamlike Photoreal [10] LOKI [51] FLUX [5], SD1.42.1 [42], Midjourney [30], StyleGAN [22], pix2pix [18], CUT [36] SynthScars [20] RichHF [27], Chameleon [50], Midjourney [30], DALLE 3 [32], SD1.x [42] ArtiBench SD3.5 [11], FLUX [5], Qwen-Image [49], Nano-Banana [13] Sample Bin. Loc. Exp. 955 229 1K 1K achieve fair comparison across unbalanced datasets and biased predictions. F1 scores are relatively calculated for positive and negative samples to be averaged, exposing the models capability to precisely capture both artifact presence and absence without random guessing. F.2. Localization We alleviate the unfairness between divergent representations of artifact regions, including bounding boxes, polygonal segmentation maps, and heatmaps, by mapping all representations to pixelwise binary maps. IoU. With the binary maps obtained, we calculate the pixelwise IoU between the foreground region predictions and ground-truth areas. IoU scores prefer tight and highly overlapping predictions, providing intuitions on precise predictions but highly possible of penalizing bounding box representations for overprediction. F1. Pixelwise predictions are measured to capture true predictions on fine-grained basis. Unlike the macro F1 score used in binary detection tasks, F1 scores are measured only for positive ground truth regions to prioritize true detections. With each pixel prediction classified, we use the pixel count for true positive (TP), false positive (FP), and false negative (FN )predictions to use them for calculating the F1 score. This method lessens the penalty on loose region representations of bounding boxes over segmentation maps or heatmaps. F.3. Explanation ROUGE-L. The ROUGE-L score shows the proportion of the longest overlapping phrase among the full data. This captures the words or phrases that focus on specific artifact regions and objects, with higher scores showing that the model better understands the visual artifact. CSS. Cosine similarity was measured on sentence embeddings generated by sentence-transformers [40]. CSS portrays the general similarity in context between the descriptions regarding the full scenerys plausibility. F.4. Evaluation Prompt of VLMs Figure 19 shows the specific prompts used for the evaluation on VLMs. Artifact priors are shared for all tasks, providing understanding on the structural artifacts we are focusing on. Figure 19. Evaluation prompts for VLMs. G. VQA Dataset Structure ( 6.1) This section presents the structure of our multi-turn visual question answering (VQA) dataset, which provides the supervision used to train VLMs in artifact detection, localization, and explanation. Each data instance is derived from 15 paired cleanartifact image generated by ArtiAgent with the synthesized annotations as in Figure 10, enabling the construction of tightly aligned conversations that elicits the models ability to identify normal content and reason about artifact regions. G.1. VQA Template Each ArtiAgent instance yields two conversations: one for the clean reconstruction image and one for the corresponding artifact image. Tables 5 and 6 summarize the question answer templates implemented across these two settings. Task Template Bin. Loc. 1 Loc. 2 Q: Does this image contain any visual artifacts? A: No. Q: Locate the entitys subentity. A: bbox Q: Is there entitys subentity in bbox? A: Yes / No Exp. Q: Describe the clean image. A: image caption H. Reward-guided generation ( 6.2.1) Verifier Training. We train artifact verifier modeled with Bradley-Terry model [6] to score images by how likely they are to be artifact-free. The model is trained to assign higher scores to the clean image and lower scores to the artifactinjected image. The verifier uses frozen ViT-B/16 encoder with lightweight MLP head, and is optimized using 104, L2 103, weight decay 1 AdamW (learning rate 1 104). Training uses batch size of 32 for regularization 1 5 epochs. Each batch additionally includes two cross-scene negative pairs to encourage content-agnostic ranking. Test-Time Scaling. At inference time, we use the verifier as reward model in compute-scaled best-of-N sampling procedure [28]. For each prompt, round samples 2r independent latent noises, generates all corresponding images, and evaluates them with the verifier. The highest-scoring image is retained, and the search space doubles in the next round. This random-search strategy expands the candidate pool exponentially, enabling the diffusion model to reliably discover images with fewer artifacts without modifying the weights of the model. Table 5. VQA templates for clean images. I. Image Correction ( 6.2.2) Prompt Instruction Text Task Template Bin. Q: Does this image contain any visual artifacts? A: Yes. Loc. 1 Loc. 2 Q: Provide bounding boxes for all artifact regions. A: [bbox] Q: Explain why region bbox is an artifact. A: label Exp. Q: Describe all artifacts in the image. A: explanation Table 6. VQA templates for artifact-injected images. G.2. VLM Training Configuration We train the VLM using two-stage supervised fine-tuning setup. We use Qwen2.5-VL-7B-Instruct [4] and Intern3.5VL-8B [57] as the backbone, and use the identical configurations. In the first stage, we fine-tune only the language model and multimodal projector while keeping the vision 105, batch size encoder frozen, using learning rate of 1 64, cosine decay scheduling, and one training epoch. In the second stage, we unfreeze the vision encoder and continue 106, with 200 training with smaller learning rate of 1 steps. Both stages use the same VQA dataset. Captioning Localization Provide the bounding box for the artifact region. Format as [x min, min, max, max]. Only output the bounding box. Describe what the image would look like if it had no artifacts. Provide short caption of the clean scene. Is there an artifact within B? Answer Yes or No. Verifying Table 7. Prompts used by the VLM in the artifact-correction loop. Algorithm 6 VLM-Guided Artifact Correction Loop Require: Image containing at least one artifact VLM(I, plocalize) 1: /* localize artifact region */ 2: 3: /* generate caption of the image */ 4: 5: while true do 6: 7: VLM(I, pcaption) Inpaint(I, B, c) /* Inpaint region using caption */ /* check artifact existence inside */ if not then return VLM(I, pverify) 8: 9: 10: 11: 16 In our artifact-correction pipeline, all VLM interactions are carried out using the Qwen2.5-VL-7B model fine-tuned with ArtiAgent supervision. This unified VLM is responsible for localizing artifact regions, generating artifact-free captions, and verifying whether the corrected content remains flawed. We describe each of the three prompts used in the loop below. The actual prompt is shown in Table 7. Localization prompt plocalize. This prompt instructs the VLM to identify the spatial extent of the artifact within the input image. The model is asked to produce single bounding box formatted as [x min, min, max, max] without additional commentary. This strict output format ensures deterministic parsing. The resulting bounding box is computed once at the beginning of the procedure and kept fixed across all subsequent iterations, providing spatial stability for the iterative refinement process. Captioning prompt pcaption. To guide inpainting, the VLM is prompted to describe the image overall image, which out specifying the artifacts in the image. Rather than summarizing the corrupted content, the model is explicitly instructed to generate short caption that reflects the intended clean version of the scene. This caption serves as the semantic conditioning signal for the FLUX inpainting pipeline [3], which synthesizes corrected content inside the region B. Verifying prompt pverify. After each inpainting step, the VLM is queried to assess whether the corrected region still contains an artifact. The prompt restricts the judgement to the localized region, returning binary response (Yes or No). This local verification prevents the algorithm from drifting to unrelated image regions and directly determines whether the loop terminates or continues. Together, these three prompts coordinate the interaction between the ArtiAgent-trained VLM and the FLUX inpainting pipeline: the VLM identifies the corrupted area, provides semantic guidance for correction, and validates the result, while the inpainting model performs the pixel-level repair. This iterative coupling enables consistent and stable reduction of visual artifacts while preserving overall image semantics. The whole procedure is shown in Algorithm 6."
        }
    ],
    "affiliations": [
        "KAIST",
        "KRAFTON",
        "Seoul National University"
    ]
}