{
    "paper_title": "Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization",
    "authors": [
        "Luca Della Libera",
        "Cem Subakan",
        "Mirco Ravanelli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast."
        },
        {
            "title": "Start",
            "content": "Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Luca Della Libera 1 2 Cem Subakan 3 1 2 Mirco Ravanelli 1 2 6 2 0 2 4 ] . [ 2 4 7 1 3 2 . 1 0 6 2 : r Abstract Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast."
        },
        {
            "title": "1 Introduction\nIn the past few years, tokenization has emerged as a key\nbuilding block in multimodal LLMs, enabling joint au-\ntoregressive modeling across modalities (Grattafiori et al.,\n2024; Jiang et al., 2024a; Comanici et al., 2025; Singh\net al., 2025; DeepSeek-AI et al., 2025). By representing\ndiverse modalities as sequences of discrete tokens, these\nmodels can operate over text, images, and audio within a\nshared modeling framework. For speech, tokenization is\ntypically achieved through neural audio codecs, which\nmap continuous waveforms to discrete tokens. Such repre-\nsentations have driven recent progress in generative speech",
            "content": "1Concordia University, Montreal, 2MilaQuebec AI Institute, Montreal, Canada 3Universite Laval, Quebec, Canada. Correspondence to: Luca Della Libera <luca.dellalibera@mail.concordia.ca>. Canada Preprint. February 5, 2026. 1 and audio modeling (Borsos et al., 2023; Chen et al., 2025; Nguyen et al., 2025; Defossez et al., 2024; Labiausse et al., 2025; Zeghidour et al., 2025), enabling LLMs to operate directly on speech. Originally developed for efficient transmission (Zeghidour et al., 2021; Defossez et al., 2023), modern codecs now produce compact yet powerful representations that support high-quality resynthesis as well as competitive performance in both discriminative and generative downstream tasks (Mousavi et al., 2025; Guo et al., 2025). Despite their success, most existing speech tokenizers rely on frame-level representations at fixed frame rate, producing tokens at constant temporal resolution. While effective for reconstruction, this design is poorly matched to the inherently variable temporal structure of speech: silence and steady regions are information-poor, whereas rapidly changing segments are information-dense (Van Kuyk et al., 2017; Dieleman et al., 2021; Cuervo et al., 2022). As result, fixed-frame-rate tokenization leads to inefficient sequence lengths and limited alignment with corresponding text, making generative modeling more challenging. Although dynamic-frame-rate codecs and text-supervised alignment methods have recently emerged, this area is still developing. Existing approaches often rely on heuristic frame merging or clustering strategies that are weakly grounded in linguistic structure (Wang et al., 2025a; Zhang et al., 2025; Zheng et al., 2026; Li et al., 2025b), or require transcriptions or alignment information at inference time (Tseng et al., 2025; Hsu et al., 2025), limiting flexibility in fully speech-based scenarios. In this work, we introduce DyCAST, Dynamic CharacterAligned Speech Tokenization framework that enables variable-frame-rate coding through soft character-level alignment. Each speech token is approximately associated with character in the underlying transcript, with the degree of alignment controlled by learned boundary predictor. At inference time, this alignment can be adjusted dynamically, allowing flexible trade-off between strict character alignment and longer token spans. As result, DyCAST can operate either with ground-truth alignments when available (e.g. for text-to-speech) or in fully alignment-free mode based on the learned soft alignment alone. Additionally, DyCAST provides explicit control over token durations during decoding via duration predictor that learns to invert the pooling operation, predicting appropriate token durations from the context. Because character-level representations Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Figure 1. DyCAST architecture. Frame-level representations extracted by frozen, self-supervised encoder are compressed and dynamically grouped into variable-length chunks, pooled, and quantized into discrete tokens. The decoding stage reverses this process by expanding token-level representations back to frame-level features before waveform reconstruction. Character-level boundaries are provided during training by frozen aligner. naturally result in very low frame rates, achieving highquality resynthesis becomes more challenging. To address this limitation, we further introduce retrieval-augmented decoding as an auxiliary mechanism to improve reconstruction quality by leveraging side information, without increasing bitrate. In summary, our contributions are as follows: We propose DyCAST, novel framework for controllable, character-aligned, variable-frame-rate speech tokenization through soft character-level alignment and explicit duration modeling. We introduce retrieval-augmented decoding as an auxiliary mechanism to enhance speech resynthesis quality by leveraging side information at inference time. We empirically show competitive performance on speech resynthesis and range of other downstream tasks, while using significantly fewer tokens compared to fixed-framerate baselines. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast."
        },
        {
            "title": "2 Related Work\nFixed-Frame-Rate Codecs. The dominant approach to\nspeech tokenization discretizes audio at a fixed temporal res-\nolution using frame-level tokens. Early research on neural\nspeech codecs primarily focused on high-fidelity acoustic\nreconstruction at medium bitrates (Zeghidour et al., 2021;\nD´efossez et al., 2023; Kumar et al., 2023). In parallel, a\nseparate line of work explored semantic tokenization by\ndiscretizing self-supervised phonetic speech representations\nto capture linguistic content (Baevski et al., 2020; Hsu et al.,\n2021; Messica & Adi, 2024; Mousavi et al., 2024b; Chang\net al., 2025), at the expense of fine-grained acoustic de-\ntail. More recently, hybrid codecs have emerged with the\ngoal of balancing semantic and acoustic information. These\napproaches combine multiple design strategies, including\nthe use of multiple codebooks (Ju et al., 2024; Jiang et al.,\n2024b; Zheng et al., 2025), dual-encoder architectures (Liu\net al., 2024), knowledge distillation (Zhang et al., 2024;\nD´efossez et al., 2024; Yang et al., 2025; Gong et al., 2025;",
            "content": "Li et al., 2025a), or supervised fine-tuning (Har-Tuv et al., 2025), to improve both reconstruction quality and speech language modeling performance. More recently, growing trend has shifted toward single-codebook designs that jointly encode semantic and acoustic information. These approaches achieve good reconstruction quality at low bitrates while substantially simplifying downstream modeling (Ji et al., 2025; Bai et al., 2024; Xin et al., 2024; Wu et al., 2025; Ye et al., 2025; Della Libera et al., 2025a;b; Song et al., 2025). Our work departs from fixed-rate tokenization by introducing dynamic, character-aligned speech tokens whose durations adapt to linguistic content, providing more efficient and linguistically grounded representations. Variable-Frame-Rate Codecs. Dynamically adjusting frame rates based on content complexity has recently attracted attention across modalities. In the text domain, several recent works (Nawrot et al., 2023; Slagle, 2024; Ahia et al., 2024; Pagnoni et al., 2025; Videau et al., 2025; Hwang et al., 2025) explore learned tokenization approaches, in which chunking is learned end-to-end according to task objective, rather than being fixed priori. In these methods, token boundaries emerge from the optimization process itself, enabling adaptive representations that better match the structure of the underlying data. similar trend is emerging in audio. Early work by Dieleman et al. (2021) proposed an audio VQ-VAE combined with run-length encoding to enable variable frame rates. More recently, CodecSlime (Wang et al., 2025a) introduced multi-stage pipeline in which fixedframe-rate codec is first trained and then temporally similar representations are merged to obtain adaptive frame rates. TFC (Zhang et al., 2025) and VARSTok (Zheng et al., 2026) propose dynamic-frame-rate strategies in which duration is encoded implicitly through the structure of the codebook. FlexiCodec (Li et al., 2025b) further builds on VARSTok by enabling substantially lower frame rates through refined merging procedures, while retaining fine-grained control over the resulting token rate. Unlike prior approaches that rely on heuristic frame merging or require transmitting durations, our method uses learned character-level soft alignment to produce semantically aligned variable-rate tokens with2 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization out duration side information, while still enabling explicit duration control at decoding time. Text-Aligned Speech Representations. Recent work has explored text-aligned tokenization as way to bridge the modality gap for joint speech-text modeling. Methods such as TASTE (Tseng et al., 2025) and TASLA (Hsu et al., 2025) learn discrete speech tokens aligned with text by enforcing transcript-speech alignment through cross-attention-based architectures. Similarly, TaDiCodec (Wang et al., 2025b) incorporates textual information in text-aware diffusionbased speech codec, where speech-text alignment is introduced implicitly through autoregressive conditioning on text rather than being enforced explicitly at the tokenization stage. SSR (Tan et al., 2025) and LST (Lu et al., 2025) pursue similar goal of speech-text alignment, but are tangential to our work, as they operate at the speech language modeling level and do not define standalone speech tokenization framework. While effective for joint modeling, these methods typically rely on ground-truth alignments or require textual input at inference time, limiting their applicability to text-free generative settings and making them closer to TTS-oriented pipelines than general-purpose speech tokenizers. In contrast, DyCAST is fully non-autoregressive, autoencoder-based speech tokenizer that operates end-t-o end without requiring text at inference time."
        },
        {
            "title": "3 DyCAST\nThe proposed DyCAST framework (see Figure 1) is inspired\nby the modular compressor-quantizer-decompressor archi-\ntecture introduced in (Della Libera et al., 2025a), while\nextending it with dedicated modules for dynamic pool-\ning. Given an input waveform, a frozen, pretrained self-\nsupervised speech encoder first extracts high-dimensional,\nsingle-stream acoustic-semantic representations at a fixed\nframe rate. These features are then projected into a lower-\ndimensional latent space by a lightweight compressor, re-\ntaining the most informative components of the representa-\ntion. A chunker module subsequently groups consecutive\nframes into variable-length chunks, yielding a temporally\nadaptive chunking of the input. Within each chunk, frame-\nlevel features are pooled to form compact chunk-level rep-\nresentations, which are then discretized by a quantizer,\nproducing a sequence of discrete tokens. A dechunker\nmodule reverses the chunking operation by expanding each\ntoken-level representation back to frame-level features. A\ndecompressor maps the expanded features back to the orig-\ninal encoder dimensionality, and a decoder finally recon-\nstructs the time-domain waveform.",
            "content": "3.1 Dynamic Chunking The chunker module consists of two components: boundary predictor, which identifies semantically meaningful chunk boundaries, and downsampler, which aggregates frame-level representations within each chunk into compact chunk-level representation. To provide training supervision for chunk boundaries, we employ frozen, pretrained character aligner. This aligner takes waveform as input and returns character-level durations, and can be conveniently implemented using CTC-based ASR model (Graves et al., 2006) with characters as the vocabulary. The resulting frame-level character boundaries are used to derive target chunk durations that serve as supervision during training. To model the boundary distribution, we adopt discrete-time hazard model (Singer & Willett, 1993; Ren et al., 2019) rather than standard frame-wise binary cross-entropy objective. Unlike independent boundary classification, the hazard formulation explicitly models the time to the next boundary, allowing boundary predictions to be temporally dependent and properly normalized over time. This is particularly important in our setting, where chunk durations are the primary quantity of interest and boundaries are sparse relative to the frame rate. Moreover, the hazard model naturally enforces single boundary per chunk and provides principled likelihood over variable-length segments, which facilitates stable training and coherent boundary decoding. Formally, given frame-level representations x1:T extracted by the compressor, the boundary predictor estimates boundary probability ht (0, 1) at each frame: ht = σ(fθ(x1:T )t) , (1) where fθ is neural network operating on the sequence of features. The probability that the next boundary occurs frames after time is then given by (T = t) = (cid:32)k1 (cid:89) (cid:33) (1 ht+i) ht+k. (2) i=0 The hazard model is trained by maximizing the likelihood of ground-truth next-boundary offsets extracted from forced alignment. At inference time, chunk boundaries are decoded directly from the hazard predictions without any textual input. Boundaries can be obtained either greedily, yielding deterministic chunking, or via sequential sampling to introduce variability. In both cases, decoding constraints on the minimum and maximum chunk duration, controlled by the hyperparameters min gap and max gap, respectively, are used to regulate the resulting frame rate. In addition, hazard threshold τh controls boundary emission: higher values of τh favor longer chunks (lower frame rates), while lower values produce finer-grained chunks (higher frame rates). Formally, boundary is emitted at frame when ht τh subject to the min gap constraint; if no boundary is emitted for max gap consecutive frames, boundary is forced. Given the decoded boundaries, the downsampler produces chunk-level representations by selecting the last frame of each chunk. While alternative strategies such as average pooling per chunk are possible, this design keeps 3 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization the operation simple and efficient, and preserves the original compressed representations without blending them across frames, which facilitates adaptation to different frame rates. 3.2 Dechunking Like the chunker, the dechunker module consists of two components: duration predictor, which estimates the number of frames associated with each character-aligned discrete token, and an upsampler, which expands each chunk-level representation back to frame-level features. The duration predictor is necessary to recover temporal structure at decoding time, since in general only the token sequence is transmitted, not the chunk boundaries used to derive it. Supervision for duration prediction comes from the same frozen character aligner used during chunking, which provides target token durations corresponding to character spans. To model the distribution of token durations, we adopt negative binomial duration model (Zen et al., 2009). Compared to alternatives such as geometric or Poisson distributions, the negative binomial provides greater flexibility by explicitly modeling over-dispersed count data, which is characteristic of speech durations. In particular, while geometric model assumes memoryless process with fixed hazard rate and Poisson model ties the mean and variance, the negative binomial decouples these quantities, allowing the model to capture the heavy-tailed and highly variable nature of character-level durations in speech. Formally, given sequence of discrete tokens c1:N , the duration model maps each token to positive free mean duration µfree = softplus(gϕ(c1:N )i) , (3) where gϕ denotes neural network operating on the full sequence of tokens. The actual mean token duration is obtained by enforcing minimum duration dmin, > 0, µfree µi = dmin + µfree . (4) By default, dmin = 1. The excess target duration yi = di dmin 0 is modeled using the negative binomial distribution with mean µfree and global dispersion parameter α > 0 shared across all tokens, (5) yi NB(µfree , α). (6) The duration model is trained by minimizing the negative log-likelihood together with normalized length regularization term, (cid:88) Ldur = log pNB(yi µfree , α) + λ i=1 (cid:32) (cid:80)N i=1 µfree Tfree (cid:33)2 , Tfree + ϵ (7) where Tfree = dmin denotes the total number of allocatable frames after enforcing minimum duration dmin per token, λ controls the strength of the length regularization, and ϵ is small constant for numerical stability. This normalized penalty encourages globally consistent pacing while remaining invariant to utterance length. Figure 2. Retrieval-augmented decoding (RAD). Discrete latents are refined via similarity search against pool of continuous latents prior to waveform reconstruction. At inference time, the duration model supports two decoding regimes. In the free decoding mode, when the total number of frames is unknown, durations are obtained greedily as ˆdi = dmin + round(µfree ), (8) yielding predicted total length ˆT = (cid:80) ˆdi. In the budgeti constrained decoding mode, when the target length is known (e.g. speech resynthesis), the predicted free means are renormalized to match the available duration budget, = µfree µfree Tfree µfree (cid:80) , (9) followed by deterministic integer rounding that enforces (cid:80) ˆdi = exactly. This hybrid formulation enables exi plicit duration control at decoding time while remaining independent of character alignment during inference. 3.3 Retrieval-Augmented Decoding Since the proposed codec is character-aligned, the resulting frame rate is naturally very low (6 18 Hz). This makes accurate waveform reconstruction increasingly challenging, as fine-grained acoustic details, high-frequency information, and speaker-specific characteristics tend to be removed by the quantization bottleneck (Yang et al., 2025; Li et al., 2025b). To mitigate this issue without increasing the bitrate, we propose retrieval-augmented decoding (RAD), decoder-side mechanism that improves reconstruction quality by selectively incorporating auxiliary information at decoding time (see Figure 2). RAD builds on the fact that self-supervised speech encoders trained with masking objectives (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022) produce intermediate representations with strong semantic structure. In these models, representations that are nearby in feature space tend to correspond to similar linguistic content, enabling meaningful retrieval at the latent level via similarity search. This property underlies the effectiveness of nearest-neighbor methods in tasks such as voice conversion (Baas et al., 2023; Della Libera et al., 2025a). RAD leverages this structure by maintaining pool of continuous latents collected from diverse set of utterances, speakers, and acoustic conditions. During decoding, this pool can be queried to retrieve similar la4 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization tent vectors. By analogy to retrieval-augmented generation, where query embedding retrieves relevant documents from latent knowledge store, RAD retrieves continuous speech latents from this pool via similarity matching against the discrete latent produced by the quantizer. Since these latents are already low-dimensional and compact by design, they are well-suited for efficient nearest-neighbor search and large-scale retrieval. For each quantized latent, if the similarity to its nearest retrieved candidate exceeds predefined threshold τ , the discrete latent is replaced with the retrieved continuous latent. By setting sufficiently high similarity threshold, retrieval can be restricted to candidates that preserve both semantic content and speaker identity. In cases where near-identical speech segments exist in the pool, RAD can recover the original continuous representation, substantially improving speaker fidelity and fine-grained acoustic detail. Interestingly, the candidate pool can be constructed offline and updated over time to adapt to evolving deployment needs (e.g. privacy, memory footprint, speed, and/or efficiency)."
        },
        {
            "title": "4 Experimental Setup\n4.1 Architecture\nWe now describe the DyCAST architecture used in our\nexperiments, inspired by FocalCodec (Della Libera et al.,\n2025a). Full hyperparameter specifications are reported in\nAppendix C.",
            "content": "Character Aligner. We employ MMS (Pratap et al., 2024). Alignment information is extracted via simple argmax CTC alignment on the frame-level log probabilities obtained by running the model on the input waveform. To handle silence segments detected by the aligner, we do not discard them; instead, we aggregate them into the subsequent non-silence token. We find this strategy to work well in practice and leave more refined treatments of silence for future work. Encoder and Decoder. We use the pretrained WavLMlarge1 model (Chen et al., 2022) as the encoder. In particular, we use the representations from the 6th transformer layer, which are semantically rich while retaining fine-grained acoustic information (Della Libera et al., 2025a; Baas et al., 2023). Waveform reconstruction from these features is performed using Vocos (Siuzdak, 2024) for efficient decoding. Compressor and Decompressor. Both modules are implemented using focal modulation blocks (Yang et al., 2022; Della Libera et al., 2024), leveraging their demonstrated effectiveness and efficiency for low-bitrate speech coding (Della Libera et al., 2025a). Boundary and Duration Predictor. We adopt the same efficient focal modulation architecture also for the boundary 1https://github.com/microsoft/unilm/tree/master/wavlm and duration predictor. The boundary predictor uses binary classification head, while the duration model predicts free mean parameter together with shared, learnable dispersion parameter α, as described in Section 3.2. Importantly, the boundary predictor is trained directly on the 1024dimensional WavLM features rather than the compressed latents, providing richer context for boundary estimation. In contrast, the duration predictor operates on the pooled, quantized low-dimensional latents. Quantizer. Della Libera et al. (2025a) employ binary spherical quantization (BSQ) (Zhao et al., 2025), which tightly couples bitrate to latent dimensionality through an implicit codebook of size = 2L. At the low frame rates considered in this work, this coupling limits representational expressivity. We therefore generalize this formulation to scalar spherical quantization (SSQ). SSQ preserves the spherical constraint of BSQ while allowing each latent dimension to take one of scalar levels in [ 1 ], enabling more flexible bitrates. Compared to finite scalar quantization (FSQ) (Mentzer et al., 2024), SSQ explicitly enforces spherical geometry via L2 normalization and includes factorized entropy regularizer for improved codebook utilization. The resulting implicit codebook has size = L. In our experiments, we set = 32 and = 4, yielding = 432. To make this high cardinality tractable in practice, we adopt factorized representation in which each token is decomposed into 32 parallel streams, each with vocabulary size of 4. , 1 4.2 Training We train DyCAST on LibriTTS (Zen et al., 2019), resampled to 16 kHz. We adopt multi-stage training strategy for improved stability: 1. Reconstruction. We train the compressor-quantizationdecompressor for WavLM features reconstruction using L2 loss, without boundary prediction, or duration modeling. Dynamic downsampling and upsampling use the character durations from the character aligner in teacherforced manner. In parallel, we train the decoder to map WavLM features back to waveform. 2. Boundary predictor training. We train the boundary predictor to map WavLM features to character boundaries, using character aligner durations as supervision. 3. Adaptation to predicted boundaries. We fine-tune the compressor-quantizer-decompressor to operate on durations predicted by the boundary predictor. To improve robustness, during training we randomly sample among character aligner boundaries and predicted boundaries greedily decoded with τh = 0.5, min gap {1, 3, 5}, and no max gap constraint (see Section 3.1). 4. Duration predictor training. Finally, we freeze all the other components and train the duration model to predict chunk durations from pooled quantized latents, using the same boundary sampling strategy as in the previous 5 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization step. This procedure yields robust system that generalizes well to different boundary configurations, including min gap values outside the training range. Hyperparameters and training details are provided in Appendix C. Inference 4.3 DyCAST provides substantial flexibility at inference time for both encoding and decoding. For encoding, chunk boundaries can be obtained in two ways: 1. Character aligner. We use boundaries provided by the character aligner. This is particularly useful for downstream applications that benefit from accurate speech-text alignment, such as text-to-speech. 2. Boundary predictor. We greedily decode or sample boundaries from the boundary predictor. By adjusting the decoding hyperparameters, we obtain direct control over the effective frame rate. For example, increasing the min gap parameter enforces larger minimum gap between boundaries, resulting in fewer, more uniformly spaced tokens. For decoding, three operating modes are available: 1. Tokens + durations. We transmit discrete tokens together with their original durations and use them as side information during decoding. The effective bitrate is the token bitrate plus the overhead required to transmit one duration per token, assuming bounded duration range. 2. Tokens + utterance length. We transmit discrete tokens and single global utterance length, which incurs negligible overhead. The duration model is then used in budget-constrained decoding mode to sample durations whose total length matches the provided value. This mode is particularly suitable for resynthesis tasks. 3. Tokens only. We transmit only discrete tokens and let the duration model infer the most likely durations. This mode is useful for modeling tasks such as text-to-speech and speech language modeling, where tokens do not come with explicit duration information. Note that, independently of the chosen setting, durations can always be resampled using the duration model or an external model to introduce natural variability in speaking rate, or manually overridden to stretch or compress individual characters. Overall, this design offers significantly greater flexibility than traditional fixed-rate codecs, enabling fine-grained control over bitrate, timing, and prosody."
        },
        {
            "title": "5 Downstream Evaluation\nFor downstream evaluation, we consider four inference con-\nfigurations: DyCAST-CA, DyCAST-BP1, DyCAST-BP3,\nand DyCAST-BP5. DyCAST-CA uses boundaries provided\nby the character aligner, while the remaining variants rely\non the boundary predictor with min gap set to 1, 3, and\n5, respectively, corresponding to average frame rates of",
            "content": "Table 1. Codecs considered in our downstream evaluation. Codec EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec DyCAST-CA DyCAST-BP1 DyCAST-BP3 DyCAST-BP5 Frame Rate (Hz) Bitrate (kbps) Sample Rate (kHz) Codebooks Params (M) 75.0 50.0 50.0 50.0 25.0 12.5 40.0 80.0 25.0 50.0 1.50 1.00 0.45 1.00 0.65 0.69 0.48 1.04 0.70 0.65 14.4 0.92 17.5 1.12 0.57 9.0 0.40 6. 24 16 16 16 16 24 24 16 16 16 16 16 16 16 2 1024 2 1024 1 512 2 1024 2 8192 5 2048 1 4096 1 8192 2 15625 1 8192 1 432 1 432 1 432 1 432 15 74 127 108 1033 82 85 160 950 142 269 269 269 approximately 14 Hz, 9 Hz, and 6 Hz. Unless otherwise stated, all experiments use the tokens + utterance length decoding mode. We provide analysis of alternative decoding strategies and visualizations in Appendix E.1 and E.2. To showcase the effectiveness of our framework, we compare against range of fixed-frame-rate baselines (see Table 1). Additional details about these baselines are provided in Appendix B. We follow the evaluation protocol of Della Libera et al. (2025a), considering speech resynthesis under different conditions, voice conversion, and both discriminative and generative downstream tasks. For generative evaluation, we focus on text-to-speech in limited-data regimes. Details about the datasets and experimental settings can be found in Appendix and D. 5.1 Speech Resynthesis and Voice Conversion We evaluate the speech resynthesis (SR) capabilities of DyCAST on LibriSpeech (Panayotov et al., 2015), MLS (Pratap et al., 2020), VoiceBank (Valentini-Botinhao et al., 2016), and Libri1Mix. Naturalness, intelligibility, and speaker similarity are assessed using UTMOS (Saeki et al., 2022) for clean speech and DNSMOS (Reddy et al., 2022) for noisy speech, dWER (Wang et al., 2021), and Sim (Della Libera et al., 2025a), respectively. Additional details are provided in Appendix D.1. We further evaluate voice conversion (VC) on VCTK (Yamagishi et al., 2017), following the protocol of Della Libera et al. (2025a), which is described in detail in Appendix D.2. As shown in Table 2, DyCAST achieves strong balance between reconstruction quality and token efficiency, operating at substantially lower frame rates than fixed-frame-rate baselines while maintaining competitive performance across all metrics. On LibriSpeech, DyCAST variants consistently reach high naturalness scores and low intelligibility degradation, closely matching strong fixed-rate codecs such as FocalCodec and Stable Codec, despite using 38x fewer frames. Notably, DyCAST-CA and DyCAST-BP1 achieve dWER values comparable to high-rate baselines, confirming that character-aligned or predicted durations preserve linguistic content even at reduced token rates. For multilingual speech, DyCAST maintains robust performance across languages, with only moderate increase in dWER as the frame rate is reduced. Importantly, speaker similarity 6 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Table 2. Speech resynthesis (SR) and voice conversion (VC). Best and second-best results are highlighted. Codec Reference EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec DyCAST-CA DyCAST-BP1 DyCAST-BP3 DyCAST-BP5 Frame Rate (Hz) Bitrate (kbps) 75.0 50.0 50.0 50.0 25.0 12.5 40.0 80.0 25.0 50.0 1.50 1.00 0.45 1.00 0.65 0.69 0.48 1.04 0.70 0.65 14.4 17.5 9.0 6.2 0.92 1.12 0.57 0.40 SR LibriSpeech SR MLS SR VoiceBank SR Libri1Mix VC VCTK UTMOS dWER Sim UTMOS dWER Sim DNSMOS dWER Sim DNSMOS dWER Sim UTMOS dWER Sim 4.09 1.58 1.29 3.75 2.28 2.91 3.29 3.78 4.11 4.32 4.05 3.99 3.92 3.95 3. 0.00 8.08 20.04 6.20 5.14 8.97 5.73 11.55 2.55 4.97 2.18 3.32 3.61 4.66 8.84 100.0 93.8 89.2 90.0 91.6 96.0 96.0 95.4 98.5 94.7 97.4 97.4 97.6 97.2 96.5 2.84 1.33 1.24 2.97 1.55 1.87 2.08 2.64 2.86 3.47 2.96 2.85 2.90 2.92 2. 0.00 29.60 56.08 44.54 56.32 36.21 30.96 49.73 15.24 56.99 12.57 16.89 16.46 21.61 34.04 100.0 95.5 89.1 89.5 92.0 97.7 96.7 97.0 99.1 95.9 98.3 98.3 98.5 98.3 97.7 3.56 2.76 2.72 3.06 2.74 3.13 3.01 3.09 3.19 3.33 3.16 3.27 3.22 3.23 3. 0.00 28.16 63.90 20.67 34.51 31.46 28.00 42.12 20.67 20.32 8.08 14.70 12.60 17.96 26.62 100.0 87.7 79.8 82.9 82.2 90.6 87.8 89.8 92.3 88.8 91.3 91.5 92.1 91.4 89.7 3.73 2.40 2.40 2.87 2.58 2.67 2.65 2.53 2.75 2.91 2.93 2.92 2.92 2.91 2. 0.00 55.17 90.92 36.60 57.26 51.18 49.14 70.10 53.26 43.52 27.89 30.95 31.37 36.96 46.42 100.0 86.3 76.6 85.9 82.8 89.9 89.4 86.3 88.3 90.0 91.6 91.8 92.0 91.4 90.0 4.09 1.24 1.25 2.90 1.49 2.02 2.40 3.13 1.31 3.76 3.38 3.19 3.26 3.24 3. 0.00 86.52 104.00 26.68 20.32 106.00 110.00 43.15 99.96 27.63 21.27 25.94 24.77 27.66 35.59 100.0 72.2 67.2 92.4 81.2 72.8 89.7 73.4 68.9 71.1 92.2 92.0 92.4 92.3 92.2 remains consistently high across all DyCAST variants, indicating that variable-rate tokenization does not compromise speaker identity, even in low-rate regimes. Notably, aside from the character aligner, which naturally supports multilingual inputs, the codec itself is trained exclusively on English speech, highlighting the strong generalization capability of the learned discrete representations. In noisy conditions, DyCAST remains competitive with dedicated speech codecs. While very aggressive compression leads to higher dWER, naturalness and speaker similarity remain stable, suggesting that explicit duration modeling provides robustness to noise by avoiding over-fragmentation of tokens. For voice conversion, DyCAST achieves strong speaker similarity and intelligibility, comparable to FocalCodec. Performance degrades gracefully as the frame rate is reduced, indicating that the learned discrete units retain speaker-relevant information even at low temporal resolution. Retrieval-Augmented Decoding. To simulate realistic deployment scenario, we construct large candidate pool containing 32-dimensional continuous latents extracted from the compressor. The pool includes all utterances from LibriSpeech train-clean-100, dev-clean, and test-clean, totaling approximately 20M vectors. This setting reflects practical use case in which the retrieval database contains diverse speakers, utterances, and acoustic conditions, while also naturally including material from the current speaker, as speakers tend to repeat similar acoustic patterns over time. Although speaker-specific pool would likely yield stronger results, this deliberately challenging setup allows us to assess the robustness of the learned latent space and its ability to retrieve relevant acoustic content from large, heterogeneous database. For efficiency, the retrieval pool is indexed using an inverted file (IVF) index (Zobel & Moffat, 2006) implemented with the FAISS library (Johnson et al., 2019). We use nlist = 4096 inverted lists, probe nprobe = 16 lists at query time, and train the IVF index coarse clustering on randomly subsampled set of 500k vectors. Table 3 evaluates the impact of retrieval-augmented decoding on LibriSpeech resynthesis across different similarity thresholds τ . Overall, retrieval consistently improves Table 3. Retrieval-augmented decoding for different similarity thresholds τ on LibriSpeech resynthesis. Best and second-best results are highlighted within each section. Codec Configuration UTMOS dWER Sim DyCAST-CA DyCAST-BP DyCAST-BP3 DyCAST-BP5 w/o retriever retriever (τ = 95) retriever (τ = 97) retriever (τ = 99) w/o retriever retriever (τ = 95) retriever (τ = 97) retriever (τ = 99) w/o retriever retriever (τ = 95) retriever (τ = 97) retriever (τ = 99) w/o retriever retriever (τ = 95) retriever (τ = 97) retriever (τ = 99) 3.99 3.85 3.91 3.99 3.92 3.80 3.84 3.92 3.95 3.81 3.87 3.94 3.97 3.80 3.91 3.96 3.32 2.97 3.00 3.33 3.61 3.40 3.36 3. 4.66 3.89 4.23 4.61 8.84 7.22 8.08 8.87 97.4 97.8 97.8 97.4 97.6 98.0 98.0 97.6 97.2 97.8 97.7 97.2 96.5 97.2 96.9 96. intelligibility and speaker similarity at low token rates, while preserving naturalness when applied conservatively. For all DyCAST variants, moderate similarity thresholds (τ [95, 97]) yield the most consistent gains. In this regime, retrieval reduces dWER relative to decoding without retriever, with the largest improvements observed for lowerrate variants. This suggests that retrieval is particularly effective when token sequences are short and reconstruction ambiguity is higher, allowing the decoder to recover fine-grained acoustic detail without increasing bitrate. Importantly, speaker similarity improves systematically with retrieval at moderate thresholds, indicating that retrieved exemplars reinforce speaker-specific characteristics rather than introducing identity drift. At the same time, UTMOS remains stable or decreases only marginally, indicating that retrieval does not harm perceived naturalness when constrained by sufficiently high similarity threshold. In contrast, very high thresholds (τ = 99) closely match the noretrieval baseline across all metrics, effectively disabling the mechanism by being overly selective. We also note that UTMOS is noisy metric and should be interpreted only as coarse indicator of perceptual quality (Della Libera et al., 2025a). Consistent with this observation, informal listening suggests that for UTMOS values above 3.50, similarity plays more dominant role in perceived quality, with even small improvements corresponding to audible gains. 7 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization 5.2 Discriminative Tasks We evaluate the representational quality of DyCAST discrete tokens on automatic speech recognition (ASR), speaker identification (SI), and speech emotion recognition (SER) probing tasks (Mousavi et al., 2024a). For ASR and SI, we use LibriSpeech, while for SER we use IEMOCAP (Busso et al., 2008). All experiments follow the same shallow probing setup as in (Della Libera et al., 2025a). We report word error rate (WER) for ASR and error rate (ER) for SI and SER. Additional details are provided in Appendix D.3D.5. Results for discriminative probing tasks are reported in Table 4. Overall, DyCAST performs well despite operating at substantially lower frame rates than fixed-rate baselines. On ASR, DyCAST-CA achieves the best WER among all codecs, outperforming both fixed-frame-rate speech codecs and prior discrete tokenizers, while using significantly fewer tokens. This highlights the benefit of character-aligned tokenization for preserving fine-grained linguistic information that is directly relevant for recognition. As the frame rate is reduced further, performance degrades gracefully. Variants relying on predicted boundaries maintain competitive WER at moderate rates, while extremely low-frame-rate configurations exhibit higher WER, reflecting the expected trade-off between compression and phonetic resolution. Importantly, this degradation is consistent with trends observed in fixed-rate codecs at comparable or higher bitrates. For SI, DyCAST achieves error rates comparable to strong baselines, indicating that speaker-related cues are well preserved even under aggressive temporal compression. Although fixed-frame-rate tokenizers optimized for speaker modeling (e.g. WavTokenizer) achieve lower SI error rates, DyCAST offers more favorable balance between speaker information retention and token efficiency. On SER, performance across all codecs remains relatively close, suggesting that coarse prosodic and affective cues are robust to different tokenization strategies. DyCAST performs on par with fixed-rate baselines, indicating that variable-rate tokenization does not impair the extraction of emotional content, despite operating at lower frame rates. Overall, these results indicate that DyCAST tokens retain rich linguistic, speaker, and paralinguistic information even at substantially reduced rates. In particular, the strong ASR performance of the character-aligned variant suggests that explicitly modeling durations and linguistic alignment yields discrete tokens that are well suited for speech understanding tasks. 5.3 Text-To-Speech We evaluate the generative capabilities of DyCAST on textto-speech (TTS) using LibriSpeech, with character-level text as input and speaker embeddings for speaker conditioning. Performance is assessed using the same metrics as for speech resynthesis, namely UTMOS, dWER, and Sim. Following Della Libera et al. (2025a), we employ an Table 4. Downstream modeling performance. Shallow probing heads are used for discriminative tasks (ASR LibriSpeech, SI LibriSpeech, SER IEMOCAP), transformer-based models are used for generative tasks (TTS LibriSpeech). Best and second-best results are highlighted. Codec Frame Rate (Hz) Bitrate (kbps) ASR SI SER TTS WER ER ER UTMOS dWER Sim EnCodec DAC WavLM6-KM SpeechTokenizer SemantiCodec Mimi WavTokenizer BigCodec Stable Codec FocalCodec 75.0 50.0 50.0 50.0 25.0 12.5 40.0 80.0 25.0 50. 1.50 1.00 0.45 1.00 0.65 0.69 0.48 1.04 0.70 0.65 27.89 35.89 19.04 14.97 41.42 22.98 35.62 26.41 16.85 17.63 3.00 47.00 3.27 45.90 22.30 42.90 2.73 41.50 15.90 51.60 5.43 44.70 2.44 49.80 2.34 47.50 16.50 46.54 4.48 45.60 1.71 1.34 3.74 2.69 2.82 3.11 3.68 3.43 3.19 4.11 64.28 47.06 38.67 35.46 48.38 28.63 47.56 54.43 49.28 28.10 DyCAST-CA DyCAST-BP1 DyCAST-BP3 DyCAST-BP5 Non-autoregressive one-to-one architecture, defined only for DyCAST-CA. 9.03 49.54 7.34 51.61 7.42 50.00 11.50 50.23 0.92 1.12 0.57 0.40 14.4 17.5 9.0 6.2 13.05 19.98 27.80 40.27 3.42 11.57 14.56 15.85 4.20 3.91 4.00 4. 83.2 85.9 88.7 89.2 91.4 93.6 92.8 89.4 88.8 93.3 96.2 93.2 92.6 92.8 autoregressive architecture over speech tokens for TTS, reflecting the formulation of TTS as text-conditioned speech language modeling. The only exception is the DyCASTCA variant: thanks to the hard character-level alignment provided by the tokenizer, characters and speech tokens are in one-to-one correspondence. This enables the use of non-autoregressive TTS model that directly maps each input character to its corresponding speech token. Details on the model architecture, hyperparameters, and training procedure are provided in Appendix D.6. As shown in Table 4, DyCAST achieves strong performance across naturalness, intelligibility, and speaker similarity while operating at substantially reduced frame rates. Since the TTS model is autoregressive, lower frame rates directly translate into shorter sequences, yielding more favorable learning regime by reducing exposure bias and easing longrange dependency modeling. Notably, DyCAST-CA clearly stands out, achieving by far the best TTS performance across all metrics. This result is enabled by the non-autoregressive one-to-one architecture uniquely supported by DyCASTCA, which eliminates sequential token prediction altogether and is particularly effective in this limited-data regime. In addition, this architecture enables extremely fast inference, as generation is performed in single parallel pass and does not require sampling or rescoring multiple hypotheses. Overall, these results highlight the dual benefit of DyCAST: efficient, controllable, low-rate tokenization that simplifies autoregressive generation, and character-aligned representations that enable state-of-the-art quality and high inference efficiency in data-limited TTS scenarios."
        },
        {
            "title": "6 Conclusions\nWe introduced DyCAST, a speech codec that enables\nvariable-frame-rate tokenization through soft character-level\nalignment and explicit duration modeling. DyCAST pro-\nduces substantially shorter token sequences than fixed-\nframe-rate codecs while maintaining competitive speech\nresynthesis quality and downstream performance, resulting",
            "content": "8 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization in improved efficiency and more tractable transformer-based sequence modeling. key advantage of DyCAST is its flexibility, enabling content-adaptive frame rates and explicit control over duration, bitrate, and reconstruction quality. Combined with the proposed retrieval-augmented decoding strategy for improved resynthesis, DyCAST offers promising foundation for next-generation speech tokenization. References Ahia, O., Kumar, S., Gonen, H., Hofmann, V., Limisiewicz, T., Tsvetkov, Y., and Smith, N. A. MAGNET: Improving the multilingual fairness of language models with adaptive gradient-based tokenization. In International Conference on Neural Information Processing Systems (NeurIPS), volume 37, pp. 4779047814, 2024. Baas, M., van Niekerk, B., and Kamper, H. Voice conversion with just nearest neighbors. In Interspeech, pp. 2053 2057, 2023. Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: framework for self-supervised learning of speech representations. In International Conference on Neural Information Processing Systems (NeurIPS), pp. 12449 12460, 2020. Bai, H., Likhomanenko, T., Zhang, R., Gu, Z., Aldeneh, Z., and Jaitly, N. dMel: Speech tokenization made simple. arXiv preprint arXiv:2407.15835, 2024. Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi, M., and Zeghidour, N. AudioLM: language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 31:25232533, 2023. Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J. N., Lee, S., and Narayanan, S. S. IEMOCAP: Interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42(4): 335359, 2008. Chang, H.-J., Gong, H., Wang, C., Glass, J., and Chung, Y.-A. DC-Spin: speaker-invariant speech tokenizer for spoken language models. In Interspeech, pp. 57235727, 2025. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., Yu, X., and Wei, F. WavLM: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, pp. 15051518, 2022. Chen, S., Wang, C., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers. IEEE Transactions on Audio, Speech and Language Processing (TASLP), 33:705718, 2025. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. In International Conference on Neural Information Processing Systems (NeurIPS), volume 36, pp. 4770447720, 2023. Cosentino, J., Pariente, M., Cornell, S., Deleforge, A., and Vincent, E. LibriMix: An open-source dataset arXiv preprint for generalizable speech separation. arXiv:2005.11262, 2020. Cuervo, S., Łancucki, A., Marxer, R., Rychlikowski, P., and Chorowski, J. Variable-rate hierarchical CPC leads to acoustic unit discovery in speech. In International Conference on Neural Information Processing Systems (NeurIPS), volume 35, pp. 3499535006, 2022. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., et al. DeepSeekV3 technical report. arXiv preprint arXiv:2412.19437, 2025. Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression. Transactions on Machine Learning Research (TMLR), 2023. Della Libera, L., Subakan, C., and Ravanelli, M. Focal modulation networks for interpretable sound classification. In IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW), pp. 853857, 2024. Della Libera, L., Paissan, F., Subakan, C., and Ravanelli, M. FocalCodec: Low-bitrate speech coding via focal modulation networks. In International Conference on Neural Information Processing Systems (NeurIPS), 2025a. Della Libera, L., Subakan, C., and Ravanelli, M. Streaming low-bitrate speech arXiv preprint FocalCodec-Stream: coding via causal distillation. arXiv:2509.16195, 2025b. Dieleman, S., Nash, C., Engel, J., and Simonyan, K. arXiv Variable-rate discrete representation learning. preprint arXiv:2103.06089, 2021. 9 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Defossez, A., Mazare, L., Orsini, M., Royer, A., Perez, P., Jegou, H., Grave, E., and Zeghidour, N. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Jiang, X., Peng, X., Zhang, Y., and Lu, Y. Universal speech token learning via low-bitrate neural codec and pretrained representations. IEEE Journal of Selected Topics in Signal Processing, pp. 113, 2024b. Gong, Y., Jin, L., Deng, R., Zhang, D., Zhang, X., Cheng, Q., Fei, Z., Li, S., and Qiu, X. XY-Tokenizer: Mitigating the semantic-acoustic conflict in low-bitrate speech codecs. arXiv preprint arXiv:2506.23325, 2025. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Graves, A., Fernandez, S., Gomez, F., and Schmidhuber, J. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In International Conference on Machine Learning (ICML), pp. 369376, 2006. Guo, Y., Li, Z., Wang, H., Li, B., Shao, C., Zhang, H., Du, C., Chen, X., Liu, S., and Yu, K. Recent advances in discrete speech tokens: review. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025. Har-Tuv, N., Tal, O., and Adi, Y. PAST: Phonetic-acoustic speech tokenizer. In Interspeech, pp. 35093513, 2025. Hsu, M.-H., Tseng, L.-H., yi Lee, H., and Wu, Z. TASLA: Text-aligned speech tokens with multiple layeraggregation. arXiv preprint arXiv:2510.14934, 2025. Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. HuBERT: Selfsupervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:34513460, 2021. Hwang, S., Wang, B., and Gu, A. Dynamic chunking for end-to-end hierarchical sequence modeling. arXiv preprint arXiv:22507.07955, 2025. Ji, S., Jiang, Z., Wang, W., Chen, Y., Fang, M., Zuo, J., Yang, Q., Cheng, X., Wang, Z., Li, R., Zhang, Z., Yang, X., Huang, R., Jiang, Y., Chen, Q., Zheng, S., Wang, W., and Zhao, Z. WavTokenizer: An efficient acoustic discrete codec tokenizer for audio language modeling. In International Conference on Learning Representations (ICLR), 2025. Johnson, J., Douze, M., and Jegou, H. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7 (3):535547, 2019. Ju, Z., Wang, Y., Shen, K., Tan, X., Xin, D., Yang, D., Liu, Y., Leng, Y., Song, K., Tang, S., Wu, Z., Qin, T., Li, X.- Y., Ye, W., Zhang, S., Bian, J., He, L., Li, J., and Zhao, S. NaturalSpeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. In International Conference on Machine Learning (ICML), 2024. Kong, J., Kim, J., and Bae, J. HiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis. In International Conference on Neural Information Processing Systems (NeurIPS), 2020. Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Kumar, K. High-fidelity audio compression with improved RVQGAN. In International Conference on Neural Information Processing Systems (NeurIPS), 2023. Labiausse, T., Mazare, L., Grave, E., Defossez, A., and Zeghidour, N. High-fidelity simultaneous speech-tospeech translation. In International Conference on Machine Learning (ICML), 2025. Li, J., Lin, X., Li, Z., Huang, S., Wang, Y., Wang, C., Zhan, Z., and Wu, Z. DualCodec: low-frame-rate, semantically-enhanced neural audio codec for speech generation. In Interspeech 2025, pp. 48834887, 2025a. Li, J., Qian, Y., Hu, Y., Zhang, L., Wang, X., Lu, H., Thakker, M., Li, J., Zhao, S., and Wu, Z. FlexiCodec: dynamic neural audio codec for low frame rates. arXiv preprint arXiv:2510.00981, 2025b. Liu, H., Xu, X., Yuan, Y., Wu, M., Wang, W., and Plumbley, M. D. SemantiCodec: An ultra low bitrate semantic audio codec for general sound. IEEE Journal of Selected Topics in Signal Processing, 18(8):14481461, 2024. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations (ICLR), 2019. Lu, Y.-J., Gaur, Y., Zhou, W., Muller, B., Villalba, J., Dehak, N., Zettlemoyer, L., Ghosh, G., Lewis, M., Iyer, S., and Le, D. Latent speech-text transformer. arXiv preprint arXiv:2510.06195, 2025. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., and others, E. B. H. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a. Mentzer, F., Minnen, D., Agustsson, E., and Tschannen, M. Finite scalar quantization: VQ-VAE made simple. In International Conference on Learning Representations (ICLR), 2024. 10 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Messica, S. and Adi, Y. NAST: Noise aware speech tokenization for speech language models. In Interspeech, pp. 41694173, 2024. Mousavi, P., Della Libera, L., Duret, J., Ploujnikov, A., Subakan, C., and Ravanelli, M. DASB - discrete audio and speech benchmark. arXiv preprint arXiv:2406.14294, 2024a. Mousavi, P., Duret, J., Zaiem, S., Della Libera, L., Ploujnikov, A., Subakan, C., and Ravanelli, M. How should we extract discrete audio tokens from self-supervised models? In Interspeech, pp. 25542558, 2024b. Mousavi, P., Maimon, G., Moumen, A., Petermann, D., Shi, J., Wu, H., Yang, H., Kuznetsova, A., Ploujnikov, A., Marxer, R., Ramabhadran, B., Elizalde, B., Lugosch, L., Li, J., Subakan, C., Woodland, P., Kim, M., Lee, H.-y., Watanabe, S., Adi, Y., and Ravanelli, M. Discrete audio tokens: More than survey! Transactions on Machine Learning Research (TMLR), 2025. Nawrot, P., Chorowski, J., Lancucki, A., and Ponti, E. M. Efficient transformers with dynamic token pooling. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 64036417, 2023. Nguyen, T. A., Muller, B., Yu, B., Costa-jussa, M. R., Elbayad, M., Popuri, S., Ropers, C., Duquenne, P.-A., Algayres, R., Mavlyutov, R., Gat, I., Williamson, M., Synnaeve, G., Pino, J., Sagot, B., and Dupoux, E. SpiRit-LM: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics (TACL), 13:3052, 2025. Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J. E., Zettlemoyer, L., Ghosh, G., Lewis, M., Holtzman, A., and Iyer, S. Byte latent transformer: Patches scale better than tokens. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 92389258, 2025. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. LibriSpeech: An ASR corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 52065210, 2015. Parker, J. D., Smirnov, A., Pons, J., Carr, C., Zukowski, Z., Evans, Z., and Liu, X. Scaling transformers for lowbitrate high-quality speech coding. In International Conference on Learning Representations (ICLR), 2025. Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R. MLS: large-scale multilingual dataset for speech research. In Interspeech, pp. 27572761, 2020. Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S., Elkahky, A., Ni, Z., Vyas, A., Fazel-Zarandi, M., Baevski, A., Adi, Y., Zhang, X., Hsu, W.-N., Conneau, A., and Auli, M. Scaling speech technology to 1,000+ languages. Journal of Machine Learning Research (JMLR), 25, 2024. Radford, A., Kim, J. W., Xu, T., Brockman, G., Mcleavey, C., and Sutskever, I. Robust speech recognition via largeIn International Conference scale weak supervision. on Machine Learning (ICML), volume 202, pp. 28492 28518, 2023. Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.- W., Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na, H., Gao, Y., Mori, R. D., and Bengio, Y. SpeechBrain: general-purpose speech toolkit. arXiv preprint arXiv:2106.04624, 2021. Ravanelli, M., Parcollet, T., Moumen, A., de Langen, S., Subakan, C., Plantinga, P., Wang, Y., Mousavi, P., Della Libera, L., Ploujnikov, A., Paissan, F., Borra, D., Zaiem, S., Zhao, Z., Zhang, S., Karakasidis, G., Yeh, S.-L., Champion, P., Rouhe, A., Braun, R., Mai, F., ZuluagaGomez, J., Mousavi, S. M., Nautsch, A., Nguyen, H., Liu, X., Sagar, S., Duret, J., Mdhaffar, S., Laperri`ere, G., Rouvier, M., Mori, R. D., and Est`eve, Y. Open-source conversational AI with SpeechBrain 1.0. Journal of Machine Learning Research (JMLR), 25(333):111, 2024. Reddy, C. K., Gopal, V., and Cutler, R. DNSMOS P.835: non-intrusive perceptual objective speech quality metric In IEEE International to evaluate noise suppressors. Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. Ren, K., Qin, J., Zheng, L., Yang, Z., Zhang, W., Qiu, L., and Yu, Y. Deep recurrent survival analysis. In AAAI Conference on Artificial Intelligence, 2019. Rix, A., Beerends, J., Hollier, M., and Hekstra, A. Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 749752, 2001. Saeki, T., Xin, D., Nakata, W., Koriyama, T., Takamichi, S., and Saruwatari, H. UTMOS: UTokyo-SaruLab system for VoiceMOS challenge 2022. In Interspeech, pp. 4521 4525, 2022. Singer, J. D. and Willett, J. B. Its about time: Using discretetime survival analysis to study duration and the timing of events. Journal of Educational Statistics, 18(2):155195, 1993. 11 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. OpenAI GPT-5 system card. arXiv preprint arXiv:2601.03267, 2025. Siuzdak, H. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis. In International Conference on Learning Representations (ICLR), 2024. Slagle, K. SpaceByte: Towards deleting tokenization from large language modeling. In International Conference on Neural Information Processing Systems (NeurIPS), volume 37, pp. 124925124950, 2024. Song, Y., Chen, J., Zhuang, X., Du, C., Ma, Z., Wu, J., Cong, J., Jia, D., Chen, Z., Wang, Y., Wang, Y., and Chen, X. MagiCodec: Simple masked gaussian-injected codec for high-fidelity reconstruction and generation. arXiv preprint arXiv:2506.00385, 2025. Taal, C. H., Hendriks, R. C., Heusdens, R., and Jensen, J. An algorithm for intelligibility prediction of timefrequency IEEE Transactions on Audio, weighted noisy speech. Speech and Language Processing (TASLP), pp. 2125 2136, 2011. Tan, W., Inaguma, H., Dong, N., D. Tomasello, P., and Ma, X. SSR: Alignment-aware modality connector for speech language models. In Salesky, E., Federico, M., and Anastasopoulos, A. (eds.), International Conference on Spoken Language Translation (IWSLT 2025), pp. 56 75, 2025. Tian, J., Shi, J., Chen, W., Arora, S., Masuyama, Y., Maekaku, T., Wu, Y., Peng, J., Bharadwaj, S., Zhao, Y., Cornell, S., Peng, Y., Yue, X., Yang, C.-H. H., Neubig, G., and Watanabe, S. ESPnet-SpeechLM: An open speech language model toolkit. In Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL): Human Language Technologies (System Demonstrations), pp. 116124, 2025. Tseng, L.-H., Chen, Y.-C., Lee, K.-Y., Shiu, D.-S., and yi Lee, H. TASTE: Text-aligned speech tokenization and embedding for spoken language modeling. arXiv preprint arXiv:2504.07053, 2025. Valentini-Botinhao, C., Wang, X., Takaki, S., and Yamagishi, J. Investigating RNN-based speech enhancement methods for noise-robust text-to-speech. In Speech Synthesis Workshop, pp. 146152, 2016. Van Kuyk, S., Kleijn, W. B., and Hendriks, R. C. On the information rate of speech communication. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 56255629, 2017. Videau, M., Idrissi, B. Y., Leite, A., Schoenauer, M., Teytaud, O., and Lopez-Paz, D. From bytes to ideas: Language modeling with autoregressive U-Nets. In International Conference on Neural Information Processing Systems (NeurIPS), 2025. Wang, H., Guo, Y., Shao, C., Li, B., Chen, X., and Yu, K. CodecSlime: Temporal redundancy compression of neural speech codec via dynamic frame rate. arXiv preprint arXiv:2506.21074, 2025a. Wang, Y., Chen, D., Zhang, X., Zhang, J., Li, J., and Wu, Z. TaDiCodec: Text-aware diffusion speech tokenizer for speech language modeling. In International Conference on Neural Information Processing Systems (NeurIPS), 2025b. Wang, Z., Zhu, X., Zhang, Z., Lv, Y., Jiang, N., Zhao, G., and Xie, L. SELM: Speech enhancement using discrete tokens and language models. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1156111565, 2024. Wang, Z.-Q. et al. Sequential multi-frame neural beamforming for speech separation and enhancement. In IEEE Spoken Language Technology Workshop (SLT), pp. 905 911, 2021. Wichern, G., Antognini, J., Flynn, M., Zhu, L. R., McQuinn, E., Crow, D., Manilow, E., and Roux, J. L. WHAM!: Extending speech separation to noisy environments. In Interspeech, pp. 13681372, 2019. Wu, H., Kanda, N., Emre Eskimez, S., and Li, J. TS3-Codec: Transformer-based simple streaming single codec. In Interspeech, pp. 604608, 2025. Xin, D., Tan, X., Takamichi, S., and Saruwatari, H. BigCodec: Pushing the limits of low-bitrate neural speech codec. arXiv preprint arXiv:2409.05377, 2024. Yamagishi, J., Veaux, C., and MacDonald, K. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 6:15, 2017. Yang, D., Liu, S., Guo, H., Zhao, J., Wang, Y., Wang, H., Ju, Z., Liu, X., Chen, X., Tan, X., Wu, X., and Meng, H. M. ALMTokenizer: low-bitrate and semantic-rich audio codec tokenizer for audio language modeling. In International Conference on Machine Learning (ICML), 2025. Yang, J., Li, C., Dai, X., and Gao, J. Focal modulation networks. In International Conference on Neural Information Processing Systems (NeurIPS), 2022. Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Ye, Z., Zhu, X., Chan, C.-M., Wang, X., Tan, X., Lei, J., Peng, Y., Liu, H., Jin, Y., Dai, Z., Lin, H., Chen, J., Du, X., Xue, L., Chen, Y., Li, Z., Xie, L., Kong, Q., Guo, Y., and Xue, W. Llasa: Scaling train-time and inferencetime compute for Llama-based speech synthesis. arXiv preprint arXiv:2502.04128, 2025. Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. SoundStream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 495507, 2021. Zeghidour, N., Kharitonov, E., Orsini, M., Volhejn, V., de Marmiesse, G., Grave, E., Perez, P., Mazare, L., and Defossez, A. Streaming sequence-to-sequence learnarXiv preprint ing with delayed streams modeling. arXiv:2509.08753, 2025. Zen, H., Tokuda, K., and Black, A. W. Statistical parametric speech synthesis. Speech Communication, 51(11):1039 1064, 2009. Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. LibriTTS: corpus derived from LibriSpeech for text-to-speech. In Interspeech, 2019. Zhang, H., Guo, Y., Li, Z., Hao, X., Chen, X., and Yu, K. Unlocking temporal flexibility: Neural speech codec with variable frame rate. In Interspeech, pp. 50035007, 2025. Zhang, X., Zhang, D., Li, S., Zhou, Y., and Qiu, X. SpeechTokenizer: Unified speech tokenizer for speech large language models. In International Conference on Learning Representations (ICLR), 2024. Zhao, Y., Xiong, Y., and Krahenbuhl, P. Image and video tokenization with binary spherical quantization. In International Conference on Learning Representations (ICLR), 2025. Zheng, R.-C., Liu, W., Du, H.-P., Zhang, Q., Deng, C., Chen, Q., Wang, W., Ai, Y., and Ling, Z.-H. Say more with less: Variable-frame-rate speech tokenization via adaptive clustering and implicit duration coding. In AAAI Conference on Artificial Intelligence, 2026. Zheng, Y., Tu, W., Kang, Y., Chen, J., Zhang, Y., Xiao, L., Yang, Y., and Ma, L. FreeCodec: disentangled neural speech codec with fewer tokens. In Interspeech, pp. 48784882, 2025. Zobel, J. and Moffat, A. Inverted files for text search engines. ACM Computing Surveys (CSUR), 38(2):6es, 2006. 13 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Datasets The following datasets were used in this work: LibriSpeech (Panayotov et al., 2015) is large-scale corpus of English read speech derived from audiobooks in the LibriVox project. It contains approximately 1000 hours of speech sampled at 16 kHz, with predefined training, validation, and test splits. License: CC BY 4.0. LibriTTS (Zen et al., 2019) is corpus designed for text-to-speech research, constructed from the same source as LibriSpeech. It consists of 585 hours of transcribed speech with predefined training, validation, and test splits. License: CC BY 4.0. MLS (Pratap et al., 2020) is an extension of LibriSpeech to multiple languages, including English, German, Dutch, French, Spanish, Italian, Portuguese and Polish. It provides approximately 44,500 hours of transcribed English speech and about 6000 hours from other languages. License: CC BY 4.0. VoiceBank (Valentini-Botinhao et al., 2016) is dataset primarily used for speech enhancement, including 11,572 utterances from 28 speakers in the training set (noise at 0 dB, 5 dB, 10 dB, and 15 dB), and 872 utterances from 2 unseen speakers in the test set (noise at 2.5 dB, 7.5 dB, 12.5 dB, and 17.5 dB). License: CC BY 4.0. LibriMix (Cosentino et al., 2020) is dataset for speech separation and enhancement, created by mixing LibriSpeech utterances with noise from the WHAM! (Wichern et al., 2019) corpus. It provides mixtures of two or three speakers at different signal-to-noise ratios. License: MIT. VCTK (Yamagishi et al., 2017) is corpus of English speech recordings from 110 speakers with various accents. It is widely used for speaker adaptation, text-to-speech, and voice conversion tasks. License: CC BY 4.0. IEMOCAP (Busso et al., 2008) is dataset designed for emotion recognition, consisting of scripted and improvised dialogues performed by 10 actors. It includes audio, video, and textual transcriptions with emotion labels such as happiness, sadness, and anger. License: https://sail.usc.edu/iemocap/iemocap release.htm. Baselines We compare DyCAST against diverse set of fixed-frame-rate neural audio codecs covering general audio, speech-focused, singleand multi-codebook, acoustic, semantic, and hybrid (additional details are provided in Table 5): EnCodec (Defossez et al., 2023): general-purpose neural audio codec supporting both causal and non-causal operation, trained on large-scale heterogeneous audio data and widely used as strong baseline for speech and audio compression. DAC (Kumar et al., 2023): high-quality non-causal audio codec optimized for perceptual reconstruction, trained on diverse speech and music datasets and commonly used in downstream speech tasks. WavLM6-KM (Wang et al., 2024): speech codec obtained by clsutering WavLM representations from the 6-th transfomer layer. SpeechTokenizer (Zhang et al., 2024): multi-codebook speech tokenizer designed to disentangle content and speaker information across codebooks, primarily trained on LibriSpeech. SemantiCodec (Liu et al., 2024): general audio codec trained on large-scale multimodal datasets, explicitly targeting semantic preservation across speech and non-speech audio. Mimi (Defossez et al., 2024): large-scale causal speech tokenizer trained on millions of hours of speech, designed for streaming and speech language modeling applications. WavTokenizer (Ji et al., 2025): unified audio tokenizer trained on mixed speech and music data, supporting multilingual and multi-domain speech modeling. BigCodec (Xin et al., 2024): speech-focused codec trained on LibriSpeech, emphasizing efficient discrete representations for downstream speech generation and modeling tasks. Stable Codec (Parker et al., 2025): transformer-based neural speech codec trained on large-scale speech corpora, supporting optional causal inference. FocalCodec (Della Libera et al., 2025a): low-bitrate speech codec based on focal modulation and binary spherical quantization, serving as the closest architectural baseline to DyCAST. 14 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Table 5. Baseline codecs. Codec Training Datasets Hours Multilingual Audio Domain Checkpoint License EnCodec (Defossez et al., 2023) DNS, CommonVoice, AudioSet, FSD50K, Jamendo 17k+ DAC (Kumar et al., 2023) DAPS, DNS, CommonVoice, VCTK, MUSDB, Jamendo 10k+ WavLM6-KM (Wang et al., 2024) Subset of LibriSpeech (in addition to Libri-Light, GigaSpeech, and VoxPopuli English for WavLM pretraining) 460 (+ 94k) SpeechTokenizer (Zhang et al., 2024) LibriSpeech SemantiCodec (Liu et al., 2024) GigaSpeech, subset of OpenSLR, Million Song Dataset, MedleyDB, MUSDB18, AudioSet, WavCaps, VGGSound 960 20k+ Yes Yes No No Yes General General encodec 24khz weights 16khz.pth MIT MIT Speech discrete-wavlm-codec Apache 2.0 Speech speechtokenizer hubert avg Apache 2.0 General semanticodec tokenrate 50 MIT Predominantly English speech (in addition to Libri-Light, GigaSpeech, and VoxPopuli English for WavLM pretraining) 7M (+ 94k) Likely Speech mimi CC BY 4.0 Mimi (Defossez et al., 2024) WavTokenizer (Ji et al., 2025) LibriTTS, VCTK, subset of CommonVoice, subset of AudioSet, Jamendo, MUSDB BigCodec (Xin et al., 2024) LibriSpeech Stable Codec (Parker et al., 2025) Libri-Light, MLS English FocalCodec (Della Libera et al., 2025a) LibriTTS (in addition to Libri-Light, GigaSpeech, and VoxPopuli English for WavLM pretraining) 585 (+ 94k) 8k 960 105k Yes No No No General WavTokenizer-large-unify-40token MIT Speech Speech Speech bigcodec.pt MIT stable-codec-speech-16k StabilityAI focalcodec 50hz Apache 2.0 Hyperparameters and Training Details We summarize the main hyperparameters and training settings used for all DyCAST components. All learnable modules are trained on LibriTTS (Zen et al., 2019) (585 hours from 2456 speakers) using full utterances and identical optimizer settings, except for the decoder, which requires different training setup. For all other modules, training is performed using the AdamW (Loshchilov & Hutter, 2019) optimizer with an initial learning rate of 0.0005, β1 = 0.8, β2 = 0.99, and weight decay of 0.01. The learning rate is reduced by factor of 0.9 when the validation loss does not improve by at least 0.0025. Gradients are clipped to maximum L2 norm of 5, and training stops when the validation loss fails to decrease for several consecutive epochs. Character Aligner. As character aligner, we use MMS (Pratap et al., 2024), 1B-parameter multilingual wav2vec2.0 ASR model pretrained on approximately 500k hours of speech across more than 1,000 languages and equipped with CTC-based character prediction head. The model is kept frozen throughout training and is used to obtain character-level durations via simple argmax decoding. While more sophisticated alignment strategies such as Viterbi decoding (Graves et al., 2006) are possible, we adopt argmax decoding for its simplicity and efficiency. Encoder. For the encoder, we use the pretrained WavLM-large2 model (Chen et al., 2022). In particular, we extract representations from the 6th transformer layer, which provide semantically rich features while retaining fine-grained acoustic information (Della Libera et al., 2025a; Baas et al., 2023). The encoder is kept frozen throughout training. Compressor. The compressor takes as input 1024-dimensional WavLM features and processes them through 3 focal downscaling blocks (Della Libera et al., 2025a) while preserving the original temporal resolution (50 Hz), each with hidden dimension of 1024. Each block uses 2 focal levels, window size of 14, focal factor of 4, and layer scale initialization of 0.0001. final projection maps the 1024-dimensional hidden states to 32-dimensional latent representations. Chunker. Within the chunker, the boundary predictor adopts the same architecture as the compressor. final binary classification head maps the 1024-dimensional hidden states to boundary logits. The dynamic downsampling operation itself is implemented as simple frame-selection and does not introduce additional trainable parameters. Quantizer. The scalar spherical quantizer discretizes the continuous 32-dimensional latent representations using 4 levels per dimension, resulting in an implicit codebook of size = 432. In practice, it is infeasible to enumerate all codes explicitly; instead, we represent tokens by returning per-dimension indices in {0, 1, 2, 3}. The quantizer has no trainable parameters, and we set the entropy loss weight to 0.1. Dechunker. Within the dechunker, the duration predictor follows the same architectural design but processes 32dimensional pooled quantized latents. The model predicts free mean parameter (a scalar value) together with shared, learnable scalar dispersion parameter α. Dynamic upsampling is implemented by frame repetition and introduces no additional parameters; the length regularization strength is set to λ = 0.05. 2https://github.com/microsoft/unilm/tree/master/wavlm 15 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization Decompressor. The decompressor mirrors the compressor architecture, replacing focal downscaling blocks with focal upscaling blocks to reconstruct 1024-dimensional continuous representations from the quantized latent codes. Decoder. The Vocos (Siuzdak, 2024) decoder operates on 1024-dimensional WavLM features and processes them through 8 ConvNeXt blocks with hidden dimension of 512, feed-forward dimension of 1536, kernel size of 7, and padding of 3. For the STFT, we use an FFT size of 1024 samples and hop length of 320. The feature-matching loss is computed using 80-dimensional log-Mel spectrograms with the same STFT configuration. The discriminator follows the convolutional architecture introduced in (Kong et al., 2020). Training is conducted on LibriTTS train-clean-100 using audio chunks of 7040 samples and batch size of 16. We use the AdamW optimizer with an initial learning rate of 0.0002, β1 of 0.8, β2 of 0.99, and weight decay of 0.01. The learning rate follows an exponential decay schedule with factor of 0.999. Training continues until perceived audio quality saturates, which occurs after approximately 3M steps. Downstream Evaluation In this section, we outline the downstream tasks used to evaluate the proposed codec and describe the corresponding experimental setups. Unless otherwise stated, all downstream models are trained using the AdamW optimizer (Loshchilov & Hutter, 2019) with batch size of 16, an initial learning rate of 0.0001, β1 = 0.8, β2 = 0.99, weight decay of 0.01, and dropout of 0.1. The learning rate is reduced by factor of 0.9 if validation loss does not improve within margin of 0.0025. Gradients are clipped to maximum L2 norm of 0.01. Training stops if validation loss does not decrease for several consecutive epochs. Each model is trained on single NVIDIA H100 GPU with 80 GB of memory. Software for the downstream evaluation was implemented in Python using the SpeechBrain (Ravanelli et al., 2021; 2024) toolkit. D.1 Speech Resynthesis (SR) This task evaluates the ability of the codec to reconstruct high-quality speech from discrete tokens while preserving naturalness, speaker identity, and intelligibility. For English, we use the LibriSpeech (Panayotov et al., 2015) test-clean split. For multilingual evaluation, we use the dataset constructed by Della Libera et al. (2025a), which was built by selecting 100 utterances from each of the seven non-English languages in MLS (Pratap et al., 2020) (Dutch, French, German, Italian, Polish, Portuguese, and Spanish) for total of 700 utterances3. To assess robustness to environmental noise, we additionally evaluate on the test sets of VoiceBank (Valentini-Botinhao et al., 2016) and Libri1Mix, constructed by mixing clean utterances from the first speaker of LibriMix (Cosentino et al., 2020) with noise from WHAM! (Wichern et al., 2019). Naturalness is measured using UTMOS (Saeki et al., 2022) for clean speech and DNSMOS (Reddy et al., 2022) for noisy speech. Intelligibility is evaluated using differential word error rate (dWER) (Wang et al., 2021), computed from transcriptions obtained with Whisper-small (Radford et al., 2023)4. Speaker fidelity is assessed via cosine similarity (Sim) between speaker embeddings extracted using WavLM-base-SV (Chen et al., 2022)5. We do not report signal-level metrics such as SNR, PESQ (Rix et al., 2001), or STOI (Taal et al., 2011), as they correlate poorly with perceived reconstruction quality (Parker et al., 2025; Wang et al., 2024; Della Libera et al., 2025a), and we avoid stronger ASR models to prevent masking reconstruction artifacts. D.2 Voice Conversion (VC) This task evaluates the ability of the codec to disentangle speaker identity from linguistic content despite its single-codebook design. Voice conversion is performed by converting speech from source speaker to target speaker using reference speech from the target speaker. For single-codebook baselines, including DyCAST, we follow the k-nearest neighbors (kNN) approach of Baas et al. (2023). Specifically, each frame in the reconstructed feature sequence (immediately before the decoder) is replaced by the average of the = 4 nearest neighbors (measured by cosine similarity) from continuous features extracted from the target speaker reference utterance. For multi-codebook baselines, we follow the procedure of Zhang et al. (2024). Both source and reference speech are tokenized, and voice conversion is performed by concatenating the first codebook tokens from the source with the second-to-last codebook tokens from the reference, before decoding. If sequence lengths differ, the reference sequence is truncated or circularly padded as needed. Effective disentanglement of content and speaker information across codebooks is expected to yield strong voice conversion performance. We conduct voice conversion experiments on VCTK (Yamagishi et al., 2017), which contains parallel utterances from multiple speakers. For each sample, we randomly select (i) an utterance from source speaker, (ii) the corresponding parallel utterance from target speaker, and (iii) different utterance from the same target speaker to serve as the reference. Among candidate 3https://zenodo.org/records/14791114 4https://huggingface.co/openai/whisper-small 5https://huggingface.co/microsoft/wavlm-base-sv Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization reference utterances, we select the longest to minimize padding. Repeating this procedure for each speaker and each of the approximately 24 parallel utterances yields total of 2,521 evaluation samples. Performance is evaluated using the same metrics as in speech resynthesis: UTMOS, dWER, and Sim. D.3 Automatic Speech Recognition (ASR) This task evaluates the quality of the learned discrete representations through probing setup, where shallow model is trained on top of fixed codec tokens to assess their linguistic content. We use LibriSpeech (Panayotov et al., 2015) train-clean-100 and train-clean-360 for training, dev-clean for validation, and test-clean for evaluation. The ASR probe consists of 2-layer bidirectional LSTM with 512-dimensional hidden states, followed by CTC (Graves et al., 2006) prediction head. The model is trained to predict either characters or byte-pair encoding (BPE) units, using BPE vocabularies of size 1000, except for Mimi, which achieves its best performance with vocabulary size of 500. When codec employs multiple codebooks, we compute learned weighted sum of the embeddings from each codebook, following (Chen et al., 2022). The embedding layer is initialized using the discrete embeddings from the codec quantizer. Performance is reported in terms of word error rate (WER). D.4 Speaker Identification (SI) This task probes whether the learned discrete representations encode speaker identity through probing setup, where shallow model is trained on top of fixed codec tokens to assess their acoustic content. We use LibriSpeech (Panayotov et al., 2015), grouping utterances from train-clean-100 and train-clean-360 by speaker ID. The data are randomly split into training, validation, and test sets with ratio of 80% / 10% / 10%. The SI probe closely mirrors the ASR setup. 2-layer bidirectional LSTM with 512-dimensional hidden states processes the token sequence, and the output is aggregated using statistics pooling, followed by cross-entropy classification head. When multiple codebooks are present, embeddings are combined via learned weighted sum, as in ASR. Performance is reported in terms of speaker error rate (ER). D.5 Speech Emotion Recognition (SER) This task probes whether the learned representations capture paralinguistic information related to emotion. We again adopt probing setup with shallow classifier trained on top of frozen codec tokens. We use the IEMOCAP dataset (Busso et al., 2008), focusing on four emotion classes: sadness, happiness, anger, and neutral. Sessions 14 are used for training, session 5F for validation, and session 5M for testing. The SER setup is identical to SI, with the only difference being the number of output classes in the classification head. Performance is reported in terms of emotion error rate (ER). D.6 Text-To-Speech (TTS) This task evaluates the suitability of discrete speech representations for speech generation from text. We use LibriSpeech train-clean-100 and train-clean-360 for training, dev-clean for validation, and test-clean for evaluation. The test-clean split contains small fraction of long utterances (approximately 4%) exceeding 20 seconds, which are largely absent from the training data; to reduce this train-test mismatch, we remove these long utterances from the test set. The model input consists of character-level text tokens, while the target sequence comprises speech tokens extracted from the corresponding utterances. We consider two TTS model variants. An autoregressive model is used for all codecs except DyCAST-CA, while non-autoregressive model is used exclusively for DyCAST-CA to exploit the availability of character-aligned tokens. Performance is evaluated using UTMOS, dWER, and Sim. Autoregressive TTS. The autoregressive model is Llama 3 decoder (Grattafiori et al., 2024) with 12 layers, 4 attention heads, 1 key-value head, model dimension of 512, feed-forward dimension of 2048, and base RoPE frequency of 10,000. Speaker identity is provided by extracting embeddings from the target utterance using WavLM-base (Chen et al., 2022), fine-tuned for speaker verification. The pooled speaker embedding is prepended to the character embeddings to condition the model on speaker identity. The embedding layer is initialized using the discrete embeddings from the codec quantizer. Training is performed using next-token prediction, where the input sequence consists of the pooled speaker embedding, character embeddings, and speech token embeddings. The cross-entropy loss is computed only over speech tokens, while character and speaker embeddings are excluded from the loss. If the codec has multiple codebooks, we flatten tokens across the codebook dimension, as this approach has been shown to provide the best downstream performance given sufficiently high computational budget (Copet et al., 2023). At inference time, we use top-p sampling with β1 = 0.9 and temperature of 1.0. Following Tian et al. (2025); Della Libera et al. (2025a), we generate five samples per utterance and select the one with the lowest word error rate relative to the input text, computed using Whisper-small (Radford et al., 2023). Non-autoregressive TTS. The non-autoregressive model employs the same Llama 3 architecture, where causal selfattention is replaced with bidirectional self-attention, while all other components remain unchanged. The pooled speaker 17 Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization embedding is replicated for the number of character positions and summed with the hidden representations at each layer to condition the model on speaker identity. Training is performed using per-character target prediction with cross-entropy loss at each step. At inference time, decoding is performed using argmax. Additional Results E.1 Decoding Mode Table 6 analyzes the impact of different decoding modes on LibriSpeech speech resynthesis. Overall, explicitly providing duration information consistently improves intelligibility, while naturalness and speaker similarity remain largely stable across decoding modes. For all DyCAST variants, tokens + durations yields the lowest dWER, confirming that explicit duration supervision enables more accurate temporal reconstruction and reduces linguistic distortions. This effect is particularly pronounced for predicted-duration variants, where dWER increases moderately as duration information is removed, highlighting the importance of duration modeling at low token rates. In contrast, tokens only decoding achieves comparable or slightly higher UTMOS across all variants, indicating that naturalness is largely preserved even without explicit timing cues. However, this comes at the cost of degraded intelligibility, especially for aggressive compression regimes, where the absence of duration constraints leads to temporal misalignment and increased recognition errors. The intermediate tokens + utterance length mode offers favorable trade-off, recovering most of the intelligibility gains of full duration decoding while requiring only global length constraint. Notably, this mode performs robustly across all variants and is used as the default setting in our experiments unless otherwise stated. Overall, these results show that DyCAST enables flexible decoding strategies with explicit control over the trade-off between timing accuracy, intelligibility, and decoding complexity, further emphasizing the practical advantages of variable-rate tokenization with explicit duration modeling. Table 6. Effect of different decoding modes on LibriSpeech resynthesis."
        },
        {
            "title": "Decoding Mode",
            "content": "UTMOS dWER Sim DyCAST-CA DyCAST-BP1 DyCAST-BP3 DyCAST-BP5 Tokens + durations Tokens + utterance length Tokens only Tokens + durations Tokens + utterance length Tokens only Tokens + durations Tokens + utterance length Tokens only Tokens + durations Tokens + utterance length Tokens only 3.85 3.99 4.03 3.94 3.92 3.93 3.95 3.95 3. 3.93 3.97 3.98 3.07 3.32 3.41 2.37 3.61 4.49 2.99 4.66 5.13 5.12 8.84 8.67 97.5 97.4 97. 97.7 97.6 97.6 97.4 97.2 97.2 96.6 96.5 96.5 E.2 Qualitative Analysis of DyCAST Chunk Boundaries Figure 3 provides qualitative spectrogram visualizations of DyCAST chunk boundaries for the four variants considered in this work: DyCAST-CA, DyCAST-BP1, DyCAST-BP3, and DyCAST-BP5. DyCAST-CA relies on boundaries provided by the character aligner, while the remaining variants use the boundary predictor with min gap set to 1, 3, and 5, respectively, resulting in average frame rates of approximately 14 Hz, 17 Hz, 9 Hz, and 6 Hz. As the frame rate decreases, chunk boundaries become less frequent and more widely spaced, reflecting longer token durations and coarser temporal resolution. DyCAST-CA DyCAST-BP DyCAST-BP3 DyCAST-BP5 Figure 3. DyCAST chunk boundaries at average frame rates of approximately 14 Hz, 17 Hz, 9 Hz, and 6 Hz (from left to right)."
        }
    ],
    "affiliations": [
        "Concordia University, Montreal",
        "Mila Quebec AI Institute, Montreal, Canada",
        "Universite Laval, Quebec, Canada"
    ]
}