{
    "paper_title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning",
    "authors": [
        "Zhaoyang Chu",
        "Yao Wan",
        "Zhikun Zhang",
        "Di Wang",
        "Zhou Yang",
        "Hongyu Zhang",
        "Pan Zhou",
        "Xuanhua Shi",
        "Hai Jin",
        "David Lo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks such as code generation and summarization, recent empirical studies reveal a critical privacy vulnerability: these models exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information when specifically prompted. To address this issue, several approaches, including training data de-duplication and differential privacy augmentation, have been proposed. However, these methods require full-model retraining for deployed CLMs, which incurs substantial computational costs. In this paper, we aim to answer the following research question: Can sensitive information memorized by CLMs be erased effectively and efficiently? We conduct a pioneering investigation into erasing sensitive memorization in CLMs through machine unlearning - a post-hoc modification method that removes specific information from trained models without requiring full retraining. Specifically, we first quantify the memorization risks of sensitive data within CLM training datasets and curate a high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We study two widely used gradient ascent-based unlearning approaches: the vanilla and constraint-based methods, and introduce CodeEraser, an advanced variant that selectively unlearns sensitive memorized segments in code while preserving the structural integrity and functional correctness of the surrounding code. Extensive experiments on three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder, validate the effectiveness and efficiency of CodeEraser in erasing targeted sensitive memorization while maintaining model utility."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 5 5 7 3 1 . 9 0 5 2 : r Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning Zhaoyang Chu Huazhong University of Science and Technology Wuhan, China chuzhaoyang@hust.edu.cn Yao Wan Huazhong University of Science and Technology Wuhan, China wanyao@hust.edu.cn Di Wang King Abdullah University of Science and Technology Thuwal, Saudi Arabia di.wang@kaust.edu.sa Zhou Yang University of Alberta Edmonton, Canada zy25@ualberta.ca Zhikun Zhang Zhejiang University Hangzhou, China zhikun@zju.edu.cn Hongyu Zhang Chongqing University Chongqing, China hyzhang@cqu.edu.cn Pan Zhou Huazhong University of Science and Technology Wuhan, China panzhou@hust.edu.cn Xuanhua Shi Huazhong University of Science and Technology Wuhan, China xhshi@hust.edu.cn Hai Jin Huazhong University of Science and Technology Wuhan, China hjin@hust.edu.cn David Lo Singapore Management University Singapore, Singapore davidlo@smu.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks such as code generation and summarization, recent empirical studies reveal critical privacy vulnerability: these models exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information when specifically prompted. To address this issue, several approaches, including training data de-duplication and differential privacy augmentation, have been proposed. However, these methods require full-model retraining for deployed CLMs, which incurs substantial computational costs. In this paper, we aim to answer the following research question: Can sensitive information memorized by CLMs be erased effectively and efficiently? We conduct pioneering investigation into erasing sensitive memorization in CLMs through machine unlearninga post-hoc modification method that removes specific information from trained Also with National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China. Yao Wan is the corresponding author. models without requiring full retraining. Specifically, we first quantify the memorization risks of sensitive data within CLM training datasets and curate high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We study two widely used gradient ascent-based unlearning approaches: the vanilla and constraintbased methods, and introduce CodeEraser, an advanced variant that selectively unlearns sensitive memorized segments in code while preserving the structural integrity and functional correctness of the surrounding code. Extensive experiments on three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder, validate the effectiveness and efficiency of CodeEraser in erasing targeted sensitive memorization while maintaining model utility. CCS Concepts Software and its engineering Software development techniques; Security and privacy;"
        },
        {
            "title": "Keywords",
            "content": "Code Language Models, Code Generation, Privacy, Sensitive Memorization, Machine Unlearning Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICSE 26, Rio de Janeiro, Brazil 2026 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2025-3/26/04 https://doi.org/10.1145/3744916.3764573 ACM Reference Format: Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, and David Lo. 2026. Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning. In 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE 26), April 1218, 2026, Rio de Janeiro, Brazil. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3744916.3764573 ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, and David Lo"
        },
        {
            "title": "1 Introduction",
            "content": "Recently, Code Language Models (CLMs), such as CodeGen [50], Code Llama [54], and Qwen2.5-Coder [34], have demonstrated significant potential in automating various aspects of software engineering, including code generation [22, 38, 63], code summarization [2, 5, 62], program repair [65, 66], and type inference [44]. Their success can be attributed to pre-training with autoregressive language modeling on large-scale code corpora [60, 61], where the model predicts the next token given sequence of previous tokens. However, studies have shown that the current pre-training paradigm may retain sensitive data, e.g., emails and passwords, encountered during the training phase [3, 15, 17, 51, 68]. This retention occurs because CLMs, trained on vast amounts of code collected from GitHub repositories, may inadvertently memorize sensitive data embedded within these repositories, including personally identifiable information (e.g., names, emails, and phone numbers) and private authentication credentials (e.g., passwords, API keys, and cryptographic secrets) [7, 27, 33, 46, 51, 68]. From another perspective, to strengthen individual control over personal data, global legislative frameworks such as the European Unions General Data Protection Regulation (GDPR) [57] and the California Consumer Privacy Act (CCPA) [52] have established the Right to Be Forgotten (RTBF) [58, 59]. These regulations empower individuals to request the deletion of their personal data, providing critical safeguard for privacy protection. To mitigate the privacy risks posed by potential data leaks in CLMs and ensure compliance with the RTBF, we investigate the following question: Can sensitive information memorized by CLMs be erased effectively and efficiently? Intuitive Approaches and Limitations. Our investigation reveals two distinct research directions. The first focuses on training data de-duplication, as illustrated in Figure 1 (a). Prior studies [15, 39, 43] have demonstrated that de-duplication can mitigate the memorization tendencies of LMs. However, experimental evidence indicates that LMs still retain substantial memorization capabilities even under this paradigm [8, 9, 37]. Another line of research falls into Differential Privacy (DP) [1, 74], as shown in Figure 1 (b). This approach enforces the formal guarantee that the addition or removal of any training data point does not substantially affect the final model [73], thereby providing formal privacy guarantees for individual training samples. However, DP-based training fundamentally limits LMs ability to capture longtail patterns in data distributions, resulting in significant utility degradation [4, 25, 26]. Furthermore, both DP and de-duplication methods are typically applied during the initial training phase. For already deployed CLMs, these methods lack the ability to selectively remove specific data as requested by users, often necessitating retraining the entire model [18, 37, 48]. Such retraining is costly and time-consuming, especially given the escalating scale of contemporary CLMs. This limitation prevents their flexibility in addressing dynamic user requests and evolving privacy demands in real-world scenarios. Our Work: Machine Unlearning Perspective. More recently, machine unlearning has emerged as promising alternative for LMs, as shown in Figure 1 (c), which seeks to remove specific information by post-hoc modifying the trained model [11, 14, 18, 30, 37]. Existing approaches typically employ gradient ascent to reverse the learning of specific data, thus proactively removing its influence [18, 37]. Compared to DP and de-duplication techniques, machine unlearning enables LMs to quickly forget certain information with just few parameter updates without full retraining, thereby reducing the training time from 9001800 to 0.001 A100 GPU days [37]. Nevertheless, we argue that these approaches often indiscriminately forget entire text instances rather than selectively targeting specific sensitive information. As result, they struggle to erase sensitive segments (e.g., API key strings) embedded in code without disrupting the structural integrity and functional correctness of the surrounding code. In this paper, we perform pioneering investigation into sensitive memorization erasure 1 in CLMs through machine unlearning. Specifically, we first quantify the memorization risks of sensitive data within CLM training corpora and curate high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We investigate two widely used gradient ascent-based unlearning approaches: the vanilla method and the constraint-based method, and further develop an advanced variant, termed CodeEraser, which selectively unlearns sensitive memorized segments in code while preserving the surrounding codes integrity and functionality. To assess the effectiveness and efficiency of CodeEraser, we conduct extensive experiments on three widely used suites of CLMs, i.e., CodeParrot [24], CodeGen-Mono [50], and Qwen2.5-Coder [34]. The results demonstrate CodeErasers ability to effectively and efficiently mitigate the memorization issue in CLMs, thus protecting sensitive data against potential extraction attacks. Using the Qwen2.5-Coder-7B model as an example, CodeEraser successfully reduces memorization by 93.89% on the targeted forgotten set (sampled from the sensitive memorization dataset), while retaining 99.00% of the models original performance, with an average processing time of 46.88 seconds per sample. Contributions. The key contributions of this paper are as follows. New Problem and Dataset. To the best of our knowledge, we are the first to formulate the problem of erasing sensitive memorization within CLMs. As an initial step, we curate sensitive memorization dataset to support further research in this area. Pioneering Study. We conduct the first comprehensive study on sensitive memorization erasure in CLMs via machine unlearning. We also introduce selective gradient-ascent approach CodeEraser to target and remove sensitive memorized segments while preserving code integrity. Extensive Evaluation. We conduct comprehensive experiments on three widely used families of CLMs, namely CodeParrot [24], CodeGen-Mono [50], and Qwen2.5-Coder [34]. The results demonstrate the effectiveness and efficiency of CodeEraser in erasing sensitive memorization within CLMs while maintaining acceptable levels of model utility."
        },
        {
            "title": "2 Background",
            "content": "We begin by introducing the background of LMs, followed by formal definition of memorization in these models. 1We adopt the term sensitive memorization to denote the phenomenon where CLMs retain and reproduce sensitive training data (e.g., API keys). Thus, by memorization erasure, we mean techniques designed to remove such retained sensitive content. Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Figure 1: An illustration of existing methods, i.e., (a) data de-duplication, (b) differential privacy, and (c) machine unlearning, to mitigate the memorization of sensitive information in CLMs. We mask the personal details for ethical considerations."
        },
        {
            "title": "2.2 Memorization in Language Models",
            "content": "Language Models (LMs) are designed to predict the probability of token sequence by utilizing the empirical distribution of token occurrences in the training data. One widely adopted unsupervised approach for training LMs is autoregressive language modeling, also known as next-token prediction, where the model sequentially predicts tokens from left to right [12, 47, 53]. Autoregressive Language Modeling. Given token sequence = (洧논1, 洧논2, . . . , 洧논洧녜 ), the LM employs the chain rule to model its joint probability as the product of conditional probabilities: Pr(洧논1, 洧논2, . . . , 洧논洧녜 ) = 洧녜 (cid:214) Pr(洧논洧녰 洧논1, . . . , 洧논洧녰 1) . (1) 洧녰=1 In this paradigm, neural network model 洧녭 , parameterized by 洧랚 , is typically employed to estimate the likelihood of each token 洧논洧녰 conditioned on preceding tokens, denoted as 洧녭洧랚 (洧논洧녰 洧논1, . . . , 洧논洧녰 1). The parameters 洧랚 are optimized by maximizing the probability of each sample within the training dataset D. This is achieved by minimizing the loss function as follows: L洧洧 (x) = log 洧녜 (cid:214) 洧녰=1 洧녭洧랚 (洧논洧녰 洧논1, . . . , 洧논洧녰 1) . (2) Model Inference via Prefix Prompt. Once the LM 洧녭洧랚 is trained, it can generate outputs based on given prefix prompt during inference. Given prefix prompt 洧녷 = (洧논1, . . . , 洧논洧녰 1), the trained LM iteratively predicts the next tokens to complete the suffix 洧. Specifically, the model samples 틙洧논洧녰 洧녭洧랚 (洧논洧녰 洧논1, . . . , 洧논洧녰 1) and subsequently feeds 틙洧논洧녰 back into the model to sample 틙洧논洧녰+1 洧녭洧랚 (洧논洧녰 洧논1, . . . , 틙洧논洧녰 ), iteratively. Each newly generated token is conditioned on both the initial prompt and all previously generated tokens. This decoding process is repeated until termination condition is met, e.g., generating special token </s>, indicating the end of the sequence, or reaching predefined maximum length of the token sequence. Memorization, often seen as the antithesis of generalization, arises from overfitting, causing models to retain specific details of their training data [3, 25]. This phenomenon raises remarkable privacy concerns in the context of LMs, as these models may inadvertently memorize sensitive information and regurgitate it verbatim in response to certain prompts. Extensive research has been undertaken to qualitatively and quantitatively examine memorization in LMs [3, 1517, 33, 35, 49, 51, 68]. Following these prior studies, we define memorization in LMs grounded in the extractability of training data. In particular, we conceptualize memorization as models ability to store and reproduce exact pieces of information encountered during training. Definition 1 (Verbatim Memorization). string 洧 is considered memorized by an LM 洧녭洧랚 if there exists prefix 洧녷 such that: 洧 = arg max 틙洧 洧녭洧랚 ( 틙洧 洧녷) [洧녷 洧] . (3) Here, 洧녭洧랚 ( 틙洧 洧녷) denotes the models likelihood of generating an entire sequence 틙洧 given the prefix 洧녷, [洧녷 洧] represents the concatenation of strings 洧녷 and 洧, and denotes the training dataset of 洧녭洧랚 . The arg max operation can be replaced by an appropriate decoding strategy (e.g., greedy sampling) to determine the models outputs in practical applications. Example 1. Assume that the LMs training dataset contains sequence # Copyright (C) [2003] Daniel <daniel@gmail.com>. If the and model is prompted with # Copyright (C) [2003] Daniel the most likely continuation is <daniel@gmail.com>, then the generated string is deemed memorized."
        },
        {
            "title": "3 Preliminary Study",
            "content": "We first conduct preliminary study to quantitatively examine the presence and severity of sensitive memorization in CLMs. ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, and David Lo Table 1: toy example to illustrate the calculation process of MA and EL洧녵 with 洧녵 = 3."
        },
        {
            "title": "3.1 Study Subjects",
            "content": "Studied CLMs. To systematically analyze model memorization, we select representative CLMs varying in size and architecture. Following [3, 68], we examine four widely used models: CodeParrot-small (110M), CodeParrot (1.5B) [24], and CodeGen-{350M, 2B}-Mono[50]. Our analysis also includes Qwen2.5-Coder-7B [34], state-of-theart CLM with over 35.3k monthly downloads on HuggingFace at the time of writing. All selected models are accessible on HuggingFace Hub, enabling ethical and reproducible memorization analysis. Studied Datasets. We utilize codeparrot-clean-train [21], 50GB dataset comprising 5 million Python files. We select it for two key reasons: (1) It is high-quality, cleaned subset of GitHub corpora, extracted via Googles BigQuery [31], making it representative of standard CLM training data. (2) It offers the repository source for each code instance, enabling realistic unlearning simulations where users request the removal of sensitive data unknowingly included in their repositories. These make it ideal for standardized memorization analysis and unlearning evaluations across CLMs."
        },
        {
            "title": "3.2 Memorization Quantification",
            "content": "3.2.1 Memorization Metrics. To accurately assess whether and to what extent CLMs retain specific data, our analysis adopts two quantitative metrics: Memorization Accuracy (MA) [56] and Extraction Likelihood (EL) [37]. As illustrated in Table 1, these metrics measure memorization by comparing the CLMs generation with the true continuation, at the token and 洧녵-gram levels, respectively. Memorization Accuracy (MA) [56]. Given specific token sequence = (洧논1, 洧논2, . . . , 洧논洧녜 ), we let the CLM 洧녭洧랚 sequentially process this sequence from left to right and predict each token based on its preceding context. Then, MA calculates the accuracy of these predictions by comparing the generated tokens with their corresponding actual tokens in the sequence x: (cid:205)洧녜 洧녭洧랚 ( 틙洧논洧녰 洧논<洧녰 ) = 洧논洧녰 } 洧녰=2 1{arg max 틙洧논洧녰 , (4) MA(x) = 洧녜 1 accurately matches the actual token 洧논洧녰 ) and 0 otherwise, and the notion 洧논<洧녰 denotes the sequence of all tokens before position 洧녰. Example 2. As shown in columns 1-3 of Table 1, given various prefixes 洧논<洧녰 , 틙洧논洧녰 matches 洧논洧녰 with 6 times (marked in green ), resulting in an MA score of 0.5455. We can see that MA measures the proportion of tokens in sequence that the CLM can recall exactly, reflecting its capacity to memorize and reproduce training data. Extraction Likelihood (EL) [37]. Compared with MA computed at the token level, EL enables stricter standard for quantifying memorization by assessing the extent to which the generated sequence 틙洧논 洧녰 matches the true continuation 洧논 洧녰 at the 洧녵-gram level: (cid:205)洧녜 洧녰=2Overlap洧녵 (arg max 틙洧논 洧녰 洧녭洧랚 ( 틙洧논 洧녰 洧논<洧녰 ), 洧논 洧녰 ) , EL洧녵 (x) = Overlap洧녵 (a, b) = , (5) 洧녜 洧녵 (cid:205)洧녫 洧녵洧녮 (a) 1{洧녫 洧녵洧녮(b)} 洧녵洧녮(a) where Overlap洧녵 measures the overlap of 洧녵-grams between two sequences, 洧녵洧녮() denotes the list of 洧녵-grams within sequence. Higher 洧녵 values represent stricter standards for memorization quantification, adhering to higher privacy requirements. Following [37], our study chooses 洧녵 values of 3, 5, and 10. Example 3. As shown in columns 1 and 4-6 of Table 1, in the first row, given the prefix This, the number of the 3-gram matches between 틙洧논 洧녰 and 洧논 洧녰 is 2 (marked in green ), leading to an Overlap3 score of 0.2222. After iterating through all the prefixes 洧논<洧녰 , all the Overlap3 values are averaged to obtain final EL3 score of 0.1091. 3.2.2 Memorization Thresholds. Memorization in CLMs varies, ranging from rarely reproduced sequences to verbatim repetition easily exploitable by adversaries. Without clear boundaries between them, it is difficult to prioritize and address genuine privacy vulnerabilities. To this end, we empirically establish explicit memorization thresholds based on the metrics MA and EL洧녵: 洧녢MA = 1 x MA(x) , 洧녢EL洧녵 = 1 x EL洧녵 (x) , (6) where 1 is the indicator function that returns 1 if the condition within the braces is true (i.e., the CLMs most likely prediction 틙洧논洧녰 where denotes dataset consisting entirely of samples unseen during the CLMs training phase. Intuitively, training sample Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Table 2: Memorization thresholds for the studied CLMs. CLM CodeParrot-small CodeParrot CodeGen-350M-Mono CodeGen-2B-Mono Qwen2.5-Coder-7B MA (%) 45.57 46.34 48.79 53.61 40.99 EL3 (%) 17.66 16.56 18.24 19.32 15.65 EL5 (%) 10.82 10.17 11.03 11.71 12.45 EL10 (%) 5.49 5.14 5.92 6.28 8. with memorization scores below these thresholds, appearing as if the model had never seen it, indicates safe memorization and is thus resistant to extraction attacks. Conversely, samples exceeding these thresholds indicate potential risks of exposure and leakage. The resulting memorization thresholds for the studied CLMs are presented in Table 2. Unseen Dataset. For the Qwen2.5-Coder-7B model, we compile from two popular evaluation datasets, i.e., HumanEval [19] and MBPP [6], which have been explicitly excluded from the CLMs training corpus via data decontamination [34]. For the CodeParrot and CodeGen-Mono families, we compile using data crawling tool provided by [67], collecting 10,000 high-quality de-duplicated code files from GitHub repositories. Repository selection criteria include: each must have at least 500 stars and be created after the release dates of the CLMs. Moreover, to address potential concerns that files might be copied from older versions already exposed to the CLMs, we only collect code files written in programming languages absent from the CLMs training, e.g., Ruby, PHP, Rust, and Lua. This strategy ensures that is high-quality and genuinely unseen by the studied CLMs."
        },
        {
            "title": "3.3 Sensitive Memorization Identification",
            "content": "Not all memorization poses privacy risks; for instance, retaining public code snippets is far less concerning than memorizing private keys. While several techniques have been developed to extract memorized contents from CLMs [3, 35, 68], they mainly focus on analyzing non-sensitive code memorization. Recent studies [33, 51] have highlighted privacy risks by eliciting sensitive information from CLMs using well-crafted prompts. However, they only reveal small-scale, isolated examples of sensitive memorization, lacking systematic analysis of the broader extent of sensitive data retained by CLMs. Thus, we aim to address the question: To what extent do CLMs memorize sensitive information from their training data? Sensitive Data Identification. To comprehensively identify sensitive data within code (e.g., emails, IP addresses, and API/SSH keys), we employ detect-secrets [71], widely used regular expressionbased detection tool, to scan the entire codeparrot-clean-train dataset. After filtering out local IPs and emails containing example, we find that 939,665 out of 5,300,000 training samples (approximately 18%) contain sensitive information. Sensitive Memorization Quantification. We assess the memorization levels of sensitive segments in identified samples using the MA metric. MA is preferred over EL洧녵 due to its efficiency in token matching, making it more suitable for large-scale analysis than the 洧녵-gram approach of EL洧녵. Given computational constraints, we restrict our analysis to two relatively small models, i.e., CodeParrotsmall and CodeGen-350M-Mono, and limit the examined samples to those containing sensitive data within maximum token length Figure 2: The distribution of MA across sensitive data. (e.g., 512). Moreover, only sensitive segments are considered in this quantification; surrounding non-sensitive code is excluded. For each sensitive segment, we prepend fixed non-sensitive prefix (up to 128 tokens) when computing memorization. For instances containing multiple sensitive segments, we calculate the average MA score across all segments. These measures allow us to complete quantification on the full training dataset within 6 hours using single GPU equipped with 80GB of memory. As shown in Figure 2, we find that 376,740 out of 473,994 training samples in CodeParrot-small and 363,806 out of 501,549 in CodeGen-350M-Mono (approximately 7% of training data) exhibit sensitive memorization, with MA scores exceeding the established memorization thresholds. Finding : CLMs such as CodeParrot-small and CodeGen-350MMono memorize approximately 7% training samples containing sensitive data, posing considerable privacy risks. Building upon this finding, we extend our analysis to additional models, i.e., CodeParrot, CodeGen-2B-Mono, and Qwen2.5-Coder7B. For each studied CLM, we ultimately collect 10,000 highly memorized sensitive samples (e.g., MA 0.9), resulting in 50,000 samples in total. We compile them into Sensitive Memorization Dataset, which documents the positions of all sensitive segments within each code sample along with their corresponding memorization scores. This dataset serves as the foundation for subsequent unlearning experiments, providing standardized benchmark for evaluation. The overall dataset collection pipeline is illustrated in Figure 3."
        },
        {
            "title": "4 Unlearning Techniques",
            "content": "Our preliminary study reveals that CLMs memorize substantial sensitive data from training corpora. To mitigate this issue, we explore unlearning techniques that enable targeted forgetting of memorized content. We formally define the unlearning problem for CLMs and introduce three gradient ascent-based unlearning approaches: the vanilla method, the constraint-based method, and our proposed CodeEraser. ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, and David Lo Figure 3: An illustration of the sensitive memorization detection pipeline."
        },
        {
            "title": "4.1 Problem Statement",
            "content": "Forgetting, the inverse of memorization, is typically studied in the context of catastrophic forgetting [40, 41], where models lose prior knowledge when learning new tasks. These studies treat forgetting as an undesirable trait in training. Recently, Jagielski et al. [36] reinterpret forgetting positively, viewing it as relaxed form of differential privacy. However, they mainly examine passive forgetting during large-scale training. In contrast, our study embraces an active form of forgetting, i.e., machine unlearning [14], which intentionally modifies trained models to erase previously memorized information. Conceptually, we define forgetting as reduction in the models memorization of specific training samples. ,. . . ,洧논 洧녭 Formally, let 洧녭洧랚 be CLM trained on dataset D, and let 洧녭 = {x洧녭 } denote the forgotten set, where each forgotten sample x洧녭 = (洧논 洧녭 洧녜 ) is token sequence containing sensitive data. The set size 洧녭 =洧녲 represents the number of samples undergoing unlearning simultaneously. The goal of unlearning is to update the CLM to new version, 洧녭 洧랚 , that no longer retains any information from each x洧녭 . Specifically, after unlearning, each x洧녭 should satisfy the following conditions, appearing as if never seen by the CLM: ,洧논 洧녭 2 1 MA(x 洧녭 ) 洧녢MA , EL洧녵 (x 洧녭 ) 洧녢EL洧녵 . (7)"
        },
        {
            "title": "4.2 Gradient Ascent-Based Unlearning",
            "content": "Vanilla Unlearning. Gradient Ascent (GA) [13, 37, 45] is simple yet effective unlearning method designed to reduce the models likelihood of predicting specific forgotten samples, thereby actively encouraging the removal of their information. Specifically, for each x洧녭 , GA reverses the standard autoregressive language modeling objective by maximizing the negative log-likelihood, forcing the CLM to deviate from its original predictions. Formally, as illustrated in Figure 4 (a), GA updates the unlearned CLM 洧녭 洧랚 using the following loss function: L洧냨洧냢 (x 洧녭 ) = 1 L洧洧 (x 洧녭 ) = log 洧녜 (cid:214) 洧녰= 洧랚 (洧논 洧녭 洧녭 洧녰 洧논 洧녭 1 , . . . , 洧논 洧녭 洧녰 1) . (8) Constraint-Based Unlearning. key challenge in unlearning is to remove targeted data without degrading the models original utility. Directly applying gradient ascent to the CLM may risk erasing unrelated yet valuable code knowledge. To address this, the Constraint-Based Unlearning (CU) method [18, 42, 69, 70] seeks to minimize the Kullback-Leibler (KL) divergence between the predictions of the original CLM 洧녭洧랚 and the unlearned CLM 洧녭 洧랚 on the data to be retained, while maximizing divergence for the data targeted for forgetting. Formally, given retained set D洧 ={(x洧 )} D, the CLM is updated using the following contrastive loss: L洧쮫롏(x 洧)) , (9) 洧녭)) + 洧띺 洧쮫롏(洧녭洧랚(x 洧쮫롏(洧녭洧랚(x 洧 ) = 洧녭)洧녭 洧)洧녭 洧녭,x 洧랚(x 洧랚(x x洧녭 x洧 where 洧띺 is hyperparameter controlling the balance between forgetting x洧녭 and retaining x洧 . Minimizing KL divergence on retained data ensures alignment with the original predictions, while maximizing it on forgotten data actively encourages effective forgetting. In practice, as illustrated in Figure 4 (b), this KL divergence-based loss L洧쮫롏 is typically combined with the GA-based loss L洧냨洧냢 for collaborative optimization: 洧 ) , 洧녭 ,x L洧냤洧녣 = L洧냨洧냢 (x 洧녭 ) + 洧랝 L洧쮫롏 (x (10) where the hyperparameter 洧랝 balances the intensity between gradient ascent updates and KL divergence-based constraints. CodeEraser: Proposed Selective Unlearning. While techniques like gradient ascent and KL divergence-based constraint enable effective unlearning, they typically indiscriminately forget entire code samples, unnecessarily removing non-sensitive content. Motivated by insights from our preliminary study, we propose CodeEraser, an adapted unlearning method that selectively targets and erases sensitive memorized segments (e.g., API keys) without compromising the integrity and functionality of surrounding code. This segmentation design builds on the tool-based identification of sensitive elements within code (e.g., via detect-secrets) described in Section 3.3, enabling accurate and targeted unlearning. Formally, for each forgotten sample x洧녭 = (洧논 洧녭 洧녜 ) 洧녭 , 1 ,. . . ,洧 洧녭 we segment sensitive sequences s洧녭 = (洧 洧녭 洧녴) from their nonsensitive contexts c洧녭 = (洧녫 洧녭 ,. . . ,洧녫 洧녭 洧녵 ). To achieve selective unlearning, we apply gradient ascent exclusively on s洧녭 to actively diminish their memorization, while applying gradient descent on c洧녭 to preserve their integrity. Accordingly, we apply targeted KL divergence-based constraint on sensitive segments s洧녭 rather than ,. . . ,洧논 洧녭 ,洧논 洧녭 ,洧녫 洧녭 2 ,洧 洧녭 2 1 1 Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Figure 4: An illustration of gradient ascent-based unlearning methods: (a) vanilla unlearning, (b) constraint-based unlearning, and (c) our proposed CodeEraser. Personal details are masked for ethical considerations. the entire sample x洧녭 . Specifically, as illustrated in Figure 4 (c), we define the selective unlearning loss as follows: 洧 ) , 洧녭 ,x L洧녡洧녣 = (L洧냨洧냢 (s 洧녭 ) + 洧 L洧洧 (c 洧녭 )) + 洧랝 L洧쮫롏 (s (11) where the hyperparameter 洧 balances the trade-off between forgetting s洧녭 and preserving c洧녭 . This selective framework precisely restricts ascent updates to sensitive segments, minimizing the impact of unlearning on the CLMs broader utility. Example 4. Given piece of code snippet user_config = {email: daniel@gmail.com, password: ABC}, the sensitive segments 洧 洧녭 are daniel@gmail.com and ABC, while the non-sensitive contexts 洧녫 洧녭 are user_config = {email: [placeholder], password: [placeholder]}. This segmentation preserves the original sequential structure required by autoregressive CLMs. To stabilize the max-min optimization of L洧쮫롏 during unlearning, we adopt an iterative training strategy. Specifically, we alternate training epochs between the forgotten set 洧녭 and the retained set D洧 . This strategy ensures balanced training dynamics, preventing either term from excessively dominating the optimization process."
        },
        {
            "title": "4.3 Connections and Discussion",
            "content": "Here, we establish connections between our selective unlearning framework and other methods in Figure 4. Specifically, when removing the segmentation between sensitive and non-sensitive parts and setting the hyperparameters 洧 and 洧랝 in Eq. (11) to 0, the selective unlearning loss collapses directly into the standard gradient ascent loss defined in Eq. (8). In this scenario, the CLM indiscriminately performs gradient ascent on entire code instances rather than selectively targeting sensitive segments. Similarly, by removing segmentlevel targeting in Eq. (11) and applying the KL divergence-based constraint to entire forgotten samples x洧녭 , the selective unlearning loss reverts to the original constraint-based formulation in Eq. (10). In this case, the constraint-based unlearning considers whole-sample consistency, without explicitly distinguishing between sensitive and non-sensitive segments. These derivations demonstrate that our framework subsumes prior unlearning methods while extending them to support fine-grained, code-specific erasure of sensitive memorized segments."
        },
        {
            "title": "5 Experiments and Analysis",
            "content": "To evaluate the performance of various unlearning techniques for CLMs, we investigate the following Research Questions (RQs): RQ1: Effectiveness and Efficiency. How do the unlearning methods perform in terms of removing targeted sensitive information from CLMs (effectiveness) with minimal computational resources (efficiency)? RQ2: Model Utility Post-Unlearning. How do the unlearning methods affect the original utility of CLMs, particularly the code generation performance on the HumanEval benchmark? RQ3: Analysis on Forgotten Data. How do the characteristics of forgotten data (e.g., the number of samples 洧녲, their occurrence frequency in training, and the types of sensitive segments) impact unlearning performance? RQ4: Impact of Hyperparameters. How do hyperparameter settings (e.g., learning rate, 洧, 洧띺, and 洧랝) impact unlearning effects?"
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Forgotten Set. Following [37], we build the forgotten set for each studied CLM by randomly sampling 洧녲 instances from the corresponding sensitive memorization dataset, which are then subjected to unlearning. To reduce the potential bias of random selection, we report the average result from 5 independent runs for each unlearning routine. Unless otherwise specified, we report experimental results with default setting of 洧녲 = 32. We examine various values of 洧녲 from {8, 16, 32, 64, 128, 256, 512}, as detailed in Section 5.4, to demonstrate CodeErasers scalability in handling varying numbers of unlearning requests from users. Retained Set. The retained set is built to include non-targeted, nonsensitive data, serving as benchmark for measuring the CLMs memorization retention after the unlearning process. We leverage code benchmark [3] that offers 1,000 non-sensitive samples from BigQuery [31]. These samples have been demonstrated to be memorized by various CLMs, such as CodeParrot [24], CodeGen [50], and InCoder [28], making them suitable for building the retained set in our experiments. Specifically, for each studied CLM, we extract its corresponding memorized data and randomly sample an equivalent number of 洧녲 instances to form the retained set. Implementation Details. In our experiments, the maximum token lengths are set to 512 for the forgotten set and 128 for the unseen dataset and the retained set, with any excess truncated. These lengths are chosen based on computational constraints while ensuring sufficient data is available for analysis. For computing MA and EL洧녵 scores, we adopt greedy sampling strategy. Following [37], we set the global batch size equal to 洧녲 during unlearning. When ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, and David Lo Table 3: Evaluation of unlearning effectiveness. All values are reported as percentages (with % symbol omitted). CLM Method CodeParrot -small CodeParrot CodeGen -350M-Mono CodeGen -2B-Mono Qwen2.5 -Coder-7B Original Threshold GA CU CodeEraser Original Threshold GA CU CodeEraser Original Threshold GA CU CodeEraser Original Threshold GA CU CodeEraser Original Threshold GA CU CodeEraser MA 99.74 45.57 30.71 22.14 18.69 99.69 46.34 27.53 24.18 15.22 99.25 48.79 25.53 18.65 45.13 99.89 53.61 17.95 11.80 31.66 96.26 40.99 24.15 16.63 8.49 EL3 98.55 17.66 10.17 7.79 6.69 98.90 16.56 6.33 6.11 6.36 97.14 18.24 8.45 6.98 11.44 99.79 19.32 6.80 6.40 10.01 84.71 15.65 14.23 8.77 4.93 EL5 98.12 10.82 7.07 6.19 5.78 98.36 10.17 4.21 4.39 5.40 96.39 11.03 6.98 5.73 7.05 99.76 11.71 5.52 5.99 7.73 81.07 12.45 10.49 6.84 3.99 EL10 Red. 97.97 5.49 4.18 4.72 5.18 97.62 5.14 3.47 3.09 4.65 95.93 5.92 4.95 4.88 3.46 99.70 6.28 4.83 5.54 6.05 75.15 8.82 8.24 5.48 3.68 - - 86.85 89.69 90.82 - - 89.54 90.48 92.01 - - 88.29 90.75 82.96 - - 91.21 92.55 86.11 - - 83.55 89.16 93.89 processing group of 洧녲 instances, we average their MA and EL洧녵 scores to empirically decide whether they have been forgotten. The learning rate is fixed at 3e-6, selected through empirical testing from the range {1e-5, 8e-6, 5e-6, 3e-6, 1e-6}, and we maintain constant learning rate schedule throughout unlearning. Dropout and weight decay rates are both set to 0 to avoid regularization that might interfere with the unlearning process. We select 洧띺 = 1.0 from {0.5, 0.8, 1.0, 1.2, 1.5}, and 洧 = 0.5 and 洧랝 = 0.1 from {0.1, 0.2, 0.3, 0.4, 0.5} through systematic grid search, with detailed hyperparameter analysis in Section 5.5."
        },
        {
            "title": "5.2 RQ1: Effectiveness and Efficiency",
            "content": "We assess the effectiveness and efficiency of various unlearning techniques when applied to the studied CLMs. In our context, effectiveness refers to the capability of the unlearning approach to successfully erase specific sensitive information retained by the CLM, while efficiency refers to the computational costs required to achieve this unlearning. Unlearning Effectiveness. To quantitatively evaluate the effectiveness of unlearning, we calculate MA, EL3, EL5, and EL10 for the sensitive data targeted for removal in the forgotten set. An unlearning approach is deemed effective when the targeted sensitive data becomes difficult to extract from the model post-unlearning, characterized by MA and EL洧녵 scores falling below their respective memorization thresholds defined in Section 3.2.2. We also report the average memorization reduction rate of these metrics post-unlearning (abbreviated as Red.). Figure 5: Evaluation of unlearning efficiency. As shown in Table 3, the results indicate that CodeEraser achieves substantial reduction in MA and EL洧녵 scores across all models, successfully lowering them below the predetermined memorization thresholds outlined in Table 2. For instance, with the Qwen2.5-Coder-7B model, CodeEraser results in an average memorization reduction of 93.89%. It is important to note that an unlearning method is considered effective as long as it reaches the forgetting criteria; it is not required to reduce more memorization than the baselines (i.e., GA and CU), as over-unlearning could lead to loss of model utility. Unlearning Efficiency. For efficiency, we measure the cumulative GPU time required to perform unlearning updates on the CLM until the memorization thresholds are reached for group of 洧녲 = 32 instances. Additionally, we monitor peak memory usage across 4 GPUs during unlearning by leveraging PyTorchs memory check API, and report the total footprint as the sum of these values. These two metrics are chosen because they directly reflect the computational resources consumed during unlearning. As shown in Figure 5, with the Qwen2.5-Coder-7B model, our proposed CodeEraser completes the unlearning process within approximately 1500 seconds (averaging 46.88 seconds per sample), with peak memory usage of around 200GB. This cost is considerably lower than alternatives such as differentially-private training or retraining the CLM after de-duplication, which are reported to typically require on the order of hundreds of A100 GPU days [37]. Moreover, unlike the baselines that focus on the unlearning of entire code samples, CodeEraser exclusively targets the forgetting of specific sensitive data, enabling it to complete unlearning in relatively shorter duration. Although CodeEraser exhibits higher memory usage than GA (due to additional training steps on the retained set), it outperforms in terms of preserving the post-unlearning utility of CLMs, which will be further discussed in Section 5.3. Answer to RQ1: CodeEraser demonstrates effectiveness and efficiency in erasing specific sensitive information from CLMs, thereby reducing potential security and privacy risks without incurring excessive computational costs. Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Table 4: Evaluation of model utility post-unlearning. All values are reported as percentages (with % symbol omitted). indicates that higher values correspond to better preservation of model utility. The best-performing unlearning method in each column is highlighted in bold. CLM Method CodeParrot -small CodeParrot CodeGen -350M-Mono CodeGen -2B-Mono Qwen2.5 -Coder-7B Original GA CU CodeEraser Original GA CU CodeEraser Original GA CU CodeEraser Original GA CU CodeEraser Original GA CU CodeEraser P@1 3.48 2.14 2.62 3. P@5 4.56 3.02 3.43 4.59 P@10 4.96 3.20 3.66 4.87 Ret. - 64.08 74.77 102.10 4.34 2.08 2.04 3.86 13.37 11.68 10.79 13.36 24.72 21.20 20.57 23. 61.07 40.67 48.54 61.65 5.81 3.29 2.94 5.08 18.79 16.59 14.91 18.02 31.49 28.34 27.73 29.91 73.61 53.81 64.70 73.41 6.22 3.83 3.28 5. 21.12 18.51 16.41 19.96 34.16 31.40 30.63 32.94 77.23 57.63 69.59 76.69 - 55.38 50.11 88.96 - 87.76 79.25 96.78 - 89.23 86.98 94. - 71.44 85.83 99."
        },
        {
            "title": "5.3 RQ2: Model Utility Post-Unlearning",
            "content": "Ensuring robust privacy protections necessitates delicate balance: the CLM must selectively forget targeted sensitive information to safeguard privacy without compromising its inherent capacity to perform general coding tasks. We evaluate the efficacy of CodeEraser in achieving this balance. Setup. To evaluate the impact of unlearning on the CLMs utility, we adopt the HumanEval benchmark [19], widely recognized standard for assessing code generation performance in CLMs [10], with over 95.9k monthly downloads on HuggingFace at the time of writing. This benchmark measures the CLMs ability to solve programming tasks, where we report the Pass@1, Pass@5, and Pass@10 scores [19], which measure the accuracy of generating correct solutions within 1, 5, and 10 attempts for each task, respectively (abbreviated as P@1, P@5, and P@10). By comparing these scores preand post-unlearning, we can observe the changes in the CLMs general coding performance. To quantify these changes, we also report the average performance retention rate across these metrics post-unlearning (abbreviated as Ret.). Results. As shown in Table 4, CodeEraser has only minor impact on model utility compared to the baselines. Take Qwen2.5-Coder7B as an example, CodeEraser preserves 99.99% of the CLMs code generation performance. This lesser degree of degradation can be attributed to CodeErasers sensitive information-targeted selective unlearning mechanism, which minimizes the impact of unlearning on model utility, ensuring that the code knowledge outside the specified forgotten set remains intact. Among the baselines, notable performance decline is observed in most cases when applying GA to the studied CLMs. This decline may stem from the gradient ascent updates, which, although performed only on the forgotten set, tend to soften the probability distribution of generating each token across the vocabulary. This results in more uniform distribution, which inadvertently dilutes the CLMs inherent knowledge base and reduces its overall utility. Moreover, the CU approach does not demonstrate the expected level of utility preservation compared to GA in some cases. This may be due to the alignment of model behavior on shorter instances (128 tokens) being insufficient to offset the impact of forgetting longer instances (512 tokens). Instead, it could affect the stability of the models updates, further compromising the integrity of the CLMs knowledge base. Given this unexpected phenomenon, we plan to investigate it further in future work to fully understand the dynamics of unlearning in CLMs. Answer to RQ2: CodeEraser has only minor impact on the CLMs coding performance compared to baselines, validating the efficacy of our selective unlearning mechanism in preserving model utility while achieving targeted forgetting of sensitive information in code."
        },
        {
            "title": "5.4 RQ3: Analysis on Forgotten Data",
            "content": "Recent studies have highlighted the importance of training data characteristics, such as duplication frequency and sensitive data type, in influencing memorization patterns of CLMs [3, 15, 68]. These findings reveal the intricate nature of memorization in CLMs and imply potential impact of such data attributes on the efficacy of unlearning. To examine this, we evaluate CodeEraser on the CodeParrot model, focusing on targeted sensitive data samples that vary in number, duplication frequency, and type. For each setting, we report the average results of the HumanEval scores preand post-unlearning in 5 independent runs. Influence of Forgotten Sample Number 洧녲. Our analysis examines how varying the number of forgotten samples (洧녲) influences the efficacy of CodeEraser. As shown in Figure 6 (a), CodeEraser remains robust when unlearning moderate number of sensitive samples (e.g., 洧녲 128), with the CLM effectively preserving its utility post-unlearning. However, as the size of the forgotten set increases significantly (e.g., 洧녲 = 256 or 洧녲 = 512), utility scores such as P@5 and P@10 exhibit noticeable decline. These results indicate potential scalability limitations in CodeEraser, particularly for larger-scale unlearning tasks involving extensive sensitive datasets (e.g., 10,000 samples). Ensuring effective unlearning at scale while minimizing utility degradation remains key challenge, which we leave for future work. Influence of Data Duplication. To examine the impact of data duplications on unlearning, we utilize the duplication frequency statistics of training samples provided by the codeparrot-clean-train dataset. We roughly divide duplication frequencies into four ranges: [5, 10), [10, 25), [25, 50), and [50, ), which allows us to assess how varying levels of data duplication, from relatively low to very high frequencies, affect the unlearning process. For each duplication level, we randomly select 洧녲 = 16 samples that contain sensitive information to perform unlearning. ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, and David Lo Figure 6: Analysis on forgotten data. Dashed lines - - represent the initial HumanEval scores of the CodeParrot model. As shown in Figure 6 (b), the frequency of duplication significantly influences the CLMs utility post-unlearning. Interestingly, we can see that CodeEraser exhibits higher utility-preserving performance at the extremes of the duplication spectrum (i.e., [5, 10) and [50, )) compared to the intermediate duplication levels. This phenomenon may be explained by the nature of the data involved. Low-duplicated memorized samples, potentially acting as outliers within the data distribution [16], may have less entrenched influence on the model, making their removal less disruptive. On the other hand, highly duplicated samples are likely to cause model overfitting, meaning that their removal could reduce redundancy and mitigate overfitting, resulting in minor impact on overall model performance. These findings suggest that the impact of unlearning on model utility is not uniform across different levels of data duplication, and understanding these dynamics is crucial for optimizing our unlearning approach in the future. Influence of Sensitive Data Type. To evaluate the influence of distinct sensitive data types (e.g., email, IP address, and API/SSH Key) on the unlearning process, we leverage the constructed sensitive memorization dataset for the CodeParrot model. For each type, we randomly select 洧녲 = 32 samples containing only the corresponding sensitive data for unlearning. As shown in Figure 6 (c), the influence on the CLMs utility postunlearning varies depending on the type of sensitive data. This variation is likely due to differences in how the CLM memorizes these data types. Surprisingly, the removal of API/SSH keys results in an improvement in model performance. This improvement may be attributed to the fact that, unlike emails and IP addresses, specific secret keys are usually atypical patterns within the data and are less likely to be heavily duplicated in the training dataset, making them outliers in the data distribution. Such atypical data Figure 7: Parameter analysis of learning rate, 洧, 洧띺, and 洧랝. outliers often distract the model and negatively impact its overall generalization. Therefore, eliminating them can refine the CLMs representation space and shift its focus to more representative data, thereby enhancing overall performance. Given these preliminary insights, future work will explore the effects of unlearning across broader spectrum of sensitive data types. Answer to RQ3: The characteristics of the targeted sensitive data, e.g., number, duplication frequency, and type, significantly influence the CLMs utility post-unlearning. These findings reveal that unlearning effects are conditioned by data attributes, motivating future exploration of unlearning dynamics and robust strategies to minimize negative impacts on models."
        },
        {
            "title": "5.5 RQ4: Impact of Hyperparameters",
            "content": "We analyze the impact of hyperparameters, including the learning rate and regularization factors (i.e., 洧, 洧띺, and 洧랝), on model utility post-unlearning. As illustrated in Figure 7, the learning rate substantially influences model utility after unlearning, with excessively large values resulting in noticeable declines in utility metrics (e.g., P@5 and P@10). This underscores the importance of carefully tuning the learning rate to balance forgetting effectiveness against maintaining model performance. In contrast, varying the hyperparameters 洧, 洧띺, and 洧랝 within reasonable ranges results in only minor changes in post-unlearning utility, indicating robustness and flexibility of our method toward these parameters. Nonetheless, moderate values for these parameters are recommended, as overly aggressive settings may still negatively impact utility or insufficiently support effective forgetting. Based on these insights, we select empirically determined optimal values for all hyperparameters to balance the trade-off between effective forgetting and model performance retention. The final settings employed in our experiments are detailed in Section 5.1. Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Answer to RQ4: The learning rate substantially influences model utility after unlearning and must be carefully tuned. Regularization hyperparameters (洧, 洧띺, 洧랝) have comparatively minor impacts, allowing greater flexibility in their selection."
        },
        {
            "title": "6 Threats to Validity",
            "content": "Threats to Internal Validity. Internal validity concerns whether our methodology introduces biases or errors that may distort the results. Our study identifies sensitive segments within code using regular expression-based method and then quantifies their memorization. However, this method constrains both the accuracy and coverage of secret detection, since regex rules can capture only limited set of patterns. To mitigate this threat, we employ detect-secrets [71], state-of-the-art tool widely used for secret detection in large-scale code bases. This tool covers broad spectrum of high-risk categories (e.g., API keys, tokens, and credentials) that are most relevant to real-world security incidents. Future work may incorporate additional detection methods [7, 27, 33] to broaden coverage; however, such extensions are unlikely to alter our principal finding that CLMs manifest substantial sensitive memorization. Threats to External Validity. External validity concerns the extent to which our findings can be generalized to other settings. This study focuses on three CLM families, i.e., CodeParrot, CodeGen, and Qwen2.5-Code, spanning 110M to 7B parameters; however, the results may not generalize to other CLMs. To alleviate this threat, we select these families since they are widely adopted in research and practice, making them representative of the current CLM landscape. Moreover, the observed patterns are consistent across different model sizes within these families, suggesting that our findings are not tied to specific scale. This limitation is also shared by many prior studies, which typically examine few representative families rather than exhaustively covering all models. Thus, while additional CLM families might provide further evidence, our methodological choices sufficiently support external validity."
        },
        {
            "title": "7 Related Work",
            "content": "Memorization in LMs. Memorization, often seen as the antithesis of generalization, arises from overfitting, leading models to remember specific details of their training data [3, 25]. This phenomenon raises remarkable privacy concerns in the context of LMs, as these models may memorize and regurgitate sensitive information verbatim. Extensive research has been undertaken to understand memorization in LMs qualitatively and quantitatively [8, 1517, 55, 56, 72]. Recent research [3, 35, 68] has also explored memorization within CLMs, offering empirical studies to examine the extent to which CLMs inadvertently memorize and disclose their training data. Additionally, recent studies [33, 51] have highlighted privacy risks by extracting sensitive information from CLMs using well-crafted prompts. Following this line, in this paper, we conduct pioneering investigation into mitigating sensitive memorization in CLMs through machine unlearning. Machine Unlearning. Machine unlearning, first proposed by Cao et al. [14], also known as selective forgetting [30] or data removal/deletion [29, 32], aims to remove the influence of specific set of training samples from the trained model. Existing studies in this field can be categorized into two groups: 1) Exact Unlearning: Exact unlearning seeks to remove specific samples influence from the model completely. straightforward method is to retrain the whole model from scratch after removing targeted data from the training set. However, this method is computationally infeasible for large datasets. Despite efforts to reduce the computational cost, they either primarily cater to simple machine learning models [14, 29] or rely on training data partitioning [11, 20], limiting their applicability to complex and large CLMs. 2) Approximate Unlearning: Approximate unlearning has recently emerged as promising alternative, prioritizing efficiency and cost by relaxing the requirement for exactness. Existing methods typically adjust the models weights via gradient-based updates to approximate the weights of the model retrained from scratch [23, 30, 32, 37]. Building on this paradigm, gradient ascent-based methods [18, 37, 69] have emerged as dominant direction for efficient unlearning by reversing the learning of specific data, which also constitutes the focus of this study. However, they often indiscriminately erase entire text instances rather than selectively targeting specific sensitive data (e.g., API keys embedded in code). While Wang et al. [64] proposed heuristic that designates high-perplexity tokens in plain text as privacy tokens for unlearning, this approach is unsuitable for source code: identifiers are often assigned unique names with high perplexity, resulting in erroneous removal, whereas actual secrets such as API key strings typically follow predictable patterns with lower perplexity and may therefore escape removal. In contrast, our approach employs specialized tool (i.e., detect-secrets) to precisely identify secrets in code, enabling targeted unlearning while preserving the integrity and functionality of the surrounding code."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we pioneer the use of machine unlearning to erase sensitive memorization in CLMs. We first construct novel dataset by systematically identifying and assessing high-risk code instances in the CLMs training data. Then, we introduce CodeEraser, selective unlearning approach that uses gradient ascent to remove sensitive information while preserving surrounding non-sensitive code via gradient descent. Additionally, CodeEraser employs KL divergence-based constraint to maintain model utility postunlearning. Extensive experiments on CodeParrot, CodeGen-Mono, and Qwen2.5-Coder demonstrate that CodeEraser effectively eliminates sensitive memorization while preserving overall model performance. Our study highlights the potential of unlearning in reinforcing data privacy in CLMs, providing practical technique to actively mitigate the harms of model memorization. Data Availability. All the experimental data and code used in this paper are available at https://github.com/CGCL-codes/naturalcc/ tree/main/examples/code-unlearning."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is supported by the Major Program (JD) of Hubei Province (Grant No. 2023BAA024). We would like to thank all the anonymous reviewers for their insightful comments. ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, and David Lo References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS 16). Association for Computing Machinery, New York, NY, USA, 308318. [2] Ali Al-Kaswan, Toufique Ahmed, Maliheh Izadi, Anand Ashok Sawant, Premkumar Devanbu, and Arie van Deursen. 2023. Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binaries. In Proceedings of the 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 23). 260271. [3] Ali Al-Kaswan, Maliheh Izadi, and Arie van Deursen. 2024. Traces of Memorisation in Large Language Models for Code. In Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE 24). IEEE Computer Society, Los Alamitos, CA, USA, 862862. [4] Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. 2022. Large-Scale Differentially Private BERT. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 22). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates. [5] Shushan Arakelyan, Rocktim Das, Yi Mao, and Xiang Ren. 2023. Exploring Distributional Shifts in Large Language Models for Code Analysis. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 23). Association for Computational Linguistics, Singapore, 1629816314. [6] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models. arXiv preprint arXiv:2108.07732 (2021). [7] Setu Kumar Basak, Lorenzo Neil, Bradley Reaves, and Laurie Williams. 2023. SecretBench: Dataset of Software Secrets. In Proceedings of the 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR 23). 347351. [8] Stella Biderman, Usvsn Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. 2023. Emergent and Predictable Memorization in Large Language Models. In Proceedings of the 37th Annual Conference on Neural Information Processing Systems (NeurIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 1219, 19 pages. [9] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. 2023. Pythia: Suite for Analyzing Large Language Models Across Training and Scaling. In Proceedings of the 40th International Conference on Machine Learning (ICML 23). JMLR.org, Article 102, 34 pages. [10] Bigcode. 2024. Big Code Models Leaderboard. https://huggingface.co/spaces/ bigcode/bigcode-models-leaderboard. Accessed: 2025-09-01. [11] Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine Unlearning. In Proceedings of the 2021 IEEE Symposium on Security and Privacy (SP 21). 141159. [12] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS 20). Curran Associates, Inc., Red Hook, NY, USA, Article 159, 18771901 pages. [13] George-Octavian B캒rbulescu and Peter Triantafillou. 2024. To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models. In Proceedings of the 41st International Conference on Machine Learning (ICML 24). Article 121, 21 pages. [14] Yinzhi Cao and Junfeng Yang. 2015. Towards Making Systems Forget with Machine Unlearning. In Proceedings of the 2015 IEEE Symposium on Security and Privacy (SP 15). IEEE Computer Society, USA, 463480. [15] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram칟r, and Chiyuan Zhang. 2023. Quantifying Memorization Across Neural Language Models. In Proceedings of the 11th International Conference on Learning Representations (ICLR 23). [16] Nicholas Carlini, Chang Liu, 칔lfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. In Proceedings of the 28th USENIX Conference on Security Symposium (SEC 19). USENIX Association, USA, 267284. [17] Nicholas Carlini, Florian Tram칟r, Eric Wallace, Matthew Jagielski, Ariel HerbertVoss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, 칔lfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting Training Data from Large Language Models. In Proceedings of the 30th USENIX Conference on Security Symposium (SEC 21). USENIX Association, 26332650. [18] Jiaao Chen and Diyi Yang. 2023. Unlearn What You Want to Forget: Efficient Unlearning for LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 23). Association for Computational Linguistics, Singapore, 1204112052. [19] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond칠 de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremb. 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374 (2021). [20] Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang. 2022. Graph Unlearning. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (CCS 22). Association for Computing Machinery, New York, NY, USA, 499513. [21] CodeParrot. 2022. Codeparrot-clean-train. https://huggingface.co/datasets/ codeparrot/codeparrot-clean-train. Accessed: 2025-09-01. [22] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2024. Evaluating Large Language Models in Class-Level Code Generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE 24). Association for Computing Machinery, New York, NY, USA, Article 81, 13 pages. [23] Ronen Eldan and Mark Russinovich. 2023. Whos Harry Potter? Approximate Unlearning in LLMs. [24] Hugging Face. 2022. CodeParrot. https://huggingface.co/codeparrot. Accessed: 2025-09-01. [25] Vitaly Feldman. 2020. Does Learning Require Memorization? Short Tale about Long Tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC 20). Association for Computing Machinery, New York, NY, USA, 954959. [26] Vitaly Feldman and Chiyuan Zhang. 2020. What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation. In Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS 20). Curran Associates Inc., Red Hook, NY, USA, Article 242, 11 pages. [27] Runhan Feng, Ziyang Yan, Shiyan Peng, and Yuanyuan Zhang. 2022. Automated Detection of Password Leakage from Public GitHub Repositories. In Proceedings of the 44th International Conference on Software Engineering (ICSE 22). Association for Computing Machinery, New York, NY, USA, 175186. [28] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: Generative Model for Code Infilling and Synthesis. In Proceedings of the 11th International Conference on Learning Representations (ICLR 23). [29] Antonio A. Ginart, Melody Y. Guan, Gregory Valiant, and James Zou. 2019. Making AI Forget You: Data Deletion in Machine Learning. In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS 19). Curran Associates Inc., Red Hook, NY, USA, Article 316, 14 pages. [30] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 20). 93019309. [31] Google. 2016. BigQuery. https://console.cloud.google.com/marketplace/details/ github/github-repos. Accessed: 2025-09-01. [32] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. 2020. Certified Data Removal from Machine Learning Models. In Proceedings of the 37th International Conference on Machine Learning (ICML 20). JMLR.org, Article 359, 11 pages. [33] Yizhan Huang, Yichen Li, Weibin Wu, Jianping Zhang, and Michael R. Lyu. 2024. Your Code Secret Belongs to Me: Neural Code Completion Tools Can Memorize Hard-Coded Credentials. Proceedings of the ACM on Software Engineering 1, FSE, Article 111 (2024), 23 pages. [34] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. 2024. Qwen2.5-Coder Technical Report. arXiv preprint arXiv:2409.12186 (2024). [35] Daniel Huynh. 2023. StarCoder Memorization Experiment Highlights Privacy Risks of Fine-Tuning On Code. https://huggingface.co/blog/dhuynh95/starcodermemorization-experiment. Accessed: 2025-09-01. [36] Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Guha Thakurta, Nicolas Papernot, and Chiyuan Zhang. 2023. Measuring Forgetting of Memorized Training Examples. In Proceedings of the 11th International Conference on Learning Representations (ICLR 23). Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning ICSE 26, April 1218, 2026, Rio de Janeiro, Brazil [37] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2023. Knowledge Unlearning for Mitigating Privacy Risks in Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL 23). Association for Computational Linguistics, Toronto, Canada, 1438914408. [38] Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao. 2024. Self-Planning Code Generation with Large Language Models. ACM Transactions on Software Engineering and Methodology 33, 7, Article 182 (2024), 30 pages. [39] Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating Training Data Mitigates Privacy Risks in Language Models. In Proceedings of the 39th International Conference on Machine Learning (ICML 22). PMLR, 1069710707. [40] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L. Hayes, and Christopher Kanan. 2018. Measuring Catastrophic Forgetting in Neural Networks. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence (AAAI 18/IAAI 18/EAAI 18). AAAI Press, Article 415, 9 pages. [41] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming Catastrophic Forgetting in Neural Networks. Proceedings of the National Academy of Sciences 114, 13 (2017), 35213526. [42] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. 2023. Towards Unbounded Machine Unlearning. In Proceedings of the 37th Annual Conference on Neural Information Processing Systems (NeurIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 95, 31 pages. [43] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training Data Makes Language Models Better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 22). Association for Computational Linguistics, Dublin, Ireland, 84248445. [44] Gen Li, Yao Wan, Hongyu Zhang, Zhou Zhao, Wenbin Jiang, Xuanhua Shi, Hai Jin, and Zheng Wang. 2025. Dataflow-Guided Neuro-Symbolic Language Models for Type Inference. In Proceedings of the 42nd International Conference on Machine Learning (ICML 25). [45] Bo Liu, Qiang Liu, and Peter Stone. 2022. Continual Learning and Private Unlearning. In Proceedings of The 1st Conference on Lifelong Learning Agents, Vol. 199. PMLR, 243254. [46] Michael Meli, Matthew R. McNiece, and Bradley Reaves. 2019. How Bad Can It Git? Characterizing Secret Leakage in Public GitHub Repositories. In Proceedings of the 26th Annual Network and Distributed System Security Symposium (NDSS 19). The Internet Society. [47] Tom치s Mikolov, Martin Karafi치t, Luk치s Burget, Jan Cernock칳, and Sanjeev Khudanpur. 2010. Recurrent Neural Network Based Language Model. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 10). ISCA, 10451048. [48] Thanh Tam Nguyen, Thanh Trung Huynh, Zhao Ren, Phi Le Nguyen, Alan WeeChung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. 2025. Survey of Machine Unlearning. ACM Transactions on Intelligent Systems and Technology (2025). [49] Yuqing Nie, Chong Wang, Kailong Wang, Guoai Xu, Guosheng Xu, and Haoyu Wang. 2025. Decoding Secret Memorization in Code LLMs Through Token-Level Characterization. In Proceedings of the 47th IEEE/ACM International Conference on Software Engineering (ICSE 25). IEEE, 28802892. [50] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In Proceedings of the 11th International Conference on Learning Representations (ICLR 23). [51] Liang Niu, Shujaat Mirza, Zayd Maradni, and Christina P칬pper. 2023. CodexLeaks: Privacy Leaks from Code Generation Language Models in GitHub Copilot. In Proceedings of the 32nd USENIX Conference on Security Symposium (SEC 23). USENIX Association, Anaheim, CA, USA, Article 120, 18 pages. [52] State of California. 2023. California Consumer Privacy Act (CCPA). https: //oag.ca.gov/privacy/ccpa. Accessed: 2025-09-01. [53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. OpenAI Blog 1, 8 (2019), 9. [54] Baptiste Rozi칟re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J칠r칠my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D칠fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. arXiv preprint arXiv:2308.12950 (2023). [55] Ali Satvaty, Suzan Verberne, and Fatih Turkmen. 2024. Undesirable Memorization in Large Language Models: Survey. arXiv preprint arXiv:2410.02650 (2024). [56] Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022. Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models. In Proceedings of the 36th Annual Conference on Neural Information Processing Systems (NeurIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 2773, 17 pages. [57] European Union. 2018. General Data Protection Regulation (GDPR). https://gdprinfo.eu. Accessed: 2025-09-01. [58] Michael Veale, Reuben Binns, and Lilian Edwards. 2018. Algorithms that Remember: Model Inversion Attacks and Data Protection Law. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 376, 2133 (2018), 20180083. [59] Eduard Fosch Villaronga, Peter Kieseberg, and Tiffany Li. 2018. Humans Forget, Machines Remember: Artificial Intelligence and the Right to Be Forgotten. Computer Law & Security Review 34, 2 (2018), 304313. [60] Yao Wan, Zhangqian Bi, Yang He, Jianguo Zhang, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin, and Philip Yu. 2024. Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit. ACM Comput. Surv. 56, 12, Article 309 (2024), 41 pages. [61] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, and Hai Jin. 2022. What Do They Capture? Structural Analysis of Pre-Trained Language Models for Source Code. In Proceedings of the 44th International Conference on Software Engineering (ICSE 22). Association for Computing Machinery, New York, NY, USA, 23772388. [62] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Improving automatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering (ASE 18). Association for Computing Machinery, New York, NY, USA, 397407. [63] Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Hai Jin, and Dongping Chen. 2025. CodeSync: Synchronizing Large Language Models with Dynamic Code Evolution at Scale. In Proceedings of the 42nd International Conference on Machine Learning (ICML 25). [64] Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, and Georg Gottlob. 2025. Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models. Proceedings of the AAAI Conference on Artificial Intelligence 39, 1 (2025), 843851. [65] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated Program Repair in the Era of Large Pre-Trained Language Models. In Proceedings of the 45th International Conference on Software Engineering (ICSE 23). IEEE Press, 14821494. [66] Chunqiu Steven Xia and Lingming Zhang. 2024. Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 24). Association for Computing Machinery, New York, NY, USA, 819831. [67] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. Systematic Evaluation of Large Language Models of Code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming (MAPS 22). Association for Computing Machinery, New York, NY, USA, 110. [68] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, Donggyun Han, and David Lo. 2024. Unveiling Memorization in Code Models. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE 24). Association for Computing Machinery, New York, NY, USA, Article 72, 13 pages. [69] Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. 2024. Machine Unlearning of Pre-trained Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 24). Association for Computational Linguistics, Bangkok, Thailand, 84038419. [70] Yuanshun Yao, Xiaojun Xu, and YangLiu. 2024. Large Language Model Unlearning. In Proceedings of the 38th Annual Conference on Neural Information Processing Systems (NeurIPS 24). Curran Associates, Inc., 105425105475. [71] Yelp. 2024. Detect-secrets. https://github.com/Yelp/detect-secrets. Accessed: 2025-09-01. [72] Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tram칟r, and Nicholas Carlini. 2023. Counterfactual Memorization in Neural Language Models. In Proceedings of the 37th Annual Conference on Neural Information Processing Systems (NeurIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 1708, 42 pages. [73] Tianqing Zhu, Gang Li, Wanlei Zhou, and Philip S. Yu. 2017. Differentially Private Data Publishing and Analysis: Survey. IEEE Transactions on Knowledge and Data Engineering 29, 8 (2017), 16191638. [74] Tianqing Zhu, Dayong Ye, Wei Wang, Wanlei Zhou, and Philip S. Yu. 2022. More Than Privacy: Applying Differential Privacy in Key Areas of Artificial Intelligence. IEEE Transactions on Knowledge and Data Engineering 34, 6 (2022), 28242843."
        }
    ],
    "affiliations": [
        "Chongqing University, Chongqing, China",
        "Huazhong University of Science and Technology, Wuhan, China",
        "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia",
        "Singapore Management University, Singapore, Singapore",
        "University of Alberta, Edmonton, Canada",
        "Zhejiang University, Hangzhou, China"
    ]
}