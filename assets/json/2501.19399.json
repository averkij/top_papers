{
    "paper_title": "Scalable-Softmax Is Superior for Attention",
    "authors": [
        "Ken M. Nakanishi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining."
        },
        {
            "title": "Start",
            "content": "Scalable-Softmax Is Superior for Attention Ken M. Nakanishi 1 5 2 0 2 1 3 ] . [ 1 9 9 3 9 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the models ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining. 1. Introduction Length generalization, the ability to handle longer context sizes than those used during training, is key challenge for Transformer-based large language models (LLMs) (Vaswani et al., 2017; Kazemnejad et al., 2023). Processing longer contexts enhances LLMs ability to leverage in-context learning (Brown et al., 2020) and chain-of-thought reasoning (Wei et al., 2022). However, the computational and memory requirements for training Transformers grow quadrat1Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan. Correspondence to: Ken M. Nakanishi <ken-nakanishi@g.ecc.u-tokyo.ac.jp>. Figure 1. Comparison of Softmax and SSMax, illustrating the issue of attention fading and the effectiveness of SSMax in preventing it. As the input vector size increases, the maximum value of the output vector produced by Softmax decreases, demonstrating the problem of attention fading. In contrast, SSMax keeps the maximum value close to 1, regardless of the input size. The input vector consists of -2 for all elements except the last, which is set to +3. The scaling parameter of SSMax is set to 0.43. ically with context size, imposing practical limits on the context sizes used during training. Consequently, LLMs must acquire the ability to generalize to longer context sizes beyond those seen during training. There are four primary approaches to addressing length generalization: improving positional encoding methods (Wei et al., 2022; Press et al., 2021; Su et al., 2024; Kazemnejad et al., 2023; Shaw et al., 2018), adopting sparse attention mechanisms (Ainslie et al., 2020; Gupta & Berant, 2020; Zaheer et al., 2020; Beltagy et al., 2020; Child et al., 2019; Kitaev et al., 2020; Roy et al., 2021; Sukhbaatar et al., 2019), further training on longer contexts after modifying positional encodings (Liu et al., 2024b;a; Ye et al., 2024), and enhancing attention mechanisms (Ye et al., 2024; Wang et al., 2024). This work focuses on enhancing attention mechanisms as an approach to address length generalization, specifically by replacing the Softmax function within the attention layers of Transformers. Softmax converts an input vector into vector that can be interpreted as probability distribution, where all elements are non-negative and sum to one. In deep learning, Softmax is commonly used in multi-class classification tasks to convert logits into probability distribution (LeCun et al., 1998). In the attention layers of Transformer-based language models, 1 Scalable-Softmax Is Superior for Attention Softmax computes probability distribution over all tokens in the context, determining how much attention is allocated to each token (Vaswani et al., 2017). Unlike the Softmax used in classification tasks, where the input vector size is fixed, the Softmax in attention layers has an input vector size that varies with the context size. The denominator of Softmax is the sum of the exponentials of all input elements, and its value increases as the context size grows, due to the larger number of input elements. In contrast, the numerator, the exponential of single input element, is independent of the input vector size. As the input vector size grows, the resulting probability distribution becomes increasingly flat. In this paper, we refer to this phenomenon as attention fading, which we hypothesize reduces the models ability to focus on key tokens in the context, thereby limiting length generalization. To address this issue, we propose Scalable-Softmax (SSMax), novel alternative to Softmax in attention layers that mitigates attention fading and enhances length generalization. SSMax transforms an input vector into probability distribution, similar to Softmax, but avoids attention fading even for large input vector sizes. Figure 1 illustrates the effectiveness of SSMax, contrasting it with Softmax, which suffers from attention fading. The name Scalable-Softmax reflects its ability to handle varying input vector sizes while preventing attention fading. Note that Scalable does not refer to the scaling parameter s, which will be introduced later. Additionally, as shown in Section 2.3, SSMax integrates seamlessly into existing architectures with minimal code modifications, maintaining compatibility and efficiency. In this study, we conducted several evaluations to assess the effectiveness of SSMax in improving Transformer performance. First, we compared the learning curves of Transformer models during pretraining and found that those with SSMax consistently achieved lower loss values compared to the standard Transformer. In all subsequent evaluations, we modified RoPEs θ to 50 times its training value without additional training, allowing us to assess the models ability to generalize to large context sizes under this drastic change. Second, unlike the standard Transformer, models with SSMax maintained significantly lower loss values even as the context size increased well beyond the training sequence length. Third, in key information retrieval tasks, models with SSMax retrieved key information reasonably accurately, even when the context size was extended up to 10 times the training sequence length. Further analysis of attention scores revealed that models with SSMax allocate significantly more attention to key tokens, even in longcontext scenarios. Lastly, while models trained with SSMax from the beginning achieve superior length generalization, we showed that replacing Softmax with SSMax in pretrained models can still provide some degree of improvement. 2. Scalable-Softmax (SSMax) The Softmax function transforms an input vector into vector that can be interpreted as probability distribution, where all elements are non-negative and sum to one. For an input vector of size n, with components zi (i = 1, 2, . . . , n), Softmax is defined as follows: zi (cid:55) ezi j=1 ezj (cid:80)n . (1) In the attention layers of Transformers, the input vector size increases as the context size grows. Softmax plays critical role in computing probability distribution over all tokens in the context, determining how much attention is allocated to each token. When grows, the denominator in Equation (1) increases, while the numerator remains independent of n. As result, the resulting probability distribution becomes increasingly flat. This phenomenon, which we refer to as attention fading, reduces the models ability to focus on key tokens within the context, potentially limiting its length generalization capability. To address this issue, we propose the Scalable-Softmax (SSMax) function, defined as: zi (cid:55) nszi j=1 nszj (cid:80)n = e(s log n)zi j=1 e(s log n)zj (cid:80)n , (2) where is scalar referred to as the scaling parameter. Similar to Softmax, SSMax transforms an input vector into probability distribution. However, the key difference lies in the dependence of the exponential base on input vector size n. This design aims to prevent attention fading by incorporating the input vector size into the functions formulation, which helps balance the scaling between the numerator and denominator in Equation (1). As result, the probability distribution remains focused on key tokens even as the input size grows. In the following subsections, we provide experimental evidence to justify the design of SSMax (Section 2.1) and present theoretical analysis to further explain its design and effectiveness in addressing attention fading (Section 2.2). Finally, we discuss its ease of implementation, requiring minimal modifications to existing architectures (Section 2.3). 2.1. Rationale Behind the Design of SSMax To investigate the optimal formulation of SSMax, we conducted experiments to analyze how attention scores should ideally depend on the input vector size n. Specifically, we replaced Softmax in all attention layers with the following modified formulation: zi (cid:55) e(spn+b)zi j=1 e(spn+b)zj (cid:80)n , (3) 2 Scalable-Softmax Is Superior for Attention Figure 2. Relationship between pn and the input vector size n. The red dots represent the learned values of pn after training, and the blue curve is fitted logarithmic function of the form pn a1 log n+a2. This result suggests that pn depends logarithmically on n, motivating the reformulation of Softmax in Equation (4). where and are scalar learnable parameters unique to each layer and head, and pn (n = 1, 2, . . . , ) represents learnable parameters shared across all layers and heads, depending solely on the input vector size n. Here, denotes the sequence length used during training. Since sparse attention mechanisms were not employed in this study, corresponds to the context size. We initialized all pn to 1, to 1, and to 0. The model has 162M parameters, and its architecture details are described in Appendix A. Training was conducted with sequence length of = 1024 and batch size of 2048 for 25,000 iterations (approximately 52B tokens). Detailed training hyperparameters are provided in Appendix B. After training, we plotted the values of pn, as shown in Figure 2. Figure 2 demonstrates that pn closely follows logarithmic relationship of the form pn a1 log + a2, where a1 and a2 are fitted constants. This finding suggests that the attention mechanism in Transformers could benefit from reformulating Softmax as: zi (cid:55) e(s log n+b)zi j=1 e(s log n+b)zj (cid:80)n = nszi ebzi j=1 nszj ebzj (cid:80)n , (4) where and are layerand head-specific learnable scalar parameters. We refer to as the bias parameter in this formulation. Based on these findings, further evaluations conducted in Section 3 reveal that while the inclusion of the bias parameter slightly accelerates loss reduction during pretraining, omitting leads to better length generalization performance. 2.2. Justification for the Design of SSMax Let = (z1, z2, . . . , zn) be an input vector of size n, where zmax, z2nd, and zmin denote its maximum, second largest, and minimum elements, respectively. For simplicity, we 2 1 n2 , n2 , . . . , n1 Figure 3. An example illustrating the behavior of Softmax and SSMax for an input vector of size given by n2 , 1, zmax). The horizontal axis represents (0, the value of zmax, while the vertical axis represents its transformed value. The red and orange lines correspond to SSMax with different scaling parameters s, and the blue lines correspond to Softmax, with line styles indicating different input vector sizes. This figure demonstrates that, under Softmax, the value of zmax required to focus attention increases indefinitely as grows. In contrast, SSMax ensures that attention is focused as long as zmax exceeds the other values by approximately , regardless of n. assume zmax > z2nd in the following analysis. When is processed by Softmax, zmax is transformed as zmax (cid:55) ezmax j=1 ezj (cid:80)n . The right-hand side of Equation (5) can be evaluated as ezmax j=1 ezj (cid:80)n = ezmax (n 1)ezmin + ezmax 1 n1 ezmaxzmin + 1 . (5) (6) From Equation (6), it follows that the maximum element of the output vector produced by Softmax approaches zero as the input vector size increases. On the other hand, when is processed by SSMax, zmax is transformed as zmax (cid:55) nszmax j=1 nszj (cid:80)n . (7) Assuming > 0 for simplicity1, the right-hand side of Equation (7) can be evaluated as nszmax j=1 nszj (cid:80)n = nszmax (n 1)nszmin + nszmax 1 n1 ns(zmaxzmin) + , (8) 1For < 0, similar argument applies by substituting (cid:55) and (cid:55) z. 3 Scalable-Softmax Is Superior for Attention nszmax j=1 nszj (cid:80)n = nszmax (n 1)nsz2nd + nszmax 1 n1 ns(zmaxz2nd) + . tation can be reformulated based on Equation (2) as (9) an = SSMax (cid:19) (cid:18) qnK 1:n From Equations (8) and (9), the maximum element of the output vector produced by SSMax exhibits the following properties as increases: If zmax z2nd > 1 , the maximum value approaches 1, indicating that attention is focused on the element with the highest value. If zmax zmin < 1 , the maximum value approaches 0, meaning that attention is distributed across all elements. Thus, SSMax ensures that attention is focused on elements whose values exceed others by approximately 1 , while distributing attention when all values are within range of approximately 1 . This design allows the model to adapt its attention allocation dynamically, focusing on key tokens when significant differences exist or distributing attention when the input values are relatively uniform. 2 1 n2 , n2 , . . . , n1 For instance, in Figure 1, since the condition zmax z2nd > 1 is satisfied, the maximum element of the output vector produced by SSMax approaches 1 as the input vector size increases. Another example is illustrated in Figure 3, where the input vector of size is given by n2 , 1, zmax). The figure demonstrates (0, that, under Softmax, the value of zmax required to focus attention on the corresponding element grows indefinitely as increases. In contrast, SSMax ensures that attention is focused on the element associated with zmax as long as it exceeds other values by approximately 1 , regardless of n. This property allows SSMax to effectively allocate attention to key elements, even as the input vector size increases significantly. 2.3. Seamless Implementation of SSMax The implementation of SSMax is straightforward and requires minimal modifications to existing Transformer architectures. For simplicity, we describe the standard dense attention mechanism without sparse attention variants. In standard Transformers, the attention scores for the n-th token an Rn are typically computed using the query vector of the n-th token qn Rd and the key tensor K1:n Rnd, which contains the key vectors of the first tokens, as an = Softmax . (10) (cid:19) (cid:18) qnK 1:n When replacing Softmax with SSMax, the attention compu4 = Softmax (cid:18) (s log n)qnK 1:n (cid:19) , (11) where is learnable scaling parameter. As shown in Equation (11), replacing Softmax with SSMax simply requires multiplying the query vector qn by log n. This simplicity ensures that SSMax can be seamlessly integrated into existing architectures with minimal modifications. 3. Evaluations To evaluate the impact of replacing Softmax with SSMax in the attention layers, we conducted series of experiments focusing on long-context generalization and attention mechanisms. First, we compared the learning curves during pretraining to assess whether SSMax improves training efficiency (Section 3.1). Next, we analyzed the models ability to generalize to longer sequences by measuring per-position test loss on sequences that were approximately 20 times longer than during training (Section 3.2). We then evaluated key information retrieval performance using the Needle-InA-Haystack test, assessing whether SSMax improves key information extraction even when the context size is extended up to approximately 10 times the training sequence length (Section 3.3). Finally, we analyzed attention score allocation to assess how SSMax influences focus on key information during retrieval tasks (Section 3.4). Transformer Architecture and Configurations The Transformer architecture used in these experiments includes enhancements similar to those in Llama 2 (Touvron et al., 2023), such as RoPE (Su et al., 2024), RMSNorm (Zhang & Sennrich, 2019), SwiGLU (Shazeer, 2020; Ramachandran et al., 2017), and the removal of bias in projection layers. The model has 12 layers, 12 attention heads, and 162M parameters. RoPE was initialized with θ = 10, 000. Details of the model configuration are provided in Appendix A. For tokenization, we employed the GPT-2 tokenizer (Radford et al., 2019), and the training sequence length was set to 1024. Evaluated Configurations We evaluated the following six variants, each replacing Softmax in the attention layers: (a) Softmax: The standard Softmax function, as defined in Equation (1). (b) SSMax: The proposed Scalable-Softmax (SSMax) function, as defined in Equation (2). The scaling parameter is modeled as learnable scalar, independently Scalable-Softmax Is Superior for Attention Figure 4. Learning curves comparing the standard Transformer (a) and SSMax variants (b)(d). All SSMax variants achieve consistently lower training loss compared to (a). Among them, the model with SSMax incorporating bias parameter (d) exhibits the lowest loss throughout training. The results also indicate that removing the scaling parameter, as in (c), has little impact on the learning curve compared to (b). learned for each attention layer and head. Since the model consists of 12 layers and 12 heads, this modification adds only 144 additional parameters, which is negligible compared to the total model size of 162M. (c) SSMax without Scaling Parameter: simplified SSMax variant with the scaling parameter removed, equivalent to substituting = 1 in Equation (2). (d) SSMax with Bias Parameter: variant of SSMax that incorporates learnable bias parameter b, as defined in Equation (4). Both and are learnable scalars, each specific to every layer and head, resulting in an additional 288 parameters in total. (e) Softmax Replaced by SSMax After Pretraining: After pretraining, Softmax is replaced with SSMax. The scaling parameter is initialized to the reciprocal of the average log value during training, which is approximately 0.168, as given by 0.168. (cid:80)1024 1024 n=1 log (f) Softmax Replaced by SSMax During Pretraining: The model is first pretrained with Softmax for 175,000 iterations, then switched to SSMax and further trained for 25,000 iterations. The scaling parameter is initialized as in (e), and 1000-iteration learning rate warmup is applied following the switch. 3.1. Learning Curve Analysis We pretrained the models on the SlimPajama dataset (Soboleva et al., 2023), compressed version of RedPajama (Computer, 2023). The models were trained with batch size of 5 Figure 5. Per-position test loss across context sizes up to 20,000. The x-axis represents context size, and the y-axis represents test loss. RoPEs θ was set to 50 times the training value, with no additional training after modification. The gray dotted line indicates the training sequence length of 1024. Results correspond to configurations (a)(f). SSMax models (b) and (c) demonstrate improved long-context generalization compared to (a), while (d) exhibits degraded performance due to the bias parameter. Model (e), where Softmax was replaced with SSMax post-training, struggles with shorter contexts, whereas (f), which switched to SSMax during the final phase of pretraining, achieves performance somewhat close to (b), though not entirely equivalent. 2048 using the AdamW optimizer (Loshchilov & Hutter, 2019), with learning rate of 6 104. Training spanned 200,000 iterations, corresponding to approximately 419B tokens. Details of the pretraining hyperparameters are provided in Appendix B. We compared the learning curves of the standard Transformer (a) and SSMax variants (b)(d). Figure 4 shows that all SSMax variants achieved consistently lower training loss compared to the standard Transformer (a). For example, SSMax (b) resulted in an approximate 0.008 reduction in training loss. Among the variants, the model with SSMax incorporating bias parameter (d) exhibited the lowest loss throughout training. Additionally, comparison between (b) and (c) indicates that removing the scaling parameter has little impact on the learning curve. 3.2. Generalization to Longer Contexts To assess long-sequence modeling capabilities, we increased RoPEs θ from 10,000 to 500,000. Note that no additional training was performed after modifying θ. This was done to isolate the impact of adjusting RoPEs θ and evaluate how well models generalize to longer contexts without further training. We evaluated models (a)(f) by computing the perposition test loss using 100,000 sequences of length 20,000 randomly sampled from the SlimPajama test set. Figure Scalable-Softmax Is Superior for Attention presents the results. The standard Transformer (a) struggles significantly with extended context sizes, showing noticeable increase in loss. Furthermore, (a) is not robust to substantial changes in RoPEs θ, exhibiting degraded performance even at shorter context sizes compared to the original training setup. In contrast, SSMax models (b) and (c) maintain lower test loss across long contexts, demonstrating improved generalization to sequence lengths up to approximately 10 times the training sequence length. Additionally, Figure 5 indicates that SSMax models are notably more robust to modifications in RoPEs θ than (a). The bias parameter (d), while improving training efficiency (Section 3.1), weakens long-context performance. Although (d) still outperforms (a), it fails to fully preserve the benefits of SSMax, resulting in an intermediate performance between (a) and (b), indicating that the bias parameter degrades its long-context performance. Regarding models where Softmax was replaced with SSMax after or during pretraining, Figure 5 shows that (e), in which Softmax was replaced with SSMax post-training, exhibits higher loss at shorter context sizes, suggesting that posttraining replacement struggles to adapt to shorter contexts. Meanwhile, (f), where SSMax was introduced during the final phase of pretraining, achieves performance somewhat close to (b), though not entirely equivalent. 3.3. Key Information Retrieval The Needle-In-A-Haystack test (Kamradt, 2023) is widely used to evaluate models ability to retrieve key information embedded in long context. In this study, we conduct similar test, where the model must recall randomly assigned seven-digit number corresponding to randomly chosen city name, following the setup proposed in (Arize AI, 2023). Each sample contains sentence, referred to as the needle, such as The special magic Tokyo number is: 8106422. This needle is inserted at random location within the context, and the model is prompted to retrieve the correct number given the city name. To evaluate key information retrieval, we first fine-tuned all six pretrained models (a)(f) using Supervised Fine-Tuning. We used the SQuAD 2.0 dataset (Rajpurkar et al., 2018; 2016), selecting 86,820 examples where the context, question, and answer fields were all non-empty. Fine-tuning was conducted for 10 epochs, and the loss was computed as the sum of token-level losses for the answer span. Further details on fine-tuning hyperparameters are provided in Appendix C. Figure 6. Needle-In-A-Haystack test results. The horizontal axis represents context size, while the vertical axis denotes the depth at which the needle is embedded within the context. Colors indicate retrieval accuracy. RoPEs θ was set to 500,000, 50-fold increase from the pretraining value. The standard Transformer (a) fails to retrieve key information beyond short context sizes, while the SSMax model (b) maintains high retrieval accuracy even at context sizes approximately 10 times longer than in training. Models (c) and (d) show lower retrieval accuracy than (b), demonstrating that removing the scaling parameter or introducing bias parameter degrades retrieval performance. Models where Softmax was replaced with SSMax after pretraining (e) and during pretraining (f) show partial improvements over (a) but remain far below (b). generalization to longer contexts without adaptation. For evaluation, we inserted the needle at five different depths within the context: 10%, 30%, 50%, 70%, and 90%. Each configuration was tested with 1000 samples for each depthcontext size combination. Decoding was performed greedily, selecting the most probable non-eos token at each step without sampling. Following fine-tuning, we increased RoPEs θ to 500,000, 50-fold increase from training. No additional training was performed after modifying θ, allowing us to assess Figure 6 presents the results of the Needle-In-A-Haystack test, demonstrating how different attention mechanisms affect key information retrieval performance in extended con6 Scalable-Softmax Is Superior for Attention 3.4. Attention Allocation to Key Information To further investigate key information retrieval, we analyzed how much attention each model allocates to key information during inference. Specifically, we define the needle score as the sum of attention scores assigned to the span starting immediately after the colon (:) and extending to the end of the needle, fully covering the seven-digit number. This metric quantifies how effectively attention layers and heads focus on key information. The attention scores are measured when generating the first token of the models response, which is expected to correspond to the initial part of the correct seven-digit number. As preliminary analysis, we examined needle scores in fixed setting with RoPEs θ set to 500,000, context size of 8000, and the needle sentence The special magic Tokyo number is: 8106422. positioned at depth of 50%. Figure 7 presents the needle scores for all layers and attention heads, sorted in descending order. This example highlights stark contrast in attention allocation between models with SSMax and the standard Transformer. Among all models, model (b) demonstrates particularly strong ability to focus on key information, achieving the highest needle scores. In contrast, the standard Transformer (a) fails to allocate meaningful attention to key information, with its highest needle scores remaining close to zero. To obtain more comprehensive understanding, we conducted large-scale evaluation with 100 trials per model, keeping the context size fixed at 8000 tokens while varying the needle position and needle content. Figure 8 presents the top needle scores observed in each trial, ranked in descending order. The figure also categorizes retrieval outcomes using different markers to indicate whether the full seven-digit number was correctly retrieved, only the first digit was correct, or the retrieval failed entirely. While not perfectly correlated, the results reveal strong association between higher top needle scores and successful key information retrieval. Among all models, model (b) with SSMax consistently produces top needle scores that remain among the highest across trials, demonstrating its strong ability to focus attention on key information, whereas the standard Transformer (a) fails to allocate meaningful attention. Model (c), which removes the scaling parameter, exhibits lower top needle scores than (b), despite previously demonstrating comparable performance in Sections 3.1 and 3.2. This suggests that the scaling parameter contributes to more effective attention allocation to key information. Model (d), which incorporates bias parameter, also shows reduced top needle scores, performing better than (a) but significantly worse than (b). Similarly, models where Softmax was replaced with SSMax after or during pretraining show partial improvements over (a), but their top needle scores remain lower than those of model (b). In particular, model Figure 7. Needle score distribution across attention layers and heads. The horizontal axis represents attention heads ranked by needle score (highest to lowest), while the vertical axis shows the corresponding needle score. Note that only the top 25 heads are shown for clarity, rather than all 144 heads. RoPEs θ was set to 500,000, 50-fold increase from pretraining. The context size was 8000, with the needle sentence The special magic Tokyo number is: 8106422. inserted at depth of 50%. The results demonstrate that the standard Transformer (a) fails to allocate significant attention to key tokens, whereas SSMax (b) effectively concentrates attention on them. Models (c), (d), (e), and (f) allocate more attention than (a) but fail to match the focus achieved by (b). Inference results indicate that (a) failed retrieval entirely, (b) and (c) successfully retrieved the correct number, and (d), (e), and (f) retrieved only the first digit but failed to recall the full number. texts. The standard Transformer (a) fails to retrieve key information beyond short context sizes. In contrast, the SSMax model (b) achieves significantly improved retrieval performance, successfully retrieving key information even at context sizes approximately 10 times longer than in training. Model (c), which removes the scaling parameter, performs noticeably worse than (b), despite demonstrating comparable performance in Sections 3.1 and 3.2. This suggests that the scaling parameter plays role in key information retrieval. Model (d), incorporating bias parameter, performs better than (a) but exhibits substantial degradation in performance compared to (b), indicating that the bias parameter negatively impacts key information retrieval. For models where Softmax was replaced with SSMax after pretraining (e) or during pretraining (f), (e) achieves substantial improvement over (a) but still performs noticeably worse than (b), suggesting limited adaptability when switching post-training. While (f) slightly outperforms (e), it remains significantly behind (b), highlighting the advantage of training with SSMax from the start rather than switching later. These findings confirm SSMaxs effectiveness in longcontext retrieval, particularly when applied throughout pretraining. 7 Scalable-Softmax Is Superior for Attention Figure 8. Top needle score distribution across models. Each model was evaluated over 100 trials, and the highest needle score from each trial (corresponding to the leftmost value in Figure 7) was recorded. The horizontal axis represents the rank of the top needle scores, sorted in descending order, while the vertical axis shows the corresponding score. Different markers indicate whether the retrieved number was fully correct (), incorrect but with the first digit correct (), or completely incorrect (). RoPEs θ was set to 500,000, 50-fold increase from pretraining. Context size was fixed at 8000, with city names, numbers, and insertion depths randomly assigned. The results confirm that the standard Transformer (a) fails to focus attention on key tokens, whereas SSMax (b) exhibits strong concentration. Models (c), (d), (e), and (f) show partial improvements over (a) but fail to match (b)s level of attention focus. (f), where SSMax was introduced in the later stage of pretraining, achieves higher top needle scores than model (e), yet still falls short of (b). These results indicate that SSMax significantly enhances attention allocation to key information, especially when incorporated from the beginning of pretraining. Furthermore, they highlight the importance of the scaling parameter in maintaining effective attention distribution and suggest that introducing bias parameter weakens attention to key information. 4. Conclusion In this paper, we proposed Scalable-Softmax (SSMax), novel alternative to Softmax in Transformer attention layers. SSMax addresses the issue of attention fading and enhances length generalization, enabling models to maintain attention over long contexts. Unlike Softmax, which suppresses attention scores as the input size increases, SSMax helps models retain focus on key information. Through extensive evaluations, we demonstrated the effectiveness of SSMax across multiple aspects of Transformer performance. Models with SSMax consistently achieved lower loss values during pretraining, indicating improved optimization efficiency. When applied to longer contexts, these models retained significantly lower test loss than the standard Transformer, even as the context size extended well beyond the training sequence length. In key information retrieval tasks, SSMax models exhibited superior accuracy, successfully extracting relevant information even at context sizes up to ten times longer than those seen during training. Attention score analysis confirmed that SSMax improves attention allocation to key tokens, enhancing models ability to retrieve key information in long-context scenarios. While models trained with SSMax from the beginning of pretraining demonstrated the strongest generalization ability, we also found that models could benefit from SSMax even when introduced at later stages. Replacing Softmax with SSMax during or after pretraining led to noticeable improvements, demonstrating its adaptability as an enhancement for both newly trained and existing pretrained models. These findings suggest that SSMax is promising approach for addressing the limitations of Softmax in Transformer attention mechanisms, particularly for tasks involving extended contexts. In the future, SSMax has the potential to replace Softmax in the attention layers of all Transformerbased LLMs, including existing pretrained models. Its adaptability and ability to improve length generalization position it as strong candidate for standard adoption in Transformer architectures."
        },
        {
            "title": "Acknowledgements",
            "content": "This research was conducted using NVIDIA GPGPU at the Center of Innovations for Sustainable Quantum AI (JST Grant Number JPMJPF2221), and the FUJITSU Supercomputer PRIMEHPC FX1000 and FUJITSU Server PRIMERGY GX2570 (Wisteria/BDEC-01) at the Informa8 Scalable-Softmax Is Superior for Attention tion Technology Center, The University of Tokyo. KMN is supported by the Daikin Endowed Research Unit: Research on Physics of Intelligence, School of Science, the University of Tokyo, and the Center of Innovation for Sustainable Quantum AI (JST Grant Number JPMJPF2221)."
        },
        {
            "title": "References",
            "content": "Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. Etc: Encoding long and structured inputs in transformers. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268284, 2020. Arize AI. Needle in haystack - pressure testing llms, 2023. URL https://github.com/Arize-ai/ LLMTest_NeedleInAHaystack2. Accessed on Jan 19, 2024. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:18771901, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Computer, T. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. URL https://github.com/togethercomputer/ RedPajama-Data. Gupta, A. and Berant, J. Gmat: Global memory augmentation for transformers. arXiv preprint arXiv:2006.03274, 2020. Kamradt, G. Needle in haystack - pressure testing llms, 2023. URL https://github.com/gkamradt/ LLMTest_NeedleInAHaystack. Accessed on Jan 19, 2024. Kazemnejad, A., Padhi, I., Natesan Ramamurthy, K., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024a. Liu, X., Yan, H., An, C., Qiu, X., and Lin, D. Scaling laws of roPE-based extrapolation. The Twelfth International Conference on Learning Representations, 2024b. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. International Conference on Learning Representations, 2019. Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. OpenAI blog, 2019. Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, 2016. Rajpurkar, P., Jia, R., and Liang, P. Know what you dont know: Unanswerable questions for SQuAD. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 2:784789, 2018. Ramachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:5368, 2021. Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. North American Chapter of the Association for Computational Linguistics, pp. 464468, 2018. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., SlimPajama: 627B Hestness, J., and Dey, N. 9 Scalable-Softmax Is Superior for Attention token cleaned and deduplicated version of RedPahttps://www.cerebras.net/blog/ jama. slimpajama-a-627b-token-cleaned-anddeduplicated-version-of-redpajama, June 2023. URL https://huggingface.co/ datasets/cerebras/SlimPajama-627B. longer sequences. Advances in Neural Information Processing Systems, 33:1728317297, 2020. Zhang, B. and Sennrich, R. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A. Adaptive attention span in transformers. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 331335, 2019. Touvron, H., Martin, L., Stone, K. R., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D. M., Blecher, L., Canton Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A. S., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I. M., Korenev, A. V., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Hall, M., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. Wang, J., Ji, T., Wu, Y., Yan, H., Gui, T., Zhang, Q., Huang, X., and Wang, X. Length generalization of causal transformers without position encoding. arXiv preprint arXiv:2404.12224, 2024. Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 2482424837, 2022. Ye, T., Dong, L., Xia, Y., Sun, Y., Zhu, Y., Huang, G., and Wei, F. Differential transformer. arXiv preprint arXiv:2410.05258, 2024. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big bird: Transformers for 10 Scalable-Softmax Is Superior for Attention A. Model Architecture Details Table 1 presents the architectural details of the models used in Sections 2.1 and 3. Parameter"
        },
        {
            "title": "Number of Layers\nNumber of Attention Heads\nHidden Size\nFeedforward Size\nVocabulary Size",
            "content": "Value 12 12 768 2,048 50,257 Table 1. Model architecture details. B. Pretraining Hyperparameters Table 2 presents the detailed hyperparameters for pretraining in Sections 2.1 and 3.1."
        },
        {
            "title": "Value",
            "content": "Optimizer Adam β parameters Weight decay Gradient clipping threshold Learning rate Learning rate scheduler Warmup steps Sequence length Batch size (tokens per update) RoPE θ Dropout Data type AdamW (0.9, 0.95) 0.1 (applied only to parameters of rank 2) 1.0 6 104 Constant 1,000 1,024 2,048 (2,097,152 tokens) 10,000 0.0 bfloat16 Table 2. Pretraining hyperparameters. C. Supervised Fine-Tuning Hyperparameters Table 3 presents the detailed hyperparameters for supervised fine-tuning in Section 3.3. Parameter Value Optimizer Adam β parameters Weight decay Gradient clipping threshold Learning rate Learning rate scheduler Warmup period (epochs) Sequence length Batch size (tokens per update) RoPE θ Dropout Data type AdamW (0.9, 0.999) 0.0 1.0 2 105 Cosine 1.0 1,024 128 (131,072 tokens) 10,000 0.0 bfloat16 Table 3. Fine-tuning hyperparameters."
        }
    ],
    "affiliations": [
        "Institute for Physics of Intelligence, The University of Tokyo, Tokyo 113-0033, Japan"
    ]
}