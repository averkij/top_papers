{
    "paper_title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
    "authors": [
        "Chujie Zheng",
        "Kai Dang",
        "Bowen Yu",
        "Mingze Li",
        "Huiqiang Jiang",
        "Junrong Lin",
        "Yuqiong Liu",
        "Hao Lin",
        "Chencan Wu",
        "Feng Hu",
        "An Yang",
        "Jingren Zhou",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research."
        },
        {
            "title": "Start",
            "content": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices 2025-12-04 Chujie Zheng Junrong Lin Kai Dang Yuqiong Liu An Yang Bowen Yu Mingze Li Hao Lin Jingren Zhou Chencan Wu Junyang Lin Huiqiang Jiang Feng Hu 5 2 0 2 3 ] . [ 3 4 7 3 1 0 . 2 1 5 2 : r Qwen Team, Alibaba Inc."
        },
        {
            "title": "Abstract",
            "content": "This paper proposes novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through first-order approximation, we show that this surrogate becomes increasingly valid only when both the traininginference discrepancy and policy staleness are minimized. This insight provides principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixtureof-Experts (MoE) models. Through extensive experiments with 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has become key technical paradigm for enhancing large language models (LLMs) ability to tackle complex problem-solving tasks (OpenAI, 2024; Guo et al., 2025; Yang et al., 2025), while stable training process1 is crucial for successfully scaling RL. Due to the contextual nature of language, RL with LLMs usually employs sequence-level rewards, i.e., scalar score assigned based on the complete model response. However, mainstream RL algorithms, such as REINFORCE and GRPO, typically employ token-level optimization objectives. This mismatch between the reward (assigned at the sequence level) and the optimization unit (typically at the token level) raises concerns about the soundness and training stability of such approaches, while some studies have proposed directly adopting sequence-level optimization objectives (Zheng et al., 2025; Liu et al., 2025a). In particular, token-level optimization objectives also pose unique challenges for RL training with Mixture-of-Experts (MoE) models. For instance, the dynamic expert routing mechanism can invalidate the token-level importance sampling ratios in MoE models (Zheng et al., 2025). However, it remains unclear whether optimizing sequence-level rewards using token-level objectives is justified, and if so, to what extent (or under what conditions) such an approach is valid. In this paper, we propose novel formulation for RL with LLMs. The key insight is that, to optimize the expected sequence-level reward, we can employ surrogate token-level objective as its first-order approximation. Specifically, this approximation is likely to hold only when both (1) the numerical discrepancy between the training and inference engines (i.e., the traininginference discrepancy) and (2) the discrepancy between the rollout policy that samples responses and the target policy to be optimized (i.e., policy staleness) are minimized. This insight provides principled explanation of how several Corresponding authors. 1By stable training, we refer to training process in which model performance steadily improves over training stepsreflected in both the training reward and benchmark scoresand, crucially, the models internal state evolves smoothly and without abrupt shifts. The model state can be monitored via set of diagnostic metrics, including the training-inference KL divergence and entropy reported in our later experiments ( 4). Stable training implies that the model can consistently improve in healthy manner throughout extended or multi-stage training, with lower risk of unexpected behaviors. 1 techniques for stabilizing RL training work. For example, (1) the importance sampling weight is an inherent component of the surrogate token-level objective under the first-order approximation; (2) the clipping mechanism can restrain policy staleness by preventing aggressive policy updates; (3) for MoE models, the Routing Replay approach (Zheng et al., 2025; Ma et al., 2025), which fixes the routed experts during policy optimization, can reduce both the traininginference discrepancy and policy staleness. To empirically validate our insight and investigate practical recipes for stable RL training, we conduct extensive experiments with 30B MoE model, amounting to hundreds of thousands of GPU hours. Our main conclusions include: (1) For on-policy training2, the basic policy gradient algorithm with importance sampling correction yields the highest training stability; (2) When off-policy updates are introduced to accelerate convergence, i.e., large batch of responses is split into mini-batches for multiple gradient updates, combining clipping and Routing Replay becomes necessary to mitigate instability caused by policy staleness; (3) Once training is stabilized, models with different cold-start initializations consistently achieve comparable final performance. This motivates future work to focus more on RL itself rather than overly on the specifics of cold-start initialization, as differences arising from the latter are expected to vanish given prolonged RL training. In summary, this paper makes contributions along two axes: Theoretically, we propose novel formulation for reinforcement learning with LLMs, revealing the conditions under which optimizing sequence-level rewards via token-level objectives is justified. Specifically, the validity of the underlying first-order approximation hinges on jointly minimizing the traininginference discrepancy and policy staleness. Empirically, through extensive experiments with MoE models spanning hundreds of thousands of GPU hours, we demonstrate that several techniques that preserve the validity of the first-order approximation consistently exhibit practical efficacy in stabilizing RL training, particularly the Routing Replay approach tailored for MoE models. We hope that the developed recipes for stable RL training will facilitate future research."
        },
        {
            "title": "2 Formulation for Reinforcement Learning with LLMs",
            "content": "2.1 Notation We define an autoregressive LLM parameterized by θ as policy πθ. We use to denote an input prompt and as the prompt set. Under the policy πθ, the likelihood of response to prompt is denoted as πθ(yx) = t=1 πθ(ytx, y<t) where is the number of tokens in y. Given the contextual nature of language, we focus on the sequence-level reward setting, where whole response is assigned single scalar reward R(x, y). We do not consider the value-based setting (e.g., PPO, Schulman et al. 2017), where policy optimization is steered by value model that assigns scalar scores to each token in response y. This is because we found it inherently difficult (if not impossible) to devise general and scalable approaches to obtaining reliable value models. 2.2 Expected Sequence-level Reward is Hard to Directly Optimize Our formulation starts from the true sequence-level reward that we aim to maximize: seq(θ) = xD, yπθ (x) [R(x, y)] , where πθ is the target policy to be optimized. Since the responses are typically not sampled in the training engine (e.g., Megatron and FSDP) but instead in the inference engine (e.g., SGLang and vLLM), we adopt the importance sampling (IS) trick to do simple transformation: seq(θ) = xD, yπθ (x) [R(x, y)] = xD, yµθold (x) πθ(yx) (yx) µθold (cid:123)(cid:122) (cid:125) (cid:124) sequence-level IS weight R(x, y) , (1) where µθold denotes the rollout policy that samples responses. Note that we use the notation µ to distinguish the policy in the inference engine from the policy (notated as π) in the training engine, as 2In this paper, we use the term on-policy to indicate that the rollout policy that samples responses is identical to the target policy to be optimized using these responses (omitting the traininginference discrepancy), while off-policy indicates that the two policies are different. 2 there typically exists numerical discrepancy between training and inference engines (Yao et al., 2025). The sequence-level objective in Equation (1) has the following gradient: θ seq(θ) = xD, yµθold (x) = xD, yµθold (x) (cid:20) πθ(yx) (yx) µθold πθ(yx) (yx) µθold (cid:34) R(x, y) θ log πθ(yx) (cid:21) R(x, y) t=1 θ log πθ(ytx, y<t) . (2) (cid:35) However, this gradient is usually intractable to utilize due to the large numerical range and high variance (yx)), making it difficult to directly optimize the sequenceof sequence likelihood (i.e., πθ(yx) and µθold level objective in Equation (1). 2.3 Token-level Objective as First-order Approximation to Sequence-level Objective The critical step in our formulation is to consider the following surrogate token-level objective: token(θ) = xD, yµθold (x) t=1 πθ(ytx, y<t) (ytx, y<t) µθold (cid:124) (cid:125) (cid:123)(cid:122) token-level IS weight R(x, y) , (3) with the following gradient: θ token(θ) = xD, yµθold (x) (cid:34) t=1 πθ(ytx, y<t) (ytx, y<t) µθold R(x, y) θ log πθ(ytx, y<t) . (4) (cid:35) which is actually the basic policy gradient algorithm (i.e., REINFORCE) equipped with token-level IS weight. The core insight here is that we can view the token-level optimization objective in Equation (3) as first-order approximation to the sequence-level objective in Equation (1) that we truly aim to optimize. To be specific, suppose πθ and µθold are slightly different, let πθ (ytx,y<t) (ytx,y<t) = 1 + δt where δt is small quantity. We have the following approximation: µθold πθ(yx) (yx) µθold = t=1 (1 + δt) 1 + δt + (cid:16) δ2(cid:17) 1 + t=1 t=1 δt, where the rightmost derivation neglects second-order and higher-order small terms like δiδj. So we have: θ seq(θ) = xD, yµθold (x) (cid:20) R(x, y) θ (cid:19)(cid:21) (cid:18) πθ(yx) (yx) µθold (cid:32) R(x, y) θ 1 + (cid:33)(cid:35) t=1 δt xD, yµθold (x) (cid:34) (cid:34) = xD, yµθold (x) R(x, y) θ (cid:32) t=1 πθ(ytx, y<t) (ytx, y<t) µθold (cid:33)(cid:35) = θ token(θ). This is why we say that Equation (3) is first-order approximation to Equation (1). Therefore, when πθ is close to µθold , we can improve the sequence-level objective in Equation (1) by updating the model parameters θ with the gradient in Equation (4). 2.4 Conditions for First-order Approximation to Hold For the first-order approximation to hold, we require that the target policy πθ and the rollout policy µθold are close, which, however, is less intuitive. To be clear, given and for each token yt, we rewrite its IS weight as: πθ(ytx, y<t) (ytx, y<t) µθold = πθold µθold (cid:124) (ytx, y<t) (ytx, y<t) (cid:125) (cid:123)(cid:122) traininginference discrepancy πθ(ytx, y<t) (ytx, y<t) πθold (cid:125) (cid:124) (cid:123)(cid:122) policy staleness , (5) 3 where πθold denotes the rollout policy computed by the training engine, differing from the one µθold in the inference engine. Therefore, from the decomposition in Equation (5), the gap between πθ and µθold comes from two aspects: the traininginference discrepancy and policy staleness. Regarding the traininginference discrepancyi.e., the numerical differences between training and inference enginesthe causes are usually complex and heavily tied to the underlying infrastructure. For example, training and inference engines typically employ different computational kernels for peak performance, which would yield inconsistent outputs given the same model input. Even within single engine, particularly the inference side, batch-invariant kernels (He and Lab, 2025) are often disabled for maximizing throughput, so the same model input can still receive variant outputs. In the case of MoE models, the traininginference discrepancy is further amplified by inconsistent expert routing, which we will discuss detailedly in 3. Regarding policy stalenessi.e., the discrepancy between the rollout policy that samples responses and the target policy to be optimizedit usually arises from the trade-offs made to improve training efficiency and computational utilization. Since the rollout stage in RL is typically bounded in time by the generation length, to accelerate convergence through increased computational resources, we often split large batch of sampled responses into mini-batches for multiple gradient updates. Consequently, mini-batches consumed later may exhibit greater policy staleness. In asynchronous RL frameworks, single response can be generated sequentially by multiple model versions, which also introduces policy staleness. Therefore, to ensure the validity of the first-order approximation that underlies the surrogate tokenlevel objective in Equation (3), we should, in principle, narrow the gap between πθ and µθold from two directions: reducing the numerical discrepancy between training and inference engines, and controlling policy staleness within moderate range."
        },
        {
            "title": "3 Challenge for Mixture of Experts, and Routing Replay",
            "content": "3.1 Expert Routing Hinders First-order Approximation to Hold When it comes to Mixture-of-Experts (MoE) models (Guo et al., 2025; Yang et al., 2025), the conditions for the first-order approximation to hold become less straightforward. Specifically, during the forward pass of generating each token, MoE models dynamically select and activate only small subset of expert parameters via the expert routing mechanism. Incorporating expert routing into Equation (5), we can write the token-level IS weight for an MoE model as: πθ(ytx, y<t) (ytx, y<t) µθold = πθ(ytx, y<t, eπ ) (ytx, y<t, eµ old, t) µθold = πθold µθold (cid:124) (ytx, y<t, eπ (ytx, y<t, eµ (cid:123)(cid:122) traininginference discrepancy old, t) old, t) (cid:125) πθ(ytx, y<t, eπ ) (ytx, y<t, eπ old, t) (cid:123)(cid:122) (cid:125) policy staleness πθold (cid:124) , (6) where eπ and eµ denote the routed experts in the training and inference engines, respectively, and the subscript old corresponds to the rollout policy. At this point, the challenge of reinforcement learning with MoE models becomes clear: expert routing is entangled with the traininginference discrepancy and policy staleness, increasing the likelihood that the first-order approximation underlying the surrogate token-level optimization objective in Equation (3) breaks down. More specifically, the traininginference discrepancy can cause inconsistent routed experts in the training and inference engines (i.e., eπ old, t) given the same model parameters and input. This divergence in expert routing, in turn, further amplifies the discrepancy in final outputs. Furthermore, policy staleness manifests not only in changes in the model parameters (i.e., θ versus θold) but also in shifts of routed experts (i.e., eπ old, t), which can heavily alter the resulting policy defined by activated parameters. old, versus eµ versus eπ 3.2 Routing Replay Restores First-order Approximation, Yet May Introduce Bias Identifying that expert routing undermines the validity of the first-order approximation in MoE models, we can eliminate this impact through the Routing Replay (Zheng et al., 2025) approach. The core idea of Routing Replay is to stabilize RL training of MoE models by fixing the routed experts during policy optimization, thereby enabling the model to be optimized like dense one. Upon Equation (6), we formalize the following two concrete implementations of Routing Replay, namely Vanilla Routing Replay and Rollout Routing Replay: 4 Vanilla Routing Replay (R2) (Zheng et al., 2025) focuses on mitigating the impact of expert routing on policy staleness by replaying, during gradient updates, the routed experts determined by the rollout policy in the training engine (i.e., eπ old, t): πR2 µθold θ (ytx, y<t) (ytx, y<t) = πθ(ytx, y<t, eπ (ytx, y<t, eµ µθold old, t) old, t) = πθold µθold (ytx, y<t, eπ (ytx, y<t, eµ old, t) old, t) πθ(ytx, y<t, eπ (ytx, y<t, eπ πθold (cid:123)(cid:122) (cid:124) policy staleness old, t) old, t) (cid:125) . Rollout Routing Replay (R3) (Ma et al., 2025) aims to reduce the impact of expert routing on the traininginference discrepancy by uniformly replaying, within the training engine, the routed experts determined by the rollout policy in the inference engine (i.e., eµ old, t), which also simultaneously mitigates the impact of expert routing on policy staleness: (ytx, y<t, eµ (ytx, y<t, eµ (cid:123)(cid:122) traininginference discrepancy πθ(ytx, y<t, eµ (ytx, y<t, eµ πθold (cid:123)(cid:122) (cid:124) policy staleness πθ(ytx, y<t, eµ (ytx, y<t, eµ µθold θ (ytx, y<t) (ytx, y<t) old, t) old, t) (cid:125) old, t) old, t) (cid:125) πθold µθold (cid:124) old, t) old, t) πR3 µθold = = . Therefore, Routing Replay intuitively restores the validity of the first-order approximation in MoE models by reducing the traininginference discrepancy (in R3) and alleviating policy staleness (in R2 and R3). However, we point out that it also implicitly biases the target policy, as suggested by the notations πR2 θ and πR3 θ . Specifically, the original target policy we aime to optimize in Equation (3) is πθ, where the likelihood of each token yt should be governed by the naturally-routed experts eπ . However, Routing old, or eµ Replay constrains the routed experts to be eπ old, t, leading to another target policy πR2 that deviates from the original πθ defined by eπ . In particular, when we split large batch into mini-batches for multiple gradient updates, R2 and R3 can possess different degrees of bias, as shown in Table 1. The key difference is that R2 does not alter the original target policy in the first mini-batch, which we conjecture may lead to R2 and R3 exhibiting different performance, especially when the ratio between batch size and mini-batch size (i.e., the degree of off-policiness) is varied. θ or πR3 θ Table 1: Comparison between R2 and R3 in how they alter the original target policy πθ. (replaying eπ old, t) R3 (replaying eµ old, t) First mini-batch old, = eπ eπ , target policy is not altered eµ old, = eπ , target policy is altered Non-first mini-batch old, = eπ eπ , target policy is altered eµ old, = eπ , target policy is altered Nevertheless, it is difficult to definitively assess whether the advantages or disadvantages of Routing Replay outweigh each other. Altering routed experts, while introducing bias into the optimization objective, also makes the first-order approximationon which the altered token-level objective using πR2 θ or πR3 θ as the target policy reliesmore likely to hold. We need further experiments to validate the practical utility of Routing Replay."
        },
        {
            "title": "4 Empirical Analyses",
            "content": "4.1 MiniRL: Minimalist Baseline Algorithm In our experiments, we employ two minimal modifications to the REINFORCE optimization objective in Equation (3) as minimalist baseline algorithm. First, we apply group-normalization (Shao et al., 2024) to (x) [R(x, y)], the raw rewards as the advantage estimate for each response y: (cid:98)A(x, y) = R(x, y) which also lowers the variance of the raw rewards. Second, we adopt the clipping mechanism in PPO (Schulman et al., 2017) that prevents aggressive policy updates by stopping gradients for certain tokens, which can hopefully restrain policy staleness. We follow the decoupled PPO approach (Hilton et al., 2022) and use πθold as the proximal policy to decide whether to clip the token yt based on the ratio of (ytx, y<t)3. The obtained minimalist baseline algorithm, which we call MiniRL, is πθ(ytx, y<t) and πθold yµθold 3While there are alternative clipping strategies, such as clipping whole response based on the ratio of sequence likelihood (Zheng et al., 2025), we found that the current clipping strategy has worked decently. Therefore, we leave 5 as follows: JMiniRL(θ) = (x)"
        },
        {
            "title": "Mt sg",
            "content": "(cid:20) πθ(ytx, y<t) (ytx, y<t) µθold xD, yµθold (cid:34) t=1 0 if (cid:98)A(x, y) > 0 and rt > 1 + εhigh, 0 if (cid:98)A(x, y) < 0 and rt < 1 εlow, 1 otherwise, (cid:21) (cid:98)A(x, y) log πθ(ytx, y<t) , (7) (cid:35) rt = πθ(ytx, y<t) (ytx, y<t) πθold , Mt = where sg denotes the operation of stopping gradient. It is noteworthy that MiniRL is adopted as the baseline algorithm to maintain consistencyas closely as possible(in gradient) with the surrogate token-level objective in Equation 3, which has been justified by our formulation in 2. In Appendix A, we will provide comparison of MiniRL against other algorithms such as GRPO (Shao et al., 2024) and CISPO (Chen et al., 2025). All our experiments will be implemented based on MiniRL. 4.2 Experimental Setup We conduct experiments on the mathematical reasoning task, where the model response is compared with the ground truth answer and then assigned binary reward (i.e., R(x, y) {0, 1}). We curate 4,096 math problems with verified answers as the prompt set for RL training. We report the average accuracy over 32 sampled responses on the HMMT25, AIME25, and AIME24 benchmarks, each consisting of 30 competition-level math problems (90 in total). We experiment with cold-start model fine-tuned from Qwen3-30B-A3B-Base. We adopt the setting of FP8 inference and BF16 training, providing stress test for algorithmic correctness where the inference precision is lower than the training and the traininginference discrepancy is large. Besides the training reward, we also report the dynamics of two metrics: (1) the token-level entropy of the target policy, approximated by: (cid:34) (cid:35) [πθ] wV where denotes the vocabulary, and (2) the KL divergence between the rollout policies in the inference and training engines, calculated as: πθ(wx, y<t) log πθ(wx, y<t) xD, y<tµθold(x) , KL (cid:2)µθold πθold (cid:3) = xD, ytµθold(x, y<t ) (cid:20) log µθold πθold (ytx, y<t) (ytx, y<t) (cid:21) . We report the latter metric because recent work (Yao et al., 2025; Liu et al., 2025a) has revealed that the instability or collapse in RL training is often accompanied by sharp increase in the training-inference discrepancy. To conduct controlled experiments, we employ the standard synchronous RL framework. In each global step, we first sample batch of prompts and sample responses for each prompt using the rollout policy in the inference engine. Then, we split the responses into mini-batches and apply gradient updates in the training engine. The finally updated policy in this global step is used as the new rollout policy in the next global step. Across all experimental runs, we use the same mini-batch size of 1,024 responses (B = 64 and = 16) for each gradient update. For other hyperparameters, we set the maximum generation length to 32,768, and set εhigh to 0.27 and εlow to 0.2 in MiniRL. We additionally apply the Truncated Importance Sampling (TIS) trick (Yao et al., 2025) to the token-level IS weight in MiniRL, with the truncation threshold set to 5. Our experiments total hundreds of thousands of GPU hours, and the consumed compute can be estimated as 5 6 GPU hours per gradient step. 4.3 Results of On-policy Training We first verify, under on-policy training where the global batch size equals the mini-batch size, whether the validity of the first-order approximation underlying the token-level optimization objective is correlated with training stability. Under this on-policy setting where θ = θold, MiniRL degenerates to the following basic policy gradient algorithm: JMiniRL(θ) = xD, yµθold (x) (cid:34) t=1 πθold µθold (ytx, y<t) (ytx, y<t) (cid:98)A(x, y) log πθ(ytx, y<t) (cid:35) , the study of clipping or masking strategies for future work. Similarly, exploring better advantage estimates (cid:98)A(x, y) may also be helpful, but falls outside the scope of this work. 6 so the IS weight here serves only as correction for the traininginference discrepancy. We notice that existing RL algorithms, such as GRPO and CISPO, often employ length normalization in their optimization objectives, and their original objectives do not consider IS correction for the traininginference discrepancy. We thus include the following two ablated variants of MiniRL in our experiments: (cid:35) (cid:34) length-norm MiniRL (θ) = xD, yµθold (x) 1 y t= πθold µθold (ytx, y<t) (ytx, y<t) (cid:98)A(x, y) log πθ(ytx, y<t) , which additionally employs length normalization, and wo train-infer-is MiniRL (θ) = xD, yµθold (x) (cid:98)A(x, y) log πθ(ytx, y<t) , (cid:35) (cid:34) t=1 which omits the IS correction for the traininginference discrepancy. Note that the two variants have no longer satisfied the aforementioned first-order approximation, as their gradients are neither equal to nor linearly correlated with the gradient of the true sequence-level objective in Equation (1) (ignoring the reward normalization). We also equip MiniRL and the two variants with R3 (R2 is inapplicable here, see Table 1) for comparison. Figure 1: Results of on-policy training with gbs (global batch size) = mbs (mini-batch size) = 1, 024. From Figure 1, we draw the following observations and conclusions: MiniRL, i.e., the basic policy gradient algorithm with IS correction, achieves the best performance and training stability. Adding length normalization leads to suboptimal performance4, although training remains stable. This is as expected, since length normalization invalidates the first-order approximation to the true expected sequence-level reward, resulting in biased token-level optimization objective. Removing the traininginference IS correction causes rapid training collapse and sharp drop in entropy. This confirms that the IS weight is an inherent component of the first-order approximation, and omitting it immediately invalidates the token-level optimization objective. 4Our conclusion regarding length normalization aligns with that of Liu et al. (2025b), albeit with markedly different motivation. 7 Applying R3 in on-policy training does not yield performance gains, despite effectively reducing the traininginference discrepancy (as reflected by the traininginference KL divergence). Moreover, combining R3 with length normalization even degrades the benchmark score further, and applying R3 without the training-inference IS correction still fails rapidly5. This empirically confirms our speculation in 3.2that Routing Replay can alter the original target policy and introduce bias into the optimization objective. These results demonstrate that, in designing token-level optimization objectives, only those that preserve the validity of the first-order approximation to the expected sequence-level reward lead to improved training stability and performance. This also validates the soundness of our proposed formulation. 4.4 Results of Off-policy Training The inference time in RL is typically bounded by the generation length and cannot be accelerated by increasing computational resources. To leverage increased compute for faster convergence, common practice is to introduce off-policy updates. Within synchronous RL framework, this means that large batch of responses is split into mini-batches for multiple gradient updates. To investigate the recipes for stable RL training under off-policy settings, we experiment with three levels of off-policiness: with the mini-batch size fixed at 1,024 responses, the global batch size is varied to 2,048, 4,096, and 8,192, corresponding to = 2, 4, and 8, respectively. With MiniRL as the baseline, we compare the following methods: MiniRL (no clipping), MiniRL + R2 (no clipping), MiniRL + R2, and MiniRL + R3. Figure 2: Results of off-policy training with gbs = 2 mbs = 2, 048. From Figures 2 to 4, we draw the following observations and conclusions: Once off-policy updates are introduced, both Routing Replay and clipping become essential for stable training. As shown in Figures 2 and 3, omitting either Routing Replay or clipping causes training to collapse prematurely, thereby degrading peak performance. This indicates that Routing 5Our findings regarding R3 under the on-policy setting contradict those reported in Ma et al. (2025), which may stem from two factors: (1) They only validate R3 on small-scale experiments, with maximum of 180 global steps; (2) Our FP8 inference setting imposes more stringent stress test on algorithmic correctness, which cannot be adequately captured under their BF16 inference setting. 8 Figure 3: Results of off-policy training with gbs = 4 mbs = 4, 096. Figure 4: Results of off-policy training with gbs = 8 mbs = 8, 192. 9 Replay alleviates the impact of expert routing, and the clipping mechanism also effectively prevents aggressive policy updates, thereby both restraining policy staleness. When off-policiness is small (gbs = 2 mbs), R2 outperforms R3, while when off-policiness is large (gbs = 4 mbs and gbs = 8 mbs), R3 surpasses R2. Notably, under high off-policiness, R2 fails to sustain stable training, and its peak performance achieved before training collapse is also slightly lower than that of R3. Combining our analysis in 3.2particularly that R2 leaves the target policy of the first mini-batch unchanged while R3 alters itand the on-policy experimental results in 4.3, we hypothesize that when off-policiness is small, the detrimental impact of R3s alteration to the target policy outweighs its benefit in preserving the validity of the first-order approximation, while under larger off-policiness, the opposite holds true. In summary, we find that Routing Replay and clipping are necessary for stable off-policy training. When off-policiness is small, R2 is sufficient and more effective at stabilizing RL training for MoE models, whereas R3 becomes necessary under larger off-policiness. 4.5 Results of Varying Cold-start Initializations Recall the motivation for stabilizing RL training: given base model, once we can reach its performance limit through sufficiently long RL training, we can reliably enhance the models capabilities by investing computational resources into RL. To this end, we investigate whether models initialized with different cold-start data can achieve similar performance when trained using stable RL recipe. We compare three versions of cold-start data distilled from three frontier models: Qwen3-Max-Thinking-Preview, DeepSeek-R1-0528, and gpt-oss-120b (high mode). We report results based on an early-experimental small Qwen3Next MoE model, trained with global batch size of 4,096, mini-batch size of 2,048 (B = 128, = 16, = 2), and generation length of 65,536 tokens. We employ MiniRL + R2 as the training recipe. Figure 5: Results of varying cold-start initializations. In Figure 5, we show that the three cold-start initializations consistently achieve comparable final performance, which encourages us to focus more on RL itself rather than overly on the specifics of cold-start initialization. Furthermore, comparing Figures 1 to 4, we find that both on-policy and off-policy trainingonce stabilizedalso consistently achieve similar peak performance. These results further suggest that stable training plays decisive role in successfully scaling RL."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose new formulation for reinforcement learning with LLMs, viewing the token-level optimization objective as first-order approximation to the true expected sequence-level reward. Through extensive experiments, we demonstrate that techniques that preserve the validity of this first-order approximationsuch as importance sampling correction, clipping, and Routing Replay for MoE modelsall effectively stabilize RL training. We further investigate recipes for stable RL training across varying degrees of off-policiness and show that, once training is stabilized, the same base model consistently converges to similar performance with prolonged RL. We hope that the insights and empirical results shared in this paper will inspire and facilitate future research."
        },
        {
            "title": "References",
            "content": "Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Horace He and Thinking Machines Lab. Defeating nondeterminism in llm inference. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20250910. https://thinkingmachines.ai/blog/defeatingnondeterminism-in-llm-inference/. Jacob Hilton, Karl Cobbe, and John Schulman. Batch size-invariance for policy optimization. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=lXuZaxEaI7. Jiacai Liu, Yingru Li, Yuqian Fu, Jiawei Wang, Qian Liu, and Yu Shen. When speed kills stability: Demystifying RL collapse from the training-inference mismatch, September 2025a. URL https: //richardli.xyz/rl-collapse. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Wenhan Ma, Hailin Zhang, Liang Zhao, Yifan Song, Yudong Wang, Zhifang Sui, and Fuli Luo. Stabilizing moe reinforcement learning by aligning training and inference routers. arXiv preprint arXiv:2510.11370, 2025. OpenAI. Learning to reason with LLMs, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https://fengyao.notion. site/off-policy-rl. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025."
        },
        {
            "title": "A Comparison of MiniRL against GRPO and CISPO",
            "content": "We compare the optimization objective of MiniRL against those of GRPO (Shao et al., 2024) and CISPO (Chen et al., 2025). With the notations in this paper, GRPO employs the following objective: JGRPO(θ) = xD, {yi}G (cid:34) 1 i=1 i= µθold yi t=1 1 yi (x) (cid:16) min ri,t(θ) (cid:98)Ai,t, clip(ri,t(θ), 1 εlow, 1 + εhigh) (cid:98)Ai,t (cid:35) (cid:17) , and CISPO is as follows: JCISPO(θ) = (x) xD, yµθold (cid:34) 1 i=1 yi i=1 yi t=1 (cid:104) sg clip(ri,t(θ), 1 εlow, 1 + εhigh) (cid:105) (cid:98)Ai,t log πθ(ytx, y<t) , (cid:35) where in both objectives: ri,t(θ) = πθ(yi,tx, yi,<t) (yi,tx, yi,<t) πθold , (cid:98)Ai,t = R(x, yi) mean (cid:0){R(x, yi)}G i=1 std (cid:0){R(x, yi)}G i= (cid:1) (cid:1) . Their key differences from MiniRL include the following: (1) Their original objectives do not consider the traininginference discrepancy; (2) They both employ length normalization, which we show in 4.3 invalidates the first-order approximation to the true expected sequence-level reward and can lead to biased token-level optimization objective and suboptimal performance; (3) CISPO does not clip the gradient of certain tokens, which we show in 4.4 can result in unstable training."
        },
        {
            "title": "B Detailed Benchmark Results",
            "content": "Figure 6: Detailed benchmark results of on-policy training with gbs = mbs = 1, 024. Figure 7: Detailed benchmark results of off-policy training with gbs = 2 mbs = 2, 048. Figure 8: Detailed benchmark results of off-policy training with gbs = 4 mbs = 4, 096. 12 Figure 9: Detailed benchmark results of off-policy training with gbs = 8 mbs = 8, 192. Figure 10: Detailed benchmark results of varying cold-start initializations."
        }
    ],
    "affiliations": [
        "Alibaba Inc."
    ]
}