{
    "paper_title": "CAMAR: Continuous Actions Multi-Agent Routing",
    "authors": [
        "Artem Pshenitsyn",
        "Aleksandr Panov",
        "Alexey Skrynnik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving cooperative and competitive decision-making problems. While many MARL benchmarks have been proposed, few combine continuous state and action spaces with challenging coordination and planning tasks. We introduce CAMAR, a new MARL benchmark designed explicitly for multi-agent pathfinding in environments with continuous actions. CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000 environment steps per second. We also propose a three-tier evaluation protocol to better track algorithmic progress and enable deeper analysis of performance. In addition, CAMAR allows the integration of classical planning methods such as RRT and RRT* into MARL pipelines. We use them as standalone baselines and combine RRT* with popular MARL algorithms to create hybrid approaches. We provide a suite of test scenarios and benchmarking tools to ensure reproducibility and fair comparison. Experiments show that CAMAR presents a challenging and realistic testbed for the MARL community."
        },
        {
            "title": "Start",
            "content": "CAMAR: Continuous Actions Multi-Agent Routing Artem Pshenitsyn1,2, Aleksandr Panov1,2, Alexey Skrynnik1,2 1 AIRI, Moscow, Russia 2 MIPT, Moscow, Russia {pshenitsyn, panov, skrynnik}@airi.net 5 2 0 2 8 1 ] . [ 1 5 4 8 2 1 . 8 0 5 2 : r Abstract Multi-agent reinforcement learning (MARL) is powerful paradigm for solving cooperative and competitive decisionmaking problems. While many MARL benchmarks have been proposed, few combine continuous state and action spaces with challenging coordination and planning tasks. We introduce CAMAR, new MARL benchmark designed explicitly for multi-agent pathfinding in environments with continuous actions. CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000 environment steps per second. We also propose three-tier evaluation protocol to better track algorithmic progress and enable deeper analysis of performance. In addition, CAMAR allows the integration of classical planning methods such as RRT and RRT* into MARL pipelines. We use them as standalone baselines and combine RRT* with popular MARL algorithms to create hybrid approaches. We provide suite of test scenarios and benchmarking tools to ensure reproducibility and fair comparison. Experiments show that CAMAR presents challenging and realistic testbed for the MARL community. Code https://github.com/AIRI-Institute/CAMAR.git Introduction Multi-agent reinforcement learning (MARL) has shown strong results in cooperative and competitive settings. Many recent studies explore how MARL can solve various tasks, including navigation and coordination, even in complex environments [1, 2, 3, 4, 5, 6]. However, current MARL benchmarks do not fully match the needs of real-world applications such as robotics. Among the many MARL scenarios, navigation and path planning are especially important for robotic systems. Yet, most existing benchmarks simplify navigation into grid worlds and discrete actions that fail to capture the smooth motion and collision avoidance that robots require. In addition, learnable methods have recently become the norm for pathfinding tasks because they can handle many agents at once [7, 5, 6, 4]. At the same time, non-learnable approaches struggle with computation as the number of agents grows. Therefore, this work focuses on learnable approaches for path finding in continuous multi-agent settings. Robots move in continuous space and follow real physics, so they need to plan smooth paths that avoid walls and other robots simultaneously. The task of multi-robot navigation and avoiding collisions with dynamic obstacles is fundamental Figure 1: An example scenario from the proposed CAMAR benchmark. Agents are represented as filled circles. Each agent aims to reach its goal while avoiding collisions. The small arrows for the red and green agents indicate segments of paths generated by RRT*, providing guidance for the RL algorithms. in robotics and has been studied for long time [8, 9, 7, 10]. At the same time, most of the work uses its own specific environments and tasks, primarily built using slow simulators such as Gazebo1 and Isaac Sim2. To do this well, we need environments that combine continuous movement with multi-agent interaction. These environments must run fast, use realistic physics, and support many agents learning to navigate without collisions. High speed lets researchers run many experiments and test methods quickly. They should also include built-in benchmarks and evaluation tasks so researchers can compare different methods under the same conditions easily. We have identified three main limitations in existing MARL environments. First, many use discrete environments, which cannot represent the smooth movements and physical dynamics important in real applications [11, 12, 13]. Second, while some benchmarks do support continuous environments 1https://gazebosim.org/home 2https://developer.nvidia.com/isaac/sim and complex tasks, they do not scale well when the number of agents and obstacles becomes large [14]. Third, some environments can scale and use continuous states and actions, but their tasks are too simple and do not challenge agents to develop advanced policies [15, 16, 17]. Because of these limitations, new MARL benchmark for robotic navigation tasks that combines three key features is clearly needed: It should support continuous state and action spaces, handle hundreds or thousands of agents, and include tasks complex enough to reflect the challenges of real-world problems. To bridge the gap between multi-robot systems and MARL research, we introduce the CAMAR (Continuous Actions Multi-Agent Routing) Benchmark. Specifically, we make the following contributions: We introduce CAMAR, an extremely fast environment with GPU acceleration support (using JAX), achieving speeds exceeding 100,000 steps per second. It is designed for multi-agent navigation and collision avoidance tasks in continuous state and action spaces. We propose an evaluation protocol that includes both training and holdout task instances, as well as suite of metrics and performance indicators to assess agents generalization capabilities. We provide strong baselines for benchmarking, including state-of-the-art MARL algorithms and classical path planning methods commonly used in robotics, and conduct an extensive experimental study to evaluate their performance across diverse scenarios. Related Work Many multi-agent reinforcement learning (MARL) benchmarks have been developed in recent years, each focusing on different challenges such as coordination, navigation, or multi-agent planning. key ability for agents in robotics is to move and adapt in complex environments. Some recent environments explore these abilities, but they differ widely in important properties. These differences include whether the environment supports continuous spaces, GPU acceleration, and high simulation throughput. Some platforms support only discrete actions, while others offer more realistic continuous control. Environments also vary in their ability to handle large groups of agents or support partial observability. Other practical features also matter for researchers. Some environments are fully implemented in Python and support flexible task generation, which makes them easier to extend and use in large experiments. Others may lack documentation or require heavy customization. Finally, only few MARL environments offer standard evaluation protocols, automated tests, or package installation via PyPi. These features are useful for ensuring fair comparisons between methods, improving reproducibility, and making environments easier to maintain. In our work, we aim to address these points by extending existing protocols and providing high-performance, flexible platform. To sum up, our benchmark builds on the strengths of existing environments and aims to complement ongoing efforts in MARL research. Over the years, range of benchmarks has helped researchers tackle challenges in coordination, planning, and navigation. Well-known examples include SMAC [18, 15, 17], Jumanji [19], POGEMA [12, 13], MPE [20, 16], and VMAS, each of which introduces useful features for different kinds of tasks. SMAC [18, 15, 17] is popular for testing strategic decisionmaking, but it uses discrete actions and does not scale well for large environments. Jumanji [19] supports GPU acceleration and procedural generation, but its focus is not on navigation or planning. POGEMA [12, 13] is strong in grid-based navigation and procedural tasks with many agents, but it does not use continuous states or actions, which are important in robotics. MPE [20, 16] has played an important role in early MARL research, but it cannot scale to hundreds of agents efficiently. VMAS [14] builds on MPE [20, 16] by adding its physics and continuous dynamics, making it better for robotics. However, it can still be slow and difficult to scale to larger agent populations or complex maps. Many environments also lack evaluation protocols. This makes it difficult to compare algorithms fairly and limits the reproducibility of results. Performance issues also appear in environments that depend heavily on CPU-GPU communication, which slows down training and makes large-scale experiments harder. Finally, popular robotics simulators such as Gazebo [21], Webots [22] and ARGoS [23] offer continuous observations and actions with realistic physics, but they do not provide the ability for fast training in multi-agent scenarios. This gap shows the need for new simulation environment that brings together high performance and support for large-scale multiagent navigation and planning. For more detailed analysis of each environment see Appendix H. Continuous Observations and Actions Robots usually operate in continuous state and action spaces, so its important for benchmarks to reflect these conditions. Environments like VMAS and MPE [20, 16] use continuous actions and observations, while others, like Trash Pickup [25] or POGEMA [13], use discrete representations, which limit realism in robotics simulations. GPU Support In multi-agent environments, each agent often has its observations and rewards. This can lead to heavy data transfers between the CPU and the GPU, especially when training deep RL models. Benchmarks like Jumanji [19] and SMAX [17] are initially built with GPU acceleration, helping to reduce training time. In contrast, many older environments like SMAC [18, 15] and MPE [20, 16] do not support GPU-based simulation. Scalability >500 Agents When the number of agents grows, decision-making becomes harder. good benchmark should scale well to hundreds or thousands of agents. Environments like POGEMA [12, 13] and Trash Pickup [25] can run with thousands and even millions of agents. Others, like VMAS, MPE [20, 16], and SMAC [18, 15], are limited to much smaller groups. Agents SPS agents observable >10K Cont.Observations >500 Heterogeneous Cont.Actions Support Performance Repository Scalability Partially Python GPU generalization Proceduralgeneration protocols PyPIListed CI Evaluation Requires & Tests based link link link link link link link link link link link link link link link link link link link / 3 / 3 / 4 / 4 Environment / Simulator Waterworld (SISL) [24] RWare [11] RWare (Jumanji) [19] RWare (Pufferlib) [25] Trash Pickup (Pufferlib) [25] SMAC [18] SMACv2 [15] SMAX (JaxMARL) [17] MPE [20, 16] MPE (JaxMARL) [17] JaxNav (JaxMARL) [26] Nocturne [27] POGEMA [12] VMAS 4 [14] SMART [28] Gazebo [21] Webots [22] ARGoS [23] CAMAR (Ours) Table 1: Comparison of multi-agent reinforcement learning (MARL) environments and simulators. Each row corresponds to specific environment or particular implementation of it. The columns indicate key properties, including support for continuous observations and actions, GPU acceleration, scalability beyond 500 agents, partial observability, heterogeneous agents, and whether the simulator can exceed 10K simulation steps per second (SPS). Additional columns specify if the environment is implemented fully in Python, supports procedural generation, requires generalization across different maps or tasks, includes evaluation protocols, and provides built-in tests or continuous integration (CI). The Repository column contains links to the official source code for each environment. CAMAR, listed at the bottom, is our proposed environment. Partially Observable In most real-world scenarios, agents see only part of the environment. This is known as partial observability and is common feature in almost every RL environment. SMAC [18, 15], RWare [11], and POGEMA [12, 13] support this feature by limiting each agents view. Heterogeneous Agents In real life, agents and robots can have different sensors, shapes, or goals. Benchmarks like VMAS and SMAC [18, 15] allow agents to behave differently, making cooperation more complex. Others, like POGEMA [12, 13] or RWare [11], usually involve homogeneous agents. Performance >10K Steps/s High simulation speed is essential when agents need millions of interactions to learn. For example, the Pufferlib [25] environment can run with speed of up to 1M steps per second. This allows researchers to train and develop models more quickly. Environments like SMAC [18, 15] or VMAS can be slower, especially when running with many agents. Python Based Using Python makes it easier for researchers to understand and modify the environment. SMAX [17], VMAS, and POGEMA [12, 13] are implemented entirely in Python. This helps with faster development and integration into learning frameworks. Procedural Generation Procedural generation helps create diverse tasks that reduce the chance of overfitting. POGEMA [12, 13] and Jumanji [19] use this method to generate complex tasks automatically. Other environments, like RWare [11], use fixed layouts that limit diversity. Requires Generalization Only few environments offer separate training and testing tasks, making it difficult to test whether agents generalize well to new situations. POGEMA [12, 13] and SMACv2 [15] include test scenarios that allow researchers to check generalization. Many others use the same tasks during both training and testing. Evaluation Protocols To compare algorithms fairly, we need well-defined test cases and metrics. Benchmarks like SMACv2 [15] and Jumanji [19] include evaluation protocols to make results reproducible and meaningful. Many other 3 SPS decreases gracefully with many agents and obstacles. 4 VMAS is framework consisting of many different scenarios, while some scenarios run efficiently with speed exceeding 10K SPS, other, complex ones dont; the same applies for generalization. environments do not have standard evaluation tools or test setups. Tests & CI CI pipeline and testing suite are vital for collaborative development and maintenance of open-source projects. These practices help ensure code reliability and facilitate contributions from the whole research community. PyPi Listed When an environment is available on PyPi, it becomes easier for others to install and use. Benchmarks like Jumanji [19], and Trash Pickup [25] are available on PyPi. This lowers the barrier to entry and helps increase adoption in the research community. CAMAR Environment CAMAR is designed for continuous-space planning tasks in multi-agent environments. In this environment, multiple agents move toward their goals while avoiding both static obstacles and other moving agents. The simulation happens in fully continuous two-dimensional space, without any predefined grids. Agents interact by applying forces, which control their movement through simple and computationally efficient dynamic model. This approach makes the environment more realistic and easier to scale for many agents. Dynamic Model & Action Space key part of CAMAR is its collision model. Similar to MPE [20, 16] and VMAS [14], CAMAR uses force-based system. Agents receive repulsive forces from nearby agents and obstacles. The collision force applied to agent from object is calculated using smooth contact model, as shown in the equation below. collision ij (t) = xij (t) xij (t) log if xij(t) < dmin; (cid:18) (xij (t)dmin) 1 + (cid:19) , (t) = 0, collision ij otherwise. Here, contact force f0 regulates the magnitude of the repulsive force, penetration softness controls the smoothness of the contact dynamics, xij(t) is displacement vector between agent and an object at time t, dmin defines the minimum allowable distance before collision is activated. When two objects overlap, the force grows smoothly without sudden changes. When there is no overlap, the collision force is zero. This smooth behavior helps keep agent movement more realistic and stable. The total collision force acting on agent is calculated by summing forces from all nearby objects: ic(t) = (cid:80) collision (t). The full environment state is updated using the collision force and agents actions. CAMAR supports multiple types of dynamic models. Currently, we provide two built-in models: HolonomicDynamic and DiffDriveDynamic. ij HolonomicDynamic This model is simple and similar to the one used in MPE [20, 16]. Each agent has position and velocity. The agent moves by applying 2D force. The Figure 3: LIDAR-inspired vector observations in CAMAR. Each agent detects nearby objects using penetration vectors, and receives normalized goal direction. next state is calculated using the semi-implicit Euler method, as described in the equation below. (cid:40) vi(t + dt) = (1 damping)vi(t) + posi(t + dt) = posi(t) + vi(t + dt)dt vi(t + dt) := vi(t + dt), if vi(t + dt) < max speed vi(t+dt) vi(t+dt) max speed, otherwise. (t)+ f (t) dt Here, i (t) is the 2D action force of agent i, damping - scalar in the range [0, 1) that controls velocity decay over time, is the agent mass for applying forces, max speed regulates maximum speed of an agent preserving direction but limiting speed, dt is the time step duration between updates. DiffDriveDynamic Differential-drive robot model is another built-in dynamic in CAMAR. Each agent has 2D position posi(t) and heading angle θi(t). The agent chooses 2D action: one value for linear speed ua (t) and one for angular speed ωa (t). The motion is updated based on this action using equation below. (t), max u, max u) (t), max w, max w) ui(t) = clip(ua ωi(t) = clip(ωa vi(t) = [ui(t) cos(θi(t)); ui(t) sin(θi(t)] posi(t + dt) = posi(t) + vi(t)dt θi(t + dt) = θi(t) + ωi(t)dt Here, max and max are constraints on agent velocities. Observations Each agent in CAMAR receives local observation centered around itself. The size of the observation window can be set by the user. This observation system is inspired by LIDAR sensors but avoids using ray tracing. Instead, CAMAR provides simple and efficient vector-based observation. Each agent observes nearby objects using penetrationbased vector representation, which ensures smooth and continuous observations. For every object in the environment (a) random grid (b) labmaze grid (c) movingai (d) caves cont (e) string grid (f) batched string grid Figure 4: rich collection of maps for multi-agent planning in continuous spaces in CAMAR: support for both continuous and grid landscapes together with MovingAI collection [29]. (either an agent or static landmark), the observation is computed as normalized vector pointing from the agent to the object. If the object is far away, outside the agents sensing window, the observation becomes zero vector. This method avoids discontinuities and helps agents better generalize across different object sizes. (cid:17) (cid:16) obs = oj = oj pos 1 window+Rj oj oj if oj Rj < window 0, otherwise. obs obs := window , This observation is computed for each agent in fully vectorized manner. Afterward, only the top max obs closest objects are kept to form the final observation vector. Here, oj is the 2D position of object j, is the agent radius, Rj the radius of object j, window is parameter that sets how far the agent can sense objects nearby. In addition to obstacle information, each agent also gets an ego-centric vector pointing to its goal. This vector is clipped, normalized and concatenated to the final observation. This structure helps agents understand both their surroundings and the direction they need to move. Circle-Based Discretization One simple way to simulate world is to use geometry rules to check if objects overlap. Ray tracing is common method for detecting collisions based on the shapes of objects. However, ray tracing is hard to implement in way that is fast on GPU. Simple versions of ray tracing are slow and better suited for CPU simulations. Another method is to discretize the world and only check collisions with nearby objects. Based on the dynamic model described above, it is enough to calculate the distance between objects. In CAMAR, every object is represented as circle. This choice has several advantages. Checking the distance between two circles is simple and fast. It does not require special cases like ray tracing does. It also avoids the complexity of handling different shapes like rectangles or polygons. Because of this, the simulation can easily run on GPUs with many agents at the same time. This design allows CAMAR to simulate largescale multi-agent tasks efficiently and with high performance. Map Generators Although every object in CAMAR is represented as circle, it is still possible to create complex and detailed maps. complex structure can be made by combining many smaller circles close together. The more circles that are used, the more accurate the map becomes. This method allows users to simulate walls, tunnels, mazes, and other complicated shapes even though the basic element is always circle. CAMAR includes several types of built-in maps. It also gives users the ability to add custom map generators, even if they are not compatible with JAX just-in-time compilation [30]. custom map generator can be connected easily by using the string grid or batched string grid formats. This flexibility allows users to design many kinds of environments, from simple random setups to complex and realistic maps. The current set of built-in maps and generators includes (see Fig. 4): 4a random grid: map where obstacles, agents, and goals are placed randomly on grid with predefined size. 4b labmaze grid: Maps generated using LabMaze5 [31] - maze generator with connected rooms. 4c movingai: Integrated two-dimensional maps from the MovingAI benchmark [29], adapted for continuous planning tasks. They can also be used in batched string grid manner. 4d caves cont: continuous type of map where caves are generated using Perlin noise, common method in video games for creating realistic and varied landscapes. 4e string grid: grid map based on text layout. Obstacles are placed according to characters in string, similar to the MovingAI benchmark [29]. Agent and goal positions can be fixed or random, depending on the free cells in the string. 4f batched string grid: Similar to string grid, but supports different obstacle layouts across parallel environments. This allows training on multiple map variations at once. Reward Function CAMAR uses scalar reward for each agent at every time step. This reward is the sum of four terms: goal reward, collision penalty, movement-based reward, and collective success reward: 5https://github.com/google-deepmind/labmaze ri(t) = rall g(t) + ron gi The terms are defined as follows: (t) + rcollisioni(t) + rg disti (t) Rg; Rg; (t) = +shaping xi(t) xgi (cid:0)xi(t dt) xgi rall g(t) = +0.5, if : xi(t) xgi (t) = +0.5, if xi(t) xgi ron gi rcollisioni(t) = 1, if : xij(t) < dmin; rg disti Here, xi(t) is the position of agent at time t, xgi is the goal position for agent i, Rg is the distance threshold to count as reaching the goal (goal radius), xij(t) is the vector between agent and another object j, dmin is the minimal distance between agent and object (for circle objects dmin = Ri + Rj where Ri and Rj are radii), shaping is user-defined coefficient that controls the strength of the movement-based term. (cid:1) To support cooperation, the environment gives an extra reward when all agents reach their goals. In this case, each agent receives an additional reward of +0.5. Figure 5: Illustration of heterogeneous agents with different sizes and dynamics supported by CAMAR. Blue agents are governed by HolonomicDynamic, while green agents follow DiffDriveDynamic. All agents navigate shared environment while avoiding gray obstacles. Heterogeneous agents Additionally, CAMAR supports heterogeneous agents in both size and dynamics. All map generators can produce agents with different properties, making it possible to study diverse multi-agent systems inspired by real-world scenarios. For can some while agents example, use HolonomicDynamic, follow DiffDriveDynamic. These agents operate together in shared space, interact with the same obstacles, and must coordinate their movements despite having different control rules  (Fig. 5)  . Each agent follows its own dynamics model, but all agents contribute to single global simulation. others CAMAR also supports agents with different sizes. Each agent can have its own radius, which affects how it moves and avoids collisions. This adds complexity to coordination and planning. Metrics In multi-agent pathfinding research, it is common to use metrics such as success rate (SR), flowtime (FT), makespan (MS), and coordination (CO) to compare different methods. We adopt these metrics in our work and define them more formally as follows in the equation below: 1{xi(T ) xgoal Rg}; SR = 1 N (cid:80) i=1 (cid:80) i=1 MS = max FT = 1 ti; ti; i=1,...,N CO = 1 T Here, is the number of agents, is the maximum episode length, ti is the time step when agent first reaches its goal, is the total number of collisions over the episode. Evaluation Protocols To support rigorous and reproducible benchmarking, CAMAR includes standardized suite of evaluation protocols inspired by and extending prior work on cooperative MARL evaluation [32]. We adapt that framework for continuous multi-agent pathfinding, focusing on generalization across both agent count and map structure. We propose three evaluation tiers: Easy, Medium, and Hard, each targeting different level of generalization. Easy evaluates performance on unseen start and goal positions using the same map type and number of agents as during training. Medium tests generalization to maps with similar structure but different number of agents and obstacle parameters. Hard measures generalization to fully unseen map types from the MovingAI street collection, often with different number of agents than during training. Each tier follows defined training and evaluation setup using introduced metrics, aggregated by the Interquartile Mean (IQM) with 95% confidence intervals (CI95) for fair comparison across methods and difficulty levels. All experiments use fixed JAX [30] random seeds for reproducibility. In total, each protocol involves training multiple models and running thousands of evaluation episodes. We include detailed training and evaluation scripts and provide all protocol maps in the public CAMAR repository. These protocols help the community better track progress on continuous multi-agent pathfinding and identify which algorithms generalize well to more realistic conditions. Sample efficiency curves, metric-vs-agent-count plots, and performance profiles are supported and recommended for deeper analysis. Experimental Evaluation In this section, we evaluate both the scalability and benchmarking capabilities of our environment. We begin by training and testing set of popular MARL algorithms, as well as classical non-learnable and hybrid methods. These experiments show that the environment supports wide range of navigation and coordination strategies. We also present results from simple heterogeneous-agent scenario to demonstrate support for heterogeneous MARL research. Finally, we random grid labmaze grid SR FT MS CO SR FT MS CO Algorithm IPPO MAPPO IDDPG MADDPG ISAC MASAC RRT*+IPPO RRT*+MAPPO RRT*+IDDPG RRT*+MADDPG RRT*+ISAC RRT*+MASAC 0.4100.001 0.8300.001 0.3350.001 0.0410.000 0.1150.001 0.2810.001 0.4200.001 0.8280.001 0.2800.001 0.0370.000 0.1430.000 0.0540.000 169510 9845 185110 250812 252314 184311 14269 9715 218112 295315 261813 251114 RRT*+PD RRT+PD 0.6780.002 0.4130.014 201059 2440264 160.00.0 151.40.3 160.00.0 160.00.0 160.00.0 160.00.0 160.00.0 150.40.3 160.00.0 160.10.0 160.00.0 160.00.0 160.00.0 160.00.0 1.0000.000 1.0000.000 1.0000.000 0.9130.001 1.0000.000 0.8560. 1.0000.000 1.0000.000 1.0000.000 0.9840.000 1.0000.000 1.0000.000 0.2130.013 0.5680.004 0.1670.000 0.0270.000 0.0470.000 0.1050.001 0.5110.001 0.5560.001 0.1890.000 0.0370.000 0.0580.000 0.0340.000 210414 14848 277214 274512 280812 209812 13166 13267 263514 291814 274913 285415 0.9970.000 0.7880. 0.6920.004 0.5280.021 180749 2049251 160.00.0 160.00.0 160.00.0 160.00.0 160.00.0 160.00.0 160.00.0 160.00.0 160.00.0 160.10.0 160.00.0 160.00.0 160.00.0 160.00.0 1.0000.000 1.0000.000 0.9960.000 0.8540.001 1.0000.000 0.7810. 0.9990.000 0.9990.000 0.9970.000 0.9690.000 1.0000.000 0.9940.000 0.9710.002 0.5580.025 Table 2: Extended performance comparison across different algorithms in the random grid and labmaze grid environments including integrated RRT* with off-policy algorithms additionally. Reported metrics are SR (Success Rate), FT (Flowtime), MS (Makespan), and CO (Coordination), each shown as IQMCI95. Confidence intervals are symmetric for clarity and computed using 1K bootstrapped samples. Arrows indicate the direction of improvement: denotes higher is better, indicates lower is better. Tan boxes highlight the best-performing approach. The CO metric is not colored here for visibility. measure the performance of the simulator in terms of simulation speed, and compare it with VMAS using shared experimental setup. Experimental Setup We evaluate six MARL algorithms: IPPO, MAPPO, IDDPG, MADDPG, ISAC [33], and MASAC. In addition, we include two non-learnable baselines, RRT+PD and RRT*+PD, and two hybrid methods, RRT*+IPPO and RRT*+MAPPO. All methods are evaluated on two procedurally generated map types: random grid and labmaze grid, each with 6 versions varying in obstacle density and agent count (8 or 32). For labmaze grid, an additional connection probability from 0.4 to 1.0 tests different maze complexities. Generation details for training and evaluation maps are available in the Appendix D. The independent variants (IPPO [1], IDDPG, ISAC) train each agent using its own policy and value function. These methods do not use centralized critics or information sharing across agents. In contrast, the multi-agent versions (MAPPO, MADDPG, MASAC) use centralized critics during training to improve coordination. All approaches use parameter sharing, meaning that agents use the same neural network weights. Each algorithm is trained for 20M (IPPO, MAPPO) and 2M (IDDPG, MADDPG, ISAC, MASAC) steps per scenario. The training is done independently for each of the 12 map variations. After training, we evaluate the models on both seen and unseen tasks to test their generalization. In total, we train 532 models and evaluate them across 5184 tasks, with 1000 episodes per task. The experiments were run on single NVIDIA H100 GPU and took around 1000 hours in total. For the non-learning baselines, we use RRT+PD and RRT*+PD. These methods use classical planning algorithms to generate path to the goal for each agent. Each path is generated using either RRT (with 50,000 iterations) [34] or RRT* (with 3000 iterations). The agent then follows the path using simple PD controller. We also evaluate hybrid methods where agents receive additional RRT* information during training and evaluation. At the start of each episode, RRT* generates sample paths from the goal to the agents position. These paths and their estimated costs are included in the agents observation, enabling the policy to learn from approximate cost-to-go values without invoking RRT* at every step. To evaluate simulator performance, we measure simulation speed in steps per second (SPS) on 20 20 random grid map with 0.3 obstacle density (120 obstacles). We vary the number of agents and parallel environments to assess CAMARs scalability. For fair comparison, we benchmark CAMAR against VMAS [14] using identical map size, agent count, and single NVIDIA H100 GPU. Benchmark The main results are presented in Table 2, which reports success rate (SR), flowtime (FT), makespan (MS), and coordination (CO) for each algorithm on two types of maps. On the random grid map, MAPPO achieves the highest SR, with good FT and full CO. Its centralized critic helps agents plan jointly and share useful information. RRT*+MAPPO shows similar SR but performs better in FT and MS, which shows that adding planning support improves movement efficiency. RRT*+IPPO also improves over IPPO alone, although the gains are smaller. RRT*+PD achieves high SR without learning, but it performs worse in CO. This is expected because it plans for each agent independently. The planner uses full map knowledge but does not consider other agents during execution. As result, agents sometimes collide in narrow areas. RRT* with Off-Policy Algorithms We also evaluate hybrid methods that combine RRT* with off-policy MARL algorithms. As shown in Table 2, the results are mixed. Some methods benefit from additional planning information. For example, RRT*+ISAC slightly improves over ISAC alone in both success rate (SR) and coordination (CO), especially in the random grid setting. RRT*+IDDPG also shows moderate improvement, achieving better SR and lower flowtime (FT) than IDDPG. However, in other cases, the integration leads to weaker results. RRT*+MASAC and RRT*+MADDPG perform poorly, with SR staying below 0.05 in both maps. One possible reason is the increased input size to the models. Off-policy algorithms with centralized critics, such as MASAC and MADDPG, must process long input vectors that include observations and RRT* features from all agents. This can make training unstable and reduce generalization. Overall, while combining RRT* with off-policy methods can help in some settings, the results suggest that better integration strategies or more expressive policy architectures may be needed to fully benefit from hybrid approaches. Among MARL methods, IPPO and IDDPG perform well in SR, but their FT is worse than MAPPO. ISAC shows high CO but low SR, possibly due to its focus on smooth motion and risk reduction. MASAC and MADDPG perform poorly. In particular, MADDPG shows very low SR and fails to explore even in the simpler tasks. We investigate these issues in more detail in the Appendix D. The labmaze grid map is more challenging. Narrow passages make reward signals sparse. In this setting, SR drops across all MARL methods. RRT*+PD performs the best in SR, showing the value of full-path planning when learning signals are limited. However, its CO score remains lower than MARL baselines. The simpler RRT+PD baseline performs worse than RRT*+PD across all metrics. It produces longer and less efficient paths because RRT does not optimize for path cost and often generates jagged routes. On the labmaze grid, RRT+PD still performs better in SR than many learningbased methods, but with higher FT and lower CO. This result shows that even basic planning methods can outperform MARL baselines in sparse reward environments, but their inability to consider other agents during execution limits coordination and overall efficiency. These findings highlight trade-off. Planning methods like RRT*+PD solve complex tasks more reliably, while learning-based policies handle coordination better. To explore this further, we evaluated hybrid methods. These approaches combine RRT* samples with MARL observations to guide exploration. However, results show only small gains or similar performance. One possible reason is the size of the observation vector, which may be too large for the simple policy network to process effectively. Future work may improve integration with better architectures. See Appendix for more detailed analysis of performance of algorithms."
        },
        {
            "title": "Heterogeneous Agents",
            "content": "To show CAMARs support for heterogeneous agents, we modify simple coordination task called give way. In this new version, two agents must pass through narrow corridor, but only the smaller red agent can fit into the central chamber. The larger blue agent cannot enter this area and must wait for the other to pass. We evaluate both standard and heterogeneous versions of IPPO and MAPPO. In the heterogeneous versions (HetIPPO and HetMAPPO), agents use separate policy models. Results in Fig. 6b show that HetIPPO performs better than its sharedpolicy version. However, HetMAPPO fails, likely because the centralized critic struggles with large and diverse input spaces. Algorithm SR 0.5 IPPO 0.7 HetIPPO MAPPO 0.5 HetMAPPO 0.0 (a) hetero give way scenario (b) SR results. Figure 6: Example of heterogeneous agent coordination (a). Success rates of algorithms are shown in (b). This experiment shows that CAMAR supports agents with different sizes and models. It can be used to study coordination and policy learning in heterogeneous teams. More details and discussion are provided in the Appendix D."
        },
        {
            "title": "Scalability Analysis",
            "content": "(a) CAMAR (b) VMAS Figure 7: Scalability comparison between CAMAR and VMAS across different settings, evaluating the impact of increasing parallel environments, agent count, and obstacles on simulation speed. We evaluate the scalability of our environment by measuring how the simulation speed changes when we increase the number of parallel environments, agents, and obstacles. Fig. 7 and 8 show the results of these experiments. Fig. 7a shows snapshot of the CAMAR environment with 32 agents on the 20 20 random grid map. Agents move smoothly while avoiding the 120 fixed obstacles. Fig. 7b shows the same scenario in VMAS [14]. First, we measure the SPS as we increase the number of parallel environments from 5 to 7000, keeping 32 agents fixed. VMAS [14] scales almost linearly but reaches less than 10 000 SPS at 7000 environments. In contrast, CAMAR scales quickly up to 2000 environments and then maintains around 50 000 SPS even as more are added. This shows that CAMAR can simulate many parallel tasks without losing speed, which is important for fast training. Next, we fix the number of environments at 2000 and vary the agent count from 4 to 128. CAMAR keeps over 100 000 SPS with fewer than 16 agents and remains above 10 000 SPS even with 128 agents. VMAS [14] starts near 20 000 SPS with 4 agents but drops sharply to 500 SPS at 128 agents. In this comparison, CAMAR is up to 20x faster than VMAS when many agents act at once. Overall, CAMAR delivers much higher throughput and keeps strong performance as the number of agents increases. VMAS can boost its speed by adding more parallel environments, but it would need about 50 000 parallel environments to match the throughput that CAMAR provides at 2000 environments. We also compare CAMAR scalability with other modern MARL environments, which can be found in the Appendix E. Figure 8: Scalability of CAMAR with increasing numbers of agents and obstacles under more extreme conditions. Despite decrease in Steps per Second (SPS) as the number of agents grows to 800, CAMAR maintains approximately 1400 SPS, demonstrating its capability to simulate large multi-agent teams. Additionally, although SPS decreases, the total number of gathered observations remains high, as it scales proportionally with the number of agents. We also study CAMAR under more extreme conditions. In Fig. 8, we increase the number of agents up to 800 while keeping the number of circle obstacles fixed at 4160. As the number of agents grows, the simulation speed drops, but our environment still maintains about 1400 SPS with 800 agents. This result shows that CAMAR can simulate very large multiagent teams and still produce enough experience for learning. It also means that the total number of observations remains high, because each agent generates one observation per step. Finally, we test how the number of obstacles affects scalability. We fix the number of agents at 32 and increase the number of obstacles up to 9920. The results, shown in Fig. 8, demonstrate that although SPS decreases with more obstacles, CAMAR still achieves about 15 400 SPS at the highest tested complexity. This indicates that CAMAR remains robust even in very cluttered environments. Conclusion This paper introduces CAMAR, high-performance benchmark for continuous-space multi-agent reinforcement learning. CAMAR combines realistic dynamics with efficient simulation, supporting over 100 000 steps per second using JAX [30]. It includes diverse set of navigation tasks, standardized evaluation protocol with built-in metrics, and range of strong baselines from both classical, learning-based, and hybrid methods. These components enable reliable, scalable, and reproducible evaluation of MARL algorithms. References [1] Christian Schroeder De Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020. [2] Matteo Bettini, Ajay Shankar, and Amanda Prorok. Heterogeneous multi-robot reinforcement learning. In AAMAS, 2023. [3] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in neural information processing systems, 35:2461124624, 2022. [4] Mehul Damani, Zhiyao Luo, Emerson Wenzel, and Guillaume Sartoretti. Primal 2: Pathfinding via reinforcement and imitation multi-agent learning-lifelong. IEEE Robotics and Automation Letters, 6(2):2666 2673, 2021. [5] Alexey Skrynnik, Anton Andreychuk, Maria Nesterova, Konstantin Yakovlev, and Aleksandr Panov. Learn to follow: Decentralized lifelong multi-agent pathfinding via planning and learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1754117549, 2024. [6] Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, and Alexey Skrynnik. Mapf-gpt: Imitation learning for multi-agent pathfinding at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2312623134, 2025. [7] Brian Angulo, Aleksandr Panov, and Konstantin Yakovlev. Policy optimization to learn adaptive motion primitives in path planning with dynamic obstacles. IEEE Robotics and Automation Letters, 8(2):824831, 2022. [8] Weinan Chen, Wenzheng Chi, Sehua Ji, Hanjing Ye, Jie Liu, Yunjie Jia, Jiajie Yu, and Jiyu Cheng. survey of autonomous robots and multi-robot navigation: Perception, planning and collaboration. Biomimetic Intelligence and Robotics, page 100203, 2024. [9] Binyu Wang, Zhe Liu, Qingbiao Li, and Amanda Prorok. Mobile robot path planning in dynamic environments through globally guided reinforcement learning. IEEE Robotics and Automation Letters, 5(4):6932 6939, 2020. [10] Vassilissa Lehoux-Lebacque, Tomi Silander, Christelle Loiodice, Seungjoon Lee, Albert Wang, and Sofia Michel. Multi-agent path finding with real robot dynamics and interdependent tasks for automated warehouses. In ECAI 2024, pages 43934401. IOS Press, 2024. [11] Georgios Papoudakis, Filippos Christianos, Lukas Schafer, and Stefano Albrecht. Benchmarking multiagent deep reinforcement learning algorithms in cooperative tasks. arXiv preprint arXiv:2006.07869, 2020. [12] Alexey Skrynnik, Anton Andreychuk, Konstantin Yakovlev, and Aleksandr Panov. Pogema: partially observable grid environment for multiple agents. arXiv preprint arXiv:2206.10944, 2022. [13] Alexey Skrynnik, Anton Andreychuk, Anatolii Borzilov, Alexander Chernyavskiy, Konstantin Yakovlev, and Aleksandr Panov. Pogema: benchmark platform for cooperative multi-agent pathfinding. In The Thirteenth International Conference on Learning Representations, 2025. [14] Matteo Bettini, Ryan Kortvelesy, Jan Blumenkamp, and Amanda Prorok. Vmas: vectorized multi-agent simulator for collective robot learning. In International Symposium on Distributed Autonomous Robotic Systems, pages 4256. Springer, 2022. [15] Benjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob Foerster, and Shimon Whiteson. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 36:3756737593, 2023. [16] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017. [17] Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, et al. Jaxmarl: Multi-agent rl environments in jax. arXiv preprint arXiv:2311.10090, 2023. Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The arXiv preprint starcraft multi-agent challenge. arXiv:1902.04043, 2019. [18] Mikayel Samvelyan, [19] Clement Bonnet, Daniel Luo, Donal Byrne, Shikha Surana, Sasha Abramowitz, Paul Duckworth, Vincent Coyette, Laurence Midgley, Elshadai Tegegn, Tristan Kalloniatis, et al. Jumanji: diverse suite of scalable reinforcement learning environments in jax. arXiv preprint arXiv:2306.09884, 2023. [20] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations. arXiv preprint arXiv:1703.04908, 2017. [21] Nathan Koenig and Andrew Howard. Design and use paradigms for gazebo, an open-source multi-robot simulator. In 2004 IEEE/RSJ international conference on intelligent robots and systems (IROS)(IEEE Cat. No. 04CH37566), volume 3, pages 21492154. Ieee, 2004. [22] Olivier Michel. Cyberbotics ltd. webots: professional mobile robot simulation. International Journal of Advanced Robotic Systems, 1(1):5, 2004. [23] Carlo Pinciroli, Vito Trianni, Rehan OGrady, Giovanni Pini, Arne Brutschy, Manuele Brambilla, Nithin Mathews, Eliseo Ferrante, Gianni Di Caro, Frederick Ducatelle, et al. Argos: modular, parallel, multiengine simulator for multi-robot systems. Swarm intelligence, 6:271295, 2012. [24] Jayesh Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In Autonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Best Papers, Sao Paulo, Brazil, May 8-12, 2017, Revised Selected Papers 16, pages 6683. Springer, 2017. [25] Joseph Suarez. Pufferlib: Making reinforcement learning libraries and environments play nice. arXiv preprint arXiv:2406.12905, 2024. [26] Alexander Rutherford, Michael Beukman, Timon Willi, Bruno Lacerda, Nick Hawes, and Jakob Nicolaus Foerster. No regrets: Investigating and improving regret approximations for curriculum discovery. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. [27] Eugene Vinitsky, Nathan Lichtle, Xiaomeng Yang, Brandon Amos, and Jakob Foerster. Nocturne: scalable driving benchmark for bringing multi-agent learning one step closer to the real world. Advances in Neural Information Processing Systems, 35:39623974, 2022. [28] Jingtian Yan, Zhifei Li, William Kang, Yulun Zhang, Stephen Smith, and Jiaoyang Li. Advancing mapf towards the real world: scalable multi-agent realistic testbed (smart). arXiv preprint arXiv:2503.04798, 2025. [29] Nathan Sturtevant. Benchmarks for grid-based pathfinding. IEEE Transactions on Computational Intelligence and AI in Games, 4(2):144148, 2012. [30] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Jax: composable Skye Wanderman-Milne, et al. transformations of python+ numpy programs. GitHub repository, 2018. [31] Charles Beattie, Joel Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Kuttler, Andrew Lefrancq, Simon Green, Vıctor Valdes, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. [32] Rihab Gorsane, Omayma Mahjoub, Ruan John de Kock, Roland Dubb, Siddarth Singh, and Arnu Pretorius. Towards standardised performance evaluation protocol for cooperative marl. Advances in Neural Information Processing Systems, 35:55105521, 2022. [33] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pages 18611870. Pmlr, 2018. [34] Steven LaValle. Rapidly-exploring random trees: new tool for path planning. Research Report 9811, 1998. [35] Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang, Gianni De Fabritiis, and Vincent Moens. Torchrl: data-driven decision-making library for pytorch, 2023. [36] Matteo Bettini, Amanda Prorok, and Vincent Moens. Benchmarl: Benchmarking multi-agent reinforcement Journal of Machine Learning Research, learning. 25(217):110, 2024."
        },
        {
            "title": "Appendix H Extended Related Work",
            "content": "Appendix Evaluation Protocol Evaluation Suite Overview. To make results easy to compare and fully reproducible, CAMAR adopts the Standardised Performance Evaluation Protocol for Cooperative MARL by [32] and extends it to continuous multi-agent path-finding. We keep the core ideas, fixed training budgets, multiple random seeds, and strict uncertainty estimates, but add path-finding-specific stress-tests: Axis 1: Agent count. We vary the number of agents to see whether method still works when the team grows. Axis 2: Map difficulty. We change obstacle density and map geometry to check if the learned policy still solves harder layouts. Three difficulty tiers. Easy uses the same map, agent count, and obstacle parameters for training and testing. Only the random seeds that place starts and goals change. This tells us whether an algorithm can solve the problem at all. Medium trains on one map setting but evaluates on 12 variants that share the same map types while changing agent count and obstacle density. This probes generalisation within the same domain and includes testing generalization across agents. Hard trains on any user-chosen mapsexcept the MovingAI street setand then tests on those unseen street maps with new agent counts. This is near-real-world stress test. Standardized Evaluation Suite for CAMAR Input. set of maps M, task sets Tm, and pool of algorithms A. 1. Default settings Training steps : 2M (off-policy) or 20M (on-policy). Independent runs R: 3 seeds. Evaluation episodes E: 1 000 per interval. Evaluation intervals (based on available resources): every 10K-100K steps (off-policy) or 100K-1000K steps (on-policy). 2. Metrics Returns for sample-efficiency plots. Success Rate (SR), Flowtime (FT), Makespan (MS), and Coordination (CO). over episodes with 95% CIs. Per-task: mean Ga Per-protocol: build an (RT ) matrix of normalised returns, then report IQM, optimality gap, probability of improvement, and performance profiles (all with 95% bootstrap CIs). 3. Three difficulty tiers 1. Easy: train and test on the same map/agent count. 12 models 3 seeds. 2. Medium: train on one map, test on all 12 maps and aggregate. 12 models 3 seeds. 3. Hard: train on any maps (except MovingAI), test on MovingAI [29] street maps with new agent counts. Single model (preferably 3 seeds). 4. Reporting checklist Hyper-parameters, network sizes, and compute budget. Map generation settings for each tier. Final IQM 95% CI for SR, FT, MS, CO (mandatory). Sample-efficiency curves are recommended for Easy and Medium, optional for Hard. In Hard also plot each metric against the number of agents to test the ability to scale. Each tier follows the default budget: 2M steps for offpolicy and 20M steps for on-policy algorithms, three random seeds, and evaluations every 10K-100K or 100K-1000K steps. We keep the JAX seed fixed at 5 and split it to generate all Easy Medium Hard Purpose How to train and evaluate Test that the method can solve the problem without testing generalization. Test that the method can solve the problem and generalise across similar map types including varying number of agents. Train on map (see Appendix). Train on map (see Appendix). Evaluate on the same map X. Evaluate on all 12 maps. Repeat for all 12 maps Repeat for all 12 maps Number of models Number of evals 12 trained models 3 seeds 12 evaluations 1K episodes 12 trained models 3 seeds 144 evaluations 1K episodes Report Metrics - if needed sample-efficiency strongly recommended curves - Metrics - mandatory sample-efficiency curves needed - if Test that the method can generalise to near-real-world settings. Train on any map collection excluding MovingAI Evaluate on MovingAI street collection with varying agent counts 1 trained model 30 evaluations 1K episodes varying number of agents Metrics - mandatory sample-efficiency curves - if resources allow metrics vs num agents - mandatory Table 3: Overview on 3-tier evaluation protocols presenting the purpose, number of trained models and evaluations for each protocol evaluation keys, which makes every run exactly repeatable. Reporting and analysis. We require final IQM 95% CI of Success Rate, Flowtime, Makespan, and Coordination. Sample-efficiency curves are strongly recommended for Easy and Medium and optional for Hard if resources allow. For the Hard tier, we also ask for plots of each metric versus the number of agents because scalability is key question. Appendix Code Examples The CAMAR library provides simple and flexible interface for building custom multi-agent pathfinding environments. Below we show two examples that demonstrate how to create environments using different map generators and agent dynamics. Example 1  (Fig. 9)  shows how to create an environment with the random grid map. It uses the string-based API to pass all configuration options directly. This includes the number of agents, the size range for agents and goals, and the dynamic model to use. We set the dynamic to HolonomicDynamic, and also customize the observation window and the shaping factor. This example shows how to use heterogeneous sizes for agents and goals, which can help simulate more realistic scenarios. Example 2  (Fig. 10)  shows how to use maps from the MovingAI benchmark [29]. These maps are loaded by name using the map generator function movingai(). This example also shows how to use heterogeneous agent dynamics by combining different models (e.g. DiffDriveDynamic and HolonomicDynamic). This is done using the classbased API with MixedDynamic. The number of agents of each type is specified, and the total is passed to the environment. This allows testing how different types of agents can cooperate in complex environments. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from camar import camar_v0 env1 = camar_v0( map_generator=\"random_grid\", dynamic=\"HolonomicDynamic\", pos_shaping_factor=2.0, window=0.25, # obs window max_obs=10, # max num of obj in obs map_kwargs={ \"num_agents\": 16, \"agent_rad_range\": (0.01, \"goal_rad_range\": 0.05 ), (0.001, 0.005), }, dynamic_kwargs={ \"mass\": 2.0, } ) Figure 9: Example 1. Creating CAMAR environment using random grid maps and holonomic agents. The environment is loaded using the string-based API, which supports configuration via YAML or inline dictionary. These examples can be used as templates for building new scenarios in CAMAR. Users can adjust map parameters, agent settings, or dynamic models to suit their research needs. The CAMAR design supports quick changes to both environment structure and agent behavior using only few lines of code. Appendix Limitations CAMAR offers fast and flexible simulation, but there are still important gaps. First, all built-in scenarios use static obstacles. We do not yet provide maps with moving obstacles, so agents cannot train on fully dynamic scenes. While the engine 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from camar import camar_v0 from camar.maps import movingai from camar.dynamics import MixedDynamic, DiffDriveDynamic, HolonomicDynamic dynamics_batch = [ DiffDriveDynamic(mass=1.0), HolonomicDynamic(mass=10.0), ] # 8 diffdrive + 24 holonomic = 32 total num_agents_batch = [8, 24] mixed_dynamic = MixedDynamic( dynamics_batch=dynamics_batch, num_agents_batch=num_agents_batch, ) map_generator = movingai( map_names=[\"street/Denver_0_1024\", \"bg_maps/AR0072SR\"], num_agents=sum(num_agents_batch), ) env2 = camar_v0( map_generator=map_generator, dynamic=mixed_dynamic, pos_shaping_factor=2.0, window=0.25, max_obs=10, ) Figure 10: Example 2. Creating CAMAR environment with MovingAI maps [29] and mixed agent dynamics. This example demonstrates the class-based API for explicitly specifying heterogeneous agent behavior. supports different sizes for agents, goals, and landmarks, the built-in map generators do not include variations in landmark sizes. We believe that adding new maps with obstacles of different sizes and dynamic behavior is an important direction for future work. Fortunately, CAMARs modular design makes it easy to extend in this way. Second, although we support two built-in dynamics models (HolonomicDynamic and DiffDriveDynamic), and allow mixing them to build heterogeneous teams, both use basic integration schemes: either explicit or non-explicit Euler. To achieve stable simulation with larger time steps (for example, dt = 0.1), frameskip must be applied. Without frameskip, simulation becomes unstable with large dt values, and smaller integration step like dt = 0.005 must be used together with frameskip = 20. Adding more stable integration methods, such as Runge-Kutta, could improve stability and allow efficient simulation with fewer steps. For example, using Runge-Kutta with dt = 0.05 and frameskip = 2 (for total of 8 integration steps per forward pass) might offer better trade-off between speed and accuracy. Third, there is no built-in communication mechanism between agents. Adding message passing would help study how algorithms use shared information. Appendix Extended Benchmark"
        },
        {
            "title": "Performance Analysis",
            "content": "Figure 11: Performance profiles based on normalized return. Each line shows the fraction of evaluation runs that achieved score above given threshold τ . Figure 12: Performance profiles based on success rate. Each line shows the fraction of evaluation runs that achieved SR above threshold τ . Aggregate Analysis and Performance Profiles We also report aggregate return metrics across all evaluated methods in Fig.13. These charts, produced using the MARL-EVAL toolkit[32], show median, interquartile mean (IQM), mean return, and optimality gap, with normalized scores. Among MARL baselines, MAPPO performs best across all metrics, followed by IPPO. ISAC and IDDPG achieve lower scores and are less consistent, while MADDPG performs the worst and shows the largest optimality gap. Among hybrid methods, RRT*+MAPPO and RRT*+IPPO achieve strong results that match or exceed their baselines. However, RRT*+MASAC and RRT*+MADDPG perform poorly. This may be caused by the larger input space, especially for centralized critics that struggle with high-dimensional observations. To further evaluate consistency and reliability, we present performance profiles in Fig.11 and Fig.12. These plots show the fraction of evaluation runs where each algorithm achieves score above given threshold. The results confirm our earlier findings: MAPPO and IPPO maintain good performance across tasks, while MADDPG and MASAC drop quickly. RRT*+MAPPO and RRT*+IPPO remain strong among hybrid approaches, while RRT*+MADDPG and RRT*+MASAC again show poor results. These charts together highlight the differences in generalization ability, robustness, and effectiveness across MARL and hybrid methods. They also support our earlier analysis of SR, FT, MS, CO  (Table 2)  . Overall, these findings show that RRT* integration can improve performance for some methods, but not all. The effect depends on the algorithm and how it handles the added input complexity. Figure 13: Normalized return scores. Top: MARL baselines. Bottom: RRT* hybrid methods. Metrics include median, interquartile mean (IQM), mean, and optimality gap. Higher values are better except for optimality gap. Each bar shows 95% confidence intervals. Algorithm SR FT MS CO Ep. Return IDDPG (SC) ISAC (SC) MADDPG (SC) MADDPG (NSC) MASAC (SC) MASAC (NSC) 0.77 0.21 0.00 0.00 0.26 0.44 406 1060 1241 1259 813 780 134 160 160 160 159 160 0.996 0.999 0.997 0.998 0.996 1.000 266 92 -1.1 -0.9 67 193 Table 4: Performance comparison of multi-agent DDPG and SAC variants on random grid with 8 agents. SC: shared critic, NSC: not shared critic. single centralized critic must estimate value based on the combined actions and states of all agents. The critics target is simple mean across agents in this version, which can blur important agent-specific differences. This makes learning slow and unstable. Second, the input size of the centralized critic grows with the number of agents. For example, in our setting with 8 agents, the critic receives long concatenated vector of all observations and actions. This high-dimensional input makes the learning problem more difficult, especially for relatively small networks like MLPs. Previously discussed performance of RRT*+MASAC and RRT*+MADDPG also supports this hypothesis. These hybrid methods show much lower returns and success rates compared to RRT*+IPPO or RRT*+MAPPO. This drop in performance is likely caused by the added RRT* information increasing the input size even further, making the learning task more difficult for centralized critics. To test these ideas, we trained new versions of MADDPG and MASAC where the critic network is no longer shared between agents. Instead, each agent has its own critic (without parameter sharing), while still using centralized training. Figure 14: Sample-efficiency curves: mean success rate versus environment steps. Computed using MARL-EVAL. Sample Efficiency. Figure 14 shows the mean success rate evolving during training, computed with the MARL-EVAL toolkit [32]. MAPPO learns the fastest, rising above 0.60 after roughly 3M steps and then remaining stable. MASAC improves quickly at first but plateaus below 0.45. IPPO starts slowly yet keeps improving and nearly reaches the MASAC curve near the end of training. IDDPG attains modest plateau early and then changes little. ISAC climbs slightly in the first million steps and then flattens out, while MADDPG stays under 0.15 for the entire run. DDPG and SAC analysis We further investigated the weak performance of multi-agent DDPG and SAC algorithms in our benchmark. Specifically, we observed that both MADDPG and MASAC achieved lower success rates and higher episode lengths compared to their independent counterparts (IDDPG and ISAC). To understand these results, we considered two possible factors that could negatively affect learning with centralized critics. First, the credit assignment problem becomes harder as the number of agents increases. In both MADDPG and MASAC, The results are shown in Table 4. For MADDPG, there was no improvement: both the shared and non-shared versions failed to learn the task, with success rate remaining at 0.0. This further supports the hypothesis that MADDPG struggles with large input vectors and credit assignment, even when the critics are separated. In contrast, MASAC improved with non-shared critics: the success rate rose from 0.26 to 0.44 and return nearly tripled. This suggests that SAC can better tolerate large input sizes, likely due to its entropy regularization and smoother policy updates. However, even this improved MASAC variant still performs worse than ISAC, which does not face centralized credit assignment at all. This shows that credit assignment remains challenge in MASAC, despite its robustness to large inputs. Notably, the poor performance of RRT*+MADDPG and RRT*+MASAC in earlier experiments aligns with these findings. Together, these findings confirm that both factorsthe size of the critic input and the difficulty of shared credit assignmentcan reduce the performance of centralized offpolicy MARL methods. We include these results in Table 4. Benchmark Maps CAMAR includes three benchmark map types designed to evaluate navigation and coordination in multi-agent scenarios. These maps differ in layout, complexity, and agent configurations. random grid map is 20 20 grid with randomly placed rectangular obstacles (with circle-based discretization). Agents are initialized in free cells, and goals are placed at randomly chosen locations. The scenario uses holonomic dynamics and includes parameters for obstacle density, agent radius, goal radius, and others. During benchmarking, we generate six different random grid instances: three maps with 8 agents and three with 32 agents. Each of these variations is evaluated under three different obstacle densities (0.0, 0.05, 0.15), which define how cluttered the map becomes. These tasks are relatively simple but still useful to test multi-agent pathfinding (MAPF) algorithms. labmaze grid is based on procedural maze generation. It creates more structured maps using multiple rectangular rooms and corridors. The parameter extra connection probability controls how many connections exist between rooms: 1.0 means the map is fully connected, while lower values introduce more isolated areas and bottlenecks. As with the random grid, we evaluate 6 instances: three maps with 8 agents and three with 32 agents, each under three different connection probabilities (0.4, 0.65, 1.0). The dynamics are the same as in the random grid, allowing consistent comparison across layouts. These tasks provide harder challenges for coordination and navigation. These benchmark maps are scalable, fast to simulate, and allow flexible control over structure and difficulty. Together, they form comprehensive testbed for MAPF. Heterogeneous Support The hetero give way map tests heterogeneous multiagent coordination. It is based on simple corridor scenario where one larger agent cannot enter the central chamber, while the smaller agent can pass through. This setup forces agents to learn implicit turn-taking or yielding behavior. Although the dynamics remain the same (HolonomicDynamic), this configuration highlights differences in agent capabilities. Basic algorithms like MAPPO and IPPO struggle in this task. Nonetheless, CAMAR supports such heterogeneous settings out of the box and can be used to advance research in coordination strategies for agents with diverse sizes, dynamics, and behaviors. Appendix Extended Scalability Analysis We also study CAMAR scalability in comparison to other popular MARL environments to better understand its limits. Comparing with Other Environments. In Table 5, we compare CAMAR to other environments with GPU support like JaxMARL and Jumanji. CAMAR is faster than VMAS and the MPE baselines, and it reaches speeds close to the best GPU simulators. For example, CAMAR runs at 338K SPS with 4 agents and 50K SPS with 32 agents. Some JaxMARL environments reach higher throughputfor example, SMAX achieves up to 1.2M SPSbut their performance drops more quickly as the number of agents grows. JaxNav, for instance, cannot run with more than 32 agents. While SMAX and RWARE show strong throughput, they use simplified settings like open space or discrete grids and do not support procedurally generated obstacle layouts. CAMAR, in contrast, supports continuous control and dense obstacles, which brings more realism. It trades small drop in speed for richer environment that better matches real-world MAPF challenges. Agents 4 8 16 32 64 CAMAR VMAS JaxNav SMAX RWARE MPE-2 MPE338K 16K 159K 0.8M 1.4M 0.6M 11.8K 189K 10K 61K 1.0M 1.1M 0.8M 11.2K 99K 5.5K 13.6K 1.2M 0.8M 1.2M 11.9K 50K 2.7K 353 0.8M 0.5M 0.7M 11.1K 128 13K 447 - 25K 1.2K - 0.3M 0.1M 0.3M 0.1M 0.5M 0.2M 7.7K 9.7K Table 5: SPS vs number of agents. Comparison across environments with GPU support. MPE-2 uses the default simpletag configuration with 2 circle obstacles, while MPE-1120 uses 1120 circles matching the number of circle obstacles in CAMAR. - indicates that the environment was too slow to even finish episode. Appendix Future Work We highlight several research directions that can be explored using CAMAR. First, CAMAR allows testing with many agents, which supports the development of scalable multi-agent reinforcement learning (MARL) methods. Future work can focus on improving how algorithms handle hundreds of agents in complex environments. Second, CAMAR is good platform for studying communication between agents. In large-scale tasks, agents often need to share information with nearby teammates. This is an important challenge that is still not well studied. CAMAR provides the tools to explore this type of localized communication. Third, CAMAR includes realistic navigation tasks, which makes it useful for combining planning methods (like RRT, RRT*) with learning-based agents. This combination could improve both sample efficiency and robustness by using longterm planning together with learned policies. Fourth, CAMAR supports agents with different abilities and dynamics. This enables research on heterogeneous teams, where agents may have different sizes, speeds, or types of control. In future work, researchers can develop methods that automatically adapt to such diverse agent groups. We hope CAMAR will support the community in studying these and other open problems in multi-agent learning and planning. Appendix Implementation Details Integrations CAMAR is designed to work easily with modern reinforcement learning tools. It follows the Gymnax interface, which is already familiar to many researchers working with RL environments on top of JAX [30]. We also provide wrapper for TorchRL [35], which allows users to integrate CAMAR into PyTorch-based pipelines. In addition, CAMAR supports integration with BenchMARL [36], framework for evaluating MARL algorithms. These integrations make it easy to use CAMAR with popular RL frameworks. Vectorized setup To maintain high simulation speed, CAMAR uses efficient vectorized operations based on JAX [30]. Agents with the same dynamic model are grouped together and updated in parallel. For agent sizes, two cases are supported: if all agents have the same radius, it is treated as constant during JAX compilation [30], giving faster simulation. If agents have different sizes, their radii are passed as vectors being part of the environment state, which still allows efficient processing and supports randomized sizes during training. Appendix Extended Related Work Waterworld (SISL) The Waterworld environment, part of the SISL (Stanford Intelligent Systems Laboratory) suite [24], is continuous control benchmark where multiple agents move in bounded two-dimensional space to collect moving targets (food) while avoiding harmful objects (poison). Agents have continuous observations and actions, and their motion dynamics are simple and lightweight, which allows high simulation speeds for training. The environment places small number of obstacles randomly in the scene, with the default setup containing only one obstacle. This limited variation does not require agents to generalize across different maps in meaningful way. Waterworld does not support heterogeneous agents, evaluation protocols, or complex multi-stage tasks, and the scenario remains structurally the same across runs. Multi-Robot Warehouse (RWare) The Multi-Robot Warehouse environment [11] simulates robots moving in warehouse to collect and deliver requested goods. The layout is grid-based, and agents must coordinate to navigate around shelves and other robots. The default version is implemented in Python using discrete state and action space. It supports partial observability but does not provide procedural generation: the warehouse layout is fixed. As result, agents do not need to generalize across different maps. Several re-implementations of RWare exist. The Jumanji version [19] rewrites the environment in JAX, which improves simulation speed and allows hardware acceleration on GPUs, but keeps the original fixed-layout design and task structure. The PufferLib version [25] modifies the environment to support large-scale parallel simulation. None of the RWare versions support heterogeneous agents, continuous control, or evaluation protocols. Despite this, RWare remains widely used benchmark for discrete-space multi-agent pathfinding and cooperative delivery tasks. Trash Pickup (PufferLib) The Trash Pickup environment [25] is grid-based multi-agent task where agents move around map to collect pieces of trash and deliver them to designated drop-off locations. It is implemented in with Python API for controlling agents, which allows efficient simulation. The environment uses discrete state and action spaces and supports partial observability. The layout is fixed, and trash positions follow predefined spawn pattern. While trash locations may vary between episodes, the underlying map structure remains the same. This means the environment does not require generalization across different maps. The design supports scalability to more than 500 agents. The Trash Pickup benchmark does not provide continuous control, heterogeneous agents, or formal evaluation protocols. However, it offers simple and repeatable cooperative task that can be scaled to large numbers of agents, making it useful test case for studying coordination efficiency in discrete environments. StarCraft Multi-Agent Challenge (SMAC) The StarCraft Multi-Agent Challenge [18] is one of the most widely used benchmarks in the MARL community. In SMAC, team of StarCraft II units controlled by independent agents must cooperate to defeat an opposing team. The environment is partially observable, and by default uses discrete action spaces. The underlying maps are fixed, which allows agents to solve the tasks without relying on observation inputs by simply memorizing the optimal action sequence for each map. This significantly reduces the need for generalization across scenarios. SMACv2 SMACv2 [15] addresses one of the key limitations of SMAC by introducing randomly generated maps with random positions of units, which prevents memorization of fixed action sequences and forces agents to generalize their policies across start positions. The rest of the environment remains the same as SMAC, with partially observable states and discrete actions by default. Like SMAC, SMACv2 does not include an evaluation protocol, but it can be evaluated using the protocol proposed in [32]. SMAX (JaxMARL) SMAX [17] is JAX-based reimplementation of SMACv2 that leverages hardware acceleration for faster simulation. In addition to performance improvements, SMAX introduces the option to switch from discrete to continuous action spaces, making it more flexible for testing different types of MARL algorithms. Multi-Agent Particle Environment (MPE) The MultiAgent Particle Environment [20, 16] is lightweight 2D simulator for testing cooperative, competitive, and communication multi-agent tasks. Agents and landmarks are represented as simple geometric shapes, and their interactions follow basic physical dynamics. MPE supports both continuous and discrete action spaces, and scenarios can be either fully or partially observable. Procedural generation is not used in the default scenarios, although obstacle or landmark positions can be randomized in some tasks. This means generalization across different maps is not strictly required. The environment does not include built-in evaluation protocol. MPE (JaxMARL) The JaxMARL re-implementation of MPE [17] offers hardware acceleration through JAX, which enables much faster simulation compared to the original Python implementation. Functionally, it mirrors MPE in terms of available scenarios, physics, and agent capabilities. It supports both continuous and discrete action spaces. Like the original MPE, it does not include procedural map generation by default and does not provide an evaluation protocol. JaxNav (JaxMARL) JaxNav [26] is navigation-focused benchmark implemented within the JaxMARL framework. It places agents in continuous 2D space where they must navigate to goal locations while avoiding static obstacles. The environment supports both continuous and discrete action spaces. Maps are fixed in the default setup, so agents do not need to generalize to unseen layouts. JaxNav is implemented in JAX for hardware acceleration but does not provide an evaluation protocol. Nocturne Nocturne [27] is 2D partially observed driving simulator implemented in C++ for performance, with Python API for training and evaluation. It focuses on realistic autonomous driving scenarios sourced from the Waymo Open Dataset. The environment provides set of benchmark tasks in which agents control autonomous vehicles interacting with traffic participants whose trajectories are taken from recorded data. This setup captures complex multi-agent interactions requiring coordination, prediction of other agents intentions, and handling of partial observability. Nocturne does not use procedurally generated maps, as all scenes are drawn from fixed datasets. While the simulator benefits from its C++ implementation, it does not provide GPU acceleration, which limits its throughput for large-scale MARL experiments. POGEMA The Partially Observable Grid Environment for Multiple Agents [12] is grid-based benchmark for partially observable multi-agent pathfinding. Agents operate with ego-centric local observations and must reach individual goals while avoiding collisions. key feature of POGEMA is its procedural map generation, producing diverse layoutsrandom maps, mazes, and warehouse-style scenariosthat require agents to generalize to unseen environments. It also offers integrated evaluation protocols with standardized metrics, enabling consistent comparison across reinforcement learning, planning, and hybrid methods. Implemented in Python, POGEMA supports scalable multi-agent experiments with both classical and learning-based policies. VMAS The Vectorized Multi-Agent Simulator for Collective Robot Learning [14] is PyTorch-based, vectorized 2D physics framework designed for efficient multi-robot benchmarking. It includes modular interface for defining custom scenarios, alongside set of built-in multi-robot tasks, that each involve relatively small number of agents. VMAS stands out for its GPU-accelerated, batch-simulated environments. It supports inter-agent communication and sensors (e.g., LIDAR). SMART Scalable Multi-Agent Realistic Testbed [28] is physics-based simulator tailored for bridging Multi-Agent Path Finding (MAPF) algorithms and real-world performance. Designed for scalability, SMART supports simulation of thousands of agents, enabling evaluation of large-scale deployments. However, its simulation speed is relatively low (around 335 SPS for 100 agents), which can limit large-scale reinforcement learning experiments. The platform targets both academic researchers and industrial users who lack access to extensive physical robot fleets, offering realistic environment to test MAPF performance under near-real conditions. Gazebo Gazebo is widely used open-source 3D robotics simulator that integrates tightly with ROS. It supports detailed physics simulation, realistic sensors (e.g., LIDAR, cameras), and high-quality rendering. While Gazebo allows multi-robot experimentation and moderate scalability, it does not support GPU acceleration for simulation logic - GPU use is limited to rendering and sensor visuals only. This limitation can make simulations of large multi-agent systems slow or resourceheavy. Webots Webots is commercial-grade 3D robotics simulator offering wide array of built-in robot models, sensors, and actuator libraries. It provides realistic physics simulation, high-quality graphics, and ROS support. Webots is well-suited for prototyping and academic or industrial robotic research. However, from multi-agent perspective, its not optimized for running large numbers of agents at scalesimulation speed tends to drop significantly as agent count grows, making massive multi-robot evaluations challenging. ARGoS ARGoS [23] is an open-source, modular simulator designed specifically for swarm robotics and large-scale multi-agent systems. Its architecture supports running thousands of simple agents efficiently by combining multiple physics engines and leveraging parallel computation. ARGoS is customizable, allowing researchers to define their own robots, sensors, actuators, etc."
        }
    ],
    "affiliations": [
        "AIRI, Moscow, Russia",
        "MIPT, Moscow, Russia"
    ]
}