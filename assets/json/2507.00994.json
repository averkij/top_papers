{
    "paper_title": "Should We Still Pretrain Encoders with Masked Language Modeling?",
    "authors": [
        "Hippolyte Gisserot-Boukhlef",
        "Nicolas Boizard",
        "Manuel Faysse",
        "Duarte M. Alves",
        "Emmanuel Malherbe",
        "André F. T. Martins",
        "Céline Hudelot",
        "Pierre Colombo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 38 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models, reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research."
        },
        {
            "title": "Start",
            "content": "Should We Still Pretrain Encoders with Masked Language Modeling? Hippolyte Gisserot-Boukhlef1,6,* Nicolas Boizard2,6,* Manuel Faysse3,6 Duarte M. Alves7,8 André F. T. Martins5,7,8 Céline Hudelot6 1Artefact Research Center 2Diabolocom 3Illuin Technology Emmanuel Malherbe1 Pierre Colombo4,6 4Equall 5Unbabel 5 2 0 2 4 ] . [ 2 4 9 9 0 0 . 7 0 5 2 : r 6MICS, CentraleSupélec, Université Paris-Saclay 7Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit) 8Instituto de Telecomunicações hippolyte.gisserot-boukhlef@centralesupelec.fr"
        },
        {
            "title": "Abstract",
            "content": "Learning high-quality text representations is fundamental to wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through series of large-scale, carefully controlled pretraining ablations, training total of 38 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models, reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research."
        },
        {
            "title": "1 Introduction",
            "content": "Learning meaningful representations is essential for wide range of NLP tasks, such as sequence classification, named entity recognition, extractive question answering, and information retrieval. Traditionally, these representations are learned with encoders pretrained solely with Masked Language Modeling (MLM) (Devlin et al., 2019). More recently, however, decoder models pretrained with Causal Language Modeling (CLM) and later adapted with MLM have challenged the MLM-only paradigm (BehnamGhader et al., 2024), achieving state-of-the-art results on the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2023; Enevoldsen et al., 2025). Yet, these results have so far only been observed on models that are significantly larger than typical encoders and trained on substantially more data (Lee et al., 2024; Meng et al., 2024; Kim et al., 2024; Muennighoff et al., 2024). As result, it remains unclear whether the success of this new training paradigm stems from the causal objective itself, or merely from increased scale. *Equal contribution 1https://huggingface.co/MLMvsCLM 2https://github.com/Nicolas-BZRD/EuroBERT/tree/MLM_vs_CLM 3https://github.com/hgissbkh/EncodEval/tree/MLM_vs_CLM 1 Figure 1: Experimental setup overview and key results on sequence classification (610M model size, 40% MLM ratio). The upper plot shows downstream performance as function of the CLMto-MLM step ratio during pretraining (configurations outperforming MLM-only are highlighted in yellow). The lower plot illustrates the effect of applying MLM CPT to models initially trained with either MLM or CLM (Base). In this paper, we conduct controlled study to investigate the impact of training with MLM and CLM objectives when learning textual representations. We compare models with equal architecture and size, trained on equal amount of data, and evaluate them on an extensive range of text representation tasks. We start by comparing pretraining exclusively on either objective, and then investigate two-stage pretraining alternative, where training starts with the CLM objective and is followed by MLM. Additionally, given the availability of existing pretrained models, we investigate continued pretraining scenario, where models are initialized from checkpoints trained exclusively with MLM or CLM, and then trained with MLM for fraction of the pretraining steps. In total, we train total of 38 models ranging from 210 million to 1 billion parameters and conduct over 15,000 fine-tuning and evaluation runs, amounting to 110k MI250X GPU hours. Backed by these large-scale experiments, our results show that: Although CLM training delivers strong performance on certain tasks and demonstrate good data efficiency and fine-tuning stability, MLM remains essential for achieving robust performance across all tasks, underscoring that bidirectional training remains indispensable (Section 3). When pretraining model from scratch, two-stage protocol that first applies CLM followed by MLM offers data-efficient way to build strong text representations by combining the strengths of both objectives (Section 4). In continued pretraining setting, adapting CLM-pretrained model with MLM proves more effective than continuing MLM training from an MLM-pretrained model. This result suggests that leveraging widely pretrained decoder models is currently the best approach to obtain strong encoder model (Section 5). To facilitate reproducibility and support future research, we release all pretrained model checkpoints,1 along with the training2 and evaluation code."
        },
        {
            "title": "2 Experimental Setup",
            "content": "In this section, we describe our controlled experimental setup, covering model architectures, pretraining and fine-tuning protocols, as summarized in Figure 1. 2 Models. The model architectures closely follow those of the EuroBERT models (Boizard et al., 2025), with sizes of 210M, 610M, and 1B parameters. All models use maximum context length of 2,048 tokens and RoPE θ value of 10,000.4 Pretraining data. Models are trained on unique English tokens from the FineWeb-Edu dataset (Penedo et al., 2024), which is known for supporting efficient model training. To ensure fair comparison across model configurations and training setups, all models are exposed to the same sequence of samples during training. Pretraining objectives. Models are trained using one of 3 approaches: 1. CLM uses next-token prediction, where each token is predicted autoregressively using causal attention mask. The training objective is to minimize the negative log-likelihood: LCLM = t=1 log (xt x<t) (causal attention) (1) where = (x1, x2, . . . , xT ) is the input sequence and (xt x<t) is the models predicted distribution over the vocabulary given the preceding tokens. 2. MLM, by contrast, randomly masks subset of tokens and trains the model to reconstruct them using bidirectional attention mask. The objective is: LMLM = iM log (xi xM) (bidirectional attention) (2) where {1, . . . , } denotes the indices of masked tokens, and xM is the input sequence with masked tokens replaced by special placeholders. We experiment with masking ratios of 20%, 30%, 40%, and 50%. 3. Finally, the two-stage CLM+MLM approach sequentially applies CLM pretraining followed by MLM. Pretraining hyperparameters. Pretraining is performed with per-device batch size of 12 samples across 192 GPUs, yielding an effective batch size of 2,373,120 tokens.5 We employ Warmup-Stable-Decay (WSD) learning rate schedule: 2,000-step warmup phase, followed by 38,000 steps with constant learning rate of 5e-4,6 ending with 2,000-step linear decay phase, for total of 42,000 training steps. Pretraining setups. To reflect two common real-world scenarios, we define two distinct pretraining setups, depending on whether the goal is to train model from scratch or to continue training from an existing checkpoint: 1. Pretraining From Scratch (PFS): Models are trained from random initialization for fixed number of steps using one of 3 objectives: CLM, MLM, or sequential CLM+MLM. For CLM and MLM, standard WSD learning rate scheduler is applied. For CLM+MLM models, training is first performed with the CLM objective, and then resumed with MLM from CLM checkpoints that have not undergone learning rate decay. 2. Continued PreTraining (CPT): Models are initialized from existing checkpoints pretrained with either CLM or MLM, and training is resumed using the MLM objective. In contrast to the PFS setup, the pretrained models used for CPT have already undergone learning rate decay during their initial training phase, reflecting real-world constraints and continued training practices. Another key difference is that CPT starts from checkpoints where the loss has already converged, whereas in PFS, the objective switch typically occurs while gradient norms are still large and learning is active. 4Additional details on the model architectures can be found in Appendix A. 5Inputs consist of variable-length sequences ranging from 12 tokens up to the maximum of 2,048. 6The optimal learning rate was selected from the range 1e-4 to 2e-3 based on experiments using 10% of the training data. Fine-tuning tasks and datasets. We evaluate all models across broad range of text representation tasks, focusing on 4 key categories commonly used in real-world applications. For Sequence Classification (SC), we use SST-2 (Socher et al., 2013), MNLI (Williams et al., 2018), and QQP (Wang et al., 2017). For Token Classification (TC), we evaluate on the English subsets of CoNLL (Tjong Kim Sang & De Meulder, 2003), OntoNotes (Hovy et al., 2006), and UNER (Mayhew et al., 2024). Question Answering (QA) is assessed using SQuAD (Rajpurkar et al., 2016), SQuAD-v2 (Rajpurkar et al., 2018), and ReCoRD (Wang et al., 2019). For Information Retrieval (IR), we use MS MARCO (Bajaj et al., 2016), NQ (Kwiatkowski et al., 2019), and the English subset of MLDR (Chen et al., 2024) for long-context evaluation.7 Fine-tuning protocol. To ensure fair comparison, all models are fine-tuned using consistent protocol. Each model is trained for up to 1,000 steps or one full epoch, whichever comes first, using batch size of 32. To account for differences in model architecture and task requirements, we perform grid search over 6 learning rates (1e-5, 2e-5, 5e-5, 1e-4, 2e-4, and 5e-4) for each model-dataset pair, with 10% warmup followed by linear decay. The learning rate yielding the best validation performance is selected. Fine-tuning employs bidirectional attention mask with task-specific loss functions: for SC, we use cross-entropy on mean-pooled token embeddings; TC and QA are trained using token-level cross-entropy; and for IR, we rely on the InfoNCE loss (Oord et al., 2018) with in-batch negatives, using mean pooling. To account for the fine-tuning instability commonly observed in BERT-style models for representation learning (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020; Zhang et al., 2020), the entire procedure is repeated across 5 random seeds. Fine-tuning is conducted on the in-domain training set, except for NQ and MLDR, for which training is performed on MS MARCO. Evaluation protocol. SC is assessed with accuracy, TC and QA with F1 score, and IR with NDCG@10. We report results averaged across seeds, along with 95% confidence intervals. Infrastructure. All pretraining and fine-tuning experiments are carried out on MI250X GPUs provided by the Adastra supercomputer, using the AMD-optimized EuroBERT codebase (Boizard et al., 2025). Experimental scale. Drawing reliable conclusions about model design choices typically requires scaling up and repeating experiments, as statistically sound results often require large number of runs and sufficient training to reach minimal convergence (Hoffmann et al., 2022). We therefore carefully design our experiments to ensure robust and well-supported findings. The default pretraining setup uses 100B tokens, which corresponds to 5 times the optimal data budget proposed by Hoffmann et al. (2022) for decoder models in the 1B-parameter range. Pretraining runs are executed on 24 nodes with 8 GPUs each (192 GPUs total), while fine-tuning is carried out on single GPUs. The total compute budget amounts to 110k GPU hours, broken down as follows: Base pretraining includes all 3 model sizes (210M, 610M, and 1B), trained on both CLM and MLM objectives. For MLM, 4 masking ratios (20%, 30%, 40%, and 50%) are explored, resulting in total of 15 models, each trained for 42,000 steps (100B tokens), and accounting for 81k GPU hours. PFS experiments explore various CLM-to-MLM step ratios (0%100%, 25%75%, 50%50%, 75%25%, and 100%0%) across 3 training lengths (12,000, 22,000, and 42,000 steps), focusing on the 610M model size with 40% masking ratio for MLM. This setup is also used to evaluate the effect of masking ratio on models pretrained with CLM followed by MLM. To minimize redundant computation, we reuse checkpoints from the base pretraining runs whenever possible, applying learning rate decay only during the final 2,000 steps. In total, these runs result in 17 new models and account for 120,000 additional training steps and 16k GPU hours. CPT experiments are performed on 610M models with 40% masking ratio and varying training lengths (2,000, 12,000, and 22,000 steps) applied to both CLMand MLM-pretrained models. To avoid redundant computation, training is resumed from shared checkpoints whenever possible. These runs yield 6 new models, totaling 26,000 training steps and 4k GPU hours. 7Further details are provided in Appendix B. 4 Figure 2: MLM vs. CLM downstream performance, averaged across tasks and reported for all model sizes. For MLM, results correspond to 40% masking ratio. Figure 3: Task-wise downstream performance across different masking ratios for all model sizes. Model evaluation involves fine-tuning 50 model checkpoints in total, including 38 final pretrained checkpoints and 12 intermediate ones. Each model is fine-tuned on 12 datasets, across 6 learning rates and 5 random seeds, resulting in 15,120 fine-tuning runs which total about 9k GPU hours. Allocating substantial compute to the evaluation phase is particularly important in representation learning, where fine-tuning can exhibit high variance and instability (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020; Zhang et al., 2020)."
        },
        {
            "title": "3 Pretraining with CLM or MLM",
            "content": "This section presents preliminary results comparing the MLM and CLM pretraining objectives in terms of downstream performance, data efficiency, and training and fine-tuning stability. MLM generally outperforms CLM on text representation tasks. As expected, bidirectional attention during pretraining tends to enhance representation quality on downstream tasks (Figure 2). The performance gap between models trained with MLM and those trained with CLM is especially noticeable on SC and QA tasks, and remains consistent across model sizes. The largest discrepancy is observed on QA, which appears particularly sensitive to the absence of bidirectional attention during pretraining. Interestingly, some task-specific trends emerge: for instance, the MLMto-CLM gap widens with increasing model size on SC, but narrows on IR. There is no universally optimal masking ratio. The masking ratio is key design choice when pretraining models with MLM. As shown in Figure 3, there is no single best ratio, as it depends on both model size and downstream task, making MLM pretraining delicate balance. Larger models tend to benefit from higher masking ratios, consistent with prior findings from Wettig et al. (2023). Across tasks, IR datasets 5 Figure 4: Downstream performance as function of pretraining steps for CLM and MLM objectives. Results are reported for 610M models, with 40% masking ratio for MLM. Figure 5: Impact of the fine-tuning learning rate on MLMvs. CLM-pretrained models. Error bars indicate the standard deviation of metric scores across all seeds and learning rates between 1e-5 and 1e-4. Results are shown for 610M-parameter models, with 40% masking ratio for MLM. consistently prefer higher masking ratios regardless of model size. In contrast, for token-level tasks such as TC and QA, smaller models perform better with lower masking ratios. For larger models (610M and 1B), the performance curves exhibit U-shape, indicating improved performance at both low and high masking ratios.8 To reduce computational overhead, most subsequent experiments report results on 610M models trained with 40% masking ratio for MLM, which provides strong overall compromise across tasks. CLM models can perform competitively. Interestingly, although CLM-pretrained models tend to underperform on SC, QA, and IR tasks, they achieve competitive results on TC, and even outperform MLM models at the 610M size (Figure 2). This suggests that causal pretraining can produce strong token-level representations, highlighting its potential despite the absence of bidirectional context. CLM is more data-efficient than MLM in the early stages of training. As shown in Figure 4, CLM consistently outperforms MLM in downstream performance during the early stages of training (up to step 10,000 for SC and QA, 20,000 for IR, and even until the end for TC). However, as training progresses, MLM models tend to catch up and often surpass CLM, while CLM shows more limited gains in later steps. This suggests that although CLM saturates earlier, it enables more efficient representation learning in fewer steps. Notably, this makes CLM an appealing option for data-scarce scenarios, such as pretraining on low-resource languages, or simply as warmup stage before MLM-based encoder training. CLM-based pretraining improves fine-tuning stability. key challenge in fine-tuning models for text representation tasks is selecting optimal hyperparameters, particularly the learning rate, which can be sensitive to factors like model size, task type, or dataset scale, making exhaustive grid searches computa8Additional results in Appendix further highlight the influence of masking ratio, showing substantial variation even among datasets within the same task category. 6 Figure 6: Impact of two-stage CLM+MLM pretraining on downstream performance under different training budgets (12,000, 22,000, and 42,000 steps). The x-axis shows the percentage of CLM steps allocated in the first phase. Experiments are conducted on 610M models, with 40% masking ratio during MLM training. Figure 7: Comparison of downstream performance variability across different masking ratios (20%, 30%, 40%, 50%) for CLM and CLM+MLM pretraining configurations. Error bars indicate the standard deviation across fine-tuning seeds and masking ratios. Results are reported for 610M models, using 42,000 training steps for MLM and schedule of 40,000 CLM steps followed by 2,000 MLM steps for CLM+MLM. tionally expensive. As shown in Figure 5, models pretrained with CLM demonstrate lower sensitivity to learning rate choices than those pretrained with MLM. This indicates that CLM pretraining provides more stable initialization for fine-tuning, facilitating more reliable performance and reducing the need for extensive hyperparameter tuning."
        },
        {
            "title": "4 Two-Stage CLM+MLM Pretraining",
            "content": "Building on Section 3, which shows that CLM outperforms MLM in TC performance, data efficiency, and training stability, we explore the benefits of applying CLM and MLM sequentially during pretraining. Under fixed compute constraints, starting pretraining with CLM and continuing with MLM yields better results than MLM alone. Motivated by the strong performance of CLM in learning text representations, we investigate the impact of two-stage pretraining objectives, starting with CLM and transitioning to MLM. We evaluate this under fixed compute budgets of 12,000, 22,000, and 42,000 training steps, using different splits: 100% MLM, 25%-75%, 50%-50%, 75%-25%, and 100% CLM. Interestingly, as shown in Figure 6, combining CLM and MLM consistently improves downstream performance compared to using MLM alone, though the effect varies by task and training budget. Overall, split between 25%-75% and 50%-50% seems to provide the best balance. CLM-based models exhibit lower sensitivity to masking ratio. As shown in Figure 7, adapted CLM models show less variation in performance across different masking ratios compared to fully MLMtrained models. Initial CLM pretraining appears to stabilize model weights, making adaptation more robust 7 Figure 8: Impact of performing MLM CPT on either CLMor MLM-pretrained models (denoted as Base). CPT is conducted for 22,000 steps on 610M models with 40% masking ratio, following 42,000 steps of initial pretraining. Figure 9: MLM loss curves for CLMand MLM-pretrained models across the 3 CPT compute budgets (2,000, 12,000, and 22,000 steps). Results are reported for 610M models, with MLM using 40% masking ratio. to masking ratio choices and yielding more consistent downstream performance less sensitive to this design parameter."
        },
        {
            "title": "5 Continued Pretraining from CLM and MLM Models",
            "content": "In this section, we focus on the CPT setting and analyze whether it is more effective to adapt CLMpretrained model using MLM or to continue MLM training from an MLM-pretrained model. MLM CPT on CLM-pretrained model outperforms MLM-only training. We consider setting in which equally-sized CLM and MLM models have been pretrained on the same data. The key question is whether it is more beneficial to spend additional compute on applying MLM CPT to the CLM model or to further train the MLM model. To keep computational costs manageable, we perform 22,000-step CPT on the 610M model using 40% masking ratio. As shown in Figure 8, the MLM-adapted CLM model consistently achieves superior downstream performance. On TC, where CLM-only models were already strong, performance is maintained and the gap to MLM remains. For QA and IR, the gap is effectively closed, while for SC, the MLM-adapted CLM model significantly outperforms the MLM-only model. Interestingly, we observe that it is not necessary Fewer CPT steps already show strong performance. to run as many as 22,000 CPT steps to achieve comparable performance between MLM and adapted CLM. As early as 12,000 steps, the results are already strong and broadly match those of MLM-only CPT in terms of loss (Figure 9) and downstream performance (Figure 10), with better results on TC and IR, comparable performance on SC, and nearly on par on QA. In terms of trends, applying MLM CPT on CLM model also 8 Figure 10: Downstream performance as function of CPT length for CLMand MLM-pretrained models, reported for settings of 2,000, 12,000, and 22,000 training steps. Experiments use 610M models with 40% MLM ratio. appears more promising, showing steeper improvement curve toward the end, while MLM-only training seems to plateau (particularly noticeable on SC)."
        },
        {
            "title": "6 Related Work",
            "content": "Learning universal text representations has been central focus in NLP, with MLM emerging as the dominant pretraining objective following the success of models like BERT (Devlin et al., 2019). MLM-based encoders leverage bidirectional attention to capture rich contextual dependencies, enabling strong performance across wide range of tasks such as classification, question answering, and named entity recognition. Subsequent work has explored architectural extensions (e.g., RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2021), ModernBert (Warner et al., 2024)) and more efficient pretraining strategies (Clark et al., 2020). In contrast, decoder-only architectures such as GPT-style models were originally optimized for autoregressive generation (Brown et al., 2020; Grattafiori et al., 2024; Jiang et al., 2023; Almazrouei et al., 2023), making them less naturally suited for text representation tasks. Nevertheless, numerous studies have successfully explored adapting decoder-only models (typically larger and trained on more data than standard encoders) for learning high-quality text representations (Muennighoff, 2022; Lee et al., 2024; Meng et al., 2024; Zhang et al., 2025; Lee et al., 2025). As result, these adapted decoders now rank among the top-performing models on prominent embedding benchmarks such as MTEB (Muennighoff et al., 2023; Enevoldsen et al., 2025). However, their strong performance does not disentangle the underlying factors (likely combination of model scale and data regime) notably leaving open the question of whether causal pretraining itself plays meaningful role in downstream representation quality. Several recent studies have investigated the key factors influencing the performance of text representation models. BehnamGhader et al. (2024) propose general framework for adapting decoder-only models to embedding tasks, accompanied by series of ablations aimed at isolating the primary contributors to their performance. Some papers explore how to reintroduce bidirectional attention in causal decoder models, by repeating sequences twice (Springer et al., 2024), training with custom-shaped attention masks (Raffel et al., 2023) or selectively removing the causal attention mask on parts of the sequence in post-training phase (Kopiczko et al., 2024; Beyer et al., 2024). Wettig et al. (2023) explore the impact of the masking ratio on downstream tasks such as sequence classification and question answering, with focus on learning dynamics, model scale, and fine-tuning behavior. Building on this line of work, we present comprehensive analysis of how the choice of learning objective and attention mask during training influences downstream representation quality. By employing unified and controlled pretraining setup, we ensure fair comparisons and minimize confounding factors, allowing for more precise attribution of the effects observed."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "Our study challenges the long-standing assumption that Masked Language Modeling (MLM) is the universally optimal approach for encoder pretraining. Through large-scale experiments at the 100B training token scale, we provide robust empirical evidence that encoders can be trained more data-efficiently without relying solely on MLM objectives. In particular, we demonstrate that decoder-only models trained with Causal Language Modeling (CLM) offer clear advantages in data efficiency, training stability, and performance on specific text representation tasks. Moreover, we show that sequential training (first with CLM followed by MLM) is more effective than MLM alone. These results suggest that the most effective path to high-performing encoder models may involve leveraging pretrained decoder models and continuing with MLM-based training, paving the way for future state of the art encoders at discounted price. We hope our findings and released resources will support further advancements in this area. Future work. Future research may extend our findings to Vision-Language Models (VLMs), many of which use decoder-based architectures, potentially improving their representation learning capabilities. Further investigations could also apply our insights to pretrain state-of-the-art encoders by capitalizing on the complementary strengths of CLM and MLM, and leveraging the availability of powerful open-source decoders, as suggested in this work. Limitations. Our experiments are conducted at the 100B training token scale, over 5 times the computeoptimal ratio suggested for decoders in Hoffmann et al. (2022), but our loss curves suggest that further performance improvements are possible with extended training. In extremely high-compute and high-data regimes, the data-efficiency gains from CLM may diminish as they are counterbalanced by increased training capacity. Additionally, our study focuses on large-scale ablations conducted solely in English. It remains uncertain whether our conclusions generalize to other languages, where linguistic and structural differences may affect pretraining efficacy. We also do not examine the impact of common post-training practices applied to decoder models, such as fine-tuning on reasoning or mathematical datasets. Consequently, we have not analyzed how this post-training phase might influence model behavior or scaling trends relative to models pretrained from scratch with encoder-style data. Lastly, our evaluation centers on standard encoder benchmarks. Emerging use cases, such as late interaction (Khattab & Zaharia, 2020) or late chunking (Günther et al., 2024; Conti et al., 2025), which depend heavily on bidirectional attention, may surface even larger performance gaps."
        },
        {
            "title": "Acknowledgments",
            "content": "We gratefully acknowledge the ADASTRA supercomputer (CINES) for providing technical support and high-performance computing (HPC) resources through projects C1615122, GDA2401, and CAD16490. We also thank the French government for its support through the France 2030 program, as part of the ArGiMi project. This work was further supported by the European Unions Horizon Europe Research and Innovation Actions (UTTER, grant 101070631), the DECOLLAGE project (ERC-2022-CoG, grant 101088763), and the Portuguese Recovery and Resilience Plan via project C645008882-00000055 (Center for Responsible AI). Additional support was provided by FCT/MECI through national funds and, when applicable, co-funded EU funds under UID/50008: Instituto de Telecomunicações. Finally, we thank Antoine Chaffin for the insightful discussions."
        },
        {
            "title": "References",
            "content": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. URL https://arxiv.org/abs/1611. 09268. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=IW1PR7vEBf#discussion. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. URL https://arxiv.org/abs/2407.07726. Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Duarte M. Alves, André Martins, Ayoub Hammal, Caio Corro, Céline Hudelot, Emmanuel Malherbe, Etienne Malaboeuf, Fanny Jourdan, Gabriel Hautreux, João Alves, Kevin El-Haddad, Manuel Faysse, Maxime Peyrard, Nuno M. Guerreiro, Patrick Fernandes, Ricardo Rei, and Pierre Colombo. Eurobert: Scaling multilingual encoders for european languages, 2025. URL https://arxiv.org/abs/2503.05500. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. URL https://proceedings. neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-embedding: Multilinguality, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 23182335, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.137. URL https://aclanthology.org/2024. findings-acl.137/. Kevin Clark, Minh-Thang Luong, Quoc Le, and Christopher Manning. Electra: Pre-training text encoders as discriminators rather than generators. In The Eighth International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB. Max Conti, Manuel Faysse, Gautier Viaud, Antoine Bosselut, Céline Hudelot, and Pierre Colombo. Context is gold to find the gold passage: Evaluating and training contextual document embeddings. arXiv preprint arXiv:2505.24782, 2025. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423/. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Finetuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305, 2020. URL https://arxiv.org/abs/2002.06305. Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, et al. Mmteb: Massive multilingual text embedding benchmark. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=zl3pfz4VCV. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Michael Günther, Isabelle Mohr, Daniel James Williams, Bo Wang, and Han Xiao. Late chunking: Contextual chunk embeddings using long-context embedding models, 2024. URL https://arxiv.org/abs/ 2409.04701. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In The Ninth International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=XPZIaotutsD. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. URL https://arxiv.org/abs/2203. 15556. Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. OntoNotes: The 90% solution. In Robert C. Moore, Jeff Bilmes, Jennifer Chu-Carroll, and Mark Sanderson (eds.), Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pp. 5760, New York City, USA, June 2006. Association for Computational Linguistics. URL https://aclanthology.org/N06-2015/. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert, 2020. URL https://arxiv.org/abs/2004.12832. Jihoon Kwon Sangmo Gu Yejin Kim, Minkyung Cho Jy-yong Sohn Chanyeol, Choi Junseong Kim, and Seolhwa Lee. Linq-embed-mistral: Elevating text retrieval with improved gpt data through task-specific control and quality refinement. linq ai research blog, 2024. Dawid J. Kopiczko, Tijmen Blankevoort, and Yuki M. Asano. Bitune: Bidirectional instruction-tuning, 2024. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026/. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. URL https://arxiv.org/abs/2405.17428. Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to finetune largescale pretrained language models. In International Conference on Learning Representations, 2020. URL https://arxiv.org/abs/1909.11299. Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gustavo Hernández Ábrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv preprint arXiv:2503.07891, 2025. URL https://arxiv.org/abs/2503.07891. 12 Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. URL https://arxiv.org/abs/1907.11692. Stephen Mayhew, Terra Blevins, Shuheng Liu, Marek Suppa, Hila Gonen, Joseph Marvin Imperial, Börje F. Karlsson, Peiqin Lin, Nikola Ljubešić, Nikola Ljubešić, LJ Miranda, Barbara Plank, Arij Riabi, and Yuval Pinter. Universal NER: gold-standard multilingual named entity recognition benchmark. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 43224337, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.243. URL https://aclanthology.org/2024.naacl-long.243/. Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfrembeddingmistral: enhance text retrieval with transfer learning. Salesforce AI Research Blog, 3:6, 2024. URL https://www.salesforce.com/blog/sfr-embedding/. Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904, 2022. URL https://arxiv.org/abs/2202.08904. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 20142037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.148. URL https: //aclanthology.org/2023.eacl-main.148/. Niklas Muennighoff, SU Hongjin, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. In ICLR 2024 Workshop: How Far Are We From AGI, 2024. URL https://arxiv.org/abs/2402.09906. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. URL https://arxiv.org/abs/1807.03748. Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024. URL https://arxiv.org/abs/2406.17557. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2023. URL https://arxiv.org/abs/1910.10683. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https: //aclanthology.org/D16-1264/. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for SQuAD. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://aclanthology.org/P18-2124/. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (eds.), Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631 1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170/. 13 Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings, 2024. URL https://arxiv.org/abs/2402.15449. Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pp. 142147, 2003. URL https://aclanthology.org/W03-0419/. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: multitask benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupała, and Afra Alishahi (eds.), Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446/. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Superglue: stickier benchmark for general-purpose language unLevy, and Samuel Bowman. derstanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ Associates, 4496bf24afe7fab6f046bf4923da8de6-Paper.pdf. Inc., 2019. Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural language sentences. arXiv preprint arXiv:1702.03814, 2017. URL https://arxiv.org/abs/1702.03814. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference, 2024. URL https://arxiv.org/abs/2412.13663. Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask 15% in masked language modeling? In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 29853000, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.217. URL https: //aclanthology.org/2023.eacl-main.217/. Adina Williams, Nikita Nangia, and Samuel Bowman. broad-coverage challenge corpus for sentence understanding through inference. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 11121122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://aclanthology. org/N18-1101/. Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Weinberger, and Yoav Artzi. Revisiting few-sample bert fine-tuning. arXiv preprint arXiv:2006.05987, 2020. URL https://arxiv.org/abs/2006.05987. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. URL https://arxiv. org/abs/2506.05176."
        },
        {
            "title": "A Training Setup Details",
            "content": "This appendix provides additional information on the training parameters used in this study. Table 1 outlines both the architectural configurations and training settings."
        },
        {
            "title": "Model Size",
            "content": "210M 610M 1B"
        },
        {
            "title": "Architecture",
            "content": "Layers Embedding Dimension FFN Dimension Attention Heads Key/Value Heads Layer Normalization RMSNorm ϵ Activation Function Maximum Context Length Positional Embeddings RoPE θ Tokenizer Vocabulary Size 28 1,728 5,120 18 6 12 768 3,072 12 12 26 1,152 4,096 18 6 RMSNorm 1 105 SwiGLU 2,048 RoPE 10,000 LLaMA 3 128,256 Training Weight Initialisation Learning Rate Scheduler Warmup Steps Decay Steps Optimizer Adam β1 Adam β2 Adam ϵ Weight Decay Gradient Clipping Norm Per-GPU Batch Size Gradient Accumulation Steps GPUs Tokens/Step (µ = 0, σ2 = 0.2) 5 104 WSD (linear decay) 2,000 5% AdamW 0.9 0.95 1 105 0.1 1.0 12 1 192 2,359,296 Table 1: Architecture and training settings for the 3 model sizes under consideration."
        },
        {
            "title": "B Details on Evaluation Datasets",
            "content": "This appendix offers additional details on the datasets used for evaluation. SC datasets: All SC datasets are sourced form the GLUE (Wang et al., 2018) benchmark. MNLI (Williams et al., 2018) MNLI is large-scale benchmark for natural language inference, where each example pairs premise with hypothesis, and the task is to determine whether the hypothesis is entailed by, contradicts, or is neutral with respect to the premise. Since the test set is not publicly available, we report results on the validation set and reserve 5% of the training data for validation. We evaluate using the matched subset, which reflects in-domain performance. 15 QQP (Wang et al., 2017) Binary classification dataset from Quora that asks whether pair of questions are semantically equivalent. Similarly to MNLI, we report results on the validation set and reserve 5% of the training data for validation. SST-2 (Socher et al., 2013) Sentiment classification dataset consisting of movie review sentences. Each sentence is labeled as expressing either positive or negative sentiment. We report results on the validation set and reserve 5% of the training data for validation. TC datasets: CoNLL (Tjong Kim Sang & De Meulder, 2003) NER dataset comprising English and German newswire text, annotated with 4 entity types: person, organization, location, and miscellaneous. In this work, we use only the English portion. OntoNotes (Hovy et al., 2006) large-scale dataset for NER and other NLP tasks, spanning diverse genres including newswire, broadcast news, and conversational speech in both English and Chinese. For this study, we focus on the English portion and evaluate on named entity spans. UNER (Mayhew et al., 2024) Multilingual NER dataset based on Wikipedia and Wikidata annotations. For this work, we use only the English subset. QA datasets: ReCoRD (Wang et al., 2019) Challenging extractive QA dataset that tests commonsense reasoning by requiring models to select entities from passage to answer cloze-style questions. Since the test set is not publicly available, we report results on the validation set and reserve 5% of the training data for validation. SQuAD (Rajpurkar et al., 2016) Benchmark for extractive question answering. It consists of questions posed on Wikipedia articles, with answers given as text spans within the corresponding passages. We report results on the validation set and reserve 5% of the training data for validation. SQuAD-v2 (Rajpurkar et al., 2018) Extension of SQuAD that includes both answerable and unanswerable questions. We report results on the validation set and reserve 5% of the training data for validation. IR datasets: MLDR (Chen et al., 2024) Multilingual retrieval benchmark focused on long documents. In this work, we use only the English subset. Since MLDR does not provide training data, models are evaluated directly on the validation and test splits without task-specific fine-tuning. MS MARCO (Bajaj et al., 2016) Large-scale English passage retrieval dataset derived from real Bing search queries and web documents. MS MARCO is used in this work for both training and evaluation. NQ (Kwiatkowski et al., 2019) Open-domain English retrieval dataset based on real Google search queries, where answers are typically found in Wikipedia articles. As for MLDR, models are evaluated directly on the validation and test splits without task-specific fine-tuning. To reduce computational overhead during evaluation on retrieval datasets, we restrict the corpus to labeled documents only, that is, those marked as positive or negative. Unlabeled documents are excluded from the evaluation."
        },
        {
            "title": "C Detailed Results",
            "content": "C.1 MLM vs. CLM This appendix provides detailed results comparing CLM and MLM objectives, including variations in masking ratios and model sizes. Table 2, Table 3, Table 4, and Table 5 present dataset-level results for SC, TC, QA, and IR, respectively. 16 Model Size Objective Masking SST-2 QQP MNLI Average 210M 610M 1B MLM CLM MLM CLM MLM CLM 20% 30% 40% 50% - 20% 30% 40% 50% - 20% 30% 40% 50% - 89.93 (0.26) 89.84 (1.24) 90.44 (0.71) 91.61 (0.35) 86.00 (0.18) 85.82 (0.36) 85.52 (0.21) 85.52 (0.49) 79.29 (0.32) 79.69 (0.22) 78.72 (0.20) 78.00 (0.51) 85.07 (0.12) 85.12 (0.40) 84.89 (0.30) 85.04 (0.25) 90.00 (0.30) 83.84 (0.27) 74.56 (0.16) 82.80 (0.02) 92.61 (0.57) 91.49 (0.69) 91.42 (0.62) 91.93 (0.26) 86.98 (0.06) 86.48 (0.25) 86.98 (0.15) 86.38 (0.34) 83.08 (0.33) 81.31 (1.27) 82.59 (0.16) 81.74 (0.36) 87.56 (0.16) 86.43 (0.51) 87.00 (0.18) 86.68 (0.13) 91.10 (0.39) 84.14 (0.12) 75.49 (0.22) 83.58 (0.17) 92.39 (0.47) 92.13 (0.56) 92.89 (0.22) 93.28 (0.25) 87.20 (0.18) 86.46 (0.20) 87.41 (0.17) 87.27 (0.17) 84.33 (0.13) 82.40 (0.24) 84.40 (0.25) 84.09 (0.16) 87.97 (0.18) 87.00 (0.13) 88.23 (0.06) 88.21 (0.12) 92.48 (0.46) 83.66 (0.11) 71.89 (0.44) 82.68 (0.30) Table 2: MLM vs. CLM downstream performance results on SC datasets. Model Size Objective Masking OntoNotes CoNLL UNER Average 210M 610M 1B MLM CLM MLM CLM MLM CLM 20% 30% 40% 50% - 20% 30% 40% 50% - 20% 30% 40% 50% - 92.68 (0.06) 92.71 (0.14) 92.61 (0.09) 92.46 (0.12) 91.38 (0.15) 91.37 (0.19) 91.70 (0.19) 91.56 (0.10) 92.46 (0.10) 92.45 (0.13) 92.08 (0.28) 92.25 (0.16) 92.17 (0.09) 92.18 (0.07) 92.13 (0.11) 92.09 (0.07) 92.18 (0.07) 91.67 (0.32) 92.70 (0.10) 92.18 (0.09) 92.93 (0.13) 92.69 (0.15) 92.97 (0.15) 92.95 (0.20) 92.34 (0.17) 91.37 (0.40) 91.58 (0.44) 91.79 (0.16) 93.10 (0.29) 91.94 (0.64) 92.07 (0.77) 92.84 (0.33) 92.79 (0.09) 92.00 (0.27) 92.21 (0.26) 92.53 (0.15) 92.79 (0.05) 92.16 (0.21) 93.12 (0.29) 92.69 (0.14) 93.03 (0.15) 92.93 (0.17) 92.98 (0.22) 93.12 (0.25) 92.28 (0.30) 92.06 (0.10) 92.05 (0.14) 92.46 (0.30) 92.84 (0.31) 92.55 (0.16) 92.51 (0.47) 93.06 (0.55) 92.72 (0.19) 92.51 (0.09) 92.51 (0.16) 92.88 (0.19) 92.60 (0.09) 91.93 (0.19) 92.84 (0.12) 92.46 (0.10) Table 3: MLM vs. CLM downstream performance results on TC datasets. C.2 Data Efficiency This appendix provides detailed results on data efficiency for MLM vs CLM models at different pretraining steps. Results are presented for 610M models with 40% masking for MLM. Table 6, Table 7, Table 8, and Table 9 present dataset-level results for SC, TC, QA, and IR, respectively. C.3 CLM-to-MLM Ratio This appendix provides detailed results in the PFS setting, analyzing the impact of varying the CLMMLM step ratio during pretraining. Results are presented for 610M models with 40% masking for MLM. Table 13, Table 11, Table 12, and Table 13 present dataset-level results for SC, TC, QA, and IR, respectively."
        },
        {
            "title": "Model Size Objective Masking",
            "content": "SQuAD SQuAD-v2 ReCoRD"
        },
        {
            "title": "Average",
            "content": "210M 610M 1B"
        },
        {
            "title": "MLM",
            "content": "CLM 20% 30% 40% 50% - 20% 30% 40% 50% - 20% 30% 40% 50% - 74.66 (0.26) 74.59 (0.11) 73.73 (0.11) 72.75 (0.50) 64.39 (0.44) 61.87 (1.02) 61.10 (1.34) 58.47 (1.96) 16.15 (6.01) 16.57 (7.13) 13.45 (2.93) 14.74 (6.65) 51.73 (2.03) 51.01 (2.58) 49.43 (1.36) 48.65 (2.99) 64.00 (0.54) 52.60 (0.69) 1.17 (0.78) 39.26 (0.32) 76.97 (0.19) 77.51 (0.20) 77.76 (0.29) 77.41 (0.47) 71.31 (0.30) 65.98 (2.56) 70.63 (0.58) 68.61 (0.53) 46.74 (1.11) 40.92 (1.75) 39.93 (5.26) 45.82 (0.63) 65.00 (0.31) 61.47 (0.61) 62.77 (1.86) 63.95 (0.47) 69.42 (0.30) 56.44 (0.10) 0.42 (0.39) 42.09 (0.20) 77.81 (0.57) 76.82 (0.61) 79.93 (0.27) 79.62 (0.27) 73.03 (0.35) 66.09 (2.13) 74.96 (0.41) 73.53 (0.46) 54.38 (0.92) 43.13 (4.94) 55.95 (0.99) 55.67 (1.19) 68.41 (0.33) 62.02 (1.84) 70.28 (0.32) 69.61 (0.56) 70.54 (0.69) 57.99 (0.96) 4.90 (4.31) 44.48 (1.65) Table 4: MLM vs. CLM downstream performance results on QA datasets. Model Size Objective Masking NQ MS MARCO MLDR Average 210M 610M 1B MLM CLM MLM CLM MLM CLM 20% 30% 40% 50% - 20% 30% 40% 50% - 20% 30% 40% 50% - 80.23 (0.54) 81.20 (0.92) 81.43 (1.35) 82.97 (0.42) 90.24 (0.76) 90.44 (0.75) 89.35 (0.98) 89.31 (0.35) 53.76 (0.90) 55.14 (1.02) 56.56 (0.62) 58.85 (0.46) 74.74 (0.35) 75.59 (0.57) 75.78 (0.43) 77.04 (0.27) 82.08 (0.93) 86.45 (0.38) 49.95 (1.86) 72.83 (0.86) 85.14 (0.49) 83.47 (0.93) 86.76 (0.63) 87.64 (0.74) 88.82 (0.78) 90.36 (0.81) 91.65 (0.45) 90.73 (1.26) 57.07 (0.93) 58.65 (0.42) 60.25 (0.76) 60.49 (0.73) 77.01 (0.52) 77.50 (0.42) 79.55 (0.35) 79.62 (0.54) 85.57 (0.34) 89.88 (1.17) 53.16 (0.42) 76.20 (0.54) 86.02 (0.55) 86.36 (0.54) 87.89 (0.66) 89.28 (0.40) 91.78 (0.56) 90.90 (0.61) 91.45 (0.40) 92.98 (0.65) 59.85 (0.40) 61.11 (0.25) 60.51 (0.80) 63.64 (0.34) 79.22 (0.19) 79.46 (0.04) 79.95 (0.41) 81.97 (0.27) 88.73 (0.54) 90.85 (1.22) 56.45 (0.39) 78.68 (0.55) Table 5: MLM vs. CLM downstream performance results on IR datasets. C.4 Continued Pretraining This appendix presents detailed results for the CPT setting, where different CPT lengths are evaluated. All results correspond to 610M-parameter models trained with 40% masking ratio under the MLM objective. Table 14, Table 15, Table 16, and Table 17 report dataset-level performance for SC, TC, QA, and IR, respectively."
        },
        {
            "title": "Objective Training Step",
            "content": "SST-"
        },
        {
            "title": "CLM",
            "content": "1K 2K 5K 10K 20K 40K 1K 2K 5K 10K 20K 40K 81.06 (0.63) 81.40 (0.81) 82.11 (0.62) 86.90 (1.93) 91.38 (0.54) 90.94 (0.67) 79.52 (1.00) 84.24 (0.47) 87.11 (0.93) 89.33 (0.29) 90.50 (0.35) 90.94 (0.43) 73.78 (0.18) 73.88 (0.12) 78.44 (1.06) 83.83 (0.27) 85.97 (0.28) 86.42 (0.25) 73.56 (0.24) 76.03 (0.70) 81.80 (0.31) 82.66 (0.24) 83.74 (0.08) 84.20 (0.21) 45.90 (0.22) 49.40 (0.33) 56.90 (2.44) 73.37 (1.62) 79.36 (0.17) 81.84 (0.30) 51.89 (0.49) 60.54 (0.89) 66.76 (0.34) 69.75 (0.22) 73.87 (0.45) 75.31 (0.43) 66.91 (0.26) 68.23 (0.34) 72.49 (1.17) 81.37 (0.90) 85.57 (0.20) 86.40 (0.27) 68.32 (0.28) 73.61 (0.26) 78.56 (0.46) 80.58 (0.14) 82.70 (0.07) 83.48 (0.21) Table 6: MLM vs. CLM downstream performance on SC datasets at various pretraining checkpoints. Results correspond to 610M models with MLM using 40% masking. Objective Training Step OntoNotes CoNLL UNER Average MLM CLM 1K 2K 5K 10K 20K 40K 1K 2K 5K 10K 20K 40K 84.39 (0.45) 88.64 (0.16) 90.93 (0.07) 92.03 (0.14) 92.62 (0.07) 92.86 (0.17) 88.09 (0.13) 90.16 (0.11) 91.54 (0.10) 92.27 (0.11) 92.69 (0.08) 92.75 (0.11) 82.47 (0.28) 86.02 (0.19) 88.10 (0.94) 90.60 (0.22) 91.36 (0.21) 91.45 (0.28) 85.51 (0.19) 88.82 (0.21) 90.82 (0.29) 91.47 (0.18) 92.21 (0.16) 92.25 (0.14) 82.58 (0.78) 85.74 (0.64) 89.05 (0.89) 91.31 (0.13) 92.06 (0.12) 91.75 (0.59) 87.44 (0.35) 89.57 (0.31) 91.15 (0.28) 92.49 (0.22) 92.99 (0.30) 92.97 (0.06) 83.15 (0.26) 86.80 (0.23) 89.36 (0.61) 91.31 (0.08) 92.02 (0.03) 92.02 (0.19) 87.01 (0.15) 89.52 (0.15) 91.17 (0.08) 92.08 (0.10) 92.63 (0.12) 92.65 (0.03) Table 7: MLM vs. CLM downstream performance on TC datasets at various pretraining checkpoints. Results correspond to 610M models with MLM using 40% masking. Objective Training Step SQuAD SQuAD-v2 ReCoRD Average MLM CLM 1K 2K 5K 10K 20K 40K 1K 2K 5K 10K 20K 40K 0.00 (0.00) 0.94 (0.50) 33.80 (15.60) 59.19 (12.12) 73.09 (0.29) 77.00 (0.27) 4.52 (0.75) 30.93 (11.46) 52.27 (0.76) 62.31 (0.51) 66.68 (0.57) 69.40 (0.38) 50.07 (0.00) 50.07 (0.00) 49.87 (0.27) 51.43 (1.79) 58.38 (0.63) 68.72 (0.87) 50.00 (0.09) 49.65 (0.27) 50.52 (0.49) 53.17 (0.52) 54.99 (0.62) 56.17 (0.47) 0.00 (0.00) 0.00 (0.00) 0.01 (0.01) 4.38 (2.42) 13.79 (3.53) 27.98 (10.00) 0.00 (0.00) 0.10 (0.05) 0.11 (0.11) 0.32 (0.07) 0.57 (0.29) 0.41 (0.08) 16.69 (0.00) 17.00 (0.17) 27.89 (5.17) 38.33 (4.13) 48.42 (1.27) 57.90 (3.27) 18.17 (0.24) 26.89 (3.86) 34.30 (0.11) 38.60 (0.32) 40.75 (0.19) 41.99 (0.13) Table 8: MLM vs. CLM downstream performance on QA datasets at various pretraining checkpoints. Results correspond to 610M models with MLM using 40% masking."
        },
        {
            "title": "Objective Training Step",
            "content": "NQ"
        },
        {
            "title": "CLM",
            "content": "1K 2K 5K 10K 20K 40K 1K 2K 5K 10K 20K 40K 8.41 (0.61) 21.64 (0.82) 50.47 (0.74) 69.32 (0.90) 82.50 (0.54) 85.46 (0.24) 22.24 (0.92) 45.12 (0.68) 70.67 (0.89) 78.47 (1.94) 83.80 (0.88) 84.88 (0.71) 43.97 (2.88) 54.88 (1.33) 78.14 (0.59) 85.35 (1.73) 89.60 (0.65) 91.50 (0.54) 54.20 (1.70) 72.93 (3.32) 85.19 (0.81) 88.23 (0.68) 89.49 (0.75) 90.52 (0.83) 8.20 (0.87) 15.15 (0.97) 34.89 (0.69) 44.30 (0.57) 56.36 (1.19) 60.83 (0.29) 13.39 (0.79) 25.26 (0.87) 39.79 (1.47) 46.14 (1.36) 53.38 (0.68) 52.76 (1.65) 20.19 (1.38) 30.55 (0.84) 54.50 (0.44) 66.33 (0.61) 76.16 (0.62) 79.26 (0.23) 29.94 (0.49) 47.77 (1.09) 65.22 (0.60) 70.95 (1.05) 75.56 (0.45) 76.05 (0.59) Table 9: MLM vs. CLM downstream performance on IR datasets at various pretraining checkpoints. Results correspond to 610M models with MLM using 40% masking. Total Steps CLM+MLM Mix SST-2 QQP MNLI Average 12K 22K 42K 0K+12K 3K+9K 6K+6K 9K+3K 12K+0K 0K+22K 5K+17K 11K+11K 17K+5K 22K+0K 0K+42K 10K+32K 21K+21K 32K+10K 42K+0K 89.33 (0.68) 90.09 (0.45) 90.02 (0.34) 89.77 (0.77) 89.66 (0.51) 91.63 (0.30) 91.28 (0.80) 91.90 (0.71) 90.94 (0.40) 90.80 (0.22) 91.42 (0.62) 92.41 (0.18) 92.36 (0.39) 92.16 (0.43) 91.10 (0.39) 84.62 (0.18) 84.59 (0.10) 84.70 (0.16) 83.93 (0.07) 82.54 (0.13) 86.30 (0.08) 86.42 (0.32) 86.69 (0.12) 85.86 (0.16) 83.86 (0.17) 86.98 (0.15) 87.32 (0.23) 87.11 (0.18) 87.14 (0.10) 84.14 (0.12) 75.17 (0.33) 76.32 (0.36) 76.15 (0.32) 73.25 (0.15) 70.40 (0.38) 80.27 (0.21) 81.24 (0.31) 80.39 (0.22) 79.07 (0.07) 74.26 (0.27) 82.59 (0.16) 83.83 (0.25) 83.28 (0.14) 81.90 (0.24) 75.49 (0.22) 83.04 (0.28) 83.67 (0.19) 83.63 (0.17) 82.32 (0.25) 80.86 (0.13) 86.07 (0.07) 86.31 (0.29) 86.33 (0.25) 85.29 (0.18) 82.97 (0.15) 87.00 (0.18) 87.85 (0.13) 87.58 (0.11) 87.06 (0.15) 83.58 (0.17) Table 10: Impact of the CLM-to-MLM pretraining steps ratio on downstream performance in the PFS setup for SC datasets. Results are reported for 610M models with MLM using 40% masking. Total Steps CLM+MLM Mix OntoNotes"
        },
        {
            "title": "Average",
            "content": "12K 22K 42K 0K+12K 3K+9K 6K+6K 9K+3K 12K+0K 0K+22K 5K+17K 11K+11K 17K+5K 22K+0K 0K+42K 10K+32K 21K+21K 32K+10K 42K+0K 92.32 (0.15) 92.64 (0.10) 92.69 (0.12) 92.79 (0.06) 92.29 (0.09) 92.83 (0.15) 93.09 (0.08) 93.15 (0.04) 93.13 (0.12) 92.73 (0.06) 92.97 (0.15) 93.35 (0.07) 93.21 (0.15) 93.32 (0.04) 92.79 (0.05) 90.81 (0.22) 91.63 (0.09) 91.62 (0.20) 91.83 (0.26) 91.67 (0.25) 91.62 (0.14) 92.15 (0.16) 92.42 (0.25) 92.46 (0.18) 92.22 (0.37) 91.58 (0.44) 92.39 (0.20) 92.43 (0.13) 92.41 (0.17) 92.16 (0.21) 91.59 (0.20) 92.20 (0.24) 92.15 (0.23) 92.96 (0.15) 92.47 (0.22) 92.39 (0.24) 93.08 (0.53) 92.93 (0.30) 92.86 (0.24) 92.94 (0.52) 92.07 (0.77) 93.45 (0.36) 93.02 (0.43) 93.02 (0.24) 93.12 (0.29) 91.58 (0.10) 92.16 (0.11) 92.15 (0.09) 92.53 (0.10) 92.14 (0.06) 92.28 (0.15) 92.77 (0.13) 92.83 (0.08) 92.82 (0.12) 92.63 (0.15) 92.21 (0.26) 93.07 (0.15) 92.89 (0.15) 92.92 (0.11) 92.69 (0.14) Table 11: Impact of the CLM-to-MLM pretraining steps ratio on downstream performance in the PFS setup for TC datasets. Results are reported for 610M models with MLM using 40% masking. Total Steps CLM+MLM Mix SQuAD SQuAD-v2 ReCoRD Average 12K 22K 42K 0K+12K 3K+9K 6K+6K 9K+3K 12K+0K 0K+22K 5K+17K 11K+11K 17K+5K 22K+0K 0K+42K 10K+32K 21K+21K 32K+10K 42K+0K 67.95 (0.65) 71.36 (0.63) 71.29 (0.27) 68.20 (0.83) 63.40 (0.72) 74.02 (0.38) 75.71 (0.93) 75.43 (0.21) 74.15 (0.46) 67.52 (0.40) 77.76 (0.29) 79.20 (0.22) 77.12 (0.28) 76.54 (0.31) 69.42 (0.30) 53.73 (2.04) 56.28 (0.44) 55.85 (0.89) 53.85 (1.09) 53.49 (0.40) 61.02 (0.92) 61.50 (2.71) 61.33 (0.73) 59.03 (1.53) 55.44 (0.66) 70.63 (0.58) 72.33 (0.81) 67.81 (0.66) 64.49 (0.82) 56.44 (0.10) 2.85 (1.06) 3.68 (1.81) 1.19 (0.40) 2.55 (4.07) 0.32 (0.11) 19.29 (5.45) 26.08 (6.25) 31.14 (6.92) 26.99 (3.43) 0.47 (0.14) 39.93 (5.26) 51.79 (1.50) 33.60 (11.35) 43.20 (1.29) 0.42 (0.39) 41.51 (0.55) 43.77 (0.63) 42.78 (0.37) 41.53 (1.81) 39.07 (0.34) 51.44 (1.81) 54.43 (1.51) 55.97 (2.43) 53.39 (1.31) 41.14 (0.25) 62.77 (1.86) 67.77 (0.53) 59.51 (3.84) 61.41 (0.66) 42.09 (0.20) Table 12: Impact of the CLM-to-MLM pretraining steps ratio on downstream performance in the PFS setup for QA datasets. Results are reported for 610M models with MLM using 40% masking. Total Steps CLM+MLM Mix NQ"
        },
        {
            "title": "Average",
            "content": "12K 22K 42K 0K+12K 3K+9K 6K+6K 9K+3K 12K+0K 0K+22K 5K+17K 11K+11K 17K+5K 22K+0K 0K+42K 10K+32K 21K+21K 32K+10K 42K+0K 74.15 (1.46) 80.08 (0.44) 80.67 (0.61) 80.10 (0.44) 80.16 (1.16) 83.72 (0.85) 84.07 (0.52) 84.48 (0.50) 83.36 (0.57) 84.01 (0.75) 86.76 (0.63) 86.79 (0.28) 86.80 (0.86) 86.82 (0.49) 85.57 (0.34) 85.95 (1.83) 89.63 (0.87) 88.30 (0.79) 89.23 (1.28) 88.88 (1.27) 90.42 (0.38) 89.38 (0.77) 89.72 (0.56) 90.97 (0.56) 90.36 (0.98) 91.65 (0.45) 91.98 (1.35) 90.98 (0.92) 91.40 (0.92) 89.88 (1.17) 49.59 (0.56) 55.09 (0.56) 56.70 (0.58) 55.63 (0.87) 49.26 (0.54) 58.47 (0.45) 58.42 (1.00) 58.22 (0.47) 59.12 (0.26) 52.75 (2.03) 60.25 (0.76) 60.86 (1.77) 61.22 (1.27) 61.14 (0.84) 53.16 (0.42) 69.90 (1.21) 74.94 (0.22) 75.22 (0.41) 74.99 (0.31) 72.77 (0.82) 77.54 (0.28) 77.29 (0.46) 77.47 (0.40) 77.81 (0.18) 75.71 (0.34) 79.55 (0.35) 79.88 (0.87) 79.67 (0.81) 79.79 (0.36) 76.20 (0.54) Table 13: Impact of the CLM-to-MLM pretraining steps ratio on downstream performance in the PFS setup for IR datasets. Results are reported for 610M models with MLM using 40% masking. PT Objective CPT Steps SST-2 QQP MNLI Average MLM CLM - 2K 12K 22K - 2K 12K 22K 91.42 (0.62) 91.54 (0.54) 92.71 (0.37) 92.16 (0.59) 91.10 (0.39) 92.06 (0.48) 92.98 (0.34) 93.62 (0.33) 86.98 (0.15) 86.84 (0.16) 86.89 (0.31) 86.94 (0.10) 84.14 (0.12) 85.93 (0.15) 87.48 (0.13) 87.56 (0.08) 82.59 (0.16) 82.85 (0.27) 83.23 (0.29) 83.31 (0.35) 75.49 (0.22) 78.18 (0.23) 83.05 (0.18) 84.02 (0.10) 87.00 (0.18) 87.08 (0.20) 87.61 (0.21) 87.47 (0.24) 83.58 (0.17) 85.39 (0.17) 87.84 (0.15) 88.40 (0.11) Table 14: Impact of continued MLM pretraining on both MLMand CLM-pretrained models across different sequence lengths on SC datasets. Results are reported for 610M models with 40% masking for MLM. PT Objective CPT Steps OntoNotes CoNLL UNER Average MLM CLM - 2K 12K 22K - 2K 12K 22K 92.97 (0.15) 92.85 (0.26) 93.02 (0.17) 92.82 (0.45) 92.79 (0.05) 92.91 (0.07) 92.78 (0.56) 92.85 (0.13) 91.58 (0.44) 90.89 (1.10) 91.19 (0.82) 91.41 (0.16) 92.16 (0.21) 92.71 (0.26) 92.53 (0.13) 92.30 (0.19) 92.07 (0.77) 92.74 (0.39) 92.59 (0.51) 92.10 (0.57) 93.12 (0.29) 93.02 (0.26) 93.01 (0.29) 92.94 (0.11) 92.21 (0.26) 92.16 (0.41) 92.27 (0.41) 92.11 (0.32) 92.69 (0.14) 92.88 (0.13) 92.77 (0.12) 92.70 (0.05) Table 15: Impact of continued MLM pretraining on both MLMand CLM-pretrained models across different sequence lengths on TC datasets. Results are reported for 610M models with 40% masking for MLM."
        },
        {
            "title": "PT Objective CPT Steps",
            "content": "SQuAD SQuAD-v2 ReCoRD"
        },
        {
            "title": "CLM",
            "content": "- 2K 12K 22K - 2K 12K 22K 77.76 (0.29) 77.92 (0.21) 78.71 (0.23) 79.01 (0.18) 69.42 (0.30) 73.48 (0.28) 77.25 (0.29) 78.13 (0.27) 70.63 (0.58) 71.07 (0.85) 72.35 (0.67) 72.77 (0.47) 56.44 (0.10) 59.13 (0.21) 65.62 (1.15) 69.69 (0.56) 39.93 (5.26) 41.69 (4.59) 48.66 (2.17) 51.21 (1.32) 0.42 (0.39) 10.94 (5.90) 46.75 (1.38) 52.06 (0.72) 62.77 (1.86) 63.56 (1.77) 66.57 (0.85) 67.66 (0.53) 42.09 (0.20) 47.85 (2.04) 63.21 (0.17) 66.62 (0.34) Table 16: Impact of continued MLM pretraining on both MLMand CLM-pretrained models across different sequence lengths on QA datasets. Results are reported for 610M models with 40% masking for MLM. PT Objective CPT Steps NQ MS MARCO MLDR Average MLM CLM - 2K 12K 22K - 2K 12K 22K 86.76 (0.63) 86.77 (0.84) 87.01 (0.52) 87.58 (0.62) 85.57 (0.34) 86.46 (0.38) 88.29 (0.26) 87.67 (0.93) 91.65 (0.45) 92.18 (0.63) 92.32 (0.92) 92.52 (0.79) 89.88 (1.17) 88.80 (0.76) 89.56 (0.92) 90.71 (0.23) 60.25 (0.76) 62.31 (0.42) 60.44 (1.02) 61.26 (0.53) 53.16 (0.42) 60.45 (0.43) 62.97 (0.52) 63.72 (0.41) 79.55 (0.35) 80.42 (0.27) 79.92 (0.47) 80.45 (0.27) 76.20 (0.54) 78.57 (0.22) 80.27 (0.29) 80.70 (0.26) Table 17: Impact of continued MLM pretraining on both MLMand CLM-pretrained models across different sequence lengths on IR datasets. Results are reported for 610M models with 40% masking for MLM."
        }
    ],
    "affiliations": [
        "Artefact Research Center",
        "Diabolocom",
        "Equall",
        "Illuin Technology",
        "Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit)",
        "Instituto de Telecomunicações",
        "MICS, CentraleSupélec, Université Paris-Saclay",
        "Unbabel"
    ]
}