{
    "paper_title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images",
    "authors": [
        "Lingen Li",
        "Zhaoyang Zhang",
        "Yaowei Li",
        "Jiale Xu",
        "Xiaoyu Li",
        "Wenbo Hu",
        "Weihao Cheng",
        "Jinwei Gu",
        "Tianfan Xue",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 7 1 5 3 0 . 2 1 4 2 : r NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images Lingen Li1, Zhaoyang Zhang2* Yaowei Li2,3 Weihao Cheng2 Jinwei Gu1 Jiale Xu2 Wenbo Hu2 Xiaoyu Li2 Tianfan Xue1 Ying Shan2 1The Chinese University of Hong Kong 2ARC Lab, Tencent PCG 3Peking University Figure 1. As the number of unposed input views increases, NVComposer (blue circle) effectively uses the extra information to improve NVS quality. In contrast, ViewCrafter [43] (green triangle), which relies on external multi-view alignment (via pre-reconstruction from DUSt3R [34]), suffers performance degradation as the number of views grows due to instability of the external alignment. This result contradicts the common expectation that more views lead to better performance. Please refer to Sec. 4.2 for full results."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multiview alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dualstream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) geometryaware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves stateof-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems. *Project Lead. With recent advances in generative models, generative novel view synthesis (NVS) methods have drawn considerable attention [7, 20, 21, 27, 39, 43] due to its ability to synthesize novel views with only one or few images. Unlike reconstruction-based NVS methods, where dense-view images with full coverage of the scene are required [15, 16, 22, 34], generative NVS methods could take only one or few views as inputs, completing unseen parts of scene with plausible content [25, 42]. This capability is particularly useful in applications where capturing extensive views is impractical, offering greater flexibility and efficiency for virtual scene exploration and content creation. In addition to generate novel views from single input image, generative NVS methods [7, 19, 43] have demonstrated more flexible utility by reducing ambiguity through giving additional input images [41]. To leverage multiview images in the generative NVS tasks, existing methods [7, 39, 43] all rely on external multi-view alignment processes before generation. For example, assuming accurate poses of condition images are given (through explicit pose estimation) [7] or generate novel views conditioned on results extracted from reconstructive NVS methods (through pre-reconstruction) [19, 39, 43]. However, in the case of the overlap region being small and hard to do Figure 2. Framework illustration of NVComposer. It contains an image-pose dual-stream diffusion model that generates novel views while implicitly estimating camera poses for conditional images, and geometry-aware feature alignment adapter that uses geometric priors distilled from pretrained dense stereo models [34]. stereo matching, external multi-view alignment processes like camera pose estimation becomes unreliable [41]. As result, multi-view generative NVS methods which heavily rely on the external alignment also tend to fail as shown in Fig. 1. To overcome this limitation, we explore the possibility of removing the dependency on the external alignment process and propose Novel View Composer (NVComposer) which could generate novel views from spare unposed images without relying on any external alignment process. Our method is able to generate reasonable results in the sparse views with small overlap and large occlusion. Firstly, to leverage the powerful generation ability of the video diffusion model, we use pre-trained video diffusion model as the backbone of our NVComposer with unposed images as the condition in synthesis. Without external alignment of these images, we introduce novel dualstream diffusion model in NVComposer to learn the relative poses of condition images during generation. The dualstream diffusion model not only generates novel views but also implicitly predicts the correct pose relationships between the condition images, ensuring that the model understands the relative positioning of the condition images in the scene and uses them to synthesize novel views correctly. Moreover, to generate more view-consistent results, we employ features produced by pretrained dense stereo model [34] to train our model with geometry awareness. Unlike previous methods [19, 43] that directly rely on the reconstruction results from dense stereo model as input, we propose more flexible and accessible geometry-aware feature alignment adapter. This adapter aligns our models features with the predicted 3D features of the dense stereo model and requires no explicit reconstruction during inference. This strategy allows us to distill 3D knowledge implicitly from the dense stereo model [34]. Experiments (Sec. 4.2) demonstrate that this implicit geometry-aware learning achieves competitive performance compared to explicit reconstruction-relied methods [34, 43]. It provides enhanced accessibility and flexibility, as it operates in an end-to-end manner and eliminates the need for an extra step of explicit pose estimation or pre-reconstruction during the inference. To train our model, we construct mixed dataset from different sources such as video [18, 24, 47] data and 3D [4] data with real indoor and outdoor scenes as well as synthetic 3D objects. NVComposer is trained on this diverse dataset using as few as one to four randomly sampled unposed condition images. Extensive experiments demonstrate that, when provided with multiple unposed input views, NVComposer outperforms state-of-the-art controllable video diffusion models and generative NVS models. Our key contributions are summarized as follows: We introduce the first pose-free multi-view generative NVS model for both scenes and objects, without the requirement for explicit multi-view alignment processes on input images. Our proposed design which includes image-pose dualstream diffusion and geometry-aware feature alignment adapter, highlights promising direction for creating more flexible and accessible generative NVS systems. Our NVComposer achieves state-of-the-art performance on generative NVS tasks for both scenes and objects when given multiple unposed input views. 2. Related Work Single-View Generative Novel View Synthesis. Early NVS methods using feed-forward networks map single input image to new views [30, 33], but are limited to small rotations and translations due to the restricted information from one input image. Generative NVS method effectively hallucinate unseen views given limited input [3, 25, 37]. Recent advances in diffusion models [11, 29] leverage rich image priors for NVS to synthesize more reasonable multiview content [14, 20, 21, 32] by utilizing pretrained image diffusion models [26]. Zero-1-to-3[20] fine-tunes latent diffusion model [26] with image pairs and their relative poses for novel view synthesis from single image. Wonder3D[21] incorporates image-normal joint training and view-wise attention to enhance generative quality. SV3D [32] fine-tunes video diffusion model [1] for NVS of synthetic objects. However, single-view NVS models struggle to infer occluded or missing details due to the limited information from one viewpoint, making them less practical for realworld applications requiring complete scene understanding. Multi-View Generative Novel View Synthesis. To overcome single-view limitations, multi-view conditioned generative NVS utilizes images from multiple viewpoints [7, 19, 38, 43], enhancing the fidelity of generated views by capturing finer details and accurate spatial relationships. iFusion [38] employs pretrained Zero-1-to-3 [20] as an inverse pose estimator and tunes LoRA [13] adapter for each object to support multi-view NVS. CAT3D [7] uses Plucker ray embeddings [28] as pose representations and masks the target view for inpainting, allowing flexibility in the number of conditioning images. ViewCrafter [43] reconstructs an initial point cloud using dense stereo model [34] and then employs video diffusion model to inpaint missing regions in rendered novel views. These methods, however, rely on accurate pre-computed poses of conditional images. Sparse views that lead to inaccurate poses can significantly degrade the quality of generated views, limiting their robustness in practical scenarios. Video Diffusion Models. Advancements in diffusion models have extended their capabilities from static images to dynamic videos, enabling temporally coherent video generation conditioned on various inputs [1, 2, 6, 9, 12, 36, 40]. Ho et al. [12] first introduced diffusion models for video generation. Video LDM [2] operates in the latent space [26] to reduce computational demands. Subsequent works enhance controllability by incorporating additional conditions. AnimateDiff [8] extends text-to-image diffusion models to video by attaching motion modules while keeping the original model frozen. DynamiCrafter [40] introduces an image adapter for image-conditioned video generation. MotionCtrl [36] and CameraCtrl [9] incorporate camera trajectory control using pose matrices and Plucker embeddings, respectively. ReCapture [44] generates new camera trajectory views based on given video. Building upon the video diffusion model, our method leverages temporal coherence to synthesize unseen areas not included in the input images. Compared to previous controllable video diffusion models, our approach achieves better accuracy in camera controllability, offering robust performance for generative NVS tasks. 3. Methodology The objective of NVComposer is to develop model capable of generating novel views at specified target camera poses, using multiple unposed conditional images without requiring external multi-view alignment (e.g., explicit pose estimation). To achieve this, we propose to enable the model itself to infer the spatial relationships of the conditional views during generation. We introduce this capability through two key strategies: (1) instead of explicitly solving for camera poses, we model pose estimation as generative task that jointly happens with the image generation, and (2) we distill effective geometric knowledge from expert models into our generative model. This leads to two main components of our NVComposer, as shown in Fig. 2: an image-pose dual-stream diffusion model that generates novel target views while implicitly estimating camera poses for conditional images, and geometry-aware feature alignment adapter that uses geometric priors distilled from pretrained dense stereo models [34]. The design and implementation of these components are detailed below. 3.1. Image-Pose Dual-Stream Diffusion Assume the model accepts elements as input and produces elements as output, where each element corresponds to an image captured within the current scene, accompanied by its pose annotation. We refer to these elements as image-pose bundles. We partition these bundles into two segments: the first bundles constitute the target segment, and the remaining bundles form the condition segment, as illustrated in Fig. 2. Image-Pose Bundles. Specifically, let It R3HW denote the t-th RGB image, and Pt R6HW be the corresponding Plucker ray embedding [28] representing the camera pose. The conditional input and generated output are sequences of image-pose bundles for specific scene, denoted as = {[I t=1, where the t-th image-pose bundle consists of the concatenation (along the channel dimension) of the latent image 8 and R6 the resized Plucker ray 8 . Here, [] denotes concatenation and represents the VAE encoder of latent diffusion. = E(It) R4 t, ]}T 8 8 The main difference between the target output and the conditional input Bc is that is complete, while Bc is partially masked. Specifically, for Bc, image latents in the target segment (i.e., the first elements) and pose embeddings in the condition segment are set to zero. Additionally, we utilize relative camera poses in our model and designate the first elements of both the target and condition segments as the anchor view for this relative coordinate system. This implies that these two elements are expected to have the 3 same image content, and their camera extrinsic matrices are identity transformations. The image for the anchor view is always provided during training and inference, corresponding to the scenario where at least one conditional image is available. With this design, our image-pose dual-stream diffusion model accepts unposed conditional images and target poses as input, and output novel view images at target poses along with predicted poses for conditional images, where N, 1 and + = . Video Prior. Video priors from video diffusion model has been validated to be useful for generative NVS tasks [7, 19, 43]. To fully leverage the generative priors in NVComposer, we initialize the dual-stream diffusion model using the pretrained weights of the video diffusion model DynamiCrafter [40]. We omit the frame rate and text conditions from the original video diffusion model, focusing solely on the relevant components for our task. We retain the image CLIP [23] feature conditioning with its Q-Formerlike [17] image adapter in the cross-attention layers, using the anchor view as the conditioning image. Additionally, we enhance the models capacity to capture cross-view correspondences at different spatial locations by adding extra spatio-temporal self-attention layers after each Res-Block of original video diffusion model. Although camera pose information is provided in Bc as Plucker ray embeddings, we further incorporate the sequence of target camera extrinsics embedding encoded by learnable multilayer perceptron layer from the corresponding 3 4 camera-to-world matrices Rc RT 34 of all image-pose bundles, where the last elements on the temporal dimension are masked with zeros like what we have done with Bc. These embeddings are added to the time-step embedding and serve as supplemental signals indicating the poses for each imagepose bundle in the target segment. Pose Decoding Head Separation. We observed that training the model to generate image-pose bundles directly is hard to converge. This is caused by the difference of the two modality: images latents contain complex latent features representing diverse content, while the pose embeddings are dominated by low-frequency component. This disparity can lead to interference when jointly denoising [I ] using tightly coupled network layers. t, To address this issue, we design an additional decoding head specifically for denoising . As shown in Fig. 2, this pose decoding head operates in parallel with the original decoding part of the diffusion denoising U-Net and follows fully convolutional architecture similar to the original decoder. It takes as input the bottleneck features, residual connections from the encoding part, and the denoising time-step embeddings. Since the Plucker ray embeddings of poses are predominantly low-frequency and relatively Figure 3. Structure of the geometry-aware feature alignment adapter in NVComposer, which aligns the internal features of the dual-stream diffusion models with the 3D point maps produces by DUSt3R [34] during training. Block with notation 2, 4, and 8 refer to bilinear upsampling on spatial dimensions. The four red bars refer to the channel-wise MLPs. straightforward to denoise, we empirically reduce the base channel number of the pose decoding head to one-tenth of that of the original decoder for images and remove its attention layers. The outputs of the pose decoding head are concatenated along the channel dimension with those of the original decoding part of the diffusion U-Net to form the final output. 3.2. Geometry-aware Feature Alignment Since the dual-stream diffusion model is initialized from video diffusion model that is not inherently trained with geometric constraints across views, we introduce geometryaware feature alignment mechanism in NVComposer. This mechanism distills effective geometric knowledge from an external model with strong geometry priors during training. Specifically, we leverage the dense stereo model DUSt3R [34], which performs well with dense views (both to compute pointmaps target and condition images), across all views (Tt, for = 1, 2, . . . , ) relative to the anchor view T1. We then align the internal features of our diffusion model with these point maps through geometry-aware feature alignment adapter during training. Specifically, as illustrated in Fig. 3, the alignment adapter (the red block on the left) takes features from the encoding part of the dualstream diffusion U-Net, immediately after each spatialtemporal self-attention layer. These features are resized to match the spatial dimensions of the image latent inputs, 8 8 (the white squares in Fig. 3). The resized features retain their temporal dimension of length , and all operations within the geometry-aware feature alignment adapter are temporally independent. The features are then processed by channel-wise MLPs (four red bars in Fig. 3) to reduce them to 320 channels, followed by convolutional residual block that outputs 4-D tensor RT 6 8 8 . For all item ft, = 1, 2, .., in along the temporal dimension, we minimize the mean squared error (MSE) with the concatenated point maps produced by DUSt3R [34] 4 given the t-th view It and the anchor view I1: size of 128. For more details, please refer to the supplementary material. Lalign ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 ft D(I1, It)2 2. (1) 4.2. Results 3.3. Training Objectives We train the image-pose dual-stream diffusion model hθ in NVComposer to predict noises ϵ given the uniformly sampled denoising time step k, the noisy version of complete image-pose bundles B{k} at time step k, the conditional image-pose bundles Scond, and CLIP [23] image feature of the anchor view Φ(I1): Ldiff = EB,ϵ,Bc,Φ(I0),Rc,k[ϵhθ(B{k}, Bc, Φ(I1), Rc, k)], (2) where θ is trainable parameters of the image-pose dualstream diffusion model, Φ is the CLIP [23] image feature encoder. Our total loss combines the diffusion loss Ldiff and the feature alignment loss Lalign: Ltotal = Ldiff + λLalign, (3) where λ is loss re-weighting factor. 4. Experiments In this section, we evaluate the performance of NVComposer on generative NVS tasks for real-world scenes and synthetic 3D objects, followed by an analysis of the models sub-components. More results are in the supplementary. 4.1. Training Details We train our model on large-scale mixed dataset built from Objaverse [4], RealEstate10K [47], CO3D [24], and DL3DV [18]. The sequence length of image-pose bundles is set to 16. To get samples from video datasets (RealEstate10K [47], CO3D [24], and DL3DV [18]), we randomly select frame interval between 1 and Tmax/N to sample consecutive frames, where Tmax is the total number of frames in the scene. The value of Tmax ranges from 100 to 400 depending on the data sample. We randomly sample condition views and shuffle them within the image-pose bundle. For samples from the 3D dataset (Objaverse [4]), we render each 3D object in two versions: one with 36 orbit views and another with 32 random views. Target views are sampled from the orbit renderings, and condition views are sampled from the random renderings, following the procedure described above. We firstly train the model at resolution of 512 512 for 10, 000 steps across all datasets. Next, we tune the model on RealEstate10K and DL3DV at resolution of 5761024 for 20, 000 steps for higher resolution support. We use learning rate of 1 105 and perform all training on cluster with 64 NVIDIA V100 GPUs, with an effective batch 5 4.2.1. Generative NVS in Scenes Benchmark Settings. We evaluate the performance of NVComposer on generative NVS tasks for scenes, comparing it with four state-of-the-art models: MotionCtrl [36], CameraCtrl [9], DUSt3R [34], and ViewCrafter [43]. MotionCtrl and CameraCtrl are controllable video generation models that work with single input image, while DUSt3R is dense stereo model for multi-view reconstructive NVS, and ViewCrafter is multi-view generative NVS method that relies on explicit pose estimation and point cloud guidance. For the RealEstate10K [47] dataset, we categorize scenes into three difficulty levels: easy, medium, and hard. The difficulty is based on the angular distances between views, specifically the rotation angle between the anchor view and the furthest target view (θtarget), and between the anchor and the furthest condition view (θcond), when more than one condition image is used. Samples are classified as follows: (1) Easy: θcond < 10 and θtarget < 10; (2) Medium: 10 θcond < 30 and 10 θtarget < 30; (3) Hard: 60 θcond < 120 and 30 θtarget < 60. We then randomly select 20 samples from the easy set, 60 from the medium set, and 20 from the hard set for evaluation. For the DL3DV [18] dataset, we randomly select 20 test scenes. Results. We measure performance by comparing generated novel views to reference images using several metrics: peak signal-to-noise ratio (PSNR), structural similarity index (SSIM)[35], and perceptual distance metrics including LPIPS[46] and DISTS [5]. Tab. 1 shows the numerical results on RealEstate10K and Tab. 2 shows the results on the DL3DV test set. As seen, NVComposer outperforms other methods across both datasets. Fig. 4 further demonstrates the visualized comparison among all these methods. For MotionCtrl [36] and CameraCtrl [9], pose controllability is limited. When the target camera poses involve large rotations or translations, these models generate sequences with minimal motion, failing to accurately follow the given instructions. These visual results align with the poor numerical performance observed in Tab. 1. It it noteworthy that, when there are more given input views, the performance of our method consistently increases, as we also showed in Fig. 1 before. In contrast, ViewCrafter [43] suffers from performance drop when the number of given views increases from one to two in the hard set. This is because the two conditional views in the hard set has large rotation difference, i.e., small overlapping region and possibly some occlusion are there between the two given views. This makes the external alignment process (explicit pose estimation and pre-reconstruction) tends to proModel Views Easy (θcond < 10 and θtarget < 10) Medium (10 θcond < 30 and 10 θtarget < 30) Hard (60 θcond < 120 and 30 θtarget < 60) MotionCtrl [36] CameraCtrl [9] DUSt3R [34] ViewCrafter [43] NVComposer (Ours) PSNR SSIM LPIPS DISTS PSNR SSIM LPIPS DISTS PSNR SSIM LPIPS DISTS 15.0741 0.6071 0.3616 0.0999 12. 0.5667 0.5439 0.1584 11.6381 0.5276 0. 0.1633 13.6082 0.5050 0.4234 0.1458 11. 0.4934 0.5217 0.1957 11.7599 0.4716 0. 0.2021 13.9443 17.4837 17.2341 17.3545 17.3750 18.8906 18.4531 18.4844 18.7227 20.7395 21.5278 22.5519 0.5582 0.6148 0.6097 0.6193 0.6670 0.6685 0.6548 0. 0.7215 0.7681 0.7981 0.8226 0.3914 0.3582 0.3585 0.3541 0.2849 0.3079 0.3024 0.3068 0.2354 0.1781 0.1522 0.1188 0.1565 0.1503 0.1504 0.1481 0.1221 0.1334 0.1294 0. 0.0996 0.0793 0.0716 0.0537 11.4854 13.3077 13.2212 14.6845 13.6015 14.2891 14.1172 14.7421 15.3101 16.9100 17.7071 19.5346 0.4520 0.4886 0.4978 0.5534 0.6016 0.5947 0.5913 0. 0.6056 0.6445 0.7418 0.7847 0.5570 0.5434 0.5287 0.4892 0.4315 0.4478 0.4401 0.4230 0.3445 0.2742 0.2759 0.2030 0.2294 0.2126 0.2056 0.1870 0.1762 0.1761 0.1717 0. 0.1516 0.1198 0.1097 0.0851 10.9003 11.5381 11.9211 14.2381 14.0781 13.5859 13.7031 15.1875 15.2115 15.3461 15.3825 17.8181 0.4029 0.4003 0.4387 0.5280 0.5894 0.5537 0.5620 0. 0.6408 0.6638 0.6822 0.7359 0.6089 0.6407 0.5942 0.5295 0.4293 0.5100 0.4867 0.4327 0.4048 0.3789 0.3699 0.2644 0.2495 0.2551 0.2313 0.1917 0.1676 0.1925 0.1784 0. 0.1462 0.1384 0.1324 0.0988 1 1 1 2 3 4 1 2 3 4 1 2 3 Table 1. NVS evaluation with varying numbers of input views on RealEstate10K [47] for controllable video models MotionCtrl [36] and CameraCtrl [9], reconstructive model DUSt3R [34], and generative models ViewCrafter [43] and NVComposer. θtarget denotes the rotation angle between the anchor view and the furthest target view, while θcond indicates the angle between the anchor view and the furthest conditional view (when multiple conditions are used)."
        },
        {
            "title": "Views",
            "content": "PSNR SSIM LPIPS DISTS"
        },
        {
            "title": "Method",
            "content": "FID FVD KVD MotionCtrl [36] CameraCtrl [9] DUSt3R [34] ViewCrafter [43] NVComposer (Ours) 1 1 1 2 3 4 1 2 3 4 1 2 3 13.4003 0.5539 0.4004 0.1396 12.2995 0. 0.4337 0.1829 11.7650 14.6660 13.9156 14.8716 15.5625 15.6875 14.8593 15.0625 15.3101 16.9100 17.3115 17.9248 0.4652 0.5158 0.5010 0. 0.4932 0.4775 0.4670 0.4712 0.6056 0.6445 0.6687 0.6958 0.4900 0.4531 0.4699 0.4478 0.4122 0.4417 0.4617 0.4549 0.3445 0.2742 0.2558 0.2277 0.2295 0.2104 0.2127 0. 0.2125 0.2212 0.2273 0.2301 0.1516 0.1198 0.1122 0.1023 Table 2. NVS evaluation on DL3DV [18]. When more unposed input views are provided, our model consistently reports higher performance. duce unstable results, thus leading to poor generative NVS performance. Distribution Evaluation. In addition to evaluating perview NVS performance, we assess the distribution of generated novel view sequences using several metrics: Frechet Inception Distance (FID) [10], Frechet Video Distance (FVD) [31], and Kernel Video Distance (KVD) [31]. For FID, we treat the novel views as individual images, while for FVD and KVD, we treat them as video clips. We compute these metrics for each model using 1,000 ground truth sequences. To ensure fairness, we report results based on single input view conditions. Results are shown in Tab. 3. Our method achieves comparable FID to ViewCrafter, but outperforms it in both FVD and KVD. This suggests that MotionCtrl [36] CameraCtrl [9] ViewCrafter [43] NVComposer (Ours) 60.83 52.33 46.08 46.19 509.96 561.97 485.11 425.44 14.26 24.38 13.06 8.04 Table 3. Distribution evaluation on generated views of MotionCtrl [36], CameraCtrl [9], ViewCrafter [43], and our NVComposer using FID [10], FVD [31], and KVD [31] metrics. our method produces more accurate novel views when considering the entire multi-view sequence. Overall, our model generates results that are closer to the ground truth, both in terms of image and video generation. 4.2.2. Generative NVS in Objects In addition to scenes, another important scenario involves generating novel views for synthetic 3D objects. To evaluate the versatility of our proposed pipeline, we compare the generative NVS performance of our model with the objectbased generative model SV3D [32] on the Objaverse test set. The numerical and visual results of this comparison are presented in Tab. 4 and Fig. 5. Our model achieves better PSNR and comparable SSIM with SV3D when only single conditional view is provided. Furthermore, as more unposed input views are added, our model effectively leverages the additional information, producing results that are closer to the real reference. 4.3. Analysis In this section, we perform several ablation studies and analyses to validate the effectiveness of our model. 6 Figure 4. Visual comparison of NVS results on the RealEstate10K [47] and DL3DV [18] test sets. MotionCtrl [36] and CameraCtrl [9] uses the first view as input while other methods use two views as input. MotionCtrl and CameraCtrl produce incorrect camera trajectories. DUSt3R and ViewCrafter exhibit better camera control but introduce artifacts due to occlusions or misaligned multi-view inputs. Our model generates views that are visually closer to the reference. We provide zoomed-in details of the first three scenes in white boxes for closer look. Additional visual comparisons can be found in the supplementary material."
        },
        {
            "title": "Views",
            "content": "PSNR SSIM LPIPS SV3D [32] NVComposer (Ours) 1 1 2 13.8861 0.8130 0.2731 16.3764 17.1507 17.7234 0.8218 0.8268 0.8352 0.2286 0.2067 0. Table 4. Generative NVS results on the Objaverse [4] test set. When only single conditional view is provided, NVComposer achieves performance comparable to SV3D [32]. As more random unposed condition views are added, NVComposer performance improves significantly. Ablation on Image-Pose Dual Stream Diffusion. To ensure both fairness and feasibility, we train two models with and without the dual-stream diffusion design on subset of Objaverse [4] containing 5, 000 objects for one epoch from the initial weight of the video diffusion model and evaluate them on test set with 100 objects. The results shown in Tab. 5 demonstrate that the dual-stream diffusion significantly improves the models performance on generaFigure 5. Visual comparison of novel view generation results on the Objaverse [4] test set. All input views are unposed and randomly rendered from the same 3D object. tive NVS tasks with unposed multiple condition views. Ablation on Geometry-Aware Feature Alignment. We further conduct an ablation study on the geometry-aware Dual-Stream PSNR SSIM LPIPS w/ w/o 17.0510 14.6857 0.7501 0.7458 0.1353 0. Table 5. Ablation experiments on dual-stream diffusion on Objaverse [4]. We train the two models (initialized from the same checkpoint) for one epoch on small subset of Objaverse. The model without dual-stream only generates images instead of the image-pose bundles."
        },
        {
            "title": "Alignment",
            "content": "PSNR SSIM LPIPS DISTS w/o w/ 14.7218 15.6568 0.6291 0.6440 0.3799 0. 0.1494 0.1340 Table 6. Ablation experiments on the geometry-aware feature alignment (Alignment in table). We initialize two models with and without the alignment mechanism from same checkpoint, and train the two models for an epoch, then evaluate them on RealEstate10K [47]. feature alignment mechanism using the RealEstate10K dataset [47]. In this experiment, we train two models from the same initial checkpoint for one epoch, with and without geometry-aware feature alignment. Tab. 6 demonstrate the numerical results and Fig. 6 shows the visualized results of this ablation study. We can clearly tell that this feature alignment mechanism helps our model learn the generative NVS task with unposed multiple conditional views. Sparse-View Pose Estimation Thanks to the unique design of the dual-stream diffusion, NVComposer can implicitly estimate the pose information. We follow the method [45] to solve the camera poses from the Plucker rays generated by the dual-stream diffusion of NVComposer. We perform the evaluation on the RealEstate10K dataset, asking the model to estimate the two sparse condition images given in the easy and hard subsets we discussed in Tab. 1. The accuracy of estimated poses is quantitatively evaluated in the average degrees of rotation angle differences and the average translation difference (with normalization according to the 2-norm of the translation of the furthest view). The results are given in Tab. 7, where we can see that our method is comparable to the performance of DUSt3R [34] in the easy case and outperforms DUSt3R in the hard case. This is because DUSt3R estimates poses using explicit deep feature correspondences, while NVComposer implicitly generates pose information during novel view generation. When given sparse condition views with minimal overlap (i.e., in ill-posed cases), our methods implicit pose estimation proves more robust, delivering accurate pose estimates directly corresponding to the current scene in novel view generation. Figure 6. visual sample in the ablation results of the geometryaware feature alignment with two input views given. Some patches are zoomed in for better view. The feature alignment helps NVComposer to properly utilize contents from other views."
        },
        {
            "title": "Method",
            "content": "ˆR ˆT"
        },
        {
            "title": "Hard",
            "content": "DUSt3R [34] NVComposer (Ours) 9.6968 2.7225 DUSt3R [34] NVComposer (Ours) 58.3987 5.8566 0.5757 0.0257 0.7603 0. Table 7. Comparison with pose estimation accuracy on two spare condition images in our RealEstate10K [47] test sets. Our NVComposer implicitly predicts camera poses by generating ray embeddings of condition views while generating target views. 5. Conclusion We presented NVComposer, novel multi-view generative NVS model that eliminates the need for external multi-view alignment, such as explicit camera pose estimation or prereconstruction of conditional images. By introducing an image-pose dual-stream diffusion model and geometryaware feature alignment module, NVComposer is able to effectively synthesize novel views from sparse and unposed condition images. Our extensive experiments demonstrate that NVComposer outperforms state-of-the-art methods that rely on external alignment processes. Notably, we show that the models performance improves as the number of unposed conditional images increases, highlighting its ability to implicitly infer spatial relationships and leverage available information from unposed views. This paves the way for more flexible, scalable, and robust generative NVS systems that do not depend on external alignment processes."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [3] Eric Chan, Koki Nagano, Matthew Chan, Alexander Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42174229, 2023. 2 [4] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 5, 7, 8 [5] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture IEEE transactions on pattern analysis and masimilarity. chine intelligence, 44(5):25672581, 2020. 5 [6] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73467356, 2023. 3 [7] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything arXiv preprint in 3d with multi-view diffusion models. arXiv:2405.10314, 2024. 1, 3, 4 [8] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [9] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3, 5, 6, 7 [10] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [13] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [14] Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, and Igor Gilitschenski. Spad: Spatially aware multi-view diffusers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1002610038, 2024. 3 [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1 [16] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. GroundarXiv preprint ing image matching in 3d with mast3r. arXiv:2406.09756, 2024. 1 [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 4 [18] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 2, 5, 6, 7 [19] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. 1, 2, 3, 4 [20] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 1, 3 [21] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. 1, [22] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1 [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4, 5 9 [24] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation In Proceedings of of real-life 3d category reconstruction. the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. 2, 5 [25] Chris Rockwell, David Fouhey, and Justin Johnson. Pixelsynth: Generating 3d-consistent experience from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1410414113, 2021. 1, 2 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [27] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 1 [28] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34: 1931319325, 2021. 3 [29] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 and Stefano Ermon. arXiv preprint [30] Richard Tucker and Noah Snavely. Single-view view synthesis with multiplane images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 551560, 2020. [31] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [32] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2025. 3, 6, 7 [33] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. IbrIn Pronet: Learning multi-view image-based rendering. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2021. 2 [34] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 1, 2, 3, 4, 5, 6, 8 [35] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 5 10 [36] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3, 5, 6, [37] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 74677477, 2020. 2 [38] Chin-Hsuan Wu, Yen-Chun Chen, Bolivar Solarte, Lu Yuan, Inverting diffusion for posearXiv preprint and Min Sun. free reconstruction from sparse views. arXiv:2312.17250, 2023. 3 ifusion: [39] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: In Proceedings of 3d reconstruction with diffusion priors. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2155121561, 2024. 1 [40] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2025. 3, [41] Chao Xu, Ang Li, Linghao Chen, Yulin Liu, Ruoxi Shi, Hao Su, and Minghua Liu. Sparp: Fast 3d object reconstruction and pose estimation from sparse views. In European Conference on Computer Vision, pages 143163. Springer, 2025. 1, 2 [42] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45784587, 2021. 1 [43] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 1, 2, 3, 4, 5, 6 [44] David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David E. Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for user-provided arXiv preprint videos using masked video fine-tuning. arXiv:2411.05003, 2024. 3 [45] Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. arXiv preprint arXiv:2402.14817, 2024. 8 [46] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [47] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. ACM Trans. Graph, 37, 2018. 2, 5, 6, 7,"
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Peking University",
        "The Chinese University of Hong Kong"
    ]
}