{
    "paper_title": "Multiview Equivariance Improves 3D Correspondence Understanding with Minimal Feature Finetuning",
    "authors": [
        "Yang You",
        "Yixin Li",
        "Congyue Deng",
        "Yue Wang",
        "Leonidas Guibas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision foundation models, particularly the ViT family, have revolutionized image understanding by providing rich semantic features. However, despite their success in 2D comprehension, their abilities on grasping 3D spatial relationships are still unclear. In this work, we evaluate and enhance the 3D awareness of ViT-based models. We begin by systematically assessing their ability to learn 3D equivariant features, specifically examining the consistency of semantic embeddings across different viewpoints. Our findings indicate that improved 3D equivariance leads to better performance on various downstream tasks, including pose estimation, tracking, and semantic transfer. Building on this insight, we propose a simple yet effective finetuning strategy based on 3D correspondences, which significantly enhances the 3D correspondence understanding of existing vision models. Remarkably, even finetuning on a single object for just one iteration results in substantial performance gains. All code and resources will be made publicly available to support further advancements in 3D-aware vision models. Our code is available at https://github.com/qq456cvb/3DCorrEnhance."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 8 5 4 9 1 . 1 1 4 2 : r MULTIVIEW EQUIVARIANCE IMPROVES 3D CORRESPONDENCE UNDERSTANDING WITH MINIMAL FEATURE FINETUNING Yang You1, Yixin Li1, Congyue Deng1, Yue Wang2, Leonidas Guibas1,(cid:0) 1 Department of Computer Science, Stanford University, U.S.A. 2 Department of Computer Science, University of Southern California, U.S.A. (cid:0) guibas@cs.stanford.edu Figure 1: Improving 3D correspondence understanding through finetuning on feature equivariance. Left: finetuning feature equivariance on one synthetic object can already enhance the vision transformers ability to generate better 3D feature correspondences on general objects. Right: This improvement further leads to superior performance across multiple 3D tasks, including pose estimation, video tracking, and semantic correspondence."
        },
        {
            "title": "ABSTRACT",
            "content": "Vision foundation models, particularly the ViT family, have revolutionized image understanding by providing rich semantic features. However, despite their success in 2D comprehension, their abilities on grasping 3D spatial relationships are still unclear. In this work, we evaluate and enhance the 3D awareness of ViT-based models. We begin by systematically assessing their ability to learn 3D equivariant features, specifically examining the consistency of semantic embeddings across different viewpoints. Our findings indicate that improved 3D equivariance leads to better performance on various downstream tasks, including pose estimation, tracking, and semantic transfer. Building on this insight, we propose simple yet effective finetuning strategy based on 3D correspondences, which significantly enhances the 3D correspondence understanding of existing vision models. Remarkably, even finetuning on single object for just one iteration results in substantial performance gains. All code and resources will be made publicly available to support further advancements in 3D-aware vision models. Our code is available at https://github.com/qq456cvb/3DCorrEnhance."
        },
        {
            "title": "INTRODUCTION",
            "content": "Common camera imaging systems struggle to depict the 3D world due to the limitation of capturing only single perspective at any given moment. In contrast, human perceptual capabilities exhibit remarkable trait known as view equivariance Kohler (1967); Koffka (2013); Wilson & Farah (2003), allowing us to robustly understand 3D spatial relationships, as seen in tasks ranging from basic object recognition Vetter et al. (1995); DiCarlo & Cox (2007) to more complex processes like mental rotation and simulation Stewart et al. (2022). 1 Current large vision models, however, are primarily trained on 2D images, owing to the ease of data acquisition and annotation in 2D. Consequently, their performance is typically evaluated on 2D tasks Amir et al. (2021); Hedlin et al. (2023); Tang et al. (2023); Zhang et al. (2023). This raises critical questions: To what extent do these models possess an inherent awareness of 3D structures? How does this awareness impact their performance on image-based 3D vision tasks? And, can we further enhance the 3D awareness of these vision foundation models? Many image-based 3D scene understanding and content generation tasks depend heavily on large 2D vision models, underscoring the importance of investigating these questions. Existing works have begun to explore this area in task-specific contexts. For example, DietNeRF Jain et al. (2021) finds that CLIP Radford et al. (2021) demonstrates higher feature similarities between views from the same scene than from different scenes, which aids 3D reconstruction. LeRF Kerr et al. (2023) shows that regularizing CLIP with DINO Caron et al. (2021) features improves 3D feature distillation from multiple views. However, these studies are tied to specific tasks such as feature distillation. El Banani et al. (2024) probes the multi-view consistency of ViTs on the NAVI Jampani et al. (2023) and ScanNet Dai et al. (2017) datasets. However, the limited size of these datasets makes it challenging to draw comprehensive conclusions. To address the first question, how well do vision models understand 3D structures, we present comprehensive study of the 3D awareness of large 2D vision models. Specifically, we investigate the view equivariance of latent featuresi.e., the consistency of multi-view 2D image features representing the same 3D point across different views. Using off-the-shelf multiview correspondences rendered from Objaverse Deitke et al. (2023) (synthetic) and MVImgNet Yu et al. (2023) (realworld), we find that current large vision models do exhibit some degree of view-consistent feature generation, with DINOv2 demonstrating the strongest performance. To answer the second question, how does this awareness influence performance in image-based 3D vision tasks, we find that the quality of 3D equivariance is strongly correlated with performance on three downstream tasks requiring 3D correspondence understanding: pose estimation, video tracking, and semantic correspondence. Consistent with previous findings Ornek et al. (2023); Tumanyan et al. (2024); Zhang et al. (2023), DINOv2 Oquab et al. (2023) excels in these tasks. Finally, to address the third question, can we improve the 3D awareness of vision foundation models, we propose simple yet effective method to enhance the view equivariance of 2D foundation models, thereby significantly improving their 3D correspondence understanding. During training, we randomly select two different views of the same object from Objaverse and sample corresponding pixels. We apply the SmoothAP Brown et al. (2020) loss to enforce feature similarity between these corresponding pixels. This finetuning process, requiring only 10K iterations with LoRA and an additional convolutional layer of Vision Transformer (ViT), significantly improves the performance of all tested models on 3D tasks. For instance, DINOv2 gains improvements of 9.58 (3cm-3deg in pose estimation), 5.0 (Average Jaccard in tracking), and 5.06 (PCK@0.05 in semantic correspondence). Surprisingly, even finetuning on single multi-view pair sampled from one object for just one iteration yields notable gains in 3D correspondence understanding. In such cases, DINOv2s performance improves by 4.85, 3.55, and 3.47 for 3cm-3deg (pose estimation), Average Jaccard (tracking), and PCK@0.05 (semantic correspondence), respectively. To summarize, our key contributions are: (i) We conduct comprehensive evaluation of 3D equivariance capabilities in 2D vision foundation models. (ii) We demonstrate that the quality of 3D equivariance is closely tied to performance on three downstream tasks that require 3D correspondence understanding: pose estimation, video tracking, and semantic correspondence. (iii) We propose simple but effective finetuning method that improves the 3D correspondence understanding of 2D foundation models, leading to marked performance gains across all evaluated tasks."
        },
        {
            "title": "2 EVALUATION OF MULTIVIEW FEATURE EQUIVARIANCE",
            "content": "To assess how effectively current vision transformers capture 3D correspondence understanding, we introduce 3D equivariance evaluation benchmark focused on the quality of correspondences between 2D points across different views for the same object. Additionally, we present three wellestablished application tasks that rely on 3D correspondence, demonstrating strong correlation between the quality of 3D equivariance and downstream task performance. We evaluate five stateof-the-art vision transformers: DINOv2 Oquab et al. (2023), DINOv2-Reg Darcet et al. (2023), MAE He et al. (2022a), CLIP Radford et al. (2021) and DeiT Touvron et al. (2022), extracting their 2 final-layer features with L2 normalization. For DINOv2, we use the base model; results for other variants are provided in the supplementary material. To evaluate 3D equivariance, we utilize rendered or annotated multiview correspondences from Objaverse Deitke et al. (2023) and MVImgNet Yu et al. (2023), covering both synthetic and real images. For Objaverse, we randomly select 1,000 objects from the Objaverse repository, rendered across 42 uniformly distributed camera views, producing 42,000 images. Dense correspondences are computed for each object across every unique ordered pair of views, resulting in 1.8 billion correspondence pairs for evaluation. Similarly, 1,000 objects are randomly drawn from MVImgNet, yielding 33.3 million annotated correspondence pairs for evaluation. Since MVImgNet employs COLMAP to reconstruct 3D points, it provides sparser correspondences compared to Objaverse. Figure 2: Feature visualizations of different models. The sample image is rendered from Objeverse. Colors are computed from the high-dimensional features using PCA. We can see that MAE struggles to distinguish different parts of the content (e.g.similar features between head and body). Both CLIP and DeiT produce inconsistent features for the chest region between View 1 and View 2. DINOv2 gives the best correspondence. Metric and Results We propose the Average Pixel Error% (APE), metric that quantifies the average distance between predicted and ground-truth pixel correspondences, normalized by the length of the shortest image edge. The predicted correspondence is determined by identifying the nearest neighbor in the second view, given reference point feature in the first view. APE for Objaverse is shown in Figure 3, where APE is plotted on the x-axis, meaning lower values (towards the left) indicate better performance. APE and PCDP for MVImgNet are plotted on Figure 5s y-axis with representing the evaluted pretrained models (fine-tuning results hollow circle will be discussed later). Percentage of Correct Dense Points% (PCDP) is metric designed to evaluate dense correspondences, similar to Percentage of Correct Keypoints% (PCK). It is reported at various thresholds (5%, 10%, and 20% of the shortest image edge). We can see that DINOv2 and its registered version outperform other vision transformers, highlighting DINOv2s superior capability for 3D equivariance. In Figure 2, we provide feature visualizations using PCA, where DINOv2 again demonstrates the best multiview feature consistency. and striped bar 2.1 FEATURE EQUIVARIANCE CORRELATES TO CERTAIN TASK PERFORMANCES 3D Equivariance itself is not interesting unless it can be used. Below, we will talk about three mature downstream applications that require 3D equivariance capability, and show correlation between the quality of 3D equivariance and the downstream applications. 2.1.1 TASK DEFINITIONS One-Shot Object Pose Estimation In one-shot pose estimation, we assume access to video sequence or 3D mesh of the target object and aim to estimate its pose in arbitrary environments. During onboarding, we store dense 2D image features from all rendered or annotated views in database. At inference, we compute correspondences between the input image and the stored features to match 2D keypoints in the image to their 3D counterparts. Pose estimation from these 2D-3D correspondences is achieved using RANSAC Fischler & Bolles (1981) PnP (Perspectiven-Point). Points are uniformly sampled using stratified sampling (stride 4) on 512 512 resized images. RANSAC PnP runs for 10,000 iterations with threshold of 8. Figure 3: Correlation between multiview feature equivariance and the task performances. Along the horizontal axis, lower APE indicates better feature equivariance, while the vertical axis reflects higher task performance across all four plots. The data points align roughly along the diagonal from the top left to the bottom right, suggesting strong correlation between improved feature equivariance and better task performance. We evaluate on the OnePose-LowTexture and YCB-Video datasets. OnePose-LowTexture He et al. (2022b) includes 40 low-textured household items captured in two videos: one for reference and one for testing, simulating one-shot scenario. Following He et al. (2022b), pose accuracy is evaluated using 1cm-1deg, 3cm-3deg, and 5cm-5deg thresholds. The YCB-Video dataset Xiang et al. (2017) comprises 21 objects and 92 RGB-D video sequences with pose annotations and CAD models for one-shot generalization. database is created by rendering objects from 96 icospherical viewpoints. We report Average Recall (AR) for Visible Surface Discrepancy (VSD), Maximum Symmetry-Aware Surface Distance (MSSD), and Maximum Symmetry-Aware Projection Distance (MSPD) following Hodaˇn et al. (2020). Video Tracking For video tracking, given the reference frame, we identify corresponding points in other frames by computing cosine similarities between the dense features of the target object. To improve robustness and accuracy, we follow the process in DINO-Tracker Tumanyan et al. (2024), which applies softmax operation within the neighborhood of the location with highest similarity. We evaluate the models on the TAP-Vid-DAVIS Doersch et al. (2022) dataset, benchmark designed for testing video tracking in complex, real-world scenarios. Performance is measured using commonly applied metrics Tumanyan et al. (2024), including the Average Jaccard Index (AJ), Position Accuracy (δx avg), and Occlusion Accuracy (OA). Semantic Correspondence In the semantic correspondence task, we utilize feature correspondences to establish precise keypoint matches between images captured from different instances from the same category. Following the method in Zhang et al. (2023), for given reference keypoint, we identify the best match by selecting the location with the highest cosine feature similarity. We use the PF-PASCAL Ham et al. (2017) dataset as our evaluation benchmark. This dataset typically consists of image pairs taken from the same viewpoint, but we additionally report the result by shuffling the image pairs to include different viewpoints, thereby increasing the challenge. We follow standard practice to use PCK@0.05, PCK@0.10, and PCK@0.15 as evaluation metrics. The pipelines for all three tasks are illustrated in the figures provided in the supplementary material. 4 Figure 4: Illustration of different types of correspondence tasks evaluated in our work."
        },
        {
            "title": "2.1.2 ON THE CHOICE OF THREE TASKS",
            "content": "Correspondence estimation is fundamental component of 3D vision understanding, underlying key tasks such as epipolar geometry, stereo vision for 3D reconstruction, and optical flow or tracking to describe the motion of perceived 3D world. Stereo cameras, and even human perception, rely on disparity mapseffectively, correspondences between projected 3D parts to understand depth and spatial relationships. The three tasks we evaluatedpose estimation, video tracking, and semantic correspondenceare intentionally selected to cover diverse aspects of correspondence estimation, ranging from simpler to more complex scenarios: 1. Pose Estimation examines correspondences within the same instance under rigid transformations (SE(3)); 2. Video Tracking extends this to correspondences for the same instance under potential non-rigid or articulated transformations, such as humans or animals in motion; 3. Semantic Correspondence requires correspondences across different instances with similar semantics, often under arbitrary viewpoint changes. An qualitative illustration of these correspondence types is shown in Figure 4. 2.1.3 RESULTS AND FINDINGS Quantitative results are presented in Figure 3, where the y-axis in each graph shows the performance of the vision models. DINOv2 consistently outperforms all other models across all three tasks, in alignment with the rankings for 3D equivariance on the x-axis. There is clear correlation between the quality of 3D equivariance and performance on the downstream tasks: methods with lower APE tend to perform better across all tasks, clustering towards the top-left of the graphs."
        },
        {
            "title": "3 FEATURE FINETUNING WITH MULTIVIEW EQUIVARIANCE",
            "content": "Given the correlation between the multiview equivariance of network features and task performances, we naturally come up with question: Can we finetune the networks on feature equivariance to improve their 3D correspondence understanding and achieve better task performances? Finetuning method The high-level intuition of improving the multiview equivariance of the network features is to enforce the similarity between features of corresponding pixels in 3D space. We experiment with multiple strategies including different training objectives and network architectures. For the training loss, rather than employing conventional contrastive loss, we opted for the SmoothAP Brown et al. (2020) loss, which demonstrated superior performance. While contrastive loss can help align the features of corresponding pixels, it relies on predefined fixed margin for positive and negative samples, which is ad hoc and often suboptimal. In contrast, SmoothAP optimizes ranking loss directly, leading to an improved average precision for feature retrieval between corresponding pixels. We also experimented with the differentiable Procrustes alignment loss Li et al. (2022), but it did not outperform. Detailed ablation results are given in Section 4.3. In terms of architecture, besides the common practice of using LoRA to finetune large foundation models, we introduced single convolutional layer with kernel size of 3 and stride of 1. The motivation behind this addition is rooted in the observation that ViT-family models process image tokens as patches, resulting in much lower-resolution feature maps (e.g., 14x smaller in DINOv2). The standard approach to obtain high-resolution per-pixel features is to apply linear interpolation. Consequently, it is beneficial to explicitly exchange information between neighboring patches before interpolation to achieve more accurate results. More ablation results are given in Section 4.1. During training, we randomly select two views of the same object from 10K subset of Objaverse at each iteration and sample corresponding pixels. The model is trained for 10K iterations using the AdamW optimizer with learning rate of 1e-5 and weight decay of 1e-4. In the supplementary, we show that our finetuning method is robust to the choice of learning rate. Figure 5: Generalization from synthetic images (Objaverse) to real images (MVImgNet). Left: Data points roughly around the diagonal from the bottom left to the upper right indicate the correlation between the APE tested on the two datasets. The * next to the model name means it is finetuned. All finetuning is done on Objaverse with only synthetic data. Right: Finetuned on Objaverse, the feature equivariance of the model (measured in PCDP) improves on MVImgNet. Figure 6: Feature visualization of DINOv2 before and after finetuning on MVImgNet objects (left two) and TAP-VID-DAVIS scenes (right one). For each example, we select three different views. The first column provides reference color produced by PCA, while the second and third columns show the predicted feature correspondences. Our finetuned model demonstrates reduced noise and smoother feature boundaries, particularly noticeable in the reduction of jagged edges. 3.1 IMPROVED FEATURE EQUIVARIANCE WITH GENERALIZATION Figure 5 illustrates the performance of various models before and after finetuning. After finetuning on Objaverse, all models show improved 3D equivariance on both Objaverse (synthetic) and MVImgNet (real-world). This demonstrates the capacity of vision foundation models to perform sim-to-real transfer, as finetuning on synthetic Objaverse objects results in enhanced performance on the real-world MVImgNet dataset. Additionally, the performance on the two datasets is correlated, with data points roughly aligning along the diagonal, indicating that improvements in synthetic environments translate well to real-world settings. DINOv2 stands out as the best model. We also compare the feature visualizations before and after finetuning in Figure 6, from which we can see that after finetuning the model produces more consistent features with less noise. 3. IMPROVED TASK PERFORMANCES One-shot Object Pose Estimation Figure 7 shows the performance of pose estimation on the OnePose-LowTex and YCB-Video datasets before and after fine-tuning. As illustrated, all Vision Transformers (ViTs) exhibit noticeable improvements after being fine-tuned on synthetic Objaverse data. For instance, the best-performing model, DINOv2-Reg, improves by 3.46, 6.67, and 6.92 for the 1cm-1deg, 3cm-3deg, and 5cm-5deg thresholds, respectively. Additionally, models that performed weaker before fine-tuning show larger gains. For example, DeiT improves by 4.65, 16.39, and 17.76. Similar trends are observed for the YCB-Video dataset, where models like MAE, initially the weakest, show substantial improvement after fine-tuning. 6 Figure 7: One-shot pose estimation results before and after feature equivariance finetuning. Figure 8: Video tracking results before and after feature equivariance finetuning. Video Tracking Similarly, in the video tracking task, we observe consistent improvements across all ViTs after fine-tuning, as shown in Figure 8. The top-performing model, DINOv2, achieves improvements of 6.45, 5.73, and 2.69 in AJ, δx avg, and OA, respectively. Semantic Correspondence In the semantic correspondence task, shown in Figure 9, DINOv2 exhibits improvements of 5.06, 3.86, and 1.98 for PCK@0.05, PCK@0.10, and PCK@0.15, respectively. Notably, we find that fine-tuned models show enhanced understanding of keypoint semantics across different instances, even from the same viewpoint. This suggests that 3D equivariance contributes to better understanding of fine-grained semantics, despite not finetuned for that purpose. We also compared with FiT Yue et al. (2024) and DUSt3R Wang et al. (2024), while their performance are much worse than ours. Detailed quantitative results including FiT and DUSt3R on all these tasks are available in the supplementary materials. 3.3 EXTREMELY FEW-SHOT FINETUNING Training with Only One Object We plot the performance relative to the number of training objects used, as shown in Figure 10, keeping the total number of iterations fixed at 10K. Surprisingly, fine-tuning on just one object already provides significant performance improvements. Additionally, the object was randomly selected from Objaverse. We tested six different objects, all of which yielded similar results. The results are shown in Figure 11. Notably, even simple shapes like an untextured hemisphere can enhance the 3D correspondence understanding of the ViTs in these tasks. Convergence Within Few Iterations Figure 12 plots the performance of downstream tasks versus the number of training iterations on single object. Interestingly, our experiments reveal that training with just single multi-view pair of one object for single iteration significantly boosts the models 3D equivariance, as shown by the sharp improvement at the first elbow of Figure 12. This finding is remarkable, indicating that fine-tuning for 3D correspondence in vision transformers is highly efficient in capturing essential 3D spatial relationships with minimal data. Even with such minimal training setup, the model effectively learns the desired 3D properties, substantially improving performance across tasks without requiring extensive training or large datasets. 3.4 FINETUNING VIT ENHANCES 3D TASKS IN THE WILD key advantage of theViT features studied here are highly generalizable across diverse datasets and tasks, supporting even wider range of applications. For example, SparseDFF Wang et al. (2023) 7 Figure 9: Semantic correspondence results before and after feature equivariance finetuneing. Figure 10: Finetuned performances w.r.t. #training objects. We evaluate the performances of the DINOv2 model finetuned with 0, 1, 5, 10, 20, 50, 100 objects on the three tasks. uses DINO to aggregate and fine-tune consistent features across views for few-shot transfer manipulation policy learning; LERF Kerr et al. (2023) employs dense DINO features for regularization; and Wild Gaussians Kulhanek et al. (2024) utilizes off-the-shelf DINO features as priors to estimate occlusions and reconstruct 3D scenes in complex settings. To demonstrate the effectiveness of our fine-tuned features, we conducted experiments on Wild Gaussians (W-G) and found that replacing the original features with our fine-tuned DINO features improved novel view synthesis quality in the wild, as shown in Table 1. Additionally, in the supplementary we show that substituting LERFs DINO regularizer with our fine-tuned version enhances language-embedded field performance, with detailed results and analysis provided therein. Mountain Fountain Corner Patio Spot Patio-High PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS W-G Ours 20.82 21.01 0.668 0.672 0.239 0.234 20.90 20.97 0.668 0. 0.213 0.212 23.51 23.74 0.810 0.810 0.152 0.151 21.31 21.23 0.802 0. 0.134 0.133 23.96 24.01 0.777 0.778 0.165 0.163 22.04 22.11 0.734 0. 0.202 0.201 Table 1: Quantitative comparison of novel view synthesis quality across different scenes. Our fine-tuned DINO features consistently improve performance over the original Wild-Gaussians method, showing higher PSNR and SSIM scores, and lower LPIPS values."
        },
        {
            "title": "4 DESIGN CHOICES FOR FINETUNING",
            "content": "In this section, we ablate and verify the design choices of our finetuning strategy and share some findings. We use the best DINOv2 base model for all our ablations. 4.1 ADDITIONAL CONVOLUTION LAYER HEAD We append single convolution layer to the original model architecture and find that gives surprisingly good performance. Adding single convolutional layer to the finetuning architecture was motivated by the need to improve the resolution and consistency of the dense feature maps produced by Vision Transformer (ViT) models. The typical ViT models process images as low-resolution patches, and while global attention mechanisms facilitate communication between patches, they are not optimized for generating dense per-pixel features during interpolation. By incorporating convolutional layer with kernel size of 3 and stride of 1, we can explicitly exchange information between neighboring patches, allowing the model to generate more accurate and high-resolution 8 Figure 11: Finetuning with different objects. All results are tested with finetuned DINOv2. Dashed lines indicate the performances of the original pretrained model. The feature finetuning method is effective with as few as one single object. It also shows insensitivity to the specific choice of the object, even if the object has limited textures or is uncommon in daily life. Figure 12: Finetuned DINOv2 performances w.r.t. #training iterations, trained with only one object over 0, 1, 5, 10, 20, 50, 100, 1000, 10000 training iterations. feature maps before interpolation. We ablate the number of convolutional layers and table 2 shows that one conv layer gives the best performance. ViT models OnePose-LowTex 1cm-1deg 3cm-3deg 5cm-5deg DINOv2-FT (Conv 0) DINOv2-FT (Conv 1) DINOv2-FT (Conv 2) DINOv2-FT (Conv 3) 11.69 13.58 13.12 12.15 53.85 58.03 56.14 53.63 72.83 77.35 75.45 74.46 AJ OA TAP-VID-DAVIS δx avg 60.79 63.84 63.25 62.14 44.50 46.85 47.42 46.84 84.08 84.15 84.12 82.90 PF-PASCAL (Diff. View) PCK0.05 PCK0. PCK0.15 44.82 47.25 46.32 41.60 57.14 60.76 58.05 53.97 65.26 67.57 64.90 60.22 Table 2: Ablation on the number of appended conv layers. 4.2 TRAINING DATA MVImgNet v.s. Objaverse Our results indicate that finetuning on MVImgNet is slightly worse compared to finetuning on Objaverse, likely due to the denser correspondences provided by Objaverse. Both datasets provide similar object-centric multi-view setup. Although Objaverse is synthetic dataset and MVImgNet consists of real-world captures, large foundation models tend to be largely agnostic to the distinction between simulated and real images. Object-centric datasets v.s. scene-centric datasets An interesting result, as shown in Table 3, is that finetuning on scene-centric datasets (e.g. RealEstate10K Zhou et al. (2018), Spaces Flynn et al. (2019), and LLFF Mildenhall et al. (2019), which contain diverse real-world scenes with complex backgrounds, does not necessarily improve the performance but sometimes make it worse (e.g. PFPASCAL). This may indicate that 3D objects themselves have already encoded enough 3D spatial reasoning information. And scene-centric dataset does include much more background clutter that may distract the network, leading to less accurate feature representations. 9 ViT models OnePose-LowTex 1cm-1deg 3cm-3deg 5cm-5deg DINOv2-FT (Objaverse) DINOv2-FT (MVImgNet) DINOv2-FT (Scene-Centric) 13.58 13.65 15.95 58.03 56.98 60.79 77.35 74.61 76. AJ OA TAP-VID-DAVIS δx avg 63.84 58.89 63.07 84.15 82.67 80.27 46.85 41.53 47.36 PF-PASCAL (Diff. View) PCK0.05 PCK0.10 PCK0.15 47.25 45.13 41.73 60.76 57.93 52.33 67.57 65.40 60. Table 3: Ablation on the dataset used for finetuning. ViT models OnePose-LowTex 1cm-1deg 3cm-3deg 5cm-5deg DINOv2-FT (SmoothAP) DINOv2-FT (Contrastive) DINOv2-FT (DiffProc) 13.58 13.28 12.92 58.03 55.57 55.00 77.35 75.68 74.86 AJ TAP-VID-DAVIS δx avg 63.84 62.20 61. 46.85 43.79 43.60 84.15 81.84 82.74 OA PF-PASCAL (Diff. View) PCK0.05 PCK0. PCK0.15 47.25 46.70 43.89 60.76 58.08 57.22 67.57 66.21 64.66 Table 4: Ablation on the loss function used. SmoothAP delivers the best overall performance."
        },
        {
            "title": "4.3 LOSS FUNCTIONS",
            "content": "We start with naive contrastive loss and found that it does not perform as well. This is because contrastive loss does not directly optimize for the correspondence. In contrast, SmoothAP optimizes ranking loss directly, leading to an improved average precision for feature retrieval between corresponding pixels. We also experimented with the differentiable Procrustes alignment loss from Li et al. (2022), but it did not outperform SmoothAP. Detailed comparisons are given in Table 4."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Vision Transformers Dosovitskiy (2020) (ViTs) have made significant strides in image understanding by employing self-attention mechanisms to capture global contextual information, outperforming traditional convolutional neural networks (CNNs) in tasks such as image classification and object detection. However, despite their success in 2D applications, adapting these models to grasp 3D spatial relationships remains challenging and relatively unexplored area. There is growing interest in assessing the 3D comprehension of vision models. While some studies have investigated how well generative models capture geometric information from single image Bhattad et al. (2024); Du et al. (2023); Sarkar et al. (2024), these efforts are generally specific to generative models, limiting their applicability to broader vision tasks. More closely aligned with our work is El Banani et al. (2024), which evaluated the 3D awareness of visual foundation models through task-specific probes and zero-shot inference using frozen features. In contrast, we delve deeper and introduce simple yet effective method for finetuning 3D awareness in ViTs. Several researchers have also explored applying large-scale models to 3D tasks. For instance, some approaches utilize features from pre-trained models for tasks such as correspondence matching Zhang et al. (2023); Cheng et al. (2024) and pose estimation Ornek et al. (2023). ImageNet3D Ma et al. (2024) investigates how global tokens from ViT vary across views to aid pose estimation. While their work focuses on view-dependent global features, ours emphasizes dense, pixel-level features invariant to viewpoint changes. Their top-down pose estimation approach classifies poses using pretrained features with domain-specific linear layer, which limits its applicability across diverse datasets. In contrast, we argue that finding correspondences, or learning equivariant representations, is more effective strategy for general unseen tasks and datasets. Recent works, such as FiT Yue et al. (2024) and DVT Yang et al. (2024), attempt to finetune pretrained features. FiT lifts 2D features into 3D space and then projects them back into 2D to enforce 3D consistency. DVT, on the other hand, implements denoising process to reduce periodic noise artifacts in images, method that is orthogonal to our approach. Additionally, DUSt3R Wang et al. (2024) directly predicts 3D coordinates for each 2D pixel, but it lacks shared consistent feature space and forfeits the rich semantic information provided by large vision models."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we systematically evaluated the 3D awareness of large vision models, with specific focus on their ability to maintain view equivariance. Our comprehensive study demonstrates that current vision transformers, particularly DINOv2, exhibit strong 3D equivariant properties, which significantly correlate with performance on downstream tasks such as pose estimation, video track10 ing, and semantic transfer. Building on these insights, we introduced simple yet effective finetuning method that enhances the 3D correspondence understanding of 2D ViTs. By leveraging multiview correspondences and applying loss function that enforces feature consistency across views, our approach yields substantial improvements in task performance with minimal computational overhead. Remarkably, even single iteration of finetuning can lead to notable performance gains. Our findings highlight the importance of 3D equivariance in vision models and provide practical path to improving 3D correspondence understanding in existing models. We believe this work opens up new opportunities for enhancing the 3D capabilities of vision transformers. All code and resources will be made publicly available to support further research in this direction."
        },
        {
            "title": "7 ACKNOWLEDGEMENTS",
            "content": "Yang You, Congyue Deng and Leonidas Guibas acknowledge support from the Toyota Research Institute University 2.0 Program, ARL grant W911NF-21-2-0104, Vannevar Bush Faculty Fellowship, and gift from the Flexiv corporation. Yue Wang acknowledges funding supports from Toyota Research Institute, Dolby, and Google DeepMind. Yue Wang is also supported by Powell Research Award."
        },
        {
            "title": "REFERENCES",
            "content": "Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2(3):4, 2021. Anand Bhattad, Daniel McKee, Derek Hoiem, and David Forsyth. Stylegan knows normal, depth, albedo, and more. Advances in Neural Information Processing Systems, 36, 2024. Irving Biederman. Recognition-by-components: theory of human image understanding. Psychological review, 94(2):115, 1987. Andrew Brown, Weidi Xie, Vicky Kalogeiton, and Andrew Zisserman. Smooth-ap: Smoothing In European conference on computer vision, pp. the path towards large-scale image retrieval. 677694. Springer, 2020. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Xinle Cheng, Congyue Deng, Adam Harley, Yixin Zhu, and Leonidas Guibas. Zero-shot image feature consensus with deep functional maps. arXiv preprint arXiv:2403.12038, 2024. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1314213153, 2023. James DiCarlo and David Cox. Untangling invariant object recognition. Trends in cognitive sciences, 11(8):333341, 2007. Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 11 Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do they know? do they know things? lets find out! arXiv preprint arXiv:2311.17137, 2023. Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2179521806, 2024. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88: 303338, 2010. Martin Fischler and Robert Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24 (6):381395, 1981. John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and Richard Tucker. Deepview: View synthesis with learned gradient descent. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 23672376, 2019. Bumsub Ham, Minsu Cho, Cordelia Schmid, and Jean Ponce. Proposal flow: Semantic correspondences from object proposals. IEEE transactions on pattern analysis and machine intelligence, 40(7):17111725, 2017. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022a. Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free one-shot object pose estimation without cad models. Advances in Neural Information Processing Systems, 35:3510335115, 2022b. Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. arXiv preprint arXiv:2305.15581, 2023. Tomaˇs Hodaˇn, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric Brachmann, Frank Michel, In European Carsten Rother, and Jiˇrı Matas. Bop challenge 2020 on 6d object localization. Conference on Computer Vision, pp. 577594. Springer, 2020. Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on diet: Semantically consistent fewIn Proceedings of the IEEE/CVF International Conference on Computer shot view synthesis. Vision, pp. 58855894, 2021. Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov, Andre Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen Li, and Howard Zhou. Navi: Category-agnostic In NeurIPS, 2023. URL image collections with high-quality 3d shape and pose annotations. https://navidataset.github.io/. Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv preprint arXiv:2307.07635, 2023. Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1972919739, 2023. Kurt Koffka. Principles of Gestalt psychology, volume 44. Routledge, 2013. Wolfgang Kohler. Gestalt psychology. Psychologische Forschung, 31(1):XVIIIXXX, 1967. Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, and Torsten Sattler. Wildgaussians: 3d gaussian splatting in the wild. arXiv preprint arXiv:2407.08447, 2024. Yann Labbe, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. Megapose: 6d pose estimation of novel objects via render & compare. arXiv preprint arXiv:2212.06870, 2022. Lei Li, Hongbo Fu, and Maks Ovsjanikov. Wsdesc: Weakly supervised 3d local descriptor learning for point cloud registration. IEEE Transactions on Visualization and Computer Graphics, 29(7): 33683379, 2022. Wufei Ma, Guanning Zeng, Guofeng Zhang, Qihao Liu, Letian Zhang, Adam Kortylewski, Yaoyao Liu, and Alan Yuille. Imagenet3d: Towards general-purpose object-level 3d understanding. arXiv preprint arXiv:2406.09613, 2024. Ben Mildenhall, Pratul Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (ToG), 38(4):114, 2019. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Evin Pınar Ornek, Yann Labbe, Bugra Tekin, Lingni Ma, Cem Keskin, Christian Forster, and Tomas Hodan. Foundpose: Unseen object pose estimation with foundation features. arXiv preprint arXiv:2311.18809, 2023. Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondˇrej Chum. Revisiting oxford and paris: Large-scale image retrieval benchmarking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 57065715, 2018. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Ayush Sarkar, Hanlin Mai, Amitabh Mahapatra, Svetlana Lazebnik, David Forsyth, and Anand Bhattad. Shadows dont lie and lines cant bend! generative models dont know projective geometry... for now. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2814028149, 2024. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pp. 746760. Springer, 2012. Emma EM Stewart, Frieder Hartmann, Yaniv Morgenstern, Katherine Storrs, Guido Maiello, and Roland Fleming. Mental object rotation based on two-dimensional visual representations. Current Biology, 32(21):R1224R1225, 2022. Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. arXiv preprint arXiv:2306.03881, 2023. Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. In European conference on computer vision, pp. 516533. Springer, 2022. Narek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel. Dino-tracker: Taming dino for selfsupervised point tracking in single video. arXiv preprint arXiv:2403.14548, 2024. Thomas Vetter, Anya Hurlbert, and Tomaso Poggio. View-based models of 3d object recognition: invariance to imaging transformations. Cerebral Cortex, 5(3):261269, 1995. Qianxu Wang, Haotong Zhang, Congyue Deng, Yang You, Hao Dong, Yixin Zhu, and Leonidas Guibas. Sparsedff: Sparse-view feature distillation for one-shot dexterous manipulation. arXiv preprint arXiv:2310.16838, 2023. 13 Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024. Kevin Wilson and Martha Farah. When does the visual system use viewpoint-invariant representations during recognition? Cognitive Brain Research, 16(3):399415, 2003. Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: convolutional neural network for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:1711.00199, 2017. Jiawei Yang, Katie Luo, Jiefeng Li, Kilian Weinberger, Yonglong Tian, and Yue Wang. Denoising vision transformers. arXiv preprint arXiv:2401.02957, 2024. Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 91509161, 2023. Yuanwen Yue, Anurag Das, Francis Engelmann, Siyu Tang, and Jan Eric Lenssen. Improving 2d feature representations by 3d-aware fine-tuning. arXiv preprint arXiv:2407.20229, 2024. Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. arXiv preprint arXiv:2305.15347, 2023. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 METRIC AND LOSS IMPLEMENTATION DETAILS In this section, we give the detailed mathematical definitions of the evaluation metrics and the loss used in our method. Average Pixel Error (APE): Suppose we have objects, each rendered from = 42 different views. For pixel x1 in the first image, the ground-truth corresponding pixel x2 in the second image is determined via back-projection into 3D and re-rendering, excluding occluded points. The evaluated method predicts x2. APE is computed as: (cid:88) (cid:88) (cid:88) (cid:88) AP = x1x2 x2 x22 min(W, H) where W, are the image width and height. Percentage of Correct Dense Points (PCDP): PCDP measures the proportion of predicted points x2 that fall within normalized threshold δ of the ground-truth point x2: CDP = (cid:88) (cid:88) (cid:88) (cid:88) x1x ( x2 x22 min(W, H) < δ) Here () is the indicator function and δ is threshold (commonly 0.05, 0.1 or 0.15). Smooth Average Precision (SmoothAP): SmoothAP is used as the training loss to enforce accurate feature correspondences: SmoothAP = 1 SP (cid:88) iSP 1 + (cid:80) σ(Dij) jSP σ(Dij) + (cid:80) 1 + (cid:80) jSP σ(Dij) jSN where given query point x1, SP is the positive set containing ground-truth points {x2},SN is the negative set containing all other points in the second view, and σ is the sigmoid function, and Dij = fj fx1 fi fx1 measures the difference in feature similarity with respect to the query point x1. Ideally, we want all negative points to have smaller similarities with respect to x1 than all positive ones. In this case, (cid:80) σ(Dij) = 0 and we get SmoothAP = 1. In training, we optimize the loss: 1 SmoothAP . jSN A.2 QUANTITATIVE RESULTS ON OBJAVERSE AND MVIMGNET The detailed quantitative results on 3D equivariance of Objaverse and MVImgNet are given in Table 5 and 6. Model DINOv2 Oquab et al. (2023) finetuned DINOv2-Reg Darcet et al. (2023) finetuned MAE He et al. (2022a) finetuned CLIP Radford et al. (2021) finetuned DeiT Touvron et al. (2022) finetuned PCDP(%) 0.1 0.2 APE(%) 36.84 43.65 37.24 36.39 30.71 35.94 33.00 38.01 33.89 38. 58.88 61.78 58.23 57.84 55.46 56.93 57.17 59.71 58.05 59.95 19.12 17. 19.51 19.48 20.58 19.88 20.11 19.17 19.72 19.00 0.05 22.60 30. 23.05 22.81 16.25 22.57 17.05 22.54 18.07 23.39 Table 5: Evaluation of dense correspondence on Objaverse."
        },
        {
            "title": "Model",
            "content": "DINOv2 Oquab et al. (2023) finetuned DINOv2-Reg Darcet et al. (2023) finetuned MAE He et al. (2022a) finetuned CLIP Radford et al. (2021) finetuned DeiT Touvron et al. (2022) finetuned PCDP(%) 0.1 0.2 APE(%) 77.94 83.12 78.99 78.38 75.82 82. 63.49 72.78 72.36 80.12 92.49 93.41 92.25 92.36 91.42 92.75 80.53 85. 87.64 91.63 6.24 4.96 6.06 5.90 6.73 4.76 11.34 8.42 8.34 5. 0.05 62.09 71.74 64.54 64.35 59.10 73.76 46.63 60.23 54.63 67. Table 6: Evaluation of dense correspondence on MVImgNet. Method OnePose++ He et al. (2022b) DUSt3R Wang et al. (2024) FiT Yue et al. (2024) FiT-Reg Yue et al. (2024) DINOv2 Oquab et al. (2023) Finetuned DINOv2-Reg Darcet et al. (2023) Finetuned MAE He et al. (2022a) Finetuned CLIP Radford et al. (2021) Finetuned DeiT Touvron et al. (2022) Finetuned OnePose-LowTex 1cm-1deg 3cm-3deg 5cm-5deg 16.8 2.88 1.05 3.44 9.43 13.58 9.95 13.41 4.41 10. 2.85 6.72 2.55 7.20 57.7 16.61 9.18 23.51 48.45 58.03 52.65 59. 20.76 39.37 19.65 35.63 16.85 33.24 72.1 26.79 16.52 37.68 67.45 77. 71.72 78.64 32.27 52.97 33.84 52.94 31.67 49.43 Table 7: Quantitative results of one-shot pose estimation on OnePose-LowTex. A.3 QUANTITATIVE AND QUALITATIVE RESULTS ON THE THREE TASKS We present detailed quantitative results for the three tasks (pose estimation, video tracking, and semantic transfer) in this section. Additionally, we compare our method with DUSt3R Wang et al. (2024), FiT Yue et al. (2024) and FiT-Reg Yue et al. (2024). FiT-Reg is FiT finetuned on DINOv2 with registers Darcet et al. (2023). For pose estimation and tracking, we also provide comparisons with state-of-the-art methods such as OnePose++ He et al. (2022b), MegaPose Labbe et al. (2022), and Co-Tracker Karaev et al. (2023), which are specifically trained on these tasks. The results are summarized in Tables 7, 8, 9, 10, and 11. Our experiments reveal that although FiT aims for 3D consistency, it significantly disrupts the semantics of certain parts, as shown in Figure 13. While this semantic disruption may be acceptable for FiTs original tasks like semantic segmentation and depth estimationwhere an additional linear head can correct these issuesit becomes problematic for our tasks that require 3D-consistent, dense, pixel-level features. We hypothesize that FiTs poor performance stems from its naive approach to learning 3D consistency through an explicit 3D Gaussian field. When outliers or noise are present, the simple mean square error causes feature representations to shift toward these outliers."
        },
        {
            "title": "VSD MSSD MSPD AR",
            "content": "MegaPose Labbe et al. (2022) DUSt3R Wang et al. (2024) FiT Yue et al. (2024) FiT-Reg Yue et al. (2024) DINOv2 Oquab et al. (2023) Finetuned DINOv2-Reg Darcet et al. (2023) Finetuned MAE He et al. (2022a) Finetuned CLIP Radford et al. (2021) Finetuned DeiT Touvron et al. (2022) Finetuned 53.5 11.6 4.4 10.2 34.9 39.9 34.2 38.1 15.9 32. 17.0 28.3 19.4 29.4 59.7 11.5 3.2 9.4 39.4 44.4 37.9 42. 17.9 36.8 19.1 31.3 19.8 31.1 72.8 15.8 3.4 11.3 58.8 63. 55.4 60.0 26.8 54.0 31.0 35.6 31.2 45.6 62.0 13.0 3.7 10. 44.4 49.4 42.5 46.8 20.2 41.0 22.4 28.3 23.5 35.4 Table 8: Quantitative results of one-shot pose estimation on YCB-Video. Method Co-Tracker Karaev et al. (2023) DUSt3R Wang et al. (2024) FiT Yue et al. (2024) FiT-Reg DINOv2 Oquab et al. (2023) Finetuned DINOv2-Reg Darcet et al. (2023) Finetuned MAE He et al. (2022a) Finetuned CLIP Radford et al. (2021) Finetuned DeiT Touvron et al. (2022) Finetuned AJ TAP-VID-DAVIS δx avg 79.4 65.6 OA 89.5 13.06 20.45 23.28 40.40 46.85 37.89 44.91 29.99 36.04 25.86 32. 26.80 32.55 22.64 33.46 37.30 58.11 63.84 55.43 62.23 48.16 54.97 41.17 49. 42.06 48.41 77.27 77.27 77.27 81.46 84.15 80.77 83.85 77.27 77.27 79.28 79. 78.45 78.49 Table 9: Quantitative results of tracking on TAP-VID-DAVIS. A.4 QUANTITATIVE RESULTS FOR OTHER VARIANTS OF DINOV2 In addition to evaluating the DINOv2 base model, we tested our finetuning method on other variants, including small, large, and giant. Our method consistently yields improvements across almost all metrics for these model variants. The full results are presented in Table 12. A.5 RESULTS FOR OTHER FOUNDATION MODELS WITH DIFFERENT ARCHITECTURES. In addition to ViT, we apply our method to other architectures like ConvNeXt and find that we can consistently improve its performance on downstream tasks as well as shown in Table 13. However, weve also observed that ConvNeXt features are not as good as those of modern ViTs. Nonetheless, we do expect and observe improvements in non-ViT based methods like ConvNeXt. This finding is particularly interesting as it teaches us valuable lesson: with relatively simple 3D fine-tuning,"
        },
        {
            "title": "Method",
            "content": "PF-PASCAL PCK0.05 PCK0.10 PCK0.15 DUSt3R Wang et al. (2024) FiT Yue et al. (2024) FiT-Reg Yue et al. (2024) DINOv2 Oquab et al. (2023) Ours DINOv2-Reg Darcet et al. (2023) Finetuned MAE He et al. (2022a) Finetuned CLIP Radford et al. (2021) Finetuned DeiT Touvron et al. (2022) Finetuned 4.70 13.10 22.39 42.18 47. 38.29 44.44 11.98 14.45 13.87 20.39 17.73 20.24 8.21 23.99 36.45 56.90 60. 53.74 57.27 20.16 23.79 24.85 32.36 31.17 33.29 13.01 33.45 45.27 65.59 67. 61.94 65.27 28.16 32.56 35.13 42.58 41.17 41.62 Table 10: Quantitative results of PF-PASCAL (Different Viewpoints). Method PF-PASCAL PCK0.05 PCK0.10 PCK0.15 DUSt3R Wang et al. (2024) FiT Yue et al. (2024) FiT-Reg Yue et al. (2024) DINOv2 Oquab et al. (2023) Finetuned DINOv2-Reg Darcet et al. (2023) Finetuned MAE He et al. (2022a) Finetuned CLIP Radford et al. (2021) Finetuned DeiT Touvron et al. (2022) Finetuned 2.64 13.96 26.47 60.22 69. 52.86 62.63 17.16 21.26 17.44 27.40 21.21 30.18 8.01 27.42 45.74 79.05 84. 71.93 79.24 31.52 36.16 31.38 42.72 38.96 49.69 15.00 37.39 55.32 85.95 89. 80.11 86.69 43.54 48.52 41.81 52.67 50.36 60.34 Table 11: Quantitative results of PF-PASCAL (Same Viewpoint). we can achieve even better 3D features than those obtained through pretraining on vast set of unstructured 2D images. A.6 RESULTS ON OTHER SEMANTIC-RELATED TASKS Here, we report results on other semantic-related tasks less focused on 3D understanding. As shown in Table 14, our finetuning method performs on par or slightly worse compared to baseline models in these tasks. We recon that these tasks do not benefit as much from the dense 3D equivariant features our method emphasizes, but rather from coarse, object-level global features. For instance, in tasks where plane or side of box should share the same semantic mask and depth, pixel-level dense features are unnecessary to achieve satisfactory results. Future work can be explored to enhance object-level global feature representation. 18 Figure 13: FiT and DINOv2 semantic correspondence visualization. We find that FiT significantly disrupts the semantics of certain parts. ViT models DINOv2-S Finetuned DINOv2-L Finetuned DINOv2-G Finetuned DINOv2-S-reg Finetuned DINOv2-L-reg Finetuned DINOv2-G-reg Finetuned OnePose-LowTex 1cm-1deg 3cm-3deg 5cm-5deg 8.14 12.85 10.83 13.86 13.58 14. 10.25 12.25 10.89 14.00 11.14 14.24 45.77 56.17 51.68 58.79 58.73 60. 49.04 56.69 51.17 58.58 53.84 59.88 65.79 74.27 70.01 77.46 76.27 78. 68.83 75.66 69.99 77.12 72.28 79.19 AJ TAP-VID-DAVIS δx avg 55.04 61.35 80.54 83. OA 37.56 45.17 42.56 49.10 44.79 50.77 34.61 40.53 39.47 46. 41.39 47.93 59.88 65.00 61.01 66.00 52.21 58.50 56.69 63.20 58.62 64. 83.29 85.42 85.27 85.82 79.35 81.14 82.26 84.43 83.09 85.38 PF-PASCAL (Diff. View) PCK0.05 PCK0.10 PCK0.15 39.02 41.02 44.22 51.66 44.57 50. 31.30 38.78 41.26 48.03 40.94 47.36 53.26 53.78 57.92 62.96 57.63 61. 45.47 52.08 56.24 60.17 53.84 59.20 61.49 60.95 65.85 70.48 65.76 68. 54.73 59.26 63.38 67.13 61.87 66.55 Table 12: Other dino variant results on OnePose-LowTex, TAP-VID-DAVIS, and PF-PASCAL. Instance Recognition The objective for this task is to identify and differentiate individual object instances within scene, even when multiple objects belong to the same class (e.g., recognizing distinct cars in street scene). This task was evaluated using the Paris-H(ard) Radenovic et al. OnePose-LowTex TAP-VID-DAVIS PF-PASCAL 1cm 1deg 3cm 3deg 5cm 5deg AJ ConvNext-small small-finetuned ConvNext-base base-finetuned ConvNext-large large-finetuned 3.25 5.28 5.10 8. 4.71 7.21 13.46 19.98 22.22 32.69 25.33 30.68 21.39 28.23 34.81 46. 36.48 44.47 15.98 16.70 17.57 18.53 19.43 19.45 δavg 26.08 26.56 28.21 28. 30.24 30.68 OA PCK0.05 PCK0.10 PCK0.15 74.72 74. 72.47 71.24 73.71 74.33 10.32 11.61 13.62 15.64 11.05 14.56 16.30 19. 21.03 25.37 17.57 24.04 22.17 25.56 27.81 32.13 24.19 31.57 Table 13: ConvNext finetuning results on OnePose-LowTex, TAP-VID-DAVIS, and PFPASCAL. (2018) dataset, with performance measured by mean Average Precision (mAP), which captures the precision-recall trade-off. Two probe training configurations were explored: one utilizing only the class token (Cls) and another concatenating patch tokens with the class token (Cls+Patch). Our finetuned model demonstrated performance on par with the DINOv2 baseline, achieving mAP scores of 76.23 for Cls and 75.43 for Cls+Patch. Semantic Segmentation This task involves assigning semantic label to each pixel in an image, thereby grouping regions based on their object class, without distinguishing between individual instances of the same class. The VOC2012 Everingham et al. (2010) dataset was used to evaluate this task, with performance metrics including mean Intersection over Union (mIoU) and mean Accuracy (mAcc). These metrics assess the overlap between predicted segmentation and ground truth, as well as pixel-wise classification accuracy. Our fine-tuned model achieved an mIoU of 82.65 and mAcc of 90.21, performing slightly below but comparable to DINOv2. Depth Estimation This task aims to predict the distance to each pixel in an image, effectively generating depth map that represents the 3D structure of the scene. This task is critical for applications requiring spatial understanding, such as indoor navigation and scene reconstruction. We used the NYUv2 Silberman et al. (2012) dataset for evaluation, employing the δ1 accuracy and absolute relative error (abs rel) metrics to assess depth prediction performance. Our fine-tuned model achieved δ1 score of 85.48 and an abs rel of 0.1299, slightly underperforming but comparable to the DINOv2 baseline. Model Paris-H Inst. Recognition VOC2012 Segmentation NYUv2 Depth Estimation DINOv2 Oquab et al. (2023) Finetuned Cls 75.92 76.23 Cls+Patch mIoU mAcc 73.69 75. 83.60 82.65 90.82 90.21 δ1 86.88 85.48 abs rel 0.1238 0.1299 Table 14: Quantitative results of instance recoginition, semantic segmentation and depth estimation. ViT models OnePose-LowTex 1cm-1deg 3cm-3deg 5cm-5deg DINOv2-FT (LR 1e-6) DINOv2-FT (LR 3e-6) DINOv2-FT (LR 1e-5) DINOv2-FT (LR 3e-5) 11.86 13.05 13.58 13.15 55.03 57.45 58.03 58.33 73.12 75.89 77.35 77.49 AJ OA TAP-VID-DAVIS δx avg 62.56 63.32 63.84 63. 83.17 83.73 84.15 83.35 44.79 45.93 46.85 46.70 PF-PASCAL (Diff. View) PCK0.05 PCK0.10 PCK0. 47.34 47.20 47.25 45.70 60.10 60.50 60.76 57.96 68.23 67.21 67.57 65.99 Table 15: Ablation on the learning rate for finetuning. A.7 MORE RESULTS ON LERF In addition to the Wild-Gaussians experiment in our main paper, we visualize LERF 3D features after replacing its DINO regularizer with our fine-tuned version in Figure 14. When given the text query plate, LERF with our fine-tuned DINO produced better relevancy map than the original. Our relevancy map localizes of the plate region better and reduces noise in irrelevant areas such as 20 Figure 14: Visualization of LERF relevancy maps for the query plate. Our finetuned DINO features produce more focused and accurate relevancy map compared to the original DINO features, with better localization of the plate region and reduced noise in irrelevant areas such as cookies. cookies. These experiments demonstrate that our 3D fine-tuning produces better general-purpose features that enhance various applications. A.8 MORE ABLATION STUDY ANALYSIS A.8.1 ABLATIONS ON MULTI-LAYER FEATURE FUSION In addition to extracting features solely from the last layer, we experiment with two different variations: concatenating the features from the last 4 layers and concatenating features from the 2nd, 5th, 8th, and 11th layers. The results are presented in the Table. We find that fusing features from different layers does improve the instance-level correspondence little bit but greatly harms semantic correspondences in tracking and semantic transfer. This indicates that features from earlier layers focus more on instance-level details, while the final layer captures more semantic information. A.8.2 ABLATIONS ON LEARNING RATE Our finetuning method is insensitive to the choice of learning rate, and it can work within reasonable range of learning rates, as shown in Table 15. A.8.3 QUALITATIVE RESULTS ON NUMBER OF CONVOLUTION LAYERS Upon analyzing the effect of additional convolutional layers, we find that while one additional convolutional layer significantly improves the performance, adding two or three layers introduces noise into the features. This noise likely arises from the increased parameter freedom, which can overfit to local patterns and reduce the consistency of dense pixel-wise features, as shown in Figure 15. It clearly show that the additional layers produce less coherent features, leading to degradation in downstream task performance. A.9 OTHER FINDINGS AND DISCUSSIONS Why Untextured Symmetric Hemisphere can Enhance 3D Understanding? Unlike perfect sphere, the hemisphere we used is not completely symmetric and provides information about edges and viewpoint orientation. Our visualization of the learned embeddings in Figure 16 shows that after fine-tuning on the hemisphere, the network achieves better edge correspondences and can differentiate between inward and outward views. Even though the object lacks texture, the shadows and edge features provide sufficient cues for the ViT features to develop 3D understanding. Similarly, in cognitive science, scientists have discovered that the human brain excels at inferring 3D structure. Biedermans Recognition-by-Components (RBC) theory Biederman (1987) suggests that humans recognize objects through simple 3D primitives called geons (geometrical ions)basic shapes such as cubes, cylinders, and cones. Training without Background Enhances Background-invariance Interestingly, we observed that finetuning on object-centric datasets without backgrounds enhanced the foundation models background invariance. Specifically, when comparing an object on black background (i.e., no background) with the same object on natural background from the same viewpoint, the finetuned model demonstrated superior feature consistency across corresponding pixels. We quantitatively validated this finding using pairs of images from random 1K subset from the MSCOCO val dataset. 21 Figure 15: Comparison of feature visualizations with varying convolutional layers. Adding more than one convolutional layer introduces noise and reduces feature coherence, as shown by the highlighted regions. Figure 16: Feature visualization of an untextured hemisphere from different viewpoints. Top row: Input hemisphere rendered from four different angles. Middle row: Feature embeddings from DINOv2 visualized using RGB mapping, showing inconsistent features across views and edges (highlighted by white circles). Bottom row: Our fine-tuned DINOv2 produces more consistent features that better preserve correspondences across viewpoints, particularly at edges and inward outward views. For each annotated object, one image crop was masked while the other was unmasked. We measured the number of inliers by counting mutual nearest neighbors in the feature space that were within 1 pixel of the ground truth. The results confirmed that our finetuned model significantly improved feature consistency across these variations."
        },
        {
            "title": "Method",
            "content": "#Inliers DINOv2 Oquab et al. (2023) Finetuned DINOv2-RegDarcet et al. (2023) Finetuned MAE He et al. (2022a) Finetuned CLIP Radford et al. (2021) Finetuned DeiT Touvron et al. (2022) Finetuned 99 159 76 148 97 196 18 61 25 81 Table 16: Quantitative results on the number of feature inliers that are backgroundinvariant. Figure 17: Visualization of DINOv2s feature correspondence before and after finetuning, using mutual nearest neighbor. After finetuning, we get more feature correspondences. Figure 18: Pose estimation pipeline. During the onboarding phase, 2D dense features are extracted from the provided reference video and stored in database. During inference, features are matched between single query image and the database, followed by 3D-2D RANSAC-PnP to compute the final pose. A.10 PIPELINE VISUALIZATION FOR DOWNSTREAM APPLICATIONS Figures 18, 19, 20, and 21 illustrate the detailed pipelines for various downstream tasks. Note that for pose estimation, tracking, and semantic transfer, no linear fine-tuning is applied. These tasks exclusively assess the quality of the pretrained features from the ViT. A.11 QUALITATIVE RESULTS FOR POSE ESTIMATION/TRACKING/SEMANTIC TRANSFER In this section, we provide qualitative comparisons on various downstream tasks. The results for pose estimation, tracking, and semantic correspondence are shown in Figures 22, 23 and 24, respectively. Since DINOv2-Reg exhibits performance highly similar to DINOv2, we omit its qualitative results. 23 Figure 19: Tracking pipeline. For each point in the source frame, its nearest neighbors are located in the feature space across other frames. Figure 20: Semantic transfer pipeline. For the given keypoints in the reference image, descriptors are extracted using the frozen ViT, and their nearest neighbors are identified in the query images feature space. Figure 21: Semantic segmentation and depth estimation pipeline. Given an input image, linear layer is fine-tuned on top of the frozen ViT to predict segmentation or depth. 24 Figure 22: Qualitative results on YCB-Video pose estimation for different models, both before and after finetuning, are presented. Ground-truth poses are shown in green, while predictions are depicted in red. It can be observed that, in most cases, pose accuracy improves after finetuning, particularly for the MAE, CLIP, and DeiT models. 25 Figure 23: Qualitative results on TAP-VID-DAVIS for different models, both before and after finetuning, are shown. Query points are marked in various colors in the first frame, with red lines indicating the trajectory of the points. Prior to finetuning, the trajectories are highly noisy and inconsistent. However, after finetuning, tracking becomes significantly more stable. 26 Figure 24: Qualitative results on PF-PASCAL (different views) for various models, both before and after finetuning, are presented. For each pair, the left image is the reference, and the right is the query. Ground-truth correspondences are shown in green, while predictions are depicted in red. It can be observed that, in most cases, finetuning improves accuracy by aligning the keypoints closer to their correct positions."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Stanford University, U.S.A.",
        "Department of Computer Science, University of Southern California, U.S.A."
    ]
}