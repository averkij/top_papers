{
    "paper_title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning",
    "authors": [
        "Zhenpeng Su",
        "Leiyu Pan",
        "Minxuan Lv",
        "Yuntao Li",
        "Wenping Hu",
        "Fuzheng Zhang",
        "Kun Gai",
        "Guorui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \\textbf{C}ontrolling \\textbf{E}ntropy via \\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 2 1 7 0 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Technical Report",
            "content": "CE-GPPO: CONTROLLING ENTROPY VIA GRADIENTPRESERVING CLIPPING POLICY OPTIMIZATION IN REINFORCEMENT LEARNING Zhenpeng Su1 Leiyu Pan1 Minxuan Lv1 Yuntao Li2 Wenping Hu1 Kun Gai1 Guorui Zhou1 Fuzheng Zhang1 1Klear Team, Kuaishou Technology 2Independent Github:https://github.com/Kwai-Klear/CE-GPPO"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning (RL) has become powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play critical yet overlooked role in regulating entropy evolution. We propose Controlling Entropy via Gradient-Preserving Policy Optimization (CE-GPPO), novel algorithm that reintroduces gradients from clipped tokens in native PPO in gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an explorationexploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning (RL) has increasingly become paradigm for fine-tuning large language models (LLMs), shifting the focus from mere imitation learning to goal-directed optimization (Ouyang et al., 2022; Shao et al., 2024; DeepSeek-AI et al., 2025). Unlike supervised learning, which only fits the observed data distribution, RL directly optimizes model behavior through environmental feedback, enabling improvements on abstract objectives such as factual accuracy, coherence, and reasoning capability (Yu et al., 2025). In particular, Reinforcement Learning with Verifiable Rewards (RLVR) have attracted growing attention (Lambert et al., 2024). By providing regularized and automatically evaluable reward signals, these approaches offer stable and interpretable guidance, significantly enhancing model performance in reasoning tasks (Su et al., 2025b). Despite the promise of reinforcement learning for goal-driven model optimization, training dynamics remain challenging, particularly in regulating policy entropy, key indicator of the models exploration capability (Ahmed et al., 2019). Policy entropy measures the uncertainty in action selection (Cheng et al., 2025). Our in-depth analysis reveals that the dynamic behavior of entropy in RL can be traced to the intrinsic interaction between the advantage function and the token probability distribution. Specifically, gradient updates can be categorized into four typical patterns: *Equal contribution. This work was completed by Leiyu Pan during an internship at Kuaishou. Corresponding authors."
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: Left: Importance sampling distribution of tokens with different probabilities. Based on the distribution, all tokens can be categorized into four types: PA&HP, NA&LP, PA&LP and NA&HP. Center: The effect of the four token types on entropy dynamics. The two categories shown at the top contribute to entropy reduction, while those at the bottom contribute to entropy increase. Green check marks indicate tokens that lie within the clipping interval, whereas dashed circles denote tokens that partly fall outside the clipping interval. Right: Entropy instability curves caused by the absence of some PA&LP or NA&LP tokens. Positive-advantage high-probability (PA&HP) tokens and negative-advantage low-probability (NA&LP) tokens: optimizing these tokens reinforces high-probability choices, accelerating policy convergence and exacerbating entropy collapse. Positive-advantage low-probability (PA&LP) tokens and negative-advantage high-probability (NA&HP) tokens: optimizing these tokens encourages exploration of low-probability actions, maintaining response diversity and mitigating entropy collapse. Further investigation shows strong connection between token probabilities and importance sampling. As shown in Figure 1, high-probability tokens typically lie within the PPO clipping interval (Schulman et al., 2017), whereas tokens outside the clip interval are predominantly low-probability tokens. For reinforcement learning, clipped importance sampling is commonly used technique. The clipping mechanism primarily controls the magnitude of updates to the policy model to ensure training stability. This results in primary focus on optimizing unclipped high-probability tokens, but we find that ignoring the clipped low-probability tokens, i.e., NA&LP and PA&LP tokens, introduces the following issues: Entropy collapse due to the absence of PA&LP tokens. Tokens truncated beyond the importance sampling threshold 1 + ϵ, namely PA&LP tokens, often include high-entropy tokens that correspond to valuable exploratory behaviors at critical decision points. Directly clipping the gradients of these tokens restricts exploration, leading to entropy collapse. Although DAPO mitigates this issue by extending the upper clipping bound to 1 + ϵh through the clip-higher strategy, high-entropy tokens that exceed this bound still suffer from the same problem. Entropy explosion due to the absence of NA&LP tokens. Tokens truncated beyond the importance sampling threshold 1ϵ, namely NA&LP tokens, include tokens that guide the model toward exploitation. Clipping the gradients of these tokens forces the model into excessive exploration, delays convergence, and consequently induces entropy explosion. natural idea is to merge the gradients of tokens outside the clipping range: PA&LP tokens promote exploration, while NA&LP tokens encourage exploitation, and by leveraging the magnitude of their gradients, it is possible to strike balance between exploration and exploitation and ensure entropy stability during training. Based on these insights, we propose CE-GPPO, an algorithm that reframes the control of entropy dynamics as problem of managing gradients from tokens outside the clipping interval. Specifically, CE-GPPO uses stop-gradient operation to merge gradients from tokens beyond the clipping interval and adjusts their magnitude to maintain policy entropy at high and stable level, which we find to be beneficial for improving model performance. Importantly, we provide both theoretical"
        },
        {
            "title": "Technical Report",
            "content": "justification and empirical evidence showing that incorporating gradients from tokens outside the PPO trust region, i.e., the tokens clipped by PPO, does not cause the policy model to deviate excessively from the old policy model and still preserves stable training. Furthermore, we observe that incorporating PA&LP tokens during the early stage prevents entropy collapse and promotes exploration, while incorporating subset of NA&LP tokens in the later stage facilitates convergence and exploitation, leading to optimal performance. Unlike prior approaches such as DAPOs clip-higher strategy (Yu et al., 2025), our method leverages broader spectrum of gradient signals, enabling more precise control over the dynamics of policy entropy and achieving superior balance between exploration and exploitation. The main contributions of our work can be summarized as follows: We uncover the intrinsic mechanism behind entropy dynamics in RL for LLMs and identify novel perspective for controlling entropy evolution. We propose CE-GPPO, an algorithm that regulates gradients from tokens outside both sides of the clip interval, enabling fine-grained control of policy entropy and update stability. We empirically demonstrate that CE-GPPO effectively prevents entropy collapse and improves the balance between exploration and exploitation."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "2.1 POLICY OPTIMIZATION ALGORITHMS Proximal Policy Optimization (PPO) PPO (Schulman et al., 2017) is widely used policy gradient method in reinforcement learning, designed to balance learning stability and sample efficiency. It improves upon classical policy gradient approaches by constraining the magnitude of policy updates, preventing destructive updates that could destabilize training. Concretely, PPO optimizes the following objective function: JPPO(θ) = ExD,yπθold (x) 1 y (cid:88) t=1 (cid:16) min rt(θ) ˆAt, clip (rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17) (1) Here, denotes prompt sampled from the data distribution D, and = (y1, . . . , yy) are output sequences sampled from the old policy πθold. The term rt(θ) = πθ(ytx,y<t) πθold (ytx,y<t) is the importance sampling ratio, measuring the change between the new policy πθ and the previous policy πθold at step t. ˆAt represents the estimated advantage, often computed using Generalized Advantage Estimation (GAE) (Schulman et al., 2016). ϵ is hyperparameter controlling the clipping range. Group Relative Policy Optimization (GRPO) GRPO (Shao et al., 2024) is critic-free reinforcement learning method that eliminates explicit value function estimation. Instead of learning separate critic model, GRPO estimates advantages by normalizing rewards within groups of sampled responses for each prompt. For prompt with responses and rewards {ri}G i=1, the groupnormalized advantage is: ˆAi,t = ri mean({ri}G std({ri}G i=1) i=1) (2) The GRPO objective integrates this advantage estimation into clipped policy gradient framework: JGRPO(θ) = 1 (cid:88) i=1 1 yi yi (cid:88) t=1 min (cid:0)ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:1) (3) where ri,t(θ) = πθ(yi,tx,yi,<t) in sparse reward settings while avoiding critic approximation errors. πθold (yi,tx,yi,<t) is the importance ratio. This approach preserves gradient reliability"
        },
        {
            "title": "Technical Report",
            "content": "Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) DAPO (Yu et al., 2025) is an RL approach tailored for reasoning tasks recently. It optimizes the objective as follows: JDAPO(θ) = 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) (cid:16) min i=1 t= ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵlow, 1 + ϵhigh) ˆAi,t (cid:17) (4) DAPOs main innovations lie in its decoupled clipping ranges (1 ϵlow, 1 + ϵhigh), which allow asymmetric policy updates to encourage exploration, dynamic sample filtering that discards batches where all responses share identical correctness, and token-level loss aggregation with reward shaping to handle variations in response lengths."
        },
        {
            "title": "2.2 POLICY ENTROPY IN REINFORCEMENT LEARNING",
            "content": "Policy entropy measures the uncertainty of policy and reflects the balance between exploration and exploitation in reinforcement learning (Ahmed et al., 2019). high-entropy policy encourages diverse outputs and exploration of the action space, whereas low-entropy policy favors exploiting the currently learned behavior. For policy model πθ and dataset of prompts D, the token-level entropy is: H(πθ, D) = ExD,yπθ(x) 1 y (cid:88) t=1 log πθ(yty<t, x) (5) major challenge in RL is entropy collapse, in which the policy distribution becomes overly concentrated, resulting in premature convergence, reduced output diversity, and degraded task performance (Cui et al., 2025b). One common approach to mitigate this issue is to introduce entropy regularization into the policy objective (Haarnoja et al., 2017; 2018). Specifically, in the context of policy gradient methods, the objective with an entropy regularization term can be written as: (θ) = ExD,yπθ(x) (cid:88) ˆAt log πθ(yty<t, x) + α H(πθ(x)) (6) t=1 where α is the entropy regularization coefficient that controls the relative strength of the entropy term. larger α encourages exploration by maintaining higher policy entropy, while smaller α focuses more on exploitation. In practice, additional strategies such as techniques like the cliphigher method in DAPO (Yu et al., 2025) can also prevent premature entropy collapse and improve the performance of the policy."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 IMPACT OF CLIPPED-TOKEN GRADIENTS ON ENTROPY DYNAMICS In policy optimization-based reinforcement learning, PPO and its variants typically employ clipping operation to constrain the magnitude of policy updates, aiming to stabilize training. Specifically, when tokens importance sampling ratio exceeds 1 + ϵ with positive advantage, or falls below 1 ϵ with negative advantage, the corresponding gradient is clipped. While this mechanism effectively prevents overly aggressive updates, it introduces new issue: policy entropy often becomes unstable during training, typically manifesting as either entropy collapse or entropy explosion. Some existing methods attempt to alleviate this issue by expanding the clipping interval. For example, DAPOs clip-higher strategy extends the upper bound from 1 + ϵ to 1 + ϵh, incorporating some tokens originally outside the clip interval that contribute higher entropy. This approach primarily addresses entropy collapse, suggesting that gradients from tokens outside the clipping interval play critical role in controlling the dynamics of policy entropy."
        },
        {
            "title": "Technical Report",
            "content": "From theoretical perspective (Cui et al., 2025a), the change in policy entropy can be approximated as follows, with the detailed proof provided in Appendix A.2. H(πk+1 θ y<t, x) H(πk θ y<t, x) η Covyπk θ (x) (cid:16) log πk θ (yty<t, x), πk θ (yty<t, x) ˆAt (cid:17) (7) where η denotes the learning rate. This expression shows that the evolution of policy entropy is governed by the covariance between log πk θ (yty<t, x) and πk θ (yty<t, x) ˆAt. Further analysis indicates that tokens lying outside the clipping interval are predominantly lowprobability tokens, as illustrated in Figure 1. Considering their interaction with the advantage function, the effect of these out-of-clip gradients on entropy dynamics can be described more precisely: For PA&LP tokens, their gradients would encourage the model to explore new possibilities, reducing the covariance and slowing the decrease of entropy. For NA&LP tokens, their gradients would reinforce high-probability tokens, increasing the covariance and accelerating policy convergence, which leads to reduction in entropy. These observations demonstrate that gradients from out-of-clip tokens are far from negligible; they directly influence the evolution of policy entropy. Appropriately incorporating and regulating outof-clip gradients enables dynamic control of entropy, guiding the model to achieve more effective balance between exploration and exploitation at different training stages. 3.2 GRADIENT-PRESERVING CLIPPING POLICY OPTIMIZATION Based on the preceding analysis, we propose that incorporating gradients from tokens outside the clipping interval can effectively control policy entropy. To this end, we introduce the GradientPreserving Clipping Policy Optimization (CE-GPPO) algorithm. The core idea of CE-GPPO is to preserve the gradients of tokens outside the clipping interval and adjust their magnitudes while ensuring stable policy updates, enabling explicit regulation of entropy. Specifically, CE-GPPO decouples the forward and backward passes by introducing stop gradient operation, allowing gradient updates to no longer be strictly constrained by the original clipping interval. The objective function is defined as: JCE-GPPO(θ) = 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) i= t=1 ℓ(i) , where ℓ(i) = β1 β2 1 ϵ sg(δ) 1 + ϵ sg(δ) δ ˆAi,t, δ ˆAi,t, if δ < 1 ϵ and ˆAi,t < 0, (8) δ ˆAi,t, if δ > 1 + ϵ and ˆAi,t > 0, otherwise (i.e., δ ˆAi,t clip(δ, 1 ϵ, 1 + ϵ) ˆAi,t). Here, δ = ri,t denotes the importance sampling ratio, and sg() represents the stop gradient operation. The coefficients β1 and β2 control the scaling of gradients outside the left and right clipping boundaries, respectively. It is worth noting that when β1 = β2 = 0, CE-GPPO is equivalent to PPO. Through this design, CE-GPPO effectively incorporates the gradients of originally clipped PA&LP and NA&LP tokens into the update. Specifically, larger β1 amplifies PA&LP gradients, slowing the decline of policy entropy and promoting exploration, whereas larger β2 amplifies NA&LP gradients, accelerating policy convergence and reducing entropy to facilitate exploitation. By adjusting these coefficients, CE-GPPO can flexibly modulate the dynamics of policy entropy, balancing exploration and exploitation during training. 3.3 ENSURING STABLE OPTIMIZATION IN CE-GPPO Although CE-GPPO allows the incorporation of token gradients outside the clipping interval in order to regulate the dynamic evolution of policy entropy, the overall optimization process remains stable."
        },
        {
            "title": "Technical Report",
            "content": "This property can be understood by analyzing its gradient formulation. The gradient of CE-GPPO is given as follows, with detailed derivation provided in Appendix A.3. θJCE-GPPO(θ) = 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) i=1 t=1 Fi,t(θ) θ log πθ(yi,ty<t, x) ˆAi,t (9) The term Fi,t(θ) is defined as: Fi,t(θ) = β1 (1 ϵ), β2 (1 + ϵ), δ, if δ < 1 ϵ and ˆAi,t < 0, if δ > 1 + ϵ and ˆAi,t > 0, otherwise (i.e., δ ˆAi,t clip(δ, 1 ϵ, 1 + ϵ) ˆAi,t). (10) It can be observed that when the importance sampling ratio δ falls outside the clipping interval, CEGPPO does not amplify the gradient without bound. Instead, it restricts the update to β1 (1 ϵ) or β2 (1 + ϵ). Since β1 and β2 are typically close to 1, the overall gradient magnitude remains within reasonable range. Moreover, other terms are structurally identical to those in the standard PPO gradient, and therefore contribute to optimization stability in the same way as the original PPO. In summary, while CE-GPPO introduces gradient signals beyond the clipping interval, it still preserves stability comparable to that of standard PPO, ensuring controllable optimization process."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENTAL SETUP Datasets Our RL training dataset is KlearReasoner-MathSub-30K (Su et al., 2025a)1, which consists of approximately 30k samples collected from several high-quality sources, including SkyworkOR1 (He et al., 2025), Acereason (Chen et al., 2025), NuminaMath (LI et al., 2024), and DeepScaleR (Luo et al., 2025). To mitigate potential data contamination, the dataset has been further processed with 9-gram deduplication against the evaluation benchmarks. Training We conducted training with CE-GPPO on two model sizes, DeepSeek-R1-Distill-Qwen1.5B2 and DeepSeek-R1-Distill-Qwen-7B3 . The maximum training sequence length was set to 16k, and the learning rate was fixed at 1 106. For each prompt, we generated 8 rollouts. The clipping parameter ϵ was fixed at 0.2 with symmetric upper and lower bounds. Following He et al. (2025), no KL loss term was included in the objective. Each run was trained for up to 1000 steps, corresponding to approximately 10 epochs. The experimental configurations for the baseline methods are provided in Appendix A.1. Evaluation To evaluate the effectiveness of our approach, we conducted experiments on multiple mathematical benchmarks, including AIME24, AIME25, HMMT25, MATH500 (Lightman et al., 2024), and AMC23. For all benchmarks except MATH500, we reported the avg@32 score, while for MATH500 we reported the avg@4 score. During evaluation, inference on AIME24 and AIME25 was performed with maximum sequence length of 32k, whereas for the other benchmarks we used maximum sequence length of 16k. Following prior works (Yang et al., 2024), we extracted the predicted answers from the boxed{} in the models responses. 1https://huggingface.co/datasets/Kwai-Klear/KlearReasoner-MathSub-30K 2https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B 3https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        },
        {
            "title": "Technical Report",
            "content": "Method AIME24 AIME25 HMMT25 MATH500 AMC23 Avg. DS-R1-Distill-Qwen-1.5B + GRPO + DAPO + CE-GPPO DS-R1-Distill-Qwen-7B + GRPO + DAPO + CE-GPPO 29.2 33.4 40.0 42.0 54.5 55.3 59.7 66.0 24.1 28.1 28.4 33.9 39.1 40.3 48.7 51.4 13.1 16.6 19.17 21.6 26.2 24.5 25.6 30. 86.0 88.3 90.0 91.0 93.6 93.7 95.1 95.6 73.7 79.3 84.4 85.9 90.6 88.8 93.4 93.8 45.2 49.1 52.4 54.9 60.8 60.5 64.5 67. Table 1: Performance comparison of CE-GPPO and baseline methods on multiple benchmarks across different models. DS-R1-Distill-Qwen-1.5B and DS-R1-Distill-Qwen-7B denote the DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models, respectively. DAPO stands for GRPO training with Clip Higher trick. (a) Entropy Dynamics (b) Benchmark Accuracy Figure 2: Based on DeepSeek-R1-Distill-Qwen-7B, comparison of GRPO, DAPO, and GPPO in terms of entropy dynamics and AIME25 benchmark accuracy. 4.2 MAIN RESULTS As shown in Table 1, we report the performance comparison between our proposed CE-GPPO and other baseline methods. DAPO represents GRPO training with the clip-higher strategy, where we set ϵhigh to 0.28 with reference to (Yu et al., 2025). It can be observed that CE-GPPO consistently outperforms the baselines across different benchmarks, with particularly pronounced gains on more challenging tasks such as AIME25 and HMMT25. Moreover, the advantages of CE-GPPO scale with model size: the 1.5B model achieves 2.5-point improvement over the best baseline, while the 7B model achieves 3-point improvement. This indicates that larger models can more effectively leverage the benefits of CE-GPPO. Furthermore, as shown in Figure 2, we examine the training dynamics of entropy and AIME24 accuracy, and obtain three key observations. First, Native GRPO suffers from entropy collapse, while both DAPO and CE-GPPO alleviate this issue and yield significant improvements. DAPO addresses the collapse by adjusting Clip Higher, whereas CE-GPPO propagates the gradients of clipped PA&LP tokens back in bounded and moderate manner; both approaches mitigate entropy collapse. Second, at the early stage of DAPO training, entropy increases significantly and remains at high level, while CE-GPPO maintains stable entropy throughout training and achieves clear improvements over DAPO. This suggests that DAPO is prone to over-exploration, whereas CE-GPPO resolves this problem by backpropagating the gradients of clipped NA&LP tokens. Third, by appropriately setting the parameters β1 and β2, CE-GPPO achieves balance between exploration and exploitation, leading to more effective optimization."
        },
        {
            "title": "Technical Report",
            "content": "(a) Entropy Dynamics (1.5B) (b) Benchmark Accuracy (1.5B) (c) Entropy Dynamics (7B) (d) Benchmark Accuracy (7B) Figure 3: Entropy dynamics and benchmark accuracy under different β1/β2 configurations."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 IMPACT OF DIFFERENT β HYPERPARAMETERS ON ENTROPY DYNAMICS To empirically validate that CE-GPPO can regulate entropy dynamics through parameter settings, we conducted experiments with different β1/β2 configurations, as shown in Figure 3. We observe that entropy decreases much faster when β1 is large or β2 is small, whereas it decreases much more slowly when β1 is small or β2 is large. This confirms our key finding that the choice of β1 and β2 directly governs the evolution of entropy. The underlying mechanism is that: larger β1 amplifies gradients beyond the left clip boundary (mainly from NA&LP tokens). These gradients strengthen high-probability tokens, accelerating exploitation and thus causing entropy to collapse quickly. larger β2, on the other hand, amplifies gradients beyond the right clip boundary (mainly from PA&LP tokens). These gradients encourage exploration of new tokens, slowing entropy reduction. 5.2 ENTROPY-GUIDED TRAINING DYNAMICS Further experiments reveal the relationship between entropy dynamics and model performance. As shown in Figure 2, when entropy decreases too rapidly, such as in the 1.5B model with the setting β1 = 1, β2 = 0.5, the model performance degrades quickly once entropy falls below certain threshold. In contrast, when entropy remains relatively high and stable in the early stage of training, as in the 1.5B model with the setting β1 = 0.5, β2 = 1, the model avoids premature convergence to suboptimal solutions and continues to improve on benchmarks. However, excessively high entropy does not necessarily lead to better results, and it must be maintained within reasonable range. In the 7B model with the setting β1 = 0, β2 = 1, entropy increases"
        },
        {
            "title": "Technical Report",
            "content": "(a) Entropy Dynamics (b) Benchmark Accuracy Figure 4: Entropy dynamics and benchmark accuracy under different β1/β2 configurations. For β1 = 0/β2 = 1, the setting is maintained consistently across 01000 steps, whereas in the β1 = 0/β2 = 1 β1 = 0.5/β2 = 1 configuration, the transition occurs at step 585. consistently during the early stage of training, yet model performance does not show clear gains. Instead, the configuration β1 = 0.75, β2 = 1, which keeps entropy more stable, achieves the best performance throughout training. Although the entropy dynamics differ between the 1.5B and 7B models, both reveal consistent conclusion: maintaining relatively high and stable entropy is generally beneficial for sustained performance improvement during training. Building on this observation, we further investigate the role of entropy at different stages of training. The results show that maintaining high and stable entropy in the early stage facilitates exploration, while in the later stages, gradual convergence of entropy within reasonable range helps stabilize performance and consolidate the learned knowledge. For example, in Figure 3, under the setting β1 = 0, β2 = 1, entropy shows slight upward trend in the late stage. By switching from β1 = 0, β2 = 1 to β1 = 0.5, β2 = 1 in the middle stage, entropy stabilizes and decreases in the later stage, which brings further performance improvements. This finding is consistent with the conclusion of Kimi K2 (Bai et al., 2025) that exploration should be encouraged in the early stage and exploitation should be emphasized later. Our method is able to regulate the dynamics of entropy to achieve such balance between exploration and exploitation, unlocking greater model performance. 5.3 TRAINING STABILITY ANALYSIS OF CE-GPPO To achieve finer control over policy entropy, CE-GPPO introduces the gradients of tokens outside the clipping interval on top of GRPO. Although this operation to some extent relaxes the trust region constraint of standard PPO, we theoretically prove that the additional gradients incorporated by CE-GPPO are stable and do not lead to training collapse. To further validate this conclusion, we compare GRPO and CE-GPPO in terms of the KL divergence between the old policy model and the policy model during training, as well as the variation in gradient norms, as shown in Figure 5. The results show that CE-GPPO maintains stable trend in both metrics throughout training, without abrupt fluctuations or abnormal values beyond reasonable ranges. These findings provide empirical evidence that CE-GPPO backpropagates gradients of out-of-trust-region tokens in mild and bounded manner, preventing the policy model from drifting too far from the old policy model, and ensuring that CE-GPPO training remains stable. 5.4 COMPARISON WITH OTHER RL ALGORITHMS In this section, we compare CE-GPPO with broader set of reinforcement learning algorithms, including CISPO (Zheng et al., 2025), and GSPO (Zheng et al., 2025). The experimental configurations are provided in Appendix A.1. As summarized in Table 2, we evaluate each model on several mathematical reasoning benchmarks. CE-GPPO achieves the best performance on 4 out of"
        },
        {
            "title": "Technical Report",
            "content": "(a) KL Divergence (b) Gradient Norm Figure 5: Comparison of KL divergence and gradient norm dynamics between GRPO and CE-GPPO during training. Method AIME24 AIME25 HMMT25 MATH500 AMC23 Avg. DS-R1-Distill-Qwen-1.5B + CISPO + GSPO + CE-GPPO 29.2 32.9 42.5 42.0 24.1 25.1 33.6 33.9 13.1 13.2 19.0 21.6 86.0 85.8 90.3 91.0 73.7 80.9 85.9 85. 45.2 47.6 54.3 54.9 Table 2: Comparison of CE-GPPO, CISPO and GSPO in mathematical RL training. 5 datasets, demonstrating significant improvements over the baseline methods and underscoring the effectiveness of the proposed approach. We observe that CISPO exhibits model collapse during training: in later stages, its performance declines sharply alongside rapid drop in entropy. CISPO also retains gradients from all tokens while applying constraints on the gradient magnitudes. This suggests that constraining gradient norms alone is insufficient to ensure training stability when gradients from all tokens are retained. In contrast, CE-GPPO shows steady improvement throughout training. We identify two main reasons for this robustness: CE-GPPO assigns smaller weight β1 to the gradients of NA&LP tokens, while assigning larger weight β2 to the gradients of PA&LP tokens. Compared with CISPO, it achieves better balance between exploration and exploitation. Compared with CISPO, CE-GPPO inherits the pessimistic update mechanism of PPO. Specifically, when δ < 1 ϵl and ˆAi,t > 0, CE-GPPO sets Fj,t(θ) = δ (where 0 < δ < 1 ϵl), suppressing overly optimistic improvements and thus updating more conservatively; whereas CISPO uses larger update magnitude 1 ϵl. When δ > 1 + ϵl and ˆAi,t < 0, CE-GPPO still sets Fj,t(θ) = δ (with δ > ϵh), fully trusting negative feedback without suppression, while CISPO applies smaller update magnitude 1 + ϵl. The pessimistic update strategy of CE-GPPO avoids excessive optimism while fully incorporating negative feedback, enhancing algorithmic stability and preventing policy collapse. Compared with GSPO, CE-GPPO shows clear advantages on three benchmarksAIME2025, HMMT25, and MATH500and also achieves higher average score overall. We attribute this improvement to CE-GPPOs ability to preserve gradients across more tokens. During GSPO training, nearly 15% of tokens are clipped and do not contribute to gradient updates. Thus, CE-GPPO not only achieves better performance but also higher token utilization efficiency. 5.5 COMPARISON WITH EXISTING ENTROPY COLLAPSE MITIGATION METHODS In this section, we compare CE-GPPO with other methods designed to mitigate entropy collapse, focusing on the relationship between entropy dynamics and model performance. As shown in Figure"
        },
        {
            "title": "Technical Report",
            "content": "(a) Entropy Dynamics (b) Benchmark Accuracy Figure 6: Comparison of CE-GPPO with other entropy collapse mitigation strategies. Native GRPO denotes the baseline without any mitigation strategy, while α = 0.001 and α = 0.003 indicate the addition of an entropy loss term to the Native GRPO baseline, where α represents the entropy loss coefficient. DAPO refers to applying the Clip Higher strategy on top of the Native GRPO baseline. 6, the Native GRPO without any entropy collapse mitigation strategy suffers from severe entropy collapse during training, with entropy eventually converging to around 0.06. Both the clip higher strategy adopted by DAPO and the traditional entropy regularization method can slow down entropy decay in reinforcement learning to some extent. However, we find that entropy regularization is highly sensitive to the choice of coefficient. When the entropy loss coefficient is set to α = 0.001, the collapse slows but entropy still converges to around 0.06, leading to noticeable performance drop on the AIME25 benchmark. When the coefficient is increased to α = 0.003, entropy explosion occurs, accompanied by significant degradation in performance. In contrast, both the clip higher strategy in DAPO and our CE-GPPO method effectively suppress entropy collapse, maintaining entropy at higher level while enabling steady improvements in model performance during training. Notably, DAPO exhibits an entropy rebound in the later stage (starting around step 300), which further suggests potential issue of over-exploration. In contrast, CE-GPPO shows slow but stable decline and achieves superior performance on the AIME25 benchmark. This indicates that CE-GPPO, when equipped with well-tuned β1 and β2, better balances exploration and exploitation."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we investigate the intrinsic mechanisms that drive entropy dynamics in reinforcement learning for LLMs and identify clipped low-probability tokens as critical factors in balancing exploration and exploitation. Based on this understanding, we propose CE-GPPO, novel algorithm that incorporates gradient signals from out-of-clip tokens in controlled and theoretically grounded manner. By introducing stop-gradient operation and tunable scaling, CE-GPPO effectively preserves training stability while enabling fine-grained control of policy entropy. Extensive experiments demonstrate that CE-GPPO prevents entropy collapse, avoids excessive exploration, and achieves superior performance compared to strong baselines including GRPO, DAPO, CISPO, and GSPO."
        },
        {
            "title": "REFERENCES",
            "content": "Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 151160. PMLR, 2019. URL http://proceedings.mlr.press/v97/ ahmed19a.html."
        },
        {
            "title": "Technical Report",
            "content": "Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, and Haiming Wang. Kimi K2: open agentic intelligence. CoRR, abs/2507.20534, 2025. doi: 10.48550/ARXIV.2507.20534. URL https://doi.org/ 10.48550/arXiv.2507.20534. Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. CoRR, abs/2505.16400, 2025. doi: 10.48550/ARXIV.2505.16400. URL https://doi.org/10.48550/arXiv.2505.16400. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. CoRR, abs/2506.14758, 2025. doi: 10.48550/ARXIV.2506.14758. URL https://doi.org/10.48550/arXiv.2506. 14758. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Hao-Si Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models. ArXiv, abs/2505.22617, 2025a. URL https://api. semanticscholar.org/CorpusID:278959427. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models. CoRR, abs/2505.22617, 2025b. doi: 10.48550/ARXIV.2505.22617. URL https://doi.org/10.48550/arXiv.2505.22617. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10. 48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with In Doina Precup and Yee Whye Teh (eds.), Proceedings of the deep energy-based policies. 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 13521361. PMLR, 2017. URL http://proceedings.mlr.press/v70/haarnoja17a.html."
        },
        {
            "title": "Technical Report",
            "content": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with stochastic actor. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 18561865. PMLR, 2018. URL http: //proceedings.mlr.press/v80/haarnoja18b.html. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner 1 technical report. CoRR, abs/2505.22312, 2025. doi: 10.48550/ARXIV.2505.22312. URL https://doi.org/10.48550/arXiv. 2505.22312. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10. 48550/ARXIV.2411.15124. URL https://doi.org/10.48550/arXiv.2411.15124. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan In The Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= v8L0pN6EOi. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html. John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http: //arxiv.org/abs/1506.02438. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402.03300."
        },
        {
            "title": "Technical Report",
            "content": "Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Fuzheng Zhang, Kun Gai, and Guorui Zhou. Klear-reasoner: Advancing reasoning capability via gradient-preserving clipping policy optimization. CoRR, abs/2508.07629, 2025a. doi: 10.48550/ ARXIV.2508.07629. URL https://doi.org/10.48550/arXiv.2508.07629. Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, and Guorui Zhou. Klear-reasoner: Advancing reasoning capability via gradient-preserving clipping policy optimization. arXiv preprint arXiv:2508.07629, 2025b. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an opensource LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. doi: 10. 48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. CoRR, abs/2507.18071, 2025. doi: 10.48550/ARXIV.2507.18071. URL https://doi.org/10.48550/arXiv.2507.18071."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 EXPERIMENTAL SETUP FOR OTHER BASELINE RL METHODS GRPO is reinforcement learning algorithm designed to fine-tune LLMs by optimizing policies through group-based comparisons. Following (Shao et al., 2024), we set the clipping parameter ϵ for the upper and lower bounds to 0.2. DAPO builds upon and refines the GRPO framework, addressing key limitations such as entropy collapse and training instability. Following (Shao et al., 2024), the lower and upper clipping thresholds were set to ϵl = 0.2 and ϵh = 0.28, respectively. CISPO lies in the direct application of clipping mechanism to the Importance Sampling (IS) weights, as opposed to clipping the final policy update (Cui et al., 2025a). We set the clipping parameter ϵ for the upper and lower bounds to 0.2. GSPO is reinforcement learning algorithm developed to enhance the training stability, efficiency, and scalability of LLMs. The core of the algorithm lies in its use of sequence-level importance ratio (Zheng et al., 2025). Following (Zheng et al., 2025), the lower and upper clipping thresholds were set to ϵl = 0.0003 and ϵh = 0.0004. A.2 PROOF OF THE FORMULA FOR POLICY ENTROPY CHANGE A.2.1 PROBLEM SETUP AND ASSUMPTIONS Let πθ(a s) denote stochastic policy parameterized by θ, where represents the state and represents an action. The policy entropy at state is defined as: H(πθ s) = (cid:88) aA πθ(a s) log πθ(a s) (11) We make the following standard assumptions:"
        },
        {
            "title": "Technical Report",
            "content": "The policy follows tabular softmax parameterization: πθ(a s) = exp(zs,a) aA exp(zs,a) (cid:80) (12) where zs,a are the logits corresponding to state-action pairs. Policy updates are performed using the policy gradient theorem with sufficiently small learning rate η > 0, such that first-order Taylor approximations remain valid. The advantage function ˆA(s, a) is baseline-centered, satisfying: This is common practice in policy gradient methods where advantages are computed as Q(s, a) (s). Eaπk [ ˆA(s, a)] = 0 (13) Our goal is to derive an approximation for the entropy change between successive policy updates: = H(πk+1 θ s) H(πk θ s) (14) A.2.2 DERIVATION Step 1: First-Order Taylor Expansion of Entropy Treating the entropy as function of the logits zs,a at fixed state s, we employ first-order Taylor expansion around the current parameters zk: H(πk+1 s) H(πk s) + (cid:88) aA H(πk s) zs,a The entropy change can therefore be approximated as: (zk+1 s,a zk s,a) (cid:88) aA H(πk s) zs,a (zk+1 s,a zk s,a) (15) (16) Step 2: Gradient of Entropy with Respect to Logits We now compute the gradient term H(πks) zs,a . Starting from the entropy definition: H(πk s) = (cid:88) aA πk(a s) log πk(a s) (17) Following standard derivations for the softmax parameterization, we obtain: H(πk s) zs,a = = (cid:88) (cid:88) πk(a s) zs,a (log πk(a s) + 1) πk(a s) (cid:0)1a=a πk (a s)(cid:1) (log πk(a s) + 1) (18) = πk(a s)(log πk(a s) + 1) + πk(a s) π(a s)(log πk(a s) + 1) (cid:88) = πk(a s) (cid:0)log πk(a s) Eaπk [log πk(a s)](cid:1) Step 3: Policy Gradient Update for Logits Under the policy gradient theorem and our baselinecentered advantage assumption, the logit update simplifies to:"
        },
        {
            "title": "Technical Report",
            "content": "s,a zk zk+1 s,a = Eaπθ(s) (cid:20) log πθ(as) zs,a (cid:21) ˆA(s, a) = Eaπθ (cid:88) = (cid:104) (cid:105) (1a=a πθ(as)) ˆA(s, a) πθ(as)(1a=a πθ(as)) ˆA(s, a) = πθ(as) ˆA(s, a) πθ(as) πθ(as) ˆA(s, a) (cid:88) = πθ(as) (cid:16) ˆA(s, a) Eaπθ [ ˆA(s, a)] (cid:17) = ηπk(a s) ˆA(s, a) (19) This follows directly from the policy gradient theorem and the baseline-centered advantage assumption Eaπk [ ˆA(s, a)] = 0. Step 4: Combining the Results Substituting Equations 18 and 19 into the Taylor expansion from Equation 16: = (cid:88) aA (cid:88) aA H(πk s) zs,a (zk+1 s,a zk s,a) (cid:2)πk(a s) (cid:0)log πk(a s) Eaπk [log πk(a s)](cid:1)(cid:3) (cid:104) (cid:105) ηπk(a s) ˆA(s, a) (20) Rewriting this expression as an expectation over the policy: ηEaπk (cid:105) (cid:104)(cid:0)log πk(a s) Eaπk [log πk(a s)](cid:1) πk(a s) ˆA(s, a) (21) Expanding the expectation: η (cid:16) Eaπk (cid:104) (cid:105) log πk(a s)πk(a s) ˆA(s, a) Eaπk [log πk(a s)]Eaπk (cid:104) πk(a s) ˆA(s, a) (cid:105)(cid:17) By the baseline-centered advantage assumption, the second expectation term vanishes: Eaπk (cid:105) (cid:104) πk(a s) ˆA(s, a) = Eaπk (cid:105) (cid:104) ˆA(s, a) = 0 Thus, the expression simplifies to: (22) (23) η (cid:16) Eaπk (cid:104) (cid:105) log πk(a s)πk(a s) ˆA(s, a) Eaπk [log πk(a s)] (cid:17) (24) Recognizing the remaining expression as covariance: ηCovaπk (cid:16) (cid:17) log πk(a s), πk(a s) ˆA(s, a) (25) This completes the derivation of the desired formula."
        },
        {
            "title": "Technical Report",
            "content": "A.3 PROOF OF THE GRADIENT OF CE-GPPO OBJECTIVE In this section, we derive the gradient of the proposed CE-GPPO loss function. Recall that the objective is defined as JCE-GPPO(θ) = 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) i=1 t=1 ℓ(i) , where ℓ(i) = β1 β2 1 ϵ sg(δ) 1 + ϵ sg(δ) δ ˆAi,t, δ ˆAi,t, if δ < 1 ϵ and ˆAi,t < 0, (26) δ ˆAi,t, if δ > 1 + ϵ and ˆAi,t > 0, otherwise. Here, δ is the importance sampling ratio between the updated and reference policies: δ = πθ(yi,t y<t, x) πθold(yi,t y<t, x) (27) Note that sg() denotes the stop-gradient operator, ensuring that the scaling terms (1 ϵ)/ sg(δ) and (1 + ϵ)/ sg(δ) do not propagate gradients through δ. Taking the gradient of JCE-GPPO(θ), we obtain θJCE-GPPO(θ) = 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) i=1 t= θℓ(i) We now consider the three cases in the definition of ℓ(i): Case 1: δ < 1 ϵ and ˆAi,t < 0. ℓ(i) = β1 1 ϵ sg(δ) δ ˆAi,t Since sg(δ) is treated as constant, the gradient only flows through δ: (28) (29) θℓ(i) = β = β1 (1 ϵ) πθ(yi,ty<t,x) πθold (yi,ty<t,x) (1 ϵ) πθ(yi,ty<t,x) πθold (yi,ty<t,x) θπθ(yi,t y<t, x) πθold(yi,t y<t, x) ˆAi,t πθ(yi,t y<t, x) θ log πθ(yi,t y<t, x) πθold(yi,t y<t, x) ˆAi,t (30) = β1(1 ϵ) ˆAi,t θ log πθ(yi,t y<t, x) Case 2: δ > 1 + ϵ and ˆAi,t > 0. Similarly, Case 3: Otherwise. Thus, ℓ(i) = β2 1 + ϵ sg(δ) δ ˆAi,t θℓ(i) = β2(1 + ϵ) ˆAi,t θ log πθ(yi,t y<t, x) ℓ(i) = δ ˆAi,t θℓ(i) = δ ˆAi,t θ log πθ(yi,t y<t, x) 17 (31) (32) (33) (34)"
        },
        {
            "title": "Technical Report",
            "content": "Combining the three cases, we may summarize the gradient as: θJCE-GPPO(θ) = 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) i=1 t=1 Fi,t(θ) θ log πθ(yi,t y<t, x) ˆAi,t , where the weighting factor Fi,t(θ) is defined as Fi,t(θ) = β1 (1 ϵ), β2 (1 + ϵ), if δ < 1 ϵ and ˆAi,t < 0, if δ > 1 + ϵ and ˆAi,t > 0, δ, otherwise. (35) (36) This completes the proof."
        }
    ],
    "affiliations": [
        "Independent",
        "Klear Team, Kuaishou Technology"
    ]
}