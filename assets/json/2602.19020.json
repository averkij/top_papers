{
    "paper_title": "Learning to Detect Language Model Training Data via Active Reconstruction",
    "authors": [
        "Junjie Oscar Yin",
        "John X. Morris",
        "Vitaly Shmatikov",
        "Sewon Min",
        "Hannaneh Hajishirzi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce \\textbf{Active Data Reconstruction Attack} (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are \\textit{more reconstructible} than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, \\textsc{ADRA} and its adaptive variant \\textsc{ADRA+}, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection."
        },
        {
            "title": "Start",
            "content": "Junjie Oscar Yin 1 John X. Morris 2 Vitaly Shmatikov 2 Sewon Min 3 4 Hannaneh Hajishirzi 1 4 6 2 0 2 2 2 ] . [ 1 0 2 0 9 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Detecting LLM training data is generally framed as membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), family of MIA that actively induces model to reconstruct given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, posttraining, and distillation data, with an average improvement of 10.7% over the previous runner-up. In particular, ADRA+ improves over Min-K%++ by 18.8% on BookMIA for pre-training detection and by 7.6% on AIME for post-training detection. 1. Introduction Modern language models are known to memorize and regurgitate training data, raising concerns about copyright, privacy, and data contamination (Carlini et al., 2021; Ahmed et al., 2026). Membership inference attacks (MIAs) are methods that detect whether given input was present in the training data (Shokri et al., 2017), and have shown some success when applied to LLMs (Shi et al., 2023). 1University of Washington 2Cornell University 3UC Berkeley 4Allen Institute for Artificial Intelligence. Correspondence to: Junjie Oscar Yin <osey@cs.washington.edu>. Preprint. February 24, 2026. Figure 1. Active Data Reconstruction Attack. Language model generates reconstructions from candidate prefix and is rewarded via contrastive objective. Members become easier to reconstruct than non-members over RL training, improving MIA performance. Prior MIAs query the target model without modifying its weights, passively extracting signals exposed through the output layersuch as loss (Shokri et al., 2017; Yeom et al., 2018; Carlini et al., 2021), log-probabilities (Shi et al., 2023; Zhang et al., 2024b), or text generations (Hallinan et al., 2025). We hypothesize that model weights encode latent membership signal that passive methods fail to reliably expose. If such signal exists, it could be actively elicited through model weight updates. We introduce Active Data Reconstruction Attack (ADRA), family of MIAs that uses Reinforcement Learning (RL) with reconstruction rewards to elicit latent membership signals. Because model weights encode traces of training data, RL training exploits the difference in member and non-member reconstructability for detection. Our approach is motivated by recent findings that RL sharpens behaviors already encoded in weights (Yue et al., 2025; Wang et al., 2025) while transferring minimal new information (Schulman & Lab, 2025), suggesting RL is well-suited to surface latent reconstruction signal. Learning to Detect Language Model Training Data via Active Reconstruction Figure 1 illustrates our attack: given candidate prefix, the model generates continuations (reconstructions) and is rewarded for producing generations similar to the candidate suffix. To effectively leverage RL for MIA, we design reconstruction metrics and contrastive rewards that match generations against pool containing the true suffix and negative distractors. This yields two algorithms: ADRA, which rewards the best match among the pool, and ADRA+, which adaptively adjusts how often the true suffix should be included during matching based on prior derived from loss-based scores. Empirically, we run comprehensive experiments spanning pre-training, post-training, and distillation settings across 6 open-weight LLMs. To better match frontier models knowledge cutoffs and heavier post-training, we construct 6 new MIA datasets and evaluate on 2 established benchmarks. We find that ADRA and ADRA+ consistently outperform prior MIAs across all settings, with an average improvement of 10.7% over the previous runner-up. For pre-training data detection, on the challenging WikiMIA2024 Hard benchmarkwhere most baselines hover below randomADRA+ reaches 60.6% AUROC, outperforming Min-K%++ by 10%. For post-training, under controlled contamination on AIME, ADRA+ achieves 85.9% AUROC, improving over N-Sampling by 13.2% and Min-K%++ by 7.6%. For distillation, on prefix-matched distillation traces, ADRA attains near-perfect membership inference, reaching 98.4% under Deepseek-R1 distillation. Ablations confirm that RL-based optimization, reconstruction-reward design, and the contrastive objective are critical to these gains. Our results suggest that model weights encode more about training data than fixed outputs reveal. RL training help surface this latent information, enabling stronger reconstruction and membership inference. Codebase, models, and datasets are available at https://github.com/oseyosey/MIA-RL. 2. Background Membership inference (Shokri et al., 2017) aims to determine if datapoint was present in the training data of model θ. We focus on the specific problem of detecting training data from LLMs, which is known to be difficult (Duan et al., 2024). Notation. Let pθ be the target LLM with parameters θ, trained on dataset Dtrain. Given candidate document Dcand, write = (c, y) where is the observed prefix and is the held-out suffix. Membership inference is the binary classification problem of determining whether was drawn from Dtrain or not. Define scoring function (x; θ) that maps each candidate to scalar membership score; MIAs predict member iff (x; θ) ϵ for threshold ϵ. Loss-based Methods. Most prior MIAs score candidate by the models token-level log-probabilities on x, with the intuition that models are likely to have lower loss on sequences that have been seen during training: Loss (Yeom et al., 2018): (x) = L(x; θ). R-Loss (Carlini et al., 2021): calibrate by reference model θref , = L(x; θ) L(x; θref ). Zlib Entropy (Carlini et al., 2021): normalize by the zlib compression size of x, = L(x; θ)/zlib(x). Min-K% (Shi et al., 2023): average over the k% lowestimin k(x) log p(xi likelihood tokens, = 1 x<i; θ). k(x) (cid:80) Min-K%++ (Zhang et al., 2024b): z-score each nexttoken log-prob by the vocab mean/std under θ, then log p(xix<i;θ)µx<i average the k% smallest: si = , σx<i = 1 (cid:80) imin k(x) si. k(x) Reconstruction-based Methods. Hallinan et al. (2025) propose sampling completions and scoring membership via n-gram metricseffectively reconstruction-based MIA. Formally, given fixed prompt and prefix c, we sample completions {ˆy}N i=1 from pθ with nucleus sampling: {ˆy(i)}N i=1 pθ( q, c). (1) We score each candidate by comparing each completion ˆy(i) to the ground truth suffix using similarity function sim(ˆy(i), y): (x) = 1 (cid:88) i=1 sim(cid:0)ˆy(i), y(cid:1). (2) 3. Active Data Reconstruction Attack The above-mentioned MIAs operate on fixed model weights, querying the target models log-probabilities or generations. They assume that passive querying extracts all available membership signal. Conversely, we ask if training data leaves latent membership signal in models parameters that is not captured by existing MIA methods. If such latent signal exists, we can design an Active MIA that actively elicit the latent signal rather than passive measurements. In this work, we propose ACTIVE DATA RECONSTRUCTION ATTACK (ADRA). We first describe our RL formulation in Section 3.1, then detail our reward design. Specifically, Section 3.2 defines similarity functions that reward reconstruction quality, and Section 3.3 specifies how we shape these rewards via contrastive structure to constrain supervision. Learning to Detect Language Model Training Data via Active Reconstruction Algorithm 1 ACTIVE DATA RECONSTRUCTION ATTACK (single candidate = (c, y); batched in practice). input target LM pθ, (optional) prompt q, candidate = (c, y), threshold ϵ, RL steps K, samples per step , reward mode MODE {MATCH, ADAPT}, similarity sim(, ). 1: Initialize θ0 θ 2: for = 0 to 1 do 3: 4: Sample rollouts ˆy(1:N ) pθk ( q, c) Compute rewards r(i) rMODE(ˆy(i), x) {rMATCH / rADAPT} Update θk+1 using GRPO/PPO (KL/clipping) 5: 6: end for 7: Evaluate S(x) by computing sim(cid:0)ˆy(i), x(cid:1) over new samoutput membership score S(x), reconstructions ˆy1:N θK , and ples from pθK ( c) decision I{S(x) ϵ}. 3.1. RL-induced Data Reconstruction Recent work shows that RL sharpens behaviors already encoded in weights (Yue et al., 2025; Wang et al., 2025) while transferring minimal new information (Schulman & Lab, 2025), suggesting RL could be well-suited to elicit latent membership signals. We leverage this insight by explicitly rewarding the model for reconstructing candidate data. Concretely, we fine-tune policy initialized from target model pθ with on-policy RL, reusing the similarity function from Equation (2) as the reward: r(i) sim(ˆy(i), y), {1, ..., }. (3) For candidate x, the policy generates completions ˆy given prefix (q, c) and receives scalar reward r(ˆy, x) = sim(ˆy, y). We update the policy by maximizing the expected return: Jx(θ) = Eˆypθ(q,c) (cid:2)r(ˆy, x)(cid:3). (4) We estimate Equation (4) with Monte Carlo average over the samples above. In practice we apply Group Relative Policy Optimization (GRPO) (Shao et al., 2024b), which optimizes clipped/KL-regularized surrogate of Equation (4). After on-policy RL updates, we obtain θK. To evaluate x, we resample completions ˆy(1:M ) pθK ( c) and re-aggregate their similarities into membership score. 3.2. Data Reconstruction Metrics We instantiate reconstruction rewards with lexical similarity metrics sim(ˆy, y) that measure the fraction of the reference suffix reconstructed by models completion ˆy. Since RL learning algorithms suffer from sparse rewards (Sutton 3 Table 1. Lexical reconstruction rewards used for sim(ˆy, y). Metric Token set similarity Definition stok(ˆy, y) = Longest common sequence slcs(ˆy, y) = N-gram set coverage sng(ˆy, y) = (ˆy) (y) (y) LCS(ˆy, y) (cid:12)N[Lmin,Lmax](ˆy) N[Lmin,Lmax](y)(cid:12) (cid:12) (cid:12) (cid:12)N[Lmin,Lmax](y)(cid:12) (cid:12) (cid:12) & Barto, 2018), we propose variety of surrogate dense rewards that yields sufficient learning signals. All functions sim share the property that if = y, sim(y, y) = 1. Definitions and properties are detailed in Table 1. Token Set Similarity (Morris et al., 2025b; Shao et al., 2024a). simtok(y, y) measures the fraction of unique reference tokens in that appear in y, where () denote the set of unique tokens. This metric emphasizes vocabulary recall and penalizes repetition via set overlap. Longest Common Subsequence (Saggion et al., 2002; Lin, 2004). simlcs(y, y) uses the length of the longest common subsequence LCS(y, y), normalized by y. This rewards correct content in the correct order. N-gram Set Coverage (Hallinan et al., 2025). simng(y, y) measures the fraction of reference n-grams that appear in y, where [Lmin, Lmax]() denotes the set of contiguous n-grams with [Lmin, Lmax]. Treating n-grams as set to discourage reward hacking via repetition. Following previous works, we fix Lmin = 3 and set Lmax = LCS(y, y). 3.3. Contrastive Rewards Directly maximizing the reconstruction reward in Equation (3) can introduce overly strong supervision, obscuring the latent membership signal we aim to elicit. To constrain supervision, we use contrastive reward: each rollout ˆy is against pool of reference suffixes {gj}K j=0 containing the true suffix g0 = and distractors sampled from other candidates, with the best match as reward. Contrastive rewards provide relative supervision (ground truth vs. negative distractors), limiting what the policy can learn beyond what model weights already encode. We propose two contrastive reward variants that form our methods: matching (ADRA) and adaptive matching (ADRA+). Matching (ADRA). For each candidate = (c, y), set g0 := and sample additional distractor suffixes {gj}K j=1 i.i.d. from Dcand. Given rollout ˆy pθ( q, c), we compute similarities sim(ˆy, gj) [0, 1], = 0, . . . , K, using the reconstruction metrics from Section 3.2. The matching reward then scores ˆy against all suffixes and takes Learning to Detect Language Model Training Data via Active Reconstruction Table 2. Datasets, models, cutoffs, and maximum sequence lengths for pre-training, post-training, and distillation settings. Newly constructed MIA datasets are marked with . Cutoff dates marked with * are estimated. Task Dataset Dataset Cutoff Model Model Cutoff Max Tokens Pre-training BookMIA WikiMIA2024 Hard Dolma3 (Olmo3 Mix) Post-training AIME Olympia Math Tulu3 Mix Distillation S1 / S1.1 2023 2024 2025 2024 2025 2024 Llama2-7B Qwen2-7B Olmo3-7B Tulu2-7B Tulu2-7B Tulu3-8B 2022* 2023* 2024 2023 2023 2024 Qwen2.5-7B 2024* 1024 1024 2048 4096 4096 1024 2048 the maximum: 4. Experimental Setup rmatch(ˆy, x) = max 0jK sim(ˆy, gj). (5) Because member data is seen during models training, their rollouts are more likely to match to g0 during matching max0jK sim(ˆy, gj). We term this method ADRA. Adaptive Matching (ADRA+). Matching can be further sharpened by incorporating membership prior p(x) [0, 1]1, derived from the base models log-probabilities (e.g., Min-K% or Min-K%++ scores). For simplicity, we treat all loss-based variants as p(x) for notation. For each we have the same pool of references {gj}K and scores {simj}K j=0 and define j=0 cunion := max 0jK cdistract := max 1jK sim(ˆy, gj), sim(ˆy, gj). We then sample Bernoulli switch Bernoulli(cid:0)p(x)(cid:1) and define the adaptive matching reward: radapt(ˆy, x) = (cid:40) cunion, cdistract, = 1, = 0. (6) where = 1 matches among {g0} {gj}K j=1 (true suffix + distractors), and = 0 matches among {gj}K j=1 (distractors only). In practice, for fixed (ˆy, x), we compute the expected reward: Ez[radapt(ˆy, x)] = p(x) cunion + (1 p(x)) cdistract. (7) Intuitively, when p(x) is high (likely member), we often include g0, making it easier for RL to reinforce reconstruction of the true suffix. When p(x) is low, we more often match only against distractors, receiving weaker and noisier supervision. We term this method as ADRA+. For both variants, the underlying RL algorithm (GRPO) is unchanged: we replace r(y, x) in (4) with either rmatch or radapt. 1We define p(x) such that larger values indicate higher membership likelihood. As shown in Table 2, we evaluate membership inference on LLMs spanning three training stages: pre-training, posttraining, and distillation. See Section for full details. Pre-training. Following (Hallinan et al., 2025), we assess pre-training data detection on BookMIA and WikiMIA24Hard. BookMIA (Shi et al., 2023) contains 512-word snippet sampled from 100 books, and non-members come from books after 2023. WikiMIA2024-Hard (Hallinan et al., 2025) pairs Wikipedia article versions across date cutoffs and minimizes temporal distribution shift between members and non-members. We use Llama2-7B and Qwen2-7Btheir knowledge cutoff aligns well with the dataset cutoffs. In addition, we evaluate the fully-open language model Olmo3 (Olmo et al., 2025) on their pre-training mix Dolma3. Post-training. Post-training has become growing focus of LLMs development (Guo et al., 2025). We study two settings: (i) Controlled contamination: we simulate testset contamination by SFTing on member examples from AIME and Olympia Math. (ii) Post-training mixtures: we evaluate membership on the open Tulu3 post-training mix on Tulu3 model across domain (Lambert et al., 2024). Distillation. Finally, we study data detection under model distillation. Following Muennighoff et al. (2025), we construct dataset of reasoning traces from Deepseek-R1 and Gemini-2.0-Flash. 4.1. LLM-MIA Dataset Construction Most MIA datasets target pre-training and use cutoffs around 2023 (Shi et al., 2023). To match frontier LLMs with later cutoffs and substantially more post-training data, we develop 6 new LLM-MIA datasets. Dolma3 / Tulu3 Mix: Fully open models enable controlled membership evaluation. We construct pre-training and posttraining MIA datasets from the Dolma3 pre-training mix (Olmo et al., 2025) and Tulu3 SFT mix (Lambert et al., 2024). For Dolma3, we choose arXiv documents as members, and sample non-members from later-dated arXiv paLearning to Detect Language Model Training Data via Active Reconstruction Table 3. Pre-training MIA results on BookMIA, WikiMIA2024 Hard, and Dolma3 arXiv. Bold denotes the top-2 performance for each dataset. Our active MIA methods (ADRA and ADRA+) consistently outperform passive MIAs. Model Type ADRA+ ADRA N-Sampling Loss R-Loss Zlib Min-K% Min-K%++ ACTIVE MIAS PASSIVE MIAS BookMIA Llama2-7B Llama2-7B WikiMIA2024 Hard Qwen2-7B Qwen2-7B Dolma3 arXiv Orig. Para. Orig. Para. Olmo3-7B-Instruct Orig. Olmo3-7B-Instruct Para. 78.4 74.5 60.6 56.6 92.4 90. 76.2 73.0 59.1 58.4 91.8 84.7 73. 68.7 54.3 55.8 81.5 70.4 60.6 57. 45.7 47.4 71.1 69.4 N/A N/A N/A N/A N/A N/A 50.4 40.6 44.5 47.3 42. 43.9 63.3 61.0 45.4 48.3 64.4 57. 59.6 53.7 50.6 48.5 37.4 29.9 Table 4. Pre-training member data reconstruction for original verbatim setting. Bold denotes the best average performance for each dataset. ADRA+ and ADRA consistently outperform the N-Sampling across all metrics. See Section for paraphrased setting results. Model Type Method BookMIA Llama2-7B Orig. ADRA ADRA+ WikiMIA2024 Hard N-Sampling ADRA+ Qwen2-7B Orig. ADRA Dolma3 arXiv Olmo3-7B-Instruct Orig. N-Sampling ADRA+ ADRA N-Sampling Budget (N ) Lexical Jaccard (Best / Avg) Lexical LCS (Best / Avg) Lexical Coverage (Best / Avg) Embedding Cosine (Best / Avg) 32 32 32 32 32 32 32 32 21.9 / 18.5 18.9 / 15.9 19.9 / 15. 20.1 / 14.1 17.3 / 14.8 16.6 / 11.4 15.2 / 12.6 15.9 / 13.0 15.7 / 11. 62 / 54 56 / 48 57 / 46 51 / 33 34 / 27 52 / 69 / 60 69 / 58 59 / 35 5.7 / 2.4 4.2 / 2.0 4.9 / 1. 16.1 / 6.0 7.0 / 4.3 16.7 / 7.6 9.5 / 5.0 9.7 / 4.9 8.1 / 2. 89.0 / 83.9 88.6 / 84.4 89.6 / 82.7 88.4 / 82.3 87.0 / 84.3 87.9 / 81. 87.5 / 84.3 87.4 / 83.9 87.6 / 83.4 pers beyond the mix cutoff. For Tulu3, we focus on Aya (Singh et al., 2024b) and WildChat (Zhao et al., 2024); members are the subset included in the Tulu3 mix while non-members are excluded in the mix. AIME / Olympiad Math: We construct controlledcontamination math MIA datasets by defining members as the data used for SFT and non-members as held-out data. For AIME, we collect problems from 20212025, using 20212024 as members and 2025 as non-members. For Olympia Math (AI-MO, 2025), we split problems into member and non-member under random partitions. 4.2. MIA Baselines & Evaluation Following (Duan et al., 2024), we consider five loss-based baselines, each described in Section 2. We also include the reconstruction-based baseline from (Hallinan et al., 2025): given prefix, sample continuations and score each by n-gram overlap metrics to the ground truth suffix. We generalize (Hallinan et al., 2025)s method to N-SAMPLING: score each continuation with suite of similarity metrics, and aggregate scores using either the Average-of-N (AoN) or Best-of-N (BoN). We use lexical metrics in Table 1 and include embedding metrics and length-normalized variants. S1 / S1.1 Distillation: Following Muennighoff et al. (2025), we use reasoning traces from two teacher models, Gemini2.0-Flash (S1) and DeepSeek-R1 (S1.1). We treat either group as members depending on the experiment. We report AUROC for MIA performance, following (Shi et al., 2023; Duan et al., 2024; Hallinan et al., 2025), and evaluate reconstruction using the same lexical and embedding similarity metrics. Overall, we evaluate on 8 MIA datasets. Additional to the verbatim (original) setting, we evaluate the paraphrased setting by paraphrasing the constructed datasets with Gemini2.5-Flash. See Section for full dataset construction details. For full metric definitions and evaluation details see Section D. Learning to Detect Language Model Training Data via Active Reconstruction Table 5. Post-training MIA results on AIME, Olympia Math, and Tulu3 Mix. Our active MIAs (ADRA and ADRA+) outperform calibration-free passive methods and match the calibration-based reference method (R-Loss). ACTIVE MIAS PASSIVE MIAS Model Type ADRA+ ADRA N-Sampling Loss R-Loss Zlib Min-K% Min-K%++ Olympia Math Tulu2-7B Tulu2-7B AIME Tulu2-7B Tulu2-7B Orig. Para. Orig. Para. Tulu3 Mix Tulu3-8B Aya Tulu3-8B Wildchat 70.5 68.5 85.9 86. 66.5 65.6 67.1 69.4 85.6 82.2 65. 65.2 59.5 58.2 72.7 75.4 61. 56.9 53.7 49.6 71.1 66.8 62. 51.8 70.3 60.1 67.8 58.4 68. 66.2 55.6 54.9 61.7 60.2 56. 46.7 53.2 51.1 75.8 72.1 61. 52.8 60.2 52.9 78.3 73.2 56. 64.3 Table 6. Post-training member data reconstruction for original verbatim setting. ADRA+ and ADRA consistently outperform the N-Sampling across both lexical and semantic metrics. See Appendix Section for paraphrased setting results. Model Type Method Budget (N ) Lexical Jaccard (Best / Avg) Lexical LCS (Best / Avg) Lexical Coverage (Best / Avg) Embedding Cosine (Best / Avg) Olympia Math Tulu2-7B Orig. AIME Tulu2-7B Orig. Tulu3 Mix Tulu3-8B Aya ADRA+ ADRA N-Sampling ADRA+ ADRA N-Sampling ADRA+ ADRA N-Sampling ADRA+ Tulu3-8B Wildchat ADRA N-Sampling 32 32 32 32 32 32 32 32 32 32 24.1 / 20. 25.6 / 23.4 22.4 / 15.5 22.3 / 18.1 21.7 / 18.5 19.9 / 12.9 44.3 / 43. 51.6 / 51.2 41.6 / 26.5 41.4 / 40.3 37.5 / 34.9 35.8 / 28.1 68 / 79 / 68 60 / 34 55 / 39 58 / 43 50 / 27 14 / 16 / 16 13 / 10 63 / 61 54 / 49 58 / 48 15.3 / 10. 12.3 / 10.2 11.7 / 5.0 17.2 / 11.9 15.5 / 12.5 14.7 / 5.7 47.8 / 47. 53.0 / 52.5 40.0 / 23.9 42.6 / 40.4 36.7 / 31.7 33.7 / 24.0 91.5 / 88. 92.8 / 91.3 92.2 / 87.3 91.9 / 88.9 91.8 / 89.0 91.8 / 85.1 89.2 / 88. 91.5 / 91.1 90.4 / 84.9 92.0 / 91.4 91.7 / 90.2 92.9 / 89.5 5. Results 5.1. Pre-training Table 3 reports AUROC (%) for pre-training MIAs, comparing ADRA/ADRA+ to N-Sampling and 4 loss-based baselines. We find that ADRA and ADRA+ achieve significant improvements over current methods. In the original setting, ADRA+ on average outperforms the best loss-based MIAs by 15.1%, 10.0%, 21.3% in BookMIA, WikiMIA2024 Hard, and Dolma3 arXiv respectively; ADRA+ improves over N-sampling consistently by at least 5% in all benchmarks. In the paraphrased setting, ADRA+ is also the best-performing approach across all datasets. Table 4 shows the member reconstruction quality on NSampling, ADRA, ADRA+ in the original setting. On average, we find that ADRA and ADRA+ consistently improve data reconstruction, particularly in the lexical Jaccard and embedding cosine. On WikiMIA2024 Hard, RL increases Jaccard and embedding metrics but reduces LCS and n-gram coverage, suggesting metric trade-off caused by reward dominance. Results on paraphrased setting are nearly identical and can be found in Section E. Overall, RLinduced data reconstructions yield more faithful extracts and correspondingly stronger membership signals. 5.2. Post-training Table 5 reports AUROC (%) under two post-training regimes: (i) controlled contamination and (ii) post-training mixture. For AIME and Olympia Math, we simulate contamination by finetuning Tulu2-7B on held-out member 6 Learning to Detect Language Model Training Data via Active Reconstruction Table 7. Distillation MIA results. ADRA outperforms all passive MIAs on both distillation datasets (S1.1 and S1). ACTIVE MIA PASSIVE MIAS ADRA N-Sampling Loss R-Loss ZLib Min-K% Min-K%++ S1.1 Distillation: Deepseek-R1 98.4 90.4 55.1 70. 52.6 55.7 S1 Distillation: Gemini-2.0-flash 85.2 80.8 69. 79.3 74.3 71.9 70.6 81.7 Table 8. Distillation member data reconstruction. ADRA achieves significantly better reconstruction across all metrics. Method Lexical Jaccard (Best / Avg) Lexical LCS (Best / Avg) Lexical Coverage (Best / Avg) Embedding Cosine (Best / Avg) S1.1 Distillation: Deepseek-R ADRA N-Sampling 45.4 / 44.5 32.7 / 25.5 S1 Distillation: Gemini-2.0-Flash ADRA N-Sampling 31.0 / 27.4 27.8 / 21.2 145 / 143 111 / 117 / 106 111 / 80 48.6 / 47.7 34.9 / 22.3 32.8 / 28.3 27.4 / 17.5 97.6 / 97.5 97.4 / 95.6 96.4 / 95.6 96.2 / 93.7 examples while mixing in Tulu2-Mix, with 10% contamination rate. For the mixture setting, we evaluate Tulu3-8B on Aya and WildChat from the Tulu3 Mix. For R-Loss, we calibrate with backbone-matched references: Llama2-7B for Tulu2-7B and Llama3-8B for Tulu3-8B. On Olympia Math and AIME, ADRA+ achieves the best AUROC in both original and paraphrased settings, and ADRA is consistently second. Relative to N-Sampling, ADRA+ improves AUROC by 11.0% on Olympia Math, from 59.5% to 70.5%, and by 13.2% on AIME, from 72.7% to 85.9%, in the original setting, with similar gains under paraphrasing. On Tulu3 SFT Mix, ADRA+ equals R-Loss on WildChat (65.6%) AUROC and matches Aya (66.5% vs. 68.0%), while outperforming the remaining baselines. Unlike R-Loss, our attack does not require an additional calibration model. Table 6 shows post-training member reconstruction quality. Compared to pre-training in Table 4, on average our method achieves higher reconstruction on all metrics than N-Sampling. The gains from RL finetuning are more pronounced over sampling: on Olympia Math for example, average Jaccard improves by 7.9, LCS by 34, coverage by 5.9, and embedding cosine by 4.9. Overall, post-training data appear more extractable than pre-training, consistent with stronger memorization from finetuning late in the training pipeline. 5.3. Distillation Table 7 reports AUROC (%) in distillation setting where student (Qwen2.5-7B-Instruct) is trained for one epoch on member reasoning traces produced by teacher (DeepseekR1 or Gemini-2.0-Flash), following Muennighoff et al. (2025). We do not mix additional data, so the distilled traces Figure 2. Performance comparison between RL and SFT. As RL training continues, AUROC improves, whereas SFT decreases. ADRA and ADRA+ meaningfully improves over naive RL. constitute the entire training data. In this setup, member and non-member examples share the same prefix and differ only in the generated suffix, making ADRA+ not directly applicable (see Section C.3). We therefore use ADRA only with generation budget of = 16. Notably, ADRA yields near-perfect membership inference under DeepSeek-R1 distillation, reaching 97.0%, and achieves 86.9% under Gemini distillation. It improves over N-Sampling by 8.0% and 4.4%, and over the strongest lossbased baseline Min-K%++ by 27.8% and 3.5%, respectively. As reported in Table 8, ADRA also substantially improves reconstruction over N-Sampling in both settings; for example, average Jaccard increases by 19.0 and 6.2, and coverage by 25.4 and 10.8, under DeepSeek-R1 and Gemini-2.0-Flash distillation, respectively. 6. Ablations & Analysis RL vs. SFT. We perform ablation studies by comparing our method with supervised fine-tuning, across 3 learning rate, and removing our proposed contrastive RL-objectives. We run on AIME on first seed and report top-5 average AUROC scores evaluated from our suite of similarity metrics. As shown in Figure 2, RL-based optimization and contrastive objectives are critical to gains in MIA performance. Model Size. We scale to Tulu2-13B on AIME  (Table 9)  . Results mirrors Tulu2-7B, with ADRA and ADRA+ outperforming existing MIAs. Reward Type. We compare our lexical reconstruction rewards with two model-based alternatives under noncontrastive RL: embedding (Qwen3-8B-Embedding) and Learning to Detect Language Model Training Data via Active Reconstruction Table 9. Tulu2-13B MIA results on AIME. ACTIVE MIAS PASSIVE MIAS ADRA+ ADRA N-Sampling Loss R-Loss ZLib Min-K% Min-K%++ 89. 84.4 72.7 70.7 63.3 60.6 77. 82.8 Table 10. Reward type comparison. Lexical rewards outperform generative-based alternatives (Embedding and LLM-as-Judge). Method Lexical Embedding LLM-as-Judge No-Training AUROC (Top-5 Mean) 81. 76.9 78.8 74.8 LLM-as-judge (Qwen3-32B). Shown in Table 10, both model-based variants improve over baseline but achieve lower performance than lexical rewards. Lexical rewards are directly verifiable, while model-based rewards are more prone to reward hacking. See Section for implementation details. 7. Related Work Membership Inference Attacks (MIA). Given target model and candidate data, MIA determines whether that data is in the models training set (Shokri et al., 2017). In language modeling, most prior work uses the models log likelihoods on the candidate sequence as proxy for membership. Yeom et al. (2018) directly threshold loss, while Carlini et al. (2021) calibrate the targets loss with reference model. Shi et al. (2023) propose Min-K% to score only the lowest k% log-probabilities, and Zhang et al. (2024b) normalize Min-K% by the expected token log-probabilities over the vocabulary. Other works combine loss with zlib compression (Carlini et al., 2021). smaller line of works leverages models generations. Duarte et al. (2024) leverage models tendency to distinguish verbatim training data from paraphrased counterparts. Closest to our work is (Hallinan et al., 2025), which finds that by generating multiple samples given 50% of the prefixes, simple n-gram metrics on these generations approach the performance of loss-based MIAs. Despite using generation, these methods remain passive: they elicit one-shot continuations under fixed prompts/prefixes and then score those outputs post hoc. One prior work (Tram`er et al., 2022) proposes class of training-based membership inference attacks that poison training data to increase likelihood of extraction of other non-poisoned data. In contrast, our approach to MIA actively incentivizes LLMs to reconstruct their own training data. By maximizing reconstruction-aligned reward, we trained generative policy that continually up weights member sequences and elicits memorization. Training Data Detection in LLMs. Reliably detecting training data is key challenge: training corpora span trillions of tokens and, for many frontier models, the underlying datasets are undisclosed (Touvron et al., 2023; Balloccu et al., 2024). Maini et al. (2024); Duan et al. (2024); Das et al. (2025) have shown that the efficacy of MIAs is confounded by the distribution shifts between member and non8 members. Rigorously proving data membership requires low false-positive rate Carlini et al. (2022); Zhang et al. (2024a), which is difficult to evaluate on close-data models. Instead of inferring membership about individual data, (Maini et al., 2024) proposes dataset inference for LLMs to detect datasets used for training. In parallel, (Jacovi et al., 2023; Elazar et al., 2023) find test-set contamination to be growing concern in LLMs evaluation (Jacovi et al., 2023; Elazar et al., 2023). Paraphrases, translations, and synthetic restatements can inflate benchmark scores while evading detection (Yang et al., 2023; Dekoninck et al., 2024). Sainz et al. (2023); Singh et al. (2024a); Liu et al. (2025a) argues that data membership for LLMs should be defined behaviorally rather than exact match. Our work explores both strict membershipverbatim inclusionand soft membership, which counts semantically or functionally equivalent variants such as paraphrases as members. Under this formulation, false positives are less concerning as semantically equivalent data can constitute effective contamination. Training Data Extraction and Memorization. Modern language models are known to memorize and regurgitate training data, which includes harmful and copyrighted information (Carlini et al., 2021; Biderman et al., 2023; Nasr et al., 2023; Zhang et al., 2023; Cooper & Grimmelmann, 2025). Pretraining dataset size often far exceeds model size, making data extraction difficult (Morris et al., 2025a). Models are also often intentionally aligned to make extraction more difficult after the alignment stage. Even with these theoretical and empirical safeguards in place, models still exhibit memorization of data after finetuning and post-training (Barbero et al., 2025; Ahmed et al., 2026). Data extraction commonly works by discoverable extraction: splitting data into prefix and suffix, prompting the LLM with the prefix, and evaluating similarity between sampled generations and the suffix (Carlini et al., 2021; Hayes et al., 2025). Our work improves extraction by fine-tuning the target model with on-policy RL, actively eliciting latent memorization through weight updates. 8. Conclusion We propose Active Data Reconstruction Attack (ADRA) that leverages reinforcement learning for data detection and reconstruction. ADRA is the first active MIA, consistently improving over passives MIAs across all stages of model training. Results suggest that model weights encode more about training data than previous methods reveal. Learning to Detect Language Model Training Data via Active Reconstruction"
        },
        {
            "title": "Impact Statement",
            "content": "This paper investigates whether training data can be detected and reconstructed from language model weights through membership inference and active reconstruction methods. Our goal is to understand the privacy risks inherent in current language models across their lifecycle, from pretraining through post-training. The purpose of this work is to evaluate the limits of existing privacy protections and to motivate the development of more robust defenses against unintended data leakage."
        },
        {
            "title": "Acknowledgment",
            "content": "Junjie Oscar Yin thanks Rulin Shao, Zhiyuan Zeng, Hamish Ivison, Weijia Shi for the helpful comments and discussions, Lea Li, Jacqueline He, Stella Li for the support. This research was developed with funding from the Defense Advanced Research Projects Agencys (DARPA) SciFy program (Agreement No. HR00112520300) and NSF Grant No. IIS2142739. The views expressed are those of the author and do not reflect the official policy or position of the Department of Defense or the U.S. Government."
        },
        {
            "title": "References",
            "content": "Ahmed, A., Cooper, A. F., Koyejo, S., and Liang, P. Extracting books from production language models. arXiv preprint arXiv:2601.02671, 2026. AI-MO. Ai-mo olympiad reference dataset. Hugging Face Dataset, 2025. https://huggingface.co/ datasets/AI-MO/olympiads-ref. Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Balloccu, S., Schmidtova, P., Lango, M., and Duˇsek, O. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms. arXiv preprint arXiv:2402.03927, 2024. Barbero, F., Gu, X., Choquette-Choo, C. A., Sitawarin, C., Jagielski, M., Yona, I., Veliˇckovic, P., Shumailov, I., and Hayes, J. Extracting alignment data in open models, 2025. URL https://arxiv.org/abs/2510.18554. Biderman, S., Prashanth, U. S., Sutawika, L., Schoelkopf, H., Anthony, Q., Purohit, S., and Raff, E. Emergent and predictable memorization in large language models, 2023. URL https://arxiv.org/abs/2304.11158. Carlini, N., Tramer, F., Wallace, E., Jagielski, M., HerbertVoss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21), pp. 26332650, 2021. Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., and Tramer, F. Membership inference attacks from first principles. In 2022 IEEE symposium on security and privacy (SP), pp. 18971914. IEEE, 2022. Cooper, A. F. and Grimmelmann, J. The files are in the computer: On copyright, memorization, and generative ai, 2025. URL https://arxiv.org/abs/2404. 12590. Das, D., Zhang, J., and Trant`er, F. Blind baselines beat membership inference attacks for foundation models. In 2025 IEEE Security and Privacy Workshops (SPW), pp. 118125. IEEE, 2025. Dekoninck, J., Muller, M. N., Baader, M., Fischer, M., and Vechev, M. Evading data contamination detection for language models is (too) easy. arXiv preprint arXiv:2402.02823, 2024. Duan, M., Suri, A., Mireshghallah, N., Min, S., Shi, W., Zettlemoyer, L., Tsvetkov, Y., Choi, Y., Evans, D., and Hajishirzi, H. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841, 2024. Duarte, A. V., Zhao, X., Oliveira, A. L., and Li, L. Decop: Detecting copyrighted content in language models training data. arXiv preprint arXiv:2402.09910, 2024. Elazar, Y., Bhagia, A., Magnusson, I., Ravichander, A., Schwenk, D., Suhr, A., Walsh, P., Groeneveld, D., Soldaini, L., Singh, S., et al. Whats in my big data? arXiv preprint arXiv:2310.20707, 2023. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hallinan, S., Jung, J., Sclar, M., Lu, X., Ravichander, A., Ramnath, S., Choi, Y., Karimireddy, S. P., Mireshghallah, N., and Ren, X. The surprising effectiveness of membership inference with simple n-gram coverage. arXiv preprint arXiv:2508.09603, 2025. Hayes, J., Swanberg, M., Chaudhari, H., Yona, I., Shumailov, I., Nasr, M., Choquette-Choo, C. A., Lee, K., and Cooper, A. F. Measuring memorization in language models via probabilistic extraction. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 92669291, 2025. 9 Learning to Detect Language Model Training Data via Active Reconstruction Ivison, H., Wang, Y., Pyatkin, V., Lambert, N., Peters, M., Dasigi, P., Jang, J., Wadden, D., Smith, N. A., Beltagy, I., et al. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023. Jacovi, A., Caciularu, A., Goldman, O., and Goldberg, Y. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. arXiv preprint arXiv:2305.10160, 2023. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Lin, C.-Y. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Liu, K. Z., Choquette-Choo, C. A., Jagielski, M., Kairouz, P., Koyejo, S., Liang, P., and Papernot, N. Language models may verbatim complete text they were not explicitly trained on. arXiv preprint arXiv:2503.17514, 2025a. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Maini, P., Jia, H., Papernot, N., and Dziedzic, A. Llm dataset inference: Did you train on my dataset? Advances in Neural Information Processing Systems, 37:124069 124092, 2024. Morris, J. X., Sitawarin, C., Guo, C., Kokhlikyan, N., Suh, G. E., Rush, A. M., Chaudhuri, K., and Mahloujifar, S. How much do language models memorize?, 2025a. URL https://arxiv.org/abs/2505.24832. Morris, J. X., Yin, J. O., Kim, W., Shmatikov, V., and Rush, A. M. Approximating language model training data from weights. arXiv preprint arXiv:2506.15553, 2025b. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. B. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 20286 20332, 2025. Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., Choquette-Choo, C. A., Wallace, E., Tram`er, F., and Lee, K. Scalable extraction of training data from (production) language models, 2023. URL https://arxiv.org/abs/2311.17035. Olmo, T., Ettinger, A., Bertsch, A., Kuehl, B., Graham, D., Heineman, D., Groeneveld, D., Brahman, F., Timbers, F., Ivison, H., et al. Olmo 3. arXiv preprint arXiv:2512.13961, 2025. Qi, P., Liu, Z., Zhou, X., Pang, T., Du, C., Lee, W. S., and Lin, M. Defeating the training-inference mismatch via fp16. arXiv preprint arXiv:2510.26788, 2025. Saggion, H., Radev, D., Teufel, S., and Lam, W. Metaevaluation of summaries in cross-lingual environment using content-based metrics. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. Sainz, O., Campos, J. A., Garcıa-Ferrero, I., Etxaniz, J., de Lacalle, O. L., and Agirre, E. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. arXiv preprint arXiv:2310.18018, 2023. Schulman, J. and Lab, T. M. Lora without regret. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml. 20250929. https://thinkingmachines.ai/blog/lora/. Shao, R., He, J., Asai, A., Shi, W., Dettmers, T., Min, S., Zettlemoyer, L., and Koh, P. W. W. Scaling retrievalbased language models with trillion-token datastore. Advances in Neural Information Processing Systems, 37: 9126091299, 2024a. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789, 2023. Shokri, R., Stronati, M., Song, C., and Shmatikov, V. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 318. IEEE, 2017. Singh, A. K., Kocyigit, M. Y., Poulton, A., Esiobu, D., Lomeli, M., Szilvasy, G., and Hupkes, D. Evaluation data contamination in llms: how do we measure it and (when) does it matter? arXiv preprint arXiv:2411.03923, 2024a. Singh, S., Vargus, F., Dsouza, D., Karlsson, B. F., Mahendiran, A., Ko, W.-Y., Shandilya, H., Patel, J., Mataciunas, D., OMahony, L., et al. Aya dataset: An open-access collection for multilingual instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1152111567, 2024b. 10 Learning to Detect Language Model Training Data via Active Reconstruction Zhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Zhao, W., Ren, X., Hessel, J., Cardie, C., Choi, Y., and Deng, Y. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, second edition, 2018. Tanzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 75647578, 2022. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Tram`er, F., Shokri, R., Joaquin, A. S., Le, H., Jagielski, M., Hong, S., and Carlini, N. Truth serum: Poisoning machine learning models to reveal their secrets, 2022. URL https://arxiv.org/abs/2204.00032. Wang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. Yang, S., Chiang, W.-L., Zheng, L., Gonzalez, J. E., and Stoica, I. Rethinking benchmark and contamination for language models with rephrased samples. arXiv preprint arXiv:2311.04850, 2023. Yeom, S., Giacomelli, I., Fredrikson, M., and Jha, S. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security foundations symposium (CSF), pp. 268282. IEEE, 2018. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zhang, C., Ippolito, D., Lee, K., Jagielski, M., Tram`er, F., and Carlini, N. Counterfactual memorization in neural language models, 2023. URL https://arxiv.org/ abs/2112.12938. Zhang, J., Das, D., Kamath, G., and Tram`er, F. Membership inference attacks cannot prove that model was trained on your data. arXiv preprint arXiv:2409.19798, 2024a. Zhang, J., Sun, J., Yeats, E., Ouyang, Y., Kuo, M., Zhang, J., Yang, H. F., and Li, H. Min-k%++: Improved baseline for detecting pre-training data from large language models. arXiv preprint arXiv:2404.02936, 2024b. 11 Learning to Detect Language Model Training Data via Active Reconstruction Table 11. Prompts used for pre-training experiments across datasets. The {prefix} placeholder indicates where the input text is inserted. Dataset Prompt Template You will receive prefix from passage and be asked to complete it based on the text of famous work. Provide only the continuation for the last given prefix without any extra commentary, formatting, or additional text. Complete the prefix: {prefix} Continue the generation as closely to verbatim as possible. {prefix} Continue the generation as closely to verbatim as possible. {prefix} BookMIA WikiMIA2024 Hard Dolma3 arXiv Table 12. Supervised fine-tuning hyperparameters for controlled contamination setting. Dataset AIME Olympia Math Learning rate Eff. batch size LR Schedule Precision Epochs Warmup ratio Max seq. len. 2e5 5e6 128 Linear Constant BF16 1 0. 4096 A. Experimental Details We provide implementation details for replicating our experimental results, including prefix construction and hyperparameters for each training stage. All trainings are done in single node consisting of 8 H200s. A.1. Pre-training Following Hallinan et al. (2025), we use approximately 50% of the candidate data as the prefix and reconstruct the remaining held-out suffix. We limit generation length to half the maximum token count of the dataset. We list the prompts we use before the candidate prefix in Table 11. A.2. Post-training Controlled contamination. To simulate test-set contamination, we fine-tune on member examples from AIME and Olympia Math. We leverage the natural problem-solution structure in both math datasets by including the full problem plus 25% of the solution as the prefix. To simulate practical contamination scenarios, we also mix member examples with other SFT data at fixed 10% contamination rate using the Tulu2 SFT mix (Ivison et al., 2023). Table 12 details our fine-tuning hyperparameters. Post-training mixtures. We leverage dataset structure to determine the amount of prefix. Both Aya and WildChat are instruction-tuning datasets containing user requests and assistant responses, and our curated MIA datasets only contain single-turn conversations. We include the full user request plus 25% of the assistant response as the prefix. A.3. Distillation S1 and S1.1 consist primarily of problem-solution pairs. Since problems and solutions have comparable lengths, we include only the problem as the prefix. Following Muennighoff et al. (2025), we perform model distillation on Qwen2.5-7BInstruct by fine-tuning on the entire S1 or S1.1 dataset without additional data mixing. Table 13 details our fine-tuning hyperparameters. Qualitatively, we find that Qwen2.5-7B-Instruct output resembles that of Deepseek-R1 more closely, so smaller learning rate is used relative to distilling from Gemini-2.0-Flash. 12 Learning to Detect Language Model Training Data via Active Reconstruction Table 13. Supervised fine-tuning hyperparameters for model distillation setting. Dataset Learning rate Eff. batch size LR Schedule Precision Epochs Warmup ratio Max seq. len. S1 - Gemini-2.0-Flash S1.1 - Deepseek-R1 2e5 5e6 Paraphrasing Prompt Template 128 Linear BF16 1 0.00 8192 You will be given an ORIGINAL block that may contain natural-language, math, code, or other texts. Your job is to rephrase the natural-language content while mostly PRESERVING NON-NATURAL-LANGUAGE CONTENT. 1. Please ensure the paraphrased output have similar length to the original text (15%). 2. When encountering mathematical formulas, if necessary, you can replace variable names. For example, you can replace with or a. 3. No extra commentary: Do not add explanations, apologies, or any other text besides the required output. 4. Output directly the paraphrased output after rewrite output:. ORIGINAL: {input} Figure 3. Prompts used to paraphrase datasets. The {input} placeholder indicates where the input text is inserted. B. MIA Dataset Construction We provide details on our MIA dataset curation process. Due to computational constraints, we sample subsets from each constructed dataset and average results across multiple random seeds. In addition to the original (verbatim) setting, we also evaluate paraphrased setting where member and non-member examples are rewritten while preserving their semantic content. Specifically, we use the GEMINI-2.5-FLASH-V3.0 model API via LiteLLM to generate paraphrased instances. For experiments evaluating existing models, we use paraphrased examples as the evaluation set. For experiments involving fine-tuning, we use paraphrased examples as input to the training set and original examples as the evaluation set. We list the prompts used for each dataset in Figure 3. B.1. Dolma3 Pre-training Mix For the pre-training setting, we collect arXiv documents from the Dolma3 pre-training data mix. These documents originate from the Proof-Pile-2 dataset (Azerbayev et al., 2023) and have cutoff date of April 2023. We sub-sample 3,000 documents to form our member set. To construct non-members, we collect 1,000 arXiv papers released after 2025 and verify using OlmoTrace that they are not present in the pre-training data. For each seed, we sample 128 examples (64 members, 64 non-members). B.2. Tulu3 SFT Mix For the post-training mix setting, we collect two datasets from the Tulu3 SFT Mix: Aya and WildChat. Aya is multilingual instruction-tuning dataset containing human-annotated prompt-completion pairs (Singh et al., 2024b), while WildChat contains real-world user interactions with ChatGPT (Zhao et al., 2024). For both datasets, members are sampled from data included in the Tulu3 SFT Mix, and non-members are data from the original datasets that were excluded from the mix. To control for length variation between members and non-members, we limit both user and assistant turns to maximum of 1,024 tokens. For each seed, we sample 128 examples (64 members, 64 non-members). B.3. AIME / Olympia Math For the controlled-contamination setting, we collect challenging math datasets: AIME and Olympia Math. For AIME, we collect problems and solutions from 20212025, using 20212024 as members and 2025 as non-members. Because each 13 Learning to Detect Language Model Training Data via Active Reconstruction Table 14. GRPO hyperparameters used with verl. Only core settings are shown; batch size, rollouts, sequence lengths, and training epochs vary by dataset. Hyperparameters Setting GRPO Advantage estimator 5e 05 Learning rate (actor) Normalize advantages (GRPO) True Use KL loss (not reward penalty) True 0.005 KL loss coef. 0 Entropy coefficient 1.0 Generation temperature Generation top-p 0.95 Generation top-k 50 LoRA rank / α 64 / 128 0 Critic warmup year contains only 30 problems, our member set contains 120 examples while the non-member set contains 30. For each seed, we sample 32 examples (16 members, 16 non-members). For Olympia Math (AI-MO, 2025), because the dataset was released in 2025, we split problems into members and non-members using random partitions. For each seed, we sample 64 examples (32 members, 32 non-members). B.4. S1 / S1.1 For the distillation setting, we follow Muennighoff et al. (2025) and collect reasoning trace datasets from two teacher models: Gemini-2.0-Flash (S1) and DeepSeek-R1 (S1.1). Depending on the experiment, we treat either group as members while the other serves as non-members. For each seed, we sample 256 examples (128 members, 128 non-members). C. Active Data Reconstruction Attack Details Below we provide more details on implementation of our method Active Data Reconstruction Attck. C.1. GRPO Hyperparamters We implement our method ADRA using VERL and the GRPO training hyperparameters are listed in Table 14. C.2. Reconstruction Metrics Details We propose three lexical reconstruction metrics from which our policy attempts to maximize, as detailed in Table 1. In practice, we experiment with two aggregated variants: (1) TRIO, which averages token set similarity, longest common subsequence, and N-gram set coverage; and (2) N-GRAM, which uses N-gram set coverage alone. To constrain generation length and prevent reward hacking through excessive output, we apply length penalty to both metrics. The penalty operates on threshold-based ratio system: candidate outputs are penalized if their word count falls outside the range [ℓref/τ, ℓref τ ], where ℓref is the reference length and τ is the threshold parameter. We set τ = 1.50 for both TRIO and N-GRAM, allowing candidates to be as short as 67% or as long as 150% of the reference length without penalty. Outside this range, linear penalty proportional to the length deviation is applied, multiplying the base similarity score by min(1.0, min(ℓcand/ℓref, ℓref/ℓcand)). We provide dataset-specific hyperparameters for these metrics in Table 15. C.3. Contrastive Reward Details We use = 7 distractors, so each generation is scored against 8 candidate suffixes: 1 ground truth plus 7 distractors. Distractors are sampled i.i.d. from the evaluation set. For WikiMIA2024 Hard, which contains paired Wikipedia articles from different date cutoffs (one member, one non-member), Learning to Detect Language Model Training Data via Active Reconstruction Table 15. Dataset-specific hyperparameters for ADRA training. Dataset Batch size Rollouts Max prompt / response tokens Steps Reconstruction Metrics Distractors Length penalty BookMIA WikiMIA2024 Hard Dolma3 arXiv AIME OlympiaMath Tulu3 Aya / Wildchat S1 / S1. 64 64 64 64 64 64 64 32 32 32 32 32 32 16 1028 / 1028 1028 / 1028 1028 / 1028 2048 / 2048 2048 / 2048 1024 / 512 1028 / 4096 30 30 30 100 100 200 200 TRIO, N-GRAM TRIO, N-GRAM TRIO, N-GRAM TRIO, N-GRAM TRIO, N-GRAM TRIO, N-GRAM TRIO, N-GRAM 7 7 7 7 7 7 N/A 1.5 1.5 1.5 1.5 2.0 / 1.5 1.5 1.25 we include the paired article as one distractor in addition to 1 randomly sampled distractors. In the distillation setting, member and non-member pairs share identical prefixes. We treat each such pair as single example and maximize the reconstruction score for whichever suffix (member or non-member) achieves the higher lexical metric. D. Evaluation Details D.1. Evaluation Metrics We evaluate reconstruction quality using comprehensive suite of lexical and semantic similarity metrics. For each input, we sample candidate completions and report both the average-of-N (mean score across all samples) and best-of-N (maximum score among samples) aggregations. Evaluation Target. During training, we provide the model with the first p% of the ground truth as an assistant prefix to seed generation (see Table 15 for dataset-specific values). At evaluation time, we run two evaluation protocols: 1. Suffix-only evaluation: We truncate the first p% of words from before computing similarity, evaluating only the models ability to reconstruct the novel suffix portion that was not provided as prefix. 2. Full evaluation: We evaluate against the complete (prefix + suffix), measuring overall reconstruction quality. The suffix-only evaluation provides stricter measure of memorization, as it excludes the free tokens the model was given. Specifically, we report reconstruction quality under the suffix-only evaluation . Metric Definitions. Table 16 summarizes the evaluation metrics. We adopt consistent notation with the training rewards  (Table 1)  , where ˆy denotes the models completion and denotes the ground truth. Let () denote the set of unique tokens in string, LCS(, ) denote the longest common subsequence length, and N[Lmin,Lmax]() denote the set of contiguous n-grams with [Lmin, Lmax]. Metric Details. Token-based metrics measure vocabulary overlap between the completion and ground truth. Jaccard similarity normalizes by the union of token sets, penalizing both missing tokens and extraneous tokens equally. Token overlap metrics normalize by either the reference (measuring recall of ground truth tokens) or candidate (measuring precision of generated tokens). Reference-normalized metrics are more suitable for membership inference as they measure what fraction of the target was successfully reconstructed. Sequence-based metrics capture ordering information via the longest common subsequence. The raw LCS length provides an absolute measure of matched content, while normalized variants express this as fraction of reference or candidate length. N-gram metrics measure the coverage of contiguous token sequences. Following prior work (Hallinan et al., 2025), we compute n-grams for [3, LCS(ˆy, y)], treating n-grams as sets to discourage reward hacking via repetition. Embedding similarity provides semantic measure using Qwen3-Embedding-8B (Zhang et al., 2025). The cosine similarity between embedding vectors ϕ() captures meaning beyond lexical overlap. 15 Learning to Detect Language Model Training Data via Active Reconstruction Table 16. Evaluation metrics for measuring reconstruction quality and producing membership score. Each metric is computed for both average-of-N and best-of-N aggregations."
        },
        {
            "title": "Normalization",
            "content": "Token-based Metrics"
        },
        {
            "title": "Jaccard similarity",
            "content": "Token overlap (ref) Token overlap (cand) Sequence-based Metrics LCS length LCS ratio (ref) LCS ratio (cand) N-gram Metrics N-gram coverage (cand) N-gram coverage (ref) Semantic Metrics sjacc(ˆy, y) = stok(ˆy, y) = tok (ˆy, y) = scand (ˆy) (y) (ˆy) (y) (ˆy) (y) (y) (ˆy) (y) (ˆy) LCS(ˆy, y) slcs(ˆy, y) = lcs (ˆy, y) = scand LCS(ˆy, y) LCS(ˆy, y) ˆy (cid:12)N (ˆy) (y)(cid:12) (cid:12) (cid:12) ng (ˆy, y) = scand (cid:12)N (ˆy)(cid:12) (cid:12) (cid:12) (cid:12)N (ˆy) (y)(cid:12) (cid:12) (cid:12) (cid:12)N (y)(cid:12) (cid:12) (cid:12) sng(ˆy, y) = Union (symmetric)"
        },
        {
            "title": "Candidate",
            "content": "Raw (unnormalized)"
        },
        {
            "title": "Candidate",
            "content": "Candidate Reference Embedding similarity semb(ˆy, y) = ϕ(ˆy) ϕ(y) ϕ(ˆy)ϕ(y) Cosine similarity Reference vs. Candidate Normalization. Reference-normalized and candidate-normalized metrics capture complementary information. Reference-normalized metrics (normalizing by or (y)) measure the fraction of the ground truth that was successfully reconstructed, reflecting recall. Candidate-normalized metrics (normalizing by ˆy or (ˆy)) measure what fraction of the generated output matches the target, reflecting precision. We report both variants. In our RL training (Section C), we use reference-normalized metrics as rewards, as they cannot be trivially maximized by generating shorter outputs. D.2. N-Sampling Baseline Evaluation For the N-sampling baseline, we generate independent rollouts for each input and compute the metrics defined above. To ensure fair comparison with our RL-trained models, we match the sampling configuration used during RL training: temperature τ = 1.0, top-p = 0.95, and top-k = 50. The number of rollouts also matches the rollout budget used in RL training (see Table 15). D.3. ADRA Evaluation After RL training, we evaluate the trained policy by generating rollouts per input, matching the rollout budget used in -sampling and RL training for fair comparison. We reduce the sampling temperature to τ = 0.7 while keeping top-p = 0.95 and top-k = 50. The lower temperature reflects that the RL-trained policy has already learned to generate reconstructions, so we bias toward higher-likelihood outputs rather than encouraging exploration. This also follows typical practice in RL evaluation, where exploitation is preferred over exploration at test time. Learning to Detect Language Model Training Data via Active Reconstruction Table 17. Pre-training member data reconstruction for paraphrased setting. Bold denotes the best average performance for each dataset. ADRA+ and ADRA consistently outperform the N-Sampling across all metrics. See Table 4 for original verbatim setting results. Model Type Method BookMIA Llama2-7B Para. ADRA ADRA+ WikiMIA24-Hard N-Sampling ADRA+ Qwen2-7B Para. ADRA Olmo3 Training Mix Olmo3-7B-Instruct Para. N-Sampling ADRA+ ADRA N-Sampling Budget (N ) Lexical Jaccard (Best / Avg) Lexical LCS (Best / Avg) Lexical Coverage (Best / Avg) Embedding Cosine (Best / Avg) 32 32 32 32 32 32 32 15.8 / 13.5 14.7 / 13.0 15.5 / 12.3 16.4 / 13.1 17.3 / 13.4 14.3 / 9. 14.3 / 11.8 14.6 / 12.0 13.8 / 10.2 51 / 44 47 / 42 48 / 33 / 25 40 / 27 47 / 37 56 / 48 56 / 47 40 / 3.0 / 1.0 2.2 / 0.9 2.5 / 0.5 8.1 / 4.0 9.5 / 4.2 12.8 / 5. 6.3 / 2.9 6.4 / 3.0 6.2 / 1.6 88.2 / 84.2 87.3 / 84.3 88.2 / 81. 86.5 / 82.0 88.1 / 84.1 87.2 / 80.0 86.3 / 83.1 86.5 / 83.2 86.8 / 82. Table 18. Post-training member data reconstruction for paraphrased setting. ADRA+ and ADRA consistently outperform the N-Sampling across both lexical and semantic metrics. See Table 6 for original verbatim setting results. Model Type Method Olympia Math ADRA+ Tulu2-7B Para. ADRA AIME N-Sampling ADRA+ Tulu2-7B Para. ADRA N-Sampling Budget (N ) Lexical Jaccard (Best / Avg) Lexical LCS (Best / Avg) Lexical Coverage (Best / Avg) Embedding Cosine (Best / Avg) 32 32 32 32 32 23.8 / 21.0 24.1 / 21.4 22.8 / 15.5 22.7 / 18.8 21.1 / 17.9 18.4 / 12. 67 / 60 68 / 61 60 / 34 54 / 40 56 / 42 48 / 16.0 / 11.4 12.7 / 8.5 11.9 / 5.1 17.5 / 12.7 15.4 / 11.7 13.6 / 4. 91.7 / 89.3 92.1 / 89.6 92.4 / 87.3 92.2 / 88.8 91.8 / 89.0 89.2 / 83. E. Additional Results: Pre-training Reconstruction Table 17 presents reconstruction quality for paraphrased members across pre-training datasets. Results closely mirror the verbatim setting  (Table 4)  . ADRA and ADRA+ consistently outperform N-Sampling on average across lexical Jaccard and embedding cosine metrics. On BookMIA, ADRA+ achieves the strongest average performance (13.5 Jaccard, 44 LCS, 1.0 coverage), while ADRA attains the best embedding cosine (84.3). On WikiMIA2024 Hard, ADRA reaches the best average Jaccard (13.4) and embedding cosine (84.1), though N-Sampling achieves higher LCS (37) and coverage (5.3)consistent with the metric trade-off observed in the verbatim setting where RL optimization favors reward-aligned metrics. On Olmo3 Training Mix, ADRA attains the best average Jaccard (12.0), coverage (3.0), and embedding cosine (83.2), while ADRA+ achieves the highest LCS (48). Overall, results suggest that RL improves training data extraction despite semantic perturbations. F. Additional Results: Post-training Reconstruction Table 18 presents reconstruction quality for paraphrased members in post-training settings. Results closely mirror the verbatim setting  (Table 6)  with better extraction compared to pre-training. ADRA and ADRA+ consistently outperform N-Sampling across all metrics on both controlled contamination datasets. On Olympia Math, ADRA achieves the best average Jaccard (21.4), LCS (61), and embedding cosine (89.6), while ADRA+ attains the highest coverage (11.4). On AIME, ADRA excels in average Jaccard (18.8) and Coverage (12.7) while ADRAexcels in average LCS (42) and embedding cosine (89.0). The improvements over N-Sampling remain substantial: on AIME, ADRA improves average Jaccard by 6.7, LCS by 15, coverage by 7.9, and embedding cosine by 5.7. Like in pre-training, results confirms that RL improves training 17 Learning to Detect Language Model Training Data via Active Reconstruction Ground Truth The novelty of our approach is in probabilistically modelling the HT as continuous function of frequency, rather than using multiple independent response models (psychometric functions) for discrete set of standard frequencies. This is by endowing the threshold curve with Gaussian process (GP) prior citep{rasmussen gaussian 2006}, which is why we refer to our method as GP-PTA. If the responses are binary (audible or non-audible), the full model can be interpreted as two-dimensional binary classifier that is specified by GP in the frequency dimension and by psychometric (sigmoid) function in the intensity dimension. It provides uncertainty bands on the resulting threshold estimate , which enables fundamental and objective stopping criteria. Moreover, it allows one to find the optimal next stimulus based on the data processed so far, and stimuli at any frequency can be used. These properties reduce the number of trials required to achieve the desired accuracy. Minimizing the amount or required trials is important to reduce the cognitive burden on the (often elderly) patients. [. . . ] probabilistic response model is specified to capture uncertainty in the patients responses [. . . ] generative probabilistic model is derived by combining the response model with GP. The GP provides natural way to take prior knowledge about the threshold curve into account, and relaxes the piecewise linear assumption for the threshold curve to more natural smoothness assumption. Reconstruction However, these approaches typically assume that the hearing thresholds at different frequencies are independently distributed [. . . ] Recent work using Gaussian process (GP) models citep{zhang2017gp, garrido2019gp, rahman2020gp} has begun to address this by modeling the HT curve as Gaussian process , which naturally captures correlations between adjacent frequency points. These models are capable of providing not only point estimates for the threshold at each frequency but also uncertainty estimates, which can be used to determine an objective stopping criterion [. . . ] GP models can be regularized with prior information about typical HT curves [. . . ] In this work, we propose simplified yet effective framework for estimating the pure tone hearing threshold curve using structured Gaussian process. Our approach models the entire threshold curve as function of frequency using covariance function that explicitly encodes smoothness and periodicity constraints [. . . ] This structure allows the model to extrapolate between measured points and to exploit the inherent correlation between thresholds at neighboring frequencies, thus reducing the number of required measurements. We also incorporate prior based on standardized audiograms [. . . ] method to use the output of the model for adaptive data collection, thresholds. including stopping criterion based on the uncertainty of the estimated Figure 4. Qualitative example from Olmo3 Mix Arxiv: Reconstruction of GP-based audiometry paper. Highlighted phrases show semantic and lexical overlap in core technical conceptsGaussian process (GP), threshold curve, function of frequency, uncertainty estimates/bands, stopping criterion/criteria, smoothness, and prior knowledge/informationsuggesting that the reconstruction recovers the papers key methodological vocabulary and conceptual structure. data extraction despite semantic perturbations. G. Reconstructions Examples We show qualitative reconstruction examples across all three training stages: pre-training (Figure 4, Olmo3 Mix arXiv), post-training (Figures 5 and 6, AIME), and distillation (Figure 7, S1.1 Deepseek-R1). H. Ablation Details Due to computational constraints, all ablations are conducted on AIME using single seed. Model-Based Reward Implementation. We implement model-based rewards by hosting dedicated embedding and vLLM inference servers, which the verl trainer queries via FastAPI. Specifically, we use Qwen3-8B-Embedding as the embedding model and Qwen3-32B as the LLM-as-judge. For embedding-based rewards, the model embeds both the candidate and ground-truth suffix and returns their cosine 18 Learning to Detect Language Model Training Data via Active Reconstruction Ground Truth = (a c) (a 1) 6! = 720, so 28. It follows that and are consecutive composites with prime factors no other than 2, 3, 5, and 7. The smallest values for and are 36 and 35, respectively. So, we have {a, b, c} = {2, 3, 6}, {d, e, } = {1, 5, 7}, and {g, h, i} = {4, 8, 9}, from which abcdef 288 . If we do = 1 ghi not minimize the numerator , then > 1. Note that abcdef ghi 2 789 > 1 288 .Together, we conclude that the minimum possible positive value of abcdef ghi is 1 288 . Therefore, the answer is 1 + 288 = 289 . Reconstruction = 1. Therefore, the minimum possible positive value of abcdef Similarly, abcdef ghi to minimize positive fraction, we minimize its numerator and maximize its denominator. 1 It is clear that 789 . If we minimize the numerator , then = 1. Note that = 1. is 1 789 . ghi Therefore, the minimum possible positive value of abcdef 1 789 . Therefore, the sum of the relatively prime positive integers is 1 + 1 = 2. ghi is Therefore, we need to find the minimum possible positive value of abcdef ghi . In this case, the minimum possible positive value is 1 789 . Therefore, the answer is 1 789 . Therefore, the answer is 2. Figure 5. Qualitative Example 1 from AIME. The reconstruction captures core mathematical reasoning patterns and expressions from the ground truth, despite yielding wrong final answer. similarity as the reward signal. For LLM-as-judge rewards, the model is prompted to assess similarity between the candidate and ground-truth based on structured prompt and returns numerical score. The prompt used for LLM-as-judge can be found in Figure 8. Observations. In preliminary experiments, we find that model-based rewards are particularly prone to reward hacking, with different target models exhibiting distinct hacking behaviors. Our ablations suggest that model-based reward RL training can improve reconstruction performance, but its efficacy depends on both prompt design and model choice. We leave further exploration of model-based rewards for future work. I. Limitations & Discussions There are limitations to our method and analysis; we detail them below. Compute ADRA is much more compute-intensive than current MIAs because it requires on-policy RL training. Performing MIA on large-scale data may be prohibitively expensive. Further, while we find that RL on average improves MIA and reconstruction over -sampling, the efficacy of scaling RL training varies across datasets. In some seeds and datasets, MIA performance peaks during early RL training steps, and continued training could degrade the performance. Memorization & Generalization We hypothesize that latent traces of training data exist in the model weights that can be elicited through RL training. However, it is difficult to measure precisely how much our RL formulation elicits latent memorization versus generalization, which is also dataset-dependent. Precisely disentangling memorization and generalization has been an ongoing challenge (Tanzer et al., 2022; Morris et al., 2025a), and we hope future work can more carefully examine these effects at scale. 19 Learning to Detect Language Model Training Data via Active Reconstruction Ground Truth before the bus arrives are adults. Similarly, we know that + 50 0 (mod 25), as 11 25 of the people at the party are adults after the bus arrives. + 50 0 (mod 25) can be reduced to 0 (mod 25), and since we are looking for the minimum amount of people, is 300. That means there are 350 people at the party after the bus arrives, and thus there are 350 11 25 = 154 adults at the party. Reconstruction are adults. Similarly, let be the number of people at the party before the bus cut. We know that 0 (mod 12), 12 of people at the party are adults. as 5 Therefore, the number of adults at the party before the bus arrives is 0. After there are total of 50 + 25 = 75 the bus arrives, people at the party . Let be the number of people at the party after the bus arrives. We know that 0 (mod 12), as 11 25 of people at the party are adults. Therefore, the minimum number of adults at the party after the bus arrives is 0. Therefore, the answer is 0. We are looking for the minimum number of adults at the party after the bus arrives. In this case, there are no adults at the party after the bus arrives. Therefore, the answer is 0. Figure 6. Qualitative Example 2 from AIME. The reconstruction captures key mathematical patterns and modular arithmetic reasoning despite different final answer. RL Algorithms. We use vanilla GRPO (Shao et al., 2024b) throughout the paper. Recent work (Liu et al., 2025b; Yu et al., 2025; Qi et al., 2025) has identified failure modes of GRPO, such as training collapse and instability, and proposed several remedies. These improvements could be readily incorporated into ADRA to further boost reconstruction and MIA performance. We leave exploration of better RL algorithms to future work. Overall, we find that ADRA, the first active MIA, consistently improves over passive MIAs, revealing that model weights encode substantially more information about their training data than passive methods can surface finding with important implications for both membership inference and memorization research. 20 Learning to Detect Language Model Training Data via Active Reconstruction Ground Truth Given the matrix = (cid:0) ω2 ω (cid:1), where ω is nonreal root of x3 = 1, 0 we need to find the sum of the entries of + M2 + M3 + + M2009. First, we note that ω is primitive cube root of unity, satisfying ω3 = 1 and 1 + ω + ω2 = 0. We compute the powers of M: 1. = (cid:0) ω2 ω (cid:1) 3. M3 = ( 1 0 (cid:1) 2. M2 = (cid:0) 0 0 1 ) (the identity matrix) 1 ω2 ω 1 0 Since M3 = I, the powers of cycle every 3 terms. We decompose the sum [. . . ] into cycles of 3 terms. There are 2009 terms in total, which is 669 complete cycles (each containing 3 terms) and 2 remaining terms. The sum for each cycle is + M2 + I. The remaining terms are M2008 and M2009 . The sum of the entries of [. . . ] Sum of entries of M2: 0 + 1 ω2 ω = 1 ω2 ω - Sum of entries of I: 1 + 0 + 0 + 1 = 2 Total sum for one cycle: 2(1 ω2 ω) + 2 = 6. [. . . ] Therefore, the total sum is: 669 6 + 4 = 4014 + 4 = 4018 is computed as follows: - Sum of entries of M: ω2 ω + 1 + 0 = 1 ω2 ω - The sum of the entries of + M2 + M3 + + M2009 is 4018 . Reconstruction Given the matrix = (cid:0) ω2 ω (cid:1), where ω is nonreal root of x3 = 1, 0 we need to find the sum of the entries of + M2 + M3 + + M2009. First, we note that the nonreal roots of x3 = 1 are ω and ω2, where ω and ω2 are the complex cube roots of unity satisfying ω3 = 1 and 1 + ω + ω2 = 0. To find the powers of M, we compute the first few powers of M: M2 = (cid:0) ω 1 ω2 ω (cid:1) M3 = ( 1 0 1 ) = Since M3 = I, the powers of cycle every 3 terms. Therefore, the sequence M, M2, M3, M4, . . . repeats every 3 terms. Next, we find the sum of the entries of each matrix in one cycle: - The sum of the entries of is (ω2) + (ω) + 1 + 0 = 1 ω ω2 = 0. - The sum of the entries of M2 is ω + 1 ω2 ω = 1. - The sum of the entries of is 1 + 0 + 0 + 1 = 2. Thus, the sum of the entries in one complete cycle (3 terms) is 0 + 1 + 2 = 3. [. . . ] we note that there are 2009 terms, which can be divided into 669 complete cycles of 3 terms and 2 remaining terms. The sum of the entries for the complete cycles is 669 3 = 2007. The sum of the entries for the remaining 2 terms ( M2008 and M2009 ) is 1 + 2 = 3. Therefore, the total sum is: 2007 + 3 = 2010 Thus, the sum of the entries of + M2 + M3 + + M2009 is 2010 . Figure 7. Qualitative example from S1.1 Deepseek-R1 Distillation. Near-verbatim reconstruction of matrix powers problem. Highlighted parts show overlap in algebraic identities, the cyclic structure insight (M3 = I), cycle decomposition logic, and entry-sum computation steps. Despite closely reproducing the full solution template, the reconstruction arrives at an incorrect final answer (2010 vs. 4018). 21 Learning to Detect Language Model Training Data via Active Reconstruction LLM-as-Judge Prompt Template TASK DESCRIPTION Rate two solutions to the same math problem (one reference, one candidate) for similarity. The final answer in each solution is enclosed in boxed{}. Return real-valued score between 0 and 1 with exactly 3 decimals. EVALUATION CRITERIA (consider all, equally) 1. Mathematical correctness Is the candidate mathematically sound as in the reference? 2. Solution approach Does the candidate use methods/reasoning similar to the reference? 3. Final answer Does the candidate arrive at the same boxed{} answer as the reference? 4. Overall clarity Are the candidates reasoning and steps as consistent and logically sound as the reference? 5. Surface & semantic resemblance Do the two solutions look very similar in wording, symbols, step order, and intermediate expressions (i.e., high lexical/token overlap and near-paraphrase semantics)? Penalize added filler or unnecessary rephrasing. INPUTS Reference solution: {REFERENCE SOLUTION} Candidate solution: {CANDIDATE SOLUTION} OUTPUT FORMAT (must follow exactly) Output ONLY one line: REWARD: <real value between 0 and 1 with 3 decimals> Figure 8. Prompts used for LLM-as-Judge reward model evaluation."
        }
    ],
    "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Cornell University",
        "UC Berkeley",
        "University of Washington"
    ]
}