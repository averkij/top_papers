{
    "paper_title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search",
    "authors": [
        "Maohao Shen",
        "Guangtao Zeng",
        "Zhenting Qi",
        "Zhang-Wei Hong",
        "Zhenfang Chen",
        "Wei Lu",
        "Gregory Wornell",
        "Subhro Das",
        "David Cox",
        "Chuang Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 8 0 5 2 0 . 2 0 5 2 : r Satori: Reinforcement Learning with Chain-of-Action-Thought"
        },
        {
            "title": "Enhances LLM Reasoning via Autoregressive Search",
            "content": "Maohao Shen * 1 Guangtao Zeng * 2 Zhenting Qi * 3 Zhang-Wei Hong 1 Zhenfang Chen 4 Wei Lu 2 Gregory Wornell 1 Subhro Das 4 David Cox 4 Chuang Gan"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of single LLM to tackle complex tasks. Thus, we pose new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chainof-Action-Thought (COAT) reasoning and twostage training paradigm: 1) small-scale format tuning stage to internalize the COAT reasoning format and 2) large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced1. * Core contributors of Satori team, contributed equally to this work. Order determined by dice roll. 1MIT 2Singapore University of Technology and Design 3Harvard 4MIT-IBM Watson AI Lab, IBM Research 5UMass Amherst. Correspondence to: Maohao Shen <maohao@mit.edu>. Preprint, under review 1https://satori-reasoning.github.io/ 1 Large language models (LLMs) have demonstrated remarkable performance across wide range of reasoning tasks, including mathematical problems (Cobbe et al., 2021; Hendrycks et al., 2021a), programming (Chen et al., 2021; Zhuo et al., 2024) and logical reasoning (Han et al., 2024; Liu et al., 2020). One of the key techniques enabling these strong reasoning capabilities is Chain-of-Thought (CoT) prompting (Wei et al., 2022), which allows LLMs to address complex tasks by generating series of intermediate reasoning steps. As result, many early efforts focus on finetuning LLMs using large-scale, high-quality CoT reasoning chains, either through human annotation (Hendrycks et al., 2021a; Yue et al., 2024) or by distilling synthetic data from more advanced models (Yu et al., 2024; Toshniwal et al., 2024a; Ding et al., 2024). However, human annotation is extremely labor intensive, and distillation often limits the models reasoning capabilities to certain level. Apart from scaling up training resources, more recent work has focused on test-time scaling, i.e., allocating additional inference-time compute to search for more accurate solutions. This often involves extensive sampling, either by generating multiple complete solutions (Wang et al., 2023) or by sampling multiple intermediate reasoning steps (Yao et al., 2024; Wan et al., 2024). These methods typically require external feedback to guide the search process, usually through training an auxiliary reward model to rate final solutions or intermediate steps (Sun et al., 2024; Wang et al., 2024a). However, such two-player frameworks incur more model-deployment costs and do not internalize the search capabilities into single LLM. Orthogonal to the above work, our study investigates new direction that enables LLMs with autoregressive search capabilities, i.e., an extended reasoning process with selfreflection and self-exploration of new strategies. Specifically, we introduce the Chain-of-Action-Thought (COAT) mechanism, which enables LLMs to take various metaactions during problem solving. Unlike conventional posttraining consisting of large-scale supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), we propose novel two-stage training paradigm: (1) small-scale format tuning (FT) stage to internalize the COAT reasoning format and (2) large-scale selfimprovement stage that utilizes reinforcement learning with Restart and Explore (RAE) techniques. Our approach leads to the development of Satori, 7B LLM trained on open-source base models and mathematic data that achieve superior performance on both in-domain and out-of-domain tasks. To summarize, our contributions are threefold, 1. Efficiency: Satori is single LLM capable of autoregressive search without external guidance (Section 6 and Section A). Moreover, this is achieved with minimal supervision and large-scale self-improvement. 2. Effectiveness: Satori demonstrates superior performance on in-domain mathematical reasoning tasks and outperforms the instruct model built on the same base model (Section 5.1). 3. Generalizability: Unlike recent research on math reasoning, Satori exhibits strong transferability to out-ofdomain tasks and demonstrates universal capabilities for self-reflection and self-exploration (Section 5.2). 2. Related Work We summarize the literature that is closely aligned with the scope of this paper (refer to Section for more discussions). Concurrent Work. Building on the impact of OpenAIs o1 (OpenAI, 2024), significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. The most common approach relies on distilling knowledge from stronger teacher models (Huang et al., 2024a; Zhao et al., 2024; Min et al., 2024). In contrast, Satori addresses this problem from reinforcement learning (RL) perspective and requires minimal supervision (only 10K samples in the format tuning stage). The most related concurrent work is DeepSeeks recently released R1 (Guo et al., 2025), which adopts similar high-level strategy of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms. Additionally, DeepSeek-R1 focuses on training large-scale LLMs (671B), whereas our work provides insights into the development of smaller-scale LLMs (7B) for research purpose. Finally, as an industry-developed model, the technical details of DeepSeek-R1 (Guo et al., 2025) are not fully disclosed, making reproduction difficult, whereas our work is fully transparent study that aims to open-source training data and training recipes. Post-training LLMs for Reasoning. Recent advancements have focused on extensive post-training to enhance reasoning. line of work focus on constructing high-quality instruction-tuning datasets (Hendrycks et al., 2021a; Yue et al., 2024; Yu et al., 2024; Toshniwal et al., 2024a; Ding et al., 2024), but suffers from expensive annotatoin costs. More recent research has focused on self-improvement approaches, where models are trained on data generated by themselves (Zelikman et al., 2022; 2024; Singh et al., 2024; Zhang et al., 2024a). Additionally, reinforcement learning methods, particularly those based on Proximal Policy Optimization (PPO) (Schulman et al., 2017a; Ouyang et al., 2022), have been demonstrated to be more effective, which typically leverage reward models to guide the learning process (Sun et al., 2024; Wang et al., 2024a; Yuan et al., 2024). Enabling LLMs with Searching Abilities. Promptingbased approaches (Yao et al., 2024; Shinn et al., 2024; Hao et al., 2023; Qi et al., 2024a) guide LLMs to search for solutions via error correction and exploring alternative paths. However, such approaches cannot fundamentally enhance the LLMs reasoning abilities. Moreover, recent work has pointed out the difficulties of LLMs in self-correction (Zhang et al., 2024b; Kamoi et al., 2024). Recent research has pivoted toward training LLMs for selfexploration. Some focused on enabling trajectory-level searchiteratively identify errors in previous complete responses and produce improved responses (Saunders et al., 2022a; Kumar et al., 2024; Qu et al., 2024; Havrilla et al., 2024). Another line of research has explored step-level search, which enables LLMs to identify and correct mistakes in more fine-grained manner. Some achieve this using another model to provide step-level feedback (Xi et al., 2024; Setlur et al., 2024; Zhang et al., 2024c; Guan et al., 2025; Zhang et al., 2024d), but such two-player frameworks suffer from high costs for model deployment. SoS (Gandhi et al., 2024) is another closely related work that attempts to train single LLM to perform tree search as flattened string. However, the effectiveness of SoS has primarily been shown on simple symbolic tasks, and its ability to generalize to more complex problems remains to be explored. 3. Preliminaries We address mathematical problem-solving by training language model πθ to generate solution that matches the ground truth y, given problem prompt x. All sequences x, y, and consist of tokens from predefined dictionary. Since our approach uses reinforcement learning (RL) to train the model for solving math problems, we outline the key RL concepts below. Reinforcement Learning (RL). RL (Kaelbling et al., 1996) involves an agent making sequential decisions to maxFigure 1: High-level Overview of Satori Training Framework: Format Tuning (FT) + Self-improvement. First, Satori learns COAT reasoning format through imitation learning on small-scale demonstration trajectories. Next, Satori further leverages COAT reasoning format to self-improve via large-scale reinforcement learning. imize the expected cumulative rewards through interactions with an environment. Here, the language model πθ acts as the agents policy. Starting from an initial state z0, at each step l, the agent observes the current state zl, receives reward rl, selects an action based on πθ, transitions to the next state zl+1, and continues until reaching terminal state. trajectory is the sequence of states and actions during this interaction. RL optimizes the policy to maximize the l=1 rl, where is the trajectory length. expected rewards 4. Method (cid:80) We start this section by introducing the formulation of reasoning and how reasoning can be formulated as sequential decision-making problem. Goal: We want to train LLMs to solve problems by reasoning through multiple steps rather than directly predicting the final answer. Given problem statement x, the model generates sequence of , where yL provides the reasoning steps final answer. However, not all intermediate steps are helpfulrepeating errors does not improve accuracy. Effective reasoning requires verifying correctness, identifying mistakes, and considering alternative solutions. For instance, given = 1 + 1 =?, the model might initially output y1 = 3, then recognize the mistake with y2 (e.g., Wait, let me verify...), before correcting it to y3 = 2. y1, y2, . . . , yL} { Chain-of-Action-Thought reasoning (COAT). The key challenge is enabling the model to determine when to reflect, continue, or explore alternatives without external intervention. To enable this, we introduce special meta-action tokens that guide the models reasoning process beyond standard text generation. These tokens serve as hint for the model to determine when to reassess its reasoning before proceeding. Continue Reasoning (<continue>): Encourages the model to build upon its current reasoning trajectory by generating the next intermediate step. Reflect (<reflect>): Prompts the model to pause and verify the correctness of prior reasoning steps. Explore Alternative Solution (<explore>): Signals the model to identify critical flaws in its reasoning and explore new solution. Each reasoning step yl is sequence of tokens, with the starting token potentially being one of the designated metaaction tokens. We refer to this formulation as Chain-ofAction-Thought reasoning (COAT). In particular, typical Chain-of-Thought reasoning (CoT) (Wei et al., 2022) can be viewed as special case of COAT, where each reasoning step in CoT is restricted to continuation, without explicitly incorporating other types of meta-actions. yl, where Learning to Reason via RL. We formulate reasoning as sequential decision-making problem, where reasoning is process of constructing and refining an answer step by step. Specifically, the model πθ starts with an input context (initial state z0), generates reasoning step yl (action), updates the context by appending yl (next state denotes string concatenation), and zl+1 = zl repeats this process until it produces final answer yL. The reasoning terminates when the model signals completion (e.g., omitting EOS token). The simplest reward function can be , evaluates whether the final answer } yL matches the ground truth y. With this formulation, we could train the model to reason using RL, aiming to generate reasoning steps that maximize the expected reward. However, applying RL to reasoning presents two key challenges: yL = { 1. Unawareness of meta-action tokens: The model doesnt understand the purpose of special tokens and fails to recognize that encountering special meta-action tokens may require reflection or proposing alternatives. 2. Long horizon and sparse rewards: Reasoning requires long-term decision-making with rewards only at the end, which hinders learning effectiveness (Bellemare et al., 2016). The model must take many correct reasoning steps before receiving rewards, and failures force it to restart from the initial state (i.e., the problem statement). This makes learning difficult because training data associated with rewards is scarce, yet rewards are essential for driving RL progress. 3 Overview of Proposed Method. To address the models initial unawareness of meta-action tokens, we introduce warm-up format-tuning stage: we fine-tune pre-trained LLM on small dataset featuring few demonstrated reasoning trajectories (Section 4.1). This step familiarizes the model with using and reacting to meta-action tokens. Second, to tackle the challenges of long horizons and sparse rewards, we propose restart and explore (RAE) strategy, inspired by Go-explore (Ecoffet et al., 2019). Here, the model restarts from intermediate steps, including those points where previous reasoning attempts failed, allowing it to focus on correcting errors rather than starting from scratch. We also add exploration bonuses to encourage deeper reflection, further increasing opportunities for the model to arrive at correct answers (Section 4.2). 4.1. Format Tuning Through Imitation Learning Training base LLM πθ to perform COAT reasoning presents significant challenge: LLMs are typically not pre-trained on COAT reasoning data that incorporates trials and errors, necessitating post-training stage to inject this capability. To address this, we introduce format tuning (FT), method designed to train LLMs to emulate expert COAT trajectories through imitation learning. Imitation learning techniques (Hussein et al., 2017) are widely used in the robotics domain, where agents are trained using demonstration trajectories provided by human experts (Ross and Bagnell, 2010; Ross et al., 2011; Ho and Ermon, 2016). However, generating high-quality demonstration trajectories for LLMs is prohibitively expensive for complex tasks. To efficiently construct demonstration trajectory dataset i=1, we propose multi-agent data } synthesis framework that leverages three LLMs: (x(i), y(i)) { syn = Generator: Given an input problem, generator πg generates multiple reasoning paths for given input problem using classical CoT techniques. Critic: critic πc evaluates the correctness of the reasoning paths generated by the generator, providing feedback to refine the reasoning and address suboptimal steps. Reward Model: Additionally, reward model πr assigns scores to the refined reasoning paths and selects the most effective path as the final demonstration trajectory. These three models collaborate to construct high-quality demonstration trajectories (details on the trajectory synthesis are provided in Appendix C). For this work, we adopt the simplest imitation learning approach, behavior cloning, which utilizes supervised fine-tuning to train the LLM policy syn. Noon the expert COAT demonstration trajectories tably, we observe that even small number (10K) of COAT demonstration trajectories is sufficient for πθ to effectively follow the COAT reasoning format. 4.2. Self-improvement via Reinforcement Learning After format tuning, the LLM policy πθ adopts the COAT reasoning style but struggles to generalize, particularly in using meta-actions for self-reflection. This limitation arises from the scarcity of demonstrations during format tuning. While collecting more demonstrations could help, it is costly and time-consuming. Instead, we explore whether the model can self-improve its reasoning via RL. We start with the format-tuned LLM and train it using PPO (Schulman et al., 2017b) algorithm, widely used RL method. In addition to training on problems from the dataset , we also train the model πθ to begin reasoning from partial trajectories generated by the format-tuned LLM. Since reasoning errors typically arise from minor mistakes rather than fundamental flaws, re-exploring from the start is inefficient. Instead, we allow the model to restart from intermediate steps to correct errors and finally achieve correct answers. Inspired by Go-Explore (Ecoffet et al., 2019), we introduce the Restart and Explore (RAE) strategy. Algorithm 1 Restart and Explore (RAE) input Dataset { format tuning; maximum back-track steps (x(i), y(i)) i=1; LLM policy πθ after } = + Initialize restart for = 1, 2, . . . , do ; Initialize restart Given input problem x(i), sample πθ and collect multiple initial trajectories. Randomly select one correct trajectory y+ and one incorrect trajectory y. Randomly backtrack last t, z+ Obtain intermediate states at time-step = [x(i), y+ 2 , . . . , y+ = [x(i), y1 , y2 , . . . , yL t]. t]; zL Add reflect special token to trigger self-reflection action, z+ 2 , . . . , y+ 1 , y+ = [x(i), y+ = [x(i), y1 , y2 , . . . , yL t, <reflect>]. + z+ + t; Update restart restart t, <reflect>]; zL actions from y+ and y. restart t. zL restart 1 , y+ D end restart = x(i) { i=1 + restart } restart. output Augmented initial states dataset restart. Initial States. RAE trains the model to reason not only from the problem statement but also from intermediate steps sampled from past trajectories, both correct and incorrect. This enables deeper exploration without redundant recomputation. As detailed in Algorithm 1, given an input problem , the format-tuned LLM first generates multiple reax soning trajectories. We then randomly backtrack 0 steps and append reflect token <reflect> to prompt the model to refine its reasoning. To encourage diverse exploration, correct and incorrect trajectories are stored sep- + arately in restart buffers ( restart). RL training restart and then optimizes reasoning across these buffers along with the original problem dataset, sampling initial states from the merged dataset restart. Reward Design. RAE gives the model multiple opportunities to refine its reasoning, but effective reflection is key to making use of these chances. In addition to using correctness as rewards, we introduce the following bonuses rewards as hints to guide the model to reach correct answers: Rule-based Reward: Rule-based reward simply evaluates the correctness of the final answer. rrule(yL, y) = 1 yL=y 1 { 1, 0 } Reflection Bonuses: To reinforce self-reflection, we introduce reflection bonus rbonus. If the model starts from an incorrect reasoning trajectory stored in the negative restart buffer ( restart) and successfully solves the problem, it obtains reward bonus, encouraging it to correct past mistakes. Conversely, if it starts from correct tra- + jectory in the positive restart buffer ( restart) but fails to solve the problem, it incurs penalty, discouraging unnecessary revisions when it was already on the right track. Formally, the reflection bonus is defined as: β rbonus(z, y) = if β if otherwise, restart and yL = y, + = y, restart and yL where β is bonus scale hyperparameter. Preference Bonuses: Since correct answers are rare at initial training stage, reward signals are often too sparse for effective RL training. Even with reflection, the model may fail to generate any correct reasoning trajectories, resulting in sparse reward problem. To mitigate this, we train an Outcome Reward Model (ORM) using BradleyTerry (BT) preference framework. The ORM rates reasoning trajectories, assigning higher values to correct (preferred) ones. For each problem , we generate multiple trajectories using πθ and construct preference dataset by pairing correct and incorrect outputs. BT model is trained to maximize the score gap between these pairs. The ORMs output, σ [0, 1], serves as fine-grained reward signal, helping the model further (cid:0) refine its reasoning. See Appendix D.3 for details. rψ(z, y) (cid:1) For an initial state the overall reward function r(z, y) is defined as: restart and sampled trajectory y, r(z, y) = rrule(yL, y) + σ rψ(z, y) + rbonus(z, y) Iterative Self-improvement. RL enables policy to selfimprove from self-generated trajectories, but it can also (cid:0) (cid:1) Figure 2: Number of Training Samples of Satori-Qwen-7B and Qwen-2.5-Math-7B-Instruct. Satori-Qwen-7B requires significantly less supervision (small-scale FT) and relies more on self-improvement (large-scale RL). lead to vicious cycle, where the policy converges to local sub-optimum and cannot further improve. Inspired by (Agarwal et al., 2022; Schmitt et al., 2018), we propose an iterative self-improvement strategy to mitigate this issue. Specifically, after each round of RL training, we distill the knowledge of the current well-optimized policy into the base model through supervised fine-tuning (SFT). Starting from the newly fine-tuned model, we then perform another round of RL training. Intuitively, from an optimization perspective, each round of distillation can be viewed as parameter reset mechanism that helps the policy escape local optima in the loss landscape, allowing it to continue self-improving (more details are included in Section D.3). In the next section, we provide empirical evidence to validate this approach. 5. Experiment Implementation Details. We employ Qwen-2.5-Math-7B as the base model due to its strong mathematical capabilities. Our training data is sourced from the publicly available math instruction datasets, OpenMathInstruct-2 and NuminaMathCoT. For the multi-agent data synthesis framework, the generator is required to generate high-quality, step-by-step reasoning trajectories. Therefore, we use Qwen-2.5-MathInstruct as the generator. Meanwhile, the critic must have robust instruction-following capabilities, so we choose Llama3.1-70B-Instruct as the critic. To ensure data quality, we filter out problems with invalid questions or incorrect labels, resulting in approximately 550k samples. Additional implementation details can be found in Appendix D. Benchmark and Evaluation. We conduct the main evaluation of the models using math benchmarks to assess their problem-solving abilities, including GSM8K, MATH500 (a subset of the MATH test set (Lightman et al., 2023)), AMC2023, AIME2024, and OlympiadBench. Except for GSM8K, all other datasets feature competition-level problems. The evaluation is performed using greedy decoding Table 1: Results on Mathematic Benchmarks. Satori-Qwen-7B achieves SOTA performance across five benchmarks, and outperforms Qwen-2.5-Math-7B-Instruct which uses the same base model Qwen-2.5-Math-7B. After round-2 training, Satori-Qwen-7B (Round 2) demonstrates even stronger performance on hard tasks. Scale Model GSM8K MATH500 OlympiadBench AMC2023 AIME2024 Avg. Large Small GPT-4o o1-preview Llama-3.1-70B-Instruct OpenMath2-Llama3.1-70B QwQ-32B-Preview Llama-3.1-8b-Instruct OpenMath2-Llama3.1-8B NuminaMath-7B-CoT Qwen-2.5-7B-Instruct Qwen-2.5-Math-7B-Instruct Satori-Qwen-7B Satori-Qwen-7B (Round 2) / / 94.1 94.1 95.5 84.4 90.5 78.9 91.6 95.2 93.2 93.9 60.3 85.5 68.0 71.8 90. 51.9 67.8 54.6 75.5 83.6 85.6 83.6 43.3 / 29.4 30.1 61.2 15.1 28.9 15.9 35.5 41.6 46.6 48.5 / 82.5 42.5 45.0 77. 22.5 37.5 20.0 52.5 62.5 67.5 72.5 9.3 44.6 13.3 13.3 50.0 3.3 6.7 10.0 6.7 16.7 20.0 23.3 / / 49.5 50.9 75. 35.4 46.3 35.9 52.4 59.9 62.6 64.4 Table 2: Results on Out-of-domain Benchmarks. Trained only on math datasets, Satori-Qwen-7B exhibits strong transferability across diverse out-of-domain benchmarks and outperforms Qwen-2.5-Math-7B-Instruct by large margin. Moreover, despite not being trained in other domains, Satori-Qwen-7B achieves performance comparable to or exceeding other small-scale general instruct models. Scale Model FOLIO BGQA CRUXEval StrategyQA TableBench STEM Avg. Large Llama-3.1-70B-Instruct OpenMath2-Llama3.1-70B QwQ-32B-Preview Small Llama-3.1-8b-Instruct OpenMath2-Llama3.1-8B NuminaMath-7B-CoT Qwen-2.5-7B-Instruct Qwen-2.5-Math-7B-Instruct Satori-Qwen-7B Satori-Qwen-7B (Round 2) 65.0 68.5 84.2 63.5 57.1 53.2 72.4 68.9 71.4 72.9 58.3 68.7 71.1 50.3 49.0 44.6 53.0 51.3 61.8 58. 59.6 35.1 65.2 38.5 11.1 28.0 58.1 28.0 42.5 41.1 88.8 95.6 88.2 92.2 84.4 77.8 91.3 85.3 86.3 90. 34.2 46.8 51.5 32.4 34.2 29.1 43.2 36.2 43.4 44.6 61.7 15.1 71.3 43.4 10.9 11.3 57.1 45.2 56.7 57. 61.3 55.0 71.9 53.4 41.1 40.7 62.5 52.5 60.4 60.8 without tool integration. The main metric reported is the zero-shot pass@1 accuracy, which measures the percentage of problems correctly solved on the first attempt. We also conduct additional evaluations on wide range of benchmarks beyond the math domain to evaluate general reasoning capabilities. This includes logical reasoning (FOLIO (Han et al., 2024), BoardgameQA (BGQA) (Kazemi et al., 2024)), code reasoning (CRUXEval (Gu et al., 2024)), commonsense reasoning (StrategyQA (STGQA) (Geva et al., 2021)), tabular reasoning (TableBench (Wu et al., 2024a)), and domain-specific reasoning (STEM subsets of MMLUPro (Wang et al., 2024b)), including physics, chemistry, computer science, engineering, biology, and economics. For more evaluation details, please refer to Appendix D.4. Baseline Models. We compare our developed model, Satori-Qwen-7B, with several industry-developed LLMs. The main comparison is between our model and Qwen-2.5Math-7B-Instruct (Yang et al., 2024a), math-specialized model built on the same base model (Qwen-2.5-Math-7B) as ours. Additionally, we report the performance of larger models, including o1-preview and QwQ-32B-Preview, which exhibit strong reasoning capabilities and serve as performance upper bounds. 5.1. Main Results on Math Domain We present math benchmark results in Table 1, where Satori-Qwen-7B outperforms all small-scale baseline models. Notably, using Qwen-2.5-Math-7B as the base model, Satori-Qwen-7B achieves superior performance compared to Qwen-2.5-Math-7B-Instruct, despite requiring significantly less supervision (i.e., less SFT data) and relying more on self-improvement (i.e., more RL data) (see Figure 2). 5.2. Out-of-Domain Transferability Although Satori-Qwen-7B is trained only on math domain datasets, we observe that it can extrapolate its reasoning capabilities to other domains. In Table 2, we evaluate SatoriQwen-7B on diverse set of out-of-domain benchmarks that require reasoning capabilities but are not directly related to math. Similar to the observation on the math domain, Satori demonstrates superior performance on several benchmarks, 6 outperforming Qwen-2.5-Math-7B-Instruct. In particular, on the challenging reasoning benchmark BoardgameQA, Satori-Qwen-7B surpasses all baseline models of the same scale. These results and demo examples in Appendix suggest that Satori has acquired general reasoning capabilities rather than simply math problem solving skills. In Section 6, we present further analysis to show that this transferability emerges as result of large-scale reinforcement learning. 5.3. Results on Iterative Self-improvement Finally, we present the results of the second-round training of Satori. As shown in Table 1 and Table 2, compared to Satori-Qwen-7B, Satori-Qwen-7B (Round 2) demonstrates continuous performance gains across most in-domain and out-of-domain benchmarks. This suggests the significant potential of iterative self-improvement to push the limit of LLMs reasoning performance. 6. Analysis In this section, we provide comprehensive analysis of Satori. First, we demonstrate that Satori effectively leverages self-reflection to seek better solutions and enhance its overall reasoning performance. Next, we observe that Satori exhibits test-scaling behavior through RL training, where it progressively acquires more tokens to improve its reasoning capabilities. Finally, we conduct ablation studies on various components of Satoris training framework. Additional results are provided in Appendix E. Table 3: COAT Training v.s. CoT Training. Qwen-2.5-Math-7B trained with COAT reasoning format (Satori-Qwen-7B) outperforms the same base model but trained with classical CoT reasoning format (Qwen-7B-CoT) Model GSM8K MATH500 Olym. AMC2023 AIME2024 Qwen-2.5-Math-7B-Instruct Qwen-7B-CoT (SFT+RL) Satori-Qwen-7B 95.2 93.1 93.2 83.6 84. 85.6 41.6 42.7 46.6 62.5 60.0 67.5 16.7 10. 20.0 COAT Reasoning v.s. CoT Reasoning. We begin by conducting an ablation study to demonstrate the benefits of COAT reasoning compared to the classical CoT reasoning. Specifically, starting from the synthesis of demonstration trajectories in the format tuning stage, we ablate the reflect and explore actions, retaining only the continue actions. Next, we maintain all other training settings, including the same amount of SFT and RL data and consistent hyper-parameters. This results in typical CoT LLM (Qwen-7B-CoT) without self-reflection or self-exploration capabilities. As shown in Table 3, the performance of Qwen7B-CoT is suboptimal compared to Satori-Qwen-7B and fails to surpass Qwen-2.5-Math-7B-Instruct, suggesting the advantages of COAT reasoning over CoT reasoning. Table 4: Satoris Self-correction Capability. TF: negative self-correction; FT: positive self-correction. In-Domain Out-of-Domain Model MATH500 Satori-Qwen-7B-FT Satori-Qwen-7B T OlympiadBench MMLUProSTEM T F 79.4% 20.6% 65.6% 34.4% 59.2% 39.0% 61.0% 42.1% 57.9% 46.5% 40.8% 53.5% Satori Exhibits Self-correction Capability. We observe that Satori frequently engages in self-reflection during the reasoning process (see demos in Section A), which occurs in two scenarios: (1) it triggers self-reflection at intermediate reasoning steps, and (2) after completing problem, it initiates second attempt through self-reflection. We focus on quantitatively evaluating Satoris self-correction capability in the second scenario. Specifically, we extract responses where the final answer before self-reflection differs from the answer after self-reflection. We then quantify the percentage of responses in which Satoris self-correction is positive (i.e., the solution is corrected from incorrect to correct) or negative (i.e., the solution changes from correct to incorrect). The evaluation results on in-domain datasets (MATH500 and Olympiad) and out-of-domain datasets (MMLUPro) are presented in Table 4. First, compared to Satori-Qwen-FT which lacks the RL training stage, Satori-Qwen demonstrates significantly stronger self-correction capability. Second, we observe that this self-correction capability extends to out-of-domain tasks (MMLUProSTEM). These results suggest that RL plays crucial role in enhancing the models true reasoning capabilities. Figure 3: Policy Training Acc. & Response length v.s. RL Traintime Compute. Through RL training, Satori learns to improve its reasoning performance through longer thinking. RL Enables Satori with Test-time Scaling Behavior. Next, we aim to explain how reinforcement learning (RL) incentivizes Satoris autoregressive search capability. First, as shown in Figure 3, we observe that Satori consistently improves policy accuracy and increases the average length of generated tokens with more RL training-time compute. This suggests that Satori learns to allocate more time to reasoning, Figure 4: Above: Test-time Response Length v.s. Problem Difficulty Level; Below: Test-time Accuracy v.s. Problem Difficulty Level. Compared to FT model (Satori-Qwen-FT), Satori-Qwen uses more test-time compute to tackle more challenging problems. thereby solving problems more accurately. One interesting observation is that the response length first decreases from 0 to 200 steps and then increases. Upon closer investigation of the model response, we observe that in the early stage, our model has not yet learned self-reflection capabilities. During this stage, RL optimization may prioritize the model to find shot-cut solution without redundant reflection, leading to temporary reduction in response length. However, in later stage, the model becomes increasingly good at using reflection to self-correct and find better solution, leading to longer response length. Additionally, in Figure 4, we evaluate Satoris test accuracy and response length on MATH datasets across different difficulty levels. Interestingly, through RL training, Satori naturally allocates more test-time compute to tackle more challenging problems, which leads to consistent performance improvements compared to the format-tuned (FT) model. Table 5: Large-scale FT V.S. Large-scale RL Satori-Qwen (10K FT data + 300K RL data) outperforms same base model Qwen-2.5Math-7B trained with 300K FT data (w/o RL) across all math and out-of-domain benchmarks. (In-domain) GSM8K MATH500 Olym. AMC2023 AIME Qwen-2.5-Math-7B-Instruct Satori-Qwen-7B-FT (300K) Satori-Qwen-7B 95.2 92.3 93.2 83.6 78.2 85.6 41.6 40.9 46.6 62.5 65.0 67.5 16.7 16.7 20. (Out-of-domain) BGQA CRUX STGQA TableBench STEM Qwen-2.5-Math-7B-Instruct Satori-Qwen-7B-FT (300K) Satori-Qwen-7B 51.3 50.5 61. 28.0 29.5 42.5 85.3 74.0 86.3 36.3 35.0 43.4 45.2 47.8 56.7 Large-scale FT v.s. Large-scale RL. We investigate whether scaling up format tuning (FT) can achieve performance gains comparable to RL training. We conduct an ablation study using Qwen-2.5-Math-7B, trained with an equivalent amount of FT data (300K). As shown in Table 5, Figure 5: Format Tuning v.s. Distillation. Distilling from Stronger model (Satori-Qwen-7B) to weaker base models (Llama8B and Granite-8B) are more effective than directly applying format tuning on weaker base models. on the math domain benchmarks, the model trained with large-scale FT (300K) fails to match the performance of the model trained with small-scale FT (10K) and large-scale RL (300K). Additionally, the large-scale FT model performs significantly worse on out-of-domain tasks, demonstrates RLs advantage in generalization. Distillation Enables Weak-to-Strong Generalization. Finally, we investigate whether distilling stronger reasoning model can enhance the reasoning performance of weaker base models. Specifically, we use Satori-Qwen-7B to generate 240K synthetic data to train weaker base models, Llama-3.1-8B and Granite-3.1-8B. For comparison, we also synthesize 240K FT data (following Section 4.1) to train the same models. We evaluate the average test accuracy of these models across all math benchmark datasets, with the results presented in Figure 5. The results show that the distilled models outperform the format-tuned models. This suggests new, efficient approach to improve the reasoning capabilities of weaker base models: (1) train strong reasoning model through small-scale FT and large-scale RL (our Satori-Qwen-7B) and (2) distill the strong reasoning capabilities of the model into weaker base models. Since RL only requires answer labels as supervision, this approach introduces minimal costs for data synthesis, i.e., the costs induced by multi-agent data synthesis framework or even more expensive human annotation. 7. Concluding Remarks The training framework of Satori exhibits significant potential for enhancing LLM reasoning capabilities. The smallscale format tuning stage serves as warm-up phase, allowing the LLM policy to internalize specific reasoning format, while large-scale reinforcement learning (RL) plays crucial role in incentivizing intrinsic reasoning abilities. We believe that this framework can inspire the research community to explore more methods for achieving autoregressive search, such as developing reasoning formats with broader range of meta-actions, designing more advanced RL algorithms, and extending this approach to general domain."
        },
        {
            "title": "References",
            "content": "K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. 1, 24, 36 D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, Measuring mathematical problem solving with the MATH dataset, in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 1, 2, 24, 36 M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., Evaluating large language models trained on code, arXiv preprint arXiv:2107.03374, 2021. 1, 24 T. Y. Zhuo, M. C. Vu, J. Chim, H. Hu, W. Yu, R. Widyasari, I. N. B. Yusuf, H. Zhan, J. He, I. Paul et al., Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions, arXiv preprint arXiv:2406.15877, 2024. 1, 24 S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell et al., FOLIO: Natural language reasoning with first-order logic, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Y. AlOnaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida, USA: Association for Computational Linguistics, November 2024, pp. 22 01722 031. 1, 6, 24, 36 J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, LogiQA: challenge dataset for machine reading comprehension with logical reasoning, in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, C. Bessiere, Ed. ijcai.org, 2020, pp. 36223628. 1, J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in neural information processing systems, vol. 35, pp. 24 82424 837, 2022. 1, 3, 24 X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen, MAmmoTH: Building math generalist models through hybrid instruction tuning, in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1, 2, 24 L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu, MetaMath: Bootstrap your own mathematical questions for large language models, in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1, 2, 24 S. Toshniwal, W. Du, I. Moshkov, B. Kisacanin, A. Ayrapetyan, and I. Gitman, Openmathinstruct-2: Accelerating AI for math with massive open-source instruction data, arXiv preprint arXiv:2410.01560, 2024. 1, 2, 24, 27 Y. Ding, X. Shi, X. Liang, J. Li, Q. Zhu, and M. Zhang, Unleashing reasoning capability of llms via scalable question synthesis from scratch, arXiv preprint arXiv:2410.18693, 2024. 1, 2, X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou, Selfconsistency improves chain of thought reasoning in language models, in The Eleventh International Conference on Learning Representations, 2023. 1 S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, Tree of thoughts: Deliberate problem solving with large language models, Advances in Neural Information Processing Systems, vol. 36, 2024. 1, 2, 24 Z. Wan, X. Feng, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang, Alphazero-like tree-search can guide large language model decoding and training, in Forty-first International Conference on Machine Learning, 2024. 1 Z. Sun, L. Yu, Y. Shen, W. Liu, Y. Yang, S. Welleck, and C. Gan, Easy-to-hard generalization: Scalable alignment beyond human supervision, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 1, 2, 24 P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui, Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), L.-W. Ku, A. Martins, and V. Srikumar, Eds. Bangkok, Thailand: Association for Computational Linguistics, August 2024, pp. 94269439. 1, 2, 24 OpenAI, Learning to reason with llms, 2024, accessed: 2024-12-18. [Online]. Available: https://openai.com/ index/learning-to-reason-with-llms/ Z. Huang, H. Zou, X. Li, Y. Liu, Y. Zheng, E. Chern, S. Xia, Y. Qin, W. Yuan, and P. Liu, O1 replication journeypart 2: Surpassing o1-preview through simple 9 distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. 2, 24 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. 2, 24 Y. Zhao, H. Yin, B. Zeng, H. Wang, T. Shi, C. Lyu, L. Wang, W. Luo, and K. Zhang, Marco-o1: Towards open reasoning models for open-ended solutions, arXiv preprint arXiv:2411.14405, 2024. 2, 24 L. Yuan, W. Li, H. Chen, G. Cui, N. Ding, K. Zhang, B. Zhou, Z. Liu, and H. Peng, Free process rewards without process labels, arXiv preprint arXiv:2412.01981, 2024. 2, Y. Min, Z. Chen, J. Jiang, J. Chen, J. Deng, Y. Hu, Y. Tang, J. Wang, X. Cheng, H. Song et al., Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems, arXiv preprint arXiv:2412.09413, 2024. 2, 24 D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. 2 E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman, STaR: Bootstrapping reasoning with reasoning, in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. 2, 24 E. Zelikman, G. R. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. Goodman, Quiet-STaR: Language models can teach themselves to think before speaking, in First Conference on Language Modeling, 2024. 2, 24 A. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, X. Garcia, P. J. Liu, J. Harrison et al., Beyond human data: Scaling self-training for problem-solving with language models, Trans. Mach. Learn. Res., vol. 2024, 2024. 2, 24 X. Zhang, C. Du, T. Pang, Q. Liu, W. Gao, and M. Lin, Chain of preference optimization: Improving chain-ofthought reasoning in LLMs, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. 2, 24, 35 L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, Training language models to follow instructions with human feedback, in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, Reflexion: Language agents with verbal reinforcement learning, Advances in Neural Information Processing Systems, vol. 36, 2024. 2, 24 S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu, Reasoning with language model is planning with world model, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, H. Bouamor, J. Pino, and K. Bali, Eds. Association for Computational Linguistics, 2023, pp. 81548173. 2, 24 Z. Qi, M. Ma, J. Xu, L. L. Zhang, F. Yang, and M. Yang, Mutual reasoning makes smaller llms stronger problemsolvers, arXiv preprint arXiv:2408.06195, 2024. 2, 24, 27 Y. Zhang, M. Khalifa, L. Logeswaran, J. Kim, M. Lee, H. Lee, and L. Wang, Small language models need strong verifiers to self-correct reasoning, in Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, L. Ku, A. Martins, and V. Srikumar, Eds. Association for Computational Linguistics, 2024, pp. 15 637 15 653. 2, R. Kamoi, Y. Zhang, N. Zhang, J. Han, and R. Zhang, When can llms actually correct their own mistakes? critical survey of self-correction of llms, Transactions of the Association for Computational Linguistics, vol. 12, pp. 14171440, 2024. 2, 24 W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike, Self-critiquing models for assisting human evaluators, arXiv preprint arXiv:2206.05802, 2022. 2, 24 A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs et al., Training language models to self-correct via reinforcement learning, arXiv preprint arXiv:2409.12917, 2024. 2, 24 Y. Qu, T. Zhang, N. Garg, and A. Kumar, Recursive introspection: Teaching language model agents how to self-improve, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 24 10 A. Havrilla, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, and R. Raileanu, GLoRe: When, where, and how to improve LLM reasoning via global and local refinements, in Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 2, Z. Xi, D. Yang, J. Huang, J. Tang, G. Li, Y. Ding, W. He, B. Hong, S. Do, W. Zhan et al., Enhancing llm reasoning via critique models with test-time and training-time supervision, arXiv preprint arXiv:2411.16579, 2024. 2, 24 A. Setlur, C. Nagpal, A. Fisch, X. Geng, J. Eisenstein, R. Agarwal, A. Agarwal, J. Berant, and A. Kumar, Rewarding progress: Scaling automated process verifiers for llm reasoning, arXiv preprint arXiv:2410.08146, 2024. 2, 24 L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal, Generative verifiers: Reward modeling as next-token prediction, in The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. 2, 24 X. Guan, L. L. Zhang, Y. Liu, N. Shang, Y. Sun, Y. Zhu, F. Yang, and M. Yang, rstar-math: Small llms can master math reasoning with self-evolved deep thinking, arXiv preprint arXiv:2501.04519, 2025. 2, 24 D. Zhang, S. Zhoubian, Z. Hu, Y. Yue, Y. Dong, and J. Tang, ReST-MCTS*: LLM self-training via process reward guided tree search, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 24 K. Gandhi, D. H. J. Lee, G. Grand, M. Liu, W. Cheng, A. Sharma, and N. Goodman, Stream of search (SoS): Learning to search in language, in First Conference on Language Modeling, 2024. 2, L. P. Kaelbling, M. L. Littman, and A. W. Moore, Reinforcement learning: survey, Journal of artificial intelligence research, vol. 4, pp. 237285, 1996. 2 M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, Unifying count-based exploration and intrinsic motivation, Advances in neural information processing systems, vol. 29, 2016. 3 A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and new approach for hardJ. Clune, Go-explore: exploration problems, arXiv preprint arXiv:1901.10995, 2019. 4 A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, Imitation learning: survey of learning methods, ACM Computing Surveys (CSUR), vol. 50, no. 2, pp. 135, 2017. 4 S. Ross and D. Bagnell, Efficient reductions for imitation learning, in Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010, pp. 661 668. 4 S. Ross, G. Gordon, and D. Bagnell, reduction of imitation learning and structured prediction to no-regret online learning, in Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011, pp. 627635. J. Ho and S. Ermon, Generative adversarial imitation learning, Advances in neural information processing systems, vol. 29, 2016. 4 J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. 4 R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. Bellemare, Reincarnating reinforcement learning: Reusing prior computation to accelerate progress, Advances in neural information processing systems, vol. 35, pp. 28 95528 971, 2022. 5 S. Schmitt, J. J. Hudson, A. Zidek, S. Osindero, C. Doersch, W. M. Czarnecki, J. Z. Leibo, H. Kuttler, A. Zisserman, K. Simonyan et al., Kickstarting deep reinforcement learning, arXiv preprint arXiv:1803.03835, 2018. 5 H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, I. Sutskever, and T. Lee, J. Leike, J. Schulman, K. Cobbe, Lets verify step by step, arXiv preprint arXiv:2305.20050, 2023. 5, 36 M. Kazemi, Q. Yuan, D. Bhatia, N. Kim, X. Xu, V. Imbrasaite, and D. Ramachandran, Boardgameqa: dataset for natural language reasoning with contradictory information, Advances in Neural Information Processing Systems, vol. 36, 2024. 6, A. Gu, B. Rozi`ere, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang, Cruxeval: benchmark for code reasoning, understanding and execution, arXiv preprint arXiv:2401.03065, 2024. 6, 24, 36 M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant, Did aristotle use laptop? question answering benchmark with implicit reasoning strategies, Transactions of the Association for Computational Linguistics, vol. 9, pp. 346361, 2021. 6, 24, 36 X. Wu, J. Yang, L. Chai, G. Zhang, J. Liu, X. Du, D. Liang, D. Shu, X. Cheng, T. Sun et al., Tablebench: comprehensive and complex benchmark for table question answering, arXiv preprint arXiv:2408.09174, 2024. 6, 24, 36 11 Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang et al., Mmlu-pro: more robust and challenging multi-task language understanding benchmark, arXiv preprint arXiv:2406.01574, 2024. 6, 24, 36 A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin et al., Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement, arXiv preprint arXiv:2409.12122, 2024. E. Glazer, E. Erdil, T. Besiroglu, D. Chicharro, E. Chen, A. Gunning, C. F. Olsson, J.-S. Denain, A. Ho, E. d. O. Santos et al., Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai, arXiv preprint arXiv:2411.04872, 2024. 24 I. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel, S. Bengio, and M. Farajtabar, Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models, arXiv preprint arXiv:2410.05229, 2024. 24 J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., Program synthesis with large language models, arXiv preprint arXiv:2108.07732, 2021. 24 J. Dai, J. Lu, Y. Feng, D. Huang, G. Zeng, R. Ruan, M. Cheng, H. Tan, and Z. Guo, Mhpp: Exploring the capabilities and limitations of language models beyond basic code generation, arXiv preprint arXiv:2405.11430, 2024. 24 C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan, SWE-bench: Can language models resolve real-world github issues? in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 24 C. Liu, J. Shen, H. Xin, Z. Liu, Y. Yuan, H. Wang, W. Ju, C. Zheng, Y. Yin, L. Li et al., Fimo: challenge formal dataset for automated theorem proving, arXiv preprint arXiv:2309.04295, 2023. A. Talmor, J. Herzig, N. Lourie, and J. Berant, Commonsenseqa: question answering challenge targeting commonsense knowledge, arXiv preprint arXiv:1811.00937, 2018. 24 P. Veliˇckovic, A. P. Badia, D. Budden, R. Pascanu, A. Banino, M. Dashevskiy, R. Hadsell, and C. Blundell, The CLRS algorithmic reasoning benchmark, in International Conference on Machine Learning. PMLR, 2022, pp. 22 08422 102. 24 L. Markeeva, S. McLeish, B. Ibarz, W. Bounsi, O. Kozlova, A. Vitvitskyi, C. Blundell, T. Goldstein, A. Schwarzschild, and P. Veliˇckovic, The CLRS-Text algorithmic reasoning language benchmark, arXiv preprint arXiv:2406.04229, 2024. 24 Z. Qi, H. Luo, X. Huang, Z. Zhao, Y. Jiang, X. Fan, H. Lakkaraju, and J. Glass, Quantifying generalization complexity for large language models, arXiv preprint arXiv:2410.01769, 2024. 24 L. Fan, W. Hua, L. Li, H. Ling, and Y. Zhang, NPHardEval: Dynamic benchmark on reasoning ability of large language models via complexity classes, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, L. Ku, A. Martins, and V. Srikumar, Eds. Association for Computational Linguistics, 2024, pp. 40924114. 24 P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning, arXiv preprint arXiv:2209.14610, 2022. X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Loomba, S. Zhang, Y. Sun, and W. Wang, SciBench: Evaluating college-level scientific problemsolving abilities of large language models, in Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 24 D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, GPQA: graduate-level google-proof q&a benchmark, in First Conference on Language Modeling, 2024. 24 O. Tafjord, B. Dalvi, and P. Clark, ProofWriter: Generating implications, proofs, and abductive statements over natural language, in Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, ser. Findings of ACL, C. Zong, F. Xia, W. Li, and R. Navigli, Eds., vol. ACL/IJCNLP 2021. Association for Computational Linguistics, 2021, pp. 3621 3634. 24 D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, Measuring massive multitask language understanding, in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 24, 36 A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown et al., Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, Trans. Mach. Learn. Res., vol. 2023, 2023. 24 Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 24 P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga et al., Holistic evaluation of language models, Trans. Mach. Learn. Res., vol. 2023, 2023. 24 L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, S. Shi, M. Choi, A. Agrawal, A. Chopra, A. Khoja, R. Kim, J. Hausenloy et al., Humanitys last exam, 2025. [Online]. Available: https://arxiv.org/abs/2501.14249 24 W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike, Self-critiquing models for assisting human evaluators, arXiv preprint arXiv:2206.05802, 2022. 24 S. Toshniwal, I. Moshkov, S. Narenthiran, D. Gitman, F. Jia, and I. Gitman, OpenMathInstruct-1: 1.8 million math instruction tuning dataset, in The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang, Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct, arXiv preprint arXiv:2308.09583, 2023. 24 M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann et al., Phi-4 technical report, arXiv preprint arXiv:2412.08905, 2024. 24 I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson, The curse of recursion: Training on generated data makes models forget, arXiv preprint arXiv:2305.17493, 2023. 24 T. Wu, X. Li, and P. Liu, Progress or regress? selfimprovement reversal in post-training, arXiv preprint arXiv:2407.05013, 2024. 24 M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk et al., Graph of thoughts: Solving elaborate problems with large language models, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 68217 690. A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang et al., Self-refine: Iterative refinement with selffeedback, Advances in Neural Information Processing Systems, vol. 36, 2024. 24 L. Yang, Z. Yu, T. Zhang, S. Cao, M. Xu, W. Zhang, J. E. Gonzalez, and B. CUI, Buffer of Thoughts: Thoughtaugmented reasoning with large language models, in The J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou, Large language models cannot self-correct reasoning yet, in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 24 S. Welleck, X. Lu, P. West, F. Brahman, T. Shen, D. Khashabi, and Y. Choi, Generating sequences by learning to self-correct, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 24 D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings, REFINER: reasoning feedback on intermediate representations, in Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St. Julians, Malta, March 17-22, 2024, Y. Graham and M. Purver, Eds. Association for Computational Linguistics, 2024, pp. 11001126. 24 D. Zhang, J. Wu, J. Lei, T. Che, J. Li, T. Xie, X. Huang, S. Zhang, M. Pavone, Y. Li et al., Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning, arXiv preprint arXiv:2410.02884, 2024. J. LI, E. Beeching, L. Tunstall, B. Lipkin, R. Soletskyi, S. C. Huang, K. Rasul, L. Yu, A. Jiang, Z. Shen, Z. Qin, B. Dong, L. Zhou, Y. Fleureau, G. Lample, and S. Polu, NuminaMath, [https: //huggingface.co/AI-MO/NuminaMath-CoT](https: //github.com/project-numina/aimo-progress-prize/blob/ main/report/numina dataset.pdf), 2024. 27 Qwen, QwQ: Reflect deeply on the boundaries of the unknown, November 2024. [Online]. Available: https://qwenlm.github.io/blog/qwq-32b-preview/ 27 D. Kocetkov, R. Li, L. B. allal, J. LI, C. Mou, Y. Jernite, M. Mitchell, C. M. Ferrandis, S. Hughes, T. Wolf, D. Bahdanau, L. V. Werra, and H. de Vries, The Stack: 3 TB of permissively licensed source code, Transactions on Machine Learning Research, 2023. 27 Y. Zheng, R. Zhang, J. Zhang, Y. Ye, and Z. Luo, LlamaFactory: Unified efficient fine-tuning of 100+ language models, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Y. Cao, Y. Feng, and D. Xiong, Eds. Bangkok, Thailand: Association for Computational Linguistics, August 2024, pp. 400410. 35 13 J. Hu, X. Wu, Z. Zhu, Xianyu, W. Wang, D. Zhang, and Y. Cao, OpenRLHF: An easy-to-use, scalable and high-performance rlhf framework, arXiv preprint arXiv:2405.11143, 2024. P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, Deep reinforcement learning from human preferences, Advances in neural information processing systems, vol. 30, 2017. 35 C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang et al., Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems, arXiv preprint arXiv:2402.14008, 2024."
        },
        {
            "title": "Appendix",
            "content": "A Satoris Demo Examples Additional Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Post-training LLMs for Reasoning . B.2 Enabling LLMs with Searching Abilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details about Data Synthesis Framework Experimental Setup . . . . . . D.1 Data Processing . . . D.1.1 Prompt Templates . D.1.2 Synthetic Data Examples . . . . . D.2 Format Tuning . D.3 Reinforcement Learning . . D.4 Evaluation Details. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Results E.1 Ablation on Reflection Bonus . E.2 Offline Restart Buffer v.s. Online Restart Buffer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 24 24 24 25 27 27 28 31 35 35 36 37 37 A. Satoris Demo Examples Figure 6: Math Domain Example. Satori verifies the correctness of the intermediate steps and proceeds to the next reasoning step. 15 Figure 7: Math Domain Example. Satori identifies the mistakes in the previous solution and proposes an alternative correct solution. 16 Figure 8: Math Domain Example. Satori verifies the correctness of previous solution and initiates different solution. Figure 9: Math Domain Example. Satori verifies the correctness of previous solution and further explores simpler solution. 17 Figure 10: Math Domain Example. 1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution. 18 Figure 11: Out-of-domain Example. 1) Satori identifies the potential mistakes in intermediate steps and initiates another solution. 2) Satori realizes that the pervious solution is still erroneous and then proposes an alternative correct solution. Figure 12: Out-of-domain Example. Satori identifies the potential mistakes in intermediate steps and initiates another correct solution. 19 Figure 13: Out-of-domain Example. 1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution. 20 Figure 14: Out-of-domain Example. Satori engages in multiple self-reflection processes during intermediate reasoning steps. 21 Figure 15: Out-of-domain Example. 1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution. 22 Figure 16: Out-of-domain Example. Satori identifies the mistakes in previous solution and proposes an alternative correct solution. 23 B. Additional Related Work B.1. Post-training LLMs for Reasoning State-of-the-art LLMs have achieved human-level and, in some cases, superhuman performance across diverse reasoning benchmarks. These benchmarks span various domains, including mathematics (Cobbe et al., 2021; Hendrycks et al., 2021a; Glazer et al., 2024; Mirzadeh et al., 2024), programming (Chen et al., 2021; Austin et al., 2021; Zhuo et al., 2024; Dai et al., 2024; Jimenez et al., 2024; Gu et al., 2024), logical reasoning (Han et al., 2024; Liu et al., 2020; 2023; Tafjord et al., 2021), commonsense reasoning (Geva et al., 2021; Talmor et al., 2018), algorithmic reasoning (Veliˇckovic et al., 2022; Markeeva et al., 2024; Qi et al., 2024b; Fan et al., 2024), semi-structured data (Wu et al., 2024a; Lu et al., 2022), scientific knowledge (Wang et al., 2024c; Rein et al., 2024), and world knowledge (Hendrycks et al., 2021b; Wang et al., 2024b; Srivastava et al., 2023; Liang et al., 2023; Phan et al., 2025). Recent advancements have concentrated on extensive post-training to enhance LLMs reasoning abilities. One research direction in this area involves constructing instruction-tuning datasets annotated with high-quality CoT-like reasoning chains. These datasets are created either through extensive human annotation (Hendrycks et al., 2021a; Saunders et al., 2022b; Yue et al., 2024) or by distilling data from more advanced models (Yu et al., 2024; Toshniwal et al., 2024b;a; Ding et al., 2024; Luo et al., 2023; Huang et al., 2024a; Zhao et al., 2024; Abdin et al., 2024; Min et al., 2024). However, human annotation is resource-intensive, and model-generated data inherently caps the student models potential at the level of the teacher model. More recent research has focused on self-improvement approaches, where models are trained on data generated by themselves (Zelikman et al., 2022; 2024; Singh et al., 2024; Zhang et al., 2024a). While self-training mitigates the reliance on external resources, it has raised concerns about potential model collapse, phenomenon where the iterative use of model-generated data degrades performance (Shumailov et al., 2023; Wu et al., 2024b). Additionally, reinforcement learning methods, particularly those based on Proximal Policy Optimization (PPO) (Schulman et al., 2017a; Ouyang et al., 2022), have been explored to enhance reasoning capabilities. These approaches typically utilize reward models, such as Process-Reward Models (PRMs) or Outcome-Reward Models (ORMs), to guide the learning process (Sun et al., 2024; Wang et al., 2024a; Yuan et al., 2024), resulting in significant performance improvements compared to supervised fine-tuning. B.2. Enabling LLMs with Searching Abilities Chain-of-Thought (CoT) prompting (Wei et al., 2022) demonstrated its potential to improve reasoning but lacked mechanisms to correct previous errors once committed. To address this, subsequent work proposed more sophisticated methods (Yao et al., 2024; Shinn et al., 2024; Besta et al., 2024; Madaan et al., 2024; Yang et al., 2024b) that prompt LLMs to search for solutions via forward exploration, backtracking from errors, and finding alternate paths. Heuristic search methods (Hao et al., 2023; Qi et al., 2024a) have also been adopted to enable more effective exploration of high-quality solutions. However, prompting-based approaches improve task-specific performance without fundamentally enhancing the LLMs intrinsic reasoning capabilities. Moreover, recent work has pointed out the inherent difficulties of current LLMs in conducting effective self-correction (Huang et al., 2024b; Zhang et al., 2024b; Kamoi et al., 2024). Recent research has pivoted toward training LLMs explicitly for exploration and backtracking. large body of work has focused on enabling trajectory-level search abilities, which train LLMs to iteratively identify errors in their previous complete responses and produce improved responses, relying on either human-annotated revisions (Saunders et al., 2022a) or model-generated data (Kumar et al., 2024; Qu et al., 2024; Havrilla et al., 2024) as training data. Another line of research has investigated step-level search techniques, which induce more fine-grained and real-time correction that enables LLMs to identify and correct mistakes once they occur. Some achieve this by leveraging another model to provide step-level feedback to an actor model in the reasoning process (Xi et al., 2024; Welleck et al., 2023; Paul et al., 2024; Zhang et al., 2024e; Setlur et al., 2024; Zhang et al., 2024c; Guan et al., 2025; Zhang et al., 2024d), but such two-player frameworks suffer from high costs for model deployment. The most related to our work is SoS (Gandhi et al., 2024), which attempted to train single LLM to perform tree search as flattened string. However, the effectiveness of SoS has primarily been demonstrated on simple symbolic tasks, and the ability to generalize to more complex problems, such as math word problems, remains to be explored. Figure 17: Demonstration Trajectories Synthesis. First, multiple initial reasoning trajectories are sampled from the generator and sent to critic to ask for feedback. The critic model identifies the mistake for trajectories with incorrect final answers and proposes an alternative solution. For trajectories with correct final answers, the critic model provides verification of its correctness. Based on the feedback, the generator self-refines its previous trajectories, and the incorrect trajectories are sent to the critic again for additional feedback with maximum iterations. At each step, those trajectories with successful refinements are preserved and finally, reward model rates and collects high-quality demonstration trajectories to form the synthetic dataset Dsyn. C. Details about Data Synthesis Framework Sample Initial Trajectories. The details of data synthesis framework are illustrated in Figure 17. Given an input problem , we begin by sampling the generator πg to generate initial reasoning trajectories. For each trajectory x), we evaluate whether the final answer yL matches the ground-truth answer y. Based = [y1, y2, . . . , yL] on the evaluation, the generated trajectories are divided into two subsets according to their correctness, which are then processed differently in subsequent steps. πg( Critic and Refinement. For those incorrect trajectories, the critic πc provides feedback to help the generator address its flaws. Specifically, the critic, given the ground-truth solution, identifies the first erroneous step yl and generates summary x, y1, . . . , yl; y). yl+1 of the mistake as reflection, along with exploration direction (hint), yl+2, i.e., [yl+1, yl+2] Next, we ask the generator πg to self-refine its current trajectory based on the feedback provided by the critic, performing conditional generation of the remaining reasoning steps, [yl+3, . . . , yL] πg( πc( x, y1, . . . , yl; yl+1, yl+2). For correct trajectories, the critic πc focuses on verifying the correctness of the generators reasoning. random intermediate reasoning step yl is selected, and the critic provides summary explaining why the preceding steps are progressing toward x, y1, . . . , yl; y), where yl+1. Similarly, the generator continues from the current the correct solution, i.e., yl+1 solution, generating the subsequent steps as [yl+2, . . . , yL] Finally, we check whether the final answer yL aligns with y. The above procedure is repeated iteratively, up to maximum of iterations, until the generator produces the correct final answer. All feedback and refinements are then contaminated to synthesize the final demonstration trajectories. Additionally, the trajectories are post-processed by inserting meta-action tokens at the beginning of each reasoning step to indicate its meta-action type. x, y1, . . . , yl; yl+1). πg( πc( 25 Trajectory Filtering. The above procedure may yield multiple demonstration trajectories for each problem x. We then select the top-k (k < K) trajectories based on the reward scores = πr(y, x) assigned by the reward model πr. This approach allows us to construct diverse synthetic dataset syn containing high-quality demonstration trajectories, including (1) short-cut COAT paths that boil down CoT reasoning paths and (2) more complex COAT paths involving multiple rounds of self-reflection and exploration of alternative solutions. 26 D. Experimental Setup D.1. Data Processing Data Source. We construct our training dataset by combining two open-source synthetic datasets: OpenMathInstruct2 (Toshniwal et al., 2024a) and NuminaMath-COT (LI et al., 2024). After careful review of the synthetic data, we identify and remove invalid questions to improve data reliability. To further enhance the quality of the dataset, we adopt the mutual consistency filtering method inspired by rStar (Qi et al., 2024a), which removes examples with inconsistent answers provided by different models. Specifically, we utilize QwQ (Qwen, 2024) to relabel the questions and compared the newly generated answers with the original answers from the source datasets. Only examples with consistent answers were retained. Additionally, we apply de-duplication tools from (Kocetkov et al., 2023) to eliminate redundant examples. Through these filtering processes, we finalized high-quality dataset with approximately 550K samples in total. Multi-agent COAT Data Synthesis. For the multi-agent demonstration trajectory synthesis framework, we utilize three models: Qwen-2.5-Math-7B-Instruct as the generator, Llama-3.1-70B-Instruct as the critic, and Skywork-Reward-Llama3.1-8B-v0.2 as the outcome reward model. For the generator, we set the temperature to 0.3 and the maximum generation token limit to 2048. First, the generator samples = 100 initial solutions for each problem, dividing the generated solutions into three subsets: correct, incorrect, and invalid (those that fail to produce final answer). Invalid solutions are discarded, and we randomly select four correct solutions and four incorrect solutions. Next, we set sampling temperature of critic to 0.2 and maximum token limit of 256, and let critic provides feedback on these selected solutions, allowing the generator to perform conditional generation. This iterative process is repeated for maximum of = 2 iterations, potentially resulting in up to 2 = 16 demonstration trajectories. During the generation process, various situations require different prompt templates: 1. The generator produces an initial solution. 2. The initial solution is correct, and the critic verifies its correctness. 3. The initial solution is incorrect, and the critic identifies mistakes. 4. The generator generates continuations after the critic verifies the correctness of its correct initial solution. 5. The generator generates continuations after the critic identifies mistakes in its incorrect initial solution. 6. The generator fails to solve the problem after refinement, and the critic provides an additional feedback to identify errors in the generators second attempt. The prompt templates for these situations are detailed in Appendix D.1.1. Among the synthetic trajectories, we categorize them into the following types: 1. Type-I: Synthetic trajectories without critic feedback, i.e., no reflection actions. 2. Type-II-I: Synthetic trajectories that include an intermediate reflection action to verify the correctness of previous reasoning steps. 3. Type-II-II: Synthetic trajectories that include 1) an intermediate reflection action to verify the correctness of previous reasoning steps, and 2) second reflection action to correct mistakes in the previous solution, followed by an explore action to propose an alternative solution. 4. Type-III-I: Synthetic trajectories that include reflection action to correct mistakes in the previous solution and an explore action to propose an alternative solution. 5. Type-III-II: Synthetic trajectories that include two rounds of self-reflection and self-explore. Examples of these five types of synthetic trajectories are provided in Appendix D.1.2. Finally, the outcome reward model is applied to select the top-1 (k=1) sample of each type from the 16 demonstration trajectories based on the reward score, if such type exists. 27 D.1.1. PROMPT TEMPLATES Prompt Template 1.1 Generator generates initial solution <im_start>user Solve the following math problem efficiently and clearly. Please reason step by step, and put your final answer within boxed{}. Problem: <<<instruction>>><im_end> <im_start>assistant Prompt Template 1.2.1 Generator generates continuations for correct partial solutions <im_start>system Your task is to continue writing solution to problem. Given problem, correct partial solution, along with sanity check of previous steps, you should continue the current solution to solve the problem. Think step by step using English and adhere to the following format. Step: [Brief explanation and calculations] Step: [Brief explanation and calculations] ... **Please do not use other language than English.** Your solution should conclude with \"Therefore, the final answer is: (boxed{answer})\", where [answer] is the final number or expression that solves the problem. All steps in the solutions should be brief and always start with \"Step:\" and end with two line breaks. Each subsequent step should not explicitly refer to previous steps, and the steps should constitute coherent progression towards the final answer.<im_end> <im_start>user ## Problem $$frac{16ˆm}{4ˆn}=2$$ Find $m+n$. ## Partial solution with sanity check Step: First, we express 16 and 4 as powers of 2: [ 16 = 2ˆ4 ] [ 4 = 2ˆ2 ] Step: Substitute these expressions into the given equation: [ frac{(2ˆ4)ˆm}{(2ˆ2)ˆn} = 2 ] Step: Use the power of power property ((aˆm)ˆn = aˆ{mn}) to simplify the exponents: [ frac{2ˆ{4m}}{2ˆ{2n}} = 2 ] Step: Use the property of exponents (frac{aˆm}{aˆn} = aˆ{m-n}) to combine the exponents: [ 2ˆ{4m-2n} = 2ˆ1 ] Step: Since the bases are the same, we can set the exponents equal to each other: [ 4m - 2n = 1 ] correctly simplified the exponents and equated (4m - 2n) to 1, which is essential to find the relationship between (m) and (n).<im_end> <im_start>assistant ## Continued solution Step: To find ( + ), we need to solve this equation for ( ) and ( ). First, divide the entire equation by 2 to simplify: [ 2m - = frac{1}{2} ] Step: Add ( ) to both sides to isolate ( 2m ): [ 2m = + frac{1}{2} ] Step: Add ( ) to both sides to find ( + ): [ + = frac{1}{2} + frac{1}{2} ] Step: Simplify the right side: [ + = 1 ] Therefore, the value of ( + ) is: [ boxed{1} ]<im_end> <im_start>user ## Problem <<<instruction>>> ## Partial solution with sanity check <<<trajectory>>><im_end> <im_start>assistant ## Continued solution Step: 28 Prompt Template 1.2.2 Generator generates continuations for incorrect partial solutions <im_start>system Your task is to continue writing solution to problem. Given problem, an incorrect partial solution where the last step is wrong, along with revision feedback and hint for next steps, you should continue the solution using the hint. Do not directly copy contents under \"## Incorrect partial solution with revision feedback and hint for next steps\". Think step by step using English and adhere to the following format. Step: [Brief explanation and calculations] Step: [Brief explanation and calculations] ... **Please do not use other language than English.** Your solution should conclude with \"Therefore, the final answer is: (boxed{answer})\", where [answer] is the final number or expression that solves the problem. All steps in the solutions should be brief and always start with \"Step:\" and end with two line breaks. Each subsequent step should not explicitly refer to previous steps, and the steps should constitute coherent progression towards the final answer.<im_end> <im_start>user ## Problem Devanti wears her hair in various styles. sleek bun is tied in top of the head, half on the other with long extension hanging down to the shoulders. To prepare for his next photo shoot, Devanti decides to ward off few days of the daily grays until it reaches there full potential. Research shows that one-fourth of the gray hairs are deposited on the first day, and the second day is when half the remaining hair has disappeared, leaving Devanti with 15 gray hairs on her full head. Calculate the total number of the gray hairs that Devanti was able to tadpoles from her hair. ## Incorrect partial solution with revision feedback and hint for next steps Step: Let ( ) be the total number of gray hairs Devanti had initially. Step: On the first day, one-fourth of the gray hairs are deposited on the head. Therefore, the number of gray hairs remaining after the first day is: [ - frac{1}{4}x = frac{3}{4}x ] Wait. subtracted one-fourth of instead of adding it, but gray hairs are deposited (added) on the first day. Alternatively, should add one-fourth of to find the total number of gray hairs after the first day.<im_end> <im_start>assistant ## Continued solution Step: Let me correct the first days calculation by adding one-fourth of the gray hairs. [ text{Initial gray hairs} = ] [ text{Gray hairs deposited on the first day} = frac{1}{4}x ] [ text{Total gray hairs after the first day} = + frac{1}{4}x = frac{5}{4}x ] Step: Then, calculate the number of gray hairs remaining after the second day, where half of them disappear. [ text{Gray hairs after the second day} = frac{1}{2} times frac{5}{4}x = frac{5}{8}x ] Step: Next, set up the equation based on the information that 15 gray hairs remain after the second day. [ frac{5}{8}x = 15 ] Step: Finally, lets solve for ( ) to find the initial number of gray hairs. [ = 15 times frac{8}{5} = 24 ] Therefore, the final answer is: (boxed{24}).<im_end> <im_start>user ## Problem <<<instruction>>> ## Incorrect partial solution with revision feedback and hint for next steps <<<trajectory>>><im_end> <im_start>assistant ## Continued solution Step: 29 Prompt Template 2.1 Critic verifies correctness ## General Guidelines You are student. Your task is to carefully review your own correct partial solution to math problem, and adhere to the following guidelines: 1. Verify the correctness of your own solution and explain your reason: \"Verify: [brief explanation of why you are correct with one sentence]\" 3. You are provided with the question, the ground truth solution, and your step-by-step partial solution. 4. Your response should not include phrases like \"ground truth solution\". 5. Your response should be exactly in the following format: Verify: [brief explanation of why you are correct with one sentence] ## Test Example ### Question <<<question>>> ### Ground truth solution <<<gt_solution>>> ### Your partial solution <<<student_solution>>> ### Your review Prompt Template 2.2.1 Critic corrects mistakes ## General Guidelines You are student. Your task is to carefully review your own solution to math problem, and adhere to the following guidelines: 1. Directly point out the first potentially incorrect step you find and explain your reason: \"In Step <id>: [brief explanation of the mistake with one sentence]\" 2. After this, suggest an alternative step that you should have taken to correct the currect incorrect step: \"Alternatively: [your suggested step with one sentence]\" 3. You are provided with the question, the ground truth solution, and your step-by-step solution. 4. The alternative step you propose should not include phrases like \"ground truth solution\". 5. Your response should be exactly in the following format: In Step <id>: [brief explanation of the mistake in this step, with one sentence] Alternatively: [your suggested new step, with one sentence] ## Test Example ### Question <<<question>>> ### Ground truth solution <<<gt_solution>>> ### Your incorrect solution <<<student_solution>>> ### Your review 30 Prompt Template 2.2.2 Critic corrects mistakes for second round You are student in math class. You are collaborating with partner to solve math problems. First, your partner provided partial solution to math problem and identified mistake. Then, you continue your partners partial solution and finish solving the problem. Now, your task is to carefully review your continued solution, adhering to the following guidelines. ## Guidelines 1. \"Your partners partial solution\" includes your partners partial solution to the problem. It might be long and contain several substeps. \"Your continued solution\" includes your completion of the problem based on your partners partial solution. 2. Your task is NOT to check the correctness of \"Your partners partial solution\", but to directly point out the first potentially incorrect step in \"Your continued solution\", and explain the reason why you made the mistake: \"In Step <id>: [brief explanation of the mistake in your own solution with one sentence]\". 3. After this, suggest an alternative step that you should have taken to correct your incorrect step: \"Alternatively: [your suggested step with one sentence]\" 4. You are provided with the question, the ground truth solution, and your partners partial solution, and your continued solution. 5. The alternative step you propose SHOULD NOT explicitly include phrases like \"ground truth solution\", \"as shown in the ground truth solution\", etc. 6. You should only focus on your own solution. DO NOT refer to any mistake in your partners partial solution. 7. Your response should be exactly in the following format: In Step <id>: [brief explanation of the mistake in your own solution with one sentence] Alternatively: [your suggested new step with one sentence] ## Test Example ### Question <<<question>>> ### Ground truth solution <<<gt_solution>>> ### Your partners partial solution <<<partner_solution>>> ### Your continued solution <<<student_solution>>> ### Your review D.1.2. SYNTHETIC DATA EXAMPLES Synthetic Data Example Type I: No reflection [Input] Five couples plan to take their boats on trip down river. Each boat can carry 4 people. What is the smallest number of boats that can accommodate all five couples? [Output] First, we determine the total number of people. Since there are five couples, and each couple consists of 2 people, the total number of people is: [ 5 times 2 = 10 ]<continue> Next, we need to figure out how many boats are required to carry all 10 people. Each boat can carry 4 people. To find the minimum number of boats, we divide the total number of people by the capacity of each boat: [ frac{10}{4} = 2.5 ]<continue> Since we cannot have half boat, we need to round up to the next whole number. Therefore, we need 3 boats to accommodate all 10 people. To verify, lets consider how the people can be distributed among the boats: - Boat 1: 4 people - Boat 2: 4 people - Boat 3: 2 people All 10 people are accounted for, and no boat exceeds its capacity. Thus, the smallest number of boats that can accommodate all five couples is: [ boxed{3} ] Synthetic Data Example Type II-I: Verify correctness [Input] library has total of $50$ books, each of which is categorized as biography, novel, or self-help book. The librarys patrons borrow $30$ books in one week, and the ratio of borrowed biographies to borrowed novels to borrowed self-help books is $2:3:5$. If the library has an equal number of biographies, novels, and self-help books, how many self-help books were borrowed in that week? [Output] First, we determine the number of biographies, novels, and self_help books in the library. Since the library has an equal number of each type of book and the total number of books is 50, we can divide 50 by 3: [ text{Number of biographies} = text{Number of novels} = text{Number of self_help books} = frac{50}{3} ] However, since the number of books must be an integer and the problem states that the library has an equal number of each type of book, we need to re-evaluate the problem statement. The problem states that the library has an equal number of biographies, novels, and self_help books, but the ratio of borrowed biographies to borrowed novels to borrowed self_help books is (2:3:5). This means the total number of books borrowed must be multiple of (2+3+5=10). Since 30 books were borrowed, it is indeed multiple of 10. <reflect> Let me stop here to verify. The calculation of the number of each type of book in the library is correct, but the focus should be on the ratio of borrowed books, which is consistent with the given total of 30 borrowed books.<continue> Let ( ), ( ), and ( ) represent the number of biographies, novels, and self_help books borrowed respectively. According to the given ratio (2:3:5), we can write: [ = 2x ] [ = 3x ] [ = 5x ]<continue> The total number of books borrowed is 30: [ + + = 30 ]<continue> Substitute the expressions for ( ), ( ), and ( ) into the equation: [ 2x + 3x + 5x = 30 ]<continue> Combine like terms: [ 10x = 30 ]<continue> Solve for ( ): [ = frac{30}{10} = 3 ]<continue> Find the number of self_help books borrowed: [ = 5x = 5 times 3 = 15 ] Therefore, the number of self_help books borrowed in that week is: [ boxed{15} ] Synthetic Data Example Type II-II: Verify correctness then correct mistake [Input] Consider the set of all positive integers whose digits, when read from left to right, form valid English word. For example, 325 is member of this set because \"three two five\" is valid English phrase. What is the largest member of this set that is also perfect square? [Output] To find the largest member of the set of positive integers whose digits form valid English word and is also perfect square, we need to consider the constraints of the problem. The number must be perfect square and its digits must form valid English word. <reflect> want to double-check my calculations. The step correctly identifies the two main constraints of the problem: the number must be perfect square and its digits must form valid English word.<continue> We start by checking the largest perfect squares in descending order until we find one whose digits form valid English word. <continue> Lets check ( 9801 ): [ 9801 = 99ˆ2 ] The digits are 9, 8, 0, 1. \"Nine eight zero one\" is valid English phrase. Since ( 9801 ) meets both conditions (it is perfect square and its digits form valid English word), it is the largest such number. Therefore, the largest member of the set that is also perfect square is: [ boxed{9801} ]<reflect> But wait. The digits 9, 8, 0, 1 do not form valid English word because \"zero\" is not valid word in this context. <explore> will now explore another method. We should check the next largest perfect square, which is 961, and verify if its digits form valid English word.<continue> Lets check ( 961 ): [ 961 = 31ˆ2 ] The digits are 9, 6, 1. \"Nine six one\" is valid English phrase. Since ( 961 ) meets both conditions (it is perfect square and its digits form valid English word), it is the largest such number. Therefore, the largest member of the set that is also perfect square is: [ boxed{961} ] 32 Synthetic Data Example Type III-I: Correct mistake [Input] Given $A=left{xin mathbb{N}ˆ* mid -xˆ{2}+3xgeqslant 0right}$ and $B=left{x mid log_{frac{1}{2}}xleqslant 0right}$, then $Acap B=$ A: $left[3,+infty right)$ B: $left[0,1right]$ C: $left[1,3right]$ D: $left{1,2,3right}$ [Output] First, we solve the inequality for set ( ): [ -xˆ2 + 3x geq 0 ]<continue> Factor out the common term: [ x(-x + 3) geq 0 ]<continue> Determine the critical points by setting each factor to zero: [ = 0 quad text{or} quad -x + 3 = 0 implies = 3 ]<continue> The critical points divide the real number line into three intervals: ( (-infty, 0) ), ( (0, 3) ), and ( (3, infty) ). We test point from each interval in the inequality ( x(-x + 3) geq 0 ). - For ( in (-infty, 0) ), choose ( = -1 ): [ (-1)(-1 + 3) = (-1)(2) = -2 quad (text{not} geq 0) ] - For ( in (0, 3) ), choose ( = 1 ): [ (1)(1 + 3) = (1)(2) = 2 quad (geq 0) ] - For ( in (3, infty) ), choose ( = 4 ): [ (4)(4 - 3) = (4)(1) = 4 quad (geq 0) ]<continue> Since the inequality is non-strict ((geq)), we include the critical points ( = 0 ) and ( = 3 ). Therefore, the solution to the inequality is: [ = [0, 3] ]<continue> Next, we solve the inequality for set ( ): [ log_{frac{1}{2}} leq 0 ]<continue> Recall that the base of the logarithm is (frac{1}{2}), which is less than 1. For logarithm with base less than 1, the inequality (log_{frac{1}{2}} leq 0) implies: [ geq 1 ] <reflect> may have overlooked something. The solution to the inequality incorrectly includes the critical point ( = 0 ) because ( in mathbb{N}ˆ* ) (the set of positive natural numbers), and ( 0 ) is not positive natural number.<explore> To find better solution, The solution to the inequality should be ( = left{1, 2, 3right} ) since ( ) must be natural number and ( 0 < leqslant 3 ).<continue> Correct the solution for set ( ) considering ( in mathbb{N}ˆ* ): [ = {1, 2, 3} ]<continue> Set ( ) remains the same: [ = [1, infty) ]<continue> Find the intersection ( cap ): [ cap = {1, 2, 3} cap [1, infty) = {1, 2, 3} ] Therefore, the final answer is: [ boxed{D} ] 33 Synthetic Data Example Type III-II: Correct mistake multiple times [Input] In $triangle ABC$, $a$, $b$, $c$ are the sides opposite to the angles $A$, $B$, $C$ respectively. Given $c=4$, $tan A=3$, $cos C= frac{ sqrt{5}}{5} $, find the area of $triangle ABC$. [Output] First, we find (sin C) using the Pythagorean identity (sinˆ2 + cosˆ2 = 1): [ sinˆ2 = 1 - cosˆ2 = 1 - left(frac{sqrt{5}}{5}right)ˆ2 = 1 - frac{5}{25} = frac{20}{25} = frac{4}{5} ] Since (C) is an angle in triangle, (sin C) is positive: [ sin = frac{2}{5} ]<continue> Next, we find (sin A) using the given (tan = 3). Recall that (tan = frac{sin A}{cos A}). Let (sin = 3k) and (cos = k). Using the Pythagorean identity (sinˆ2 + cosˆ2 = 1): [ (3k)ˆ2 + kˆ2 = 1 implies 9kˆ2 + kˆ2 = 1 implies 10kˆ2 = 1 implies kˆ2 = frac{1}{10} implies = frac{1}{sqrt{10}} = frac{sqrt{10}}{10} ] Thus, (sin = 3k = 3 cdot frac{sqrt{10}}{10} = frac{3sqrt{10}}{10}) and (cos = = frac{sqrt{10}} {10}).<continue> We use the Law of Sines to find (a): [ frac{a}{sin A} = frac{c}{sin C} implies frac{a}{frac{3sqrt{10}}{10}} = frac{4}{frac{2}{5}} implies frac{a} {frac{3sqrt{10}}{10}} = 4 cdot frac{5}{2} = 10 implies = 10 cdot frac{3sqrt{10}}{10} = 3sqrt{10} ] <reflect> Wait minute. The calculation of (sin C) is incorrect because (sin C) should be (frac{2}{sqrt{5}}) instead of (frac{2}{5}).<explore> This time, Use the correct value of (sin = frac{2}{sqrt{5}}) in subsequent calculations.<continue> Correct the calculation for (sin C): [ sin = frac{2}{sqrt{5}} ]<continue> Use the correct value of (sin C) in the Law of Sines to find (a): [ frac{a}{sin A} = frac{c}{sin C} implies frac{a}{frac{3sqrt{10}}{10}} = frac{4}{frac{2}{sqrt{5}}} implies frac{a}{frac{3sqrt{10}}{10}} = 4 cdot frac{sqrt{5}}{2} = 2sqrt{5} implies = 2sqrt{5} cdot frac{3sqrt{10}}{10} = frac{6sqrt{50}}{10} = frac{6 cdot 5sqrt{2}}{10} = 3sqrt{2} ]<continue> Use the formula for the area of triangle ( text{Area} = frac{1}{2}ab sin ): [ text{Area} = frac{1}{2} cdot 3sqrt{2} cdot 4 cdot frac{2}{sqrt{5}} = frac{1}{2} cdot 12sqrt{2} cdot frac{2} {sqrt{5}} = frac{1}{2} cdot frac{24sqrt{2}}{sqrt{5}} = 12 cdot frac{sqrt{2}}{sqrt{5}} = 12 cdot sqrt{frac{2}{5}} = 12 cdot frac{sqrt{10}}{5} = frac{12sqrt{10}}{5} ] Therefore, the area of (triangle ABC) is: (boxed{frac{12sqrt{10}}{5}}).<reflect> Lets reassess my previous conclusion. The mistake is in the calculation of the area, where the value of (sin B) should have been used instead of (sin C).<explore> To find better solution, Use the correct value of (sin = frac{sqrt{2}}{2}) in the area formula: (text{Area} = frac{1}{2} cdot 3sqrt{2} cdot 4 cdot frac{sqrt{2}}{2} = 6).<continue> correct the calculation of the area using the correct value of (sin = frac{sqrt{2}}{2}$: (text{ Area} = frac{1}{2} cdot 3sqrt{2} cdot 4 cdot frac{sqrt{2}}{2} = 6). Therefore, the area of (triangle ABC) is: (boxed{6}). 34 D.2. Format Tuning Training Details. We perform supervised fine-tuning (SFT) on the Qwen-2.5-Math-7B base model using our synthetic COAT dataset with 10K unique questions. The SFT prompt template is illustrated below. We utilize cosine learning rate scheduler with an initial learning rate of 2e-5. The batch size is set to 128, the maximum sequence length is 4096, and the model is trained for maximum of two epochs. We add the following special tokens <continue>,<reflect>,<explore> into the vocabulary. All experiments are implemented using the LLaMA-Factory framework (Zheng et al., 2024). Prompt Template 3: SFT and RL <im_start>user Solve the following math problem efficiently and clearly. Please reason step by step, and put your final answerwithin boxed{}. Problem: <<<your instruction>>> <im_start>assistant D.3. Reinforcement Learning ORM Training. To construct the preference data for our ORM models, we utilize our format-tuned model, Satori-Qwen7B-FT, to generate trajectories. Starting with our filtered training dataset of 550K unique questions, we follow these steps: (1) allow the FT model to sample eight solutions for each question; (2) evaluate the correctness of these solutions and label them accordingly; and (3) select only those questions that contain correct and incorrect solutions. For these selected questions, we construct preference data by pairing correct solutions with their corresponding incorrect ones, resulting in preference dataset of approximately 300K unique questions. For each problem x, we allow πθ to randomly generate multiple reasoning trajectories, constructing dataset Dr with positive and negative pairs of trajectories. We select trajectories with the correct final answer as positive trajectories y+ and trajectories with incorrect final answer as negative trajectories y. Assuming the Bradley-Terry (BT) preference model, we optimize the reward model rψ(x, y) through negative log-likelihood, Lrm(ψ) := (x, y+, y) log σ rψ(x, y+) rψ(x, y) τ (cid:0) where τ denotes target reward margin. In practice, we observe that setting τ > 0 improves the performance of the reward model. (cid:1)(cid:1)(cid:3) (cid:0) (cid:2) RL Data. Our RL training dataset consists of 300K unique questions from the preference dataset. This ensures that the questions are neither too easy (where the FT model always produces correct solutions) nor too difficult (where the FT model never succeeds). This encourages policy to learn through trial and error during RL training. To further guide the model to learn self-reflection capabilities, we apply the proposed RAE technique, augmenting input problems with restart buffers, i.e., intermediate reasoning steps collected from the FT model. These intermediate steps are extracted from the preference dataset, and for each question, we randomly select one correct and one incorrect trajectory, applying the back-track technique for up to = 2 steps. Training Details. For both ORM and RL training, we implement our experiments using the OpenRLHF framework (Hu et al., 2024). For ORM training, we employ cosine learning rate scheduler with an initial learning rate of 2e-6. The batch size is set to 128, the maximum sequence length to 4096, and the model is trained for two epochs. As the objective function, we use PairWiseLoss (Christiano et al., 2017) with margin of τ = 2. For evaluation, we select the optimal ORM model checkpoint based on RM@8 performance, measured using the SFT model on held-out validation dataset. Specifically, we allow the FT model to sample eight trajectories and let ORM select the best trajectory according to the highest reward score. The RM@8 accuracy is then computed based on the selected trajectories. For RL training, we use the PPO algorithm (Schulman et al., 2017a). The critic model is initialized from our ORM model, while the actor model is initialized from our FT model. We optimize the models using cosine learning rate scheduler, setting the learning rate to 2e-7 for the actor model and 5e-6 for the critic model. During PPO training, we sample one trajectory per prompt. The training batch size is set to 128, while the rollout batch size is 1024. Both the number of epochs 35 and episodes are set to 1. The maximum sequence length for prompts and generations is fixed at 2048. Additional parameter settings include KL coefficient of 0.0, sampling temperature of 0.6, and bonus scale of rbonus = 0.5. Second-round Self-improvement. We begin with set of 240K unique questions, also used in the distillation experiments shown in Table 5. The policy of the first round of RL training serves as teacher model to generate synthetic reasoning trajectories. Among these 240K questions and corresponding trajectories, we filter the data based on question difficulty, selecting the most challenging 180K samples for distillation. This process results in new fine-tuned (FT) model checkpoint, obtained from supervised fine-tuning (SFT) on these 180K trajectories. Since the new FT model has been trained on numerous high-quality trajectories, including reflection actions distilled from the teacher model, we do not apply restart and exploration (RAE) techniques in the second round of RL training to further encourage reflection. Additionally, we increase the sampling temperature from 0.6 to 1.2, generating eight samples per prompt to encourage more aggressive exploration to push the performance limit. D.4. Evaluation Details. For each model, we use the same zero-shot CoT prompt template to obtain results on all test datasets. For Satori and all its variants, we use Prompt Template 3 (Appendix D.2). We set the temperature to 0 (greedy decoding) for every model, and collect pass@1 accuracies. Details of each test dataset are as follows. MATH500 (Lightman et al., 2023) is subset of MATH (Hendrycks et al., 2021a) of uniformly sampled 500 test problems. The distribution of difficulty levels and subjects in MATH500 was shown to be representative of the entire MATH test set. GSM8K (Cobbe et al., 2021) is math dataset that consists of 8.5K high-quality, linguistically diverse grade-school math word problems designed for multi-step reasoning (2 to 8 steps). Solutions involve elementary arithmetic operations and require no concepts beyond early algebra. Its test set contains 1319 unique problems. OlympiadBench (He et al., 2024) is bilingual, multimodal scientific benchmark with 8,476 Olympiad-level math and physics problems, including those from the Chinese college entrance exam. We use the open-ended, text-only math competition subset, containing 674 problems in total. AMC2023 and AIME2024 contain 40 text-only problems from American Mathematics Competitions 2023 and 30 text-only problems from American Invitational Mathematics Examination 2024, respectively. FOLIO (Han et al., 2024) is human-annotated dataset designed to evaluate complex logical reasoning in natural language, featuring 1,430 unique conclusions paired with 487 sets of premises, all validated with first-order logic (FOL) annotations. Its test set contains 203 unique problems. BoardgameQA (BGQA) (Kazemi et al., 2024) is logical reasoning dataset designed to evaluate language models ability to reason with contradictory information using defeasible reasoning, where conflicts are resolved based on source preferences (e.g., credibility or recency). Its test set contains 15K unique problems. CRUXEval (Gu et al., 2024) is benchmark for evaluating code reasoning, understanding, and execution, featuring 800 Python functions (3-13 lines) with input-output pairs for input and output prediction tasks. Given function snippet and an input example, LLMs are tasked to generate the corresponding outputs. Its test set contains 800 unique problems. StrategyQA (Geva et al., 2021) is question-answering benchmark designed for multi-hop reasoning where the necessary reasoning steps are implicit and must be inferred using strategy. Each of the 2,780 examples includes strategy question, its step-by-step decomposition, and supporting Wikipedia evidence. TableBench (Wu et al., 2024a) is tabular reasoning benchmark designed to evaluate LLMs on real-world tabular data challenges, covering 18 fields across four major TableQA categories: Fact checking, numerical reasoning, data analysis, and code generation for visualization. We test all models on fact checking and numerical reasoning subsets for simplicity of answer validation, resulting in 491 unique problems. MMLUProSTEM is subset of MMLU-Pro (Wang et al., 2024b). MMLU-Pro is an enhanced benchmark designed to extend MMLU (Hendrycks et al., 2021b) by incorporating more reasoning-focused questions, expanding answer choices from four to ten, and removing trivial or noisy items. We select six STEM subsets: physics, chemistry, computer science, engineering, biology, and economics (we remove the math subset as it belongs to in-domain tasks). Finally, we obtain 5371 unique problems in total. 36 E. Additional Results E.1. Ablation on Reflection Bonus Table 6: Ablation Study on Reflection Bonus. Bonus Scale GSM8K MATH500 Olym. AMC2023 AIME2024 0.0 0.5 (default) 93.6 93.2 84.4 85.6 48.9 46. 62.5 67.5 16.7 20.0 During RL training, we introduce reflection bonus to facilitate the policy to learn self-reflection capabilities. The default value of the reflection bonus is set to rreflect = 0.5. To analyze its impact on performance, we also evaluate the model with the reflection bonus set to rreflect = 0. The results are presented in Table 6. We observe that the performance slightly degrades on challenging benchmark AMC2023 and AIME2024 when set rreflect = 0 compared to rreflect = 0.5. E.2. Offline Restart Buffer v.s. Online Restart Buffer Complementary to the reflection bonus, the restart buffer is designed to enhance the policys self-reflection capabilities by augmenting the initial states with diverse set of intermediate states. This includes trajectories processed from both correct and incorrect reasoning paths, which are then categorized into positive ( restart) restart buffers, as described in Section 4.2. + restart) and negative ( In addition to constructing the restart buffer offline, we also explore an online restart buffer approach. Specifically, after each PPO episode, we use the updated policy to construct the restart buffer and collect rollouts from this buffer to optimize the policy, iteratively repeating this process. However, this approach is suboptimal. During PPO training, we observe that the majority of sampled trajectories are correct, leading to significant imbalance between correct and incorrect intermediate states in the online restart buffer. As result, the model fail to adequately learn from incorrect paths, which are essential for incentivize self-reflection actions. To overcome this limitation, we opt for an offline restart buffer approach to mitigate the bias introduced by online collection. Offline sampling ensures balanced inclusion of intermediate states from both correct and incorrect trajectories."
        }
    ],
    "affiliations": [
        "Harvard",
        "MIT",
        "MIT-IBM Watson AI Lab, IBM Research",
        "Singapore University of Technology and Design",
        "UMass Amherst"
    ]
}