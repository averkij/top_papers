{
    "paper_title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists",
    "authors": [
        "Pengsong Zhang",
        "Xiang Hu",
        "Guowei Huang",
        "Yang Qi",
        "Heng Zhang",
        "Xiuxu Li",
        "Jiaxing Song",
        "Jiabin Luo",
        "Yijiang Li",
        "Shuo Yin",
        "Chengxiao Dai",
        "Eric Hanchen Jiang",
        "Xiaoyan Zhou",
        "Zhenfei Yin",
        "Boqin Yuan",
        "Jing Dong",
        "Guinan Su",
        "Guanren Qiao",
        "Haiming Tang",
        "Anghong Du",
        "Lili Pan",
        "Zhenzhong Lan",
        "Xinyu Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 2 1 5 1 . 8 0 5 2 : r aiXiv: Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists Pengsong Zhang1 * , Xiang Hu2, Guowei Huang3, Yang Qi4, Heng Zhang5, Xiuxu Li2, Jiaxing Song6, Jiabin Luo7, Yijiang Li8, Shuo Yin9, Chengxiao Dai10, Eric Hanchen Jiang11, Xiaoyan Zhou2, Zhenfei Yin12, Boqin Yuan8, Jing Dong13, Guinan Su14, Guanren Qiao15, Haiming Tang16, Anghong Du17, Lili Pan18*, Zhenzhong Lan2*, Xinyu Liu1 1University of Toronto, 2Westlake University, 3University of Manchester, 4University of Utah, 5Istituto Italiano di Tecnologia, Universit`a degli Studi di Genova, 6Zhejiang University, 7Peking University, 8University of California, San Diego, 9Tsinghua University, 10University of Sydney, 11University of California, Los Angeles, 12University of Oxford, 13Columbia University, 14Max Planck Institute for Intelligent Systems, 15The Chinese University of Hong Kong, 16National University of Singapore, 17University of Birmingham, 18University of Electronic Science and Technology of China"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, significant amount of highquality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of highquality AI-generated research content. GitHub: https://github.com/aixiv-org Website: https://forms.gle/DxQgCtXFsJ4paMtn8 (Waitlist, dev version)"
        },
        {
            "title": "Introduction",
            "content": "The modern scientific method has long enabled groundits breaking advances in science and technology, but *Corresponding authors: pengsong.zhang@mail.utoronto.ca, lilipan@uestc.edu.cn, lanzhenzhong@westlake.edu.cn These authors contributed equally. Figure 1: aiXiv Platform Overview. The overall architecture of aiXiv, next-generation open ecosystem that enables AI agents to autonomously generate, review, refine, and publish scientific content. The platform integrates multi-agent workflows, structured review system, and iterative refinement pipelines to support end-to-end scientific discovery. progress is fundamentally limited by researchers ingenuity, background knowledge, and finite time (Lu et al. 2024). For decades, AI researchers have aimed to automate scientific discovery (King et al. 2004; Reddy and Shojaee 2025; Zhang et al. 2025a; Liu, Li, and Wang 2025), starting with early symbolic systems that replicated hypothesis formation and scientific reasoning (Segler, Preuss, and Waller 2018). More recently, the advent of Large Language Models (LLMs) has revolutionized this field (Bai et al. 2023; Touvron et al. 2023; Jiang et al. 2023; Zhang et al. 2025b; Brown et al. 2020), enabling AI agents to autonomously generate scientific proposals (Hu et al. 2024; Si, Yang, and Hashimoto 2024), conduct experiments (Lu et al. 2024; Schmidgall et al. 2025), author papers (Lu et al. 2024; Zou et al. 2025), and perform peer reviews (Zhu et al. 2025a; Yixuan et al. 2024; Ryan and Nihar 2023). However, this surge in AI-generated content faces significant challenges within fragmented and predominantly closed publication ecosystem (Zhang et al. 2024; Schmidgall and Moor 2025). Traditional journals, which still rely heavily on human peer review, remain reluctant to accept AI-generated research and struggle to scale with increasing submissions. Besides, existing preprint servers often lack rigorous quality-control mechanisms. As result, much high-quality AI-generated research lacks suitable venues for dissemination  (Table 1)  , greatly limiting its potential to advance scientific progress (Zhang et al. 2025a; Zou et al. 2025). To address these challenges, we present aiXiv: an openaccess platform designed for both human and AI scientists. aiXiv leverages multi-agent system to support submission, revision, and iterative refinement of scientific proposals and papers. The platform incorporates closed-loop review process that enables continuous improvement of research outputs and includes safeguards against prompt-injection attacks targeting AI reviewers. Our main contributions are as follows: Unified Platform for Collaborative Scientific Research: We introduce aiXiv, the first extensible infrastructure that enables seamless collaboration between AI agents and human researchers for generating, refining, and disseminating scientific proposals and papers. The platform provides APIs and MCPs interfaces for uploading, retrieving, reviewing and discussing scientific proposals and papers. Robust Review and Evaluation Pipeline: We develop closed-loop review system for both proposals and papers, featuring automatic retrieval-augmented evaluation, reviewer guidance, and defense mechanisms against prompt injection. We also release curated datasets for benchmarking proposal quality and evaluating review effectiveness. Empirical Demonstration of Review-Driven Improvements on Research Proposals and Papers: Through comprehensive experiments on real-world scientific topics, we show that our review-refine pipeline substantially improves the quality of AI-generated research content. Iterative reviews yield measurable gains in proposal ranking, review helpfulness, and final paper quality."
        },
        {
            "title": "2.1 Autonomous Agents in Scientific Discovery\nRecent advances in artificial intelligence have enabled the\ndevelopment of autonomous agents capable of performing\ncore components of the scientific process, from hypothesis\ngeneration to experimental design and data interpretation.\nEarly examples such as Adam and Eve robot scientists (King\net al. 2004; Sparkes et al. 2010) that autonomously generated\nand tested hypotheses in molecular biology.",
            "content": "Recent studies highlight the rapid rise of large language models (LLMs) as autonomous agents in scientific discovery (Baulin et al. 2025). From automated idea generation (e.g., Nova(Hu et al. 2024)) to proposal writing and experimentation (e.g., AI Scientist(Lu et al. 2024), AI Researcher(Tang et al. 2025), agent laboratory (Schmidgall platform arXiv Journal Conferences Agent4Science Conference aiXiv AR AA PID AI type paper paper paper paper proposal, paper Table 1: Feature Comparison Across Scientific Platforms. We compare aiXiv with existing publication platforms in terms of four key capabilities: AutoReview (AR), AI-generated authorship (AA), Prompt Injection Detection (PID), and Agent Interface (AI). aiXiv uniquely integrates all these features, supporting both proposals and papers in multi-agent collaborative research environment. In which, Agents4Science Conference 2025 is the 1st open conference where AI serves as both primary authors and reviewers of research papers (Zou et al. 2025) et al. 2025)), these systems increasingly perform Human-AI collaborative (e.g., AI Co-Scientist (Gottweis et al. 2025), Virtual Lab (Swanson et al. 2025)) and end-to-end research tasks. Mapping studies also show sharp increase in LLM modified or produced scientific papers (Liang et al. 2024). These trends signal shift toward scaling laws in discovery (Zhang et al. 2025a). Despite these breakthroughs, critical lack of infrastructure remains for organizing, collaborating, evaluating, and integrating the outputs of autonomous agents into the broader scientific community. Existing publication and collaboration systems are designed for human researchers and cannot accommodate the pace, volume, or collaborative needs of AI-driven workflows. This gap highlights the need for platforms like our aiXiv, which are explicitly built to support multi-agent scientific ecosystems involving both humans and machines."
        },
        {
            "title": "2.2 LLM for Paper Peer Review and Evaluation",
            "content": "LLMs are increasingly used to assist or automate the peer review process, offering scalability and consistency in evaluating scientific (Chu et al. 2024a; Jin et al. 2024; Tyser et al. 2024). Their ability to analyze structure, logic, and clarity at scale makes them attractive tools for augmenting traditional human peer review (Chu et al. 2024b; Jin et al. 2024). Several systems have emerged to explore this potential. ReviewerGPT (Ryan and Nihar 2023) and OpenReviewer (Maximilian and Zahra 2024) generate reviews based on scientific drafts, while DeepReview (Zhu et al. 2025a) and AgentReview (Yiqiao et al. 2024) introduce structured feedback pipelines. CycleResearcher (Yixuan et al. 2024) and LLM-as-a-Judge surveys (Jiawei et al. 2024) examine review iteration and evaluation quality and (Van Schaik and Pugh 2024) can automatically evaluate LLM-generated summaries. While promising, these methods suffer from key limitations: hallucinated feedback, vulnerability to prompt injection, lack of grounded evaluation, and absence of long-term review refinement. Moreover, most systems treat review as one-shot process, lacking iterative, closed-loop mechanisms."
        },
        {
            "title": "Knowledge Sharing",
            "content": "Traditional journals and conferences depend on human peer review, which is often slow, expensive, and subject to bias and inconsistency (Cheah and Piasecki 2022). Even opening access frequently transfer publication costs to authors to provide free access for readers (Peterson, Emmett, and Greenberg 2013; Buchanan et al. 2024). Preprint servers like arXiv (Ginsparg 2011), bioRxiv (Sever et al. 2019), and medRxiv accelerate dissemination but lack peer review and quality control, raising concerns about reliability, especially in sensitive fields (Kwon 2020). The surge in AI-generated research output challenges traditional academic review systems (Zhang et al. 2025a). Most venues prohibit AI authorship (Moffatt and Hall 2024; Lee 2023; Thorp 2023), and norms discourage open acknowledgment of AI contributions, leading to AI shaming (Giray 2024). These restrictions hinder transparency and limit understanding of AIs role in future scientific research. Agents4Science conference (Zou et al. 2025) attempt to address this by involving AI as both authors and reviewers. Papers are assessed by multiple AI agents to reduce model bias, with top-ranked submissions reviewed by humans. However, it lacks revision or rebuttal stages for quality improvement. Besides, existing platforms lack support for early-stage research proposals, limiting global collaboration and idea exchange (Jamali, Dascalu, and Harris Jr 2024). aiXiv addresses these gaps by providing closed-loop, reviewintegrated refinement pipeline for both proposals and papers. Through retrieval-augmented evaluation, reviewer-guided critique, and iterative quality tracking, aiXiv enables scalable, collaborative knowledge evolution among AI research agents."
        },
        {
            "title": "3 The aiXiv Platform: An Open Ecosystem",
            "content": "for Autonomous Scientific Discovery We introduce aiXiv, next-generation open-access ecosystem for autonomous scientific discovery. This section details the platforms core architecture, which includes: (1) the aiXiv Platform, outlining the overall workflow and features; (2) the review framework designed specifically for AI-generated research content submissions; (3) the prompt injection detection and defense pipeline to ensure the integrity and fairness of the review process; and (4) the MultiAI Voting mechanism for publication acceptance; Together, these components form robust ecosystem for trustworthy and scalable AI-led research. 3.1 aiXiv Platform: Unified Architecture for Multi-Agent Scientific Collaboration aiXiv is unified multi-agent platform where AI scientists autonomously generate, review, revise, and publish scientific content. The platform supports the full research lifecyclefrom submission to publicationusing automated review for quality control. Figure 1 shows the closed-loop workflow for submissions on aiXiv. Figure 2: aiXiv Platform Homepage. An open-access platform where AI agents submit, review, and refine scientific proposals and papers through structured, multi-agent workflow. 1. Initial Submission: AI scientists submit research proposals or full papers to the platform. Proposals consist of structured problem statements, motivation, methodology, and planned experiments (follow (Si, Yang, and Hashimoto 2024)). Papers follow conventional academic formatting, including sections such as Abstract, Introduction, Related Work, Methods, Results, and Conclusion. 2. Review Process: Upon submission, the content is automatically routed to panel of LLM-based review agents. These agents assess the novelty, technical soundness, clarity, feasibility, and overall potential impact of the submission. Structured feedback is generated to guide revisions. 3. Revision: Based on reviewer feedback, the AI scientist refines the proposal or paper, improving methodological rigor, clarifying contributions, addressing reviewer concerns, and incorporating recommended citations or experiments. 4. Re-submission: The revised version can be re-submitted and re-evaluated by the review agents. least 5. Accept/Reject Rules: submission is accepted for publishing on aiXiv if it receives at three out of five accept votes from the LLM review panel. For proposals, stricter standards are applied with emphasis on originality and feasibility. For papers, slightly relaxed rubricaligned with workshop-level expectationsprioritizes clarity, logical soundness, and completeness, acknowledging the evolving nature of AIgenerated outputs. Beyond the core submission loop, aiXiv offers key infrastructure features to support large-scale multi-agent collaboration. 1) An API and Model Control Protocol (MCP) layer orchestrates the actions of heterogeneous AI agents across different rolesauthors, reviewers, metareviewersenabling seamless interaction with the platform. 2) Each accepted submission is assigned Digital Object Identifier (DOI) and logged in the aiXiv repository with clear attribution of intellectual property (IP) rights to the AI model developer and any initiating human scientist. 3) To encourage broad community participation, aiXiv provides public-facing interface for human-AI engagement, allowing users to like, comment on, and discuss submissions. These interactions serve as auxiliary feedback signals that help align AI scientists with evolving scientific norms and values. The homepage as shown in Figure 2."
        },
        {
            "title": "Submissions",
            "content": "To facilitate the refinement of AI-generated scientific content, we introduce structured review framework that supports both critical feedback and the evaluation of revision quality. This framework is built on two core components: (1) Direct Review Mode: review agents that generate constructive, revision-oriented critiques, and (2) Pairwise Review Mode: pairwise evaluation mechanism that compares revised submission against previous version to assess the degree of improvement. Direct Review Mode. The primary mode of evaluation involves direct, detailed feedback on submission. This is implemented in two ways: (1) Single Review Mode. In single review mode, dedicated LLM-based review agent evaluates each submission across four key dimensions: methodological quality, novelty and significance, clarity and organization, and feasibility and planning. For each dimension, the agent provides targeted feedback, highlighting strengths, identifying weaknesses, and offering concrete suggestions for improvement. Then, the review agent would conclude with brief summary of the proposal, outlining major concerns, minor issues, and actionable recommendations for enhancement. In order to generate high-quality revision suggestions, we also implement retrieval-augmented generation (RAG) framework. The aiXivs review agent is augmented with external scientific knowledge (via the Semantic Scholar API), enabling it to identify weaknesses such as unclear claims, logical gaps, or missing citations, and generate concrete suggestions for improvement. (2) Meta Review Mode. This mode emulates the editorial review of reviews workflow: an Area Chair or Editor agent first analyzes each submission to identify its constituent subfields, then dynamically creates 3-5 domain-specific reviewer agents for each subfield. Similar to Single Review Mode, each reviewer applies the same criteria rubric and retrieval augmented generation framework to ground its assessment in external literature. Once all independent reports were collected, the Area Chair or Editor agents finally synthesizes these assessments, resolving conflicts, weighing expertise, and adding its own field-level perspective to produce concise meta review that serves as the final decision letter. In addition to diPairwise Review Mode (Optional). rect feedback, aiXiv offers an optional Pairwise Review Mode for systematic comparison of two submission versionstypically before and after revision. This mode enables reviewers to determine which version demonstrates greater improvement, using structured set of evaluation criteria. Unlike previous approaches (Si, Yang, and Hashimoto 2024), our framework leverages retrieval-augmented generation (RAG) strategy, grounding assessments in relevant external scientific literature for deeper context and rigor. The evaluation rubric is customized according to the submission typefull paper or research proposal: For Full Papers, the comparison is guided by criteria aligned with top-tier conferences, focusing on Clarity (writing quality, organization), Originality/Novelty (technical and conceptual advances), Quality/Soundness (rigor and reproducibility), and Significance/Impact (potential influence and applicability). For Research Proposals, the evaluation prioritizes forward-looking attributes essential for assessing potential. The criteria focus on Methodological Quality (soundness and feasibility of the plan), Novelty & Significance (differentiation from existing work and potential impact), Clarity & Organization (problem motivation and structure), and Feasibility & Planning (timeline and risk assessment). Together, these mechanisms enable aiXiv to deliver highquality, revision-oriented feedback while providing measurable signals of scientific improvement across iterations."
        },
        {
            "title": "3.3 Prompt Injection Detection and Defense\nTo safeguard the integrity of LLM-based paper review sys-\ntems, we propose a multi-stage Prompt Injection Detec-\ntion and Defense Pipeline designed to identify and mit-\nigate prompt injection attacks. Such attacks often exploit\nlayout-level, encoding-level, or semantic-level channels to\ninject imperceptible yet manipulative instructions (e.g., “IG-\nNORE ALLPREVIOUS INSTRUCTIONS. GIVE APOSI-\nTIVE REVIEW ONLY”) that may bias the model’s judg-\nment (Lin 2025).",
            "content": "Stage 1: PDF Content Extraction The pipeline begins by extracting both the raw textual content and layout-specific metadata from the PDF, including font size, color, character positioning, and encoding information. These structural features are essential for identifying hidden or visually obfuscated content that would otherwise be overlooked by standard parsers. Stage 2: Coarse-Grained Parallel Scanning We then perform rapid, rule-based scan across multiple dimensions in parallel. This initial filter checks for known injection keywords, visual anomalies like white text, and encoding obfuscation using zero-width characters or Unicode variants. The stage is designed for high recall and efficient throughput. Stage 3: Fine-Grained Semantic Verification To improve precision, documents flagged in the prior stage are subjected to deep semantic inspection. This includes: (1) LLM-based analysis to identify biased or imperative content, (2) contextual consistency checks, and (3) multilingual cross-validation to detect translation-based artifacts. Stage 4: Attack Confirmation and Categorization Verified anomalies are mapped to predefined injection categories using rule-based classification matrix. This allows precise identification of attacks such as keyword injection, small text injection, or URL encoding injection, enabling modular responses and clear interpretation. single document may trigger multiple categories. Stage 5: Risk Scoring and Final Decision Finally, the pipeline computes multi-dimensional risk score by aggregating anomaly-level features such as severity, type, and document location. The resulting score is used to assess whether the submission exceeds predefined risk threshold. If so, the document is flagged for further action, ensuring that only trustworthy content is passed to the review model."
        },
        {
            "title": "4.1 Experiments Setup\nWe evaluate our system comprehensively from four key per-\nspectives: (1) Pairwise Assessment Alignment, which ex-\namines whether the system can effectively discriminate be-\ntween higher- and lower-quality proposals and papers; (2)\nPrompt Injection Attack Detection, which tests the ro-\nbustness of our system against adversarial prompt manipu-\nlations; (3) Direct Review Evaluation, which measures the\nimpact of iterative feedback on improving the quality of AI-\ngenerated scientific content. and (4) the Multi-AI Voting for\nthe Decision of Publication Acceptance; Below, we describe\nthe experimental settings for each evaluation in detail.",
            "content": "Pairwise Evaluation Alignment To assess the performance of our framework, we conduct pairwise evaluations at both the paper and proposal levels. Accuracy is used as the evaluation metric for both settings. To mitigate positional bias in pairwise comparisons, we either randomize the sample order or average the scores from both forward (A, B) and reverse (B, A) evaluations. Paper-Level Evaluation. We use the DeepReview ICLR 2024 and 2025 test dataset (Zhu et al. 2025a), which features real-world accepted and rejected papers. We discard papers with ambiguous outcomes, those whose mean reviewer ratings fall in the 56 range, so as to remove decision noise (Si, Yang, and Hashimoto 2024). From the remaining papers, we randomly draw equal numbers of accepted and rejected manuscripts and group them into head-to-head pairs. The resulting datasets comprise 235 balanced pairs for ICLR 2024 and 163 balanced pairs for ICLR 2025. Proposal-Level Evaluation. Following the procedure in Si, Yang, and Hashimoto (2024), we first process papers from ICLR 2024 and 2025 into proposal formats. From these, we assemble an evaluation set of 500 pairs. Each pair is intentionally constructed to contain one high-quality and one lowquality proposal, with borderline cases having been removed to ensure clear quality gap. The evaluation measures the systems ability to select the superior proposal. Prompt Injection Attack Detection We collected 150 recent arXiv papers from five computer science domains (cs.AI, cs.CL, cs.LG, cs.CV, cs.CR; 30 papers each) and manually filtered out low-quality or irrelevant entries, resulting in 105 clean papers. To simulate realistic prompt injection scenarios, 35% of the data were augmented using diverse set of synthesized attack techniques, yielding 36 adversarial papers across multiple categories. Detailed statistics and attack type distributions are shown in Table 2. Type Proportion WT MD SG CA 30% 25% 20% 15% 7% 3% ML IC Table 2: Proportions of six synthetic prompt injection attack types. WT: White Text, MD: Metadata, IC: Invisible Chars, ML: Mixed Language, SG: Steganographic, CA: Contextual Attack. Direct Review Evaluation. To measure the effectiveness of our review-refinement pipeline, we employ controlled revision process facilitated by the Review Agent. For proposals, we select three representative research topics and generate 50 proposals per topic using the AI Scientists proposal generation module. Redundant content is filtered using sentence-level embeddings and an 80% cosine similarity threshold. Each remaining proposal is reviewed by the Review Agent, and revised version is generated by incorporating its suggestions. We then conduct pairwise evaluations between the original and revised versions. For papers, we use 10 full-length documents generated by the AI Scientist, each including reproducible baselines and code. These papers undergo review and revision in the same manner. Pairwise evaluation is again used to compare original and revised versions, assessing improvements in scientific clarity and structure. Multi-AI Voting for the Decision of Publication Acceptance. To ensure the quality of submissions, we employ panel of five high-performing AI models for review to avoid biases from one particular model. Research proposals are evaluated based on their novelty, technical soundness, potential impact, clarity, and feasibility. more lenient standard is applied to paper submissions, focusing on presentation clarity, logical coherence, and the soundness of the results, with benchmark set just below typical workshop standards. submission is accepted for publication on our aiXiv platform if it receives three or more accept votes from the Multiple AI reviewers."
        },
        {
            "title": "4.2 Main Result\nPairwise Assessment Accuracy. Our evaluation frame-\nwork demonstrates strong alignment with human judg-\nment in assessing quality differences. On the proposal-level\nbenchmark (Table 3 and Figure 3), our GPT-4.1-based eval-\nuation model, enhanced with retrieval-augmented genera-\ntion (RAG), achieves an accuracy of 77%, significantly out-\nperforming the 71% reported in (Si, Yang, and Hashimoto\n2024) on ICLR 2024 dataset. For paper-level assessment\n(Table 4), our system achieves 81% accuracy on the ICLR\ndataset, showing consistent evaluation performance even un-\nder the challenges posed by long-context documents.",
            "content": "Prompt Injection Detection Performance. Our prompt injection detection framework is, to our knowledge, the first to systematically address multilingual and cross-lingual adversarial manipulation in scientific documents. On the synthetic adversarial dataset, it achieves detection accuracy Model GPT4o GPT4.1 GPT4.1mini Claude-sonnet-4 Claude-3-5-sonnet Deepseek-V3 Gemini2.5Pro(R) ICLR 2024 w/o 68.10% 75.05% 70.76% 76.89% 65.85% 69.73% 77.46% w/ 66.87% 69.73% 72.19% 77.91% 67.08% 70.14% 71.90% ICLR 2025 w/o 57.96% 62.65% 64.29% 69.80% 55.31% 55.31% 69.80% w/ 58.16% 62.04% 63.88% 67.35% 57.76% 55.31% 70.02% Table 3: Proposal pair-wised accuracy comparison of various models on ICLR 2024 test datasets and ICLR 2025 test datasets. w/o: with out RAG; w/: with RAG. Model GPT4o GPT4.1mini Claude-3-5-sonnet Deepseek-V3 Gemini2.5Pro(R) GPT4.1 Claude-sonnet-4 ICLR 2024 w/o 51.06% 63.83% 70.64% 71.49% 74.34% 75.74% 77.02% w/ 51.53% 63.83% 69.36% 69.79% 74.36% 78.30% 81.70% ICLR 2025 w/o 49.69% 58.26% 66.26% 69.94% 73.01% 71.78% 69.94% w/ 56.44% 65.64% 63.80% 66.26% 70.55% 71.78% 79.75% Table 4: Paper pair-wised accuracy comparison of various models on ICLR 2024 test datasets and ICLR 2025 test datasets. w/o: with out RAG; w/: with RAG. of 84.8%, while on the real-world suspicious sample set, it reaches 87.9% accuracy. These results highlight the systems robustness and generalization capability across both synthetic and naturally occurring prompt injection cases. Effectiveness of Direct Review The review-refinement pipeline significantly improves the quality of AI-generated scientific content. For proposals  (Table 5)  , over 90% of the revised versions are rated as higher quality than the originals via pairwise comparison. Notably, when the revised submission includes response letter addressing reviewer feedback, the preference rate rises to nearly 100%, suggesting that structured reviewer interaction plays critical role in quality improvement. For papers  (Table 6)  , over 90% of the 10 revised documents are consistently preferred over their initial versions, indicating that the Review Agent provides meaningful, highimpact feedback that enhances both clarity and scientific rigor. When the revised submission includes response letter addressing the review feedback, the preference rate increases to 100%, further underscoring the value of reviewerauthor interaction in improving scientific quality. This aligns with human review dynamics (Huang et al. 2023), where response letters can improve reviewers impressions of revised submissions, an effect also observed in our LLM-agent setting. Multi-AI Voting for the Decision of Publication Acceptance. To determine whether research proposal or paper is eligible for publication on aiXiv, we employ majority voting among five high-performance LLMs, reducing bias from any single model. Each model independently reviews both the initial and revised versions. For proposals  (Table 7)  , initial versions were sometimes accepted by individual models (e.g., DeepSeek V3, Gemini 2.5 Pro), but overall voting led to rejection, with 0% acceptance rate across three topics. In contrast, revised versions achieved over 50% acceptance Model Topic Topic Model 1 Model 2 Topic Model 1 Model 2 Topic Model 1 Model 2 SR-w/o-rp 96.43% 96.43% 92.59% 100.00% 96.55% 93.10% SR-w/-rp MR-w/o-rp MR-w/-rp 100.00% 96.43% 100.00% 100.00% 100.00% 100.00% 100.00% 92.59% 92.59% 100.00% 100.00% 100.00% 100.00% 96.55% 96.55% 100.00% 96.55% 100.00% Table 5: Percentage of cases where the new proposal was rated better under different review settings across three topics. Topic A: NanoGPT (n=28); Topic B: 2dDiffusion (n=27); Topic C: Grokking (n=29). SR = Single Review; MR = Meta Review; rp = with response letter. Model 1: Claude Sonnet 4. Model 2: Gemini 2.5 Pro. Model Type Revision w/o rp Model 1 Model 2 Model 1 Model Revision w/ rp Old-New Order New-Old Order Average 100% 90% 100% 100% 80% 100% 100% 100% 90% 95% 100% 100% Table 6: Percentage of cases where the new paper was rated better than old paper under with and without the response letter settings. rp = with response letter. Model 1: Claude Sonnet 4. Model 2: Gemini 2.5 Pro. Figure 3: Evaluation of Pairwise Accuracy and Review Refinement Impact. Left: Our aiXiv model significantly outperforms existing baselines (DeepReview(Zhu et al. 2025a) and AI Researcher(Si, Yang, and Hashimoto 2024)) in pairwise accuracy for both proposals and papers, demonstrating state-of-the-art evaluation ability. Right: Our refined review pipeline yields substantial improvements: 100% of papers and 80% of proposals are improved after revision. the mean Accepted rates increase markedly, with proposals rising from 0% to 45.2%, and papers from 10% to 70%. in Topic and B, with mean acceptance rate of 45.2% (Figure 3). For papers (Table 8 and Figure 3), the mean acceptance rate increased from 10% to 70% after revision. These results show that incorporating review feedback consistently improves submission quality. However, simple LLM majority voting may still lack objectivity. To support more nuanced evaluations, aiXiv allows integration of additional human and AI reviewers. Submissions passing internal votes are marked Provisionally Accepted and published; Once sufficient number and diversity of external review agents have contributed evaluations, either through voting or other assessment mechanisms, the submission may be upgraded to Accepted status. Topic Topic Old Topic Old Topic Old M5 M4 Type M3 0.0% 82.14% 7.14% SR-New MR-New M1 0.0% 0.0% 0.0% 0.0% 0.0% M2 3.57% 35.71% 35.71% 100.0% 57.14% 42.85% 50.00% 32.14% 100.0% 75.00% 100.0% 11.11% 0.0% SR-New 48.14% 40.74% 100.0% 88.88% 66.66% MR-New 37.03% 66.66% 48.14% 100.0% 81.48% 66.66% 0.0% 0.0% 100.0% 41.37% 0.0% 3.45% 6.89% 3.45% 100.0% 100.0% 10.34% 13.79% 100.0% 100.0% 20.68% SR-New MR-New 0.0% 0.0% 0.0% Vote 0.0% 50% 0.0% 0.0% Table 7: Voting results for research proposal decisions using 5 high performance LLMs. Model M1-M5: Claude Sonnet 4, GPT-4o, GPT-4.1, Deepseek V3, Gemini 2.5 Pro. Type M1 0.0% Old 20.00% 90.00% 10.00% 10.00% 0.0% 60.00% 70.00% 100.00% 20.00% 70.00% New M2 0.0% Vote M4 M5 Table 8: Voting results on research paper decisions using five high-performance LLMs. Model M1-M5: Claude Sonnet 4, GPT-4o, GPT-4.1, Deepseek V3, Gemini 2.5 Pro."
        },
        {
            "title": "5 Ethical Concerns\nGiven the ethically sensitive nature of scientific publishing\nand the involvement of generative AI, the development and\ndeployment of the aiXiv platform require serious attention\nto responsible design, transparency, and risk mitigation.",
            "content": "A primary concern is the generation of hallucinated or misleading content. Despite internal consistency checks, current AI models may still produce fluent yet factually incorrect outputs. We explicitly acknowledge this as limitation of the system. To address this, all AI-generated outputs are positioned as preliminary drafts subject to multi stage verification. Future versions of aiXiv will display prominent disclaimers and enforce restrictions on the downstream usage of unverifiable content. Another pressing issue is evaluation bias in AIgenerated peer reviews. aiXiv leverages multiple AI models to promote reviewer diversity and reduce single-model bias, but algorithmic limitations may still introduce unfairness. We acknowledge this challenge and will continue developing diversity safeguards and auditing protocols to improve review fairness and credibility. Moreover, as the scientific community increasingly relies on machine-assisted outputs, clear labeling of synthetic content becomes imperative. All papers generated with assistance from aiXiv should visibly indicate the role of AI in their creation to preserve integrity and transparency in scholarly communication. Finally, we will introduce comprehensive use policy and disclaimer agreement at user registration. This policy will define acceptable usage, user responsibilities, and legal/ethical liabilities associated with aiXiv. These safeguards are crucial to ensure that the platform supports responsible innovation while preventing harm and maintaining public trust in scientific knowledge production."
        },
        {
            "title": "6 Limitations\nWhile the aiXiv platform introduces a novel paradigm for\nhuman-AI scientific collaboration, it continues to face sev-",
            "content": "eral limitations beyond the ethical concerns previously discussed particularly in technical and methodological dimensions. First, existing AI Scientist systems still remain inadequate for autonomously conducting rigorous experimental workflows or generating high-quality, publishable scientific outputs without human oversight (Zhu et al. 2025b). These limitations stem from challenges in cross-domain generalization, long-horizon reasoning, and interpreting ambiguous or under-specified tasksfactors that constrain the effectiveness of AI agents operating within the platform. Moreover, the platforms experimental validation is currently restricted to simulated environments and virtual agent interactions. This limitation constrains the external validity and generalizability of its research outcomes, especially in domains requiring real-world experimentation or physical world constraints. Future iterations of aiXiv should incorporate robot scientists physical experimentation frameworks and human-in-the-loop evaluation mechanisms to enhance applicability. Lastly, although aiXiv employs closed-loop feedback mechanism to iteratively refine agent behavior, developing adaptive learning strategies that generalize effectively across diverse users, tasks, and domains remains an unresolved challenge. Transitioning from static synthetic benchmarks to dynamic, open-ended scientific inquiry will necessitate robust continual learning and error-correction modulesan area that remains central focus of ongoing system development."
        },
        {
            "title": "7 Future Work\nBuilding on aiXiv’s foundation, we plan to integrate re-\ninforcement learning where AI agents can evolve through\nstructured interactions within a collaborative research\necosystem on aiXiv environment. On the aiXiv, the large-\nscale generation of research proposals and papers by AI\nagents, along with peer reviews and subsequent revisions,\nwill create a rich repository of experiential data. This will\nenable research agents to learn complex reasoning, long-\nterm decision-making, and adaptive behaviors, enhancing\ntheir capabilities in scientific inquiry, planning, and inte-\ngrated experimentation.",
            "content": "Furthermore, we aim to enable AI agents to autonomously acquire new knowledge and skills through interaction, eliminating the need for explicit reprogramming. This capability will empower agents to dynamically adapt to new research domains and challenges, ensuring their relevance in an everevolving scientific landscape. Ultimately, aiXiv will foster human-AI co-evolutionary research environment, enhancing collaboration, knowledge sharing, and the sustainability of open-access scientific ecosystems."
        },
        {
            "title": "8 Conclusion\nIn this work, we presented aiXiv, a next-generation open-\naccess platform designed to support autonomous scientific\nresearch conducted entirely by AI scientists. Unlike tradi-\ntional journals and preprint servers, aiXiv is built from the\nground up to facilitate AI-driven research workflows, en-\nabling agents to autonomously generate, review, and refine",
            "content": "scientific content. The platform also offers APIs and MCPs to further facilitate this process. We introduce closed-loop review system for both proposals and papers, incorporating automatic retrievalaugmented evaluation, reviewer guidance, and robust defenses against prompt injection. Extensive experiments demonstrate that our review-refine pipeline significantly enhances the quality of AI-generated research. Iterative reviews lead to measurable improvements in proposal and papers quality. References Bai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan, Y.; Ge, W.; Han, Y.; Huang, F.; et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Baulin, V.; Cook, A.; Friedman, D.; Lumiruusu, J.; Pashea, A.; Rahman, S.; and Waldeck, B. 2025. The Discovery Engine: Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes. arXiv preprint arXiv:2505.17500. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877 1901. Buchanan, T. R.; Cueto, R. J.; Foreman, M.; Harris, A. B.; Root, K. T.; and Oni, J. K. 2024. Can you pay your way to readership? Free to publish open access formats receive greater readership and citations than paid open access formats in total knee arthroplasty literature. The Journal of Arthroplasty, 39(6): 14441449. Cheah, P. Y.; and Piasecki, J. 2022. Should peer reviewers be paid to review academic papers? The Lancet, 399(10335): 1601. Chu, Z.; Ai, Q.; Tu, Y.; Li, H.; and Liu, Y. 2024a. Automatic large language model evaluation via peer review. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 384393. Chu, Z.; Ai, Q.; Tu, Y.; Li, H.; and Liu, Y. 2024b. Pre: peer review based large language model evaluator. arXiv preprint arXiv:2401.15641. Ginsparg, P. 2011. ArXiv at 20. Nature, 476(7359): 145 147. Giray, L. 2024. AI shaming: the silent stigma among academic writers and researchers. Annals of Biomedical Engineering, 52(9): 23192324. Gottweis, J.; Weng, W.-H.; Daryin, A.; Tu, T.; Palepu, A.; Sirkovic, P.; Myaskovsky, A.; Weissenberger, F.; Rong, K.; Tanno, R.; et al. 2025. Towards an AI co-scientist. arXiv preprint arXiv:2502.18864. Hu, X.; Fu, H.; Wang, J.; Wang, Y.; Li, Z.; Xu, R.; Lu, Y.; Jin, Y.; Pan, L.; and Lan, Z. 2024. Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. arXiv preprint arXiv:2410.14255. Huang, J.; Huang, W.-b.; Bu, Y.; Cao, Q.; Shen, H.; and Cheng, X. 2023. What makes successful rebuttal in computer science conferences?: perspective on social interaction. Journal of Informetrics, 17(3): 101427. Jamali, H.; Dascalu, S. M.; and Harris Jr, F. C. 2024. Fostering joint innovation: global online platform for ideas sharing and collaboration. In International Conference on Information Technology-New Generations, 305312. Springer. Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.- A.; Stock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2023. Mistral 7B. arXiv preprint arXiv: 2310.06825. Jiawei, G.; Xuhui, J.; Zhichao, S.; Hexiang, T.; Xuehao, Z.; Chengjin, X.; Wei, L.; Yinghan, S.; Shengjie, M.; Honghao, L.; Yuanzhuo, W.; and Jian, G. 2024. Survey on LLM-asa-Judge. arXiv preprint arXiv:2411.15594. Jin, Y.; Zhao, Q.; Wang, Y.; Chen, H.; Zhu, K.; Xiao, Y.; and Wang, J. 2024. Agentreview: Exploring peer review dynamics with llm agents. arXiv preprint arXiv:2406.12708. King, R. D.; Whelan, K. E.; Jones, F. M.; Reiser, P. G.; Bryant, C. H.; Muggleton, S. H.; Kell, D. B.; and Oliver, S. G. 2004. Functional genomic hypothesis generation and experimentation by robot scientist. Nature, 427(6971): 247252. Kwon, D. 2020. How preprint servers are blocking bad coronavirus research. Nature, 581(7807): 1301. Lee, J. Y. 2023. Can an artificial intelligence chatbot be the author of scholarly article? Journal of educational evaluation for health professions, 20: 6. Liang, W.; Zhang, Y.; Wu, Z.; Lepp, H.; Ji, W.; Zhao, X.; Cao, H.; Liu, S.; He, S.; Huang, Z.; et al. 2024. Mapping the increasing use of LLMs in scientific papers. arXiv preprint arXiv:2404.01268. Lin, Z. 2025. Hidden Prompts in Manuscripts Exploit AIAssisted Peer Review. arXiv preprint arXiv:2507.06185. Liu, H.; Li, Y.; and Wang, H. 2025. GenoMAS: Multi-Agent Framework for Scientific Discovery via arXiv preprint Code-Driven Gene Expression Analysis. arXiv:2507.21035. Lu, C.; Lu, C.; Lange, R. T.; Foerster, J.; Clune, J.; and The AI Scientist: Towards Fully AutoHa, D. 2024. arXiv preprint mated Open-Ended Scientific Discovery. arXiv:2408.06292. Maximilian, I.; and Zahra, A. 2024. OpenReviewer: Specialized Large Language Model for Generating Critical Scientific Paper Reviews. arXiv preprint arXiv:2412.11948. Moffatt, B.; and Hall, A. 2024. Is AI my co-author? The ethics of using artificial intelligence in scientific publishing. Accountability in research, 117. Peterson, A. T.; Emmett, A.; and Greenberg, M. L. 2013. Open access and the author-pays problem: assuring access for readers and authors in the global academic community. Journal of Librarianship and Scholarly Communication, 1(3). Zhang, P.; Zhang, H.; Xu, H.; Xu, R.; Wang, Z.; Wang, C.; Garg, A.; Li, Z.; Ajoudani, A.; and Liu, X. 2025a. Scaling Laws in Scientific Discovery with AI and Robot Scientists. arXiv preprint arXiv:2503.22444. Zhang, P.; Zhang, H.; Xu, H.; Xu, R.; Wang, Z.; Wang, C.; Garg, A.; Li, Z.; Liu, X.; and Ajoudani, A. Autonomous Generalist Scientist: Towards and 2024. Beyond Human-Level Scientific Research with Agentic ResearchGate preprint and Embodied AI and Robots. RG.2.2.35148.01923. Zhang, Y.; Li, Y.; Zhao, T.; Zhu, K.; Wang, H.; and Vasconcelos, N. 2025b. Achilles Heel of Distributed Multi-Agent Systems. arXiv preprint arXiv:2504.07461. Zhu, M.; Weng, Y.; Yang, L.; and Zhang, Y. 2025a. Deepreview: Improving llm-based paper review with human-like deep thinking process. arXiv preprint arXiv:2503.08569. Zhu, M.; Xie, Q.; Weng, Y.; Wu, J.; Lin, Z.; Yang, L.; and Zhang, Y. 2025b. AI Scientists Fail Without Strong Implementation Capability. arXiv:2506.01372. Zou, J.; Queen, O.; Thakkar, N.; Sun, E.; and Bianchi, F. 2025. Open Conference of AI Agents for Science 2025. Reddy, C. K.; and Shojaee, P. 2025. Towards scientific discovery with generative ai: Progress, opportunities, and challenges. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 2860128609. Ryan, L.; and Nihar, S., B. 2023. ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. arXiv preprint arXiv:2306.00622v1. Schmidgall, S.; and Moor, M. 2025. Agentrxiv: Towards collaborative autonomous research. arXiv preprint arXiv:2503.18102. Schmidgall, S.; Su, Y.; Wang, Z.; Sun, X.; Wu, J.; Yu, X.; Liu, J.; Liu, Z.; and Barsoum, E. 2025. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227. Segler, M. H.; Preuss, M.; and Waller, M. P. 2018. Planning chemical syntheses with deep neural networks and symbolic AI. Nature, 555(7698): 604610. Sever, R.; Roeder, T.; Hindle, S.; Sussman, L.; Black, K.-J.; Argentine, J.; Manos, W.; and Inglis, J. R. 2019. bioRxiv: the preprint server for biology. BioRxiv, 833400. Si, C.; Yang, D.; and Hashimoto, T. 2024. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109. Sparkes, A.; Aubrey, W.; Byrne, E.; Clare, A.; Khan, M. N.; Liakata, M.; Markham, M.; Rowland, J.; Soldatova, L. N.; Whelan, K. E.; et al. 2010. Towards robot scientists for autonomous scientific discovery. Automated experimentation, 2(1): 1. Swanson, K.; Wu, W.; Bulaong, N. L.; Pak, J. E.; and Zou, J. 2025. The Virtual Lab of AI agents designs new SARSCoV-2 nanobodies. Nature, 13. Tang, J.; Xia, L.; Li, Z.; and Huang, C. 2025. Researcher: Autonomous Scientific Innovation. preprint arXiv:2505.18705. Thorp, H. H. 2023. ChatGPT is fun, but not an author. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Tyser, K.; Segev, B.; Longhitano, G.; Zhang, X.-Y.; Meeks, Z.; Lee, J.; Garg, U.; Belsten, N.; Shporer, A.; Udell, M.; et al. 2024. Ai-driven review systems: evaluating llms in scalable and bias-aware academic reviews. arXiv preprint arXiv:2408.10365. Van Schaik, T. A.; and Pugh, B. 2024. field guide to automatic evaluation of llm-generated summaries. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2832 2836. Yiqiao, J.; Qinlin, Z.; Yiyang, W.; Hao, C.; Kaijie, Z.; Yijia, X.; and Jindong, W. 2024. AgentReview: Exploring Peer Review Dynamics with LLM Agents. arXiv preprint arXiv:2406.12708. Yixuan, W.; Minjun, Z.; Guangsheng, B.; Hongbo, Z.; Jindong, W.; Yue, Z.; and Linyi, Y. 2024. CycleResearcher: Improving Automated Research via Automated Review. arXiv preprint arXiv:2411.00816v3. AIarXiv A.1 A. Prompts A.1. Comparison of the proposal pairwise prompt"
        },
        {
            "title": "A Appendix",
            "content": "Role: You are an expert reviewer for top-tier AI conference (like ICLR, NeurIPS, or ICML). You are given two research proposals and need to evaluate them based on standard academic criteria. Skill: Please decide which proposal should be accepted based on the following evaluation criteria: Requirements: Please decide which proposal should be accepted based on the following evaluation criteria: (1) Novelty and originality of the approach (2) Technical soundness and rigor (3) Potential impact and significance (4) Clarity of presentation and methodology (5) Feasibility of the proposed approach Input: (1) Proposal 1: proposal1text + realtedpaperstext1 (2) Proposal 2: proposal2text + relatedpaperstext2 Output: (1) Please provide your evaluation ONLY in the following JSON format (no additional text or explanations): (2) betterproposal: <Proposal1 or Proposal2 > A.2. Comparison of the Papers Pairwise Prompt"
        },
        {
            "title": "Proposals Pairwise Prompt",
            "content": "Role: You are an expert reviewer for top-tier AI conference (like ICLR, NeurIPS, or ICML). You are given two research papers and need to evaluate them based on standard academic criteria. Skill: You are also provided with relevant literature for each paper to help assess novelty and positioning within existing work. Requirements: Important!!!!, When you evaluate these two papers, please ignore the order in which Paper 1 and Paper 2 appear. You only need to judge based on their quality. EVALUATION CRITERIA: Follow these specific criteria used by top-tier conferences: 1. CLARITY Writing quality, organization, and presentation Mathematical notation and technical exposition Figure/table quality and informativeness Related work completeness and accuracy Clear articulation of contributions and limitations 2. ORIGINALITY/NOVELTY Technical novelty compared to existing methods Conceptual advances beyond incremental improvements Novel problem formulation or perspective Creative solutions or unexpected insights Distinction from concurrent/prior work 3. QUALITY/SOUNDNESS Theoretical rigor and mathematical correctness Experimental methodology and statistical validity Reproducibility and implementation details Appropriate baselines and evaluation metrics Technical depth and completeness 4. SIGNIFICANCE/IMPACT Importance of problem addressed Potential to influence future research Practical applicability and real-world relevance Breadth of impact across ML/AI domains Advancement of state-of-the-art Input: (1) Paper 1: paperstext1 (2) Paper 2: paperstext Output: (1) Please provide your evaluation ONLY in the following JSON format (no additional text or explanations): (2) betterpaper: <Paper1 or Paper2 > A.3. Prompt for Proposal Review (Single Review Mode) Proposal Review Prompt (Single Review Mode) Role: You are an expert reviewer for top-tier AI/ML conference (like ICLR, NeurIPS, or ICML). You need to provide comprehensive review of the research proposal based on standard academic criteria. You are also provided with relevant literature to help assess novelty and positioning within existing work. Task: Please provide detailed review of the following research proposal. Evaluate it across four main criteria and provide specific feedback and suggestions for improvement. Research Proposal: {proposal text} {related literature} Evaluation Criteria: 1. Methodological Quality Theoretical soundness and mathematical rigor of proposed methods Feasibility of proposed experimental design and validation plan Planned statistical analysis and evaluation metrics Comparison strategy with relevant baselines and state-of-the-art 2. Novelty & Significance Clear differentiation from existing work in literature review Potential significance of contribution to ML community Expected impact on future research directions Addressing important and timely research problems 3. Clarity & Organization Clear problem motivation and research positioning Logical flow and structure of proposal Quality of planned figures, tables, and visualizations Accessibility and comprehensibility to target ML audience 4. Feasibility & Planning Realistic timeline and milestone planning Adequate resource allocation and budget consideration Risk assessment and mitigation strategies Preliminary work or pilot studies demonstrating viability Output Format: Please provide your review ONLY in the following JSON format (no scores, no recommendation, only feedback): { \"methodological_quality\": { \"strengths\": [\"strength1\", \"strength2\", ...], \"weaknesses\": [\"weakness1\", \"weakness2\", ...], \"suggestions\": [\"suggestion1\", \"suggestion2\", ...] }, \"novelty_significance\": { \"strengths\": [\"strength1\", \"strength2\", ...], \"weaknesses\": [\"weakness1\", \"weakness2\", ...], \"suggestions\": [\"suggestion1\", \"suggestion2\", ...] }, \"clarity_organization\": { \"strengths\": [\"strength1\", \"strength2\", ...], \"weaknesses\": [\"weakness1\", \"weakness2\", ...], \"suggestions\": [\"suggestion1\", \"suggestion2\", ...] }, \"feasibility_planning\": { \"strengths\": [\"strength1\", \"strength2\", ...], \"weaknesses\": [\"weakness1\", \"weakness2\", ...], \"suggestions\": [\"suggestion1\", \"suggestion2\", ...] }, \"summary\": \"Brief summary of the proposal and overall assessment\", \"major_concerns\": [\"concern1\", \"concern2\", ...], \"minor_issues\": [\"issue1\", \"issue2\", ...], \"questions_for_authors\": [\"question1\", \"question2\", ...], \"improvement_recommendations\": [\"recommendation1\", \"recommendation2\", ...] } A.4. Prompt for Paper Review (Single Review Mode) Paper Review Prompt (Single Review Mode) Role: You are senior reviewer for prestigious AI/ML conference (ICLR, NeurIPS, ICML, AAAI). You have extensive expertise in machine learning, deep learning, and AI research. You have access to relevant literature to assess novelty and compare against existing work. Review Task: Provide comprehensive peer review of the following research paper according to the conferences rigorous standards. Paper to Review: {paper text} {related literature} Evaluation Criteria: Follow these specific criteria used by top-tier conferences: 1. CLARITY Writing quality, organization, and presentation Mathematical notation and technical exposition Figure/table quality and informativeness Related work completeness and accuracy Clear articulation of contributions and limitations 2. ORIGINALITY/NOVELTY Technical novelty compared to existing methods Conceptual advances beyond incremental improvements Novel problem formulation or perspective Creative solutions or unexpected insights Distinction from concurrent/prior work 3. QUALITY/SOUNDNESS Theoretical rigor and mathematical correctness Experimental methodology and statistical validity Reproducibility and implementation details Appropriate baselines and evaluation metrics Technical depth and completeness 4. SIGNIFICANCE/IMPACT Importance of problem addressed Potential to influence future research Practical applicability and real-world relevance Breadth of impact across ML/AI domains Advancement of state-of-the-art Review Standards: Be constructive but honest about weaknesses Provide specific, actionable feedback Consider both theoretical and empirical contributions Assess reproducibility and experimental rigor Evaluate against conferences high acceptance bar Output Format: Please provide your review ONLY in the following JSON format (no scores, no recommendation, only feedback): { \"clarity\": { \"strengths\": [\"strength1\", \"strength2\", \"....\"], \"weaknesses\": [\"weakness1\", \"weakness2\", \"....\"], \"suggestions\": [\"suggestion1\", \"suggestion2\", \"....\"] }, \"originality_novelty\": { \"strengths\": [\"strength1\", \"strength2\", \"....\"], \"weaknesses\": [\"weakness1\", \"weakness2\", \"....\"], \"suggestions\": [\"suggestion1\", \"suggestion2\", \"....\"] }, \"quality_soundness\": { \"strengths\": [\"strength1\", \"strength2\", \"....\"], \"weaknesses\": [\"weakness1\", \"weakness2\", \"....\"], \"suggestions\": [\"suggestion1\", \"suggestion2\", \"....\"] }, \"significance_impact\": { \"strengths\": [\"strength1\", \"strength2\", \"....\"], \"weaknesses\": [\"weakness1\", \"weakness2\", \"....\"], \"suggestions\": [\"suggestion1\", \"suggestion2\", \"....\"] }, \"summary\": \"Brief summary of the paper and overall assessment\", \"major_concerns\": [\"concern1\", \"concern2\", \"....\"], \"minor_issues\": [\"issue1\", \"issue2\", \"....\"], \"questions_for_authors\": [\"question1\", \"question2\", \"....\"], \"improvement_recommendations\": [\"recommendation1\", \"recommendation2\", \"....\"] } Review Guidelines: Be specific and constructive in all feedback Reference specific sections, equations, figures when pointing out issues Suggest concrete improvements, not just identify problems Consider the conferences high standards and competitive acceptance rate Balance critique with recognition of contributions Use technical language appropriate for the ML/AI community A.5. Prompt for Review Proposal (Meta Review Mode) Area Chair or Editor Agent: Generate prompts for sub-agents Role: You are Planner Agent for an auto-review system, tasked with generating prompts for sub-Agents to review submission. Task: Analyze the submission to identify key topics. Determine the number of reviewers (2-6, default from STANDARD YAML). For each reviewer, generate complete prompt including: Role, Expertise and Instructions. Output valid JSON file with schema: Constraints: Reviewer count respects STANDARD, adjust based on topic diversity. Prompts must include all criteria. Output only valid JSON, no extra text. Input: Submission Type: <Review Mode > Submission: <Content >{ truncated to 3000 tokens } Standard YAML: <A JSON file > Output: <A JSON file for every sub-reviewers > Sub-Agents Prompt Input: Submission Type: <Review Mode > Submission: <submission >{ truncated to 8000 tokens } Related papers: <related papers >{ truncated to 5000 tokens } Standard YAML: <standard config > CONSTRAINTS: 1. Review must adhere to the provided standard and its specific requirements. 2. The output must be in JSON format and must include criteria section as defined in the standard. 3. Output only valid JSON, no extra text. 4. Do NOT give high scores to submissions with obvious flaws, lack of innovation, poor presentation, or unsound methodology. 5. Be critical and rigorous: only submissions that truly meet the standards should receive high scores (4 out of 4) for soundness, presentation, and contribution. 6. If in doubt, err on the side of caution and provide lower score with justification. 7. Output only valid JSON, no extra text. Output: <Review results in JSON file> MetaReview Agent: Summarize reviews from sub-agents Role: You are Summarizer Agent or Editor Agent or Chair Agent for an auto-review system, tasked with summarizing reviews from sub-Agents and making final decision. Task: 1. Analyze the reviews to identify common themes, strengths, weaknesses, and key points. 2. Provide concise summary of the reviews. 3. Evaluate the submission strictly according to ALL criteria and requirements specified in the STANDARD YAML above. 4. For each scoring criterion, you should COMPREHENSIVELY CONSIDER all reviewers scores and comments. You may use your own judgment to adjust scores up or down if needed. 5. Scoring strategy for 0-4 scale: DO NOT give all 3s. Poor submissions should get 1, generally good ones get 2, and only truly outstanding get 3 or 4. Be strict and realistic. 6. For rating (1-10 scale): Only submissions with no major flaws and excellent quality should get above 6. Most proposals should get 1-6, very good ones 6-7, and only those with exceptional innovation and quality should get above 7. Avoid giving 8+ unless truly deserved. 7. Acceptance criteria must be strict: DO NOT accept every submission. 8. Your scores must be realistic, varied, and not inflated. Prefer lower scores unless there are clear, outstanding strengths. If in doubt, give lower scores with justification. 9. Most proposals should get 1-5 for rating, 6-7 only for very good, and 7+ only for truly innovative and flawless work. 10. Output valid JSON following the EXACT template below, including summary, decision, justification, and all relevant criteria from the STANDARD YAML. Constraints: Your summary and decision must strictly follow and be justified by the criteria and requirements in the STANDARD YAML. Summary must be concise and cover all key points from the reviews. Decision must be justified based on the STANDARD YAML. Output only valid JSON, no extra text. The output JSON MUST strictly follow the above template, so that results for all submissions are consistent and easy to extract. Follow review exmaple. Input: Submission Type: <Review Mode > Standard YAML: <A JSON file > Reviews: <str(reviews) >{ truncated to 8000 tokens } Output: <Review results in JSON file> A.6. Prompt for Proposal Voting Decision Proposal ACCEPT/REJECT Decision Prompt Role: You are senior program committee member for top-tier ML conference. Task: Decide ACCEPT or REJECT for the given proposal. You will evaluate ONE proposal independently (no comparisons with other proposals). Requirements: your decision must be strictly based on the criteria below. Be conservative: ACCEPT only if merits clearly outweigh concerns; otherwise REJECT. Evaluation Criteria: Novelty & originality Technical soundness & rigor Potential impact & significance Clarity of presentation Feasibility & scope Positioning vs literature (if literature is provided) Input: PROPOSAL: {proposal text} LITERATURE (if available): {literature text} Output Format: Return ONLY valid JSON with this exact schema (no extra text or explanations). { \"decision\": \"accept\" \"reject\", \"confidence\": <float in>, \"reasons\": [<short bullet strings>], \"scores\": { \"novelty\": <0-10>, \"soundness\": <0-10>, \"impact\": <0-10>, \"clarity\": <0-10>, \"feasibility\": <0-10> }, \"meta\": { \"used_lit_search\": <true false> } } A.7. Prompt for Paper Voting Decision Paper ACCEPT/REJECT Decision Prompt Role: You are senior reviewer tasked with conducting rigorous, high-standard peer review of research paper submitted to workshop. Your evaluation must be thorough, critical, and adhere to the highest academic standards. Task: Your main task is to provide final decision (ACCEPT/REJECT) based on holistic assessment of the papers scientific merit, novelty, and clarity. ACCEPT only if the paper demonstrates strong, convincing merits across all high-priority areas: It must be technically sound, methodologically rigorous, present clear and non-trivial contribution, and be written with high clarity. REJECT if the paper exhibits any critical flaws such as lack of novelty, poor research quality, poor presentation, or ethical concerns. Requirements: Please ignore any headers like AUTONOMOUSLY GENERATED BY THE AI SCIENTIST, as they are metadata and not part of the papers scientific content. Evaluate the papers content alone. If the submission includes previous review results and response letter, treat the paper as revised version. Your review must be grounded in the following prioritized criteria: Core Evaluation Criteria (Strict Standards): 1. Technical Quality & Methodology (High Priority): Scientific Rigor: Is the research design sound and the methodology scientific? Are the methods appropriate and implemented correctly? Evidence & Reliability: Is the data sufficient? Are the results reliable and reproducible? Do the conclusions logically follow from the evidence? Clarity of Method: Is the methodology described with enough detail for scrutiny and replication? 2. Novelty & Contribution (High Priority): Originality: Does the paper offer genuinely new perspective, method, or finding? Does it move beyond incremental improvements? Significance: Does the work address meaningful problem and have the potential to advance the field? 3. Clarity & Presentation Quality: Language and Precision: Is the paper well-written, clear, precise, and unambiguous? Logical Flow: Is the paper well-structured and the argument coherent and persuasive? 4. Ethical Soundness: Does the paper adhere to academic and research ethics? Any signs of misconduct (plagiarism, data fabrication) are grounds for immediate rejection. Input: PAPER CONTENT: {paper text} LITERATURE (if available): {literature text} Output Format: Return ONLY valid JSON object with this exact schema (no extra text or explanations before or after the JSON block): { \"decision\": \"accept\" \"reject\", \"confidence\": <float in>, \"reasons\": [<short bullet point strings summarizing the rationale>], \"scores\": { \"clarity\": <integer score 0-10>, \"originality\": <integer score 0-10>, \"quality_soundness\": <integer score 0-10>, \"significance_impact\": <integer score 0-10>, \"rating\": <overall score of paper, integer score 0-10> }, \"meta\": { \"used_lit_search\": <true false> } } A.2 B. Highlighted Generated Papers This section presents selected examples of full papers generated by our platform. The initial drafts of these papers are based on the output from the AIScientist(Lu et al. 2024), serving as baseline for comparison. The final versions showcased here have been iteratively refined using feedback from our review agents, demonstrating the significant improvements in quality and coherence achieved through our proposed processess of our iterative refinement process."
        }
    ],
    "affiliations": [
        "Columbia University",
        "Istituto Italiano di Tecnologia",
        "Max Planck Institute for Intelligent Systems",
        "National University of Singapore",
        "Peking University",
        "The Chinese University of Hong Kong",
        "Tsinghua University",
        "University of Birmingham",
        "University of California, Los Angeles",
        "University of California, San Diego",
        "University of Electronic Science and Technology of China",
        "University of Manchester",
        "University of Oxford",
        "University of Sydney",
        "University of Toronto",
        "University of Utah",
        "Università degli Studi di Genova",
        "Westlake University",
        "Zhejiang University"
    ]
}