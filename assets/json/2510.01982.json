{
    "paper_title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models",
    "authors": [
        "Yujie Zhou",
        "Pengyang Ling",
        "Jiazi Bu",
        "Yibin Wang",
        "Yuhang Zang",
        "Jiaqi Wang",
        "Li Niu",
        "Guangtao Zhai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness."
        },
        {
            "title": "Start",
            "content": "arXiv preprint G2RPO: GRANULAR GRPO FOR PRECISE REWARD IN FLOW MODELS Yujie Zhou1,4 Pengyang Ling2,4 Yuhang Zang4 Jiaqi Wang4,5 Li Niu1 Guangtao Zhai1 1Shanghai Jiao Tong University 2University of Science and Technology of China 5Shanghai Innovation Institute 3Fudan University https://github.com/bcmi/Granular-GRPO Jiazi Bu1,4 Yibin Wang3,5 4Shanghai AI Laboratory 5 2 0 O 2 ] . [ 1 2 8 9 1 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose novel Granular-GRPO (G2RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing high correlation between the reward and the injected noise, thereby facilitating faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our G2RPO significantly outperforms existing flow-based GRPO baselines, highlighting its effectiveness and robustness."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in generative models, particularly diffusion models (Ho et al., 2020; Song et al., 2020a;b) and flow models (Lipman et al., 2022; Liu et al., 2022; Peebles & Xie, 2023), have revolutionized visual content creation, offering unprecedented capabilities in generating high-quality images (Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024; Labs, 2024) and videos (Blattmann et al., 2023; Chen et al., 2024; Guo et al., 2023; Kong et al., 2024; Wan et al., 2025). However, key challenge remains in aligning model outputs with the diverse and complex human preferences. To tackle this challenge, reinforcement learning from human feedback (RLHF) (Fan et al., 2023; Black et al., 2023) has emerged as promising solution, characterized by its adaptability and costeffectiveness. Paradigms such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), Direct Policy Optimization (DPO) (Rafailov et al., 2023), and Group Relative Policy Optimization (GRPO) (Shao et al., 2024) have been introduced. Among these, GRPO stands out as an innovative online reinforcement learning approach. By leveraging group comparisons to optimize policies, GRPO eliminates the need for separate value model, achieving greater flexibility and scalability. To integrate GRPO into flow-based generative models, Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025) substitute the deterministic ODE sampler with an SDE formulation, wherein the injected stochasticity deliberately perturbs the denoising direction at each step. Although the resulting samples enable exhaustive per-step exploration for reinforcement learning, they simultaneously underscore the difficulty of attributing the final reward to any specific random per- *Equal contribution. Corresponding author. 1 arXiv preprint Figure 1: Comparison between our G2RPOand existing studies. (a) G2RPOsignificantly outperforms DanceGRPO in reward scores (HPS-v2.1 in this figure). (b) Sampling strategy comparison. G2RPOacquire dense reward by confining stochasticity to individual sampling steps. (c) Sampling grain comparison. G2RPOachieves comprehensive evaluation of each sampling direction by integrating advantages from multi-granularity ODE denoising. (d) Visual Comparison. Compared to the baseline method, the images generated by G2RPOare more aligned with human preferences. turbation, thereby constraining model trainability. Specifically, most existing flow-based GRPO methods encounter two core issues in evaluating group denoising directions: 1) Sparse reward: As shown in Fig. 1 (b), the final reward signal is uniformly assigned to each SDE sampling step, which cannot be precisely aligned with the sampling direction at each step, leading to inaccuracies for the optimization at individual steps. 2) Incomplete evaluation: As depicted in Fig. 1 (c), each denoising direction is bound to fixed number of denoising steps, resulting in singular granularity of denoised images, which impairs the reward models ability to conduct comprehensive comparison across the group. To address these limitations, we propose Granular-GRPO (G2RPO), novel online reinforcement learning framework specifically designed for precise and comprehensive reward signals. First, mirroring the sparse-reward problem (Hare, 2019; Liang et al., 2024) that plagues RLHF, the reward signal in the SDE sampling process is delivered only after an entire sequence of decisions. This long delay undermines the credit-assignment chain, preventing the linking of the terminal reward with any specific earlier action and thereby inducing sluggish, unstable learning. Therefore, we propose simple yet effective sampling strategy, termed Singular Stochastic Sampling. As illustrated in Fig 1 (b), this strategy applies the SDE formulation at single time step to generate group of denoising directions, while employing deterministic ODE sampling for all other steps. By concentrating stochasticity at one specific step, the proposed method establishes strong correlation between the reward signal and the injected noise, enabling stable model optimization. Secondly, we propose Multi-Granularity Advantage Integration (MGAI) module. As depicted in Fig. 1(c), instead of binding each denoising direction to fixed subsequent denoising granularity, the denoising directions in the same group are assigned to spectrum of denoising steps, producing images with different granularities. The corresponding reward signals of these images are then fused into unified advantage estimate, yielding comprehensive evaluation of the current states value. With the support of the Singular Stochastic Sampling strategy and the Multi-Granularity Advantage Integration module, G2RPOcan provide more precise and comprehensive reward signal, thereby enhancing the upper limit of the GRPO model training. As shown in Fig 1 (a), our reward curves exhibit stable and significant improvements over the baseline during training. Additionally, Fig 1 (d) illustrates the images generated by G2RPO, highlighting its advantages in text prompt adherence and detail fidelity. 2 arXiv preprint Our contributions can be summarized as follows: (1) Granular-GRPO: novel flow-based GRPO framework designed to provide precise and comprehensive evaluation of the denoising directions sampled by the SDE, thereby improving the precision of model optimization. (2) Singular Stochastic Sampling: sampling strategy confines stochasticity to individual sampling steps, addressing the sparse reward issue associated with long-range stochasticity injection. (3) Multi-Granularity Advantage Integration: module integrates the advantages of multi-granularity denoised images and enables comprehensive evaluation of each sampling direction. (4) Superior Performance: Extensive experiments across various reward models demonstrate that our G2RPOsignificantly outperforms existing baselines, demonstrating its effectiveness and robustness."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Alignment for Large Language Models. Recent years have witnessed paradigm shift from supervised fine-tuning (Dong et al., 2023; Sun, 2024) to multi-turn online reinforcement learning (Shani et al., 2024; Abdulhai et al., 2023) when aligning Large Language Models (LLMs) with human intent, which is known as Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2025; Ouyang et al., 2022). Early RLHF pipelines typically involve training reward model from pairwise comparisons to predict human preferences and guide policy model through reinforcement learning algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017). Despite their effectiveness, PPO introduces intensive computational overhead and is sensitive to reward model inaccuracies, motivating value-free alternatives such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Adopted by leading LLMs including OpenAI-o1 (Jaech et al., 2024) and DeepSeekR1 (Guo et al., 2025), GRPO aims to optimize policies based on relative preferences within group of samples, providing robust signal for policy improvement, particularly when absolute rewards are difficult to define or noisy. These advancements in LLM alignment provide strong foundation for exploring similar human-centric optimization strategies in visual generation domains. Alignment for Flow Models. Diffusion and Flow models (Ho et al., 2020; Song et al., 2020a;b; Peebles & Xie, 2023; Rombach et al., 2022), which offer flexible visual creation through an iterative denoising process, have revolutionized the field of visual synthesis and become pivotal part of generative models. Building on the success of aligning LLMs with human preferences, similar techniques have recently been transplanted to diffusion and flow models (Podell et al., 2023; Esser et al., 2024; Labs, 2024). Pioneer works like DDPO (Black et al., 2023) and ReFL (Xu et al., 2023) apply PPO to finetune diffusion models for improved aesthetic performance and human feedback alignment. These methods face challenges inherent to RL, including high variance, low efficiency, and sparse reward. Diffusion-DPO (Wallace et al., 2024) adapts the Direct Preference Optimization (DPO) (Rafailov et al., 2023) framework to directly optimize diffusion models from paired preference data, bypassing the need for an explicit reward model but suffering from distribution shift since no new samples are collected during training. Recent efforts such as DanceGRPO (Xue et al., 2025) and Flow-GRPO (Liu et al., 2025) enable GRPO-style policy updates by converting the ODE sampling into an equivalent SDE to each timestep, thereby acquiring group of denoising directions for statistical sampling and RL exploration for flow models. More recently, MixGRPO (Li et al., 2025) has improved training efficiency through hybrid ODE-SDE sampling approach while maintaining comparable performance. However, these methods are generally constrained by sparse rewards due to long-range stochasticity injection and the binding of each sampling direction to fixed denoising granularity. These paradigms restrict the ability to conduct comprehensive evaluation of each sampling direction, limiting the optimization ceiling of GRPO training."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "For the flow-based GRPO methods (Xue et al., 2025; Liu et al., 2025; Li et al., 2025), the denoising process is first modeled as multi-step Markov decision process (MDP). Given prompt c, the agent with flow model pθ produce reverse-time trajectories defined as Γ = (sT , aT , sT 1, aT 1, . . . , s0, a0), where st = (c, t, xt) is the state at timestep and xt is the corresponding noisy sample. Specifically, xT (0, I) and x0 is the denoised image. The action at represents the single step denoising process with the policy πθ, indicating sampling direction dx dt from xt to xt1, i.e. xt1 πθ(xt1xt, c). 3 arXiv preprint SDE Sampling. As an online RL algorithm, GRPO needs grouped outputs and relative advantages to optimize policies. However, the flow matching model utilizes deterministic ODE to sample the denoising direction: dxt = vθ(xt, t)dt, (1) where, vθ(xt, t) is the model output, given the noisy sample xt and timestep t. To match GRPOs stochastic sampling requirements, Flow-GRPO (Liu et al., 2025) converts the ODE into an equivalent SDE sampling with the same marginal distribution: (cid:18) dxt = vθ (xt, t) + σ2 2t (xt + (1 t)vθ (xt, t)) dt + σtdwt, (2) (cid:19) where dwt denotes Wiener process increments, and σt controls the stochasticity injected into the sampling direction. Furthermore, it can be discretized via the EulerMaruyama scheme: (cid:18) xt+t = xt + vθ (xt, t) + σ2 2t (xt + (1 t)vθ (xt, t)) + σt (cid:19) tϵ, ϵ (0, I). (3) As defined in Flow-GRPO, σt = η (cid:113) 1t , where the noise level is controlled by hyperparameter η. GRPO Training. With SDE sampling, flow-based GRPO methods introduce stochasticity at each timestep to generate group of images {xi 0, c) to xi 0, and the advantage is computed as: i=1. Then the reward model assigns score R(xi 0}G Ai = R(xi 0, c) mean({R(xj 0, c)}G std({R(xj 0, c)}G j=1) j=1) . (4) 0 obtained from the final step image are uniformly broadcast to each step to evaluate the SDE sampling directions. Finally, the policy model is optimized by maximizing Note that the advantages Ai Ai the following objective: JFlow-GRPO (θ) = cC,{xi}G i=1πθold (c)f (r, A, θ, ε, β), (5) where (r, A, θ, ε, β) = 1 (cid:88) i= 1 1 (cid:88) t=0 (cid:0)min (cid:0)ri t(θ)Ai t, clip (cid:0)ri t(θ), 1 ε, 1 + ε(cid:1) Ai (cid:1) βDKL (πθπref )(cid:1) , (cid:0)xi (cid:0)xi Notably, β is the hyperparameter that controls the proportion of KL loss. Following the practices of DanceGRPO and MixGRPO, we set β = 0 to achieve more stable training process. t1 xi t1 xi t, c(cid:1) t, c(cid:1) . ri t(θ) = pθ pθold (7) (6)"
        },
        {
            "title": "4 GRANULAR-GRPO",
            "content": "As an online RL algorithm, flow-based GRPO methods utilize SDE to sample group of denoising directions for optimization. core issue underlying this paradigm is to obtain precise and comprehensive assessments of each sampling direction. To this end, we introduce the G2RPOframework to (i) confine stochasticity to individual steps (Singular Stochastic Sampling) for more precise reward signals, and (ii) integrate the advantages derived from multi-granularity denoising results (MultiGranularity Advantage Integration) to acquire more comprehensive evaluation, as shown in Fig. 2. 4.1 SINGULAR STOCHASTIC SAMPLING Traditional flow-based GRPO methods introduce SDE sampling at every step to inject stochasticity and uniformly assign the final image reward to each steps sampling direction. According to Eq. 4, the advantage Ai of each sampling direction at step is equally assigned with Ai 0. However, the reward signal available only after multiple decision steps impedes the models capability to link the final reward to each decision, thereby resulting in imprecise and sparse rewards. 4 arXiv preprint Figure 2: Overview of G2RPO. Given text prompt and an initial noise, our Singular Stochastic Sampling strategy employs SDE sampling solely at single step and samples group of distinct denoising directions. Then, the Multi-Granularity Advantage Integration module executes multigranularity ODE denoising for each direction and integrates the advantages to produce comprehensive evaluation for each sampling direction. For simplicity, the figure shows one coarse-grained path (denoted as c) and one fine-grained path (denoted as ). To acquire dense reward for each SDE sampling direction, simple yet effective strategy is to confine the stochasticity to the single step selected for optimization. Firstly, we designate set of candidate SDE timesteps {1, . . . , } with = . As shown in Fig. 2, given prompt and initial noise xT , each timestep denoted by will be optimized in the training phase. Then, common starting point xk for the group is acquired using ODE sampling from Eq. 1. Notably, the Singular Stochastic Sampling strategy employs SDE sampling only at xk and samples distinct denoising directions to get the next noisy state {xi k1 undergoes 1 steps of ODE sampling to generate deterministic denoised image xi 0k. Based on our sampling strategy, the variance of the group reward {R(xi i=1 is entirely determined by the distinct denoising directions introduced by the SDE sampling at step k. And step-aware, precise advantage can be acquired: i=1. Then each xi 0k, c)}G k1}G Ai = R(xi 0k, c) mean({R(xi 0k, c)}G std({R(xi 0k, c)}G i=1) i=1) . (8) Consequently, the (r, A, θ, ε, β) in Eq 5 can be formulated as: (r, A, θ, ε, β) = 1 (cid:88) i= 1 (cid:88) kM (cid:0)min (cid:0)ri k(θ)Ai k, clip (cid:0)ri k(θ), 1 ε, 1 + ε(cid:1) Ai (cid:1)(cid:1) . (9) k1 xi After the sampling phase is completed, the training of GRPO requires the computation of k, c(cid:1) to obtain ri (cid:0)xi pθ k(θ) refer to eq 7. In practice, each distinct sampling starting point xi needs to be fed into the flow model to compute the corresponding ODE denoising direction vi k. However, our sampling strategy shares common starting point xk, allowing group of samples to reuse the same vk, which in turn improves training efficiency. 4.2 MULTI-GRANULARITY ADVANTAGE INTEGRATION Singular Stochastic Sampling accurately constrains stochasticity into the single SDE step, ensuring strong correlation between the reward and the injected noise. Nevertheless, how to acquire comprehensive reward for the denoising direction of the current step still requires further investigation. 5 arXiv preprint Figure 3: Visual Comparison of Images Denoised at Different Granularities. Images denoised at different granularities exhibit variations in fine details and textures, leading to inconsistent scoring by the Reward Model (HPS-v2.1). This observation reveals the insufficiency of single-granularity evaluation of group advantage. As shown in Fig. 3, we observe that under identical xk and prompt conditions, the denoising trajectory generated by singular stochastic sampling is not robust when assessing the corresponding SDE denoising direction. Images generated from denoising trajectories with different granularities show similar overall content but exhibit discrepancies in detail due to the varying denoising intervals. Such differences are evident in the scores assigned by the Reward Model, further influencing the numerical values of the advantage within the group, and even the optimization direction. To this end, we propose Multi-Granularity Advantage Integration module to perform multigranularity denoising on the sampled denoising directions within group. The advantages of the images denoised at different granularities are then integrated to form the final evaluation. Specifically, as shown in Fig. 2, step is the SDE sampling step, and distinct denoising directions are sampled to acquire next noisy state {xi i=1. Under the conventional granularity condition, each xi k1 undergoes 1 steps to obtain the final denoised image. The sequence of denoising timesteps can be represented as: = {1, 2, . . . , 1}. For our Multi-Granularity denoising module, set of integer scaling factors Λ = {λ1, λ2, ..., λj}, Λ = is defined to represent different denoising granularities. Each λj implements interval sampling for different denoising granularity, that means sample every λj-th step from the total 1 steps. The denoising timestep sequence Sj can be formally represented as: k1}G Sj = {1, 1 + λj, 1 + 2λj, . . . , (cid:25) (cid:24) 1 λj λj}, (10) Our interval sampling approach ensures that the denoising process is performed at regular intervals defined by λj. As λj increases, the granularity becomes coarser, allowing for more flexible and adaptive denoising process. For ease of illustration, = 2 in the Fig. 2. 0 }G After Nj subsequent steps denoising for {xi i=1 are generated. Subsequently, different groups images gets the reward {R(xi,j i=1 from reward model and then compute the intra-group advantages {Ai,j i=1 with Eq. 4. Similar to the joint training with multiple reward models (e.g., HPS-v2.1 and CLIP Score) in DanceGRPO, where the advantages from different reward models are directly summed, we combines the advantages from different granularities to get {Ai,mix i=1, group of noise-free images {xi,j 0k, c)}G k1}G }G }G i=1: Ai,mix = (cid:88) Ai,j Finally, (r, A, θ, ε, β) is updated to: (r, A, θ, ε, β) = 1 (cid:88) i= 1 (cid:88) (cid:16) min (cid:16) k(θ)Ai,mix ri kM 6 , clip (cid:0)ri k(θ), 1 ε, 1 + ε(cid:1) Ai,mix (11) (cid:17)(cid:17) , (12) arXiv preprint and using Eq. 5 to optimize the policy πθ. detailed algorithm is illustrated in Algorithm 1. Init same noise xT (0, I) for do Update old policy model: πθold πθ Sample batch prompts Cb for prompt Cb do Algorithm 1 G2RPOTraining Process 1: Require: Prompt dataset C, policy model πθ, reward model R, total sampling steps 2: Require: SDE sampling timestep set , Denoising granularities set Λ (Λ = J) 3: for training iteration = 1 to do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: ODE Sampling with granularity λj: xi,j t1 SDE Sampling group samples: xi else if > then for λj Λ do for = to 0 do if > then // i-th direction in the group ODE Sampling: xt1 else if == then end for 0k) end if end for Get group of reward: R(xi,j R(xi,j 0k,c)µj Aj (cid:80)J j=1 Aj Amix end for Compute GRPO loss J(θ) σj end for Update policy: gradient ascent on J(θ) 21: 22: 23: 24: 25: 26: 27: end for"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 IMPLEMENTATION DETAILS Datasets and Backbone. Following DanceGRPO (Xue et al., 2025) and MixGRPO (Li et al., 2025), we evaluate our G2RPOusing the HPSv2 (Wu et al., 2023) dataset. It contains 103,700 text prompts for training and 400 diverse prompts for testing. The text-to-image model employed for reinforcement learning is Flux.1-dev (Labs, 2024), leading flow model in the community. Evaluation Metrics. To evaluate the effectiveness and robustness of our G2RPO, multiple reward models are applied as evaluation metrics. These reward models assess the alignment between generated images and human preferences from multiple dimensions. Specifically, HPS-v2.1 (Wu et al., 2023), CLIP Score (Radford et al., 2021), and Pick Score (Kirstain et al., 2023) collectively assess semantic alignment and visual coherence. Image Reward (Xu et al., 2023) focuses on visual quality and aesthetic appeal, while Unified Reward (Wang et al., 2025) is the SOTA unified reward model that comprehensively evaluates both alignment with the caption and overall image quality. Evaluation Setting. Similar to DanceGRPO and MixGRPO, two experimental settings are employed. Firstly, single HPS-v2.1 reward model is utilized for training to verify the upper limit of improvement for in-domain performance. However, as demonstrated by DanceGRPO, HPS-v2.1 is prone to model hacking due to biases in the training set, leading to degradation in other evaluation metrics. Therefore, our primary experiment also involves joint training with both HPS-v2.1 and CLIP Score as reward models to acquire stable and robust results. Sampling Phase. Following DanceGRPO, shared initialization noise is used to generate group of 12 images from the same text prompt. The total sampling step = 16 to enhance computational efficiency and the advantage clip ε = 5 in Eq. 6. Note that, the parameter η in Eq. 3 directly arXiv preprint determines the noise level, which in turn defines the size of the stochastic exploration space in the SDE. Leveraging the Singular Stochastic Sampling strategy, our precise reward can tolerate larger η = 0.7. The set of candidate SDE timesteps consists of the first 8 timesteps to improve training efficiency. For the Multi-Granularity Advantage Integration strategy, the set of distinct granularities Λ = {1, 2, 3}. Training Phase. All experiments are conducted using 16 NVIDIA H200 GPUs, with batch size of 1. The AdamW optimizer is used, configured with learning rate of 2 106 and weight decay of 1 104. Mixed precision training is implemented using bfloat16 (bf16) format. The training iteration is 300."
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "Quantitative Evaluation. As shown in Tab. 1, DanceGRPO and MixGRPO serve as the baselines for comparison with our G2RPO. Additionally, the performance of employing the Singular Stochastic Sampling strategy without multi-granularity denoising (G2RPOw/o MGAI) is also proposed. It can be observed that when HPS-v2.1 is used solely as the training reward model, our Singular Stochastic Sampling achieves relative improvement of 6.52% compared to DanceGRPO. This indicates that constraining the stochasticity to single step yields precise reward signal, which in turn provides more faithful optimization signal and enables GRPO to enhance the optimization ceiling. However, as demonstrated by DanceGRPO, optimizing solely with HPS-v2.1 can induce model hacking, which in turn compromises other out-of-domain evaluation metrics. Secondly, for the setting of multi-reward (HPS-v2.1 and CLIP Score) optimization, the results indicate that G2RPOattains superior performance across both in-domain and out-of-domain rewards. Under the multi-granularity denoising condition, groups of images at different granularities provide more comprehensive evaluation for the SDE sampling direction. This multi-granularity paradigm also allows for more flexible adaptation to the preferences of various reward models, thereby achieving significant improvements in various out-of-domain dimensions. Table 1: Quantitative Results. Comparison of results on in-domain and out-of-domain rewards. Reward Model Method In-Domain Out-of-Domain HPS-v2.1 CLIP Score Pick Score ImageReward Unified Reward / HPS-v2.1 HPS-v2.1 & CLIP Flux.1-dev DanceGRPO MixGRPO G2RPOw/o MGAI G2RPO DanceGRPO MixGRPO G2RPOw/o MGAI G2RPO 0.305 0.353 0.378 0.376 0.385 0.331 0.363 0.372 0.376 0. 0.375 0.358 0.351 0.355 0.389 0.399 0.395 0.406 0.226 0.228 0.225 0.228 0.229 0.227 0.230 0.234 0.235 1. 1.233 1.266 1.286 1.313 1.128 1.436 1.421 1.483 3.621 3.548 3.421 3.469 3.487 3.569 3.661 3.688 3.783 Qualitative Comparison. Fig. 4 presents the qualitative comparison among the original Flux.1dev, DanceGRPO, MixGRPO, and our proposed G2RPO. It can be observed that G2RPOprovides enhanced detail fidelity and improves the consistency with the text prompt, achieving superior alignment with human preferences. For instance, in the second column, our G2RPOfaithfully captures the specified expressions and even the nuances of the chess pieces as described in the prompt, delivering finer details and higher visual quality. Moreover, in the poster case depicted in the last column, G2RPOnot only adheres to the spatial requirement of clear left-right demarcation but also renders the reflections of the trees with remarkable clarity. Additionally, the overall style of the image generated by G2RPOis more consistent with the aesthetic demands of poster design. 5.3 ABLATION STUDY As described in Section 4, the Multi-Granularity Advantage Integration module integrates multigranularity ODE sampling results to evaluate the SDE sampling direction comprehensively. Different granularities represent diverse sampling intervals during denoising, controlled by the parameter set Λ. To validate the effectiveness of multi-granularity fusion, we perform ablation studies on various Λ set shown in Tab. 2. It can be found that as the number of selected granularities increases, 8 arXiv preprint Figure 4: Qualitative Results.Comparison with existing flow-based GRPO methods, in which our G2RPOdemonstrates superior performance in human preference alignment. the evaluation of each sampling direction becomes more comprehensive, facilitating robust assessment through multi-granularity advantage fusion, which significantly improves performance across both in-domain and out-of-domain reward models. Table 2: Ablation Study. Comparison for different denoising granularities. Reward Model Λ In-Domain Out-of-Domain HPS-v2.1 CLIP Score Pick Score ImageReward Unified Reward HPS-v2.1 & CLIP {1} {1, 2} {1, 3} {1, 2, 3} 0.372 0.375 0.378 0.376 0.395 0.404 0.404 0.406 0.234 0.234 0.234 0. 1.421 1.468 1.465 1.483 3.688 3.759 3.760 3.783 5.4 FURTHER EXPLORATION OF VARYING INFERENCE STEPS In the main experiment Tab. 1, we maintained the same inference settings as DanceGRPO and MixGRPO, generating images with resolution of 1024 1024 in 50 steps. Notably, our MGAI module, which evaluates denoising samples of different granularities in mixed manner, provides more comprehensive assessment. With the assistance of this module, G2RPOexhibits stronger robustness to varying denoising step configurations. As shown in Tab. 3, all flow-based GRPO methods are jointly trained with HPS-v2.1 and CLIP. When the total inference timesteps are reduced to 20 or even 10 steps, G2RPOstill achieved significant performance improvements across various in-domain and out-of-domain evaluation reward models. 9 arXiv preprint Table 3: Comprehensive evaluation of total denoising steps. Reward Model Method In-Domain Out-of-Domain HPS-v2.1 CLIP Score Pick Score ImageReward Unified Reward 10 Step Inference 20 Step Inference Flux.1-dev DanceGRPO MixGRPO G2RPO Flux.1-dev DanceGRPO MixGRPO G2RPO 0.289 0.325 0.358 0. 0.300 0.329 0.363 0.376 0.388 0.390 0.401 0.408 0.389 0.388 0.401 0.407 0.225 0.227 0.230 0.235 0.226 0.228 0.230 0.235 0.939 1.129 1.431 1. 1.034 1.136 1.430 1.511 3.504 3.576 3.641 3.805 3.575 3.586 3.651 3."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper addresses the critical limitations of precisely evaluating the quality of denoising directions sampled by Flow-based GRPO for human preference alignment. We introduce G2RPO, novel online RL framework that precisely localizes stochasticity to single step within the denoising process and provides comprehensive evaluation of SDE denoising directions by integrating the advantages derived from images at different denoising granularities. This innovative design enables the provision of dense, precise reward signals, thereby fundamentally improving optimization accuracy and leading to more robust and higher-quality alignment. Our extensive experiments consistently demonstrate that G2RPOachieves superior performance across diverse reward conditions, marking significant advancement in aligning generative models with human preferences. 10 arXiv preprint"
        },
        {
            "title": "7 ADDITIONAL QUALITATIVE EVALUATION",
            "content": "Figure 5: Qualitative comparison with existing GRPO methods. Best viewed zoomed in. 11 arXiv preprint Figure 6: Qualitative comparison with existing GRPO methods. Best viewed zoomed in. 12 arXiv preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73107320, 2024. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Joshua Hare. Dealing with sparse rewards in reinforcement learning. arXiv preprint arXiv:1910.09281, 2019. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 13 arXiv preprint Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2(5):7, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, et al. Multi-turn reinforcement learning with preference human feedback. Advances in Neural Information Processing Systems, 37:118953 118993, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Hao Sun. Supervised fine-tuning as inverse reinforcement learning. arXiv preprint arXiv:2403.12017, 2024. 14 arXiv preprint Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "University of Science and Technology of China"
    ]
}