{
    "paper_title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
    "authors": [
        "Ebubekir Tosun",
        "Mehmet Emin Buldur",
        "Özay Ezerceli",
        "Mahmoud ElHussieni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 1 ] . [ 1 1 5 2 3 1 . 1 0 6 2 : r Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in 15-Million Node Turkish Synonym Graph Ebubekir Tosun ebubekirsoftware@gmail.com Mehmet Emin Buldur mehmeteminbuldur@outlook.com Özay Ezerceli ezerceli804@gmail.com Mahmoud ElHussieni mahmoud.elhussieni@outlook.com"
        },
        {
            "title": "Abstract",
            "content": "Neural embeddings have notorious blind spot: they cant reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. Weve built large-scale semantic clustering system specifically designed to tackle this problem headon. Our pipeline chews through 15 million lexical items, evaluates massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot spicy pain depression) while simultaneously resolving polysemy. Our approach employs topology-aware two-stage expansionpruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse."
        },
        {
            "title": "Introduction",
            "content": "The foundational assumption underlying most neural semantic search systems that high cosine similarity between embedding vectors correlates with synonymy is demonstrably false. Embedding models trained on contrastive objectives (Wang and Isola, 2020) or word2vec-style objectives (Levy 1 and Goldberg, 2014) optimize for capturing distributional similarity, which conflates multiple semantic relationships into single continuous measure (Elekes et al., 2017; Schneidermann et al., 2020). Antonyms and synonyms alike often occupy proximate regions in embedding space (Scheible et al., 2013; Rizkallah et al., 2020; Samenko et al., 2020) because they appear in nearly identical linguistic contexts (Liu et al., 2017; Chaurasia and McDonnell, 2016), merely differing in implicit negation or opposition. This issue is especially pronounced when synonym databases are built by applying high similarity cutoff (e.g., cosine similarity 0.85), which often admits false positives such as antonyms or broadly related terms. At scale, two factors further aggravate semantic ambiguity in graph clustering. First, modularitybased methods such as Louvain (Blondel et al., 2008) and Leiden (Traag et al., 2019) impose hard partitions, forcing polysemous words into single cluster. For example, the Turkish word yüz (face / 100) cannot be represented in multiple senses simultaneously, which either removes valid synonym links or collapses unrelated regions into noisy clusters. Second, semantic drift arises from transitivity: chains of locally plausible links can connect semantically distant concepts (e.g., Sıcak hot Acı spicy Üzüntü pain Depresyon depression), and standard clustering methods (e.g., connected components or hierarchical agglomeration) typically cannot distinguish such spurious paths from genuine synonym structure. These difficulties are amplified in morphologically rich, non-English settings. Turkish, in particular, lacks large machine-readable synonym resources comparable to WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2012). Existing resources suffer from two fundamental limitations: (1) they are sparse, containing insufficient coverage of domain-specific terminology (particularly in legal, medical, and technical domains); (2) they are brittle, often created through heuristic rule-based systems or simple distributional similarity thresholds without explicit disambiguation of semantic relationships. The construction of Turkish synonym resource at scale requires overcoming both antonym intrusion (false positives) and semantic drift (transitivity artifacts). This paper contributes three substantial advances addressing these challenges: Large-Scale Disambiguated Training Dataset We present rigorous methodology for generating 843,000 labeled semantic relationship pairs (Synonym, Antonym, Co-hyponym classes) through LLM-augmented synthesis paired with high-confidence dictionary extraction. We generated this dataset using Gemini 2.5-Flash (Comanici et al., 2025) for about $65 which is pretty costeffective, all things considered. It serves dual purpose: its novel research contribution in its own right, and it provides the training foundation for our discriminator model. The dataset strikes balance between synthetic LLM-generated pairs (827,000 instances) and carefully curated dictionary entries (16,000 instances). Whats especially useful here is that weve established reproducible pipeline that anyone can use to generate supervised semantic relationship data for any language as long as youve got sufficient terminological resources to work with. Specialized Multi-Class Discriminator Instead of just tweaking unsupervised similarity thresholds and hoping for the best, we train explicit three-way classifiers that can confidently tell synonyms apart from antonyms and co-hyponyms. Our best model hits 90% F1-macro with confidence scores above 0.98 on clear-cut cases, and weve validated that it shows solid cross-model consensus in practice. Think of this discriminator as strict semantic gatekeeper which it only lets through high-confidence synonym predictions (confidence 0.70) while actively filtering out antonyms and those murky low-confidence co-hyponyms. Novel Disambiguation-Aware Clustering Algorithm We propose two-stage graph clustering procedure (Expansion followed by Reduction/Pruning) that departs from standard community detection. In the Expansion stage, we apply soft clustering via overlap-based neighbor inclusion, allowing terms to participate in multiple candidate clusters to reflect polysemy. In the Reduction stage, we resolve ambiguity through topological voting, threelevel hierarchical scheme (majority rule, specificity preference, and deterministic tiebreak) that assigns each term to single best cluster while preserving recall. The method also mitigates semantic drift by seeding clusters with high-confidence pairs, sorting candidates by confidence, and enforcing intersection-ratio thresholds ( 0.51) to prevent weak transitive chains from inflating clusters. At scale, the pipeline processes 15 million unique terms, evaluates 520 million candidate synonym links through successive filters, and yields 2,905,071 final clusters (median size 3, mean 4.58, maximum 86). The resulting synonym graph enforces symmetry and conflict resolution, and it favors standardized domain terminology over noise such as OCR artifacts or informal abbreviations during parent selection."
        },
        {
            "title": "2 Related Work",
            "content": "Distinguishing true synonymy from broader semantic relatedness remains well-known limitation of distributional semantics (Mohammad and Hirst, 2012; McCarthy et al., 2010; Walde, 2013). Building on the distributional hypothesis (Harris, 1954; Faruqui and Dyer, 2014), vector-space models infer meaning from contextual co-occurrence. Although this reliably captures general semantic proximity (Baroni and Lenci, 2010; Goyal et al., 2013), it also creates systematic ambiguity: words that share contexts are embedded close together even when they stand in relations such as equivalence, opposition, or categorical relatedness (Bertolotti and Cazzola, 2024). Consequently, antonyms (e.g., hot vs. cold) and co-hyponyms (e.g., cat vs. dog) can appear as similar as genuine synonyms under cosine similarity (Plas and Tiedemann, 2006; Yih et al., 2012). Mohammad et al. (2013) quantified this effect, showing that standard similarity measures fail to represent relational polarity and can exhibit simultaneous similarity and contrast. Later studies report that the same limitation persists in transformer embeddings, even when trained with large-scale contrastive or retrieval objectives (Etcheverry and Wonsever, 2023). Thus, despite architectural progress, these models primarily encode distributional overlap rather than semantic equivalence, which makes synonym extraction via simple similarity thresholding unreliable (Garcia, 2021). 2 2.1 Constraint-Based Approaches to 2.3 Positioning of the Present Work Antonymy One influential line of research sought to address antonym intrusion by modifying embedding spaces using explicit lexical constraints. Counter-fitting (Mrkšic et al., 2016) introduced post-training adjustment procedure that pulls known synonyms closer together while pushing antonyms apart. Variants of this approach have shown strong improvements in intrinsic evaluation tasks and downstream lexical inference. However, their effectiveness is tightly coupled to the availability of large, highquality lexical resources (Dunietz et al., 2013) such as WordNet (Miller, 1995) or PPDB (Ganitkevitch et al., 2013). For morphologically rich and comparatively lowresource languages, this dependency presents serious bottleneck. Turkish, in particular, exhibits extensive inflectional and derivational productivity, yielding combinatorial explosion of valid word forms that are sparsely represented in manually curated lexicons (Kırkıcı and Clahsen, 2013; Durrant, 2013). Existing resources such as TurkishEnglish Parallel TreeBank (Ehsani et al., 2018) and KeNet (Bakay et al., 2021) are largely constructed through translation-based projection or limited expert annotation, resulting in insufficient coverage for large-scale semantic modeling. As consequence, constraint-based refinement methods do not scale effectively to the lexical breadth required for comprehensive Turkish synonym graphs."
        },
        {
            "title": "Drift",
            "content": "We address these issues with an integrated approach that combines semantic disambiguation, large-scale supervision, and topology-aware clustering. Instead of globally reshaping embeddings or depending on fixed lexical constraints, we train an explicit semantic relation discriminator on large disambiguated dataset, enabling direct filtering of synonym candidates rather than equating high similarity with synonymy. At the graph level, we avoid both strict hard partitioning and full sense induction. Our softto-hard clustering strategy (Appendix C) allows temporary overlap to reflect polysemy, then applies deterministic topological voting step to resolve ambiguity. This retains key advantages of senseaware clustering while remaining scalable. Unlike prior work, we target morphologically rich, low-resource settings. Using LLMaugmented supervision and confidence-based graph construction, we show that high-precision synonym graphs can be built without exhaustive manual lexicons, even at 15 million nodes."
        },
        {
            "title": "3 Methodology",
            "content": "Our system is organized as sequence of seven processing stages, each designed to address concrete challenge encountered when building large-scale synonym graph that is robust to antonym intrusion. Rather than treating the pipeline as monolithic procedure, we describe each stage separately to clarify its role and design rationale. In particular, Phases 35 introduce the core methodological contributions of this work, where semantic disambiguation and drift-aware clustering are explicitly enforced. Many synonymy methods treat the task as graph clustering, with words as nodes and semantic proximity as edges, but they are vulnerable to semantic drift: transitive chaining of valid local links can yield globally incoherent synonym sets. Standard community detection (e.g., Louvain; (Blondel et al., 2008), Leiden; (Traag et al., 2019)) further aggravates this by enforcing hard partitions that ignore polysemy, often merging unrelated senses or discarding legitimate links. Sense-aware approaches such as Watset (Ustalov et al., 2017) mitigate this by inducing senses before clustering, but the repeated local clustering steps become computationally impractical at very large scale."
        },
        {
            "title": "Initial Data Generation",
            "content": "We adopt dual-track approach for initial data generation, combining pre-trained multilingual embeddings with LLM-augmented synonym pair synthesis. Embedding Model Specialization Our primary semantic encoder is initialized from multilinguale5-large (Wang et al., 2024), an XLM-RoBERTa architecture trained on large-scale multilingual entailment and retrieval datasets. We fine-tune this model using contrastive learning on Turkish synonym pairs, employing CachedMultipleNegatives3 RankingLoss with temperature parameter τ = 0.07. The loss function is formalized as: Lcontrastive = log exp(sim(q, p+)/τ ) exp(sim(q, pi)/τ ) (cid:80) (1) where sim denotes cosine similarity between query embeddings and positive/negative examples, with cached negatives drawn from earlier batch iterations to improve sample efficiency. Model hyperparameters include maximum sequence length of 512 tokens, hidden dimension 1024, and L2 normalization of output embeddings. This training produces embeddings specifically optimized for synonym pairs rather than generic semantic similarity. Concept-Term List Expansion We begin with an expert-curated list of 77,000 legal and domainspecific concept-terms assembled by domain specialists. To increase coverage, we extract unique concept-terms from Named Entity Recognition datasets accumulated during prior information extraction work, expanding the list to 110,000 terms. This expanded list serves as the universe for all subsequent operations. LLM-Augmented Dataset Generation We employ Gemini 2.5-Flash to generate semantic relationship labels through clustering-based prompt synthesis. The procedure operates as follows: (1) FastText Turkish embeddings (cc_tr_300) are computed for all 110,000 terms; (2) agglomerative clustering using cosine distance with threshold 0.4 produces 13,000 semantic clusters; (3) each cluster is submitted to Gemini 2.5-Flash with system prompt requesting classification of intracluster relationships as Synonym, Antonym, or Cohyponym. This synthetic generation yields approximately 827,000 labeled pairs at $65 total cost. Dictionary Integration To supplement synthetic generation with high-confidence examples, we incorporate Türkçe Es Anlamlılar Sözlügü (Turkish Synonym Dictionary) containing approximately 20,000 words. Strict quality filtering retains only word entries having at most 2 synonyms per entry, contributing 16,000 high-confidence pairs. The combined dataset totals 843,000 labeled pairs with three-way categorical labels as shown in Table 1. 3.2 Phase 2: Large-Scale Candidate Generation via GPU-Accelerated Vector Search Given 15 million terms and approximately 15 million existing embedding vectors, computing pairwise similarities naively would require O(N 2) comparisons an intractable 225 trillion operations. We employ FAISS (Johnson et al., 2019) to reduce this to sublinear complexity through quantization and hierarchical indexing. Vector Space Compression The 15 million embedding vectors (dimension 1024) require roughly 60 GB in full floating-point precision. To fit them within single-GPU VRAM limits, we apply 8-bit scalar quantization (SQ8), reducing the footprint to about 15 GB (a 4 compression) with minimal loss in accuracy: (cid:22) qi = 127 vi minj vj maxj vj minj vj (cid:23) (2) Hierarchical Index Construction We construct an Inverted File (IVF) index partitioning the search space into 16,384 Voronoi cells via k-means clustering. At query time, only cells near the query vector are searched, reducing computation from O(N ) to approximately O(log ) cluster probes. Similarity Search and Thresholding For each of the 15 million terms, we compute embeddings using the fine-tuned e5-large model and retrieve the top-k=100 neighbors by cosine similarity. We retain only pairs with cosine similarity exceeding 0.70, permissive threshold intentionally chosen to prioritize recall over precision. This generates approximately 1.3 billion candidate pairs for Phase 3."
        },
        {
            "title": "Classification",
            "content": "The 1.3 billion candidate pairs represent diverse semantic relationships beyond synonymy. To discriminate true synonyms from antonyms and cohyponyms, we train multi-class classification models on the 843,000 labeled dataset. Pair Classification Architecture We treat semantic relationship identification as sentence pair classification task. Each input pair undergoes tokenization: [CLS] term1 [SEP] term2 [SEP] 4 The [CLS] tokens final hidden representation is passed through three-way linear classifier predicting classes: Synonym (label 2), Antonym (label 0), Co-hyponym (label 1). Maximum sequence length is set to 64 tokens. Training with Weighted Loss The training architecture is illustrated in Figure 1. We employ weighted CrossEntropy loss to address class imbalance: LCE = 3 (cid:88) c=1 wc yc log(ˆyc) (3) where wc = N/(k nc) assigns higher weights to underrepresented classes. Training uses batch size 128 with BF16 precision on NVIDIA L40S GPU. Standard community detection algorithms fail due to semantic drift and inability to handle polysemy. Two-Stage Soft-to-Hard Architecture We replace standard algorithms with two-stage hybrid approach: Stage 1 (Expansion) allows soft clustering via overlap-based membership; Stage 2 (Pruning/Reduction) resolves polysemy through topological voting. Stage 1: Expansion and Soft Clustering Verified synonym pairs are sorted descending by confidence score, ensuring strongest bonds initialize clusters. These pairs are converted into an adjacency map. Cluster construction proceeds iteratively: term joins cluster if the intersection ratio exceeds threshold: et (SentenceTransformers, al., (NewMind AI, Model Selection We evaluated six transformerbased sentence embedding models, namely 2025), TurkEmbed4STS (Ezerceli 2025), modernbert-base-tr mpnet bertbase-turkish-128k (Bayerische Staatsbibliothek, 2024), turkish-e5-large (Izdas et al., 2025), and multilingual-e5-large (Wang et al., 2024) as shown in Table 2. Based on superior F1-macro performance (0.87) and stable training dynamics, turkish-e5-large was selected for subsequent experiments. 2024),"
        },
        {
            "title": "3.4 Phase 4: Filtering and Conflict Resolution",
            "content": "Confidence and Class Filter We retain pairs satisfying: (1) classification label is Synonym; (2) prediction confidence score exceeds 0.70. Pairs classified as Antonym or Co-hyponym are discarded. Symmetry and Conflict Checking Semantic relationships should be approximately symmetric: if (A, B) is synonym pair, then (B, A) should also be synonymous. We implement explicit symmetry validation by comparing predictions on both orderings. Pairs exhibiting conflict where one direction predicts Synonym but the reverse predicts Antonym are entirely removed. This filtering reduces the 1.3 billion candidates to approximately 520 million verified edges."
        },
        {
            "title": "3.5 Phase 5: Novel Clustering Algorithm for\nSemantic Drift and Polysemy Resolution",
            "content": "The 520 million verified synonym pairs form massive undirected graph with 15 million nodes. 5 synonyms(t) members(C) members(C) > 0.51 (4) This threshold prevents weak transitive chains while allowing legitimate multi-context membership. Critically, term may satisfy this condition for multiple clusters, creating soft membership assignments. Stage 2: Polysemy Resolution via Topological Voting Terms with multiple cluster memberships are resolved through three-level hierarchical mechanism: 1. Majority Rule: For each polysemous term, count shared synonyms with each candidate cluster. The cluster containing the most common synonyms wins: arg maxC synonyms(t) members(C). 2. Specificity Principle: If clusters tie on majority rule, prefer the cluster with smaller cardinality: arg minC C. This prevents absorption into massive catch-all clusters. 3. Determinism by ID: If specificity also ties, assign to the cluster with smallest ID for reproducibility."
        },
        {
            "title": "Representation",
            "content": "Each cluster requires representative parent term serving as the canonical referent. We implement two-stage selection pipeline: Figure 1: Classification model training architecture and weighted loss calculation flow. Word pairs are tokenized and processed through transformer encoder, with class weights computed from inverse frequencies to address label imbalance. Stage 1: Dictionary-Based Guarantee We maintain curated list of 90,000 fundamental concept-terms. If any cluster member appears in this dictionary, that term is immediately designated parent without further computation. Stage 2: Centroid-Based Selection For clusters lacking dictionary members (97% of clusters), we compute the semantic centroid by averaging L2-normalized embeddings of all members. The member with highest cosine similarity to this centroid becomes parent, representing the semantic center of gravity."
        },
        {
            "title": "3.7 Phase 7: Final Output Generation",
            "content": "The output obtained at the end of Phase 7 consists of total of 2,905,071.454 semantic clusters. The median size of these clusters is 3, and the mean cluster size is calculated as 4.58. The largest cluster size observed is 86. All of this structure has been generated in JSON format, which clearly represents the parentchild equivalence relationships."
        },
        {
            "title": "4.1 Dataset",
            "content": "Table 1: Dataset Specifications Component Total Size Data Sources Labels Format Details 843k unique pairs Synthetic (Gemini 2.5-Flash): 827k External Dictionary: 16k Synonym, Antonym, Co-hyponym {\"sentence1\": \"term_A\", \"sentence2\": \"term_B\", \"label\": \"rel\"}"
        },
        {
            "title": "4.2.1 Phase 1: Model Candidate Selection",
            "content": "Six transformer-based models were evaluated under identical conditions  (Table 2)  . All runs used identical hyperparameters on an NVIDIA RTX 3060, with learning rate of 3 105, batch size of 64, and training conducted for Table 2: Candidate models for Phase 1 selection. Model Arch. Params TurkEmbed4STS modernbert-base-tr mpnet bert-base-turkish-128k turkish-e5-large multilingual-e5-large GTE-multilingual-base ModernBERT XLM-RoBERTa BERT XLM-RoBERTa XLM-RoBERTa 305M 135M 278M 184M 560M 560M Table 3: Classification model comparison. Phase 2 training on L40S with larger batch size yields best performance. Model F1-Macro TurkEmbed4STS modernbert-base-tr mpnet bert-base-turkish-128k turkish-e5-large (Phase 1) multilingual-e5-large turkish-e5-large (Phase 2) 0.82 0.79 0.83 0.81 0.87 0.84 0.90 5 epochs using maximum sequence length of 64. Computations were performed in BF16 precision, optimized with Fused AdamW, and cosine learning rate scheduler with warmup ratio of 0.1 was applied throughout training."
        },
        {
            "title": "4.2.3 Evaluation Strategy\nusing\nperformance was monitored\nModel\nThe\nF1-Macro score on the validation set.\nload_best_model_at_end=True\nen-\nsured final checkpoints correspond to peak\nvalidation performance.",
            "content": "strategy"
        },
        {
            "title": "4.3 Comparison of Experiments",
            "content": "The turkish-e5-large model demonstrated superior performance and stability across training runs (Figure 2). The final model achieves an overall 90% F1-Macro score (in Table 3), with 94% F1 on 6 Table 4: Per-class performance of final model. Class Precision Recall Synonym Antonym Co-hyponym Macro Avg. 0.76 0.91 0.93 0.88 0.90 0.93 0.95 0. 0.83 0.92 0.94 0.90 co-hyponym classification, 92% F1 on antonym detection, and 83% F1 on synonym identification as in Table 4 and Appendix B. 4.4 Clustering Results Our synonym graph construction pipeline processed 15 million unique terms, generating approximately 1.3 billion candidate term pairs for verification. After semantic filtering, 520 million pairs were confirmed as valid synonym relationships, resulting in 2,905,071 final semantic clusters. The resulting clusters are compact and well-balanced, with median size of 3, mean size of 4.58, and maximum cluster size of 86, indicating limited semantic drift while maintaining broad lexical coverage."
        },
        {
            "title": "4.5 Qualitative Analysis: Parent Selection",
            "content": "Our parent selection demonstrates robust handling of complex linguistic phenomena: OCR Error Correction Cluster [Mücbir Sebe, Mücbir Sebep Halleri, Mucbir Sebepler, Mücbir Sebep] correctly selects parent Mücbir Sebep (Force Majeure), eliminating truncated OCR output Sebe and diacritic errors Mucbir. Formal Terminology Priority Cluster [VUK, Vergi Usul K., 213 Sayılı Kanun, Vergi Usul Kanunu, Vergi Usul Yasası] correctly identifies Vergi Usul Kanunu (Tax Procedure Code) as parent rather than abbreviation VUK. Institutional Name Standardization Cluster [SSK, Sosyal Sigortalar, SGK, Sosyal Güvenlik Kurumu, Sosyal Güvenlik Teskilatı] correctly selects Sosyal Güvenlik Kurumu (Social Security Institution, current official name)."
        },
        {
            "title": "5.1 Key Findings",
            "content": "Our experiments demonstrate several important findings: LLM-Generated Labels Are Reliable The 90% F1-macro achieved by classifiers trained on Gemini 2.5-Flash labels indicates that LLM-generated semantic relationships exhibit sufficient quality for downstream applications. This validates the costeffective approach of synthetic data generation over manual annotation. Turkish-Specialized Models Outperform Multilingual Baselines The turkish-e5-large model, pre-trained on Turkish text, outperforms the multilingual model multilingual-e5-large despite identical architecture, suggesting that languagespecific pre-training remains valuable even for wellresourced multilingual models. Soft-to-Hard Clustering Resolves Polysemy Our two-stage clustering algorithm successfully handles polysemous terms that would be misclassified by traditional hard-partitioning algorithms. The topological voting mechanism provides principled disambiguation while maintaining computational tractability. Class Distribution and Performance Analysis The dataset exhibits notable imbalance, with the co-hyponym class being dominant in terms of sample count. This class distribution directly accounts for the 94.8% F1-score on co-hyponyms, as the model learns this relation from abundant training examples. Interpreting the synonym classs 83.1% F1-score, however, requires situating it within the central motivation of this study. The primary objective is not simply to distinguish synonyms from antonyms, but to separate synonyms from co-hyponyms. This distinction matters because cohyponyms share hypernyms and often exhibit high semantic similarity, yet they encode fundamentally different relation. As result, their embeddings tend to cluster near synonym pairs, which makes discrimination substantially more challenging than separating synonyms from antonyms that occupy clearly opposing regions of semantic space. The models ability to achieve 90% macro F1score while successfully separating these semantically proximate classes demonstrates that it has learned to model not just semantic similarity, but also the conceptual specificity required to distinguish same meaning (synonymy) from same category (co-hyponymy). This is precisely the capability needed for constructing antonym-free synonym graphs that avoid false merging of related but non-synonymous terms. (a) Training Loss (b) Validation Loss (c) F1-Macro Score Figure 2: Training dynamics of the semantic relation classifier. Subfigures (a) and (b) demonstrate the convergence of cross-entropy loss on training and validation sets, respectively. Subfigure (c) shows the corresponding rise in F1-Macro scores, where the turkish-e5-large variant (green line) demonstrates superior efficiency, reaching peak score of 0.90 significantly faster than competing baselines."
        },
        {
            "title": "6 Conclusion",
            "content": "We present comprehensive system for constructing antonym-free synonym graphs at massive scale, addressing the fundamental challenge that distributional similarity conflates synonymy with other semantic relationships. Our three contributions large-scale disambiguated dataset, specialized discriminator achieving 90% F1-macro, and novel soft-to-hard clustering algorithm together enable the construction of the largest native Turkish semantic resource to date: 2.9 million synonym clusters derived from 15 million terms. The resulting resource enables high-precision semantic search and retrieval-augmented generation for morphologically rich languages where existing synonym databases remain sparse. Our methodology is designed for cross-linguistic transfer, requiring only FastText embeddings, LLM API access, and basic dictionary resources components available for hundreds of languages. Future work includes extending the approach to additional morphologically rich languages, incorporating explicit morphological paradigm expansion, and developing dynamic update mechanisms to maintain temporal validity as terminology evolves."
        },
        {
            "title": "References",
            "content": "Özge Bakay, Begüm Ergelen, Elif Sarmis, Safiye Betül Yıldız, Aslı Gül Özdemir, and Gökhan Ercan. 2021. turkish wordnet from scratch. In Proceedings of the 11th Global Wordnet Conference, pages 7278. Marco Baroni and Alessandro Lenci. 2010. Distributional memory: general framework for corpusbased semantics. Computational Linguistics. Bayerische Staatsbibliothek. 2024. dbmdz/bert-baseturkish-cased. https://huggingface.co/dbmdz/ bert-base-turkish-cased. Francesco Bertolotti and Walter Cazzola. 2024. By 8 tying embeddings you are assuming the distributional hypothesis. Vincent Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfoldJournal ing of communities in large networks. of Statistical Mechanics: Theory and Experiment, 2008(10):P10008. S. Chaurasia and Tyler McDonnell. 2016. Synonymy and antonymy detection in distributional models. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, and 3416 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Preprint, arXiv:2507.06261. Jesse Dunietz, Lori S. Levin, and J. Carbonell. 2013. The effects of lexical resource quality on preference violation detection. Philip Durrant. 2013. Formulaicity in an agglutinating language: the case of turkish. Corpus Linguistics and Linguistic Theory. Razieh Ehsani, Ercan Solak, and Olcay Taner Yildiz. 2018. Constructing lexical database for turkish. In Proceedings of the LREC 2018 Workshop on Language Resources and Technologies for Turkic Languages, pages 2732. Ábel Elekes, Martin Schäler, and Klemens Böhm. 2017. On the various semantics of similarity in word embedding models. 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL). Mathías Etcheverry and Dina Wonsever. 2023. Antonymy-synonymy discrimination through re2023 Inpelling parasiamese neural networks. ternational Joint Conference on Neural Networks (IJCNN). Özay Ezerceli, Gizem Gümüsçekiçci, Tugba Erkoç, and Berke Özenç. 2025. Turkembed: Turkish embedding model on natural language inference and sentence In Proceedings of the 2025 text similarity tasks. Innovations in Intelligent Systems and Applications Conference (ASYU), pages 16. Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. OPAL (Open@LaTrobe) (La Trobe University). Juri Ganitkevitch, Benjamin Van Durme, and Chris Ppdb: The paraphrase Callison-Burch. 2013. database. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 758764. Marcos Garcia. 2021. Exploring the representation of word meanings in context: case study on homonymy and synonymy. ArXiv. Kartik Goyal, S. Jauhar, Huiying Li, Mrinmaya Sachan, Shashank Srivastava, and E. Hovy. 2013. structured distributional semantic model : Integrating structure with semantics. Zellig Harris. 1954. Distributional structure. Word, 10(2-3):146162. Tolga Izdas, Omerhan Sancak, H. Toprak Kesgin, M. Kaan Yuce, and M. Fatih Amasyali. 2025. Turkish-e5: E5 model enhanced for turkish with In 2025 33rd multi-positive contrastive learning. Signal Processing and Communications Applications Conference (SIU). Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. IEEE Billion-scale similarity search with gpus. Transactions on Big Data, 7(3):535547. B. Kırkıcı and H. Clahsen. 2013. Inflection and derivation in native and non-native language processing: Masked priming experiments on turkish. Bilingualism: Language and Cognition. Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. Jiawei Liu, Zhenyu Liu, and Huanhuan Chen. 2017. Revisit word embeddings with semantic lexicons for modeling lexical contrast. Diana McCarthy, B. Keller, and Roberto Navigli. 2010. Getting synonym candidates from raw data in the english lexical substitution task. George Miller. 1995. Wordnet: lexical database for english. Communications of the ACM, 38(11):3941. Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Peter Turney. 2013. Computing lexical contrast. volume 39, pages 555590. Saif M. Mohammad and Graeme Hirst. 2012. Distributional measures as proxies for semantic relatedness. ArXiv. Nikola Mrkšic, Diarmuid Séaghdha, Blaise Thomson, Milica Gašic, Lina Rojas-Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2016. Counter-fitting word vectors to linguistic constraints. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142148. Roberto Navigli and Simone Paolo Ponzetto. 2012. Babelnet: The automatic construction, evaluation and application of wide-coverage multilingual semantic network. Artificial intelligence, 193:217250. 9 NewMind AI. 2025. newmindai/modernbertsenTurkish all-nlion https://huggingface.co/newmindai/ base-tr-uncased-allnli-stsb: tence tr. modernbert-base-tr-uncased-allnli-stsb. transformer fine-tuned Lonneke van der Plas and J. Tiedemann. 2006. Finding synonyms using automatic word alignment and measures of distributional similarity. Sandra Rizkallah, A. Atiya, and S. Shaheen. 2020. polarity capturing sphere for word to vector representation. Applied Sciences. Igor Samenko, Alexey Tikhonov, and Ivan P. Yamshchikov. 2020. Synonyms and antonyms: Embedded conflict. ArXiv. Silke Scheible, Sabine Schulte im Walde, and Sylvia Springorum. 2013. Uncovering distributional differences between synonyms and antonyms in word space model. Nina Schneidermann, Rasmus Hvingelby, and Bolette S. Pedersen. 2020. Towards gold standard for evaluating danish word embeddings. SentenceTransformers. 2024. sentence-transformers/allhttps://huggingface.co/ mpnet-base-v2. sentence-transformers/all-mpnet-base-v2. Vincent Traag, Ludo Waltman, and Nees Jan Van Eck. 2019. From louvain to leiden: guaranteeing well-connected communities. Scientific Reports, 9(1):5233. Dmitry Ustalov, Alexander Panchenko, and Chris Biemann. 2017. Watset: Automatic induction of synsets from graph of synonyms. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 15791590. Sabine Schulte im Walde. 2013. Potential and limits of distributional approaches for semantic relatedness. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual e5 text embeddings: technical report. arXiv preprint arXiv:2402.05672. Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. ArXiv. Wen-tau Yih, G. Zweig, and John C. Platt. 2012. Polarity inducing latent semantic analysis."
        },
        {
            "title": "A Additional Examples",
            "content": "Semantic Drift Prevention Without our filtering, the chain Sıcak (Hot) Acı (Spicy) Acı (Pain) Üzüntü (Sadness) Depresyon (Depression) would create single cluster conflating temperature with mental health. Our confidencebased filtering and intersection ratio thresholds prevent such spurious chaining. Polysemy Handling The term yüz (face/100) is initially soft-assigned to both an anatomical cluster (containing çehre, surat, sima) and numerical cluster (containing yüzer, yüzde). Topological voting assigns it to the anatomical cluster based on higher synonym overlap, while the numerical sense is captured through related terms."
        },
        {
            "title": "B Detailed Training Metrics",
            "content": "(a) Synonym F1 (b) Co-hyponym F1 Figure 3: Per-class F1 scores during training. Cohyponym classification converges quickly to 0.94, while synonym detection shows more variation across models, stabilizing at 0.83 for the best model."
        },
        {
            "title": "C Algorithm Pseudocode",
            "content": "Algorithm 1 Soft-to-Hard Clustering Require: Synonym pairs with confidence scores Ensure: Final hard clusters 1: Sort by confidence descending 2: Initialize cluster set 3: Initialize membership map 4: for each pair (u, v, conf) in do 5: Find candidate clusters for and if both unassigned then Create new cluster = {u, v} {c} else if intersection ratio > 0.51 then Add unassigned term to existing cluster Update membership map end if 12: 13: end for 14: for each term with [t] > 1 do Apply topological voting: 15: 1. Majority rule 2. Specificity principle 3. Determinism tiebreak Assign to winning cluster 19: 20: end for 21: return 6: 7: 8: 9: 10: 11: 16: 17: 18:"
        }
    ],
    "affiliations": []
}