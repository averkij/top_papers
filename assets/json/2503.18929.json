{
    "paper_title": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training",
    "authors": [
        "Brian R. Bartoldson",
        "Siddarth Venkatraman",
        "James Diffenderfer",
        "Moksh Jain",
        "Tal Ben-Nun",
        "Seanie Lee",
        "Minsu Kim",
        "Johan Obando-Ceron",
        "Yoshua Bengio",
        "Bhavya Kailkhura"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, existing on-policy algorithms used for post-training are inherently incompatible with the use of experience replay buffers, which can be populated scalably by distributed off-policy actors to enhance exploration as compute increases. We propose efficiently obtaining this benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a massively scalable LLM RL system. In contrast to existing approaches, TBA uses a larger fraction of compute on search, constantly generating off-policy data for a central replay buffer. A training node simultaneously samples data from this buffer based on reward or recency to update the policy using Trajectory Balance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA offers three key advantages: (1) decoupled training and search, speeding up training wall-clock time by 4x or more; (2) improved diversity through large-scale off-policy sampling; and (3) scalable search for sparse reward settings. On mathematical reasoning, preference-tuning, and automated red-teaming (diverse and representative post-training tasks), TBA produces speed and performance improvements over strong baselines."
        },
        {
            "title": "Start",
            "content": "TRAJECTORY BALANCE WITH ASYNCHRONY: DECOUPLING EXPLORATION AND LEARNING FOR FAST, SCALABLE LLM POST-TRAINING Brian R. Bartoldson1, Siddarth Venkatraman2,3, James Diffenderfer1, Moksh Jain2,3, Tal Ben-Nun1, Seanie Lee4, Minsu Kim2,4, Johan Obando-Ceron2,3, Yoshua Bengio2,3,5 Bhavya Kailkhura1 1Lawrence Livermore National Laboratory, 2Mila Quebec AI Institute 3Universite de Montreal 4KAIST 5CIFAR Fellow {bartoldson,diffenderfer2,kailkhura1}@llnl.gov, {siddarth.venkatraman,moksh.jain}@mila.quebec"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning (RL) is critical component of large language model (LLM) post-training. However, existing on-policy algorithms used for post-training are inherently incompatible with the use of experience replay buffers, which can be populated scalably by distributed off-policy actors to enhance exploration as compute increases. We propose efficiently obtaining this benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), massively scalable LLM RL system. In contrast to existing approaches, TBA uses larger fraction of compute on search, constantly generating off-policy data for central replay buffer. training node simultaneously samples data from this buffer based on reward or recency to update the policy using Trajectory Balance (TB), diversity-seeking RL objective introduced for GFlowNets. TBA offers three key advantages: (1) decoupled training and search, speeding up training wall-clock time by 4x or more; (2) improved diversity through large-scale off-policy sampling; and (3) scalable search for sparse reward settings. On mathematical reasoning, preference-tuning, and automated red-teaming (diverse and representative post-training tasks), TBA produces speed and performance improvements over strong baselines. 5 2 0 2 4 2 ] . [ 1 9 2 9 8 1 . 3 0 5 2 : r Figure 1: TBA performs rapid, scalable exploration of model responses, improving RL efficiency on the GSM8K mathematical reasoning task. All plotted points use 4xA100 GPUs (or comparable L40S GPUs). DPO and RLOO baselines taken from Noukhovitch et al. (2025), PPO and VinePPO baselines taken from Kazemnejad et al. (2024). The baseline model is the SFTed RhoMath-1B (Lin et al., 2024) model, which obtains 40.3% accuracy after SFT and before RL. Appendix has details."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Post-training through reinforcement learning (RL) is critical step in enhancing large language models (LLMs), aligning them with human preferences and improving their reasoning abilities (Christiano et al., 2017). However, widely used RL algorithms such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and REINFORCE Leave-One-Out (RLOO) (Ahmadian et al., 2024) suffer from fundamental limitation: they are on-policy, meaning that data generation and policy updates occur sequentially. This dependence creates bottlenecks that reduce resource utilization. Moreover, the benefit of scaling on-policy data generation may be limited (Hou et al., 2024). We introduce Trajectory Balance with Asynchrony (TBA), distributed RL framework designed to efficiently and scalably leverage compute for LLM post-training. TBA decouples data generation from policy updates using an off-policy training objective based on Trajectory Balance (TB) (Malkin et al., 2022a). The framework uses multiple searcher nodes that independently generate diverse trajectories and store them in central replay buffer, while single trainer node asynchronously samples from this buffer to update the policy. By removing the dependency between data generation and training, TBA ensures high resource utilization and facilitates scalable search. By computing model updates with large, efficiently-generated off-policy response sets, the TB objective leverages response scaling to produce better models than popular RL baseline methods. TBA offers three key advantages over existing RL-based LLM post-training approaches: 1) Asynchrony enables massive parallelization, significantly reducing training wall-clock time as shown in Figures 1 and 3. 2) Diverse off-policy sampling from replay buffer improves exploration and prevents mode collapse. 3) Scalable search capabilities make TBA particularly effective in sparse reward settings, such as automated red-teaming. We validate TBA on mathematical reasoning, preference-tuning, and automated red-teaming. Our results show that TBA achieves performance comparable to or better than existing methods while significantly improving training speed. Our key contributions are summarized as follows: We introduce TBA, novel distributed RL framework for LLM post-training. We decouple data generation and policy updates, improving training speed and scalability. We demonstrate the effectiveness of trajectory balance for LLM post-training, exploiting its ability to efficiently leverage large-scale off-policy data. We demonstrate significant speedups (4x or more) in RL for mathematical reasoning, preference-tuning, and automated red-teaming. By enabling high-quality and fast off-policy post-training, TBA contributes to the broader goal of scalable and effective LLM alignment, ensuring that large models can be refined more efficiently for real-world deployment."
        },
        {
            "title": "2 RELATED WORK",
            "content": "RL fine-tuning of language models Reinforcement Learning (RL) has been an integral component for training LLMs (Google Gemini Team, 2024; OpenAI, 2023). In particular, RL has became the de facto approach for aligning language models with human preferences Christiano et al. (2017); Ziegler et al. (2019); Stiennon et al. (2020); Ouyang et al. (2022). Much of this work relies on Proximal Policy Optimization (PPO) (Schulman et al., 2017), an on-policy RL algorithm which has become default choice for fine-tuning LLMs due to its strong performance across different setups. Aside from PPO, other on-policy objectives such as REINFORCE (Ahmadian et al., 2024) and variants like GRPO (Shao et al., 2024) and VinePPO (Kazemnejad et al., 2024) have also been studied in the context of language models. An alternative to PPO-based fine-tuning is rejection sampling fine-tuning, inspired by the best-of-n inference approach proposed by Nakano et al. (2021). Recent work by Dong et al. (2023); Gulcehre et al. (2023), and Wang et al. (2024) extends this concept by generating candidate responses for each prompt, ranking them with learned reward function, and fine-tuning the model based on the highest-ranking responses. On the other hand, direct preference learning approaches (Rafailov et al., 2023; Azar et al., 2024; Tang et al., 2024) skip reward modeling entirely and train language models to"
        },
        {
            "title": "Preprint",
            "content": "directly optimize responses under preference model. Finally, Hu et al. (2024) introduced GFlowNet fine-tuning, leveraging off-policy GFlowNet algorithms for fine-tuning language models, which we build upon in this work. Asynchronous distributed RL Distributed RL spreads actors/searchers, learners, and environments across collection of computing resources. Asynchronous distributed RL does this such that searcher and trainer processes do not necessarily share the same weights, which can significantly improve training speed (Nair et al., 2015; Wang et al., 2025) and facilitate RL in complex, high-dimensional domains (Hessel et al., 2021; Huang et al., 2023; Horgan et al., 2018). foundational method in this area is Asynchronous Advantage Actor-Critic (A3C) (Mnih, 2016). In A3C, multiple parallel workers asynchronously interact with the environment and communicate gradients to central node. Our approach to async distributed RL more closely resembles the Importance-Weighted Actor-Learner Architecture (IMPALA) method (Espeholt et al., 2018), which communicates experience trajectories (state, action, and reward tuples) to the central node. Automated red-teaming Through adversarial interactions, LLM red-teaming clarifies the robustness and risks of target LLM. Automating the generation of these adversarial scenarios, automated red-teaming frameworks can help uncover vulnerabilities, biases, and unintended behaviors in models more quickly, enabling preemptive mitigation before deployment (Wei et al., 2023; Ji et al., 2024). Perez et al. (2022) proposed training language models using RL to discover prompts that elicit harmful responses from some target LLM. Standard RL approaches, however, are susceptible to mode collapse and fail to achieve the attack diversity that is critical for successful red-teaming. Hong et al. (2024) introduced curiosity bonus to encourage generation of diverse red-teaming prompts, whereas Samvelyan et al. (2024) proposed sampling an attack prompt from pool and iteratively mutating the prompt with auxiliary LLMs. Lee et al. (2025) proposed using GFlowNet fine-tuning followed by MLE smoothing to generate diverse, transferable, and effective prompts. Our red-teaming experiments augment their TB objective optimization with our distributed asynchronous framework."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "KL regularized RL as probabilistic inference We study the problem of fine-tuning pretrained language model πref with reward model rϕ. For mathematical reasoning (Cobbe et al., 2021) our reward model simply computes responses correctness, while our preference-tuning for alignment (Ziegler et al., 2019) and red teaming (Perez et al., 2022) experiments use reward models optimized with human preference data. Notably, reward maximizing RL with learned reward functions is susceptible to spurious modes of the reward (Skalse et al., 2022; Pan et al., 2022; Gao et al., 2023), resulting in poor performance and low diversity in responses. This is addressed by constraining the fine-tuned model to be close to the initial model in terms of the KL divergence: π = arg max π ExD[Eyπ(yx)[rϕ(y; x)] βDKL(π(. x)πref(. x))]. (1) Online RL algorithms such as PPO (Schulman et al., 2017; Stiennon et al., 2020) and REINFORCE (Williams, 1992; Kool et al., 2019; Ahmadian et al., 2024) can be used to optimize the model (which is the policy), whereas offline objectives such as DPO (Rafailov et al., 2023) can be used to train the model directly on the preference data and achieve the same optimal policy asymptotically. Eq. 1 can be interpreted from probabilistic perspective as Bayesian posterior inference problem (Korbak et al., 2022). The optimal policy for Eq. 1 is given as: π(y x) πref(y x) exp(β1rϕ(y; x)). (2) Approaches such as Gibbs sampling can be used to produce samples from the optimal policy without any fine-tuning (Xiong et al., 2024). These MCMC approaches can be very expensive at inference time and intractable for long sequences. Within the probabilistic interpretation, on-policy RL to maximize Eq. 1 is equivalent to amortized variational inference to minimize the reverse KL with"
        },
        {
            "title": "Preprint",
            "content": "respect to the posterior density (Korbak et al., 2022). However, reverse KL optimization is susceptible to mode collapse and requires on-policy samples. The reliance on on-policy samples can limit the scalabality as we cannot use replay buffers that can be populated in parallel at scale, since new updates of the policy invalidate the on-policy nature of the older replay buffer examples. In practice this means that the policy can get stuck in suboptimal solutions and stop exploring, or may obtain high reward at the cost of diversity. This motivates us to consider an alternative off-policy amortized variational inference to efficiently leverage scalable computational resources for flexible exploration. GFlowNets Generative Flow Networks (GFlowNets; Bengio et al., 2021; 2023) are framework for off-policy training of hierarchical generative models to sample proportional to given unnormalized density (reward) function. GFlowNets frame probabilistic inference as sequential decision-making problem, learning policy to construct the objects (e.g. sequences) by putting together building blocks (e.g. tokens) and optimizing consistency-based objectives. GFlowNet objectives have been used for fine-tuning autoregressive (Hu et al., 2024; Lee et al., 2025) as well as discrete diffusion language models (Venkatraman et al., 2024). To fine-tune language model to sample from Eq. 2, we can set as reward R(y; x) = πref(y x) exp(β1rϕ(y; x)). Following Lee et al. (2025), we use the trajectory balance objective (Malkin et al., 2022b) for training the language model policy πθ, which is defined over response as LTB(y, x; θ) = log (cid:18) Z(x)πθ(y x) R(y; x) (cid:19)2 . (3) Z(x) is positive scalar function of the query x, and the response is sequence of tokens. When LTB is minimized, Z(x) is the partition function of the posterior (i.e. Z(x) = (cid:80) R(y; x)). Instead of training value network for Z, we use the VarGrad variant of trajectory balance which replaces learned with batch estimate (Richter et al., 2020; Nusken & Richter, 2021; Zhang et al., 2023; Sendera et al., 2024; Venkatraman et al., 2024). Given responses {y(i,j)}K j=1 for query x(i), batch estimate of can be computed as follows log ˆZ(x(i)) = 1 (cid:88) (cid:18) (cid:88) j= t=1 log πref(y(i,j) y(i,j) <t , x(i)) log πθ(y(i,j) y(i,j) <t , x(i)) + rϕ(y(i,j); x(i)) (cid:19) . 1 β The estimate ˆZ can be plugged into Eq. 3 for batch = {(x(i), y(i,j))K j=1}B i=1 to get LVarGrad TB (B; θ) = 1 BK i=B,j=K (cid:88) (cid:32) i=1,j=1 log ˆZ(x(i)) + (cid:88) t= log πθ(y(i,j) y(i,j) <t , x(i)) log πref(y(i,j) y(i,j) <t , x(i)) (cid:33)2 r(y(i,j); x(i)) . 1 β (4) (5) An important property of the trajectory balance is that it is off-policy. During training, can be sampled from any distribution with full support (Bengio et al., 2021). This enables the use of various exploration strategies (Rector-Brooks et al., 2023; Kim et al., 2024) as well as the use of replay buffers (Shen et al., 2023; Vemgal et al., 2023). In the context of fine-tuning language models, this off-policy nature of the objective makes it natural choice for large-scale distributed training."
        },
        {
            "title": "4 TBA: FAST, SCALABLE LLM POST-TRAINING",
            "content": "TBA is an asynchronous distributed RL framework for post-training language models (see Figure 2 for visualization). TBA uses the off-policy trajectory balance objective (Equation 5) to efficiently leverage scaled data generation, and it decouples data generation from model updates to ensure high resource utilization. TBA has two key components: single TRAINER node and one or more SEARCHER nodes that collect off-policy data into shared replay buffer Dglobal."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Fast, scalable LLM post-training with TBA. Continuously (solid lines), multiple SEARCHER nodes (left) collect trajectories, while TRAINER node (right) samples from replay buffer to train the policy off-policy. Periodically (dashed lines), updated policy weights are sent to SEARCHER nodes, and new trajectories are added to the TRAINER nodes buffer. This avoids bottlenecks at any given node, which can be 1 or more GPUs, keeping resource utilization high. TBA can be implemented by, e.g., modifying the RLOO trainer class of the Hugging Face TRL library (von Werra et al., 2020) to use the TB objective, creating dictionary to hold trajectories, and assigning training and search to distinct nodes. Here, node is computational resource sufficient to perform all the operations needed for training or search it can be collection of multiple GPUs or even subset of 1 GPU. In our experiments, node is always 1 GPU; e.g., given 16 GPUs, we would have 15 SEARCHER nodes and 1 TRAINER node operating. Separation of the SEARCHER and TRAINER is highly desirable even in 2 node (e.g. 2 GPU) cluster because LLM policy rollouts are costly sequential decoding procedures, and training requires only parallel likelihood evaluation of an entire sequence through single forward pass. Thus, given an objective that can tolerate off-policy asynchronous online training, massive wall clock time speedups can be realized by continuously running training on 1 node without pausing for rollout generation. 4.1 SCALING DATA COLLECTION WITH SEARCHER NODES In TBA, the SEARCHER nodes each carry local delayed copy πθ of the TRAINER policy πθ. To produce policy rollouts, queries are sampled from dataset, and the local policy generates batch of responses πθ(yx) that are evaluated with the reward model rϕ(y; x). Like Noukhovitch et al. (2025), we use vLLM (Kwon et al., 2023) for faster generation. The (x, y, rϕ(y; x)) tuples are stashed in the SEARCHERs local replay buffer Dlocal (note that can instead be stored as dictionary key to save space). We also add to the stored tuple the step of the trainer when syncing last occurred, giving us notion of how off-policy the generated data is this can later be used by the TRAINER to prioritize sampling from more recent generations for relatively on-policy updates. Periodically, search and training pause to pull each SEARCHER nodes local replay buffer Dlocal into the global replay buffer Dglobal, and to update the searchers local policy with the trainers version. The global buffer maintains list of all generated responses and rewards for each query. key motivation for scaling up data generation through large number of SEARCHER nodes is improving exploration. For example, we generate > samples for given query, even when only updating the model using samples per query this can mitigate the lack of diversity caused by the fact that independent model rollouts are not guaranteed to produce unique sequences (when duplicates are encountered, we keep the most recently generated version). Relatedly, future work could apply simple off-policy inference techniques in the SEARCHER nodes such as randomly sampling the softmax temperature, or using alternative decoding techniques like beam search. We expect such approaches to particularly aid solution discovery in sparse reward settings."
        },
        {
            "title": "4.2 ASYNCHRONOUS UPDATES WITH TRAINER",
            "content": "The TRAINER uses off-policy trajectory balance (Eq. 5) to train the policy on the global replay buffer Dglobal. We sample batch of queries, each with responses and corresponding rewards: {x(i), y(i,j), rϕ(y(i,j); x(i))}i=B,j=K i=1,j=1 Dglobal. We then compute the loss in Eq. 5 and use it to update the policy. We sample with replacement if fewer than unique samples exist for given query. critical design choice is the strategy for sampling from the replay buffer Dglobal. The most naive approach is uniform sampling over queries, then uniform sampling over samples associated with the selected query, which may not be optimal if high-reward samples are sparse. Reward prioritization can address this by tilting the sampling distribution toward high-reward sequences; however, focusing solely on high-reward data can lead to mode collapse and reduce policy diversity. To balance between these concerns, we alternate between two sampling strategies: one prioritizing recency i.e., whether the trajectory was added to the buffer in the most recent sync step and another prioritizing rewards. When prioritizing rewards, we consider both softmax of the reward value (to encourage sampling high reward responses) and uniform distribution (to encourage sampling high and low reward responses equally). We randomly switch between prioritizing rewards and recency for each query in batch, with the fraction of queries allocated to each strategy treated as tunable hyperparameter m, which we study in Section 5.4."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate the effectiveness of TBA in three common LLM post-training RL pipelines. Posttraining RL for enhancing LLM capabilitiesparticularly for agentic and reasoning tasksis critical but nascent area where baseline approaches require many hours or days. These conventional methods often rely on costly-to-generate on-policy data and thus inefficiently leverage available computing resources. Notably, this inefficiency can become particularly harmful when scaling to larger distributed systems, which may be necessity for domains with sparse rewards that demand increased sampling. Broadly, we find TBA is highly-performant, fast, and scalable solution. 5.1 TASKS Mathematical reasoning (MR): We study the GSM8K task which consists of grade-school level math problems and binary reward based on exact match for the correct final answer (Cobbe et al., 2021). We adopt the setup from Kazemnejad et al. (2024); Noukhovitch et al. (2025), using an SFTed RhoMath-1B (Lin et al., 2024) model as base for RL post-training. Preference fine-tuning (PFT): We consider the task of fine-tuning language model with reward function learned from human preference data. Specifically, we study the TL;DR summarization task where the goal is to write short summaries for reddit posts (Stiennon et al., 2020). Following Noukhovitch et al. (2025), we consider Pythia (Biderman et al., 2023) as the base model for the policy and the reward models, using the SFTed versions used by Noukhovitch et al. (2025). Red-teaming (RT): We investigate automated red-teaming, another critical step for LLM posttraining. The goal is to discover prompts that elicit harmful responses from target model, as measured by toxicity classifier. We follow the setup from Lee et al. (2025), applying the same models: our smaller-scale experiments use GPT-2 (Radford et al., 2019) as an attacker model, GPT-2 (instruction-tuned) as victim model, and RoBERTa-based toxicity classifier (Vidgen et al., 2021); our larger-scale experiments use Llama-3.2-1B (Meta AI Llama Team, 2024) as an attacker model, Llama-3.1-8B-Instruct as victim model, and LlamaGuard-3-8B (Meta AI Llama Team, 2024) for measuring toxicity, averaging over multiple responses from the victim model. For all tasks, we study solely the RL post-training components of baselines workflows. In particular, we start with SFTed checkpoints, then perform RL. Notably, Lee et al. (2025) also include maximum likelihood estimation training phase after RL that uses the buffer produced during RL, which boosts performance but is not investigated here. Using these baselines codes, we follow their setup and hyperparameters, with deviations noted in Appendix A. For MR and PFT, we implement TBA by"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: TBA scales search and improves RL efficiency on the TL;DR summarization task. All plotted points use 4xA100 GPUs, but TBA allocates 3 GPUs to search, and Online DPO allocates 1 GPU to search. TBA produces large-scale off-policy data that its trajectory balance objective can leverage, creating massive efficiency benefits. Online DPO baselines taken from Noukhovitch et al. (2025). Dashed and solid lines use 256 and 425 updates, respectively. Appendix has details. augmenting the RLOO trainer of Noukhovitch et al. (2025) with our distributed asynchronous RL framework and the TB objective (Equation 5). For RT, we implement TBA by augmenting the trajectory balance trainer of Lee et al. (2025) with our distributed asynchronous RL framework. 5.2 METRICS AND BASELINES MR Metrics. We follow the evaluation setup of Noukhovitch et al. (2025), computing the pass@1 on the GSM8K (Cobbe et al., 2021) test dataset with greedy decoding. MR Baselines. Prior work (Kazemnejad et al., 2024; Noukhovitch et al., 2025) applies RL posttraining with the GSM8K training set to the SFTed RhoMath-1B (Lin et al., 2024) model, which initially obtains 40.3% accuracy on the test set. We compare TBA post-training to the methods used in these prior works: VinePPO (Kazemnejad et al., 2024), Online-DPO (Guo et al., 2024), PPO (Schulman et al., 2017), and RLOO (Ahmadian et al., 2024). PFT Metrics. We follow the evaluation setup of Noukhovitch et al. (2025), using the win-rate under 6.7B gold reward model (Huang et al., 2024) as the primary metric. We additionally report approximate KL distanceapproximated by perplexity to match the evaluation of Noukhovitch et al. (2025)between the learned policy and the reference policy. PFT Baselines. Following Noukhovitch et al. (2025), we compare TBA with: Online-DPO (Guo et al., 2024), PPO (Schulman et al., 2017), and RLOO (Ahmadian et al., 2024). RT Metrics. We follow Lee et al. (2025), measuring the attack success rate on 1024 sampled prompts for victim model. We also measure the diversity of these test-time generated prompts by computing the average pairwise cosine distance. RT Baselines. We compare against: SFT, PPO (Schulman et al., 2017) (with the novelty reward from Lee et al. (2025)), REINFORCE (Williams, 1992; Sutton et al., 1999), RLOO (Ahmadi & Mahmudi, 2023), Online DPO (Rafailov et al., 2023), GFlowNet (one-actor TB) (Lee et al., 2025). 5.3 TBA REDEFINES EFFICIENCY-PERFORMANCE PARETO FRONTIERS We hypothesize that, by mixing asynchronous RL with the trajectory balance objective for learning from off-policy data, TBA can (a) reduce resource waste and training time and (b) improve performance via scaled generation and ingestion of diverse responses. To test this, we primarily consider"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: TBA defines new KL vs. win-rate Pareto frontier for the TL;DR summarization task. The baseline Online DPO frontier is created by increasing the degree of off-policyness, starting from on-policy Online DPO, results from (Noukhovitch et al., 2025). The TBA frontier is created by altering the training steps, searcher count, and KL annealing schedule as described in Appendix A. Table 1: TBA Pareto dominates baseline on-policy algorithms in the PFT task (Pythia 410M). Baseline results taken from Noukhovitch et al. (2025). Please see Appendix for details. Method Perplexity/KL Win Rate Online DPO PPO RLOO TBA (Ours) 1.13 1.14 1.13 1. 0.85 0.86 0.82 0.86 compute-matched settings where all methods have access to the same number of GPUs, though we also evaluate TBA with resource scaling. Even in our compute-matched experiments, TBA generates responses relatively rapidly by running asynchronous search on all but one of the available GPUs. Training happens quickly with TBA because it isnt bottlenecked by on-policy generation, and TBAs rapid response generation ensures training on diverse and previously unseen (though off-policy) data. When tested on established RL problems, an alternative possibility is that TBA will underperform due to its departures from conventional approaches: it is asynchronous, off-policy, reliant on the less-common trajectory balance objective, and makes updates using many responses per query.1 Indeed, Noukhovitch et al. (2025) contemporaneously suggests potential limitations to asynchronous, off-policy RL for LLMs, finding that increasing off-policyness can harm performance metrics like win-rate or exacerbate policy deviations from the reference model. Further, Hou et al. (2024) found limited benefits to scaling response generation from 4 to 8 (or 16) responses when using PPO. We study this question by computing Pareto frontiers for MR (Figure 1), for PFT (Figures 3 and 4 and Table 1), and for RT (Figure 5 and Table 2). See Appendix for experimental details. Notably, TBA produces results on or beyond the Pareto frontiers of all three tasks at multiple model scales, consistent with our hypothesis that TBA can efficiently train LLMs on off-policy data. Speed is vastly improved with TBA training, which proceeds entirely asynchronously without trainingbound or generation-bound processes the only non-training time occurs briefly every steps. In the compute-matched MR experiments (Figure 1), TBA speeds up the training of the only method with comparable performance (VinePPO) by nearly 50, while improving accuracy by 1.8% and speed by 1.5 relative to the speed-optimized asynchronous DPO baseline (Noukhovitch et al., 2025). In the compute-matched PFT experiments (Figure 3), TBA produces 5 speedups over speedoptimized asynchronous DPO baselines (Noukhovitch et al., 2025). In the non-compute-matched automated red-teaming experiments  (Table 2)  , TBA gives 7 speedups for GPT-2 and Llama 3.2 1B compared to the non-distributed, synchronous GFlowNet baseline (Lee et al., 2025). these results 1We compute the TBA loss with more samples per query than analogous approaches like RLOO (e.g., 20 samples versus 4) to reduce the variance of the gradient of the TB objective estimate (Equation 5) that we optimize. We use fewer queries per batch to keep the batch size small despite this response scaling."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: TBA reaches the RT diversity-toxicity Pareto frontier and improves as search is scaled. (Left) On the GPT-2 automated red-teaming task of Lee et al. (2025), TBA produces results on the diversity vs. toxicity Pareto frontier in less training time. Baselines taken from Lee et al. (2025). (Right) Each searcher uses one V100 GPU for generating attacks. We report means and standard errors from multiple runs of the automated red-teaming task with GPT-2 at each searcher/GPU count. suggest that TBA is an effective, parallel, and scalable search framework for distributed learning, offering substantial speed-ups while remaining competitive with leading approaches. 5.4 DOES OFF-POLICYNESS HURT PERFORMANCE? The results in the prior section are perhaps surprising given evidence of off-policynesss harmfulness to RL post-training (Noukhovitch et al., 2025). Thus, we investigate the effect of off-policyness with TBA by studying its related hyperparameters, which we first review here. The fraction controls how often sampling attempts to approximate an on-policy distribution. When = 1, training occurs exclusively with samples added in the most recent sync step, while = 0 corresponds to selecting data without regard for how recently it was generated (e.g., using reward weighting). Additionally, recall that parameters and buffer data are shared between searchers and trainers every training steps: since TBA runs exploration and training in parallel, TBA trains off-policy even when = 1 and = 1. Specifically, with probability m, TBA selects sample from the central replay buffer that is at most 2k 1 updates off-policy. With probability 1 m, TBA samples data produced by the model at any point, which can be as off-policy as the number of training steps minus 1. To understand the effect of increasing off-policyness on TBA, we test the effect of modifying for the MR (see Figure 6) and PFT tasks. For PFT, we train Pythia-410M on the TL;DR dataset with three values of (0.4, 0.5, 0.6) keeping all other parameters constant. We found that win rate fluctuated, with = 0.4 corresponding to the lowest win rate (0.67), and = 0.5 and = 0.6 attaining higher win rates of 0.82 and 0.8, respectively. These results show that higher values of generally lead to higher win rate, reinforcing the idea that on-policy updates are in fact the most effective. However, for reasonably high values of m, incorporating more off-policy data does not significantly degrade performance and, in some cases, may even provides benefits. Regardless, our findings for both MR and PFT further support the idea that we can perform massively distributed training that works well with off-policy updates, as long as recent samples are thrown into the mix. Importantly, the choice of reinforcement learning (RL) algorithm is crucial in determining how effectively we can leverage off-policy updates. TB is fully off-policy compatible algorithm, and as shown in Figure 4, it significantly outperforms asynchronous Online DPO, even in the latters most on-policy setting. Noukhovitch et al. (2025) identified Online DPO as the best-performing off-policy algorithm in their experiments, making it particularly insightful that TB improves upon this. 5.5 DISCOVERY OF HIGH-REWARD SAMPLES VIA SCALING SEARCH Beyond improvements at given level of compute, we also observe improvements when we scale the amount of total compute, showing TBAs promise for large-scale distributed RL. In our asynchronous setup, we find that adding more searchers consistently improves the attack success rate and"
        },
        {
            "title": "Preprint",
            "content": "Table 2: TBA speeds up the wall-clock time required to reach the Pareto frontier for the redteaming task. The GFlowNet performances are taken from Lee et al. (2025), while the training speeds are computed by us with their code. With the GPT-2 models, TBA performance improves with searcher count. With the Llama models, we trade attack toxicity for attack diversity by scaling the TBA buffers maximum size from 130,000 to 150,000 samples*, retaining more off-policy data. Attacker / Victim Model Training Method Hardware Time (h) Speedup Cosine % Toxic Distance Prompts GPT-2 / GPT-2 + SFT Llama 3.2 1B / Llama 3.1 8B - Instruct GFlowNet - Sync TBA (Ours) TBA (Ours) TBA (Ours) GFlowNet - Sync TBA (Ours) TBA (Ours)* 1V100 4V100 16V100 64V100 1A100 8A100 8A100 11.9 1.7 2.5 2.9 37.4 5.7 5.7 1x 7x 4.8x 4.1x 1x 6.6x 6.6x 0.65 0.42 0.45 0.49 0.32 0.35 0.37 96.6 94.5 96 97.6 100.0 98.1 94.8 diversity for RT (see Figure 5, right). This improvement likely stems from having more searchers exploring different regions of the solution space simultaneously, enabling more effective discovery of high-reward samples. Moreover, asynchronous updates introduce opportunities for beneficial randomness, and thus potential expansion of the search coverage in the combinatorial space of language. Interestingly, we also find some evidence for scalings helpfulness in PFT (see Figure 7)."
        },
        {
            "title": "6 DISCUSSION",
            "content": "In this work, we introduced TBA, novel post-training method for large language models (LLMs) that combined an off-policy reinforcement learning (RL) objective with distributed asynchronous search. By decoupling searcher and trainer nodes, our approach enabled efficient distributed training and avoided bottlenecks, leading to significant performance gains in post-training tasks such as mathematical reasoning, automated red-teaming, and RLHF. We expect that our highly parallelizable and performant framework for RL post-training can be extended to other valuable tasks, including self-improvement training (Zelikman et al., 2022; Hu et al., 2024; Gulcehre et al., 2023), search-based reasoning (Wan et al., 2024), as well as the emerging paradigm of training LLMs to reason with RL (Guo et al., 2025). Towards multi-agent search system Our approach can potentially be extended to multi-agent search system aimed at identifying multiple diverse modes within the large combinatorial space of language. Our current searchers do not explicitly target different regions, but we do show that multiple distributed searchers can enhance exploration and populate global replay buffer with varied experiences. By evolving this concept into an explicit multi-agent system, we can encourage each agent to explore distinct regions in divide-and-conquer manner. This cooperative strategy allows agents to effectively identify and report local modes to centralized replay buffer. The off-policy post-training agent can then learn from the diverse samples stored in this centralized buffer, benefiting from wider range of experiences. We consider this promising future direction. Improving local credit assignment The trajectory balance objective can suffer from high gradient variance as it operates on the trajectory level. We addressed this by sampling more responses per query. Future work can leverage learning partial energy functions (Madan et al., 2023; Zhang et al., 2025) to balance bias and variance during policy updates."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "The authors acknowledge funding from CIFAR, NSERC, IVADO, and Samsung. Prepared by LLNL under Contract DE-AC52-07NA27344 and supported by the LLNL-LDRD Program under Project No. 24-ERD-058 (LLNL-CONF-2003261). This manuscript has been authored by Lawrence Livermore National Security, LLC under Contract No. DE-AC52-07NA27344 with the U.S. Department of Energy. The United States Government retains, and the publisher, by accepting the article for publication, acknowledges that the United States Government retains non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. This research used resources of the National Energy Research Scientific Computing Center (NERSC), Department of Energy Office of Science User Facility using NERSC awards ASCR-ERCAP0032802 and ASCR-ERCAP0032812."
        },
        {
            "title": "REFERENCES",
            "content": "Sina Ahmadi and Aso Mahmudi. Revisiting and amending Central Kurdish data on UniMorph In Proceedings of the 20th SIGMORPHON workshop on Computational Research in 4.0. Phonetics, Phonology, and Morphology, pp. 3848, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.sigmorphon-1.5. URL https: //aclanthology.org/2023.sigmorphon-1.5. Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2024. Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS), 2021. Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. GFlowNet foundations. Journal of Machine Learning Research, 24(210):155, 2023. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning (ICML), 2023. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. @articlechristiano2017deep, title=Deep reinforcement learning from human preferences, author=Christiano, Paul and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario, journal=Advances in neural information processing systems, volume=30, year=2017 . Advances in neural information processing systems, 30, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance"
        },
        {
            "title": "Preprint",
            "content": "weighted actor-learner architectures. In International Conference on Machine Learning (ICML), 2018. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning (ICML), 2023. Google Gemini Team. Gemini: family of highly capable multimodal models. 2024. URL https://arxiv.org/abs/2312.11805. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online AI feedback. arXiv preprint arXiv:2402.04792, 2024. Matteo Hessel, Manuel Kroiss, Aidan Clark, Iurii Kemaev, John Quan, Thomas Keck, Fabio Viola, and Hado van Hasselt. Podracer architectures for scalable reinforcement learning. arXiv preprint arXiv:2104.06272, 2021. Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James R. Glass, Akash Srivastava, and Pulkit Agrawal. Curiosity-driven red-teaming for large language models. In International Conference on Learning Representations (ICLR), 2024. Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018. Zhenyu Hou, Pengfan Du, Yilin Niu, Zhengxiao Du, Aohan Zeng, Xiao Liu, Minlie Huang, Hongning Wang, Jie Tang, and Yuxiao Dong. Does rlhf scale? exploring the impacts from data, model, and method. arXiv preprint arXiv:2412.06000, 2024. Edward Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In International Conference on Learning Representations (ICLR), 2024. Shengyi Huang, Jiayi Weng, Rujikorn Charakorn, Min Lin, Zhongwen Xu, and Santiago Ontanon. Cleanba: reproducible and efficient distributed reinforcement learning platform. In International Conference on Learning Representations (ICLR), 2023. Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul, Weixun Wang, and Lewis Tunstall. The n+ implementation details of rlhf with ppo: case study on tl; dr summarization. arXiv preprint arXiv:2403.17031, 2024. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via human-preference dataset. Neural Information Processing Systems (NeurIPS), 2024. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment, 2024. URL https://arxiv.org/abs/2410.01679. Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search GFlowNets. International Conference on Learning Representations (ICLR), 2024. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 REINFORCE samples, get baseline for free!, 2019. URL https://openreview.net/forum?id=r1lgTGL5DE."
        },
        {
            "title": "Preprint",
            "content": "Tomasz Korbak, Ethan Perez, and Christopher Buckley. Rl with kl penalties is better viewed as bayesian inference. arXiv preprint arXiv:2205.11275, 2022. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. International Conference on Learning Representations (ICLR), 2025. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability, 2023. URL https://arxiv.org/abs/2209.12782. Dan Malkin, Tomasz Limisiewicz, and Gabriel Stanovsky. balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 49034915, Seattle, United States, July 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.361. URL https://aclanthology.org/ 2022.naacl-main.361. Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in GFlowNets. Neural Information Processing Systems (NeurIPS), 2022b. Meta AI Llama Team. The llama 3 herd of models. August 2024. URL https://arxiv.org/ abs/2407.21783. Volodymyr Mnih. Asynchronous methods for deep reinforcement learning. arXiv preprint arXiv:1602.01783, 2016. Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, et al. Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296, 2015. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback, 2021. URL https://arxiv. org/abs/2112.09332, 2021. Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, and Aaron Courville. Asynchronous rlhf: Faster and more efficient off-policy rl for language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025. OpenReview.net, 2025. Nikolas Nusken and Lorenz Richter. Solving high-dimensional HamiltonJacobiBellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space. Partial Differential Equations and Applications, 2(4):48, 2021. OpenAI. Gpt-4 technical report, 2023. URL https://openai.com/research/gpt-4. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155."
        },
        {
            "title": "Preprint",
            "content": "Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. International Conference on Learning Representations (ICLR), 2022. Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 34193448, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.225. URL https://aclanthology.org/ 2022.emnlp-main.225. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Neural Information Processing Systems (NeurIPS), 2023. Jarrid Rector-Brooks, Kanika Madan, Moksh Jain, Maksym Korablyov, Cheng-Hao Liu, Sarath Chandar, Nikolay Malkin, and Yoshua Bengio. Thompson sampling for improved exploration in gflownets. arXiv preprint arXiv:2306.17693, 2023. Lorenz Richter, Ayman Boustati, Nikolas Nusken, Francisco J. R. Ruiz, and Omer Deniz Akyildiz. Vargrad: low-variance gradient estimator for variational inference. Neural Information Processing Systems (NeruIPS), 2020. Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktaschel, and Roberta Raileanu. Rainbow teaming: Open-ended generation of diverse adversarial prompts. arXiv preprint arXiv:2402.16822, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Marcin Sendera, Minsu Kim, Sarthak Mittal, Pablo Lemos, Luca Scimeca, Jarrid Rector-Brooks, Alexandre Adam, Yoshua Bengio, and Nikolay Malkin. Improved off-policy training of diffusion samplers. Neural Information Processing Systems (NeurIPS), 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Max Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving GFlowNet training. International Conference on Machine Learning (ICML), 2023. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Neural Information Processing Systems (NeurIPS), 2022. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Neural Information Processing Systems (NeurIPS), 2020. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Remi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Avila Pires, and Bilal Piot. Generalized preference optimization: unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024. Nikhil Vemgal, Elaine Lau, and Doina Precup. An empirical study of the effectiveness of using replay buffer on mode discovery in gflownets. arXiv preprint arXiv:2307.07674, 2023."
        },
        {
            "title": "Preprint",
            "content": "Siddarth Venkatraman, Moksh Jain, Luca Scimeca, Minsu Kim, Marcin Sendera, Mohsin Hasan, Luke Rowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, Alexandre Adam, Jarrid Rector-Brooks, Yoshua Bengio, Glen Berseth, and Nikolay Malkin. Amortizing intractable inference in diffusion models for vision, language, and control. Neural Information Processing Systems (NeurIPS), 2024. Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. Learning from the worst: Dynamically generated datasets to improve online hate detection. In ACL, 2021. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouedec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. In International Conference on Machine Learning (ICML), 2024. Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. arXiv preprint arXiv:2402.18571, 2024. Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. Distrl: An asynchronous In International distributed reinforcement learning framework for on-device control agents. Conference on Learning Representations (ICLR), 2025. Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? Neural Information Processing Systems (NeurIPS), 2023. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In International Conference on Machine Learning (ICML), 2024. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Neural Information Processing Systems (NeurIPS), 2022. David Zhang, Corrado Rainone, Markus Peschl, and Roberto Bondesan. Robust scheduling with GFlownets. International Conference on Learning Representations (ICLR), 2023. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "As discussed in Sections 4 and 5.4, TBA introduces new hyperparameters. Most notably, these include (1) the sync period k, which is the number of training steps between two successive model-buffer synchronizations; and (2) the most-on-policy probability m, which is the probability of sampling the data that is most on-policy i.e., the samples that were added to the buffer during the most recent synchronization. When listing hyperparameter values, we clarify those that are specific to TBA and include references to their discussion/visualization in the text. For MR and PFT, we implement TBA by building on the RLOO trainer class of the Hugging Face TRL library (von Werra et al., 2020). For MR and PFT, we make modifications to the baseline hyperparameters as shown in Table 3 and Table 4, respectively. Our RT implementation of TBA built on the TB trainer used in Lee et al. (2025), and we largely utilize their codes default hyperparameters, with differences noted in Appendix A.3. A.1 GSM8K MATHEMATICAL REASONING (MR) All baselines and TBA results use the same starting point, RhoMath-1B (Lin et al., 2024) model SFTed on GSM8K training data by Kazemnejad et al. (2024) realtreetune/rho-1b-sft-GSM8K on Hugging Face. The baseline model achieves 40.3% test accuracy. For the VinePPO baseline, training time is estimated using their reported 380 seconds per training step and 650 steps for GSM8K training (Kazemnejad et al., 2024). Hyperparameter Value Reference Model Learning Rate Learning Rate Schedule Learning Rate Warmup Steps Learning Rate Stable Steps Learning Rate Decay Steps Generation Temperature Max Prompt Token Length Response Length Number of Prompts per Batch Number of Completions per Prompt Batch Size (effective) Number of Training Steps Total Prompts Seen Total Episodes Rho-1B SFT on GSM8K 1 105 Warmup Stable Decay 50 450 500 0.7 512 512 7 20 140 1000 7000 140000 TBA-specific hyperparameters Beta (KL coefficient) Initial Value Beta Final Value Beta Linear Decay Schedule End Step Number of Samples per Prompt Most-On-Policy Sampling Probability Sync Period Number of Searchers Number of Trainers Reward-Based Sampling Prioritization Uniform Initial Completions in Buffer 0.012 0.004 500 24 0.95 2 3 1 500 in Section 4.1 β in Equation 5 β in Equation 5 β in Equation 5 in Section 4.1 in Section 5.4 in Section 5.4, Figure 2 Searchers in Figure 2 Trainer in Figure 2 Section 4.2 Buffer at Step 0 in Figure 2 Table 3: TBA Training Hyperparameters for the GSM8K MR task. In Figure 1, TBA uses the settings in Table 3 with few modifications to shorten training time by an additional 30%, down to 82 minutes on 4xA100 GPUs. In particular, we observed in initial testing (see Appendix B) that using the hyperparameters in Table 3 led to no improvement in performance for the final 300 steps. Thus, we shrank the training duration from 1000 steps to 700 (98000 episodes"
        },
        {
            "title": "Preprint",
            "content": "Hyperparameter Value Reference Model Learning Rate Learning Rate Schedule Generation Temperature Max Token Length Max Prompt Token Length Response Length Number of Prompts per Batch Number of Completions per Prompt Batch Size (effective) Number of Training Steps Total Prompts Seen Total Episodes Pythia SFTed on TL;DR 3 106 Linear 0.7 1024 512 128 8 20 160 256 2048 40960 TBA-specific hyperparameters in Section 4.1 Beta (KL coefficient) Initial Value Beta Final Value Beta Linear Decay Schedule End Step Number of Samples per Prompt Most-On-Policy Sampling Probability Sync Period Number of Searchers Number of Trainers Reward-Based Sampling Prioritization Initial Completions in Buffer 1 0.05 See caption 20 0.5 10 3 1 Softmax of Score 10000 β in Equation 5 β in Equation 5 β in Equation 5 in Section 4.1 in Section 5.4 in Section 5.4, Figure 2 Searchers in Figure 2 Trainer in Figure 2 Section 4.2 Buffer at Step 0 in Figure 2 Table 4: TBA Training Hyperparameters for the TL;DR PFT task. For the PFT task, we accelerate the decay of Beta. In particular, the Beta Linear Decay Schedule End Step is set to be half the number of training steps, but we abruptly end this decay and set Beta to its final value at one eighth the number of training steps (e.g., step 32 for 256 steps). This has the effect of trading off KL/perplexity, which we found to be relatively low with our TBA setup, for win rate. with batch size 140). Additionally, we made the following modifications to attempt to reduce the variance of the shortened run: 350 stable learning rate steps (down from 450), 0.014 Beta Initial Value (up from 0.012). We ran this experiment three times obtaining performances of 55.8%, 53.9%, and 54.1% and reported the mean accuracy 54.6% in Figure 1. Limitations and future work The 700-step result we show in Figure 1 has standard error 0.6%, which is little more variance than what we observe in the original 1000 step setup (the blue line in the bottom left plot of Figure 6 shows the mean and standard errors for the 1000 step runs). Future work could further explore variance reduction approaches/hyperparameters for shorter runs (and for TBA/RL more generally). One way to deal with variance is to choose checkpoint based on held out validation set, strategy used to produce the VinePPO result (Kazemnejad et al., 2024). We do not use this approach but note that our results would likely benefit significantly ( 1%) from it. In particular, each of our runs tends to produce (at some point during training) higher performance than the performance at the final step this is expected if you consider that the model performance varies around the average value it converges to towards the end of training (e.g., see again the blue line in the bottom left plot of Figure 6). Despite its being lower than the maximum performance achieved at any point during training, we report this final step performance, which is what user of TBA could expect to obtain without applying techniques like early-stopping. A.2 TL;DR PREFERENCE FINE TUNING (PFT) For PFT in Figure 3, we use the settings in Table 4 as well as longer-duration run with 425 updates."
        },
        {
            "title": "Preprint",
            "content": "For PFT in Figure 4, we create the TBA frontier by modifying the training steps, searcher count, and beta linear decay schedule shown in Table 4. We train for 256, 425, 625, 725, and 825 steps, and we search with 2, 3, 4, and 7 searcher nodes. We did not notice significant pattern in performance when changing searcher count, but we found tendency for higher KL/perplexity values and win-rates with more training steps (an expected tradeoff from optimizing the policy further). We noticed that the 2.8B model did not create wide range of KL/perplexity values with these initial runs, so we also performed runs at that model size (with 825 steps, and with 2 and 4 searchers) using less rapid beta linear decay schedule (reaching the beta final value at step 140 instead of step 104). This schedule change had the effect of reducing the KL/perplexity and win-rate (another expected tradeoff). For PFT in Table 1, we use the settings in Table 4, except we train for 625 steps (100000 episodes) because we found use of more steps tended to improve win rate without significant increase in perplexity (approximate KL). Additionally, we only use 2 searchers in this run. See Figure 7 for depiction of the effects of step count and searcher count. Limitations and future work All of our PFT results were run in 32-bit precision and without DeepSpeed, which was used by baselines we compared against (Noukhovitch et al., 2025). Lower precision and such training acceleration packages could further improve the speedups we show. Relatedly, for our 2.8B runs, we used gradient checkpointing to fit more examples into micro batch, which led to slowdowns at this scale (i.e., we only have 3.8x speedup over the baseline in this setting). We leave the optimization of our framework with appropriate packages to future work. Finally, we used large number (10000) of initial completions in the buffer, and future work should confirm that smaller number (e.g. 1000) works equally well note that small number worked well for GSM8K. A.3 AUTOMATED RED TEAMING (RT) Unlike our MR and PFT implementations, our RT implementation uses the code of Lee et al. (2025) as baseline and thus does not follow the Hugging Face TRL trainer style. We discuss hyperparameters in the context of their trajectory balance trainer approach below. We adopt their code largely as it is, with the exception that our TBA implementation uses larger replay buffers than Lee et al. (2025) to accommodate our scaled search for trajectories. Additionally, unlike Lee et al. (2025), we remove the oldest samples when the buffers fill up as opposed to the lowest reward samples. For the Llama results in Table 2, our hyperparameter choices largely follow those of Lee et al. (2025). We train for 5000 steps with batch size 128. For temperature sampling, we use low and high of 0.7 and 2.0, respectively. We use reward schedule horizon of 1000. The language model schedule end is 1.2, and its schedules horizon is 2000. We prioritize sampling based on reward. We use Beta 0.05. We use sync period (k) 10. We use 6 searchers. We use most-on-policy probability (m) 0.5 and 0.6 in combination with maximum buffer sizes 150000 and 130000, respectively. By having smaller maximum buffer size and larger m, the latter setting is expected to focus on more recently generated data and prioritize reward over diversity, which is what we observe in Table 2. For the GPT2 results in Figure 5 and Table 2, our hyperparameter choices again largely follow those of Lee et al. (2025). We train for 5000 steps with batch size 128. We use Beta 0.05. We use sync period (k) 10. We use most-on-policy probability (m) 0.5. We cap the maximum buffer size at 100000 in all experiments; we additionally prevent the buffer from growing past its size as of step 4000 to encourage focusing on more recent data (larger most on policy probabilities may provide similar effect). We test searcher counts 2, 6, 14, 30, 62. Limitations and future work In Table 2, there is slowdown as the number of searchers scales. We believe this is largely addressed by newer version of our code that uses more efficient buffer communication, but we have not re-run these results yet to confirm this. In any case, developing more efficient TBA implementations is an interesting direction for future work given TBAs ability to effectively leverage large-scale data generation."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: GSM8K ablation studies. All experiments begin with the base hyperparameters listed in Table 3 and make the depicted modifications, except when studying the synchronization period in the top right plot (where we use Beta Final Value 0.005 because 0.004 led to instability with = 4). We report the mean and standard error from 2 runs of each configuration. GSM8K ABLATION STUDIES We adopted the hyperparameters for our GSM8K result in Figure 1 based on series of trial experiments centered around the hyperparameters shown in Table 3. In Figure 6, we show the effects of changing what we found to be key hyperparameters. We note the following observations about TBA hyperparameter effects on GSM8K. 1. Unlike PFT, it was important for to be somewhat large for the GSM8K MR task (Figure 6, top left). Similarly, syncing more frequently was beneficial (Figure 6, top right). Together, these results suggest that GSM8K performance is more sensitive to off-policyness than performance on other tasks (e.g., PFT). 2. We found that the WSD schedule could add stability (Figure 6, bottom right). 3. Using smaller Beta Final Values tended to improve performance (Figure 6, bottom left), but training became unstable around 0.003. This suggests tradeoff between stability and accuracy for GSM8K that is mediated by the KL coefficient β. 4. batch size of 140 did not provide significantly better or worse results than larger batch sizes in initial testing, but smaller batch sizes allow for faster training steps, motivating our use of 140."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: TL;DR ablation studies. All experiments begin with the base hyperparameters listed in Table 4 then make the depicted modifications. More searcher nodes and more training steps tend to improve win rate at the cost of higher perplexity. TL;DR ABLATION STUDIES With TBA, we take many more optimization steps per hour of training than competing methods take. Accordingly, we sought to understand how taking more steps or using more searchers (to reach compute-parity with competing methods) affects performance. As shown in Figure 7, win rate tends to improve with increased compute through changes to these variables, though perplexity suffers as expected when we over-optimize the policy (in the case of step scaling). It is not clear why scaling the searcher count seems to have an effect similar to step scaling. However, there is notable mechanism through which searcher scaling could have an effect: using more searchers should reduce the probability of selecting training prompt thats been seen before when sampling off-policy (because scaling the searchers scales the number of unique prompts with completions added to the buffer).2 However, the effect size is small and inconsistent enough to suggest this searcher-scaling trend (in the context of PFT) needs further investigation before its confirmed. 2Notably, the effect of scaling search is different in the case of RT, where there is single prompt and scaling searchers makes it more likely that higher-reward samples are found and trained on. Here, with PFT, scaling search means that more prompts per second have set of completions generated for them, but any given prompt is not expected to have higher-reward samples as result of scaling the searcher count this is an implementation choice and could be changed to gain the effect that scaling searchers has with RT, however."
        }
    ],
    "affiliations": [
        "CIFAR Fellow",
        "KAIST",
        "Lawrence Livermore National Laboratory",
        "Mila Quebec AI Institute",
        "Universite de Montreal"
    ]
}