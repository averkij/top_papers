{
    "paper_title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
    "authors": [
        "Yaoxiang Wang",
        "Haoling Li",
        "Xin Zhang",
        "Jie Wu",
        "Xiao Liu",
        "Wenxiang Hu",
        "Zhongxin Guo",
        "Yangyu Huang",
        "Ying Xin",
        "Yujiu Yang",
        "Jinsong Su",
        "Qi Chen",
        "Scarlett Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective instruction tuning is indispensable for optimizing code LLMs, aligning model behavior with user expectations and enhancing model performance in real-world applications. However, most existing methods focus on code snippets, which are limited to specific functionalities and rigid structures, restricting the complexity and diversity of the synthesized data. To address these limitations, we introduce a novel feature tree-based synthesis framework inspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic structure of code, our framework models semantic relationships between code elements, enabling the generation of more nuanced and diverse data. The feature tree is constructed from raw data and refined iteratively to increase the quantity and diversity of the extracted features. This process enables the identification of more complex patterns and relationships within the code. By sampling subtrees with controlled depth and breadth, our framework allows precise adjustments to the complexity of the generated code, supporting a wide range of tasks from simple function-level operations to intricate multi-file scenarios. We fine-tuned widely-used base models to create the EpiCoder series, achieving state-of-the-art performance at both the function and file levels across multiple benchmarks. Notably, empirical evidence indicates that our approach shows significant potential in synthesizing highly complex repository-level code data. Further analysis elucidates the merits of this approach by rigorously assessing data complexity and diversity through software engineering principles and LLM-as-a-judge method."
        },
        {
            "title": "Start",
            "content": "EpiCoder: Encompassing Diversity and Complexity in Code Generation Yaoxiang Wang χϕ Haoling Liπϕ Xin Zhangϕ Jie Wuπϕ Xiao Liuϕ Wenxiang Huϕ Zhongxin Guoϕ Yangyu Huangϕ Ying Xinϕ Yujiu Yang π Jinsong Suχ Qi Chenϕ Scarlett Liϕ χXiamen University πTsinghua University ϕMicrosoft"
        },
        {
            "title": "Abstract",
            "content": "Effective instruction tuning is indispensable for optimizing code LLMs, aligning model behavior with user expectations and enhancing model performance in real-world applications. However, most existing methods focus on code snippets, which are limited to specific functionalities and rigid structures, restricting the complexity and diversity of the synthesized data. To address these limitations, we introduce novel feature tree-based synthesis framework inspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic structure of code, our framework models semantic relationships between code elements, enabling the generation of more nuanced and diverse data. The feature tree is constructed from raw data and refined iteratively to increase the quantity and diversity of the extracted features. This process enables the identification of more complex patterns and relationships within the code. By sampling subtrees with controlled depth and breadth, our framework allows precise adjustments to the complexity of the generated code, supporting wide range of tasks from simple function-level operations to intricate multi-file scenarios. We fine-tuned widely-used base models to create the EpiCoder series, achieving state-of-the-art performance at both the function and file levels across multiple benchmarks. Notably, empirical evidence indicates that our approach shows significant potential in synthesizing highly complex repository-level code data. Further analysis elucidates the merits of this approach by rigorously assessing data complexity and diversity through software engineering principles and LLM-as-a-judge method. 5 2 0 2 8 ] . [ 1 4 9 6 4 0 . 1 0 5 2 : r Figure 1: Benchmark performance of EpiCoder-Qwen-7B (fine-tuned on Qwen2.5-Coder-7B-Base) and its counterparts. XFileDep is file-level code generation benchmark, all others are function-level. Equal contribution. This work was done during the internships of Yaoxiang Wang, Haoling Li and Jie Wu at Microsoft. (cid:66): wyx7653@gmail.com; li-hl23@mails.tsinghua.edu.cn; xinzhang3@microsoft.com. Corresponding authors. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [OpenAI, 2023, Zhang et al., 2022] have demonstrated significant potential in the field of code understanding and generation, particularly through pre-training on large-scale code data [Zhu et al., 2024]. However, the latent code knowledge in these models is often underutilized without fine-tuning on high-quality instruction data. Instruction tuning [Wei et al., 2021] has emerged as critical step in unlocking the full capabilities of code LLMs, enabling more precise alignment with user intent and improving the usability of the model in real-world scenarios. Despite recent advances, most existing methods for synthesizing high-quality instruction data still rely on code snippets as seed data Chaudhary [2023], Wei et al. [2024a], Yu et al. [2023a]. While code snippets demonstrate specific functionalities, they fail to capture the full range of programming constructs, patterns, and interactions that are common in real-world programming scenarios. Additionally, due to the inherent rigidity of code snippets, it is difficult to rearrange or recombine them to generate new and diverse combinations. These limitations restrict the overall complexity and diversity of the synthesized data, highlighting the critical need for more structured representations as seed data to overcome these constraints and address real-world programming challenges. Inspired by Abstract Syntax Tree (AST), we propose feature tree-based code data synthesis framework that revolves around hierarchical code features derived from high-level abstractions such as variable types and control flow. While AST captures the syntactic structure of code, our approach extends this idea by organizing features into tree structure that captures the semantic relationships between code elements. Specifically, we extract features from the seed data and use hierarchical clustering to generate tree structure demonstration, starting from feature set and proceeding bottom-up. This demonstration serves as guideline for the LLM to directly extract tree structures from raw code data. To ensure comprehensive coverage of real-world scenarios, we enhance the diversity of features by iteratively expanding the tree structure both in breadth and in depth. Compared to methods that evolve based on individual code snippets or single instructions Luo et al. [2023], our approach is more efficient and achieves broader coverage, as the tree structure provides clear and organized directions for evolution. The resulting feature tree is large and hierarchical structure, from which we can sample subtrees to generate code data. Our feature tree-based synthesis method offers two advantages: (1) Scalable Complexity: Our framework allows for controlling the complexity of synthesized data by adjusting the depth and breadth of the subtrees, enabling us to create code at different levels of complexity, ranging from simple function-level tasks to comprehensive multiple file solutions. (2) Targeted Learning: By adjusting the probability of sampling features, we can prioritize specific knowledge areas that are underrepresented, ensuring more balanced and focused learning process for the LLMs. We conduct extensive experiments to validate our feature tree-based code data synthesis framework, training Qwen2.5-Coder-7B-Base and DeepSeek-Coder-6.7B-Base to derive the EpiCoder series model. Our EpiCoder-Qwen-7B trained on 380k frequency-level and 53k file-level data, respectively, achieves state-of-the-art (SOTA) performance among models of comparable size on average across five function-level code generation benchmarks. Furthermore, on file-level benchmark XFileDep, LLM trained with multi-file data exhibits superior performance, underscoring its capability to address programming problems of varying complexity. Surprisingly, our approach also shows remarkable potential in synthesizing highly complex repository-level data, as evidenced by Figure 5. Definitions from software engineering principles and the LLM-as-a-judge methodology were employed to evaluate the complexity of the data. In addition, we constructed feature trees to assess data diversity to highlight the advantages of our approach. In summary, our work makes the following key contributions: We propose feature tree-based code synthesis framework that enables controllable synthesis of diverse and complex instruction data, ranging from function-level to file-level tasks. We synthesize 433k instruction data and train EpiCoder. Our EpiCoder-Qwen-7B achieves state-of-the-art performance among comparably sized models in multiple function-level and file-level benchmarks, demonstrating its capability to tackle programming problems of varying complexity. By conducting further analysis, we showcase the advantages of our data in terms of complexity and diversity, as well as its potential for synthesizing large-scale repositories. 2 Figure 2: Overview of our feature tree-based code generation framework, which consists of three steps: (a) Feature Tree Extraction, where we first extract the feature set to construct the tree structure demonstration and then extract the feature trees; (b) Feature Tree Evolution, where the feature tree is iteratively expanded in depth and breadth; and (c) Feature Tree-Based Code Generation, where the evolved feature tree is used to generate diverse code instruction data. detailed example of feature evolution and code generation is shown in Appendix A."
        },
        {
            "title": "2 Method",
            "content": "In this section, we present our feature tree-based code generation framework, which consists of three key steps: (1) Feature Tree Extraction (Section 2.1), where we construct the tree demonstration and extract feature trees from seed data; (2) Feature Tree Evolution (Section 2.2), where we iteratively expand the feature tree to enhance diversity and coverage; and (3) Feature Tree-Based Code Generation (Section 2.3), where we use the evolved feature tree to generate diverse code instruction data with varying complexity. An overview of the framework is illustrated in Figure 2."
        },
        {
            "title": "2.1 Feature Tree Extraction",
            "content": "Inspired by Abstract Syntax Trees (AST), which represent the syntactic relationships in code, we use the LLM to extract hierarchical representation that organizes key elements of code into tree structure to capture more fundamental semantic relationships. Raw Code Collection To ensure data diversity and comprehensiveness, we obtain seed data from The Stack v2 [Lozhkov et al., 2024], large-scale dataset widely used for pre-training code LLMs. Following [Yu et al., 2023a], we apply the KCenterGreedy algorithm [Sener and Savarese, 2018] to select core set of diverse samples based on the code embeddings encoded by roberta-large-v1 [Liu et al., 2019]. Tree Demonstration Construction To extract features from the seed data, we leverage powerful large language model (LLM), specifically GPT-4o. 3 Since the initial prompt provided to the LLM can significantly impact its response, we propose an iterative method to optimize the demonstration 3Unless otherwise specified, the strong LLM refers to GPT-4o. 3 Figure 3: An example of file-level code generation (including test code file). Different files contain different functional modules, with dependencies existing across files. tree structure within the prompt. As shown in Figure 2(a), the construction of the demonstration consists of the following two steps: 1. Feature Pre-Extraction: With draft version of the prompt, the LLM is used to extract an initial set of features from the seed data, producing collection of feature keywords. 2. Iterative Demonstration Generation: For each step, we take subset of the feature set and prompt the LLM to perform hierarchical clustering, producing tree structure that represents the relationships among the features. We iteratively refine the tree structure demonstration by adjusting the clustering of features, ensuring well-organized hierarchical relationship. Feature Tree Extraction Using the refined demonstration of the tree structure, the LLM is tasked with extracting tree-structured feature representation for each code snippet. These individual feature trees are then merged into comprehensive structure that unifies the extracted features across all code snippets. During this process, we record the frequency of each node to reflect the distribution of features in the seed data. Since the seed data is derived from the pre-training data, this frequency can serve as an approximate measure of the knowledge distribution within the pre-trained model."
        },
        {
            "title": "2.2 Feature Tree Evolution",
            "content": "To overcome the limitations in both diversity and quantity of features in the seed data, we expand the feature tree through an evolutionary process. Compared to evolving directly from the seed code or instructions, this approach ensures broader feature coverage and improves efficiency by leveraging the trees structured representation, which provides clear and systematic directions for evolution. As illustrated in Figure 2(b), at each iteration, subtree is sampled from the full feature tree. This subtree is then evolved by the LLM along two dimensions, depth and breadth, by adding finer-grained child nodes to existing nodes as well as sibling nodes at the same hierarchical level. These newly evolved subtrees are then merged back into the overall structure, significantly enriching the feature space and facilitating the generation of more diverse and complex code. An example of the evolution of single subtree is provided in Appendix A.2. One key challenge in feature evolution is to estimate the frequency of newly generated features. Unlike the feature extraction stage, where frequencies are calculated directly, the frequency of an evolved feature is estimated as the average frequency of its siblings, as illustrated in Algorithm 1. This approach reasonably estimates the frequency of new features according to the existing feature distribution, ensuring that evolved features integrate seamlessly into the broader feature tree. 4 Algorithm 1 Feature Evolution with Frequency Estimation Require: Feature tree , frequency library containing the frequency of each node in , maximum steps S find_siblings(node, expanded_t) if = then find_siblings(node, ) 1: for step = 1 to do sample(T ) 2: expanded_t LLM.evolve(t) 3: for each node expanded_t do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for end if if = then (node) 1 (node) sS (s) end for end if else (cid:80) Sample subtree from Evolve along depth and breadth New nodes in the expanded subtree Find siblings of node in expanded_t If no siblings in expanded_t, find siblings in Still no siblings found Default frequency set to 1 Update frequency"
        },
        {
            "title": "2.3 Feature Tree-Based Code Generation",
            "content": "Traditional code generation methods that rely solely on code snippets often produce outputs that closely resemble the original seed data, limiting diversity and complexity. We mitigate this issue by generating code based on feature trees and the process is outlined as follows. Feature Distribution Adjustment The previously recorded feature frequencies partially reflect the distribution of natural data, helping to simulate real-world scenarios. However, some high-frequency but easy features, such as config and initialize, do not require strong focus during instruction tuning. To address this issue, we adjust the probability distribution of nodes child features as follows: = (cid:80) exp(log pi/t) jC exp(log pj/t) (1) where pi represents the normalized original frequency of the child feature and is the adjusted probability. As detailed in Algorithm 2, the summation is over all child features in the set C, which denotes the set of child nodes for the current parent node. higher temperature value leads to smoother distribution, allowing less dominant features higher probability of being selected. To further enhance the diversity of the generated data, we employed multiple temperature values during the data synthesis process for broader range of feature distributions. Task Generation To generate diverse coding tasks, we sample subtree of candidate features from the feature tree according to the adjusted probability distribution. The sampling process is guided by predefined shape of the subtree to sample, where we recursively apply Algorithm 2 to get the subtree. The LLM then uses this subtree to sequentially generate scenario and corresponding task. By adjusting the depth and breadth of the subtree to sample, we can flexibly create tasks with varying complexity, providing broad spectrum of coding challenges. Code Generation Based on the generated task, the LLM proceeds to generate an analysis and the corresponding code. The solution code can range from single function to comprehensive multi-file project, depending on the tasks complexity. By supporting multi-file solutions, this approach enables the generation of code that reflects realistic project structure and effectively captures cross-file information for problems requiring interactions across different components. As illustrated in Figure 3, different files implement distinct functionalities and collectively form an integrated system through their interdependencies. To improve the quality of the generated data, the solution code is accompanied by relevant test files, which are also generated by the LLM. These tests are executed in an isolated environment, allowing us to identify and filter out incorrect solutions. Through an iterative debugging process, information 5 Algorithm 2 Feature Sampling for Task Generation Retrieve child nodes of current node Require: Current root node R, frequency library , temperature t, sample size 1: selected_set 2: for = 1 to do 3: 4: 5: 6: 7: 8: get_children(R) if = then break (cid:80) end if fi [i] for all pi fi Compute cur_node sample_node(C, [p selected_set.add(cur_node) using (1) for all 1, for all jC fj 9: 10: 11: 12: end for 13: return selected_set 2, . . . ]) Get frequency for each child node Normalize frequencies to probabilities Adjust probabilities for child nodes Sample child node Add the selected node to the feature set Exit if there are no children from the execution results is used to guide the LLM to refine the solution, ensuring the correctness of the generated code."
        },
        {
            "title": "3 Evaluation",
            "content": "In this section, we introduce the details of the synthetic data (3.1) and evaluate the models performance in code generation at different levels. Specifically, in Section 3.2, we evaluate the models ability using five function-level code generation benchmarks. In Section 3.3, we employ our meticulously crafted benchmark to evaluate the models file-level code generation capabilities. 3."
        },
        {
            "title": "Implementation Details",
            "content": "We utilized our pipeline to extract 5k features from the core set of 150k Python language files in The Stack V2. These features were then expanded through evolutionary methods to 140k features. To validate the complexity of the synthetic data, we synthesized 380k function-level data samples and 53k file-level data samples, respectively. We chose DEEPSEEK-CODER-BASE-6.7B [Guo et al., 2024] and QWEN2.5-CODER-7B [Hui et al., 2024] as the base LLMs. To ensure fair comparison with other baselines, we incorporated the EVOL-CODEALPACA-V1 4 (applied the same filtering criteria as described in [Yu et al., 2023a]) dataset into the training of only the DEEPSEEK-CODER-BASE-6.7B model. We obtained the EpiCoder-DS-6.7B and EpiCoder-Qwen-7B models after training. For models trained solely on file-level data, we employed additional notation. We evaluated the models on benchmarks corresponding to their respective training levels."
        },
        {
            "title": "3.2 Function-level Generation",
            "content": "Many previous code LLMs have exhibited overfitting to specific benchmarks after fine-tuning, which constrains their ability to generalize to other benchmarks. To prevent this, we employed five benchmarks for evaluation, ensuring they are broad, comprehensive, reliable, and decontaminated. HumanEval and MBPP HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021] are popular benchmarks for assessing code generation capabilities. Considering the limited test cases in these benchmarks, we followed previous work [Wei et al., 2024a, Zheng et al., 2024a] and utilized the EvalPlus [Liu et al., 2023] framework to evaluate the models robustness across broader range of test cases. To be fair in comparison, we used version 0.2.0 of MBPP+ provided by EvalPlus, which removes some broken tasks (399 378 tasks). Table 1 shows the results of different LLMs on these benchmarks. BigCodeBench BigCodeBench [Zhuo et al., 2024] is comprehensive benchmark designed to assess models ability to handle demanding real-world programming tasks, particularly focusing on its 4https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1 5https://evalplus.github.io/leaderboard.html 6 Table 1: Pass@1 (%) results of different LLMs on HumanEval (+) and MBPP (+) computed with greedy decoding. We report the results uniformly from the EvalPlus Leaderboard 5. Model Base Model HumanEval Plus Base MBPP Base Plus Average Closed-source Model GPT-4-Turbo (April 2024) GPT-4 (May 2023) GPT-3.5-Turbo (Nov 2023) claude-3-opus (Mar 2024) claude-3-sonnet (Mar 2024) claude-3-haiku (Mar 2024) Qwen2.5-Coder-32B-Instruct DeepSeek-Coder-V2-Instruct OpenCoder-8B-Instruct DeepSeek-Coder-33B-instruct Codestral-22B-v0.1 DSCoder-6.7B-Base DeepSeekCoder-6.7b-Instruct Magicoder-S-DS WaveCoder-Ultra-6.7B OpenCodeInterpreter-DS-6.7B EpiCoder-DS-6.7B Qwen2.5-Coder-7B-Base Qwen2.5-Coder-7B-Instruct EpiCoder-Qwen-7B - - - - - - 90.2 88.4 76.8 82.9 70.7 76.8 7B+ Scale - - - - - 92.1 85.4 81.7 81.1 79. 7B Scale - - 47.6 74.4 76.8 75.0 77.4 80.5 61.6 88.4 89.0 86.6 79.3 70.7 77.4 64.0 68. 87.2 82.3 77.4 75.0 73.8 39.6 71.3 71.3 69.5 72.0 76.8 53.0 84.1 82.3 85.7 - 82.5 89.4 83.6 80.2 90.5 89.4 82.0 80.4 72.5 72.0 74.9 79.4 74.9 76.5 81. 76.9 83.5 84.1 73.3 - 69.7 73.3 69.3 68.8 77.0 75.1 71.4 70.1 61.9 58.7 65.6 69.0 63.5 66.4 68.3 62.9 71.7 71.4 84.0 - 75.0 80.8 71.9 73. 86.7 83.1 78.1 76.7 72.0 54.5 71.6 74.1 70.7 73.1 76.8 63.6 81.9 81.7 effectiveness in utilizing various function calls as tools. Our models ability to adeptly manage these high-complexity programming scenarios underscores its suitability for BigCodeBench. Table 2 shows the results of different LLMs on this benchmark. EvoEval Many existing benchmarks are prone to data leakage, and to attenuate its impact, we comprehensively evaluate the coding capabilities of LLM on EvoEval [Xia et al., 2024], which is constructed by evolving HumanEval to different target domains (Difficult, Creative, Subtle, Combine and Tool Use). Table 3 shows the results of different LLMs on this benchmarks. We obtained the results from the paper [Xia et al., 2024], and for models without reported results, we conducted tests using their default prompts. FullStackBench Most existing code evaluation datasets cover only limited application areas, such as basic programming and data analysis, lacking comprehensive assessment of code LLMs capabilities across broader computer science domains. To demonstrate our models strong performance across wide and diverse range of areas, we utilize FullStackBench [Liu et al., 2024]. This benchmark encompasses 16 programming languages and various computer science domains, aiming to thoroughly evaluate the abilities of large models in diverse real-world coding scenarios. Table 4 shows the results of different LLMs on this benchmark. Five function-level benchmarks involve evaluating the models basic programming skills, practical programming ability in real-world scenarios, diverse programming objectives, and full-stack programming capabilities. The experimental results indicate that EpiCoder-Qwen-7B achieves state-of-the-art (SOTA) average performance for models of the same size. The evaluation of the EpiCoder series models on these benchmarks highlights their capability to solve challenge and complex programming problems. This also demonstrates that the feature tree-based code generation method can provide high-quality and diverse synthetic data tailored to function-level programming problems. 6https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard 7 Table 2: Pass@1 (%) results of different LLMs on BigCodeBench computed with greedy decoding. We conducted the evaluation on the Full and Hard subsets of this benchmark, including the Complete and Instruct tasks. Except for the results underlined, which are sourced from their respective papers, all other results are obtained from the BigCodeBench-Leaderboard6. Model Base BigCodeBench-Full BigCodeBench-Hard Instruct Complete Complete Instruct GPT-4o (May 2024) DeepSeek-V2-Chat (June 2024) Claude-3.5-Sonnet (June 2024) Qwen2.5-Coder-32B-Instruct DeepSeek-Coder-V2-Instruct Llama-3.3-70B-Instruct Codestral-22B-v0.1 DeepSeek-Coder-33B-Instruct OpenCoder-8B-Instruct DSCoder-6.7B-Base DeepSeekCoder-6.7b-Instruct Magicoder-S-DS WaveCoder-Ultra-6.7B OpenCodeInterpreter-DS-6.7B EpiCoder-DS-6.7B Qwen2.5-Coder-7B-Base Qwen2.5-Coder-7B-Instruct EpiCoder-Qwen-7B Closed-source Model - - - - - - - - - - - 61.1 59.4 58.6 7B+ Scale 58.0 59.7 57.5 52.5 51.1 50.9 7B Scale 41.8 43.8 47.6 43.7 44.6 50. 45.8 48.8 51.9 51.1 48.9 46.8 49.0 48.2 46.9 41.8 42.0 43.2 - 35.5 36.2 33.9 37.1 37.9 - 40.4 43.8 29.1 32.4 33. 33.8 29.7 28.4 24.3 20.9 18.9 13.5 15.5 12.8 16.9 16.9 19.6 16.2 20.3 27.7 25.0 25.0 25.7 27.7 24.3 28.4 16.9 17.6 18.2 - 10.1 13.5 12.8 13.5 12. - 20.9 22.3 Avg 41.6 41.4 41.1 42.1 40.5 40.3 33.9 32.9 32.8 - 26.2 27.5 26.8 28.0 30.2 - 32.6 36. Table 3: Pass@1 (%) results of different LLMs on EvoEval computed with greedy decoding. Model Difficult Creative Subtle Combine Tool Use Avg Closed-source Model GPT-4-Turbo GPT-4 Claude-3 ChatGPT Claude-3-haiku DeepSeekCoder-33b-Instruct WizardCoder-33b-1.1 CodeLlama-70b-Instruct OpenCoder-8B-Instruct 50.0 52.0 50.0 33.0 40.0 47.0 48.0 31.0 45.0 61.0 66.0 53.0 42.0 47.0 7B+ Scale 47.0 48.0 41.0 50. 7B Scale 21.0 DeepSeek-Coder-6.7B-base 40.0 DeepSeekCoder-6.7b-Instruct 40.0 Magicoder-S-DS-6.7B WaveCoder-Ultra-6.7B 38.0 OpenCodeInterpreter-DS-6.7B 43.0 EpiCoder-DS-6.7B 40.0 Qwen2.5-Coder-7B-Base Qwen2.5-Coder-7B-Instruct EpiCoder-Qwen-7B 35.0 48.0 53.0 24.0 37.0 34.0 42.0 37.0 45.0 20.0 49.0 48. 8 82.0 76.0 81.0 70.0 65.0 67.0 66.0 65.0 73.0 47.0 61.0 67.0 71.0 65.0 70.0 55.0 77.0 78.0 45.0 53.0 42.0 33.0 17. 31.0 20.0 18.0 28.0 5.0 18.0 21.0 24.0 25.0 30.0 27.0 37.0 47.0 69.0 68.0 69.0 64.0 56.0 66.0 64.0 65.0 50.0 55.0 51.0 61.0 35.0 51.0 65. 41.0 65.0 68.0 61.4 63.0 59.0 48.4 45.0 51.6 49.2 44.0 49.2 30.4 41.4 44.6 42.0 44.2 50.0 35.6 55.2 58.8 Table 4: Model performance across domains of Python in the English Subset of FullStackBench. Others Overall MA DW ML Model MM DB AP DP OS BP SC SE OpenAI o1-preview OpenAI o1-mini Claude-35-Sonnet GPT 4o-0806 Doubao-Coder-Preview DeepSeek-v2.5 Qwen-Max GLM-4-Plus DeepSeekCoder-v2-Instruct Qwen2.5-Coder-32B-Instruct DeepSeekCoder-33B-Instruct CodeLlama-34B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeekCoder-v2-Lite-Instruct StarCoder2-15B-Instruct-v0.1 CodeLlama-13B-Instruct Qwen2.5-Coder-7B-Instruct Yi-Coder-9B-Chat DeepSeek-Coder-7B-Instruct-v1.5 OpenCoder-8B-Instruct DeepSeek-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat CodeLlama-7B-Instruct EpiCoder-DS-6.7B EpiCoder-Qwen-7B 55.56 72.22 50.00 72.22 55.56 55.56 50.00 55.56 55.56 50.00 50.00 5.56 55.56 50.00 61.11 11. 33.33 61.11 50.00 44.44 61.11 38.89 27.78 61.11 44.44 78.61 75.62 75.62 72.14 69.65 68.16 70.15 65.67 68.66 70.15 59.70 22.89 62.69 64.68 44.28 22.39 58.21 50.25 51.74 53.73 49.75 45.77 23. 47.26 61.19 Close-Sourced API Model 64.29 50.00 71.43 53.57 50.00 50.00 39.29 39.29 76.80 76.00 76.00 78.40 77.60 76.00 77.60 76.80 79.14 80.58 76.26 76.98 75.54 76.26 72.66 74.82 18.75 28.75 13.75 21.25 27.50 20.00 13.75 13. 51.28 56.41 51.28 66.67 51.28 48.72 56.41 58.97 20B+ Instruction Tuned Coder 35.71 50.00 21.43 14.29 81.60 77.60 71.20 40.00 79.14 66.19 48.92 17.27 16.25 17.50 18.75 16. 48.72 61.54 48.72 15.38 13B+ Instruction Tuned Coder 32.14 32.14 32.14 25.00 39.29 32.14 25.00 28.57 28.57 50.00 25.00 25.00 17.86 76.00 64.00 63.20 24. 70.50 56.12 36.69 20.86 18.75 26.25 31.25 30.00 53.85 43.59 53.85 20.51 6B+ Instruction Tuned Coder 66.40 66.40 64.80 57.60 65.60 58.40 28.00 61.60 72. 48.92 46.76 37.41 35.97 38.13 31.65 20.86 41.01 61.15 18.75 26.25 25.00 26.25 18.75 15.00 23.75 40.00 28.75 38.46 43.59 30.77 28.21 38.46 33.33 10.26 41.03 51. 61.76 56.62 61.76 55.15 60.29 56.62 57.35 50.00 53.68 43.38 40.44 18.38 38.97 33.82 28.68 13.97 32.35 36.76 34.56 28.68 22.79 22.79 11.76 27.21 27.94 40.00 40.00 50.00 40.00 20.00 40.00 30.00 40. 40.00 30.00 30.00 30.00 30.00 60.00 60.00 40.00 40.00 50.00 20.00 0.00 30.00 20.00 50.00 50.00 20.00 47.37 57.89 63.16 68.42 63.16 63.16 47.37 52.63 52.63 47.37 42.11 26. 57.89 21.05 36.84 10.53 47.37 36.84 52.63 47.37 31.58 31.58 10.53 36.84 47.37 100.00 100.00 100.00 100.00 50.00 50.00 50.00 100.00 50.00 100.00 50.00 0.00 100.00 50.00 50.00 50. 50.00 50.00 50.00 0.00 50.00 0.00 0.00 50.00 50.00 74.47 72.34 78.72 72.34 55.32 65.96 63.83 53.19 57.45 61.70 44.68 23.40 55.32 53.19 53.19 23.40 59.57 48.94 48.94 44.68 42.55 42.55 21. 42.55 40.43 66.47 66.23 65.52 65.05 62.91 61.85 60.78 58.77 61.26 58.41 49.05 22.27 55.57 50.47 43.01 21.56 47.51 46.56 43.60 41.11 40.88 37.20 21.33 43.25 50."
        },
        {
            "title": "3.3 File-level Generation",
            "content": "Figure 4: Pass@1 (%) results of different LLMs on XFileDep computed with greedy decoding. In increasingly complex programming tasks, code LLMs at the function-level have become insufficient, necessitating shift towards more sophisticated file-level code generation. Our feature tree-based code generation method allows for seamless scaling of code-related features. By controlling the depth and width of the sampled sub-feature tree, we can adjust the complexity, thereby integrating large amount of feature information and synthesizing file-level code responsible for various functional modules. XFileDep Benchmark Many existing code benchmarks focus on function-level code generation and lack evaluations of file-level generation capabilities. To address this limitation, we have developed Cross-File Dependency Benchmark (XFileDep) specifically designed to assess the file-level code 9 Figure 5: An example of our repo-level code generation. The left part shows the original LLaMAFactory repository structure, the middle part presents the structure of LLMTune, which we generated based on the extracted feature tree, and the right part illustrates an example file from the generated repository. generation capabilities of LLMs while considering cross-file dependencies. XFileDep provides comprehensive framework by treating multiple interdependent code files as context and testing the models ability to generate missing files. This benchmark not only measures the models ability to generate isolated files, but also evaluates its proficiency in understanding and managing cross-file dependencies. The detailed process for constructing the benchmark is described in Appendix B.1. Evaluation Details The XFIleDep benchmark comprises total of 930 problems. For all problems, we employed pass@1 as the evaluation metric, as we believe that only code passing all test cases can be considered to be functioning correctly. Consequently, we did not account for partial pass rates. To ensure stable evaluation, we used docker to standardize the running environment for all problems. During the testing of different models, we consistently applied their default prompts and greedy decoding, with maximum token length of 8192. All open-source models were accelerated using vLLM [Kwon et al., 2023]. Figure 4 shows the results of different LLMs on this benchmark. The experimental results demonstrate that the EpiCoder series of LLMs, which have been specifically trained on dataset consisting of over 53k multi-file code samples, significantly outperforms baselines."
        },
        {
            "title": "3.4 Potential Repo-level Generation",
            "content": "Our approach benefits from hierarchical structure of the feature tree, enabling the synthesis of instructions and corresponding outputs of varying complexities. We explore its limit by attempting to synthesize much more complex real-world code repository data than the file-level data that contains only several files. Utilizing feature tree extracted from the popular open-source GitHub repository LLaMA-Factory7, we successfully generate repository mirroring the structure with over 50 files. The example files within this repository demonstrate the feasibility and scalability of our approach, as illustrated in Figure 5. This example highlights the potential of feature-based code generation to produce complex and structured repositories, offering promising direction for future research in repository-level code synthesis. 7https://github.com/hiyouga/LLaMA-Factory"
        },
        {
            "title": "4.1 Complexity Evaluation",
            "content": "We emphasize that our ability to generate more complex code data stems from its hierarchical feature tree structure. By flexibly adjusting the depth and width of the feature tree, we can dynamically control the complexity of the synthetic data, ensuring adaptability to diverse requirements. To evaluate code complexity, we employ two approaches: 1) using various software engineering principles to assess complexity; and 2) leveraging an external LLM judge to evaluate from multiple perspectives."
        },
        {
            "title": "4.1.1 Evaluation from the Software Engineering Perspective",
            "content": "To achieve precise and robust evaluation of code complexity, we first adopt software engineering perspective to compare the complexity of our generated code (at both the function and file levels) with existing code datasets. This comparison is based on three well-established software engineering metrics: Halstead Complexity Halstead [1977], Strictness Complexity Ray et al. [2016], and Cyclomatic Complexity McCabe [1976]. Halstead Complexity measures programs logical complexity by evaluating operands and operators; Strictness Complexity assesses the strictness of execution paths in the main function; and Cyclomatic Complexity quantifies code control flow complexity by analyzing its branching structure. Table 5 presents the overall Halstead complexity comparison, with detailed results in Appendix C.2.1. Our function-level data exceeds the runner-up by 2.55 and 20.99 for unique operators and operands, respectively. For total operators and operands, our function-level results are significantly higher, reaching 56.98 and 100.36, roughly double the current baseline. Additionally, the file-level data shows even greater Halstead complexity, far surpassing the function-level results. Table 5: Comparison of Halstead complexity between ours and existing codebase. Dataset Unique Operators Unique Operands Total Operators Total Operands Code Alpaca Chaudhary [2023] Evol CodeAlpaca Luo et al. [2023] CodeFeedBack Zheng et al. [2024b] OSS Instruct Wei et al. [2024b] Ours (func-level) Ours (file-level) 4.83 7.94 8.11 7.44 10.66 11.64 8.22 18.97 20.42 20.99 44.32 72.87 10.66 29.91 30.98 28.05 56.98 100.24 15.89 46.70 50.05 47.55 100.36 179.98 Table 6 shows that our data exhibits significantly higher strictness and cyclomatic complexity. For strictness, the function-level mean is 4.95 (median = 4.00), and the file-level mean is 5.41, both much higher than the other datasets, which range from 0.18 to 1.50. For cyclomatic complexity, the function-level mean is 5.14 (median = 5.00), and the file-level mean is 14.93 (median = 14.00). Table 6: Comparison of Strictness complexity (left) and Cyclomatic complexity (right). Std Dataset Mean Median Mean Median Dataset Std Code Alpaca Evol CodeAlpaca CodeFeedBack OSS Instruct Ours (func-level) Ours (file-level) 0.18 0.82 0.97 1.50 4.95 5. 0.00 0.00 0.00 1.00 4.00 4.00 0.52 1.63 2.09 2.19 3.77 3.85 Code Alpaca Evol CodeAlpaca CodeFeedBack OSS Instruct Ours (func-level) Ours (file-level) 2.10 3.76 3.96 3.45 5.14 14.93 1.00 3.00 3.00 3.00 5.00 14.00 1.66 3.48 3.33 2.98 3.01 6."
        },
        {
            "title": "4.1.2 Evaluating Code Complexity using LLMs",
            "content": "We adopt the LLM-as-a-judge methodology for complexity evaluation, comparing our samples with the existing codebase using GPT-4o. Specifically, we evaluate code complexity across four dimensions: Error Handling, Modularity, Data Structure Complexity, and Third-Party Dependencies. Detailed evaluation criteria and prompts are provided in the Appendix C.2.2. Table 7 presents the average scores of 5k random selected code samples across these dimensions, demonstrating that our 11 framework achieves significant improvements in complex code generation. On average, our functionlevel code demonstrates 1.08% improvement over OSS-Instruct, while our file-level performance exceeds it by 1.74%, underscoring our ability to generate complex and high-quality code. Table 7: Comparison of code complexity across four dimensions using GPT-4o. Dataset Error Handling Modularity Dependency Data Structure Avg. Code Alpaca Evol CodeAlpaca CodeFeedBack OSS Instruct Ours (func-level) Ours (file-level) 2.04 2.53 2.71 2.74 4.11 4.23 2.10 3.32 3.47 3.79 4.71 5.94 2.09 2.66 2.23 2.78 3.83 4.62 2.38 3.58 3.75 3.92 4.90 5.41 2.15 3.02 3.04 3.31 4.39 5."
        },
        {
            "title": "4.2 Diversity Evaluation",
            "content": "Code generation based on feature trees not only results in greater code complexity but also introduces richer diversity. To assess the feature diversity, we sample 1k instances from ours, CodeFeedback, and other datasets for comparison. Features are extracted from each sample using GPT-4o (with prompt in Appendix C.3.1), and the number of unique features in each dataset is counted. Table 8 shows that our data outperforms other datasets in feature diversity, with an average of 8.53 unique features for function-level data and 8.95 for file-level data. Our function-level data significantly improves in areas like data processing (2,533 features), error handling (357 features), dependency relations (305 features), and user interaction (363 features), all 2-3 times higher than in existing codebases. While leading in most categories, our data also remains competitive in aspects like functionality and algorithms. Additionally, our data also surpasses existing codebases in total feature count, as detailed in Table 14 of Appendix C.3.2. Datasets Workflow Table 8: Distribution of unique features. Resource Usage Computation Operation Functionality Security Implementation Style 994 2079 2163 2254 2422 2475 6 6 11 5 6 11 393 535 591 669 657 7 18 21 39 37 43 282 689 783 413 819 536 8 48 60 49 156 103 User Interaction Data Processing 82 143 134 192 363 221 895 1401 903 2533 2196 File Operation Error Handling Logging Dependency Relations Algorithm Data Structures Implementation Logic Advanced Techniques 11 39 55 102 203 387 54 229 212 211 357 311 1 10 15 62 96 43 121 226 238 305 447 232 427 414 150 316 293 72 100 130 140 116 140 67 49 74 82 40 69 10 63 94 26 100 110 Avg. 2.48 5.45 6.38 5.54 8.53 8.95 Alpaca CodeFeedback Evol-Alpaca OSS-Instruct Ours (func-level) Ours (file-level) Datasets Alpaca CodeFeedback Evol-Alpaca OSS-Instruct Ours (func-level) Ours (file-level)"
        },
        {
            "title": "4.3 Data Leakage",
            "content": "In this section, we investigate potential data leakage issues to ensure that our synthetic data are free from such risks. Specifically, we use the jinaai/jina-embeddings-v3 embedding model to generate embeddings for the output segments of all training data, including our synthetic data and other training datasets used for comparison. For the HumanEval, MBPP, and BigCodeBench benchmarks, we encode their test datasets and compute the cosine similarity between each test instance and all training samples. For each test instance in the benchmarks, we identify the trainingtest data pair with the highest similarity score and plot the distribution of these scores in Figure 6. Furthermore, through case-based analysis of similarity scores, we define threshold for potential leakage (Similarity Score=0.9), with detailed explanations provided in Appendix C.1. Despite the large scale of our dataset, which puts it at disadvantage when identifying the most similar sample for each test instance, Figure 6 demonstrates that our 380k synthetic function-level data show minimal evidence of data leakage, even for the HumanEval benchmark, where the risk of leakage is most pronounced. Further analysis of similarity scores across other benchmarks supports the conclusion that our synthetic data are not strongly similar to the benchmark. This also confirms that 12 Figure 6: The distribution of cosine similarity scores between different various datasets and the benchmark datasets HumanEval, MBPP, and BigCodeBench. Figure 7: The scaling law of code instruction data. The results obtained from randomly sampled subsets of 380k data points across the HumanEval, MBPP, and BigCodeBench benchmarks. the performance gains of our model are not due to overfitting to the benchmarks but stem from the quality and diversity of the data."
        },
        {
            "title": "4.4 Scaling of Code Instruction Data",
            "content": "Although both the fields of mathematics and code are characterized by rigorous logic and precision, they exhibit different phenomena in terms of the quantity of instruction data. Motivated by previous analyses of instruction data scaling laws in the mathematical domain, we design experiments to understand the scaling laws in the code domain. We randomly sample 380k data points and set series of data quantities for our experiments. The results on the HumanEval, MBPP, and BigCodeBench benchmarks are depicted in Figure 7. It is evident that with the increase in data volume, the performance improves significantly. Moreover, even with the data size reaching 380k, there is still an upward trend, demonstrating that our dataset possesses sufficient diversity to effectively combat overfitting."
        },
        {
            "title": "5.1 Code LLMs",
            "content": "Following the success of general LLMs such as GPT-3 [Brown, 2020] and PaLM [Chowdhery et al., 2023] in both academia and industry, models like CodeX [Chen et al., 2021] have catalyzed new surge of research in code intelligence. The applications of code intelligence have expanded beyond mere code-related tasks to encompass broader real-world scenarios Zhu et al. [2024]. Currently, code LLMs are typically developed through continual pre-training [Roziere et al., 2023] and supervised fine-tuning [Yu et al., 2023a] based on general LLMs, leveraging either unannotated code corpora or high-quality labeled datasets. These methodologies conserve resources compared to training LLMs from scratch and enhances performance in downstream tasks. Given that general LLMs have already extensively utilized real-world data during their pre-training phases, the construction of data for post-training stages remains critical issue that requires urgent attention [Ding et al., 2024]."
        },
        {
            "title": "5.2 Data Synthesis for Code",
            "content": "Meticulously crafting [Zhou et al., 2024] diverse and complex instruction-solution examples is costly, particularly in the coding domain, which necessitates specialized expertise. Current research indicates that using LLMs to generate synthetic instruction pairs is an effective strategy to address the scarcity of instructions data for both coding and general tasks [Wang et al., 2023, Yu et al., 2023b]. WizardCoder [Luo et al., 2023] employs the Evol-Instruct method to synthesize more complex and 13 diverse instructional data. Similarly, Magicoder Wei et al. [2024a] utilizes open-source code snippets to guide LLMs in generating high-quality programming problems and solutions. This approach leverages the diversity and realism of open-source code to mitigate inherent biases in synthetic data, thereby enhancing the overall quality of the generated content. WaveCoder [Yu et al., 2023a] proposed generator-discriminator framework based on LLMs to produce diverse and high-quality instructional data. OpenCodeInterpreter [Zheng et al., 2024a] constructs datasets through the interactions of users, LLMs, and compilers, aiming to meet specific criteria such as diversity, challenge, and multi-turn dialogue structure. Genetic-Instruct [Majumdar et al., 2024] simulates the evolutionary processes of natural selection and genetic algorithms, employing crossover and mutation operations to generate new instructional samples from set of high-quality initial seed data."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce feature tree-based synthesis framework for generating diverse and complex code instruction data. Inspired by Abstract Syntax Trees (AST), our approach constructs hierarchical feature trees to capture semantic relationships within code, enabling scalable synthesis of instruction data with controllable complexity. By evolving the feature tree and sampling features, we enhance both the diversity and complexity of the generated data, offering significant advantages for instruction tuning of code LLMs. The experimental results demonstrate that our synthesized data excels in both diversity and complexity, and the trained EpiCoder achieves outstanding performance in tasks of varying complexity, from function-level to file-level benchmarks. Moreover, our approach shows strong potential for scaling to repository-level code synthesis and advancing the usability of code LLMs in increasingly complex programming environments."
        },
        {
            "title": "References",
            "content": "OpenAI. GPT-4 Technical Report. arXiv e-prints, 2023. Susan Zhang, Stephen Roller, Naman Goyal, and Artetxe. OPT: Open Pre-trained Transformer Language Models. arXiv e-prints, 2022. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned Language Models Are Zero-Shot Learners. arXiv e-prints, 2021. Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https: //github.com/sahil280114/codealpaca, 2023. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with oss-instruct. In Forty-first International Conference on Machine Learning, 2024a. Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint arXiv:2312.14187, 2023a. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024. Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: core-set approach. In International Conference on Learning Representations (ICLR), 2018. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv preprint arXiv:2402.14658, 2024a. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=1qvx610Cu7. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. Chunqiu Steven Xia, Yinlin Deng, and Lingming Zhang. Top leaderboard ranking= top coding proficiency, always? evoeval: Evolving coding benchmarks via llm. arXiv preprint arXiv:2403.19114, 2024. Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, ZY Peng, et al. Fullstack bench: Evaluating llms as full stack coder. arXiv preprint arXiv:2412.00535, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Maurice H. Halstead. Elements of Software Science (Operating and programming systems series). Elsevier Science Inc., USA, 1977. ISBN 0444002057. Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, and Premkumar Devanbu. On the \"naturalness\" of buggy code. In Proceedings of the 38th International Conference on Software Engineering, ICSE 16, page 428439, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450339001. doi: 10.1145/2884781.2884848. URL https://doi.org/10.1145/2884781.2884848. Thomas J. McCabe. complexity measure. In Proceedings of the 2nd International Conference on Software Engineering, ICSE 76, page 407, Washington, DC, USA, 1976. IEEE Computer Society Press. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1283412859. Association for Computational Linguistics, 2024b. doi: 10.18653/V1/ 2024.FINDINGS-ACL.762. URL https://doi.org/10.18653/v1/2024.findings-acl.762. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: EmpowIn Forty-first International Conference on Machine ering code generation with oss-instruct. Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024b. URL https://openreview.net/forum?id=XUeoOBid3x. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. 16 Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, and Shafiq Joty. Data augmentation using llms: Data perspectives, learning paradigms and challenges. arXiv preprint arXiv:2403.02990, 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, 2023. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023b. Somshubra Majumdar, Vahid Noroozi, Sean Narenthiran, Aleksander Ficek, Jagadeesh Balam, and Boris Ginsburg. Genetic instruct: Scaling up synthetic generation of coding instructions for large language models. arXiv preprint arXiv:2407.21077, 2024."
        },
        {
            "title": "Appendices Content",
            "content": "A Appendix of Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 19 A.1 Prompts of Feature Tree Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.2 Case of Feature Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Case of Task Generation . 22 A.4 Case of Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.4.1 Prompt for Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . 23 A.4.2 Initial Code Implementation . . . . . . . . . . . . . . . . . . . . . . . . . 26 A.4.3 Debugging and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . Appendix of Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 27 30 B.1 Cross-File Dependency Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Case of File-Level Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . Appendix of Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 34 C.1 Leakage Threshold Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Complexity Evaluation . 35 C.2.1 Detailed Metrics from the Software Engineering Perspective. . . . . . . . . 36 . . . . . . . . . . C.2.2 Prompts for Evaluating Code Complexity using GPT-4o. 39 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 C.3.1 Prompt for Feature Extraction. . . . . . . . . . . . . . . . . . . . . . . . . 40 C.3.2 Total Feature Diversity Comparison. . . . . . . . . . . . . . . . . . . . . . C.3 Diversity Evaluation . . ."
        },
        {
            "title": "A Appendix of Method",
            "content": "A.1 Prompts of Feature Tree Extraction Here are our draft prompts for pre-extraction and the refined prompts for feature tree extraction. For brevity, only portion is shown here. The complete prompts can be found in our released code. Draft Prompts for Pre-extraction Extract high-level information from code snippet using keywords separated by \"##\". For example: 1. Function Description: Describe the main functionality of the code. Use keywords such as list sorting ## input parsing ## data storage ## image processing. 2. Algorithm Details: Describe the specific algorithm used and its characteristics. Use keywords such as dynamic programming ## greedy algorithm ## divide and conquer ## backtracking ## graph traversal. 3. ... Please use this code as input and extract as much of the specified information as possible based on the content of the code. Input: {source_code} Output: <your answer>"
        },
        {
            "title": "Part of Refined Prompts for Feature Tree Extraction",
            "content": "Extract features from the provided code snippets, following the requirements for each category below, formatted in JSON structure. Categories to extract: 1. Programming Language: Note the specific programming language used. Example: [\"Python\", \"Java\"]. 2. Workflow: Outline the main steps and operations the code performs. Example: [\"data loading\", \"preprocessing\", \"model training\", \"evaluation\", \"results saving\"]. 3. Implementation Style: What programming paradigm the code follows. Example: [\"procedural\", \"object-oriented\", \"functional\"]. 4. Functionality: Explain the function of the code. Example: [\"data processing\", \"user interaction\", \"system control\"]. 5. Resource Usage: Analyze how the code utilizes system resources. Example: [\"CPU Cycles\", \"GPU ComputeOperations\", \"Network Bandwidth\"]. 6. Data Processing: Describe how the data is processed. This category can include the following subcategories: 6.1 Data Preparation: Steps taken to prepare data for further processing. Example: [\"validate units\", \"strip whitespace\"]. 6.2 Data Retrieval: Methods for obtaining data. Example: [\"fetching records\", \"retrieve top-level items\"]. 6.3 Data Transformation: Describe data transformation operations. Example: [\"convert to numpy array\", \"jsonschema\"]. Other relevant subcategories... 7. Computation Operation: What computation operations are used. This category can include the following subcategories: 7.1 Mathematical Operation: Specify mathematical computations, such as calculations involving statistics or algebra. Example: [\"standard deviation calculation\", \"compute power flow\"]. 7.2 Algorithmic Operation: Identify algorithm-based operations, such as those in optimization or data sorting. Example: [\"simulated annealing\", \"Best-Fit Decreasing\"]. 7.3 Statistical Operation: Note operations involving statistical analysis. Example: [\"calculate min and max\", \"calculate percentage positions\"]. Other relevant subcategories... 8. More content is omitted here: The demonstration tree for extracting additional categories has been truncated for brevity. For the full list of categories and detailed instructions, please refer to our code. Input: {source_code} Output: <your answer> 19 A.2 Case of Feature Evolution Figure 8 presents an example of the feature evolution. In the experiment, after 9000 steps of evolution, the number of features increased from 5000 to 1,40,000. Figure 8: An example of feature evolution."
        },
        {
            "title": "Prompts for Feature Evolution",
            "content": "Feature Tree Evolution Task: You are provided with feature tree represented as nested JSON structure. Each node in this tree represents feature or sub-feature of software system, with the leaves being the most specific features. Your task is to expand this feature tree both in depth and breadth. Depth expansion means adding more specific sub-features to existing leaves. Breadth expansion means adding more sibling features at the current levels. Here are some explanations of the features: {explanations} The input feature tree will be provided in JSON format, and your output should be JSON structure that represents the expanded feature tree. Output Format: - Expanded Feature Tree: Provide the expanded feature tree as JSON structure. {example} Constraints: 1. For breadth expansion, add at least 2 new sibling features to each existing node. 2. For deep expansion, add new sub-features to any leaf node that could have more fine-grained feature. 3. Focus on generating new and innovative features that are not present in the provided examples. Please follow the above constraints and expand the feature tree accordingly. Input: {feature_tree} Output: <begin>expanded feature tree<end> A.3 Case of Task Generation To ensure that the language model (LLM) does not consistently default to familiar or common content, we introduced strategy to guide the selection of features. From the sampled optional features, 20 certain features are designated as mandatory, and the LLM is directed to incorporate them into the scenario and task. Below is an example of how this approach is applied."
        },
        {
            "title": "Prompts for Task Generation",
            "content": "You are provided with set of features/keywords, and specific programming language. Your task is to use these inputs to conceive specific real-world application scenario that effectively integrates some of these features. Then, based on the scenario, formulate task or problem that needs to be addressed with code. Procedures: 1. Receive Inputs: These can range from technical specifics like data processing to broader concepts like system interaction. 2. Select Features: Choose combination of features from the provided set that can be realistically integrated into cohesive scenario. 3. Conceive Scenario: Think of practical application where the selected features play critical role. 4. Formulate Task Description: Based on the scenario, formulate task that needs to be addressed with code. The task should have certain level of difficulty and test the programmers coding skills through logical reasoning. Specific details, such as numerical values or environmental conditions, should be included to create well-defined setting for the task, ensuring the programmer doesnt need to guess any missing information. The task description should not include any code or detailed guidance on code implementation. 5. Generate an Instruction: Generate high-level instruction one or two sentence that describes the overall goal or problem to be solved, without diving into the specific implementation details. Enclose the selected features with <f> and </f>. Enclose the scenario with <s> and </s>. Enclose the task with <t> and </t>. Enclose the instruction with <i> and </i>. Inputs: Optional Features: {optional_features} Mandatory Features: {mandatory_features} Output: Features: <f>your answer</f> Scenario: <s>your answer</s> Task Description: <t>your answer</t> Instruction: <i>your answer</i> Optional Features: { \"dependency relations\": { \"time\": { \"time zones handling\": [] }, \"cv2\": { \"cvtColor\": [] } }, \"file operation\": { \"read configuration file\": [ \"read YAML configuration file\" ], \"write data to file\": [ \"write to CSV file\" ] }, \"workflow\": { 21 \"data augmentation\": [ \"audio augmentation\" ], \"validation\": [ \"check data integrity\" ] }, \"programming language\": \"Python\" } Mandatory Features: [\"read YAML configuration file\"] Selected Features: [\"read YAML configuration file, write to CSV file, check data integrity\"] Scenario: company wants to automate the generation of daily status report for its distributed servers. The configuration for each server (such as IP address, location, and operational status) is stored in YAML file. The system needs to read this configuration file, validate the data to ensure there are no missing or malformed entries, and then write the validated data to CSV file that can be used for further analysis. Task: Create function in Python that reads server configuration data from YAML file, validates the integrity of the data (checking for missing or malformed entries), and writes the validated data to CSV file. The YAML file will contain list of servers with their respective details such as IP address, location, and operational status. The function should ensure that all required fields are present and correctly formatted before writing the data to the CSV file. The CSV file should have columns for each server detail and only include entries that have passed validation. Instruction: Develop Python function that reads, validates, and exports server configuration data from YAML file to CSV file. A.4 Case of Code Generation To demonstrate the process of generating code based on the previously outlined task, we present the prompts used, the generated code, its test cases, and the debugging process leading to the final implementation. A.4.1 Prompt for Code Generation The prompt provided to the model specifies detailed structure for code generation, including the expected outputs and format."
        },
        {
            "title": "Code Generation Prompt",
            "content": "Now that you are code expert, have provided you with the QUESTION. Complete the problem with awesome code logic and give richly commented analysis in the code of your answer. Include the necessary packages and test cases. - QUESTION {task} - Full code implementation with test cases Enclose the python code with ```python and ``` and enclose the file name with <file> and </file>. For example: <file>add.py</file> ```python # add.py # Code implementation here def add(x, y): return + ``` The test code should be in single file. <file>test.py</file> Note that the following code will be executed directly, so only the test cases that can be executed directly need to be retained. You only need to test some simple functions in the code. Tests that depend on external files cannot be executed because these files do not exist. ```python from add import add def test(): assert add(3, 5) == 8 assert add(4, 6) == 10 test() ``` - File names in order and packages required Answer file names and packages in JSON format, wrapped in <json> and </json> tags. For example: <json> { } \"file_names\": [\"add.py\", \"test.py\"], \"packages\": [\"package1\", \"package2\"] </json> A.4."
        },
        {
            "title": "Initial Code Implementation",
            "content": "# server_config.py import yaml import csv import re def validate_ip(ip): \"\"\"Validate the IP address format\"\"\" pattern = re.compile(r\"^(?:[0-9]{1,3}.){3}[0-9]{1,3}$\") return pattern.match(ip) is not None def validate_server_data(server): \"\"\"Validate the server data\"\"\" required_fields = [ip_address, location, operational_status] # Check for missing fields for field in required_fields: if field not in server: return False 23 # Validate IP address format if not validate_ip(server[ip_address]): return False # Check operational status (assuming it should be string, non-empty) if not isinstance(server[operational_status], str) or not server[operational_status].strip(): return False return True def read_and_validate_yaml(yaml_file): \"\"\"Read and validate the YAML file\"\"\" with open(yaml_file, r) as file: try: data = yaml.safe_load(file) if not isinstance(data, list): raise ValueError(\"YAML file format is incorrect, expected list of servers.\") valid_servers = [server for server in data if validate_server_data(server)] return valid_servers except yaml.YAMLError as exc: print(f\"Error in YAML file: {exc}\") return [] except ValueError as ve: print(f\"Validation error: {ve}\") return [] def export_to_csv(servers, csv_file): \"\"\"Export validated server data to CSV\"\"\" if not servers: print(\"No valid server data to export.\") return keys = servers[0].keys() with open(csv_file, w, newline=) as file: writer = csv.DictWriter(file, fieldnames=keys) writer.writeheader() writer.writerows(servers) def process_server_config(yaml_file, csv_file): \"\"\"Main function to process server configuration from YAML to CSV\"\"\" valid_servers = read_and_validate_yaml(yaml_file) export_to_csv(valid_servers, csv_file) File 1: server_config.py # test_server_config.py import unittest import os from server_config import read_and_validate_yaml, export_to_csv, validate_ip class TestServerConfig(unittest.TestCase): def setUp(self): \"\"\"Set up the test environment\"\"\" self.valid_yaml_content = \"\"\" - ip_address: 192.168.1.1 location: Data Center 1 operational_status: operational - ip_address: 10.0.0.2 location: Data Center 2 operational_status: maintenance 24 \"\"\" self.invalid_yaml_content = \"\"\" - ip_address: 192.168.1.999 location: Data Center 1 operational_status: operational - ip_address: 10.0.0.2 location: Data Center 2 operational_status: \"\"\" self.mixed_yaml_content = \"\"\" - ip_address: 192.168.1.1 location: Data Center 1 operational_status: operational - ip_address: 10.0.0.999 location: Data Center 2 operational_status: maintenance \"\"\" with open(valid_servers.yaml, w) as file: file.write(self.valid_yaml_content) with open(invalid_servers.yaml, w) as file: file.write(self.invalid_yaml_content) with open(mixed_servers.yaml, w) as file: file.write(self.mixed_yaml_content) def tearDown(self): \"\"\"Clean up after tests\"\"\" os.remove(valid_servers.yaml) os.remove(invalid_servers.yaml) os.remove(mixed_servers.yaml) if os.path.exists(output.csv): os.remove(output.csv) def test_validate_ip(self): \"\"\"Test IP validation function\"\"\" self.assertTrue(validate_ip(192.168.1.1)) self.assertFalse(validate_ip(192.168.1.999)) self.assertFalse(validate_ip(invalid_ip)) def test_read_and_validate_yaml(self): \"\"\"Test reading and validating YAML data\"\"\" valid_servers = read_and_validate_yaml(valid_servers.yaml) self.assertEqual(len(valid_servers), 2) invalid_servers = read_and_validate_yaml(invalid_servers.yaml) self.assertEqual(len(invalid_servers), 0) mixed_servers = read_and_validate_yaml(mixed_servers.yaml) self.assertEqual(len(mixed_servers), 1) def test_export_to_csv(self): \"\"\"Test exporting data to CSV\"\"\" valid_servers = read_and_validate_yaml(valid_servers.yaml) export_to_csv(valid_servers, output.csv) with open(output.csv, r) as file: content = file.read() self.assertIn(ip_address,location,operational_status, content) self.assertIn(192.168.1.1,Data Center 1,operational, content) self.assertIn(10.0.0.2,Data Center 2,maintenance, content) 25 if __name__ == \"__main__\": unittest.main() File 2: test_server_config.py A.4.3 Debugging and Optimization Initial execution of the tests revealed several issues: The IP validation function incorrectly accepted some invalid IP addresses, such as 192.168.1.999. Edge cases in operational status validation were not handled properly. Given the error message and the original code, the LLM gives corrected implementation. # server_config.py import yaml import csv import re def validate_ip(ip): \"\"\"Validate the IP address format\"\"\" pattern = re.compile(r\"^(?:[0-9]{1,3}.){3}[0-9]{1,3}$\") if not pattern.match(ip): return False parts = ip.split(.) for part in parts: if not 0 <= int(part) <= 255: return False return True # The rest remains unchanged File 3: corrected server_config.py"
        },
        {
            "title": "B Appendix of Evaluation",
            "content": "B.1 Cross-File Dependency Benchmark The Cross-File Dependency Benchmark (XFileDep) is specialized standard benchmark designed to evaluate the performance of code generation models in handling cross-file dependencies. In many real-world programming scenarios, there exists complex web of dependencies between different code files. XFileDep provides comprehensive framework that tests models ability to generate missing files by considering multiple interdependent code files as context. This benchmark not only measures the models capability to generate individual isolated files but also assesses its proficiency in understanding and managing dependencies between files. As illustrated in Figure 9 , the process of constructing the benchmark consists of six distinct steps. Figure 9: The Sankey diagram for the creation of the XFileDep benchmark, with numbers indicating the quantity of data samples. Step 1: Data Sample Selection. From the initial cross-file dataset of 35,000 data samples constructed using our pipeline of synthetic data based on features, we filtered out cross-file data samples with fewer than 5 files (excluding any test files), resulting in 3,435 samples. Figure 10 displays the distribution of file counts and the average file length for each data sample. Figure 10: the distribution of file quantities and the average file length for each data sample. Step 2: Dependency File Selection. We utilized Abstract Syntax Trees (AST) to conduct dependency analysis and structural identification of the Python code files. AST allows parsing of the syntactic structure of Python files, enabling extraction of module import dependencies, function definitions, and class definitions. By parsing all code files within the data sample and identifying the collaboration between classes and functions, we documented the details of defined functions and classes along with the information on imported modules. With these capabilities, we were able to traverse the entire data sample, analyze the dependency relationships between files, discern key files, and select representative candidate file (to serve as the target file for the code generation task) that has substantial amount of cross-file dependencies. This approach allows us to generate structured data analysis report. The systematic nature of this analysis allows us to efficiently handle large cross-file data, providing clear dependency graphs and detailed information on code structure. We also filtered out data samples that lacked rich cross-file dependencies, retaining 2,934 samples that met the criteria. 27 Step 3: Filtering. We analyzed the runtime environment, required libraries, and external files (such as .npy, .xlsx, .json, .log) for each data sample. Based on this analysis, data samples that could not provide the necessary files or dependencies were filtered out. We also excluded data samples that had long runtimes or required the generation of specific output files, which made obtaining test results difficult. In addition, to increase the overall difficulty of the task and ensure that cross-file code generation operates at an appropriate file-level length, we filtered out candidate files whose length was less than 300 characters. Finally, we obtained total of 2,231 data samples. Step 4: Test Case Augmentation. To enhance the test coverage of the code within each data sample, we utilized GPT-4o to generate additional test cases. This process ensured that the core functional methods were robustly and comprehensively tested. In Table 9, we have compiled the statistical data before and after the augmentation of the test cases. We conducted runtime check on the data after augmenting the test cases and obtained 611 samples that successfully passed the test cases. The prompt for augmenting the test case with GPT-4o is illustrated as follows:"
        },
        {
            "title": "Prompts for Augmenting Test Cases",
            "content": "You are an expert in Python programming and test-driven development. have repository of Python code with corresponding test files aimed at verifying the correctness of my code. However, believe the current test cases are insufficient and need more comprehensive and robust test cases. Requirements: - The test cases should be written using suitable testing framework (e.g., pytest, unittest). - Maintain consistency with existing test structure and naming conventions. - Ensure that all new test cases pass before finalizing. Inputs: - The file structure (including filenames) and contents of the Python code. {python_code} - The file structure (including filenames) and contents of the current test files. {test_code} Deliverables: - Updated test files with additional test cases. - Documentation or comments within the test files explaining each new test case. Your output: Returns code content only. Table 9: Comparison of Test Functions and Test Cases before and after augmentation for 930 data samples."
        },
        {
            "title": "Total\nAverage per sample\nMax in file",
            "content": "3,837 4.13 12 6,010 6.46 20 7,933 8.53 23 14,305 15.38 44 Step 5: Iterative Test Code Refinement. For data samples that fail the test cases, the code content, test cases, and error information are extracted. Based on these detailed input descriptions, we utilize GPT-4o for checking and modification, and subsequently re-run the refined test code for validation. We performed single iteration of modification on the test code, resulting in 394 successful test cases out of 1620 samples. Finally we have sample of 1,005 that pass the test cases. The prompt for refining the test code with GPT-4o is illustrated as follows:"
        },
        {
            "title": "Prompts for Refining Test Code",
            "content": "You are an expert in Python programming and test-driven development. have repository of Python code with corresponding test files aimed at verifying the correctness of my code. However, there are errors in the current test cases and test code that would like you to proofread and correct. Requirements: - The test cases should be written using suitable testing framework (e.g., pytest, unittest). - Maintain consistency with existing test structure and naming conventions. - Ensure that all new test cases pass before finalizing. Inputs: - The file structure (including filenames) and contents of the Python code: {python_code} - The file structure (including filenames) and contents of the current test files: {test_code} - Program error messages caused by running the test file: {error_messages} Deliverables: - Fully correct test code file. - Documentation or comments within the test files explaining each new test case. Your output: Returns code content only. Step 6: Unsafe Filtering. To ensure the validity of our test cases, we constructed unit test environment based on the dependency requirements specified in each Python file. We then executed all test cases and filtered out any samples that failed the tests or presented unsafe operations, such as kill, terminate, rmtree, rmdir, and rm. This approach ensures that our canonical solution is absolutely correct. Finally, we retained 930 samples of cross-file data. Step 7: Annotation. We selected target files with extensive cross-file dependencies (either frequently invoked by other files or frequently invoking other files). Using GPT-4o, we annotated all classes and methods in these files with detailed documentation, emphasizing their purpose and functionality. The annotation process did not alter the original code, and the execution of the annotated files verified the correctness of the ground truth. The prompt for annotating the target file with GPT-4o is illustrated as follows:"
        },
        {
            "title": "Prompts for Annotating Target File",
            "content": "Your task is to read through the provided Python code and add detailed docstrings that describe the purpose and functionality of each class and function. Your additions should follow the PEP 257 conventions and should not alter the original code in any way. The docstrings should provide enough detail to help other developers understand what each part of the code does and how to use it appropriately. Here is the Python code: ```python {target_file_code} ``` Please add the necessary docstrings without changing the actual code. Ensure that output is enclosed with its corresponding tags: ```python [Your code here] ``` Step 8: Benchmark Construction. To maintain high level of difficulty in the benchmark construction, we extracted all code blocks from the functions and classes within the target files, leaving only the import statements, FunctionDef, ClassDef, and the corresponding docstrings. The instruction set provided the names and contents of all other files in the cross-file data sample as context and included the target files name and skeleton structure for the completion task. B.2 Case of File-Level Code Generation This section provides comprehensive details on file-level code generation. The directory structure of the example and the detailed contents of each file are as follows: --example_root/ main.py optimizer.py parser.py scraper.py search.py storage.py --tests/ test_main.py # main.py from scraper import Scraper from parser import Parser from storage import Storage from search import Search from optimizer import Optimizer def main(): urls = [ https://example.com/products, # Add more URLs as needed ] # Step 1: Scrape Data scraper = Scraper(urls) html_data = scraper.fetch_data() # Step 2: Parse Data parser = Parser() parsed_data = parser.parse(html_data) # Step 3: Store Data storage = Storage() storage.store_data(parsed_data) # Optional: Save to JSON storage.save_to_json(parsed_data) # Step 4: Optimize Code (Example usage) code = \"\"\" def example_function(): result = 2 + 2 return result \"\"\" optimized_code = Optimizer.optimize_code(code) # Step 5: Search Data data = storage.fetch_data() keyword = example 30 search = Search() results = search.breadth_first_search(data, keyword) print(f\"Search results: {results}\") if __name__ == __main__: main() File 4: main.py # optimizer.py import ast import compileall class Optimizer: # Example of constant folding optimization @staticmethod def constant_folding(code): tree = ast.parse(code) optimized_tree = ast.fix_missing_locations(tree) return compile(optimized_tree, filename=\"<ast>\", mode=\"exec\") @staticmethod def optimize_code(code): optimized_code = Optimizer.constant_folding(code) compileall.compile_code(optimized_code) return optimized_code File 5: optimizer.py # parser.py from bs4 import BeautifulSoup class Parser: @staticmethod def parse(html_data): parsed_data = [] for html in html_data: soup = BeautifulSoup(html, html.parser) products = [] for product in soup.select(.product): name = product.select_one(.product-name).text.strip() price = product.select_one(.product-price).text.strip() description = product.select_one(.product-description).text.strip() products.append({ name: name, price: price, description: description }) parsed_data.append(products) return parsed_data File 6: parser.py # scraper.py import requests from bs4 import BeautifulSoup class Scraper: def __init__(self, urls): self.urls = urls def fetch_data(self): html_data = [] for url in self.urls: try: response = requests.get(url) response.raise_for_status() html_data.append(response.text) except requests.RequestException as e: print(f\"Error fetching data from {url}: {e}\") return html_data File 7: scraper.py # search.py from collections import deque class Search: @staticmethod def breadth_first_search(data, keyword): queue = deque(data) results = [] while queue: item = queue.popleft() if keyword.lower() in item[name].lower() or keyword.lower() in item[description].lower(): results.append(item) return results File 8: search.py # storage.py import json import sqlite3 class Storage: def __init__(self, db_name=data.db): self.conn = sqlite3.connect(db_name) self.create_table() def create_table(self): with self.conn: self.conn.execute( CREATE TABLE IF NOT EXISTS products ( id INTEGER PRIMARY KEY, name TEXT, price TEXT, description TEXT ) ) def store_data(self, data): with self.conn: for products in data: for product in products: self.conn.execute( INSERT INTO products (name, price, description) VALUES (:name, :price, :description) , product) def fetch_data(self): cursor = self.conn.cursor() cursor.execute(SELECT name, price, description FROM products) return cursor.fetchall() def save_to_json(self, data, filename=data.json): with open(filename, w) as f: json.dump(data, f, indent=4) File 9: storage.py 32 # tests/test_main.py import unittest from scraper import Scraper from parser import Parser from storage import Storage from search import Search from unittest.mock import patch class TestWebScrapingApp(unittest.TestCase): @patch(requests.get) def test_scraper(self, mock_get): mock_get.return_value.status_code = 200 mock_get.return_value.text = <html><div class=\"product\"><span class=\"product-name\">Test Product</span><span class=\"product-price\">$10</span><span class=\"product-description\">Test Description</span></div></html> scraper = Scraper([https://example.com/products]) html_data = scraper.fetch_data() self.assertEqual(len(html_data), 1) def test_parser(self): html_data = [<html><div class=\"product\"><span class=\"product-name\">Test Product</span><span class=\"product-price\">$10</span><span class=\"product-description\">Test Description</span></div></html>] parser = Parser() parsed_data = parser.parse(html_data) self.assertEqual(len(parsed_data), 1) self.assertEqual(parsed_data[0][0][name], Test Product) def test_storage(self): storage = Storage(:memory:) data = [[{name: Test Product, price: $10, description: Test Description}]] storage.store_data(data) fetched_data = storage.fetch_data() self.assertEqual(len(fetched_data), 1) def test_search(self): data = [ {name: Test Product, price: $10, description: Test Description}, {name: Another Product, price: $20, description: Another Description} ] search = Search() results = search.breadth_first_search(data, Test) self.assertEqual(len(results), 1) self.assertEqual(results[0][name], Test Product) if __name__ == __main__: unittest.main() File 10: tests/test_main.py"
        },
        {
            "title": "C Appendix of Analysis",
            "content": "C.1 Leakage Threshold Setting Figure 11: Cases from the HumanEval benchmark dataset (left) and the evol-codealpaca-v1 dataset (right) with varying similarity. The embeddings are computed based on the \"output\" portions of the training dataset and the \"prompt + canonical_solution\" of the HumanEval benchmark data. We embedded multiple training datasets and different benchmark datasets and calculated the cosine similarity between them. Additionally, we analyzed the most similar training data sample for each test data point to identify potential data leakage issues. Using the evol-codealpaca-v1 dataset, which exhibited the most severe leakage, as case study, we found that the training data contained extremely serious data leakage. Figure 11 presents pairs of benchmark data (left) and training data (right) with various similarity scores. The index of the data samples presented in the case study is provided in Table 10. After observing and analyzing multiple samples, we manually selected similarity score of 0.9 as the similarity threshold. 34 Table 10: The index of the data samples presented in the case study."
        },
        {
            "title": "Similarity",
            "content": "99% 95% 90% 85% HumanEval index evol-codealpaca-v1 index 5 81260 47 45682 43 57 51167 C.2 Complexity Evaluation C.2.1 Detailed Metrics from the Software Engineering Perspective. We supplement Halstead-derived metrics in Table 11, which are based on unique operators (n1), unique operands (n2), total operators (N1), and total operands (N2). Among these recognized metrics, our data consistently shows significant performance gains compared to the current codebase. For instance, we achieve notable complexity advantages in program volume and program difficulty. The programming effort and estimated programming time further confirm that our data requires more time and effort to achieve. While the increased complexity may suggest higher potential for bugs, we address this issue by incorporating test cases during the data systhesis. Table 11: Derived Halstead metrics. These metrics are derived from unique operators (n1), unique operands (n2), total operators (N1), and total operands (N2). Vocabulary (n) Program Length (N) = n1 + n2 = N1 + N2 Dataset 26.55 76.61 81.03 75.61 157.34 280.22 13.05 26.91 28.54 28.43 54.97 84.51 Volume (V) Difficulty (D) = log2(n) = n1 2 N2 n2 5.07 10.76 10.49 8.75 12.34 13.64 108.39 381.45 416.78 381.32 932.78 2035.63 Programming Effort (E) = 1043.26 5954.64 6204.38 4528.98 13396.28 67851.94 Estimated Time (T) = 18 57.96 330.81 344.69 251.61 744.24 3769.55 Predicted Bugs (B) = 3000 0.03 0.09 0.09 0.08 0.17 0.28 Code Alpaca Evol CodeAlpaca CodeFeedBack OSS Instruct Ours (func-level) Ours (file-level) Dataset Code Alpaca Evol CodeAlpaca CodeFeedBack OSS Instruct Ours (func-level) Ours (file-level) In Table 12, we break down cyclomatic complexity to observe scores for specific operations, such as while, for, and boolean operations. Table 12 shows that our gains in Cyclomatic complexity are mainly due to the higher occurrence of if, for, except, and return statements. This suggests that our program handles more loops and incorporates broader range of exception handling scenarios. Table 12: Comparison of different control flow and logical operation frequencies. Dataset if while for Code Alpaca Evol CodeAlpaca CodeFeedBack OSS Instruct Ours (func-level) Ours (file-level) 0.42 1.35 1.59 1.38 2.29 3.60 0.06 0.15 0.14 0.07 0.16 0.38 0.43 0.68 0.76 0.59 1.14 1. and 0.03 0.15 0.19 0.16 0.16 0.25 or except return break continue bool_op 0.01 0.14 0.14 0.08 0.20 0.21 0.01 0.12 0.08 0.07 0.70 1.11 0.66 1.59 1.62 1.58 3.06 5.24 0.01 0.03 0.05 0.04 0.05 0. 0.00 0.02 0.02 0.01 0.10 0.10 0.05 0.29 0.33 0.24 0.35 0.45 We also break down the code strictness complexity scores. Table 13 shows that our data achieves significantly improvement in Doc Strings, indicating more comprehensive and rigorous consideration to code documentation. Additionally, we demonstrate clear advantages in exception handling, return value validation, and type hints, suggesting that our data is more standardized and stringent. 35 Dataset Table 13: Detailed metrics of code strictness complexity Type Hints Validation Verification Exception Handling Assertions Parameter Value Doc Strings Return Value Validation Code Alpaca Evol CodeAlpaca CodeFeedBack OSS Instruct Ours (func-level) Ours (file-level) 0.00 0.21 0.42 0.94 0.94 0.43 0.00 0.08 0.09 0.09 0.10 0.28 0.04 0.14 0.16 0.12 0.29 0.40 0.02 0.20 0.10 0.10 0.81 1.76 0.00 0.03 0.01 0.02 0.02 0. 0.06 0.01 0.05 0.08 2.45 1.74 0.07 0.15 0.14 0.15 0.34 0.80 C.2.2 Prompts for Evaluating Code Complexity using GPT-4o. We adopt the LLM-as-judge methodology, using GPT-4o to evaluate code complexity across four dimensions: Error Handling, Modularity, Data Structure Complexity, and Third-Party Dependencies. We define four levels of standards for each dimension and leverage GPT-4o to assign score to each sample based on these criteria. Detailed evaluation criteria and prompts are shown below. Prompts for Grading Data Structure Usage Complexity. You are an expert in evaluating complexity levels of data structure implementations for given code. Please provide single integer score from 2 to 8. Criteria: Score 2 for case: Basic data types only; Only use primitive types (int, string, etc.); Simple arrays or lists; No custom data structures. Score 4 for case: Basic data structures; Uses built-in collections (sets, maps); Simple combinations of basic structures; Basic object-oriented classes. Score 6 for case: Intermediate data structures; Custom data structures for specific needs; Efficient combination of multiple structures; Clear interfaces for data access. Score 8 for case: Advanced data structures; Specialized tree/graph structures; Optimized for operation requirements; Well-designed structure relationships Inputs: You are judging the following code: ## Begin Code ## {code} ## End Code ## Output Format: Please provide your evaluation in the following format: Grade: single integer within 2, 4, 6, and 8 36 Prompts for Grading Modularity Implementation Complexity. You are an expert in evaluating code architecture complexity, with special focus on modularity implementation patterns. Please provide single integer score from 2 to 8. Criteria: Score 2 for case: Code without any modular designs, all logic in single file with no clear separation. Score 4 for case: Code with minimal modularization, basic separation of logic but with tight coupling. Score 6 for case: Code with reasonable modularization and logical separation (e.g., some decoupling and partial adherence to design patterns, but limited scalability or reuse). Score 8 for case: Code implementation with clear and practical modularization (e.g., modules have distinct responsibilities, dependencies are simple and direct, and logical layers such as database, business logic, and user interface are separated. The design is easy to read, maintain, and extend for most real-world needs). Inputs: You are judging the following code: ## Begin Code ## {code} ## End Code ## Output Format: Please provide your evaluation in the following format: Grade: single integer within 2, 4, 6, and 8 Prompts for Grading Third Party Dependency Complexity. You are an expert in evaluating third-party dependency complexity of given code. Please evaluate the complexity of third-party library usage and dependencies in the following code and provide single integer score from 2 to 8. Criteria: Score 2 for case: No external library usage, only built-in modules (e.g., os, sys, json). Score 4 for case: Uses single third-party library with basic function calls (e.g., pandas, scipy, numpy). Score 6 for case: Uses 2-3 third-party libraries. Score 8 for case: Uses at least three third-party libraries with some interaction between them. Inputs: You are judging the following code: ## Begin Code ## {code} ## End Code ## Output Format: Please provide your evaluation in the following format: Grade: single integer within 2, 4, 6, and 8 37 Prompts for Grading Error Handling Complexity. You are an expert in evaluating error handling complexity of given code. Please provide single integer score from 2 to 8. Criteria: Score 2 for case: Complete lack of error handling. Score 4 for case: Basic error handling that prevents crashes. Score 6 for case: Basic error handling that prevents crashes with informative logging info. Score 8 for case: Error handling covers major scenarios. Inputs: You are judging the following code: ## Begin Code ## {code} ## End Code ## Output Format: Please provide your evaluation in the following format: Grade: single integer within 2, 4, 6, and 8 38 C.3 Diversity Evaluation C.3.1 Prompt for Feature Extraction. Prompts for feature extraction. Extract features from the provided code snippets, following the requirements for each category below, formatted in JSON structure. Responses in the following categories should be concise and organized in JSON format surrounded with <begin> and <end>. Categories may include nested structures if applicable. Here is an example of the expected format: <begin>{ \"functionality\": [ \"data processing\" ], \"computation operation\": { \"mathematical operation\":[ \"find common divisor\", \"base conversion\", \"prime factorization\" ], \"statistical calculations\":[ \"maximum\" ] }, \"data processing\": { \"data transformation\": [ \"drop rows\" ] }, \"data structures\": [ \"string\", \"list\", \"graph\", \"tree\" ], \"implementation logic\":[\"conditional\", \"loop\"] }<end> Categories to extract: 1. Programming Language: Note the programming language used. Example: [\"Python\", \"Java\"]. 2. Workflow: Outline the main steps and operations the code performs. Example: [\"data loading\", \"preprocessing\", \"model training\", \"evaluation\", \"results saving\"]. 3. Implementation Style: What programming paradigm the code follows. Example: [\"procedural\", \"object-oriented\", \"functional\"]. 4. Functionality: Explain the function of the code. Example: [\"data processing\", \"user interaction\", \"system control\"]. ... ... 16. Advanced Techniques: Specify any sophisticated algorithms or methods applied. Example: [\"Machine Learning\", \"Linear Regression\", \"Optimization Algorithms\"]. Requirements: 1. If the code snippet contains fewer than three lines, only extract the most precise and relevant single feature. 2. For function, provide no more than five features, prioritizing the most critical and distinctive aspects. 3. Only evaluate the code snippet; disregard any natural language descriptions or comments outside the code context. 4. Try not to let feature appear in multiple categories at the same time. Inputs: {source_code} Output Format: <begin> \"workflow\": [\"your answer\"], \"implementation style\": [\"your answer\"], \"functionality\": [\"your answer\"], \"resource usage\": [\"your answer\"], \"computation operation\": [\"your answer\"], \"security\": [\"your answer\"], \"user interaction\": [\"your answer\"], \"data processing\": [\"your answer\"], \"file operation\": [\"your answer\"], \"error handling\": [\"your answer\"], \"logging\": [\"your answer\"], \"dependency relations\": [\"your answer\"], \"algorithm\": [\"your answer\"], \"data structures\": [\"your answer\"], \"implementation logic\": [\"your answer\"], \"advanced techniques\": [\"your answer\"], <end> If the features of category cannot be directly extracted from the code, please set it to an empty list []. 39 C.3.2 Total Feature Diversity Comparison. Datasets Workflow Table 14: Distribution of total features across 1k samples. Resource Implementation Usage Style Computation Operation Functionality Security User Interaction 1842 3718 3550 3106 4004 4663 926 1018 1013 1015 1050 1244 1005 1260 1305 1192 1671 2629 324 560 598 421 831 1183 525 1432 1290 585 1213 8 50 60 49 227 123 181 379 325 278 542 1407 Data Processing 331 1354 1838 1163 3436 3160 Alpaca CodeFeedback Evol-Alpaca OSS-Instruct Ours (func-level) Ours (file-level) Datasets Alpaca CodeFeedback Evol-Alpaca OSS-Instruct Ours (func-level) Ours (file-level) File Operation Error Handling Logging Dependency Relations Algorithm Data Structures Implementation Logic Advanced Techniques 18 46 85 236 567 1453 77 421 354 394 887 1029 1 13 15 73 205 166 453 811 799 2499 4010 365 719 579 170 451 454 1309 1624 1661 1804 2297 2565 1143 1770 1480 1453 1866 2187 17 81 135 30 143 212 Avg. 8.24 14.90 15.10 12.77 21.89 27."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Tsinghua University",
        "Xiamen University"
    ]
}