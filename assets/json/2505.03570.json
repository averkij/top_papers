{
    "paper_title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
    "authors": [
        "Mariya Davydova",
        "Daniel Jeffries",
        "Patrick Barker",
        "Arturo Márquez Flores",
        "Sinéad Ryan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 0 7 5 3 0 . 5 0 5 2 : r OSUNIVERSE: BENCHMARK FOR MULTIMODAL GUI-NAVIGATION AI AGENTS Mariya Davydova*, Daniel Jeffries*, Patrick Barker, Arturo Márquez Flores, Sinéad Ryan Kentauros AI Inc. research@kentauros.ai"
        },
        {
            "title": "ABSTRACT",
            "content": "In this paper, we introduce OSUniverse: benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse. Keywords Artiﬁcial Intelligence AI Agents GUI Navigation Benchmark"
        },
        {
            "title": "1 Introduction",
            "content": "Multimodal GUI-navigation agents are AI systems that perceive graphical user interfaces (GUIs) (via vision and sometimes text) and interact with desktop using mouse, touch, or keyboard actions to perform tasks in similar way as human interacts with desktop. Purely multimodal agents are different from agents that do browser-based activities and rely on working with the underlying code, such as JavaScript DOM scraping to exact data from web pages Document Object Model (DOM), to augment their visual reasoning. While these code scraping agents, such as Browser Use [1], have proven effective for many browsing tasks, their capabilities do not carry over to desktop applications because these applications rely on that code for their action accuracy. During the last decade, many benchmarks have been proposed to evaluate multimodal agents, both in academia and industry. These benchmarks typically measure an agents task success rate or completion accuracy, sometimes speed / efﬁciency (e.g., time or steps taken to complete tasks), and robustness or generalization to new interfaces. However, most of these benchmarks fail to capture the richness and complexity of how humans interact with computers, complexity people take for granted because it seems so easy and intuitive to them. Yet complex app usage and GUI navigation remains incredibly challenging for machines, involving reasoning, visual perception, and precise action execution. GUI navigation is classic example of Moravecs paradox which essentially states that things that are easy for humans are difﬁcult for robots, and vice versa. Any mistakes can cause cascading errors into the task that are challenging for the agent to recover from, such as clicking out of long wizard-based sequence and needing to start over and redo many steps, or more serious errors like adding too many items to cart so user is overcharged for something they dont need in checkout sequence. Despite the growing popularity of these multimodal agent systems, there are very few benchmarks designed for them. In addition, these benchmarks fail to capture that nuance of human-computer interactions or reﬂect real-world tasks, *Equal contribution such as moving ﬂuidly between applications, synthesizing information from one application and converting or compressing it for use in other applications, or copying selective pieces of information over to new application. Even ofﬁce workers with minimal experience and training do incredibly complex tasks with ease, and there is need to capture these kinds of tasks better with benchmark. We highlight three well-known benchmarks in the list below. The ﬁrst two are web-based, while the third, OSWorld, is the only desktop-oriented benchmark. We also highlight some of the limitations of each of them and why new benchmark is needed to better capture the skills of human ofﬁce workers. WebShop (2022) [2]: An academic benchmark from Princeton that simulates an e-commerce website with 1.18M real products and more than 12k crowd-sourced instructions. An agent must parse text instruction (for example, \"ﬁnd and buy red dress under $50\"), then navigate through search pages, product listings, and item pages to ﬁnd and purchase the correct item. WebShop tests language grounding in realistic web setting, including query formulation and handling noisy text. Success is measured by whether the correct item is purchased (task success rate), and the benchmark also considers the number of steps (clicks/searches) as measure of efﬁciency. In the initial evaluation, the best learning-based agent had only 29% success rate, versus 59% for human experts on the same tasks. simple rulebased heuristic achieved 9.6%, highlighting the tasks difﬁculty. This gap between human performance underlines that agents still struggle with complex, multi-step web tasks. Mind2Web (2023) [3]: large-scale academic dataset (NeurIPS 2023) for generalist web agents, with 2,350 tasks collected from 137 real websites across 31 domains. Tasks are open-ended (e.g., making travel reservations, using social media) and come with crowd-sourced action sequences as demonstrations. Unlike MiniWoBs [4] simulated pages, Mind2Web uses actual websites (with HTML and dynamic content), aiming to test an agents ability to generalize to unseen websites and tasks. Performance is evaluated by task completion rate (did the agent reach the goal) and also intermediate checkpoint accuracy in some evaluations. Because real web pages are large, solutions often involve ﬁrst ﬁltering the DOM or screenshot before feeding it to model. Initial baselines using large language models (LLMs) showed moderate success. For example, GPT-4 based agent achieved about 48.8% task completion on live subset of Mind2Web, but with only 23.1% strict success rate when requiring every intermediate step to be correct. The authors note there is substantial room to improve towards truly generalizable agents.\" Mind2Web also introduced robustness tests e.g., evaluating on entirely new websites or domains the agent never saw in training to gauge generalization. OSWorld (2024) [5]: recent academic benchmark (NeurIPS 2024) that provides full-ﬂedged computer environment (a real Operating System (OS)) for evaluating multimodal agents. OSWorld includes suite of 369 diverse tasks on Ubuntu and Windows, spanning desktop application control, ﬁle operations, web browsing, and multi-app workﬂows. Each task is derived from real use cases (with detailed initial state and an automated evaluation script to check success). For example, tasks include sending an email via mail client, editing spreadsheet, or downloading and opening ﬁle from the Web. Evaluation is execution-based the agent must actually carry out the steps in virtual machine and performance is measured by success or failure on each task (with all-or-nothing criteria deﬁned by the script). Human testers can solve about 72.4% of the OSWorld tasks, whereas the best AI agent (a state-of-the-art vision-language model augmented for actions) succeeded on only 12.2% of tasks at the time of the OSWorld paper publication. At the time of publication of this article, the best reported result in OSWorld is 38.1%. This stark gap indicates current agents struggle with complex GUI grounding and procedural knowledge in general computer use. Since its publication, OSWorld has become renowned as very hard benchmark that proves incredibly challenging for even SOTA GUI-navigation AI agents. However, while this benchmark deserves credit for providing challenging set of tasks and being the ﬁrst to test both browsing and OS capabilities, it has several major shortcomings, which we highlight below: It is challenging to run this benchmark; the environments are executed in proprietary VMWare Workstation or VirtualBox virtual machine (or via their Docker support, which simply runs the KVM VM inside Docker), which makes each run quite resource intensive and challenging to set up. The benchmark is largely limited to ReACT-style agents and it is hard to implement any other kind of agent, which limits its usefulness as SOTA agent may have breakthrough that does not use ReACT-style framework: the benchmark executor accepts only one agent interaction model and does not allow editing the agent architecture or even prompts; it is possible to write custom executors and inject that into the benchmark architecture; however, this possibility is not documented and therefore requires lot of effort to ﬁgure out and implement through reverse engineering, which is not ideal. The tests require deterministic validation; as many of the tests deal with real-life dynamic environments (like websites of various companies), deterministic validation is prone to errors, thus adding noise to the benchmark scores. In addition, many useful agentic use cases that are worth testing simply cannot be tested deterministically (for example, searching for accommodation, ﬂights, or goods). 2 Finally, the prompts for the benchmark are vague and require substantial amount of inductive and deductive reasoning for machine to even interpret what to do, much less perform its task. While it is valid to test the reasoning of models, GUI navigation benchmark should have clear, precise tasks laid out for the model and not require massive intuitive leaps to even understand the prompt. If person were given such prompt, they would be able to ask clarifying questions, or research steps to take to achieve their goal, both of which are not possible for the model in the benchmark, which signiﬁcantly hinders its performance, resulting in much lower scores not because model cannot do the task in many cases, but because it cannot understand the prompt. We designed OSUniverse to overcome these limitations and provide robust framework for the next generation of GUI-navigation agents."
        },
        {
            "title": "2 Overview",
            "content": "In the domain of benchmarking the GUI-navigation AI Agents, there are several principal components: The environment where the agent is operating: in most cases, we are talking about virtual machines (such as KVM or VMware Workstation) or containers (such as Docker) with pre-installed software, but some agents run directly on the users machine as well. The action space: the kinds of action an agent can take. GUI navigation involves mouse and keyboard operations. The observation space: the kinds of signal an agent can receive from the environment. As we are talking about general-purpose GUI navigation, we currently rely on screenshots alone, though in the future these systems may rely on continuous video streams. However, some more specialized agents may rely on additional information, such as the DOM of the web page, as noted earlier. The agentic architecture: the way in which the agent is built; one of the most popular architectures today is ReACT, but there are many others. The agentic model(s): as there are many possible architectures, there are also many possible machine learning models (typically visual LLMs or omni-models though other architectures like ConvNets are possible) to power those architectures as the \"brain\" of the agent; it is worth noting that some models work better than others for certain architectures, and also there are architectures that require multiple models at once (multiagent consensus architectures) and there are architectures that use one model. The agentic runtime: the actual framework that runs the agent in the environment. The validator: This component is crucial for ensuring the stability and representability of the use-cases. The majority of the benchmarks out there lock almost all of these components, allowing someone to choose only the agentic model, such as Claude 3.5 or OpenAIs o3 or Llama 3.2. In reality, though, even the agentic models are not perfectly substitutable inside one architecture, as there are nuances to strong and efﬁcient prompting from model to model, and some models are better or worse at tool calling, memory usage or returning accurate JSON, to name few of the challenges. Benchmarking custom architectures becomes signiﬁcant issue with this level of variability in both the architectures and the models themselves. In OSUniverse, our aim is to support as many combinations of the components as possible, by abstracting out communication to the desktop environment to allow the most ﬂexibility for testing: The default environment is AgentDesk: framework that allows virtual desktops to run in Docker containers. However, one may prefer to use different environment; in this case, using the suggested Docker image as base is recommended. The action space is connected to the environment. The custom agent implementations (see below) contain converters between the agent action space and the AgentDesk action space. The observation space is locked for screenshots only. The default agentic runtime is SurfKit [6]: we support running any SurfKit-compatible agents from the YAML conﬁg or from the Docker image. However, one may implement custom runtime and use it, as long as the artifacts of the agent test case runs are the ones expected by the validator (see below). For the validation, we offer automatic validation (which uses Gemini models due to the size of their context window and their excellent reasoning capabilities) and human validation (we provide handy Streamlit UX to go through the test case runs and mark them as successes or failures) for precision accuracy. 3 The agentic architecture is the most open-ended component of our benchmark. user may run benchmark with: One of the reference agents we provide: Claude-based ReACT agent * Requires Anthropic API key QWEN 2.5 VL ReACT agent * Requires Alibaba API key CUA (Computer Use Agent from OpenAI) ReACT agent * Requires OpenAI API key baseline ReACT agent compatible with any model that can return structured JSON output Any SurfKit-compatible agent Any other agent that is wrapped into the Runner interface (see below)."
        },
        {
            "title": "3 Architecture",
            "content": "The architecture of OSUniverse is based on running tests in Docker containers and validating them using the Gemini 2.0 (gemini-2.0-ﬂash-001) or 2.5 (gemini-2.5-pro-preview-03-25) model with their long context windows. The benchmark consists of several components: Test cases: the test units are deﬁned in YAML ﬁles and contain all the information required to run and validate the test: the environment container and the setup, the task, and the checks (detailed next). The test cases are also divided by categories and levels, which are described in the following. Checks: the validation units. Each test case has to contain at least one check; the amount of checks is unlimited, and the test is only considered pass if ALL checks pass. The following checks are available: ReturnedResultCheck: compares the result an agent returned (as text) with the expected result. This type of check is perfect for Q&A-type tests: for example, an agent has to look up some information and return it to the user. FinalScreenshotCheck: compares the ﬁnal state screenshot with the description of the expected state. This type of check ﬁts use cases where the ﬁnal state of the screen clearly deﬁnes the success criteria: for example, an agent has to open certain application or ﬁle. CommandOutputCheck: compares the output of an arbitrary bash command with the description of the expected result. This type of check works very well when the result of the agents work can be represented in structured format: for example, if an agent has to edit an ODT ﬁle, we can convert it to Markdown in check and send this Markdown to the validator. ExpectedFlowCheck: compares the complete ﬂow (all steps with images, actions, and comments) with the description of the expected ﬂow. This type of check is required for complex scenarios that involve going through several stages: for example, an agent may have to open terminal, do some work in it, and close the terminal. Runners: the modules that take the TestCase and produce the TestCaseRun. This data structure contains all the information about the test case, plus conﬁguration data, plus the execution trajectory (the sequence of actions and related desktop screenshots). The following runner is supported: SurfKitAgentRunner runs any SurfKit-compatible agent, either from the YAML conﬁguration or from Docker image. This runner creates the AgentDesk desktop in Docker container, runs the setup commands from the test case conﬁguration, executes the task with given agent, and generates the trajectory from the agent actions and threads (depending on the data that the agent produces during the run). Validators: the modules that take the TestCaseRun and score them. The following validator is supported: COTGeminiValidator uses Gemini as scoring model and introduces checklist-based thinking on the check result. Benchmark: the module that orchestrates the running and validation of given set of test cases. Allows parallel execution of test cases. As each test case is completely independent from other test cases, we can parallelize the process as much as our resources permit. Viewer: the Streamlit application that allows reviewing the runs, including the trajectories and scores, assigning human score (optionally) and observing the summary information about the benchmark run."
        },
        {
            "title": "4 Classiﬁcation of Test Cases",
            "content": "We classify the tests using two dimensions: the level and the category. The category is the core application that this test case addresses: browser, desktop, terminal, games, gimp, LibreOfﬁce application, or combination of applications, which we refer to as \"multiapp.\" The level is the measure of the complexity of the test. To pass tests of certain level, the agent should possess speciﬁc skills, which become increasingly complex at each level. However, all tasks remain simple for an average white-collar ofﬁce worker. Table 1 presents the requirements for each level, the weight of the tasks, the maximum recommended number of steps per task execution, and the number of tests of this level present in the benchmark."
        },
        {
            "title": "Paper",
            "content": "Wood"
        },
        {
            "title": "Weightlevel Max Steps",
            "content": "# of Tests 0.5 5 11 See the screen through screenshots Understand the task Return the result corresponding to the task The current screen represents the state (no scrolling or opening popup windows required) 1 25 58 Identify positions of clearly deﬁned GUI labels, elements (buttons, inputs, icons, etc) and interact with them text Short quick tasks Bronze 2 50 The tasks take longer than on Wood level Positioning and interacting with calendars and grids Scrolling or opening dialog windows may be required (i.e. part of the state is hidden) 5 Level (cont)"
        },
        {
            "title": "Weightlevel Max steps",
            "content": "# of Tests 4 75 32 Tasks have several distinct subtasks Interacting with several applications Accumulating the information across the run and returning the summary to the user (i.e. the result is not visible on the last screenshot) Advanced interaction with GUI elements (drag-and-drop, selecting the parts of the text/tables) Independent resolution of potential problems (e.g. user gives incorrect instruction) Gold 8 100 11 Massive, open-ended tasks requiring reasoning, information accumulation, many steps Drawing with the mouse Real-time interaction with GUI playing games that require reaction) (e.g. May include multiple app interactions (like creating spreadsheet and incorporating it into word doc as dynamic element via linking) Table 1: Task Levels and Their Requirements These levels represent the current state of the agents. The SOTA agents (at the time of writing) are expected to go through Paper and Wood with minimal issues, complete most of the Bronze tasks, show moderate performance on the Silver ones, and mostly fail on the Gold level. To reﬂect the fact that these tasks vary signiﬁcantly in complexity, we introduce the weighted score, which is deﬁned as follows. Score = Plevel{Paper, Wood, Bronze, Silver, Gold} (cid:16) Weightlevel(cid:17) Plevel{Paper, Wood, Bronze, Silver, Gold} (Nlevel Weightlevel) passed level 100% where Nlevel is amount of tests of the given level of complexity; Npassed level Weightlevel is deﬁned in Table 1. is amount of tests of the given level of complexity that an agent passed;"
        },
        {
            "title": "5 SOTA Agents Scores",
            "content": "We run the benchmark against 8 conﬁgurations of agents and multimodal LLMs, as described in Table 2. In all cases, the maximum number of steps is limited per level, as described in Table 1. 6 computer-use-preview-2025-03-11 claude-3-5-sonnet-20241022(computer-use) claude-3-5-sonnet-20241022(agentdesk) qwen2.5-vl-72b-instruct gemini-2.5-pro-exp-03-25 gemini-2.0-ﬂash-001 gpt-4o-2024-11-20 gemini-1.5-pro-002 9.59 8. 6.79 6.12 0 5 10 47.8 28.36 23.44 18."
        },
        {
            "title": "20\nWeighted Score (%)",
            "content": "30 35 40 45 50 Figure 1: Agent Performance Table 3 represents the corresponding performance, cost and time agents needed to run through all tests (calculated from the moment an agent starts work, not counting the time to setup the environment). Both Table 2 and 3 show the best runs for each agent. Tables 4, 5, 6, and 7 show statistics of multiple runs of each of the top performing agents. Figures 1, 2 and 3 represent the same data visually. Agent Paper Wood Bronze Silver Gold Total Score Computer Use Agent with computer-use-preview-2025-03-11 100.00% 86.21% 75.00% 34.38% 9.09% 47.80% Claude Computer Use with claude-3-5-sonnet-20241022 AgentDesk-based ReACT with claude-3-5-sonnet-20241022 QWEN-based ReACT with qwen2.5-vl-72b-instruct AgentDesk-based ReACT with gemini-2.5-pro-exp-03-25 AgentDesk-based ReACT with gemini-2.0-ﬂash-001 AgentDesk-based ReACT with gpt-4o-2024-11AgentDesk-based ReACT with gemini-1.5-pro-002 100.00% 53.45% 43.75% 21.88% 0% 28.36% 90.91% 56.90% 39.58% 9.38% 0% 23.44% 90.91% 46.55% 31.25% 6.25% 0% 18.64% 90.91% 15.52% 18.75% 3.12% 0% 9.59% 90.91% 13.79% 14.58% 3.12% 0% 8.26% 100.00% 6.90% 12.50% 3.12% 0% 6.79% 90.91% 3.45% 12.50% 3.12% 0% 6.12% Table 2: Agent Performance 7 computer-use-preview-2025-03-11 claude-3-5-sonnet-20241022(computer-use) claude-3-5-sonnet-20241022(agentdesk) qwen2.5-vl-72b-instruct gemini-2.5-pro-exp-03gemini-2.0-ﬂash-001 gpt-4o-2024-11-20 gemini-1.5-pro-002 computer-use-preview-2025-03-11 claude-3-5-sonnet-20241022(computer-use) claude-3-5-sonnet-20241022(agentdesk) 0 10 20 30 40 Paper 50 Score (%) Bronze 43.75 39.58 qwen2.5-vl-72b-instruct gemini-2.5-pro-exp-03gemini-2.0-ﬂash-001 gpt-4o-2024-11-20 gemini-1.5-pro-002 31.25 18.75 14. 12.5 12.5 100 100 100 90.91 90.91 90.91 90.91 computer-use-preview-2025-03-11 claude-3-5-sonnet-20241022(computer-use) claude-3-5-sonnet-20241022(agentdesk) qwen2.5-vl-72b-instruct gemini-2.5-pro-exp-03-25 gemini-2.0-ﬂash-001 15.52 13. gpt-4o-2024-11-20 gemini-1.5-pro-002 6.9 3.45 Wood 53. 56.9 46.55 86.21 60 70 90 100 0 10 20 40 60 70 80 90 50 Score (%) Silver 75 computer-use-preview-2025-03-11 claude-3-5-sonnet-20241022(computer-use) claude-3-5-sonnet-20241022(agentdesk) qwen2.5-vl-72b-instruct gemini-2.5-pro-exp-03-25 gemini-2.0-ﬂash-001 gpt-4o-2024-11-20 gemini-1.5-pro-002 34. 21.88 9.38 6.25 3.12 3.12 3. 3.12 0 10 20 30 50 Score (%) Gold 60 70 80 100 0 10 20 30 50 Score (%) 60 70 80 90 computer-use-preview-2025-03-11 9.09 claude-3-5-sonnet-20241022(computer-use) claude-3-5-sonnet-20241022(agentdesk) qwen2.5-vl-72b-instruct gemini-2.5-pro-exp-03gemini-2.0-ﬂash-001 gpt-4o-2024-11-20 gemini-1.5-pro-002 0 0 0 0 0 0 0 20 30 40 50 Score (%) 60 80 90 100 Figure 2: Agent Performance Across All Levels Agent Tokens In Tokens Out Total Score Cost ($) Duration (s) Computer Use Agent with computer-use-preview-2025-03-11 40.27M 0.25M 47.80% 124 Claude Computer Use with claude-3-5-sonnet-20241022 AgentDesk-based ReACT with claude-3-5-sonnet-20241022 QWEN-based ReACT with qwen2.5-vl-72b-instruct AgentDesk-based ReACT with gemini-2.5-pro-exp-03-25 AgentDesk-based ReACT with gemini-2.0-ﬂash-001 AgentDesk-based ReACT with gpt-4o-2024-11AgentDesk-based ReACT with 23.11M 0.24M 28.36% 14.76M 0.39M 23.44% 30.27M 0.47M 18.64% 12.90M 0.82M 9.59% 36.24M 0.89M 8.26% 28.82M 0.62M 6.79% 73 49 63 24 78 26611 29335 96500 30997 50337 8 Agent (cont) gemini-1.5-pro-002 Tokens In Tokens Out Total Score Cost ($) Duration (s) 15.96M 0.41M 6.12% 23 29673 Table 3: Agent Performance vs Cost vs Time Run #1 #2 #3 Mean Std. Dev. Paper Wood Bronze Silver Gold Total Score 100.00% 86.21% 75.00% 34.38% 9.09% 100.00% 86.76% 75.00% 34.38% 0.00% 90.00% 84.48% 70.83% 21.25% 0.00% 96.67% 85.82% 73.61% 30.00% 3.03% 7.58% 5.25% 5.77% 2.41% 1.19% 47.80% 45.14% 43.07% 45.34% 2.37% Table 4: Computer Use Agent from OpenAI: Multiple Runs Run #1 #2 #3 Mean Std. Dev. Run #1 #2 #3 Mean Std. Dev. Paper Wood Bronze Silver Gold Total Score 100.00% 53.45% 43.75% 21.88% 0.00% 100.00% 53.45% 43.75% 12.50% 0.00% 100.00% 55.17% 39.58% 12.50% 0.00% 100.00% 54.02% 42.36% 15.63% 0.00% 5.42% 0.00% 2.41% 0.00% 0.99% 28.36% 25.17% 24.37% 25.97% 2.11% Table 5: Claude Computer Use Agent: Multiple Runs Paper Wood Bronze Silver Gold Total Score 100.00% 55.17% 35.42% 12.50% 0.00% 9.38% 0.00% 100.00% 53.45% 35.42% 9.38% 0.00% 90.91% 56.90% 39.58% 96.97% 55.17% 36.81% 10.42% 0.00% 1.80% 0.00% 5.25% 2.40% 1.73% 23.30% 21.97% 23.44% 22.90% 0.81% Table 6: Claude Computer Use with AgentDesk action space: Multiple Runs Run #1 #2 #3 Paper Wood Bronze Silver Gold Total Score 90.91% 46.55% 31.25% 6.25% 0.00% 90.91% 48.28% 29.17% 6.25% 0.00% 90.91% 48.28% 29.17% 6.25% 0.00% 18.64% 18.38% 18.38% 18.47% 0.15% Mean Std. Dev. 90.91% 47.70% 29.86% 6.25% 0.00% 1.20% 0.00% 0.00% 0.00% 1.00% Table 7: QWEN-based Agent: Multiple Runs We can share some observations and draw some conclusions. 9 ) ( C 100 80 60 40 20 computer-use-preview-2025-03gpt-4o-2024-11-20 claude-3-5-sonnet-20241022(computer-use) qwen2.5-vl-72b-instruct claude-3-5-sonnet-20241022(agentdesk) gemini-2.5-pro-exp-03-25 gemini-1.5-pro0 0 gemini-2.0-ﬂash-001 5 10 20 25 Weighted Score (%) 30 35 40 50 Figure 3: Agent Performance vs Cost Computer Use Preview from OpenAI is undoubtedly very spectacular model with lot of potential for GUI navigation. At the same time, it shows the highest instability in the results among the top four agents. The agentic code in the benchmark is minimalistic and relies on the models and the sample codes from the corresponding providers (if present). This means that more elaborate agents based on this model are likely to score much higher on this benchmark, around 70-80%. We observed quite few cases in which model ran out of steps before completing the task. Relaxing the steps limits will also improve the score. At the same time, Computer Use Preview is the most expensive and slow model due to reasoning capabilities that are part of its decision-making. Speed in real-world scenarios is major factor for strong GUI-navigation agent and may be taken into account in future versions of the benchmark. All tested models demonstrate the ability to read the data in the screenshot and to return the result in the expected format. All the top models (Computer Use Preview, Claude Computer Use, and QWEN 2.5 VL 72B) are specifically trained for GUI navigation, and each of those models have their own action space. That is why we implemented specialized agents for each of these models. At the same time, we can see that strong model like Claude Computer Use can generalize well enough to operate in different action space. The results in non-native action space are slightly worse, but comparable, offering hints at the future of GUI navigation models and their potential for generalization. Gemini 2.0 Flash is the cheapest model on this list; although the results are not particularly good, there is potential to ﬁne-tune the model and introduce reasoning at inference time to an agent. QWEN 2.5 VL 72B is the only open-weight model on the list. This points to problem where open source and open weights models have focused almost exclusively on text and multimodal models are only beginning to take focus in the open source and open weights space. There is distinct lack of GUI-navigation-oriented multimodal models on the market today. There is large potential to ﬁne-tune open source and open weight models that is lacking in proprietary models, which offers the tantalizing prospect of signiﬁcantly enhancing the performance on GUI reasoning and GUI navigation tasks. For instance, much of the SOTA work on models has shifted to Reinforcement 10 Learning (RL), kicked off by the rise of DeepSeek [7] R1 [8]. Currently most proprietary models are lagging in their ability to be ﬁne-tuned with SOTA RL training methodologies such as GRPO (Group Relative Policy Optimization) and DAPO (Decoupled Clip and Dynamic sAmping Policy Optimization) [9] and REINFORCE++ [10]. GPT models can only be tuned with Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO). Many proprietary models cannot be ﬁne tuned at all (many of the Claude models and the OpenAI o1-o4 line). We expect proprietary models to continue to lag in SOTA ﬁne-tuning due to the time and cost to keep up with the sheer range of available tuning possibilities and this is likely to give open source and open weights models signiﬁcant advantage in the future on GUI navigation tasks. It is also worth noting that QWEN 2.5 VL 72B is the most stable model across the top four agents and demonstrates very low variation in the results. The main conclusion we draw is that the best performing GUI-navigation agents are based on the models that are trained speciﬁcally for this use case. As consequence, these models have their own action space and need custom agentic code to demonstrate their full potential."
        },
        {
            "title": "6 Remarks on Automated Validation",
            "content": "When we designed this benchmark, our goal was to provide enough diversity among tasks and ensure that we are not limited by the tasks that can be validated deterministically (like ﬁnding very particular fact or creating very particular ﬁle). The majority of tasks we face in the digital environment are not deterministic: we search for data that depends on time and location; we use various versions of tools to produce the results that comply with our requirements; we ﬁnd solutions for problems that imply many possible solutions, etc. As one might expect, nondeterministic solutions are hard to validate automatically; an obvious approach here is to use human validation. However, the issue is that human validation is not scalable: If one reruns the benchmark reasonably often while improving their agent, validating the results manually quickly becomes tedious and time-consuming task. To address this problem, we introduce an automated validator based on Gemini models. We chose the Gemini family because of the combination of multi-modality and context window size. This combination becomes crucial for validating the ExpectedFlowChecks described in Section 3, because to validate result we have to send the LLM all the images from all states of the run trajectory, which can quickly saturate small context window. To measure the effectiveness of automated validation, we performed human review of each of the agent runs and counted the number of tests in which the human verdict is different from the AI verdict. This number, divided by the total number of tests (160), is called the disagreement rate. In Table 8 you can see the disagreement rate for two models: Gemini 2.0 Flash (gemini-2.0-ﬂash-001), which we use as the main validation model, and Gemini 2.5 Pro Preview (gemini-2.5-pro-preview-03-25), which we tested as an alternative. To keep the estimation fair, we calculated the disagreement rate in eight runs from different combinations of agents and models. It is important to note that: Gemini 2.5 Pro Preview has noticeable rate limits at the time of writing; therefore, using it for benchmark run is not reliable enough. Gemini 2.5 Pro Preview is slightly different from Gemini 2.0 Flash in terms of accuracy and details. One cannot drop-in replace the latter with the former; extra work with the prompts is required. Run # Gemini 2.0 Flash Gemini 2.5 Pro Preview 1 2 3 4 5 6 7 8 1.250% 3.125% 0.000% 0.000% 0.625% 0.000% 1.250% 2.500% 1.250% 1.875% 1.250% 2.500% 1.875% 0.625% 2.500% 1.250% 11 Run # (cont) Gemini 2.0 Flash Gemini 2.5 Pro Preview"
        },
        {
            "title": "Average",
            "content": "1.64% 1.09% Table 8: Validation Disagreement Rates You can see that the average disagreement rate during our experiments was 1.64%, which allows us to draw very reliable conclusions about the performance of agentic models without having to manually validate the runs. The latest Gemini model shows even better results. We will keep using Gemini 2.0 Flash until Gemini 2.5 Pro Preview goes out of preview mode and can be used at scale."
        },
        {
            "title": "7 Conclusions and Future Work Directions",
            "content": "Our core contributions in this paper are as follows: Presenting new benchmark for multimodal desktop-oriented GUI-navigation AI agents. Providing nondeterministic mechanism of automated validation with an error rate less than 2%. Measurement of performance of several top proprietary and open-weight models. It is clear that this is only the beginning. There is lot of development that can be done to improve both the benchmark and the agents. The current SOTA is slightly below 50%, and we may expect this benchmark to be saturated in the middleterm future. We need to create more tests, especially in the Silver and Gold categories, which would signiﬁcantly increase the failure rate for todays best models. We will leave this to version 2 of the benchmark, as these tests are challenging to create and we wanted to release strong baseline benchmark as foundation for later work. We also need to cover more applications and use cases. The current set of tests is limited by applications and websites that can be used without setting up accounts. More diversity is needed. Our agentic implementations represent the baseline; it is obvious that better prompting, shortand longterm memory, additional tools, and other applied AI approaches could meaningfully improve the agents performance. The source code of the benchmark is available at https://github.com/agentsea/osuniverse. You can ﬁnd all the prompts and instructions there. All of the work in this paper is designed to be reproducible. If you encounter issues running the benchmark, please let us know. We highly encourage the community to contribute tests and implementations of agents."
        },
        {
            "title": "References",
            "content": "[1] Magnus Müller and Gregor Žuniˇc. Browser use: Enable ai to control your browser, 2024. [2] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. WebShop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206, 2022. [3] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2Web: Towards generalist agent for the web. In NeurIPS 2023 Datasets and Benchmarks, 2023. [4] Peter Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, and Timothy Lillicrap. data-driven approach for learning to control computers, 2022. [5] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972, 2024. 12 [6] Kentauros AI. Surfkit: toolkit for building and sharing ai agents that operate on devices. https://github.com/agentsea/surfkit, 2024. Accessed: 2025-04-30. [7] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [9] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. [10] Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efﬁcient rlhf algorithm with robustness to both prompt and reward models, 2025."
        }
    ],
    "affiliations": [
        "Kentauros AI Inc."
    ]
}