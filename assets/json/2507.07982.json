{
    "paper_title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling",
    "authors": [
        "Haoyu Wu",
        "Diankun Wu",
        "Tianyu He",
        "Junliang Guo",
        "Yang Ye",
        "Yueqi Duan",
        "Jiang Bian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 2 8 9 7 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Geometry Forcing",
            "content": "GEOMETRY FORCING: MARRYING VIDEO DIFFUSION AND 3D REPRESENTATION FOR CONSISTENT WORLD MODELING Haoyu Wu1, Diankun Wu2, Tianyu He1, Junliang Guo1, Yang Ye1, Yueqi Duan2, Jiang Bian1 1Microsoft Research 2Tsinghua University"
        },
        {
            "title": "ABSTRACT",
            "content": "Videos inherently represent 2D projections of dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the models intermediate representations toward geometry-aware structure by aligning them with features from pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera viewconditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io. Figure 1: Geometry Forcing equips video diffusion models with 3D awareness. (a) We propose Geometry Forcing (GF), simple yet effective paradigm to internalize geometric-aware structure into video diffusion models by aligning with features from pretrained geometric foundation model, i.e., VGGT (Wang et al., 2025). (b) Compared to the baseline method (Song et al., 2025), our method produces more consistent generations both temporally and geometrically. (c) Features learned by the baseline model fail to reconstruct meaningful 3D geometry, whereas our method internalize 3D representation, enabling accurate 3D reconstruction from the intermediate features. Equal contribution. Project lead."
        },
        {
            "title": "Geometry Forcing",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Learning to simulate the physical world and predict future states is cornerstone of intelligent systems (Ha & Schmidhuber, 2018). Recent advances in generative modeling (Ho et al., 2020; Rombach et al., 2022; Peebles & Xie, 2023; Brown et al., 2020), coupled with the availability of largescale video datasets, have led to significant progress in generating realistic visual environments conditioned on text descriptions (OpenAI, 2024; Yang et al., 2024; Polyak et al., 2024; Google, 2025) or agent actions (Hu et al., 2023; Guo et al., 2025; Decart et al., 2024; Bar et al., 2025). However, these approaches typically aim to model pixel distributions across video frames, overlooking fundamental principle: videos are 2D projections of dynamic 3D world (Glassner, 1989). By focusing solely on image-space generation, such models often struggle to maintain geometric coherence and long-term consistency, particularly in autoregressive settings where small errors can accumulate over time (Chen et al., 2024a; Cheng et al., 2025; Huang et al., 2025b). Building on this motivation, growing line of research has explored explicitly modeling the dynamic 3D structure of the physical world (Niemeyer & Geiger, 2021; Zhu et al., 2024; Aether et al., 2025; Zhang et al., 2025a; Mai et al., 2025; Jiang et al., 2025), as opposed to implicitly learning distributions in 2D pixel space. For example, Zhang et al. (2025a) proposes transforming 3D coordinates into images and jointly modeling the RGB and geometric information using diffusion models. While effective to some extent, representing 3D information in tractable form remains challenging, and the reliance on additional annotations imposes limitations on scalability. In this work, we aim to bridge the gap between video diffusion models and the underlying dynamic 3D structure of the physical world. We begin with fundamental question: Can video diffusion models implicitly learn 3D information through training on raw video data, without explicit 3D supervision? To investigate this, we analyze pretrained autoregressive video diffusion model (Song et al., 2025) by introducing DPT (Ranftl et al., 2021) head that maps its intermediate features to corresponding depth maps (Wang et al., 2025). As illustrated in Fig. 1(c), we observe that features learned solely from raw video data fail to yield meaningful geometric representations, highlighting potential gap in the geometric understanding of video diffusion models trained without additional guidance. To address this limitation, we propose Geometry Forcing (GF), simple yet effective approach that encourages video diffusion models to internalize 3D representations during training. Inspired by recent advances in semantic REPresentation Alignment (REPA) for image diffusion models (Yu et al., 2024a), we align intermediate features of video diffusion models with the geometric representations extracted from pretrained 3D foundation model (Wang et al., 2025). To align these two representations, our method introduces two complementary alignment objectives: Angular Alignment and Scale Alignment. Angular Alignment enforces directional consistency between the diffusion models intermediate features and geometric representations by maximizing their cosine similarity. Scale Alignment, in contrast, preserves the scale information of the geometric representations by predicting unnormalized geometric features from normalized diffusion features. The decoupled formulation of Angular and Scale Alignment allows the model to capture both directional and scale-related aspects of geometry, while improves stability during training and expressiveness in the learned representations. We evaluate the effectiveness of GF on two widely adopted benchmarks: camera view-conditioned video generation on RealEstate10K (Zhou et al., 2018) and action-conditioned video generation on Minecraft environment (Baker et al., 2022). Experimental results demonstrate that our method delivers substantial gains in geometric consistency and visual quality over the baseline methods. For example, GF reduces the FVD from 364 to 243 on RealEstate10K benchmark. Moreover, the ability to reconstruct explicit geometry during inference opens up opportunities for integrating structured memory into long-term world modeling."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 INTERACTIVE WORLD SIMULATION world simulator seeks to model the underlying dynamics of the physical world by predicting future states conditioned on current observations and conditions (OpenAI, 2024; Bar et al., 2025;"
        },
        {
            "title": "Geometry Forcing",
            "content": "Bruce et al., 2024; Parker-Holder et al., 2024; Guo et al., 2025; Alonso et al., 2024; Agarwal et al., 2025). We review prior works through the lenses of interactive video generation, 4D generation, and consistent world modeling. Interactive Video Generation. Recent advancements in generative models (Ho et al., 2020; Rombach et al., 2022; Peebles & Xie, 2023; Lipman et al., 2023), fueled by the availability of large-scale video datasets, have positioned video generation as promising approach to world modeling. Beyond text-to-video synthesis (Chen et al., 2023; 2024b; Kong et al., 2024; Wan et al., 2025; Li et al., 2024; Liu et al., 2025a; Ye et al., 2025), interactive video generation (Yu et al., 2025b) that emphasizes responding interactive control signals evolves rapidly. Existing models incorporate different signals like camera controls (He et al., 2024; Yu et al., 2024b; Song et al., 2025) and action controls (Decart et al., 2024; Guo et al., 2025; Feng et al., 2024; Shin et al., 2024). Building on this progress, our work introduces novel training pipeline that enhances 3D consistency in video generation, enabling more coherent and realistic simulation of spatial scenes. Interactive 4D Generation. In contrast to data-driven video simulators, 4D-based simulators (Chung et al., 2023; Bahmani et al., 2024b; Wu et al., 2025b; Yu et al., 2025a; Lee et al., 2024) explicitly model dynamic 3D structures (Kerbl et al., 2023; Mildenhall et al., 2021; Xiang et al., 2025). Building upon static 3D content generation (Raj et al., 2023), these methods evolve from object-centric 4D modeling (Xu et al., 2024; Bahmani et al., 2024a), to more complex dynamic scenes (Niemeyer & Geiger, 2021; Zhu et al., 2024). Recent works further integrate video priors to improve the realism and temporal coherence of 4D (Aether et al., 2025; Jiang et al., 2025; Mai et al., 2025; Chen et al., 2025), and explore leveraging video priors for robust 4D world modeling. For example, TesserAct (Zhen et al., 2025) predicts RGB, depth, and surface normals to reconstruct temporally consistent 4D scenes. While our work shares the goal of unifying 3D and video generation, it differs by injecting 3D geometric priors into the video representation to improve both temporal and spatial coherence. Consistent World Modeling. key challenge in world modeling lies in maintaining consistency over long video sequences. To address this, prior works have explored different forms of memory and contextual guidance. Frame-level context mechanisms (Chen et al., 2024a; Song et al., 2025; Fuest et al., 2025; Po et al., 2025; Wu et al., 2025c) introduce frame-level context guidance by adding noise to context frames during training. Alternatively, several methods leverage 3D information to enforce spatial coherence. For example, Xiao et al. (2025) maintain memory bank indexed by fieldof-view overlap to retrieve relevant historical frames. Zhang et al. (2025a) propose jointly modeling RGB frames and point maps to main consistency. In contrast to these approaches, we propose unified method that internalizes 3D representations directly into the video diffusion model, enabling stronger and more stable geometric consistency across time. 2.2 3D FOUNDATION MODELS 3D foundation models (3DFMs) (Wang et al., 2025; Li et al., 2025; Piccinelli et al., 2024; Yang et al., 2025; Zhang et al., 2025b; Smart et al., 2024; Wang* et al., 2025; Wang et al., 2024) have recently shown remarkable progress, offering end-to-end learning with fast and robust inference. These models are capable of predicting wide range of 3D properties, such as camera poses (Zhang et al., 2025b), depth maps (Piccinelli et al., 2024), and dense point clouds (Wang et al., 2025), directly from diverse visual inputs. Due to their accuracy, efficiency, and robustness, 3DFMs are becoming essential for enabling in downstream tasks like spatial reasoning (Wu et al., 2025a; Huang et al., 2025a; Fan et al., 2025), autonomous driving (Fei et al., 2024), SLAM (Liu et al., 2025b; Maggio et al., 2025), and beyond. Inspired by their strong 3D capabilities, we explore incorporating 3D representations into video diffusion models to enhance temporal and spatial consistency for world modeling."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Our approach builds upon autoregressive video diffusion models (Chen et al., 2024a; Song et al., 2025; Cheng et al., 2025) and incorporates 3D foundation model (Wang et al., 2025) into the"
        },
        {
            "title": "Geometry Forcing",
            "content": "training process to guide geometric learning. In this section, we provide brief overview of both components to establish the foundation for our method."
        },
        {
            "title": "3.1 AUTOREGRESSIVE VIDEO DIFFUSION MODELS",
            "content": "Training. We formulate our training pipeline based on Flow Matching (Lipman et al., 2023; Liu et al., 2023) with Transformer backbone (Vaswani et al., 2017; Bao et al., 2023), aiming for both simplicity and scalability. Let = {x1, . . . , xI } denote video sequence sampled from the data distribution, we assign an independent timestep for each frame = {t1, . . . , tI } and corrupt frames via interpolation: + ti ϵi, where The target velocity field is defined as the difference between noise and clean input. We train neural network vθ to minimize the Flow Matching loss: ϵi (0, I). xti = (1 ti) x0 LFM = (cid:13) (cid:13)vθ(xt, t) (ϵ x)(cid:13) 2 (cid:13) . Sampling. At inference time, the sampling follows simple probability flow ODE: dx = vθ(xt, t) dt. In practice, we iteratively apply the standard Euler solver (Euler, 1845) to sample data from noise. For autoregressive generation, we initialize the inputs with clean context and generate subsequent frames sequentially, conditioning each prediction on the previously generated frames. 3.2 VISUAL GEOMETRY GROUNDED TRANSFORMER Visual Geometry Grounded Transformer (VGGT) (Wang et al., 2025) is feed-forward model that directly outputs various 3D attributes of scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, few of its projected 2D views. VGGT is composed of Transformer backbone and multiple prediction heads. To make the Transformer focus within each frame and globally in an alternate way, the model employ AlternatingAttention mechanism that interleaves frame-wise self-attention (intra-frame structure) and global self-attention (inter-frame context). For each frame, local and global features are integrated into unified latent representation, which is subsequently processed by set of task-specific heads to produce corresponding 3D attributes. In our work, we leverage the features from the Transformer backbone of VGGT to provide geometric priors for video diffusion models."
        },
        {
            "title": "4 GEOMETRY FORCING",
            "content": "4.1 METHOD OVERVIEW Motivation. Recent advances in video diffusion models have enabled the simulation of the physical world directly from large-scale video datasets (OpenAI, 2024; Polyak et al., 2024; Guo et al., 2025; Bar et al., 2025). However, these models often overlook fundamental property of visual data: videos are 2D projections of an inherently dynamic 3D world. To address this, we seek to narrow the gap between video diffusion models and the underlying dynamic 3D structure of the real world. Observation. We begin by examining whether video diffusion models are capable of implicitly learning 3D information when trained solely on raw video data, without access to explicit 3D supervision. To probe the geometric content of their learned representations, we adopt strategy inspired by linear probing (He et al., 2020): we freeze the parameters of pretrained video diffusion model (Song et al., 2025) and train DPT (Ranftl et al., 2021) head to map intermediate features to corresponding depth map (Wang et al., 2025). This allows us to assess the extent to which geometric information is encoded in the models feature space. The results, presented in Fig. 1(c), indicate that features learned solely from raw video data do not produce meaningful geometric representations, suggesting limited capacity of the model to encode dynamic 3D structure without explicit geometric guidance."
        },
        {
            "title": "Geometry Forcing",
            "content": "Challenge. Bridging the gap between video diffusion models and the underlying dynamic 3D structure of the physical world presents significant challenges, primarily due to the limited availability of annotated dynamic 3D data. straightforward approach is to jointly model RGB appearance and geometric information within unified end-to-end architecture. However, relying heavily on 3D annotations can hinder the scalability and generalizability of the models, particularly when applied to large and diverse real-world video datasets. In this work, inspired by recent advances in semantic REPresentation Alignment (REPA) for image diffusion models (Yu et al., 2024a), we propose Geometry Forcing (GF) that aligns the features of video diffusion models with geometric representations, encouraging the model to internalize 3D-aware structural information during training. Our approach builds upon autoregressive video diffusion models, as described in Sec. 3.1. In Sec. 4.2, we introduce two regularization objectives designed to facilitate representation alignment between the diffusion model and geometric features. The overall training objective, along with additional functional extensions, is summarized in Sec. 4.3."
        },
        {
            "title": "4.2 GEOMETRIC REPRESENTATION ALIGNMENT",
            "content": "To improve the geometric consistency of the learned representations, we introduce two complementary alignment objectives: Angular Alignment and Scale Alignment. These objectives are designed to align the latent features of the diffusion model with intermediate representations from pretrained geometric foundation model (Wang et al., 2025), ensuring both directional consistency and scale preservation of geometric features within the feature space. Angular Alignment. Angular Alignment enforces directional correspondence between the hidden states of the diffusion model, denoted by h, and specified target features, denoted by y. We select intermediate features from the Transformer backbone of VGGT (Wang et al., 2025) as y, as these features preserve both local and global information within each frame and can be further used to reconstruct various explicit geometric representations. In practice, the target features RLN D, where denotes the number of layers, denotes the number of input images, denotes the patch count, and denotes the feature dimension. To achieve Angular Alignment, we first use lightweight projector fϕ to map the diffusion latents RN D to ys shape. The Angular Alignment loss is then defined as: LAngular = 1 LN (cid:88) (cid:88) (cid:88) ℓ=1 n=1 p=1 cos (yℓ,n,p, fϕ(hn,p)) , where cos(, ) denotes cosine similarity. This loss aligns hidden states independently at both the frame and patch levels. Since the VGGT backbone already incorporates cross-frame attention, we do not explicitly enforce global alignment across frames in the loss. Scale Alignment. While Angular Alignment ensures directional consistency, it disregards feature scale that could also encode geometric information. Although direct mean squared error (MSE) loss could supervise magnitudes, it often leads to optimization instability and model collapse due to inherent scale difference across models. To address this issue, we introduce Scale Alignment, which preserves scale information through predicting the scale of target features given normalized diffusion hidden states. Specifically, we first normalize fϕ(h) to unit length. Then we use another lightweight prediction head gφ to predict the full target features from normalized inputs: ˆhℓ,n,p = fϕ(hn,p) fϕ(hn,p)2 , yℓ,n,p = gφ(ˆhℓ,n,p). The Scale Alignment loss is defined as: LScale = 1 LN (cid:88) (cid:88) (cid:88) ℓ= n=1 p=1 yℓ,n,p yℓ,n,p2 2 . This decomposition stabilizes training while capturing both directional and scale attributes of geometric representations."
        },
        {
            "title": "Geometry Forcing",
            "content": "4.3 3D-AWARE AUTOREGRESSIVE VIDEO DIFFUSION MODELS Building on the autoregressive video diffusion framework and the proposed alignment objectives, we now present the overall training objective: = LFM + λAngular LAngular + λScale LScale. Given that the intermediate features of our model are well-aligned with geometric representations, an appealing consequence is the models ability to predict explicit 3D geometry during inference. This enables unified generation of both video and 4D, effectively bridging the gap between videos and the underlying dynamic 3D structure of the physical world, as illustrated in Fig. 1. Moreover, the ability to reconstruct explicit geometry during inference provides structured and interpretable form of memory, which can be further utilized to support long-term world modeling and reasoning. We leave the exploration of such geometry-based memory mechanisms as promising direction for future work. Discussion. Teacher Forcing (Williams & Zipser, 1989) is widely adopted training paradigm for autoregressive models (Radford et al., 2019; Brown et al., 2020; Kondratyuk et al., 2024). To combine autoregressive nature with diffusion models, Diffusion Forcing (Chen et al., 2024a) is introduced, which trains video diffusion models using independently sampled noise levels for each frame. More recently, Self Forcing (Huang et al., 2025b) is proposed to addressing exposure bias in autoregressive video diffusion models. Orthogonal to these methods, Geometry Forcing focuses on improving the spatial structure of the learned representations by aligning the intermediate representation of autoregressive video diffusion models with geometry-aware signals from pretrained 3D foundation model. Our approach provides structural supervision at the representational level, encouraging the model to internalize 3D consistency throughout training."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we evaluate Geometry Forcing (GF) on camera view-conditioned video generation on RealEstate10K (Zhou et al., 2018) dataset and action-conditioned video generation on Minecraft environment (Baker et al., 2022). We also provide more illustration and visualization in Appendix. Implementation Details. For camera view-conditioned video generation (Zhou et al., 2018), we apply GF to the Diffusion Forcing Transformer (Song et al., 2025). Training uses 16-frame videos at 256256 resolution for 2,500 iterations with learning rate of 8 106 and batch size 8. During inference, we condition the model on the first frame and generate 256 frames. For action-conditioned video generation, we apply GF to Next-Frame Diffusion (Cheng et al., 2025), training on 32-frame videos at 384224 resolution for 2,000 steps with learning rate of 6 105 and batch size 32. By default, we set λAngular = 0.5 and λScale = 0.05 to balance the contribution of each loss component. All experiments are conducted on 8 NVIDIA A100 GPUs. Evaluation Metrics. We evaluate visual quality using standard video generation metrics, including FVD (Frechet Video Distance) (Unterthiner et al., 2018), PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index) (Wang et al., 2004), and LPIPS (Learned Perceptual Image Patch Similarity) (Zhang et al., 2018). To further evaluate geometric consistency, we introduce two metrics: Reprojection Error (RPE) (Duan et al., 2025) and Revisit Error (RVE) (Xiao et al., 2025). Reprojection Error (RPE) quantitatively measures multi-view geometric consistency by calculating the average reprojection discrepancy between projected and observed pixel locations across multiple views. Revisit Error (RVE) assesses long-range temporal consistency by examining discrepancies between initial and revisited frames under complete camera rotation. We provide more details of these metrics in the Appendix (Sec. A.1.1). 5.1 MAIN RESULTS This section presents the main experimental results, comparing our method against state-of-the-art approaches across different tasks. The evaluation results demonstrate the effectiveness and generalization ability of our method in both shortand long-term video generation."
        },
        {
            "title": "Geometry Forcing",
            "content": "Figure 2: Qualitative comparison of camera view-conditioned video generation under fullcircle rotation. Videos are generated from single input frame and corresponding per-frame camera poses simulating full 360 rotation. Our method (GF) is compared with DFoT (Song et al., 2025), VideoREPA (Zhang et al., 2025c), and REPA (Zhang et al., 2025c). The results demonstrate that the baseline methods fail to maintain temporal consistency, while our proposed GF consistently revisit the starting viewpoint. Table 1: Quantitative comparison on the RealEstate10K dataset for both short-term (16-Frame) and long-term (256-Frame) video generation. Our method (Geometry Forcing) achieves the best performance across all metrics. bold values denote the best, and Underlined values indicate the second best. * indicates the method is conditioned on the first frame only. Method Frames FVD LPIPS SSIM PSNR RPE RVE DFoT (Song et al., 2025) REPA (Yu et al., 2024a) VideoREPA (Zhang et al., 2025c) Geometry Forcing (ours) Geometry Forcing (ours) + REPA Cosmos* (Agarwal et al., 2025) DFoT (Song et al., 2025) REPA (Yu et al., 2024a) VideoREPA (Zhang et al., 2025c) Geometry Forcing (ours) Geometry Forcing (ours) + REPA 16 16 16 16 256 256 256 256 256 256 252 221 210 193 179 934 364 297 455 243 237 0.40 0.37 0.37 0.32 0.34 0.68 0.55 0.54 0.56 0.51 0.51 0.50 0.54 0.54 0.58 0. 0.20 0.36 0.36 0.35 0.38 0.37 14.40 15.20 15.20 14.70 15.00 10.25 11.40 11.51 11.50 11.87 12.10 0.3575 0.3337 0.3823 0.3337 0.3264 297 315 190 272 236 Camera view-conditioned Video Generation. We conduct comprehensive evaluation of our proposed GF on the RealEstate10K (Zhou et al., 2018) dataset, comparing against several state-ofthe-art baselines. We report results for both short-term (16-Frame) and long-term (256-Frame) video generation settings in Tab. 1. We compare GF against the following baselines: 1) Cosmos (Agarwal et al., 2025): large-scale image-to-video diffusion model. We use the Cosmos-Predict2-2B-Video2World model to generate videos from input images. Since Cosmos only supports generating 121 frames, we generate 256frame videos by conditioning each chunk on the last frame of the previous segment. 2) DFoT: version of DFoT (Song et al., 2025) finetuned on 16-frame clips from RealEstate10K. 3) REPA: baseline trained with REPA (Zhang et al., 2025c) loss, using DINOv2 (Oquab et al., 2023) features as target representation. 4) VideoREPA: variant that applies REPA loss to features extracted"
        },
        {
            "title": "Geometry Forcing",
            "content": "Table 2: Ablation study on target representation. We compare the effect of aligning the diffusion model with different target representations: DINOv2 (semantic), VGGT (geometric), and their combination. The joint use of both representation achieves the best FVD. Table 3: Ablation study on alignment loss. Angular and Scale Alignment losses are evaluated for long-term video generation, with MSE as naive baseline of aligning both angular and scale information. The combination of Angular and Scale Alignment yields the best results. Target Representation FVD-256 Baseline DINOv2 Only VGGT Only VGGT + DINOv2 364 297 243 Alignment Loss Baseline Angular Angular + Scale MSE FVD-256 364.0 253.0 243.0 1648.0 from the VideoMAEv2 (Wang et al., 2023) model, resembling the setup in VideoREPA (Zhang et al., 2025c). As shown in Tab. 1, our method consistently outperforms all baselines across multiple evaluation metrics, including FVD, LPIPS, SSIM, and PSNR, in both the short-term and long-term generation settings. These results highlight the effectiveness of GF in enhancing visual fidelity, temporal stability, and 3D spatial consistency, thereby enabling more realistic and coherent world modeling. Action-conditioned Video Generation. To demonstrate the generality of our method, we apply GF to pretrained Next-Frame Diffusion (Cheng et al., 2025) model. As shown in Tab. 5, the model achieves lower FVD score which indicates that GF can be seamlessly integrated into video diffusion models and leads to measurable gains. Note that, there exists large data distribution gap between real world and Minecraft. This results demonstrate that GF generalize well on out-ofdomain data distribution. 5.2 QUALITATIVE RESULTS Fig. 2 presents qualitative comparisons on the RealEstate10K dataset. Each video is generated from single input frame along with corresponding per-frame camera poses simulating full 360 rotation. We compare GF against three strong baselines: DFoT (Song et al., 2025), REPA (Yu et al., 2024a), and VideoREPA (Zhang et al., 2025c). As shown in Fig. 2, our method consistently reconstructs the initial frame when the camera completes the rotation, while producing coherent and realistic intermediate views. In contrast, the baseline methods often fail to maintain temporal coherence and scene consistency, resulting in implausible intermediate frames and unable to revisit the starting viewpoint. These results highlight the superior long-term 3D consistency and scene understanding of our approach. 5.3 ABLATION STUDIES We provide series of ablation studies to validate the design of GF. Which Representation Should be Aligned? To validate the effectiveness of geometric representation, we compare two target representations in GF: VGGT (Wang et al., 2025), trained on 3D datasets with strong geometric priors, and DINOv2 (Oquab et al., 2023), trained on 2D images focusing on semantic features. As shown in Tab. 2, aligning with VGGT consistently outperforms DINOv2 on both long-term and short-term generation tasks, highlighting the advantage of geometric alignment over semantic supervision. To further explore their complementarity, we combine VGGT and DINOv2 features as joint supervision targets. Results in Tab. 2 show that integrating geometric and semantic signals leads to additional gains, suggesting that the two types of representations are orthogonal and can enhance each other when used together. However, as we mainly focus on bringing the gap between the video diffusion model and the dynamic 3D structure of the real world, we only use VGGT features in further experiments."
        },
        {
            "title": "Geometry Forcing",
            "content": "Table 4: Ablation study on the way to integrate geometry information. We compare external geometry condition (via ControlNet) with internal geometry alignment (Geometry Forcing). Table 5: Evaluation on action-conditioned video generation in Minecraft. FVD results of NFD before and after applying Geometry Forcing (GF) on 16-Frame generation show clear improvement. Method FVD-256 Baseline Geometry ControlNet Geometry Forcing (ours) 364 275 Method FVD-16 NFD NFD + GF 216 205 Alignment Loss. GF consists of two alignment objectives: Angular Alignment and Scale Alignment. To validate their effectiveness, we compare three alignment loss types: (1) Angular Alignment alone (Sec. 4.2), (2) Angular Alignment with Scale Alignment (Sec. 4.2), and (3) MSE loss between VGGT and diffusion features. As shown in Tab. 3, the combination of Angular Alignment and Scale Alignment achieves the best performance, indicating the benefit of aligning both angular direction and scale-related information. Although direct mean squared error (MSE) supervision also supervises magnitudes, the change of feature scale of the diffusion model may cause collapse in the following layers. These results highlight that neither Angular Alignment nor Scale Alignment alone is sufficient. How Can Geometry Information be Integrated into Video Diffusion Models? To validate the effectiveness of internalizing geometric representation in the video diffusion model, we compare two strategies to incorporate geometric representation: internal alignment through GF and external guidance via an additional ControlNet (Zhang et al., 2023) (Geometry ControlNet). In the external guidance experiment, we obtain intermediate features from the transformer backbone of VGGT (identical to the one used in GF). Then we feed the intermediate features into ControlNet attached to DFoT. This approach introduces geometry information as external conditions. In contrast, GF encourages the model to internalize geometric features. As shown in Tab. 4, while the external guidance produces improvements over the baseline DFoT model, it still underperforms compared to GF. This suggests that integrating geometric priors into the model is more effective than supplying them as external conditions. By aligning internal features with geometric representations, GF enables deeper geometric understanding and yields better performance in terms of perceptual quality and structural consistency. Which Layer Should be Aligned? As shown in Fig. 3, we also explore applying alignment at different layers of the video diffusion model (Song et al., 2025), which uses 7-layer U-ViT (Bao et al., 2023) backbone (3 downsampling layers, 1 bottleneck layer, 3 upsampling layers). Aligning at layer 3 yields the best FVD-256 score while preserving FVD-16 performance. Mitigating Exposure Bias in Autoregressive Video Diffusion Model via Geometry Forcing. Exposure bias is long-standing challenge in autoregressive video generation (Chen et al., 2024a; Valevski et al., 2024; Song et al., 2025; Sun et al., 2025; Cheng et al., 2025; Huang et al., 2025b). While previous methods have attempted to address this through memory mechanisms or context guidance, our proposed GF offers an orthogonal solution. As shown in Fig. 4, GF mitigates longterm drift and reduces the accumulation of error during generation significantly by aligning 3D geometric representation. These results validate integrating consistent 3D representation enables more reliable and coherent long-term video synthesis. 5.4 USER STUDY While Reprojection Error (RPE) and Revisit Error (RVE) provide useful signals for measuring 3D consistency, they only capture specific geometric aspects and may miss perceptual artifacts or unrealistic dynamics that humans can easily notice. Additionally, we conduct user study focusing on three aspects of 3D consistency. 1) Camera Following: Whether the camera in the video moves smoothly and accurately follows the given pose trajectory. 2) Object Consistency: Whether objects"
        },
        {
            "title": "Geometry Forcing",
            "content": "Figure 3: Ablation study on alignment depth. We present FVD-256 and FVD-16 results for aligning VGGT to different layers of the diffusion model. The results suggest that mid-level feature alignment is most effective for improving long-term video quality. Figure 4: Exposure bias analysis. This figure shows the trend of FVD scores during longterm video generation. Compared to the baseline, GF results in significantly lower FVD after 100 frames. Table 6: User study. Average scores (15) on Camera Following, Object Consistency, and Scene Continuity. Each user was shown one case at time and asked to rate each dimension on scale of 1 to 5. Higher values indicate better quality. Method Camera Following Object Consistency Scene Continuity DFoT REPA VideoREPA Geometry Forcing 3.56 3.82 3.31 4.40 2.73 3.55 3.05 4.44 2.74 3.66 2.82 4.52 remain consistent in shape, appearance, and position across frames. 3) Scene Continuity: Whether the newly generated parts of the scene beyond the context frames remain coherent and reasonable given the original scene. We compare GF with DFoT (Song et al., 2025), REPA (Yu et al., 2024a), and VideoREPA (Zhang et al., 2025c) on the human evaluation benchmarks described above. As shown in Tab. 6, GF consistently outperforms all baselines across the three aspects of 3D consistency. These results demonstrate its effectiveness in producing videos that are both perceptually and geometrically coherent."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduces Geometry Forcing (GF), simple yet effective framework that enhances the geometric consistency of autoregressive video diffusion models by aligning their internal representations with geometry-aware features. Motivated by the observation that video diffusion models trained on raw pixel data often fail to capture meaningful 3D structure, our method proposes two complementary alignment objectives (Angular Alignment and Scale Alignment) to guide the latent space toward 3D-aware representations extracted from pretrained geometric foundation model. Empirical results on both camera-conditioned and action-conditioned video generation benchmarks demonstrate that GF significantly improves visual quality and 3D consistency, yielding lower FVD scores and more stable scene dynamics. Limitations. The primary limitation of this work lies in its scale. While GF consistently improves geometric consistency and visual quality, its full potential remains unexplored under large-scale training. In particular, we have not yet investigated its effectiveness when applied to larger models and more extensive video datasets, which may further amplify its benefits."
        },
        {
            "title": "Geometry Forcing",
            "content": "Future Work. Future directions include scaling GF on larger datasets to build 3D-consistent world simulators, and applications for ultra-long video generation by treating 3D representation as persistent memory."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "We would like to acknowledge Kiwhan Song and Boyuan Chen for their valuable advice and assistance in helping us reproduce the results of Diffusion Forcing Transformer."
        },
        {
            "title": "REFERENCES",
            "content": "Aether, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, et al. Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945, 2025. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and Francois Fleuret. Diffusion for world modeling: Visual details matter in atari. Advances in Neural Information Processing Systems, 37:5875758791, 2024. Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned textto-4d generation. In European Conference on Computer Vision, pp. 5372. Springer, 2024a. Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 79968006, 2024b. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth In Proceedings of the IEEE/CVF conference on words: vit backbone for diffusion models. computer vision and pattern recognition, pp. 2266922679, 2023. Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1579115801, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:18771901, 2020. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024a. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023."
        },
        {
            "title": "Geometry Forcing",
            "content": "Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024b. Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, and Chongxuan Li. Flexworld: Progressively expanding 3d scenes for flexiable-view synthesis. arXiv preprint arXiv:2503.13265, 2025. Xinle Cheng, Tianyu He, Jiayi Xu, Junliang Guo, Di He, and Jiang Bian. Playing with transformer at 30+ fps via next-frame diffusion. arXiv preprint arXiv:2506.01380, 2025. Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. Decart, Quevedo Julian, McIntyre Quinn, Campbell Spruce, Chen Xinlei, and Wachen Robert. Oasis: universe in transformer. 2024. URL https://oasis-model.github.io/. Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. Leonhard Euler. Institutionum calculi integralis, volume 4. impensis Academiae imperialis scientiarum, 1845. Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, and Jiwen Lu. Driv3r: Learning dense 4d reconstruction for autonomous driving. ArXiv, abs/2412.06777, 2024. URL https://api.semanticscholar.org/CorpusID:274610426. Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with realtime moving control. arXiv preprint arXiv:2412.03568, 2024. Michael Fuest, Vincent Tao Hu, and Bjorn Ommer. Maskflow: Discrete flows for flexible and efficient long video generation. arXiv preprint arXiv:2502.11234, 2025. Andrew Glassner. An introduction to ray tracing. Morgan Kaufmann, 1989. Google. Veo 3. https://deepmind.google/models/veo/, 2025. Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, and Jiang Bian. Mineworld: real-time and open-source interactive world model on minecraft. arXiv preprint arXiv:2504.08388, 2025. David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan arXiv preprint Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv:2404.02101, 2024. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on unsupervised visual representation learning. computer vision and pattern recognition, pp. 97299738, 2020. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 33:68406851, 2020."
        },
        {
            "title": "Geometry Forcing",
            "content": "Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. Xiaohu Huang, Jingjing Wu, Qunyi Xie, and Kai Han. Mllms need 3d-aware representation supervision for scene understanding. arXiv preprint arXiv:2506.01946, 2025a. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025b. Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for geometric 4d scene reconstruction. arXiv preprint arXiv:2504.07961, 2025. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):114, 2023. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language In International Conference on Machine Learning, pp. model for zero-shot video generation. 2510525124. PMLR, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Yao-Chih Lee, Yi-Ting Chen, Andrew Wang, Ting-Hsuan Liao, Brandon Feng, and JiaarXiv preprint Bin Huang. Vividdream: Generating 3d scene with ambient dynamics. arXiv:2405.20334, 2024. Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv preprint arXiv:2405.18750, 2024. Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. Runtao Liu, Haoyu Wu, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. In Proceedings of the Videodpo: Omni-preference alignment for video diffusion generation. Computer Vision and Pattern Recognition Conference, pp. 80098019, 2025a. Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yingda Yin, Yanchao Yang, Qingnan Fan, and Baoquan Chen. Slam3r: Real-time dense scene reconstruction from monocular rgb videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1665116662, 2025b. Dominic Maggio, Hyungtae Lim, and Luca Carlone. Vggt-slam: Dense rgb slam optimized on the sl(4) manifold. ArXiv, abs/2505.12549, 2025. URL https://api.semanticscholar. org/CorpusID:278739766. Jinjie Mai, Wenxuan Zhu, Haozhe Liu, Bing Li, Cheng Zheng, Jurgen Schmidhuber, and Bernard Ghanem. Can video diffusion model reconstruct 4d geometry? arXiv preprint arXiv:2503.21082, 2025. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021."
        },
        {
            "title": "Geometry Forcing",
            "content": "Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1145311464, 2021. OpenAI. Sora. https://openai.com/index/sora/, 2024. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Parker-Holder, Ball, Bruce, Dasagi, Holsheimer, Kaplanis, Moufarek, Scully, Shar, Shi, et al. Genie 2: large-scale foundation world model. URL: https://deepmind. google/discover/blog/genie-2-a-large-scale-foundation-world-model, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and In Proceedings of the Fisher Yu. Unidepth: Universal monocular metric depth estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1010610116, 2024. Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models. arXiv preprint arXiv:2505.20171, 2025. Polyak, Zohar, Brown, Tjandra, Sinha, Lee, Vyas, Shi, CY Ma, CY Chuang, et al. Movie gen: cast of media foundation models. 2024a. arXiv preprint arXiv:2410.13720, 2024. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven In Proceedings of the IEEE/CVF international conference on computer text-to-3d generation. vision, pp. 23492359, 2023. Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1217912188, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, 2022. Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. Wham: Reconstructing worldIn Proceedings of the IEEE/CVF Conference on grounded humans with accurate 3d motion. Computer Vision and Pattern Recognition, pp. 20702080, 2024. Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. arXiv preprint arXiv:2408.13912, 2024. Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, and Jing Liu. Ar-diffusion: Asynchronous video generation with auto-regressive diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 7364 7373, 2025. Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:1655816569, 2021."
        },
        {
            "title": "Geometry Forcing",
            "content": "Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and In ProceedYu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. ings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1454914560, 2023. Qianqian Wang*, Yifei Zhang*, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Ronald Williams and David Zipser. learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270280, 1989. Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025a. Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yue Qian, Xiaohang Zhan, and Yueqi Duan. 4d-fly: Fast 4d reconstruction from single monocular video. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 1666316673, June 2025b. Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Video world models with long-term spatial memory. arXiv preprint arXiv:2506.05284, 2025c. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2146921480, 2025. Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025. Dejia Xu, Hanwen Liang, Neel Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos Plataniotis, and Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint arXiv:2403.16993, 2024."
        },
        {
            "title": "Geometry Forcing",
            "content": "Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Yang Ye, Junliang Guo, Haoyu Wu, Tianyu He, Tim Pearce, Tabish Rashid, Katja Hofmann, and Jiang Bian. Fast autoregressive video generation with diagonal decoding. arXiv preprint arXiv:2503.14070, 2025. Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 59165926, 2025a. Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun arXiv preprint Gai, Hao Chen, and Xihui Liu. survey of interactive generative video. arXiv:2504.21853, 2025b. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024a. Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, TienTsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024b. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Qihang Zhang, Shuangfei Zhai, Miguel Angel Bautista Martin, Kevin Miao, Alexander Toshev, Joshua Susskind, and Jiatao Gu. World-consistent video diffusion with explicit 3d modeling. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2168521695, 2025a. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera esIn Proceedings of the Computer Vision and Pattern timation from uncalibrated sparse views. Recognition Conference, pp. 2193621947, 2025b. Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, and Yu Cheng. Videorepa: Learning physics for video generation through relational alignment with foundation models. arXiv preprint arXiv:2505.23656, 2025c. Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, and Chuang Gan. Tesseract: learning 4d embodied world models. arXiv preprint arXiv:2504.20995, 2025. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. Hanxin Zhu, Tianyu He, Anni Tang, Junliang Guo, Zhibo Chen, and Jiang Bian. Compositional 3d-aware video generation with llm director. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "A.1.1 METRICS In this section,we introduce the detailed implementation of Reprojection Error (RPE) and Revisit Error (RVE). Reprojection Error. Reprojection error (RPE) is widely used metric in visual SLAM to evaluate multi-view geometric consistency. Following Duan et al. (2025), we utilize DROIDSLAM (Teed & Deng, 2021) to reconstruct scene. Specifically, DROID-SLAM first extracts corresponding features across frames and then refines camera poses (Gt) and per-pixel depth estimates (dt) through its differentiable Dense Bundle Adjustment (DBA) optimization, enforcing optical flow constraints and achieving robust structure-from-motion. The reprojection error is then computed by measuring the average Euclidean distance between the projected and observed pixel locations of co-visible 3D points across multiple frames. Formally, RPE is defined as: RE = 1 (cid:88) (cid:13) (cid:13)p ij Π(Pij)(cid:13) (cid:13)2 , (i,j)V (1) where denotes the set of valid feature correspondences, pij is the observed pixel location in generated video frames, Pij represents the corresponding reconstructed 3D point derived from refined depths and camera poses, and Π denotes the camera projection function. Lower RPE values indicate better 3D alignment, reduced spatial artifacts, and enhanced spatio-temporal stability, thereby effectively reflecting the overall geometric coherence and consistency of the generated videos. Revisit Error. Revisit Error evaluates long-range temporal consistency under full camera rotation, inspired by the setup proposed in WorldMem (Xiao et al., 2025). For each of 100 randomly sampled RealEstate10K video clips, we extract the first frame and initial camera pose. camera trajectory of 256 frames is then constructed by rotating the initial camera pose around the Y-axis. We assess revisit consistency by comparing the first and final frame using reconstruction FID (rFID) (Heusel et al., 2017). Larger discrepancies indicate greater geometric or appearance drift, suggesting weaker long-term 3D consistency. A.2 3D RECONSTRUCTION FROM DIFFUSION FEATURES. In this section, we provide detailed overview of the 3D reconstruction process illustrated in Fig. 1(c). Reconstruction Using Geometry Forcing Features We extract features from the Geometry Forcing (GF) model and pass them through the depth prediction head of VGGT to obtain the predicted depth map. Reconstruction Using Diffusion Features Motivated by our linear probing experiments, we investigate the 3D reconstruction capability of intermediate features extracted from DFoT (Song et al., 2025). Specifically, we freeze the pretrained DFoT backbone and train DPT head (Ranftl et al., 2021) to regress depth maps from its intermediate representations. The target depth maps are provided by the VGGT model (Wang et al., 2025), serving as ground-truth supervision. The DPT head adopts the same architecture as the depth prediction module used in VGGT but is trained from scratch. We optimize the DPT head for 2500 steps using learning rate of 1 104 and batch size of 4."
        },
        {
            "title": "Geometry Forcing",
            "content": "A.3 SUPPLEMENTARY VISUALIZATIONS In order to better understand the geometry influences, we provide comprehensive visual results. Figure 5: Qualitative comparisons on camera-conditioned video generation. All the videos are generated given first frame and per-frame camera pose. We comprehensively compare GF (ours) with DFoT (Song et al., 2025), VideoREPA (Zhang et al., 2025c), REPA (Zhang et al., 2025c). The results demostrate consistency in long-term video generation both inside (left) and outside (right) scenes. Fig. 5 presents qualitative comparisons on the RealEstate10K dataset. Given the same first frame and per-frame camera trajectory as input, we compare our proposed GF method with three strong baselines: DFoT (Song et al., 2025), REPA (Zhang et al., 2025c), and VideoREPA (Zhang et al., 2025c). As shown in Fig. 5, our method generates visually coherent and geometrically consistent videos over long time horizons even context is limited. In particular, GF better preserves object shapes and scene layouts that is visible in context, while generating reasonable scenes not seen in the In contrast, baseline models often exhibit drift, shape distortion, or abrupt transitions. context. These results highlight the effectiveness of internalizing geometric priors to enhance spatial and temporal consistency in video generation."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Tsinghua University"
    ]
}