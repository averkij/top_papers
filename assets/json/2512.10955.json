{
    "paper_title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "authors": [
        "Tsai-Shien Chen",
        "Aliaksandr Siarohin",
        "Guocheng Gordon Qian",
        "Kuan-Chieh Jackson Wang",
        "Egor Nemchinov",
        "Moayed Haji-Ali",
        "Riza Alp Guler",
        "Willi Menapace",
        "Ivan Skorokhodov",
        "Anil Kag",
        "Jun-Yan Zhu",
        "Sergey Tulyakov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks."
        },
        {
            "title": "Start",
            "content": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization Tsai-Shien Chen1,2, Aliaksandr Siarohin1 Guocheng Gordon Qian1 Kuan-Chieh Jackson Wang1 Egor Nemchinov1 Moayed Haji-Ali1 Riza Alp Guler1 Willi Menapace1 Ivan Skorokhodov1 Anil Kag1 Jun-Yan Zhu3 Sergey Tulyakov1 1Snap Inc. 2UC Merced 3CMU https://snap-research.github.io/omni-attribute 5 2 0 2 1 1 ] . [ 1 5 5 9 0 1 . 2 1 5 2 : r Figure 1. Omni-Attribute is an open-vocabulary image attribute encoder that learns to extract attribute-specific representations from visual inputs. Given reference images (top row) paired with textual attribute descriptions (colored text boxes), Omni-Attribute encodes attribute representations that can be coherently synthesized in new contexts (middle and bottom rows) in fully feed-forward manner, without any test-time optimization. These learned representations possess two key properties: (a) they capture high-fidelity, fine-grained details of the user-specified image attributes while suppressing irrelevant visual information, thereby mitigating copy-and-paste artifacts; and (b) they are composable, enabling the seamless integration of representations from multiple images into single coherent generated image. This work was done while interning at Snap."
        },
        {
            "title": "Abstract",
            "content": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first openvocabulary image attribute encoder designed to learn highfidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-ofthe-art performance across multiple benchmarks. 1. Introduction Reality is mosaic of independent but interconnected things. Inspired by Gottfried Wilhelm Leibniz Images are bags of visual words [12], encapsulating rich yet entangled combinations of attributes such as identity, expression, pose, background, lighting, camera angle, and art style. This intrinsic complexity makes image attribute disentanglement and manipulation particularly challenging problem. Recent advances in personalization [16, 72] have demonstrated remarkable capabilities in transferring image attributes to novel contexts, enabling applications such as generating my dog based on the reference image. To achieve these objectives, most existing methods rely on general-purpose image encoders, such as CLIP [54], DINOv2 [46], or the VAEs [31] for image generation, to extract holistic representations of input images that are then used to guide the synthesis. However, this design presents fundamental limitation. Since these encoders compress and entangle all visual information into single representation, they often suffer from information leakage of irrelevant attributes, resulting in undesirable copy-and-paste artifacts [11]. For example, as shown in the leftmost column of Fig. 5, even when personalizing the identity, existing image embeddings inadvertently transfer the lighting and clothing details from the reference image. In this paper, we revisit the attribute manipulation problem from new perspective and focus on learning attributelevel representations directly on the encoder side. We introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to process an image alongside textual attribute descriptions jointly. Unlike generalpurpose encoders that indiscriminately capture all visual content, Omni-Attribute is built with two goals: (i) to extract attribute-related information exclusively and faithfully, and (ii) to suppress other irrelevant visual information. We jointly design the data and model for such attributelevel representation learning. On the data side, our training samples consist of pairs of semantically linked images. To guide the encoder in learning attribute-specific representations, we introduce novel annotation form to establish semantic connections between image pairs. It includes (i) positive attributes that describe the semantics shared by the two images and (ii) negative attributes that highlight the characteristics that differ between them. This pairing structure explicitly teaches the encoder which visual concepts should be preserved and which should be suppressed. On the model side, we formulate attribute-level representation learning as dual-objective optimization problem. On one hand, the attribute embeddings are required to capture sufficient information for the high-fidelity reconstruction of the target attributes; on the other hand, they need to discard irrelevant cues from unrelated attributes. To achieve both goals, we minimize two complementary losses: (i) generative loss that ensures embeddings extracted from reference image can effectively reconstruct its paired image, and (ii) contrastive loss that introduces repulsive force between embeddings associated with negative attributes. As illustrated in Fig. 1(a), these two losses together drive the encoder to accurately and exclusively disentangle attributespecific information. Finally, we demonstrate the versatility of Omni-Attribute in the task of attribute composition, where multiple attribute embeddings extracted from different images can be seamlessly combined into single image, as shown in Fig. 1(b). Our key contributions can be summarized as follows: We present Omni-Attribute, the first open-vocabulary attribute encoder designed to jointly process an image along with textual attribute description to extract attributespecific representations. To learn such attribute-level embeddings, we introduce new data annotation strategy together with novel training scheme that balances high-fidelity encoding with the effective suppression of irrelevant information. We showcase the versatility of Omni-Attribute across several downstream tasks, including attribute-oriented image retrieval, personalization, and composition. We further visualize its embedding spaces for better interpretability. 2. Related Work Visual Representation Learning. Omni-Attribute learns attribute-level embeddings by combining supervised, contrastive, and multimodal learning, which are the three key pillars of visual representation learning over the past 2 Figure 2. Training data annotation. Our training data consist of semantically linked image pairs annotated with positive and negative attributes that define their relationships through the shared and differing characteristics. The word cloud on the right highlights the richness and diversity of our attribute annotations, facilitating the training of an open-vocabulary attribute encoder. 15 years. Early approaches, such as AlexNet [32] and ResNet [23], relied on supervised pretraining on large-scale datasets like ImageNet [13] to produce hierarchical features transferable across recognition tasks. Subsequently, selfsupervised methods [5, 7, 8, 20, 24] introduced instancelevel contrastive objectives to eliminate dependence on labeled data while retaining discriminative representations. More recently, CLIP [54] unified vision and language through multimodal representation learning, aligning the embedding spaces of both modalities. Building on this foundation, subsequent works like DINO [6, 46, 59] and MAE [25] advanced visual abstraction but still encoded holistic global features that entangle diverse image attributes. Extending this line of work, Omni-Attribute explicitly models attribute-level representations, producing disentangled and composable embeddings that bridge representation learning and controllable image generation. Image-guided Generation. The goal of image-guided generation is to manipulate or extend the visual attributes of reference image while synthesizing the remaining context in coherent and semantically consistent setting. Recent breakthroughs in diffusion models [28, 61, 62] and transformer-based architectures [14, 43, 64] have significantly enhanced generation quality, enabling applications such as editing [18, 26, 35, 44, 63, 68, 70] and personalization [911, 15, 16, 33, 5153, 55, 57, 72]. common strategy is the encoder-based approach, where reference image is first mapped to latent embeddings, which are then used to condition the generative model. For instance, IP-adapter [72] injects CLIP-encoded image features via lightweight decoupled cross-attention layers for personalization. Qwen-Image-Edit [68] jointly encodes textual and visual instructions using multimodal encoder [66] and VAE [31] for unified visionlanguage these conditioning embeddings conditioning. However, often entangle multiple visual attributes, leading to information leakage and undesirable copy-and-paste artifacts [11]. Omni-Attribute addresses this issue on the encoder side by learning attribute-specific embeddings, resulting in cleaner, more controllable synthesis. Visual Concept Disentanglement. Images inherently blend multiple visual attributes across shared pixels, making attribute disentanglement long-standing challenge. Early works, such as Break-A-Scene [1] and ConceptExpress [22], attempt to separate concepts using user-defined or attention-derived spatial masks, but these methods are limited to isolating spatially separable elements. Inspiration Tree [65] introduces hierarchical decomposition of visual concepts but lacks predictability in its representations. Recent methods like Token-Verse [17] (optimizationbased) and Mod-Adapter [76] (encoder-based) manipulate the modulation space of DiTs [48] to represent image attributes but face two limitations: (i) per-token modulation hinders the personalization of multi-token (phraselevel) concepts; and (ii) the usage of AdaLN conditioning restricts control to simple, limited affine transformations (scale-and-shift). Closer to our setting, OADis [56] and DeCLIP [71] leverage text-guided contrastive objectives for attribute disentanglement, but they are restricted to fixed, closed set of attributes. In contrast, Omni-Attribute is able to faithfully extract open-vocabulary attribute embeddings, enabling precise and flexible image generation. 3. Omni-Attribute Our target is to learn an open-vocabulary attribute encoder that jointly takes images and textual attribute descriptions as inputs and produces disentangled, attribute-specific representations while suppressing other visual information. 3.1. Semantic Connections between Image Pairs As illustrated in Fig. 2, our training samples consist of semantically linked image pairs. To learn attribute-level representations, we design new annotation scheme that establishes semantic connections between each image pair (i) positive attributes, through two types of attributes: which describe shared semantic properties, and (ii) negative attributes, which highlight distinct characteristics. Annotation of Positive and Negative Attributes. Labeling high-quality attribute annotations requires strong vision-language understanding and long, detailed instruc3 Figure 3. Dual-objective representation learning. Our attributelevel representation learning is guided by two complementary objectives: generative loss (top) to maximize embedding information and encourage the capture of fine-grained, high-fidelity details, and contrastive loss (bottom) that extracts attribute-specific information while suppressing irrelevant content. tion prompt, which can be prohibitively expensive for largescale inference. To balance annotation quality and cost, we adopt two-stage annotation pipeline. In the first stage, we leverage the powerful but computationally expensive 72Bparameter multimodal large-language model (MLLM) [66] with the input of detailed instruction prompt to curate sub-dataset with high-quality attribute annotations. Inspired by Chain-of-Thought [67], we enhance label quality by explicitly prompting the model to describe fine-grained similarities and differences for each positive and negative attribute (see descriptions in the brackets of Fig. 2). In the second stage, we finetune 32B-parameter MLLM on these annotated image pairs to learn an expert annotator model specialized in this task. Through supervised finetuning, this student model internalizes the task-specific reasoning behavior and the structured annotation output, enabling it to perform high-quality labeling without the detailed instruction prompt. This substantially reduces annotation costs by cutting input token length by 3.1 and persample annotation latency by 6.3. App. A.2 details the MLLM finetuning and the annotation pipeline. The word cloud in Fig. 2 (right) illustrates the richness and diversity of the resulting attribute annotations, facilitating the learning of an open-vocabulary attribute encoder. 3.2. Attribute-level Representation Learning Learning high-fidelity, attribute-level embeddings is inherently dual-objective optimization problem. On one hand, the embeddings must maximize attribute-specific information to represent fine-grained details. On the other hand, they must suppress signals from irrelevant attributes. As Figure 4. Model architecture. Our attribute encoder is LoRAtuned MLLM followed by trainable lightweight connector to preserve strong vision-language prior while capable of adapting to our attribute disentanglement task. The image decoder is frozen generator with trainable IP-Adapter [72] modules for personalization. in Fig. 3, we jointly optimize two complementary losses to achieve both goals: (i) generative loss that encourages faithful attribute reconstruction and (ii) contrastive loss that enforces attribute separation and disentanglement. Generative Loss. The training framework consists of an encoder and decoder (or generator) D. Given training image pair (Ix, Iy) with positive attributes {a+ 1 , . . . , a+ m} and negative attributes {a }, we randomly assign one image as the reference image Ir and the other as the ground truth image Ig to compute the generative loss. As shown in Fig. 3 (top), the generative loss guides the model to reconstruct Ig conditioned on the attribute embeddings extracted from Ir and its corresponding text prompt cg: 1 , . . . , Lgen = ϕ(I , Ig), = D(E(Ir, {a+ 1 , . . . , a+ m}), cg) (1) where ϕ is generic similarity or distance function between images (e.g., L2 or flow-matching loss). Notably, here, all positive attributes are used as inputs to guide in extracting complete attribute information shared by both images. Empirically, dropping any positive attribute during training causes to encode the entire image and further leads to the copy-and-paste artifacts rather than focusing on the specified attributes. Contrastive Loss. To enhance attribute disentanglement, we introduce contrastive loss that attracts the positive attribute embeddings encoded from the paired images (Ix, Iy), while repelling embeddings associated with negative or different attributes, as depicted in Fig. 3 (bottom). Formally, we sample one positive attribute a+ and one negative attribute , and optimize: ψ(a+ ,a+ )+ψ(a+ ,a ,a+ )+ψ(a ,a ) ,a+ ψ(a+ ) )+ψ(a (2) Lcon = log 4 The similarity function ψ is defined as: ψ(ax, ay) = sim(pool(E(Ix, ax)), pool(E(Iy, ay))), (3) (uv) where sim(u, v) = exp( 1 uv ) measures the similarity τ of two pooled attribute embeddings, with temperature of τ . This loss encourages the encoder to maximize the similarity between embeddings of the positive attribute while minimizing similarity across negative or different attributes (even when derived from the same image pair), thereby producing discriminative attribute-level embedding space."
        },
        {
            "title": "The final training objective combines both generative",
            "content": "and contrastive losses: = λgen Lgen + λcon Lcon (4) with hyperparameters λgen and λcon balancing reconstruction fidelity against attribute disentanglement. 3.3. Model Architecture Fig. 4 illustrates our model architecture, which consists of an attribute encoder and an image decoder D. Attribute Encoder. The encoder is designed to satisfy two key requirements: (i) the ability to jointly process text and image inputs, and (ii) strong vision-language prior to support our attribute disentanglement objective. To meet these criteria, we adopt MLLM [66] as the backbone. Empirically, we observe that LoRA [29] tuning better preserves pretrained representations and mitigates catastrophic forgetting compared to full finetuning, consistent with the recent findings of Shuttleworth et al. [58]. To further adapt the model to the attribute disentanglement task, we attach lightweight, trainable connector [39] after the backbone. The encoder takes as input multimodal prompt composed of the input attributes and an image, as shown in Fig. 4 (upper-left), and produces sequence of tokens, = [a1, . . . , al], serving as attribute embeddings. Contrastive Head. To compute the contrastive loss, we aggregate the 2-D attribute embeddings into 1-D representation via average pooling: pool(A) = (cid:80)l Image Decoder. For the generative loss, the full attribute embeddings are directly passed into downstream decoder, which consists of frozen image generator preceded by trainable IP-Adapter [72] modules for personalization. i=1 ai/l. 3.4. Composition of Multiple Attributes Compositional image generation aims to synthesize coherent output by integrating multiple objects or concepts from different reference sources. We empirically find that the learned attribute embeddings are composable, providing an alternative approach to compositional generation. Composable Diffusion [38] achieves multi-condition synthesis by generalizing classifier-free guidance (CFG) [27] to handle multiple conditioning signals. Specifically, it combines conditional score estimates, each of which represents the gradient direction associated with distinct concept and is obtained by taking the difference between the models conditional and unconditional predictions. We adopt similar concept in our flow-matching [37] generator. Given set of reference images {I1, . . . , IN } with associated attribute descriptions {a1, . . . , aN }, where denotes the number of reference sources, we first compute the conditional flow field for each image-attribute pair: (Ii,ai) = D(E(Ii, ai), ) D(, ). (5) We then evaluate the final velocity based on the linear combination of the conditional flow fields: = D(, c) + (cid:88) i= wi (Ii,ai), (6) where is the prompt and wi controls the strength of each conditioning signal. Note that Eq. (6) only applies CFG to the attribute conditions. In practice, we also apply CFG on following the formulation of InstructPix2Pix [4]. 4. Experiments In this section, we demonstrate the versatility of OmniAttribute across several downstream tasks, including attribute-oriented retrieval, personalization, and composition. Implementation details are provided in App. A. 4.1. Open-vocabulary Attribute Personalization We compare Omni-Attribute with existing methods for open-vocabulary attribute personalization and present qualitative and quantitative results in Figs. 5 and 6, respectively. Baseline Models. We conduct extensive comparisons with two groups of models that support image personalization: First, we evaluate different image encoders, including CLIP [54], DINOv2 [46], and Qwen-VL [66], by training IP-Adapter modules between each encoder and the same frozen image generation backbone to ensure fair comparison. For Qwen-VL, we provide multimodal prompt that includes both the reference attribute and image, as shown in Fig. 4 (upper-left). For vision-only encoders, such as CLIP and DINOv2, we only provide the reference image. Next, recent advances in image editing enable personalization through models such as OmniGen2 [70], FLUXKontext [35], and Qwen-Image-Edit [68]. To adapt these models for attribute personalization, we reformulate the instruction prompt as: Preserve the <attribute> of the image and generate <prompt>. Evaluation Dataset. To comprehensively evaluate openvocabulary attribute personalization, we construct benchmark comprising 15 reference attributes across two categories: concrete objects and abstract concepts (see the full list in App. B.1). For each attribute, we select 5 images 5 Figure 5. Qualitative comparisons of open-vocabulary attribute personalization. Each row, from top to bottom, shows (i) the reference image-attribute pair and the prompt, (ii) results generated using CLIP [54], DINOv2 [46], and Qwen-VL [66] embeddings, (iii) results from editing models, including OmniGen2 [70], FLUX-Kontext [35], and Qwen-Image-Edit [68], and (iv) results by Omni-Attribute. As shown, Omni-Attribute achieves the best balance between faithfully encoding the target attribute and coherently synthesizing it into new contexts aligned with the prompt, while minimizing undesired copy-and-paste artifacts. Full prompts are provided in App. B.1. Figure 6. Quantitative comparisons of open-vocabulary attribute personalization. We compare Omni-Attribute with baseline methods on the personalization of two types of attributes: (a) concrete objects and (b) abstract concepts. We perform the evaluation across two metrics, image naturalness (higher is better) and conditioning fidelity (higher is better), using both MLLM [45] and human evaluations. Omni-Attribute consistently outperforms existing methods, especially for abstract concepts. Full numerical results are in App. B.1. (partially sourced from DreamOmni2 [69]) and apply an LLM to generate 5 prompts that deliberately exclude content related to the reference attribute to avoid semantic conflicts. By cross-pairing the images and prompts, we obtain 25 samples per attribute, resulting in total of 350 samples. Evaluation Metrics. Personalization aims to faithfully integrate the reference visual concept into new contexts specified by text prompt while maintaining coherent overall composition. We evaluate this objective using three metrics: (i) attribute fidelity score, which measures the faithfulness of the personalized attributes; (ii) text fidelity score, which assesses the semantic consistency between the generated image and the input prompt; and (iii) image naturalness score, which evaluates the overall visual coherence of the image. An effective personalization approach should achieve balanced performance across all three metrics. Following DreamBench++ [49], we employ GPT-4o [45] to assign scores to each generated image on scale from 0 (poor) to 10 (excellent). We normalize the results to the scale of [0, 1] and report them in Fig. 6, where the attribute fidelity and text fidelity are averaged into single conditioning fidelity score for visualization purposes. App. B.1 includes the original numerical results. User Study. To complement automated evaluation, we conduct user study with 10 participants. Each participant rates the three metrics described above, resulting in 10.5K individual ratings. Details of the user study are provided in App. B.1, and the results are shown in Fig. 6. Fig. 5 illustrates that CLIP [54] and DINOv2 [46] struggle to personalize abstract concepts due to the lack of support from reference attribute inputs. Although QwenVL [66] accepts additional attribute inputs, the encoder lacks architectural refinement and attribute-level contrastive learning, which results in poor adaptation to attribute personalization. In contrast, image editing models [35, 68, 70] can generate outputs that more closely resemble the reference image but often fail to disentangle the target attribute, leading to visible copy-and-paste artifacts and weak text alignment. Omni-Attribute, by comparison, achieves the best balance between image naturalness and alignment with both text and reference attribute conditioning. These observations are consistent with the quantitative results from both MLLM and human evaluations presented in Fig. 6. 4.2. Compositional Image Generation As described in Sec. 3.4, we empirically find that our attribute embeddings can achieve compositional image generation through the linear combination of conditional flow fields, as defined in Eqs. (5) and (6). Here, we qualitatively demonstrate the effect of each conditional flow field in Fig. 7 (middle right) and then progressively compose them into single coherent image in Fig. 7 (bottom), demonstrating the composability of our attribute embeddings. More results can be found in Fig. 1(b) and App. C. Figure 7. Composability of attribute embeddings. From top to bottom, each row shows the input conditions, the effect of single image-attribute pair, and the compositional results of multiple attributes, showing the composability of our attribute embeddings. The prompt is vase is standing against plain background. 4.3. Analysis of Attribute Embeddings To gain deeper understanding of the learned attribute representations, this section qualitatively analyzes them using two methods to enhance interpretability. T-SNE Visualizations of Embedding Spaces. We sample 60 images from Animal Dataset [2]. For each image, we extract embeddings with three distinct attributes: animal color, animal species, and background environment. We then project them into two dimensions using t-SNE [42] and visualize the attribute-specific embedding spaces in Fig. 8. Although derived from the same set of images, the embeddings cluster differently and meaningfully depending on the attribute, highlighting the models ability to disentangle attribute-specific information. Attribute-oriented Image Retrieval. Next, to demonstrate the models versatility in the retrieval task, we sample 17.7k images from CelebA [40] and compute embeddings for each image conditioned on three attributes: clothing, facial expression, and hairstyle. Given single query image, we retrieve the most similar image (measured by the cosine similarity of the pooled embeddings) under each attribute and visualize the results in Fig. 9. Since no existing image encoder naively supports this task, we construct text-guided retrieval baseline by combining GPT-4o [45] and CLIP [54] as detailed in B.2. Compared to the baseline, Omni-Attribute captures fine-grained, attribute-specific visual details more accurately, retrieving images that exhibit stronger alignment with the target semantic attributes. 7 Table 1. Ablation study. From top to bottom, we ablate: [a-b] the connectors with varying numbers of self-attention and linear layers; [b-d] different training strategies for the MLLM (frozen, LoRA, or full finetuning); [e-h] the hyperparameters of the contrastive loss; and [i] our final setting. We report two sets of results: (i) the gap between the cosine similarities of positive and negative attributes and (ii) text fidelity, attribute fidelity, image naturalness, and their average on attribute personalization. We color the models with top 3 performance in the groups of [a-d] and [e-h], respectively. Index Attribute Encoder Contrastive Loss MLLM Connector Weight λcon Temp. τ Cosine Sim. (pos,neg) Attribute Personalization Text-F Attr-F Natural Average Ablation of Attribute Encoder Design and Training [a] [b] [c] [d] [e] [f] [g] [h] [i] Frozen Frozen LoRA Full 1 Linear 8 Self-Attn + 1 Linear 8 Self-Attn + 1 Linear 8 Self-Attn + 1 Linear 0 0 0 - - - - 0.003 0.003 -0.002 -0.003 0.929 0.920 0.873 0.867 Ablation of Hyperparameters of Contrastive Loss LoRA LoRA LoRA LoRA 8 Self-Attn + 1 Linear 8 Self-Attn + 1 Linear 8 Self-Attn + 1 Linear 8 Self-Attn + 1 Linear LoRA 8 Self-Attn + 1 Linear 0.01 0.01 0.1 0.001 0.01 0.5 0.02 0.1 0.1 0. 0.738 0.121 0.641 0.502 0.608 0.914 0.875 0.900 0.883 0.896 0.479 0.494 0.651 0.600 0.513 0.639 0.577 0. 0.641 0.827 0.823 0.797 0.774 0.855 0.803 0.842 0.812 0.831 0.745 0.746 0.774 0.747 0.761 0.772 0.773 0. 0.789 Figure 8. T-SNE visualizations of attribute embedding spaces. We visualize the embedding spaces of the same 60 animal images across three different attributes and show that this same set of images is distributed differently and meaningfully across varying attributes. Figure 9. Qualitative results of attribute-oriented image retrieval on CelebA [40]. Our embeddings enable image retrieval based on specified attribute. Omni-Attribute surpasses the performance of text-guided retrieval by GPT-4o [45] and CLIP [54]. 8 4.4. Ablation Study We perform an ablation study to assess the impact of different training strategies and architectural choices. The quantitative results are summarized in Tab. 1. Cosine Similarity of Attribute Embeddings. To evaluate how effectively our embeddings encode attribute-specific information while suppressing unrelated visual concepts, we measure the cosine similarities of two positive attribute embeddings encoded from the semantically-linked image pair and apply the same procedure to the negative attribute. High-quality representations are expected to exhibit high similarity for positive attribute embedding pairs and low similarity for negative ones, resulting in large gap between them. We measure on 1K validation pairs, each with one randomly selected positive attribute and one negative attribute. The results are reported in Tab. 1. Our observations are as follows: Comparing [a-d] to [e-h], the models without Lcon tend to encode similar embeddings, regardless of the positive or negative attribute (i.e., trivial ), indicating the necessity of contrastive learning for attribute-level representation. Comparing [c] to [a-b], increasing the number of trainable parameters significantly improves attribute fidelity but slightly degrades text fidelity and image naturalness. Comparing [d] to [c], full finetuning the MLLM, instead, leads to knowledge forgetting [58], resulting in degraded performance on the evaluation set. From [e-h], hyperparameter choices for contrastive loss play critical role in balancing high-fidelity encoding and attribute disentanglement. Larger λcon or τ values ([e], [g]) learn more discriminative attribute embeddings (i.e., larger ), but reduce attribute fidelity, and vice versa. With careful designs of the architecture, training strategy, and hyperparameters of Lcon, Omni-Attribute ([i]) achieves the most balanced performance on attribute personalization (i.e., highest average score). 5. Limitations We identify the following limitations of our work: Attribute-specific Embeddings. Our attribute embeddings are designed to capture image information related to one or few specific attributes. This inherently constrains the applicability to tasks such as image editing, where most of the visual content must remain unchanged, and only limited set of attributes should be modified. Entanglement of Correlated Attributes. We observe that the model occasionally struggles to disentangle attributes that are often correlated, such as person identity and hairstyle. For example, as illustrated in Fig. 1, while we attempt to transfer the identity of Vincent van Gogh to new contexts, the generated images mostly preserve his hairstyle, indicating information leakage. One potential solution is to increase the sampling weight of the hairstyle dataset (as depicted in Fig. 10(d)) to better learn how to separate these factors. However, it remains an open question whether certain attributes can ever be perfectly disentangled (e.g., whether hairstyle is inherently part of identity). Sensitivity to Contrastive Learning Hyperparameters. Prior contrastive learning studies [7, 8] noted that the hyperparameters of contrastive loss, such as temperature, typically have strong and dataset-dependent impact on model performance. In this work, we also notice that the selection of these hyperparameters has huge impact on the quality of the learned attribute embeddings, as shown in Tab. 1. We leave the study of these limitations for future work. 6. Conclusion We have presented Omni-Attribute, an open-vocabulary attribute encoder that learns high-fidelity, attribute-specific representations for visual concept personalization. It is achieved through novel attribute-level annotation strategy with new dual-objective training framework that jointly optimizes generative fidelity and contrastive disentanglement. The experiments demonstrate the models versatility across attribute-based retrieval, personalization, and compositional generation, and show consistent improvements over existing baselines. We believe Omni-Attribute provides step toward controllable, interpretable visual representation learning, bridging the gap between multimodal understanding and generative manipulation. Impact Statement. This work focuses on learning attribute-level image representations for visual concept personalization. The resulting embeddings can facilitate beneficial applications, such as boosting creativity and supporting educational content creation. Beyond these considerations, we do not identify any additional ethical or societal implications beyond those already known to accompany personalized generative modeling."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from single image. In SIGGRAPH Asia, 2023. 3 [2] Sourav Banerjee. Animal image dataset. https://www. kaggle . com / datasets / iamsouravbanerjee / animal - image - dataset - 90 - different - animals, 2024. 7 [3] Edurne Bernal-Berdun, Ana Serrano, Belen Masia, Matheus Gadelha, Yannick Hold-Geoffroy, Xin Sun, and Diego Gutierrez. Precisecam: Precise camera control for text-toimage generation. In CVPR, 2025. 13 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 5 9 [5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020. 3 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. 3, 9 [8] Tsai-Shien Chen, Wei-Chih Hung, Hung-Yu Tseng, Shao-Yi Incremental false negative Chien, and Ming-Hsuan Yang. detection for contrastive learning. In ICLR, 2022. 3, 9 [9] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, TsungYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023. 3 [10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In CVPR, 2024. [11] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. In CVPR, 2025. 2, 3 [12] Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta Willamowski, and Cedric Bray. Visual categorization with bags of keypoints. In ECCVW, 2004. [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 3 [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3 [15] Yuwei Fang, Willi Menapace, Aliaksandr Siarohin, TsaiShien Chen, Kuan-Chien Wang, Ivan Skorokhodov, Graham Neubig, and Sergey Tulyakov. VIMI: Grounding video generation through multi-modal instruction. In EMNLP, 2024. 3 [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023. 2, 3 [17] Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. Tokenverse: Versatile multi-concept personalization in token modulation space. SIGGRAPH, 2025. 3 [18] Google. Nano banana. https://aistudio.google. com/models/gemini-2-5-flash-image, 2025. 3 [19] Anujraaj Argo Goyal, Guocheng Gordon Qian, Huseyin Coskun, Aarush Gupta, Himmy Tam, Daniil Ostashev, Ju Hu, Dhritiman Sagar, Sergey Tulyakov, Kfir Aberman, and Kuan-Chieh Jackson Wang. Preventing shortcuts in adapter training via providing the shortcuts. In NeurIPS, 2025. [20] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. NeurIPS, 2020. 3 [21] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 13 [22] Shaozhe Hao, Kai Han, Zhengyao Lv, Shihao Zhao, and Kwan-Yee Wong. Conceptexpress: Harnessing diffusion models for single-image unsupervised concept extraction. In ECCV, 2024. 3 [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 3 [24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 3 [25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. [26] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 3 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop, 2021. 5 [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3 [29] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. 5, [30] InstantX. Instantx flux.1-dev ip-adapter page. https:// huggingface.co/InstantX/FLUX.1devIPAdapter, 2024. 16 [31] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3 [32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, 2012. 3 [33] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, 2023. 3 [34] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX.1-dev, 2024. 13, Flux.1-dev. [35] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 3, 5, 6, 7, 17 10 [36] Jia Li, Jinming Su, Changqun Xia, and Yonghong Tian. Distortion-adaptive salient object detection in 360 omnidirectional images. IEEE Journal of Selected Topics in Signal Processing, 2019. 13 [37] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 5, 16 [38] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022. [39] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 5, 16 [40] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. In ICCV, 2015. Deep learning face attributes in the wild. 7, 8 [41] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 16 [42] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 2008. 7 [43] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. In CVPR, 2024. [44] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 3 [45] OpenAI. Gpt-4o. https://openai.com/index/ hello-gpt-4o/, 2025. 6, 7, 8, 16, 19 [46] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 2, 3, 5, 6, 7, 17 [47] Semih Orhan and Yalin Bastanlar. Semantic segmentation of outdoor panoramic images. Signal, Image and Video Processing, 2021. 13 [48] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3, 16 [49] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned bencharXiv preprint mark for personalized image generation. arXiv:2406.16855, 2024. 7 [50] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 13 [51] Guocheng Qian, Kuan-Chieh Wang, Or Patashnik, Negin Heravi, Daniil Ostashev, Sergey Tulyakov, Daniel CohenOr, and Kfir Aberman. Omni-id: Holistic identity representation designed for generative tasks. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 87868795, 2025. 3 [52] Guocheng Gordon Qian, Daniil Ostashev, Egor Nemchinov, Avihay Assouline, Sergey Tulyakov, Kuan-Chieh Jackson Wang, and Kfir Aberman. Composeme: Attribute-specific image prompts for controllable human image generation. arXiv preprint arXiv:2509.18092, 2025. [53] Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil OstaInteractive personalized shev, et al. arXiv preprint t2i via spatially-aware layered canvas. arXiv:2510.20820, 2025. Layercomposer: [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 3, 5, 6, 7, 8, 17, 19 [55] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 3 [56] Nirat Saini, Khoi Pham, and Abhinav Shrivastava. Disentangling visual embeddings for attributes and objects. In CVPR, 2022. 3 [57] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning. In CVPR, 2024. 3 [58] Reece Shuttleworth, Jacob Andreas, Antonio Torralba, and Pratyusha Sharma. Lora vs full fine-tuning: An illusion of equivalence. arXiv preprint arXiv:2410.21228, 2024. 5, [59] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 3 [60] Vincent Sitzmann, Ana Serrano, Amy Pavel, Maneesh Agrawala, Diego Gutierrez, Belen Masia, and Gordon Wetzstein. Saliency in vr: How do people explore virtual environments? IEEE transactions on visualization and computer graphics, 2018. 13 [61] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 3 [62] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [63] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. In CVPR, 2023. 3 [64] Vaswani. Attention is all you need. In NeurIPS, 2017. 3 [65] Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual exploration and inspiration. ACM TOG, 2023. 3 11 [66] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 4, 5, 6, 7, 13, 14, 16, 17 [67] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. 4, 14 [68] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3, 5, 6, 7, 13, 17 [69] Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, Chengyao Wang, Yitong Wang, et al. Dreamomni2: Multimodal arXiv preprint instruction-based editing and generation. arXiv:2510.06679, 2025. [70] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In CVPR, 2025. 3, 5, 6, 7, 17 [71] Zhichao Yang, Leida Li, Pengfei Chen, Jinjian Wu, and Giuseppe Valenzise. Language-guided visual perception disentanglement for image quality assessment and conditional image generation. arXiv preprint arXiv:2503.02206, 2025. 3 [72] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 4, 5, 15 [73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. ICCV, 2023. 13 [74] Yi Zhang, Lu Zhang, Wassim Hamidouche, and Olivier Deforges. fixation-based 360 benchmark dataset for salient object detection. In ICIP, 2020. 13 [75] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, ChienChin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Pytorch fsdp: [76] Weizhi Zhong, Huan Yang, Zheng Liu, Huiguo He, Zijian He, Xuesong Niu, Di Zhang, and Guanbin Li. Mod-adapter: Tuning-free and versatile multi-concept personalization via modulation adapter. arXiv preprint arXiv:2505.18612, 2025. 3 12 Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Training Datasets To learn high-quality representations for open-vocabulary attributes, we collect semantically linked image pairs from two complementary sources, resulting in nine datasets, as illustrated in Fig. 10. First, we collect 23.7M image pairs from an in-house image collection dataset, where images are organized into thematic collections. As shown in Fig. 10(a), images within the same collection are typically captured during single photo session, exhibiting both shared and distinct characteristics across multiple visual aspects. We randomly sample two images from each theme to form pairs, producing diverse combinations of positive and negative attributes. In total, the dataset contains 600K unique attribute labels. We qualitatively demonstrate the richness and diversity of these labels through word cloud visualization in Fig. 2. To further enhance representations for person identity, we additionally sample an identity-centric subset consisting of 2.21M image pairs, where paired images depict the same individual(s), as shown in Fig. 10(b). While this image collection dataset is large-scale and rich in attribute diversity, its image pairs often exhibit multiple entangled positive attributes, making it challenging to isolate single user-specified attribute. To address this limitation, we construct seven additional datasets, each focusing on specific attribute (e.g., facial expression, background, or lighting). As illustrated in Fig. 10(c-i), image pairs in these datasets are designed to share only one or few positive attributes, facilitating the learning of attribute-specific representations. The detailed curation process for these datasets is summarized as follows: Dataset Scale Curation Process Facial Expression 51.0K Hairstyle Pose 8.77K 106K Background 35.1K Camera Angle 98.7K Lighting and Tone 159K Style and Material 27.5K We employ the expression editing model, LivePortrait [21], to manipulate two human images initially with neutral facial expressions (according to the detailed descriptions labeled during the attribute annotation). For each pair, we randomly sample set of editing parameters to generate two images exhibiting the same facial expression. We use an in-house hairstyle editing model to process two face-centric images. Given reference image with target hairstyle, the model generates two edited images sharing the same hairstyle. We extract face, body, and hand keypoints from human image, and synthesize its paired image using ControlNet [73] to ensure the same pose as the source image. We first sample clean background images filtered by Qwen-VL [66] (by asking whether images depict clear backgrounds). Then, we use Qwen-Image-Edit [68] to randomly insert foreground objects and modify contextual factors (e.g., time of day, weather, and camera angle). Each background image is edited twice to form pair with consistent background. We collect 2,081 panoramic images from multiple sources [36, 47, 60, 74] and apply PreciseCam [3] to crop views corresponding to specific camera angles. For each pair, cropping hyperparameters are randomly sampled to ensure both images share the same camera angle. We construct set of detailed prompts describing various image lighting conditions, along with separate set of prompts specifying identities and actions. By fixing the lighting prompts while varying the identity prompts, we synthesize paired images via FLUX [34], ensuring consistent lighting across each pair. Similar to the Lighting and Tone dataset, we design descriptive prompts focusing on style and material properties. Images are synthesized using Stable Diffusion XL [50], where the same style or material description is preserved across pairs. During training, we assign sampling weight of 100 to both the image collection dataset and its identity-centric subset, and weight of 1 to each attribute-specific dataset. 13 Figure 10. Training datasets. Our training image pairs are constructed from two sources: (i) image collection datasets (ab), with image pairs captured during the same photo session, exhibiting varying positive and negative attributes; and (ii) attribute-specific datasets (ci), with image pairs synthesized via generative or editing models that differ mainly in single target positive attribute. A.2. Annotation of Positive and Negative Attributes As described in Sec. 3.1, we adopt two-stage annotation pipeline to balance annotation quality and computational cost. In the first stage, we employ powerful vision-language model, Qwen2.5-VL-72B [66], to generate high-quality attribute annotations based on comprehensive instruction prompt (see Fig. 11). Due to the substantial computational overhead, this stage is applied only to subset of 200K samples. We introduce two key design choices to further improve annotation effectiveness: (i) Inspired by Chain-of-Thoughts [67], we prompt the model to explicitly reason the detailed similarities and differences behind each positive and negative attribute. Empirically, this approach enhances annotation quality. (ii) We design the instruction prompt, ending with the initial segment of the target output format (i.e., {positive:[ at the bottom of Fig. 11) and activate the continue final message setting. This guides the model to continue generation in the intended structured format and can improve the rate of syntactically valid outputs. In the second stage, we finetune Qwen2.5-VL-32B [66] into task-dependent model tailored for this annotation task, eliminating the need for lengthy instruction prompt. During training, the input is concise multimodal prompt: <image 1> <image 2> What are the positive attributes shared by the two images and the negative attributes that differentiate them? For the similarities and differences, explain the reasons in brackets. The output is string following structured dictionary-style format. This design reduces the input token length by 3.1 and lowers the per-sample forward latency by 6.3. The model is finetuned on 200K annotated samples using 32 80GB H100 GPUs, with learning rate of 2e-7 (with linear warm-up and cosine decay strategy), batch size of 512, and 15 training epochs. For large-scale inference, we enhance computational efficiency by constraining image dimensions so that the total pixel count does not exceed 1280 28 28. This constraint yields maximum resolutions of 1336 752 for 16 : 9 images, 14 Figure 11. Instruction prompt for the first stage of attribute annotation. 1157 868 for 4 : 3 images, and 1002 1002 for square images. We perform inference on 80GB H100 GPUs, where each image pair takes approximately 2.54 seconds to annotate. A.3. Model Architecture As described in Sec. 3.3, our attribute encoder consists of LoRA-finetuned multimodal large language model (MLLM) followed by fully trainable connector module, while our image generator builds upon frozen FLUX.1-dev [34] backbone equipped with trainable IP-adapter modules [72]. We detail the model architecture as follows: 15 Module Description MLLM Connector Image Generator IP-Adapter We adopt Qwen2.5-VL-7B [66] as the base MLLM, where all model parameters are frozen, and LoRA adapters [29] are inserted into every linear projection within both the vision encoder and the MLLM modules. Each LoRA module uses rank of 256 and an alpha value of 512. The token dimensionality is 3584, and we restrict the maximum number of image tokens at 1000. To bridge the MLLM with the image generator, we follow the Step1X-Edit [39] design and incorporate linear projection layer followed by eight self-attention layers as the connector. The linear layer projects the token dimensionality from 3584 to 4096 to align with the image generator. Following the observations of Goyal et al. [19], we note that finetuning the model can diminish FLUXs ability to perform distillation guidance (i.e., it tends to overfit to the non-distillation-guidance setting used during training). To mitigate this, we adopt their proposed Shortcut-Rerouted Adapter, which helps preserve the models prior for distillation guidance. We follow the implementation of the InstantX team [30], where each DiT block [48] in FLUX includes two MLP modules (i.e., to and to v) that compute key and value embeddings, respectively. These embeddings are then injected into query image tokens through multi-head cross-attention, enabling the generator to incorporate conditioning signals for attribute personalization. A.4. Model Training We train the model in two stages to enhance training efficiency. In the first stage, the model is optimized solely with the generative loss for 100K steps; in the second stage, we introduce an additional contrastive loss and continue training for 10K more steps. This two-stage design is due to the computational overhead of the contrastive loss, which requires four additional forward passes through the MLLM for each training sample (i.e., two images cross-paired with the positive and negative attributes). Therefore, it could substantially slow down convergence if we optimize the contrastive loss from the start. We conduct all experiments using 64 80 GB H100 GPUs with total batch size of 256. Training is performed in mixed precision with parameter and reduction data types set to bf16 and fp32, respectively. We apply gradient clipping with maximum gradient norm of 1.0, and employ Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP) [75] strategies for efficient large-scale training. We use the AdamW optimizer [41] with learning rate of 1e-5, weight decay of 0.01, and β of [0.9, 0.99]. linear warmup is applied during the first 1K steps of both stages. During the first stage, we only finetune the connector and IP-Adapter modules for the first 10K steps while keeping the MLLM parameters frozen to prevent disruption of pretrained representations. For image preprocessing, we resize each reference image such that its total pixel count does not exceed 1000 28 28. To improve robustness to low-resolution inputs during inference, we apply 10% probability of downsampling augmentation. The target images are resized and center-cropped to 512512 resolution. For the generative loss, we adopt the flow-matching objective [37] with λgen of 1. We adjust the balance between generative and contrastive losses through λcon, as ablated in Sec. 4.4 and Tab. 1. B. Evaluation Details B.1. Open-vocabulary Attribute Personalization Sec. 4.1 compares Omni-Attribute with the existing models for personalization across 15 attributes, grouped into two categories. We list all evaluation attributes below: Concrete objects: man identity, woman identity, object identity, clothing, and background. Abstract concepts: hairstyle, facial expression, makeup, pose, foreground material, texture, camera angle, image lighting, image tone, and artistic style. To complement the qualitative and quantitative evaluation shown in Figs. 5 and 6, we list the full evaluation prompts used in Fig. 5 in Tab. 2, and report the original numerical values visualized in Fig. 6 in Tab. 3. As described in Sec. 4.1, we apply both MLLM-based and human evaluations for comprehensive assessment. For the MLLM evaluation, we query GPT-4o [45] three times using the prompts shown in Fig. 12 to measure text fidelity, image fidelity, and image naturalness. For the user study, participants are presented with the reference image-attribute pair, the prompt, and the generated image for each sample, as shown in Fig. 13. They are then asked to rate the three evaluation metrics on scale from 1 (poor) to 5 (excellent). All scores are subsequently normalized to the range of [0,1]. 16 Table 2. Full evaluation prompts used in Fig. 5. Reference Attribute Full Prompt Person Identity woman in side view that looks festive and blowing party horn with laughing expression, surrounded by colorful streamers and balloons. Dog Identity Clothing Hairstyle dog playing in bright, minimalist art gallery with polished floors and white walls displaying modern paintings. Natural light filters through glass doors, highlighting clean, contemporary aesthetic of calm and creativity. joyful young woman with long brunette hair leaping midair in white space, frozen in motion. Her athletic form radiates freedom, energy, and the graceful strength of dance. woman wearing denim jacket over white top. Her ears are adorned with large hoop earrings, and she has bold makeup featuring defined eyebrows and bright lipstick. The background is soft, solid pink hue. Facial Expression woman with dark hair styled in messy bun. She is wearing plain white shirt, and the background is clean, neutral white. Makeup Pose woman with long, wavy brown hair, wearing white t-shirt featuring bold red and blue graphics. The background includes bright turquoise frame against neutral-colored wall, creating striking contrast. low-angle view joyful young woman midair against bright blue sky. Wearing orange trousers and green sneakers, she embodies freedom, exuberance, and carefree spontaneity. Foreground Material handbag equipped with chain strap resting on clean white block. The background is soft gradient of light blue, creating calm and sophisticated setting. Texture Image Lighting Artistic Style sleek Mclaren sports car parked on paved street. The vehicles aerodynamic design and shiny exterior reflect its high-performance nature. In the background, brick building with large windows and various utility fixtures stands, along with striped awning and some greenery. pristine armchair with tufted backrest and decorative buttons standing against clean, minimalist backdrop. Its elegant design features slender, spindle-like legs that add touch of classic charm to its modern aesthetic. The seat cushion appears soft and inviting, complementing the chairs overall refined look. graceful white horse galloping across dirt path, its mane and tail flowing in the breeze. The background is lush, dense forest bathed in soft, golden light, creating serene and natural setting. Table 3. Numerical results of open-vocabulary attribute personalization. We complement the quantitative comparison graphs  (Fig. 6)  by providing the exact measurements of text fidelity (Text-F), attribute fidelity (Attr-F), image naturalness (Natural), and their average. Concrete Objects Abstract Concepts Text-F Attr-F Natural Average Text-F Attr-F Natural Average Method MLLM Evaluation CLIP [54] DINOv2 [46] Qwen-VL [66] OmniGen2 [70] FLUX-Kontext [35] Qwen-Image-Edit [68] 0.9000 0.8460 0.8820 0.9140 0.8540 0.5910 0.6550 0.7747 0.6848 0.7890 0.8910 0.8980 0.8400 0.8160 0.8460 0.7810 0.7091 0.8091 0.7983 0.8122 0.8043 0.8280 0.8180 0.7660 0.9504 0.9168 0.9760 0.9512 0.9304 0. 0.3120 0.3568 0.2736 0.2863 0.3363 0.7515 0.8056 0.8073 0.8072 0.7435 0.7440 0.6895 Omni-Attribute 0.9381 0.7634 0. 0.8518 0.8539 0.5181 0.8079 Human Evaluation CLIP [54] DINOv2 [46] Qwen-VL [66] OmniGen2 [70] FLUX-Kontext [35] Qwen-Image-Edit [68] 0.9300 0.9087 0.9242 0.9376 0.9054 0. 0.6050 0.6844 0.5505 0.7575 0.8785 0.8913 0.8477 0.8434 0.8542 0.8110 0.8032 0.8426 0.7942 0.8122 0.7763 0.8354 0.8624 0.8131 0.9159 0.9179 0.9445 0.9462 0.9323 0.6133 0.4331 0.4363 0.3957 0.4392 0.4688 0.8622 0.8576 0.8684 0.8754 0.8533 0.8910 0. Omni-Attribute 0.9564 0.7691 0.8680 0.8645 0. 0.7031 0.9584 17 0.6893 0.6936 0.6856 0.6603 0.6702 0.6074 0.7267 0.7356 0.7409 0.7385 0.7463 0.7641 0. 0.8663 Figure 12. Instruction prompt for MLLM evaluation. Figure 13. Interface of the user study. Given the input conditions (top and right) and the generated image (center), participants are asked to rate three aspects: image naturalness, text fidelity, and attribute fidelity on 1 (poor) to 5 (excellent) scale using the sliders (left). 18 B.2. Attribute-oriented Image Retrieval Since there is no existing model directly supporting attribute-oriented image retrieval, we construct text-guided baseline using GPT-4o [45] and CLIP [54]. Specifically, we first prompt GPT-4o to generate descriptive texts of approximately 60 words for each target attribute. These descriptions are then converted into text embeddings using CLIP, which are subsequently used to retrieve the most semantically similar images corresponding to the given attribute. C. Additional Results Fig. 1(a) illustrates that Omni-Attribute can extract high-fidelity, attribute-specific information while suppressing irrelevant visual details. This helps reduce copy-and-paste artifacts and leads to more coherent synthesis of the user-specified attribute in new contexts. Additional results demonstrating such attribute disentanglement are shown in Fig. 14. To further showcase the practical utility of Omni-Attribute, we design four real-world application scenarios: (i) advertisement image synthesis, (ii) hairstyle customization, (iii) storytelling visualization, and (iv) creative content generation. The corresponding results are shown in Fig. 15. 19 Figure 14. Additional results of attribute disentanglement. Each row shows three generated images (right), which are conditioned on the same reference image (left) and the same textual prompt, but with different attribute inputs (colored boxes). As seen, given the same reference image, Omni-Attribute effectively extracts attribute-specific representations, enabling the coherent synthesis of the user-specified attribute in new contexts while reducing the leakage of irrelevant visual information from the reference image. Figure 15. Practical and creative applications of Omni-Attribute. From top to bottom, each row demonstrates the practical utility of Omni-Attribute across four real-world applications: (i) advertisement image synthesis, (ii) hairstyle customization, (iii) storytelling visualization, and (iv) creative content generation."
        }
    ],
    "affiliations": [
        "CMU",
        "Snap Inc.",
        "UC Merced"
    ]
}