{
    "paper_title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
    "authors": [
        "Shengda Fan",
        "Xuyan Ye",
        "Yankai Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC."
        },
        {
            "title": "Start",
            "content": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution Shengda Fan1*, Xuyan Ye1, Yankai Lin1 1 Gaoling School of Artificial Intelligence, Renmin University of China {fanshengda, yexvyan0923, yankailin}@ruc.edu.cn 6 2 0 2 0 2 ] . [ 1 1 6 7 3 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Self-play with large language models has emerged as promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where document-augmented teacher generates highquality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is modelagnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations. The code is available at https: //github.com/RUCBM/DARC."
        },
        {
            "title": "Introduction",
            "content": "Self-improving artificial intelligence, which enables models to autonomously refine their capabilities without human intervention, is widely viewed as an important step toward more general and potentially superhuman intelligence (Schmidhuber, 2007). While large language models (LLMs) have demonstrated remarkable progress in complex reasoning tasks (Luo et al., 2023; Liu et al., 2025c), * The first two authors contributed equally. Corresponding author. Figure 1: Comparison between DARC and previous coupled self-play methods. their success largely relies on extensive human supervision. However, as the availability of highquality human-annotated data approaches its limit, this scarcity becomes fundamental bottleneck for further scaling. Consequently, developing reliable and efficient self-evolution mechanisms that operate independently of human data has emerged as pivotal research frontier. widely adopted approach for LLM selfevolution is self-play, which establishes coevolutionary loop between Questioner that proposes tasks and Solver that attempts to solve them (Silver et al., 2017; Sukhbaatar et al., 2017). In this paradigm, the Questioner optimizes difficulty-calibration reward based on the Solvers current performance, while the Solver is updated on pseudo-labeled data constructed from the tasks generated by the Questioner. Despite its appeal, such tightly coupled systems are prone to premature plateaus or even performance collapse (Huang et al., 2025). We argue that this instability stems from two inherent issues in the coupled design: non-stationary optimization targets and selfconfirmation bias. As shown in the upper part of Figure 1, the Questioner is continually optimized against moving target determined by the evolving Solver. Its learned difficulty signal can become stale or even misaligned after each Solver update, inducing oscillatory shifts in task difficulty and unstable optimization dynamics. In parallel, the Solver is trained on self-generated pseudolabels that are inevitably noisy, so errors can be propagated and amplified across iterations. To address the coupled instability in self-play, we propose Decoupled Asymmetric Reasoning Curriculum (DARC), which restructures selfevolution into two decoupled and sequential stages: Questioner training and Solver training, as illustrated in the lower part of Figure 1. In Stage 1, instead of chasing moving Solver boundary, we train the Questioner with objective supervision from explicit difficulty levels and external documents, enabling it to generate corpus-grounded questions that are calibrated to specified difficulty. This decoupling removes the dependence of question generation on the Solvers real-time performance, thereby mitigating non-stationary optimization targets. In Stage 2, we replace selftraining on noisy pseudo-labels with an asymmetric self-distillation scheme. Concretely, we introduce privileged teacher Solver that has access to the source document and performs majority voting to construct pseudo-labels, and distill its outputs into student Solver that receives only the question as input. This asymmetric design (documentaugmented teacher vs. question-only student) reduces label noise and alleviates self-confirmation bias, yielding more stable and reliable learning signal for Solver improvement. Our experiments demonstrate that DARC is model-agnostic, consistently improving the reasoning abilities of both Qwen-based (Yang et al., 2025) and LLaMA-based (Wang et al., 2025b) backbones. Notably, applying DARC improves the average accuracy by 10.9 points over the base models, outperforming label-free self-evolving methods RZero (Huang et al., 2025) and Absolute-Zero (Zhao et al., 2025), corpus-grounded self-play method SPICE (Liu et al., 2025b) and weakly supervised self-play methods R-Few (Yu et al., 2025). Notably, DARC approaches the performance of GeneralReasoner (Ma et al., 2025), which is trained on the full 232K WebInstruct dataset (Yue et al., 2024), despite using no human annotations. Beyond empirical gains, we further demonstrate several key properties of DARC: (i) it does not merely memorize the training corpus; (ii) the difficulty rankings are solver-independent; and (iii) the decoupled Questioner learns curriculum improving the performance of heterogeneous Solver backbones."
        },
        {
            "title": "2 Related Work",
            "content": "Reinforcement Learning for LLM Reasoning. Reinforcement learning (RL) is pivotal for advancing LLM reasoning, but key bottleneck lies in how to obtain reliable supervision signals. primary paradigm employs external verifiers (Guo et al., 2025) to provide rule-based rewards, but this approach is largely restricted to deterministic fields like mathematics and code. Alternatively, selfsupervision methods derive signals via maximizing confidence (Prabhudesai et al., 2025) or getting pseudo-labels by majority voting (Zuo et al., 2025), yet they remain dependent on curated question sets. In contrast, DARC transcends these limitations by operating on raw corpora, generating both tasks and supervision signals in self-evolving manner. Self-Play for LLM Self-Evolution. Self-play is widely adopted paradigm for enabling selfevolution in LLMs (Chen et al., 2024). One line of research focuses on code-centric self-play, where execution results provide grounded supervision and enable stable co-evolution between coder and verifier (Zhao et al., 2025; Wang et al., 2025a; Lin et al., 2025). parallel line extends selfplay to general reasoning via questionersolver framework, in which Questioner proposes tasks and Solver learns from self-generated supervision (Huang et al., 2025; Liu et al., 2025b; Chen et al., 2025). Unlike code-centric settings that can rely on executable feedback for supervision, general-reasoning self-play often suffers from nonstationary rewards and unstable optimization. In contrast, our approach stabilizes self-evolving optimization by decoupling training and introducing asymmetric self-distillation. Data Synthesis for LLM Training. Synthetic data generation is pivotal for scaling reasoning supervision. Bootstrapping frameworks, including STaR (Zelikman et al., 2022), ReST (Gulcehre et al., 2023), and WizardLM (Xu et al., 2024), expand datasets by iteratively evolving instructions from seed exemplars. Meanwhile, corpusdriven pipelines like WebInstruct (Yue et al., 2024) and General-Reasoner (Ma et al., 2025) synthesize question-answer pairs by leveraging web mining or model priors to broaden domain coverage. While these approaches effectively scale data quantity or diversity, they often rely on superior external models (e.g., GPT-4o, Gemini) to guarantee data quality. In contrast, DARC dispenses with external teacher models by deriving both task difficulty and supervision signals internally, enabling stable self-evolution directly from unlabeled corpora."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we present the DARC framework, which decomposes LLM self-evolution into two sequential stages. In the first stage (Section 3.1), the Questioner is trained for controllable question generation based on explicit difficulty levels and an external corpus. In the second stage (Section 3.2), the trained Questioner is used to construct an offline reasoning curriculum for training the Solver via asymmetric self-distillation. Finally, in Section 3.3, we provide theoretical analysis that explains how the proposed decoupling and training strategy mitigates optimization instability in selfevolving systems. Figure 2 visually outlines the DARC framework. 3.1 Questioner: Difficulty-Aware Generation Given document sampled from the corpus and target difficulty scalar τ [0, 1], the Questioner Qθ defines conditional generation policy Qθ( d, τ ), which aims to produce questions grounded in and calibrated to the specified difficulty level. To train the Questioner, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to optimize the policy using reward function that jointly enforces document grounding and difficulty alignment. Concretely, for each generated question q, the reward is computed through two-stage evaluation procedure. First, we employ an LLM-as-a-Judge to verify whether the question is grounded in the source document d. Questions that fail this grounding check are assigned negative reward to discourage hallucinated or irrelevant generations. Second, for grounded questions, we estimate their empirical difficulty using fixed Solver S( q). Specifically, we sample candidate answers {ˆa1, . . . , ˆaN } from fixed base model serving as the Solver, and compute the empirical success rate ˆs ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 I[ˆaj = a], (1) where denotes the pseudo-label obtained via majority voting from document-augmented Solver (see Section 3.2). We denote this empirical success rate as the difficulty estimator D(q) = ˆs."
        },
        {
            "title": "The final reward for question q is then defined as",
            "content": "rQ(q) = (cid:40) 1 D(q) τ , 1, if is grounded in d, otherwise. (2) This reward formulation penalizes ungrounded questions while encouraging the generated questions to match the target difficulty τ . 3. Solver: Offline Curriculum Learning via Asymmetric Self-Distillation After training the Questioner, we freeze its parameters and construct an offline question set = {(di, τi, qi)}M i=1, qi Qθ( di, τi). (3) We adopt curriculum learning (Bengio et al., 2009) by ordering questions from easy to hard according to the specified difficulty level τi, and train the Solver progressively along this curriculum. To obtain supervision without external annotations, we employ asymmetric self-distillation. For each (d, q) U, we introduce privileged teacher Solver with access to the source document, which generates multiple candidate answers: 1 , . . . , ˆa(d) ˆa(d) Sϕ( d, q). (4) }N We obtain pseudo-label by majority voting over {ˆa(d) i=1. We discard samples with vote agreement below γ to reduce label noise. The remaining samples are used to train student Solver Sϕ( q), which shares parameters with the privileged teacher but doesnt have access to d. This asymmetry discourages trivial copying from the document and reduces confirmation bias, forcing the student to learn to solve problems from questions alone. We optimize the student Solver with correctness reward: rS(a) = I[a = a], Sϕ( q). (5) Figure 2: Illustration of the two-stage DARC framework. In the first stage (the upper half), the Questioner learns to generate questions matching specified difficulty τ with difficulty-anchored reward. In the second stage (the lower half), the Solver is trained on an offline curriculum with an answer correctness reward. Together, this offline curriculum learning scheme with asymmetric self-distillation provides stable and scalable training signal, enabling effective optimization without external supervision."
        },
        {
            "title": "3.3 Theoretical Analysis",
            "content": "We provide self-contained theoretical analysis to elucidate why coupled self-play is inherently unstable from an optimization perspective, and how the proposed decoupling strategy alleviates the issue."
        },
        {
            "title": "3.3.1 Toy Model of Coupled Self-Play\nWe consider a simplified one-dimensional ab-\nstraction that captures the essential dynamics of\ndifficulty-controlled self-play.",
            "content": "Setup. For theoretical tractability, each question is characterized by scalar difficulty τ (relaxed from [0, 1]), where larger values indicate harder questions. The Solver is parameterized by scalar ability parameter ϕt at iteration t. The probability that the Solver successfully answers question of difficulty τ is defined as vϕt(τ ) := σ(ϕt τ ), (6) where σ(z) = (1 + ez)1 is the logistic function. The Questioner is parameterized by θ and induces questions of distribution over difficulties, Questioner is trained to generate boundary questions whose empirical success rate is close to 0.5. This is formalized via the shaped objective (cid:104) ψ(vϕt(τ )) (cid:105) Jt(θ) := Eτ πθ , ψ(u) := 1 2 . (8) By construction, Jt(θ) is maximized when πθ concentrates its mass near τ = ϕt."
        },
        {
            "title": "3.3.2 Structural Instability in Coupled",
            "content": "Self-Play We now show that in the above minimal setting, Solver updates can cause the Questioners reward ascent direction to become stale, such that an update that is optimal for the current objective provably harms the next objective. Theorem 1 (Gradient direction reversal under coupling). Consider the toy model above. Assume the Solver updates according to ϕt+1 = ϕt + η, η = 0. (9) Let the Questioner perform single gradient ascent step θt+1 = θt + αgt, gt Jt(θt), (10) where Jt(θt) denotes (sub)gradient and α > 0 is sufficiently small. τ πθ(). (7) Then, for any δ satisfying Following common practice in self-play systems (Huang et al., 2025), we assume the δ (cid:40) (0, η), (η, 0), if η > 0, if η < 0. (11) if πθt concentrates around τ = ϕt + δ (e.g., [τ (ϕt + δ) ϵ] 1 for sufficiently Prτ πθt small ϵ), there exists constant c(η, δ) > 0 such that Jt+1(θt+1) Jt+1(θt) c(η, δ) α < 0. (12) That is, Questioner update that ascends Jt necessarily decreases the next-round objective Jt+1. Proof sketch. Under the stated concentration condition, the sign of the local ascent direction is governed by the typical difficulty τ ϕt + δ. If δ > 0, then vϕt(τ ) = σ(δ) < 1/2 and the ascent direction pushes πθ toward smaller difficulties (easier questions). If δ < 0, then vϕt(τ ) = σ(δ) > 1/2 and the ascent direction pushes πθ toward larger difficulties and harder questions. After the Solver update, we have ϕt+1 τ = η δ, which changes sign whenever δ and η have the same sign and δ < η, implying vϕt+1(τ ) = σ(η δ), (13) which lies on the opposite side of 1 2 compared to vϕt(τ ). Therefore, the previous ascent direction becomes descent direction for Jt+1, i.e., Jt+1(θt), gt < 0. Applying first-order Taylor expansion around θt gives Jt+1(θt+1)Jt+1(θt) = αJt+1(θt), gt+O(α2). (14) Since the directional derivative is strictly negative under the sign flip condition, choosing α > 0 sufficiently small ensures the O(α2) term is dominated, yielding Jt+1(θt+1) Jt+1(θt) c(η, δ) α < 0, (15) where c(η, δ) > 0. Interpretation. Theorem 1 formalizes structural instability in coupled self-play: even when the Questioner follows an ascent direction of its current objective, Solver learning can immediately invalidate this direction. This phenomenon is not caused by stochastic noise or large learning rates, but by objective drift induced by Solver updates. As result, proposer gradients become intrinsically stale, making stable optimization difficult without severely restricting Solver learning. Empirical Analysis. To nonstationarity predicted by Theorem 1, we reproduce R-Zero (Huang et al., 2025), coupled self-play probe the Figure 3: Cross-Iteration Accuracy Heatmap in Coupled Self-Play. Golden label is annotated by DeepSeek3.2 (Liu et al., 2025a) and only for analytical purposes. loop, that alternates (i) training Questioner Qi against the frozen Solver Si1 and (ii) training new Solver Si on questions sampled from Qi. Figure 3 reports cross-iteration accuracies If the coupled between the above iterations. loop were stable, we would expect coherent for fixed Solver (each column), structure: later Questioners should become systematically harder (accuracy decreasing with i), and for fixed Questioner (each row), later Solvers should improve (accuracy increasing with t). The heatmap shows pronounced non-monotonicity along both axes, indicating instability in the coupled self-play loop. Diagonal accuracies remain below the 0.5 boundary (mean 0.443) with no convergence trend, while row-wise means oscillate: after Q3 becomes harder, Q4 turns uniformly easier for all Solvers (showing bright horizontal band), followed by harder Q5. This behavior aligns since the with solver-induced objective drift: Questioner objective Jt(θ) is defined relative to moving Solver boundary vϕt(τ ), Solver updates can invalidate or reverse previous ascent directions, producing non-monotonic and solver-specific difficulty shifts, rather than globally ordered and progressively aligned curriculum."
        },
        {
            "title": "3.3.3 Why Decoupling Resolves the Instability\nIn DARC, the Questioner is trained using a\ndifficulty-matching objective",
            "content": "(cid:101)J(θ) = E(d,τ ) Eqπθ(d,τ ) (cid:2) ρ(D(q), τ ) (cid:3), (16) where D(q) is fixed difficulty estimator and ρ encourages D(q) to match the target difficulty τ . Crucially, (cid:101)J doesnt depend on the time-varying Solver. Therefore, gradient ascent steps on (cid:101)J do not suffer from the direction reversal phenomenon described in Theorem 1. And the Questioner can be trained stably using standard policy optimization methods, and difficulty progression is controlled explicitly via curriculum scheduling rather than implicitly through unstable self-play dynamics."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Models To evaluate the generality of DARC across model scales and architectures, we adopt representative backbones from both the Qwen and LLaMA families, including Qwen3-4B/8B-Base and OctoThinker-8B-Hybrid-Base. Baselines We compare DARC with representative baselines: (1) Base Model, the pretrained checkpoint without post-training; (2) Absolute Zero (Zhao et al., 2025), domain-specific grounded self-play method; (3) R-Zero (Huang et al., 2025), label-free self-evolving approach; (4) R-Few (Yu et al., 2025), weakly supervised self-evolution method; and (5) SPICE (Liu et al., 2025b), corpus-grounded self-play framework. In addition, we report General-Reasoner (Ma et al., 2025), trained on approximately 230K humanannotated data, as supervised reference for calibrating model capabilities. We reproduce SPICE using the same prompts and corpus as DARC, while results for other baselines are taken from Yu et al. (2025). For R-Few, we report its 1% setting, which uses approximately 2.3K human-labeled WebInstruct samples. Benchmarks We evaluate DARC on both mathematical and general reasoning benchmarks. For mathematical reasoning, we use MATH500 (Hendrycks et al.), GSM8K (Cobbe et al., 2021), OlympiadBench (He et al., 2024), Minerva Math (Lewkowycz et al., 2022), and AMC (Mathematical Association of America, n.d.), following the simple-evals protocol with GPT-4o as an automatic judge. For general reasoning, we evaluate on MMLU-Pro (Wang et al., 2024), SuperGPQA (Du et al., 2025), GPQA-Diamond (Rein et al., 2024), and BBEH (Kazemi et al., 2025), using greedy decoding and exact-match evaluation. Refer to Appendix and for experimental details."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 1 summarizes the results on both mathematical and general-domain reasoning benchmarks. We draw three observations. First, self-evolution consistently improves reasoning, with larger gains on weaker backbones and stronger gains on math. Across all backbones, self-evolving methods improve over the corresponding base checkpoints, with larger relative gains on initially weaker models such as OctoThinker. Improvements are also typically larger on mathematical reasoning than on general reasoning, likely due to more deterministic supervision and clearer correctness signals in math evaluations. Second, DARC is the strongest among label-free self-evolving baselines and remains competitive with weakly supervised self-play. DARC consistently outperforms all label-free baselines including R-Zero, Absolute Zero, and SPICE on both average score of mathematical and general reasoning benchmarks across all three backbones. Relative to weakly supervised approaches such as R-Few, DARC remains competitive while requiring no human annotations. On average, DARC improves the average score by 10.9 points over the base models, validating the effectiveness of our decoupled framework without tightly coupled co-evolution. Third, DARC can approach supervised pipelines at sufficient model scale. With label-free Qwen3-8B as the backbone model, DARC matches the overall average performance of the supervised General-Reasoner. This indicates that strong base models can achieve competitive reasoning performance through self-evolution alone, without relying on annotation-intensive supervised training."
        },
        {
            "title": "4.3 Analysis & Discussion",
            "content": "Solver Training dynamics. We analyze the Solver training dynamics to assess training stability. As shown in Figure 4(a), the training reward increases sharply at the beginning and then gradually saturates. We observe two transient drops around steps 32 and 64, which coincide with scheduled curriculum transitions from Easy to Medium, and from Medium to Hard, respectively. This interpretation is corroborated by Figure 4(c), where the number of active prompts rises at the same steps, suggesting that the optimization is exposed to more challenging prompt set. Importantly, as shown in Figure 4(b), the validation reward exhibits steady upward trend without degradation, contrasting with the training collapse reported in prior self-play systems. Overall, these dynamics provide evidence that DARC improves the stability Models AMC Minerva MATH GSM8K Olympiad Math Avg. MMLU Pro Super GPQA GPQA Diamond BBEH General Avg. Avg. Mathematical Reasoning General Reasoning Qwen3-4B-Base General-Reasoner Base Model + R-Zero + Absolute Zero + SPICE + R-Few (1%) + DARC (Ours) Qwen3-8B-Base General-Reasoner Base Model + R-Zero + Absolute Zero + SPICE + R-Few (1%) + DARC (Ours) 60.0 47.5 48.2 50.0 50.9 52.7 60.3 64.8 61.5 62.8 62.5 60.9 69.3 68.9 OctoThinker-8B-Hybrid-Base Base Model + R-Zero + Absolute Zero + SPICE + DARC (Ours) 27.5 32.5 32.5 35.2 31.9 57.7 42.3 51.2 41.9 55.5 52.1 57. 62.6 49.3 58.8 52.9 55.2 59.6 61.4 22.1 33.1 34.9 40.8 43.0 80.6 68.2 74.8 76.2 77.9 77.8 77.6 83.4 74.4 80.6 76.6 81.4 81.6 83.0 44.2 58.4 56.8 58.4 62.4 92.2 72.6 90.6 89.3 91.9 92.3 91. 92.7 90.9 92.4 92.0 93.8 94.0 94.0 68.6 85.2 87.0 87.3 88.0 47.7 34.8 40.6 41.5 41.9 42.4 45.8 46.3 40.4 43.4 47.8 48.0 44.0 48.4 16.7 22.6 25.6 25.6 30.7 67.6 53.1 61.1 59.8 63.6 63.5 66. 70.0 63.3 67.6 66.4 67.9 69.7 71.1 35.8 46.4 47.4 49.5 51.2 62.8 51.6 54.2 52.6 56.5 55.9 56.9 65.1 58.0 61.6 62.5 61.0 62.8 62.3 14.7 37.4 31.4 41.3 43.8 32.5 25.4 27.8 27.1 28.3 29.4 29. 35.3 30.4 31.8 33.5 32.4 32.7 32.8 11.4 17.9 18.8 19.9 22.3 42.9 26.3 36.4 35.3 37.9 35.4 38.9 42.9 33.3 40.5 36.8 40.4 40.4 44.4 15.7 21.7 27.8 29.8 32.3 12.2 8.1 10.4 8.3 11.3 11.2 11. 10.8 10.5 11.3 10.8 12.1 11.8 11.8 0.6 7.8 5.0 7.2 10.8 37.6 27.9 32.2 30.8 33.5 33.0 34.1 38.5 33.1 36.3 35.9 36.5 36.9 37.8 10.6 21.2 20.8 24.5 27.3 54.3 41.9 48.2 46.9 50.2 49.9 52. 56.0 49.9 53.7 52.8 53.9 55.1 56.3 24.6 35.2 35.5 38.4 40.6 Table 1: Performance comparison on reasoning benchmarks (%). Bold and underlined values indicate the best and second-best performance within each model scale. Shaded rows indicate that General-Reasoner is supervised reference and is not directly comparable to self-evolving baselines due to its different supervision regime. (a) Training reward. (b) Validation reward. (c) Active prompts in GRPO. Figure 4: Training dynamics of DARC under curriculum-ordered training and random shuffling on the same offline question set. The validation reward in (b) is evaluated on the Math12K test set. of LLM evolution by maintaining more reliable training signals. Analysis of Questioner training dynamics is listed in Appendix C. the supervisory signal. These results validate our asymmetric distillation framework while highlighting document length as key constraint. Effect of Asymmetric Self-Distillation. To assess the efficacy of asymmetric distillation, we compare document-grounded prompting (questions + documents) against question-only prompting, measuring relative gains via Avg@8 win rates. Table 2 shows that document augmentation consistently yields win rates > 50% in shortand mediumcontext regimes, confirming its utility for supervision. Notably, Qwen2.5-7B-Instruct outperforms the base model, indicating that instruction tuning improves evidence extraction and noise resilience. However, gains diminish in long-context scenarios (> 5K tokens), likely due to excessive document length introduces extraneous noise that may dilute"
        },
        {
            "title": "Model",
            "content": "Short Med."
        },
        {
            "title": "Long",
            "content": "Qwen3-4B-Base Qwen2.5-7B-Instruct 52.9 54.5 54.8 60.0 39.2 48.7 Avg. 50.9 53. Table 2: Avg@8 win rate (%) of document-augmented prompting over non-augmented prompting on discrepant instances, grouped by document lengths. Cross-model consistency of question difficulty. To evaluate whether reinforcement learning enables the Questioner to generate questions of different difficulty, we measure various Solvers average accuracy. As shown in Figure 5, all Solver accuracy decreases monotonically from Easy to Hard across"
        },
        {
            "title": "Model",
            "content": "Qwen3-1.7B-Base + DARC Qwen3-8B-Base + DARC Qwen3-4B-Base (Human) + DARC Math Avg. General Avg."
        },
        {
            "title": "Model",
            "content": "Math Avg. General Avg. 49.6 51.4(+1.8) 63.3 70.9(+7.6) 65.5 66.7(+1.2) 21.3 25.9(+4.6) 33.1 37.5(+4.4) 33.2 34.4(+1.2) Qwen3-4B-Base + Vanilla FT + DARC Qwen3-8B-Base + Vanilla FT + DARC 53.1 62.9(+9.8) 66.7(+13.6) 63.3 65.5(+2.2) 71.1(+7.8) 27.9 31.3(+3.4) 34.1(+6.2) 33.1 35.5(+2.4) 37.8(+4.7) Table 3: Experimental results of DARC with backbones different from Questioner training (%). Qwen3-4BBase (Human) indicates GRPO tuning on the humanannotated Math12K training dataset. Figure 5: Accuracy of different Solvers on questions generated under different difficulty levels. all models, indicating that different input conditions naturally give rise to progressively harder questions. Notably, this monotonic trend is preserved across backbones, suggesting that the induced difficulty ordering is largely independent of the Solver backbone used. These results indicate that the Questioner learns solver-agnostic and input-driven difficulty partition. Appendix presents case study of the generated questions. Cross-solver generalization of the Questioner. To evaluate whether the learned Questioner generalizes beyond the Solver backbone used during training, we reuse the question set generated by the Qwen3-4B-Base Questioner to train different Solvers. As shown in Table 3, both larger 8B model and smaller 1.7B model achieve consistent performance improvements, demonstrating robust cross-solver generalization. Moreover, further tuning the trained models on human-annotated data using our synthesized question set yields additional performance gains, indicating that the self-evolving curriculum is complementary to human supervision. These results validate our core design: decoupled question generation creates versatile, reusable curriculums that benefit heterogeneous Solvers without overfitting to specific backbone. Table 4: Comparison between DARC and finetuning with next-token prediction on the same corpus (%). hances reasoning capabilities beyond merely memorizing task-specific corpus, we compare it against baseline fine-tuned on the same data using the standard next-token prediction objective (Vanilla FT). As shown in Table 4, while both methods improve upon the base models, DARC consistently achieves larger gains across model scales and domains. Notably, although both methods exhibit diminishing absolute gains as model size increases, Vanilla FT saturates faster. Consequently, the performance gap between DARC and Vanilla FT widens from an average of 3.3 points on the 4B model to 3.9 points on the 8B model. These results indicate that DARC more effectively exploits the corpus to induce transferable reasoning abilities rather than surface-level pattern memorization."
        },
        {
            "title": "Ablations",
            "content": "Math Avg. General Avg. 66.7 34.1 w/o Asymmetric Distillation w/o Specialized Questioner w/o Difficulty Awareness 65.0 (-1.7) 65.5 (-1.2) 65.3 (-1.4) 33.6 (-0.5) 32.7 (-1.4) 32.7 (-1.4) Table 5: Ablation study (%). w/o Asymmetric Distillation: majority-voting pseudo-labels. w/o Specialized Questioner: generic model Qwen3-4B as Questioner. w/o Difficulty Awareness: easy questions only training. Ablation Study. We conduct an ablation study to isolate the contribution of each core component in DARC. As reported in Table 5, removing any of the components consistently degrades performance, indicating that these modules are complementary. Notably, replacing our trained Questioner with generic strong model Qwen3-4B also yields worse results, suggesting that the learned, difficulty-aware Questioner provides more effective curriculum."
        },
        {
            "title": "5 Conclusion",
            "content": "DARC Improves reasoning beyond corpus memorization. To examine whether DARC enIn this work, we introduce the DARC framework, which adopts decoupled training and asymmetric self-distillation to stabilize self-evolving. Extensive results suggest that DARC consistently outperforms existing baselines. We hope this work provides useful insights for LLM self-evolution."
        },
        {
            "title": "Limitations",
            "content": "While the proposed framework represents notable step toward stable LLM self-evolution, it still has several limitations. First, DARC relies on an external corpus to ground both the Questioner and the Solver, which constrains its applicability in fully data-free scenarios. Second, the pseudolabels produced via asymmetric self-distillation are inevitably noisy, potentially limiting further performance gains. Third, the current framework is primarily designed for domains with verifiable answers, which restricts its applicability to openended tasks. We leave addressing these limitations to future work."
        },
        {
            "title": "Ethical Statement",
            "content": "This work investigates self-evolution of large language models purely as research methodology, without deployment in real-world or user-facing scenarios. The models operate in an offline experimental environment and are not used for autonomous decision-making. Therefore, potential risks are minimal and largely limited to methodological concerns. We use only publicly available corpora (e.g., DataComp-LM and Nemotron-CC-Math) and do not collect any user data. We rely on the datasets documented curation/filtering procedures to reduce personally identifying information (PII) and offensive content. We do not release raw training text or analyze any information at the level of individual people; we only report aggregated benchmark results. Any remaining PII/offensive content in web-scale corpora is treated as an inherent limitation. ChatGPT was used solely to assist with language refinement and improving the clarity of presentation. It did not contribute to the development of ideas, experimental design, data analysis, interpretation of results, or drawing scientific conclusions."
        },
        {
            "title": "References",
            "content": "Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148. Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. 2025. Self-questioning language models. arXiv preprint arXiv:2508.03682. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, and 1 others. 2025. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, and 1 others. 2023. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828 3850. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. 2025. R-zero: SelfarXiv evolving reasoning llm from zero data. preprint arXiv:2508.05004. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Yuanzhu Peter Chen, and 1 others. 2025. BigIn Proceedings of the 63rd Anbench extra hard. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26473 26501. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, and 1 others. 2022. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, and 40 others. 2024. Datacomplm: In search of the next generation of training sets for language models. In Advances in Neural Information Processing Systems, volume 37, pages 14200 14282. Zi Lin, Sheng Shen, Jingbo Shang, Jason Weston, and Yixin Nie. 2025. Learning to solve and verify: selfplay framework for code and test generation. arXiv preprint arXiv:2502.14948. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and 1 others. 2025a. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Bo Liu, Chuanyang Jin, Seungone Kim, Weizhe Yuan, Wenting Zhao, Ilia Kulikov, Xian Li, Sainbayar Sukhbaatar, Jack Lanchantin, and Jason Weston. 2025b. Spice: Self-play in corpus environments improves reasoning. arXiv preprint arXiv:2510.24684. Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, and Yue Zhang. 2025c. Logical reasoning in large language models: survey. arXiv preprint arXiv:2502.09100. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. 2025. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652. Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2025. Nemotron-cc-math: 133 billion-token-scale high-quality math pretraining dataset. arXiv preprint arXiv:2508.15096. Mathematical Association of America. n.d. American mathematics competitions (amc 10/12). https: //maa.org/math-competitions/amc. Accessed: 2025-12-31. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. 2025. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Jürgen Schmidhuber. 2007. Gödel machines: Fully selfreferential optimal universal self-improvers. In Artificial general intelligence, pages 199226. Springer. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems (EuroSys), pages 12791297. ACM. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, and 1 others. 2017. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fergus. 2017. Intrinsic motivation and automatic curarXiv preprint ricula via asymmetric self-play. arXiv:1703.05407. Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. 2025a. Cure: Co-evolving coders and unit In The Thirtytesters via reinforcement learning. ninth Annual Conference on Neural Information Processing Systems. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, and 1 others. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. 2025b. Octothinker: Mid-training incentivizes arXiv preprint reinforcement learning scaling. arXiv:2506.20512. Preprint. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Wenhao Yu, Zhenwen Liang, Chengsong Huang, Kishan Panaganti, Tianqing Fang, Haitao Mi, and Dong Yu. 2025. Guided self-evolving llms with minimal human supervision. arXiv preprint arXiv:2512.02472. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. 2024. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. 2025. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, and 1 others. 2025. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084."
        },
        {
            "title": "A Experiment Details",
            "content": "This section presents the detailed hyperparameters and configurations used in our experiments. A.1 Common Experimental Setup Our experiments are conducted on 8 NVIDIA A800 (80GB) GPUs, using the veRL (Sheng et al., 2025) framework. We employ vLLM (Kwon et al., 2023) to facilitate efficient inference, utilizing tensor parallel size of 2 to handle large-scale rollouts. The models are optimized via AdamW (Loshchilov and Hutter, 2019) with learning rate of 1e-6 and weight decay of 1e-2. We choose Group Relative Policy Optimization (GRPO) as our reinforcement learning algorithm. To accommodate extensive corpus and problem descriptions, the maximum prompt length is set to 8,192 tokens and the generation limit is set to 4,096 tokens. A.2 Questioner Training Details During Questioner training phase, we source 10,000 documents from Nemotron-CC-Math (Mahabadi et al., 2025) and 10,000 documents from DataComp-LM (Li et al., 2024), respectively. Training is conducted for 1 epoch with global batch size of 16. For each document-difficulty pair, the model samples = 8 candidate questions. And for each question, the privileged Solver samples = 8 trajectories to get the pseudo-label. For each backbone reported in Table 1, we use its corresponding base model as the Solver when computing the empirical success rate (Eq. 1), ensuring fair comparison with prior work. We further show that the relative difficulty of questions exhibits strong ranking consistency across different Solvers (Section 4.3). Difficulty is calibrated by setting the target parameter τ to 0.8, 0.5, and 0.2, corresponding to easy (80% accuracy), medium (50%), and hard (20%) difficulty tiers, respectively. In the Questioner training stage, the judge model is used only to provide binary feedback on whether generated question is grounded in the given document, which does not constitute answer labels or knowledge distillation. Our experiments show that any model with basic instructionfollowing ability can serve as the judge. We initially attempted to use pretrained base models as judges; however, without instruction tuning, they often violate the required output format and produce invalid structured responses. Therefore, without loss of generality, we adopt Qwen2.5-7BInstruct as the judge in all reported experiments. This choice is motivated by implementation stability and cost-effectiveness rather than methodological dependence on stronger external teacher. Alternative implementations, such as ROUGE-based heuristics or embedding-similarity matching, are equally compatible with DARC. Accordingly, the LLM-as-a-Judge component offers limited guidance and is not essential to the label-free nature of the proposed self-evolution framework. A.3 Solver Training Details The Solver is trained on questions generated by the Questioner. We generate up to 60,000 candidate questions (20,000 per difficulty level), and retain only those that satisfy the required output format. The resulting set of valid questions defines the final training set of size , without any additional filtering. To balance training efficiency and pseudo-label reliability, we set the number of rollouts = 8 and the acceptance threshold γ = 0.3. The privileged teacher Solver shares parameters with the target student Solver, i.e., the teacher is continuously updated alongside training and reflects the current state of the student model. Optimization is performed with global batch size of 512 for single training epoch. During trainingtime inference, we use temperature of 1.0 and top-p sampling with = 0.99."
        },
        {
            "title": "Training Solver",
            "content": "We isolate the impact of curriculum ordering by comparing it against random shuffling strategy on the same offline question set. As shown in Figure 4(a), curriculum learning significantly enhances early-stage sample efficiency. Specifically, the model reaches validation reward threshold of 0.7 in just 24 steps, compared to 32 steps for random shuffling. Moreover, we found that this efficiency gain is not due to simply accessing more active prompts. The number of active prompts remains comparable between the two settings, as shown in Figure 4(c). This suggests that curriculum ordering improves early-stage alignment between the Solver and question difficulty, enabling smoother optimization and faster initial progress while preserving robust final performance."
        },
        {
            "title": "C Training Dynamics of the Questioner",
            "content": "the We examine the training dynamics of Questioner as an additional diagnostic of optimization behavior. Figure 6 reports (a) the Questioner reward and (b) the KL loss during GRPO training. In both model scales, the reward increases rapidly in the early stage and then plateaus, while the KL loss rises gradually before stabilizing. Overall, these trends suggest stable training process, with the optimization dynamics gradually approaching steady regime. We also observe that the 8B Questioner attains higher reward and converges faster than the 4B Questioner, suggesting that larger models can more readily adapt their generation to match the specified difficulty level."
        },
        {
            "title": "D Case Study of Generated Questions",
            "content": "To qualitatively illustrate how the difficulty level affects question generation, we sample single input (a) Questioner reward. (b) KL loss during GRPO. Figure 6: Training dynamics of the Questioner for Qwen3-4B and Qwen3-8B backbones. Curves are smoothed for readability; shaded regions indicate within-run variability measured by rolling standard deviation over steps. document and prompt the trained Questioner with three difficulty conditions (Easy/Medium/Hard). Figure 7 shows that the Questioner produces questions with progressively higher solving difficulty and self-contained questions without access to the source document. Specifically, the Easy question is more direct and relies on shallow reasoning with clearer dis-tractors, whereas the Medium and Hard questions introduce more nuanced constraints and more competitive dis-tractors, requiring the Solver to disambiguate closely related alternatives based solely on the problem statement."
        },
        {
            "title": "E Prompt Design",
            "content": "We present the prompt used for the Solver and Questioner in Figure 8 and Figure 9, respectively."
        },
        {
            "title": "Questions at Different Difficulty Levels",
            "content": "Document: Monitoring flows at routers for flow analysis or deep packet inspection requires the monitors to update monitored flow information at the transmission line rate and needs to use high-speed memory such as SRAM. Therefore, it is difficult to measure all flows, and the monitors need to limit the monitoring target to part of the flows. However, if monitoring targets are randomly selected, an identical flow will be monitored at multiple routers on its route, or flow will not be monitored at any routers on its route. To maximize the number of flows monitored in the entire network, the monitors are required to select the monitoring targets while maintaining balanced load among the monitors... Generated Questions: Easy The primary goal in flow monitoring at routers is to: A. Maximize the use of high-speed memory. B. Ensure every flow is monitored. C. Limit the monitoring to prevent redundant or missed flows while balancing the load. D. Monitor all flows but avoid using high-speed memory. Answer: Medium What is the primary reason for ensuring balanced load among routers in the context of flow monitoring for deep packet inspection? A. To prevent identical flows from being monitored at multiple routers. B. To ensure that no flows are overlooked by routers. C. To reduce the load on routers with high-speed memory. D. To maximize the efficiency of flow measurement across the entire network. Answer: Hard Given the context that routers use SRAM for high-speed memory to monitor flows at transmission line rate for flow analysis or deep packet inspection, which option best describes the challenge and solution for effective flow monitoring? A. SRAM is used to load balance among monitors without considering flow target. B. Flow monitoring in SRAM is limited by the need to randomly select monitoring targets. C. Load balancing and efficient target selection are crucial for maximizing global flow monitoring in SRAM. D. Flow monitoring in SRAM requires frequent memory updates to target every flow equally. E. Monitoring targets should be limited to one flow per router to avoid SRAM overload. Answer: Figure 7: Case Study of generated questions with different difficulty levels."
        },
        {
            "title": "Solver Prompt Template",
            "content": "For Student Solver: Please reason step by step, and put your final answer option within //boxed{}. Only put the letter in the box, e.g. //boxed{A}. There is only one correct answer. {Question} For Teacher Solver: Read the following context and answer the question. {Document} Please reason step by step, and put your final answer option within //boxed{}. Only put the letter in the box, e.g. //boxed{A}. There is only one correct answer. {Question} Note: {Question} and {Document} are placeholders for the actual problem and document, respectively. Figure 8: The prompt used for the Solver model."
        },
        {
            "title": "Questioner Prompt Template",
            "content": "Your task is to generate single self-contained question and its correct answer inspired by the given document. The question must strictly satisfy both the difficulty level and the answer_type constraints. You must output exactly one JSON object as specified below. All reasoning MUST be placed inside the analysis field of the JSON. Difficulty Level You are given target difficulty level: {Difficulty ID} You must follow these operational definitions: {Difficulty ID Definitions} Your generated question and solution process must match the target difficulty level as closely as possible. Answer Type You must generate question whose answer has the following type: {Answer Type} Rules: {Answer Type Definitions}"
        },
        {
            "title": "Core Requirements for the Question",
            "content": "The question must be inspired by the document (but self-contained). The question must not reference the document or the text. The question must be understandable by someone who only sees the question. The reasoning steps must match the difficulty level. The question must combine the number of spans required by difficulty level. The answer must be unique and consistent with the document. All variables must be defined in the question itself. No ambiguity. Dont copy the question from the document. Final Output Format (STRICT) Your output must be exactly one JSON object with the following fields: analysis (string) question (string) intermediate_results (object) answer (string) solving_time_estimate (number) required_concepts (array of strings) potential_errors (array of strings)"
        },
        {
            "title": "Field Specifications",
            "content": "analysis (string): full internal reasoning (spans, difficulty mapping, design, calculations, uniqueness, answer_type). question (string): one self-contained exam-style question; no reasoning/hints/metadata. intermediate_results (object): short step names 120 sentence summaries. answer (categorical): one uppercase letter. solving_time_estimate (number): minutes. required_concepts (array of strings): 110 items. potential_errors (array of strings): 110 items. Example (do not copy; only follow structure) The example below is for structure demonstration only. Your output must be based on the provided {Document} above and strictly follow the assigned difficulty_id and answer_type. Example Input: {Example Input} Note: {Document}, {Example Input}, {Difficulty ID} ,{Difficulty ID Definitions} , {Answer Type} and {Answer Type Definitions}) are placeholders for their corresponding actual content. Figure 9: The prompt used for the Questioner model."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}