{
    "paper_title": "START: Self-taught Reasoner with Tools",
    "authors": [
        "Chengpeng Li",
        "Mingfeng Xue",
        "Zhenru Zhang",
        "Jiaxi Yang",
        "Beichen Zhang",
        "Xiang Wang",
        "Bowen Yu",
        "Binyuan Hui",
        "Junyang Lin",
        "Dayiheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 5 2 6 4 0 . 3 0 5 2 : r START: Self-taught Reasoner with Tools Chengpeng Li1,2*, Mingfeng Xue2, Zhenru Zhang2, Jiaxi Yang2, Beichen Zhang2, Xiang Wang1, Bowen Yu2, Binyuan Hui2, Junyang Lin2, Dayiheng Liu2 1University of Science and Technology of China 2Alibaba Group {lichengpeng.lcp,liudayiheng.ldyh}@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview. Large reasoning models (LRMs) like OpenAIo1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chainof-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal In this paper, we inreasoning processes. troduce START (Self-Taught Reasoner with Tools), novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations , selfchecking, exploring diverse methods, and selfdebugging, thereby addressing limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., Wait, maybe using Python here is good idea.) during the inference process of LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve the START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open- *Work done during internships at Alibaba Group. Corresponding author"
        },
        {
            "title": "Introduction",
            "content": "The evolution of reasoning capabilities in large language models (LLMs) has followed paradigm shift marked by increasingly sophisticated thinking patterns. The chain-of-thought (CoT) approach (Wei et al., 2022) pioneers this progression by enabling models to decompose problems through explicit intermediate reasoning steps. the breakthrough comes with reinforceThen, ment learning exemplified by OpenAI-o1 (OpenAI, 2024b) and DeepSeek-R1 (DeepSeek-AI, 2025), establishing new paradigm termed long CoT, which emulates human-like cognitive strategies including self-refine, self-reflection, multi-strategy exploration and so on. Despite these advancements, fundamental limitations persist in long Chain-ofthought (CoT) approaches, such as hallucinations when facing complex computations or simulations, primarily due to their exclusive dependence on internal reasoning mechanisms (Li et al., 2025). Tool-integrated reasoning (TIR) (Gou et al., 2024), another approach that improves traditional CoT through tool invocation(typically code interpreter), can effectively mitigate the issues that arise in Long CoT. OpenAI o1 (OpenAI, 2024b) reported that they trained o1 to use external tools, especially for writing and executing code in secure environment, but they did not provide any technical details on how this is achieved. Therefore, straightforward yet important question arises: How can we synergistically combine long CoT with TIR? In this paper, we focus exclusively on Python interpreter invocation, as it is both important and representative of many reasoning tasks (Shao et al., 2024; Yang et al., 2024). The fundamental challenge lies in synthesizing data that includes calls to Python interpreter within Long Chain-of-thought (CoT). We have tried using direct prompt, wellFigure 1: Training framework for START. Training Framework for START. STARTs training involves two phases: Hint-RFT followed by RFT. a) Hint-infer: code/math data is processed by QwQ, with responses truncated at predefined terminators. Context-aware hints from Hint-Library are injected at truncation points (including endpoints), and QwQ resumes inference using code interpreter for Python execution feedback. b) Hint-RFT: Hint-infer outputs undergo rule-based scoring, filtering, and content modification to create Dseed . QwQ is then fine-tuned on Dseed to produce START-0, enabling self-aware tool usage. c) RFT: START-0 generates self-distilled trajectories to build DSTART (enhancing diversity/tool-use patterns), followed by fine-tuning to produce START. designed prompt (Li et al., 2025), and in-context prompt (Gou et al., 2024; Schick et al., 2023) on AIME24 and LivecodeBench with QwQ (Qwen Team, 2024) and DeepSeek-R1, but none were successful in prompting the model to invoke the Python tool during the long CoT(see more in Appendix A.3). possible reason is that large reasoning models(LRMs) typically focus solely on problem-solving during training for complex reasoning tasks, resulting in loss of generalization in instruction following. Considering the nature of next-token prediction in LRMs, we attempt to insert some hints directly during or at the end of the LRMs reasoning process, aiming to directly prompt the model to write code and invoke code interpreter. We are surprised to discover that LLMs indeed possess the corresponding capabilities. For mathematical tasks, simply inserting basic hints along with Python identifiers enables the LLM to follow the hints and write the appropriate code. In contrast, for coding generation tasks, carefully designed hints and code templates are necessary to activate the models ability to execute candidate code on test cases on its own during the long CoT. We refer to the paradigm of LRM inference aided by hints as Hint-infer. Based on above Hint-infer, we present START: Self-Taught Reasoner with Tools, LRM that synergizes Long CoT and TIR, which we refer to as Long TIR. The whole traing framework is illustrated in Figure1. First, we design set of hints with different functionalities based on the cognitive characteristics of LLMs, which we refer to as the Hint-Library. Figure 3 presents some representative hints from the Hint Library. Second, these hints are randomly inserted after certain highfrequency conjunctions, such as \"Alternatively\" and \"Wait\" (see more in Figure 4), because these words typically indicate that the model begins to introspect or seek new solutions to the problem (Li et al., 2025). Additionally, we also add hints before the stop token of long CoT, as this approach provides the LRM with more time to think without disrupting its original reasoning process. We find it intriguing that when hints are added before the stop token of Long Chain-of-thought (CoT), the model exhibits sequential test time scaling effect; that is, as the thinking time increases, the success Figure 2: Comparison between the responses generated by QwQ and START. This is question from LiveCodeBench with difficulty level of \"hard\". QwQ employs long-chain CoT with self-reflection and trying different approaches, yet hallucinates during complex test case analysis, leading to flawed solutions. START retains QwQs cognitive framework but integrates code execution: (1) Runs code via interpreter, (2) Detects output mismatch, (3) Iteratively analyzes and debugs, and (4) Gives the final solution. See more cases of START in Appendix A.5 rate of problem-solving also gets higher(see more in 4.5.2). Through series of data scoring, filtering, modifications, and rejection sampling fine-tuning (RFT) (Yuan et al., 2023; Zelikman et al., 2022), we eventually obtain our START from QwQ. From Figure 2, it can be seen that there is comparison between the reasoning of QwQ and START. When encountering complex case analysis, QwQ-32 generates hallucinations and provides incorrect answers, while START utilizes code interpreter to self-debug, delivering the correct answer. Empirical evaluations across suite of benchmarks encompassing mathematical problemsolving, scientific inquiries, coding challenges, and GPQA tasks demonstrate that START-RL markedly surpasses existing tool-integrated and long CoT models, including QwQ, o1-mini, and o1-preview in MATH benchmarks. These results underscore the efficacy of integrating external tools into the long CoT framework, highlighting STARTRL as the first open-source tool-integrated long CoT reasoning model that sets new standard for LLM performance in complex reasoning domains. In summary, our contributions are threefold: We introduce Hint-infer, simple and effective sequential test-time scaling method for LRMs such as QwQ and START. We introduce Hint-RFT, self-training framework that enables large language model (LRM) to teach itself how to utilize code interpreter. We present START, the first open-source LRM that utilizes long CoT and code interpreter to address complex reasoning tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Large Language Models have demonstrated remarkable dominance across numerous Natural Language Processing tasks. To enhance the complex reasoning capabilities of LLMs, Wei et al. (2022) introduce Chain-of-Thought (CoT), which incorporates multi-step intermediate reasoning before arriving at final conclusions. CoT exhibits significant advantages across multiple domains, including mathematics, science, and programming. Subsequently, OpenAI (2024b) further explore CoT and propose the Long Chainof-Thought framework. In Long CoT, LLMs demonstrate advanced cognitive behaviors such Figure 3: Hint-Library. Code generation tasks: Debug hint guides test case review and local code validation. The code template is in A.4. Math reasoning: Domain-specific hints (e.g., Complex Calculations, Self-Reflection, Logic Check, Alternative Methods) steer code-aided reasoning behaviors. models through simple distillation. Nevertheless, significant challenges persist, particularly in addressing hallucination phenomena and computational inaccuracies that impede optimal performance. Drawing parallels with human cognition, where external aids such as scratch paper and calculators substantially mitigate computational errors, LLMs can similarly benefit from the integration of auxiliary tools. Research by Shao et al. (2024) demonstrates that codebased pre-training protocols significantly augment LLMs mathematical reasoning proficiency. Various works successfully implemente Python-based computational tools to enhance model performance (Chen et al., 2023; Gou et al., 2024; Liao et al., 2024; Li et al., 2024). In the domain of mathematical proof verification, the incorporation of Lean yield notable advancements (Xin et al., 2024; Wu et al., 2024). This study synthesizes the advantages of Pythonbased tools and long CoT methodologies, advancing QwQ-type long CoT models through the integration of tool utilization capabilities. This integrated approach yields improved performance metrics across mathematical and coding benchmarks. Figure 4: Word cloud of conjunction frequency statistics from QwQ infering on Dseed. as reflection, verification, correction, and multipath exploration, thereby further enhancing their problem-solving capabilities in complex reasoning tasks. Moreover, Long CoT exhibits excellent test-time scaling properties, where increased computational resources correlate with improved reasoning outcomes. Models like QwQ (Qwen Team, 2024), DeepSeek-R1 (DeepSeek-AI, 2025), k1.5 (Team et al., 2025),and InternThinker (Cai et al., 2024) have successfully experimented with Long CoT for enhanced reasoning, combining fine-tuning and Reinforcement Learning to elevate the performance of open-source reasoning models to unprecedented levels. Notably, subsequent models such as Open-R1 (Huggingface, 2025), O1Replication (Qin et al., 2024), S1 (Muennighoff et al., 2025a) and LIMO (Ye et al., 2025) observes significant benefits from Long CoT even in smaller"
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Training data Our training data comprises two parts: one consists of math data sourced from previous AIME problems 1(before 2024), MATH (Hendrycks et al., 2021), and Numina-MATH (LI et al., 2024), while the other includes code data from Codeforces 2, code contests 3 and LiveCodeBench(before July 2024) (Jain et al., 2024). We apply the same decontamination method as described in (Yang et al., 2024) to the training set in order to minimize potential test data leakage risks. There are total of 40K math problems and 10K code problems, and the specific quantity distribution can be referred to in Appendix A.1. 3.2 Hint-RFT Construct Hint we have designed series of hints(Hint-Library) tailored to the various scenarios that may arise during LLM reasoning. Since mathematical reasoning with tools can be quite complex, we develop different hints focused on reflection, logical verification, exploring new methods, and more. These diverse hints enable the model to adopt different strategies based on the specific situation it encounters. For coding tasks, we concentrate on designing hints that promote the models self-debugging capabilities. By encouraging the model to check its code against test cases, it can verify the correctness of its solutions and make necessary adjustments as needed. We find that adding code template to the hint can effectively prompt the model to generate desired debugging code. Figure 3 show the hints. Hint-infer For mathematical reasoning, we strategically insert hints after specific conjunction tokens, such as Alternatively and Wait as these tokens typically indicate that the model may be questioning its own reasoning or considering alternative approaches. It is important to note that after the hints are inserted, the model continues its reasoning process. The generated code is then sent to Python interpreter for execution, and upon obtaining the results, the model proceeds to generate further outputs based on that information. Similarly, we can insert hints before the stop token to 1https://huggingface.co/datasets/gneubig/ aime-1983-2024 2https://codeforces.com/problemset 3https://github.com/google-deepmind/code_ contests encourage the model to engage in deeper reasoning based on its existing reasoning. By inserting hints at these critical junctures at random, we encourage the model to explore broader reasoning space. For code reasoning, we primarily concentrate on code generation tasks. We insert hints that prompt the model to test its own code right before the model generates the final code solution. This strategic placement encourages the model to engage in self-assessment, thereby enhancing the accuracy and reliability of the generated code. more intuitive description is in Figure 1. Data process and model fine-tuning Inspired by (Lightman et al., 2024), we adopt an active learning method, where we perform greedy inference and hint inference using QwQ on all training data, and we recall data from reasoning tasks where QwQ would not succeed without tools, but succeeded with hint inference. This is incorporated into our startup data Dseed with 10K math data and 2K code data. It is important to note that, in addition to scoring the generated reasoning trajectories based on the rules, we also filter out responses that contain repetitive patterns. Additionally, we modify the Python identifiers in the code data hints to \"Debug Code Template\" and remove the output placeholders. We fine-tune QwQ based on Dseed to obtain START-0. The purpose of this fine-tuning step is to enable the model to learn the response paradigm for utilizing tools. 3.3 RFT To further enhance the diversity and quantity of the training data, as illustrated in Figure 1, we utilize the obtained START-0 to perform rejection sampling fine-tuning on all training data. Specifically, we use sampling parameters of temperature 0.6 and top-p 0.95 with START-0 to perform 16 rounds of sampling. We score the sampled long TIR data, filter out responses with repetitive patterns, and manually modify any unreasonable content. We retain maximum of one response per question, resulting in our dataset DSTART. Using the 40,000 math data entries and 10,000 code data entries from DSTART, we fine-tune QwQ once again, resulting in our final LRM named START."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Benchmarks In this work, we primarily focus on integrating Python tools into long CoT reasoning. Given Table 1: Main results on challenging reasoning tasks, including PhD-level science QA, math, and code benchmarks. We report Pass@1 metric for all tasks. For models with 32B parameters, the best results are in bold and the second-best are underlined. Symbol indicates results from their official releases. Method GPQA MATH500 AMC AIME24 AIME25 LiveCodeBench Qwen2.5-32B Qwen2.5-Coder-32B Llama3.3-70B DeepSeek-V3-671B GPT-4o API Only o1-preview o1-mini o1 o3-mini(low) Open weights R1-Distill-Qwen-32B s1-32B Search-o1-32B QwQ 46.4 33.8 43.4 59.1 50.6 73.3 - 77.3 70.6 62.1 59.6 63.6 58.1 75.8 71.2 70.8 90.2 60.3 85.5 90.0 94.8 95. 94.3 93.0 86.4 90.6 General LLMs 57.5 67.5 47.5 - - Reasoning LLMs 81.8 - - - 93.8 - 85.0 80. 23.3 20.0 36.7 39.2 9.3 44.6 63.6 74.4 60.0 72.6 50.0 56.7 50.0 - - - - - 37.5 50.8 - 44.2 46.7 33.3 - 40. 22.3 25.0 34.8 40.5 33.4 53.6 - 63.4 75.6 57.2 - 33.0 41.4 START 63.6(+5.5) 94.4(+3.8) 95.0(+15.0) 66.7(+16.7) 47.1(+7.1) 47.3(+5.9) Pythons effectiveness in enhancing computational and programming aspects of reasoning, we select several representative and challenging reasoning benchmarks to validate our methodology. GPQA: This benchmark comprises 448 graduate-level multiple-choice questions authored by experts in biology, physics, and chemistry (Rein et al., 2023). These questions present significant challenges, as even domain experts achieved less than 75% accuracy in testing (OpenAI, 2024b). Math Benchmarks: Mathematical performance of LLMs remains focal point for researchers. In the mathematical domain, we select MATH500 (Lightman et al., 2024) at the high school level, along with competition-level AMC23 4, AIME24 5 and AIME25 6 as our evaluation datasets. These datasets encompass various mathematical question types, including algebra, calculus, number theory, probability, and geometry, enabling comprehensive assessment of LLMs mathematical problem-solving capabilities. LiveCodeBench: This benchmark evaluates LLMs programming capabilities, with test cases 4https://huggingface.co/datasets/AI-MO/ aimo-validation-amc 5https://huggingface.co/datasets/AI-MO/ aimo-validation-aime 6https://huggingface.co/datasets/TIGER-Lab/ AIME categorized into easy, medium, and difficult levels (Jain et al., 2024). We choose 112 problems from August 2024 to November 2024 as the code benchmark. These questions are categorized as hard, medium, and easy based on difficulty. 4.2 Baselines We evaluate our approach against the following baseline methods: General LLMs: These methods are general LLMs without the long CoT reasoning. The open-source models include Qwen2.5-32BInstruct (Yang et al., 2025), Qwen2.5-Coder32B-Instruct (Hui et al., 2024),DeepSeek-V3671B (DeepSeek-AI et al., 2024), Llama3.3-70BInstruct (Dubey et al., 2024) and GPT-4o (OpenAI, 2024a). LRMs: These methods are equipped with long (1) API only: These models CoT reasoning. can only be accessed through the API, including o1-series (OpenAI, 2024b) and o3-mini (OpenAI, 2025). (2) Open weights: we compare with some open weights LLMs, including DeepSeek-r1 series (DeepSeek-AI, 2025), QwQ (Qwen Team, 2024), s1 (Muennighoff et al., 2025b) and Searcho1 (Li et al., 2025). 4. Implementation Details We fine-tune QwQ with DSTART to get START. We train the base models with key settings including 7e-6 learning rate, 128 global batch size, cosine scheduler with 3% warm-up, maximum context length of 16,384 tokens and 3 training epochs. Checkpoints are not selected with early stops. The training process employs full-parameter fine-tuning with DeepSpeed ZeRO-3 (Rajbhandari et al., 2020) optimization and FlashAttention2 (Dao et al., 2022). Responses are generated using greedy decoding with maximum sequence length of 32,768 and limit of 6 maximum tool uses. For all benchmarks, we report the pass@1 performance and the evaluation metric is from (Yang et al., 2024). We use the same training and inference chat template as QwQ (Qwen Team, 2024). The hardware setup involves 32 NVIDIA A100 GPUs. Table 2: Scores on GPQA in various subjects. Model Physics Chemistry Biology QwQ Search-o1 START 73.8 77.9 80.0 41.9 47.3 47.3 68.4 78.9 68.4 Table 3: Scores on questions of different difficulty levels on LiveCodeBench. Model Easy Medium Hard 92.3 QwQ START 92.3 46.0 84.6 10.2 12.2 4.4 Main Results Table 1 presents the evaluation results of START across various benchmarks, demonstrating its superior reasoning performance in scientific, mathematical, and coding domains compared to other open-source models. Overall, general LLMs, even domain-specific LLMs, are difficult to compete with LRMs in complex tasks. PHD-level Science QA Performance It can be observed that on the ScienceQA benchmark, START demonstrates an absolute improvement of 5.5% over QwQ, achieving the same score as the state-of-the-art model, search-o1-32B. Table 2 presents the scores of QwQ, Search-o1, and START across three subjects of GPQA: Physics, Chemistry, and Biology. Specifically, START achieves the highest score in Physics, while Searcho1 outperforms QwQ significantly in Biology. This discrepancy can be attributed to the fact that Physics often necessitate extensive computational reasoning, whereas Biology primarily relies on knowledge-based reasoning. Consequently, the utilization of Python-based tools(START) yields more pronounced efficacy in the former disciplines, while the utilization of internet knowledge(searcho1-32B) works better on the latter. MATH Benchmarks Performance On the MATH benchmarks, START also demonstrates considerable advantages over QwQ. Specifically, it achieves absolute improvements of 3.8%, 15.0%, 16.7% and 7.1% on the MATH500, AMC23, AIME24 and AIME25, respectively. The performance of START is comparable to that of R1Distill-Qwen-32B, which is distilled from 671B DeepSeek-R1, and overall it exceeds o1-preview. These results highlight the significant role of Python-based tools in enhancing mathematical reasoning capabilities. LiveCodeBench Performace On the LiveCodeBench, START, by equipping the model with the capability to invoke debugging tools, achieves an absolute improvement of 5.9% over QwQ. We find that START improves the most compared to QwQ on questions with medium difficulty from 3. The possible reason is that for easy questions, QwQ can generate the correct answers with high probability without debugging, and for hard questions, based on the current capabilities of the model, limited number of debugs is also difficult to solve. 4.5 Analysis 4.5.1 Long CoT vs Long TIR To ascertain whether our performance gains stem from the additional training questions or from the tool invocation capability, we conduct an experiment using the same set of queries from DST ART but only apply RFT with QwQ. For each query, we sampled 32 responses with temperature of 0.7 and top-p value of 0.95. After filtering out incorrect responses and responses with repeating strings, we retain at most one response per question and get the long CoT dataset DRFT. Based on DRFT, we fine-tune QwQ, yielding QwQ-RFT. This methodological approach allows us to isolate the impact of tool invocation from that of the expanded training dataset. Table 4: Compare long cot with long tir on challenging reasoning tasks, including PhD-level science QA, math, and code benchmarks. We report Pass@1 metric for all tasks. Method QwQ QWQ-RFT GPQA MATH500 AMC23 AIME24 AIME25 LiveCodeBench 58.1 58.5 90.6 91.8 80.0 82.5 50.0 53.3 40.0 33.3 41.4 42. START (Ours) 63.6(+5.5) 94.4(+3.8) 95.0(+15.0) 66.7(+16.7) 47.1(+7.1) 47.3(+5.9) Figure 5: Test time scaling for QwQ and START on challenge math bench marks via Hint-infer. The results presented in Table 4 indicate that the performance of QwQ-RFT is nearly on par with that of QwQ. Therefore, the observed performance advantage of START is likely predominantly driven by its tool invocation capability, suggesting that this feature plays critical role in enhancing its effectiveness. 4.5.2 Analysis of Hint-infer Compare QwQ with Hint-infer and START Through Hint-RFT, we discover that QwQ inherently possesses the potential to invoke tools, although this capability is challenging to elicit through prompting alone and instead requires explicit hints to activate. START, which is finetuned from QwQ using Hint-RFT, allows us to directly compare the performance of QwQ with Hint-infer against that of START. To avoid interrupting QwQs reasoning process, we only insert hints before the stop token of QwQ(see more in Appendix A.4). This comparison provides basis for evaluating the necessity of fine-tuning, as it helps to determine whether the enhanced performance of START is primarily due to the fine-tuning process or can be sufficiently achieved through hint-based prompting alone. From Table 6, it is evident that incorporating hints during the inference process of QwQ leads to improvements across all benchmarks. However, these improvements are relatively modest compared to the gains achieved by START. Consequently, START, through Hint-RFT, significantly enhances QwQs tool invocation capabilities, demonstrating the effectiveness of fine-tuning in unlocking the models latent potential. Test-time scaling via Hint By inserting hints at the end of QwQs inference process, we can simultaneously increase both the models thinking time and its accuracy(see Figure 5) by multiple rounds of inserting hint before stop token. It indicates that Hint-infer is simple yet effective method for achieving sequential test-time scaling. Unlike the strategy outlined in (Muennighoff et al., 2025b), which merely increases the number of \"wait\" tokens, our method augments the number of tool invocation. The use of Hint-infer for START, on the other hand, does not work as well as on QwQ, and the reason behind this may be that those hints we inserted were already available during the reasoning process between STARTs, reducing the amount of information in the added hints. More results and analysis are list in A.2."
        },
        {
            "title": "5 Conclusion",
            "content": "this paper presents START, groundbreaking toolintegrated long Chain-of-Thought reasoning model that effectively mitigates the limitations of existing large reasoning models (LRMs) through the innovative integration of external tools and selflearning techniques. Our contributions, namely Hint-infer and Hint-RFT, showcase novel approach to enhance reasoning capabilities by enabling LRM to leverage coding interpreters for complex computations and self-debugging. The empirical results demonstrate significant improvements in performance across range of challenging benchmarks, establishing START as leading open-source solution for advanced reasoning tasks. By combining long CoT with tool integration, START sets new standard for the future development of LLMs, paving the way for more reliable and efficient reasoning in higher-level cognitive tasks."
        },
        {
            "title": "6 Limitations",
            "content": "While our work on START demonstrates significant advancements in tool-integrated long Chainof-Thought reasoning, it is essential to acknowledge several limitations inherent in our approach. Firstly, our research exclusively focuses on the integration of Python interpreter as the sole external tool. Although this choice was made for its relevance to many reasoning tasks, we believe that incorporating wider variety of toolssuch as search engines, specialized libraries, or different computational resourcescould potentially enhance the models performance and versatility. Future work could explore how diverse toolsets might contribute to more robust reasoning across various domains. Secondly, the manual design of hints for insertion into the long CoT reasoning process may inadvertently disrupt the models original flow of thought. While we aimed to strategically position these hints to optimize performance, the effectiveness of hint positioning and selection could vary based on the specific task or context. More nuanced criteria for determining the most effective types and placement of hints might result in further improvements in reasoning fluidity and accuracy. Additionally, our empirical evaluations were conducted on limited set of benchmarks. Although results reported demonstrate promising outcomes, the generalizability of our findings remains to be established across broader and more diverse datasets. The performance of START may be sensitive to variations in task complexity, domain specificity, and the characteristics of the input data. Lastly, potential risks associated with the misuse of the technology must be considered. The ability of our model to generate code or suggest problem-solving strategies could be inadvertently leveraged for malicious purposes, such as crafting disinformation or automating harmful tasks. It is crucial to implement safeguards and establish ethical guidelines to monitor and mitigate such risks. In summary, while our research provides significant step forward, acknowledging these limitations is essential to paving the way for future improvements and ensuring the responsible development and application of tool-integrated reasoning models."
        },
        {
            "title": "References",
            "content": "Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Internlm2 technical report. Preprint, Lin. 2024. arXiv:2403.17297. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2024. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The Llama 3 herd of models. CoRR, abs/2407.21783. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2024. Tora: tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks. Huggingface. 2025. Open r1. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2.5-Coder technical report. CoRR, abs/2409.12186. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974. Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, and Dayiheng Liu. 2024. Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning. CoRR, abs/2407.04078. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume NuminaLample, and Stanislas Polu. 2024. math. [https://github.com/project-numina/ aimo-progress-prize](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf). Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-enhanced large reasoning models. Preprint, arXiv:2501.05366. Minpeng Liao, Chengxi Li, Wei Luo, Jing Wu, and Kai Fan. 2024. MARIO: math reasoning with code interpreter output - reproducible pipeline. In ACL (Findings), pages 905924. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025a. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025b. s1: Simple test-time scaling. OpenAI. 2024a. Hello GPT-4o. OpenAI. 2024b. Learning to reason with LLMs. OpenAI. 2025. Openai o3-mini. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. 2024. O1 replication journey: strategic progress report part 1. Preprint, arXiv:2410.18982. Qwen Team. 2024. QwQ: Reflect deeply on the boundaries of the unknown. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. GPQA: graduate-level Google-proof Q&A benchmark. CoRR, abs/2311.12022. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. 2025. Kimi k1.5: Scaling reinforcement learning with llms. Preprint, arXiv:2501.12599. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837. Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. 2024. Internlm2.5-stepprover: Advancing automated theorem proving via expert iteration on large-scale LEAN problems. CoRR, abs/2410.15700. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. 2024. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. CoRR, abs/2405.14333. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. Preprint, arXiv:2502.03387. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. Preprint, arXiv:2308.01825. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Training set of START A.2 More results about Hint-infer Building upon the observed trends, the detailed results in Table A.2 further underscore the efficacy of the QwQ-Hint-infer and START-Hint-infer methods across diverse challenging reasoning tasks. Specifically, for datasets such as aime24, aime25, gpqa, amc23, MATH500, and LiveCodeBench, QWQ consistently demonstrates performance enhancements with each subsequent round of hint insertion. For instance, aime24 improves from 50.0% in Round 0 to 60.0% in Round 3, and MATH500 shows marginal yet steady increase from 90.6% to 92.4% over the same rounds. This consistent upward trend highlights the methods ability to incrementally refine the models reasoning capabilities through iterative hint integration. In contrast, the START-Hint-infer approach exhibits more varied performance across different datasets. While there are improvements in some areas, such as AIME25, where the Pass@1 metric reaches 60.0% by Round 3 and LiveCodeBench sees an increase from 47.3% to 50.0%, other datasets like GPQA and LiveCodeBench show relatively modest gains and even no gains. This disparity suggests that the effectiveness of Hint-infer may be contingent on the inherent characteristics of the dataset and the nature of the reasoning tasks involved. A.3 Prompting Methods for Data annotation We investigated three common methods to trigger existing reasoning LLMs to generate long CoT with Python tool calls in mathematical reasoning tasks. The first method is \"direct prompt\"(Please integrate natural language reasoning with programs to solve the problem.), which instructs the model to directly use Python tools during reasoning. The second method, \"well-designed prompt\" is derived from search-o1 (Li et al., 2025) and provides detailed instructions on how to use the tools; this prompt successfully triggers the model to generate special tokens for browser calls in searcho1. The third method is \"in-context prompt\"(Give some demonstrations in the prompt) which leverages examples to guide the model in generating data in the same format. We do not use general LLMs, as they typically cannot produce long CoTs. For the O1 series, we can only assess whether the summary includes Python tool invocation. As Table 5: Sources of Dataset Source AIME problems (before 2024) MATH (Hendrycks et al., 2021) Numina-MATH (LI et al., 2024) Code Data Codeforces Code contests LiveCodeBench (before July 2024) (Jain et al., 2024) Total Quantity 890 7500 28505 7505 2011 558 49969 Table 6: Comparison of QWQ-Hint-infer and START-Hint-infer on challenging reasoning tasks, including PhDlevel science QA, math, and code benchmarks. We report Pass@1 metric for all tasks. Dataset QWQ START Round 0 Round 1 Round 2 Round 3 Round Round 1 Round 2 Round 3 aime24 aime25 gpqa amc23 MATH500 LiveCodeBench 50.0% 40.0% 58.5% 80.0% 90.6% 41.4% 53.3% 47.1% 58.6% 85.0% 92.0% 42.0% 56.7% 47.1% 59.6% 90.0% 92.0% 42.0% 60.0% 53.3% 59.6% 92.5% 92.4% 42.0% 66.7% 47.1% 63.6% 95.0% 94.4% 47.3% 66.7% 47.1% 61.6% 92.5% 95.0% 48.2% 66.7% 60.0% 60.6% 95.0% 95.6% 50.0% 70.0% 60.0% 61.6% 95.0% 95.2% 50.0% result, we found that neither QwQ, DeepSeek-R1, nor o1-mini could successfully generate long CoTs with code interpreter calls using the three promptbased methods. In contrast, the hint-infer method was able to trigger the model to produce Python code with 100% success. A.4 Hint-infer for test time scaling The three rounds hints of GPQA and MATH for Hint-infer are: Wait, can use Python to check if my approach is correct and refine it, if necessary.python, Wait, need to utilize Python code again to meticulously check to make sure understand the question correctly as well as reasoning correctly.python and Wait, can think more deeply about this problem through python tools.python. Hints of LivecodeBench is the same in Hint-Library. For code problem with starter code, the code template is {startcoder} # Test the example inputs solution = Solution() # Example input1 test_input1 = ... # Example input2 test_input2 = ... # Print output print(solution.function_name(test_input1)) print(solution.function_name(test_input2)) # Check the output ''' '''output [...] ''' For code problem without starter code, the code template is def function_name(parameters): #Implementationn # Test the example inputs solution = Solution() # Example input1 test_input1 = ... # Example input2 test_input2 = ... # Print output print(solution.function_name(test_input1)) print(solution.function_name(test_input2)) # Check the output ''' '''output [...] ''' A.5 Case Study For START A.5.1 case form AMC23 case form AMC23 Problem: In the state of Coinland, coins have values 6, 10, and 15 cents. Suppose is the value in cents of the most expensive item in Coinland that cannot be purchased using these coins with exact change. What is the sum of the digits of x? Solution: So Ive got this problem from the state of Coinland, where they have coins worth 6, 10, and 15 cents. The question is asking for the value of the most expensive item that you cant buy using these coins with exact change. Then, need to find the sum of the digits of that value. First off, need to understand what this is asking. It seems like problem related to the concept of the Frobenius coin problem, which is about finding the largest amount that cannot be obtained using given coin denominations. In the classic case, with two coprime denominations, theres formula to find this number, but here we have three denominations: 6, 10, and 15 cents. know that for two coprime numbers, and b, the largest number that cant be expressed as non-negative integer combination of and is ab b. But here, we have three numbers, and they arent all pairwise coprime. For instance, 6 and 10 share common factor of 2, and 6 and 15 share common factor of 3. So, the standard Frobenius number formula doesnt directly apply. Maybe should start by looking at the greatest common divisor (GCD) of these numbers. Lets find the GCD of 6, 10, and 15. The GCD of 6 and 10 is 2. The GCD of 2 and 15 is 1. So, the overall GCD is 1. That means that eventually, any sufficiently large number can be expressed as combination of these coins, since their GCD is 1. But, there might be some gaps before we reach certain point where all numbers can be expressed. Perhaps should look for the Frobenius number for these three denominations. However, recall that for more than two denominations, there isnt general formula for the Frobenius number. So, might need to approach this problem differently. One approach could be to consider the possible combinations of these coins and see which numbers can be expressed. Maybe can start by listing out the possible sums. Lets list out some combinations: - 0 coins: 0 cents - 1 coin: 6, 10, or 15 cents - 2 coins: 6 + 6 = 12, 6 + 10 = 16, 6 + 15 = 21, 10 + 10 = 20, 10 + 15 = 25, 15 + 15 = 30 cents - 3 coins: 6 + 6 + 6 = 18, 6 + 6 + 10 = 22, 6 + 6 + 15 = 27, 6 + 10 + 10 = 26, 6 + 10 + 15 = 31, 6 + 15 + 15 = 36, 10 + 10 + 10 = 30, 10 + 10 + 15 = 35, 10 + 15 + 15 = 40, 15 + 15 + 15 = 45 cents - And so on... This could take while, and might miss some numbers. Maybe theres better way. Another thought: since the GCD is 1, know that every number greater than or equal to certain number can be expressed as combination of these coins. need to find the largest number that cannot be expressed this way. Perhaps can look at the problem in terms of modulo arithmetic. For example, since 6 is divisible by 3, 10 is not, and 15 is divisible by 3, can consider how these combine in terms of modulo 3. Wait, but that might not be directly helpful. Let me think differently. Ive heard of the concept of the \"Frobenius number\" for two coins, but for three or more, its more complicated. Maybe can reduce this problem to combination of smaller problems. Let me consider pairs of these denominations and find their Frobenius numbers, then see how that helps. First, consider only 6 and 10 cent coins. Since GCD(6, 10) = 2, which is greater than 1, the Frobenius number for these two is not defined because you cant make all even numbers greater than certain point with just 6 and 10. Wait, no. Actually, since their GCD is 2, you can make any even number greater than or equal to certain value. Wait, the Frobenius number formula ab applies only when and are coprime. Here, 6 and 10 are not coprime, so that formula doesnt apply. Similarly, for 6 and 15, GCD(6, 15) = 3, which is greater than 1. And for 10 and 15, GCD(10, 15) = 5. So, none of the pairs are coprime. This complicates things. Alternatively, maybe can look at the problem in terms of linear combinations. need to find the largest such that there are no non-negative integers a, b, satisfying 6a + 10b + 15c = x. This seems tricky. Maybe can look for pattern in the possible sums. Let me try to list out the possible sums in order: 0, 6, 10, 12, 15, 16, 18, 20, 21, 22, 24, 25, 26, 27, 30, etc. Wait, can use Python to generate specific examples that fit the problems conditions. Python Code from itertools import product def can_be_paid(x, coins): for in range(x//coins[0] + 1): for in range(x//coins[1] + 1): for in range(x//coins[2] + 1): if a*coins[0] + b*coins[1] + c*coins[2] == x: return True return False coins = [6, 10, 15] max_x = 100 # arbitrary upper limit to search for the largest that cannot be paid possible_x = [] for in range(1, max_x): if not can_be_paid(x, coins): possible_x.append(x) print((max(possible_x), sum(int(digit) for digit in str(max(possible_x))))) Output (29, 11) From the code, found that the largest value that cannot be purchased using coins of 6, 10, and 15 cents is 29 cents. The sum of the digits of 29 is 2 + 9 = 11. So, the answer is 11. **Final Answer** 11 A.5.2 case form AIME24 case form AIME24 Problem: Let be the least prime number for which there exists positive integer such that n4 + 1 is divisible by p2. Find the least positive integer such that m4 + 1 is divisible by p2. Solution: Im trying to solve this problem. It says: Let be the least prime number for which there exists positive integer such that n4 + 1 is divisible by p2. Find the least positive integer such that m4 + 1 is divisible by p2. First, need to find the smallest prime such that theres some positive integer making n4 + 1 divisible by p2. Then, using that p, have to find the smallest positive integer where m4 + 1 is divisible by p2. So, to start, need to understand when n4 + 1 is divisible by p2. That means p2 divides n4 + 1, or in other words, n4 1 (mod p2). know that for primes, certain properties hold due to Fermats Little Theorem and Eulers Theorem, but Im not sure how to apply them here directly because were dealing with p2, not just p. Maybe should look at the equation n4 1 (mod p2). This implies that n8 1 (mod p2), because if n4 1, then squaring both sides gives n8 1 (mod p2). So, the order of modulo p2 divides 8, meaning the order is 1, 2, 4, or 8. But since n4 1 (mod p2), the order cant be 1, 2, or 4, because in those cases, n4 would be 1, not -1. Therefore, the order must be 8. This means that 8 divides the order of the multiplicative group modulo p2, which is ϕ(p2) = p(p 1), where ϕ is Eulers totient function. So, 8 divides p(p 1). Since is prime, there are few cases to consider: 1. = 2 2. is an odd prime, and 8 divides 1 Lets consider = 2 first. If = 2, then p2 = 4. We need to check if there exists an such that n4 + 1 is divisible by 4. Lets compute n4 + 1 modulo 4: - If is even, n4 is divisible by 16, so n4 0 (mod 4), thus n4 + 1 1 (mod 4), which is not divisible by 4. - If is odd, n4 1 (mod 4), so n4 + 1 2 (mod 4), which is also not divisible by 4. Therefore, there is no positive integer such that n4 + 1 is divisible by 4. So, = 2 is not valid choice. Next, consider odd primes where 8 divides 1. That is, 1 (mod 8). The smallest such primes are 17, 41, 73, etc. Alternatively, there could be other primes where divides n4 + 1, but for p2 to divide n4 + 1, its more restrictive. Wait minute, maybe should consider quadratic residues or higher power residues. Alternatively, perhaps should look for primes where 1 is fourth power residue modulo p2. recall that for primes, 1 is quadratic residue if 1 (mod 4), but here were dealing with fourth powers. Maybe should consider the Legendre symbol or higher power residues. Alternatively, perhaps can use Hensels Lemma to lift solutions from modulo to modulo p2. First, need to find primes such that there exists an with n4 1 (mod p), and then check if that solution can be lifted to modulo p2. So, start by finding primes for which n4 1 (mod p) has solution. This is equivalent to saying that 1 is fourth power residue modulo p. The multiplicative group modulo is cyclic of order 1, so n4 1 (mod p) has solution if and only if there exists an integer such that 4k (p1) modulo if 1 (mod 4). Wait, can use Python to automate these extensive calculations. 2 (mod 1), since 1 is square Python Code def find_least_p(): = 3 # Start with the smallest odd prime while True: if any(pow(n, 4, p) == - 1 for in range(1, p)): # Check if n^4 = -1 mod has solution for in range(1, p): if (n**4) % == - 1: # Now check if there exists such that m^4 = -1 mod p^2 p_squared = * for in range(1, p_squared): if (m**4) % p_squared == p_squared - 1: return p, p = next_prime(p) def next_prime(p): # Implement function to get the next prime after if == 2: return 3 candidate = + 2 while not is_prime(candidate): candidate += 2 return candidate def is_prime(num): # Implement function to check if number is prime if num <= 1: return False if num == 2: return True if num % 2 == 0: return False for in range(3, int(num**0.5) + 1, 2): if num % == 0: return False return True # Find the least prime and the least p, = find_least_p() print((p, m)) Output SyntaxError: invalid syntax It seems there was syntax error in the code due to the way the condition was written. Lets correct the code and try again. need to ensure that the condition for checking if n4 1 (mod p) is properly implemented. Also, need to make sure that the function to find the next prime is correctly defined. Lets fix the code accordingly. Python Code def find_least_p(): = 3 # Start with the smallest odd prime while True: # Check if there exists such that n^4 = -1 mod found_n = False for in range(1, p): if (n**4) % == - 1: found_n = True break if found_n: # Now check if there exists such that m^4 = -1 mod p^2 p_squared = * for in range(1, p_squared): if (m**4) % p_squared == p_squared - 1: return p, = next_prime(p) def next_prime(p): # Implement function to get the next prime after if == 2: return 3 candidate = + 2 while not is_prime(candidate): candidate += 2 return candidate Python Code def is_prime(num): # Implement function to check if number is prime if num <= 1: return False if num == 2: return True if num % 2 == 0: return False for in range(3, int(num**0.5) + 1, 2): if num % == 0: return False return True # Find the least prime and the least p, = find_least_p() print((p, m)) Output (17, 110) The code has determined that the least prime for which there exists positive integer such that n4 + 1 is divisible by p2 is = 17, and the least positive integer such that m4 + 1 is divisible by 172 = 289 is = 110. To verify this, lets check if 1104 + 1 is indeed divisible by 289. First, compute 1104: 1102 = 12100 1104 = 121002 = 146, 410, 000 Then, 1104 + 1 = 146, 410, 001. Now, divide by 289: Lets perform the division: 146, 410, 001 289 506, 613 = 289 500, 000 = 144, 500, 000 289 6, 613 = 289 6, 000 = 1, 734, 000 289 613 = 289 600 = 173, 400 289 13 = 3, 757 Adding them up: 144, 500, 000 + 1, 734, 000 = 146, 234, 146, 234, 000 + 173, 400 = 146, 407, 400 146, 407, 400 + 3, 757 = 146, 411, 157 Wait, thats higher than 146,410,001. Maybe miscalculated the multiplication. Alternatively, perhaps should compute 146, 410, 001 146, 411, 157 = 1, 156, which is not zero, indicating that 289 does not divide 146,410,001. This suggests that there might be an error in the codes result. Alternatively, maybe need to double-check the calculation. Alternatively, perhaps there is better mathematical approach to find and m. Lets consider that n4 1 (mod p2), which implies that n8 1 (mod p2). So, the order of modulo p2 divides 8. As earlier, the order must be 8 because smaller exponents wouldnt satisfy n4 1. The multiplicative group modulo p2 has order p(p 1), so 8 must divide p(p 1). Since is prime, and greater than 2, 1 is even, and for 8 to divide p(p 1), either: - 1 (mod 8), or - 3 (mod 4) and 1 is divisible by 4, but 8 divides p(p 1), which would require 1 to be divisible by 8. Wait, actually, since is odd (as = 2 doesnt work), 1 is even, and for 8 to divide p(p 1), since is odd, 1 must be divisible by 8. Therefore, 1 (mod 8). So, the smallest primes to check are 17, 41, 73, etc. Earlier, the code suggested = 17, but the verification seems off. Let me try to compute = 110, = 17, p2 = 289, and check 1104 + 1 modulo 289. Compute 110 mod 17: 110 17 = 6 17 = 102, remainder 8, so 110 8 (mod 17). Wait, but we need modulo 289. Alternatively, perhaps can compute 1104 + 1 modulo 289 directly. Lets compute 1102 mod 289: 1102 = 12100. 12100 289: 289 41 = 11849. 12100 - 11849 = 251. So, 1102 251 (mod 289). Now, 1104 = (1102)2 2512 (mod 289). Compute 2512: 2512 = 63001. Now, 63001 289: 289 217 = 289 200 = 57,800; 289 17 = 4,913; total 57,800 + 4,913 = 62,713. 63,001 - 62,713 = 288. So, 2512 288 (mod 289). Therefore, 1104 288 (mod 289), so 1104 + 1 288 + 1 = 289 0 (mod 289). So, indeed, = 110 is solution for = 17. Therefore, the least prime is 17, and the least positive integer such that m4 + 1 is divisible by 172 = 289 is 110. **Final Answer** 110 A.5.3 case form AIME25 case form AIME25 Problem: The set of points in 3-dimensional coordinate space that lie in the plane + + = 75 whose coordinates satisfy the inequalities yz < zx < xy forms three disjoint convex regions. Exactly one of those regions has finite area. The area of this finite region can be expressed in the b, where and are positive integers and is not divisible by the square of any prime. form Find + b. Solution: So Ive got this problem here. Its about finding the area of specific region in 3-dimensional space defined by some equations and inequalities. Let me try to wrap my head around it. First, theres plane given by the equation + + = 75. So, all the points were considering lie on this plane. Then, there are these inequalities: yz < zx < xy. The problem says that these conditions define three disjoint convex regions on the plane, and exactly one of them has finite area. We need to find the area of that finite region, and its expressed in the form Okay, lets start by understanding the inequalities. They are: b, where is square-free, and then find + b. yz < zx < xy This is chain of inequalities, so actually, there are two inequalities here: 1. yz < zx 2. zx < xy need to manipulate these inequalities to get better understanding of the regions they define. Lets work on the first inequality: Lets bring all terms to one side: Factor terms: Factor out (1 + z): yz < zx yz + zx < 0 x(1 + z) y(1 + z) < (x y)(1 + z) < 0 So, this inequality implies that (x y)(1 + z) < 0. Now, the second inequality: Bring all terms to one side: Factor terms: Factor out (1 + x): zx < xy zx + xy < 0 y(1 + x) z(1 + x) < 0 (y z)(1 + x) < 0 So, the second inequality is (y z)(1 + x) < 0. Now, combining both inequalities, we have: 1. (x y)(1 + z) < 0 2. (y z)(1 + x) < 0 These are the conditions that, along with the plane equation + + = 75, define the regions in question. Next, need to consider the plane + + = 75. Since were dealing with 3D space, but constrained to this plane, its essentially 2D surface. To find the area of region on this plane, might need to parameterize the plane or find way to project it onto 2D coordinate system. But before that, need to understand how these inequalities divide the plane into regions. Lets consider the equalities corresponding to the inequalities to find the boundaries: 1. (x y)(1 + z) = 0 2. (y z)(1 + x) = 0 These equalities hold when either = or = 1, and = or = 1. But since were dealing with plane where + + = 75, and presumably x, y, are real numbers (unless specified otherwise), need to see how these boundaries intersect with the plane. Wait minute, maybe its better to consider substituting = 75 into the inequalities to express everything in terms of and y, reducing the dimensionality. Lets try that. Given = 75 y, substitute into the inequalities. First inequality: Substitute z: Simplify: Second inequality: Substitute z: Simplify: Combine like terms: (x y)(1 + z) < 0 (x y)(1 + 75 y) < (x y)(76 y) < 0 (y z)(1 + x) < 0 (y (75 y))(1 + x) < 0 (y 75 + + y)(1 + x) < 0 (x + 2y 75)(1 + x) < 0 So now, the inequalities in terms of and are: 1. (x y)(76 y) < 0 2. (x + 2y 75)(1 + x) < More reasoning content are omitted here Wait, using Python, can experiment with new method to tackle this challenge. Python Code import sympy as sp # Define variables x, y, = sp.symbols('x z') # Plane equation plane_eq = + + - # Inequalities ineq1 = - y*z < - z*x ineq2 = - z*x < - x*y # Solve plane equation for z_expr = sp.solve(plane_eq, z)[0] # Substitute in inequalities ineq1_sub = ineq1.subs(z, z_expr) ineq2_sub = ineq2.subs(z, z_expr) # Simplify inequalities ineq1_simplified = sp.simplify(ineq1_sub) ineq2_simplified = sp.simplify(ineq2_sub) print((ineq1_simplified, ineq2_simplified)) Output (x + y*(x + - 75) < x*(x + - 75) + y, x*y + + - 75 < -x*(x + - 75) - y) More reasoning content are omitted here To find the area of this triangle, we can use the formula for the area of triangle given by three points in 3D space: First, lets find the vectors BC and BF : Area = BC BF 1 2 BC = = (1 25, 1 25, 77 25) = (26, 26, 52) BF = = (1 25, 38 25, 38 25) = (26, 13, 13) BC Next, compute the cross product Wait, can use Python to perform complex calculations for this problem. BF . Python Code import sympy as sp # Define the symbols for the components of the vectors x1, y1, z1 = -26, -26, 52 # Components of vector BC # Components of vector BF x2, y2, z2 = -26, 13, 13 # Create symbolic vectors BC = sp.Matrix([x1, y1, z1]) BF = sp.Matrix([x2, y2, z2]) # Compute the cross product BC BF cross_product = BC.cross(BF) # Print the cross product print(\"Cross product BC BF:\", cross_product) # Calculate the magnitude of the cross product magnitude = sp.sqrt(cross_product.dot(cross_product)) print(magnitude) Output 1014*sqrt(3) Therefore, the area of the parallelogram is 1014 3, and the area of the triangle is half of that: Area = 1 2 3 = 507 3 Hence, = 507 and = 3, so + = 510. **Final Answer** A.5.4 case form MATH500 case form MATH500 Problem: The function satisfies the functional equation (x) + (y) = (x + y) xy 1 for all real numbers and y. If (1) = 1, then find all integers such that (n) = n. Enter all such integers, separated by commas. Solution: So have this functional equation: (x) + (y) = (x + y) xy 1 for all real numbers and y. And Im given that (1) = 1. need to find all integers such that (n) = n. First, should probably try to find general expression for (x). To do that, can start by plugging in some specific values for and to see if can find pattern or determine the form of . Let me try setting = 0. Then the equation becomes: Which simplifies to: So: (x) + (0) = (x + 0) 0 1 (x) + (0) = (x) 0 1 Thats helpful. Now know that (0) = 1. Next, maybe can try setting = 1, since know (1) = 1. So: (0) = Substituting (1) = 1: Let me solve for (x + 1): (x) + (1) = (x + 1) 1 1 (x) + 1 = (x + 1) 1 (x + 1) = (x) + 1 + + 1 = (x) + + 2 So, have recursive relation: (x + 1) = (x) + + 2. This seems like it could be useful for finding pattern or for induction. Let me try to find (2), (3), and so on, to see if can find pattern. Starting with = 1: (2) = (1) + 1 + 2 = 1 + 1 + 2 = 4 Now = 2: (3) = (2) + 2 + 2 = 4 + 2 + 2 = 8 Wait, that doesnt seem right. Let me check the recursive formula again. have (x + 1) = (x) + + 2. So, for = 1: For = 2: (2) = (1) + 1 + 2 = 1 + 1 + 2 = Wait, but if continue this pattern, (3) = 8, then for = 3: (3) = (2) + 2 + 2 = 4 + 2 + 2 = 8 (4) = (3) + 3 + 2 = 8 + 3 + 2 = 13 Hmm, Im not seeing an immediate pattern here. Maybe theres better way to approach this. Alternatively, perhaps can assume that (x) is quadratic function, since the functional equation involves term like xy, which is linear in both variables. Let me assume that (x) = ax2 + bx + c, and try to find the coefficients a, b, and c. Substituting into the functional equation: So: (x) + (y) = (x + y) xy (ax2 + bx + c) + (ay2 + by + c) = a(x + y)2 + b(x + y) + xy 1 Let me expand both sides. Left side: Right side: ax2 + ay2 + bx + by + 2c a(x2 + 2xy + y2) + b(x + y) + xy 1 = ax2 + 2axy + ay2 + bx + by + xy 1 Now, set the left side equal to the right side: ax2 + ay2 + bx + by + 2c = ax2 + 2axy + ay2 + bx + by + xy 1 Subtract ax2 + ay2 + bx + by from both sides: 2c = 2axy + xy 1 Wait, that seems off. Let me double-check the expansion. Actually, perhaps Python can assist in managing these detailed computations automatically. Python Code from sympy import symbols, Eq, solve # Define symbols a, b, c, x, = symbols('a y') # Define f(x), f(y), and f(x + y) f_x = a*x**2 + b*x + f_y = a*y**2 + b*y + f_x_y = a*(x + y)**2 + b*(x + y) + # Define the functional equation functional_eq = Eq(f_x + f_y, f_x_y - x*y - 1) # Expand both sides lhs = f_x + f_y rhs = f_x_y - x*y - 1 # Simplify the equation simplified_eq = Eq(lhs - rhs, 0) # Collect like terms collected_eq = simplified_eq.expand() # Solve for coefficients a, b, solution = solve(collected_eq, (a, b, c)) print(solution) Output [(1/2, b, -1)] From the symbolic computation, the solution for the coefficients is: = 1 , = b, = 1 Wait, it says = b, which means can be any value. That suggests that theres some flexibility in choosing b, but likely, theres another condition that can help determine b. Given that (1) = 1, can use this to find b. Given (x) = 1 2 x2 + bx 1, and (1) = 1: 1 2 (1)2 + b(1) 1 = + 1 = (1) = = 1 1 2 1 2 So: Therefore, the function is: 1 2 = 1 = = 1 + 1 2 = 3 (x) = x2 + 1 2 3 2 1 Now, need to verify if this function satisfies the original functional equation. Lets plug (x) = 1 2 1 into the left side: 2 1 and (y) = 1 2 x2 + 3 2 y2 + 3 (x) + (y) = (cid:18) 1 2 x2 + 3 2 (cid:19) 1 + (cid:18) 1 2 y2 + 3 2 (cid:19) 1 = x2 + 1 y2 + 1 2 3 2 + 3 2 Now, the right side is: (x + y) xy 1 = (cid:18) 1 2 (x + y)2 + 3 2 (cid:19) (x + y) 1 xy 1 Expand (x + y): (x2 + 2xy + y2) + 1 2 3 + 3 2 1 = 1 2 x2 + xy + y2 + 1 2 3 2 + 3 2 1 More reasoning content are omitted here Now, need to find all integers such that (n) = n. Set (n) = n: Subtract from both sides: Simplify: n2 + 1 2 3 1 = n2 + 1 2 3 2 1 = 0 n2 + 1 2 (cid:18) 3 2 (cid:19) 1 1 = 0 1 2 Multiply both sides by 2 to eliminate fractions: n2 + 1 2 1 = 0 Now, solve for n: So, two solutions: n2 + 2 = 1 = 2 1 + 8 = 1 3 2 = 1 + 3 2 = 1 = 1 3 = 2 Therefore, the integers that satisfy (n) = are = 1 and = 2. **Final Answer** 2, 1 A.5.5 case form GPQA case form GPQA Problem: Astronomers are studying star with 1.5 solar radius and 1.1 solar masses. When the stars surface is not covered by dark spots, its Teff is 6000 K. However, when 40% of its surface is covered by spots, the overall photospheric effective temperature decreases to 5500 K. In the stellar photosphere, when examining the ratio of the number of neutral atoms of Ti in two energetic levels (level 1 and level 2), astronomers have observed that this ratio decreases when the star has spots. What is the factor by which this ratio changes when the star does not have spots compared to when it has spots? Note that the transition between the energy levels under consideration corresponds to wavelength of approximately 1448 Å. Assume that the stellar photosphere is in LTE. Answer Choices: (A) 2.9 (B) 4.5 (C) 1.1 (D) 7.8 Output your final answer in boxes, such as . Solution: So Im trying to solve this astronomy problem. Its about star with some specific properties, and its talking about how the temperature changes when there are spots on its surface. Then, its about how the ratio of neutral titanium atoms in two different energy levels changes when there are spots. Its bit complicated, but Ill try to break it down step by step. First, lets understand whats given: - The star has 1.5 solar radius and 1.1 solar masses. - When there are no spots, its effective temperature (Teff) is 6000 K. - When 40% of its surface is covered by spots, the overall Teff decreases to 5500 K. - We need to find the factor by which the ratio of neutral titanium atoms in two energy levels changes when the star has spots compared to when it doesnt. - The transition between the levels corresponds to wavelength of about 1448 Å. - Were to assume that the stellar photosphere is in local thermodynamic equilibrium (LTE). Okay, so the key here is to relate the change in effective temperature due to spots to the change in the ratio of titanium atoms in different energy levels. First, need to recall what effective temperature is. Teff is measure of the stars surface temperature, and its related to the stars luminosity and radius through the Stefan-Boltzmann law: = 4πR2σT 4 eff Where is luminosity, is radius, and σ is the Stefan-Boltzmann constant. But in this problem, the stars radius is given, and the masses are given, but Im not sure if need to use that information directly. Maybe its just to provide context. The main change is in Teff, from 6000 to 5500 when 40% of the surface is covered by spots. need to find out how this change in Teff affects the ratio of titanium atoms in two energy levels. First, should recall that in LTE, the ratio of atoms in two energy levels is given by the Boltzmann equation: N2 N1 = g2 (cid:18) exp (cid:19) kT Where: - N2 and N1 are the number densities of atoms in energy levels 2 and 1, respectively. - g2 and g1 are the statistical weights of the two levels. - is the energy difference between the two levels. - is the Boltzmann constant. - is the temperature. In this case, since were dealing with the same element (titanium), and assuming that the statistical weights dont change, the ratio simplifies to: N2 N1 (cid:18) exp (cid:19) kT So, the ratio depends exponentially on the temperature. Now, the problem states that the ratio decreases when the star has spots. That makes sense because spots are cooler regions on the stars surface, so the overall Teff decreases, which would lead to lower ratio according to the equation above. But the question is asking for the factor by which the ratio changes when the star does not have spots compared to when it has spots. So, need to find the ratio of Lets denote: N2 N1 (cid:12) (cid:12) (cid:12) (cid:12)no spots to N2 N1 (cid:12) (cid:12) (cid:12) (cid:12)with spots . Rno spots = Rwith spots = N2 N2 N1 at Teff = 6000 at Teff = 5500 But that seems too large. Maybe using Python here is good idea. More reasoning content are omitted here Python Code , text{K} begin{verbatim} import math # Given values T_{text{eff}} = 5500 , text{K} T_u = 6000 text{fraction_spots} = 0.4 text{fraction_unspotted} = 0.6 # Calculate T_{text{eff}}^4 T_{text{eff_4}} = T_{text{eff}}^{4} # Calculate T_u^4 T_{u_4} = T_u^{4} # Calculate T_s^4 T_{s_4} = frac{T_{text{eff_4}} - text{fraction_unspotted} cdot T_{u_4}}{text{fraction_spots}} # Calculate T_s T_s = T_{s_4}^{1/4} print(T_s) Output 4305.574037901423 Wait, according to the calculation, Ts 4305.57 K, which is lower than the unspotted temperature of 6000 K, as expected for spots. must have made mistake in my earlier manual calculation. So, the temperature of the spots is approximately 4305.57 K. Given that, and considering that the answer choices are discrete, option C) 1.1 is the closest. Alternatively, perhaps there is miscalculation in E/k. Let me double-check that with Python code. Python Code import numpy as np # Constants = 6.626e-34 # Planck's constant in Joules second = 3.00e8 wavelength = 1.448e-7 # Wavelength in meters = 1.38e-23 # Speed of light in meters per second # Boltzmann constant in Joules per Kelvin # Calculate the energy difference Delta delta_E = (h * c) / wavelength print(f\"Delta = {delta_E:.2e} J\") # Output energy in scientific notation # Calculate the ratio Delta / delta_E_over_k = delta_E / print(f\"Delta / = {delta_E_over_k:.2f} K\") # Output in Kelvin # Calculate the final factor T1 = 5500 # Temperature in Kelvin T2 = 6000 # Temperature in Kelvin factor = np.exp(delta_E_over_k * (1/T1 - 1/T2)) print(f\"Factor = {factor:.3f}\") # Output the factor Output = 1.37 1018 = 99477.54 Factor = 4. Given that, and considering the answer choices, option B) 4.5 is the closest. Final Answer A.5.6 case form LiveCodeBench case form LiveCodeBench Problem: You are given an array of positive integers nums of length n. We call pair of non-negative integer arrays (arr1, arr2) monotonic if: The lengths of both arrays are n. arr1 is monotonically non-decreasing, in other words, arr1[0] arr1[1] . . . arr1[n 1]. arr2 is monotonically non-increasing, in other words, arr2[0] arr2[1] . . . arr2[n 1]. arr1[i] + arr2[i] == nums[i] for all 0 1. Return the count of monotonic pairs. Since the answer may be very large, return it modulo 109 + 7. Example 1: Input: nums = [2,3,2] Output: 4 Explanation: The good pairs are: ([0, 1, 1], [2, 2, 1]) ([0, 1, 2], [2, 2, 0]) ([0, 2, 2], [2, 1, 0]) ([1, 2, 2], [1, 1, 0]) Example 2: Input: nums = [5,5,5,5] Output: Constraints: 1 == nums.length 2000 1 nums[i] 50 You will use the following starter code to write the solution to the problem and enclose your code within delimiters. Python Code class Solution: def countOfPairs(self, nums: List[int]) -> int: Solution: Alright, Ive got this problem to solve. Lets see what its asking for. Im given an array of positive integers called nums, and need to find the count of monotonic pairs (arr1, arr2). The conditions are: 1. Both arr1 and arr2 have the same length as nums. 2. arr1 is monotonically non-decreasing, meaning each element is less than or equal to the next one. 3. arr2 is monotonically non-increasing, meaning each element is greater than or equal to the next one. 4. For every index i, arr1[i] + arr2[i] equals nums[i]. And finally, need to return the count of such pairs, modulo 109 + 7 because the number can be really big. Okay, first things first, need to understand what constitutes valid pair (arr1, arr2). Given that arr1 is non-decreasing and arr2 is non-increasing, and their sum at each position equals nums[i], there must be some relationship between the possible values in arr1 and arr2. Let me consider simple example to get feel for the problem. Take nums = [2, 3, 2], which is in the example. The output is 4, and they provided four pairs that satisfy the conditions. Lets list them out again: 1. ([0, 1, 1], [2, 2, 1]) 2. ([0, 1, 2], [2, 2, 0]) 3. ([0, 2, 2], [2, 1, 0]) 4. ([1, 2, 2], [1, 1, 0]) can see that in each pair, arr1 is non-decreasing, arr2 is non-increasing, and their sums equal the corresponding nums[i]. Now, how can generalize this to any given nums array? need to find all possible arr1 and arr2 that satisfy these properties. One approach that comes to mind is to iterate through all possible arr1 arrays that are nondecreasing, and for each arr1, determine if the corresponding arr2 array, which is nums - arr1, is non-increasing. But this seems inefficient, especially since can be up to 2000, and the numbers can be up to 50. That means nums[i] can be up to 50, so arr1[i] and arr2[i] can range from 0 to 50. Wait, but arr1 and arr2 are arrays of non-negative integers, and their sum is nums[i], which is at least 1 according to the constraints. So arr1[i] can range from 0 to nums[i], and arr2[i] would be nums[i] - arr1[i]. Given that, for each index i, arr1[i] can be any integer from 0 to nums[i], and arr2[i] is determined once arr1[i] is chosen. But also have the constraints on the monotonicity of arr1 and arr2. So, arr1 has to be non-decreasing, and arr2 has to be non-increasing. This means that for arr1, arr1[i] arr1[i + 1], and for arr2, arr2[i] arr2[i + 1]. Given that arr2[i] = nums[i] arr1[i], can substitute this into the condition for arr2. So, nums[i] arr1[i] nums[i + 1] arr1[i + 1]. Simplifying this, arr1[i + 1] arr1[i] nums[i + 1] nums[i]. Let me denote d[i] = nums[i + 1] nums[i]. Then, arr1[i + 1] arr1[i] d[i]. Additionally, since arr1 is non-decreasing, arr1[i + 1] arr1[i] 0. Combining these, arr1[i + 1] arr1[i] max(0, d[i]). This is interesting. It means that the difference between consecutive elements in arr1 has lower bound based on the difference in nums. So, to build arr1, need to ensure that each step increases by at least max(0, d[i]), and overall, arr1 is non-decreasing. Given that, perhaps can model this as series of choices for the differences between consecutive arr1 elements. Let me define and diff[i] = arr1[i + 1] arr1[i], diff[i] max(0, d[i]). Also, since arr1 is non-decreasing, diff[i] 0. But actually, diff[i] max(0, d[i]) already ensures that diff[i] 0 when d[i] 0. When d[i] < 0, diff[i] 0 suffices because arr1 is non-decreasing. Wait, lets think about d[i] being negative. If d[i] = nums[i + 1] nums[i] < 0, then arr1[i + 1] arr1[i] d[i], but since d[i] is negative, and arr1 is non-decreasing, arr1[i + 1] arr1[i] 0 is already stronger than arr1[i + 1] arr1[i] d[i]. Therefore, for all i, diff[i] max(0, d[i]). More reasoning content are omitted here To ensure that my code runs correctly, need to embed all test case inputs directly into my code and print the corresponding output, following the sample structure below: Debug Code Template s l o : c t a ( f , nums : t [ ] ) > : i r # l n u e m i t p s t = u n ( ) # t t _ u 1 = . . . t _ u 2 = . . . # n t p t ( u n . n P s ( t _ u 1 ) ) n ( u n . n P s ( t _ u 2 ) ) # Check o u This is template, not for execution. need to write code that processes the actual given sample inputs locally for the task. Alright, with this structure, can write and execute my code in Python compiler using real example inputs. By comparing the actual outputs with the expected outputs, can initially assess the correctness of my code. If the outputs do not match, can debug accordingly. Recall the test cases in the problem statement. Example 1: Input: nums = [2,3,2] Output: Explanation: The good pairs are: ([0, 1, 1], [2, 2, 1]) ([0, 1, 2], [2, 2, 0]) ([0, 2, 2], [2, 1, 0]) ([1, 2, 2], [1, 1, 0]) Example 2: Input: nums = [5,5,5,5] Output: 126 Alright, now can write debug code with samples input. Python Code class Solution: def countOfPairs(self, nums: List[int]) -> int: MOD = 10**9 + 7 = len(nums) if == 0: return 0 max_num = max(nums) dp = [[0] * (max_num + 1) for _ in range(2)] for in range(nums[0] + 1): dp[0][x] = 1 for in range(1, n): = max(0, nums[i] - nums[i-1]) dp[i % 2] = [0] * (max_num + 1) prefix = [0] * (max_num + 1) for in range(max_num + 1): if - >= 0: dp[i % 2][x] = dp[(i-1) % 2][x - m] else: dp[i % 2][x] = prefix[x] = (prefix[x-1] + dp[i % 2][x]) % MOD if >= 1 else dp[i % 2][x] dp[(i-1) % 2] = prefix return sum(dp[(n-1) % 2][:nums[-1]+1]) % MOD Python Code # Test the example inputs solution = Solution() # Example input1 test_input1 = [2,3,2] # Print output print(solution.countOfPairs(test_input1)) # Example input2 test_input2 = [5,5,5,5] # Print output print(solution.countOfPairs(test_input2)) # Expected Output: 4 # Expected Output: 126 Output 2 6 It appears that the current implementation is not producing the correct outputs for the provided examples. According to the explanation, the first example should yield 4, but its giving 2, and the second example should yield 126, but its giving 6. This indicates flaw in the logic or implementation. Lets revisit the approach. Corrected Approach 1. Initialization: We need to initialize DP table where dp[i][x] represents the number of ways to form the first elements of arr1 such that arr1[i 1] = x. 2. Transition: For each position from 1 to 1, and for each possible value that arr1[i] can take (from 0 to nums[i]), we determine the valid range of arr1[i 1] based on the constraints. Specifically, arr1[i] arr1[i 1] and arr2[i] arr2[i 1], which translates to arr1[i] arr1[i 1] + m[i], where m[i] = max(0, nums[i] nums[i 1]). 3. Prefix Sums: Use prefix sums to efficiently compute the sum of dp[i 1][y] for in the valid range. 4. Final Result: Sum up dp[n 1][x] for all from 0 to nums[1]. More reasoning content are omitted here To fix the issues, we need to ensure that the prefix sums are correctly maintained and used in the DP transitions. Here is the revised code: Python Code from typing import List class Solution: def countOfPairs(self, nums: List[int]) -> int: MOD = 10**9 + 7 = len(nums) if == 0: return max_num = max(nums) dp = [[0] * (max_num + 1) for _ in range(2)] for in range(nums[0] + 1): dp[0][x] = 1 prefix = [0] * (max_num + 1) for in range(max_num + 1): prefix[x] = dp[0][x] if == 0 else (prefix[x-1] + dp[0][x]) % MOD for in range(1, n): = max(0, nums[i] - nums[i-1]) current_row = % 2 dp[current_row] = [0] * (max_num + 1) new_prefix = [0] * (max_num + 1) for in range(max_num + 1): if - >= 0: dp[current_row][x] = prefix[x - m] else: dp[current_row][x] = 0 new_prefix[x] = (new_prefix[x-1] + dp[current_row][x]) % MOD if >= 1 else dp[current_row][x] prefix = new_prefix return sum(dp[(n-1) % 2][:nums[-1]+1]) % MOD # Test the example inputs solution = Solution() # Example input1 test_input1 = [2,3,2] # Print output print(solution.countOfPairs(test_input1)) # Example input2 test_input2 = [5,5,5,5] # Print output print(solution.countOfPairs(test_input2)) # Expected Output: 4 # Expected Output: 126 Output 4 126 Great! The revised code now produces the correct outputs for the provided examples. This indicates that the approach and implementation are accurate. More reasoning content are omitted here Python Code from typing import List class Solution: def countOfPairs(self, nums: List[int]) -> int: MOD = 10**9 + 7 = len(nums) if == 0: return 0 max_num = max(nums) dp = [[0] * (max_num + 1) for _ in range(2)] for in range(nums[0] + 1): dp[0][x] = 1 prefix = [0] * (max_num + 1) for in range(max_num + 1): prefix[x] = dp[0][x] if == 0 else (prefix[x-1] + dp[0][x]) % MOD for in range(1, n): = max(0, nums[i] - nums[i-1]) current_row = % 2 dp[current_row] = [0] * (max_num + 1) new_prefix = [0] * (max_num + 1) for in range(max_num + 1): if - >= 0: dp[current_row][x] = prefix[x - m] else: dp[current_row][x] = 0 new_prefix[x] = (new_prefix[x-1] + dp[current_row][x]) % MOD if >= 1 else dp[current_row][x] prefix = new_prefix return sum(dp[(n-1) % 2][:nums[-1]+1]) % MOD The final solution has been successfully implemented and validated with the provided test cases. This approach ensures that we efficiently count the number of monotonic pairs while handling the constraints and large input sizes appropriately. Explanation: - Initialization: We initialize DP table for the first element of nums, setting possible values for arr1[0]. - Transition: For each subsequent element, we calculate the minimum difference and use prefix sums to efficiently compute the number of valid sequences. - Final Calculation: We sum up the valid sequences for the last element to get the total count of monotonic pairs. This method ensures that we handle the constraints effectively and compute the result efficiently even for larger input sizes."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "University of Science and Technology of China"
    ]
}