{
    "paper_title": "Visual Diffusion Models are Geometric Solvers",
    "authors": [
        "Nir Goren",
        "Shai Yehezkel",
        "Omer Dahary",
        "Andrey Voynov",
        "Or Patashnik",
        "Daniel Cohen-Or"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem. Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation. Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks."
        },
        {
            "title": "Start",
            "content": "Visual Diffusion Models are Geometric Solvers Nir Goren1,, Shai Yehezkel1,, Omer Dahary1, Andrey Voynov2, Or Patashnik1, Daniel Cohen-Or1 1Tel Aviv University 2Google DeepMind 5 2 0 2 4 2 ] . [ 1 7 9 6 1 2 . 0 1 5 2 : r Abstract In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, long-standing problem in geometry that asks whether every Jordan curve contains four points forming square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem. Our method treats each problem instance as an image and trains standard visual diffusion model that transforms Gaussian noise into an image representing valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation. Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward broader paradigm: operating in image space provides general and practical framework for approximating notoriously hard problems, and opens the door to tackling far wider class of challenging geometric tasks. 1 introduction Diffusion models have emerged as transformative force in generative AI. Initially developed for image synthesis, they have quickly proven to be among the most powerful and versatile generative models across wide range of media, including audio, video, and 3D content. Their ability to progressively denoise random signals into coherent and high-fidelity samples has enabled breakthrough applications, from photorealistic image generation to controllable editing and cross-modal translation. Beyond their remarkable empirical success, diffusion models are increasingly recognized as general framework for modeling complex, multimodal distributions. In this work, we take different perspective on diffusion models: rather than focusing on their creative generative capacity, we demonstrate their potential as solvers of hard geometric problems. We show that the sampling process of diffusion can be harnessed to directly reason about and discover geometric structures, guided only by pixel-level formulations of the problem. This visual diffusion approach allows us to treat abstract geometric challenges as image generation tasks, bridging the gap between visual synthesis and mathematical problem-solving. *Equal contribution. Figure 1: We introduce visual diffusion approach to solving hard geometric problems directly in pixel space. Shown here on the Inscribed Square Problem, where we task the model with finding square such that all of its four vertices lie on given curve. Our method uncovers diverse approximate solutions, corresponding to different random seeds. Diffusion models have been used in various contexts to tackle optimization and reasoning problems, including combinatorial tasks such as the traveling salesman problem [26, 36, 40]. These approaches typically formulate the problem in symbolic or graphbased representations, leveraging the probabilistic nature of diffusion to search solution spaces. In contrast, our method operates purely in the visual domain. By representing geometric problems as images and reasoning directly in pixel space, we exploit the intrinsic strength of diffusion models in handling multimodal distributions and ambiguous solutions. This visual formulation makes our approach fundamentally distinct from prior problem-solving applications of diffusion. To ground our approach, we begin with the Inscribed Square Problem, long-standing problem that asks whether every simple closed curve in the plane admits an inscribed square. The problem is still unsolved in the general case. Furthermore, given curve may admit multiple and often very different inscribed squares, and enumerating them is non-trivial even in restricted settings [42]. This multiplicity naturally forms distribution, which makes the problem especially well suited to diffusion models. Our method addresses it in an unexpected way, operating directly in image space on visual representation of the geometric challenge. By starting from different random seeds, the model can uncover diverse valid squares, each corresponding to distinct solution of the problem. Figure 1 illustrates the setting and highlights both the complexity and the variability of possible solutions. We next illustrate the competence of our diffusion-in-imagespace approach on two additional hard geometric problems. The first is the Steiner Tree Problem, which asks for the shortest possible network connecting given set of points. Its solution may introduce auxiliary nodes, known as Steiner points, and finding the optimal configuration is NP-hard [17]. The second is the problem of connecting set of points into simple polygon of maximum area, which was featured in the CG:SHOP global optimization challenge of 2019 [9]. This task is known for its combinatorial complexity and strict geometric constraints, and is also NP-hard. As with the inscribed square problem, our method addresses these problems directly in image space, operating in the pixel domain. While discretization at finite resolution imposes limitations, it nonetheless enables valid approximations to problems that are otherwise extremely hard, with solutions that can be naturally refined. We evaluate this aspect rigorously in the paper. Training follows deliberately simple yet effective strategy. We generate large distribution of valid solutions directly in image space and train diffusion process to denoise random Gaussian samples into this distribution. This strategy builds on the observation that, in many cases, constructing examples of valid solutions is far easier than deriving one for specific input instance. Our image-space formulation therefore offers notable simplicity: it requires no elaborate encodings or specialized representations. At the same time, it provides natural entry point to wide spectrum of hard geometric problems that admit natural visual representations, beyond the three studied in this work."
        },
        {
            "title": "2 Related Work\nSeveral works have attempted to use diffusion models in order to\nlearn to solve problems for which no known efficient algorithm\nexists, mostly of combinatorial nature. Most existing diffusion\nworks operate directly on the parameter space of the problem.\nDIFUSCO [40], a graph-based diffusion framework, casts a broad\nfamily of NP-complete problems into {0, 1}ùëÅ indicator vectors and\nlearns a denoiser over graphs. They compare continuous (Gaussian)\nand discrete (Bernoulli) noise processes, and show strong results\non traveling salesman problem (TSP) and maximum independent\nset (MIS). Complementing the supervised approach, the T2T [26]\nline of work learns a distribution of high-quality solutions dur-\ning training and then performs gradient-guided optimization at\ntest time by iteratively noising the current solution and denois-\ning while guiding towards lower energy solutions of some relaxed\nobjective, achieving competitive quality‚Äìefficiency trade-offs on\nTSP and MIS. Fast T2T [27] significantly speeds this up via an\noptimization-consistency objective, matching or surpassing multi-\nstep diffusion solvers performance with single-step generation plus\na single gradient step.",
            "content": "Goren and Yehezkel et al. Figure 2: Example of curve (black) with three inscribed squares. Note that the inscribed squares are defined only by having all four vertices on the curve: they need not be fully contained within the curve, and they may overlap with each other. Closer to our approach, some methods solve constrained problems in pixel-space representation. [20] train an unconditional diffusion model on pixel-space representations of TSP instances. They then solve new instances with stochastic optimization using differential renderer, utilizing the prior of the learned model. Differently from us, they optimize parametric representation of the solution to given instance, while we generate solutions with DDIM sampling of conditional model. [47] train noise-prediction UNet in pixel space on visual representation of Sudoku, which is another NP-hard combinatorial problem. They depart from fully-parallel image diffusion by (i) assigning individual noise levels to patches, and (ii) sampling patches in learned order or hand-crafted order. They demonstrate that sampling order matters, and can substantially outperform conditional DDPM baseline that operates fully in parallel."
        },
        {
            "title": "3 Inscribed Square Problem\nWe present our visual diffusion approach as a solver for hard geo-\nmetric problems by structuring the paper around a set of case\nstudies on well-known challenges. Each case study begins with\nthe problem statement and its mathematical context, followed by a\nbrief review of existing methods. We then describe how our image-\nspace diffusion formulation addresses the task, highlighting both\nits capabilities and limitations. The first and central case we study is\nthe inscribed square problem, which serves as an illustrative entry\npoint into our approach.",
            "content": "Problem Statement. The Inscribed Square Problem, also known as Toeplitzs Square Peg Problem first posed in 1911, asks whether every Jordan curve in the Euclidean plane contains four points that form square. Formally, the conjecture states that for every Jordan curve ùê∂ R2, there exist four points {ùëù1, ùëù2, ùëù3, ùëù4} ùê∂ such that ùëù1, ùëù2, ùëù3, ùëù4 are the vertices of non-degenerate square. Figure 2 illustrates this setting, showing curve with several inscribed squares. The problem has since been resolved in several restricted settings. Some works show the conjecture holds for convex and piecewise analytic curves [1113] and later results prove it for ùê∂1-smooth curves Visual Diffusion Models are Geometric Solvers Figure 3: Inscribed square ùë•0 predictions across denoising steps. Each row corresponds to different seed (inscribed square). Columns show selected ùë•0 predictions for decreasing timesteps ùë° from left to right (leftmost: ùë°=ùëá ; penultimate: ùë°=0). For ùë° 0 we render only the filled mask; at ùë°=0 we also draw square edges and the minimum-area bounding box. The rightmost column (snapped) shows the rigidly snapped version of the ùë°=0 prediction on the curve, with an arrow from the original centroid to the snapped centroid. and for curves of finite total curvature or generic ùê∂1 curves [4, 39]. More recent work demonstrates validity under additional lowregularity assumptions [41], yet it remains open for the general Jordan curve case. Existing Methods. Algorithmic approaches for finding inscribed squares exist primarily for discrete or polygonal cases. For convex polygons, efficient procedures have been developed to detect or enumerate inscribed squares in subquadratic time [5, 38]. Discrete relaxations on grid polygons have likewise been studied and verified computationally for bounded cases [33]. All these methods represent the curve symbolically or combinatorially, relying on exact geometric descriptions. In contrast, our work formulates the problem directly in pixel space. Method. We address the inscribed square problem by formulating it entirely in image space and training conditional diffusion model to recover inscribed squares from noisy inputs. To this end, we generated dataset of synthetic samples. Each sample consists of smooth Jordan curve ùê∂ together with one to five inscribed squares. The curves were constructed procedurally such that they pass through the square vertices while avoiding self-intersections, and both curves and squares were rasterized into 128 128 binary images. Each training example pairs curve image with one inscribed square. Full details of the curve-generation process are provided in the supplementary material, under Section B.1. Our model follows the standard diffusion framework with UNet [35] backbone with self-attention layers [43], similar to those employed in text-to-image diffusion models such as Stable Diffusion [34]. The conditioning signal is the clean binary image of the curve, while the ground truth ùë•0 is the clean image of the square, both represented in two-dimensional pixel space. The conditioning image is concatenated as an additional channel to the noisy input ùë•ùë° at each timestep, effectively treating the curve as non-noisy color channel. We train with 100 denoising steps using standard noise schedule and mean-squared error objective, enabling the model to transform noisy square images into valid inscribed squares consistent with the given curve. During sampling, at each step the network predicts an ùë•0 estimate of the square conditioned on the curve. To inspect this behavior, we visualize ùë•0 predictions at subset of timesteps arranged leftto-right from ùë°=ùëá to ùë°=0 (penultimate column) in Fig. 3. Square Enhancement (Snapping). As final post-processing step, we refine each predicted square (cid:98)ùëÜ by snapping it to the conditioning curve ùê∂ of the respective problem instance. Let ùëâ (ùëÜ) = {ùëù1, ùëù2, ùëù3, ùëù4} denote the set of four vertices of square ùëÜ. To quantify how well ùëÜ aligns with ùê∂, we define the negative average cornerto-curve distance as the alignment score: (ùëÜ, ùê∂) = 1 4 ùëù ùëâ (ùëÜ ) dist(ùëù, ùê∂), (1) where dist(ùëù, ùê∂) is the Euclidean distance from vertex ùëù to the curve ùê∂. We then apply small rigid transformation (ùëÖùúÉ , ùë°) to (cid:98)ùëÜ and select the configuration that maximizes this alignment score. In practice, we approximate this optimization with coarse grid search over small rotations and translations (see Section B.2 for details). This procedure nudges the predicted square so that its corners sit more closely on ùê∂, as visualized in the rightmost column of Fig. 3. Evaluation. Since multiple valid squares could potentially fit the curves beyond the ground truth squares used in construction, we avoid comparing distances to the closest ground truth square and instead focus on evaluating the geometric properties of our generated solutions. Goren and Yehezkel et al. Figure 4: Solutions produced by our model. Each Jordan curve (black) is accompanied by predicted inscribed squares (colored). Table 1: Evaluation Results. We report alignment (ùëÜ, ùê∂) and squareness under three conditions: before snapping, after snapping, and ground truth (GT). w/o snapping w/ snapping Data (GT) Align Square Align Square Align Square -1.60 0.892 -0.90 0. -0.14 0.924 We evaluate our method along two complementary axes: alignment and quality. For alignment, we report the score (ùëÜ, ùê∂) defined in Eq. 1, which directly measures how well the vertices of predicted square lie on the conditioning curve. For quality, we introduce squareness metric that captures how close predicted shape is to valid square. Given predicted square ùëÜ, let area(ùëÜ) denote its contour area, and let (ùë§, ‚Ñé) denote the side lengths of its minimum-area enclosing rectangle. We define: Interpretation of Evaluation Results. The evaluation demonstrates that our model consistently produces shapes that closely approximate true inscribed squares with strong accuracy. Even though the alignment of predicted squares with the conditioning curve is not always pixel-perfect, the snapping step leads to substantial refinement, bringing the results impressively close to the ground truth. In practice, this shows that the diffusion process is highly effective at capturing both the structural regularity and the correct placement of squares, with only minimal residual deviation that often manifests at the sub-pixel level. Importantly, such deviations are expected given the inherent discretization of the pixel domain, which naturally puts an upper bound even on the ground truth results. Within this setting, our method reliably recovers high-quality approximations of valid inscribed squares. The reported numbers confirm that the model does not merely suggest plausible candidates but in fact achieves precise and robust approximations, validating the strength of the visual diffusion framework as solver for this classical geometric challenge. (ùëÜ) = area(ùëÜ) ùë§ ‚Ñé exp(cid:16) 2 (cid:12) (cid:12) (cid:12) max(ùë§,‚Ñé) min(ùë§,‚Ñé) 1 (cid:17) (cid:12) (cid:12) (cid:12) . (2) This produces score in [0, 1] that is high only when ùëÜ tightly fills nearly equilateral rectangle, i.e., when it closely resembles true square. We report both alignment and quality metrics under three conditions: (i) predictions before snapping, (ii) predictions after snapping, and (iii) the ground-truth squares from the dataset (Tab. 1). This evaluation disentangles the intrinsic generative ability of the diffusion model from the gains achieved by the geometric snapping refinement."
        },
        {
            "title": "4 Steiner Tree Problem\nThe second problem we cover in our case study is that of the Steiner\nTree Problem. The Steiner tree problem asks, given a set of terminal\npoints, to find a network of minimum total length that connects all\nterminals, where the construction is allowed to introduce additional\npoints (Steiner points) to reduce length. In the Euclidean variant,\nwhich we focus on, Steiner points may be placed anywhere in\nthe plane. The Steiner formulation is central to many applications\nwhere minimizing connection cost is critical. Some typical use cases\nfor it include telecommunication [44], PCB routing [6], as well as\ninfrastructure layout (roads, pipelines) [8, 37].",
            "content": "Visual Diffusion Models are Geometric Solvers Figure 5: Example of Steiner Minimal Tree. Left: The input terminal nodes, colored in red. Right: The Steiner Minimal Tree for this instance, where auxiliary Steiner points are colored in dark gray. Problem statement. Formally, given finite set of terminals ùëÉ = {ùëù1, . . . , ùëùùëõ } R2, the Euclidean Steiner Tree (EST) problem asks for straight-line embedded tree ùëá = (ùëâ , ùê∏) with ùëÉ ùëâ that minimizes total Euclidean length ùêø(ùëá ) = (cid:205)ùë¢ùë£ ùê∏ ùë¢ ùë£ 2. Vertices in ùëâ ùëÉ are Steiner points. An optimal solution is called Steiner minimal tree (SMT) and its length is denoted ùêø(ùëÉ) [18]. Figure 5 illustrates an instance of the problem and its optimal solution. Complexity and Approximability. For general ùëõ (the number of initial given points), the EST problem is NP-hard already in the plane [17]. However, there is polynomial-time approximation scheme (PTAS) for Euclidean Steiner tree in fixed dimensions: for any fixed ùúÄ > 0 one can compute (1 + ùúÄ)-approximate tree in time polynomial in ùëõ (for fixed ùúÄ and dimension). Structure and Properties. SMTs in the plane are highly structured: (i) no two edges cross; (ii) each Steiner point has degree exactly 3 and the three incident edges meet at 120; (iii) all angles in the tree are at least 120 (at terminals that have degree > 1); (iv) the number of Steiner points is at most ùëõ 2; and (v) all Steiner points lie in the convex hull of ùëÉ [3, 22]. These properties provide strong geometric constraints that are exploited by both exact and approximate methods, as discussed next. Algorithmic Approaches. Exact algorithms typically rely on generating locally optimal full Steiner trees and then selecting minimumlength subset, with implementations such as GeoSteiner solving large 2D instances [23, 46]. On the approximation side, PTASes based on dissection or guillotine methods provide near-optimal guarantees in fixed dimensions [2, 30]. Learning-based Approaches. While classical exact/approximate algorithms dominate practice, there have been several attempts to use learning-based solvers for the problem. For the Euclidean case, Wang et al. propose Deep-Steiner, which casts SMT construction as sequential decision process: it discretizes the continuous search space, prunes candidates via KNN/MST neighborhoods, and then uses an attention-based policy trained with REINFORCE to add Steiner points iteratively [45]. For non-Euclidean variants (rectilinear SMT and graph STP), several learning-based methods have been explored, including RL and GNN-driven solvers and mixed neuralalgorithmic pipelines [1, 10, 24, 28, 29, 32]. Method. We generate synthetic dataset by sampling random points (between 10 and 20 for each instance), and finding their SMT Figure 6: Steiner tree ùë•0 predictions across denoising steps. Each row corresponds to different seed. Columns show selected ùë•0 predictions for decreasing timesteps ùë° from left to right (leftmost: ùë°=ùëá ; rightmost: ùë°=0). Input points are overlaid in red. using the GeoSteiner solver [23]. Each solution is then rasterized into grayscale image with different values for edges, nodes and background. The full details for data generation are found in Section B.3. We employ the same U-Net backbone as in the inscribed square experiment. The conditioning input consists of the rasterized terminal points, concatenated as an additional channel to the noisy input ùë•ùë° at each denoising step. Recovering graph structure from the generated image is performed in two stages. We begin by node detection, where we binarize the output with threshold and detect centers of connected components. The centroid of each blob is then taken as node position, with nodes falling within small radius of an input terminal being snapped to that terminals location. In the second stage, we extract edges by considering the complete graph over the detected nodes. For each candidate edge, we compute the fraction of pixels along the straight line segment that are marked as foreground. If this fraction exceeds threshold (70% in our implementation), the edge is retained. If two vertices are very close to each other, we assume they are connected via an edge. In cases of ambiguity where multiple potential edges with shared node overlap, we retain the shortest one and discard the rest. Evaluation. We evaluate our trained model on test set containing instances with 10-20 input points, matching the number of points seen during training, and four other test sets containing 1120, 21-30, 31-40 and 41-50 input points. After extracting the graph from the generated solution, we check the validity of the solution by verifying that the resulting graph is tree and that it contains all of the input points . If the solution is valid, we then measure the total Euclidean length of the tree. For each instance, we generate in parallel 10 solutions from different noise seeds and select the one with minimal total edge length that is also valid. In Table 2 we report for each test set the rate of valid solutions as well as the mean ratio between the total Euclidean length of the best solution produced by our model compared to that of the optimal solution (ùêø(ùëÉ)). For comparison, we also report the ratio between the total Euclidean length of random planar tree and the optimal solution Goren and Yehezkel et al. Figure 7: Optimal solutions (left) vs. our models solutions (middle) and the difference between them (right). Input points are overlaid over both optimal and produced solutions as red circles. Table 2: Steiner Tree Evaluation Results. Comparison of our method, MST, and random solutions. Reported are valid tree rates and mean Euclidean length ratios ( std) relative to the optimal solution across input point ranges. instances with up to 50 points [21]. Heuristic approaches are commonly applied to larger instances, including constrained triangulations [25], simulated annealing [19, 25], and divide-and-conquer strategies for very large point sets of up to 1,000,000 points [7, 19]. #Input Points Valid Rate Ours Ratio Mean Std MST Ratio Mean Std Random Ratio Mean Std 1020 0.996 1.0008 0.0005 2130 0.986 1.0018 0.0011 3140 0.834 1.0044 0.0035 4150 0.334 1.0092 0.0055 1.0363 0.0124 1.0416 0.0095 1.0470 0.0079 1.0522 0.0072 1.8344 0.2363 1.9044 0.1827 1.8981 0.1656 1.8605 0. and that of the solution produced by the minimum spanning tree of the full graph and ùêø(ùëÉ). Our model is able to successfully produce high quality solutions even for instances with markedly more input points than were seen during training, and often produces solutions that align with the optimal ones (see Figure 7). While there is generally one-to-one coupling between an input and the optimal solution, for some instances the model still produces variations, often of similar quality, for different noise initializations (see Figure 6). However, some of these variations can happen to be invalid, especially for instances with large number of input points. This is evident in the third row, where the solution produced by the noise initialization contains loop and is not tree."
        },
        {
            "title": "5 Maximum Area Polygon Problem\nThe third problem we attempt to tackle with our approach is the\nMaximum Area Polygonization Problem (MAXAP), a well-established\nproblem in computational geometry. Given a set of vertices in the\nplane, the problem asks to find a simple polygon (a polygon that\ndoes not intersect itself and has no holes) that passes through all\nthe vertices and has the largest possible area.",
            "content": "MAXAP is known to be NP-complete [14, 15] and difficult to solve both in theory and in practice [16], with no known algorithm that provides better than 1 2 -approximation factor in polynomial time. Furthermore, deciding whether there exists simple polygon that contains strictly more than 2/3 of the area of the convex hull is also NP-complete [14]. Exact approaches based on integer programming are able to solve instances with up to 25 points [16], while recent mixed-integer-programming approach is able to solve Method. To address the maximum area polygonization problem, we adopt the same visual diffusion architecture as in the previous two tasks, using an identical U-Net backbone. As with the previous methods, we generate synthetic dataset of examples to train the model on. For each training example we sample input points randomly on the grid, and compute their optimal polygonizations through exhaustive search over all valid simple polygons, which is feasible at this scale using DFS procedure. Each polygon is rasterized to an image, while the input points are rasterized into separate image that is concatenated as an additional conditioning channel. The data generation procedure is described in full in Section B.4 At inference time, we recover the polygon structure from the generated image by testing candidate edges between all point pairs. Each edge is retained if more than 70% of its pixels align with foreground edge pixels in the output. The resulting set of edges is then validated to ensure that no intersections occur, with mild tolerance for nearly parallel overlaps. Finally, we search for simple cycle that passes through all input vertices, which we consider the recovered polygonization produced by our model. Evaluation. We evaluate the trained model on test set containing 712 points, matching the range used during training. To further assess generalization, we also test the model on set with 1315 points . For each instance, we generate 10 candidate solutions from different random seeds and select the one that achieves the largest area while remaining valid. In Table 3, we report the mean and standard deviation of the ratio between the best solution found for each instance and the corresponding optimal solution, and show comparison against random simple polygons on the same point sets. We also report the rate of valid polygons (the proportion of instances for which valid polygon was produced), as well as the frequency with which the recovered polygon coincides exactly with the optimal one. Figure 8 shows qualitative examples of polygons produced by our model. In many cases, the generated polygons align almost perfectly with the optimal ones. When discrepancies occur, the Visual Diffusion Models are Geometric Solvers Figure 8: Qualitative examples of maximum area polygons (left) vs. polygons produced by our model (middle) and the difference between them (right). Areas depicted in red and blue in the difference map correspond to regions that are exclusive to the optimal solution and our solution, respectively. It can be noticed that even in cases where there is disparity between the optimal solution and the one produced by the model, the area difference between the exclusive regions tends to be small, amounting to solution of similar quality. Input points are overlaid over both optimal and produced solutions as red circles. Table 3: Maximum Area Polygon Evaluation Results. Comparison of our method, random polygons, and optimal solutions. Metrics include polygon validity rate, mean area ratio ( std), and optimal solution rate for different input point ranges. #Input Points Valid Rate Ours Ratio Mean Std Random Polygon Ratio Mean Std Opt. Solutions Rate 7-12 0.953 0.9887 0.0205 13-15 0.620 0.9624 0.0418 0.7711 0.1361 0.4779 0.2717 0.574 0."
        },
        {
            "title": "6 Discussion and Conclusions\nIn this work, we presented visual diffusion as a general framework\nfor approximating solutions to notoriously hard geometric prob-\nlems. Through three case studies, the Inscribed Square problem, the\nSteiner Tree Problem, and the Simple Polygon Problem, we demon-\nstrated that diffusion models can operate in image space to uncover\nvalid geometric structures.",
            "content": "We do not claim that our method outperforms specialized solvers tailored to any single problem. Indeed, for each of these problems, carefully designed algorithms may yield more efficient or more accurate solutions. Instead, our contribution is to reveal paradigm: visual diffusion provides single, simple framework that applies across diverse set of problems without requiring custom formulations. Specifically, each task uses the very same diffusion architecture without modification, varying only in task-specific training data. Our approach produces accurate and diverse approximations, naturally recovering multiple valid solutions through diffusion, as illustrated in the Inscribed Square problem. These solutions can be further refined if desired. Importantly, we also observe that models trained on relatively simple instances generalize to more complex inputs, such as handling larger number of points than those seen in training. This behavior is particularly valuable for problems where complexity grows with the number of points. This contrasts with traditional geometric solvers, whose runtime typically grows polynomially or even exponentially with input size. Despite the diversity of the problems they are trained to solve, the models exhibit consistent behavior, evident in the denoising Figure 9: Maximum area polygon ùë•0 predictions across denoising steps. Each row corresponds to different seed. Columns show selected ùë•0 predictions for decreasing timesteps ùë° from left to right (leftmost: ùë°=ùëá ; rightmost: ùë°=0). Input points are overlayed in red. differences often balance out, with areas lost in one region largely compensated elsewhere (see instances 9, 10 in the figure). Owing to the non-local nature of the problem, instances with larger number of points are substantially more challenging, and the rate of valid solutions drops noticeably for the 1315 point test set. Typical failure cases include polygons that do not pass through all input points or that contain holes (see the third and fourth rows in Figure 9). Nevertheless, whenever valid polygon is produced, its area is typically very close to that of the optimal solution. Figure We note that for this problem, like the last one, there is generally only single optimal solution per problem instance. Therefore the benefit of training conditional diffusion models which are generally used to learn conditional distributions in order to solve these instances is not immediately clear. In Section we demonstrate on the MAXAP problem that there is performance advantage over using regression model even for problems of this kind. progression (Figures 3, 6 and 9). Already in the early steps of the sampling process, the global structure of the solution becomes apparent, suggesting that the essence of the solution lies primarily in low-frequency geometric features that can be recovered quickly. The subsequent denoising steps refine these structures to achieve high accuracy. This observation indicates that inference time could be further optimized, with only small trade-off in precision, by using denoising schedulers that allocate more of the sampling steps budget to earlier timesteps. More broadly, this progressive reasoning mirrors how humans intuitively reason about geometric problems: they first sketch mental image of coarse solution in their mind, which they can then try to translate to concrete solution, with details settled upon after the initial structure is clear. The key message of this work is that image diffusion models, long celebrated for their generative capacity, also serve as geometric solvers. This outlook opens the door to exploring wide spectrum of geometric challenges under single methodology, and suggests new opportunities for bridging generative modeling with geometric problem solving. References [1] Reyan Ahmed, Md Asadullah Turja, Faryad Darabi Sahneh, Mithun Ghosh, Keaton Hamm, and Stephen Kobourov. Computing steiner trees using graph neural networks, 2021. URL https://arxiv.org/abs/2108.08368. [2] Sanjeev Arora. Polynomial-time approximation schemes for Euclidean TSP and other geometric problems. Journal of the ACM, 45(5):753782, 1998. doi: 10.1145/290179.290180. [3] Marcus Brazil, Pawel Winter, and Martin Zachariasen. Flexibility of steiner trees in uniform orientation metrics. pp. 196208, 12 2004. ISBN 978-3-540-24131-7. doi: 10.1007/978-3-540-30551-4_19. Jason Cantarella, Elizabeth Denne, and John McCleary. Transversality for configuration spaces and the \"square-peg\" theorem, 2021. URL https://arxiv.org/abs/ 1402.6174. [4] [5] Bernard Chazelle. The Polygon Containment Problem. Advances in Computing Research, 1(1):133, 1983. URL https://www.cs.princeton.edu/chazelle/pubs/ PolygContainmentProb.pdf. [6] Chris Chu and Yiu-Chung Wong. Flute: Fast lookup table based rectilinear steiner minimal tree algorithm for VLSI design. IEEE Transactions on ComputerAided Design of Integrated Circuits and Systems, 27(1):7083, 2008. URL https: //www.engineering.iastate.edu/cnchu/pubs/j29.pdf. [7] Lo√Øc Crombez, Guilherme D. da Fonseca, and Yan Gerard. Greedy and local search heuristics to build area-optimal polygons. ACM J. Exp. Algorithmics, 27, 2022. ISSN 1084-6654. doi: 10.1145/3503999. URL https://doi.org/10.1145/3503999. [8] Ziyuan Cui, Hai Lin, Yan Wu, Yufei Wang, and Xiao Feng. Optimization of pipeline network layout for multiple heat sources distributed energy systems considering reliability evaluation. Processes, 9(8):1308, 2021. doi: 10.3390/pr9081308. URL https://www.mdpi.com/2227-9717/9/8/1308. [9] Erik D. Demaine, S√°ndor P. Fekete, Phillip Keldenich, Dominik Krupke, and Joseph S. B. Mitchell. Area-optimal simple polygonalizations: The cg challenge 2019. ACM J. Exp. Algorithmics, 27, March 2022. ISSN 1084-6654. doi: 10.1145/ 3504000. URL https://doi.org/10.1145/3504000. [10] Haizhou Du, Zong Yan, Qiao Xiang, and Qinqing Zhan. Vulcan: Solving the steiner tree problem with graph neural networks and deep reinforcement learning. arXiv, 2021. [11] Arnold Emch. Some properties of closed convex curves in plane. American Journal of Mathematics, 35(4):407412, 1913. ISSN 00029327, 10806377. URL http://www.jstor.org/stable/2370404. [12] Arnold Emch. On the medians of closed convex polygon. American Journal of Mathematics, 37(1):1928, 1915. ISSN 00029327, 10806377. URL http://www.jstor. org/stable/2370252. [13] Arnold Emch. On some properties of the medians of closed continuous curves formed by analytic arcs. American Journal of Mathematics, 38(1):618, 1916. ISSN 00029327, 10806377. URL http://www.jstor.org/stable/2370541. [14] S√°ndor Fekete. Geometry and the travelling salesman problem. University of Waterloo, 1992. [15] S√°ndor Fekete and William Pulleyblank. Area optimization of simple polygons. In Proceedings of the ninth annual symposium on computational geometry, pp. 173182, 1993. [16] S√°ndor P. Fekete, Andreas Haas, Phillip Keldenich, Michael Perk, and Arne Schmidt. Computing area-optimal simple polygonizations, 2021. URL https: Goren and Yehezkel et al. //arxiv.org/abs/2111.05386. [17] M. R. Garey, R. L. Graham, and D. S. Johnson. The complexity of computing steiner minimal trees. SIAM Journal on Applied Mathematics, 32(4):835859, 1977. ISSN 00361399. URL http://www.jstor.org/stable/2100193. [18] Edgar N. Gilbert and Henry O. Pollak. Steiner minimal trees. Siam Journal on Applied Mathematics, 16:129, 1968. URL https://api.semanticscholar.org/ CorpusID:123196263. [19] Nir Goren, Efi Fogel, and Dan Halperin. Area optimal polygonization using simulated annealing. ACM J. Exp. Algorithmics, 27, 2022. ISSN 1084-6654. doi: 10.1145/3500911. URL https://doi.org/10.1145/3500911. [20] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors, 2023. URL https://arxiv.org/abs/2206.09012. [21] Hip√≥lito Hern√°ndez-P√©rez, Jorge Riera-Ledesma, Inmaculada Rodr√≠guez-Mart√≠n, and Juan-Jos√© Salazar-Gonz√°lez. Optimal area polygonisation problems: Mixed integer linear programming models. European Journal of Operational Research, 2025. ISSN 0377-2217. doi: 10.1016/j.ejor.2025.08.023. URL https: //www.sciencedirect.com/science/article/pii/S0377221725006393. [22] F. K. Hwang and Dana S. Richards. Steiner tree problems. Networks, 22(1):5589, 1992. doi: https://doi.org/10.1002/net.3230220105. URL https://onlinelibrary. wiley.com/doi/abs/10.1002/net.3230220105. [23] D. Juhl, D. M. Warme, P. Winter, and M. Zachariasen. The GeoSteiner software package for computing steiner trees in the plane: An updated computational study. Mathematical Programming Computation, 10(4):487532, 2018. doi: 10. 1007/s12532-018-0135-8. [24] Andrew B. Kahng, Robert R. Nerem, Yusu Wang, and Chien-Yi Yang. Nn-steiner: mixed neural-algorithmic approach for the rectilinear steiner minimum tree problem, 2023. URL https://arxiv.org/abs/2312.10589. Julien Lepagnot, Laurent Moalic, and Dominique Schmitt. Optimal area polygonization by triangulation and visibility search. ACM J. Exp. Algorithmics, 27, 2023. ISSN 1084-6654. doi: 10.1145/3503953. URL https://doi.org/10.1145/3503953. [25] [26] Yang Li, Jinpei Guo, Runzhong Wang, and Junchi Yan. T2t: From distribution learning in training to gradient search in testing for combinatorial optimization. In Advances in Neural Information Processing Systems, 2023. [27] Yang Li, Jinpei Guo, Runzhong Wang, Hongyuan Zha, and Junchi Yan. Fast t2t: Optimization consistency speeds up diffusion-based training-to-testing solving for combinatorial optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id= xDrKZOZEOc. Jinwei Liu, Guojie Chen, and Evangeline F. Young. Rest: Constructing rectilinear steiner minimum tree via reinforcement learning. In Proc. DAC, pp. 11351140, 2021. doi: 10.1109/DAC18074.2021.9586209. [28] [30] [29] Ruizhi Liu, Zhisheng Zeng, Shizhe Ding, Jingyan Sui, Xingquan Li, and Dongbo Bu. Neuralsteiner: Learning steiner tree for overflow-avoiding global routing in chip design. In Proc. NeurIPS, 2024. Joseph S. B. Mitchell. Guillotine subdivisions approximate polygonal subdivisions: simple PTAS for geometric TSP, ùëò-MST, and related problems. SIAM Journal on Computing, 28(4):12981309, 1999. doi: 10.1137/S0097539796309764. Joseph ORourke. Computational Geometry in C. Cambridge University Press, 2 edition, 1998. Section 1.3: Area of Polygon. [31] [32] Youngjoon Park, Han-Seul Jeong, Kyunghyun Lee, Sungdong Yoo, Hyungseok Song, and Woohyung Lim. Steben: Steiner tree problem benchmark for neural combinatorial optimization on graphs, 2025. URL https://openreview.net/forum? id=tKif2rXQ6V. [33] Ville H. Pettersson, Helge A. Tverberg, and Patric R.J. √ñsterg√•rd. note on toeplitz conjecture. Discrete & Computational Geometry, 51(3):722728, 2014. doi: 10.1007/s00454-014-9578-5. [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/abs/2112.10752. [35] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. URL https://arxiv.org/abs/ 1505.04597. [37] [36] Sebastian Sanokowski, Sepp Hochreiter, and Sebastian Lehner. diffusion model framework for unsupervised neural combinatorial optimization, 2025. URL https://arxiv.org/abs/2406.01661. Justus Schwartz and J√ºrg St√ºckelberger. Computing lower bounds for steiner trees in road network design. In Proceedings of the 7th International Symposium on Operations Research and Its Applications (ISORA08), pp. 172181, Lijiang, China, 2008. ORSC & APORC. URL https://www.aporc.org/LNOR/8/ISORA2008F22.pdf. [38] Micha Sharir and Sivan Toledo. External polygon containment problems. Computational Geometry, 4(2):99118, 1994. ISSN 0925-7721. doi: https://doi.org/10. 1016/0925-7721(94)90011-6. URL https://www.sciencedirect.com/science/article/ pii/0925772194900116. [39] Walter Stromquist. Inscribed squares and square-like quadrilaterals in closed curves. Mathematika, 36(2):187197, 1989. doi: https://doi.org/10.1112/ S0025579300013061. URL https://londmathsoc.onlinelibrary.wiley.com/doi/abs/ 10.1112/S0025579300013061. Visual Diffusion Models are Geometric Solvers [40] Zhiqing Sun and Yiming Yang. Difusco: Graph-based diffusion solvers for combinatorial optimization, 2023. URL https://arxiv.org/abs/2302.08224. [41] Terence Tao. An integration approach to the toeplitz square peg problem, 2017. URL https://arxiv.org/abs/1611.07441. [42] Wouter van Heijst. The algebraic square peg problem, 2014. URL https://arxiv. org/abs/1403.5979. [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762. [44] Stefan Voss. Steiner Tree Problems in Telecommunications, pp. 459492. 01 2006. ISBN 9780387306629. doi: 10.1007/978-0-387-30165-5_18. [45] Siqi Wang, Yifan Wang, and Guangmo Tong. Deep-steiner: Learning to solve the euclidean steiner tree problem, 2022. URL https://arxiv.org/abs/2209.09983. [46] D. M. Warme, P. Winter, and M. Zachariasen. Exact Algorithms for Plane Steiner Tree Problems: Computational Study, pp. 81116. Springer US, Boston, MA, 2000. ISBN 978-1-4757-3171-2. doi: 10.1007/978-1-4757-3171-2_6. URL https: //doi.org/10.1007/978-1-4757-3171-2_6. [47] Christopher Wewer, Bart Pogodzinski, Bernt Schiele, and Jan Eric Lenssen. Spatial reasoning with denoising models, 2025. URL https://arxiv.org/abs/2502.21075. Appendix Regression Model Ablation For certain geometric problems, each input instance admits unique optimal solution. In such cases, one may wonder whether training diffusion model is necessary, since the task appears to reduce to learning deterministic mapping. Indeed, the conditional distribution to be learned degenerates into collection of Dirac delta functions. To examine this, we compared our diffusion framework with direct regression baseline. For the Maximum Area Polygonization problem, we trained regression model with the same U-Net backbone as our diffusion model, using identical training data and budget. The regression model outputs polygon image in single forward pass, conditioned only on the rasterized input points. While the regression model succeeds on simpler instances, it often produces polygons with blurry edges or missing segments for more complex cases. This degrades our polygon extraction stage, which frequently fails to recover valid polygon from such outputs. The quantitative results, shown in the Regression Valid Polygons Rate column of Table 4, confirm this limitation. In contrast, the diffusion model retains key advantage: stochasticity. By conditioning on both the input points and noise vector, we can generate multiple candidate solutions and resample until valid polygon is obtained. This property directly improves the robustness of the approach, and highlights why diffusion remains beneficial even in problems that might superficially appear deterministic. This ablation study demonstrates our key finding: diffusion models offer versatile and robust solution framework. This framework can also surpass the capabilities of deterministic regression approaches, even in scenarios where the underlying function maintains one-to-one correspondence. Implementation Details B.1 Curve Generation For harmonic-based curves, we construct random radial profile ùëü (ùúÉ ) = 1 + ùêª ‚Ñé=1 ùúå‚Ñé sin(‚ÑéùúÉ + ùúô‚Ñé), with ùêª sampled uniformly from [ùêªmin, ùêªmax] and amplitudes ùúå‚Ñé drawn from decaying envelope to produce smooth perturbations. Square vertices are first placed in Cartesian coordinates, then converted to polar coordinates (ùúÉùëñ, ùëüùëñ ). periodic cubic spline is fit to the radius corrections ùëüùëñ ùëü (ùúÉùëñ ), ensuring that the resulting contour passes exactly through all square vertices. To guarantee validity, we enforce periodicity in the spline domain and regenerate until non-self-intersecting (Jordan) curve is obtained. random global translation is then applied, and both the curve and its inscribed squares are normalized to fit inside [1, 1]2 before rasterization. In practice, we used the following parameter ranges: ùêª [6, 30], 500 angular samples, square side lengths sampled from [0.3, 0.7], rotations from [0, 2ùúã], and global translations up to 0.5 units. Each curve contains between 1 and 5 inscribed squares. As an additional augmentation, with probability 0.1 we replace the harmonic-based curve with perfect circle of random radius. Since circle trivially admits infinitely many inscribed squares Goren and Yehezkel et al. Table 4: Regression Model Evaluation Results. Comparison of diffusion (best-of-10) and regression models. Reported are valid polygon rates and mean area ratios ( std) across input point ranges. #Input Points 7-12 13-"
        },
        {
            "title": "Regression\nValid\nRate",
            "content": "Diffusion Ratio Mean Std Regression Ratio Mean Std 0.953 0.620 0.361 0.016 0.9887 0.0205 0.9624 0.0418 0.9994 0.0025 0.9988 0. (obtained by rotation), we sample small number of representative ones to enrich the dataset. The samples are finally rasterized into 128 128 binary images, with curves as one-pixel-wide strokes and squares as filled shapes. In total, the dataset contains 100,000 examples. B.2 Square Enhancement To extract the initial square (cid:98)ùëÜ from the predicted mask, we fit contour-aligned minimum-area rectangle and take its four vertices as ùëâ ((cid:98)ùëÜ) = {ùëùùëñ }4 ùëñ=1. For candidate rigid transform (ùëÖùúÉ , ùë°) with ùúÉ in radians and ùë° R2, we form the transformed square ùëÜ (ùúÉ, ùë°) = ùëÖùúÉ (cid:98)ùëÜ + ùë°, with vertices ùëâ (ùëÜ (ùúÉ, ùë°)) = {ùëûùëñ (ùúÉ, ùë°)}4 ùëñ=1, where ùëûùëñ (ùúÉ, ùë°) = ùëÖùúÉ ùëùùëñ + ùë°. We then evaluate the alignment score with the curve ùê∂ as defined in Eq. 1. The final snapped square is obtained by selecting the rigid transform that maximizes this score: (ùúÉ , ùë° ) = arg max ùúÉ,ùë° (ùëÜ (ùúÉ, ùë°), ùê∂), ùëÜ = ùëÖùúÉ (cid:98)ùëÜ + ùë° . We approximate this maximization with discrete grid search. Specifically, we sample ùúÉ [ùúÉmin, ùúÉmax] with step ŒîùúÉ , and translations ùë° = (Œîùë•, Œîùë¶) with Œîùë•, Œîùë¶ {ùëá , . . . ,ùëá } in steps of one pixel. For each candidate (ùúÉ, ùë°), the square mask is rigidly warped, its corners recomputed, and (ùëÜ (ùúÉ, ùë°), ùê∂) evaluated. B.3 Steiner Tree Generation Generation of synthetic instance begins by sampling ùëõ terminal nodes within the unit square, with ùëõ drawn uniformly from [10, 20]. To prevent rasterization artifacts, we enforce minimum separation between terminals with rejection sampling. For each sampled configuration, we compute the Steiner Minimal Tree using the GeoSteiner solver [23]. The resulting solutions are rasterized into grayscale images of fixed resolution (128 128), where terminals and Steiner points are depicted as small filled black circles with radius of 2 pixels and edges as thin white lines that are 2 pixels wide, while the background is gray. The final dataset contains 1,000,000 instances. B.4 Maximum Area Polygon Generation For MAXAP instance generation, we first sample ùëõ points within the unit square, with ùëõ drawn uniformly from [7, 12]. Also here we enforce minimum separation between points with rejection sampling. For each set of point, we exhaustively go over all valid Visual Diffusion Models are Geometric Solvers polygon configurations and find the one with the largest area. We employ backtracking depth-first search to systematically explore all valid simple polygons formed by given point set. The method fixes an anchor point at the bottommost-leftmost position to eliminate rotational symmetry, then incrementally constructs polygons by selecting vertices in angular order around the centroid. At each step, the algorithm prunes invalid branches by rejecting vertices that would create edge intersections with the existing partial polygon. When complete polygon is formed, the closing edge is validated for intersections, and the polygon area is computed using the shoelace formula [31]. The search maintains the globally optimal solution by comparing areas and updating the best configuration found. This approach guarantees finding the maximum area simple polygon while significantly reducing the exponential search space through geometric pruning, making the ùëÇ (ùëõ!) worst-case complexity manageable and runtime that is fast in practice for small point sets (ùëõ 15). Finally, the polygon is rasterized into grayscale images of fixed resolution (128 128), where the polygon edges are drawn as white lines that are 1 pixel wide, the polygon interior is black and the background is gray. In total the dataset contains 1,000,000 instances. B.5 Model Architecture Our approach employs conditional diffusion model based on U-Net architecture for generating geometric solutions. The U-Net consists of 4 encoder and decoder levels with base channel count of 64, following standard channel progression of 64 128 256 512 in the encoder path. The model takes 2-channel input (noisy target and condition images) and produces single-channel denoised prediction. Multi-head self-attention with 8 heads is integrated at the bottleneck and at encoder/decoder levels 2 and 3. The attention mechanism uses GroupNorm with 32 groups. Each level incorporates residual blocks with two 3 3 convolutional layers, BatchNorm, and ReLU activations, along with time embedding injection through learned linear projections. Sinusoidal time embeddings with 128 dimensions condition the model on the diffusion timestep. B.6 Training Procedure The model is trained using the DDIM (Denoising Diffusion Implicit Models) framework with 100 diffusion steps, linear beta schedule and deterministic sampling (ùúÇ = 0.0). We employ an ùêø2 loss on noise prediction, where the model learns to predict the noise ùúñ added to the clean image at timestep ùë°: = Eùë°,ùúñN (0,ùêº ) (cid:2)ùúñ ùúñùúÉ (ùë•ùë°, ùë°, ùëê) 2 2 (cid:3) . (3) Training is performed using the AdamW optimizer with learning rate of 6 104 and cosine annealing with warm restarts over 0.5 cycles, including 100 warm-up steps and minimum learning rate factor of 0.1. We use gradient accumulation over 8 steps with gradient clipping at maximum norm of 1.0, and batch size of 128 per GPU. The training employs mixed precision with bfloat16 autocast for efficiency and is distributed across 4 NVIDIA GTX 3090 GPUs for 100 epochs."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Tel Aviv University"
    ]
}