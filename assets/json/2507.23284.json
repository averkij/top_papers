{
    "paper_title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval",
    "authors": [
        "Dohwan Ko",
        "Ji Soo Lee",
        "Minhyuk Choi",
        "Zihang Meng",
        "Hyunwoo J. Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-Video Retrieval aims to find the most relevant text (or video) candidate given a video (or text) query from large-scale online databases. Recent work leverages multi-modal large language models (MLLMs) to improve retrieval, especially for long or complex query-candidate pairs. However, we observe that the naive application of MLLMs, i.e., retrieval based on candidate likelihood, introduces candidate prior bias, favoring candidates with inherently higher priors over those more relevant to the query. To this end, we propose a novel retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM), which leverages both query and candidate likelihoods by training the model to generate text from a given video as well as video features from a given text. Furthermore, we introduce Candidate Prior Normalization (CPN), a simple yet effective training-free score calibration module designed to mitigate candidate prior bias in candidate likelihood. On four Text-Video Retrieval benchmarks, our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4 R@1 on average, effectively alleviating candidate prior bias and emphasizing query-candidate relevance. Our in-depth analysis across various multi-modal tasks beyond retrieval highlights the broad applicability of CPN which enhances visual understanding by reducing reliance on textual priors. Code is available at https://github.com/mlvlab/BLiM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 4 8 2 3 2 . 7 0 5 2 : r Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval Dohwan Ko1 Ji Soo Lee1 Minhyuk Choi1 Zihang Meng2 Hyunwoo J. Kim3 1Korea University 2Meta GenAI 3KAIST {ikodoh, simplewhite9, sodlqnf123}@korea.ac.kr zihang@meta.com hyunwoojkim@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Text-Video Retrieval aims to find the most relevant text (or video) candidate given video (or text) query from largescale online databases. Recent work leverages multi-modal large language models (MLLMs) to improve retrieval, especially for long or complex query-candidate pairs. However, we observe that the naive application of MLLMs, i.e., retrieval based on candidate likelihood, introduces candidate prior bias, favoring candidates with inherently higher priors over those more relevant to the query. To this end, we propose novel retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM), which leverages both query and candidate likelihoods by training the model to generate text from given video as well as video features from given text. Furthermore, we introduce Candidate Prior Normalization (CPN), simple yet effective training-free score calibration module designed to mitigate candidate prior bias in candidate likelihood. On four TextVideo Retrieval benchmarks, our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4 R@1 on average, effectively alleviating candidate prior bias and emphasizing query-candidate relevance. Our in-depth analysis across various multi-modal tasks beyond retrieval highlights the broad applicability of CPN which enhances visual understanding by reducing reliance on textual priors. Code is available at https://github.com/mlvlab/BLiM. 1. Introduction Text-Video Retrieval [15] aims to retrieve the most relevant text (or video) candidate given video (or text) query. To scale retrieval systems, previous works [6, 7] have primarily adopted dual-encoder architectures, leveraging encoder models such as BERT [8] and CLIP [9]. These models encode each query and candidate separately into Equal contribution. Corresponding authors. single embeddings, enabling efficient retrieval via similarity between two embeddings. While computationally efficient, its reliance on shallow similarity-based interactions restricts token-level alignment between queries and candidates, often leading to suboptimal retrieval performance. To overcome this limitation, multi-modal large language models (MLLMs)-based [1018] retrieval systems have been recently introduced [1921]. Unlike dualencoders, MLLM-based retrievers process concatenated query-candidate pairs, enabling deep token-level interactions, resulting in superior retrieval performance, particularly for long and complex query-candidate pairs. However, we observe that naively maximizing candidate likelihood leads to candidate prior bias, where candidates with higher prior probabilities are favored over those truly relevant to the query. For instance, in Fig. 1b, given video query and text candidates in video-to-text retrieval, an MLLM retriever based on candidate likelihood (tv) tends to prioritize text candidates with frequently occurring patterns over those that are more semantically aligned with the video query. In this example, such bias arises because MLLMs, due to their autoregressive nature, inherently assign higher probabilities to long and repetitive text, overlooking the actual content of the video query [22]. This prior bias is also prevalent in other multi-modal tasks, including visual question answering and captioning, where models tend to rely more on textual content than visual information when generating text responses [2326]. Similarly, in text-to-video retrieval, MLLMs often favor videos with static scenes over those exhibiting dynamic transitions. To address candidate prior bias in MLLM-based retrieval systems, we propose novel framework, Bidirectional Likelihood Estimation with MLLM (BLiM), which considers query likelihood as well as candidate likelihood. Specifically, BLiM aims to generate text from given video (P (tv)) and video features from given text (P (vt)). During inference, as in Fig. 1, jointly considering both likelihoods allows BLiM to mitigate candidate prior bias 1 Figure 1. (a) provides an overview of BLiM for video-to-text retrieval, which leverages the bidirectional likelihood estimation with query and candidate likelihoods to mitigate candidate prior bias. (b) Candidate likelihood estimation tends to prioritize long and repetitive text with high prior probability. In contrast, bidirectional likelihood estimation of BLiM, effectively selects the most relevant text. by focusing on the semantic relevance between the query and candidate. Additionally, we introduce Candidate Prior Normalization (CPN), simple yet effective training-free score calibration module to reduce candidate prior bias in candidate likelihood estimation. Equipped with CPN, BLiM achieves state-of-the-art performance by remarkable margin on four popular Text-Video Retrieval benchmark datasets: DiDeMo [5], ActivityNet [4], LSMDC [27], and MSRVTT [3]. Furthermore, CPN enhances performance across various multi-modal tasks beyond retrieval by improving visual understanding through reduced reliance on textual priors, underscoring its broad applicability. To sum up, our contributions are as follows: To the best of our knowledge, within the context of MLLMs for Text-Video Retrieval, this paper is the first to study the candidate prior bias in candidate likelihood. We propose BLiM, novel MLLM-based retrieval system trained to generate text from video and video features from text, enabling bidirectional likelihood estimation. We also present simple yet effective score calibration module, CPN, which further reduces the candidate prior bias in candidate likelihood estimation. Our BLiM, equipped with CPN, outperforms previous state-of-the-art models by an average margin of 6.4 in R@1, effectively alleviating candidate prior bias and emphasizing the relevance between the query and candidate. = arg max = arg max (t(n)v) (cid:125) (cid:123)(cid:122) (cid:124) candidate likelihood (vt(n)) (cid:124) (cid:125) (cid:123)(cid:122) query likelihood (t(n)) (cid:124) (cid:123)(cid:122) (cid:125) candidate prior = arg max (vt(n))P (t(n)) (v) . (1) In Eq. (1), the retrieval process is influenced by both the query likelihood (vt(n)) and the candidate prior (t(n)). Ideally, retrieval should primarily rely on query likelihood to ensure semantic relevance between the query and the candidate. However, since the candidate prior is independent of the query, it may introduce bias by prioritizing text candidates with frequently occurring patterns, even when they are less relevant to the given video query. This bias leads to suboptimal retrieval, as in the following proposition: Proposition 1. Let (t(m)v(m)) denote the candidate likelihood for retrieving the most relevant text t(m) given query video v(m). Suppose that: 1. The query likelihood correctly ranks t(m) over any negative sample t(n) and the gap is bounded as: 0 < log (v(m)t(m)) log (v(m)t(n)) < ε. (2) 2. There exists text candidate t(n) with larger prior probability gap: 2. Candidate Prior Bias log (t(n)) log (t(m)) > cε, for some > 1. (3) We first analyze candidate prior bias where retrieval using candidate likelihood of MLLMs heavily depends on the candidate prior probabilities, while ignoring actual querycandidate relevance. The inference procedure of video-totext retrieval using candidate likelihood is as follows: Then, the candidate likelihood ranking is reversed: (t(m)v(m)) < (t(n)v(m)). (4) Proof. See Sec. of the supplement. 2 Figure 2. In video-to-text retrieval, similarities between queries and candidates using (a) candidate likelihood (tv) and (b) query likelihood (vt) are provided. (c) shows the candidate prior (t) for each text. To reduce visual clutter, 50 text-video pairs are sampled. Based on the candidate prior (t), the 24th text (highlighted in red) exhibits the highest prior probability in (c). While the query videos correctly retrieve their corresponding text using query likelihood (vt) in (b), as indicated by the high similarity in diagonal elements, the text with the highest prior probability (red box) is frequently retrieved for irrelevant videos (374 out of 1,003) when using candidate likelihood (tv) in (a). Fig. 2 visualizes the impact of candidate prior bias. Interestingly, although query likelihood (Fig. 2b) yields relatively accurate retrieval results, the undesirable influence of candidate prior (Fig. 2c) distorts candidate likelihood estimation (Fig. 2a). Specifically, the 24th text candidate (highlighted in red box) exhibits the highest prior probability among all text candidates. As result, this text is retrieved for 374 out of 1,003 videos (37%), when using candidate likelihood estimation, demonstrating that candidate likelihood is skewed by over-relying on the candidate prior. Moreover, text-to-video retrieval follows similar inference procedure, i.e., = arg maxn (v(n)t) = arg maxn (tv(n))P (v(n)) and we find that candidate prior bias also exists in text-to-video retrieval, where the candidate likelihood (v(n)t) overestimates the candidate prior (v(n)), leading to the retrieval of irrelevant video candidates. Further discussion on candidate prior bias in text-to-video retrieval is provided in Sec. E.1 and G.1 of the supplement. These observations motivate us to consider both directions of likelihood, candidate and query likelihoods, to refine retrieval results by mitigating candidate prior bias in candidate likelihood estimation. 3. Method Based on these observations, in Sec. 3.1, we propose Bidirectional Likelihood Estimation with MLLM (BLiM), novel MLLM-based retrieval framework that incorporates both candidate and query likelihoods for Text-Video Retrieval. Additionally, in Sec. 3.2, we present simple yet effective score calibration module, Candidate Prior Normalization (CPN), to mitigate candidate prior bias in the candidate likelihood estimation. 3.1. Bidirectional Likelihood Estimation of MLLM Unlike standard MLLMs usually trained to maximize (tv), we here propose BLiM, an MLLM that jointly maximizes bidirectional likelihoods (tv) and (vt). The overall architecture of BLiM is depicted in Fig. 3a. Model architecture. BLiM is built upon the pretrained Video MLLM, VideoChat-Flash 7B [10], which consists of three key components: video encoder (UMT [28]), linear projection layer, and an LLM (Qwen2 [29]). Given an input video, it is first segmented into Lv clips, and the video encoder extracts visual features for each clip. These features are then projected into the LLMs embedding space via the linear projection layer, forming visual tokens denoted as = [v1, . . . , vLv ] RLvD, where is the hidden dimension. These visual tokens are then concatenated with Lt text tokens = [t1, . . . , tLv ] RLtD before being fed into the LLM. We update only the linear projection layer and apply LoRA [30] for parameter-efficient fine-tuning. Training procedure. BLiM is trained using bidirectional likelihood maximization objective. The first objective, video-grounded text generation (tv), follows the common pretraining paradigm of MLLMs as: Ltv = log (tv) = Lt(cid:88) i=1 log (tit<i, v) (5) = Softmax(Linear(ti1)), where ti1 RD denotes the LLMs output representation corresponding to the (i1)-th text token. Additionally, we define second objective, textgrounded video feature generation (vt), inspired by [31], as follows: Lvt = log (vt) = Lv(cid:88) i=1 log (viv<i, t) = Lv(cid:88) i= log (cid:1) exp (cid:0)v i1vi (cid:16) i1v(n) n=1 exp (cid:80)N (6) (cid:17) , where is the number of videos in the training set and vi1 RD is the LLMs output representation of vi1 RD. Here, v0 corresponds to the last token of the text sequence, allowing the model to generate video features conditioned on the entire text input. In Eq. (6), the model learns to autoregressively predict the next video clip feature vi given the preceding clips v<i and the text. The probability distribution is computed via softmax function over candidate clips v(1) i1vi determines the likelihood of vi being the correct next clip. This encourages the model to generate temporally coherent and text-consistent video representations. Overall, we train BLiM by maximizing both likelihoods as: , where the similarity score , . . . , v(N ) i LBLiM = Ltv + Lvt. (7) 3 Figure 3. Overall architecture. (a) illustrates BLiM, which jointly maximizes both (tv) and (vt). (b) presents the attention masks used for estimating likelihoods and prior probabilities. To compute prior probabilities, attention masking is applied to all tokens of the input modality while generating the output modality. We note that the input modality order is swapped for each likelihood, as illustrated in Fig. 3a. We use the prompt Describe this video. for Ltv and Generate video given the caption. for Lvt. Inference procedure. During inference, we combine both likelihoods (tv) and (vt) to find the most relevant candidate for given query. Given video query and text candidates in video-to-text retrieval, (tv) and (vt) represent candidate likelihood and query likelihood, respectively, and the inference procedure is as follows: V2T = arg max (t(n)v) (cid:125) (cid:123)(cid:122) (cid:124) candidate likelihood + (vt(n)) (cid:125) (cid:123)(cid:122) (cid:124) query likelihood . (8) On the other hand, in text-to-video retrieval with text query and video candidates, the roles of likelihoods are reversed. Then, the inference procedure is written as: T2V = arg max (tv(n)) (cid:124) (cid:125) (cid:123)(cid:122) query likelihood + (v(n)t) (cid:125) (cid:123)(cid:122) candidate likelihood (cid:124) . (9) In both Eq. (8) and (9), candidate likelihood estimation identifies the candidate that is most likely to be generated by the query. Conversely, query likelihood estimation identifies the candidate that is most likely to generate the query. By jointly considering both likelihoods, BLiM mitigates the bias introduced by candidate prior probabilities, ensuring that the final prediction is based primarily on the semantic alignment between the query and the candidate. Inference details are provided in Sec. of the supplement. Furthermore, to boost the retrieval efficiency, we adopt two-stage retrieval pipeline [15, 28, 3236], which first 4 efficiently retrieves top-K candidates with small retrieval model and then refines the ranking with large reranking model. Specifically, we use InternVideo2 1B [37] to retrieve top-K candidates and rerank their rankings using our BLiM. This approach significantly reduces the inference time complexity from O(N 2) to O(KN ) where , resulting in more efficient inference than traversing all candidates (e.g., 307 times faster on ActivityNet). 3.2. Candidate Prior Normalization To further alleviate candidate prior bias in candidate likelihood, we here introduce training-free score calibration module, CPN. In video-to-text retrieval with text candidates, we aim to calibrate the candidate likelihood (tv) by normalizing the effect of the candidate prior (t) as: (tv) = (vt)P (t) (v) (tv) (t)α = (vt)P (t)1α (v) , (10) where α [0, 1] is hyperparameter which determines normalization strength. Instead of directly using the standard candidate likelihood of (tv) in Eq. (10) (left), we normalize it with the candidate prior (t) in Eq. (10) (right). When α = 0, the likelihood remains unchanged, while larger values of α apply stronger normalization to reduce the effect of the candidate prior. Then, the normalized candidate likelihood α(tv) is defined as: log α(tv) log (tv) (t)α = log (tv) α log (t). (11) DiDeMo ActivityNet LSMDC MSRVTT DiDeMo ActivityNet LSMDC MSRVTT R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Text-to-Video Video-to-Text zero-shot ViCLIP [38] InternVideo [39] VideoCoCa [40] VideoPrism [41] UMT [28] InternVideo2 1B [37] InternVideo2 6B [37] BLiM (Ours) 18.4 31.5 - - - - - - 48.6 72.9 57.0 80.0 57.9 80.0 - - - - 79.0 85.1 84.6 - - 15.1 30.7 34.5 63.2 52.7 79.4 41.9 68.9 60.4 83.9 63.2 85. - - 76.6 - 80.3 90.8 92.5 20.1 17.6 - - - - - - 24.9 41.7 32.0 52.4 33.8 55.9 - - - - 51.8 59.4 62.2 - - 42.4 40.7 34.3 57.8 52.7 77.2 40.7 63.4 51.9 75.3 55.9 78.3 - - 67.0 - 71.8 82.5 85.1 27.9 33.5 - - - - - - 49.9 74.8 54.3 77.2 57.1 79.9 - - - - 81.4 83.5 85. - - 24.0 31.4 33.0 61.6 50.3 77.1 39.4 66.8 54.8 81.5 56.5 82.8 - - 75.3 - 78.3 89.5 90.3 16.9 13.2 - - - - - - 21.9 37.8 27.3 44.2 30.1 47. - - - - 45.7 51.6 54.8 - - 41.3 39.6 33.0 61.6 51.7 75.2 37.1 58.7 50.9 73.4 53.7 77.5 - - 75.3 - 68.9 81.8 84.1 69.8 84.5 87. 71.4 88.3 92.0 40.7 57.3 61.9 57.2 76.7 83. 62.9 83.2 86.3 58.6 83.9 89.5 32.9 50.2 55. 54.1 76.6 84.1 - fine-tuned 42.8 68.5 CLIP4Clip [6] 49.4 ViCLIP [38] 44.3 72.1 MV-Adapter [42] 57.9 82.4 InternVideo [39] 70.4 90.1 UMT [28] Cap4Video [43] 52.0 79.4 InternVideo2 1B [37] 75.3 92.5 InternVideo2 6B [37] 74.2 - 79.2 - 80.5 88.9 93.5 87.5 95.8 - - 40.5 72.4 49.8 42.9 74.5 62.2 85.9 66.8 89.1 - - 68.8 89.7 74. - 83.4 - 85.7 93.2 94.9 - 94.7 - - 21.6 41.8 33.0 23.2 43.9 34.0 53.7 43.0 65.5 - - 44.9 68.6 46.4 - 49.8 - 53.2 62.9 73.0 - 75.5 - - 44.5 71.4 52.5 46.2 73.2 55.2 79.6 58.8 81.0 51.4 75.7 59.4 80.9 62.8 - 81.6 - 82.7 87.5 87.1 83.9 86.6 - - 42.5 70.6 50.2 42.7 73.0 59.1 81.8 67.9 88.6 - - 73.1 92.1 71. - 80.2 - 81.9 89.0 93.0 - 94.9 - - 42.6 73.4 48.1 43.6 75.0 62.8 86.2 64.4 89.1 - - 65.3 88.0 68.7 - 85.6 - 86.5 93.3 94.8 - 94.2 - - 20.9 40.7 32.5 24.0 42.8 34.9 54.6 41.4 64.3 - - 45.2 66.6 46.7 - 49.1 - 52.1 63.1 71.5 - 73.1 - - 43.1 70.5 51.8 47.2 74.8 57.9 79.2 58.6 81.6 49.0 75.2 56.9 76.9 60. - 81.2 - 83.9 86.4 86.5 85.0 84.6 - BLiM (Ours) 86.4 95.6 96.4 81.0 94. 96.6 55.7 73.1 78.2 64.7 83.9 88.2 82.8 95. 96.4 74.4 92.6 96.2 49.1 71.0 77.1 62.2 82. 87.0 Table 1. Results on retrieval datasets. means that the prediction is performed without (vt), and denotes our reproduced results. Also, in text-to-video retrieval with video candidates, the normalized candidate likelihood α(vt) is similarly defined to reduce the effect of the video candidate prior (v). To calculate prior probabilities (t) = (cid:81) (tit<i) and (v) = (cid:81) (viv<i), attention masking, as illustrated in Fig. 3b, is applied to all tokens within the condition modality when predicting the other modality. During inference, we use the normalized likelihood α(tv) and α(vt) to search for the optimal candidate in video-to-text and textto-video retrievals, respectively. This reduces bias toward the candidate prior and leads to more balanced predictions. Specifically, we replace candidate likelihood (t(n)v) in Eq. (8) with α(t(n)v) (similarly in Eq. (9)). The sensitivity study of α is available in Sec. F.2 of the supplement. We also observe that the prior bias is prevalent in diverse multi-modal tasks. Therefore, we extend CPN into decoding scheme for wide range of multi-modal tasks, e.g., visual question answering and visual captioning. In these tasks, standard decoding introduces prior bias toward text, leading to hallucination problems due to ungrounded generation that neglects the visual content. To mitigate this issue, instead of the standard decoding based on the likelihood (tv), we use normalized likelihood α(tv) to decode the text sequence. By applying our normalized likelihood to various sampling strategies (e.g., nucleus sampling), the model generates debiased text sequence, reducing the reliance on textual content and focusing more on visual content, thus minimizing hallucinations. 4. Experiments In this section, we first showcase the result of BLiM on four popular Text-Video Retrieval benchmark datasets in Sec. 4.1. We then verify the effectiveness of Bidirectional Likelihood Estimation in Sec. 4.2, and provide an extensive analysis of Candidate Prior Normalization in Sec. 4.3. Datasets. For Text-Video Retrieval, we use DiDeMo [5], ActivityNet [4], LSMDC [27], and MSRVTT [3] which contain diverse-length video and caption pairs. We use the Recall@K (R@1, R@5, R@10) metric to evaluate the models performance. Implementation details. An input video is divided into four clips, and each clip consists of four frames, resulting in total of 16 frames per video. During inference, we retrieve the top-16 candidates per query using InternVideo2 1B [37] and conduct reranking among these candidates using our BLiM for accurate retrieval. Further dataset and implementation details are in Sec. and of the supplement. 4.1. Results of BLiM Comparison with state-of-the-art models. In Tab. 1, we compare our model with state-of-the-art models on both text-to-video and video-to-text retrievals. First, in the zeroshot setting, since pretrained MLLMs are typically trained to maximize (tv) and lack the ability to estimate (vt), retrieval is performed solely with (tv) with our CPN, denoted as BLiM. Even without query likelihood estimation, BLiM significantly outperforms previous state-of-the-art models, surpassing InternVideo2 6B by an average of 4.9 in R@1 across all datasets. Moreover, with the bidirectional likelihood estimation in the fine-tuning setting, our BLiM achieves new state-of-the-art performance on all benchmarks. For example, on DiDeMo in text-to-video retrieval, BLiM improves R@1 by 12.2 compared to InternVideo2 6B. As result, the average R@1 gap between BLiM and InternVideo2 6B is 6.4. Overall, BLiM achieves remarkable performance gain both in zero-shot and finetuned settings, underscoring its effectiveness in Text-Video Retrieval. 5 BEiT-3 [44] ALBEF [45] BLIP [46] BLIP-2 [36] BLiM COCO Flickr30K T2I I2T T2I I2T 65.1 82.7 89.1 97.5 60.7 77. 82.8 94.1 65.1 82.4 86.7 96.7 68.3 85.4 89.7 97.6 69.7 84. 92.1 97.9 Models R@1 GPU memory GB Latency (seconds) Total Per query InternVideo2-1B InternVideo2-6B BLiM-7B (Ours) 62.1 64.4 72.0 24 27 0.37 1.29 1.75 730.12 2625.26 3767.01 Table 2. Results in Text-Image Retrieval on Flickr30K and COCO. We only report R@1 both in text-to-image (T2I) and image-to-text (I2T) retrieval. Table 4. Computational cost on text-to-video retrieval. We report average results across four datasets. Latency is measured using 8 A6000 GPUs. DiDeMo T2V V2T ActivityNet T2V V2T LSMDC T2V V2T MSRVTT T2V V2T MM-Embed [19] RagVL [20] LamRA [21] BLiM (Ours) 81.6 83.2 83.4 86.4 79.7 81.0 79.2 82.8 78.5 80.1 76.0 81. 70.7 70.9 68.7 74.4 52.8 53.1 51.9 55.7 48.1 48.5 47.8 49. 61.2 63.0 59.7 64.7 60.5 60.8 60.7 62.2 Table 3. Comparison with other MLLM-based reranking methods. We only report R@1 both in text-to-video (T2V) retrieval and video-to-text retrieval (V2T). Extension to Text-Image Retrieval. We observe that the bidirectional likelihood estimation of BLiM can be generally applicable to other multi-modal retrieval tasks. To validate its adaptability, we conduct experiments on TextImage Retrieval by slightly modifying BLiM for imagebased retrieval. Specifically, instead of predicting sequence of video clips for (vt), BLiM directly predicts single image feature, while the estimation of (tv) remains unchanged. Tab. 2 presents results on Flickr30k [2] and COCO [1]. Notably, BLiM outperforms strong TextImage Retrieval baselines, including BLIP-2 [36], achieving new state-of-the-art performance in 3 out of 4 settings. For instance, in text-to-image retrieval on Flickr30k, R@1 is increased by 2.4 over BLIP-2, demonstrating the effectiveness of bidirectional likelihood estimation even in image-based retrieval tasks. Comparison with MLLM-based retrieval methods. We here compare BLiM with other MLLM-based retrieval methods [1921]. Since MLLM-based retrievers have not been explored in the context of Text-Video Retrieval, we reproduce these algorithms in this setting. For example, MM-Embed [19], prompts the MLLM to assess whether query-candidate pair is semantically aligned by answering either True or False to the question: Does the video match the caption? The model then reranks candidates based on the logit of True. On the other hand, BLiM, equipped with CPN for candidate prior bias alleviation, directly estimates the likelihood (tv), capturing how likely the text is generated by the given video and vice versa. For fair comparison, we employ the same backbone MLLM (VideoChat-Flash) and apply reranking to the top-16 candidates per query retrieved by InternVideo2 1B across all methods. As shown in Tab. 3, BLiM consistently DiDeMo ActivityNet LSMDC MSRVTT avg. CLE QLE BLE (CLE + QLE) 34.4 72.5 74.1 29.0 69.5 69.9 19.2 43.7 44.4 26.4 56.4 56. 27.3 60.5 61.3 Table 5. Ablation study on bidirectional likelihood estimation. We compare the performance of each likelihood estimation: candidate likelihood estimation (CLE), query likelihood estimation (QLE), and bidirectional likelihood estimation (BLE). We report the average R@1 for text-to-video and video-to-text retrieval, and exclude CPN in this experiment. outperforms other MLLM-based retrieval methods across all datasets, underscoring the advantages of using bidirectional likelihood estimation on MLLM-based retrieval. Discussion on computational cost. In Tab. 4, we analyze the computational cost of BLiM by comparing its GPU memory usage and latency with strong retrieval baseline, InternVideo2 [37]. BLiM, 7B-parameter model, employs two-stage retrieval process: it first retrieves the top-K candidates using InternVideo2 1B, and then reranks them via bidirectional likelihood estimation. As result, its overall latency includes the retrieval time of InternVideo2 1B. In text-to-video retrieval, BLiM improves the average R@1 by 7.6 over InternVideo2 6B, with only an additional 0.46 seconds required to process single query, while consuming comparable GPU memory. 4.2. Analysis of Bidirectional Likelihood Estimation Quantitative analysis. In Tab. 5, we conduct an ablation study on bidirectional likelihood estimation of BLiM to verify its effectiveness in alleviating candidate prior bias. To isolate the impact of bidirectional likelihood estimation, we exclude CPN from this analysis. Across all datasets, candidate likelihood estimation (CLE) is highly susceptible to candidate prior bias, leading to suboptimal retrieval performance, whereas query likelihood estimation (QLE) achieves notable improvements over candidate likelihood estimation by alleviating such bias. Specifically, R@1 is improved by 38.1, 40.5, 24.5, and 30.0 on DiDeMo, ActivityNet, LSMDC, and MSRVTT, respectively. Moreover, bidirectional likelihood estimation (BCE) further enhances performance over query likelihood estimation alone, e.g., 1.6 R@1 gain on DiDeMo. As result, the integration of Image Understanding Benchmark Video Understanding Benchmark Model MME MMBench SeedBench MVBench VideoMME MLVU NExT-QA SeedBench avg. perception cognition en-dev image GPT-4V [47] VILA [48] IXC-2.5 [49] 1409.0 517.0 1762.0 2229.0 VideoChat2 [12] VideoChat2 (Ours) LLaVA-Onevision [13] LLaVA-Onevision (Ours) InternVL2 [50] InternVL2 (Ours) 1231.4 1284.5 1696.7 1708.6 1622.7 1642.1 274.3 322. 514.6 535.0 582.5 590.0 75.0 82.4 82.2 63.9 66.2 79.8 81.3 81.8 82. 49.9 75.8 75.4 67.8 68.0 75.0 75.3 76.1 76.2 test 43.5 - 69. 60.1 62.3 57.1 58.9 65.8 67.1 w/o subtitle w/ subtitle m-avg mc-val video 59.9 60.1 55.8 42.2 47.1 58.5 61.7 51.3 54.7 63.3 61.1 58.8 53.0 56. 57.8 62.1 51.7 55.1 49.2 - 37.3 45.8 48.5 65.3 65.8 50.8 55. - 67.9 71.0 78.9 79.4 79.4 79.5 80.4 80.8 60.5 - - 54.5 55. 56.9 57.0 56.4 56.6 - - - - +11.8 - +4.4 - +4. Table 6. Results of CPN decoding. The performances on seven different benchmarks are reported. means the model with CPN decoding. CPN DiDeMo ActivityNet LSMDC MSRVTT avg. CLE CLE BLE BLE 34.4 59.2 74.1 81.3 29.0 46.3 69.9 73.7 19.2 31.7 44.4 47. 26.4 44.3 56.7 59.3 - +18.1 - +4.2 Table 7. Ablation study on CPN. The average R@1 is reported. candidate prior probability can hinder accurate retrieval, as it leads to preference for common or verbose texts rather than contextually appropriate ones. similar trend is observed in text-to-video retrieval, where candidate likelihood estimation tends to prefer high-prior videos that often contain static scenes with limited temporal dynamics (see Sec. E.1). In contrast, our bidirectional likelihood estimation approach mitigates this bias by prioritizing semantic alignment over statistical frequency. 4.3. Analysis of Candidate Prior Normalization Abaltion study on CPN. Tab. 7 demonstrates an ablation study on CPN in Text-Video Retrieval. We observe substantial performance improvement after applying CPN to candidate likelihood estimation, with R@1 gains of 24.8, 17.3, 12.5, and 17.9 on each dataset. Consequently, incorporating CPN leads to an average R@1 improvement of 4.2 in bidirectional likelihood estimation. These findings suggest that CPN serves as simple yet effective plug-and-play module for mitigating candidate prior bias. Detailed results for both text-to-video and video-to-text retrieval tasks are presented in Sec. F.4 of the supplement. CPN decoding on various multi-modal benchmarks. We present an in-depth analysis of CPN decoding on multi-modal understanding benchmarks beyond mere retrieval tasks. Tab. 6 presents evaluation results on seven image and video understanding benchmarks (MME [51], MMBench [52], SeedBench [53], MVBench [12], VideoMME [54], MLVU [55], and NExT-QA [56]) encompassing comprehensive tasks that assess the models image and video understanding as well as reasoning Figure 4. retrieval example in video-to-text retrieval on DiDeMo. Green indicates correct prediction, while red denotes an incorrect one. Repeated phrases are highlighted in red. query likelihood estimation is pivotal in mitigating candidate prior bias in model predictions. Detailed results for both text-to-video and video-to-text retrieval tasks are presented in Sec. F.3 of the supplement. Qualitative analysis. We provide qualitative example in Fig. 4 to show the impact of bidirectional likelihood estimation. We observe that bidirectional likelihood estimation successfully retrieves the ground-truth text from the given video, while the candidate likelihood estimation tends to retrieve incorrect text that disregards the video content. Notably, the ground-truth text ranks 908 out of 1,003 based on candidate prior probability, while the incorrect text predicted by candidate likelihood estimation holds the highest prior probability (ranked 1). We also find that texts with high candidate prior probabilities tend to be longer and contain repetitive phrases (e.g., ornament and gingerbread men) due to the autoregressive nature of LLMs. Surprisingly, the correlation between prior probabilities and the text length is 0.97, and the correlation between prior probabilities and the number of repetitive phrases is 0.93 (see Sec. E.3). Overall, our analysis underscores that high text 7 shared embedding space between images and text, extending its text-image representations to the text-video domain. For instance, CLIP4Clip [6] introduced image aggregation modules on top of CLIP to enhance temporal understanding in the video domain. Another line of research has explored video foundation models such as UMT [28], InternVideo [39], and InternVideo2 [37], which are trained on large-scale text-video datasets, achieving strong retrieval performance. More recently, Cap4Video [43] proposed utilizing auxiliary data, e.g., video captions, to enrich video representations by bridging the modality gap. MLLM-based retrieval systems. With the emergence of multi-modal large language models (MLLMs) demonstrating impressive performance in diverse multi-modal understanding tasks, recent studies [1921] have explored their application in multi-modal retrieval. Unlike traditional dual-encoder architectures that rely on shallow similaritybased interactions between text and video, MLLMs enable fine-grained token-level interactions, capturing deeper semantic relationships. For example, MM-Embed [19] prompts an MLLM to evaluate the semantic alignment between given query and candidate by assessing the logits of True in response to the question, Does the image match the caption?. Also, RagVL [20] enhances retrieval accuracy through knowledge-enriched reranking with retrieval-augmented generation, leveraging image captions to improve video representation. In this work, we observe that MLLM-based retrievers tend to favor candidates with higher prior probabilities rather than those most relevant to the query. To address this issue, we propose bidirectional likelihood estimation and candidate prior normalization to mitigate bias and improve retrieval accuracy. 6. Conclusion In this paper, we observe that candidate likelihood estimation using MLLMs in Text-Video Retrieval tends to retrieve incorrect text from given video (and vice versa) due to candidate prior bias. To address this over-reliance on candidate priors, we propose Bidirectional Likelihood Estimation with MLLM (BLiM), which considers both candidate and query likelihoods. Additionally, our simple plug-andplay score calibration module, Candidate Prior Normalization (CPN), further enhances performance alongside BLiM in Text-Video Retrieval by reducing dependence on candidate priors. Our experimental results demonstrate the effectiveness of CPN decoding applied to MLLMs, facilitating more balanced consideration of both textual and visual information across various multi-modal tasks. Acknowledgments. This work was partly supported by IITP grant funded by MSIP & MSIT (No. RS-202400443251, No. RS-2024-00457882), NRF grant funded by MSIT (NRF-2023R1A2C2005373), and IITP-ITRC grant funded by MSIT (IITP-2025-RS-2024-00436857). Figure 5. qualitative example of CPN decoding on MVBench. Green signifies the accurate prediction, while red denotes the incorrect prediction. indicates the model with CPN decoding. abilities. By applying CPN decoding to three different MLLMs (VideoChat2 [12], LLaVA-Onevision [13], and InternVL2 [50]), the performances are consistently improved across all the benchmarks by average margins of 11.8, 4.4, and 4.1, respectively. This result indicates that our training-free score calibration method not only enhances retrieval but also significantly boosts the overall reasoning and comprehension capabilities of the models. To illustrate how CPN decoding corrects the models output, we provide qualitative results in Fig. 5, which compares the predictions of VideoChat2, VideoChat2 and VideoChat2 + CPN decoding (i.e., w/o video, VideoChat2). We note that the VideoChat2 w/o video model relies solely on textual information, i.e., text priors, for prediction. We find that standard VideoChat2 often adheres to predictions based on the text prior (VideoChat2 w/o video), resulting in incorrect answers. For the question, What happened before the person opened the door?, the VideoChat2 w/o video model assigns high text prior probability to the option (C) Opened the door due to the repetition of the phrase in the question. Thus, the standard VideoChat2s over-reliance on the wrong text prior results in inaccurate outputs, while our CPN decoding successfully mitigates this bias by encouraging the model to refer more to visual information. Overall, CPN decoding is both modeland task-agnostic, serving as an effective score calibration module that reduces reliance on linguistic cues and ensures more balanced consideration of visual and textual information for accurate predictions. We provide an analysis of CPN decoding in visual captioning in Section E.2 of the supplement, highlighting its effectiveness in enhancing generation quality by reducing hallucination problems. 5. Related Works Text-Video Retrieval. Text-Video Retrieval is widely studied multi-modal task that aims to find the most relevant video based on text query or vice versa. Early studies [6, 7, 43, 57, 58] have leveraged CLIP [9], dualencoder architecture trained with contrastive loss to learn"
        },
        {
            "title": "References",
            "content": "[1] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 1, 6, 14 [2] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV, 2015. 6 [3] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In CVPR, 2016. 2, 5, 12 [4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In CVPR, benchmark for human activity understanding. 2015. 2, 5, 12 [5] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, 2017. 1, 2, 5, 12 [6] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. arXiv preprint arXiv:2104.08860, 2022. 1, 5, 8, [7] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pretrained image-text model to video-language representation alignment. In ICLR, 2023. 1, 8 [8] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. 1 [9] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 8 [10] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 1, 3 [11] Qwen Team. Qwen2.5-vl, January 2025. [12] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. CVPR, 2024. 7, 8, 12, 14, 15 [13] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 7, 8, 14 [14] Jinyoung Park, Minseong Bae, Dohwan Ko, and Hyunwoo Kim. Llamo: Large language model-based molecular graph assistant. In NeurIPS, 2024. [15] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Internvideo2. 5: Empowering video Jianfei Gao, et al. mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 4 [16] Ji Soo Lee, Jongha Kim, Jeehye Na, Jinyoung Park, and Hyunwoo Kim. Vidchain: Chain-of-tasks with metricbased direct preference optimization for dense video captioning. In AAAI, 2025. [17] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [18] Dohwan Ko, Sihyeon Kim, Yumin Suh, Minseo Yoon, Manmohan Chandraker, Hyunwoo Kim, et al. St-vlm: Kinematic instruction tuning for spatio-temporal reasoning in vision-language models. arXiv preprint arXiv:2503.19355, 2025. 1 [19] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. Mm-embed: In Universal multimodal retrieval with multimodal llms. ICLR, 2025. 1, 6, [20] Zhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo. Mllm is strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced arXiv preprint reranking and noise-injected training. arXiv:2407.21439, 2024. 6, 8 [21] Yikun Liu, Pingan Chen, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiangchao Yao, Yanfeng Wang, and Weidi Xie. Lamra: Large multimodal model as your advanced retrieval assistant. arXiv preprint arXiv:2412.01720, 2024. 1, 6, 8 [22] Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, and Ying Wei. Mitigating the language mismatch and repetition issues in llm-based machine translation via model editing. In EMNLP, 2024. 1, 16 [23] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, XianSheng Hua, and Ji-Rong Wen. Counterfactual vqa: causeeffect look at language bias. In CVPR, 2021. 1 [24] Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee. Overcoming language priors in visual question answering with adversarial regularization. In NeurIPS, 2018. [25] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal biases for visual question answering. In NeurIPS, 2019. [26] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In CVPR, 2024. 1 [27] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. Movie description. IJCV, 2017. 2, 5, 12 [28] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In ICCV, 2023. 3, 4, 5, 8, 12, 13, 16 [29] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 3, 13 9 [30] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 3 [31] Dohwan Ko, Ji Soo Lee, Wooyoung Kang, Byungseok Roh, and Hyunwoo Kim. Large language models are temporal and causal reasoners for video question answering. In EMNLP, 2023. [32] Christopher Burges, Robert Ragno, and Quoc Le. Learning to rank with nonsmooth cost functions. In NeurIPS, 2006. 4 [33] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019. [34] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. Dense passage retrieval for open-domain question answering. In EMNLP, 2020. [35] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In SIGIR, 2020. [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 4, 6 [37] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In ECCV, 2024. 4, 5, 6, 8, 13, [38] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In ICLR, 2024. 5 [39] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 5, 8 [40] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu. Videococa: Videotext modeling with zero-shot transfer from contrastive captioners. arXiv preprint arXiv:2212.04979, 2022. 5 [41] Long Zhao, Nitesh Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et al. Videoprism: foundational visual encoder for video understanding. In ICML, 2024. 5 [42] Xiaojie Jin, Bowen Zhang, Weibo Gong, Kai Xu, Xueqing Deng, Peng Wang, Zhao Zhang, Xiaohui Shen, and Jiashi Feng. Mv-adapter: Multimodal video transfer learning for video text retrieval. In CVPR, 2024. 5 [43] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. Cap4video: What can auxiliary captions do for text-video retrieval? In CVPR, 2023. 5, 8, 12, 16 [44] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as foreign language: Beit pretraining for all vision and visionlanguage tasks. arXiv preprint arXiv:2208.10442, 2022. 6 [45] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS, 2021. 6 [46] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. [47] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 7 [48] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. 7 [49] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Internlm-xcomposer-2.5: versatile Linke Ouyang, et al. large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 7 [50] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 7, 8 [51] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 7, 12 [52] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 7, [53] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 7, 12 [54] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 7, 12 [55] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark arXiv preprint for multi-task long video understanding. arXiv:2406.04264, 2024. 7, 12 [56] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, 2021. 7, 12 [57] Satya Krishna Gorti, Noel Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu. X-pool: Cross-modal language-video attention for textvideo retrieval. In CVPR, 2022. 8 10 [75] Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, YenChun Chen, and Yu-Chiang Frank Wang. Rextime: benchmark suite for reasoning-across-time in videos. In NeurIPS, 2024. [58] Yuqi Liu, Pengfei Xiong, Luhui Xu, Shengming Cao, and Qin Jin. Ts2-net: Token shift and selection transformer for text-video retrieval. In ECCV, 2022. 8 [59] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 12 [60] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, 2021. [61] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, and Gedas Bertasius. Vindlu: recipe for effective video-and-language pretraining. In CVPR, 2023. 12 [62] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In ECCV, 2018. [63] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, 2020. 12 [64] Youngjae Yu, Jongseok Kim, and Gunhee Kim. joint sequence fusion model for video question answering and retrieval. In ECCV, 2018. [65] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, and Josef Sivic. Ivan Laptev, Makarand Tapaswi, Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In ICCV, 2019. 12 [66] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. 13 [67] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In ICCV, 2019. 14 [68] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 14 [69] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI, 2018. 14 [70] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. [71] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking finegrained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 14 [72] Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. Noise reduction in speech processing, 2009. 15 [73] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In ACL, 2011. 15 [74] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, highquality multilingual dataset for video-and-language research. In ICCV, 2019."
        },
        {
            "title": "Appendix",
            "content": "The appendix is organized into the following sections: Appendix A: Dataset Details A.1 Text-Video Retrieval A.2 Comprehensive Multi-Modal Understanding Appendix B: Implementation Details Appendix C: Inference Details of BLiM Appendix D: Proof of Proposition 1 Appendix E: Further Discussion on CPN E.1 Alleviation of Candidate Prior Bias E.2 CPN Decoding in Visual Captioning E.3 Analysis on Text Candidate Prior E.4 Discussion on Computational Cost Appendix F: Further Quantitative Results F.1 Results on Multi-Text Retrieval Settings F.2 Sensitivity Study of α in CPN F.3 Results on Bidirectional Likelihood Estimation F.4 Results on Candidate Prior Normalization Appendix G: Further Qualitative Results G.1 Results on Bidirectional Likelihood Estimation G.2 Results on Candidate Prior Normalization G.3 Results on Instruction-based Retrieval A. Dataset Details A.1. Text-Video Retrieval DiDeMo [5]. Distinct Describable Moments (DiDeMo) contains 10K videos which are divided into 5-second segments. It has total of 26K moments whose descriptions are detailed and contain camera movement, temporal transition indicators, and activities. We follow the previous works [6, 28, 43, 5961] by concatenating all captions of one video and solving the task as paragraph-video retrieval task. The number of training and test samples is 8,394 and 1,003, respectively. ActivityNet [4]. ActivityNet dataset contains 19K videos from YouTube, which are categorized into 200 different types of activities. On average, each category has 137 videos and each video has 1.41 activities which are annotated with temporal boundaries. Similar to DiDeMo, we also concatenate all the captions of video to form paragraph-video retrieval task on the val1 split by following [6, 28, 6163]. Therefore, the number of training and test samples is 10,009 and 4,917, respectively. LSMDC [27]. Large Scale Movie Description Challenge (LSMDC) contains 118K short video clips from 202 movies with captions from the movie script or from transcribed DVS (descriptive video services) for the visually impaired. Our model is trained with 101,055 videos and evaluated on 1,000 videos. MSRVTT [3]. Microsoft Research Video to Text (MSRVTT) contains 10K video clips from 20 categories, with each video clip annotated with 20 sentences. There are 29K unique words in all captions. Following the literature [6, 28, 43, 61, 6365], we train our model with 9,000 20 training samples and 1,000 test samples. A.2. Comprehensive Multi-Modal Understanding MME [51]. Multi-modal large language Model Evaluation benchmark (MME) is composed of 14 subtasks where all the samples are manually annotated. MME targets to assess MLLMs perception and cognition abilities including OCR, existence of objects, commonsense reasoning, numerical calculation, code reasoning, etc. MMBench [52]. MMBench is bilingual benchmark to evaluate the MLLMs multi-modal understanding abilities. This benchmark includes multiple-choice questions across the 20 ability dimensions like spatial relationship, physical property, attribute recognition, object localization, etc. SeedBench [53]. SeedBench aims at comprehensive assessment of generative models and contains 19K manually annotated multiple-choice questions across the 12 ability dimensions both on the image and video domain. The questions cover both spatial and temporal understanding like scene understanding, action prediction, procedure understanding, etc. MVBench [12]. Multi-modal Video understanding Benchmark (MVBench) consists of 20 challenging video understanding tasks that can effectively assess the ability to comprehend temporal evolution in dynamic videos. It consists of 9 main tasks for spatial understanding, which are then further split into total of 20 tasks for temporal understanding. VideoMME [54]. Multi-Modal Evaluation benchmark of MLLMs in Video analysis (VideoMME) evaluates the ability of MLLMs to handle sequential visual data on 6 primary visual domains with 30 subcategories. The videos are categorized as short, medium, and long, ranging from 11 seconds to 1 hour. total of 900 videos are in the benchmark with 2,700 questions. MLVU [55]. Multi-task Long Video Understanding benchmark (MLVU) targets to assess long video understanding performance spanning 7 video genres including movies, egocentric videos, cartoons, etc. MLVU contains 2,593 questions on 9 categories like topic reasoning, plot question answering, action count, ego reasoning, etc. NExT-QA [56]. NExT-QA is video question answering task aiming to evaluate causal action reasoning, temporal action reasoning, and common scene comprehension. This dataset includes 47,692 multiple-choice questions and 52,044 open-ended questions on total of 5,440 videos. B. Implementation Details BLiM details. Our BLiM is built upon VideoChatFlash [12] and is further fine-tuned on each Text-Video Retrieval dataset. Specifically, VideoChat-Flash consists of 12 (a) Video-to-Text Retrieval. (b) Text-to-Video Retrieval. Figure 6. Inference details of BLiM in (a) video-to-text and (b) text-to-video retrievals. DiDeMo ActivityNet LSMDC MSRVTT optimizer optimizer momentum weight decay warmup epochs input frames α for α(tv) α for α(vt) total epochs learning rate batch size AdamW β1 = 0.9, β2 = 0.95 1.0 1 16 0.8 0.0 5 2e-4 32 0.9 0.2 5 1e-4 32 1.0 0.2 3 1e-4 256 0.9 0.0 3 1e-4 Table 8. Experimental settings in Text-Video Retrieval. video encoder, linear projection layer, and LLM. The visual encoder and LLM are initialized with UMT-L [28] and Qwen2 [29], respectively. We freeze parameters in the video encoder and LLM, and only update parameters in the linear projection layer and LoRA for parameter-efficient fine-tuning, resulting in 10M trainable parameters among 7B total parameters (8%). We accumulate gradients from (tv) and (vt), and update the trainable parameters at once. Experimental settings. The self-attention mechanism in our model is implemented under FlashAttention2 [66] and we sample 16 frames per video for all datasets. These 16 frames are divided into four clips with four frames each. The learning rate is 2e-4 for DiDeMo and 1e-4 for ActivityNet, LSMDC, and MSRVTT with AdamW optimizer. We train our model on 8 A6000 GPUs with batch size of 32, 32, 256, and 512 for DiDeMo, ActivityNet, LSMDC, and MSRVTT, respectively. For inference, we select the top-16 candidates according to the similarity from InternVideo2 1B [37] and rerank them by leveraging bidirectional likelihoods. More details are summarized in Tab. 8. C. Inference Details of BLiM In inference, BLiM calculates candidate and query likelihood, and ensembles them for final prediction. Fig. 6a and 6b illustrate the inference procedure of video-to-text and text-to-video retrieval, respectively. For example, on candidate likelihood estimation in Fig. 6a (left) and 6b (left), we fix the input of the model as video (or text) query and seek the best text (or video) content by replacing the output with text (or video) candidates. On the other hand, on query likelihood estimation in Fig. 6a (right) and 6b (right), we fix the output of the model as text (or video) query and seek the best video (or text) content by replacing the input with video (or text) candidates. D. Proof of Proposition Proposition 1. Let (t(m)v(m)) denote the candidate likelihood for retrieving the most relevant text t(m) given query video v(m). Suppose that: 1. The query likelihood correctly ranks t(m) over any negative sample t(n) and the gap is bounded as: 0 < log (v(m)t(m)) log (v(m)t(n)) < ε. (12) 2. There exists text candidate t(n) with larger prior probability gap: log (t(n)) log (t(m)) > cε, for some > 1. (13) 13 (a) w/o CPN in video-to-text. (b) w/ CPN in video-to-text. (c) w/o CPN in text-to-video. (d) w/ CPN in text-to-video. Figure 7. Visualization of retrieval results on the candidate likelihood estimation w/ and w/o CPN. 50 text-video pairs are sampled to avoid visual clutter. Then, the candidate likelihood ranking is reversed: (t(m)v(m)) < (t(n)v(m)). (14) Proof. The candidate likelihood cap between t(m) and t(n) given the video query v(m) is written as: log (t(m)v(m)) log (t(n)v(m)) = log (v(m)t(m)) + log (t(m)) log (v(m)t(n)) log (t(n)) (15) (by Bayes Rule) (16) < ε + log (t(m)) log (t(n)) (by Eq. (12)) < ε cε = ε(1 c) (by Eq. (13)) < 0. (by > 1) Therefore, (t(m)v(m)) < (t(n)v(m)). (17) (18) (19) This proposition indicates that the candidate likelihood ranking is reversed, leading to the retrieval of an incorrect candidate, although the query likelihood identifies the accurate candidate in Eq. (12). The inaccurate relevance prediction arises due to substantial gap in candidate prior probabilities, as shown in Eq. (13). This motivates us to jointly consider query and candidate likelihood (i.e., Bidirectional Likelihood Estimation) along with CPN to mitigate bias towards candidate prior probability. E. Further Discussion on CPN E.1. Alleviation of Candidate Prior Bias To verify the alleviation of candidate prior bias, we provide heatmaps in Fig. 7 w/ and w/o CPN on the candidate likelihood estimation. For example, in video-to-text retrieval, the candidate likelihood estimation w/o CPN demonstrates suboptimal retrieval results since the text with the highest prior probability, i.e., the 24th text, is retrieved for most videos. COCO NoCaps LLaVA-Wild YouCook2 VDC TemporalBench LLaVA-Onevision [13] LLaVA-Onevision (Ours) 140.5 142.1 87.7 89.9 83.2 84.1 19.0 22. 2.5 3.0 36.1 37.6 Table 9. Results on visual captioning. We report CIDEr for COCO, NoCaps, and YouCook2, and average GPT score for LLaVA-Wild and VideoDetailCaption (VDC). The TemporalBench score is reported for TemporalBench, which is based on the embedding similarity. On the other hand, the candidate likelihood w/ CPN leads to balanced prediction where each text is retrieved for its own paired video in Fig. 7b. This reveals that CPN successfully alleviates candidate prior bias and encourages the model to consider text-video correspondences more. Furthermore, candidate prior bias is more pronounced in video-to-text retrieval due to the high reliance of MLLMs on LLMs pretrained knowledge. This becomes evident when comparing Fig. 7a and Fig. 7c, clear vertical line is observed on video-to-text retrieval in Fig. 7a. E.2. CPN Decoding in Visual Captioning Tab. 9 demonstrates the quantitative results of CPN decoding to visual captioning. We apply CPN decoding to LLaVA-Onevision [13] and evaluate its performance on six benchmarks (COCO [1], NoCaps [67], LLaVA-Wild [68], YouCook2 [69], VideoDetailCaption [70], and TemporalBench [71]) covering both image and video captioning tasks. Our results show that CPN decoding consistently enhances performance across all datasets, underscoring its effectiveness in visual captioning. To show how CPN decoding improves the performance in visual captioning, we provide qualitative results in Fig. 8 by applying CPN decoding to VideoChat2 [12]. The standard VideoChat2 usually generates hallucinated text by overlooking the visual content. For example, in Fig. 8a, the word apple is hallucinated which does not appear in the video. Similarly, in Fig. 8b, the standard VideoChat2 also generates hallucinated phrase They are trimming the dogs nails while the dog licks his feet in the video. How14 (a) (b) Figure 8. Qualitative results of CPN decoding in video captioning on ActivityNet. stands for the model with CPN decoding. The hallucinated text is highlighted in red. Model MME MMBench MVBench VideoMME MLVU NExT-QA SeedBench avg. VideoChat2 [12] VideoChat2 (Ours) 1505.7 (1.5) 1607.0 (2.0) 63.9 (1.2) 66.2 (1.2) 60.1 (2.4) 62.3 (2.4) 42.2 (4.1) 47.1 (4.1) 45.8 (6.9) 48.5 (7.1) 78.9 (1.4) 79.4 (1.5) 61.2 (0.9) 61.7 (1.0) - +16.3 (+4.9%) Table 10. Inference time comparison of CPN decoding. The inference time (seconds per sample) is reported in parentheses. stands for the model with CPN decoding. Pearson Correlation Coefficient [72], we find that the correlation in Fig. 9a is 0.97, and that in Fig. 9b is 0.93, indicating strong relationship between text candidate prior probabilities and these linguistic properties. E.4. Dicussion on Computational Cost Finally, Tab. 10 demonstrates the additional inference time overhead of CPN decoding on the benchmarks in Tab. 5 of the main paper. Since these benchmarks consist of multichoice questions, the number of newly generated tokens by the model is less than 10 tokens. This implies that CPN decoding introduces only marginal increase in inference time. In Tab. 10, the average performance is improved by 16.3 while the additional inference time is only increased by 4.9%. On the other hand, the inference time might be increased if the number of newly generated tokens becomes large. F. Further Quantitative Results F.1. Results on Multi-Text Retrieval Settings Tab. 11 demonstrates the result of BLiM in multi-text TextVideo Retrieval on MSVD [73] and VATEX [74]. In textto-video retrieval on VATEX, BLiM surpasses InternVideo2 6B by 2.7. Consequently, BLiM achieves new state-of- (a) Prior vs Text Lengths. (b) Prior vs Repetitive Phrases. Figure 9. Visualization of the correlation between (a) prior probabilities and text length and (b) prior probabilities and the number of repetitive phrases. The texts are sorted in ascending order based on prior probabilities. ever, with our CPN decoding (denoted as VideoChat2), the hallucinated text is successfully removed by encouraging the model to take into account visual contents more. E.3. Analysis on Text Candidate Prior We visualize the correlation between text candidate prior probabilities and text lengths in Fig. 9a, as well as the correlation between text candidate prior probabilities and the number of repetitive phrases in Fig. 9b. Interestingly, both text length and the number of repetitive phrases increase as the text candidate prior probability increases. Using the 15 Cap4Video [43] UMT [28] InternVideo2 6B [37] BLiM MSVD VATEX T2V V2T T2V V2T 51.8 - 66.6 - 58.2 82. 72.0 86.0 61.4 85.2 75.5 89.3 63.2 85.7 78.2 83.9 DiDeMo T2V V2T ActivtyNet T2V V2T LSMDC T2V V2T MSRVTT T2V V2T CLE QLE BLE (CLE + QLE) 45.1 85.2 85.9 23.7 59.7 62. 39.8 80.0 80.0 18.2 59.0 59.7 27.7 53.8 53.8 10.7 33.5 34.9 38.5 62.8 62.8 14.2 49.9 50. Table 11. Results on multi-text Text-Video Retrieval. We only report R@1 both in text-to-video (T2V) and video-to-text (V2T) retrieval. Figure 10. Video-to-text retrieval performance on various α. the-art performance in 3 out of 4 settings. F.2. Sensitivity Study of α in CPN Fig. 10 presents the video-to-text retrieval performance across various values of α in CPN (Eq. (8) of the main paper). α = 0 indicates that CPN is not applied to the prediction. Our findings reveal that an α range from 0.8 to 1.0 consistently yields the best performance across all datasets. This highlights the importance of mitigating the influence of candidate priors in candidate likelihood through the application of CPN. F.3. Results on Bidirectional Likelihood Estimation In Tab. 12, we provide detailed results on bidirectional likeIn text-to-video retrieval, R@1 is imlihood estimation. proved by 40.1, 40.2, 26.1, and 24.3 increase on DiDeMo, ActivityNet, LSMDC, and MSRVTT, respectively. Similarly, by reducing the effect of text candidate prior in videoto-text retrieval, dramatic performance gain is observed in query likelihood estimation, with R@1 increasing by 36.0, 40.8, 22.8, and 35.7 on each dataset. Finally, bidirectional likelihood estimation (BLE) further enhances performance beyond query likelihood estimation, especially in video-totext retrieval. F.4. Results on Candidate Prior Normalization Tab. 13 demonstrates detailed results on CPN. First, in video-to-text retrieval, we observe substantial performance improvement after applying CPN to candidate likelihood estimation, with R@1 gains of 49.6, 33.1, 23.8, and 16 Table 12. Ablation study on bidirectional likelihood estimation. We compare the performance of each likelihood estimation: candidate likelihood estimation (CLE), query likelihood estimation (QLE), and bidirectional likelihood estimation (BLE). We exclude CPN in this experiment. CPN DiDeMo T2V V2T ActivityNet T2V V2T LSMDC T2V V2T MSRVTT T2V V2T CLE CLE BLE BLE 45.1 45.1 85.9 85.9 23.7 73.3 62.2 76. 39.8 41.3 80.0 80.0 18.2 51.3 59.7 67.4 27.7 28.9 53.8 53. 10.7 34.5 34.9 41.3 38.5 38.5 62.8 62.8 14.2 50.0 50.6 55. Table 13. Ablation study on CPN. 35.8 on each dataset. We hypothesize that candidate prior bias is more pronounced in textual candidates, i.e., video-totext retrieval, due to the powerful LLMs pretrained knowledge in MLLM. On the other hand, the performance gain is relatively marginal in text-to-video retrieval since video representations are inherently less influenced by LLMs knowledge. Overall, incorporating CPN leads to an average R@1 improvement of 8.5 in bidirectional likelihood estimation. G. Further Qualitative Results G.1. Results on Bidirectional Likelihood Estimation In Fig. 11, we provide additional qualitative results on bidirectional likelihood estimation for both video-to-text and text-to-video retrieval. We observe that candidate likelihood estimation tends to favor text and video candidates with high prior probability (ranked 2nd and 7th out of 1,003 candidates) on video-to-text (Fig. 11a) and text-to-video (Fig. 11b) retrieval, respectively. Interestingly, the highprior text candidate contains repetitive phrases due to the autoregressive property of the LLM [22]. Likewise, the high-prior video candidate consists of static scenes, while the ground-truth video exhibits richer temporal dynamics. However, our bidirectional likelihood estimation successfully retrieves the correct text and video in both tasks. These results demonstrate that candidate prior bias can lead to inaccurate retrieval, while our method effectively mitigates this bias, resulting in improved retrieval performance. G.2. Results on Candidate Prior Normalization We provide further qualitative results of CPN decoding in Fig. 12 and identify bias towards frequent co-occurrence. (a) Video-to-Text Retrieval. (b) Text-to-Video Retrieval. Figure 11. Qualitative results of the bidirectional likelihood estimation in (a) video-to-text and (b) text-to-video retrieval. The VideoChat2 w/o video model prioritizes the likely action sequence (B) Took the cup/glass/bottle in response to the question What happened after the person held the dish?, based on the frequent co-occurrence derived from the LLMs pretrained knowledge. Consequently, the standard VideoChat2s high dependence on incorrect text priors leads to inaccurate outputs, whereas our CPN decoding effectively reduces this bias by leading the model to focus more on visual information. G.3. Results on Instruction-based Retrieval In this section, we explore the MLLMs versatility in the human instruction-based retrieval task. We note that the benchmark for human instruction-based retrieval is not yet studied, so we customize ReXTime [75], originally released for the moment-retrieval task, adequately to our setting and we provide qualitative results on several examples. In Fig. 13, we mainly ask the model to retrieve certain part of the video and the answer given the video and question, i.e., multi-modal queries and multi-modal contents. Specifically, in Fig. 13a, the user asks to retrieve the answer and the relevant part of the video to What does the man do after walking the tube back?. Our BLiM successfully retrieves the relevant part of the video including the 3rd, 4th, and 5th frames along with the text The man goes up the tow rope., as the action walking the tube back occurs in the 3rd frame. This retrieved video includes the action where the man goes up the tow rope. Furthermore, we ask two different questions with the same video in Fig. 13b and 13c. Our model retrieves the relevant part of the video and the answer well by following the instructions. In Fig. 13b, the scene of gaining momentum for throwing the javelin and the text To gain momentum for throwing the javelin off 17 qualitative example of CPN decoding on Figure 12. MVBench. Green signifies the accurate prediction, while red denotes the incorrect prediction. indicates the model with CPN decoding. into the distance. are retrieved given the question Why does the person begin running down the track? and the full video. Interestingly, as the question is changed to How does the person throw the javelin off into the distance?, the retrieved scene and text are changed to the content depicting running down the track. Overall, integrating the retrieval task into MLLMs enables them to handle complex human instruction-based retrieval in the real-world chatting system. (a) (b) (c) Figure 13. Qualitative results of human instruction-based retrieval on ReXTime."
        }
    ],
    "affiliations": [
        "KAIST",
        "Korea University",
        "Meta GenAI"
    ]
}