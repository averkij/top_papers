{
    "paper_title": "RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback",
    "authors": [
        "Qiaoyu Tang",
        "Hao Xiang",
        "Le Yu",
        "Bowen Yu",
        "Hongyu Lin",
        "Yaojie Lu",
        "Xianpei Han",
        "Le Sun",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of Large Language Models (LLMs), developing effective critic modules for precise guidance has become crucial yet challenging. In this paper, we initially demonstrate that supervised fine-tuning for building critic modules (which is widely adopted in current solutions) fails to genuinely enhance models' critique abilities, producing superficial critiques with insufficient reflections and verifications. To unlock the unprecedented critique capabilities, we propose RefCritic, a long-chain-of-thought critic module based on reinforcement learning with dual rule-based rewards: (1) instance-level correctness of solution judgments and (2) refinement accuracies of the policy model based on critiques, aiming to generate high-quality evaluations with actionable feedback that effectively guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement settings, RefCritic demonstrates consistent advantages across all benchmarks, e.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably, under majority voting, policy models filtered by RefCritic show superior scaling with increased voting numbers. Moreover, despite training on solution-level supervision, RefCritic outperforms step-level supervised approaches on ProcessBench, a benchmark to identify erroneous steps in mathematical reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 4 2 0 5 1 . 7 0 5 2 : r RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback Qiaoyu Tang1,3*, Hao Xiang1,3*, Le Yu2, Bowen Yu2, Hongyu Lin1, Yaojie Lu1, Xianpei Han1, Le Sun1, Junyang Lin2 1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences 2Alibaba Group 3University of Chinese Academy of Sciences {tangqiaoyu2020, xianghao2022, hongyu, luyaojie, xianpei, sunle}@iscas.ac.cn {chuanyi.yl, yubowen.ybw, junyang.ljy}@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "With the rapid advancement of Large Language Models (LLMs), developing effective critic modules for precise guidance has become crucial yet challenging. In this paper, we initially demonstrate that supervised fine-tuning for building critic modules (which is widely adopted in current solutions) fails to genuinely enhance models critique abilities, producing superficial critiques with insufficient reflections and verifications. To unlock the unprecedented critique capabilities, we propose RefCritic, long-chain-of-thought critic module based on reinforcement learning with dual rule-based rewards: (1) instance-level correctness of solution judgments and (2) refinement accuracies of the policy model based on critiques, aiming to generate high-quality evaluations with actionable feedback that effectively guides model refinement. We evaluate RefCritic on Qwen2.514B-Instruct and DeepSeek-R1-Distill-Qwen14B across five benchmarks. On critique and refinement settings, RefCritic demonstrates consistent advantages across all benchmarks, e.g., 6.8% and 7.2% gains on AIME25 for the respective base models. Notably, under majority voting, policy models filtered by RefCritic show superior scaling with increased voting numbers. Moreover, despite training on solution-level supervision, RefCritic outperforms step-level supervised approaches on ProcessBench, benchmark to identify erroneous steps in mathematical reasoning."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in executing complex reasoning tasks such as mathematical problem-solving and code generation (Yang et al., 2025a; Hui et al., 2024). As these models continue to evolve, their reasoning processes have grown increasingly sophisticated, encompassing * Equal contribution 1 multiple elaborate steps and diverse pathways (Guo et al., 2025; Team, 2025). This progression introduces critical challenge: reasoning processes are becoming substantially difficult for humans to supervise, making errors within these intricate chains harder to identify and rectify. The escalating complexity of LLM-generated solutions necessitates more effective analytical frameworks to evaluate and enhance reasoning quality, extending beyond the constraints of human supervisory oversight. Developing LLM critics has become promising direction for evaluating complex reasoning tasks (Liu et al., 2025; Zhang et al., 2024; Mahan et al., 2024; Wang et al., 2023; Ankner et al., 2024), functioning as specialized modules to analyze reasoning processes and identify errors. Ideally, LLM critics are expected to comprehensively analyze content generated by the policy model, delivering targeted critiques that identify logical inconsistencies or factual errors and improve the refinement quality of the policy model. However, contemporary approaches exhibit two critical limitations. Firstly, they frequently produce superficial evaluations characterized by insufficient analytical depth (Zheng et al., 2025; Tang et al., 2025a) and typically necessitate granular step-level annotations of the solution for optimization (Yang et al., 2025b). Secondly, current implementations mainly focus on metrics of critic performance while overlooking the practical utility of critiques in enhancing policy model refinement. In this paper, we propose RefCritic, long chainof-thought critic model with refinement feedback to tackle the above limitations, which could generate in-depth critiques that not only achieve superior critic performance but also effectively guide policy model refinement through actionable feedback. This process begins with prompting open-source models (e.g., DeepSeek-R1-Distill-Qwen) to generate seed data containing three essential components: long CoT analysis, solution validity judgFigure 1: The Critic model with Refinement Feedback RefCritic framework consists of two steps: (1) cold-start via rejective sampling fine-tuning, (2) rule-based reinforcement learning with refinement feedback. With this two-stage optimization, RefCritic generates in-depth critiques that achieve superior critic performance and effectively guide policy model refinement through actionable feedback. ments, and refinement suggestions. After rigorous quality filtering based on judgment accuracy, about 10K valid samples are obtained, which are utilized to establish cold-start critic models via Supervised Fine-Tuning (SFT). We then conduct preliminary assessments to verify whether SFT is sufficient for producing comprehensive critiques. Results reveal that SFT alone struggles to produce in-depth critiques despite generating lengthy CoT content, as models frequently exhibit misleading analytical patterns where correct judgments emerge from flawed reasoning processes (a persistent issue observed in existing LLM critics), resulting in unreliable performance evaluations. Furthermore, the absence of explicit policy model interaction during SFT leads to critiques that inadequately prioritize feasible guidance for effective refinement. The above observations highlight the difficulty of SFT in producing critiques with both accurate evaluations and practical feedback for refinement. To further enhance the critic reliability and establish causal connection between critique quality and policy refinement outcomes, we implement dual-reward reinforcement learning framework based on cold-start models for finalizing RefCritic. The first reward signal stems from the instancelevel binary accuracy metric (0/1 values) for evaluating the critic models in solution judgment capability. The second reward quantifies policy model improvement through accuracy gains after incorporating refinement suggestions. Critically, the second reward establishes an explicit feedback loop where the quality of effective critique is operationally defined by its capacity to drive measurable enhancements to the policy model. This dual-reward design ensures that high-reward critiques are those that not only accurately identify solution flaws but also provide actionable guidance leading to verifiable performance gains. We validate the effectiveness of RefCritic on multiple challenging mathematical datasets: AIME24, AIME25, and Olympiad (He et al., 2024). In the refinement after critique setting, feedback generated by RefCritic based on Qwen2.5-14B and DeepSeek-R1-Distill-Qwen-14B consistently enhances the corresponding policy models performance, with improvements of 6.8% and 7.2% on AIME25, and 9.9% and 2.6% on Olympiad, respectively. In the majority vote with critique setting, RefCritic demonstrates increasingly significant performance gains as the sampling count increases. With 64 samples, RefCritic achieves an average improvement of 3.6 percentage points on AIME25 compared to scenarios without critique, consistently outperforming other critique baselines. Moreover, RefCritic effectively enhances majority vote performance even when applied to more powerful models, like QwQ and DeepSeek-Distill-Qwen-32B. These performance improvements indicate that our dual reward mechanism successfully aligns critique generation with both evaluation accuracy and refinement utility, enabling critic models to produce not only precise solution assessments but also actionable feedback that effectively guides refinement processes. Furthermore, it is worth noting that RefCritic generalizes effectively to step-level critique tasks without requiring any step-level labels during training, achieving remarkable performance on ProcessBench (Zheng et al., 2024)."
        },
        {
            "title": "2 Related Work",
            "content": "Test-time Scaling Test-time scaling techniques have emerged as powerful approach to enhance LLM reasoning capabilities through increased computational resources during inference (Charniak and Johnson, 2005; Snell et al., 2024; Wu et al., 2024; Yao et al., 2023; Chen et al., 2024; Jaech et al., 2024; Guo et al., 2025; Team, 2025). The effectiveness of these approaches can be improved by judgment or verification mechanisms. Besides traditional process reward models (PRMs) that directly predict numerical correctness scores for solution steps (Uesato et al., 2022; Lightman et al., 2023; Zheng et al., 2025, 2023; Chen et al., 2025; Zhang et al., 2025), recent methods frame judgement as language generation tasks that offer greater interpretability and scalability (Liu et al., 2025; Zhang et al., 2024; Mahan et al., 2024; Wang et al., 2023; Ankner et al., 2024). Among them, promising approach is LLM Critics, which uses LLMs as critic models to verify solutions (McAleese et al., 2024; Zheng et al., 2025; Yang et al., 2025b). Critique for Math The judgment ability of LLMs has garnered significant research interest due to their potential to enhance mathematical reasoning through explicit error detection and correction guidance (Lan et al., 2024; Lin et al., 2024; Zheng et al., 2024). Current approaches fall into two main categories: LLM-as-a-Critic (Zheng et al., 2025; Yang et al., 2025b) leverages off-the-shelf models through careful instruction design, and specialized critic models(McAleese et al., 2024; Lan et al., 2024; Shi and Jin, 2025) that employ fine-tuning or reinforcement learning to enhance judgment ability. While recent research emphasizes that critique effectiveness should be validated through correction outcomes (Tang et al., 2025b; Zheng et al., 2024), most existing critic models focus exclusively on critique and ignore the future benefit it can bring to refinement. Our work addresses these challenges through RefCritic, novel framework that leverages reasoning models critique abilities and incorporates refinement performance as direct reward signal during critic training. The most closely related concurrent works are ThinkPRM (Khalifa et al., 2025) and DeepCritic (Yang et al., 2025b). The former uses SFT to enhance the capabilities of reasoning models, while the latter improves the critique performance of instruction models on individual steps through complex fine-tuning and simple RL. However, they fail to recognize how critiques serve as valuable feedback mechanisms for policy model refinement."
        },
        {
            "title": "3 SFT is Insufficient for Deep Critiques",
            "content": "To better understand the challenges in developing effective critic models, we first examined straightforward approach widely adopted in previous research: supervised fine-tuning with rejection sampling. This approach has demonstrated success in improving judgment capabilities of critics in several studies (Tang et al., 2025a; Zheng et al., 2025; Khalifa et al., 2025). response, Specifically, we employed Qwen2.5-14BInstruct/DeepSeek-Distill-Qwen-14B as our policy model and DeepSeek-Distill-Qwen-32B as the critic model1. Employing rejection sampling, we collected approximately 10K critique training samples from subset of NuminaMath. Each training sample comprised problem statement, reasoning, model judgment on solution correctness, and refinement suggestions. We subsequently fine-tuned the policy models on these datasets to develop critique capabilities. To evaluate the effectiveness of these fine-tuned critic models, we tested them on responses collected from AIME25, assessing their ability to identify errors and provide feedback that could meaningfully improve policy model performance. chain-of-thought Our experiments revealed significant disparity between the critic model capabilities and their practical utility. As shown in Table 1, whether it is Qwen-based critic or DeepSeek-Distill-Qwenbased critic, SFT-trained models significantly outperformed self-critique approaches in critique ac1For Qwen2.5-14B-Instruct, we provide an empty thinking process and only use the content after \"</think>\"."
        },
        {
            "title": "Critique\nAccuracy",
            "content": "Pass@1 after Refinement Qwen2.5-14B as Base Model Pass@1 Self-Critique SFT - 51.8 80.6 R1-Qwen-14B as Base Model Pass@1 Self-Critique SFT - 71.5 78.9 14.4 14.5 15. 49.2 52.1 51.4 Table 1: Preliminary experiment on AIME25 to verify whether SFT can emerge deep critic. We can see that although the SFT model achieves strong performance in critique evaluation, incorporating its feedback into the policy model yields only marginal performance gains. R1-Qwen represents DeepSeek-Distill-Qwen. curacy metrics. However, when these critiques were used to refine policy model outputs, the resulting performance improvements were minimal, sometimes even inferior to those achieved through self-critique methods. This counterintuitive result suggests that conventional evaluation metrics for critics may not align with their actual utility in improving model performance. Further analysis of the SFT model outputs revealed two critical limitations. First, critics often arrive at correct judgments through flawed or superficial reasoning processes, creating false impression of reliability despite inconsistent analytical quality. This problem is particularly evident in Qwen-based models, as the critique length after SFT showed no significant increase, with an average length of less than 500 tokens. Second, the feedback typically identified error locations but lacked specific, actionable guidance for improvement. Critics frequently offered vague suggestions or restated problem requirements rather than providing concrete directions for correcting mathematical misconceptions or reasoning flaws. These findings directly support our hypothesis that conventional SFT approaches, while successful in training critics to make binary judgments, fail to develop models that can provide the actionable, improvement-oriented feedback necessary for effective solution refinement."
        },
        {
            "title": "4 RefCritic",
            "content": "We propose RefCritic, novel approach for developing effective critic modules that provide actionable feedback for mathematical reasoning tasks. As illustrated in Figure 1, RefCritic employs twostage methodology. First, we develop cold-start critic model via supervised fine-tuning that activates the models reasoning judgment capabilities and enables structured output generation. Second, we introduce rule-based reinforcement learning framework with dual rewards optimizing critics for both solution-level correctness and refinement effectiveness, measured by concrete improvements in policy model performance. This dual-reward mechanism ensures our critic models not only accurately evaluate solutions but also provide guidance that leads to substantive improvements in reasoning capabilities. When faced with complex tasks such as critique generation, LLMs often exhibit problematic behaviors, including instruction unfollowing (He et al., 2025) and answer leakage (Yang et al., 2025b). To address these challenges, we implement SFT with rejection sampling to standardize model outputs. Following our preliminary experimental setup, we leverage more powerful model to generate initial critic responses, then systematically filter out responses containing erroneous judgments, instruction violations, or solution leakage risks through rule-based screening. This curated dataset serves as the foundation for our LLM training, ensuring the resulting critic model adheres to desired output formats while maintaining evaluation integrity. Despite the effectiveness of the supervised finetuning approach in producing format-compliant critics, the preliminary experiments revealed fundamental shortcomings that limit practical utility. As previously demonstrated, SFT models exhibit misleading combination of accurate judgments built upon superficial reasoning, alongside feedback that identifies errors without providing actionable remediation strategies. Rather than reiterating these limitations, we recognize them as symptomatic of deeper issue: conventional training methods optimize for solution classification accuracy rather than refinement capability. This insight motivates us to shift from purely supervised learning toward reinforcement learning framework that explicitly rewards critics not just for evaluation correctness, but for generating feedback that demonstrably improves subsequent solutions. To address these limitations, we introduce dualreward reinforcement learning framework that optimizes both judgment accuracy and refinement effectiveness. Our approach evaluates critics based on two key metrics: (1) their ability to correctly classify solutions as right or wrong, and (2) the tangible improvement their feedback produces when used by the policy model to revise incorrect solutions. This framework ensures our critics develop both strong evaluation capabilities and the ability to generate constructive feedback that leads to measurable improvements in reasoning outcomes. Formally, let Cθ represent our critic model with parameters θ, and Pϕ denote the policy model with parameters ϕ that generates mathematical solutions. For problem with ground truth answer from dataset D, the policy model produces an initial solution y0 Pϕ(yx). The correctness of this solution is determined by = I[y0 = a] {0, 1}, where I[] is the math equal function. The critic model then performs an evaluation Cθ(x, y0) (z, ˆc, ), where represents an extensive reasoning process, ˆc {0, 1} denotes the predicted correctness judgment, and provides actionable refinement suggestions. Based on these critiques, the policy model generates refined solutions {yi}m i=1 where yi Pϕ(yx, y0, ) for {1, 2, . . . , m}. Our reinforcement learning objective maximizes: (θ) = E[Rj(c, ˆc) + λRr(c, ˆc, a, {yi}m i=1)] where Rj represents the binary judgment score: Rj(c, ˆc) = (cid:26) 1 0 if = ˆc, otherwise and Rr denotes the refinement score, which is nonzero only when the critic correctly identifies an incorrect solution (c = 0 and ˆc = 0): Rr(c, ˆc, a, {yi}m i=1) = 1 0 (cid:80) i=1 I[yi = a] if = 0 and = ˆc otherwise λ is hyperparameter that balances the importance between judgment accuracy and refinement effectiveness. higher λ value places greater emphasis on the critics ability to provide actionable feedback that leads to correct solutions, while lower value prioritizes accurate solution classification. In summary, our RefCritic framework alleviates the key limitations of existing critic models through this dual-reward reinforcement learning approach. By explicitly optimizing for both judgment accuracy and refinement effectiveness, we develop critics that not only accurately evaluate mathematical solutions but also provide actionable feedback that leads to concrete improvements in reasoning outcomes."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Models For our implementation of RefCritic, we utilize Qwen2.5-14B-Instruct (Yang et al., 2025a) and DeepSeek-R1-Distill-Qwen-14B (Guo et al., 2025) as the backbone. In our framework, these models perform two distinct functions: first, as policy models that generate solutions for mathematical problems; and second, as the foundation models to develop our critic models. Data Construction We construct our training dataset by filtering approximately 120k highquality mathematical problems from the 900k problems in NuminaMath-1.5 (LI et al., 2024). Our filtering pipeline includes: (1) deduplication through exact string matching and semantic similarity using gte-multilingual-base embeddings (removing pairs with cosine similarity > 0.95); (2) problem filtering using Qwen2.5-72B-Instruct to remove unsolvable, proof, and multiple-choice problems; (3) difficulty balancing by sampling eight solutions and excluding problems where all attempts succeed or fail. For critic training, we sample 8 responses per problem and retain at most two responses per problem (one correct, one incorrect) to ensure balanced training data. Detailed process can be found in Appendix A. Benchmarks We evaluate the performance of RefCritic on challenging mathematical benchmarks, including AIME 2024/2025 (American Invitational Mathematics Examination problems), and OlympiadBench (He et al., 2024) (a collection of mathematical Olympiad problems). Since RefCritic was trained only on math problems, we conduct out-of-distribution (OOD) generalization tests on the code generation task LiveCodeBench (Jain et al., 2024) and the science QA benchmark GPQADiamond (Rein et al., 2024). Furthermore, to evaluate RefCritics capability for fine-grained error localization, we leverage ProcessBench (Zheng et al., 2024) to assess its ability to accurately identify the specific step where an error occurs. Evaluation Details For evaluation, both policy and critic models use sampling strategy with temperature=0.6 and top_p=0.95. For AIME24/25, we pre-sample 128 responses as the response pool for subsequent performance calculations. For OlympiadBench/GPQA/LiveCodeBench, we sample only 32 responses due to its larger scale. During 5 Model Qwen-14B Majority Vote Qwen-14B as Critic RefCritic-Qwen-14B(Ours) SF RLλ=0 RLλ=0 af ter RLλ=1 R1-Qwen-14B Majority Vote R1-Qwen-14B as Critic RefCritic-R1-14B(Ours) SF RLλ=0 RLλ=0 af ter RLλ=1 AIME25 AIME24 Olympiad assr@1 ajc@8 ajc@64 assr@1 ajc@8 ajc@64 assr@1 ajc@8 ajc@ 14.4 14.5 15.0 18.5 21.2 49.1 50.0 51.3 55.1 56. 19.2 19.1 19.3 20.8 21.5 61.6 60.6 61.6 64.2 65. 23.3 22.7 23.4 22.4 24.4 62.0 62.9 62.8 67.1 68. 13.7 13.7 15.2 19.1 23.0 67.6 70.5 71.4 73.5 72. 16.5 18.5 19.2 20.5 21.4 78.7 79.3 79.4 80.4 80. 16.6 21.2 23.9 23.8 26.6 80.1 82.4 83.1 82.8 82. 45.8 45.8 46.6 51.4 55.7 77.7 78.8 78.7 80.4 80. 52.2 52.3 52.5 55.4 57.3 82.7 82.7 83.0 83.8 83. 53.6 54.0 54.3 57.4 59.2 83.3 83.3 84.4 84.5 84. Table 2: Performance comparison of different approaches on AIME24/25 and Olympiad. assr indicates the performance after one round of critique and refinement. ajc indicates the majority vote performance after using the critic filtering solutions. RLλ=1 indicate RL with Refinement Feedback. Considering the cost of sampling refinements, we initially set λ = 0 to achieve rapid improvement in critic performance. evaluation, we randomly select responses from the response pool for metric calculation and report the average results over 1000 trials. We adopt multiple evaluation settings: Majority Vote with Critique: The critic model first evaluates each of the sampled solutions and filters out those judged as incorrect. We then apply majority vote to the remaining solutions to select the final answer, denoted as ajc@N . As baseline, we also report standard majority vote accuracy without critique filtering. Refinement after Critique: The policy model generates an initial solution, which is then critiqued by the critic model. If judged incorrect, the policy model refines the solution based on the critique feedback. We report pass@1 accuracy of the final refined answer, denoted as assr@1. Process Critique Evaluation: For process-level evaluation, since our critic models were trained to output natural language critiques rather than explicit step indices, we use Qwen2.5-14B-Instruct to identify the step index the critic judges incorrect.2 Following ProcessBench, we report the F1 score, which is the harmonic mean of precision for correct and incorrect solutions. Training Details In the SFT stage, we train the critic models with learning rate of 7e-6 and batch size of 512 for three epochs. For the RL stage, we employ the GRPO algorithm (Shao et al., 2024) to enhance critic performance. We sample 2We only provide the solution and critique, without the corresponding problem. Every critic we evaluated would go through this process. critics for each input, each rollout comprising 128 inputs, and conduct on-policy training with learning rate of 1e-6. We set the maximum sequence length to 8K and 16K tokens for Qwen2.5-Instruct and DeepSeek-R1-Distill-Qwen, respectively. For refinement feedback, we use policy models to generate 8 refinements for each critic. Considering the cost of sampling refinements, we initially set λ=0 to achieve rapid improvement in critic performance for 600 steps, where no refinement is generated. We subsequently adjust to λ=1 to balance the trade-off between the two reward components and continue training for 300 steps."
        },
        {
            "title": "5.2 Main Results",
            "content": "As shown in Table 2, we present the performance of RefCritic against various baselines on AIME24, AIME25, and OlympiadBench datasets. In the oneround critique and refinement settings, RefCritic consistently provides the most effective feedback for policy model improvement, demonstrating the effectiveness of incorporating refinement performance as reward in our reinforcement learning approach. Specifically, on the challenging AIME25 dataset, RefCritic-Qwen-14B and RefCritic-R114B enhance the policy models ass@1 performance by 6.8% and 7.2%, respectively, significantly outperforming both self-critique baselines and models trained via supervised fine-tuning. Similar patterns emerge across AIME24 and Olympiad benchmarks, confirming that directly optimizing for policy model refinement performance during RL training enables critic models to generate more actionable feedback."
        },
        {
            "title": "Model",
            "content": "GSM8K MATH Omni-Math Olympiad Avg. PRM Math-Shepherd-PRM-7B* RLHFlow-PRM-8B-DS* Qwen2.5-Math-PRM-7B* Prompt LLM as Critic Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct R1-Qwen-7B R1-Qwen-14B GPT-4o-0806* Baseline Critic SCRIT-72B (Tang et al., 2025a) DeepCritic-7B (Yang et al., 2025b) ThinkPRM-14B (Khalifa et al., 2025) Our Critic RefCritic-Qwen-14B RefCritic-R1-14B 47.9 38.8 68. 61.7 74.6 75.3 75.9 79.2 80.2 72.6 67.6 81.9 86.3 29.5 33.8 62.6 52.6 61.8 74.4 76.2 63.6 60.0 72.8 71. 71.2 82.0 24.8 16.9 50.7 41.3 51.7 56.9 59.6 51.4 32.5 56.0 54.8 58.1 67.6 23.8 16.9 44. 43.1 52.8 63.5 63.6 53.5 27.8 60.9 59.3 60.7 72.3 31.5 26.6 56.5 49.7 60.2 67.5 68.8 61.9 50.0 65.6 63. 68.0 77.1 Table 3: The evaluation results of PRMs, LLM as critic, and RefCritic critic models on ProcessBench. The metric is the F1 score, the harmonic mean of precision for correct and incorrect solutions. All our critic models are followed by an extract model (Qwen2.5-14B-Instruct) to get the error step for easy evaluation. Content marked with \"*\" sourced from Processbench. As shown in Table 8, we use the same template as used in Processbench. When scaling up the policy models response generation and applying critic model filtering, RefCritic achieves superior performance across nearly all experimental settings. For instance, on AIME25, RefCritic-RL improves ajc@64 with an average benefit of 3.6%(1.1% for RefCritic-Qwen and 6.1% for RefCritic-R1). These results demonstrate that our refinement-oriented critic not only enhances feedback quality but also improves critical evaluation capabilities. Notably, as the sampling scale increases from 8 to 64, the overall performance gains from RefCritic become more pronounced, indicating our critic models high discriminative accuracy in identifying and preserving high-quality solutions from larger candidate pools."
        },
        {
            "title": "5.3 Out-of-Distribution Performance",
            "content": "Additionally, we also evaluated RefCritic on out-ofdistribution tasks to demonstrate its generalizability. Considering that the model was trained on mathematical data, we chose to use LiveCodeBench to verify its performance on coding, and GPQA to evaluate its performance in challenging knowledge reasoning. We found that RefCritic still performs well on out-of-distribution benchmarks. Although the improvements are not as substantial as in the mathematical tasks, they still bring considerable gains. Specifically, RefCritic-R1-14B achieved 3.1% performance improvement on LCB3, and im3Since coding tasks cannot perform Majority Vote, we only report assr@1 performance. Model Qwen-14B Majority Vote Qwen-14B as Critic RefCritic-R1-14B(Ours) SF RLλ=0 RLλ=0 af ter RLλ=1 R1-Qwen-14B Majority Vote R1-Qwen-14B as Critic RefCritic-Qwen-14B(Ours) SF RLλ=0 RLλ=0 af ter RLλ= LiveCodeBench GPQA assr@1 assr@1 ajc@16 18.9 20.9 21.5 21. 22.9 51.0 52.4 52.3 53.6 54.1 19.5 19.5 19.2 18. 20.0 58.7 57.7 58.0 59.0 59.3 23.3 22.7 22.8 24. 24.3 61.6 60.6 62.5 64.6 65.1 Table 4: Performance comparison of different approaches on LiveCodeBench and GPQA. proved ajc@64 from 61.6% to 65.1% on the GPQA task, representing 3.5% performance gain. Similar progress also appeared in RefCritic-Qwen14B. These results suggest that RefCritics critic capabilities can be applied to wide range of tasks."
        },
        {
            "title": "5.4 Critic Performance",
            "content": "In this section, we evaluate RefCritic on ProcessBench to explore whether it can accurately identify true error locations in solutions. The experimental results presented in Table 3 demonstrate that RefCritic significantly outperforms most previous baselines, including methods that utilize step-level supervision. RefCritic-Qwen achieves an average performance of 68, while RefCritic-R1 reaches an impressive 77 average performance. This indicates 7 that our dual reward mechanism effectively guides the model in developing accurate error identification capabilities. This finding is consistent with the growth of the model output length during RL training. Specifically, the average output length of RefCritic-Qwen increased from about 500 tokens to 3500 tokens, while RefCritic-R1 increased from 3000 tokens to 8000 tokens. This indicates the increasingly detailed critiques, making step-level critique possible. These findings demonstrate that even without explicit step-level supervision, our approach enables critic models to develop nuanced understanding of solution processes and identify errors with high precision. This capability is crucial for generating actionable feedback that can effectively guide policy models toward improved solutions."
        },
        {
            "title": "5.5 Test-time Scaling",
            "content": "Scaling on Critique In this section, we investigate whether RefCritics critique capabilities can benefit from test-time scaling. We construct two evaluation sets by sampling 16 solutions for each problem from AIME25 and Olympiad. For each question, we sample 64 critiques from RefCritic to examine whether increasing the number of critiques can progressively enhance critique performance. The results presented in Figure 3 (a) demonstrate that RefCritics critique performance exhibits consistent improvement with increased sampling. On AIME25, performance steadily improves, reaching 4% increase as the number of sampled critiques increases. This finding suggests that RefCritic benefits from test-time scaling when multiple critiques are aggregated, thereby further enhancing its ability to evaluate mathematical solutions accurately. The scaling curve on Olympiad is relatively flat in comparison, which may be due to the difficulty of the tasks for the policy model (but clear scaling trend can still be observed). Scaling on Problem Solving Figure 3 (b) and (c) illustrate the scaling performance of RefCriticQwen-14B and RefCritic-R1-14B on AIME25 as measured by maj@N with increasing rollout samples. The results demonstrate that RefCritic models consistently outperform baselines across different sampling scales, with the performance gap becoming more pronounced as the number of samples increases. This trend further validates that our refinement-oriented critic approach maintains its effectiveness advantage even in larger-scale infer-"
        },
        {
            "title": "Model",
            "content": "AIME25 AIME24 Olympiad assr@1 assr@1 assr@1 Qwen-14B as Critic RefCritic-Qwen-14B(Ours) SF RLλ=0 RLλ=1 RLλ=0 RLλ=0 af ter RLλ=0 af ter RLλ= 14.5 15.0 18.5 19.5 19.6 21.2 13.7 15.2 19.1 21. 21.7 23.0 45.8 46.6 51.4 54.3 53.6 55. Table 5: Ablation results on RefCritic-Qwen-14B. ence scenarios."
        },
        {
            "title": "5.6 Supervision of Stronger Models",
            "content": "In this section, we investigate whether RefCritic can effectively provide cross-model supervision for even more powerful reasoning models. We evaluate our approach using state-of-the-art models such as QwQ, DeepSeek-Distill-Qwen-32B, and Qwen 2.5-72B on the challenging AIME25 dataset. We compare three settings: (1) standard majority voting with the base models, (2) self-critique where the models evaluate their own solutions, and (3) cross-model supervision using our RefCritic-14B as the critic model. As shown in Figure 2, even the most powerful reasoning and instruct models exhibit minimal or sometimes negative performance gains when employing self-critique compared to standard majority voting. This observation suggests consistent limitation in these models ability to critically evaluate their own solutions, regardless of their scale or overall reasoning capabilities. In contrast, our RefCritic approach consistently demonstrates positive improvements across nearly all experimental settings, even when supervising significantly larger and more capable reasoning models. Specifically, with 32 samples, RefCritic supervision improves QwQ performance by 1.5% compared to standard majority voting and by 1.1% compared to QwQs self-critique approach. Similar patterns emerge for DeepSeek-Distill-Qwen-32B and Qwen 2.5-72B, confirming that RefCritics benefits extend across model families and scales."
        },
        {
            "title": "5.7 Ablation",
            "content": "Finally, we also propose some ablation studies to understand the role of the two RL training stages in RefCritic, namely λ=0 and λ=1. Considering the training cost, we only conduct experiments on RefCritic-Qwen-14B, and ablations on DeepSeekDistill-Qwen-14B will be added in future research. 8 (a) Qwen2.5-72B Performance (b) DeepSeek-Distill-Qwen-32B Performance (c) QwQ-32B Performance Figure 2: Supervision of RefCritic-R1-14B on stronger models like Qwen2.5-72B, DeepSeek-Distill-Qwen-32B, and QwQ-32B. (a) Critique Scaling on RefCritic. (b) Scaling on RefCritic-Qwen. (c) Scaling on RefCritic-R1. Figure 3: Test-time scaling with RefCritic. Subplot (a), scaling the number of critiques with RefCritic. Subplot (b), scaling the number of sampled solutions with RefCritic-Qwen-14B on AIME24. Subplot(c), scaling the number of sampled solutions with RefCritic-R1-14B on AIME25. ages dual-reward system that jointly optimizes for judgment accuracy and refinement effectiveness, creating an explicit feedback loop between critique quality and policy model improvement. Our experiments demonstrated that while SFT alone is insufficient for producing comprehensive critiques despite generating better critiques, the integration of reinforcement learning with our designed reward signals significantly enhances both the analytical depth and practical utility of critiques. Experimental results across challenging mathematical datasets and out-of-distribution benchmarks validate RefCritics effectiveness in consistently enhancing policy model performance in both critique-refinement and majority vote settings. Further experiments on ProcessBench demonstrate that even without step-level signal, RefCritic can effectively identify the error step. Specifically, we aim to explore the importance of Refinement Reward. To this end, we mainly compared two groups of experiments: 1) RLλ=0 af ter RLλ=0 and RLλ=0 and RLλ=1. 2) RLλ=0 af ter RLλ=1. Each group of experiments is optimized with the same parameters. The results of all these ablation experiments are shown in Table 5. As expected, under the same settings, refinement reward improves the refinement performance of models. Furthermore, first using RLλ=0 for Critic optimization is also beneficial to RefCritic. RLλ=0 can quickly improve Critic performance at lower af ter cost, making RLλ=0 RLλ=1 setting that balances cost and performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced RefCritic, novel approach for training critic models to critique the correctness of solutions and provide effective refinement feedbacks from LLMs. Our method lever-"
        },
        {
            "title": "Limitations",
            "content": "Despite the promising results, RefCritic has several limitations. The dual-reward reinforcement learning framework requires significant computational resources, which may limit its scalability for very large models. Our approach primarily focuses on mathematical and logical reasoning tasks, and its generalizability to domains like commonsense reasoning or specialized professional contexts remains to be thoroughly investigated."
        },
        {
            "title": "References",
            "content": "Bo Zheng, Yibo Miao, Shanghaoran Quan, and 5 others. 2024. Qwen2.5-coder technical report. Preprint, arXiv:2409.12186. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. arXiv 2024. Critique-out-loud reward models. preprint arXiv:2408.11791. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. 2025. Process reward models that think. Preprint, arXiv:2504.16828. Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative rerankIn Proceedings of the 43rd Annual Meeting ing. of the Association for Computational Linguistics (ACL05), pages 173180. Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, and Xian-Ling Mao. 2024. Criticeval: Evaluating large-scale language model as critic. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, and Kwan-Yee K. Wong. 2025. Spc: Evolving selfplay critic via adversarial games for llm reasoning. Preprint, arXiv:2504.19162. Sijia Chen, Baochun Li, and Di Niu. 2024. Boosting of thoughts: Trial-and-error problem solving with large language models. arXiv preprint arXiv:2402.11140. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, and Bo Zheng. 2025. Can large language models detect errors in long chain-of-thought reasoning? Preprint, arXiv:2502.19361. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numinamath. [https://huggingface.co/ AI-MO/NuminaMath-1.5](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf). Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. 2024. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. 2025. Inference-time scaling for generalist reward modeling. Preprint, arXiv:2504.02495. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, JanPhilipp Fränken, Chelsea Finn, and Alon Albalak. 2024. Generative reward models. arXiv preprint arXiv:2410.12832. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 2024. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215. 10 David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024. Generative verifiers: Reward modeling as next-token prediction. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, and Le Sun. 2025. Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thought critic. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Wenlei Shi and Xing Jin. 2025. Heimdall: test-time scaling on the generative verification. Preprint, arXiv:2504.10337. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, and Junyang Lin. 2025a. Enabling scalable oversight via self-evolving critic. Preprint, arXiv:2501.05727. Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, and Junyang Lin. 2025b. Realcritic: Towards effectiveness-driven evaluation of language model critiques. Preprint, arXiv:2501.14492. Qwen Team. 2025. Qwq-32b: Embracing the power of reinforcement learning. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcomebased feedback. arXiv preprint arXiv:2211.14275. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. 2023. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025a. Qwen3 technical report. Preprint, arXiv:2505.09388. Wenkai Yang, Jingwen Chen, Yankai Lin, and Ji-Rong Wen. 2025b. Deepcritic: Deliberate critique with large language models. Preprint, arXiv:2505.00662."
        },
        {
            "title": "A Data Construction",
            "content": "We filtered about 120k problems from the 900k mathematical problems of NuminaMath-1.5 (LI et al., 2024). Detailed filter process and utilization can be found in the section A. Our training data pipeline involves rigorous filtering to ensure high-quality and diverse mathematical problems. Problem Deduplication We start with deduplication process on the 900k mathematical problems from NuminaMath-1.5 (LI et al., 2024). The deduplication process includes the string-based process by performing exact matching after removing special characters such as $, [, ], etc., and semantic deduplication, where we used gte-multilingual-base embeddings to compute cosine similarity between problem pairs and removing those with similarity scores exceeding 0.95. Problem Filter Then, we utilize Qwen2.5-72B-Instruct as judge to filter problems based on several criteria: unsolvable problems, proof problems requiring formal mathematical proofs, and multiplechoice problems. We sample eight solutions for each problem with DeepSeek-Distill-Qwen-7B to ensure appropriate difficulty distribution. We remove problems where DeepSeek-Distill-Qwen-7B either solves all eight attempts correctly or fails on all eight attempts, thus eliminating trivial or impossibly difficult issues. After this comprehensive filtering process, we obtain approximately 120k high-quality mathematical problems for training. All prompt templates are provided in the Appendix. Solution Sampling To create training data for critic models, we sample 8 responses from the policy model for each problem, remove problems where all solutions are correct or incorrect, filter out incomplete generations, and ensure balanced training by retaining at most two responses per problem (one correct and one incorrect). For efficient scaling, all responses are sampled by sglang inference services4."
        },
        {
            "title": "B Templates",
            "content": "Given student's mathematical solution, analyze it step-by-step to determine correctness. Do not solve the problem yourself, provide feedback focus on the student's work to help them learn. Conclude your feedback as: **Correctness**: Correct Incorrect (If incorrect) **Comment**: Identify the specific error in the solution and help the student recognize why their approach leads to an incorrect result. Then, provide comment that will help the student to resolve this problem. Do not expose any answer! [Problem] {problem} [Solution] {solution} Table 6: The template we used for critique. 4https://github.com/sgl-project/sglang 12 Review your solution to mathematical problem and feedback from your teacher. Create an improved version that fixes the identified errors. Please reason step by step, and put your final answer within boxed{{}}. [Problem] {problem} [Original Solution] {solution} [Teacher Feedback] {critique} Table 7: The template we used for refinement. The following is math problem and solution (split into paragraphs, enclosed with tags and indexed from 0): [Math Problem] {problem} [Solution] {solution} Your task is to review and critique the solution paragraph by paragraph. Once you identify an error in paragraph, return the index of the paragraph where the earliest error occurs. Otherwise, return the index of -1 (which typically denotes \"not found\"). Please put your final answer (i.e., the index) in boxed{}. Table 8: The template we used for evaluating processbench."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
    ]
}