{
    "paper_title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
    "authors": [
        "Alexander Goslin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits."
        },
        {
            "title": "Start",
            "content": "Terrain Diffusion: Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation Alexander Goslin alexander.goslin@gmail.com Website: https://xandergos.github.io/terrain-diffusion 5 2 0 2 9 ] . [ 1 9 0 3 8 0 . 2 1 5 2 : r Figure 1: region of world generated with Terrain Diffusion. The leftmost panel spans roughly five million square kilometers, with about 2.2 million square kilometers of land area, comparable to the size of the Congo. Red boxes denote the region shown in the next panel, illustrating coherent terrain generation across four orders of magnitude in scale. Zoom for details. Abstract For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seedconsistency, and constant-time random access. At its core is InfiniteDiffusion, novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. hierarchical stack of diffusion models couples planetary context with local detail, while compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits."
        },
        {
            "title": "1 Introduction\nProcedural terrain generation underpins the creation of virtual\nworlds, from open-world games to planetary simulations. For\nnearly four decades, procedural noise functions such as Perlin noise\n[20, 21] have defined this field. They offer three properties that make\nthem indispensable for procedural worlds: seamless infinite extensi-\nbility, seed-consistency, and constant-time random access. A single\nrandom seed can deterministically produce a boundless landscape\nwithout storing vast datasets, providing an elegant foundation for\nprocedural worlds.",
            "content": "Yet these procedural methods are inherently limited. Their patterns are smooth and lack the hierarchical organization of real geography. Continents, mountain ranges, and river basins emerge in nature from structured, multi-scale processes that simple noise cannot capture. As result, worlds built from procedural noise often appear plausible but not real. Recent advances in generative modeling, particularly diffusion models [9, 23], have transformed image synthesis by learning to reproduce natural structure with remarkable realism and control, but these methods are typically confined to smaller bounded domains. Recent work has explored infinite or large-scale generation capabilities, but these approaches generally lose one or more of the core properties that make procedural noise valuable in interactive applications. Terrain Diffusion bridges this divide, introducing learned successor to procedural noise that preserves all its core properties: seamless infinite extensibility, seed-consistency, and constant-time random access. To do so, we generalize MultiDiffusion [1] for infinite inference, enabling real-time, coherent, procedural generation. hierarchical diffusion stack unifies global and local structure: coarse planetary model establishes continental structure, refined by higher-resolution model that introduces mountain ranges, valleys, and local relief. novel elevation encoding further stabilizes training and inference across the full dynamic range of Earths terrain. Few-step consistency distillation [18, 24] enables real-time inference, and an open-source infinite-tensor framework enables constant-memory streaming and composable manipulation of infinite tensors. Together, these components establish the first learned system capable of streaming an entire planet in real time on consumer GPUs. Terrain Diffusion demonstrates that diffusion models can serve as practical foundation for infinite, seed-consistent worlds that can be explored interactively, in real-time, and without restrictions."
        },
        {
            "title": "2 Related Works\nProcedural noise. Procedural terrain traditionally relies on proce-\ndural noise such as Perlin or Simplex noise [20, 21], often combined\nwith fractal Brownian motion (fBm) [6]. These methods remain pop-\nular for their controllability, speed, seed-consistency, and infinite\ntileability, but they lack the large-scale structure of real landscapes.\nThey produce repetitive, texture-like patterns and cannot reproduce\ncomplex features such as mountain ranges, branching valleys, or\nvolcanic and glacial landforms without extensive post-processing.",
            "content": "Diffusion and consistency models. Denoising diffusion models [9, 23] generate samples by iteratively refining noise and are widely used for high-fidelity synthesis. Consistency models [24] approximate denoised diffusion outputs in one or few steps, and continuous variants [18] achieve throughput competitive with GANs while retaining most of the quality of full diffusion sampling, enabling real-time use cases. Learned terrain generation. GAN-based terrain models [2, 7, 25, 26] can generate convincing local relief, but they operate on fixed crops and do not tile, limiting them to bounded worlds. Diffusionbased terrain and Earth imagery synthesis [3, 8, 10] further improves fidelity and control, but also assumes finite canvases and requires relatively significant compute. Sharma et al. [22] achieves 1024 super-resolution with strong global coherence, yet still targets bounded images, does not run in real time, and does not generate elevation. Jain et al. [11] is closest to our work, offering infinite terrain generation by sampling diffusion-based tiles and blending them with Perlin-based kernel. Because tiles are generated independently and the kernel has no awareness of broader context, global coherence and large scale structure remain tied to procedural noise rather than the learned model. In contrast, Terrain Diffusion couples all tiles through shared global context and fuses tiles through fully learned, context-aware mechanism. Procedural noise is used only for continental-scale structure, where data is sparse and simple enough that more complex alternatives would provide little benefit while reducing user control. MultiDiffusion and unbounded generation. Several works extend generative models beyond image bounds seen in training. InfinityGAN [17] produces infinite images with GANs but does not extend to diffusion models, limiting scalability. MultiDiffusion [1] and Mixture of Diffusers [12] generate images larger than the training canvas but still assume bounded final extent. BlockFusion [27] and WorldGrow [16] generate worlds by conditioning each tile on its neighbors, producing continuous worlds but without seed consistency, since outputs depend on sampling order. In contrast, Terrain Diffusion defines seed-consistent InfiniteDiffusion algorithm whose outputs are order invariant and allow constant-time random-access generation over an infinite domain."
        },
        {
            "title": "Across Planetary Scales",
            "content": "MultiDiffusion [1] provides simple and effective way to extend diffusion sampling beyond models native resolution by averaging predictions from overlapping windows. This enables synthesis across larger images, as local predictions fuse into seamless image. However, in its standard form, MultiDiffusion remains confined to bounded domains: all windows must lie within fixed finite canvas, limiting its applicability to unbounded worlds or continuously streamed environments. We introduce InfiniteDiffusion, an extension of MultiDiffusion that lifts this constraint. By reformulating the sampling process to operate over an effectively infinite domain, InfiniteDiffusion supports seamless, consistent generation at scale. The remainder of this section reviews the principles of MultiDiffusion, and formalizes its extension to unbounded domains. We present the definitions in Z2 for clarity, but all results extend to Zğ‘‘ with minimal modification."
        },
        {
            "title": "3.1 A Review of MultiDiffusion\nMultiDiffusion extends standard diffusion sampling by averaging\noverlapping windows, producing consistent outputs from local\npredictions. Each denoising step aggregates the predictions of over-\nlapping patches, enforcing continuity across window boundaries\nand allowing generation of regions much larger than a modelâ€™s\ninput size. The process begins with a pretrained diffusion model\nÎ¦, operating on images in I = Rğ» Ã—ğ‘Š Ã—ğ¶ . The diffusion process\ngenerates a sequence of images",
            "content": "ğ¼ğ‘‡ , ğ¼ğ‘‡ 1, . . . , ğ¼0 s.t. ğ¼ğ‘¡ 1 = Î¦(ğ¼ğ‘¡ ğ‘¦) that refines the original noisy image ğ¼ğ‘‡ into fully denoised version ğ¼0, under conditioning vector ğ‘¦. MultiDiffusion defines new model Î¨ that generates in different image space = Rğ» ğ‘Š ğ¶ , producing new sequence of images ğ½ğ‘‡ , ğ½ğ‘‡ 1, . . . , ğ½0 s.t. ğ½ğ‘¡ 1 = Î¨(ğ½ğ‘¡ ğ‘§). To accomplish this, MultiDiffusion defines ğ‘› windows indexed by ğ‘– [ğ‘›]. In the finite setting, region ğ‘… is any rectangular subset of the ğ» ğ‘Š coordinate grid, while window region has fixed size ğ» ğ‘Š . In MultiDiffusion, each window ğ‘– is assigned window region ğ‘…ğ‘– . For an image ğ½ , we write ğ½ [ğ‘…] for the values of ğ½ on the coordinates in ğ‘…. Each window also has weight matrix ğ‘Šğ‘– Rğ» ğ‘Š that specifies the relative contribution of each pixel in the pretrained diffusion models output. Let ğ‘ˆğ‘– (ğ‘¥) denote the ğ» ğ‘Š image that places an ğ» ğ‘Š tensor ğ‘¥ in the region ğ‘…ğ‘– with zeros elsewhere. With these definitions, the closed form MultiDiffusion update for direct pixel or latent-space samples is Î¨(ğ½ğ‘¡ ğ‘§) = (cid:205)ğ‘› ğ‘–=1 ğ‘ˆğ‘– (ğ‘Šğ‘– Î¦(ğ½ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ğ‘– )) ğ‘—=1 ğ‘ˆ ğ‘— (ğ‘Šğ‘— ) (cid:205)ğ‘› (1) where denotes the Hadamard product. This expression represents weighted average of all local denoising predictions, where each window contributes according to its weight map. The result is global update that reconciles all overlapping diffusion paths into single image. Although MultiDiffusion elegantly unifies local diffusion paths, it remains constrained to bounded domains: the process assumes finite number of windows and requires the pretrained diffusion model to be evaluated at all windows to complete one step. Extending the same principle to infinite domains therefore requires reformulating Î¨ so that it operates locally and independently of global window layouts, key step towards the InfiniteDiffusion algorithm introduced next. 2 in tiles, which can be loaded and unloaded as necessary to conserve memory. When query ğ½ğ‘¡ [ğ‘…] occurs, we identify all the windows required to generate the region, and process all previously unprocessed windows, populating the desired regions of ğ´ğ‘¡ and ğµğ‘¡ . Then ğ½ğ‘¡ [ğ‘…] = ğ´ğ‘¡ [ğ‘…]/ğµğ‘¡ [ğ‘…]. Final generation proceeds as recursive process that begins by sampling ğ½ğ‘‡ as Gaussian noise. ğ½0 [ğ‘…] is obtained by recursively applying the query routine at all earlier steps. In summary, the routine below computes ğ½ğ‘¡ 1 [ğ‘…] by evaluating only the windows that overlap ğ‘…, caching each windows contribution in ğ´ğ‘¡ 1 and ğµğ‘¡ 1 so that future queries can reuse the same results. The tensors ğ´ğ‘¡ 1, ğµğ‘¡ 1, and the set of processed windows ğ‘ƒ are mutated in-place. Algorithm 1 Querying ğ½ğ‘¡ 1 [ğ‘…] with InfiniteDiffusion. See Fig. 2 Inputs: pretrained diffusion model Î¦ infinite noisy input image ğ½ğ‘¡ ğ´ğ‘¡ 1 infinite accumulated output image ğµğ‘¡ 1 infinite accumulated weights for ğ´ğ‘¡ 1 ğ‘… ğ‘ƒ region to query set of processed windows for each window ğ‘– in ğœ… (ğ‘…) ğ‘ƒ do ğ´ğ‘¡ 1 [ğ‘…ğ‘– ] ğ´ğ‘¡ 1 [ğ‘…ğ‘– ] + ğ‘Šğ‘– Î¦(ğ½ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ğ‘– ) ğµğ‘¡ 1 [ğ‘…ğ‘– ] ğµğ‘¡ 1 [ğ‘…ğ‘– ] + ğ‘Šğ‘– end for ğ‘ƒ ğ‘ƒ ğœ… (ğ‘…) Output: ğ½ğ‘¡ 1 [ğ‘…] = ğ´ğ‘¡ 1 [ğ‘…]/ğµğ‘¡ 1 [ğ‘…] There is only one additional challenge in making InfiniteDiffusion practical in real-world applications. Each query ğ½ğ‘¡ 1 [ğ‘…] typically requires region of ğ½ğ‘¡ larger than ğ‘… itself, as pixels near the edge of ğ‘… are generated by windows that extend beyond ğ‘…. In the sliding-window case, regions grow quadratically in area, yielding an overall ğ‘‚ (ğ‘‡ 3) time complexity for final generation. While this prevents the use of longer noise schedules, we find that the method remains both fast and effective even when ğ‘‡ is near 1, provided each ğ‘¦ğ‘– captures sufficient context. In practice, this is achieved by either using few-step model, such as consistency model, or simulating one by running several diffusion steps at time."
        },
        {
            "title": "3.4 Properties of InfiniteDiffusion\nFormal proofs for all properties are provided in appendix A",
            "content": "Seed consistency. central property of InfiniteDiffusion is that it preserves the seed-consistent behavior that makes procedural noise functions useful for procedural world generation. Seed consistency follows directly from how the process is defined. Once seed is fixed, the initial noise image ğ½ğ‘‡ is completely determined. The query routine is deterministic function of its inputs, so ğ½ğ‘¡ 1 is deterministic function of ğ½ğ‘¡ , and caching only memoizes intermediate results of this deterministic computation. This argument requires that the conditioning variables ğ‘¦ğ‘– are themselves seed-consistent, which is typically trivial when ğ‘¦ğ‘– is constant or generated by the same InfiniteDiffusion algorithm, as in this work. By composing 3 Figure 2: conceptual visualization of InfiniteDiffusion with sliding windows. The users query ğ½0 [ğ‘…] induces deterministic chain of window queries: computing ğ½0 [ğ‘…] requires region ğ½1 [ğ‘…0], which in turn requires region ğ½2 [ğ‘…1]. Querying ğ½2 [ğ‘…1] is inexpensive since it corresponds directly to Gaussian noise."
        },
        {
            "title": "3.2 From MultiDiffusion to InfiniteDiffusion\nWe now seek to extend MultiDiffusion beyond finite image domains.\nWe first redefine the MultiDiffusion image space as an unbounded\nimage, J = RZÃ—ZÃ—ğ¶ , so that generation produces an infinite output.\nConsequently, we now define a region to be any rectangular subset\nof Z2. Since generation is now over an infinite domain, window\nindices must now range over a countably infinite set ğ‘†.",
            "content": "For all applications shown in this work, we take ğ‘† = Z2, so each window is indexed by (ğ‘–, ğ‘—), and each window region ğ‘…ğ‘– ğ‘— is defined as square sliding window with side length ğ» = ğ‘Š and stride ğ‘  on both axes. Concretely, ğ‘…ğ‘– ğ‘— = [ğ‘–ğ‘ , ğ‘–ğ‘  + ğ» ) [ ğ‘—ğ‘ , ğ‘—ğ‘  + ğ‘Š ). This particular layout is not essential to the InfiniteDiffusion formulation and serves only as an implementation choice for the experiments. With an infinite number of windows, the MultiDiffusion update becomes intractable, and computation requires an infinite sum to produce the final image. Instead, we seek to generate the image lazily, by only evaluating particular regions ğ‘…. To achieve this, we define ğœ… to be the function mapping region to the set of window indices that overlap it. We assume ğœ… (ğ‘…) is always finite. This enables the InfiniteDiffusion update Î¨(ğ½ğ‘¡ ğ‘§) [ğ‘…] = (cid:18) (cid:205)ğ‘– ğœ… (ğ‘…) ğ‘ˆğ‘– (ğ‘Šğ‘– Î¦(ğ½ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ğ‘– )) (cid:205)ğ‘— ğœ… (ğ‘…) ğ‘ˆ ğ‘— (ğ‘Šğ‘— ) (cid:19) [ğ‘…], (2) which is the MultiDiffusion update with only the windows intersecting ğ‘… evaluated. In the finite setting, the full image ğ½ğ‘¡ can be generated in advance, making each ğ½ğ‘¡ [ğ‘…ğ‘– ] effectively free. In the infinite setting, precomputing ğ½ğ‘¡ is impossible, so evaluating ğ½ğ‘¡ [ğ‘…ğ‘– ] requires recursively invoking the same update. direct implementation would therefore incur exponentially growing compute, motivating an alternative strategy."
        },
        {
            "title": "3.3 Practical Querying of InfiniteDiffusion\nTo make queries practical, we avoid recomputing the same win-\ndow updates across recursive calls. Instead, for each image ğ½ğ‘¡ we\nmaintain two corresponding infinite tensors: ğ´ğ‘¡ , which stores the\nnumerator in Eq. 2, and ğµğ‘¡ , which stores the denominator. Both ten-\nsors are initialized to zero. These infinite tensors are typically stored",
            "content": "these steps, every image ğ½ğ‘¡ and every region ğ½ğ‘¡ [ğ‘…] becomes fixed function of the original seed. In particular, requesting the same region again, or in different order relative to other regions, always returns exactly the same value, since the computation depends only on the seed and the region, not on the sequence of queries. Constant-time random access. Assuming that ğœ… (ğ‘…ğ‘– ) ğ‘€ for any window region ğ‘…ğ‘– , InfiniteDiffusion guarantees constant-time random access. In particular, for any window region ğ‘…ğ‘– , the query ğ½0 [ğ‘…ğ‘– ] is ğ‘‚ (1). In practice, this allows discontinuous exploration of the world: any region can be queried independently, and seed consistency ensures that skipping intermediate tiles never alters the generated content or its quality. Parallelization. InfiniteDiffusion also admits parallel evaluation of window updates. For any fixed timestep ğ‘¡, each evaluation of Î¦ is independent, so they can be batched and executed in parallel."
        },
        {
            "title": "3.5 An Open Source Infinite Tensor Framework",
            "content": "for Unbounded Inference To support unbounded generation without exceeding memory limits, we introduce the Infinite Tensor framework, Python library that enables sliding window computation over tensors with infinite dimensions. It allows models to process arbitrarily large images as if they were standard PyTorch tensors while keeping only the visible region in memory. Each operation is performed through fixed-sized sliding window that dynamically loads and evicts data as sampling progresses, permitting inference on arbitrarily large scenes with constant memory use. This abstraction lets diffusion and consistency models operate directly on infinite images without manual data management. Windows can overlap to provide context and blend results, and multiple infinite tensors can depend on one another to form hierarchical pipelines. The framework serves as the runtime layer that links local model inference with practical, global world synthesis. Together, InfiniteDiffusion and the Infinite Tensor framework provide the foundation required for practical, unbounded generation. The remaining components of Terrain Diffusion build on this foundation by combining these capabilities with large-scale real-world training data, hierarchical modeling, and task-specific architecture."
        },
        {
            "title": "4 Data\nOur dataset combines multiple global sources to provide consistent\ncoverage of both land and ocean. Land elevations are drawn from\nthe 3-arc-second MERIT DEM [28], while ocean bathymetry is\ntaken from the 30-arc-second ETOPO dataset [19]. Since ETOPOâ€™s\nresolution is lower, it is blurred and upsampled to match MERITâ€™s\nresolution before merging. To ensure smooth coastal transitions, we\nmeasure the distance of each ocean pixel from the nearest coastline\nand linearly interpolate elevation from 0 m at the shore to the local\nocean depth 100 pixels offshore. To support climatic conditioning,\nwe supplement elevation with 30-arc-second WorldClim [5] data\nfor temperature and precipitation.",
            "content": "For efficient processing, data is downloaded and stored in contiguous 20482048 tiles, each covering equal surface area at approximately 90m resolution. To maintain uniform ground resolution, tiles Figure 3: Effects of the signed-sqrt transform. Standard deviation become more uniformly distributed with respect to mean elevation, and the range of standard deviations compress. are stretched in longitude so that each pixel represents roughly the same area regardless of latitude. This equal-area tiling ensures consistent scale across training and allows all models to train on data without distortion. Finally, 80% of the tiles are randomly assigned to the train set, and the remainder are withheld for validation. All models are trained on random crops drawn from random tiles, with sampling biased so that 99% of tiles contain at least 1% land, as ocean regions are simpler and lower priority. Each crop is randomly flipped and rotated in 90 degree increments to reflect our goal of generating infinite, directionless terrain, similar in spirit to Perlin noise."
        },
        {
            "title": "5.2 Stabilization Via Laplacian Encodings\nDue to normalization, the large dynamic range of Earth elevations\nmake model errors deceptively large in absolute units. Without\nthe signed-sqrt transform, even relatively small errors of ğœ = 0.01\ncan correspond to Â±25m noise after denormalization. Applying the\ntransform lessens the issue at lower elevations, but worsens it at\nhigher ones. While this noise is less obvious in images, it can become\nvery apparent in interactive settings, where the noise is potentially\nmuch larger than the viewer or the surrounding scenery. To mitigate",
            "content": "4 (a) Initial Input (b) Refined Coarse Map (c) Final 90m Elevation Figure 4: Multi-stage elevation generation pipeline. (a) The initial coarse map, which serves as the structural and climatic guide. Can be made by hand or generated procedurally. (b) The refined coarse map, enhanced by our lightweight coarse model to enforce realism. (c) The final 90m elevation map generated by the core latent diffusion model, with InfiniteDiffusion for tiling. this, we predict Laplacian-based representation comprising lowfrequency component, obtained by downsampling and blurring the original image, and residual/high-frequency component given by subtracting the upsampled low frequency component from the original image. Residual errors are over 30 smaller in magnitude due to their lower variance. To clean the low-frequency channel after generation, we decode the noisy lowand high-frequency components (ğ¿ + ğ» ) into provisional heightmap, then blur and downsample it to re-extract denoised low-frequency Ë†ğ¿, with any high-frequency noise redirected to the residual Ë†ğ» . Final synthesis uses Ë†ğ¿ +ğ» , so lowfrequency errors vanish while high-frequency detail is preserved. In practice, ğ¿ Ë†ğ¿ even under strong synthetic noise, confirming that re-extraction cleanly isolates low-frequency structure. This Laplacian denoising step reduces the FID of the untiled core diffusion model (introduced next) from 23.21 to 9.33, and the corresponding consistency model from 75.75 to 12.71."
        },
        {
            "title": "5.3 A Hierarchical Model\nPlanetary terrain spans several orders of magnitude in scale, from\ncontinental structure to meter-level detail, making one-pass gen-\neration infeasible. Several previous works [4, 15, 22, 29, 30] have\nshown that MultiDiffusion produces incoherent and repetitive re-\nsults when poorly conditioned, and InfiniteDiffusion does not na-\ntively provide a solution to this. We therefore organize generation\ninto a small hierarchy of models operating at progressively finer\nresolutions. Each stage refines and conditions on the one above,\nmaintaining planetary coherence while producing realistic local\ndetail. All models share a common EDM2 [14] backbone with the\nmodifications proposed in sCM [18]. The hierarchy begins with\na coarse planetary model, which generates the basic structure of\nthe world from a rough, procedural or user-provided layout. The\nnext stage is the core latent diffusion model, which transforms that\nstructure into realistic 46km tiles in latent space. Finally, a consis-\ntency decoder expands these latents into a high-fidelity elevation\nmap. We visualize the coarse-to-fine pipeline in Figure 4.",
            "content": "5 The core latent diffusion model synthesizes 512512 patches at 90m resolution in signed-sqrt space, corresponding to 46km tiles. It predicts 64x64 low-frequency elevation channel and latent map that compactly represents the corresponding residual. To supply the latent codes, we train separate VAE-style autoencoder that shares the same U-Net backbone but omits diffusion-specific components. The model is optimized using L1 and LPIPS losses with weak KL term to prevent overfitting. After training, the encoder processes each 20482048 tile in the dataset as whole, and the resulting latent image is precomputed and stored alongside the tile. To maximize local quality, the imprecise VAE decoder is discarded, and diffusion decoder learns to expand these latents into realistic and high-resolution residuals. During training of all models, we draw random crops from the latent image to learn nearly translation-invariant representation, reflecting the fact that generation should be independent of absolute location. Conditioning is implemented by concatenating nearest-neighbor interpolated latents to the noisy input image at each diffusion step. We train both the autoencoder and diffusion decoder on 128128 crops, but find the models generalize well when applied to higher-resolution patches. To facilitate long-range global coherence, the core model is conditioned on 4x4 patches of elevation data. Each pixel of the patch is about 23km, with the model prediction corresponding to the 2x2 interior. Each patch contains 3 channels: the mean elevation of the pixel, the 5th percentile elevation of the pixel, and binary mask indicating which pixels have data available. We also provide the model with the tiles mean temperature, temperature variation, annual precipitation, and precipitation seasonality for additional coherence and user control. Since climatic data is not available in the ocean, we replace missing values with standard gaussian, ensuring the model accepts any combination of climatic variables in ocean regions. Additionally, we optionally condition on target distribution over tile \"beauty\" scores. linear regressor trained on 150 handlabeled examples and hand-crafted Fourier-based features (ğ‘…2 0.8) assigns each tile beauty value; during training, we generate random score histograms, sample random score from it, select an appropriate tile, and feed the target distribution as conditioning, which later allows users to bias synthesis toward more or less visually striking terrain. All conditioning data is provided as flat tensor, and injected into the model alongside the noise embedding."
        },
        {
            "title": "5.4 Real-Time Planetary Scale Synthesis\nWhile the 46 km tiles are realistic in isolation, large-scale coherence\nrequires conditioning on broader planetary context. To provide\nthis, we introduce a compact coarse diffusion model that generates\nthe channels required for conditioning the core diffusion model.\nThe user provides initial maps for these variables using procedural\nnoise or hand-drawn sketches. During inference, these inputs are\ncorrupted with gaussian noise according to the userâ€™s preference on\na per-channel basis, and concatenated against the usual diffusion\ninputs. The model follows the EDM2 design but with no down-\nsampling or upsampling operations. This limits the receptive field\nby design, preventing the model from drifting toward the massive\ncontinental structures present in Earth data and avoiding conflicts\nwhere global priors override user guidance while still allowing\nstrong local corrections.",
            "content": "To support real-time synthesis, all diffusion models, except the coarse model, are distilled into continuous-time consistency models [18]. To improve fidelity further, we apply the guidance scheme proposed in AutoGuidance [13]. Combined, these stages form continuous generation pipeline, from planetary context to local detail, capable of on-demand, real-time synthesis."
        },
        {
            "title": "6.1 Visual Fidelity\nFor quality, we report FID-50k for (1) non-tiled diffusion samples,\n(2) non-tiled consistency samples, and (3) tiled samples generated\nwith InfiniteDiffusion1. This isolates base fidelity, the effect of con-\nsistency distillation, and the effect of tiling. Evaluations use raw\nelevation values, with images normalized by centering each tile\nand scaling by the larger of its value range or 255, ensuring that\nimages are not expanded beyond the native precision of the data.\nAll results use two-step generation for both the core consistency\nmodel and the consistency decoder. Table 1 summarizes our results.\nFor context, we also measure the decoderâ€™s standalone rFID\nat 512 Ã— 512 resolution. The one step variant obtains an rFID of\n3.08, while the two step variant reaches 1.07. The two step model\nuses a small intermediate noise level of ğœ = 0.065, indicating that",
            "content": "1FID is computed on central 984984 crops of the validation tiles to ensure adequate surrounding context. InfiniteDiffusion is applied only to the latent space; tiling the decoder would require impractically large context, and the decoder already produces near seamless outputs. the second step performs only minor refinements that may be unnecessary for speed sensitive applications. Table 1: FID-50k comparison for central crops generated with InfiniteDiffusion vs. direct generations. Lower is better. Model Type Tiling None Diffusion Consistency None InfiniteDiffusion Consistency FID 9.34 12.71 17."
        },
        {
            "title": "6.2 Real Latency: Time to First and Second Tile\nBecause generation of any fixed-size region has bounded cost, gen-\nerating a contiguous 512 Ã— 512ğ‘› strip is ğ‘‚ (ğ‘›). But neighboring tiles\nreuse cached context, so the cost for querying the first 512 Ã— 512\nregion is larger than for subsequent regions.",
            "content": "Motivated by these facts, we measure latency as the time to first tile (TTFT) and time to second tile (TTST) across resolutions. TTFT denotes the delay from model initialization to the first 512512 tile becoming available, reflecting initial setup cost. TTST measures the time to generate an adjacent 512 512 tile thereafter, reflecting interactive exploration performance. In practice, while both metrics are bounded, they vary with the specific region location because the number of intersecting windows differs across positions. To account for this, as well as minor variations between runs, we perform each evaluation 100 times at random locations and report the average and standard deviation. Table 2: Generation latency for the first and second tile. Metric TTFT TTST Seconds 7.60 1.00 2.40 1.06 An F-35, one of the fastest conventional aircraft in service at roughly 550 m/s, would traverse 512512 tile at 90 resolution in about 84 seconds. In that time, Terrain Diffusion can produce 35 additional tiles, which places the latency well within the requirements of real-time streaming for any slower application. Even if the terrain is downsampled substantially, reducing traversal time by more than an order of magnitude, the query rate still remains well within the models capacity."
        },
        {
            "title": "6.3 Qualitative Analysis\nFigure 5 shows 20 1024Ã—1024 tiles from Terrain Diffusion, all from\nthe same seed used for Fig. 1. The model produces sharp ridges,\ncoherent river basins, smooth transitions, and varied landscapes. No\nvisible tiling artifacts confirm the effectiveness of InfiniteDiffusion.\nTo demonstrate practical use, we integrate Terrain Diffusion\ninto the Minecraft engine by replacing the native world generator.\nElevation and biome queries are routed through our model, and\nclimatic outputs are mapped to Minecraft biomes using a light-\nweight rule set. The system streams terrain in real time and handles",
            "content": "6 arbitrary traversal; only features that rely on long distance biome searches, such as /locate biome and explorer maps, remain unsupported. Runtime is dominated by Minecrafts own generation logic rather than our model, and gameplay remains smooth even under rapid movement. Figure 6 shows representative in game terrain. For these interactive visualizations, we apply bilinear interpolation to upsample the heightmaps 4."
        },
        {
            "title": "7 Discussion",
            "content": "Coherence, hydrology, and diversity. Terrain Diffusion generates only elevation plus few coarse climate variables, yet the resulting terrain is highly coherent. Landscapes aligns with climate, valleys reflect plausible erosion patterns, with hydrological consistency that far exceeds procedural noise, although isolated basins still occur. Qualitatively, the model synthesizes wide range of landforms, from mountain ranges and volcanoes to canyons, fjords, and coastal plains, all without class conditioning. This diversity, combined with seamless infinite generation and real-time inference, shows that diffusion models can capture structured planetary complexity while retaining many or all the important properties of procedural noise. Limitations. The main limitation of our method is that each model relies on conditioning from the level above it, and global coherence deteriorates if this sequence is broken. Some top level must therefore be specified externally. In this work, simple Perlin noise is sufficient because continental scale structure is extremely coarse and contains only broad gradients, so Perlin provides reasonable starting point while remaining seed consistent and controllable. In domains where procedural prior is not available, the appropriate replacement is less obvious. learned generator such as InfinityGAN, or any padding-free GAN, may provide viable solution, since seed consistency is preserved and the simplicity of the distribution at very coarse scales makes the usual drawbacks of GANs much less significant. We experimented with an end-to-end hierarchy using small padding-free GAN as the base model, but its outputs were largely comparable to Perlin noise and offered reduced controllability. This direction remains open for future work. Why InfiniteDiffusion works at low ğ‘‡ . While InfiniteDiffusion is practically limited to small ğ‘‡ , we find that the method retains striking coherence even when generation is aggressively truncated toğ‘‡ = 1 or 2. We hypothesize that because global structure is largely determined from hierarchical conditioning, overlapping windows only need to enforce local consistency that is already largely aligned through shared noise. Additionally, because the diffusion models are trained on random crops, they learn an approximately translation invariant representation that bases predictions on local patterns, which are shared across overlapping windows, rather than absolute position, further reducing seams. Together, these factors make the method robust even under minimal iterative refinement."
        },
        {
            "title": "8 Future Work\nAdding features to the hierarchy is a natural next step. The coarse\nmodel, the base model, or both could incorporate additional vari-\nables such as soil properties, other climatic variables, or satellite\nimagery, enhancing control and enabling additional downstream\napplications. Resolution can also be extended beyond the current",
            "content": "7 hierarchy by adding further refinement stages. While increased resolution does require more work for the same real-world area, traversal speed and viewing distance usually scale with resolution. In this case, the lower throughput required at low resolutions typically balances the extra work at high resolutions, making highresolution generation highly efficient. Finally, the InfiniteDiffusion formulation itself is not specific to terrain. Any domain that can be decomposed into overlapping tiles can adopt the same sampling strategy, including textures, maps, and large environments in general."
        },
        {
            "title": "9 Conclusion\nWe have presented Terrain Diffusion, a diffusion-based framework\nfor coherent, real-time terrain generation across planetary scales.\nBy reformulating MultiDiffusion for unbounded domains and intro-\nducing the Infinite Tensor framework, we enable seed-consistent,\nrandom-access synthesis of infinite terrain with constant mem-\nory and compute. A hierarchical stack of diffusion and consistency\nmodels unifies planetary organization with local realism, producing\ncontinuous landscapes far beyond the reach of procedural noise.\nTogether, these components position diffusion models as a practical\nfoundation for procedural worldbuilding.",
            "content": "References [1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation. arXiv preprint arXiv:2302.08113 (2023). [2] Christopher Beckham and Christopher Pal. 2017. step towards procedural terrain generation with GANs. doi:10.48550/ARXIV.1707.03383 Version Number: 1. [3] Paul Borne-Pons, Mikolaj Czerkawski, Rosalie Martin, and Romain Rouffet. 2025. MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data. arXiv:2504.07210 [cs.GR] https://arxiv.org/abs/2504.07210 [4] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. 2024. DemoFusion: Democratising High-Resolution Image Generation With No $$$. In CVPR. [5] Stephen E. Fick and Robert J. Hijmans. 2017. WorldClim 2: new 1-km spatial resolution climate surfaces for global land areas. International Journal of Climatology 37, 12 (Oct. 2017), 43024315. doi:10.1002/joc.5086 [6] Alain Fournier, Don Fussell, and Loren Carpenter. 1982. Computer rendering of stochastic models. Commun. ACM 25, 6 (June 1982), 371384. doi:10.1145/ 358523. [7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Commun. ACM 63, 11 (Oct. 2020), 139144. doi:10.1145/3422622 [8] Ã‰ric GuÃ©rin, Julie Digne, Ã‰ric Galin, Adrien Peytavie, Christian Wolf, Bedrich Benes, and BenoÃ®t Martinez. 2017. Interactive example-based terrain authoring with conditional generative adversarial networks. ACM Transactions on Graphics 36, 6 (Dec. 2017), 113. doi:10.1145/3130800.3130804 [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 68406851. https://proceedings.neurips.cc/paper_files/paper/2020/ file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf [10] Zexin Hu, Kun Hu, Clinton Mo, Lei Pan, and Zhiyong Wang. 2024. Terrain diffusion network: climatic-aware terrain generation with geological sketch guidance. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence (AAAI24/IAAI24/EAAI24). AAAI Press, Article 1402, 9 pages. doi:10.1609/aaai.v38i11.29150 [11] Aryamaan Jain, Avinash Sharma, and Rajan. 2023. Adaptive & Multi-Resolution Procedural Infinite Terrain Generation with Diffusion Models and Perlin Noise. In Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing (Gandhinagar, India) (ICVGIP 22). Association for Computing Machinery, New York, NY, USA, Article 55, 9 pages. doi:10.1145/3571600.3571657 [12] Ãlvaro Barbero JimÃ©nez. 2023. Mixture of Diffusers for scene composidoi:10.48550/arXiv.2302.02412 tion and high resolution image generation. arXiv:2302.02412 [cs]. [13] Tero Karras, Miika Aittala, Tuomas KynkÃ¤Ã¤nniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. 2024. Guiding Diffusion Model with Bad Version of Itself. In Proc. NeurIPS. [14] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. 2024. Analyzing and Improving the Training Dynamics of Diffusion Models. In Proc. CVPR. [15] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. 2023. SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 5064850660. https://proceedings.neurips.cc/paper_files/paper/2023/file/ 9ee3a664ccfeabc0da16ac6f1f1cfe59-Paper-Conference.pdf [16] Sikuang Li, Chen Yang, Jiemin Fang, Taoran Yi, Jia Lu, Jiazhong Cen, Lingxi Xie, Wei Shen, and Qi Tian. 2025. WorldGrow: Generating Infinite 3D World. doi:10.48550/arXiv.2510.21682 arXiv:2510.21682 [cs]. [17] Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov, and MingHsuan Yang. 2022. InfinityGAN: Towards Infinite-Pixel Image Synthesis. In International Conference on Learning Representations. https://openreview.net/ forum?id=ufGMqIM0a4b [18] Cheng Lu and Yang Song. 2025. Simplifying, Stabilizing and Scaling Continuoustime Consistency Models. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=LyJi5ugyJx [19] NOAA National Geophysical Data Center. 2009. ETOPO1 1 Arc-Minute Global Relief Model. doi:10.7289/V5C8276M [20] Ken Perlin. 1985. An image synthesizer. In Proceedings of the 12th annual conference on Computer graphics and interactive techniques - SIGGRAPH 85. ACM Press, Not Known, 287296. doi:10.1145/325334.325247 [21] Ken Perlin. 2002. Improving noise. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques. ACM, San Antonio Texas, 681682. doi:10.1145/566570.566636 [22] Ansh Sharma, Albert Xiao, Praneet Rathi, Rohit Kundu, Albert Zhai, Yuan Shen, and Shenlong Wang. 2024. EarthGen: Generating the World from Top-Down Views. doi:10.48550/arXiv.2409.01491 arXiv:2409.01491 [cs]. [23] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 37), Francis Bach and David Blei (Eds.). PMLR, Lille, France, 22562265. https://proceedings.mlr.press/v37/sohl-dickstein15.html [24] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. 2023. Consistency models. In Proceedings of the 40th International Conference on Machine Learning (ICML23). JMLR.org, Honolulu, Hawaii, USA. [25] Ryan Rs Spick and James Walker. 2019. Realistic and Textured Terrain Generation using GANs. In European Conference on Visual Media Production. ACM, London United Kingdom, 110. doi:10.1145/3359998.3369407 [26] Georgios Voulgaris, Ioannis Mademlis, and Ioannis Pitas. 2021. Procedural Terrain Generation Using Generative Adversarial Networks. In 2021 29th European Signal Processing Conference (EUSIPCO). IEEE, Dublin, Ireland, 686690. doi:10.23919/ EUSIPCO54536.2021.9616151 [27] Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, and Pan Ji. 2024. BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation. ACM Transactions on Graphics 43, 4 (2024). doi:10.1145/3658188 [28] Dai Yamazaki, Daiki Ikeshima, Ryunosuke Tawatari, Tomohiro Yamaguchi, Fiachra OLoughlin, Jeffery C. Neal, Christopher C. Sampson, Shinjiro Kanae, and Paul D. Bates. 2017. high-accuracy map of global terrain elevations. Geophysical Research Letters 44, 11 (June 2017), 58445853. doi:10.1002/2017GL072874 [29] Xiaoyu Zhang, Teng Zhou, Xinlong Zhang, Jia Wei, and Yongchuan Tang. 2025. Multi-Scale Diffusion: Enhancing Spatial Layout in High-Resolution Panoramic Image Generation . In 2025 IEEE International Conference on Multimedia and Expo (ICME). IEEE Computer Society, Los Alamitos, CA, USA, 16. doi:10.1109/ ICME59968.2025.11209478 [30] Teng Zhou and Yongchuan Tang. 2024. TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image Generation with Diffusion Models. arXiv:2404.19475 (2024). doi:10.48550/arXiv.2404.19475 arXiv:2404.19475 [cs]. Figure 5: Twenty generated 1024 by 1024 regions from Terrain Diffusion. Samples cover volcanic islands, high relief mountain systems, and dissected plateaus, illustrating the models ability to reproduce diverse landscapes with coherent multi-scale structure. All emerge from one world generated with the same seed as in Figure 1. Zoom for details. 9 Figure 6: Nine Minecraft scenes generated from Terrain Diffusion using single fixed biome mapping derived from the models climatic outputs. The Distant Horizons mod is used to increase render distance, and Bliss shaders are used to enhance visuals."
        },
        {
            "title": "A Formal Properties of InfiniteDiffusion",
            "content": "A.1 Preliminaries and Notation We first make precise the InfiniteDiffusion framework used in the main text. Seeds and initial noise. Let be set of seeds. seed ğ‘  deterministically selects an initial noise field ğ½ (ğ‘  ) , and conditioning vector ğ‘§ (ğ‘  ) Z. Formally, there are deterministic functions ğ‘‡ ğº : Zğ‘‘ Rğ¶, ğ» : Z, Î› : ğ‘† such that Infinite image space. Let the spatial domain be the integer lattice Zğ‘‘ , and let ğ½ (ğ‘  ) ğ‘‡ (ğ‘) = ğº (ğ‘ , ğ‘), ğ‘§ (ğ‘  ) = ğ» (ğ‘ ), ğ‘¦ (ğ‘  ) ğ‘– = Î›(ğ‘§ (ğ‘  ), ğ‘–). := RZğ‘‘ ğ¶ denote the space of infinite images with ğ¶ channels. We write ğ½ as function ğ½ : Zğ‘‘ Rğ¶ over pixel coordinates. For the following sections, we write Î© as shorthand for the index-set [ğ»1] [ğ»ğ‘‘ ]. Regions and windows. (rectangular) region is any subset of the form ğ‘… = [ğ‘1, ğ‘1) [ğ‘ğ‘‘, ğ‘ğ‘‘ ) Zğ‘‘ with integers ğ‘ğ‘˜ < ğ‘ğ‘˜ . The set of window indices is countable set ğ‘† (e.g. ğ‘† = Zğ‘‘ ). For each ğ‘– ğ‘† we are given window region ğ‘…ğ‘– Zğ‘‘ , weight map ğ‘Šğ‘– RÎ©, and conditioning vector ğ‘¦ğ‘– . We assume the window layout is such that for every finite region ğ‘…, only finitely many windows intersect ğ‘…: Assumption 1 (Finite window overlap). For every finite region ğ‘…, ğœ… (ğ‘…) := {ğ‘– ğ‘† : ğ‘…ğ‘– ğ‘… } is finite. Embedding operator. For any tensor ğ‘¥ RÎ©ğ¶ , the operator ğ‘ˆğ‘– (ğ‘¥) denotes the infinite image obtained by placing ğ‘¥ on ğ‘…ğ‘– and zero elsewhere. Pretrained diffusion model. Let Î¦ denote fixed pretrained denoising network acting on window-sized images. We formalize it as deterministic function . For fixed seed ğ‘ , we define the entire trajectory Definition of ğ½ (ğ‘  ) (ğ½ (ğ‘  ) ğ‘¡ ğ‘¡ )ğ‘‡ ğ‘¡ =0 recursively by: ğ½ (ğ‘  ) ğ‘‡ For ğ‘¡ = ğ‘‡ ,ğ‘‡ 1, . . . , 1, define ğ½ (ğ‘  ) is given by ğº. ğ‘¦ğ‘– = ğ‘¦ (ğ‘  ) ğ‘– . ğ‘¡ 1 via (3) with ğ½ğ‘¡ = ğ½ (ğ‘  ) ğ‘¡ and Because ğ‘‡ is finite and each update uses only finite sums on any finite region, the tensors ğ½ (ğ‘  ) are well-defined for all ğ‘¡ {0, . . . ,ğ‘‡ }. ğ‘¡ Lazy evaluation algorithm. Algorithm 1 efficiently computes (3). At level ğ‘¡ it maintains images ğ´ğ‘¡ 1, ğµğ‘¡ 1 and processed set ğ‘ƒğ‘¡ 1 ğ‘†. To answer query ğ½ğ‘¡ 1 [ğ‘…], it performs: For each window ğ‘– ğœ… (ğ‘…) ğ‘ƒğ‘¡ 1: ğ´ğ‘¡ 1 [ğ‘…ğ‘– ] ğ´ğ‘¡ 1 [ğ‘…ğ‘– ] + ğ‘Šğ‘– Î¦(ğ½ğ‘¡ [ğ‘…ğ‘– ], ğ‘¦ğ‘– ), ğµğ‘¡ 1 [ğ‘…ğ‘– ] ğµğ‘¡ 1 [ğ‘…ğ‘– ] + ğ‘Šğ‘– . Add each such ğ‘– to ğ‘ƒğ‘¡ 1. The result is ğ½ğ‘¡ 1 [ğ‘…] = ğ´ğ‘¡ 1 [ğ‘…]/ğµğ‘¡ 1 [ğ‘…]. Where the division is performed elementwise. New windows are evaluated recursively by querying ğ½ğ‘¡ [] in the same way at level ğ‘¡, until reaching ğ½ (ğ‘  ) ğ‘‡ , which is given by the seed. At intialization, ğ´ğ‘¡ 1 = 0, ğµğ‘¡ 1 = 0, and ğ‘ƒğ‘¡ 1 = . We now formalize and prove the three properties stated in the main text. Î¦ : RÎ©ğ¶ RÎ©ğ¶, A.2 Seed Consistency where is the space of conditioning vectors. InfiniteDiffusion update. For given noisy image ğ½ğ‘¡ , the InfiniteDiffusion update at step ğ‘¡ ğ‘¡ 1 is defined, for any finite region ğ‘…, by Î¨(ğ½ğ‘¡ ğ‘§) [ğ‘…] = (cid:32) (cid:205)ğ‘– ğœ… (ğ‘…) ğ‘ˆğ‘– (cid:0)ğ‘Šğ‘– Î¦(ğ½ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ğ‘– )(cid:1) (cid:205)ğ‘— ğœ… (ğ‘…) ğ‘ˆ ğ‘— (ğ‘Šğ‘— ) (cid:33) [ğ‘…], (3) where denotes the Hadamard (elementwise) product and the division is also elementwise. For this update, we adopt the convention that any division by zero is defined to be zero. By Assumption 1, both sums are finite on any finite ğ‘…, so (3) is well-defined. ğ‘§ is (possibly infinite) vector from which the ğ‘¦ğ‘– are computed. Informally, seed consistency says that once the seed is fixed, every finite region ğ½ğ‘¡ [ğ‘…] is deterministic function of the seed and the region alone, and is independent of the order in which regions are queried. Definition A.1 (Seed-consistent generative process). family of random infinite images {ğ½ğ‘¡ }ğ‘‡ ğ‘¡ =0 on Zğ‘‘ is seed-consistent if there exists set of seeds such that for all ğ‘  X, ğ‘¡ {0, . . . ,ğ‘‡ } and finite ğ‘…, ğ½ğ‘¡ [ğ‘…] is function of ğ‘  and ğ‘…. Consequently, repeated queries for the same ğ‘¡, ğ‘ , ğ‘… always return the same value, irrespective of the order in which regions are requested. We show that InfiniteDiffusion, as defined in 3, is seed-consistent. 11 Lemma A.2 (InfiniteDiffusion is seed-consistent). Fix seed ğ‘  X. Then for each ğ‘¡ {0, . . . ,ğ‘‡ } and finite region ğ‘…, the tensor ğ½ (ğ‘  ) [ğ‘…] defined by the recursive update (3) is uniquely determined by ğ‘¡ ğ‘  and ğ‘…. During the query, for each ğ‘– ğœ… (ğ‘…) ğ‘ƒğ‘¡ 1 the algorithm performs ğ´ğ‘¡ 1 [ğ‘…ğ‘– ] ğ´ğ‘¡ 1 [ğ‘…ğ‘– ] + ğ‘‰ğ‘–, ğµğ‘¡ 1 [ğ‘…ğ‘– ] ğµğ‘¡ 1 [ğ‘…ğ‘– ] + ğ‘Šğ‘– and does not modify any pixels outside ğ‘…ğ‘– . Equivalently, this is Proof. We proceed by backward induction on ğ‘¡. ğ´ğ‘¡ 1 ğ´ğ‘¡ 1 + ğ‘ˆğ‘– (ğ‘‰ğ‘– ), ğµğ‘¡ 1 ğµğ‘¡ 1 + ğ‘ˆğ‘– (ğ‘Šğ‘– ). Base case (ğ‘¡ = ğ‘‡ ). By construction, ğ½ (ğ‘  ) ğ‘‡ any finite region ğ‘…, the restriction ğ½ (ğ‘  ) by ğ‘  and ğ‘…. ğ‘‡ (ğ‘) = ğº (ğ‘ , ğ‘) for all ğ‘, so for [ğ‘…] is uniquely determined Inductive step. Assume that for some ğ‘¡ {1, . . . ,ğ‘‡ }, ğ½ (ğ‘  ) [ğ‘…] is uniquely determined by ğ‘  and ğ‘… for all finite regions ğ‘…. Consider ğ½ (ğ‘  ) ğ‘¡ 1 [ğ‘…] for finite region ğ‘…. By definition (3), ğ‘¡ ğ½ (ğ‘  ) ğ‘¡ 1 [ğ‘…] = (cid:32) (cid:205)ğ‘– ğœ… (ğ‘…) ğ‘ˆğ‘– (cid:0)ğ‘Šğ‘– Î¦(ğ½ (ğ‘  ) ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ‘– )(cid:1) (cid:33) (cid:205)ğ‘— ğœ… (ğ‘…) ğ‘ˆ ğ‘— (ğ‘Šğ‘— ) [ğ‘…] For each ğ‘– ğœ… (ğ‘…), the region ğ‘…ğ‘– is finite, so by the inductive hypothesis ğ½ (ğ‘  ) [ğ‘…ğ‘– ] is uniquely determined by ğ‘  and ğ‘…ğ‘– . The model Î¦ and weight maps ğ‘Šğ‘– are deterministic. Therefore each term ğ‘¡ ğ‘ˆğ‘– (cid:0)ğ‘Šğ‘– Î¦(ğ½ (ğ‘  ) ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ‘– )(cid:1) is uniquely determined by ğ‘ , and hence the finite sums in the numerator and denominator are uniquely determined. Thus ğ½ (ğ‘  ) ğ‘¡ 1 [ğ‘…] is uniquely determined by ğ‘  and ğ‘…. By induction, the claim holds for all ğ‘¡. Lemma A.2 shows that InfiniteDiffusion has well-defined deterministic output for fixed seed. We now show that the lazy query algorithm is consistent with this definition, and is therefore also seed-consistent. Lemma A.3 (Algorithm Consistency). Fix ğ‘  and timestep ğ‘¡ {1, . . . ,ğ‘‡ }. Then immediately before any query ğ½ğ‘¡ 1 [ğ‘…] with Algorithm 1, the pair (ğ´ğ‘¡ 1, ğµğ‘¡ 1) satisfy ğ´ğ‘¡ 1 = ğµğ‘¡ 1 = ğ‘– ğ‘ƒğ‘¡ 1 ğ‘– ğ‘ƒğ‘¡ 1 ğ‘ˆğ‘– (cid:0)ğ‘Šğ‘– Î¦(ğ½ (ğ‘  ) ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ‘– )(cid:1), ğ‘ˆğ‘– (ğ‘Šğ‘– ). Furthermore, after performing one iteration of Algorithm 1, the updated state satisfies the same form with ğ‘ƒğ‘¡ 1 replaced by ğ‘ƒğ‘¡ 1 ğœ… (ğ‘…). Proof. We prove this by induction over an arbitrary sequence of queries. At initialization, ğ´ğ‘¡ 1 = 0, ğµğ‘¡ 1 = 0, and ğ‘ƒğ‘¡ 1 = , so the claim is trivially true. Now assume that before any query ğ½ğ‘¡ 1 [ğ‘…] we have ğ´ğ‘¡ 1 = ğ‘– ğ‘ƒğ‘¡ 1 ğ‘ˆğ‘– (ğ‘‰ğ‘– ), ğµğ‘¡ 1 = ğ‘ˆğ‘– (ğ‘Šğ‘– ), ğ‘– ğ‘ƒğ‘¡ After processing all such windows we obtain ğ´ğ‘¡ 1 = ğ‘– ğ‘ƒğ‘¡ 1 and similarly ğ‘ˆğ‘– (ğ‘‰ğ‘– ) + ğ‘ˆğ‘– (ğ‘‰ğ‘– ) = ğ‘ˆğ‘– (ğ‘‰ğ‘– ), ğ‘– ğœ… (ğ‘…)ğ‘ƒğ‘¡ 1 ğ‘– ğ‘ƒğ‘¡ 1ğœ… (ğ‘…) ğµğ‘¡ 1 = ğ‘ˆğ‘– (ğ‘Šğ‘– ). ğ‘– ğ‘ƒğ‘¡ 1ğœ… (ğ‘…) Finally, the algorithm updates ğ‘ƒğ‘¡ 1 ğ‘ƒğ‘¡ 1 ğœ… (ğ‘…), so the updated state satisfies the same form with ğ‘ƒğ‘¡ 1 replaced by ğ‘ƒğ‘¡ 1 ğœ… (ğ‘…). By induction, the claim holds for all queries ğ½ğ‘¡ 1 [ğ‘…]. We now show that Algorithm 1 is consistent with the formal definition in 3. Lemma A.4 (Correctness of single qery). Fix ğ‘ , ğ‘¡, and finite region ğ‘…. After any query ğ½ğ‘¡ 1 [ğ‘…] following Algorithm 1, we have ğ½ğ‘¡ 1 [ğ‘…] = ğ´ğ‘¡ 1 [ğ‘…]/ğµğ‘¡ 1 [ğ‘…] = ğ½ (ğ‘  ) ğ‘¡ 1 [ğ‘…]. Proof. By Lemma A.3, after the query finishes we have ğ´ğ‘¡ 1 = ğ‘– ğ‘ƒ ğ‘¡ 1 ğ‘ˆğ‘– (ğ‘‰ğ‘– ), ğµğ‘¡ 1 = ğ‘ˆğ‘– (ğ‘Šğ‘– ), ğ‘– ğ‘ƒ ğ‘¡ 1 for some processed set ğ‘ƒ ğ‘¡ 1 ğœ… (ğ‘…). Now restrict to the region ğ‘…. Any window ğ‘– ğœ… (ğ‘…) has ğ‘…ğ‘– ğ‘… = , so ğ‘ˆğ‘– (ğ‘‰ğ‘– ) [ğ‘…] = 0, ğ‘ˆğ‘– (ğ‘Šğ‘– ) [ğ‘…] = 0. Therefore, ğ´ğ‘¡ 1 [ğ‘…] = (cid:169) (cid:173) ğ‘– ğ‘ƒ (cid:171) ğ‘¡ ğ‘ˆğ‘– (ğ‘‰ğ‘– )(cid:170) (cid:174) (cid:172) [ğ‘…] = (cid:169) (cid:173) ğ‘– ğœ… (ğ‘…) (cid:171) ğ‘ˆğ‘– (ğ‘‰ğ‘– )(cid:170) (cid:174) (cid:172) [ğ‘…], and similarly ğµğ‘¡ 1 [ğ‘…] = (cid:169) (cid:173) ğ‘– ğœ… (ğ‘…) (cid:171) ğ‘ˆğ‘– (ğ‘Šğ‘– )(cid:170) (cid:174) (cid:172) [ğ‘…]. Comparing with the definition (3), we see that ğ´ğ‘¡ 1 [ğ‘…] ğµğ‘¡ 1 [ğ‘…] = Î¨(ğ½ (ğ‘  ) ğ‘¡ ğ‘§ (ğ‘  ) ) [ğ‘…] = ğ½ (ğ‘  ) ğ‘¡ 1 [ğ‘…], which is the claim. where we write ğ‘‰ğ‘– as shorthand for ğ‘Šğ‘– Î¦(ğ½ (ğ‘  ) ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ‘– ). We can now state the main seed consistency result. 12 Theorem A.5 (Seed consistency of the algorithm). Under Assumption 1, the InfiniteDiffusion lazy query algorithm is seedconsistent. Theorem A.7 (Uniform bound on cost for window regions). Under Assumption 2, there exists constant ğ¾ depending only on ğ‘‡ and ğ‘€ such that for all ğ‘¡ and all window indices ğ‘–, Proof. By Lemma A.2, for each ğ‘ , ğ‘¡, and finite ğ‘…, ğ½ (ğ‘  ) [ğ‘…] is uniquely determined by (ğ‘ , ğ‘…), so the process defined by (3) is seed-consistent. By Lemma A.4, the algorithm is equivalent, and therefore also seedconsistent. ğ‘¡ Theorem A.5 formalizes the informal argument in the main text: once seed is fixed, the entire trajectory (ğ½ (ğ‘  ) )ğ‘‡ ğ‘¡ =0 is fully determined, and the lazy querying and caching scheme merely memoizes deterministic computation without affecting its outcome. In particular, querying regions in different orders or repeating query for the same region cannot change the result. ğ‘¡ A.3 Constant-Time Random Access We now formalize the claim that accessing the value on any windowsized region has constant computational cost, independent of the absolute location in the infinite domain. We measure cost in terms of the number of evaluations of Î¦, which dominate runtime in practice. Let ğ¶ğ‘¡ (ğ‘…) denote the worst-case number of Î¦-calls required by the lazy algorithm to answer query for ğ½ğ‘¡ [ğ‘…], starting from an empty cache at all timesteps. Assumption 2 (Uniform overlap bound). There exists finite constant ğ‘€ such that ğœ… (ğ‘…ğ‘– ) ğ‘€ for every window region ğ‘…ğ‘– . In words, each window region overlaps the regions of at most ğ‘€ windows. We treat the total number of diffusion steps ğ‘‡ and the window shape as fixed hyperparameters of the model. Lemma A.6 (Recursive cost bound). Under Assumption 2, for any timestep ğ‘¡ and any window index ğ‘– ğ‘†, the cost ğ¶ğ‘¡ (ğ‘…ğ‘– ) satisfies ğ¶ğ‘¡ (ğ‘…ğ‘– ) ğ‘€ (1 + sup ğ‘— ğ‘† ğ¶ğ‘¡ +1 (ğ‘… ğ‘— )) for ğ‘¡ < ğ‘‡ , with base case ğ¶ğ‘‡ (ğ‘…ğ‘– ) = 0 for all ğ‘–. Proof. Consider query for ğ½ğ‘¡ [ğ‘…ğ‘– ] at some ğ‘¡ < ğ‘‡ . By the update rule, computing this query requires evaluating Î¦(ğ½ğ‘¡ +1 [ğ‘…ğ‘˜ ], ğ‘¦ğ‘˜ ) for every ğ‘˜ ğœ… (ğ‘…ğ‘– ). Each such evaluation requires one call to Î¦ at level ğ‘¡, and in order to provide the input ğ½ğ‘¡ +1 [ğ‘…ğ‘˜ ], the algorithm may in turn need to perform some number of step-(ğ‘¡ + 1) queries. That is, for each ğ‘˜ ğœ… (ğ‘…ğ‘– ) we incur cost 1 + ğ¶ğ‘¡ +1 (ğ‘…ğ‘˜ ). The cardinality of ğœ… (ğ‘…ğ‘– ) is at most ğ‘€ by Assumption 2. Thus ğ¶ğ‘¡ (ğ‘…ğ‘– ) ğ‘˜ ğœ… (ğ‘…ğ‘– ) (1 + ğ¶ğ‘¡ +1 (ğ‘…ğ‘˜ )) ğ‘€ (1 + sup ğ‘— ğ‘† ğ¶ğ‘¡ +1 (ğ‘… ğ‘— )) At the top level ğ‘¡ = ğ‘‡ , no further calls to Î¦ are required because ğ½ğ‘‡ is given directly by the noise generator, hence ğ¶ğ‘‡ (ğ‘…ğ‘– ) = 0. 13 ğ¶ğ‘¡ (ğ‘…ğ‘– ) ğ¾ . ğ‘ğ‘¡ := sup ğ‘– ğ‘† ğ¶ğ‘¡ (ğ‘…ğ‘– ). Proof. Let By Lemma A.6, ğ‘ğ‘¡ ğ‘€ (1 + ğ‘ğ‘¡ +1), ğ‘¡ < ğ‘‡ , with ğ‘ğ‘‡ = 0. Unwinding this recurrence yields ğ‘ğ‘‡ 1 ğ‘€ (1 + 0) = ğ‘€, ğ‘ğ‘‡ 2 ğ‘€ (1 + ğ‘ğ‘‡ 1) ğ‘€ + ğ‘€ğ‘ğ‘‡ 1, and in general ğ‘ğ‘¡ ğ‘€ + ğ‘€ 2 + + ğ‘€ğ‘‡ ğ‘¡ . For fixed ğ‘‡ and ğ‘€, the right-hand side is finite constant independent of ğ‘– and the absolute location of ğ‘…ğ‘– in the plane. Taking ğ¾ := ğ‘€ + ğ‘€ 2 + + ğ‘€ğ‘‡ gives the claimed uniform bound. For any window index ğ‘–, Theorem A.7 gives uniform bound ğ¶0 (ğ‘…ğ‘– ) ğ¾ on the number of calls to Î¦ needed to answer query for ğ½0 [ğ‘…ğ‘– ], starting from an empty cache. This bound depends only on ğ‘‡ and ğ‘€, which are constants, and not on ğ‘– or any regions previously processed. In standard algorithmic notation, this means that the time complexity of query ğ½0 [ğ‘…] is ğ‘‚ (1), when ğ‘… is window region. When caches are reused across multiple queries, subsequent queries typically cost far less than ğ¾, but this is not required for the asymptotic guarantee. Combined with seed consistency, Theorem A.7 formally justifies the claim that users can jump to arbitrary locations in the infinite world and query any window region efficiently, without needing to generate intermediate tiles and without affecting the content of the regions. When we assume that any region of fixed size has bounded number of window overlaps, the same argument applies to arbitrary regions of bounded size, not just individual windows. A.4 Parallelization Finally, we formalize the statement that InfiniteDiffusion admits parallel evaluation of window updates at any fixed timestep. Recall that, at fixed ğ‘¡ and seed ğ‘ , the numerator and denominator images for the update ğ½ (ğ‘  ) ğ‘¡ 1 are ğ‘ˆğ‘– (cid:0)ğ‘Šğ‘– Î¦(ğ½ (ğ‘  ) [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ½ (ğ‘  ) ğ´ (ğ‘  ) )(cid:1), ğ‘¡ ğ‘¡ ğ‘– ğ‘¡ 1 = ğµ (ğ‘  ) ğ‘¡ 1 = ğ‘– ğ‘† ğ‘– ğ‘† ğ‘ˆğ‘– (ğ‘Šğ‘– ), The updated image is then ğ‘¡ 1 = ğ´ (ğ‘  ) ğ½ (ğ‘  ) ğ‘¡ 1/ğµ (ğ‘  ) ğ‘¡ 1. We treat calls to Î¦ as the only expensive operation, and all other operations (additions, multiplications, divisions and updates to ğ´ğ‘¡ 1, ğµğ‘¡ 1) as free. computation is parallelizable if there exists an algorithm in which all calls to Î¦ can be partitioned into finitely many rounds so that within each round the calls are independent and can be executed simultaneously. We first note that, at fixed timestep, once the inputs ğ½ (ğ‘  ) known, all required model evaluations can be done in parallel. ğ‘¡ [ğ‘…ğ‘– ] are Lemma A.8 (Parallel window updates at fixed timestep). Fix seed ğ‘ , timestep ğ‘¡ {1, . . . ,ğ‘‡ }, and finite set of window indices ğ¼ ğ‘†. Suppose that for every ğ‘– ğ¼ the tensor ğ½ (ğ‘  ) [ğ‘…ğ‘– ] is available for free. Then the evaluations ğ‘¡ Î¦(ğ½ (ğ‘  ) ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ‘– ), ğ‘– ğ¼, can be performed in single parallel round, and all corresponding contributions to ğ´ (ğ‘  ) ğ‘¡ 1 on (cid:208)ğ‘– ğ¼ ğ‘…ğ‘– can be formed without any further calls to Î¦. ğ‘¡ 1 and ğµ (ğ‘  ) Proof. For each ğ‘– ğ¼ , the input to Î¦ depends only on ğ½ (ğ‘  ) and ğ‘¦ (ğ‘  ) ğ‘– Î¦(ğ½ (ğ‘  ) [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ‘¡ simultaneously. [ğ‘…ğ‘– ] , which are both already available. Thus the evaluations ) are mutually independent and can be carried out ğ‘¡ ğ‘– Once these outputs are known, the updates [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ‘– )(cid:1) ğ´ (ğ‘  ) ğ‘¡ 1 ğ´ (ğ‘  ) ğ‘¡ 1 ğµ (ğ‘  ) ğµ (ğ‘  ) ğ‘¡ ğ‘¡ 1 + ğ‘ˆğ‘– (cid:0)ğ‘Šğ‘– Î¦(ğ½ (ğ‘  ) ğ‘¡ 1 + ğ‘ˆğ‘– (ğ‘Šğ‘– ) ğ‘¡ 1/ğµ (ğ‘  ) ğ‘¡ 1 = ğ´ (ğ‘  ) and the division ğ½ (ğ‘  ) ğ‘¡ 1 involve only element-wise arithmetic and therefore require no additional model evaluations. Hence all work associated with the windows in ğ¼ can be completed using single parallel round of calls to Î¦. We now show that answering any finite collection of region queries at any timestep admits parallel schedule of model evaluations. Theorem A.9 (Parallelization of finite qery sets). Fix seed ğ‘ , timestep ğ‘¡ {0, . . . ,ğ‘‡ }, and finite collection of regions = {ğ‘… (1), . . . , ğ‘… (ğ‘š) }. Consider the computation that evaluates ğ½ (ğ‘  ) [ğ‘… (ğ‘˜ ) ] for all ğ‘˜ = 1, . . . , ğ‘š using the recursive update (3), starting from ğ½ (ğ‘  ) and withğ‘‡ out caching. Then all calls to Î¦ required by this computation can be arranged into at most ğ‘‡ ğ‘¡ parallel rounds. ğ‘¡ Proof. By Lemma A.2, once ğ‘  is fixed the values ğ½ (ğ‘  ) [ğ‘…] are uniquely determined for all ğ‘¢ and all finite regions ğ‘…. In particular, the set of model evaluations that appear in the recursive computation of {ğ½ (ğ‘  ) is fixed; only their order of execution is not. [ğ‘… (ğ‘˜ ) ]}ğ‘š ğ‘˜=1 ğ‘¢ ğ‘¡ We prove the claim by backward induction on ğ‘¡. Base case (ğ‘¡ = ğ‘‡ ). By definition, ğ½ (ğ‘  ) is given directly by the noise ğ‘‡ generator ğº and does not require any calls to Î¦. Thus any finite set of queries at ğ‘¡ = ğ‘‡ is trivially parallelizable with zero rounds. Inductive step. Fix ğ‘¡ < ğ‘‡ and assume the statement holds for ğ‘¡ + 1. Consider finite collection = {ğ‘… (1), . . . , ğ‘… (ğ‘š) } of regions at time ğ‘¡. Let ğ‘… := ğ‘… (ğ‘˜ ) ğ‘š (cid:216) ğ‘˜=1 be the union of all queried regions at level ğ‘¡. By Assumption 1, only finitely many windows intersect ğ‘…, so the index set ğ¼ğ‘¡ := ğœ… (ğ‘…) = {ğ‘– ğ‘† : ğ‘…ğ‘– ğ‘… } is finite. By (3), computing ğ½ (ğ‘  ) the regions ğ½ (ğ‘  ) ğ‘¡ +1 [ğ‘…ğ‘– ] for all ğ‘– ğ¼ğ‘¡ . ğ‘¡ [ğ‘… (ğ‘˜ ) ] for all ğ‘˜ requires knowing ğ‘¡ Each region ğ‘…ğ‘– is finite, and from the recursion defining ğ½ (ğ‘  ) ğ‘¡ we see that ğ½ (ğ‘  ) [ğ‘…ğ‘– ] itself is obtained from regions of the form ğ½ (ğ‘  ) ğ‘¡ +1 [ğ‘… ğ‘— ] for windows ğ‘— that intersect ğ‘…ğ‘– . Let Rğ‘¡ +1 denote the (finite) collection of all such window regions ğ‘… ğ‘— at timestep ğ‘¡ + 1 that are needed in this way. By the induction hypothesis applied at level ğ‘¡ + 1 to the finite set Rğ‘¡ +1, all model evaluations needed to compute {ğ½ (ğ‘  ) ğ‘¡ +1 [ğ‘… ğ‘— ] : ğ‘… ğ‘— Rğ‘¡ +1} can be scheduled in at most ğ‘‡ (ğ‘¡ + 1) parallel rounds. Once all these regions are available, the corresponding values ğ½ (ğ‘  ) [ğ‘…ğ‘– ] for ğ‘– ğ¼ğ‘¡ are determined and can be formed without furğ‘¡ ther calls to Î¦. At this point, Lemma A.8 implies that all remaining evaluations Î¦(ğ½ (ğ‘  ) ğ‘¡ [ğ‘…ğ‘– ] ğ‘¦ (ğ‘  ) ğ‘– ), ğ‘– ğ¼ğ‘¡ , required to construct ğ´ (ğ‘  ) ğ‘¡ 1 on ğ‘… can be carried out in single additional parallel round. No more model evaluations are needed to extract ğ½ (ğ‘  ) [ğ‘… (ğ‘˜ ) ] from these images. ğ‘¡ 1 and ğµ (ğ‘  ) ğ‘¡ Thus, the entire computation for the queries at timestep ğ‘¡ can be performed using at most 1 + (ğ‘‡ (ğ‘¡ + 1)) = ğ‘‡ ğ‘¡ parallel rounds of calls to Î¦. This completes the induction. 14 Theorem A.9 shows that for any fixed seed ğ‘ , timestep ğ‘¡ and finite collection of regions R, the recursive computation of {ğ½ (ğ‘  ) [ğ‘…] : ğ‘… R} admits bounded-depth parallel schedule of diffusion model evaluations. Combined with seed consistency, this formalizes the claim in the main text that all diffusion model evaluations required to serve any finite batch of queries ğ½ (ğ‘  ) [ğ‘…] can be performed in parallel, up to the intrinsic sequential dependence across diffusion steps. ğ‘¡ ğ‘¡"
        }
    ],
    "affiliations": []
}