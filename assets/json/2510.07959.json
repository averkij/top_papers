{
    "paper_title": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation",
    "authors": [
        "Alexander Rubinstein",
        "Benjamin Raible",
        "Martin Gubri",
        "Seong Joon Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that $\\textit{maximise diversity in model responses}$. Our method, $\\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. $\\textbf{DISCO}$ shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. Code is available here: https://github.com/arubique/disco-public."
        },
        {
            "title": "Start",
            "content": "DISCO: Diversifying Sample Condensation for Efficient Model Evaluation Alexander Rubinstein1 Benjamin Raible1 Martin Gubri2 1Tübingen AI Center, University of Tübingen 2Parameter Lab Seong Joon Oh1 5 2 0 O 9 ] . [ 1 9 5 9 7 0 . 0 1 5 2 : r (cid:128) Project Page DISCO Codebase"
        },
        {
            "title": "ABSTRACT",
            "content": "Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that maximise diversity in model responses. Our method, Diversifying Sample Condensation (DISCO), selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. DISCO shows empirical gains over prior methods, achieving state-ofthe-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC."
        },
        {
            "title": "INTRODUCTION",
            "content": "Model evaluation is becoming increasingly costly. Models have grown in size, which makes each inference expensive. Recent scaling of testtime computation has further raised the cost per task. End-user requirements have also broadened, covering both the content of the output and its style (Wang et al., 2018; Liang et al., 2022; Kim et al., 2023; Zhang et al., 2024). As result, evaluation on modern benchmarks often requires hundreds to thousands of GPU hours. For instance, LMMs-Eval can take between 30 and 1400 hours on 8ˆA100 GPUs (Zhang et al., 2024). HELM requires more than 4000 GPU hours (Liang et al., 2022). Figure 1: Imbalance. More evaluation budget is spent on less informative samples in test sets. Several efficient evaluation approaches have emerged. common framework works in two parts: subset selection and performance prediction. The first part selects static subset of anchor points from the evaluation dataset. The second part predicts full benchmark performance by extrapolating from accuracy on this subset. To select anchor points, existing methods often rely on clustering. Samples are grouped by the similarity of responses they induce in set of reference models (Vivek et al., 2023; Polo et al., 2024). Variants of this framework include dynamic anchor selection (Hofmann et al., 1 2025), modified prediction models (Kipnis et al., 2024), and new benchmarks for method comparison (Zhang et al., 2025). We seek to improve both parts of this framework. For subset selection, we argue that diversity among samples is not essential. What matters is diversity in model responses. We prove that inter-model disagreement is the most informative signal for estimating benchmark performance when the goal is to differentiate and rank models (Proposition 1). Evaluation should therefore focus on samples that elicit varied responses (Figure 1). For performance prediction, we argue that existing methods add unnecessary complexity by estimating hidden model parameters before predicting test performance (Polo et al., 2024; Kipnis et al., 2024). We instead propose direct route. Model signatures, defined as the concatenation of outputs on the selected subset, serve as inputs to simple predictors of benchmark performance. This framework is simpler, yet matches and surpasses more complex alternatives. We validate these ideas through Diversifying Sample Condensation (DISCO). DISCO selects small, informative subset of evaluation samples by focusing on model disagreement. Disagreement is measured by predictive diversity scoring (PDS, Rubinstein et al. (2024)), originally proposed for out-of-distribution detection. simple metamodel then predicts benchmark performance directly from the model signatures on this subset. We evaluate DISCO in both language and vision domains. On MMLU, for example, DISCO reduces evaluation cost by 99.3% with only 1.07 percentage points of error. Compared with prior methods such as Anchor Points (Vivek et al., 2023), TinyBenchmarks (Polo et al., 2024), and Metabench (Kipnis et al., 2024), DISCO achieves stronger efficiencyprecision trade-off."
        },
        {
            "title": "2 RELATED WORK",
            "content": "We review prior work relevant to our approach. We first highlight the escalating cost of evaluation for contemporary large models and motivate the need for efficiency. We then survey prior attempts at efficient benchmarking, covering instance and task reduction techniques. Finally, we describe our novelty and contributions. Cost of evaluation. The evaluation of modern large models is currently driven by increasingly sophisticated benchmarks assessing wide array of capabilities, from the foundational GLUE (Wang et al., 2018) and the comprehensive HELM (Liang et al., 2022) to LMMs-Eval for multimodal models (Zhang et al., 2024), the diverse BIG-bench (Srivastava et al., 2022), Prometheus for measuring diverse LLM capabilities (Kim et al., 2023), and GAIA for general AI assistants (Mialon et al., 2023). This progress comes at an escalating cost: models have grown significantly in size, making each inference step more resource-intensive, while the scaling of test-time computations has dramatically increased the per-task evaluation costs. Furthermore, end-user requirements have diversified to encompass not only output content but also style and manner. Consequently, single evaluation on modern benchmarks can demand hundreds to thousands of GPU hours. For examples, LMMs-Eval can require between 30 and 1400 hours on 8ˆA100 GPUs per model (Zhang et al., 2024; Polo et al., 2024), and HELM evaluations can exceed 4000 GPU hours per model (Liang et al., 2022; Polo et al., 2024). Label-efficient evaluation. In the pre-LLM context, labelling test set used to be cost bottleneck for evaluation. In this context, the concept of active testing has been explored, where labelling budget is maximally assigned to information-rich samples (Majumdar & Niksic, 2017; Ji et al., 2021; Deng & Zheng, 2021; Kossen et al., 2021; Hu et al., 2023; Kossen et al., 2022; Huang et al., 2024). In our case, we are concerned with the inference costs of evaluation. As such, active testing approaches are not directly applicable, as they require full inference over the test set to identify informative samples to label. Efficient benchmarking. In the LLM era, benchmarks have diversified to measure multiple capabilities and styles of model behaviours. Researchers have proposed strategies to build an efficient benchmark in the first place (Prabhu et al., 2024; Perlitz et al., 2023; Rädsch et al., 2025). There were attempts to compress multiple benchmarks, measuring an array of capabilities of LLMs, into single one by eliminating redundancies (Zhao et al., 2024; Yuan et al., 2025). Others have focused on selection of small, informative subsets, also known as Anchor point approaches (Vivek et al., 2023; Polo et al., 2024; Li et al., 2025; Gupta et al., 2025; Kipnis et al., 2024). Given an entire dataset, they compute small subset of data points according to the representativeness criterion, 2 Figure 2: Problem overview. We aim at selecting much smaller evaluation dataset than the original evaluation dataset, while keeping the estimated performances as close as possible. Figure 3 details the selection algorithm and the performance predictor. determined through the correctness patterns of large number of source models. Afterwards, the target model performance is estimated based on weighted accuracy computation on the selected subset. In particular, tinyBenchmarks (Polo et al., 2024) and Metabench (Kipnis et al., 2024) have adopted Item Response Theory (IRT) (Lord & Novick, 2008) to estimate model performance in principled manner. Hofmann et al. (2025) proposed an IRT-based approach to LLM evaluation that selects anchor points dynamically for each model, guided by its predictions on previously chosen anchors. To address the growing number of methods for efficient LLM evaluation, Zhang et al. (2025) recently introduced large-scale benchmark. In this work, we suggest reconsidering the approaches for both anchor point selection and target model performance estimation, as explained below. Our novelty and contribution. We differentiate our approach, Diversifying Sample Condensation (DISCO), from previous work in two aspects. (1) model disagreement (Rubinstein et al., 2024) is simpler and more effective proxy for sample informativeness than representativeness (Vivek et al., 2023; Polo et al., 2024). (2) The application of metamodels on model signatures is simpler and more effective approach than direct accuracy evaluation approaches (Vivek et al., 2023) or prior approaches that require estimating latent model parameters (Polo et al., 2024; Kipnis et al., 2024)."
        },
        {
            "title": "3 PROBLEM",
            "content": "ř 1 parg maxc fcpxiq yiq. Our task is the estimation of model performance on benchmark. Let : Ñ be predictive model over dataset : tpx1, y1q, . . . , pxN , yN qu sampled iid from some distribution. We are interested in estimating the model performance on the dataset rpf, Dq. An example metric for model performance is accuracy: for probabilistic classifier : Ñ r0, 1sC, accuracy is defined as 1 We are interested in estimating rpf, Dq in cost-effective way. We seek ways to sample subset of size ! from the original set to estimate rpf, Dq. The overall problem is described in Figure 2. An integral ingredient for both prior works and ours is the set of source models tf 1, . . . , u, held-out set of models whose ground-truth performances are known. We define the target models as the models whose performances we aim to estimate."
        },
        {
            "title": "4 SOLUTION",
            "content": "This section presents DISCO, our solution to the problem of efficient performance evaluation. DISCO is composed of two steps: (i) the dataset selection, where given an original dataset and an heldout set of source models, we identify much smaller subset of samples; and (ii) the performance prediction, where given the model outputs on our DISCO selected evaluation set, we estimate the model performance on the original set. 3 Figure 3: DISCO overview. First, we select subset of an evaluation dataset with the most informative samples. Second, we predict the performance of unseen models from their outputs on the selected samples. 4.1 DATASET SELECTION At this stage, we require score that quantifies each samples informativeness for predicting performance on the full dataset. Using this score, we rank the samples and select top-k subset that best preserves the datasets information content. 4.1.1 PRIOR SELECTION METHODS We first review existing approaches for selecting representative data points in the evaluation set, referred to as anchor points. 1 Ă txiuN Anchor-conf Vivek et al. (2023) choose anchors takuK ř distances between each data point and the closest anchor: minA epxq abbreviates the concatenated model likelihoods epx, yq : and ground-truth label px, yq for the source models tf 1, . . . , u. 1 that minimise the sum of i,k pepxiq, epakqq , where the for the input 1pxqy, . . . , pxqy Anchor-corr (Polo et al., 2024) is nearly identical to Anchor-conf, except that the embedding uses correctness scores instead of likelihoods: epx, yq : ts1px, yq, . . . , sM px, yqu, where smpx, yq : 1parg maxc mpxqc yq encodes correctness of model on sample x. 1 θm, αi, βiq sigmoidpαJ Anchor-IRT (Polo et al., 2024) uses the Item-Response Theory (IRT) to define parametric model θm ` βiq. It predicts the correctness of model on Pr psm sample xi with parameters θm Rd, αi Rd, and βi R. Using observations of the samplewise correctness of source models pxi, yi, sm q, the parameters are inferred with an ExpectationMaximisation algorithm. Now, they continue the anchor selection based on the sample-wise embeddings epxiq : pαi, βiq. Best for validation Kipnis et al. (2024) finds an anchor set through an iterative search. The algorithm first generates large number of candidate anchor sets, tA1, . . . , AP u, by uniformly sampling from the full dataset D. For each candidate set Ap, simple scalar-to-scalar regression model, gp, is trained on the source models F. This model learns to map the performance on the subset, rpf, Apq, to the known ground-truth performance on the full dataset, rpf, Dq. Each trained regressor gp is subsequently evaluated on held-out validation set of models. The final anchor set is selected as the candidate Ap whose corresponding regressor gp yields the lowest prediction error (e.g., RMSE) on this validation set. How DISCO differs. Unlike clustering, we use sample-wise statistic to determine samples with maximal information content. This greatly simplifies the sampling procedure. We exploit the model diversity, not model confidence or correctness. set of models can be highly confident and diverse at the same time. We argue that inputs that induce model diversity are more useful for performance prediction."
        },
        {
            "title": "4.1.2 DISCO SELECTION",
            "content": "We now present our selection method. We hypothesise that samples that produce the greatest diversity in model outputs carry most information about the model performance (see Figure 1). In this part, we explain how we identify such samples in the test dataset. Our sample selection strategies are illustrated in Figure 3. The main approach in Diversifying Sample Condensation (DISCO) is to select subset DDISCO of the original evaluation set by sampling the top-k samples based on disagreement score, such as Predictive Diversity Score (PDS) from Equation 1. This follows the intuition shown in Figure 1. We start with an information-theoretic observation below. Proposition 1. Let tpxi, yiquN be test set and Unift1, . . . , be the index of pxiq r0, 1s be the predictive probability for class of model on uniformly chosen model. Let input xi. We write pym pxiqq. Dei fine ensemble mean prediction to be sfcpxiq : Emrf pxiqs for each class and define corresponding sf1pxiq, . . . , sfCpxiqq. Let Spmq Spf m, Dq denote prediction random variable as pyi following Catp function of model and dataset D, such as model accuracy, that is injective with respect to m. We make additional Assumptions A1A2 in appendix. Then, for the categorical random variable following Catpf 1 pxiq, . . . , MIm,pyi pSpmq; pyiq Hppyiq Em rH ppym qs JSD py1 , . . . , pyM ` . where Hpq is entropy, MIpq is mutual information, and JSDpq is generalised Jensen-Shannon Divergence for multiple distributions (Fuglede & Topsoe, 2004). See proof in Appendix A. We conclude that the sample conveying the greatest level of information for the prediction of Spmq (e.g. model accuracy) is the one with greatest JSD . This generalised Jensen-Shannon divergence translates to the diversity of distributions (Fuglede & Topsoe, 2004). Based on the insight that model diversity matters for performance prediction, we also consider an alternative measure that measures the model diversity: predictive diversity score (PDS) (Rubinstein et al., 2024). It is more interpretable, as it is continuous generalisation of the number of unique argmax category predictions among source models: , . . . , pyM py1 ` ` PDS py1 , . . . , pyM : 1 ÿ max c pxiq. PDS is related to JSD through the enveloping inequalities below: Proposition 2. Denoting PDSi : PDS ple i, we have py1 , . . . , pyM ` , JSDi : JSD (1) ` py1 , . . . , pyM for each sam2 2 ln 2 pPDSi 1q2 ď JSDi ď 1 log pPDSi 1q. See proof in Appendix B.3. In the experiments, we consider both JSD and PDS as criteria for sample selection. 4.2 PERFORMANCE PREDICTION Once subset of dataset samples is selected, we use the responses of the target model on to estimate the true performance. 4.2.1 PRIOR PREDICTION METHODS We first review existing approaches for estimating the true performance using predictions on anchor points ta1, . . . , aKu. Weighted sum Vivek et al. (2023) estimates the true performance by directly computing the accuracy wk sm on the anchor set: WSpf, Aq : p1{Kq , where wk is the number of original training samples xi assigned to the anchor ak in the Anchor-Corr method. ř 5 p-IRT (Polo et al., 2024): makes adjustments to the vanilla accuracy on the anchor set by adding correction term derived from the IRT in Anchor-IRT in: p-IRTpf, Aq : p1{Kq kPA sk ` 1{pN Kq kRA pi, where ˆpi is the IRT estimation computed based on the parameters obtained in Anchor-IRT. ř ř gp-IRT (Polo et al., 2024) is mixture of the two approaches above: gp-IRTpf, Aq λ WSpf, Aq ` p1 λq p-IRTpf, Aq where λ r0, 1s. ability-IRT Kipnis et al. (2024) is two-stage method that uses the anchor set as diagnostic tool rather than just miniature test. First, it uses pre-calibrated IRT model to estimate latent ability score, ˆθf , from the target models pattern of correct and incorrect responses on A. Second, pre-trained regressor, g, predicts the final performance Sf using both the simple anchor set accuracy and this more informative ability score ˆθf as input features. The final prediction is given by ˆSf Sf gp ˆSf A, ˆθf q, leveraging deeper measure of the models capability to improve the estimate. How DISCO differs. Previous prediction methods rely on scalar summaries of performance, such as the (weighted or corrected) accuracy on the anchor set. In contrast, our approach leverages much richer signal: the model signature, defined as the concatenation of the models raw outputs on the selected samples. By learning direct mapping from the high-dimensional signature to the final performance, we bypass the complexities of psychometric modeling and demonstrate that simpler, more direct approach can be more effective. 4.2.2 DISCO PREDICTION Given smaller set of test dataset DDISCO, we estimate the performance of model as closely as possible to the true full test performance rpf, Dq. We deliberately opt for simple approaches here, in order to make point that simple is best; we also compare against rather complex prior work and show that our simple method wins against the stolen. Our performance prediction framework is depicted in Figure 3. Model signatures. We hypothesise that models with similar output patterns on DDISCO will exhibit similar performance. To capture this pattern, we define model signature as the concatenation of the models outputs on DDISCO: pDDISCOq : rf px1q, . . . , pxLqs. Such function signature may have large dimensionality, as it is the product of model output dimensionality (e.g. 1000 for ImageNet) and the number of selected samples DDISCO (e.g. can go up to 50k for ImageNet validation set). To reduce the storage burden and improve generalizability, we consider applying dimenionality reduction technique based on principal component analysis (PCA): pDDISCOq. KNN prediction. Built on the hypothesis that the similarities in function signature imply performance similarity, we consider the kNN predictor based on held-out set of models F. Given function to evaluate, we identify the most similar models in using the Euclidean distance between their signatures after dimensionality reduction. We estimate performance by averaging the performances of the most similar models. Parametric mapping. We also consider parametric prediction variant. single parametric mapping is trained for the prediction of model performance. As the training set, we use model signatures f1pDDISCOq, . . . fM pDDISCOq for as the training set for the regression problem of training mapping Rpq to let fmpDDISCOq approximate prpf, Dq. The predictor can be implemented using neural network, linear regression, or Random Forest, for example."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we introduce the experimental setup, present the main results of Diversifying Sample Condensation (DISCO) in language domain (5.1), analyse contributing factors (5.2), and demonstrate that the method is domain-agnostic and can also be successfully applied to the vision domain (5.3). Datasets. We evaluate DISCO on two widely used language modeling benchmarks: MMLU (Hendrycks et al., 2021) and HellaSwag (Zellers et al., 2019). MMLU covers 57 tasks about world 6 Approach Selection Prediction MMLU (14k) HS (10k) WG (1.3k) ARC (1.2k) 4.1 4.2 MAEÓ RankÒ MAEÓ RankÒ MAEÓ RankÒ MAEÓ RankÒ Baseline Random Direct eval. 3.45 0.916 2.85 0.839 3.60 0.827 2.61 0.898 tinyBenchmarks Random Anchor-IRT Anchor-corr gp-IRT gp-IRT gp-IRT 2.79 3.25 2.08 0.922 1.96 0.922 2.19 0.927 1.27 0.819 1.64 0.830 2.24 0.937 1.95 0.928 2.22 0.850 4.55 0.918 2. 0.921 0.708 0.948 Metabench Best for val. ability-IRT 2.08 0.904 0.80 0.974 1.23 0.947 1.14 0.971 Model signature Random Sig. + kNN 1.82 Sig. + RF 1.81 0.912 1.49 0.933 1. 0.899 1.58 0.938 1.29 0.920 2.30 0.926 1.72 DISCO (ours) High PDS Sig. + kNN 1.31 Sig. + RF 1.07 0.972 1.32 0.987 1. 0.956 1.19 0.984 1.00 0.951 1.96 0.967 1.47 High JSD Sig. + kNN 1.14 Sig. + RF 1.30 0.975 1.50 0.987 0.86 0.944 1.26 0.972 1. 0.955 2.11 0.973 1.75 0.905 0.938 0.937 0.971 0.939 0.938 Table 1: DISCO achieves state-of-the-art test-set compression by using model signatures combined with PDS for accurate performance prediction. Compression of MMLU, HellaSwag (HS), Winogrande (WG) and ARC datasets by DISCO (ours), tinyBenchmarks, Metabench, and other baselines. For each dataset, we reduce the test set to 100 data points (except for Metabench, see below), achieving inference cost reduction of 99.3% and 99.0%, on MMLU and HS, respectively. Sig. + RF/kNN stands for model signature with Random Forest/kNN prediction ( 4.2.2). Mean absolute error (MAE) is the %p difference in accuracy, and Rank is the Spearman rank correlation between the true model ranking and the estimated model ranking. Results for Metabench are not directly comparable, as it requires more examples to converge: 150 datapoints for MMLU and ARC (+50%), 450 for HS (+350%), and 200 for WG (+100%). knowledge and problem-solving ability, and HellaSwag focuses on commonsense natural language inference. Models. Building on the TinyBenchmarks framework (Polo et al., 2024), we evaluate 424 large language models (LLMs) from Hugging Faces Open LLM Leaderboard (Fourrier et al., 2024). The models cover GPT- (Radford et al., 2019), LLaMA- (Touvron et al., 2023), DeepSeek- (DeepSeek-AI et al., 2025), and BERT-style (Devlin et al., 2019) architectures, with model sizes ranging from 1.3 billion to 32 billion parameters. Model split. DISCO is based on meta-model approach where predictor is constructed based on the model signatures of pool of source models and tested on disjoint set of models. This approach has traditionally been criticised for its dependency on the set of existing models: the approach may fail to retain performance with unforeseen changes in future models. To address this concern, we introduce the chronological split, where the source models consist of models published before January 13, 2024 and the meta test set consists of models after the cutoff date. The train-test ratio is 9:1. Evaluation. We evaluate DISCO and baseline approaches using two complementary metrics. First, the Mean Absolute Error (MAE) of the model accuracies, reported as percentage points (%p), captures the absolute error of accuracy prediction. Second, to assess the consistency of the relative ordering of models, we report the Spearman rank correlation (Rank) in model ranking between the true and estimated model performances. 5.1 MAIN RESULTS Table 1 shows the main results. Uniform random sampling, together with direct evaluation with the corresponding annotated labels, yields 3.45%p MAE and .916 rank correlation at 100 samples. The approaches introduced in tinyBenchmarks Polo et al. (2024) improve over this baseline, confirming their findings. We measure the efficacy of DISCO in two steps: adopt model-signature approach on top of uniform random sample selections first, and then consider sampling according to predictive diversity scoring (PDS). Even without PDS, on uniform random samples, model signatures are achieving 1.81%p MAE and .933 rank correlation with Random Forest (RF), reaching the state-of-the-art performance with simple and practical ingredients. When PDS is further considered for sample selection, to diversify 7 Figure 5: MMLU performance estimation vs. compression rates. Mean absolute error (MAE), measured in %p difference in accuracy, and the Spearman rank correlation between the true model ranking and the estimated model ranking are shown. At 100 samples, the results are identical to Table 1. Main observations: DISCO hits better efficiency-precision trade-off across all range of compression rates. For extreme compression rate, kNN is better choice than random forest (RF). the model outputs, we achieve 1.07%p MAE and .987 rank correlation, demonstrating significant leap from the prior state of the art from tinyBenchmarks Polo et al. (2024) from ICML 2024. To provide an understanding of the distributional comparison of the true model performances and the estimated performances, we show scatter plot in Figure 4. As signified by the high Pearsons correlation coefficient at .986, the estimated performances closely follow the true performances. Figure 5 shows the performance against varying degrees of the test set reduction. We observe that the ranking of estimated evaluation methodologies does not change much across wide range of degrees of reduction. In particular, our DISCO is consistently the best method across all ranges of number of samples involved. For the extreme rates of compression, at 10 samples, the non-parametric performance predictor of kNN yields better performance than the parametric Random Forest, suggesting that nonparametric approaches may be more suitable at extreme compression. 5.2 FACTORS ANALYSIS Figure 4: True and estimated performance on MMLU. Scatter plot of performances of 40 models. We analyse the impact of several design choices involved in our DISCO on MMLU dataset. See Table 2 for an overview. Model split. In recent benchmark for efficient LLM evaluation Zhang et al. (2025), the authors observed that prediction performance drops sharply when test models outperform training models. We extend this idea by replacing performance-based splits with chronological splits, training on older models and testing on newer ones. This better reflects real-world usage, whereas performance-based splits create an artificial stress test. For this purpose, we introduced the chronological split in 5. We examine the impact of this model splitting on the result. We observe that our DISCO is robust to the choice of splitting strategy. Chronological splitting yields rank correlation of .987, which is nearly identical to the .986 obtained with uniform splitting (Table 2 (a)). 8 Stratification. We measure the efficacy of the stratification strategy in (Polo et al., 2024) where equal numbers of anchor points are selected from each of 57 tasks in MMLU dataset (Table 2 (b)). We find that stratification (.978) is not effective when data points are sampled according to PDS (.987). Number of source models. We analyse the sensitivity of DISCO to the number of source models (Table 2 (c)). With only 100 models (.969 rank correlation), it already outperforms TinyBenchmarks, which uses all 382 available source models (.927 in Table 1). As the number of source models increases, rank correlation steadily improves, reaching maximum of .987 for 382. Dimensionality reduction. We compare PCA with different target dimensions to Uniform Manifold Approximation and Projection (UMAP) (McInnes et al., 2020) for dimensionality reduction (Table 2 (d)). We notice that dimensionality reduction helps reduce potential overfitting: without it (using all 3100 dimensions), the correlation is .918, while with PCA at 256 dimensions, it improves to .987. Overall, PCA outperforms UMAP and remains robust across wide range of dimensions. Prediction model. We compare consider wide range of prediction models (Table 2 (e)). Random Forest achieves the highest rank correlation of .987, outperforming all other methods. Table 2: Factor analysis for DISCO on MMLU. Highlighted in bold are the default design choices for DISCO. All comparisons are based on 100 selected samples. 5.3 RESULTS FOR VISION DOMAIN In this section, we give quick overview of the DISCO applied to the vision domain. For detailed results, see C. Setup. We use ImageNet-1k (Russakovsky et al., 2015) with 1.28M images and 400 pretrained models from timm (Wightman, 2019), spanning convolutional (Krizhevsky et al., 2012) and transformer (Dosovitskiy et al., 2021) architectures (0.3M300M parameters). Following the language domain, we adopt chronological split with cutoff 5 April 2023 (88:12 traintest). Performance is evaluated using mean absolute error (MAE) and Spearman rank correlation. Approach Selection Prediction IN val (50k) MAEÓ RankÒ 4.1 4.2 Baseline Random Direct eval. 3.03 0.652 Model signature Random DISCO (ours) High PDS Sig. + kNN 1.72 0.808 0.86 0.944 Sig. + RF Sig. + kNN 1.68 0.819 0.63 0.969 Sig. + RF Table 3: DISCO compression of ImageNet validation dataset. We evaluate the generalisation of our DISCO to the computer vision domain. The main metrics are mean absolute error (MAE), measured in %p difference in accuracy, and the Spearman rank correlation (Rank) between the true model ranking and the estimated model ranking. Main observations: (1) Same as for language experiments, model signature is an effective strategy for performance estimation. (2) Using PDS on top improves performance even more. Results. Our DISCO approach significantly compresses the ImageNet validation set by reducing it to just 100 data points, achieving an inference cost reduction of 99.8%. DISCO with uniform random sampling and random forest prediction on model signatures achieve 0.86%p MAE and .944 rank correlation, surpassing the baseline. Using predictive diversity score (PDS) for data selection and Random Forest for prediction, our method achieves 0.63%p MAE and .969 rank correlation, substantially outperforming the baseline  (Table 3)  . The results demonstrate that DISCO is effective in both language and vision domains."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Evaluating ML models is increasingly expensive due to larger models, datasets, and benchmarks. It is especially true for general-purpose LLMs requiring broad evaluation. We propose DISCO, which selects small informative subset of the evaluation data that maximizes LLM disagreement and estimates model performance from predictions on it. DISCO cuts evaluation costs by over 99% with minimal error and consistently outperforms prior methods. This enables practical use: efficient evaluation on limited compute, frequent performance tracking during training, and cheap end-user checks of deployed models. Limitations. The main limitation of DISCO is robustness to distribution shifts in the model population. Shifts can arise from new architectures, training methods, or objectives, introducing patterns unseen during training and reducing estimator accuracy. Future work could address this with adaptive sample selection or periodic retraining on newer models."
        },
        {
            "title": "AUTHOR CONTRIBUTIONS",
            "content": "Benjamin, Joon, and Alexander conceived the project. Alexander led the language experiments, Benjamin led the vision experiments. Joon and Martin helped design the experiments. Alexander, Martin, and Joon led the writing of the paper. Martin and Joon provided helpful feedback throughout the project."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported by the Tübingen AI Center. AR thanks the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support. This research utilized compute resources at the Tübingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG."
        },
        {
            "title": "REFERENCES",
            "content": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, 10 Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 7 Weijian Deng and Liang Zheng. Are labels always necessary for classifier accuracy evaluation? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1506915078, 2021. 2 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. 7 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 9, Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open llm leaderboard v2, 2024. 7 B. Fuglede and F. Topsoe. Jensen-shannon divergence and hilbert space embedding. In International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings., pp. 31, 2004. doi: 10.1109/ ISIT.2004.1365067. 5, 14 Vipul Gupta, Candace Ross, David Pantoja, Rebecca J. Passonneau, Megan Ung, and Adina Williams. Improving model evaluation using SMART filtering of benchmark datasets. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 45954615, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.235. 2 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 6 Valentin Hofmann, David Heineman, Ian Magnusson, Kyle Lo, Jesse Dodge, Maarten Sap, Pang Wei Koh, Chun Wang, Hannaneh Hajishirzi, and Noah A. Smith. Fluid language model benchmarking. In Second Conference on Language Modeling, 2025. 1, Zhengyu Hu, Jieyu Zhang, Yue Yu, Yuchen Zhuang, and Hui Xiong. How many validation labels do you need? exploring the design space of label-efficient model ranking. ArXiv, abs/2312.01619, 2023. 2 Yuheng Huang, Jiayang Song, Qiang Hu, Felix Juefei-Xu, and Lei Ma. Active testing of large language model via multi-stage sampling. arXiv preprint arXiv:2408.03573, 2024. 2 Disi Ji, Robert Logan, Padhraic Smyth, and Mark Steyvers. Active bayesian assessment of blackbox classifiers. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 79357944, 2021. 2 Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evalIn The Twelfth International Conference on Learning uation capability in language models. Representations, 2023. 1, 2 Alex Kipnis, Konstantinos Voudouris, Luca M. Schulze Buschoff, and Eric Schulz. metabench sparse benchmark of reasoning and knowledge in large language models. In unknown, 2024. 2, 3, 4, Jannik Kossen, Sebastian Farquhar, Y. Gal, and Tom Rainforth. Active testing: Sample-efficient model evaluation. In International Conference on Machine Learning, 2021. 2 11 Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Thomas Rainforth. Active surrogate estimators: An active learning approach to label-efficient model evaluation. Advances in Neural Information Processing Systems, 35:2455724570, 2022. 2 Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012. 9, 18 Yang Li, Jie Ma, Miguel Ballesteros, Yassine Benajiba, and Graham Horwood. Active evaluation acquisition for efficient LLM benchmarking, 2025. 2 Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. 1, 2 Frederic Lord and Melvin Novick. Statistical theories of mental test scores. IAP, 2008. 3 Rupak Majumdar and Filip Niksic. Why is random testing effective for partition tolerance bugs? Proceedings of the ACM on Programming Languages, 2(POPL):124, 2017. 2 Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction, 2020. 9 Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: In The Twelfth International Conference on Learning benchmark for general ai assistants. Representations, 2023. 2 Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen. Efficient benchmarking of language models. arXiv preprint arXiv:2308.11696, 2023. 2 Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating LLMs with fewer examples. In Forty-first International Conference on Machine Learning, 2024. 1, 2, 3, 4, 6, 7, 8, 9 Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, and Samuel Albanie. In The Thirty-eighth Annual Efficient lifelong model evaluation in an era of rapid progress. Conference on Neural Information Processing Systems, 2024. 2 Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019. 7 Tim Rädsch, Leon Mayer, Simon Pavicic, Emre Kavur, Marcel Knopp, Barıs Öztürk, Klaus Maier-Hein, Paul Jaeger, Fabian Isensee, Annika Reinke, et al. Bridging vision language model (vlm) evaluation gaps with framework for scalable and cost-effective benchmark generation. arXiv preprint arXiv:2502.15563, 2025. 2 Alexander Rubinstein, Luca Scimeca, Damien Teney, and Seong Joon Oh. Scalable ensemble diversification for ood generalization and detection. arXiv preprint arXiv:2409.16797, 2024. 2, 3, 5 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge, 2015. 9, Igal Sason. On reverse pinsker inequalities. arXiv preprint arXiv:1503.07118, 2015. 15 Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. 2 12 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 7 Rajan Vivek, Kawin Ethayarajh, Diyi Yang, and Douwe Kiela. Anchor points: Benchmarking models with much fewer examples. arXiv preprint arXiv:2309.08638, 2023. 1, 2, 3, 4, Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. 1, 2 Ross Wightman. Pytorch image models, 2019. 9, 18 Peiwen Yuan, Yueqi Zhang, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, and Kan Li. Beyond one-size-fits-all: Tailored benchmarks for efficient evaluation. arXiv preprint arXiv:2502.13576, 2025. 2 Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. 6 Guanhua Zhang, Florian Dorner, and Moritz Hardt. How benchmark prediction from fewer data misses the mark. arXiv preprint arXiv:2506.07673, 2025. 2, 3, Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. 1, 2 Hongyu Zhao, Ming Li, Lichao Sun, and Tianyi Zhou. Bento: Benchmark task reduction with in-context transferability. arXiv preprint arXiv:2410.13804, 2024."
        },
        {
            "title": "DISCLAIMER FOR USE OF LLMS",
            "content": "We primarily used LLMs in coding co-pilot applications to facilitate experimentation and help with plotting code for result presentation. LLMs were also used as writing tools to assist in refining the paper. However, the final version was carefully reviewed and finalized by the authors. No LLMs were used in ideation and experimental design. MUTUAL INFORMATION AND JENSEN-SHANNON DIVERGENCE In this section, we show that Mutual Information is equivalent to JSD in our setting. We present the setup and assumptions, then prove the proposition. A.0.1 SETUP. 1 pxiq, . . . , Denote set of source models as tf 1, . . . , u. Let Unift1, . . . , be the index of uniformly chosen model and Spmq Spf m, Dq denote any statistic that is deterministic function of computed on (e.g. accuracy on D). Let pxiq r0, 1s be the predictive probability for class of model on input xi. We write pym for the categorical random variable following Catpf pxiqq (i.e., given m, the prediction on datapoint is categorical random variable pym pxiqs for each class and sf1pxiq, . . . , sfCpxiqq. define corresponding prediction random variable as pyi following Catp As consequence, the ensemble mean prediction random variable pyi coincides with marginal distribution of pym : ). Define ensemble mean prediction to be sfcpxiq : Emrf Prppyi cq EmUnifrM srPrppyi cmqs 1 Mÿ m1 Prppym cq. Definition 1 (JensenShannon divergence). Generalised Jensen-Shannon Divergence for multiple distributions (Fuglede & Topsoe, 2004): ` JSD , . . . , pyM py1 1 Mÿ m1 DKLppym }pyiq Hppyiq 1 Mÿ m1 Hppym q. A.0.2 ASSUMPTIONS. Assumption A1 (Deterministic predictions). Conditional on m, each prediction pym is fully deteri mined by (or more generally, any residual randomness is independent across and independent of m). Assumption A2 (Uniform prior). The model index is uniformly distributed: Unift1, . . . , u. A.0.3 PROPOSITION. Proposition 3. Under Assumptions A1A2, if Spmq is injective, then MIm,pyi pSpmq; pyiq Hppyiq 1 Hppym : JSD ` , . . . , pyM py1 . Mÿ m1 where Hpq is entropy, MIpq is mutual information. Proof. By Assumption A1 and since Spmq is deterministic function of m, we have the Markov chain pym ÐÑ ÐÑ Spmq. If is injective, then is recoverable from Spmq, hence ` MI Spmq; pyi ` MI m; pyi . By the definition of mutual information, MIpm; pyiq Hppyiq EmrH ppyimqs . 14 Conditional entropy (using Assumption A2): Hppyimq Em 1 Mÿ m"
        },
        {
            "title": "Hppym",
            "content": "i q. Combine: MIpm; pyiq Hppyiq 1 M"
        },
        {
            "title": "Hppym",
            "content": "i : JSD ` , . . . , pyM py1 . Mÿ m1 BOUNDS FOR JENSEN-SHANNON DIVERGENCE (JSD) VIA PREDICTIVE DIVERSITY SCORE (PDS) In this section, we show that JSD (Definition 1) is bounded quadratically below and linearly above by PDS. We first relate JSD to total variation ( B.1), then show total variation is monotone in PDS ( B.2), and then combine these results in B.3. B.1 BOUNDS FOR JSD VIA TOTAL VARIATION (TV) We begin by showing that JSD is bounded quadratically below and linearly above by total variation. We first introduce the setup with required definitions ( B.1.1), then prove the proposition ( B.1.2). B.1.1 SETUP. Use notation from Appendix A. Let pym classes, and let pyi be the ensemble mean prediction random variable defined as the mixture for 1, . . . , be categorical random variables on Prppyi cq 1 Mÿ m1 Prppym cq. Definition 2 (Total variation). For categorical random variables X, on the same support, TVpX, 1 2 B.1.2 PROPOSITION. ÿ PrpX cq PrpY cq. Now, we show that JSD is bounded quadratically below and linearly above by total variation. Proposition 4 (JSDTV sandwich bounds). For any ě 2 categorical random variables tpym with ensemble mean pyi, uM 2 ln 2 1 Mÿ m1 TVppym ` , pyiq2 ď JSD , . . . , pyM py1 ď 1 log 1 Mÿ m1 TVppym , pyiq. Proof. Lower bound. From Definition 1, ` JSD py1 , . . . , pyM 1 Mÿ m1 DKLppym }pyiq. By Pinskers inequality (e.g. Equation 1 in (Sason, 2015)), DKLpX}Y ě 2 ln TVpX, q2. Substituting pym , pyi and averaging over yields the lower bound. Upper bound. Fix m. For each class c, write Prppyi cq α Prppym cq ` p1 αqζpcq, α 1 , ζpcq 1 1 ÿ sm Prppys cq. 15 Define tpcq ζpcq{ Prppym 0). Then Epym rts 1 and cq when Prppym cq ą 0 (set tpcq `8 if Prppym cq 0, ζpcq ą"
        },
        {
            "title": "DKLppym",
            "content": "i }pyiq Epym ` log α ` p1 αqt . Let gpuq logpα ` p1 αquq, ě 0. Then is convex, decreasing, with gp1q 0, gp0q logp1{αq log . By convexity, gpuq ď p1 uqgp0q p1 uq log M. Thus,"
        },
        {
            "title": "DKLppym",
            "content": "}pyiq ď log Epym p1 tq ď log Epym p1 tq` . Epym rp1 tq`s ÿ c"
        },
        {
            "title": "Prppym",
            "content": "i cq maxt0, 1 ζpcq{ Prppym cqu ÿ pPrppym cq ζpcqq`. By the balance-of-deviations identity ( 1), ÿ pPrppym cq ζpcqq` TVppym , ζq. Finally, since Prppyi cq α Prppym cq ` p1 αqζpcq, one has , pyiq. , ζq 1 TVppym TVppym Combining yields DKLppym }pyiq ď 1 log TVppym , pyiq. Averaging over gives the upper bound. Remark 1. The lower bound is quadratic in total variation, the upper bound linear. Thus, JSD interpolates between quadratic growth near equality and linear growth in worst-case separation. B.2 BOUNDS FOR TOTAL VARIATION VIA PREDICTIVE DIVERSITY SCORE We next show that total variation is monotone in PDS. We introduce the setup with definitions and lemmas ( B.2.1), then prove the proposition ( B.2.2). B.2.1 SETUP. Fix class c. Let Xm Prppym Definition 3 (Envelope and spread, per class). cq and µ Prppyi cq. Define: Ec max Xm µ, Uc 1 2M Mÿ Xm µ. Definition 4 (Predictive Diversity Score). ` PDS py1 , . . . , pyM Kÿ c1 max Prppym cq. Lemma 1 (Balance-of-deviations identity). For any a1, . . . , aM with maxt0, au, Mÿ Mÿ pamq` m1 m1 pamq 1 Mÿ m1 am. 16 ř am 0, writing a` Proof. Decompose a` a, a` ` a. Summing and using ÿ ÿ ÿ ÿ ř am 0 gives am,` am, 0 ñ am,` am,."
        },
        {
            "title": "Then",
            "content": "ÿ ÿ m ÿ am pam,` ` am,q 2 am,`. m Applying Lemma 1 with am Xm µ yields Uc 1 ÿ pXm µq. m:Xmąµ B.2.2 PROPOSITION. Now, we show that total variation is monotone in PDS. Proposition 5 (Spreadenvelope bounds). Use notation from Appendix B.2.1. For each class c, if at most models satisfy Xm ą µ, then Aggregating over classes, where 1 Ec ď Uc ď 1 ď ď M Ec. E, Kÿ Ec, c1 Kÿ c1 Uc 1 Mÿ m1 TVppym , pyiq. Proof. If Ec 0, then Xm µ for all so Uc 0. Otherwise, let arg maxm Xm. Then pXm µq ě 1 pXm µq 1 Uc 1 Ec. ÿ m:Xmąµ For the upper bound, each positive term is at most Ec, and there are at most such terms, hence Uc ď Summing over classes gives the aggregated bound. Ec. B.3 FINAL SANDWICH INEQUALITY Finally, we combine results from B.1.1 and B.2 to show that JSD is bounded quadratically below and linearly above by PDS. Proposition 6 (JSDPDS sandwich). Use notation from Proposition 1. 2 2 ln 2 ` pPDS , . . . , pyM py1 ` 1q2 ď JSD , . . . , pyM py1 ď 1 ` log pPDS , . . . , pyM py1 1q. Proof. From Theorem 4, By the definitions, JSD ě 2 ln 2 2, JSD ď 1 log U. and Kÿ c1 1 2M Mÿ Prppym cq Prppyi cq 1 Mÿ m1 TVppym , pyiq, Kÿ c1 max cq Prppyi cq Prppym ` PDS , . . . , pyM py1 1. From Proposition 5, Combining and noticing that 1 ď ď yields the quadratic lower bound and linear upper bound in pPDS 1q. 1 pPDS 1q ď ď pPDS 1q. Figure 7: ImageNet performance estimation vs. compression rates. Mean absolute error (MAE), measured in %p difference in accuracy, and the Spearman rank correlation between the true model ranking and the estimated model ranking are shown. At 100 samples, the results are identical to Table 3. Main observations: Same as for language experiments DISCO hits better efficiencyprecision trade-off across all range of compression rates."
        },
        {
            "title": "C VISION RESULTS",
            "content": "We introduce the setup and present results. Dataset. We use ImageNet-1k (Russakovsky et al., 2015) with 1.28 million images. Models. We consider 400 models from timm (Wightman, 2019) that are pretrained on ImageNet. The models cover convolutional (Krizhevsky et al., 2012) and transformer (Dosovitskiy et al., 2021) architectures. Model sizes range from 0.3M to 300M parameters. Model Split. As in the language domain, we use the chronological split. The cutoff date is 5 April 2023. The train-test ratio of models is 88:12. Evaluation. We use mean absolute error (MAE) and Spearman rank correlation between the true and predicted performances. Results. Table 3 shows the main results. Uniform random sampling, together with direct evaluation with the corresponding annotated labels, yields 3.03%p MAE and .652 rank correlation at 100 samples. We evaluate the effectiveness of DISCO in two stages. First, we apply the model-signature approach using uniform random sampling. Then, we enhance it by selecting samples based on predictive diversity score (PDS). The results follow similar trend. With uniform random sampling, model signatures combined with Random Forest achieve 0.86%p MAE and rank correlation of .944, significantly outperforming the naive baseline. Incorporating PDS further improves performance, reaching 0.63%p MAE and rank correlation of .969. To illustrate how well the estimated performances align with the true values, we present scatter plot in Figure 6. The high Pearson correlation coefficient of .969 indicates strong agreement between the two. Figure 6: True and estimated accuracy on ImageNet for 50 models. Figure 7 shows performance across varying levels of test set reduction. The relative ranking of evaluation methods remains largely stable, except for the kNN predictor, which degrades as the number of anchor points increases. Notably, DISCO consistently outperforms all baselines, even under extreme compression with as few as 10 samples."
        }
    ],
    "affiliations": [
        "Parameter Lab",
        "Tübingen AI Center, University of Tübingen"
    ]
}