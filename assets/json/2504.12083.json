{
    "paper_title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization",
    "authors": [
        "Pritam Sarkar",
        "Ali Etemad"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose significant challenges to their safe and reliable deployment in real-world applications. To address these limitations, we propose a self-alignment framework that enables LVLMs to learn from their own errors. Our proposed framework first obtains a training set of preferred and non-preferred response pairs, where non-preferred responses are generated by incorporating common error patterns that often occur due to inadequate spatio-temporal understanding, spurious correlations between co-occurring concepts, and over-reliance on linguistic cues while neglecting the vision modality, among others. To facilitate self-alignment of LVLMs with the constructed preferred and non-preferred response pairs, we introduce Refined Regularized Preference Optimization (RRPO), a novel preference optimization method that utilizes sub-sequence-level refined rewards and token-wise KL regularization to address the limitations of Direct Preference Optimization (DPO). We demonstrate that RRPO achieves more precise alignment and more stable training compared to DPO. Our experiments and analysis validate the effectiveness of our approach across diverse video tasks, including video hallucination, short- and long-video understanding, and fine-grained temporal reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 3 8 0 2 1 . 4 0 5 2 : r Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization Pritam Sarkar Queens University, Canada and Vector Institute pritam.sarkar@queensu.ca Ali Etemad Queens University, Canada ali.etemad@queensu.ca Website Code Models"
        },
        {
            "title": "Abstract",
            "content": "Despite recent advances in Large Video Language Models (LVLMs), they still struggle with fine-grained temporal understanding, hallucinate, and often make simple mistakes on even simple video question-answering tasks, all of which pose significant challenges to their safe and reliable deployment in real-world applications. To address these limitations, we propose self-alignment framework that enables LVLMs to learn from their own errors. Our proposed framework first obtains training set of preferred and non-preferred response pairs, where non-preferred responses are generated by incorporating common error patterns that often occur due to inadequate spatio-temporal understanding, spurious correlations between co-occurring concepts, and over-reliance on linguistic cues while neglecting the vision modality, among others. To facilitate self-alignment of LVLMs with the constructed preferred and non-preferred response pairs, we introduce Refined Regularized Preference Optimization (RRPO), novel preference optimization method that utilizes sub-sequence-level refined rewards and token-wise KL regularization to address the limitations of Direct Preference Optimization (DPO). We demonstrate that RRPO achieves more precise alignment and more stable training compared to DPO. Our experiments and analysis validate the effectiveness of our approach across diverse video tasks, including video hallucination, shortand long-video understanding, and fine-grained temporal reasoning."
        },
        {
            "title": "Introduction",
            "content": "Despite recent progress in Large Video Language Models (LVLMs) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], these models continue to face limitations in finegrained temporal understanding [16, 17, 18], demonstrate propensity for hallucination [19, 20], struggle with long-video understanding [21, 22, 23, 24, 25], and frequently make naive mistakes in shortvideo question-answering tasks [16, 17, 18]. Please see Figure 1 for number of examples. These shortcomings severely limit their safe and reliable deployment in realworld applications. The underlying causes Preprint. Under review. Figure 1: Examples of failure cases in simple video understanding tasks by state-of-the-art LVLMs, and improvements observed after self-alignment with RRPO. of these limitations are multifaceted, encompassing factors such as inadequate spatial and temporal understanding [16, 17, 18], vision-language representational misalignments [26, 22], challenges in processing long visual sequences due to context length constraints [21, 22, 23], spurious correlations between co-occurring concepts [27, 28], and an over-reliance on linguistic cues while neglecting the visual information [16, 17, 18, 29]. To address these shortcomings and enhance video understanding in LVLMs, we design self-alignment [30] framework that enables LVLMs to learn from their own errors. Specifically, we begin by sampling video-question pairs from an open-source benchmark. We then apply spatio-temporal perturbations to the video content to mimic common errors that often arise from overreliance on linguistic cues, spurious correlations between co-occurring concepts, and insufficient spatio-temporal understanding. Using the perturbed video and the corresponding question, we perform inference with the target LVLM. If the models response is incorrect, we construct selfalignment pair by treating the incorrect response as non-preferred sample and the correct response as the preferred one, which is kept for self-alignment training. Responses that are already correct are discarded, as they offer limited self-improvement potential. Subsequently, we optimize loss function to prioritize the preferred response over the non-preferred one. Notably, our data generation pipeline is free from human annotation and can be easily scaled. high-level overview of our self-alignment framework is depicted in Figure 2. Figure 2: An overview of our self-alignment framework. To effectively train LVLMs using the generated preferred and non-preferred response pairs, we introduce Refined Regularized Preference Optimization (RRPO), an approach designed to address limitations of Direct Preference Optimization (DPO) [31]. RRPO is designed to address the key drawbacks of DPO. RRPO provides fine-grained sub-sequence-level reward to explicitly penalize the tokens containing the key concepts that are different between the preferred and non-preferred response pairs. This is contrary to DPOs response-level reward which penalizes all tokens within nonpreferred responses, thus lacking precision and often proving unsuitable for fine-grained alignment. It should be noted that our fine-grained reward further benefits from computing smaller gradient during optimization and is therefore less prone to diverge away from its initial state unlike DPOs response-level feedback along with weak regularization which can cause significant divergence from the base model, leading to suboptimal performance. However, sub-sequence-level rewards may incentivize the model to exploit shortcuts, such as outputting correct concepts without proper context or complete responses. To mitigate this, we also minimize token-wise KL-divergence [27] using reference model on the preferred responses. This ensures that the LVLM maintains tight bound on its likelihood across the entirety of the preferred response while reducing the likelihood on the non-preferred concepts. Through empirical and theoretical analysis, we demonstrate that RRPO exhibits greater stability and smoother convergence during optimization compared to DPO. We validate our method on three popular LVLMs specialized for video understanding, VideoChat2, LLaVA-Video, and longVU, covering wide range of architectures, LLMs, vision encoders, and training setups. Our in-depth evaluation demonstrates that our proposed self-alignment reduces hallucinations and improves performance in fine-grained temporal understanding, as well as in both shortand long-video understanding tasks. In summary, our contributions are as follows: We design self-alignment framework to facilitate self-improvement of LVLMs based on their own errors. We introduce RRPO, preference optimization method that addresses the limitations of DPO by utilizing sub-sequence-level refined rewards and token-wise strong KL regularizer, resulting in more precise alignment and stable training. Our rigorous evaluation demonstrates the effectiveness of our proposed method across diverse video tasks, including video hallucination, short and long video understanding, and fine-grained temporal reasoning, among others. Moreover, our experimental and theoretical analysis highlight the superiority of RRPO over DPO in aligning LVLMs. We make our code, data, and model weights public to enable fast and accurate reproducibility."
        },
        {
            "title": "2 Preference Responses for Self-alignment",
            "content": "Figure 3: An example of perturbed video. As the first step in our framework, we construct training dataset comprising both preferred responses and responses containing incorrect concepts in order to align the LVLM πθ to favor correct concepts over incorrect ones. We begin by utilizing large and diverse publicly available video instruction tuning dataset, containing triplets of video xv, question xq, and their human-preferred answers y+. To generate non-preferred responses y, we obtain perturbed videos ˆxv = fp(xv), where fp is perturbation function which masks large portion of frames and applies temporal shuffling, compromising spatiotemporal consistency. Our intention from this step is to provoke the LVLM to generate responses primarily based on language cues or their parametric knowledge. These perturbed videos ˆxv are fed to πθ as inputs to generate responses with potential erroneous concepts = πθ(xq, ˆxv). An example is illustrated in Figure 3. We then verify the correctness of y, retaining incorrect responses and discarding correct ones. Next, We employ an LLM to meticulously compare against y+ and identify key incorrect concepts within y, ensuring that for each incorrect concept y+. In the context of our work, concept can be actions, objects, attributes, relations, and other key elements in the response that contribute to the semantic understanding of the video. Furthermore, for lengthy responses, we maintain structural similarity between the incorrect and correct versions by rewriting the correct response while incorporating the incorrect concepts. Finally, we apply deduplication based on these correct-incorrect concept pairs, constructing high-quality training dataset. Examples are provided in Figure 4. Figure 4: few training samples. We bold the correct and incorrect concepts. there exists corresponding correct concept y+"
        },
        {
            "title": "3 RRPO",
            "content": "Preliminaries. Given an input = {xq, xv} with pair of responses {y+, y}, where y+ yx, DPO [31] can be employed to align πθ to favor y+ over y. This is achieved by maximizing the reward margin between πθ and reference model πref, using the following training objective: LDPO(πθ; πref) = E(x,y+,y)D (cid:20) log σ(cid:0)rθ(x, y+) rθ(x, y)(cid:1) (cid:21) , (1) where reward rθ(x, y) = β log πθ(yx) πref(yx) and β controls the deviation from πref. Note that rθ(x, y) is calculated at response-level by penalizing all the tokens in y, despite the fact that the difference between y+ and might only be limited to few key tokens. Such response-level reward can be considered coarse-grained reward modeling and is not suitable for fine-grained alignment. Moreover, as the reward is calculated by penalizing all the tokens in the response, the gradient for LDPO tends to be very large for long responses, and accordingly πθ can diverge to an undesired state thus losing its out-of-the-box capabilities [27, 32, 33]. Refined Regularized Preference Optimization. Our goal is to design method that can provide fine-grained reward to penalize individual sub-sequences consisting of the tokens belong to the key differing concepts between y+ and y. We refer to this as refined reward modeling. Let be expressed as sequence of tokens = {t1, t2, . . . , tT }. Here, the i-th sub-sequence yi can be expressed as [si : ei], where si and ei are the start and end indices of yi with 1 si ei . Therefore, the reward for yi can be computed as: rθ(x, yi) = β log (cid:81)ei (cid:81)ei j=si j=si 3 πθ(tjx, t<j) πref(tjx, t<j) . (2) Assuming there exists such sub-sequences, we can train πθ to maximize the total reward margin = (cid:88) i=1 ui = (cid:88) i=1 (cid:0)rθ(x, y+ ) rθ(x, )(cid:1). (3) Subsequently, we replace the reward formulation in Equation 1 with our refined reward modeling as: L(rank) RRPO(πθ; πref) = E(x,y+,y)D (cid:104) (cid:105) log σ(u) . (4) However, the sparsity of the rewards used to calculate L(rank) RRPO may allow πθ to exploit shortcuts and effectively game the reward function by merely outputting key concepts without appropriate context or generating complete responses. This reward hacking may occur since πθ is incentivized to maximize reward margin based on the differing sub-sequences; the model can learn to produce short, incomplete responses that contain the correct key concepts, even if those responses lack overall coherence, fluency, or completeness. To mitigate this reward hacking, we introduce regularizer between πθ and πref, based on token-wise KL (TKL) divergence [27, 34], as follows: DTKL (cid:0)x, y; πref πθ (cid:1) = (cid:88) t=1 DKL (cid:0)πref( [x, y<t]) πθ( [x, y<t])(cid:1). (5) Accordingly, to ensure πθ retains its original likelihood across the entirety of y+ while reducing the likelihood of non-preferred concepts, we optimize DTKL between πθ and πref over y+. We then modify Equation 4 and define the final RRPO loss as: (cid:20) log σ(u) + αDTKL LRRPO(πθ; πref) = E(x,y+,y)D (cid:0)x, y+(cid:1) (6) (cid:21) , where α controls the divergence between πθ and πref, and πref is kept fixed. How is RRPO update different from DPO? The gradient for LRRPO can be obtained as: (cid:20) log σ(u) + αDTKL θLRRPO = θE (cid:0)x, y+(cid:1) (cid:21) (cid:20) θ log σ(u) + α θDTKL = (cid:0)x, y+(cid:1) (cid:21) . (7) (cid:105) σ(u) θu First, lets calculate the gradient for the ranking loss. Using chain rule, we can write θ log σ(u) = (cid:104) σ(u) σ(u) = 1 σ(u) = σ(u). (cid:105) σ(u) θu . Recalling our sub-sequence-level reward modeling defined . Using the identity σ(u) = σ(u)(1 σ(u)), we have σ(u) (cid:104) Therefore, θ log σ(u) = in Equation 3, we can obtain: θu = (cid:88) i=1 θui = β (cid:88) (cid:16) e+ i(cid:88) i=1 j=s+ θ log πθ(t+ x, t+ <j) i(cid:88) j=s θ log πθ(t x, (cid:17) <j) , (8) as πref is fixed. Assuming the norm of the gradient of the log-probability for any single token is bounded, θ log πθ(tjx, t<j) for some > 0, and that each differentiating sub-sequence or y+ has an average length L, we can further bound the gradient norm of the ranking loss: θL(rank) (cid:104) RRPO (cid:105) σ(u)θu β (cid:88) i=1 (M L+ + ) βM (2N L), (9) where is the average length of the sub-sequences (L L+ ). In contrast, the DPO gradient involves sums over the entire lengths of y+ and y. similar analysis for DPO yields bound proportional to the total lengths: θLDPO βM (y+ + y). Crucially, the total length of the differentiating sub-sequences is typically much smaller than the total length of the full responses, i.e., 2N y+ + y. Therefore, the upper bound on the gradient magnitude stemming from the ranking objective is smaller for RRPO compared to DPO: (10) θL(rank) RRPO θLDPO. 4 Now, lets consider the term θDTKL() in Equation 7. Based on the formulation of DKL(πrefπθ) = (cid:80) πθ(a) where represents token in the vocabulary, and since πref fixed, we derive: πref(a) log πref(a) θDTKL(x, y+) = y+ (cid:88) θDKL() = y+ (cid:88) (cid:88) πref(a x, y+ <t) θ log πθ(a x, y+ <t). (11) Note that θDTKL is always negative. Therefore, for α > 0 in Equation 7, the gradient magnitude of LRRPO is further reduced than L(rank) RRPO. t=1 t=1 θLRRPO < θL(rank) RRPO < θLDPO This mathematical derivation confirms that the proposed RRPO loss function effectively reduces the gradient, as initially hypothesized. The reduced gradient of RRPO ensures more stable updates compared to DPO in gradient-based optimization while simultaneously enabling precise penalties on specific tokens without the risk of significant divergence. Furthermore, the DTKL term acts as trust-region constraint [35], preventing the model from making large, destabilizing updates. As result, RRPO allows larger learning rates and yielding smoother convergence in practice. We present the pseudocode of RRPO in Appendix A."
        },
        {
            "title": "4 Experiment Setup",
            "content": "Base models. We use VideoChat27B [3], LLaVA-Video7B (also known as LLaVA-Next-Video7B) [9], and LongVU7B [1] as our base models. These models are carefully selected to evaluate our method across diverse LLM architectures, vision encoders, cross-modal adapters, and training setups. For instance, among these models, VideoChat2 employs video encoder, while the others rely on image encoders. Additionally, LongVU incorporates two vision encoders, whereas the rest use single vision encoder. VideoChat2 utilizes QFormer [36] as its cross-modal adapter, whereas LLaVA-Video employs MLP projection layers. These models further differ in their LLM architectures, context lengths, and training setups, among other aspects. Training data. Based on the availability and diversity of video-language instructions, we use VideoChat-IT [3] as our primary source for training samples. Specifically, we select subset of VideoChat-IT encompassing eight video datasets: Kinetics700 [37], Something-Something-v2 [38], VideoChat [39], VideoChatGPT [40], CLEVRER [41], NEXTQA [42], EgoQA [43], and TGIF [44]. These datasets span range of tasks, including video description, question answering, reasoning, and conversation. For the perturbation step, we mask significant portion (25%-50%) of each frame and shuffle the temporal order. We explore three types of temporal perturbations: (i) random shuffling, (ii) local shuffling, and (iii) global shuffling. In random shuffling, frames are shuffled arbitrarily across time. For local shuffling, frames are initially segmented into smaller chunks, and the frames within each chunk are then shuffled. In global shuffling, the order of these chunks is shuffled, rather than individual frames. During inference, based on the LVLMs input capacity, we utilize maximum of 16, 64, and 100 frames for VideoChat2, LLaVA-Video, and LongVU, respectively. Table 1: Key statistics of the generated training samples. Following the generation of the responses, we verify their correctness. For Multiple Choice Questions (MCQ) and Binary Question Answering (BinaryQA) tasks, verification is straightforward, using regex-based checks. However, for open-ended questions, this method proves inadequate, as semantically equivalent responses can be expressed in diverse phrasings. Consequently, for open-ended questions, we employ powerful LLM, GPT-4o-mini [45], as judge [46, 47, 48, 49], to ascertain correctness by comparing the generated response with the ground truth from the video instruction tuning dataset. Additionally, for long responses, we employ GPT-4o-mini to rewrite the correct response while incorporating the incorrect concepts from the generated response. This ensures that correct and incorrect concepts are aligned across both preferred and non-preferred responses. The key statistics of our training data are presented in Table 1. The prompts used during pre-processing with GPT-4o-mini are in Appendix D. VideoChat27B LongVU7B LLaVA-Video7B Unique pairs 25K 22K 21K 18K 14K 16K # Samples Model Implementation details. For all base models, we utilize LoRA [50] for training, applying it specifically to the LLMs while freezing all other parameters. Unless otherwise specified, we utilize 16 frames for self-alignment training; the rest follows the default training setup of each respective base model. We use 4 A100 80GB GPUs for training, with the training time varying between 1 to 10 hours. Additional implementation details and hyperparameters are provided in Appendix E. Evaluation benchmarks. To assess the impact of our self-alignment framework, we conduct evaluations across diverse range of video understanding tasks. Specifically, we choose TVBench [17] and TempCompass [20] for fine-grained temporal understanding, VideoHallucer [19] and VidHalluc [51] for video hallucination, MVBench [3] and VideoMME [52] for short video understanding, and MLVU [24] and LongVideoBench [25] for long video understanding. It should be noted that these benchmarks, while selected for specific tasks, often assess overlapping video understanding capabilities. For instance, while VideoHallucer and VidHalluc are primarily used for hallucination detection, they also evaluate temporal grounding [19, 51]. Similarly, while TVBench mainly focuses on temporal understanding, it covers short video understanding as well [17]. Given its inclusion of short, medium, and long videos, VideoMME explores video understanding of varying lengths."
        },
        {
            "title": "5 Results and Analysis",
            "content": "This section details our experimental evaluation, which encompasses comprehensive and comparative analysis against other alignment methods and existing off-the-shelf aligned LVLMs. Furthermore, we present detailed analysis focusing on the trade-offs between post-alignment divergence and performance. To gain deeper insights into the impact of our proposed method, we conduct experiments exploring its effects on fine-grained temporal understanding, hallucination mitigation, and performance on comprehensive video understanding of varying lengths. Finally, our evaluation includes an investigation into the influence of data, the scaling of input frames, and the presence of subtitles on the performance of our method. We conclude with discussion regarding the generalization capabilities of our approach. Table 2: Comparison with existing preference optimization methods and RRPO ablation vari- (cid:80)(accaligned accbase) and % = ants. = 1 (cid:80)(accaligned accbase)/accbase, where is 100 the number of evaluation benchmarks used for each ablation. RRPO consistently outperforms existing alignment methods. TVB VHall VMME MLVU /% LongVU7B (base) 53.7 39.2 54.3 40.9 54.6 40.3 53.9 41.4 54.2 41. + DPO [31] + DPA [27] + TDPO [34] + DDPO [53] + RRPO w/o 54.3 43.0 + RRPO w/o DTKL 54.9 39.1 56.2 56.6 56.9 57.0 56.7 57.8 57.4 63.6 63.6 63.9 63.8 63. 64.5 63.9 0.7/1.5 0.7/1.5 0.8/1.9 0.9/2.0 1.7/3.8 0.6/1.1 + RRPO (ours) 56.5 44. 57.7 64.5 2.5/5.4 ( Abbreviations, TVB: TVBench; VHall: VideoHallucer; VMME: VideoMME) Figure 5: Performance relative to model divergence. RRPO exhibits the best performancedivergence trade-off. Q1. How well does RRPO perform compared to other alignment methods? We conduct an in-depth analysis of RRPO against other recent preference optimization methods designed to address DPOs limitations, namely TDPO [34], DDPO [53], and DPA [27] and present the results in Table 2. More specifically, DDPO was introduced to provide fine-grained rewards, TDPO to enhance DPOs regularization, and DPA to address both of these challenges. detailed discussion of their objectives, highlighting similarities and differences, is provided in Appendix B. Furthermore, we include two ablation variants of RRPO: one without the refined reward (RRPO w/o R) and the other without the token-wise KL regularizer (RRPO w/o DTKL). Our study covers various aspects of video understanding, including fine-grained temporal understanding on TVBench, hallucination mitigation on VideoHallucer, comprehensive video understanding on VideoMME, and long video understanding on MLVU. The results presented in Table 2 demonstrate that RRPO consistently outperforms DPO, DPA, TDPO, and DDPO across all benchmarks. Moreover, among the ablation variants of RRPO, the inclusion of DTKL alone yields significant performance gains, which are further enhanced by incorporating the refined reward. Table 3: Comparison with off-the-shelf aligned LVLMs. Ours LLaVA-Video-RRPO outperforms LLaVA-Video-TPO across all setups, both of which are based on LLaVA-Video7B. Model TV Bench TempCompassAvg Video Hallucer Vid Halluc MV Bench Video MME MLVUval (M-Avg) LongVideo BenchVal LLaVA-Video-TPO [54] LLaVA-Video-RRPO (ours) 51.1 52. 66.1 67.4 50.6 55.8 76.3 76.6 60.6 62.0 65.6/71.5 65.5/71.8 68.7 69. 60.1 61.3 Table 4: Evaluating our self-aligned LVLMs on diverse video understanding benchmarks. We bold the best in each group. 16f and 32f indicate the number of frames utilized during training. #F indicates number of frames used during inference. Here, we presents the overall results averaged across sub-categories where applicable, with detailed results available in Appendix C. MLVUval (M-Avg) Video Hallucer LongVideo BenchVal TempCompassAvg Vid Halluc MV Bench TV Bench Video MME Models #F Video-LLaVA7B [10] VideoLLaMA27B [5] LongVA7B [21] MiniCPM-V 2.67B [55] NVILA7B [56] LongVILA7B [8] Qwen2-VL7B [57] LLaVA-NeXT-Video-DPO7B [58] ShareGPT4Video8B [59] PLLaVA13B [60] Aria8x3.5B [61] LLaVA-NeXT-Video-DPO34B [58] Qwen2-VL72B [57] GPT-4o [62] Gemini 1.5 Pro [11] (The above models are presented for reference only, and may not be suitable for direct comparisons.) 17.8 10.0 32.0 15.8 41.2 32.3 53.3 37.8 49.9 43.4 57.0 66.3 67.9 53.8 61.5 69.6 73.8 67.1 33.8 42.9 43.8 51.0 52.7 39.9 47.6 40.3 66.3 30.9 48.2 49.5 81.2 72.8 VideoChat27B [3] VideoChat27B + DPO (16f) + RRPO (16f) (Ours) LLaVA-Video7B [9] LLaVA-Video7B + DPO (16f) + RRPO (16f) (Ours) + RRPO (32f) (Ours) LongVU7B [1] LongVU7B + DPO (16f) + RRPO (16f) (Ours) 44.0 45.7 45.8 45.6 51.0 51.9 51.9 52.2 53.7 54.3 56.5 16 16 16 64 64 64 1fps 1fps 1fps 50.8 59.3 60.0 60.2 66.0 66.4 66.8 67.4 63.9 64.3 64.5 7.8 23.1 22.1 32.9 50.0 53.3 55.7 55. 39.2 40.9 44.0 73.3 72.4 76.4 76.6 76.5 76.5 76.6 67.3 68.5 71.7 42.5 54.6 68.1 67.1 67.0 69.7 73.6 49.1 60.5 60.4 60.2 59.6 59. 58.6 61.1 60.6 62.2 62.1 66.9 65.5 65.9 66.8 39.9 62.4 52.6 60.9 64.2 60.1 63.3 39.9 67.6 71.2 71.9 75.0 39.5 41.0 43.0 44.3 63.3 64.0 63.1 64.5 64.5 56.2 56.6 57. 29.3 48.4 42.1 70.1 65.5 34.2 54.5 46.4 47.4 47.9 70.8 68.6 67.4 69.1 69.4 65.4 63.6 63.6 64.5 39.1 54.9 57.7 57.1 55.6 43.5 39.7 45.6 63.0 50.5 66.7 64.0 39.3 40.4 41.0 42. 58.2 60.1 59.4 60.4 60.1 48.6 49.4 49.7 For transparency, we report base LVLMs results from the literature where available, marked in grey. Differences from our reproduced results, both higher and lower, stem from variations in frame count, input prompt, GPT variant used for evaluation, and occasional implementation issues. For fairness, self-aligned variants are compared against our reproduced results. Q2. How much does the model diverge post alignment? Understanding the extent of model divergence after alignment is essential to ensure that the process refines behavior without undermining core capabilities as excessive divergence can lead to instability, reduced generalization, and loss of valuable pre-trained knowledge. Figure 5 presents comparative analysis of performance improvements against model divergence following preference optimization. Despite using 10 higher learning rate (which is facilitated through the smaller and more stable gradient updates), RRPO exhibits KL divergence of 1 compared to DPOs KL divergence of 20. While TDPO and DPA exhibit almost the same divergence as RRPO, their performance across all evaluation benchmarks is substantially worse. RRPO, in contrast, demonstrates the optimal performance-divergence trade-off. Q3. How does RRPO performance compared to off-the-shelf aligned LVLMs? We further evaluate our aligned LVLM against other off-the-shelf aligned LVLMs. Specifically, we compare our RRPO-trained LLaVA-Video with the concurrent work LLaVA-Video-TPO, both of which are based on the LLaVA-Video-Qwen7B [9]. LLaVA-Video-TPO is trained using combination of DPO and SFT with manually curated video-language dataset. The results in Table 3 demonstrate that RRPO is significantly more effective than concurrent methods. Our LLaVA-Video-RRPO outperforms LLaVA-Video-TPO across all setups, with performance gains of up to 5.2%. Table 5: Impact of different spatio-temporal perturbations in generating non-preferred responses. Default perturbations for each model are colored. TVB VHall VMME MLVU /% VideoChat27B (base) 44.0 23.1 + None + RS + Mask 40.7 16.0 43.0 23.3 44.0 26.2 44.6 28.2 + LS-Mask 44.2 28.8 + GS-Mask 45.8 32.9 + RS-Mask LLaVA-Video7B (base) 51.0 50.0 + LS-Mask + GS-Mask + RS-Mask LongVU7B (base) + LS-Mask + GS-Mask + RS-Mask 51.9 55.7 51.3 54.8 51.5 51. 53.7 39.2 56.5 44.0 55.0 43.9 55.1 42.4 41.0 39.9 41.8 43.4 44.6 44.1 44. 64.0 64.5 64.2 64.6 56.2 57.7 56.9 57.0 46.4 43.8 -3.5/-11.6 46.3 48.8 49.4 46.3 47.9 68.6 69.1 68.7 69.6 63. 64.5 64.5 64.3 0.0/0.1 2.0/6.1 3.1/9.7 2.2/8.1 4.1/14.4 1.9/3.7 1.4/2.7 0.9/1.6 2.5/5.4 1.9/4.3 1.5/3.3 Table 6: Impact of data size. TVB VHall VMME MLVU /% Baseline 51.0 50.0 + 5K 50.9 + 10K 51.2 + 15K 51.8 + 20K 51. 53.7 53.8 54.4 55.7 64.0 64.0 64.3 64.2 64.5 68.6 1.0/1.9 69.0 1.2/2.3 69.0 1.4/2.8 68.9 69.1 1.9/3. Table 7: Impact of varying the number of frames at inference. Table 8: Impact of using subtitles along with videos (VMME). 32 64 TVB MVB LVB without with Baseline 49.6 61.2 58.4 + RRPO 51.3 61.7 58.9 VideoChat27B + RRPO 41.0 48.0 44.3 49.4 Baseline 51.0 61.1 60.1 + RRPO 52.2 62.1 60.1 128 Baseline 49.4 60.5 60.3 + RRPO 51.3 61.2 61. + RRPO LongVU7B + RRPO LLaVA-Video7B 63.8 67.4 64.5 68.0 56.2 62.0 57.7 63.1 ( Abbreviations, RS: Random Shuffle; LS: Local Shuffle; GS: Global Shuffle; TVB: TVBench; VHall: VideoHallucer; VMME: VideoMME; LVB: LongVideoBench.) Q4. Does our method improve fine-grained temporal understanding? To evaluate this, we utilize TVBench [17] and TempCompass [20], designed to test the temporal understanding abilities of LVLMs. TVBench tests capabilities across various temporal tasks, including action localization, directional movement, and scene transitions, among others. Similarly, TempCompass evaluates performance on video captioning, caption matching, MCQ, and BinaryQA, covering video understanding tasks such as event ordering, action identification, and state change. As shown in Table 4, our method, RRPO, consistently improves base model performance by up to 2.8%, demonstrating its effectiveness in enhancing fine-grained temporal understanding and outperforming DPO across all setups. Q5. Does our method effectively mitigate hallucinations? Hallucination occurs when LVLMs produce responses that are ungrounded, referred to as intrinsic hallucination, or unverifiable, referred to as extrinsic hallucination. Hallucination presents significant obstacle to the reliable use of LVLMs. To evaluate the impact of our method on video hallucination, we employ VideoHallucer [19] and VidHalluc [51]. VideoHallucer tests LVLMs for both extrinsic and intrinsic hallucinations while including both spatial and temporal hallucinations. Additionally, VidHalluc focuses specifically on temporal hallucinations, such as action hallucination. As shown in Table 4, RRPO significantly reduces hallucination across all base models. Specifically, RRPO improves performance by 4.8% to 8.8% on VideoHallucer and by up to 4.4% on VidHalluc. In most cases, RRPO demonstrates substantial performance advantage over DPO, with gains reaching 10.8%. Q6. Does our method improve comprehensive video understanding across varying video lengths? To evaluate the comprehensive video understanding capabilities of LVLMs across varying video lengths, we leverage four benchmarks: MVBench [3], VideoMME [52], MLVU [24], and LongVideoBench [25]. Together, these benchmarks span wide variety of perception and reasoning tasks, focusing on objects, actions, their attributes, and holistic video understanding. Among these, MVBench is specifically designed for comprehensive evaluation of short videos, while MLVU and LongVideoBench offer thorough evaluations for long videos. VideoMME provides comprehensive assessment across videos of varying lengths. As shown in Table 4, consistent improvements are observed across all benchmarks for LongVU and LLaVA-Next. For VideoChat2, self-alignment leads to substantial gains in three out of four setups, with only minor regression in MVBench. Furthermore, RRPO consistently outperforms DPO, further demonstrating the advantages of fine-grained alignment in enhancing comprehensive video understanding. Q7. How do perturbations in our data generation pipeline impact the quality of non-preferred responses? To assess the impact of the perturbations on the quality of non-preferred responses, we conduct in-depth analyses and present the results in Table 5. Our key observations are as follows: First, non-preferred responses generated without video perturbations leads to diminished model performance, likely due to reduced generalizability. Second, temporal perturbations alone are ineffective, although their combination with masking significantly boosts performance. Among the spatio-temporal perturbations, random shuffling with masking (RS-Mask) improves performance for models processing fewer frames (e.g., VideoChat2) whereas local shuffling with masking (LS-Mask) proves superior for models handling longer sequences (e.g., LongVU, LLaVA-Video). 8 Q8. How does our method scale with training data? To test the impact of scaling the amount of data, we perform an experiment by varying the number of training samples from 5K to 20K, incrementing by 5K. As shown in Table 6, performance improves with data size. This suggests that our data-generation pipeline is effective in producing high-quality training samples for self-alignment. Q9. How does performance vary with the number of input frames? We investigate the effect of scaling the number of visual input frames during both inference and self-alignment training. For inference, we evaluate LLaVA-Video using 32, 64, and 128 frames across TVBench, MVBench, and LongVideoBench. The results are presented in Table 7. Our key observations are as follows: First, RRPO consistently improves performance over the base model across all setups. Second, neither the base model nor RRPO exhibits performance gains beyond 64 frames on TVBench and MVBench. This is likely due to the short-video nature of these benchmarks, where higher frame counts result in redundant frame resampling. However, for long-video understanding, increasing frame counts yields performance improvements, particularly for RRPO with 1.2% improvement compared to the base models 0.2% gain. Subsequently, we explore the impact of increasing the number of frames during self-alignment training. Specifically, we raise the frame count from our default 16 to 32. The results presented in Table 4 demonstrate that this increase enhances performance. Notably, we observe consistent performance improvement on fine-grained temporal understanding tasks, as evidenced by the gains on TVBench and TempCompass. Q10. Does our method retain its performance advantage with subtitles? Subtitles generally enhance video understanding by providing additional language cues that LVLMs can leverage. Thus, we investigate whether our method maintains its benefits over base models when subtitles are included. As shown in Table 8, our method demonstrates consistent improvements across all setups. Q11. Does our method generalize across diverse LVLM architectures and training setups? Given the rapid evolution of LVLMs, we carefully select VideoChat2, LLaVA-Video, and LongVU as the base models to cover wide variety of design choices and training methodologies (e.g., Table 4). Specifically, VideoChat2 uses the UMT [63] video encoder, while LLaVA-Video and LongVU use DINOv2 [64] and SigLIP [65] as their image encoders. On the other hand, LongVU employs dual vision encoders, unlike the others. Moreover, the cross-modal adapters range from query-based for VideoChat2 to MLP projections for LLaVA-Video, and combinations of both in the case of LongVU. Furthermore, the overlap between self-alignment training samples and instruction tuning data differs across models, with VideoChat2 having the highest overlap and LLaVA-Video having the least. Importantly, our self-alignment consistently improves performance across these diverse setups, even when reusing instruction tuning data."
        },
        {
            "title": "6 Related work",
            "content": "Recent years have seen surge in the development of LVLMs with improved video understanding capabilities [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]. This progress can be attributed to several key factors: (1) the development of diverse video benchmarks [3, 7, 66, 43, 67], which enable LVLMs to follow human instructions and reason across variety of video tasks; (2) architectural innovations that support the use of rich, dense visual features and enable efficient processing of long sequences [21, 8, 1, 68, 69, 70, 5, 71]; and (3) advancements in training algorithms for both the pre-training [72, 63, 65, 73] and post-training [74, 75, 31] stages. LVLM training generally involves multi-stage processes [76, 77], with pre-training typically focusing on representational alignment between video and language [3, 78, 36], and post-training refining the models ability to follow instructions [3, 78, 79], reduce hallucination [53, 27], improve reasoning skills [80, 81], and align the model with human preferences [80, 81]. Our introduced RRPO is post-training alignment method designed to enhance the overall video understanding capabilities of LVLMs. While concurrent works [82, 83, 54] have explored post-training alignments of LVLMs, they directly adopt DPO [31], which proves ineffective in fine-grained alignment, as discussed in Section 5."
        },
        {
            "title": "7 Conclusion",
            "content": "To improve the video understanding abilities of LVLMs, we design self-alignment framework that enables LVLMs to learn from their own errors. These errors commonly occur due to their lack of spatio-temporal reasoning, over-reliance on linguistic cues, and spurious correlations between 9 co-occurring concepts, among others. To effectively align LVLMs against such errors, we introduce RRPO, novel preference optimization method designed for fine-grained alignment through refined reward modeling with strong regularization. Our in-depth experiments and analyses show that RRPO training is more stable and highly effective compared to prior and concurrent preference optimization methods across diverse tasks. Moreover, the fine-grained reward modeling in RRPO improves capabilities without causing significant divergence from the base models. Our proposed self-alignment with RRPO exhibits consistent improvements across all setups over the base models, effectively reducing hallucination and improving spatio-temporal reasoning, thus enabling safer and more reliable use of LVLMs. Moreover, we show that the approach scales well with more data and high-resolution temporal inputs, and generalizes well across diverse LVLM architectures and training setups. Future work can further investigate iterative self-alignment methodologies with RRPO, moving beyond the static dataset used in this work."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank Ahmad Beirami for his valuable feedback and suggestions, which helped improve our paper. We also thank the Bank of Montreal and Mitacs for funding this research, and the Vector Institute for providing computational resources."
        },
        {
            "title": "References",
            "content": "[1] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 1, 5, 7, 9 [2] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv preprint arXiv:2407.15841, 2024. 1, 9 [3] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, pages 2219522206, 2024. 1, 5, 6, 7, 8, 9 [4] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023. 1, 9 [5] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1, 7, [6] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 1, 9 [7] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In CVPR, pages 1431314323, 2024. 1, 9 [8] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. 1, 7, 9 [9] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 1, 5, 7, 9 [10] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 1, 7, 9 [11] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 7, 10 [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, pages 2418524198, 2024. 1, 9 [13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 1, 9 [14] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization. Text Reading, and Beyond, 2, 2023. 1, 9 [15] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 9 [16] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, pages 95689578, 2024. 1, [17] Daniel Cores, Michael Dorkenwald, Manuel Mucientes, Cees GM Snoek, and Yuki Asano. Tvbench: Redesigning video-language evaluation. arXiv preprint arXiv:2410.07752, 2024. 1, 2, 6, 8 [18] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. In ACCV, pages 1834, 2024. 1, 2 [19] Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, and Zilong Zheng. Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models. arXiv preprint arXiv:2406.16338, 2024. 1, 6, 8 [20] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, arXiv preprint and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv:2403.00476, 2024. 1, 6, [21] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 1, 2, 7, 9 [22] Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu. V2pe: Improving multimodal long-context capability of vision-language models with variable visual position encoding. arXiv preprint arXiv:2412.09616, 2024. 1, 2 [23] Hongchen Wei and Zhenzhong Chen. Visual context window extension: new perspective for long video understanding. arXiv preprint arXiv:2409.20018, 2024. 1, 2 [24] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 1, 6, 8 [25] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for longcontext interleaved video-language understanding. NeurIPS, 37:2882828857, 2025. 1, 6, [26] Dong Shu, Haiyan Zhao, Jingyu Hu, Weiru Liu, Ali Payani, Lu Cheng, and Mengnan Du. Large vision-language model alignment and misalignment: survey through the lens of explainability. arXiv preprint arXiv:2501.01346, 2025. 2 [27] Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Ö Arık, and Tomas Pfister. Data-augmented phrase-level alignment for mitigating object hallucination. arXiv preprint arXiv:2405.18654, 2024. 2, 3, 4, 6, 9, 17 [28] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023. 2 [29] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024. 2 11 [30] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. NeurIPS, 36:25112565, 2023. [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 36:5372853741, 2023. 2, 3, 6, 9, 16 [32] Yuzi Yan, Yibo Miao, Jialian Li, Yipin Zhang, Jian Xie, Zhijie Deng, and Dong Yan. 3d-properties: Identifying challenges in dpo and charting path forward. arXiv preprint arXiv:2406.07327, 2024. 3 [33] Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata Chaudhuri, Shubham Mehrotra, Xiang-Bo Mao, Sitaram Asur, et al. comprehensive survey of llm alignment techniques: Rlhf, rlaif, ppo, dpo and more. arXiv preprint arXiv:2407.16216, 2024. 3 [34] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Tokenlevel direct preference optimization. arXiv preprint arXiv:2404.11999, 2024. 4, 6, 17 [35] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pages 18891897. PMLR, 2015. 5 [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 5, 9 [37] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Limin Wang, and Yu Qiao. Uniformerv2: Spatiotemporal learning by arming image vits with video uniformer. arXiv preprint arXiv:2211.09552, 2022. 5 [38] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In ICCV, pages 58425850, 2017. 5 [39] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 5 [40] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [41] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 29012910, 2017. 5 [42] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In CVPR, pages 97779786, 2021. 5 [43] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, pages 1899519012, 2022. 5, 9 [44] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In CVPR, pages 27582766, 2017. 5 [45] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 5 [46] Sébastien Bubeck, Varun Chadrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. [47] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. 5 [48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS, 36:4659546623, 2023. 5 12 [49] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023. [50] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5, 20 [51] Chaoyu Li, Eun Woo Im, and Pooyan Fazli. Vidhalluc: Evaluating temporal hallucinations in multimodal large language models for video understanding. arXiv preprint arXiv:2412.03735, 2024. 6, 8 [52] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 6, 8 [53] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, pages 1380713816, 2024. 6, 9, 16 [54] Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. Temporal preference optimization for long-form video understanding. arXiv preprint arXiv:2501.13919, 2025. 7, 9 [55] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [56] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. 7 [57] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 7 [58] Zhang, Li, Liu, Lee, Gui, Fu, Feng, Liu, and Li. Llava-next: strong zero-shot video understanding model. 2024. 7 [59] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. NeurIPS, 37:1947219495, 2024. 7 [60] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [61] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, et al. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. 7 [62] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 7 [63] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In ICCV, pages 1994819960, 2023. 9 [64] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 9 [65] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, pages 1197511986, 2023. [66] Orr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, and Serena Yeung-Levy. Videostar: Self-training enables video instruction tuning with any supervision. arXiv preprint arXiv:2407.06189, 2024. 9 13 [67] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, pages 17281738, 2021. 9 [68] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In CVPR, pages 1822118232, 2024. 9 [69] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. [70] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In ECCV, pages 202218. Springer, 2024. 9 [71] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. 9 [72] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. 9 [73] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PmLR, 2021. 9 [74] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. NeurIPS, 30, 2017. 9 [75] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 9 [76] Neelu Madan, Andreas Møgelmose, Rajat Modi, Yogesh Rawat, and Thomas Moeslund. Foundation models for video understanding: survey. arXiv preprint arXiv:2405.03770, 2024. 9 [77] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. arXiv preprint arXiv:2312.17432, 2023. 9 [78] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36:3489234916, 2023. 9 [79] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Instructblip: Towards general-purpose Wang, Boyang Li, Pascale Fung, and Steven Hoi. vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 9 [80] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 9 [81] Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391, 2025. 9 [82] Yogesh Kulkarni and Pooyan Fazli. Videosavi: Self-aligned video language models without human supervision. arXiv preprint arXiv:2412.00624, 2024. 9 [83] Daechul Ahn, Yura Choi, San Kim, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. i-srt: Aligning large multimodal models for videos by iterative self-retrospective judgment. arXiv preprint arXiv:2406.11280, 2024. 9 [84] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale model training. In USENIX ATC, pages 551564, 2021. [85] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zeroinfinity: Breaking the gpu memory wall for extreme scale deep learning. In SC, pages 114, 2021."
        },
        {
            "title": "Appendix",
            "content": "A RRPO Pseudocode (PyTorch Style) import torch import torch.nn.functional as def rrpo_loss(self, logits, ref_logits, phrase_ids, alpha, beta): \"\"\" logits: ref_logits: phrase_ids: correct_idx: wrong_idx: alpha: beta: \"\"\" logits from pi_theta. shape: (batch_size, sequence_length, vocab_size) logits from pi_ref. shape: (batch_size, sequence_length, vocab_size) phrase identifiers where tokens belonging to the same phrase share the same value; additionally, they remain the same between correct and misaligned phrases to maintain correspondence. shape: (batch_size, sequence_length) indices of the correct responses in batch indices of the wrong responses in batch coefficient to control the token-wise KL divergence coefficient to control reward/penalty # compute log probabilities logps = logits.log_softmax(dim=-1) ref_ps = ref_logits.softmax(dim=-1) ref_logps = ref_ps.log() # compute token-wise KL divergence token_wise_kl = (ref_ps * (ref_logps - logps)).sum(dim=-1) # compute the margin logps_margin = logps - ref_logps # accumulate log probabilities of the phrases with key concepts # here 0 indicates ignored tokens unique_phrase_ids = torch.unique(phrase_ids, sorted=True)[1:] phrase_logps_margin = torch.zeros( phrase_ids.size(0), len(unique_phrase_ids)) for i, phrase_id in enumerate(unique_phrase_ids): mask = (phrase_ids == phrase_id).float() phrase_logps_margin[:, i] = (logps_margin * mask).sum(dim=-1) chosen_logps_margin = phrase_logps_margin[correct_idx] rejected_logps_margin = phrase_logps_margin[wrong_idx] logits_margin = (chosen_logps_margin - rejected_logps_margin).sum(dim=-1) chosen_token_wise_kl = token_wise_kl.sum(dim=-1)[chosen_idx] losses = -torch.logsigmoid(beta * logits_margin) + alpha * chosen_token_wise_kl return losses 15 Figure S1: Comparison of characteristics among preference optimization methods. Both DDPO and DPO diverge significantly from their initial state after alignment. While TDPO and DPA are effective in restricting model divergence, they are less effective in improving performance. RRPO achieves excellent performance with minimal divergence from the base model."
        },
        {
            "title": "B Comparing Loss Formulation of RRPO and Other Methods",
            "content": "This section presents comparative analysis between RRPO and other preference optimization methods studied in this work, i.e., DPO, DDPO, TDPO, and DPA. We begin by outlining the respective loss functions, followed by detailed discussion of their similarities and differences. DPO. As previously discussed, the DPO [31] loss function is defined as: (cid:20) log σ(cid:0)rθ(x, y+) rθ(x, y)(cid:1) LDPO(πθ; πref) = (cid:20) = β log log σ (cid:16) β log (cid:21) , πθ(y+x) πref(y+x) πθ(yx) πref(yx) (cid:17)(cid:21) . (S1) The DPO loss function calculates reward over all tokens in y+ and y, despite the fact that there might be few sub-sequences that are conceptually different. This approach results in coarse-grained reward modeling, and because it penalizes all tokens in the response, the loss function accumulates large gradient and tends to diverge significantly from the base model, potentially resulting in weak alignment, as shown in Figure S1. RRPO is introduced to address two key challenges of DPO: to provide fine-grained feedback and restrict the divergence of the model away from its initial state. DDPO. DDPO [53] extends DPO by incorporating weighted reward based on the sub-sequence level differences between y+ and y, and is defined as: (cid:20) LDDPO(πθ; πref) = log σ (cid:16) β log πθ(y+x) πref(y+x) β log πθ(yx) πref(yx) (cid:17)(cid:21) , (S2) where log π(yx) = (cid:80) yiy log p(yix, y<i) is modified as: log π(yx) = 1 (cid:2) (cid:88) yiysame log p(yix, y<i) + γ (cid:88) log p(yix, y<i)(cid:3). yiydifferent Here, ysame and ydifferent indicate unchanged and changed segments between y+ and y. Moreover, γ > 1 is weighting hyperparameter, and larger γ indicates more weight on those changed segments. 16 While DDPO is designed to provide fine-grained feedback, its loss formulation is not as effective as RRPO and is also prone to diverging far from its initial state, similar to DPO, due to weak regularization, see Figure S1. TDPO. TDPO [34] is also derived from DPO, incorporating an additional regularization term (DTKL) between πθ and πref, which is defined as: (cid:20) LTDPO(πθ; πref) = log σ (cid:18) (cid:0)rθ(x, y+) rθ(x, y)(cid:1) (cid:16) βDTKL (x, yw; πref πθ) sg(cid:0)βDTKL (x, yc; πref πθ)(cid:1)(cid:17)(cid:19)(cid:21) . α (S3) As shown in Figure S1, TDPO is effective in restricting the divergence of the base model, but its performance is almost the same as DPO and falls short of our RRPO. DPA. DPA [27] is phrase-level alignment method unlike DPO and its variants. The DPA loss is composed of two terms where the first term computes the relative log-probability between two phrases of y+ and y, and the second term works as regularizer between πθ and πref, formulated as: LDPA(πθ; πref) = (cid:20) 1 (cid:88) i= log (y+ ) ) + (y ) (y+ + αDTKL (cid:0)x, y+; πref πθ (cid:1) (cid:21) , (S4) ) and (y where (y+ ) denote the probability of i-th phrase in y+ and y. Assume, is expressed as sequence of tokens {t1, t2, . . . , tT }, then the probability of i-th phrase can be computed as ei(cid:81) j=si πθ(tjx, t<j), where si and ei represent the start and end token indices. The loss formulation of RRPO draws inspiration from DPA to achieve fine-grained alignment without the risk of divergence. However, we identify fundamental limitation in DPA: it directly adjusts the probabilities of πθ to modify the probability ratio between preferred and non-preferred phrases. This approach leads to an inaccurate probability ratio after the initial pair of preferred and non-preferred segments, as subsequent segment probabilities become dependent on their preceding elements. Therefore, DPA is not accurate in providing fine-grained feedback for sequences composed of multiple sub-sequences of key concepts. As shown in Figure S1, DPA, while successful in controlling divergence, is ineffective in improving performance."
        },
        {
            "title": "C Additional Results",
            "content": "This section details the results for the subcategories of the evaluation benchmarks used in our study. Models VideoChat27B + RRPO LlavaVideo7B + RRPO + RRPO (32f) LongVU7B + RRPO Table S1: Detailed results on TempCompass. Caption Matching Captioning Multi-choice Yes-No Avg. 69.5 73.2 75.1 75.8 76.6 74.7 75.2 46.6 48.5 50.2 52.0 53.0 49.8 50. 58.0 56.6 67.6 68.6 68.7 63.9 64.7 63.0 62.6 71.0 70.9 71.3 67.3 67. 59.3 60.2 66.0 66.8 67.4 63.9 64.5 Table S2: Detailed results on VideoHallucer. Model Object relation Temporal Semantic Factual Non-factual Avg. VideoChat27B + RRPO LlavaVideo7B + RRPO + RRPO (32f) LongVU7B + RRPO 47.5 53.5 66.0 65.5 65.5 50.5 53.0 8.0 24.0 56.5 65.5 65.5 46.0 48. 38.5 55.0 65.5 71.0 71.5 43.0 50.0 1.0 5.0 13.5 23.5 23.5 17.0 26. 20.5 27.0 48.5 53.0 53.0 39.5 43.0 23.1 32.9 50.0 55.7 55.8 39.2 44. Table S3: Detailed results on VidHalluc. Model BinaryQA MCQ Scene Transition Avg. VideoChat27B + RRPO LlavaVideo7B + RRPO + RRPO (32f) LongVU7B + RRPO 66.8 72.7 77.9 78.4 78.6 71.4 74.2 84.9 85.5 91.4 91.6 91.7 87.0 88. 68.2 70.9 60.6 59.5 59.5 43.4 52.7 73.3 76.4 76.6 76.5 76.6 67.3 71. Table S4: Detailed results on VideoMME. Model Short Medium Long Avg. VideoChat27B + RRPO LlavaVideo7B + RRPO + RRPO (32f) LongVU7B + RRPO 49.0 52.2 76.3 76.6 76.7 66.1 67.7 38.6 41.9 62.8 63.1 62.9 54.7 55. 35.6 38.8 52.8 53.8 53.9 47.9 50.3 41.0 44.3 64.0 64.5 64.5 56.2 57."
        },
        {
            "title": "D Additional Details of Training Data",
            "content": "Prompt used in open-ended response processing stage 1 Thoroughly read the question and the given answers . Your task is to determine whether the \" Predicted answer \" is \" Correct \" or \" Wrong \" based on the \" Question \" and \" Reference answer \" . To determine correctness , focus on the key aspects in the answers , such as objects , actions , and their attributes , among others . The \" Predicted answer \" may have partial information in comparison to the \" Reference answer \" , in that case check whether at least the partial information can be fully verified based on the \" Reference answer \" . Please respond with any of the following and nothing else : - \" Correct \" if the predicted answer is correct based on the reference answer . - \" Wrong \" if the predicted answer is not fully correct based on the reference answer . - \" Undecided \" if you are not sure about their correctness . Question : { question } Reference answer : { ground_truth } Predicted answer : { generated_response } Prompt used in open-ended response processing stage 2 *** Turn 1*** Identify the key differences between these two sentences . To identify differences focus on the key aspects in the sentences , such as objects , actions , and their attributes , among others . If there are no key difference between these two sentences , please respond with \" None \" and nothing else . Sentence 1: { t e _ m _ u _ t } Sentence 2: { t e _ m _ e e _ p e } *** Turn 2*** Please rewrite the \" Sentence 1 \" by incorporating the differences you mentioned earlier . Your final response should contain only the revised sentence and nothing else . Sentence 1: { t e _ m _ u _ t }"
        },
        {
            "title": "E Implementation Details",
            "content": "Training hyperparameters. Table S5: Details of training hyperparameters. VideoChat2 LLaVA-Video LongVU LLM Vision encoder Trainable module LoRA setup [50] Learning rate Learning rate scheduler Optimizer Weight decay Warmup ratio Epoch Batch size per GPU Batch size (total) α (loss coefficient) β (loss coefficient) Memory optimization Mistral UMT LoRA in LLM and everything else is kept frozen rank=128, alpha=256 Qwen2 SigLIP+DINOv2 Qwen2 SigLIP 2e-5 Cosine AdamW 0.02 - 1 2 32 0.01 0.9 - 5e-6 Cosine AdamW 0.0 0.03 1 1 32 0.01 0.1 Zero stage 3 [84, 85] 5e-6 Cosine AdamW 0.0 0.03 1 1 32 0.05 0.5 FSDP Licenses of existing assets used. VideoChat2 (Apache License 2.0): https://huggingface.co/OpenGVLab/VideoChat2_stage3_Mistral_7B LLaVA-Video (Apache License 2.0 ): https://huggingface.co/lmms-lab/LLaVA-Video-7B-Qwen2 LongVU (Apache License 2.0): https://huggingface.co/Vision-CAIR/LongVU_Qwen2_7B VideoChat-IT (MIT): https://huggingface.co/datasets/OpenGVLab/VideoChat2-IT"
        }
    ],
    "affiliations": [
        "Queens University, Canada",
        "Vector Institute"
    ]
}