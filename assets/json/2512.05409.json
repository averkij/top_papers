{
    "paper_title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
    "authors": [
        "Ruixuan Huang",
        "Hao Zeng",
        "Hantao Huang",
        "Jinyuan Shi",
        "Minghui Yu",
        "Ian En-Hsu Yen",
        "Shuai Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 9 0 4 5 0 . 2 1 5 2 : r SQ-format: Unified Sparse-Quantized Hardware-friendly Data Format for LLMs Ruixuan Huang1, Hao Zeng2, Hantao Huang3, Jinyuan Shi2, Minghui Yu3, Ian En-Hsu Yen2, Shuai Wang 1HKUST, 2Moffett AI, 3ByteDance Seed"
        },
        {
            "title": "Abstract",
            "content": "Post-training quantization (PTQ) plays crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and lowprecision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators. Correspondence: Hantao Huang at huanghantao@bytedance.com, Minghui Yu at yuminghui.exp@bytedance.com, Ian En-Hsu Yen at ian.yan@moffett.ai"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities [1, 2], but their substantial memory and computation requirements present significant deployment challenges [3]. To address this, quantization [46] and sparsification [7, 8] have emerged as indispensable techniques, enabling LLMs to run on fewer GPUs or more cost-effective hardware. Post-Training Quantization (PTQ) is particularly appealing as it allows for the direct quantization of pre-trained models without costly retraining. Current PTQ practices [9, 10] have established that an 8-bit weight, 8-bit activation (W8A8) format can maintain near-lossless accuracy compared to BF16 setting. The primary challenge now lies in pushing beyond this frontier to even lower bit-widths. However, advanced uniform quantization, such as W4A4 format, often leads to significant degradation in model performance [6]. Furthermore, critical hardware-algorithm gap impedes the practical realization of mixed-precision benefits. Although modern GPUs feature specialized tensor cores for 4-bit and 8-bit computations, they typically lack native support for direct hybrid-precision operations (e.g., INT4 matrix multiplied by INT8 matrix). Consequently, theoretically efficient schemes like W4A8 must be emulated using higher-precision data paths, nullifying the potential performance gains, 1 creating persistent gap between theoretical throughput and deployed reality [5, 11]. The solution lies in designing hardware-friendly data format natively supporting sparsification and quantization in hybrid precisions to accelerate computation while maintaining the performance. To maintain the model performance, [4, 12] have revealed that small subset of values in both weights and activations disproportionately impacts model performance and is highly sensitive to quantization errors. Whether through quantization or sparsification, existing compression methods struggle to adapt to the non-uniform distribution of information in LLMs. Quantization formats like WxAy apply uniform bit-width across entire tensors, while sparsification approaches like NVIDIA 2:4 semi-structure sparse [7, 13] lacks the flexibility to handle non-uniform information (Figure 6). These observations motivate shift towards fine-grained, unified sparse and quantized paradigm, where critical values are preserved in higher precision while the majority are compressed to lower bit-widths (e.g., 4-bit or even 2-bit). Consider W4A8 setting: current hardware falls back to the less efficient W8A8 computation path. Our core idea is that if the 8-bit activations could be decomposed into sparse, high-precision component (8-bit) and dense, low-precision component (4-bit) in hardware-friendly way [14], the computation burden can be shifted to the much faster 4-bit tasks. In this paper, inspired by the co-design philosophy evident in NVIDIAs NVFP4 [15] and 2:4 semi-structure sparse formats [13], where hardware features and data formats are developed in tandem, we propose SparseQuantized Format (SQ-format). This format enables hybrid-precision compression frameworks. In summary, our contributions are as follows: SQ-format Definition and Its Hardware Implementation (Section 3). We define SQ-format, new data format designed for hybrid integer precision matrix multiplication. We provide its feasible hardware implementations for both current GPUs and next-generation AI accelerators. Achieving Pareto Improvement between Accuracy & Throughput (Section 4.2). We develop two algorithms leveraging SQ-format for weights and activations. Conducting extensive experiments on various LLMs, our results demonstrate that SQ-format elevates schemes with W4A4-level throughput to near W4A8-level accuracy. Compared to W4A8 baselines, SQ-format delivers superior throughput with model performance gap of less than 1%. Quantization for Static Activation Splitting (Section 4.3). Applying SQ-format to activations introduces potential runtime overhead due to dynamic value selection is not natively well supported by GPUs. To further eliminate this bottleneck, we develop static strategy that pre-determines the high-precision components by analyzing activation-weight product contributions on calibration set. This strategy removes the costly runtime operation, making the solution more hardware-friendly while achieving performance comparable to its dynamic counterpart. Through extensive experiments, this paper validates the superiority of SQ-format. We contend that our work not only offers practical solution for accelerating LLMs on current hardware but also presents compelling co-design blueprint for future AI accelerators."
        },
        {
            "title": "2 Related Works",
            "content": "Post-Training Quantization converts the weights and/or activations of model to low-precision data types after the model has completed full-precision pre-training. GPTQ [10] quantize the weights column by column, successfully compressing OPT-175B to 4 bits. AWQ [9] and Wanda [16] consider the interaction with activations when quantizing weights. SpQR [8] and QUIK [17] isolate outliers, but they often face hardware efficiency challenges due to their reliance on unstructured or irregular patterns. SmoothQuant [4] is joint quantization method for weights and activations, which achieves W8A8 PTQ by transferring the difficulty of activating quantization to the weights. SparseGPT [7] formulates pruning as series of layer-wise sparse regression problems, which can prune OPT-175B by >50% without significantly sacrificing performance. Data Representations for Quantization. Quantization methods are supported by different data formats. Standard INT8 format [18] benefits from extensive hardware support and high computing throughput. FP8 format [19] provides wider dynamic range to capture activation outliers. NVFP4 [15] uses microblock scaling to achieve extreme 4-bit compression with NVIDIA Blackwell hardware. MX formats [20] adapt to the local 2 data range through shared scaling. HiFloat8 [21] features tapered precision and better balances precision and dynamic range. Current formats apply single precision scheme to all values, which creates fundamental conflict representing non-uniform information."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first define the Sparse-Quantized Format (SQ-format) in Section 3.1. Then, we demonstrate two example algorithms leveraging SQ-format for weights and activations in Section 3.2. We finally discuss the hardware design solutions for SQ-format in Section 3.3."
        },
        {
            "title": "3.1 SQ-format Definition",
            "content": "SQ-format only applies to one operand in matrix multiplication to avoid complicated cross-format computation. The other operand can use normal uniform formats. SQ-format supports both integer (e.g., INT4) and floating point (e.g., FP4). It divides the precision used into high-precision hhigh and low-precision hlow (e.g., hhigh = INT8, hlow = INT4). The matrices in SQ-format are divided by bank. fixed bank size and sparsity avoids the load imbalance problem and removes the distributed accumulator problem encountered with unstructured sparsity, and further reduce the energyand area-hungry [22, 23]. This is the reason that GPU adopts 2:4 semi-structure sparse and our proposed SQ-format can be viewed as general extension, where = 0.5, hlow = 0, = 4. We define the SQ-format components in Equation (1). SQ-format(X) = ([Xquant], [Squant], [m], hhigh, hlow, b, s) (1) With as general matrix, [Xquant], [Squant] are the quantized matrix with hhigh/hlow and the scaling matrix. We determine highand low-precision parts by masks [m], which can be represented implicitly by low-precision parts or explicitly by additional vectors. This serves as framework whose concrete mathematical formulation varies with the hardware implementation."
        },
        {
            "title": "3.2 Example Algorithms with SQ-Format",
            "content": "We demonstrate two PTQ algorithms based on SQ-format. For weights and activations, each algorithm selects one to use SQ-format, and the other is executed with classic quantization."
        },
        {
            "title": "3.2.1 SQ-format on Weights",
            "content": "When using SQ-format on weights, weight matrix is represented by high-precision and low-precision parts. For the simplicity of hardware implementation, we adopt the symmetric quantization. Therefore, the unused largest value of the low-precision format is considered as high-precision mask, indicating that high-precision elements should be used for computation here. For example, When hlow = INT2, values in {1, 0, 1} are normal elements, and vmask = 2 is used to indicate the high-precision presence. The high-precision elements are sparsely located, while stored compactly since we adopt fixed-sparsity per-bank setting without any wasted gaps. See an INT4/INT8 example in Figure 1. Algorithm 1 (SQ-format on Weights): Based on SQ-format, we combine GPTQ [10] and SmoothQuant [4] to implement our quantization strategy. We first smooth the weight matrices on the calibration set. When quantizing the smoothed weight matrix of given layer, we use the Hessian matrix H, derived from the calibration set activations, to approximate information loss. To determine the highand low-precision elements, i,i )2, which synthesizes the follow [10, 24], for weights own magnitude with the models sensitivity to its perturbation. Within each weight bank, we rank the weights based on and select the top (1 s) fraction of weights within each column to generate high-precision mask. The detailed procedure is described in Algorithm 1. , we compute its importance score by Ir,i = (W r,i)2/(H1 r,i Figure 1 An example of weight matrix using SQ-format (hhigh = INT8, hlow = INT4, = 0.5). In this case, each column of each bank is split into high-precision part and low-precision part for quantization as group. The high-precision part is stored compactly, while the low-precision part is still stored in its original shape, but the corresponding high-precision places is valued vmask = 15."
        },
        {
            "title": "3.2.2 SQ-format on Activations",
            "content": "When using SQ-format on activations, it divides matrices into two parts: high-precision (important) and low-precision (less important), whereas there are naturally per-channel outliers in activations, which are of significant importance to information loss. However, we cannot quantize activations like Algorithm 1 owing to their dynamic nature. During inference, we need to construct SQ-format representation for activations, which introduces extra TopK overhead in addition to (de)quantization. The overhead can be easily eliminated with dedicated hardware support illustrated in Section 3.3. SQ-format is bank based and thereby can be mapped on the hardware implementation as pipelined preprocess for tensor cores. Algorithm 2 (Static Strategy for SQ-format on Activations): To broaden the SQ-format application, we also propose static strategy for activations to address this issue. We can generate mask vectors for SQ-format on the calibration set in advance to select highand low-precision elements by channel. During inference, we separate the computation tasks to two parts by masks. The lower-precision part brings extra throughput benefits. In early experiments, we find that generating static activation masks based solely on the absolute values of activations (Ij = Aj, [1, K]) leads to significant performance degradation. This is reasonable since we cannot approximate the dot product by activations only. To this end, we redefine the importance score by considering the per-channel average magnitude of W. Specifically, we first collect per-channel average activations Aj for each input channel over calibration set. Given smoothed weight matrix W, the importance score Ij for channel is Ij = Aj (cid:80) i,j. We then generate mask according to I, selecting the top (1 s) fraction channels within each bank. Furthermore, as part of the PTQ process, the columns of the weight matrices can be reordered according to I. This alignment can improve data locality for hardware kernels. The detailed procedure is described in Algorithm 2. The additional storage overhead for static mask is negligible due to its per-channel nature. For instance, the mask size for Llama-3-70B is only 5.94 MB. 4 Algorithm 1 SQ-format on Weights Algorithm 2 SQ-format on Activations (Static) 1: Input: Weight RKN , calibration dataset D, sparsity s, bank size b, high-precision hhigh, low-precision hlow. 1: Input: Weight RKN , calibration dataset D, sparsity s, bank size b, high-precision hhigh, low-precision hlow. 2: W, Smooth(W, D) 3: (W)2/(diag(H1))2 2: W, Smooth(W, D) 4: for each bank of do 5:"
        },
        {
            "title": "Generate Precision Mask mw",
            "content": "Select top (1 s) elements by Iw high) Quant(w mw) low) Quant(w mw) (w 6: (w 7: 8: end for high, low, 3: for each bank of W, of do Ij Aj (cid:80) j,i, bank 4: Generate Precision Mask mw 5: Select top (1 s) elements by Ij (w, s) Quant(w, hhigh, hlow) 6: 7: end for 8: Reorder rows of based on mask m. 9: Generate (Whigh, Wlow, Shigh, Slow). 9: Generate (Wquant, Squant, m)."
        },
        {
            "title": "3.3 Hardware Design",
            "content": "Native support for SQ-format requires dedicated hardware accelerators to fully leverage its advantages in improving accuracy and throughput. The core challenges are: (1) when applying SQ-format on weights, we need to process the compactly stored high-precision parts efficiently; and (2) when using dynamic SQ-format on activations, we need pipelined unit to dynamically compute the high-precision masks. These hardware units can function as dedicated accelerators (e.g. gather, scatter, etc) within AI chips, working in conjunction with standard tensor cores. (a) SQ-format for weights. (b) Static SQ-format for activations. Figure 2 Hardware implementation for applying SQ-format on weights and activations. As illustrated in Figure 2a, we present the computation path for applying SQ-format on weights with dedicated hardware support, where high and low-precision computations are split into two parallel paths. The lowprecision part can be directly processed by tensor cores with dense matrix multiplication. The high-precision part, however, requires detecting the vmask values in the low-precision part to gather corresponding elements from the activation tiles before sending them to tensor cores. The advantage is that since the high-precision part can be over 8x sparse, the latency of this parallel path will be completely hidden by the low-precision computation. As illustrated in Figure 2b, we demonstrate the computation path for applying static SQ-format on activations on GPUs. It relies on pre-computed activation masks to divide the entire computation into two paths with different precisions. Since the PTQ process has already reordered the banks of weight matrices, we only need selection and gathering operations for the activation tiles. Although the highand low-precision computation streams are executed serially, the overall throughput increases because significant portion of the computation (can up to 3/4) is converted to low-precision. We implement CUDA kernel for computing static SQ-format on activations on GPUs to demonstrate its practicality. As shown in Figure 3, our implementation achieves higher throughput than W8A8. larger sparsity leads to greater throughput, bringing the performance progressively closer to the theoretical ceiling of W4A4. Dynamic strategy for SQ-format on activations requires additional hardware support, as shown in Figure 4. When implemented on GPUs, we need to use CUDA cores to compute the high-precision mask for the entire activation tensor. However, an additional pipelined hardware unit could ensure that the mask for each bank is computed and ready before tensor cores execute it, thereby achieving throughput performance comparable to the static strategy. To validate the hardware feasibility, we implement the proposed unit in RTL and synthesized it using the TSMC 12nm process library. We compare it against standard integer MAC array under 12-bit I/O. Even accounting for the additional gather unit requires for dynamic masking, the overall silicon area is reduced by 35.8% compared to the baseline. Detailed synthesis results are provided in Appendix C."
        },
        {
            "title": "4 Experiments & Findings",
            "content": "Figure 3 Throughput of static SQ-format on activations from = 0.5 (2x sparse) to 0.9375 (16x sparse). Figure 4 Dedicated hardware supporting dynamic activation SQ-format. To validate the effectiveness of SQ-format and to investigate why it is effective, we conduct extensive experiments on different LLMs, including dense models and MoE models."
        },
        {
            "title": "4.1 Experimental Setups\nModels and Benchmarks. Our quantized models include: Llama-3-8B, Llama-3-70B [25], Qwen3-30B-\nA3B [26]. We compare normalized accuracy on six non-generative datasets: ARC-easy, ARC-challenge [27],\nOpenBookQA [28], Hellaswag [29], Winogrande [30], and PIQA [31]; two generative datasets: GSM8k [32]\nand AGIEval [33], as well as perplexity on Wikitext [34] and Lambada [35] with lm-evaluation [36]. GSM8k is\nevaluated in 8-shot setting while others are zero-shot.",
            "content": "Notation. We conduct the experiments with integer format to validate the effectiveness. The configuration of SQ-format is denoted as (hhigh/hlow) s, where is the bank size. The configuration based on Algorithm 1 is denoted as W(SQx)Ay, where uses SQ-format with an equivalent of bits and uses INTy quantization. The equivalent bit number is = (1 s)hhigh + hlow. Similarly, the configuration based on SQ-format on activations (dynamic and static strategy based on Algorithm 2) is denoted as WxA(SQy). 6 The equivalent bit number is = (1 s)hhigh + shlow. The static strategy also includes an additional 1-bit per-channel mask, while it is negligible for y. Baselines. As unified sparse-quantized data format, we have chosen post-training sparse baselines: SpQR [8] and SparseGPT [7]. For PTQ algorithms, we compare with two accuracy levels of W4A8 and W4A4, and compare with three baselines: GPTQ [10], SmoothQuant [4], and SpinQuant [6]. For all experiments, our calibration set consists of 32 randomly sampled text segments from Wikitext, each with length of 2048 tokens. We present (8/4) implementation that runs on modern GPUs. In Section 5 and Appendix A.1, we provide more results including (8/3), (8/2), (4/2) settings. We conducted experiments on different bank sizes (4, 8, 16, 32, 64 and 128) and sparsity (0.5, 0.75, 0.875, and 0.9375) grids. We select the best from the grid search results, while complete results will be provided in Appendix A.1. Table 1 Performance comparison of quantization methods. For each comparison set, Red bold values highlight the best performance, and Black bold values highlight the second best performance. In this table, WxA(SQy) results are using dynamic settings simulated on GPUs. Non-generative Acc (%) Generative Acc (%) Perplexity ARC-c ARC-e PIQA Hellas. Wino. OBQA GSM8k AGIEval Wiki. () 7.26 () 79.21 () 33. () 77.74 () 73.40 () 45.00 () 49.43 () 53.41 () 80. Model Method Setting BF16 W16A16 SpQR W4A16 SparseGPT W16A16 GPTQ W4A8 SmoothQuant W4A8 SpinQuant W4A8 Llama-3 8B SQ-format W4A(SQ6) W4A(SQ5) GPTQ W4A4 SmoothQuant W4A4 SpinQuant W4A4 SQ-format W(SQ6)A4 W(SQ5)A4 W(SQ4.5)A4 BF16 W16A SpQR W4A16 SparseGPT W16A16 GPTQ W4A8 SmoothQuant W4A8 SpinQuant W4A8 Llama-3 70B SQ-format W4A(SQ6) W4A(SQ5) GPTQ W4A4 SmoothQuant W4A4 SpinQuant W4A4 SQ-format W(SQ6)A4 W(SQ5)A4 W(SQ4.5)A4 BF W16A16 GPTQ W4A8 SmoothQuant W4A8 Qwen-3 30B-A3B SQ-format W4A(SQ6) W4A(SQ5) GPTQ W4A4 SmoothQuant W4A4 SQ-format W(SQ6)A4 52.13 31.74 53.15 37.20 51.78 52.13 51. 46.92 24.23 47.95 49.40 47.44 47.61 64.51 63.99 48.72 62.54 24.40 62. 62.80 61.95 59.89 25.21 37.97 61.01 59.98 59.64 55.80 52.73 40.35 55.38 55. 49.48 23.20 76.81 58.96 78.32 59.97 75.08 77.44 77.15 71.50 26.80 74. 75.42 73.15 72.01 86.11 85.19 74.45 84.13 42.42 86.32 84.81 84. 81.64 25.00 62.46 83.12 83.08 81.99 79.17 77.73 57.95 79.21 78.75 75.88 29. 79.87 68.88 79.86 72.63 80.20 79.87 79.43 77.63 51.90 77.53 79.38 76.71 78.24 84. 84.39 77.64 83.56 64.36 84.66 84.06 83.73 83.07 50.21 73.67 83.03 81.88 82. 80.58 79.37 71.98 79.54 80.25 78.50 53.42 78.84 53.69 77.62 67. 77.71 78.05 77.68 75.24 27.08 75.28 77.75 76.03 75.39 84.96 85.05 70. 84.32 64.86 84.25 84.44 84.14 83.11 26.43 65.62 84.10 83.15 83.08 77.65 76.75 58. 76.60 76.28 74.77 28.35 73.40 63.38 73.79 67.56 74.19 73. 74.27 71.11 50.90 69.53 71.74 69.06 69.22 80.19 79.87 74.82 80.42 57. 81.06 80.19 80.90 77.50 51.30 61.64 78.77 77.82 77.43 70.88 68.66 60. 70.56 70.24 67.71 51.46 45.20 33.80 44.40 37.40 44.40 45.20 44.40 43.00 25.00 41. 44.00 42.20 44.20 48.40 48.80 41.00 48.20 33.40 49.00 48.00 47. 46.40 26.30 41.40 46.40 49.40 47.40 45.80 44.20 38.00 44.80 44.20 43.20 28. 45.79 2.05 40.86 5.07 43.06 40.26 40.94 28.58 0.00 32.90 39.58 33. 34.12 81.05 79.38 31.31 78.31 0.75 79.38 78.32 79.23 74.82 0.00 1. 78.62 75.82 75.59 90.45 82.69 59.21 88.78 89.84 65.39 0.00 32.78 25. 32.88 25.71 32.52 32.35 32.35 30.46 24.72 31.14 31.91 30.36 30. 45.67 44.63 29.91 45.00 29.30 42.16 44.68 44.67 40.60 24.42 26.27 41.84 41.47 39. 63.05 60.20 35.82 61.33 61.31 52.85 25.95 Lamb. () 3.09 3.22 9. 3.41 5.87 3.38 3.47 3.50 7.55 18.25 7.98 16.51 7. 7.97 8.02 9.13 4.27 > 100 > 100 4.11 9.08 8.01 8.85 9.00 2.92 3.20 9.86 3.48 8.41 3. 3.42 3.91 3.97 2.58 2.65 3.07 2.61 23.83 2.72 2.65 2.67 3.31 3.45 4.58 2.98 > 100 > 100 15.07 15. 3.79 4.49 4.67 10.89 11.21 26.42 11.11 11.10 2.66 2.69 2.73 4. 4.49 27.19 4.47 4.51 11.81 5.06 > 100 > 100 53.75 76.22 78. 75.58 70.64 44.40 88.17 56.57 11. 4."
        },
        {
            "title": "4.2 Finding 1: SQ-Format Achieves Accuracy-Throughput Pareto Improvement",
            "content": "We obtain insights about the Pareto improvement between accuracy and throughput caused by the SQ-format from two sets of comparisons. The benchmarking results are shown in Table 1. W4A8 vs. W4A(SQ). GPUs cannot efficiently compute W4A8 algorithms; in fact, it will fallback to W8A8 execution path. However, SQ-format separates A4 and A8 computations, transforming most of the original computing tasks into W4A4 computations, significantly improving throughput. For performance, W4A(SQ6) and W4A(SQ5) achieve average accuracy and perplexity on par with GPTQ on Llama-3 models, while on Qwen-3, the average benchmark accuracy improves by 3.87% and achieves better perplexity. We illustrate the Pareto frontier in Figure 5. The x-axis represents the speedup derived from end-to-end prefilling latency measurements using SQ-format with the static activation strategy. As shown, SQ-format effectively bridges the gap between the high-precision W4A8 and the high-efficiency W4A4. Notably, for larger models like Llama-3-70B, SQ-format achieves speedup of 1.71, capturing 89% of the theoretical W4A4 acceleration while maintaining higher model performance. Detailed latency and bandwidth results are provided in Appendix D. Figure 5 Accuracy-Speed Pareto frontier on Llama-3 models. W4A4 vs. W(SQ)A4. By introducing sparse high-precision elements, the accuracy of W4A4 quantization can be economically improved. From 16x to 4x sparse, the average accuracy improvement ranges from 0.11% to 3.54%. Among them, 4x sparse setting W(SQ6)A4 achieves accuracy close to W4A8. It also allows true benefits to be realized on hardware without the need for dedicated INT6 tensor cores, which costs much more than SQ-format. SQ-format has stability advantages on larger models (Llama-3-70B and Qwen-3-30B) and more obvious improvements in generative accuracies. Since the W(SQ)A4 configuration requires dedicated hardware support as shown in Figure 2a, we currently use GPU simulation while do not provide latency comparison. To demonstrate the applicability of SQ-format on other data types, we also show the results of applying SQ-format on DeepSeek-R1 weights with FP8/FP4 quantization and 4x sparsity in Appendix B."
        },
        {
            "title": "4.3 Finding 2: SQ-Format Supports Static Activation Quantization",
            "content": "The dynamic nature of applying SQ-format to activations on GPUs requires using TopK operations on CUDA cores, which impacts actual throughput without dedicated accelerators. Fortunately, the static strategy can achieve similar performance. We compare the dynamic and static strategies, and the benchmarking results are shown in Table 2. Comparing the dynamic and static strategy results under different models and configurations, we find static strategy can successfully retain most of the performance. The average benchmark accuracy varies within 1%. 8 Table 2 Performance comparison of dynamic v.s. static SQ-format on activations. For each setting, Red bold values highlight the best performance for static strategy, and Black bold values highlight the best performance for dynamic strategy. Model Setting Static Llama-3 70B Qwen-3 30B-A3B BF16 W4A(SQ6) b-(8/4)-0.5 W4A(SQ5) b-(8/4)-0. BF16 W4A(SQ6) b-(8/4)-0.5 W4A(SQ5) b-(8/4)-0.75 - - - - - - Non-generative Acc (%) Generative Acc (%) Perplexity ARC-c ARC-e PIQA Hellas. Wino. OBQA GSM8k AGIEval Wiki. Lamb. () 64. 63.73 62.37 63.56 62.80 62.71 62.80 62.37 61.77 61.51 62.88 61.95 62.63 55. 55.46 54.18 53.33 55.38 55.63 52.82 54.69 55.20 52.90 55.46 55.03 52. () 86.11 84.51 83.67 83.54 84.81 84.39 83.88 83.92 83.88 84.04 84. 84.51 83.84 79.17 78.62 78.45 77.44 79.21 78.49 76.64 77.44 78.66 76. 78.75 78.70 76.68 () 84.39 83.89 83.46 83.13 84.06 84.00 83.62 83.24 83.40 83. 83.79 83.73 83.57 80.58 80.14 79.43 79.71 79.54 79.87 79.05 79. 79.22 78.67 80.25 79.98 78.89 () 84.96 84.42 84.06 84.35 84.44 84.32 84.34 84.20 84.06 83. 84.46 84.14 84.25 77.65 76.66 76.46 76.46 76.60 76.43 76.73 76.52 76.18 75. 76.28 76.47 76.42 () 80.19 80.03 79.63 80.26 80. 80.27 79.79 79.95 79.79 78.92 79.79 80.90 80.27 70.88 71.59 68.82 68. 70.56 69.61 69.14 70.40 69.14 67.64 70.24 69.38 68.75 () 48.40 48.20 47.40 47.60 48. 48.60 48.60 48.00 46.60 48.40 48.20 47.80 48.00 45.80 45.80 43.40 45.20 44. 45.60 45.20 45.40 42.60 44.00 44.20 44.40 45.00 () 81.05 78.77 77.93 78. 78.32 78.09 77.71 77.78 77.86 76.49 79.15 79.23 77.48 90. 88.10 89.01 87.49 88.78 89.39 88.55 88.40 88.48 88. 89.84 88.86 87.64 () 45.67 44.07 43.86 44.07 44.68 44.97 44.99 43. 43.92 43.10 44.01 44.67 44.68 63.05 60.06 59.83 58.38 61.33 60.55 60.78 59.65 57.98 57. 61.31 60.49 60.10 () 2.91 3.43 3.57 3.74 3.31 3.39 3.48 3.55 3.75 3.99 3.35 3.44 3. 10.88 11.15 11.34 11.31 11.11 11.15 11.22 11.21 11.42 11.41 11.10 11.17 11.21 () 2. 2.64 2.67 2.64 2.65 2.65 2.62 2.67 2.70 2. 2.66 2.66 2.62 4.11 4.34 4.37 4.60 4.47 4.39 4.21 4.42 4.52 4. 4.50 4.35 4.25 - 16 32 64 16 32 16 32 64 16 32 64 - 16 32 64 16 32 64 16 32 16 32 64 In fact, the dynamic strategy still relies solely on the absolute activations. Although the static strategy is only trained on the calibration set, considering the activation-weight product may compensate for this. In Appendix A.1, we also show that the static strategy is not particularly dependent on large calibration set size. The grid experiments we conduct on different calibration set sizes show relatively stable accuracy trend."
        },
        {
            "title": "5 Hardware-Algorithm Co-design & Design Exploration",
            "content": "As format requires dedicated hardware support to achieve most theoretical benefits, SQ-format will benefit from hardware-algorithm co-design. This section discusses how various parameters operate in PTQ process and suggests best practices for hardware implementation based on simulation results."
        },
        {
            "title": "5.1 Bank Size & Sparsity",
            "content": "To facilitate hardware implementation, SQ-format splits the operand matrices into banks, and each bank performs fixed sparsity precision partitioning. This is to model the numerical importance distribution of the original matrix. However, from the algorithm perspective, this requires careful setting of bank size and sparsity. We provide visualization example of static SQ-format on activations in Figure 6. The original matrix has unevenly distributed per-channel activations. Under = 3/4 (4x sparse), bank_size = 32 can successfully separate most important activations, with the low-precision parts appearing to be flat; bank_size = 4 is less flexible, and there are still per-channel outliers in the low-precision part. This resulted in NVIDIA 2:4 semi-structure sparse, implemented with SparseGPT achieving poor results in Table 1. 9 (a) Original activation. (b) Partition results, bank_size = 32. (c) Partition results, bank_size = 4. Figure 6 Activation partition results from Layer 30 of Llama-3-8B. bank_size = 32 provides more flexible precision partition than bank_size = 4. We evaluate the performance of different combinations of bank_size and sparsity, as shown in Figure 7. For using SQ-format on weights, under fixed sparsity, there exists an optimal bank_size as it changes. The larger the sparsity, the larger the optimal bank_size. As illustrated in Figure 8, when using SQ-format on activations, the trend is not as obvious as on weights, showing slow changes. Empirically, static strategy tends to prefer smaller bank_size. These results guide the design of dedicated hardware. To support at least 16x sparsity for weights, the bank_size illustrated in Figure 2a needs to be at least 64, which affects the area of MUXs. As for activations, the dedicated unit illustrated in Figure 4 should support bank_size 64 or lower. Figure 7 Average benchmark accuracy of using SQ-format on weights under different hhigh, hlow, bank_size and sparsity on Llama-3-8B. The BF16 baseline result is dashlined in gray."
        },
        {
            "title": "5.2 High/Low Precision Choice & Sparsity\nIn Figures 7 and 8, we show results of other hhigh, hlow configurations including (4/2), (8/2), (8/3).\nSQ-format support for lower precisions. For hlow = INT2, SQ-format on weights can hardly maintain accuracy,\nand only with the B − (8/2) − 0.5 setting can achieve usable performance. This results from insufficient width\nof low-precision bits, even introducing high-precision elements is difficult to compensate for the loss. Dynamic\nstrategy for SQ-format on activations is better than that for weights, and it supports up to 4x sparsity. For\nhlow = INT3 and using SQ-format on weights, (8/3) setting shows similar performance trends to (8/4). There\nis an opportunity to achieve a tradeoff between storage and performance.",
            "content": "Computation balance for sparsity. Sparsity is restricted by (hhigh/hlow) configuration, which is due to differences in tensor core computational power. For example, using SQ-format on weights, we hope that the computation of the sparse high-precision part can be masked by the low-precision part. Assuming the hardwares computational power for W8A8 is four times that of W4A4, the sparsity needs to be at least 0.75. 10 Figure 8 Average benchmark accuracy of using SQ-format on activations under different hhigh, hlow, bank_size and sparsity on Llama-3-8B. The BF16 baseline result is dashlined in gray. Dynamic strategy results are marked with while static results with . Therefore, the W(SQx)Ay results shown in Table 1 are at least 4x sparsity. When the high-precision remains unchanged, using lower low-precision such as INT2 would require an even higher minimum sparsity."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose SQ-format, novel data format enabling hybrid-precision computation. By sparsely mixing highprecision elements in hardware-friendly manner during low-precision computation, SQ-format successfully achieves Pareto improvement between precision and throughput. We also propose static activation quantization algorithm, effectively showing its potential for hardware deployment. We not only provide practical solution for accelerating LLMs on current hardware but also offer promising blueprint for the co-design of next-generation AI accelerators."
        },
        {
            "title": "References",
            "content": "[1] Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, and Fei Liu. Plangenllms: modern survey of llm planning capabilities. arXiv preprint arXiv: 2502.11221, 2025. [2] Jiawei Li, Yang Gao, Yizhe Yang, Yu Bai, Xiaofeng Zhou, Yinghao Li, Huashan Sun, Yuhang Liu, Xingpeng Si, Yuhao Ye, Yixiao Wu, Yiguan Lin, Bin Xu, Bowen Ren, Chong Feng, and Heyan Huang. Fundamental capabilities and applications of large language models: survey. ACM Comput. Surv., 58(2), 2025. ISSN 0360-0300. doi: 10.1145/3735632. URL https://doi.org/10.1145/3735632. [3] Arnav Chavan, Raghav Magazine, Shubham Kushwaha, Mérouane Debbah, and Deepak K. Gupta. Faster and lighter llms: survey on current challenges and way forward. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024, pages 79807988. ijcai.org, 2024. URL https://www.ijcai.org/proceedings/2024/883. [4] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 3808738099. PMLR, 2023. URL https://proceedings.mlr.press/v202/xiao23c.html. [5] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv: 2405.04532, 2024. [6] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: LLM quantization with learned rotations. In 11 The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=ogO6DGE6FZ. [7] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1032310337. PMLR, 2023. URL https: //proceedings.mlr.press/v202/frantar23a.html. [8] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for near-lossless LLM weight compression. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Q1u25ahSuy. [9] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv: 2306.00978, 2023. [10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv: 2210.17323, 2022. [11] Zhen Zheng, Xiaonan Song, and Chuanjie Liu. Mixllm: Llm quantization with global mixed-precision between output-features and highly-efficient system design. arXiv preprint arXiv: 2412.14590, 2024. [12] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. OWQ: outlier-aware weight quantization for efficient fine-tuning and inference of large language models. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1335513364. AAAI Press, 2024. doi: 10.1609/AAAI.V38I12.29237. URL https://doi.org/10.1609/aaai. v38i12.29237. [13] Jeff Pool, Abhishek Sawarkar, and Jay Rodge. Accelerating inference with sparsity using the nvidia ampere architecture and nvidia tensorrt. NVIDIA Technical Blog, 2021. URL https://developer.nvidia.com/blog/ accelerating-inference-with-sparsity-using-ampere-and-tensorrt/. [14] Kai Zhong, Zhenhua Zhu, Guohao Dai, Hongyi Wang, Xinhao Yang, Haoyu Zhang, Jin Si, Qiuli Mao, Shulin Zeng, Ke Hong, Genghan Zhang, Huazhong Yang, and Yu Wang. FEASTA: flexible and efficient accelerator for sparse tensor algebra in machine learning. In Rajiv Gupta, Nael B. Abu-Ghazaleh, Madan Musuvathi, and Dan Tsafrir, editors, Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS 2024, La Jolla, CA, USA, 27 April 20241 May 2024, pages 349366. ACM, 2024. doi: 10.1145/3620666.3651336. URL https://doi.org/10.1145/3620666.3651336. [15] Eduardo Alvarez, Omri Almog, Eric Chung, and Kyle Aubrey. sky, ference. introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/. NVIDIA Technical Blog, Introducing NVFP 2025. and for Simon Layton, Dusan Stosic, Ronny Krashininhttps://developer.nvidia.com/blog/ low-precision accurate efficient"
        },
        {
            "title": "URL",
            "content": "[16] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=PxoFut3dWW. [17] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Quik: Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv: 2310.09259, 2023. [18] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv: 2208.07339, 2022. [19] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. Fp8 formats for deep learning. arXiv preprint arXiv: 2209.05433, 2022. [20] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, Stosic Dusan, Venmugil Elango, Maximilian Golub, 12 Alexander Heinecke, Phil James-Roxby, Dharmesh Jani, Gaurav Kolhe, Martin Langhammer, Ada Li, Levi Melnick, Maral Mesmakhosroshahi, Andres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei Shao, Michael Siu, Pradeep Dubey, Paulius Micikevicius, Maxim Naumov, Colin Verrilli, Ralph Wittig, Doug Burger, and Eric Chung. Microscaling data formats for deep learning. arXiv preprint arXiv: 2310.10537, 2023. [21] Yuanyong Luo, Zhongxing Zhang, Richard Wu, Hu Liu, Ying Jin, Kai Zheng, Minmin Wang, Zhanying He, Guipeng Hu, Luyao Chen, Tianchi Hu, Junsong Wang, Minqi Chen, Mikhaylov Dmitry, Korviakov Vladimir, Bobrin Maxim, Yuhao Hu, Guanfu Chen, and Zeyi Huang. Ascend hifloat8 format for deep learning. arXiv preprint arXiv: 2409.16626, 2024. [22] Zhi-Gang Liu, Paul N. Whatmough, Yuhao Zhu, and Matthew Mattina. S2ta: Exploiting structured sparsity for energy-efficient mobile cnn acceleration. Proceedings of the 28th IEEE International Symposium on High-Performance Computer Architecture (HPCA-28), 2021. [23] Moor Insights & Strategy. Tenstorrents Holistic Stack Of AI Innovation. Technical report, Moor URL https://moorinsightsstrategy.com/wp-content/uploads/2020/10/ Insights & Strategy, 2020. Tenstorrents-Holistic-Stack-Of-AI-Innovation-By-Moor-Insights-And-Strategy.pdf. [24] Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: framework for accurate post-training quantization and pruning. arXiv preprint arXiv: 2208.11580, 2022. [25] Llama 3 Team. The llama 3 herd of models. arXiv preprint arXiv: 2407.21783, 2024. [26] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [27] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv: 1803.05457, 2018. [28] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260. [29] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472. [30] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 87328740. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6399. [31] Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 74327439. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6239. [32] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv: 2110.14168, 2021. [33] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: human-centric benchmark for evaluating foundation models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 22992314, Mexico City, Mexico, 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.findings-naacl.149. [34] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=Byj72udxe. 13 [35] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset, 2016. [36] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/ records/12608602."
        },
        {
            "title": "A Complete Experiment Results",
            "content": "A.1 Complete Results for Performance Comparison Table 3 Performance comparison of dynamic v.s. static SQ-format on activations. The quantized model is Llama-2-7B. Model Setting Static Sparsity W4A8 W4A(SQy) 16-(8/4) Llama-2 7B W4A(SQy) 16-(8/3) W4A(SQy) 16-(8/2) W4A(SQy) 16-(4/2) - - - - - 0 0.5 0.75 0.875 0.9375 0.5 0.75 0.875 0. 0.5 0.75 0.875 0.9375 0.5 0.75 0.875 0.9375 0.5 0.75 0.875 0.9375 0.5 0.75 0.875 0.9375 0.5 0.75 0.5 0. 8 6 5 4.5 4.25 6 5 4.5 4.25 5.5 4.25 3.625 3.3125 5.5 4.25 3.625 3. 5 3.5 2.75 2.375 5 3.5 2.75 2.375 3 2.5 3 2.5 Non-generative Acc (%) Generative Acc (%) Perplexity ARC-c ARC-e PIQA Hellas. Wino. OBQA GSM8k AGIEval Wiki. Lamb. () 44.88 44.88 44.80 44.88 44.45 44.71 44.62 44.62 43.77 44.97 44.11 43.86 41. 44.11 42.83 43.09 42.75 43.69 41.81 38.48 35.92 35.75 31.06 29.01 27.73 43.26 43.43 36.86 30.46 () 74. 74.83 74.62 74.58 73.86 74.16 74.33 73.91 73.32 74.75 73.86 73.36 72.26 73.78 71.93 70.66 71.51 74.16 70.88 67.09 62.12 63.97 54.38 48.91 45. 73.78 71.09 62.25 53.66 () 78.78 78.78 78.94 78.73 78.89 78.94 78.07 78.29 78.29 79.00 78.02 78.18 78. 77.75 77.58 77.48 77.69 78.94 77.37 76.01 73.67 73.94 69.21 65.61 63.76 78.45 76.61 74.65 68.55 () 75. 75.48 75.62 75.39 75.11 75.40 75.21 75.13 74.96 75.48 75.18 74.85 73.98 74.88 73.91 73.12 73.22 74.81 73.26 70.58 66.14 66.68 56.07 49.71 46. 74.29 72.66 66.31 56 () 68.98 68.82 68.75 69.06 67.96 69.14 68.75 68.82 67.72 69.38 67.64 68.51 68. 66.69 65.98 66.54 65.35 68.19 64.96 64.09 59.12 63.69 57.77 55.72 53.20 67.88 65.51 63.93 58.41 () 44. 44.20 43.60 43.80 45 44.80 44.00 44.40 42.80 44.20 44.40 43.20 42.20 43.80 41.80 40.00 40.60 43.80 42.00 38.20 36.40 37.80 34.00 31.60 31. 42.60 40.00 37.40 34.20 () 14.56 13.87 12.96 13.19 14.10 12.05 12.28 12.36 13.12 14.25 12.51 12.21 10. 12.13 10.77 10.39 9.02 13.04 10.08 5.84 2.73 6.22 2.20 1.74 1.97 11.98 10.84 4.62 2.27 () 28. 28.77 28.58 28.80 29.04 28.52 28.37 28.22 28.23 28.59 28.96 28.36 27.99 28.40 28.37 28.00 27.18 28.91 27.93 27.43 26.23 27.04 25.79 25.16 25. 28.45 27.26 26.62 25.76 () 8.93 8.94 8.96 8.99 9.03 8.99 9.05 9.10 9.12 8.97 9.08 9.28 9. 9.25 9.59 9.83 9.99 9.12 9.85 11.30 13.75 12.36 19.07 27.45 36.87 9.26 9.93 12.58 19.21 () 3. 3.54 3.56 3.58 3.56 3.55 3.60 3.64 3.63 3.56 3.62 3.75 3.84 3.69 3.91 4.05 4.08 3.61 3.96 4.88 6.56 4.57 8.68 15.91 31. 3.65 4.00 4.74 9.11 15 Table 4 Performance comparison of quantization settings. The quantized model is Llama-3-8B. Model Setting Static W16A16 W(SQ8)A4 b-(8/4)-0.5 W(SQ6)A4 b-(8/4)-0.75 W(SQ8)A8 b-(8/4)-0.5 Llama-3 8B W(SQ6)A8 b-(8/4)-0.75 W4A(SQ6) b-(8/4)-0.5 W4A(SQ5) b-(8/4)-0.75 - - - - - - - - 16 32 64 16 32 64 16 32 64 16 32 16 32 64 16 32 64 16 32 64 16 32 64 Non-generative Acc (%) Generative Acc (%) Perplexity ARC-c ARC-e PIQA Hellas. Wino. OBQA GSM8k AGIEval Wiki. Lamb. () 53.41 51.79 50.51 48.98 47.70 49.40 47.53 52.65 52.56 53. 50.94 53.84 52.90 49.83 50.34 51.96 50.00 51.02 52.13 50.00 49.74 50.26 49.74 50.43 51.96 () 77. 77.19 77.27 73.70 76.26 75.42 73.48 77.53 77.48 78.07 76.77 77.15 77.74 75.25 75.72 76.94 75.67 76.05 77. 74.96 75.04 76.73 75.84 76.09 77.15 () 80.85 80.03 78.67 77.53 79.16 79.38 78.07 80.85 80.85 80. 80.69 80.96 80.03 80.14 79.76 79.60 80.09 80.63 79.87 79.92 80.30 79.33 79.43 80.20 79.43 () 79. 78.23 77.77 77.23 76.74 77.75 77.06 79.10 79.29 79.19 77.99 79.05 79.23 78.03 77.93 77.19 78.33 77.94 78. 77.76 77.58 76.58 78.04 77.93 77.68 () 73.40 72.85 70.96 71.19 72.38 71.74 71.27 72.93 73.01 73. 73.01 72.77 71.90 73.16 72.85 73.32 73.48 72.93 73.48 73.32 72.53 71.51 73.16 72.61 74.27 () 45. 44.60 44.60 42.40 43.80 44.00 44.80 45.80 44.40 44.80 45.40 44.40 44.40 46.80 44.80 44.00 45.00 44.40 45. 45.80 44.40 44.80 45.20 44.00 44.40 () 49.43 42.68 41.93 39.27 36.16 39.58 37.83 48.60 48.14 48. 40.11 47.01 45.87 40.49 40.41 37.91 39.35 42.38 40.26 38.36 41.85 38.67 39.95 41.55 40.94 () 33. 32.76 31.49 31.58 31.66 31.91 31.02 33.84 33.43 33.93 32.24 33.54 33.54 32.93 32.12 31.82 32.87 32.73 32. 32.58 31.92 31.06 32.75 32.46 32.35 () 7.26 7.75 7.94 8.23 8.34 8.01 8.32 7.31 7.28 7. 7.87 7.37 7.35 7.83 7.99 8.18 7.74 7.84 7.97 7.93 8.13 8.39 7.79 7.88 8.02 () 3. 3.34 3.40 3.60 3.62 3.42 3.59 3.13 3.10 3.09 3.38 3.12 3.12 3.36 3.34 3.53 3.36 3.30 3. 3.40 3.43 3.66 3.39 3.31 3.50 16 Table 5 Performance comparison of quantization settings. The quantized model is Llama-3-70B. Non-generative Acc (%) Generative Acc (%) Perplexity ARC-c ARC-e PIQA Hellas. Wino. OBQA GSM8k AGIEval Wiki. () 2.92 () 86.11 () 80.19 () 45.67 () 64. () 84.39 () 48.40 () 84.96 () 81.05 Model Setting Static Llama-3 70B W16A16 W(SQ8)A4 b-(8/4)-0.5 W(SQ6)A4 b-(8/4)-0.75 W(SQ8)A8 b-(8/4)-0. W(SQ6)A8 b-(8/4)-0.75 W4A(SQ6) b-(8/4)-0.5 W4A(SQ5) b-(8/4)-0.75 - - - - - - - - 16 32 64 16 32 64 16 32 64 16 32 16 32 64 16 32 64 16 32 64 16 32 64 Lamb. () 2.58 2.69 2.70 2. 3.68 3.89 4.23 > 100 > 100 2.66 3.79 2.70 4.07 2.95 2.93 2.93 3.42 2.99 2.98 3.43 3.58 3.75 3.31 3.40 3. 3.56 3.76 4.00 3.36 3.45 3.55 2.58 2.59 2.58 2.68 2.58 2.59 2.67 2.67 2.64 2.65 2.65 2. 2.68 2.70 2.68 2.66 2.67 2.63 61.26 61.35 59.64 26.37 61.01 60.41 64.33 64.59 64.41 63.39 64.24 63. 63.73 62.37 63.56 62.80 62.71 62.80 62.37 61.77 61.51 62.88 61.95 62.63 83.50 83.38 81.73 25.00 83.12 81. 85.94 85.90 85.81 85.05 85.81 86.23 84.51 83.67 83.54 84.81 84.39 83.88 83.92 83.88 84.04 84.47 84.51 83. 83.68 83.41 82.59 51.31 83.03 83.24 84.43 84.38 84.49 83.89 84.27 84.11 83.89 83.46 83.13 84.06 84.00 83. 83.24 83.40 83.18 83.79 83.73 83.57 84.20 83.68 83.51 26.84 84.10 83.98 84.79 84.86 84.83 83.94 84.85 84. 84.42 84.06 84.35 84.44 84.32 84.34 84.20 84.06 83.99 84.46 84.14 84.25 79.87 78.77 77.19 56.20 78.77 77. 80.18 80.82 80.82 80.03 80.42 80.18 80.03 79.63 80.26 80.19 80.27 79.79 79.95 79.79 78.92 79.79 80.90 80. 47.40 47.60 47.00 29.80 46.40 48.20 48.60 49.00 48.00 48.00 48.80 48.40 48.20 47.40 47.60 48.00 48.60 48. 48.00 46.60 48.40 48.20 47.80 48.00 79.61 77.56 75.66 0.00 78.62 74.91 79.98 80.13 80.59 79.37 80.81 79. 78.77 77.93 78.24 78.32 78.09 77.71 77.78 77.86 76.49 79.15 79.23 77.48 42.56 42.83 40.71 24.42 41.84 41. 45.28 45.46 45.47 43.05 45.94 46.08 43.51 43.86 44.07 44.68 44.97 44.99 43.62 43.92 43.10 44.01 44.67 44. 17 Table 6 Performance comparison of quantization settings. The quantized model is Qwen3-30B-A3B. Model Setting Static W16A W(SQ8)A4 b-(8/4)-0.5 W(SQ6)A4 b-(8/4)-0.75 W(SQ8)A8 b-(8/4)-0.5 Qwen-3 30B-A3B W(SQ6)A8 b-(8/4)-0.75 W4A(SQ6) b-(8/4)-0. W4A(SQ5) b-(8/4)-0.75 - - - - - - - Non-generative Acc (%) Generative Acc (%) Perplexity ARC-c ARC-e PIQA Hellas. Wino. OBQA GSM8k AGIEval Wiki. Lamb. () 55.80 54.44 52.30 53.75 55.38 53.41 53.75 55.55 55.80 54. 55.03 55.55 54.95 55.46 54.18 53.33 55.38 55.63 52.82 54.69 55.20 52.90 55.46 55.03 52.90 () 79. 76.94 75.97 75.97 77.78 77.15 76.22 77.95 79.04 77.74 78.41 78.70 78.45 78.62 78.45 77.44 79.21 78.49 76. 77.44 78.66 76.39 78.75 78.70 76.68 () 80.58 79.38 79.00 79.38 79.22 79.38 78.02 80.09 80.79 80. 79.16 79.71 79.87 80.14 79.43 79.71 79.54 79.87 79.05 79.16 79.22 78.67 80.25 79.98 78.89 () 77. 76.20 75.92 75.72 75.03 75.82 75.58 76.70 76.87 77.04 75.80 77.12 77.00 76.66 76.46 76.46 76.60 76.43 76. 76.52 76.18 75.79 76.28 76.47 76.42 () 70.88 69.06 69.30 67.40 68.51 68.43 70.64 69.85 69.61 69. 69.69 71.11 70.17 71.59 68.82 68.59 70.56 69.61 69.14 70.40 69.14 67.64 70.24 69.38 68.75 () 45. 43.60 42.20 42.00 43.00 43.40 44.40 42.60 44.00 44.20 43.40 43.20 43.40 45.80 43.40 45.20 44.80 45.60 45. 45.40 42.60 44.00 44.20 44.40 45.00 () 90.45 89.23 87.79 87.34 5.23 87.79 88.17 89.76 89.39 88. 58.00 89.01 88.32 88.10 89.01 87.49 88.78 89.39 88.55 88.40 88.48 88.10 89.84 88.86 87.64 () 63. 58.84 57.38 54.95 52.23 56.87 56.57 61.74 61.37 60.63 57.51 61.62 62.32 60.06 59.83 58.38 61.33 60.55 60. 59.65 57.98 57.39 61.31 60.49 60.10 () 10.89 11.14 11.35 11.57 17.90 11.25 11.43 11.01 11.09 10. 14.17 11.08 11.00 11.16 11.35 11.31 11.11 11.16 11.22 11.22 11.42 11.41 11.10 11.18 11.22 () 4. 4.35 4.59 5.11 5.98 4.45 4.86 4.29 4.30 4.25 4.57 4.23 4.11 4.35 4.38 4.61 4.47 4.40 4. 4.43 4.53 4.67 4.51 4.35 4.26 - 16 32 64 16 32 16 32 64 16 32 64 16 32 64 16 32 64 16 32 64 16 32 A.2 Impact Calibrate Set Size For static strategy for SQ-format on activations, it is worth noting whether small amount of calibration sets can make the precision masks have enough generalization. To this end, we conduct experiments with different calibration set sizes, as shown in Table 7. The conclusion shows that the performance is relatively stable when using different calibration set sizes, indicating that the outlier distribution of the activations can be easily modeled. Table 7 Performance comparison of calibration sizes. Model Setting W16A Llama-3 8B W4A(SQ6) 16-(8/4)-0.5 Calib. Size - 8 16 32 64 128 256 512 Non-generative Acc (%) Generative Acc (%) Perplexity ARC-c ARC-e PIQA Hellas. Wino. OBQA GSM8k AGIEval Wiki. Lamb. () 53.41 52.39 50.09 49.83 50.34 49.83 52.22 51.88 () 77. 78.24 76.22 75.25 76.26 76.26 76.81 77.69 () 80.85 80.03 80.20 80.14 79.87 79.33 79.60 80.09 () 79.21 77.60 78.10 78.03 77.98 77.76 77.43 77.94 () 73. 73.40 73.24 73.16 72.14 73.24 73.01 73.64 () 45.00 45.60 45.20 46.80 45.00 45.60 46.00 45.00 () 49.43 39.50 39.50 40.49 40.71 39.35 41.32 40.94 () 33. 32.80 31.98 32.93 32.74 32.35 33.03 32.54 () 7.25 7.84 7.83 7.82 7.84 7.82 7.82 7.83 () 3.09 3.32 3.38 3.35 3.35 3.27 3.28 3."
        },
        {
            "title": "B FP Quantization Results",
            "content": "To demonstrate the effectiveness of SQ-format on floating point, we also conduct experiments on DeepSeek-R1 (685B) with FP8/FP4 quantization. We apply SQ-format on weights (bank_size = 64, sparsity = 0.875) and keep activations in FP8, achieving an effective bit-width of 5 bits. As shown in Table 8, SQ-format demonstrates strong scalability on massive models, maintaining near-lossless performance compared to the BF16 baseline across both generative and non-generative benchmarks. Table 8 Performance of SQ-format on DeepSeek-R1. Model Setting Sparsity DeepSeek-R W16A16 0 W(SQ5)A8 0.875 Non-generative Acc (%) Generative Acc (%) Perplexity ARC-c ARC-e PIQA Hellas. Wino. OBQA GSM8k AGIEval Wiki. Lamb. () 64.42 63.99 () 85.52 85. () 84.98 85.31 () 87.43 87.19 () 79.95 79. () 48.47 46.67 () 95.83 96.21 () 70.22 69. () 3.33 3.39 () 12.67 12."
        },
        {
            "title": "C Hardware Synthesis Analysis",
            "content": "To assess the hardware overhead of the dynamic mask handling in SQ-format, we conduct RTL synthesis experiments. We implement the SQ-format unit designed for 4x sparse weights and INT8 activations. For fair comparison, we select standard INT6 MAC array as the baseline. Table 9 presents the normalized area breakdown. This result validates that SQ-format is physically efficient design capable of increasing compute density for next-generation AI accelerators. Table 9 Area synthesis comparison normalized by adder area."
        },
        {
            "title": "Component",
            "content": "SQ-format (Ours) Standard INT6 MAC Multiplier Adder Accumulator Low-bit Multiplier Gather Unit"
        },
        {
            "title": "Total Area",
            "content": "0.42 0.24 0.21 0.42 0.783 2.073 2.23 1.00 0.002 3.232 19 End-to-End Latency Analysis We profile the end-to-end prefilling latency and effective memory bandwidth of SQ-format with static activation strategy on GPUs. The profiling is conducted using the WikiText dataset. As shown in Table 10, SQ-format significantly outperforms the W4A8 baseline, effectively bridging the gap towards the theoretical W4A4 performance limit while maintaining accuracy. Table 10 End-to-end prefilling latency and effective bandwidth on GPUs. Setup: bank_size = 64."
        },
        {
            "title": "Time",
            "content": "Effective BW (GB/s)"
        },
        {
            "title": "Speedup",
            "content": "Llama-3-8B Llama-3-70B W4A8 SQ-format SQ-format SQ-format W4A4 W4A8 SQ-format SQ-format SQ-format W4A4 - 0.5 0.75 0.875 - - 0.5 0.75 0.875 - 48s 45s 42s 41s 38s 10m 8s 7m 42s 6m 29s 5m 55s 5m 16s 34.69 35.75 36.90 38.19 40.61 10.84 14.27 16.94 18.54 20.86 1.00 1.07 1.14 1.17 1.26 1.00 1.32 1.56 1.71 1."
        }
    ],
    "affiliations": [
        "ByteDance",
        "HKUST",
        "Moffett AI"
    ]
}