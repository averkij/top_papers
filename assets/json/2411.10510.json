{
    "paper_title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers",
    "authors": [
        "Joseph Liu",
        "Joshua Geddes",
        "Ziyu Guo",
        "Haomiao Jiang",
        "Mahesh Kumar Nandwana"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 0 1 5 0 1 . 1 1 4 2 : r SmoothCache: Universal Inference Acceleration Technique for Diffusion Transformers Joseph Liu Roblox josephliu@roblox.com Joshua Geddes Queens University j.geddes@queensu.ca Ziyu Guo Roblox zguo@roblox.com Haomiao Jiang Roblox haomiaojiang@roblox.com Mahesh Kumar Nandwana Roblox mnandwana@roblox.com Figure 1. Accelerating Diffusion Transformer inference across multiple modalities with 50 DDIM Steps on DiT-XL-256x256, 100 DPMSolver++(3M) SDE steps for 10s audio sample (spectrogram shown) on Stable Audio Open, 30 Rectified Flow steps on Open-Sora 480p 2s videos."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache1, modelagnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models. 1Code can be found at https://github.com/Roblox/SmoothCache In the rapidly evolving landscape of generative modeling, Diffusion models [8, 30] have emerged as pivotal force, offering unparalleled capabilities for creating rich and diverse content. Diffusion transformers (DiT) [24, 32], in particular, harnessing the scalable architecture of transformers, demonstrated significant advancements in various domains, including but not limited to the creation of images [3], audio [5, 12, 14], video [11, 21, 36], and 3D models [23]. The central challenge limiting the broader adoption of DiTs is the computational intensity of their inference process. The cost of running the entire diffusion pipeline lies primarily in the denoising steps, and improving denoising efficiency directly contributes to faster diffusion pipeline. To address this, inference optimizations have been researched from two directions: (1) reducing the number of sampling steps, and (2) lowering the inference cost per step. The use of advanced solvers [16, 18] have been proposed to reduce the number of timesteps required for sampling, thereby accelerating inference. Techniques such as knowledge distillation [27], architecture optimizations [13], pruning [6], and quantization [7, 29] aim to reduce the compu1 tational complexity of individual denoising steps. While these methods have shown promise, there remains need for techniques that can further accelerate DiT inference across diverse modalities without compromising generation quality. Caching [1, 15, 20, 34] has emerged as potential solution by exploiting the inherent redundancy in the diffusion process. Previous and concurrent work on DiT caching has investigated uniform scheduling [28], training caching pattern with dataset [19], and exploiting qualitative characteristics about candidate model to create caching pattern [4, 35]. However, these caching techniques are limited by either overly simplistic or model-specific strategies or reliance on costly retraining procedures. To overcome these limitations, we introduce SmoothCache, simple and universal caching scheme capable of speeding up diverse spectrum of Diffusion Transformer models. SmoothCache leverages the observation that layer outputs from adjacent timesteps in any DiT-based model exhibits high cosine similarities [19, 28], suggesting potential computational redundancies. By carefully leveraging layer-wise representation errors from small calibration set, SmoothCache adaptively determines the optimal caching intensity at different stages of the denoising process. This allows for the reuse of key features during inference without significantly impacting generation quality. SmoothCache is designed with generality in mind, and can be applied to any DiT architecture without model-specific assumptions or training while still achieving performance gains over uniform caching. We show that SmoothCache with general training-free caching scheme is able to speedup the performance across image, video, audio domains to match or exceed SOTA caching scheme for each dedicated domain. We demonstrate that SmoothCache is also compatible with various common solvers in diffusion transformers. 1.1. Related Work Diffusion models [8, 30] have emerged as powerful class of generative models, capable of producing high-quality samples across various domains such as images, audio, and video. Initally, U-Net architectures were favored for this denoising step due to their strong ability to capture both global and local information in the latent space, crucial for reconstructing fine details in the data [25, 26]. However, the inherent limitations of U-Nets, particularly their struggle to scale effectively to high-dimensional data and long sequences, led to the exploration of alternative architectures. The Diffusion Transformer (DiT) [24] was proposed as solution to U-Nets to exploit the scalability of the transformer architecture [32]. Transformers, renowned for their ability to handle long-range dependencies and their inherent scalability, have proven highly effective in various sequence-based tasks. DiT successfully adapted these advantages to the diffusion framework, demonstrating strong performance in generating not only images but also extending to diverse domains like speech [5, 12, 14], video [11, 21, 36], and 3D generation [23]."
        },
        {
            "title": "Despite their",
            "content": "successes, DiT models, like many transformer-based architectures, face challenges with computational efficiency during inference. The core operations of attention and feed-forward layers within transformers In the context of diffuhave quadratic time complexity. sion models, where these operations are performed repeatedly over multiple denoising steps, this quadratic dependency leads to significant computational overhead, particularly when generating high-resolution outputs or long sequences. This high inference cost poses major challenge for deploying these models in real-world applications with limited computational resources or strict latency requirements. Efficient Diffusion Models. Diffusion models require many function evaluations to iteratively remove noise from data, and thus quickly scale in computational costs. Many previous works have focused on reducing the number of diffusion steps required to achieve high quality samples by exploring accelerated noise schedulers and fast ODE solvers [1618, 31].Other works have focused on traditional acceleration methods for neural networks in order to reduce latency per step, such as quantization [7, 13, 29] and compression techniques [6, 13, 27]. However, the effectiveness of these techniques are limited since the loss incurred by per-step acceleration methods may become large when used in combination with optimized solvers. Addressing these challenges, more adaptive solution could provide efficiency gains on top of timestep-reduction methods without compromising sample quality. Diffusion Model Caching. Caching has emerged as promising technique for accelerating DiT inference by exploiting computational redundancies in the denoising process. For U-Net based architectures, DeepCache [20] leverages cross-timestep similarities by caching and reusing up sampling feature maps, while other methods use focus on caching blocks of layers [34], cross-attention modules [15], or intermediate noise states [1]. Since these techniques rely on U-Net skip connection features not present in DiTs, further research has been conducted to apply caching to image [4, 19, 28] and video [35] DiT diffusion models. However, all the aforementioned caching techniques are designed to exploit specific principles of the candidate model or architecture, and do not generalize across different diffusion architectures and output modalities. Additionally, some techniques require large-scale training and experimental hyperparameter searches for every sampling configuration, and are not usable out-of-box when applied to new models. SmoothCache addresses these limitations by introducing caching approach that is simple to implement and 2 pθ(xt1xt) = (xt1; µθ(xt, t), Σθ(t)), (2) where µθ(xt, t) is the predicted mean and Σθ(t) is the variance, both learned by the model. The full generative process is defined as: pθ(x0:T ) = p(xT ) (cid:89) t=1 pθ(xt1xt). (3) The generative model is trained by minimizing the variational bound on the negative log likelihood, learning to denoise xt at each timestep and ultimately generate samples from the learned distribution. Traditionally, pθ(xt1xt) has been approximated with U-Net style model architectures, but recent works have begun to use Diffusion Transformer (DiT) architectures, which have shown to scale better, especially for more complex tasks such as video generation. DiT architectures consist of repeated blocks containing the Self-attention, Cross-attention, and Feed-forward layers in the traditional Transformer, which are usually the computational bottleneck for both model training and inference. key property of diffusion models, which has driven the development of caching techniques, is the high cosine similarity between layer outputs at adjacent timesteps. This pattern of similarity is observed across variety of generative models and solvers, spanning different modalities such as image, video, and speech diffusion. This high similarity suggests that there are computational redundancies within the diffusion process that can be leveraged to improve efficiency. 2.2. SmoothCache The objective of SmoothCache is to provide training-free, model-agnostic strategy of caching and reusing layer outputs such that minimal error is introduced. However, we believe single optimal static scheme that caches crosstimestep layer output may not exist across different model architectures, solvers, and modalities. For example, when examining the average representation error between layer outputs of consecutive time steps as shown in Fig. 2, we observe that layers generally exhibit higher differences in later time steps in the DiT-XL label-to-image model, while layers in the Open-Sora text-to-video model are more sensitive in the first and last diffusion time steps. This discovery emphasizes need to apply the similarity principle in generalizable technique, such that different models benefit differently based on error curves. Let Lt represent the output of some layer at timestep t, and let Ltk represent the output of the same layer at some future diffusion timestep k. By the cross-timestep layer similarity observation, Ltk Lt. Thus instead of computing Ltk during the diffusion process, we can approximate Figure 2. L1 Relative Error Curves of different architecture components. Curves are plotted with 95% confidence intervals from 10 calibration samples from all components explored in this paper and scaled to the same y-axis range. Note that OpenSora has distinct spatial and temporal diffusion blocks. training-free, but still makes caching decisions informed by specific diffusion process, outperforming uniform schedules. The model-agnostic framework adapts to various model, sampling, and solver configurations with just one calibration inference pass and single hyper-parameter α, to achieve optimal performance. 2. Method In this section, we describe the preliminary setup, base assumptions, observations and math fomulation of the proposed SmoothCache method. 2.1. Preliminaries The diffusion process transforms data x0 q(x0) by adding Gaussian noise over steps, producing sequence x1, x2, . . . , xT . This is modeled as Markov chain, where each step follows: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI), where βt is the noise variance at timestep and xT (0, I) due to the cumulative effect of the noise added during the diffusion steps. The goal of the reverse process is to recover the original data x0 from the noisy sample xT . This is learned via neural network that parameterizes the reverse transition: (1) 3 Figure 3. Illustration of SmoothCache. When the layer representation loss obtained from the calibration pass is below some threshold α, the corresponding layer is cached and used in place of the same computation on future timestep. The figure on the left shows how the layer representation error impacts whether certain layers are eligible for caching. The error of the attention (attn) layer is higher in earlier timesteps, so our schedule caches the later timesteps accordingly. The figure on the right shows the application of the caching schedule to the DiT-XL architecture. The output of the attn layer at time 1 is cached and re-used in place of computing FFN 2, since the corresponding error is below α. This cached output is introduced in the model using the properties of the residual connection. the function by using the previously computed Lt. Any time the layer is computed, our method stores Lt in cache that can be accessed and used in place of future layer computations. We apply caching to the aforementioned computational bottlenecks at the output that precedes residual connection, shown in Fig. 3. This includes Self-attention and Feed-forward layers in the DiT-XL model, Self-attention, Cross-attention, and Feed-forward layers in the StableAudio model, and Self-attention, Cross-attention, and Feedforward layers in both the spatial and temporal blocks in the OpenSora model as shown in Fig. 4. Fig. 5 shows that these components comprise of nearly all of the compute that occurs during generation. We also note that the compute distribution varies from model to model. To determine whether to use previously cached output, we define the following problem. Let represent the current timestep, + represent the timestep when the cache was previously filled, and ij represent the jth layer of type i, where = {attn, ffn, ...} depending on the model architecture. Our method is guided by the key hypothesis that caching is effective if the loss between the computed and cached outputs L(Lij ,t, Lij ,t+k), is bounded by some layer-dependent hyper parameter αij > 0. Computing L(Lij ,t, Lij ,t+k), is not possible without evaluating Lij ,t, which removes any benefit from skipping the layer computation. For given DiT architecture, we remarkably observe that the difference in layer representation error for two different samples is within negligibly small threshold, as shown the 95% confidence interval of the plots generated with 10 calibration samples in Fig. 2. This finding suggests that the error curve for specific model input L(Lij ,t, Lij ,t+k) can be closely approximated by the average error curve for an adequately large set of calibration inputs. In other words, if Lij ,t represents the calibration output for layer Lij ,t, then L(Lij ,t, Lij ,t+k) L( Lij ,t, Lij ,t+k). hyper parameter search to find all αij has exponential search space based on the number of layers and sigIn order to simplify the caching probnificantly costly. lem, we define single hyper parameter α > 0 to guide caching for all layers. We define as the average L1 relative error of all layers of type i, which is selected in Figure 4. SmoothCache-Eligible Layers of candidate models. This visualization highlights the targeted layers that precede residual connections in DiT block for each architecture. Each model contains DiT blocks. In the original DiT-XL model, Selfattention and Feed-forward layers are cached. In the Stable Audio Open model, Self-attention, Cross-attention, and Feed-forward layers are cached. In the Open Sora model, Self-attention, Crossattention, and Feed-forward layers across both the temporal and spatial partitions of the DiT block. 4 Figure 5. SmoothCache-Eligible Layers Compute Composition of candidate models. These are computed from the MACs of the default configurations experimented on in this paper. order to compare true representation errors between layers for all types and and positional depths in the network. Additionally, we recognize caching specific layers can introduce errors in future layers of the same type in the network. For example, if we approximate Lij1,t with Lij1,t+k, this may introduce noise such that the calibration error L( Lij ,t, Lij ,t+k) no longer correctly approximates the true error L(Lij ,t, Lij ,t+k), which leads to poor caching decisions. In order to mitigate the cascading impact of caching layers, we group caching decisions for all layers of type i, such that all layers in Li,t+k approximate Li,t. Thus, cached output Lij ,t+k is used in place of computing Lij ,t when L(Lij ,t, Lij ,t+k)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 Lij ,t Lij ,t+k1 Lij ,t1 < α (4) Using cached outputs is computationally inexpensive, and allows for significant speedup of the diffusion inference process. As Eq. 4 makes no prior assumptions about the specific properties of the particular diffusion process being cached, it can be applied across multiple architectures and modalities. Additionally, because caching decisions are only dependent on calibration error, they do not change at model runtime. This ensures compatibility with existing graph compilation optimizations. 3. Experiments 3.1. Experiment Setup Models, Datasets, and Solvers In order to demonstrate the effectiveness of our technique across DiT architectures and prove the model-agnostic claim, we evaluate SmoothCache on multiple candidate diffusion models across variety of modalities, across different numbers of diffusion steps. Text to Image Generation Firstly, we select the original DiT-XL-256x256 label to image diffusion model, using the original released model weights 2. Since the DiT family of models is trained on the ImageNet1k dataset, we generate 50,000 256x256 images for evaluation. We test SmoothCache with the DDIM solver, and use classifier free guidance with scale of 1.5. Text to Video Generation Secondly, we select the OpenSORA text to video model. We use the pre-trained OpenSORA v1.2 model, and evaluate its performance using VBench evaluation protocol using videos generated from the 946 prompts from the VBench prompt suite on 2 second 480p videos with 9:16 aspect ratio. We use flash attention and the bfloat16 datatype for inference. We test SmoothCache using the Rectified Flow [16] solver for 30 sampling steps across 1000 Euler steps, using the default CFG scale of 7.0. Text to Audio Generation Lastly, we select the Stable Audio Open text to audio model. We follow the same inference protocol as described in the original paper [5]. We run inference Stable Audio Open with DPM-Solver++ (3M) SDE for 100 steps with classifier free guidance scale of 7.0. Evaluation Metrics We use variety of standard metrics in each domain to demonstrate the efficiency to quality trade off. We record the Multiply-Accumulate Operations (MACs) and latency of the full diffusion process. We derive the acceleration ratio from baseline sampling latencies. To measure generation quality, we evaluate the candidate models on common metrics in the corresponding domain. For DiT-XL-256x256, we generate 50,000 images with the specified configuration and report the FID, IS, and sFID. For OpenSORA, we evaluate the performance of the model using VBench[9], and generate 946 videos based on the VBench suite prompts. We report the final scaled VBench score. Lastly, for Stable Audio Open, we use the exact same evaluation protocol described in the Stable Audio Open technical report [5] using the same evaluation code3, reporting CLAP, FDOpenL3 and KLPaSST metrics for AudioCaps [10], MusicCaps without singing prompts [2], and Song Describer without singing prompts [22]. All speed results are measured on single H100-80G GPU, and averaged across 50 runs. Implementation Details As mentioned previously, we apply caching to layers that precede residual connections. In the DiT-XL-256x256 model, this includes the Selfattention and Feed-forward modules in the DiT block. In OpenSora V1.2, we cache the temporal Self-attention, Cross-attention and Feed-forward modules as well as their equivalent spatial variants (for total of 6 types of modules). In Stable Audio Open, we cache the Self-attention, Cross-attention and Feed-forward modules. For each architecture, solver, and preset number of diffusion steps, we obtain the representation errors L(Fij ,t, Fij ,t+k) by computing differences in layer outputs for some random samples. For all modalities, we use 10 samples for calibration. Ablations show that the choice of the number of calibration samples does not matter much, something that can be observed in Fig. 2. We also choose which samples to generate the error curves based on whether we did conditional generation 2https://github.com/facebookresearch/DiT 3https://github.com/Stability-AI/stable-audio-metrics 5 Table 1. Results For DiT-XL-256x256 on using DDIM Sampling, sorted by TMACs. Note that L2C is not training free. Schedule Steps FID () sFID () IS () TMACs Latency (s) L2C No Cache Ours (α = 0.08) FORA (n=2) Ours (α = 0.18) FORA (n=3) Ours (α = 0.22) No Cache FORA (n=2) Ours (α = 0.35) No Cache FORA (n=2) Ours (α = 0.08) FORA (n=3) Ours (α = 0.12) 50 50 50 50 50 50 50 30 30 70 70 70 70 70 2.27 0.04 4.23 0.02 245.8 0.7 278.71 2.28 0.03 2.28 0.03 2.65 0.04 2.65 0.04 3.31 0.05 3.14 0. 4.30 0.02 4.29 0.02 4.69 0.03 4.65 0.03 5.71 0.06 5.19 0.04 241.6 1.1 241.8 0.9 238.5 1.1 238.7 1.1 230.1 1.3 231.7 1.0 2.66 0.04 3.79 0.04 3.72 0.04 4.42 0.03 5.72 0.05 5.51 0.05 234.6 1.0 222.2 1.2 222.9 1.0 2.17 0.02 2.36 0.02 2.37 0.02 2.80 0.02 2.68 0. 4.33 0.02 4.46 0.03 4.29 0.03 5.38 0.04 4.90 0.04 242.3 1.6 242.2 1.3 242.6 1.5 238.0 1.2 238.8 1.3 365.59 336.37 190.25 175.65 131.81 131.81 219.36 117.08 117.08 511.83 263.43 248.8 175.77 175.77 6. 8.34 7.62 5.17 4.85 4.12 4.11 4.88 3.13 3.13 11.47 7.15 6.9 5.61 5.62 during calibration or not, which we only do for OpenSora and Stable Audio Open. We fix {1, 2, 3} for DiT-XL256x256 and Stable Audio Open as we determine the layer representation error to grow too large past difference of 4 timesteps. In OpenSORA, goes up to 5 in particular due to relatively low error in the cross-attention components of the network. For DiT, we calibrate on samples generated unconditionally using the null prompt. We calibrate OpenSora on conditionally generated 480p 2s videos with randomly sampled prompts from VidProM [33]. For Stable Audio Open, we calibrate using randomly sampled prompts from the AudioCaps validation set. 3.2. Results We present the results of SmoothCache on DiT-XL256x256, OpenSora, and Stable Audio Open in Tables 1, 2, and 3 respectively. In order to provide fair comparison between acceleration and quality trade offs, we compare the default solver used in the model, such as DDIM or DPM++, with SmoothCache implementation with specified hyper parameters. We find two configurations with identical acceleration ratios show SmoothCache to perform better on various quality metrics, indicating superior performance over the typical acceleration/quality tradeoff. For all results, we run 5 trials and report the mean and standard deviation for each metric."
        },
        {
            "title": "3.2.1 Quantitative results",
            "content": "Comparing against existing literature In order to further investigate the true effectiveness of our technique, we compare SmoothCache to existing DiT caching techniques in Table 2. Results For OpenSora on Rectified Flow. Schedule Steps VBench (%) () TMACs Latency (s) No Cache Ours (α = 0.02) Ours (α = 0.03) 30 30 30 79.36 0.19 78.76 0.38 78.10 0.51 1612.1 1388.5 1321.1 28.43 26.57 26. literature developed concurrently with SmoothCache, such Fast-Forward Caching (FORA) [28] and Learning-to-Cache (L2C) [19] for Label-to-Image generation. We note that the above methods relies on specific properties of the relevant model architecture, and does not necessarily translate across different modalities, while SmoothCache is agnostic to those considerations as it models the caching scheme directly off the observed error curves. FORA would not work in OpenSora or Stable Audio Open due to the difference in the error curves as seen in Fig. 2. We see that SmoothCache outperforms FORA across different inference times at 50 sampling steps, yielding similar performance at lower inference time or better performance with the same inference time. The one exception here is L2C, which requires leveraging the full ImageNet training set to learn policy for given number of sampling steps. Changing the number of sampling steps requires full retraining, and L2C has theoretical maximum of 2x speedup because the caching policy is only learned with skipping every other step, limitations that SmoothCache does not have. Examining inference speedup/quality tradeoff Stable Audio Open and DiT-XL/2-256x256 provides the highest speed/quality tradeoff compared to OpenSora, which gives around 10% speedup latency wise and around 16-22% reduction in MACs comapred to an almost 20-60% speedup for the other modalities, whose discrepancy could be attributed by the larger overhead of non-DiT components (since inference latency is measured end-to-end). We also observe that the amount of inference speed/quality tradeoff looks directly correlated to the error deviation between calibration samples which can be seen in Fig. 2, with the higher variance in error among individual samples across timesteps for OpenSora versus that for Stable Audio Open and DiT-XL/2-256x256."
        },
        {
            "title": "3.2.2 Qualitative results",
            "content": "We show visual examples for all modalities. We show 256x256 images for image generation, using nullconditional prompts. For audio, we show the log-mel spectrogram in order to visualize the waveform for conditional prompt generation across the 3 datasets we measure. For video, we show the first, middle and last frame of 2 second 480p 24 fps videos at 9:16 aspect ratio. We see that for DiT-XL/2-256x256 outputs in Fig. 6, even though there is drop of FID with SmoothCache applied, that there is some noticeable difference in performance quality when there is higher threshold applied, but there exists threshold where the quality of the model performance is visually indistinguishable from the model without caching. This shows the fine-granularity in inference speed/quality tradeoff that SmoothCache affords, while static caching as shown in FORA can only yield the 6 Table 3. Results For Stable Audio Open on DPMSolver++(3M) SDE on 3 datasets. AudioCaps MusicCaps (No Singing) Song Describer (No Singing) Schedule FDOpenL3 () KLPaSST () CLAP () FDOpenL3 () KLPaSST () CLAP () FDOpenL3 () KLPaSST () CLAP () TMACs Latency (s) No Cache Ours (α = 0.15) Ours (α = 0.30) 81.7 6.8 84.5 6.7 89.6 6.3 2.13 0.02 0.287 0.003 2.15 0.02 0.285 0.003 2.17 0.02 0.271 0. 82.7 2.1 85.9 2.3 82.0 1.5 0.931 0.012 0.467 0.001 0.942 0.012 0.467 0.001 0.962 0.012 0.448 0.001 105.2 6.3 106.2 6.6 131.3 5.9 0.551 0.024 0.421 0.003 0.555 0.024 0.420 0.003 0.596 0.028 0.392 0.003 209.82 170.75 136.16 5.65 4.59 3. Figure 6. SmoothCache results on DiT-XL/2-256x256 for unconditional generation with 50 DDIM sampling steps on ImageNet-1k for thresholds 0.08 and 0.18, as well as for Static Caching. lower quality SmoothCache threshold. We observe particularly that this degradation is less pronounced in Stable Audio Open in Fig. 7, where the audio waveforms look visually identical, and when listened to do not have perceptible differences. This is consistent with the qualitative results which show only minor degradation across all measured metrics across all datasets. For OpenSora, we see more significant differences in when caching in Fig. 8, with noticeable artifacting when the higher caching threshold is applied. This is consistent with the quantitative results, and suggests this architecture/modality is more sensitive to caching than the other modalities, which makes sense due to the complex spatial/temporal modelling that other modalities explored do not have to deal with. 3.3. Ablations We ablate our investigations by showing how model performance varies across different step sizes, and show the robustness of SmoothCache for the DiT-XL-256x256 model. Examining the Caching/Sample Step Pareto Front We attempt to show via comparison with static caching that our technique yields better Pareto front along different sampling steps through the image generation results. We note that all caching techniques have varying performance as you vary the number of solver sampling steps, and we show that despite to the minimal assumptions SmoothCache makes about the sampler and model architecture, SmoothCache at least matches up to other caching strategies if not outright beating them across multiple fronts. Table 1 shows that even with lower or higher number of sampling steps, SmoothCache outperforms FORA across multiple inference speed/quality points at different number of sampling steps. We observe that SmoothCache outperforms Static Caching for multiple sampling configurations. While L2C slightly outperforms SmoothCache on DDIM sampling for DiT-XL, SmoothCache does not require expensive training over ImageNet1k, and generalizes to different model architectures and sampling configurations. Calibration sample size We note that the choice of the number of calibration samples does not adversely affect the caching schedule generated. Empirically, we observe that 10 samples for all 3 models investigated in this paper is usually enough to reliably regenerate the same caching schedule given the same α. We however note that the confidence interval of the different error curves vary from modality to modality as seen in Fig. 2. 4. Limitations and Future Work The main limitation of the SmoothCache technique is its reliance on the repeated DiT block architecture, particularly the residual connections following the aforementioned computational bottleneck layers. These connections allow 7 Figure 7. SmoothCache Results on Stable Audio Open for threshold 0.15 and 0.3. Log-Mel Spectrograms are shown. for the output yt to be approximated by some linear function of input and previously cached layers (xt+k) + xt, feature we consider essential for its caching effectiveness. Additionally, the performance gains from SmoothCache are strongly linked to the computational intensiveness of the candidate layers. Consequently, it may yield smaller performance improvements when applied to DiT networks with limited depth or width. Figure 8. SmoothCache Results on OpenSora for threshold 0.03 for 2s 480p videos. We show the first, middle and last frame of each video. We use the following prompts, in order (1) wave crashes against from top to bottom: rocky shore, sending spray high into the (2) Chocolate sauce is poured slowly air. over stack of fluffy pancakes. (3) The bund Shanghai, pan right 8 secondary limitation lies in the assumption that errors from approximating outputs of earlier layers have minimal impact on the loss function guiding caching decisions for deeper layers. For instance, if the first Self-attention layer is approximated using cached result from previous timestep, caching the second Self-attention layer could introduce further output discrepancies, even if the calibration loss suggests low error. This is because the calibration loss is computed when no caching is performed, which may not fully model true approximation errors during SmoothCache-enabled inference. We address this issue by grouping caching and computation decisions for layers of the same type at each timestep. However, this does not fully resolve dependency issues between different layer types, leaving room for further optimization in future work. We also highlight phenomenon that future work can investigate where the pareto front of inference speed/quality seems to correlate with the variance in error across different calibration samples, with architectures/modalities that have higher variance between sample error curves having narrower fronts than those which have lower variance. 5. Conclusion We introduce SmoothCache, training-free caching technique that works with multiple diffusion solvers and multiple modalities. By leveraging layer-wise representation error from calibration inference pass, SmoothCache identifies redundancies in the diffusion process, allowing it to cache and reuse output feature maps, reducing the number of computationally expensive layer operations. Our evaluations show it matches or exceeds the performance of existing modality-specific caching methods."
        },
        {
            "title": "References",
            "content": "[1] Shubham Agarwal, Subrata Mitra, Sarthak Chakraborty, Srikrishna Karanam, Koyel Mukherjee, and Shiv Saini. Approximate caching for efficiently serving diffusion models. arXiv preprint arXiv:2312.04429, 2023. 2 [2] Andrea Agostinelli, Timo I. Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. Musiclm: Generating music from text, 2023. 5 [3] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-$alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 1 [4] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. δ-dit: training-free acceleration method tailored for diffusion transformers. arXiv preprint arXiv:2406.01125, 2024. 2 [5] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv preprint arXiv:2407.14358, 2024. 1, 2, 5 [6] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models, 2023. 1, 2 [7] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 [8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2 [9] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 5 [10] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In NAACL-HLT, 2019. 5 [11] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 1, 2 [12] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36, 2024. 1, 2 [13] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 [14] Huadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, and Zhou Zhao. Vittts: visual text-to-speech with scalable diffusion transformer. arXiv preprint arXiv:2305.12708, 2023. 1, 2 [15] Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, Juan-Manuel Perez-Rua, and Jurgen Schmidhuber. Faster diffusion via temporal attention decomposition, 2024. 2 [16] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 1, 2, [17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. [18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 1, 2 [19] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. arXiv preprint arXiv:2406.01733, 2024. 2, 6 [20] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: In Proceedings of Accelerating diffusion models for free. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1576215772, 2024. 2 [21] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 1, 2 [22] Ilaria Manco, Benno Weck, Seungheon Doh, Minz Won, Yixiao Zhang, Dmitry Bogdanov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, Elio Quinton, Gyorgy Fazekas, and Juhan Nam. The song describer dataset: corpus of audio captions for music-and-language In Machine Learning for Audio Workshop at evaluation. NeurIPS 2023, 2023. [23] Shentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li. Dit-3d: Exploring plain diffusion transformers for 3d shape generation. Advances in neural information processing systems, 36:6796067971, 2023. 1, 2 [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2 [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 9 [27] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 1, 2 [28] Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. 2, 6 [29] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19721981, 2023. 1, 2 [30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 1, 2 [31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 2 [32] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 1, 2 [33] Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. In Thirty-eighth Conference on Neural Information Processing Systems, 2024. 6 [34] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62116220, 2024. 2 [35] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. 2 [36] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 1,"
        }
    ],
    "affiliations": [
        "Queens University",
        "Roblox"
    ]
}