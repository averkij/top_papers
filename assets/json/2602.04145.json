{
    "paper_title": "Training Data Efficiency in Multimodal Process Reward Models",
    "authors": [
        "Jinyuan Li",
        "Chengsong Huang",
        "Langlin Huang",
        "Shaoyang Xu",
        "Haolin Liu",
        "Wenxuan Zhang",
        "Jiaxin Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%."
        },
        {
            "title": "Start",
            "content": "Jinyuan Li 1 Chengsong Huang 1 Langlin Huang 1 Shaoyang Xu 2 Haolin Liu 3 Wenxuan Zhang 2 Jiaxin Huang 1 Abstract Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)- annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training. Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora. To explain this, we formalize theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by relative 4.1%. Our code is released Balanced-Info-MPRM. 6 2 0 2 4 ] . [ 1 5 4 1 4 0 . 2 0 6 2 : r 1. Introduction Process Reward Models (PRMs) (Ma et al., 2023; Zhu et al., 2025; Tan et al., 2025) provide step-level supervision for reasoning by scoring intermediate steps instead of only the final answer. In multimodal reasoning, Multimodal PRMs (MPRMs) are increasingly used for Multimodal Large 1Washington University in St. Louis 2Singapore University of Technology and Design 3University of Virginia. Correspondence to: Jinyuan Li <ljinyuan@wustl.edu>, Jiaxin Huang (Corresponding Author) <jiaxinh@wustl.edu>. Preprint. February 5, 2026. Figure 1. Overall micro-F1 on VisualProcessBench using InternVL2.5-8B trained via full training set or different subsets. Our BIS-10% effectively matches the final performance of FullData setting using 10 fewer rollouts. Language Models (MLLMs) (Wang et al., 2024e;f; Bai et al., 2023; 2025; Liu et al., 2023; Team et al., 2024; 2025) to conduct complex visual reasoning tasks both during training and at test time (Wang et al., 2025a;b; Zhang et al., 2025a; Luo et al., 2024; Du et al., 2025; Tu et al., 2025; Cao et al., 2025; Dong et al., 2025). Common practices for training MPRMs generally rely on large-scale Monte Carlo (MC)-annotated rollouts (e.g., VisualPRM400K-v1.1 (Wang et al., 2025b), with 565K rollouts and 3.17M annotated steps), which makes training computationally expensive. In this paper, we study the practical bottleneck in training data efficiency for MPRMs: how does MPRM performance scale with the rollout budget, and how can we select informative subsets that preserve full-data performance? Our preliminary study suggests substantial redundancy in MC-annotated MPRM training data. We randomly subsample the training data at varying fractions ρ and find that performance quickly saturates at small ρ, with moderate gap to the full dataset. This trend persists even when the subset is trained longer to match full-data training steps. We further compare several size-matched heuristic subsets and find that selecting rollouts that mix correct and incorrect steps is more informative than random selection, whereas rollouts with the lowest average MC scores tend to contain noisy pseudopositive labels and hurt performance. This suggests two key criteria for high-quality rollouts: mixture and reliability. Training Data Efficiency in Multimodal Process Reward Models To substantiate this intuition, we formalize teacherstudent abstraction framework for theoretical analysis, and connect the interplay between gradient signal, label noise and data redundancy. We model MC estimation noise via probabilistic label-flip model and show how it affects training gradients. This modeling supports the view that MPRM training is primarily limited by gradient noise rather than data scarcity. Moreover, our theory explains why mixture and reliability capture rollout quality: mixture tracks model uncertainty, while reliability, measured by MC scores, captures the noise level in positive steps. Their contributions interact multiplicatively in shaping informative gradients. Building on these insights, we introduce the BalancedInformation Score (BIS), rollout-level criterion that instantiates the mixed but reliable principle. BIS quantifies both label mixture (of positive and negative steps) and reliability (average MC score over positive steps). It is modelagnostic and only uses the MC signals stored in the dataset, without requiring extra model calls. Extensive experiments with two backbones (InternVL2.5-8B (Chen et al., 2024c) and Qwen2.5-VL-7B (Bai et al., 2025)) on VisualProcessBench (Wang et al., 2025b) show that BIS recovers full-data performance at small subset ratios, with the largest gain over random sub-sampling in low-budget regimes. In particular, Figure 1 shows that BIS-selected 10% subset trained for only 50 steps suffices to reach and even surpass the full-data performance on InternVL2.5-8B, saving 95.5% computational cost. Taken together, these findings provide practical recipe with grounded analysis for reducing training compute for MPRMs without sacrificing model performance. 2. Preliminary Study 2.1. Background and General Setup Previous MPRM research mainly improves supervision pipelines or training frameworks (detailed in Appendix A). In contrast, we study post-hoc rollout selection with no extra supervision or compute. We adopt the standard MPRM training setup and keep it fixed throughout. Following prior works (Wang et al., 2024d; Zhang et al., 2025g), we use the VisualPRM400K-v1.1 dataset (Wang et al., 2025b), where each reasoning step is annotated with an MC-estimated success rate from =16 sampled continuations. Step labels are binarized, so yt=1 if the MC score > 0 (at least one continuation reaches correct final answer), and yt=0 otherwise. Specifically, for reasoning rollout with steps, special token <prm> is appended after each step t. The model is trained to predict the step-level probability (Yes/No) tokens using the cross-entropy loss = (cid:80)T t=1(yt log pt + (1 yt) log(1 pt)), where pt is the probability of predicting the token Yes. We use InternVL2.5-8B (Chen et al., 2024c) in preliminary study, Table 1. Dataset statistics for different training-set settings. Steps denote reasoning steps with annotated labels. Metric Full-Data Random-25% Low-MC-25% Mixed-25% # rollouts # reasoning steps Avg. steps/rollout Avg. words/step Error-step ratio Avg. MC/step 565,096 3,174,394 5.62 27.8 3.57% 0.8566 141,288 794,756 5.63 27.8 3.61% 0.8590 141,210 796,940 5.64 29.9 12.57% 0.6010 141,253 795,752 5.63 27.6 11.02% 0.7160 and evaluate the MPRM performance on VisualProcessBench (Wang et al., 2025b), human-annotated step-level benchmark spanning five sources (MathVision (Wang et al., 2024c), MathVerse (Zhang et al., 2024), MMMU (Yue et al., 2024), DynaMath (Zou et al., 2025), and WeMath (Qiao et al., 2025)), and follow its protocol to report per-source macro-F1 and micro-averaged F1 over all sources. Training details, data statistics are provided in Appendix C, and H. 2.2. Random Sub-Sampling: Evidence of Redundancy (cid:17) Empirical Finding 1: MPRM performance quickly saturates under random subsampling, indicating strong redundancy in the training data. To assess how MPRM performance scales with processsupervision data, we use the full training corpus as FullData and evaluate random subsampling. For any keep ratio ρ, Random-ρ retains fraction ρ of rollouts from each of the 38 source subsets, preserving their relative composition. We train with single-pass fine-tuning and report micro-F1 for different Random-ρ subsets in Figure 2a. Performance improves with ρ but quickly plateaus, exhibiting pronounced diminishing returns. This suggests substantial redundancy in the MC-annotated rollouts, as discarding large fraction of rollouts only modestly degrades performance. To probe the plateau at moderate ρ, we take ρ = 25% and compare Random-25% with Full-Data under matched compute budget. We match the number of training steps by training Random-25% for four epochs, making its training cost comparable to one epoch of Full-Data. Table 1 summarizes the corpus statistics for these settings. We compare their learning curves in Figure 2b. Although the Full-Data model eventually performs better, the gap to Random-25% remains moderate, confirming substantial redundancy in the training data. We also report per-source results in Appendix D. Additionally, under matched updates, Random-25% slightly overfits and its performance degrades late in training. In the remaining experiments, we use singlepass fine-tuning, where each rollout is seen exactly once. Given the redundancy above, natural next question is: is there principled data selection method that substantially filters training data while preserving full-data performance? 2 Training Data Efficiency in Multimodal Process Reward Models (a) (b) Figure 2. Overall VisualProcessBench micro-F1 under different data regimes. (a) Single-pass scaling with random sub-sampling; per-source macro-F1 curves are shown in Figure 4. (b) Training on Full-Data vs. Random-25% under matched updates; per-source macro-F1 curves are shown in Figure 5. (c) Training on three 25% subsets for one epoch; per-source macro-F1 curves are shown in Figure 6. Full-Data denotes the best checkpoint of one-epoch Full-Data run (4 more optimization steps than 25% subsets). (c) 2.3. Characterizing Informative Rollouts (cid:17) Empirical Finding 2: Effective supervision comes from mixed rollouts that contain both correct and incorrect steps while maintaining reliable positive labels. We now shift focus from how many rollouts to use to which rollouts to keep. To study the impact of increased exposure to negative steps, we construct three subsets of VisualPRM400K: Random-25%, Low-MC-25%, and Mixed-25%. Random-25% randomly samples 25% of rollouts from each source to preserve the original dataset distribution. Low-MC-25% is constructed by ranking rollouts within each source by their average MC score per step and retaining the bottom 25%. As result, the average MC per step drops to 0.601 and the incorrect-step ratio rises to 12.57%, far higher than in Random-25% (3.61%). Many low-MC steps have only few successful continuations out of = 16, yet are still labeled as positive under the standard binarization rule, making them prone to pseudo-positive labels. Mixed-25% prioritizes rollouts with both positive and negative steps. Since mixed rollouts are only 7.67%, when source has fewer than 25% mixed rollouts, we fill the remainder by randomly sampling from the rest. It has similar incorrect-step ratio to Low-MC-25% (11.02% vs. 12.57%) but higher average MC score (0.716 vs. 0.601), exposing the model to many negative steps while still anchoring them with reasonable amount of reliable positive labels. Table 1 summarizes the statistics for these subsets. Using the same training protocol, we fine-tune each 25% subset for one epoch and plot the overall VisualProcessBench microF1 over training steps in Figure 2c. From this comparison, we observe two patterns as follows: First, at their best checkpoints, the three subsets satisfy Mixed-25% > Low-MC-25% > Random-25%. Both LowMC-25% and Mixed-25% outperform Random-25%, indicating that, under fixed data budget, exposing the model to more incorrect steps is beneficial. Second, Mixed-25% consistently yields the strongest performance even though its incorrect-step ratio is comparable to Low-MC-25% while its average MC score is notably higher. This suggests that neither maximizing negative steps nor minimizing average MC scores alone is sufficient. Extremely low-MC steps tend to be noisy pseudo-positives (labeled positive despite very low success rates), whereas rollouts that combine reasonably reliable positive steps with clear errors provide more useful supervision. These observations motivate us to find rollout-scoring mechanism that prioritizes two aspects: (1) emphasizing mixed rollouts (containing both correct and incorrect steps) while (2) avoiding noisy rollouts that contain many extremely low MC-score steps. 3. Theoretical Analysis Before introducing our scoring mechanism, we provide theoretical analysis to explain empirical findings 1 and 2. We formalize the interplay among data redundancy, label noise, and gradient behavior, which also guides the design of an effective data-selection score. 3.1. TeacherStudent Abstraction We model MPRM training using linear teacherstudent the teacher represents the ideal model that framework: knows true step-level correctness, while the student model learns from noisy MC-annotated labels. We model steplevel label prediction as logistic regression on the representation space ϕ for simplicity. For the j-th step in rollout x, let ϕx,j Rd denote the hidden representation at the <prm> token position and true x,j {0, 1} its binary label. An ideal teacher MPRM is q(ϕ) = Pr(Y true = 1 ϕ) = σ(cid:0)w, ϕ(cid:1), (1) where Rd is the optimal parameter and σ is the sigmoid function. The student MPRM (our learned model) is qw(ϕ) = σ(cid:0)w, ϕ(cid:1), (2) trained by minimizing the expected logistic loss L(w) = E(ϕ,Y )[Y log qw(ϕ) (1 ) log(1 qw(ϕ))] . (3) 3 Training Data Efficiency in Multimodal Process Reward Models In the MC-annotated training set, each step is associated with an MC score sx,j [0, 1] from sampled continx,j = I[sx,j > 0]. For the uations and binary label mc theoretical analysis, we do not model the MC sampling explicitly; instead, we model (ϕx,j, true x,j ) as i.i.d. samples from the teacher model in Eq. (1). The observed MC score sx,j (and the corresponding binarized training label mc x,j ) provides noisy estimate of the underlying correctness probability q(ϕx,j). Under this formulation, the student qw is trained with the logistic loss in Eq. (3) on the observed MC labels mc x,j , matching the objective in Section 2.1. 3.2. Understanding the Plateau of Random Subsets (cid:17) Theoretical Finding 1: MPRM training mostly suffers from noisy gradients instead of insufficient training data. In this part, we aim to explain why randomly sub-sampled subsets across varying ρ recover much of the full-dataset performance. In the teacherstudent setup of Section 3.1, we consider training the student model to minimize logistic loss L(w), with being the optimal parameter (achieved with infinite data and infinite training), and wT the parameters after finite stochastic gradient descent (SGD) steps. Under the assumptions in Appendix F.1, standard nonasymptotic analyses of SGD for logistic regression (Bach & Moulines, 2013; Bottou & Bousquet, 2007) yield bound on the excess risk (the gap in expected loss between wT and w) of the form E(cid:2)L(wT )(cid:3) L(w) Cdata 1/2 + Copt 1/2 (4) This bound contains two components: (1) data complexity term Cdata 1/2 , which decays with the effective sample size Neff , and (2) an optimization error term Copt 1/2, which decays with the number of SGD updates. Cdata, Copt > 0 are problem-dependent constants that do not scale with Neff or . detailed derivation of Eq. (4) is given in Appendix F.1. eff eff Why larger datasets help less than expected. In MCannotated training data, many steps receive noisy labels, especially when only few out of continuations succeed. This label noise increases the stochastic-gradient noise level, which effectively enlarges the constant Copt in the optimization term (Moulines & Bach, 2011). Meanwhile, for VisualPRM400K-v1.1 with 3.17M annotated steps, the data complexity term CdataN 1/2 is relatively small. Taken together, the optimization term dominates the total error, and further increasing Neff yields only marginal gains. eff Now consider random subset Random-γ that keeps each data point with probability γ (0, 1), so the effective sample size becomes γNeff while the problem-dependent constants remain comparable. Let Tγ denote the number of 4 SGD updates. In the matched-update setting we keep the update budget fixed, Tγ = , and Eq. (4) gives 1 2 CdataN 1/2 eff + CoptT E[L(wTγ )] L(w) γ Random sub-sampling therefore amplifies only the (already small) data term by γ1/2, while leaving the optimization term unchanged; once CdataN 1/2 eff CoptT 1/2, changing γ has only modest effect on the total error, explaining why Random-25% closely tracks Full-Data in Figure 2b. 1 2 . eff + CoptT 1 2 . The right-hand side can In the single-pass setting we have Tγ = γT , so Eq. (4) gives 1 2 . 1 2 + Copt(γT ) E[L(wTγ )] L(w) Cdata(γNeff ) 1 2 Let := CdataN 1 be written as Bγ := γ 2 B, so both terms are scaled by the γ1/2. For the full-data configuration with 3.17M annotated steps and 1.1k updates, we operate in lowerror regime where ϵtar. Here ϵtar is the target error level. When is already far below ϵtar, multiplying it by the constant factor γ1/2 still gives Bγ ϵtar, so both Full-Data and Random-γ single-pass training remain within the desired accuracy range. In such regime, constant factor γ1/2 in the bound is not enough to induce large performance gap, which matches the small empirical difference we observe in Figure 2c and reinforces that improving gradient quality, rather than merely enlarging Neff , is key to escaping the current optimization floor. 3.3. Why Mixed but Reliable Rollouts Are Informative? Here, we explain why mixture and reliability characterize informative rollouts: label mixture tracks teacher uncertainty, while MC scores quantify the reliability of step labels. Step-level Information from Teacher Uncertainty (cid:17) Theoretical Finding 2: Ideal teacher-model uncertainty q(ϕ)(cid:0)1 q(ϕ)(cid:1) quantifies per-step information. Under the teacherstudent framework in Section 3.1, the gradient of logistic loss for step with representation ϕ Rd and label {0, 1} is g(ϕ, ; w) = (cid:0)qw(ϕ) (cid:1) ϕ, Since we study offline data selection, which is fixed throughout training, we measure how informative step is under the teacher distribution rather than an evolving student, yielding student-independent criterion. At the teachers optimal parameter w, the second moment of the gradient has the form (derivation in Appendix F.2) qw(ϕ) = σ(w, ϕ). E(cid:2)g(ϕ, ; w)2 ϕ(cid:3) = q(ϕ)(cid:0)1 q(ϕ)(cid:1) ϕ2, (5) where q(ϕ) = qw (ϕ). E(cid:2)g(ϕ, ; w)2 ϕ(cid:3) quantifies the expected per-step learning signal. Thus, for step-level MPRM training, the most informative steps are those where Training Data Efficiency in Multimodal Process Reward Models the teacher is most uncertain (q(ϕ) 1/2), when the term q(ϕ)(cid:0)1 q(ϕ)(cid:1) reaches its maximum. conditioning on {qx,j} yields E[ˆpx(1 ˆpx) {qx,j}] = qx(1 qx) Effect of Label Noise at Step Level (cid:17) Theoretical Finding 3: Extremely low-MC positive steps behave like label-reversed samples and produce gradients that carry little true signal and harm training. To analyze how MC label noise affects learning, we adopt symmetric label noise approximation: the true label is flipped independently with probability η [0, 1/2) to produce noisy label , where restricting to η < 1/2 is without loss of generality since larger rates can be reduced to 1η by flipping the label semantics. In practice the effective noise is step-dependent, and we adopt this constant noise distribution to derive the explicit formula for the second moment of the noisy gradient at w, where g(ϕ, ; w) = (q(ϕ) ) ϕ. direct computation (Appendix F.3) yields E(cid:2)g(ϕ, ; w)2 ϕ(cid:3) (cid:16) = (1 4η) q(ϕ)(cid:0)1 q(ϕ)(cid:1) + η (cid:17) ϕ2. (6) Relative to the clean case in Eq. (5), the uncertainty term q(ϕ)(cid:0)1 q(ϕ)(cid:1) is shrunk by (1 4η), with an additional q-independent noise ηϕ2. When η is large, gradients are increasingly noise-dominated and carry little useful signal. Empirically, steps with extremely low-MC scores but positive labels are typically unstable: only few out of all continuations succeed for incidental reasons (e.g., later selfcorrection), so these steps act like label-flipped negative steps rather than real positive steps. 1 n (cid:88) j=1 qx,j(1qx,j). (7) By Jensens inequality for (cid:55) t(1 t), we have A(x) = 1 (cid:88) j=1 qx,j (cid:0)1 qx,j (cid:1) qx(1 qx) =: θx. (8) Combining (7) and (8) yields the sandwich bound (cid:16) 1 θx (cid:17) 1 E(cid:2)ˆpx(1 ˆpx) {qx,j}(cid:3) θx. (9) Thus ˆpx(1 ˆpx) is an observable estimate for the teacherlevel uncertainty θx, with only O(1/n) bias, and A(x) θx by construction. Consequently, in this noise-free setting, rollouts that are nearly all-positive or all-negative have small ˆpx(1 ˆpx), suggesting smaller θx (and hence smaller expected A(x)). In contrast, rollouts with mixed labels tend to have larger θx, which allows A(x) to be larger and can yield gradient updates with stronger learning signal. Now consider the symmetric flip noise model: Yx,j is obtained by flipping Yx,j with constant rate η. Then Yx,j qx,j Bernoulli(qx,j) with qx,j = (1 2η)qx,j + η (Appendix F.4 Eq. (22)). Averaging over steps gives qx = (12η)qx+η, and the induced rollout-level mixture satisfies (10) Eq. (10) decomposes θx into scaled uncertainty term (1 2η)2θx plus an offset η(1 η). Since θx θx only for small η (Appendix F.4 Eq. (24)), we next complement mixture with reliability signal to identify low-noise rollouts. θx := qx(1 qx) = (1 2η)2 θx + η(1 η). Rollout Label Mixture Estimates Teacher Uncertainty MC Scores as Effective Noise Indicators (cid:17) Theoretical Finding 4: Rollout label mixture ˆpx(1 ˆpx) is an O(1/n)-biased estimator of the unobserved teacher-level uncertainty θx = qx(1 qx) under noise-free labels. We now relate rollout label mixture to the teachers positivelabel probabilities as follows. For rollout with steps we define average step-wise information: A(x) := 1 (cid:88) j=1 qx,j (cid:0)1 qx,j (cid:1). Under the bounded-norm assumption in Appendix F.4, the unweighted and norm-weighted quantities differ by at most global multiplicative constants. For exposition, we begin with the teacher-consistent idealization Yx,j qx,j Bernoulli(qx,j), under which {Yx,j}n j=1 are unbiased samples from {qx,j}. Let ˆpx := 1 j=1 Yx,j be the empirical positive-label fraction, and let qx := 1 j=1 qx,j be the step-average teacher probability. Then ˆpx(1 ˆpx) is maximized when labels are balanced; it becomes 0 when the steps are all-positive or all-negative, so it directly measures label mixture within rollout x. By Lemma 2 in Appendix F.4, (cid:80)n (cid:80)n (cid:17) Theoretical Finding 5: MC scores monotonically reflect label reliability: low-MC positives exhibit high effective noise. Mixture alone is not sufficient because positive labels can be noisy. We now model the MC annotation process and connect MC scores to the label-noise level. For step in rollout x, let rx,j [0, 1] be the probability that single continuation from this step reaches the correct final answer, given its representation ϕx,j. The MC annotator generates independent continuations, records the number of successful ones Kx,j, and stores the score sx,j = Kx,j/N . Under the Binomial model Kx,j rx,j Binomial(N, rx,j) we have E[sx,j rx,j] = rx,j, Var(sx,j rx,j) = 1 rx,j(1rx,j), so sx,j is an unbiased estimator of rx,j and concentrates around it as grows. Under the standard binarization rule Yx,j = I[Kx,j > 0], the resulting step-level probability of the positive label is Pr(Yx,j = 1 ϕx,j) = 1 (1 rx,j)N which is strictly increasing in rx,j. Since rx,j is deter5 Training Data Efficiency in Multimodal Process Reward Models mined by ϕx,j, we model this probability using the teacher q(ϕx,j) = σ(w, ϕx,j) in Eq. (1). These observations link the MC score sx,j, the binarized label Yx,j, and the teacher probability q(ϕx,j) through the underlying success probability rx,j. We next quantify how this link translates into an effective noise level for positive labels. To formalize reliability, we fix threshold τ (0, 1) and define step to be τ -reliable if its one-shot success probability is at least τ , i.e., Zx,j := I[rx,j τ ]. We then relate this reliability notion to what the MC annotator actually observes. Specifically, we let the unobserved success probability rx,j vary across steps and model it with Beta distribution rx,j Beta(a, b). Conditional on rx,j, the number of successful continuations among the MC samples satisfies Kx,j rx,j Binomial(N, rx,j). Under this BetaBinomial model, observing Kx,j = yields the posterior rx,j Kx,j = Beta(a + k, + k). This induces an effective noise level for positive steps, defined as ηeff (k) := Pr(Zx,j = 0 Kx,j = k) = Pr(rx,j < τ Kx,j = k) = Iτ (a + k,b + k) where Iτ (, ) denotes the regularized incomplete beta function. For Kx,j > 0, ηeff (k) is exactly the posterior probability that positive step with Kx,j = is pseudo-positive, i.e., τ -unreliable. Moreover, ηeff (k) is strictly decreasing in (Lemma 3 in Appendix F.5). Consequently, lowMC positives (Kx,j > 0 but small sx,j = Kx,j/N ) have large ηeff and are likely to be τ -unreliable, so they behave like high-noise samples. Under the label-flipping noise model of Eq. (6), this corresponds to operating at larger noise rate η ηeff (Kx,j), which increases the noise term ηϕx,j2 and decreases the signal term q(ϕx,j)(1 q(ϕx,j))ϕx,j2. Averaging sx,j over positive steps in rollout thus yields natural rollout-level reliability estimate: since ηeff (k) decreases monotonically with the MC score sx,j = Kx,j/N , rollouts whose positive steps have higher average MC scores also have smaller average ηeff (Kx,j), so their gradient updates are less affected by label noise. Mixture and Reliability Couple Multiplicatively (cid:17) Theoretical Finding 6: Rollouts are most informative when both label mixture and reliability are high. Finally, we combine the previous two parts and analyze how label mixture and reliability jointly shape the rolloutlevel signal term in Eq. (6). We average Eq. (6) over steps in rollout x, let qx,j := q(ϕx,j) and the rollout-level q-dependent signal term takes the form S(x) := 1 (cid:88) j=1 (1 4ηx,j) qx,j(1 qx,j) ϕx,j2. where ηx,j is the step-dependent noise rate, and can be approximated by ηeff (Kx,j). The signal term S(x) is dethe term termined by the multiplication of two factors: qx,j(1 qx,j) favors rollouts with uncertain steps; while (1 4ηx,j) favors smaller label noise ηx,j (larger Kx,j or sx,j based on the monotonicity). Therefore, S(x) of rollout is large only when the multiplication of uncertainty and reliability is large. In practice, we approximate the teacher uncertainty θx := qx(1 qx) using the observable label mixture ˆpx(1 ˆpx). Section 4 distills this into the BalancedInformation Score used in our data selection method. 4. Balanced-Information Score Motivated by the empirical results above and the theoretical analysis, we introduce the Balanced-Information Score (BIS) as rollout-level scoring mechanism that prioritizes informative rollouts for MPRM training. Setting. Consider an MPRM training set where each rollout contains annotated steps with MC scores {sj}n j=1 [0, 1]. Following the standard binarization rule, each step is assigned hard label yj = I[sj > 0]. Rollout-level Quantities. For rollout x, define the positive-step ratio ppos(x) = 1 j=1 yj, used to quantify label mixture, and rollout-level reliability measure R(x): (cid:80)n R(x) = 1 npos 1, (cid:80) j:yj =1 sj, npos > 0, npos = 0, where npos = (cid:80)n j=1 yj. Rollouts with npos = 0 contain no positive labeled steps to average over, so we fix R(x) = 1. Balanced-Information Score. We define the BalancedInformation Score (BIS) of rollout as: BIS(x) = (cid:0)ppos(x) (cid:0)1 ppos(x)(cid:1) + α(cid:1) R(x), where the hyperparameter α > 0 is small smoothing constant that assigns non-zero weight to low-mixture rollouts. The term ppos(1 ppos) favors mixed rollouts that contain both correct and incorrect steps, while R(x) favors rollouts whose positive steps are reliably correct under MC estimation. Therefore, BIS(x) is highest for rollouts that provide both clear negative signals and trustworthy positive anchors. Subset Construction with Keep Ratio ρ. Given global keep-ratio ρ (0, 1) applied uniformly across all sources, we assign BIS to every MC-annotated rollout to build dataefficient training set. Within each source dataset, we rank rollouts by BIS(x) in descending order and keep the top ρ fraction. We then concatenate the selected rollouts over all sources to form the BIS-selected subset. This procedure only relies on existing step-level MC scores and does not require additional supervision or extra model calls. The 6 Training Data Efficiency in Multimodal Process Reward Models Table 2. Overall micro-F1 and per-source macro-F1 on VisualProcessBench for full-data training and sub-sampled rollouts under different keep ratios ρ. Soft uses the raw MC scores as continuous soft targets; Hard (τ ) uses binary labels with threshold τ . Bold numbers denote the column-wise maximum within each subset group. Base denotes the original backbone model without any additional training. Dataset Overall MathVision MathVerse MMMU DynaMath WeMath Dataset Overall MathVision MathVerse MMMU DynaMath WeMath Base 52.28 52.40 52.04 50.21 54.85 49. Base 49.68 50.22 49.58 49.85 49. 48.51 InternVL2.5-8B Qwen2.5-VL-7B Full-Data (565k 1,100 steps) Hard (τ =0) 65.12 Hard (τ =1/N ) 64.26 Hard (τ =2/N ) 63.02 Soft 61.54 65.77 64.91 61.61 60. 65.43 63.91 62.92 60.47 61.84 61.07 59.70 62.05 Random-5% BIS-5% Random-10% BIS-10% Random-15% BIS-15% Random-25% BIS-25% Random-35% BIS-35% Random-50% BIS-50% 5% Subsets (28k 55 steps) 64.95 66.66 62.42 64.53 58.94 60. 10% Subsets (56k 110 steps) 64.65 66.90 62.14 65.07 60.25 63.35 15% Subsets (84k 165 steps) 65.06 67. 63.00 64.44 57.23 61.80 25% Subsets (141k 275 steps) 64.49 67.98 62.60 64.86 58.32 60. 35% Subsets (198k 385 steps) 65.50 67.25 63.49 64.47 58.01 59.61 50% Subsets (283k 550 steps) 64.55 65. 63.94 63.79 60.25 63.03 63.34 64.51 62.86 65.46 63.27 64.98 63.37 65. 63.52 64.98 64.02 65.00 66.17 65.44 65.83 62.91 65.91 63.80 62.98 65.56 64.17 64. 65.83 65.72 64.00 65.79 65.38 66.66 63.56 64.80 65.09 64.38 62.57 64.40 63.25 65. 63.96 65.40 63.67 65.59 63.08 64.82 64.14 66.03 Full-Data (565k 1,100 steps) Hard (τ =0) 65.57 Hard (τ =1/N ) 65.27 Hard (τ =2/N ) 62.72 Soft 62. 66.05 66.17 62.33 62.17 65.29 64.53 62.18 61.25 63.23 63.83 62.56 61.44 Random-5% BIS-5% Random-10% BIS-10% Random-15% BIS-15% Random-25% BIS-25% Random-35% BIS-35% Random-50% BIS-50% 5% Subsets (28k 55 steps) 54.38 66.69 52.82 64. 55.85 62.72 10% Subsets (56k 110 steps) 63.42 66.05 61.85 64.14 58.37 61.33 15% Subsets (84k 165 steps) 59.32 66.50 60.58 64.03 54.97 66.40 25% Subsets (141k 275 steps) 65.87 66.84 63.38 64. 61.99 63.60 35% Subsets (198k 385 steps) 66.45 66.77 64.43 65.50 60.47 63.37 50% Subsets (283k 550 steps) 66.52 67.22 64.27 64.30 59.74 60.80 53.54 64.42 61.99 64.63 59.82 65. 64.44 65.53 64.77 65.69 64.54 65.02 66.24 66.60 63.37 62.88 52.80 63.34 62.24 64. 61.32 64.80 66.32 66.19 65.34 65.65 65.73 65.62 66.40 64.55 64.65 65.55 53.04 63. 61.97 66.05 60.36 66.58 63.44 66.66 64.84 65.99 62.89 64.99 downstream MPRM can be trained from this ρ-subset with the same training setup of Section 2.1. 5. Experiments 5.1. Training Setup We conduct experiments with two backbones: InternVL2.58B (Chen et al., 2024c) and Qwen2.5-VL-7B (Bai et al., 2025), and evaluate different methods on VisualProcessBench (Wang et al., 2025b). To study data efficiency, we sub-sample rollouts with keep ratios ρ {5, 10, 15, 25, 35, 50}% and compare BIS-ρ against Random-ρ under matched budgets. We additionally include heuristic subset baselines as ablations at selected budgets. All models are trained for single pass over their retained rollouts, with training steps and learning-rate scheduling scaled proportionally to ρ. We also report Best-of-N evaluation with MPRM reranking. Full training details are provided in Appendix C. 5.2. Main Results BIS recovers full-data performance at small ratios and consistently outperforms random sub-sampling. Table 2 compares BIS-ρ to Random-ρ under identical rollout budgets. Across both backbones, BIS reaches full-data performance at small ρ and remains consistently stronger than random sub-sampling, with the largest gains at small rollout budgets. For InternVL2.5-8B, BIS already matches the full-data performance at ρ=10%, reaching an overall micro-F1 of 65.46%, +2.6 points gain over the random baseline Random-10%, while using only one tenth of the rollouts and updates. For Qwen2.5-VL-7B, BIS shows even larger advantages in the extremely low-budget regime: it improves over random sub-sampling by +10.9 points at ρ=5% and +5.5 points at ρ=15%, and already reaches the full-data reference at ρ=25%. We report the complete training dynamics for all keep ratios in Appendix G, including both overall micro-F1 and per-source macro-F1, and show that BIS maintains clear advantages over random sub-sampling throughout training. BIS also shows clear scaling trend with ρ: performance improves rapidly at small budgets, peaks at moderate keep ratio, and can slightly drop afterwards, as increasing ρ mainly adds lower-BIS rollouts that are less informative under our mixture reliability criterion. Since ρ=25% performs strongly for both backbones, we use it for analysis in the following experiments. Effect of labeling scheme. We further study the impact of the threshold for binarizing MC scores into hard labels and show the results in Table 2. First, training with soft labels is clearly inferior to default hard-label scheme. This is consistent with MC scores being noisy and coarsely discretized estimates, thus encouraging the MPRM to fit sampling noises. Second, increasing the binarization threshold to above 0 consistently degrades performance, suggesting that low-MC labels conflate hard cases with noisy pseudo-positives and that stricter thresholding can mislabel hard cases as negatives. BIS therefore avoids this ambiguous low-MC regime by prioritizing mixed yet reliable rollouts. 7 Training Data Efficiency in Multimodal Process Reward Models Table 3. Best-of-N evaluation on four benchmarks with MPRM reranking trained on different training sets. Model MM-K12 OlympiadBench MathVerse MathVista 33.13 InternVL2.5-8B 39.00 5.87 +MPRMFull-Data +MPRMRandom-25% 39.40 6.27 41.00 7.87 +MPRMBIS-25% 8.65 12.00 3.35 11.33 2.68 12.67 4.02 35.31 52.77 39.41 4.10 57.50 4.73 39.41 4.10 58.20 5.43 40.89 5.58 59.00 6.23 Table 4. Ablations of BIS under 25% rollout budget. Subset Overall MathVision MathVerse MMMU DynaMath WeMath BIS-25% Mixed-25% 65. 64.70 0.76 Reliable-25% 62.75 2.71 Low-MC-25% 64.18 1.28 67. 66.32 1.66 62.12 5.86 66.31 1.67 64.86 64.78 0.08 63.14 1. 64.31 0.55 60.49 58.65 1.84 60.52 0.03 59.40 1.09 65. 65.66 0.06 63.85 1.87 64.16 1.56 65.59 64.51 1.08 63.14 2. 62.97 2.62 BIS improves best-of-N reranking. We further evaluate MPRM in practical best-of-N reranking setting with = 16 candidates per problem on four different benchmarks (MM-K12 (Du et al., 2025), OlympiadBench (He et al., 2024), MathVerse (Zhang et al., 2024), and MathVista (Lu et al., 2024)) in Table 3. The full evaluation protocol is described in Appendix C. Consistent with the main results, MPRMs trained on BIS-selected subsets achieve the strongest best-of-N performance across all benchmarks, outperforming both Random-25% and the full-data MPRM. This suggests that BIS yields robust improvements in MPRM effectiveness beyond benchmark-specific behavior. BIS favors moderate-R(x) rollouts. Figure 3 shows the distribution of the reliability term R(x) on VisualPRM400Kv1.1 for all rollouts and the BIS-25% subset. Additional per-source statistics are provided in Appendix H. The black curve demonstrates the selected data coverage over the full data. BIS strongly suppresses the low-reliability rollouts where R(x) are small which tend to include noisy step labels. Meanwhile, the coverage peaks at moderate R(x) (around 0.20.6) and decreases as R(x) becomes very large, showing that BIS does not simply maximize R(x). This is because BIS jointly considers reliability and mixture: rollouts with large R(x) may still have low mixture, resulting in low overall BIS score. Consistently, the right panel shows that BIS favors rollouts with higher mixture, explaining why high-R(x) rollouts are not always preferred. Ablation study: both mixture and reliability matter. Table 4 ablates the two components of BIS under the same 25% rollout budget. Mixed-25% and Low-MC-25% are heuristic subsets of Section 2.3, while Reliable-25% retains the top 25% of rollouts ranked by R(x). We observe that only using the mixture score (Mixed-25%) is competitive but still consistently weaker than the BIS both on average and on Figure 3. Distributions of the reliability term R(x) (left) and mixture term p(x)(1 p(x)) (right) on VisualPRM400K-v1.1, comparing all rollouts and the BIS-25% subset. The black curve shows the coverage (Selected / All). Table 5. Sensitivity of BIS to the smoothing constant α under fixed 25% rollout budget. Subset Overall MathVision MathVerse MMMU DynaMath WeMath α = 0.02 64.84 α = 0.05 65.46 α = 0.08 64.86 66.63 67.98 67.10 64.69 64.86 64.47 60.49 60.49 59. 64.78 65.72 64.88 65.27 65.59 65.35 separate sources. Low-MC-25% shows similar trend and remains weaker than BIS, indicating that heuristic filtering alone is insufficient to match BIS selection. In contrast, using only the reliability score (Reliable-25%) is clearly insufficient: it lags behind BIS on nearly all benchmarks and shows an advantage only on MMMU where performances are very close. This aligns with the analysis in Section 3.3 that the best performance is obtained only when both mixture and reliability are considered, where the mixture term provides contrast between positive and negative labels, and the reliability term steers away from noisy low-MC positive labels. We also plot the complete learning curves in Appendix E, and further demonstrate that BIS-25% yields the highest or near-highest performance at almost all steps. BIS is robust to α. We ablate the smoothing constant α for the mixture term, which controls the lower-bound when ppos(1 ppos) is small. Table 5 indicates that performance is broadly stable across different α values, with consistent best choice at α = 0.05. Overly small α can underweight low-mixture yet reliable trajectories and reduce the diversity of retained supervision, whereas overly large α weakens the mixture term and shifts selection toward reliability-only ranking. The intermediate value best balances these effects. 6. Conclusion We study how to select Monte Carloannotated multimodal reasoning rollouts for training MPRMs. We found that randomly discarding most rollouts only mildly degrades performance, indicating that current training sets contain substantial redundancy. Our theoretical analysis explains 8 Training Data Efficiency in Multimodal Process Reward Models that informative gradient updates concentrate on uncertain yet reliably labeled steps, while low-MC pseudo-positives mainly add variance. We propose the Balanced-Information Score (BIS), which ranks rollouts by label mixture and reliability using only the MC signals already stored in the dataset. Empirical results demonstrate that BIS-selected subsets match or surpass full-data MPRM performance with as little as 10% of rollouts. Overall, our study provides data-centric principle for curating future MPRM corpora."
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to improve the data and compute efficiency of training multimodal process reward models. Successful adaptation of our method to practical model training will benefit the society by reducing energy cost."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Han Li and Zhangchen Xu for their valuable insights. This research was supported in part by the NVIDIA Academic Grant Program and WashU Ignite Interdisciplinary Grants."
        },
        {
            "title": "References",
            "content": "Anthropic. The claude 3 model family: Opus, sonnet, haiku. Available at: https://www.anthropic.com/ne ws/claude-3-family, 2024. Accessed: 2025-1223. Bach, F. and Moulines, E. Non-strongly-convex smooth stochastic approximation with convergence rate (1/n). Advances in neural information processing systems, 26, 2013. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: frontier large visionlanguage model with versatile abilities. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Bottou, L. and Bousquet, O. The tradeoffs of large scale learning. Advances in neural information processing systems, 20, 2007. Cao, J. and Xiao, J. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th international conference on computational linguistics, pp. 15111520, 2022. Cao, Q. and Xie, P. Dreamprm-1.5: Unlocking the potential of each instance for multimodal process reward model training. arXiv preprint arXiv:2509.05542, 2025. Cao, Q., Wang, R., Zhang, R., Somayajula, S. A., and Xie, P. Dreamprm: Domain-reweighted process reward model for multimodal reasoning. arXiv preprint arXiv:2505.20241, 2025. Chang, S., Palzer, D., Li, J., Fosler-Lussier, E., and Xiao, N. MapQA: dataset for question answering on choropleth maps. In NeurIPS 2022 First Table Representation Workshop, 2022. URL https://openreview.net /forum?id=znKbVjeR0yI. Chen, G., Liao, M., Li, C., and Fan, K. Alphamath almost zero: Process supervision without process. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https: //openreview.net/forum?id=VaXnxQ3UKo. Chen, J., Li, T., Qin, J., Lu, P., Lin, L., Chen, C., and Liang, X. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 33133323, 2022. Chen, Q., Qin, L., Zhang, J., Chen, Z., Xu, X., and Che, W. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 81998221, 2024b. Chen, X., Liu, B., Wang, X., Wang, Y., and Lu, C. Vrprm: Process reward modeling via visual reasoning. arXiv preprint arXiv:2508.03556, 2025. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024c. Cui, G., Yuan, L., Wang, Z., Wang, H., Zhang, Y., Chen, J., Li, W., He, B., Fan, Y., Yu, T., et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Ding, Y., Shi, X., Li, J., Tu, Z., Zhang, M., et al. Scan: Self-denoising monte carlo annotation for robust process reward learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Dong, G., Zhang, C., Deng, M., Zhu, Y., Dou, Z., and Wen, J.-R. Progressive multimodal reasoning via active retrieval. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 9 Training Data Efficiency in Multimodal Process Reward Models 1: Long Papers), pp. 35793602, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.180. URL https://aclanthology.org/2025.ac l-long.180/. Huang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu, S., and Jawahar, C. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 15161520. IEEE, 2019. Du, L., Meng, F., Liu, Z., Zhou, Z., Luo, P., Zhang, Q., and Shao, W. Mm-prm: Enhancing multimodal mathematical reasoning with scalable step-level supervision. arXiv preprint arXiv:2505.13427, 2025. Duan, K., Liu, Z., Mao, X., Pang, T., Chen, C., Chen, Q., Shieh, M. Q., and Dou, L. Efficient process reward model training via active learning. arXiv preprint arXiv:2504.10559, 2025. Fan, K., Feng, K., Lyu, H., Zhou, D., and Yue, X. Sophiavlr1: Reinforcing mllms reasoning with thinking reward. arXiv preprint arXiv:2505.17018, 2025. Gao, J., Pi, R., Zhang, J., Ye, J., Zhong, W., Wang, Y., HONG, L., Han, J., Xu, H., Li, Z., and Kong, L. GLLaVA: Solving geometric problem with multi-modal large language model. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=px16 74Wp3C. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Han, J., Buntine, W., and Shareghi, E. Uncertainty-based methods for automated process reward data construction and output aggregation in mathematical reasoning. arXiv preprint arXiv:2508.01773, 2025. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, 2024. Hosu, V., Lin, H., Sziranyi, T., and Saupe, D. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing, 29:40414056, 2020. Hu, P., Zhang, Z., Chang, Q., Liu, S., Ma, J., Du, J., Zhang, J., Liu, Q., Gao, J., Ma, F., et al. Prm-bas: Enhancing multimodal reasoning through prm-guided beam annealing search. arXiv preprint arXiv:2504.10222, 2025. Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29012910, 2017. Kafle, K., Price, B., Cohen, S., and Kanan, C. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 56485656, 2018. Kahou, S. E., Michalski, V., Atkinson, A., Kadar, A., Trischler, A., and Bengio, Y. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. Kazemi, M., Alvari, H., Anand, A., Wu, J., Chen, X., and Soricut, R. Geomverse: systematic evaluation of large models for geometric reasoning. In AI for Math Workshop@ ICML 2024, 2024. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., and Farhadi, A. diagram is worth dozen images. In European conference on computer vision, pp. 235251. Springer, 2016. Khalifa, M., Agarwal, R., Logeswaran, L., Kim, J., Peng, H., Lee, M., Lee, H., and Wang, L. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. Kuang, P., Wang, X., Liu, W., Dong, J., Xu, K., and Wang, H. Tim-prm: Verifying multimodal reasoning with toolintegrated prm. arXiv preprint arXiv:2511.22998, 2025. Li, Z., Wang, X., Stengel-Eskin, E., Kortylewski, A., Ma, W., Van Durme, B., and Yuille, A. L. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14963 14973, 2023. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2024. URL https://openreview.net/forum ?id=v8L0pN6EOi. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 10 Training Data Efficiency in Multimodal Process Reward Models Liu, W., Li, J., Zhang, X., Zhou, F., Cheng, Y., and He, J. Diving into self-evolving training for multimodal reasoning. arXiv preprint arXiv:2412.17451, 2024. Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., and Zhu, S.-C. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 67746786, Online, August 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.528. URL https: //aclanthology.org/2021.acl-long.528/. Lu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu, Z., Liang, X., and Zhu, S.-C. IconQA: new benchmark for abstract diagram understanding and visual language reasoning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https: //openreview.net/forum?id=uXa9oBDZ9V1. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=KUNz EQMWU7. Luo, L., Liu, Y., Liu, R., Phatale, S., Guo, M., Lara, H., Li, Y., Shu, L., Zhu, Y., Meng, L., et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Luo, R., Zheng, Z., Wang, L., Wang, Y., Ni, X., Lin, Z., Jiang, S., Yu, Y., Shi, C., Chu, R., et al. Unlocking multimodal mathematical reasoning via process reward model. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Ma, Q., Zhou, H., Liu, T., Yuan, J., Liu, P., You, Y., and Yang, H. Lets reward step by step: Step-level reward model as the navigators for reasoning. arXiv preprint arXiv:2310.10080, 2023. the association for computational linguistics: ACL 2022, pp. 22632279, 2022. Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: In Proceedings dataset for vqa on document images. of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Mathew, M., Bagal, V., Tito, R., Karatzas, D., Valveny, In Proceedings E., and Jawahar, C. of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 16971706, January 2022. Infographicvqa. Moulines, E. and Bach, F. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. Advances in neural information processing systems, 24, 2011. Nesterov, Y. et al. Lectures on convex optimization, volume 137. Springer, 2018. Ong, B., Pala, T. D., Toh, V., Tjhi, W. C., and Poria, S. Training vision-language process reward models for testtime scaling in multimodal reasoning: Key insights and lessons learned. arXiv preprint arXiv:2509.23250, 2025. OpenAI. Gpt-4o system card, 2024. URL https://op enai.com/index/gpt-4o-system-card/. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Qiao, R., Tan, Q., Dong, G., MinhuiWu, M., Sun, C., Song, X., Wang, J., Gongque, Z., Lei, S., Zhang, Y., et al. Wemath: Does your large multimodal model achieve humanlike mathematical reasoning? In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2002320070, 2025. Qwen Team. QVQ: To see the world with wisdom. Available at: https://qwenlm.github.io/blog/qvq-7 2b-preview/, December 2024. Accessed: 2025-1223. Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 35053506, 2020. Masry, A., Do, X. L., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of Seo, M., Hajishirzi, H., Farhadi, A., Etzioni, O., and Malcolm, C. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 11 Training Data Efficiency in Multimodal Process Reward Models conference on empirical methods in natural language processing, pp. 14661476, 2015. large language models. arXiv preprint arXiv:2410.09671, 2024b. Shalev-Shwartz, S. and Ben-David, S. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. Shi, W., Hu, Z., Bin, Y., Liu, J., Yang, Y., Ng, S. K., Bing, L., and Lee, R. K.-W. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 46634680, 2024. Singh, S., Yadav, A., Jain, J., Shi, H., Johnson, J., and Desai, K. Benchmarking object detectors with coco: new path forward. In European Conference on Computer Vision, pp. 279295. Springer, 2024. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y. corpus for reasoning about natural language In Proceedings of the 57th grounded in photographs. annual meeting of the association for computational linguistics, pp. 64186428, 2019. Sun, W., Du, Q., Cui, F., and Zhang, J. An efficient and precise training data construction framework for process-supervised reward model in mathematical reasoning. arXiv preprint arXiv:2503.02382, 2025. Tan, X., Yao, T., Qu, C., Li, B., Yang, M., Lu, D., Wang, H., Qiu, X., Chu, W., Xu, Y., et al. Aurora: Automated training framework of universal process reward models via ensemble prompting and reverse verification. arXiv preprint arXiv:2502.11520, 2025. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M., et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Tu, H., Feng, W., Chen, H., Liu, H., Tang, X., and Xie, C. Vilbench: suite for vision-language process reward modeling. arXiv preprint arXiv:2503.20271, 2025. Wang, K., Pan, J., Shi, W., Lu, Z., Ren, H., Zhou, A., Zhan, M., and Li, H. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024c. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024d. Wang, S., Liu, Z., Wei, J., Yin, X., Li, D., and Barsoum, E. Athena: Enhancing multimodal reasoning with data-efficient process reward models. arXiv preprint arXiv:2506.09532, 2025a. Wang, W., Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Zhu, J., Zhu, X., Lu, L., Qiao, Y., et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024e. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., XiXuan, S., et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475 121499, 2024f. Wang, W., Gao, Z., Chen, L., Chen, Z., Zhu, J., Zhao, X., Liu, Y., Cao, Y., Ye, S., Zhu, X., et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025b. Wang, X., Wang, P., Pei, J., Shen, W., Peng, Y., Hao, Y., Qiu, W., Jian, A., Xie, T., Song, X., et al. Skywork-vl reward: An effective reward model for multimodal understanding and reasoning. arXiv preprint arXiv:2505.07263, 2025c. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pp. 3845, 2020. Wang, H., Xiong, W., Xie, T., Zhao, H., and Zhang, T. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024a. Xiong, W., Zhao, W., Yuan, W., Golovneva, O., Zhang, T., Weston, J., and Sukhbaatar, S. Stepwiser: Stepwise generative judges for wiser reasoning. arXiv preprint arXiv:2508.19229, 2025. Wang, J., Fang, M., Wan, Z., Wen, M., Zhu, J., Liu, A., Gong, Z., Song, Y., Chen, L., Ni, L. M., et al. Openr: An open source framework for advanced reasoning with Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. Mmmu: massive multi-discipline multimodal understanding and 12 Training Data Efficiency in Multimodal Process Reward Models Zheng, C., Zhu, J., Ou, Z., Chen, Y., Zhang, K., Shan, R., Zheng, Z., Yang, M., Lin, J., Yu, Y., et al. survey of process reward models: From outcome signals to process supervisions for large language models. arXiv preprint arXiv:2510.08049, 2025. Zhu, J., Zheng, C., Lin, J., Du, K., Wen, Y., Yu, Y., Wang, J., and Zhang, W. Retrieval-augmented process reward model for generalizable mathematical reasoning. arXiv preprint arXiv:2502.14361, 2025. Zou, C., Guo, X., Yang, R., Zhang, J., Hu, B., and Zhang, H. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. In The Thirteenth International Conference on Learning Representations, 2025. reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Zhang, J., Yan, Y., Zheng, K., Zou, X., Dai, S., and Hu, X. Gm-prm: generative multimodal process reward model for multimodal mathematical reasoning. arXiv preprint arXiv:2508.04088, 2025a. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., and Agarwal, R. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=Ccwp 4tFEtE. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Qiao, Y., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Zhang, R., Wei, X., Jiang, D., Guo, Z., Zhang, Y., Tong, C., Liu, J., Zhou, A., Zhang, S., Gao, P., and Li, H. MAVIS: Mathematical visual instruction tuning with an automatic data engine. In The Thirteenth International Conference on Learning Representations, 2025c. URL https:// openreview.net/forum?id=MnJzJ2gvuf. Zhang, Y., Wu, Y., Zhang, H., Li, W., Chen, H., Li, G., Han, Z., and Tresp, V. Groundedprm: Tree-guided and fidelityaware process reward modeling for step-level reasoning. In NeurIPS 2025 Workshop on Bridging Language, Agent, and World Models for Reasoning and Planning, 2025d. Zhang, Y.-F., Lu, X., Hu, X., Fu, C., Wen, B., Zhang, T., Liu, C., Jiang, K., Chen, K., Tang, K., et al. R1-reward: Training multimodal reward model through stable reinforcement learning. arXiv preprint arXiv:2505.02835, 2025e. Zhang, Y.-F., Yang, H., Zhang, H., Shi, Y., Chen, Z., Tian, H., Fu, C., Wang, H., Wu, K., Cui, B., et al. Basereward: strong baseline for multimodal reward model. arXiv preprint arXiv:2509.16127, 2025f. Zhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. The lessons of developing process reward models in mathematical reasoning. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1049510516, Vienna, Austria, July 2025g. Association for Computational Linguistics. ISBN 979-889176-256-5. doi: 10.18653/v1/2025.findings-acl.547. URL https://aclanthology.org/2025.fi ndings-acl.547/. 13 Training Data Efficiency in Multimodal Process Reward Models"
        },
        {
            "title": "A Related Work",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.1 Multimodal Process Reward Models under MC Process Supervision . A.2 Data-efficient Process Supervision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 . . 19 . 21 . 22 22 24 25 26 28 . 30 . ."
        },
        {
            "title": "B VisualProcessBench Statistics",
            "content": ". . . . . . . . ."
        },
        {
            "title": "C Experimental Setup and Implementation Details",
            "content": "D Extended Results for Random Sub-sampling . Extended Results for 25% Subsets Theoretical Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Step-wise Gradient Variance . . Symmetric Label Noise . F.1 Canonical Logistic Case for the Scaling Decomposition . . . F.2 . . F.3 . F.4 Rollout-Level Mixture and Representation Norms . . F.5 MC-induced Pseudo-positive Probability and Monotonicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Dynamics . . . . . Per-source BIS Histograms . Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Data Efficiency in Multimodal Process Reward Models A. Related Work A.1. Multimodal Process Reward Models under Automated Monte Carlo Process Supervision Recent work (Zheng et al., 2025) shows that MPRMs improve multimodal reasoning both as dense rewards for reinforcement learning fine-tuning (Luo et al., 2025; Wang et al., 2025c; Liu et al., 2024; Fan et al., 2025) and as stepwise verifiers for inference-time trajectory ranking (Zhang et al., 2025a; Cao et al., 2025; Cao & Xie, 2025; Wang et al., 2025a; Tu et al., 2025; Hu et al., 2025). Unlike outcome rewards that score only the final answer (Lightman et al., 2024; Zhang et al., 2025b;f;e; Wang et al., 2024a), an MPRM (Chen et al., 2025; Ong et al., 2025; Kuang et al., 2025) provides dense, step-level supervision by mapping each intermediate multimodal reasoning state to real-valued on-track score conditioned on the input images and text. Most standard MPRM corpora are built from Monte Carlo (MC) estimates computed on reasoning prefixes, with VisualPRM400K (Wang et al., 2025b) as representative example. One common approach samples multiple continuations from each prefix and uses the empirical success rate to score step correctness (Wang et al., 2024d). complementary line replaces plain sampling with structured search via Monte Carlo Tree Search, improving the stability of error localization and supervision signals (Luo et al., 2024; Wang et al., 2024b; Chen et al., 2024a). Despite their differences, MC-based annotators are inherently noisy under finite sampling and long-horizon multimodal reasoning, yielding unstable labels and low-success pseudo-positives that tightening the binarization threshold cannot simply fix and can even hurt MPRM performance (Wang et al., 2025b). Taken together, MC-annotated supervision is plentiful but highly uneven in information content, making rollout-level prioritization crucial for efficient and stable MPRM training. A.2. Data-efficient Process Supervision Recent work on data-efficient process supervision for PRMs can be broadly grouped into three complementary categories. The first line of work optimizes the annotation pipeline itself (Han et al., 2025; Sun et al., 2025; Zhang et al., 2025d; Wang et al., 2025a; Zhang et al., 2025g). These methods improve Monte Carlo or search-based annotators with techniques such as Monte Carlo tree search, tool grounding, and consensus-based filtering, enabling each trajectory to yield higher-quality step labels from fewer or cheaper model calls. second line of work focuses on learning robust process supervision from weak, noisy, or indirect feedback (Ding et al., 2025; Xiong et al., 2025; Khalifa et al., 2025; Cui et al., 2025; Chen et al., 2024a). These approaches design objectives and model forms that allow PRMs to learn effectively from imperfect MC labels and, in some cases, even from outcome-only signals, thereby reducing reliance on expensive, high-quality process annotations. Our work belongs to third line that focuses on data selection and supervision allocation. DreamPRM (Cao et al., 2025) and DreamPRM-1.5 (Cao & Xie, 2025) adjust dataset and example weights via bi-level optimization, while ACTPRM (Duan et al., 2025) and SCAN (Ding et al., 2025) select which samples to query or refine with expensive Monte Carlo estimation under limited annotation budget. In contrast, our rollout-level BIS is scalar score computed post hoc from existing MC statistics, enabling data selection with no extra model calls, relabeling, or changes to the underlying PRM architecture or training objective. 15 Training Data Efficiency in Multimodal Process Reward Models B. VisualProcessBench Statistics Table 6 summarizes the composition of VisualProcessBench (Wang et al., 2025b), including the number of problems per source dataset, the distribution of source solutions across base models, the breakdown of correct/incorrect/neutral steps, and basic length statistics. In total, the benchmark contains 2,866 solution trajectories with 26,950 annotated steps, providing reasonably large and diverse testbed for step-level evaluation. Table 6. Statistics of VisualProcessBench (Wang et al., 2025b). Item"
        },
        {
            "title": "Dataset Composition",
            "content": "MMMU (Yue et al., 2024) MathVision (Wang et al., 2024c) MathVerse (Zhang et al., 2024) DynaMath (Zou et al., 2025) WeMath (Qiao et al., 2025) Source Solutions Source Solutions GPT-4o (OpenAI, 2024) Claude-3.5-Sonnet (Anthropic, 2024) QvQ-72B-Preview (Qwen Team, 2024) InternVL2.5-78B (Chen et al., 2024c) Steps Total Steps Correct Steps Incorrect Steps Neutral Steps Value 2866 267 712 1026 570 291 2866 870 865 825 306 26950 16585 7691 2674 Length Statistics Query Word Length Quartile Response Word Length Quartile Step Word Length Quartile Number of Steps per Solution (22, 24, 50) (137, 193, 552) (13, 31, 67) 9.4 16 Training Data Efficiency in Multimodal Process Reward Models C. Experimental Setup and Implementation Details This appendix provides the full experimental setup omitted from the main text for brevity, including the model and training data, the MPRM training objective, optimization and hardware settings, and the evaluation protocol. Table 7. Training, model, and hardware hyperparameters (shared across all data-selection conditions). Item Value Optimizer Learning Rate Weight Decay AdamW Betas AdamW ϵ LR Schedule Warmup Ratio Gradient Clipping Precision"
        },
        {
            "title": "Optimization",
            "content": "AdamW 1 105 0.05 (0.9, 0.999) 1 108 Linear Warmup + Cosine Decay 0.05 Enabled via DeepSpeed (gradient clipping=auto, using the Trainer default max grad norm) bf16 Per-device Batch Size Gradient Accumulation Steps Global Batch Size Epochs Max Sequence Length Optimization Steps Backbone Trainable Modules Image Size GPUs Batching and Training Budget 2 64 = 512 1 (single pass; default for all experiments unless otherwise noted) 8192 (truncate from the end) = N/B for pool of rollouts Model, Input, and Hardware InternVL2.5-8B & Qwen2.5-VL-7B LLM + multimodal fusion MLP (vision backbone frozen) InternVL2.5-8B: 448, dynamic resolution enabled; max 6 patches. Qwen2.5-VL-7B: dynamic resizing (min/max=784/200704). 4 NVIDIA H100 80GB Model and Training Data We use InternVL2.5-8B (Chen et al., 2024c) as the default backbone, following prior MPRM work (Wang et al., 2025b; Du et al., 2025). In the main experiments, we additionally report results with second backbone, Qwen2.5-VL-7B (Bai et al., 2025). For both models, we freeze the vision encoder and fine-tune the language model together with the multimodal projector modules. We use the default vision setup and input preprocessing of each backbone; the corresponding model and input hyperparameters are summarized in Table 7. We train on VisualPRM400K-v1.11 (Wang et al., 2025b), choosing v1.1 because it exposes per-step MC scores, whereas the v1 release only provides binarized labels. This dataset was sampled from 38 different data sources. Detailed training-data statistics are reported in Table 8. MPRM Training Objective Each training example consists of the question, the associated images, and step-by-step solution, where every step is followed by special token <prm>. The tokenizer inserts this special token into the text stream and the data loader attaches the corresponding binary label to its position. During training, the model is supervised only on these <prm> positions: the logits at each placeholder are restricted to the two reward tokens (Yes, No) and optimized with two-way cross-entropy loss, so that the probability of Yes serves as the score for that step. Optimization and Implementation Table 7 summarizes the hyperparameters and implementation details shared by all experiments. The learning rate is linearly warmed up over the first 5% of optimization steps and then cosine-decayed to zero over the remaining steps. Under our default single-pass protocol (i.e., one epoch over the selected training pool), the total number of optimization steps is recomputed for each data regime as = N/B, where is the number of rollouts in the 1https://huggingface.co/datasets/OpenGVLab/VisualPRM400K-v1.1-Raw Training Data Efficiency in Multimodal Process Reward Models pool and is the global batch size. Training is implemented in PyTorch (Paszke et al., 2019) with HuggingFace Trainer from Transformers (Wolf et al., 2020) and DeepSpeed (Rasley et al., 2020) ZeRO-3 for memory efficiency. Evaluation Protocol For each VisualProcessBench (Wang et al., 2025b) instance, we concatenate the question with the provided step-by-step rationale and insert <prm> after every step, mirroring training. The model produces scalar score per step (the Yes probability at the corresponding placeholder). Given threshold τ , we classify steps with scores τ as positive and those < τ as negative, ignoring neutral labels. Following the benchmark protocol, we select single global threshold per model on held-out split by sweeping τ and maximizing the micro-averaged F1 across all sources; we then report the overall micro-F1 in the main text and provide per-source macro-F1 breakdowns in the appendix. Best-of-N Reranking Protocol We report best-of-N reranking results with = 16 for all benchmarks in Table 3. For each problem, we first sample 16 candidate step-by-step rollouts using InternVL2.5-8B with standard stochastic decoding (temperature = 0.7, top-p = 0.9, top-k = 30, and max new tokens = 2048). Each candidate is formatted as sequence of reasoning steps followed by final answer. To rerank candidates, we apply the MPRM to obtain scalar score at the step level. Given candidate rollout τ with reasoning steps and step scores {st}T t=1, we compute trajectory-level score by averaging over steps: We then select the candidate with the highest S(τ ) as the final prediction for evaluation. S(τ ) ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 st. 18 Training Data Efficiency in Multimodal Process Reward Models D. Extended Results for Random Sub-sampling (Section 2.2) Figure 4. Single-pass scaling with random sub-sampling (Random-ρ) on VisualProcessBench. The top-left panel reports Overall micro-F1; the remaining panels show macro-F1 on each source dataset. The dashed horizontal line marks the Full-Data (ρ = 100%) model. Figure 4 extends the single-pass scaling plot in Figure 2a by breaking down the Random-ρ behavior on VisualProcessBench by source. The top-left panel reproduces the Overall micro-F1 curve, while the remaining panels show macro-F1 on each benchmark. Across sources, performance rises sharply when ρ increases from 0 to small fraction (e.g., 5%), and then quickly saturates: further enlarging the random pool beyond the low two-digit range yields only mild additional gains. This per-source view mirrors the redundancy-dominated scaling discussed in Section 2.2. Figure 5 focuses on the ρ = 25% working point and compares Full-Data and Random-25% under matched update budget. The top-left panel shows Overall micro-F1, and the remaining panels report macro-F1 on each VisualProcessBench source. Across sources, the full-data run has systematic but moderate edge over the Random-25% subset, and the gap is smaller than one might expect after discarding 75% of the rollouts. This pattern is consistent with regime in which additional rollouts yield diminishing returns. 19 Training Data Efficiency in Multimodal Process Reward Models Figure 5. VisualProcessBench performance vs. training step when training on the Full-Data and Random-25% settings of VisualPRM400Kv1.1. The top-left panel shows the Overall micro-F1 aggregated over all sources, while the remaining panels show macro-F1 on each individual VisualProcessBench source. 20 Training Data Efficiency in Multimodal Process Reward Models E. Extended Results for 25% Subsets (Sections 2.3 and 5.2) Figure 6. Evaluation performance vs. training step on VisualProcessBench for four 25% subsets of VisualPRM400K-v1.1. The top-left panel shows overall micro-F1; the remaining panels show macro-F1 on each source dataset. All 25% subset models are trained for single pass over their respective training pools. Full-Data shows the best checkpoint from one-epoch Full-Data run (4 more optimization steps), shown only as reference. BIS-25% consistently outperforms other subsets on overall and on most individual sources across the training trajectory, not only at single checkpoint. Table 4 in the main text reports overall micro-F1 and per-source macro-F1 on VisualProcessBench under the same 25% rollout budget, comparing BIS-25% against three baselines: Mixed-25%, Reliable-25%, and Low-MC-25%. Here we complement Table 4 with the full training curves of these 25% subsets on VisualProcessBench, and additionally include Random-25% as standard sub-sampling baseline. Figure 6 plots overall micro-F1 and per-source macro-F1 as function of training step. These curves provide dynamic view of how BIS re-allocates the fixed update budget compared with Random-25%, Low-MC-25%, Mixed-25%, and Reliable-25%. Across sources, BIS-25% yields the highest or near-highest curve at almost all steps. Combined with the aggregate scores in Table 4, these extended results corroborate that BIS is strictly more effective than these 25% baselines under the same rollout and update budget. Training Data Efficiency in Multimodal Process Reward Models F. Theoretical Details F.1. Canonical Logistic Case for the Scaling Decomposition This section makes the decomposition in Eq. (4) precise in the logistic teacherstudent setting of Section 3.1, and derives the O(N 1/2 ) data term and O(T 1/2) optimization term step by step. eff Setup. Each training step is pair (ϕ, ) with ϕ Rd and {0, 1}. The population logistic loss is L(w) = E(ϕ,Y ) (cid:2)Y log qw(ϕ) (1 ) log(cid:0)1 qw(ϕ)(cid:1)(cid:3), qw(ϕ) = σ(w, ϕ), and denotes its minimizer. Let Neff denote the number of i.i.d. training steps after thinning the pool, and LNeff (w) ="
        },
        {
            "title": "1\nNeff",
            "content": "Neff(cid:88) i=1 ℓ(w; ϕi, Yi), ℓ(w; ϕ, ) = log qw(ϕ) (1 ) log(cid:0)1 qw(ϕ)(cid:1) be the empirical logistic loss. We write (cid:98)wNeff arg min wW LNeff (w) for an empirical minimizer, and wT for the SGD iterate after updates on this finite sample. Assumptions. We assume: (A1) (Bounded features) There exists > 0 such that ϕ2 almost surely. (A2) (Well-specified logistic teacher) There exists Rd such that Pr(Y = 1 ϕ) = σ(w, ϕ) almost surely. (A3) (Strong convexity and smoothness on bounded domain) Assume is µ-strongly convex and L-smooth on closed, convex, bounded set Rd containing w, with supwW w2 R. Such condition can be obtained, for example, by adding small ℓ2 penalty on w. (A4) (SGD with decaying steps) We run projected SGD so that wt for all t. Stochastic gradients are computed on with η0 small enough so that i.i.d. samples and have bounded second moment, and the step sizes satisfy ηt = η0/ ηt 1/L. Goal. We bound the excess population loss E(cid:2)L(wT )(cid:3) L(w) where the expectation is over both the draw of the training set and the randomness of SGD, and show that it decomposes into O(N 1/ ) data term and O(T 1/2) optimization term. eff Step 1: Decomposition into data and optimization terms. Insert and subtract (cid:98)wNeff : E(cid:2)L(wT )(cid:3) L(w) = E(cid:2)L(wT ) L( (cid:98)wNeff )(cid:3) + E(cid:2)L( (cid:98)wNeff ) L(w)(cid:3) =: Opt(T, Neff ) + Data(Neff ). (11) The first term measures optimization error after SGD updates on fixed finite sample; the second term measures the gap between the empirical and population optima due to finite data. Step 2: Bounding the finite-data term. By definition, Data(Neff ) = E(cid:2)L( (cid:98)wNeff ) L(w)(cid:3). Using the standard optimism of empirical risk minimization (ERM) argument, L( (cid:98)wNeff ) L(w) = (cid:0)L( (cid:98)wNeff ) LNeff ( (cid:98)wNeff )(cid:1) + (cid:0)LNeff ( (cid:98)wNeff ) LNeff (w)(cid:1) + (cid:0)LNeff (w) L(w)(cid:1) (cid:0)L( (cid:98)wNeff ) LNeff ( (cid:98)wNeff )(cid:1) + (cid:0)LNeff (w) L(w)(cid:1) (cid:12)L(w) LNeff (w)(cid:12) (cid:12) (cid:12). 2 sup wW Training Data Efficiency in Multimodal Process Reward Models"
        },
        {
            "title": "Taking expectations and applying uniform convergence then yields",
            "content": "(cid:20) Data(Neff ) 2 sup wW (cid:12)L(w) LNeff (w)(cid:12) (cid:12) (cid:12) (cid:21) . (12) Under (A1)(A3), the logistic loss ℓ(w; ϕ, ) is Lipschitz in on and the class {ℓ(w; , ) : W} has bounded Rademacher complexity. Standard uniform convergence bounds for Lipschitz losses in generalized linear models (ShalevShwartz & Ben-David, 2014) then imply the existence of constant Cdata > 0 such that (cid:20) sup wW (cid:12)L(w) LNeff (w)(cid:12) (cid:12) (cid:12) (cid:21) Cdata Neff . Combining (12) and (13) and absorbing the factor 2 into the constant gives Data(Neff ) Cdata Neff . (13) (14) Here Cdata depends on the feature bound and the domain radius (and thus grows with BR). Step 3: Bounding the optimization term. We now control the optimization error Opt(T, Neff ) = E[L(wT ) L( (cid:98)wNeff )]. Conditioned on the fixed sample {(ϕi, Yi)}Neff i=1 , let (w) := LNeff (w) = 1 Neff Neff(cid:88) i=1 ℓ(w; ϕi, Yi), (cid:98)wNeff arg min wW (w). We assume is µ-strongly convex and L-smooth on W. Moreover, we assume that the empirical minimizer (cid:98)wNeff lies in the interior of W, so that ( (cid:98)wNeff ) = 0. The stochastic gradients gt used by SGD satisfy E[gt wt] = (wt) and E[gt2 wt] G2 for some > 0. The SGD recursion on the empirical loss is wt+1 = ΠW (wt ηtgt) , ηt = η0t1/2, with η0 small enough so that ηt 1/L for all t. Define the mean squared distance to the empirical minimizer as Dt := E(cid:2)wt (cid:98)wNeff 2(cid:3). standard one-step expansion of wt+1 (cid:98)wNeff 2, combined with the µ-strong convexity and L-smoothness of and the bounded-variance assumption on gt, implies that the sequence (Dt) satisfies recursion of the form Dt+1 (cid:0)1 2µηt + 2L2η2 (cid:1) Dt + 2G2η2 , see Moulines & Bach (2011) for detailed derivation. Since ηt is non-increasing and wt for all t, and is bounded so that Dt diam(W)2, we may absorb the 2L2η2 Dt term and 2σ2 into single constant G2, yielding the simplified recursion Dt+1 (1 2µηt) Dt + η2 Specializing to the step-size schedule ηt = η0t1/2, (15) can be rewritten as G2. Dt+1 (cid:16) 1 (cid:17) Dt + , (15) (16) for some constants c, > 0 depending only on µ, η0, G. Lemma 1 (One-dimensional SGD recursion). Let (Dt)t1 be nonnegative sequence satisfying (16) for all 1, with c, > 0. Then there exists constant > 0, depending only on c, and D1, such that DT for all 1. 23 Training Data Efficiency in Multimodal Process Reward Models This lemma is direct corollary of standard results for stochastic approximation recursions; see, e.g., the standard meansquare error recursion and the corresponding non-asymptotic bound in Moulines & Bach (2011) (with α = 1/2) for an explicit derivation of the O(T 1/2) rate. Applying Lemma 1 to (16) yields DT = E(cid:2)wT (cid:98)wNeff 2(cid:3) . By L-smoothness of (see Lemma 1.2.3 in Nesterov et al. (2018)), we have for any (w) ( (cid:98)wNeff ) + (cid:10)F ( (cid:98)wNeff ), (cid:98)wNeff (cid:11) + 2 (cid:98)wNeff 2. Since (cid:98)wNeff is minimizer of , ( (cid:98)wNeff ) = 0, and thus (wT ) ( (cid:98)wNeff ) wT (cid:98)wNeff 2. Taking expectations and combining with (17) gives E(cid:2)LNeff (wT ) LNeff ( (cid:98)wNeff )(cid:3) Copt , (17) (18) for some constant Copt > 0 depending only on µ, L, and the initialization. In particular, higher Monte Carlo noise typically increases the second-moment bound G2, which increases Copt and can make the optimization term dominant in noise-limited regime. To relate this bound on the empirical loss to the population loss, we insert and subtract LNeff and decompose E(cid:2)L(wT ) L( (cid:98)wNeff )(cid:3) = E(cid:2)LNeff (wT ) LNeff ( (cid:98)wNeff )(cid:3) + E(cid:2)(L(wT ) LNeff (wT )) (L( (cid:98)wNeff ) LNeff ( (cid:98)wNeff ))(cid:3). (19) By the triangle inequality, (cid:12)E(cid:2)(L(wT ) LNeff (wT )) (L( (cid:98)wNeff ) LNeff ( (cid:98)wNeff ))(cid:3)(cid:12) (cid:12) (cid:12) 2 (cid:104) sup wW (cid:12)L(w) LNeff (w)(cid:12) (cid:12) (cid:12) (cid:105) . Applying the uniform-convergence bound (13) to (20) and combining with (18) and (19) yields Opt(T, Neff ) = E(cid:2)L(wT ) L( (cid:98)wNeff )(cid:3) Copt + data Neff , (20) (21) for some constant can be aggregated into single constant, leading to the overall decomposition in (4). data > 0. When we combine (21) with the finite-sample term in (14), the two O(N 1/2 eff ) contributions Step 4: Putting the pieces together. Substituting (14) and (21) into the decomposition (11) yields E(cid:2)L(wT )(cid:3) L(w) = Opt(T, Neff ) + Data(Neff ) Cdata Neff + Copt . These O(N 1/2 eff ) and O(T 1/2) rates are conservative but sufficient for the scaling decomposition used in Section 3.2. F.2. Step-wise Gradient Variance We derive the step-wise gradient variance expression used in Equation (5). Recall the teacherstudent setup in Section 3.1. For step with representation ϕ Rd and clean label {0, 1}, the logistic-loss gradient at parameter is g(ϕ, ; w) = (cid:0)qw(ϕ) (cid:1) ϕ, qw(ϕ) = σ(w, ϕ). At the teacher parameter w, write q(ϕ) := qw (ϕ), ϕ Bernoulli(cid:0)q(ϕ)(cid:1). 24 Training Data Efficiency in Multimodal Process Reward Models"
        },
        {
            "title": "Then",
            "content": "E(cid:2)Y ϕ(cid:3) = q(ϕ), E(cid:2)Y 2 ϕ(cid:3) = q(ϕ). The conditional mean of the gradient at is E(cid:2)g(ϕ, ; w) ϕ(cid:3) = (cid:0)q(ϕ) E[Y ϕ](cid:1) ϕ = 0, so the conditional second moment equals the conditional variance: E(cid:2)g(ϕ, ; w)2 ϕ(cid:3) = E(cid:2)(q(ϕ) )2 ϕ(cid:3) ϕ2 (cid:16) (cid:17) q2(ϕ) 2q(ϕ) E[Y ϕ] + E[Y 2 ϕ] ϕ2 = = (cid:0)q(ϕ) q2(ϕ)(cid:1) ϕ2 = q(ϕ)(cid:0)1 q(ϕ)(cid:1) ϕ2, which is exactly the expression in Equation (5). F.3. Symmetric Label Noise We now derive the noisy-gradient expression used in Equation (6). Fix ϕ Rd and write q(ϕ) = qw (ϕ). For brevity, set := q(ϕ) (0, 1). Let the clean label ϕ Bernoulli(q) be flipped independently with probability η [0, 1/2) to form noisy label . The noisy gradient at is g(ϕ, ; w) = (q ) ϕ. Conditioned on ϕ, we can express the distribution of the noisy label by conditioning on the clean label and applying the law of total probability: Pr( = 1 ϕ) = Pr( = 1, = 1 ϕ) + Pr( = 1, = 0 ϕ) = Pr( = 1 = 1, ϕ) Pr(Y = 1 ϕ) + Pr( = 1 = 0, ϕ) Pr(Y = 0 ϕ) = (1 η) + η (1 q) = q(1 2η) + η =: p1, where = q(ϕ) and ϕ Bernoulli(q). Thus Consequently, Pr( = 0 ϕ) = 1 p1 =: p0. = (cid:40) 1, q, = 1, = 0. and the conditional second moment of the noisy gradient is E(cid:2)g(ϕ, ; w)2 ϕ(cid:3) = E(cid:2)(q )2 ϕ(cid:3) ϕ2 = (cid:0)p1(q 1)2 + p0q2(cid:1) ϕ2. Substituting p1 = q(1 2η) + η and p0 = 1 p1 and expanding gives p1(q 1)2 + p0q2 = (1 4η) q(1 q) + η. Therefore Reinstating = q(ϕ) yields E(cid:2)g(ϕ, ; w)2 ϕ(cid:3) = (cid:16) (1 4η) q(1 q) + η (cid:17) ϕ2. E(cid:2)g(ϕ, ; w)2 ϕ(cid:3) = (cid:16) (1 4η) q(ϕ)(cid:0)1 q(ϕ)(cid:1) + η (cid:17) ϕ2, which is the form stated in Equation (6). Training Data Efficiency in Multimodal Process Reward Models F.4. Rollout-Level Mixture and Representation Norms We formalize two facts used in Section 3.3: (i) rollout-level label mixture is an approximately unbiased proxy for the latent teacher mixture, and (ii) under bounded representation norms, average q(1 q) and average q(1 q)ϕ2 differ only by constant factors. Label variance decomposition. Fix rollout with steps. For step let qj := independently conditioned on {qj}. Define x,j [0, 1] and Yj qj Bernoulli(qj), ˆp := 1 (cid:88) j=1 Yj, := 1 n (cid:88) j=1 qj, A(x) := 1 (cid:88) j=1 qj(1 qj). Lemma 2 (Label variance decomposition). Conditioned on {qj}, the empirical label variance satisfies Proof. We have E(cid:2)ˆp(1 ˆp) (cid:12) (cid:12) {qj}(cid:3) = q(1 q) 1 n2 (cid:88) j=1 qj(1 qj). ˆp = 1 n (cid:88) j=1 Yj, ˆp2 = 1 n2 (cid:88) j=1 2 + 2 n2 (cid:88) YjYk. 1j<kn Conditioned on {qj} the Yj are independent with E[Yj qj] = qj and E[Y 2 qj] = qj, so E[ˆp {qj}] = q, E[ˆp2 {qj}] = 1 n2 (cid:88) j=1 qj + 2 n2 (cid:88) qjqk. 1j<kn Using to eliminate the cross terms yields q2 = (cid:16) 1 (cid:88) j= (cid:17)2 qj = 1 n2 (cid:88) j= q2 + 2 n2 (cid:88) qjqk 1j<kn E[ˆp2 {qj}] = q2 + 1 n2 (cid:88) j=1 qj 1 n2 (cid:88) j=1 = q2 + q2 1 n2 (cid:88) j=1 qj(1 qj). Finally, as claimed. E(cid:2)ˆp(1 ˆp) (cid:12) (cid:12) {qj}(cid:3) = E[ˆp {qj}] E[ˆp2 {qj}] = q(1 q) 1 n2 (cid:88) j=1 qj(1 qj), Since (cid:55) t(1 t) is concave on [0, 1], Jensens inequality gives Using Lemma 2 and the identity we can rewrite A(x) = 1 (cid:88) j=1 qj(1 qj) q(1 q) =: θx. 1 n2 (cid:88) j=1 qj(1 qj) = 1 A(x), E(cid:2)ˆp(1 ˆp) (cid:12) (cid:12) {qj}(cid:3) = θx 1 A(x). 26 Training Data Efficiency in Multimodal Process Reward Models Since 0 A(x) θx, this immediately yields the sandwich bound (cid:16) θx 1 (cid:17) 1 E(cid:2)ˆp(1 ˆp) (cid:12) (cid:12) {qj}(cid:3) θx. Thus the conditional bias of ˆp(1 ˆp) as an estimator of the teacher-level mixture θx is at most θx/n 1/(4n), and ˆp(1 ˆp) is an approximately unbiased proxy for θx. In particular, rollouts with larger ˆp(1 ˆp) tend to have larger teacher-level mixture θx (in expectation, up to an O(1/n) bias). Since A(x) θx, larger θx simply provides more headroom for A(x) to be large, and therefore for the rollout to contain more informative steps. Symmetric flip noise and induced mixture. In Section 3.3 we also consider symmetric flip noise approximation: the observed label Yj is obtained by independently flipping the clean label Yj with probability η [0, 1/2). Let Bj Bernoulli(η) be independent of Yj and define Yj := Yj Bj. Conditioned on qj, we have qj := P( Yj = 1 qj) = P(Yj = 1, Bj = 0 qj) + P(Yj = 0, Bj = 1 qj) = (1 η)qj + η(1 qj) = (1 2η)qj + η. (22) Thus Yj qj Bernoulli(qj). Averaging (22) over steps gives = 1 (cid:88) j=1 qj = (1 2η)q + η. Defining θx := q(1 q) and θx := q(1 q), direct expansion yields θx = (cid:0)(1 2η)q + η(cid:1)(cid:16) 1 (1 2η)q η (cid:17) = (cid:0)(1 2η)q + η(cid:1)(cid:0)(1 η) (1 2η)q(cid:1) = (1 2η)2 q(1 q) + η(1 η) = (1 2η)2 θx + η(1 η). Closeness to the noise-free analysis for small η. Eq. (23) implies θx θx = (cid:0)(1 2η)2 1(cid:1)θx + η(1 η), so using 0 θx 1/4 we obtain the uniform bound θx θx 4η θx + η(1 η) 2η. (23) (24) Moreover, since Yx,j qx,j Bernoulli(qx,j) independently conditioned on {qx,j}, applying Lemma 2 with qx,j replaced by qx,j shows that ˆp,x(1 ˆp,x) estimates θx up to an additional O(1/n) bias. Therefore, when η is small (and is not too small), the mixture computed from observed labels is within O(η) + O(1/n) of the noise-free target in expectation, so the analysis in the noise-free setting applies up to small additive perturbation. Effect of bounded representation norms. Define the full average step-wise information Afull(x) := 1 (cid:88) j=1 qj(1 qj) ϕx,j2. Assume the representations are uniformly bounded: there exist constants 0 < cmin cmax < such that cmin ϕx,j2 cmax for all steps. Then cmin A(x) Afull(x) cmax A(x). Hence, up to global multiplicative constants, A(x) and Afull(x) measure the same notion of step-wise information. Qualitative comparisons between rollouts can therefore be phrased in terms of A(x). In particular, increasing A(x) increases corresponding lower bound on Afull(x), and A(x) serves as constant-factor proxy for Afull(x) under this boundedness assumption. 27 Training Data Efficiency in Multimodal Process Reward Models F.5. MC-induced Pseudo-positive Probability and Monotonicity Posterior and closed form. Let [0, 1] denote the one-shot success probability of step, and let Binomial(N, r) be the number of successful continuations. Fix τ (0, 1) and define τ -reliability by := I[r τ ]. We place Beta prior Beta(a, b) with density p(r) ra1(1 r)b1 for a, > 0. The Binomial likelihood is p(K = r) = (cid:19) (cid:18)N rk(1 r)N k. By Bayes rule, p(r = k) p(K = r) p(r) ra+k1(1 r)b+N k1, which is the density of Beta(a + k, + k). Writing αk := + and βk := + k, the normalized posterior density is fk(r) := p(r = k) = rαk1(1 r)βk1, (0, 1),"
        },
        {
            "title": "1\nB(αk, βk)",
            "content": "where B(α, β) = (cid:82) 1 noise level) 0 tα1(1 t)β1 dt is the Beta function. Define the conditional pseudo-positive probability (effective ηeff (k) := Pr(Z = 0 = k) = Pr(r < τ = k) = (cid:90) τ 0 fk(r) dr. Let B(τ ; α, β) := (cid:82) τ 0 tα1(1 t)β1 dt be the incomplete Beta function. Then ηeff (k) = B(τ ; αk, βk) B(αk, βk) =: Iτ (αk, βk), where Iτ (α, β) is the regularized incomplete beta function. Monotonicity. Lemma 3 (Monotonicity of ηeff ). For any a, > 0, 1, and τ (0, 1), the map (cid:55) ηeff (k) = Pr(r < τ = k) is strictly decreasing on {0, 1, . . . , }. Equivalently, Pr(r τ = k) = 1 ηeff (k) is strictly increasing in k. Proof. Let fk be the posterior density above with parameters αk = + and βk = + k, i.e., fk(r) = 1 B(αk, βk) rαk1(1 r)βk1, (0, 1). Fix {0, . . . , 1} and compare consecutive posteriors. direct calculation gives, for (0, 1), fk+1(r) fk(r) = = B(αk, βk) B(αk+1, βk+1) B(αk, βk) B(αk + 1, βk 1) 1 . rαk+1αk (1 r)βk+1βk Since 1 and > 0, we have βk = + + 1 > 1, so βk 1 > 0 and the Beta-function identity B(α + 1, β 1) = α β1 B(α, β) applies. Thus we obtain the explicit form fk+1(r) fk(r) = Ck 1 , Ck := βk 1 αk = + 1 + , which is strictly increasing in on (0, 1) since r/(1 r) is strictly increasing. Fix τ (0, 1) and define := fk+1(τ ) fk(τ ) = Ck τ 1τ . Because fk+1(r) fk(r) is strictly increasing in r, we have fk+1(r) fk(r) < for (0, τ ), fk+1(r) fk(r) > for (τ, 1), 28 Training Data Efficiency in Multimodal Process Reward Models and fk+1(τ ) fk(τ ) = at = τ . Multiplying by fk(r) and integrating yields (cid:90) τ 0 fk+1(r) dr (cid:90) τ 0 fk(r) dr, (cid:90) 1 τ fk+1(r) dr (cid:90) τ fk(r) dr. 0 fk(r) dr = Pr(r < τ = k). Since (cid:82) 1 0 fk = 1, the second inequality becomes 1 Ak+1 c(1 Ak). Let Ak := (cid:82) τ If 1, then the first inequality gives Ak+1 cAk Ak. If 1, then the second inequality implies 1 Ak+1 1 Ak, i.e. Ak+1 Ak. Thus in all cases Ak+1 Ak, proving monotonic non-increase. Finally, since the ratio fk+1(r) fk(r) > on (τ, 1) are fk(r) strict on sets of positive Lebesgue measure. Moreover, fk(r) > 0 for all (0, 1) when αk, βk > 0. Hence the integral inequalities above are strict, yielding Ak+1 < Ak for any τ (0, 1). Therefore ηeff (k) = Ak is strictly decreasing in on {0, 1, . . . , }. is strictly increasing in r, the inequalities fk+1(r) fk(r) < on (0, τ ) and fk+1(r) 29 Training Data Efficiency in Multimodal Process Reward Models G. Training Dynamics Figure 7,8 and 9 report the training dynamics of BIS-ρ and Random-ρ under keep ratios ρ {5, 10, 15, 25, 35, 50}%. For each ρ, we track both the overall micro-F1 and the per-source macro-F1 on VisualProcessBench over training steps, and include the full-data reference as horizontal dashed lines. Across ratios and backbones, BIS not only achieves stronger final performance, but also improves faster: it reaches high accuracy in substantially fewer steps and maintains clear advantages over random sub-sampling throughout training. This gap is most pronounced in the low-budget regime, where Random-ρ often learns slowly and remains far below the full-data reference, while BIS-ρ rapidly closes the gap and frequently approaches full-data performance early in training. Figure 7. Training dynamics on VisualProcessBench for ρ {5%, 10%}, comparing BIS-ρ and Random-ρ for both InternVL2.5-8B and Qwen2.5-VL-7B (overall micro-F1 and per-source macro-F1). 30 Training Data Efficiency in Multimodal Process Reward Models Figure 8. Training dynamics on VisualProcessBench for ρ {15%, 25%}, comparing BIS-ρ and Random-ρ for both InternVL2.5-8B and Qwen2.5-VL-7B (overall micro-F1 and per-source macro-F1). 31 Training Data Efficiency in Multimodal Process Reward Models Figure 9. Training dynamics on VisualProcessBench for ρ {35%, 50%}, comparing BIS-ρ and Random-ρ for both InternVL2.5-8B and Qwen2.5-VL-7B (overall micro-F1 and per-source macro-F1). 32 Training Data Efficiency in Multimodal Process Reward Models H. Per-source BIS Histograms Interpreting low-BIS sources via step-level MC difficulty. Figure 10 reports, for each of the 38 source subsets, the BIS score distribution over all rollouts and the per-source top-25% rollouts selected by BIS. The per-source shapes vary substantially, and interpreting them requires care: because BIS(x) = (ppos(x)(1ppos(x))+α) R(x),rollouts that are nearly pure (almost all-positive or almost all-negative) receive small mixture weight ppos(1 ppos) and thus tend to concentrate at the lower end of the BIS range, even though these two purity regimes correspond to opposite difficulty profiles. To disambiguate them, we pair the histograms with the step-level average MC score for each source  (Table 8)  , where averages MC scores over all annotated steps and serves as proxy for how often the MC solver can reach correct completion from intermediate states. Under this lens, sources whose full-pool histogram is concentrated near low BIS scores split into two qualitatively different categories: High-s sources (easy for the current MC solver). When is high, low BIS scores mainly reflect trajectories that are dominated by positive steps (ppos 1), hence little label mixture. This is exemplified by ScienceQA (s = 0.9723) and NLVR2 (s = 0.9672), as well as other high-s sources such as DVQA (0.9453). In these sources, naturally mixed trajectories are rarer, so the available high-BIS tail is intrinsically thinner. Low-s sources (hard for the current MC solver). When is low, low BIS scores instead reflect trajectories with few positive steps under MC sampling (ppos 0). Representative examples include GeoQA+ (Open) (s = 0.5720) and GeomVerse (0.6819). Here the bottleneck is the scarcity of reliable positive anchors: even when errors are plentiful, many steps rarely lead to successful continuations, so the pool contains fewer rollouts that simultaneously provide informative negatives and trustworthy positives. Between these extremes, medium-s sources tend to provide the richest substrate for BIS, because they naturally generate rollouts with non-trivial label mixture while still retaining meaningful fraction of reasonably reliable positives. For example, FigureQA (s = 0.7615) and VQAv2 (s = 0.8253) exhibit visibly heavier mass at moderate-to-high BIS scores, indicating larger supply of mixed-but-reliable trajectories that BIS can exploit. Across all sources, BIS selection produces consistent within-source reallocation: the orange histograms shift weight toward higher scores relative to the blue ones, meaning that under the same per-source budget (top-25% per file) BIS preferentially keeps the most mixed and reliable rollouts available in that source. This provides distributional explanation for the main experimental trend in Table 2: compared with Random-25%, which spends budget proportionally to the original pool, and Low-MC-25%, which can over-focus on unreliable pseudo-positives, BIS-25% systematically extracts the highest-information tail when such tail exists (e.g., medium-s sources), while avoiding wasting budget on uninformative near-pure trajectories in very easy sources or on noise-dominated trajectories in very hard sources. As result, BIS allocates the fixed update budget to subset with higher effective signal per update, aligning with its consistent gains over Random / Mixed / Low-MC under the same rollout and optimization budgets. Consistent selection patterns in mixture and reliability. Figure 11 and 12 further reports, for each source, the distributions of the reliability term R(x) and the mixture term ppos(x)(1 ppos(x)) for all rollouts and the selected top-25% subset, together with the coverage curve (Selected / All). Across sources, BIS follows two stable behaviors: it strongly suppresses low-R(x) rollouts, and it consistently favors higher mixture by shifting coverage toward moderate-to-high ppos(1 ppos). Importantly, the preference over R(x) is not monotonic: coverage often peaks at moderate R(x) and then saturates or drops as R(x) 1, since near-pure trajectories can have large R(x) but still receive small mixture weight and thus low BIS scores. The plots also highlight substantial source-level heterogeneity in the available mixed-and-reliable tail, complementing the step-level MC difficulty analysis in Table 8. 33 Training Data Efficiency in Multimodal Process Reward Models Table 8. Per-source step-level MC score statistics on VisualPRM400K-v1.1. Avg. MC is the mean MC score over all annotated steps; Avg./16 equals 16Avg. MC (expected successes out of = 16 continuations). Step/Rollout counts follow the same validity filtering as in our histogram analysis. Subset Avg. MC AI2D (Kembhavi et al., 2016) ChartQA (Masry et al., 2022) CLEVR (Johnson et al., 2017) COCO-ReM (Y/N) (Singh et al., 2024) DocVQA (Mathew et al., 2021) DVQA (Kafle et al., 2018) FigureQA (Kahou et al., 2017) Geo170K (Gao et al., 2025) Geometry3K (Lu et al., 2021a) Geometry3K (Open) (Lu et al., 2021a) GeomVerse (Kazemi et al., 2024) GeoQA+ (Open) (Cao & Xiao, 2022) GeoQA+ (MC) (Cao & Xiao, 2022) GEOS (Seo et al., 2015) GEOS (Open) (Seo et al., 2015) IconQA (Lu et al., 2021b) InfographicVQA (Mathew et al., 2022) KonIQ-10k (Hosu et al., 2020) M3CoT (Chen et al., 2024b) MAPQA-SUV (Chang et al., 2022) MathV360K (Shi et al., 2024) MAVIS-Abs (Zhang et al., 2025c) MAVIS-Cos (Zhang et al., 2025c) MAVIS-Log (Zhang et al., 2025c) MAVIS-Poly (Zhang et al., 2025c) MAVIS-Sin (Zhang et al., 2025c) MAVIS-Tan (Zhang et al., 2025c) MAVIS-Geo-0 (Zhang et al., 2025c) MAVIS-Geo-1 (Zhang et al., 2025c) MAVIS-Geo-2 (Zhang et al., 2025c) MAVIS-Geo-3 (Zhang et al., 2025c) NLVR2 (Suhr et al., 2019) ScienceQA (Lu et al., 2022) SROIE (Huang et al., 2019) Super-CLEVR (Int) (Li et al., 2023) Super-CLEVR (Y/N) (Li et al., 2023) UniGeo (Chen et al., 2022) VQAv2 (Goyal et al., 2017) 0.9409 0. 0.9106 0.8933 0.8663 0.9453 0.7615 0. 0.9243 0.8707 0.6819 0.5720 0.8486 0. 0.8217 0.8908 0.7645 0.8822 0.8811 0.7260 0. 0.8104 0.8057 0.7357 0.8836 0.8231 0. 0.9280 0.8765 0.7990 0.7719 0.9672 0. 0.8708 0.7325 0.7713 0.6015 0.8253 Avg./16 15.0539 14.5245 14.5689 14.2922 13. 15.1255 12.1843 14.4373 14.7880 13.9318 10. 9.1514 13.5772 14.4970 13.1478 14.2536 12.2323 14. 14.0971 11.6155 14.0142 12.9670 12.8916 11. 14.1375 13.1701 12.8428 14.8473 14.0243 12. 12.3508 15.4753 15.5574 13.9325 11.7205 12. 9.6239 13.2043 # Steps 146,419 136,655 109, 55,628 85,589 177,352 57,601 61,314 94, 58,569 38,156 45,287 67,715 16,847 1,563 121, 78,408 72,430 115,649 48,368 107,503 166, 86,393 162,713 206,293 88,130 60,391 78, 37,760 25,851 19,080 96,072 186,894 83, 37,279 33,171 41,523 67,166 # Rollouts 28, 28,049 24,004 18,450 21,049 35,367 12, 8,205 11,756 7,090 7,765 7,025 9, 1,827 178 25,811 17,996 13,337 19,476 12, 20,132 16,530 9,174 15,422 23,461 9, 6,180 13,149 6,728 4,801 3,706 26, 34,523 18,074 12,070 10,440 6,299 18, Training Data Efficiency in Multimodal Process Reward Models Figure 10. Per-source BIS score distributions on VisualPRM400K-v1.1. Each panel corresponds to one source dataset; light purple bars show all rollouts from that source and dark purple bars show the per-file top-25% rollouts selected by BIS. Across sources, the full-pool distributions differ markedlywith some datasets exhibiting sharp spikes near the baseline α = 0.05 and others showing heavier high-score tailsbut BIS consistently suppresses low-score mass and enriches higher-score regions within each source. 35 Training Data Efficiency in Multimodal Process Reward Models Figure 11. Per-source component histograms on VisualPRM400K-v1.1 for the reliability term R(x) and mixture term ppos(x)(1ppos(x)), comparing all rollouts and BIS top-25%. Black curve: coverage (Selected / All). Training Data Efficiency in Multimodal Process Reward Models Figure 12. Per-source component histograms on VisualPRM400K-v1.1 for the reliability term R(x) and mixture term ppos(x)(1ppos(x)), comparing all rollouts and BIS top-25%. Black curve: coverage (Selected / All). 37 Training Data Efficiency in Multimodal Process Reward Models Case Study 1 Question. The graph of the function (x) = + is shown, where the condition that is non-zero is satisfied. The function (x) takes the values 10 and 6 at = 5 and = 3, respectively. Given the graph and the aforementioned conditions, identify the zeros of the function. Ground truth. Step scores (hard label yj = I[scorej > 0]). Step Content Step-0 Step-1 Step-2 StepStep-4 Step-5 Step-6 Step-7 Step-8 Step-9 (1) (2) To determine the zeros of the function (x) = ax + b, we need to follow these steps: 1. Identify the equation of the line: - Given that (x) = 10 at = 5 and (x) = 6 at = 3, we can use these points to find the slope and the y-intercept b. - Substitute = 5, = 10 into the equation: 10 = a(5) + - Substitute = 3, = 6 into the equation: 6 = a(3) + 2. Solve the system of equations: - From equation (1): 10 = 5a + - From equation (2): 6 = 3a + 3. Simplify the equations: - Equation (1) becomes: = 10 + 5a - Equation (2) becomes: 6 = 3a + 4. Substitute values from equation (1) into equation (2): - Substitute = 10 + 5a into 6 = 3a + b: 6 = 3a + (10 + 5a) - Simplify: 6 = 3a 10 + 5a - Combine like terms: 6 = 2a 10 - Solve for a: 6 = 2a = = 3 5. Find using the value of a: - Substitute = 3 back into = 10 + 5a: = 10 + 5(3) = 10 + 15 = 5 6. Write the final equation of the function: - The equation of the function is (x) = 3x + 5. 7. Setting (x) = 0 to find the zeros: - 0 = 3x + 5 - Solve for x: 3x = 5 = = Thus, the zeros of the function (x) = 3x + 5 are = 5 3 . Final answer: 5 3 5 3 BIS computation (α = 0.05). Valid step scores are the 10 entries above. Hard labels: yj = 1 iff scorej > 0. Score 0.875 0. yj 1 1 0.9375 0.875 0.0 0. 0.0 0.0 0.0 0.0 1 1 0 0 0 0 0 = 10, npos = 4, 4 10 0.875 + 0.875 + 0.9375 + 0.875 4 ppos = = = 0.4, = 0.890625, BIS(x) = (cid:0)ppos(1 ppos) + 0.05(cid:1) = (0.4 0.6 + 0.05) 0.890625 = 0.25828125. Figure 13. Case study (MAVIS-Function (Zhang et al., 2025c)): step-level MC scores, hard labels, and BIS computation for one rollout. 38 Training Data Efficiency in Multimodal Process Reward Models Case Study 2 Question. Which part of the diagram best depicts mineralisation? Please answer the question based on the options mentioned before. Ground truth. Step scores (hard label yj = I[scorej > 0]). Step Content Step-0 Step-1 StepStep-3 Step-4 Step-5 Step-6 Step-7 To determine which part of the diagram depicts mineralization, lets break down the relevant concepts. 1. Mineralization: It refers to the process by which organic matter is converted into minerals. This often involves bacteria breaking down organic materials and releasing inorganic salts such as nitrates and phosphates back into the soil, accessible for plant uptake. 2. Flowchart Analysis: - Identify the main components in the diagram that are involved with mineralization. - Typically, mineralization follows the decomposition pathway, so look for arrows leading to outputs that could be soil minerals. 3. Options: - D: This point may involve decomposers like bacteria and fungi that convert organic matter to minerals. - F: This appears to be involved in detritus or initial organic matter feed. - M: This might be part of the nutrient cycle but may not directly depict mineralization. - A: This point seems more related to primary producers or initial growth stage. 4. Evaluation of Options: - shows arrows indicating transformation from organic to inorganic content, implying the breakdown and conversion into minerals. - F, M, do not visibly indicate this mineralization process. ### Answer: Based on these analyses, is the most consistent signifier of the mineralization process in this diagram. Therefore, the answer is: Final answer: Score yj 0.5625 0.6875 1 0.6875 1 0.0625 1 0. 0 0.0 0.0 0.0 0 0 BIS computation (α = 0.05). Valid step scores are the 8 entries above. Hard labels: yj = 1 iff scorej > 0. = 8, npos = 4, ppos = 4 = 0.5, = 0.5625 + 0.6875 + 0.6875 + 0.0625 4 = 0.5, BIS(x) = (cid:0)ppos(1 ppos) + 0.05(cid:1) = (0.5 0.5 + 0.05) 0.5 = 0.15. Figure 14. Case study (AI2D (Kembhavi et al., 2016)): step-level MC scores, hard labels, and BIS computation for one rollout. 39 Training Data Efficiency in Multimodal Process Reward Models Case Study 3 Question. Angle ABC is equivalent to π/2. Side DE extending into an equilateral triangle shape beyond the rectangle. EFG forms sector. Side HI continues into an equilateral triangle inside the rectangle. AB is equal to 72, Angle ACB measures 60, GI measures 71. What is the total surface area of the shape GFHJI? A. 954* B. 3816* C. 70 D. 1272* Ground truth. 3 3/ 3 Step scores (hard label yj = I[scorej > 0]). Step Content StepStep-2 Step-3 Step-4 Step-5 Step-6 StepStep-8 Step-9 To find the total surface area of the complex shape GFHJI, lets break it down step by step. 1. Identify the components of the shape GFHJI: - Rectangle GFHI - Equilateral triangle HIJ - Segments GH and HI, which are parts of the equilateral triangle - The sector EFG 2. Calculate the area of rectangle GFHI: - Side GF = 71 (given) - Determine the width (HI) of the rectangle: - HI is part of the equilateral triangle HIJ. - The side of the equilateral triangle is GH, which is the same as CI (71 units). - The height of the 3 2 71 = 60.98. equilateral triangle HIJ can be calculated as 3. Calculate the area of the equilateral triangle HIJ: - Area = 4 712 = 865.48. 4. Calculate the area of the sector EFG: - The radius EF = FG = 71 units. - Angle GFE is not given, so assume its part of the sector attached to the rectangle. - Calculate the area of the sector θ 5. Combine the areas: - Rectangle GFHI: 71 60.98 4326.68 - Equilateral triangle HIJ: 865.48 - Sector EFG: Assume π simplicity) 6. Summarize all areas and subtract the overlapping areas: - Total area = Area of rectangle + Area of equilateral triangle + Area of sector. Given calculations involve approximations based on assumed values. 6 712 = 2716.23 (half of the sector for 360 π r2. 3 Answer: (Based on approximations and calculated components.) Score yj 0. 0.0625 0.0625 0.0625 0.0 0.0625 0. 0.0 0.0 1 1 1 0 1 1 0 0 BIS computation (α = 0.05). Valid step scores are the 9 entries above. Hard labels: yj = 1 iff scorej > 0. = 9, = npos = 6, 6 9 0.0625 + 0.0625 + 0.0625 + 0.0625 + 0.0625 + 0.0625 6 ppos = 2 = , = 0.0625, BIS(x) = (cid:0)ppos(1 ppos) + 0.05(cid:1) 1 3 (cid:18) 2 3 + 0. = (cid:19) 0.0625 = (0.222222 . . . + 0.05) 0.0625 0.0170138889. Figure 15. Case study (MAVIS-Function (Zhang et al., 2025c)): step-level MC scores, hard labels, and BIS computation. Training Data Efficiency in Multimodal Process Reward Models I. Case Studies Figures 13, 14 and 15 illustrate why BIS is reasonable rollout selection rule when step labels are obtained by thresholding MC scores. Figure 13 is representative high-quality rollout. It contains clear step-level variation and meaningful mix of positive and negative signals, while the positive steps receive consistently high MC scores. As result, BIS assigns relatively large score, matching our motivation that the most informative rollouts should be both mixed and reliable. Figure14 is more borderline case. Although the rollout still contains mixed signals, some positive steps have low scores and the reasoning later collapses to an incorrect final choice, leading to only moderate R(x). BIS therefore assigns moderate score, reflecting that mixture alone is not sufficient if the positive supervision is not reliable. Finally, Figure 15 shows more diagnostic failure mode. Several steps contain clear, checkable geometry/number mistakes (e.g., mis-identifying given lengths or applying invalid equalities), yet their MC scores remain non-zero but very small (often at the 1/16 level). Under hard labeling, these low-but-nonzero scores are still binarized to yj=1, creating unreliable pseudo-positive supervision. This illustrates that non-zero low MC scores do not necessarily imply correctness, and can introduce noisy supervision during training. BIS mitigates this issue via the reliability term, which down-weights such low-confidence rollouts and prevents them from dominating the selected subset."
        }
    ],
    "affiliations": [
        "Singapore University of Technology and Design",
        "University of Virginia",
        "Washington University in St. Louis"
    ]
}