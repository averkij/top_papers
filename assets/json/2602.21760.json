{
    "paper_title": "Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling",
    "authors": [
        "Euisoo Jung",
        "Byunghyun Kim",
        "Hyunjin Kim",
        "Seonghye Cho",
        "Jae-Gil Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves $2.31\\times$ and $2.07\\times$ latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff."
        },
        {
            "title": "Start",
            "content": "Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling"
        },
        {
            "title": "Byunghyun Kim",
            "content": "Hyunjin Kim School of Computing, KAIST {jyssys, rooknpown, hjkim1228, orangingq, jaegil}@kaist.ac.kr"
        },
        {
            "title": "Seonghye Cho",
            "content": "Jae-Gil Lee* 6 2 0 2 5 2 ] . [ 1 0 6 7 1 2 . 2 0 6 2 : r Figure 1. Summary of the proposed hybrid data-pipeline parallelism. Our method consistently outperforms prior distributed approaches across five key aspects: Speed-up, Image Quality, Generality, High-resolution Synthesis, and Communication Cost, demonstrating robust and balanced acceleration-quality trade-offs."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose hybrid parallelism framework that combines novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves 2.31 and 2.07 latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX 3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff. indicates corresponding author. Diffusion models have emerged as powerful family of generative models because of their superior sample quality and broad applicability. However, the inherently iterative nature of diffusion processes, which consists of many denoising steps, leads to significant inference latency and computational bottlenecks. As model sizes conthese inefficiencies become increasingly tinue to scale, limiting, making diffusion inference acceleration pressing research challenge. Existing approaches have focused mainly on reducing the number of sampling steps [9, 19 21, 26, 27, 34, 36, 37], designing optimal architectures [11, 13, 14, 35, 38, 39], or leveraging mathematical approximations [1, 17, 18, 22, 28, 40, 40]. Yet, these methods often require additional training or fail to deliver strong acceleration in practice, exhibiting clear trade-off between generation quality and speed. Distributed parallelism across multiple GPUs offers promising alternative. Using modern parallel computing resources, one can achieve substantial throughput improvements in diffusion inference without additional training. This direction is especially appealing given the success of distributed strategies in natural language processing, where large-scale language models have already benefited from extensive parallelism research [24, 29]. As in other domains, distributed parallelism for generative model inference can be broadly classified into data parallelism Figure 2. Comparison of parallel strategies for diffusion inference. (a) Patch-based data parallel frameworks suffer from bottlenecks caused by all-gather operations and artifacts at patch boundaries, leading to limited acceleration and quality degradation. (b) Pipeline parallel frameworks incur excessive asynchronous communication overhead and accumulate estimate errors. (c) Our hybrid parallelism, which incorporates condition-based data parallelism, adaptively combines both paradigms to achieve high fidelity and fast generation. and pipeline parallelism [2, 12]. Both approaches enhance throughput by distributing either the input data or the model itself across multiple GPUs. Representative existing studies include DistriFusion [12] for data parallelism and AsyncDiff [2] for pipeline parallelism. In DistriFusion (Figure 2a), an input image is divided into disjoint patches, and these patches are processed in parallel across GPUs, where each device independently handles one patch. In AsyncDiff (Figure 2b), the entire model is divided into sequential components, where each component is assigned to GPU, and the output from the i-th GPU is asynchronously fed as the input to the (i + 1)-th GPU; thus, AsyncDiff enables pipelined execution across devices. In theory, each form of parallelism can improve throughput linearly with respect to the number of GPUs, up to an ideal speed-up, but in practice, the gains are often sublinear due to communication overhead and synchronization costs. In this paper, we propose hybrid strategy that combines data and model parallelism to further increase the throughput of generative model inference, achieving beyond-linear scaling relative to the number of GPUs, while maintaining generation quality. That is, if there are two GPUs, we aim to obtain more than twofold speed-up withIn practice, out noticeable degradation in output fidelity. when using two GPUs, data and model parallelism achieved 1.2 and 1.3speed-up, respectively, whereas our hybrid approach remarkably achieved 2.3speed-up under the same configuration, as shown in Figures 1 and 2. To achieve hybrid parallelism, one could combine the aforementioned representative methods. Specifically, an image is divided into disjoint patches, and each patch is fed into corresponding model component (not necessarily the first one). As result, each GPU trains 1/N portion of the model using 1/N portion of an input image. This hybrid approach can potentially achieve beyond-linear scaling; however, it may degrade generation quality for two main reasons. First, since each GPU processes only portion of the image, artifacts are likely to appear particularly along patch boundaries. Second, this issue is exacerbated by asynchronous communication between model components; that is, errors introduced by asynchronous rather than sequential denoising can worsen the artifacts. In this paper, we aim to propose and further optimize the hybrid parallelism for diffusion inference from two complementary perspectives: (1) from the data parallelism perspective, transitioning from patch-based partitioning to condition-based partitioning; and (2) from the model parallelism perspective, advancing from static parallelism switching to adaptive parallelism switching. (1) Condition-Based Partitioning. The main limitation of patch-based partitioning is that each patch represents only local subregion of an image, often leading to boundary artifacts and degraded visual coherence. To address this limitation, we leverage the classifier-free guidance (CFG) [7], technique widely adopted in diffusion models, where the model simultaneously predicts conditional (prompted) and unconditional (unprompted) noise estimates. This dualpath prediction naturally provides meaningful criterion for data partitioning: as shown in Figure 2c, the conditioned (xt, c) and unconditioned (xt) inputs form two distinct dataparallel paths. Importantly, unlike patch-based partitioning, each image partition covers the entire image, thereby preserving global consistency. Consequently, condition-based partitioning yields improved visual coherence and reduced communication overhead during feature aggregation. (2) Adaptive Parallelism Switching. Because we revise the data partitioning strategy, the pipeline parallelism must also be adapted to align with it. In the early denoising steps, the conditional and unconditional noise estimates differ substantially due to the presence or absence of the condition. Consequently, asynchronous denoising at this stage can lead to divergence between the two paths. To mitigate this issue, we defer the onset of parallel execution until the noise estimates of the two paths become sufficiently similar, beyond the conventional warm-up phase used in prior works (e.g., [2]). Similarly, toward the final denoising steps, the noise estimates from the two paths begin to diverge again; at this point, parallel execution is terminated. The specific switching points between serial and parallel execution are determined automatically based on novel metric, called the denoising discrepancy, which quantifies the difference between the two noise estimates. This adaptive switching mechanism effectively improves generation quality by reducing error propagation, while only marginally shortening the duration of parallel processing. This novel framework demonstrates consistent acceleration not only on conventional denoising diffusion models but also on recent state-of-the-art generative frameworks such as flow matching [16]. As long as the model follows sequential denoising process that allows quantifying the relative influence between conditional and unconditional branches, our framework remains robust and effective. Furthermore, due to the nature of pipeline parallelism, it is not restricted to specific architectures such as U-Net or DiT, showing strong generality across diverse networks. As summarized in Figure 1, our proposed hybrid parallelism achieves superior performance across the five key aspects. In fact, compared to single-GPU inference, our method achieves 2.3speed-up with two GPUs (i.e., > 2), while preserving generation fidelity. See Appendix and Section 5 for details of Figure 1. Finally, the key contributions are summarized as follows. Hybrid Parallelism Framework for Diffusion Inference. We introduce novel diffusion inference parallelism framework that integrates condition-based partitioning and adaptive parallelism switching into unified hybrid parallelism design. Novel Condition-Based Partitioning. At the data parallelism level, we exploit the intrinsic mechanism of diffusion by decoupling conditional and unconditional branches and performing multi-GPU denoising. Adaptive Parallelism Switching. To align pipeline parallelism with the behavior of conditional guidance, our method adaptively switches to hybrid parallelism framework during inference. Switching points are automatically determined based on the denoising discrepancy between conditional and unconditional estimates, ensuring generation efficiency throughout the denoising process. Robustness across Models and Architectures. Our framework consistently demonstrates strong acceleration and generation quality across various architectures (e.g., U-Net, DiT) and recent state-of-the-art generative frameworks, such as flow matching, even under high-resolution synthesis settings. 2. Related Work Single-GPU Diffusion Acceleration. Research on the acceleration of single-device diffusion inference can be classified into three categories. The first group focuses on reducing the number of sampling steps required for high-quality generation [9, 1921, 26, 27, 30, 34, 36, 37, 40]. These approaches enable fast sampling by either reformulating the reverse process as an ordinary differential equation (ODE), distilling multi-step models into fewer steps, or directly predicting the reverse process in latent space. The second group targets model architecture optimization, aiming to reduce computational cost through network compression and efficient design [11, 13, 14, 35, 38, 39]. The third group leverages mathematical and algorithmic strategies, either exploiting the mathematical structure of diffusion processes or reusing intermediate computations to further accelerate inference [1, 17, 18, 22, 28, 33, 40]. While these methods reduce single-device inference time, they are inherently limited by the computational capacity of individual GPUs. Multi-GPU Diffusion Acceleration. Recent studies have explored various distributed parallelism strategies to accelerate diffusion inference using multiple GPUs [2, 4, 5, 12, 32]. DistriFusion [12] introduces data-parallel approach that divides the input image into independent patches, performing denoising in parallel across GPUs. This work has established foundational paradigm for parallel diffusion inference. Building on this parallelization idea, AsyncDiff,[2] introduces model parallelism by dividing the U-Net into layer-wise segments and employing strideFigure 3. Overview of the proposed diffusion inference hybrid parallel framework. Our method adaptively switches parallelism modes at τ1 and τ2, optimizing the trade-off between computational efficiency and consistency of conditional guidance, and demonstrates superior inference acceleration performance while preserving high generation quality. based scheduling strategy to balance parallel execution, achieving notable reduction in latency. Subsequently, PipeFusion [5] and XDiT [4] combine patch-level parallelism with transformer-oriented parallelism through ring attention. While additional adaptations such as CFG-based data parallelism have been introduced, these methods remain limited to inter-image processing and lack deeper architectural integration. Moreover, transformer-specific schemes such as ring attention exhibit limited scalability and inconsistent performance when applied to general diffusion architectures. More recently, ParaStep [32] proposes reuse-then-predict mechanism that leverages the similarity of noise predictions between adjacent denoising steps. By reusing the noise from previous steps before re-prediction, ParaStep enables inter-step parallelization and significantly reduces communication overhead. However, because early and late diffusion steps exhibit larger discrepancies between adjacent noise states, the reuse mechanism can accumulate errors, leading to potential degradation in image quality or restricted speedup. 3. Preliminaries noise ϵθ(xt, t, c) and its unconditional variant ϵθ(xt, t, ). At inference, the samples follow ϵcfg = ϵθ(xt, c, t) + w(cid:0)ϵθ(xt, c, t) ϵθ(xt, t)(cid:1), where > 0 is the guidance scale. The adjusted reverse mean becomes µcfg(xt, t, y) = (cid:16) 1 αt xt βt 1 αt (cid:17) . ϵcfg Flow Matching. Given target distribution q(x) and base distribution p0(x), flow matching defines an ordinary differential equation, dx(t) dt = v(x(t), t), where the vector field vθ is learned by minimizing LFM = Et,x0q (cid:13) (cid:13) (cid:13)vθ (cid:0)xt, t(cid:1) xt x0 (cid:13) 2 (cid:13) (cid:13) , with xt = x0 + te for (0, I). Sampling proceeds by integrating = vθ(x, t) from = 1 to = 0. Denoising Diffusion Model. Let q(x0) denote the data distribution and define forward noising process by 4. Method 4.1. Overview q(xt xt1) = (cid:0)xt; (cid:112)1 βt xt1, βtI(cid:1), for = 1, . . . , , with variance schedule {βt}. The model learns parameterized reverse denoising process, pθ(xt1 xt) = (cid:0)xt1; µθ(xt, t), Σθ(t)(cid:1), by optimizing the variational lower bound, LVLB = Eq (cid:104) (cid:88) t=1 (cid:0)q(xt1 xt, x0) pθ(xt1 xt)(cid:1)(cid:105) ."
        },
        {
            "title": "DKL",
            "content": "Classifier-Free Guidance (CFG). For conditional generation with condition c, the model is trained to predict the Figure 3 illustrates the overall process of our proposed hybrid parallelism framework. The input isotropic noise latent xT is fed simultaneously into two denoising branches: the unconditional path fθ(xt, t) and the conditional path fθ(xt, c, t) guided by textual prompt c. where fθ denotes the denoising diffusion network parameterized by θ (e.g. U-Net, DiT). To exploit both global consistency and conditional fidelity, our framework incorporates two complementary dimensions of parallelism, condition-based partitioning, and adaptive parallelism switching. Formally, given the denoising model fθ, the diffusion inference across devices can be expressed as x(n) t1 = fθ(n)(x(n) {1, . . . , }, , c(bn), t), bn {cond, uncond}, conditional and unconditional branches at each timestep t, (where ϵc = ϵθ(xt, c, t) and ϵu = ϵθ(xt, t)), and is formulated as rel-MAEt(ϵc, ϵu) = Ex,ϵ[ϵθ(xt, c, t) ϵθ(xt, t)1] Ex,ϵ[ϵθ(xt, t)1] . (1) Here, ϵθ(xt, c, t) and ϵθ(xt, t) denote the noise components predicted from the conditional and unconditional denoisers, respectively. larger value indicates stronger discrepancy between the two branches, reflecting higher conditional influence on the denoising trajectory at that timestep. According to the trend of denoising discrepancy shown in Figure 4, which exhibits U-shaped curve over the entire denoising process, we divide the process into three stages: the Warm-Up Stage [T, τ1], the Parallelism Stage (τ1, τ2), and the Fully Connecting Stage [τ2, 0]. The two parameters, τ1 and τ2, define the boundaries between these stages and are automatically determined during the middle of the denoising process. The details of how they are determined are provided in the next section. Intuitively, τ1 marks the point where the denoising discrepancy ceases to decrease rapidly, while τ2 indicates the point where it begins to increase. By measuring denoising discrepancy across 5,000 prompts from the MS-COCO 2014 validation set [15], we observed that the variation of the error between the conditional and unconditional branches exhibits clear U-shaped trend, as further demonstrated in Appendix B. We now describe each denoising stage in detail. (1) Warm-Up Stage [T, τ1]. This stage captures the global outline of the generated image. The conditional branch establishes the overall composition from the text prompt, while the unconditional branch stabilizes the coarse structural forms. Since both branches have distinct influences, the denoising discrepancy remains low. Therefore, each branch is processed independently using condition-based partitioning, without adaptive parallelism switching. (2) Parallelism Stage (τ1, τ2). At this phase, the model refines local details within the preformed outline. The conditional and unconditional branches begin to converge, and the denoising discrepancy remains small and stable. To take advantage of this convergence, adaptive parallelism switching is activated, enabling more powerful acceleration of the denoising process. (3) Fully-Connecting Stage [τ2, 0]. In the final phase, fine-grained conditional cues dominate generation. The framework reverts to condition-based partitioning, integrating conditional guidance to reconstruct the final image x0. similar three stages structure has also been observed in previous studies on diffusion conditional guidance [10], which further supports the validity of our framework. Building upon this, through this three stages hybrid parallelism framework, our method achieves efficient distributed denoising while preserving generation quality. Illustration of the rel-MAEt(ϵc, ϵu) curve. The Figure 4. rel-MAEt(ϵc, ϵu) value is relatively large before τ1 and after τ2, while it converges near zero between them, indicating stable alignment between conditional and unconditional branches during the parallelism phase. where each θ(n) corresponds to the subset of model parameters assigned to the n-th device in the pipeline, reflecting adaptive parallelism switching across different network stages. Meanwhile, bn {cond, uncond} indicates whether the device handles the conditional or unconditional branch in condition-based partitioning. Accordingly, each device processes either conditional input with or an unconditional input without c. This formulation jointly represents both condition-based partitioning and adaptive parallelism switching within unified diffusion framework. To further enhance performance, the denoising process is divided into three stages according to the temporal dynamics of conditional influence: (1) Warm-Up Stage, where only ordinal communication occurs between conditional and unconditional branches; (2) Parallelism Stage, where both branches are executed in parallel with conditional exchange; and (3) Fully-Connecting Stage, which merges the two branches for the final refinement. The rationale for this three-phase division and the quantitative criteria for determining the boundary points τ1 and τ2 are discussed in Section 4.2 and Section 4.3, respectively. 4.2. Hybrid Parallel Inference Framework Figure 4 illustrates the relative-Mean Absolute Error of the predicted noise (relative-MAE; rel-MAEt(ϵc, ϵu)) across the three stages of the proposed hybrid parallelism in the denoising diffusion model. To determine when the conditional and unconditional branches should interact or remain independent, we first quantify their denoising discrepancy during the denoising process. Since conditional and unconditional denoisers contribute differently to generation, with one emphasizing semantic alignment to the text condition and the other stabilizing global structure, it is essential to measure how their noises ϵc and ϵu diverge over time. This discrepancy serves as key indicator for determining the switching points between serial and parallel execution within our hybrid framework. The denoising discrepancy, namely rel-MAEt(ϵc, ϵu), quantifies the difference in noise prediction ϵt between the The denoising discrepancy, rel-MAEt(ϵc, ϵu) can be extended to flow matching models by replacing ϵθ with the predicted velocity vθ. In this case, rel-MAEt(vc, vu) maintains the same role in quantifying the conditionalunconditional discrepancy over the velocity field. 4.3. Adaptive Switching via Denoising Discrepancy The core of the proposed hybrid parallelism is to dynamically determine the timesteps τ1 and τ2 during the realtime denoising process, sequentially switching between the Warm-Up, Parallelism, and Fully-Connecting modes, while constructing scheduling method based on the previously defined denoising discrepancy. (1) Determining τ1. For each timestep t, we compute denoising discrepancy and calculate the average slope of the most recent steps by Gt = Mt MtL . (2) We then select τ 1 = min{t 0 Gt < gslope} and constrain it by safety-cap τcap. As shown in Appendix B, τcap is defined as the global minimum point of the denoising discrepancy curve, and it serves as an upper bound for τ1 during automatic selection. The introduction of τcap ensures stability by covering cases where τ1 is assigned too late or remains undefined due to outlier behaviors, thus maintaining generation quality while maximizing acceleration. Consequently, τ1 is given by τ1 = min(τ 1, τcap), which marks the end of the warm-up stage where conditional influence stabilizes. (2) Determining τ2. During the parallelism phase, ϵc and ϵu converge to an identical value, making the denoising discrepancy measurement no longer meaningful. Therefore, τ2 is empirically fixed to certain number of steps after τ1, τ2 = τ1 + k, N, 1 < τ1. (3) larger extends the parallelism phase, resulting in faster inference but lower generation quality, while smaller improves fidelity at the cost of latency. detailed analysis of quality and speed trade-offs with respect to is presented in Section 5.4, where we empirically verify the optimal balance across various k. We also provide the algorithm of Section 4.3 in Appendix describes the overall process. 4.4. Theoretical Analysis of Adaptive Switching Analysis of Denoising Discrepancy by Score Decomposition. The denoising discrepancy can be theoretically interpreted as ratio between the conditional information strength and the unconditional data prior. From the scoredecomposition perspective [8, 31], can be approximated as rel-MAEt(ϵc, ϵu) = ϵcϵu1 ϵu1 (4) xt log p(cxt) information strength and su(xt, t) denotes the unconditional score of the conditional xt log p(cxt)1 su(xt,t) represents . the data distribution. Consequently, denoising discrepancy measures the relative magnitude between conditional and unconditional components. In the score formulation of Eq. (4), the unconditional score su(xt, t) = xt log p(xt) captures the intrinsic structure of the data distribution, while the conditional gradient xt log p(cxt) encodes the semantic influence of the conditioning signal c. Their relative magnitudes evolve naturally along the diffusion process: Warm-Up Stage: When xt is close to pure noise, su(xt, t) carries little structural information, whereas xt log p(cxt) dominates by guiding the global semantic layout from the prompt, leading to large denoising discrepancy. Parallelism Stage: As denoising progresses, su(xt, t) reconstructs meaningful local structures and becomes comparable in magnitude to xt log p(cxt). This balance satisfies su(xt, t) xt log p(cxt), yielding dt rel-MAEt(ϵc, ϵu) 0 and motivates the activation of the parallel inference phase. Fully-Connecting Stage: At high SNR, most patterns have been recovered by su(xt, t), while xt log p(cxt) contributes to fine-grained alignment and texture refinement, causing mild increase in denoising discrepancy. This interpretation provides an intuitive explanation of how the relative magnitudes of the conditional and unconditional scores evolve across timesteps, theoretically supporting the three stages proposed (Warm-Up Parallelism Fully Connecting). Detailed derivations of Eq. (4) and the robustness analysis of τ1 under stochastic denoising noise are shown in Appendix and Appendix E, respectively. 4.5. Extensibility to Many GPU Configurations While the hybrid parallelism framework is optimized for two GPUs, it also scales well to larger even-numbered configurations. We present two extension strategies. (1) Batch-Level Extension. In this approach, the model generates 2 samples across GPUs, where each pair of GPUs produces one image. This structure linearly increases acceleration with the number of GPUs while maintaining near-identical generation quality. However, it is most effective when large number of samples are generated. (2) Layer-Wise Pipeline Extension. This method extends the adaptive parallelism switching mechanism by dividing the optimal pipeline interval into layer-wise segments, thereby enabling finer-grained parallel execution across multiple devices. Unlike the batch-level scheme, it can be applied to single-sample generation, though it may incur slightly reduced acceleration efficiency and minor quality degradation due to finer partitioning. The structures and details of both strategies are provided in Appendix F. Supporting degree of parallelism greater than two for single image is deferred to future work. Base Model Devices Methods Latency (s) Speed-Up Comm. (GB) Stable Diffusion XL Stable Diffusion 3 2 1 2 Original Model DistriFusion [12] 16. 13.53 AsyncDiff [2] (stride=1) 12.54 Ours (k=5) Original Model 7.12 19. AsyncDiff [2] (stride=1) 9.82 xDiT-Ring [4] 14.31 Parastep [32] Ours (k=5) 9.98 9.33 - 1.22 1.31 2.31 - 1.97 1.35 1.94 2.07 FID LPIPS PSNR w/ G. T. w/ Orig. w/ G. T. w/ Orig. w/ G. T. w/ Orig. 23.977 - 0. - 9.618 - 24.164 4.864 0. 0.146 9.597 24.634 23.941 4.103 0. 0.108 9.586 26.387 23.831 4.100 33. - 0.796 0.810 0.107 - 9. 8.086 26.640 - 33.379 2.032 0. 0.052 8.155 27.812 - 0.525 9. 0.516 - 1.290 121.646 33.356 1. 0.809 0.047 8.085 27.857 0.032 0. 33.340 3.350 0.810 0.112 8.091 22. 33.322 1.878 0.780 0.046 8.229 27. Table 1. Quantitative comparison of parallelism methods on the Stable Diffusion XL and Stable Diffusion 3 models. We compare our method with existing distributed inference techniques under 1and 2-GPU. We report both the baseline latency and the corresponding acceleration ratio (Speed-Up), Communication efficiency (Comm.), and quantitative metrics assessing generation fidelity. Here, w/ G.T. denotes comparison with the ground-truth image, and w/ Orig. indicates comparison with the original (single-GPU) model output. 5. Experiments 5.1. Experimental Setup Models. We evaluate our proposed hybrid parallelism framework on two representative diffusion backbones: Stable Diffusion XL (SDXL) [23] and Stable Diffusion 3.0 (SD3), DiT-based flow matching model [3]. SDXL represents U-Netbased latent diffusion models [25], while SD3 reflects the transformer-based paradigm, demonstrating the generality of our approach. Datasets. All experiments are conducted on the MSCOCO Captions 2014 benchmark [15], using 5,000 validation prompts for text-to-image generation. Generated images are compared against both the ground-truth samples and the single-GPU original model outputs. Metrics. We evaluate inference efficiency and generative quality. Latency and speed-up ratio measure acceleration. For quality, we report FID (Frechet Inception Distance) [6], LPIPS (Learned Perceptual Image Patch Similarity) [41], and PSNR (Peak Signal-to-Noise Ratio). Lower FID/LPIPS and higher PSNR indicate better generation quality. For implementation details, please refer to Appendix G. 5.2. Main Results Quantitative Results. Table 1 reports quantitative comparison across SDXL and SD3 pre-train diffusion models. On SDXL, our method achieves 2.31 acceleration over the single-GPU baseline while slightly improving image fidelity. Compared to prior distributed inference methods such as DistriFusion,[12] and AsyncDiff,[2], our proposed method attains the best speedquality trade-off with minimal communication overhead. Notably, our communication cost is reduced by 19.6 compared to AsyncDiff, due to adaptive parallelism switching that dynamically determines optimal parallel intervals to minimize communication cost."
        },
        {
            "title": "Methods",
            "content": "Latency (s) Speed-Up FID (w/ Orig.) Original Model Full Condition-Based Partitioning Ours (Hybrid Parallelism) 16.49 9.24 7.12 - 1.78 2.31 - 3.623 4. Table 2. Ablation on hybrid parallel components. All experiments are conducted on the SDXL model at 10241024 resolution, comparing the single-GPU baseline, full condition-based partitioning, and our hybrid parallelism framework. For SD3, DiT-based flow-matching model, our approach not only surpasses earlier distributed frameworks such as DistriFusion and AsyncDiff, but also consistently outperforms more recent baselines, xDiT-Ring and Parastep. It achieves 2.07 speed-up with negligible communication cost while maintaining comparable or superior generation quality. These results emphasize our methods strong generality across both U-Net and DiT architectures, achieving generation efficiency. Qualitative Results. Figure 5 presents qualitative comparisons among distributed inference methods. While DistriFusion and AsyncDiff exhibit boundary artifacts or spatial inconsistency, our method preserves global coherence and fine-grained details similar to the original model. These results confirm that the proposed hybrid parallelism framework maintains high visual fidelity while achieving substantial acceleration. Further results are shown in Appendix I. 5.3. Ablation Study Ablation on Hybrid Parallel Components. Table 2 analyzes the contribution of each hybrid parallel component. We compare three settings: (1) the original singleGPU model, (2) full condition-based partitioning applied to all denoising steps, and (3) our proposed hybrid parallelism combining both condition-based partitioning and adaptive parallelism switching. Condition-based partitionFigure 5. Qualitative results of the main experiments. We compare 10241024 image generations from the SDXL model. Our method achieves the best acceleration and FID performance, while producing visuals most similar to the original. Figure 6. Visualization of speedquality trade-off across different parallelism intervals k. Smaller values preserve higher fidelity, whereas larger achieve greater acceleration. Our method consistently dominates prior works across the trade-off frontier. All experiments were conducted on 2 GPUs. ing achieves 1.78 speed-up while maintaining image quality, whereas our hybrid parallelism further improves efficiency to 2.31 with comparable quality. This demonstrates that the addition of the pipeline component maximizes generation acceleration while minimizing quality degradation. Consequently, the proposed framework effectively integrates the advantages of condition-based partitioning and adaptive parallelism switching. 5.4. Sensitivity Analysis Impact of Different Values. As shown in Figure 6, the parallelism interval clearly reveals speedquality tradeoff: smaller values preserve higher fidelity, while larger values yield faster generation. An appropriate balance is observed at k=5, achieving both strong quality and acceleration. Moreover, the interval can be flexibly chosen by practitioners to adjust the trade-off between efficiency Figure 7. Comparison of high-resolutions tasks. We compare different parallel inference methods on the SDXL model using NVIDIA H200 GPUs across 10241024, 20482048, and 25602560 high-resolutions. and fidelity. Quantitative results are summarized in Appendix H, and qualitative comparisons across different values are provided in Appendix J. High-Resolution Generation. As shown in Figure 7, our method consistently achieves superior acceleration over existing distributed inference frameworks across increasing resolutions. On the SDXL model using NVIDIA H200 GPUs, our hybrid parallelism attains up to 2.72 speed-up at 10241024, 1.54 speed-up at 20482048, and 1.62 speed-up at 25602560, demonstrating strong scalability for high-resolution image generation. 6. Conclusion In this paper, we introduced hybrid parallelism framework for diffusion inference that integrates condition-based partitioning with adaptive parallelism switching. Guided by the denoising discrepancy criterion, the method adaptively switches between parallelism modes to minimize redundant communication. It achieves 2.31 and 2.07 latency reductions on SDXL and SD3, respectively, while preserving fidelity. We also generalize across U-Net and DiT architectures, providing unified parallelism paradigm for scalable multi-GPU diffusion inference."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analyticdpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. 1, 3 [2] Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong Tan, and Xinchao Wang. Asyncdiff: Parallelizing diffusion models by asynchronous denoising. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 37:9517095197, 2024. 2, 3, 7 [3] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024. 7 [4] Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transarXiv preprint formers (dits) with massive parallelism. arXiv:2411.01738, 2024. 3, 4, 7 [5] Jiarui Fang, Jinzhe Pan, Jiannan Wang, Aoyu Li, and Pipefusion: Patch-level pipeline parallelism arXiv preprint Xibo Sun. for diffusion transformers inference. arXiv:2405.14430, 2024. 3, [6] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 30, 2017. 7 [7] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep Generative Models and Downstream Applications, 2021. 3, 2 [8] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 35:2656526577, 2022. 6, 2 [9] Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021. 1, 3 [10] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 37:122458 122483, 2024. 5 [11] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu. Efficient spatially sparse inference for conditional gans and diffusion models. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 35:2885828873, 2022. 1, [12] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 71837193, 2024. 2, 3, 7 [13] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. In Proceedings Q-diffusion: Quantizing diffusion models. of the IEEE International Conference on Computer Vision (ICCV), pages 1753517545, 2023. 1, 3 [14] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 36:20662 20678, 2023. 1, 3 [15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV), pages 740755. Springer, 2014. 5, 7, 1 [16] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative In Proceedings of the International Conference modeling. on Learning Representations (ICLR), 2023. 3 [17] Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, JuanManuel Perez-Rua, and Jurgen Schmidhuber. Faster diffusion via temporal attention decomposition. arXiv preprint arXiv:2404.02747, 2024. 1, [18] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. 1, 3 [19] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 35:57755787, 2022. 1, 3 [20] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pages 122, 2025. [21] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 1, 3 [22] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: In Proceedings of Accelerating diffusion models for free. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1576215772, 2024. 1, 3 [23] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: In Proceedings of els for high-resolution image synthesis. the International Conference on Learning Representations (ICLR), 2024. 7 [24] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters."
        },
        {
            "title": "Efficient diffusion model",
            "content": "[37] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: image superresolution by residual shifting. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 36:1329413307, 2023. 1, 3 for [38] Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu. Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models. arXiv preprint arXiv:2404.11098, 2024. 1, 3 [39] Jiale Zhang, Yulun Zhang, Jinjin Gu, Jiahua Dong, Linghe Kong, and Xiaokang Yang. Xformer: Hybrid x-shaped transformer for image denoising. Proceedings of the International Conference on Learning Representations (ICLR), 2024. 1, 3 [40] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. 1, 3 [41] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In Proceedings of deep features as perceptual metric. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 586595, 2018. 7 In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 35053506, 2020. [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 7 [26] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. Proceedings of the International Conference on Learning Representations (ICLR), 2022. 1, 3 [27] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 87103. Springer, 2024. 1, 3 [28] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 36:42634276, 2023. 1, 3 [29] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatronlm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. 1 [30] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 3, [31] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 6, 2 [32] Kunyun Wang, Bohan Li, Kai Yu, Minyi Guo, and Jieru Zhao. Communication-efficient diffusion denoising parallelization via reuse-then-predict mechanism. arXiv preprint arXiv:2505.14741, 2025. 3, 4, 7 [33] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 62116220, 2024. 3 [34] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. Proceedings of the International Conference on Learning Representations (ICLR), 2022. 1, 3 [35] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2255222562, 2023. 1, 3 [36] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 66136623, 2024. 1, 3 Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling"
        },
        {
            "title": "Methods",
            "content": "Speed-Up"
        },
        {
            "title": "Image\nQuality",
            "content": "Model General. High-res Synth. Comm. Efficiency"
        },
        {
            "title": "Distrifusion\nAsyncDiff\nOurs",
            "content": "2.5 3.0 4.7 3.5 4.5 4.5 2.5 5.0 5.0 3.3 3.5 4.4 5.0 1.0 5.0 Table 3. Quantitative metrics comparison across five evaluation aspects. Scores are normalized to 5-point scale. Higher values ( ) indicate better performance. A. Evaluation of Hybrid Parallelism Evaluation Protocol. All scores are computed based on 5-point scale unified minmax scaling scheme, where the normalized values are re-centered around an average score of 3. Specifically, each metric is assessed as follows: Speed-Up. We measure the relative acceleration ratio with respect to the SDXL baseline latency in Table 1. The measured latencies are 13.53secs for DistriFusion, 12.54secs for AsyncDiff, and 7.12secs for our method. Image Quality. We evaluate image quality using FID scores reported in Table 1 from the main results of SDXL. The reported FID values are 4.864 for DistriFusion, 4.103 for AsyncDiff, and 4.100 for our method. Model Generality. We assign scores based on architecture compatibility. Each model receives 2.5 points for supporting U-Net and an additional 2.5 points for DiT support, resulting in scores of 2.5 for DistriFusion, 5 for AsyncDiff, and 5 for Ours. High-resolution Synthesis. The score reflects both highresolution generation capability and inference latency. According to the results in Section 5.4 High-Resolution Generation, all three methods successfully generate three target resolutions. The corresponding average latencies are 14.73secs for DistriFusion, 14.27secs for AsyncDiff, and 11.99secs for Ours. Communication Efficiency. We evaluate the communication efficiency based on the measured inter-GPU data transfer communication volume in the SDXL multi-GPU setting reported in Table 1 from the main results. The measured communication volumes are 0.525 GB for DistriFusion, 9.830 GB for AsyncDiff, and 0.516 GB for our method. Figure 8. Empirical visualization of denoising discrepancy curve. B. Empirical Visualization of Denoising"
        },
        {
            "title": "Discrepancy",
            "content": "Figure 8 illustrates the average denoising discrepancy (rel-MAEt(ϵc, ϵu)) value measured during the denoising process based on 5,000 prompts from the MS-COCO The shaded region repre2014 [15] validation set. sents the 2σ range, and the denoising model used is Stable Diffusion XL. The red dot denotes τcap = rel-MAEt(ϵc, ϵu), which is employed as safetyargmin cap in the main method. C. Adaptive Parallelism Switching Algorithm Algorithm 1 Adaptive Parallelism Switching via Denoising Discrepancy Require: latent noise xt, prompt c, steps , window L, slope threshold g, safety-cap τcap, interval 1: τ1, τ2 2: for = T, 1, . . . , 1 do 3: ϵc, ϵu ϵθ(xt, c, t), ϵθ(xt, t) Ex,ϵϵc ϵu1 Ex,ϵϵu1 Mt rel-MAEt(ϵc, ϵu) 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: Gt = MtMtL if τ1 = and > and 0 Gt < then τ1 min(t, τcap); τ2 τ1 + Denoise: if τ1 then WARM-UP else if > τ2 then PARALLELISM else FULLY-CONNECTING end if xt1 STEP DENOISE(xt, ϵc, ϵu, t) 16: 17: end for 18: return x0, (τ1, τ2) from the expected slope E[Gt], and a, correspond to the minimum and maximum possible range of the observed rel-MAEt(ϵc, ϵu) values, typically normalized within [0, 1]. As increases, the variance of the estimated slope decreases, and the probability of false detection decreases exponentially. showing that larger exponentially reduces false-alarm probability. Empirically, and gslope, which are also established in our experiments, lie within stable regime due to strong autocorrelation of rel-MAEt(ϵc, ϵu) sequences. Thus, τ1 can be reliably detected as the earliest timestep satisfying 0 Gt < gslope and τcap. F. Extensibility to Many GPU Configurations"
        },
        {
            "title": "Structures",
            "content": "Figure 9 presents two extensibility structures that scale the proposed hybrid parallelism framework from the baseline 2 GPUs setup to many GPU configurations. The first structure, shown in Figure 9a, demonstrates the batch-level extension under an GPUs configuration. In this scheme, each pair of GPUs collaboratively denoises single sample while following the three stages hybrid parallelism framework. As result, the system can generate N/2 samples concurrently with GPUs, enabling near-linear throughput scaling when multiple samples are produced. The second structure, shown in Figure 9b, demonstrates the layer-wise pipeline extension on 4 GPUs configuration. Here, the denoising network is partitioned into multiple layer-wise segments distributed across devices, allowing the hybrid parallelism strategy to be applied to singlesample generation. While this configuration may exhibit slightly reduced acceleration efficiency and minor quality degradation compared to the batch-level extension, it proImvides fine-grained pipeline scheduling mechanism. portantly, the same structural principles naturally generalize beyond the 4 GPUs example to arbitrary GPUs configurations, demonstrating the flexibility and scalability of the proposed framework. D. Derivation of Score-Based Interpretation of"
        },
        {
            "title": "Denoising Discrepancy",
            "content": "The denoising discrepancy(rel-MAEt(ϵc, ϵu)) criterion in Eq. (4) can be theoretically derived from the score deFollowing the ϵcomposition of diffusion models. parameterization of score-based generative modeling [8, 31], the preconditioned score can be expressed as sθ(xt, t) ϵθ(xt, t) σt , (5) where σt denotes the noise standard deviation at timestep t. According to Bayes rule, the conditional score function can be decomposed as sc(xt, t) = su(xt, t) + xt log p(cxt), (6) where su(xt, t) is the unconditional data score, and xt log p(cxt) denotes the conditional information flow [7]. Substituting Eq. ((5)) into Eq. ((6)) yields ϵc(xt, t) ϵu(xt, t) σt xt log p(cxt), (7) which implies that the difference between conditional and unconditional denoiser outputs corresponds to the conditional gradient scaled by σt. Therefore, the rel-MAE at each timestep can be approximated as rel-MAEt = ϵc ϵu1 ϵu1 xt log p(cxt)1 su(xt, t)1 . (8) This formulation reveals that rel-MAEt(ϵc, ϵu) quantifies the relative magnitude between the conditional information and the unconditional data priorforming the theoretical basis for the main method equation (Eq. (4)). E. Robustness of Determine τ1 under"
        },
        {
            "title": "Stochastic Denoising Noise",
            "content": "Diffusion inference is stochastic denoising process; predicted noises ϵθ(xt) are subject to random sampling. Consequently, the observed {Mt} fluctuates slightly, and Gt 0 may appear prematurely. To ensure robust detection, we define finite-difference slope by Gt = Mt MtL , (9) which smooths out stochastic perturbations across timesteps. The stability of Gt can be theoretically justified by Hoeffdings inequality: (cid:16) Pr(Gt E[Gt] > δ) 2 exp 2Lδ2 (b a)2 (cid:17) . (10) Here, denotes the window length used to compute the moving-average slope, δ represents the allowable deviation (a) Batch-level extension under GPUs configuration. (b) Layer-wise pipeline extension on 4 GPUs configuration. Figure 9. Extensibility to many GPU configurations structures. This figure illustrates two strategies for scaling the proposed hybrid parallelism framework to larger GPU configurations. These structures demonstrate how the proposed framework naturally generalizes from the 2 GPUs setting to both batch-level and layer-wise many GPU configurations. Parallelism Interval Latency (s) Speed-Up k=5 k=10 k=20 k= 7.12 6.89 6.44 5.94 2.31 2.39 2.56 2.78 FID (w/ Orig.) 4.100 5.942 7.966 9.191 Table 4. Effect of speed-quality trade-off across different parallelism intervals k. All experiments are conducted on the SDXL model at 10241024 resolution with various parallelism intervals. G. Implementation Details All experiments adopt the DDIM scheduler [30] with = 50 timesteps and generate images at resolution of 1024 1024. Experiments are performed on NVIDIA GeForce 3090 GPUs (24GB each), connected via PCIe Gen3. The adaptive switching parameters are set as follows: for SDXL, we use = 12, gslope = 0.4 103, = 5, and τcap = 15; for SD3, we set = 15, gslope = 0.1 103, = 5, and τcap = 40. H. Quantitative Results on the Parallelism"
        },
        {
            "title": "Interval k",
            "content": "Table 4 summarizes the numerical values corresponding to the speedquality trade-off illustrated in Figure 6. As described in the Section 5.4, smaller parallelism interval preserve higher fidelity, whereas larger values yield powerful acceleration. The table provides concrete measurements that reflect this trade-off, confirming the same trend observed in the pareto frontier visualization. I. Additional Qualitative Results Figure 10. Additional qualitative results of the main experiments. We compare 10241024 image generations from the SDXL model. Our method achieves the best acceleration and FID performance, while producing visuals most similar to the original. J. Qualitative Comparion Results via Different Figure 11. Additional qualitative comparisons across different values. We compare 10241024 image generations from the SDXL model across various parallelism intervals. Smaller values preserve higher visual fidelity, whereas larger gradually reduce local detail due to the extended parallelism window. Although the overall appearance remains similar, fine-grained conditional attributes become subtly blurred as increases."
        }
    ],
    "affiliations": [
        "School of Computing, KAIST"
    ]
}