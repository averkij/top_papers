{
    "paper_title": "PRiSM: Benchmarking Phone Realization in Speech Models",
    "authors": [
        "Shikhar Bharadwaj",
        "Chin-Jou Li",
        "Yoonjae Kim",
        "Kwanghee Choi",
        "Eunjung Yeo",
        "Ryan Soh-Eun Shim",
        "Hanyu Zhou",
        "Brendon Boldt",
        "Karen Rosero Jacome",
        "Kalvin Chang",
        "Darsh Agrawal",
        "Keer Xu",
        "Chao-Han Huck Yang",
        "Jian Zhu",
        "Shinji Watanabe",
        "David R. Mortensen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism."
        },
        {
            "title": "Start",
            "content": "PRiSM: Benchmarking Phone Realization in Speech Models Shikhar Bharadwaj*1 Chin-Jou Li1 Yoonjae Kim2 Kwanghee Choi3 Eunjung Yeo3 Ryan Soh-Eun Shim4 Hanyu Zhou1 Brendon Boldt1 Karen Rosero Jacome1 Kalvin Chang5 Darsh Agrawal1 Keer Xu1 Chao-Han Huck Yang6 Jian Zhu7 Shinji Watanabe1 David R. Mortensen1 1CMU 2Gwangju Institute of Science and Technology 3UT Austin 4LMU Munich 5UC Berkeley 6NVIDIA 7UBC {sbharad2,chinjoul,dmortens}@andrew.cmu.edu, rladbswo12@gm.gist.ac.kr 6 2 0 2 0 2 ] . [ 1 6 4 0 4 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability1."
        },
        {
            "title": "Introduction",
            "content": "Phone recognition (PR) entails transcribing speech into phonetic units that capture the physical realization of sounds, independent of languagespecific phonological constraints. By preserving acoustic nuances often abstracted away by wordor phoneme-level models2, PR provides robust foundation for cross-lingual speech processing (Li et al., 2022; Yusuyin et al., 2025) and downstream applications in clinical (Shriberg et al., 2025; Choi et al., 2025) and educational settings (Tu et al., 2018; Inceoglu et al., 2023). PR models have scaled substantially to cover diverse linguistic settings (see 2.1), yet existing evaluations remain difficult to compare across *Equal contribution. 1https://github.com/changelinglab/prism 2For example, tell may be transcribed as [thEë] in Mainstream American English and [thEl] in Scottish English, while the phonemic form of tell is consistently /tEl/. 1 Figure 1: PRiSM is the first open-source benchmark for phone recognition systems, covering intrinsic and extrinsic evaluations, i.e., transcription task and downstream task performance. studies. For example, models often differ in language coverage and phone inventories (Zhu et al., 2025), and evaluation metrics are not standardized (Li et al., 2025). common response has been to fix metric (Taguchi et al., 2023; Li et al., 2025) and expand the number of test datasets to mitigate bias (Zhu et al., 2025). Yet this approach scales poorly due to the scarcity of phonetically transcribed data. Moreover, transcription error rates do not necessarily reflect models phonetic capabilities or practical utility. Error rates in PR are inherently noisier than in ASR, as phones, unlike lexical units, correspond to lower-level, articulatorily defined abstraction of the acoustic signal. Furthermore, the link between transcription accuracy and downstream performance is often assumed rather than empirically proven. In practice, models leverage phonetic information via two channels: explicit transcriptions and latent internal representations. The latter are especially potent, as they encode rich phonetic cues (see 2.2). Consequently, metrics based solely on transcription error fail to capture the full utility and nuanced quality of these representations. Therefore, we propose PRiSM to fairly benchmark Phone Realization in Speech Models. PRiSM assesses PR systems3 intrinsically through transcription error, and extrinsically through utility in clinical, educational, and multilingual speech tasks using generated transcriptions and hidden representations. PRiSM applies to PR systems ranging from specialized PR models to general speech-to-text (S2T) systems, including Large Audio Language Models (LALMs), which are increasingly used for general speech tasks despite limited evaluation of their phonetic abilities (Peng et al., 2024; Arora et al., 2025). PRiSM is the first open-source benchmark for PR systems, for which code, evaluation recipes, and datasets are released, where licensing permits. With its reproducible and expandable framework, PRiSM supports researchers in understanding model behavior and training strategies, and helps practitioners make informed model choices. We evaluate broad range of PR systems and find that: (i) language exposure matters: seen languages benefit from familiar patterns, unseen from multilingual training; (ii) data and architecture shape performance: broad, diverse coverage improves results, while encoder-CTC architectures are more stable; (iii) LALMs lag behind specialized PR models. Ultimately, our goal is to establish common evaluation basis to drive progress toward PR systems that capture robust and generalizable phonetic information across resource conditions."
        },
        {
            "title": "2.1 Phone Recognition Systems",
            "content": "PR can be viewed as variant of the S2T task that maps speech to phonetic symbols such as IPA (International Phonetic Association, 1999). In this work, we use PR system to refer broadly to any system capable of converting speech into IPA in language-agnostic fashion. Modern PR systems are typically fine-tuned from ASR systems (Baevski et al., 2020; Radford et al., 2023) or trained from scratch on ASR datasets (Zhu et al., 2024) with transcriptions automatically converted to IPA using grapheme-tophoneme (G2P) tools (Mortensen et al., 2018; Zhu et al., 2022). Language-specific approaches (Li et al., 2020; Gao et al., 2021) rely on phoneme inventories, while language-agnostic approaches, which we focus on, seek to learn phonetic representations generalized across languages. LALMs 3Any pipeline that converts speech into phonetic units. have recently become prominent in speech tasks and have shown competitive performance with cascaded systems that combine LLMs with speech processing modules (Yang et al., 2025), motivating interest in their application to PR (Huang et al., 2025; Wang et al., 2025). We describe the systems investigated in this work in 4."
        },
        {
            "title": "2.2 Phonetic information in PR systems",
            "content": "Explicitly generated phonetic transcriptions are easy for humans to inspect and utilize. For example, faithful phonetic transcriptions of the speech of child with speech sound disorder can help clinician understand the nature of the disorder and design interventions (Dodd, 2013). Nevertheless, representing continuous speech with discrete symbols inherently incurs information loss, filtering out non-linguistic variation. Internal model representations serve as complement that retains richer information. Speech models trained on S2T tasks produce temporally aligned representations that capture empirically useful acoustic-phonetic (Choi et al., 2024), articulatory (Cho et al., 2024), and even semantic (Ma et al., 2025) features. The most widely used S2T model representations are from end-to-end ASR models such as Whisper (Radford et al., 2023) and WavLM (Chen et al., 2022). In contrast, LALMs representations are often inaccessible or difficult to analyze, as they focus mainly on textual output and lack strict temporal alignment with input speech."
        },
        {
            "title": "2.3 Assessing phonetic/phonological ability",
            "content": "In the text modality, language models are evaluated with text input and output. PhonologyBench (Suvarna et al., 2024) evaluates G2P, syllable counting, and rhyme judgment, while Bunzeck et al. (2025) and Goriely and Buttery (2025) probe phonological knowledge using minimal pairs and word segmentation. In the speech modality, models are evaluated with speech (and optionally text) input and representation output. SUPERB (Yang et al., 2021) and Dynamic-SUPERB (Huang et al., 2025) include phoneme recognition, phonological feature analysis, and pronunciation evaluation. BabySLM (Lavechin et al., 2023) and the ZeroSpeech challenges (Nguyen et al., 2020) propose metrics that evaluate phonological and acoustic-phonetic contrasts based on minimal pairs. In contrast to previous work, PRiSM evaluates phonetic ability in both text and speech 2 Abbr. Task Dataset Intrinsic: Core Capability (Metrics Lower is better) Phone Recognition (PFER) PR-tmt PR-arc PR-saa PR-drc PR-vox PR-tsm Variation of Seen Language Variation of Seen Language Variation of Seen Language Unseen Languages Unseen Languages Unseen Languages TIMIT (Garofolo et al., 1993) L2-ARCTIC Perceived (Zhao et al., 2018b) Speech Accent Archive (Weinberger, 2015) DoReCo (Paschen et al., 2020) VoxAngeles (Chodroff et al., 2024) Tusom2021 (Mortensen et al., 2021) Extrinsic: Downstream Utility (Metrics Higher is better) Pathological Speech: Dysarthria Intelligibility Prediction (τ ) & Child Speech Disorder Detection (F1) DYS-ez DYS-ua CSD-us Dysarthria Intelligibility Prediction Dysarthria Intelligibility Prediction Child Speech Disorder Detection EasyCall (Turrisi et al., 2021) UASpeech (Kim et al., 2008) UltraSuite (Eshky et al., 2018) L2 Speech: L1 Classification (F1) & L2 Assessment (τ ) L1-eda L1-arc L2-so L1 Classification L1 Classification L2 Assessment EdAcc (Sanabria et al., 2023) Kominek and Black (2004) & Zhao et al. (2018b) Speechocean762 (Zhang et al., 2021a) Lang. English English English 45 langs 95 langs Tusom Italian English English English English English Multilingual: Lang. ID (F1), Geolocation (Recall@1) & Phone Inventory Induction (F1-PI) LID-fl GEO-v PI-drc Lang. ID (LID) Speech Geolocation Phone Inventory Induction FLEURS-24 (Conneau et al., 2023) Vaani (Ghosh et al., 2025) DoReCo (Paschen et al., 2020) 24 langs Hindi Dialects 45 langs Table 1: List of evaluation tasks. Blue denotes core capabilities, where lower scores are better. Yellow denotes downstream utility, where higher scores are better. F1-PI is described in B.1. See Appendix for license details. through intrinsic and extrinsic tasks."
        },
        {
            "title": "3 Evaluation Framework of PRiSM",
            "content": "PRiSM covers intrinsic ( 3.1) and extrinsic Intrin- ( 3.2) evaluations shown in Figure 1. sic evaluation compares predicted transcriptions to gold labels, while extrinsic evaluation measures transcriptions and internal representations on downstream tasks. In extrinsic evaluation, transcriptions provide direct and interpretable signal of explicit phonetic content, whereas representations are commonly used in downstream tasks but may encode non-phonetic information. Table 1 summarizes included datasets and metrics. 3.1 Intrinsic: Core Capability We use Phonetic Feature Error Rate (PFER) to measure the distance between reference and predicted transcriptions. Unlike Phone Error Rate (PER), which treats each phone as token, PFER computes the edit distance D(, ) over articulatory features feat() such as roundness or voicing. As shown in Equation 1, where denotes an utterance (sequence of phones where indexes this sequence) and its ground truth, PFER is calculated as the total feature edit distance across all utterances divided by the total number of phones, representing the percentage of incorrect features (Mortensen et al., 2016). PFER = 1 (cid:80) (cid:88) D(feat(u ), feat(ui)) (1) The tasks comprise two categories: Variation of seen languages includes regional and nonnative speech, testing whether PR systems rely excessively on seen patterns rather than the actual input. Unseen languages assess the systems language-agnostic phonetic knowledge. Details of each task and dataset are in A.2."
        },
        {
            "title": "3.2 Extrinsic: Downstream Utility",
            "content": "We evaluate PR systems using two downstream probes, namely the transcript probe and the representation probe. For transcript probe (TP), input consists of predicted phonetic transcriptions, and the probe is text-based bi-GRU. For representation probe (RP), following the setup in Turian et al. (2022), we use the last layers hidden representations as input and temporal pooling with attention followed by Multi-Layer Perceptron as the probe. Metrics for each task are listed in Table 1 and the detailed experimental setup is in Appendix C. 3 Model W2V2P-LV60 (Xu et al., 2022) W2V2P-XLSR53 (Xu et al., 2022) MultiIPA (Taguchi et al., 2023) ZIPA-CTC (Zhu et al., 2025) ZIPA-CTC-NS (Zhu et al., 2025) POWSM (Li et al., 2025) POWSM-CTC (ours) Architecture (Enc / Dec) Enc: Wav2Vec2 Enc: XLSR53 Enc: XLSR53 Enc: Zipformer Enc: Zipformer Enc: E-Branchformer Dec: Transformer Enc: E-Branchformer Gemini 2.5 Flash (Comanici et al., 2025) Closed Qwen3-Omni-Instruct (Xu et al., 2025) Enc: AuT Dec: MoE Transformer CR-CTC None CR-CTC None CTC-Att None Int-CTC None N/A AR Closed Closed SSL Pre-training Data Phone Recognition Data Langs Loss CTC CTC CTC LibriLight MLS, CV, Babel MLS, CV, Babel MLS, CV, Babel MLS, CV, Babel CV 11.0 IPAPack++ IPAPack++ & PL IPAPack++ IPAPack++ Closed Closed 40+ 40+ 7 88 4k 88 88 >200 19 Table 2: Included PR systems. Architecture abbrv.: Encoder (Enc), Decoder (Dec), Audio Transformer (AuT), Mixture-of-Experts (MoE); Loss abbrv.: Consistency Regularized CTC (CR-CTC) (Yao et al., 2025), Hybrid CTC/Attention (CTC-Att) (Watanabe et al., 2017), Intermediate CTC (Int-CTC) (Lee and Watanabe, 2021), Autoregressive (AR); Data abbrv.: Multilingual LibriSpeech (MLS), Common Voice (CV), Pseudo-labeled (PL). We consider three categories of downstream tasks where phonetic information is essential. In pathological speech assessment, phonetic transcriptions are used to document patients speech and support diagnosis and treatment planning In L2 (Ball et al., 2009; Nelson et al., 2020). speech assessment, phonetic cues enable pronunciation feedback (Franco et al., 2010) and accent classification (Angkititrakul and Hansen, 2006). In multilingual speech identification, analyzing phonetic and phonological differences across languages and dialects, such as phone inventories, phonotactics, and phoneme realization, is crucial (Schultz and Kirchhoff, 2006). We describe each task and dataset in detail in A.3. LALMs: We include Gemini 2.5 Flash (closed-source) and Qwen3-Omni-Instruct (open-weight), both state-of-the-art systems widely used in recent studies (Lee et al., 2025). Since their representations are not easy to access or pool, we probe them with zero-shot prompting, which is form of context-based fine-tuning (Petrov et al., 2023). The prompts are in Appendix D. Other baselines: We include naive baseline that randomly predicting the class or the most frequent location (GEO-v). We also include WavLM4(Chen et al., 2022) and Whisper5(Radford et al., 2023) as competitive baselines for representation probing."
        },
        {
            "title": "5 Results and Discussion",
            "content": "Table 2 summarizes the studied model families: Wav2Vec2Phs: MultiIPA, W2V2P-LV60, and W2V2P-XLSR53 are fine-tuned variants of Wav2Vec2 (Baevski et al., 2020), contrastively pre-trained speech SSL models, and differ in pre-training coverage and phone recognition fine-tuning datasets. ZIPAs: ZIPA-CTC and ZIPA-CTC-NS are encoder-CTC models trained from scratch on multilingual data, with ZIPA-CTC-NS further trained on large-scale pseudo-labeled data from ZIPA-CTC. POWSMs: POWSM is an attention-based encoder-decoder (AED) model trained on the same dataset as ZIPAs and augmented for other S2T tasks. Following their framework, we train POWSM-CTC, an encoderCTC variant for comparison. Table 3 presents PR performance, and Table 4 presents comprehensive breakdown of downstream evaluations. In general, ZIPA-CTC-NS performs well in all settings, while Whisper excels in RP. LALMs generally remain less competitive. 5."
        },
        {
            "title": "Intrinsic Evaluation",
            "content": "We observe consistent trend for language variation: CTC-based models generally outperform LALMs, followed by AED models. For MultiIPA, English appears during pretraining but not finetuning, highlighting the importance of language coverage in PR data. On PR-saa, POWSM performs poorly likely due to decoder search on long speech sequences; meanwhile, text-based G2P model (Zhu et al., 2022) achieves PFER of 10.2, beating 4https://huggingface.co/microsoft/wavlm-base 5https://huggingface.co/openai/whisper-small 4 Model MultiIPA W2V2P-LV60 W2V2P-XLSR53 ZIPA-CTC ZIPA-CTC-NS POWSM POWSM-CTC Gemini 2.5 Flash Qwen3-Omni-Instruct Variation of Seen Language Unseen Languages PR-tmt PR-arc PR-saa Avg. PR-drc PR-vox PR-tsm Avg. 16.3 13.2 13.5 13.1 13.1 13.7 13.1 15.2 15.1 15.5 10.9 09.9 09.7 09.7 11.3 10.3 12.7 11.9 13.8 09.4 09.0 09.0 08.9 27.6 10. 13.2 09.1 15.2 11.2 10.8 10.6 10.6 17.5 11.1 13.7 12.0 18.3 17.8 17.3 18.0 16.8 17.1 18.1 105.30 150.20 15.2 15.7 13.9 17.0 17.1 17.1 15. 19.7 49.0 30.5 24.9 31.9 23.7 23.1 22.0 32.2 21.3 19.5 21.0 19.6 19.0 18.7 21.9 36.3 117.10 53.8 105.40 Table 3: PFER of the intrinsic evaluation (). English is included during pretraining but not fine-tuning. Some of the unseen languages may have appeared in the training data. See 5.1 for details. Model DYS-ez DYS-ua CSD-us L1-eda L1-arc L2-so LID-fl GEO-v PI-drc Pathological Speech L2 Speech Multilingual Speech Score 00.7 1.6 -0.8 0.9 41.8 1.0 06.3 0. 14.3 0.3 01.5 1.0 04.3 0.3 03.3 0.0 8. Naive Baseline Transcript Probe (TP) MultiIPA W2V2P-LV60 W2V2P-XLSR53 ZIPA-CTC ZIPA-CTC-NS POWSM POWSM-CTC 48.2 0.2 42.4 1.3 49.2 0.8 55.0 0.6 56.6 0.8 52.7 1.7 53.3 0.4 45.6 1.4 50.3 0.9 47.6 0.8 57.0 0.5 51.1 1.3 46.1 0.8 46.5 0.6 93.6 1.6 95.6 1.4 92.3 2.6 91.7 2.3 99.4 0.5 94.3 1.3 96.9 0.7 10.0 1.0 07.6 0.5 09.1 0.6 06.6 0.4 06.7 0.3 06.5 0.8 06.4 0.5 50.5 0.9 38.0 0.3 43.1 0.6 30.5 0.5 30.0 0.3 28.0 0.3 29.8 0. 33.3 1.7 36.1 1.7 37.5 0.8 36.6 2.8 40.8 0.8 28.4 2.2 26.8 0.7 89.3 0.5 91.4 0.2 94.1 0.2 95.6 0.2 95.9 0.1 95.1 0.5 90.4 0.4 44.5 0.4 45.7 0.9 44.5 1.2 44.1 1.0 44.7 1.8 43.7 1.4 42.9 0.8 Gemini 2.5 Flash Qwen3-Omni-Instruct 27.9 0.6 52.5 1.8 38.5 0.4 49.4 1. 95.0 1.6 98.9 0.8 06.4 0.4 06.9 0.6 22.3 0.4 30.5 0.3 20.1 1.1 15.6 1.4 91.8 0.3 89.3 0.2 33.2 1.0 34.7 1. Representation Probe (RP) MultiIPA W2V2P-LV60 W2V2P-XLSR53 ZIPA-CTC ZIPA-CTC-NS POWSM POWSM-CTC 65.5 4.0 67.2 1.8 70.8 2.2 73.2 2.2 71.2 2.2 73.0 3.0 73.6 1.5 77.0 1.4 79.9 0.9 82.0 2.2 74.7 1.2 75.1 0.8 70.8 1.1 66.7 1.6 98.5 0.8 98.6 0.6 99.2 0.7 99.5 0.3 98.6 0.9 99.5 0.3 97.9 0.9 11.7 1.1 12.0 0.3 13.0 1.1 13.9 0.9 13.7 0.6 10.3 1.2 08.0 0.7 53.0 3.3 60.7 2.9 47.0 6.0 73.4 2.5 74.1 2.8 68.0 1.9 53.0 0. 46.3 1.9 49.9 1.0 50.7 3.1 54.0 0.8 54.3 0.5 53.1 0.3 45.7 3.0 78.2 1.0 76.6 1.3 81.0 2.3 96.1 0.7 96.8 0.3 96.5 0.1 75.4 1.5 24.5 3.7 24.6 2.1 21.5 3.1 23.0 1.2 24.0 1.5 21.5 2.2 14.1 2.6 WavLM Whisper 69.2 2.0 74.8 1.1 77.5 1.4 79.5 0. 99.0 0.5 99.5 0.3 14.4 1.0 24.3 1.6 58.3 2.0 84.3 3.0 50.2 1.4 57.2 0.8 76.2 3.2 96.3 0.5 23.5 4.6 35.0 2. Zero-shot Gemini 2.5 Flash Qwen3-Omni-Instruct 21.4 27.0 50.4 61.7 75.3 70.9 32.7 18.2 43.9 31. 35.8 49.8 91.5 59.1 06.5 05.3 40.9 51.3 56.9 55.2 56.6 48.7 57.7 39.1 44.5 44.3 42.0 43.8 43.5 44.2 39.6 40.2 31.6 39.2 56.5 59.4 58.2 62.9 62.7 60.4 55.2 59.4 68. 41.5 41.5 Table 4: PR system performance on extrinsic tasks (). Results are reported as mean standard deviation across 5 random seeds where applicable. Best numbers are bolded and second-best underlined. See 5.2 for details. The formula for aggregrated score is in B.2. Gemini 2.5 Flash despite modeling only canonical pronunciations. For unseen languages, AED and CTC models show comparable performance, whereas LALMs perform poorly, sometimes producing repeated or degenerate outputs indicative of limited training exposure (Holtzman et al., 2020). Performance also varies across datasets, and languagewise breakdowns reveal heterogeneous behavior. POWSM outperforms POWSM-CTC and exhibits performance comparable to ZIPAs, suggesting that incorporating degree of language modeling may improve generalization by capturing shared phonological patterns, as further analyzed in 6.1. These trends show that variation in seen languages benefits from outputs grounded in known patterns, whereas unseen languages benefit from multilingual training and learned phonological patterns."
        },
        {
            "title": "5.2 Extrinsic Evaluations",
            "content": "For transcript probe, ZIPAs and W2V2P-XLSR53 are generally competitive. ZIPAs perform well on pathological speech, likely due to their normalized, smaller vocabularies, which approximate broad transcription known to be reliable for 5 speech disorders (Shriberg and Lof, 1991), while W2V2P-XLSR53 benefits from diverse pretraining data. Multilingual training further improves performance, especially on multilingual tasks. We discuss PI-drc in 6.2 as an example. Whispers strength in representation probe suggests that large-scale ASR pretraining produces representations that retain phonetic information. trade-off of TP and RP emerges among specialized PR models: for example, Wav2Vec2Phs achieve strong TP results on L2 speech but show limited gains on RP, whereas ZIPAs underperform on TP yet excel on RP. Task category also influences their relative performance: Pathological speech benefits more from RP, L2 speech falls in the middle, and multilingual tasks tend to favor TP. We hypothesize that transcripts act as structured bottleneck: pathological speech relies on features such as timbre and prosody, whereas multilingual settings benefit less from acoustic detail. We investigate the behavior of TP on GEO-v in 6.3."
        },
        {
            "title": "LALMs",
            "content": "show task-dependent performance. Notably, Qwen3-Omni-Instruct achieves competitive TP on pathological speech, but they generally perform poorly in zero-shot settings and underperform on languages other than English (DYS-ez). An exception is L2 speech, where the gap is smaller, explored in 6.4. Overall, our results highlight the importance of evaluating PR systems with combination of intrinsic and extrinsic tasks. Intrinsic evaluation alone may not fully capture phonetic capabilities, while extrinsic evaluation reveals that relative performance on TP and RP is task-dependent. Multilingual pretraining and fine-tuning improve performance across model families, and encoder-CTC based architectures provide more stable PR performance in new doIn contrast, LALMs remain limited in mains. phone recognition and related tasks."
        },
        {
            "title": "6 Analysis",
            "content": "We conduct several analyses to anchor our obIn 6.1, We examine how architecservations. tural choices affect the balance between phonotactics and acoustics, echoing with evaluation results. In 6.2, we study multilingual generalization and confirm that encoder-only architectures trained with diverse language coverage at all stages perform well for PR. In 6.3, we analyze TP in detail and show that it effectively captures Figure 2: PFER vs Phone masking rate. PR model that relies only on acoustics should produce horizontal line. Encoder-only models trained with CTC loss retain acoustic fidelity at high masking levels. See 6.1. phone distribution differences across regions. Finally, in 6.4, we assess zero-shot performance of LALMs on challenging tasks, concluding that they remain insensitive to sociophonetic variation."
        },
        {
            "title": "6.1 Phonotactics or the Acoustic Signal",
            "content": "Ideally, PR systems would faithfully transcribe the actual pronunciation in the speech signal via acoustic modeling. Instead, model transcriptions often normalize toward standard pronunciations or other probabilistically likely phone patterns (Zhu et al., 2025; Li et al., 2025), essentially relying on (phone-level) language modeling (Pimentel et al., 2020).6 Additionally, models can also overfit phonotactics from the high-resource languages. In this experiment, we investigate the extent to which PR systems rely on such phonotactic patterns present in the training data, as opposed to information derived directly from acoustic signal. Using TIMIT (Garofolo et al., 1993)s timealigned phone transcripts, we replace p% of phones with silence, transcribe the modified speech using PR, and compute PFER against reference containing only the remaining phones. the phone masking rate In Figure 2, we plot against the PFER for different model families. For model that only relies on the acoustic waveform for prediction, the curve would be horizontal line. However, for models that rely on phonotactics, the PFER will increase with greater noise. While all models start at similar PFER, Wav2Vec2Phs and POWSM-CTC perform better 6A common example of such phonotactic knowledge is the intuition that brick [bôIk] is valid phone sequence in English while bnick [bnIk] is not (Chomsky and Halle, 1968). 6 than ZIPAs and POWSM at higher masking levels. Thus Wav2Vec2Phs relies more on the acoustic signal than on phonotactics. While POWSM is an AED model trained with next-token prediction, Wav2Vec2Phs and POWSM-CTC are encoderonly models trained with the CTC objective. However, ZIPAs are also encoder-only (Zipformer (Yao et al., 2024)) models, but they are trained with consistency regularized CTC (CRCTC) loss (Yao et al., 2025). The high PFER of ZIPA means that CR-CTC loss biases the model to learn from both phonotactics and acoustics. We hypothesize that ZIPAs Zipformer-based downsampling bottleneck can be another possible reason for ZIPAs high PFER. We also observe that the insertion rates for different models follow the same trend as the curves in Figure 2, showing that POWSM and ZIPA produce phonetic transcriptions even when there is no input speech. Interestingly, ZIPA-CTC-NS and POWSM perform best on unseen languages, but POWSM struggles with seen-language variation, and ZIPA-CTC-NS underperforms on pathological speech. This aligns with the idea that some tasks benefit from learned phonological patterns, while others depend more on capturing acoustic information."
        },
        {
            "title": "6.2 Zero-Shot Phonetic Inventory Induction",
            "content": "Identifying the inventory of phones in new language is an important linguistic application and often an early step toward developing standardized transcription system for it. Such task requires PR models to recognize phones correctly in unseen phonetic environments. Therefore, it relies on the phonetic diversity the models have seen in the input speech signal during training. We explore these behaviors in this set of experiments. Our dataset, derived from DoReCo, consists of low-resource languages absent from the training corpora of all models. The transcripts from all models are used to compute the phone inventory after applying PanPhon-based phone tokenization (Mortensen et al., 2016) followed by set union over detected phones. The ground truth inventory is constructed similarly using the phonetic transcriptions provided by DoReCo and set similarity metrics ( B.1) are computed. We show the macro-averaged values in Figure 3. POWSM-CTC emerges as the strongest model. The large gap between POWSM-CTC and POWSM (which differ only in architecture) suggests that the encoder-only architecture plays Figure 3: Precision and Recall scores of PR systems on phone inventory induction for unseen languages ( 6.2). CTC models trained with highly multilingual data are more stable. crucial role in high precision transcripts even in an unseen phonetic environment. As for ZIPAs, which differ in training data, ZIPA-CTCNS is more precise than ZIPA-CTC. The extended multilingual training of ZIPA-CTC-NS on pseudo-labeled data leads to more precise phone predictions for unseen languages. This suggests that noisy pseudo-labels allow for improved precision for new languages. Similar trends is seen in comparing W2V2P-XLSR53 vs W2V2P-LV60 and MultiIPA, where multilingual SSL alone is insufficient for MultiIPA. Essentially, broader language coverage in both pre-training and supervised training result in more precise model. Although the size of IPAPack++ (17k hr) is much smaller than that used for Wav2Vec2Phs (160k hr), the larger number of languages in the supervised training stage (88 vs 40) leads to better recall for ZIPAs, compared to Wav2Vec2Phs. This suggests that diversity of languages is as important as the volume of data. Most models have high recall (> 70) and low precision (< 50), suggesting that most predicted phones are incorrect and predictions have high entropy."
        },
        {
            "title": "6.3 Geolocation for Dialectal Speech",
            "content": "In Table 4, we observe that our TPs significantly outperform the RPs on Hindi dialectal geolocation (Foley et al., 2024), where the former observes an average error of 146 km, while the latter observes an average error of 253 km. As reference, our data is spread over 1478 km (East7 Figure 4: Attribution map from Vaani (Ghosh et al., 2025). Red supports and blue opposes correct geolocation. W2V2P-LV60 detects doubled phones ( 6.3). West) and 1703 km (North-South), covering the entire Hindi speaking region of India. This performance is surprising, as the cascade-based approach loses suprasegmental information such as intonation that provide strong phonetic cues for the differentiation of dialects (Vicenik and Sundara, 2013; Grabe and Post, 2002). However, our results provide empirical evidence that morphological and phonetic differences suffice for fine-grained differentiation between Hindi dialects (Gumperz, 1958). We hypothesize that part of the reason why hidden representations underperform cascade is also due to the downstream probe, where the RP employs attention pooling with an MLP, while the TP employs an RNN. As the RNN preserves phone order information, even in the case where two dialects share similar phoneme inventories, distributional differences of phone sequences between the dialects can be leveraged for finegrained differentiation (Gumperz, 1958; Shim et al., 2024). We further analyze this behavior by employing integrated gradient based attribution maps (Sundararajan et al., 2017) on TP. There is tendency of pronouncing two consonant sounds instead of one in the Bangru dialect of Haryanvi (Devi and Mishra, 2021). For example, the English loan word Cooler [ku:lar] becomes [kullar], while the Hindi word Rakha [R@.kha:] (kept) becomes [R@k.kha:]. Figure 4 shows attribution map for an utterance from GEO-v. Speaker utters these words in their native accent, W2V2PLV60 outputs [ll] and [kk], and TP aligns with one of the doubled phones. We leave more detailed interpretability analysis to future work."
        },
        {
            "title": "6.4 LALMs lack phonetic perception",
            "content": "We examine the zero-shot predictions of LALMs on two tasks: GEO-v and L1-eda. On GEO-v, LALMs perform near chance level, whereas on L1-eda, Gemini 2.5 Flash achieves the strongest performance. On GEO-v, both models exhibit geographic mode collapse. Qwen3-Omni-Instruct predicts New Delhi for nearly all inputs, while Gemini"
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce PRiSM, the first standardized benchmark to measure capabilities of PR systems on transcription task and downstream task performance. We also open-source our datasets in an easy-to-use format with our toolkit. Our evaluations reveal that models behave differently on PR and on downstream applications. Therefore, we recommend that models be benchmarked in both categories to make comparisons. Our results and analysis show that PR for seen language benefits from outputs grounded in familiar patterns, whereas unseen languages rely on multilingual training and learned phonological patterns. Broad and diverse language coverage, along with encoder-CTC architectures, improves stability across tasks, while LALMs currently lag behind specialized PR models. Together, these findings highlight the value of PRiSM as framework for evaluating PR systems across diverse languages, tasks, and architectures."
        },
        {
            "title": "Limitations",
            "content": "While PRiSM evaluates PR systems across range of intrinsic and extrinsic settings, it is constrained by the availability of curated datasets. As result, coverage of languages, dialects, accents, and speaking styles remains incomplete and may reflect biases present in the underlying corpora. In addition, phonetic transcription does not conit destitute single objective ground truth: pends on annotation guidelines, annotator judgments, and the chosen phone inventory. The IPAbased interface may also miss or normalize away language-specific or gradient phonetic phenomena. Both intrinsic and extrinsic evaluations are necessary to assess PR systems, but each has limitations. Transcript probes align with linguistic features, yet they may also overfit to spurious cues (e.g., sequence length) when datasets are biased or transcripts are noisy due to low PR quality. For representation probes, phonetic information may be distributed across different layers, and performance can depend on the chosen fusion or pooling strategy. Models may benefit from task-specific decoding hyperparameters and prompts, whereas we use default settings and prompts that only contain key instructions. Our goal is to assess fundamental phonetic capabilities and provide comparative insights; we do not claim that the reported results reflect the best possible performance achievable for each model."
        },
        {
            "title": "Ethics Statement",
            "content": "All data used in this work are ethically sourced, either through permissive licensing or with proper consent. Speech datasets, particularly those involving pathological speech, may contain sensitive personal information, and we strictly adhere to the licenses and usage conditions associated with each dataset. PR systems may be misapplied in ways that unfairly label speakers without appropriate expert supervision, especially in educational, clinical, demographic, or geographic contexts. We introduce PRiSM with the goal of supporting responsible and rigorous research, and we encourage its use to advance speech technologies that consider linguistic and cultural diversity regardless of resource availability."
        },
        {
            "title": "The Use of LLMs",
            "content": "We acknowledge the use of large language models (LLMs) to assist with refinement of the writing, including grammar correction and clarity improvements. We also used LLMs as coding assistants. All the code was then verified by authors. All conceptual, methodological, and experimental work was done independently by the authors."
        },
        {
            "title": "8 Acknowledgement",
            "content": "We thank Jinchuan and Haoran for their support with vLLM, and Brian Yan and Brian Cho for helpful discussions. This work was supported by National Science Foundation grant #2504019. This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (RS-2022-00143911, AI Excellence Global Innovative Leader Education Program). We also acknowledge the Delta and DeltaAI systems, and support from the NVIDIA Academic Hardware Grant Program 2025. This work used the Delta and DeltaAI systems at NCSA through allocations CIS210014 and IRI120008P from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program."
        },
        {
            "title": "References",
            "content": "Pongtep Angkititrakul and John HL Hansen. 2006. Advances in phone-based modeling for automatic acIEEE transactions on audio, cent classification. speech, and language processing, 14(2):634646. Siddhant Arora, Kai-Wei Chang, Chung-Ming Chien, Yifan Peng, Haibin Wu, Yossi Adi, Emmanuel Dupoux, Hung-Yi Lee, Karen Livescu, and Shinji Watanabe. 2025. On the landscape of spoken lanarXiv guage models: comprehensive survey. preprint arXiv:2504.08528. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460. Martin Ball, Nicole Müller, Marie Klopfenstein, and Ben Rutter. 2009. The importance of narrow phonetic transcription for highly unintelligible speech: Some examples. Logopedics Phoniatrics Vocology, 34(2):8490. Bastian Bunzeck, Daniel Duran, Leonie Schade, and Sina Zarrieß. 2025. Small language models also work with small vocabularies: Probing the linguistic abilities of grapheme-and phoneme-based baby llamas. In Proceedings of the 31st International Conference on Computational Linguistics, pages 6039 6048. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, and 1 others. 2022. Wavlm: Large-scale self-supervised preIEEE training for full stack speech processing. 9 Journal of Selected Topics in Signal Processing, 16(6):15051518. therapy sessions. In Interspeech 2018, pages 1888 1892. ISCA. Cheol Jun Cho, Abdelrahman Mohamed, Alan Black, and Gopala Anumanchipalli. 2024. Selfsupervised models of speech infer universal articIn ICASSP 2024-2024 IEEE ulatory kinematics. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1206112065. IEEE. Eleanor Chodroff, Blaž Pažon, Annie Baker, and Steven Moran. 2024. Phonetic segmentation of the In Proceedings of the ucla phonetics lab archive. 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1272412733. Kwanghee Choi, Ankita Pasad, Tomohiko Nakamura, Satoru Fukayama, Karen Livescu, and Shinji Watanabe. 2024. Self-Supervised Speech Representations In Interspeech are More Phonetic than Semantic. 2024, pages 45784582. Kwanghee Choi, Eunjung Yeo, Kalvin Chang, Shinji Watanabe, and David Mortensen. 2025. Leveraging allophony in self-supervised speech models for atypical pronunciation assessment. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 26132628, Albuquerque, New Mexico. Association for Computational Linguistics. Noam Chomsky and Morris Halle. 1968. The sound pattern of English. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2023. Fleurs: Few-shot learning evaluation of universal representations of In 2022 IEEE Spoken Language Technolspeech. ogy Workshop (SLT), pages 798805. IEEE. Suman Devi and Uma Mishra. 2021. Dialects of haryanvi language: comparative study. Journal of Advances and Scholarly Researches in Allied Education, 18(6):221223. Barbara Dodd. 2013. Differential diagnosis and treatment of children with speech disorder. John Wiley & Sons. Aciel Eshky, Manuel Sam Ribeiro, Joanne Cleland, Korin Richmond, Zoe Roxburgh, James Scobbie, and Alan Wrench. 2018. Ultrasuite: repository of ultrasound and acoustic data from child speech Patrick Foley, Matthew Wiesner, Bismarck Odoom, Leibny Paola Garcia Perera, Kenton Murray, and Philipp Koehn. 2024. Where are you from? geolocating speech and applications to language idenIn Proceedings of the 2024 Conference tification. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 5114 5126, Mexico City, Mexico. Association for Computational Linguistics. Horacio Franco, Harry Bratt, Romain Rossier, Venkata Rao Gadde, Elizabeth Shriberg, Victor Abrash, and Kristin Precoda. 2010. Eduspeak: speech recognition and pronunciation scoring toolkit for computer-aided language learning applications. Language Testing, 27:401 418. Heting Gao, Junrui Ni, Yang Zhang, Kaizhi Qian, Shiyu Chang, and Mark Hasegawa-Johnson. 2021. Zero-shot cross-lingual phonetic recognition with external language embedding. In Interspeech 2021, pages 13041308. John Garofolo, Lori Lamel, William Fisher, David Pallett, Nancy Dahlgren, Victor Zue, and Jonathan Fiscus. 1993. Timit acoustic-phonetic continuous speech corpus. Prasanta Kumar Ghosh, Raghu Dharmaraju, Nihar Desai, and 1 others. 2025. Vaani: Capturing the language landscape for an inclusive digital india. https://vaani.iisc.ac.in/. Zebulon Goriely and Paula Buttery. 2025. Babylms first words: Word segmentation as phonological probing task. In The SIGNLL Conference on Computational Natural Language Learning. Esther Grabe and Brechtje Post. 2002. variation in the british isles. pages 343346. Intonational In Speech prosody, John J. Gumperz. 1958. Phonological differences in three hindi dialects. Language, 34:212. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neuIn 8th International Conferral text degeneration. ence on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, and 1 others. 2025. Dynamic-superb phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 In The Thirteenth International Conference tasks. on Learning Representations. 10 Solène Inceoglu, Wen-Hsin Chen, and Hyojung Lim. 2023. Assessment of l2 intelligibility: Comparing l1 listeners and automatic speech recognition. ReCALL: the Journal of EUROCALL, 35(1):89104. International Phonetic Association. 1999. Handbook of the International Phonetic Association: guide to the use of the International Phonetic Alphabet. Cambridge University Press. Rao Ma, Mengjie Qian, Yassir Fathullah, Siyuan Tang, Mark Gales, and Kate Knill. 2025. Cross-lingual transfer learning for speech translation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 3343, Albuquerque, New Mexico. Association for Computational Linguistics. Heejin Kim, Mark Hasegawa-Johnson, Adrienne Perlman, Jon Gunderson, Thomas S. Huang, Kenneth Watkin, and Simone Frame. 2008. Dysarthric speech database for universal access research. In Interspeech 2008, pages 17411744. John Kominek and Alan Black. 2004. The cmu arctic speech databases. In SSW, pages 223224. Peter Ladefoged, Barbara Blankenship, Russell G. Schuh, Patrick Jones, Nicole Gfroerer, Emily Griffiths, Lisa Harrington, Cheryl Hipp, Mayu Kaneko, Claire Moore-Cantwell, Gunhye Oh, Karen Pfister, Keli Vaughan, Rosary Videc, Sarah Weismuller, Samara Weiss, Jamie White, Sarah Conlon, WingSze Jamie Lee, and Rafael Toribio. 2009. The UCLA Phonetics Lab Archive. Marvin Lavechin, Yaya Sy, Hadrien Titeux, María Andrea Cruz Blandón, Okko Räsänen, Hervé Bredin, Emmanuel Dupoux, and Alejandrina Cristia. 2023. Babyslm: language-acquisition-friendly benchmark of self-supervised spoken language models. In INTERSPEECH 2023, pages 45884592. ISCA. Jaesong Lee and Shinji Watanabe. 2021. Intermediate loss regularization for ctc-based speech recognition. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 62246228. IEEE. Tony Lee, Haoqin Tu, Chi Heem Wong, Zijun Wang, Siwei Yang, Yifan Mai, Yuyin Zhou, Cihang Xie, and Percy Liang. 2025. Ahelm: holistic evaluation of audio-language models. arXiv preprint arXiv:2508.21376. Chin-Jou Li, Kalvin Chang, Shikhar Bharadwaj, Eunjung Yeo, Kwanghee Choi, Jian Zhu, David Mortensen, and Shinji Watanabe. 2025. Powsm: phonetic open whisper-style speech foundation model. Preprint, arXiv:2510.24992. Xinjian Li, Siddharth Dalmia, Juncheng Li, Matthew Lee, Patrick Littell, Jiali Yao, Antonios Anastasopoulos, David Mortensen, Graham Neubig, Alan Black, and 1 others. 2020. Universal phone recognition with multilingual allophone system. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 82498253. IEEE. David Mortensen, Siddharth Dalmia, and Patrick Littell. 2018. Epitran: Precision g2p for many lanIn Proceedings of the Eleventh Internaguages. tional Conference on Language Resources and Evaluation (LREC 2018). David R. Mortensen, Patrick Littell, Akash Bharadwaj, Kartik Goyal, Chris Dyer, and Lori S. Levin. 2016. Panphon: resource for mapping IPA segments to articulatory feature vectors. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 34753484. ACL. David Mortensen, Jordan Picone, Xinjian Li, and Kathleen Siminyu. 2021. Tusom2021: phonetically transcribed speech dataset from an endangered language for universal phone recognition experiments. In Proc. Interspeech 2021, pages 3660 3664. Taylor Louise Nelson, Zaneta Mok, and Kyriaki Ttofari Eecen. 2020. Use of transcription when assessing childrens speech: Australian speech-language pathologists practices, challenges, and facilitators. Folia Phoniatrica et Logopaedica, 72(2):131142. Tu Anh Nguyen, Maureen de Seyssel, Patricia Rozé, Morgane Rivière, Evgeny Kharitonov, Alexei Baevski, Ewan Dunbar, and Emmanuel Dupoux. 2020. The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken lanIn NeuRIPS Workshop on Selfguage modeling. Supervised Learning for Speech and Audio Processing. Ludger Paschen, François Delafontaine, Christoph Draxler, Susanne Fuchs, Matthew Stave, and Frank Seifart. 2020. Building time-aligned crosslinguistic reference corpus from language documenIn Proceedings of the 12th tation data (doreco). Conference on Language Resources and Evaluation (LREC 2020). European Language Resources Association. Jing Peng, Yucheng Wang, Yangui Fang, Yu Xi, Xu Li, Xizhuo Zhang, and Kai Yu. 2024. survey on speech large language models. arXiv preprint arXiv:2410.18908. Xinjian Li, Florian Metze, David Mortensen, Alan Black, and Shinji Watanabe. 2022. Asr2k: Speech recognition for around 2000 languages without audio. arXiv preprint arXiv:2209.02842. Aleksandar Petrov, Philip HS Torr, and Adel Bibi. 2023. When do prompting and prefix-tuning work? arXiv theory of capabilities and limitations. preprint arXiv:2310.19698. 11 Tiago Pimentel, Brian Roark, and Ryan Cotterell. 2020. Phonotactic complexity and its trade-offs. Transactions of the Association for Computational Linguistics, 8:118. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Karen Rosero, Ali Salman, Shreeram Chandra, Berrak Sisman, Cortney Vant Slot, Alex Kane, Rami Hallac, and Carlos Busso. 2025a. Advancing pediatric asr: The role of voice generation in disordered speech. In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, pages 28902894. International Speech Communication Association. Karen Rosero, Eunjung Yeo, David Mortensen, Cortney Vant Slot, Rami Hallac, and Carlos Busso. 2025b. Finding my voice: Generative reconstruction of disordered speech for automated clinical evaluation. arXiv preprint arXiv:2509.19231. Ramon Sanabria, Nikolay Bogoychev, Nina Markl, Andrea Carmantini, Ondrej Klejch, and Peter Bell. 2023. The edinburgh international accents of english corpus: Towards the democratization of english asr. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Tanja Schultz and Katrin Kirchhoff. 2006. Multilingual speech processing. Elsevier. Xian Shi, Fan Yu, Yizhou Lu, Yuhao Liang, Qiangze Feng, Daliang Wang, Yanmin Qian, and Lei Xie. 2021. The accented english speech recognition challenge 2020: Open datasets, tracks, baselines, results and methods. In IEEE International Conference on Acoustics, Speech, and Signal Processing. Ryan Soh-Eun Shim, Kalvin Chang, and David Mortensen. 2024. Phonotactic complexity across In Proceedings of the 2024 Joint Indialects. ternational Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1273412748. Lawrence Shriberg, Raymond Kent, Tara McAllister, Jonathan Preston, and Marisha Speights. 2025. Clinical phonetics. Plural Publishing. Lawrence Shriberg and Gregory Lof. 1991. Reliability studies in broad and narrow phonetic transcription. Clinical Linguistics & Phonetics, 5(3):225 279. Ashima Suvarna, Harshita Khandelwal, and Nanyun Peng. 2024. Phonologybench: Evaluating phonological skills of large language models. In Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024), pages 114. Chihiro Taguchi, Yusuke Sakai, Parisa Haghani, and David Chiang. 2023. Universal automatic phonetic transcription into the international phonetic alphabet. In Interspeech 2023, pages 25482552. Ming Tu, Anna Grabek, Julie Liss, and Visar Berisha. 2018. Investigating the role of l1 in automatic pronunciation evaluation of l2 speech. arXiv preprint arXiv:1807.01738. Joseph Turian, Jordie Shier, Humair Raj Khan, Bhiksha Raj, Björn Schuller, Christian Steinmetz, Colin Malloy, George Tzanetakis, Gissel Velarde, Kirk McNally, and 1 others. 2022. Hear: Holistic evaluation of audio representations. In NeurIPS 2021 Competitions and Demonstrations Track, pages 125145. PMLR. Rosanna Turrisi, Arianna Braccia, Marco Emanuele, Simone Giulietti, Maura Pugliatti, Mariachiara Sensi, Luciano Fadiga, and Leonardo Badino. 2021. Easycall corpus: dysarthric speech dataset. In Interspeech 2021, pages 4145. Chad Vicenik and Megha Sundara. 2013. The role of intonation in language and dialect discrimination by adults. Journal of Phonetics, 41(5):297306. Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy Chen. 2025. Audiobench: universal benchmark for audio large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 42974316. Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, and Tomoki Hayashi. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics in Signal Processing, 11(8):12401253. Steven Weinberger. 2015. Speech accent archive. Retrieved from https://accent.gmu.edu. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, and 19 others. 2025. Qwen3-omni technical report. Preprint, arXiv:2509.17765. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International conference on machine learning, pages 33193328. PMLR. Qiantong Xu, Alexei Baevski, and Michael Auli. 2022. Simple and effective zero-shot cross-lingual In Interspeech 2022, pages phoneme recognition. 21132117. 12 Wei Xue, Roeland van Hout, Catia Cucchiarini, and Helmer Strik. 2023. Assessing speech intelligibility of pathological speech in sentences and word lists: The contribution of phoneme-level measures. Journal of Communication Disorders, 102:106301. Chih-Kai Yang, Neo S. Ho, and Hung-yi Lee. 2025. Towards holistic evaluation of large audio-language In Proceedings models: comprehensive survey. of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1015510181, Suzhou, China. Association for Computational Linguistics. Mu Yang, Ram C. M. C. Shekar, Okim Kang, and John H. L. Hansen. 2023. What can an accent identifier learn? probing phonetic and prosodic information in wav2vec2-based accent identification model. In INTERSPEECH 2023, page 2. Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, GuanTing Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung yi Lee. 2021. Superb: Speech processing universal performance benchmark. In Interspeech 2021, pages 11941198. Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, and Daniel Povey. 2024. Zipformer: faster and better encoder for automatic speech recognition. International Conference on Learning Representations. Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, and Daniel Povey. 2025. Crctc: Consistency regularization on ctc for improved speech recognition. In The Thirteenth International Conference on Learning Representations. Saierdaer Yusuyin, Te Ma, Hao Huang, Wenbo Zhao, and Zhijian Ou. 2025. Whistle: Data-efficient multilingual and crosslingual speech recognition via weakly phonetic supervision. IEEE Transactions on Audio, Speech and Language Processing."
        },
        {
            "title": "Piotr",
            "content": "Zelasko, Siyuan Feng, Laureano Moro Velazquez, Ali Abavisani, Saurabhchand Bhati, Odette Scharenborg, Mark Hasegawa-Johnson, and Najim Dehak. 2022. Discovering phonetic inventories with crosslingual automatic speech recognition. Computer speech & language, 74:101358. Junbo Zhang, Zhiwen Zhang, Yongqing Wang, Zhiyong Yan, Qiong Song, Yukai Huang, Ke Li, Daniel Povey, and Yujun Wang. 2021a. speechocean762: An open-source non-native english speech corpus for pronunciation assessment. In Proc. Interspeech 2021, pages 37103714. Junbo Zhang, Zhiwen Zhang, Yongqing Wang, Zhiyong Yan, Qiong Song, Yukai Huang, Ke Li, Daniel 13 Povey, and Yujun Wang. 2021b. speechocean762: An open-source non-native english speech corPreprint, pus arXiv:2104.01378. for pronunciation assessment. Guanlong Zhao, Sinem Sonsaat, Alif Silpachai, Ivana Lucic, E. Chukharev-Hudilainen, John M. Levis, and R. Gutierrez-Osuna. 2018a. L2-arctic: nonnative english speech corpus. In Interspeech. Guanlong Zhao, Sinem Sonsaat, Alif Silpachai, Ivana Lucic, Evgeny Chukharev-Hudilainen, John Levis, and Ricardo Gutierrez-Osuna. 2018b. L2-arctic: In Proc. Internon-native english speech corpus. speech 2018, pages 27832787. Jian Zhu, Farhan Samir, Eleanor Chodroff, and David R. Mortensen. 2025. ZIPA: family of efficient models for multilingual phone recognition. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1956819585, Vienna, Austria. Association for Computational Linguistics. Jian Zhu, Changbing Yang, Farhan Samir, and Jahurul Islam. 2024. The taste of IPA: Towards openvocabulary keyword spotting and forced alignment In Proc. NAACL, pages 750 in any language. 772, Mexico City, Mexico. Association for Computational Linguistics. Jian Zhu, Cong Zhang, and David Jurgens. 2022. ByT5 model for massively multilingual graphemeto-phoneme conversion. In Interspeech 2022, pages 446450."
        },
        {
            "title": "A Dataset details and Licenses",
            "content": "This section introduces the datasets and the motivation of downstream tasks. Table 5 lists the licensing information and dataset size. A.1 Data Links Phone Recognition datasets and EasyCall, EdAcc, CMU-Arctic, L2-ARCTIC, Fleurs-24 and Ultrasuite are available at https://huggingface.co/collections/ changelinglab/prism can UASpeech obtained https://speechtechnology.web. illinois.edu/uaspeech/ be from Speechocean762 downloaded can from https://github.com/jimbozhang/ speechocean762 be Dataset Licence Train Val0 Test0 A.3 Datasets in Extrinsic Evaluation LDC CC BY-NC 4. Phone Recognition TIMIT L2-ARCTIC Speech Accent Archive CC BY-NC-SA 2.0 DoReCo VoxAngeles Tusom2021 CC0 1.0 CC BY-NC 4.0 MIT - - - - - - - - - - - - 6,300 3,599 3,019 18,734 5,445 2,255 Pathological Speech EasyCall UASpeech UltraSuite L2 Speech EdAcc CMU Arctic L2-ARCTIC SpeechOcean Multilingual Speech FLEURS-24 Vaani-Hi DoReCo CC BY-NC 2.0 LICENSE CC BY-NC 4.0 11,859 9,166 1,819 4,252 5,331 311 CC BY 4.0 Free software CC BY-NC 4.0 CC BY 4. 6,917 2,264 13,450 2,260 2,525 1,132 6,787 240 4,967 6,885 287 5,497 1,132 6,630 2,500 CC BY 4.0 CC-BY-4.0 CC0 1.0 4,800 19,780 - 2,400 2,668 - 4,800 3,985 18,734 Table 5: Licence and split size (#utterance). DoReCo includes datasets of different CC licences; we use the 45-language subset created by Zhu et al. (2024). See A.1 for dataset links. A.2 Datasets in Intrinsic Evaluation Variation in Seen Language TIMIT (Garofolo et al., 1993) contains speech from six regional varieties of American English and is often used for PR evaluation. The Speech Accent Archive (Weinberger, 2015) provides read speech (the Please call Stella passage) and narrow phonetic transcriptions from non-native English speakers across 391 L1 languages. L2ARCTIC (Zhao et al., 2018b) includes read speech from non-native speakers; we use the L2-Arctic Perceived set7 which consists of manually annotated phoneme transcriptions rather than standard G2P output. Unseen Languages DoReCo (Paschen et al., 2020) is dataset of 50+ small or endangered languages with broad phonetic transcriptions; we use the same DoReCo subset as Zhu et al. (2025, 2024). VoxAngeles (Chodroff et al., 2024) is cleaned, 95-language version of the UCLA Phonetics Lab Archive (Ladefoged et al., 2009). Tusom2021 (Mortensen et al., 2021) is dataset of speech and narrow phonetic transcriptions of individual words in the low-data Tangkhulic language Tusom. We removed tones as none of the models supports them. 7https://huggingface.co/anyspeech 14 Pathological Speech Assessment Dysarthria intelligibility prediction predicts dysarthria severity levels based on phonetic representations. Increasing dysarthria severity is associated with reduced intelligibility, for which impaired phoneme production is major clinical predictor (Xue et al., 2023). Two dysarthric speech datasets are evauated: UASpeech (Kim et al., 2008), an English corpus with speaker-level intelligibility scores, and EasyCall (Turrisi et al., 2021), an Italian corpus annotated with dysarthria severity ratings. Child speech disorder detection classifies whether given utterance is produced by child with speech disorder, supporting applications in speech therapy and the selection of specialized speech models (Rosero et al., 2025b,a). We use acoustic recordings from the Ultrasuite corpus (Eshky et al., 2018), with manually corrected transcription-audio mismatches. The curated dataset is released with this paper. L2 Speech Evaluation Proficiency assessment for L2 Learners uses phonetic information to automatically assess L2 English proficiency. We use utterances and sentence-level scores on 0-10 scale from Speechocean762 (Zhang et al., 2021b), an L1 Chinese, L2 English corpus. L1 influence classification classifies speakers L1 (native language) background, which introduces distinctive articulatory patterns into speech in an L2 language (Yang et al., 2023; Shi et al., 2021). We use EdAcc (Sanabria et al., 2023) for one setup, and the other combines L2-ARCTIC (Zhao et al., 2018a) for non-native speech with CMU ARCTIC (Kominek and Black, 2004) for native speech. Multilingual Speech Identification Language identification (LID) predicts the language spoken in an utterance from audio input. We use it as coarse-grained evaluation of whether phonetic representations can distinguish both seen and unseen languages with the 102 languages in FLEURS (Conneau et al., 2023). Speech geolocation identification predicts the origin of speaker from an utterance in their native language, drawing on systematic phonetic shifts associated with geography, sociolinguistic variation, and language contact (Foley et al., 2024). We use data from the Hindi-belt of India from Vaani (Ghosh et al., 2025). The detailed algorithm for this subset creation is explained in Appendix F. Phone inventory induction is the task of inferring the set of phones used by language from speech recordings, which is useful for language documentation and helps identify systematic errors during evaluation. We use DoReCo ( B.1) by deriving phone inventories from gold phone transcriptions and comparing them against the predicted transcriptions for each language."
        },
        {
            "title": "B Metrics",
            "content": "B.1 Task Metric: F1 of Phone Inventory (F1-PI) phone inventory is the set of all phones used in language. F1-PI assesses the degree of overlap between the phones transcribed by system for given language and the ground truth phone inventory for that language ( Zelasko et al., 2022). For two sets and B, the F1-score is defined as the harmonic mean of B/A and A/B. Set membership can be based on exact matches or fuzzy matches (e.g., over phonetic features). This metric requires only reference inventory for the target language, not full transcription (although inventories can be derived from transcriptions), making it especially useful for under-resourced languages. B.2 Summary Metric: PRiSM Extrinsic"
        },
        {
            "title": "Score",
            "content": "To aggregate performance across extrinsic evaluation tasks with significantly varying test set sizes (Ni)  (Table 5)  , we compute Score using logarithmically weighted average. This approach ensures that larger datasets contribute more to the final score due to their statistical significance, while preventing them from completely dominating smaller, high-variance datasets (such as CSD-us). Let si be the model performance on task and Ni be the number of samples in that task. The aggregate score is defined as: = (cid:80)K i=1 ln(Ni) si (cid:80)K i=1 ln(Ni) (2) where = 6 corresponds to the tasks (DYS-ez, DYS-ua, CSD-us, L1-eda, L1-arc, and L2-so) that show differentiation in model behavior. The weights wi = ln(Ni) dampen the linear disparity between the largest (N = 7762) and smallest (N = 287) test sets."
        },
        {
            "title": "C Experimental Setup",
            "content": "Probe Details All transcript probes use 2 layer bi-directional GRU with mean pooling to get transcript level representation. The GRU operates on character vocabulary built from all predicted transcripts. GRU has hidden dimension of 256 and input dimension of 128 with dropout of 0.1. For hidden representation probes we use the last layers hidden representation and attention pool over time to obtain utterance level representation. This is followed by an MLP composed of 2 linear layers. First layers input dimension is the same as the dimension of the model being evaluated. It outputs an embedding half of this size and the final layer outputs single scalar for assessment taks, logits over classes for classification tasks, or unit [x z] vector for geolocation task. MSE loss is employed for regression, cross-entropy for classification and angular error loss (Foley et al., 2024) for geolocation. Hyper-parameters All the experiments can be reproduced via our open-sourced toollkit. We use learning rate of 2e-4 for all hidden representation probes and learning rate of 1e-3 for the cascade probes. We use the validation F1 (for classification), Kentall Tau (for assessment) and error (in km for geolocation) as early stopping metrics with patience of 5 epochs and minimum epochs set to 10. The checkpoint achieving best validation values on these metrics is selected for reporting numbers. Compute spent Each TP probe runs in at most 15 minutes on single 40GB GPU. Each RP probe runs in at most 3 hours on single 40GB GPU. For TP and RP based extinsic evaluations total of around 1k GPU hours were spent to get final numbers. We used almost 1k GPU hours during development phase of the evaluation toolkit as well. Besides, PRiSM supports distributed inference that scales to multiple GPUs and supports VLLM 8. For inference, we utilized around 500 GPU hours including debugging and development costs. Each POWSM-CTC model trains on 4 nodes with 4-80GB GPU each and takes 1.5 days to train, amounting to 600 GPU hours for one run. We can assume another 2k GPU hours for development and experimentation. 8https://github.com/vllm-project/vllm"
        },
        {
            "title": "D Prompts for LALMs",
            "content": "Do NOT include any explanation, markdown, or extra text. PR: Phonetic Transcription (IPA) If uncertain, give your best estimate within the valid System prompt ### Role You are an expert phonetician and linguist specializing in acoustic phonetics. Your auditory perception is calibrated to detect subtle nuances in articulation, stress, and intonation. ### Task Listen to the provided audio clip and transcribe the speech into standard International Phonetic Alphabet (IPA) symbols. ### Guidelines for Accuracy Analyze Context: Implicitly identify the speakers accent/dialect to ensure vowel qualities are accurate. Resolve Ambiguity: If sound is unclear, use your linguistic expertise to determine the single most probable phoneme. Do not provide alternatives. Strict IPA: Use standard IPA notation only. ### Output Format & Schema Adherence Strict Adherence: You must generate the output following the defined schema structure exactly. Pure Data: Return raw data only. Do NOT use Markdown code blocks (e.g., json), and do not include any conversational filler. Field transcription: Must contain EXACTLY ONE string sequence of IPA symbols. No slashes / or brackets []. Example: \"h@\"l@U\" (Correct) / \"/h@\"l@U/\" (Incorrect) Analyze the audio and produce the structured output now. User prompt Please transcribe the attached audio. need clear mapping of the speech to International Phonetic Alphabet (IPA) symbols. DYS-ez: Dysarthria Intelligibility (EasyCall) System prompt ### Role You are an experienced neurologist who assesses dysarthria severity using the Therapy Outcome Measure (TOM). ### Task Listen to the provided audio clip of Italian speech and assess the dysarthria severity level on scale of 0 to 3. ### Severity Scale (TOM-based; 4-class) For each dysarthric speaker, severity was assessed using the Therapy Outcome Measure (TOM). The TOM score ranges from 1 to 5 corresponding to: mild, mild-moderate, moderate, moderate-severe, and severe dysarthria. Use the following 4-class target mapping (score 03): 0: Control (healthy) 1: Mild 2: Mild-moderate or Moderate 3: Moderate-severe or Severe ### Output Format Return JSON with single field score containing an INTEGER from 0 to 3. range. User prompt Assess dysarthria severity using the TOM-based 4class mapping: 0=Control (healthy), 1=Mild, 2=Mildmoderate or Moderate, 3=Moderate-severe or Severe. Output only the integer score (0-3). DYS-ua: Dysarthria Intelligibility (UASpeech) System prompt ### Role You assess speech intelligibility (severity of speech disorder) using listener transcription accuracy based protocol. ### Task Listen to the provided audio clip of English speech and predict the speakers intelligibility category on scale of 0 to 4. ### Severity / Intelligibility Scale (5-class target) Speech intelligibility is used as an overall index of severity of dysarthria for each speaker and is based on word transcription tasks by human listeners. Based on averaged percent accuracy, each speaker is categorized into one of four intelligibility categories: - very low (025%) - low (2650%) - mid (5175%) - high (76100%) Use the following 04 target encoding: 0: Control (healthy) 1: High (76100%) 2: Mid (5175%) 3: Low (2650%) 4: Very low (025%) ### Output Format Return JSON with single field score containing an INTEGER from 0 to 4. Do NOT include any explanation, markdown, or extra text. If uncertain, give your best estimate within the valid range. User prompt Predict the intelligibility category using the 04 mapping: 0=Control (healthy), 1=High (76100%), 2=Mid (5175%), 3=Low (2650%), 4=Very low (025%). Output only the integer score (0-4). CSD-us: Child Speech Disorder (UltraSuite) System prompt ### Role You classify child speech from speech therapy session recordings as either typically developing speech or speech sound disorder speech. ### Task Listen to the provided audio clip and classify the child speaker as either typically developing or having speech sound disorder. Notes: - The child may hesitate, repeat, or make mistakes, and the spoken content may deviate from the prompt. ### Class Mapping 0: Typical (typically developing child) 1: Atypical (child with speech sound disorder) ### Output Format Return JSON with single field class_id containing either 0 (typical) or 1 (atypical). Do NOT include any explanation, markdown, or extra text. User prompt Classify the child speaker: 0=Typically developing, 1=Speech sound disorder. Output only the class_id (0 or 1). L1-arc: L1 Classification (CMU-ARCTIC + L2ARCTIC) System prompt ### Role You are an expert phonetician specializing in inferring speakers linguistic background from English speech. You use segmental and prosodic cues (systematic sound substitutions, vowel/consonant quality, rhythm, stress, intonation) to choose the most likely class. ### Task Listen to the provided audio clip of non-native English speech and predict the speakers native language (L1). ### Class Mapping (class_id native language) 0: Arabic (ar) 1: English (en) 2: Spanish (es) 3: Hindi (hi) 4: Korean (ko) 5: Vietnamese (vi) 6: Chinese/Mandarin (zh) ### Guidelines Focus on segmental cues (vowels/consonants) and systematic substitutions. Focus on prosodic cues (rhythm, stress, intonation). Use ONLY the class mapping above; output EXACTLY ONE class_id. If uncertain, output your single best class_id (no hedging). ### Output Format Return JSON with single field class_id containing an integer from 0 to 6. Do NOT include any explanation, markdown, or extra text. User prompt Listen to this English speech and predict the speakers native language. Output only the class_id. intonation) to choose the most likely class. ### Task Listen to the provided audio clip of English speech and classify the speakers accent into one of 13 accent clusters. ### Class Mapping (class_id accent cluster) Output EXACTLY ONE class_id from the fixed mapping below. 0: AFRICAN_ENGLISH 1: AFROASIATIC_SEMITIC 2: CARIBBEAN 3: CELTIC_ENGLISH 4: EAST_ASIAN 5: GALLO_ROMANCE 6: GERMANIC 7: INNER_CIRCLE_ENGLISH 8: INSULAR_SEA 9: MAINLAND_SEA 10: ROMANCE 11: SLAVIC_BALKAN 12: SOUTH_ASIAN ### Guidelines Focus on segmental cues (vowels/consonants) and systematic substitutions. Focus on prosodic cues (rhythm, stress, intonation). Use ONLY the class mapping above; output EXACTLY ONE class_id. If uncertain, output your single best class_id (no hedging). ### Output Format Return JSON with single field class_id containing an integer from 0 to 12. Do NOT include any explanation, markdown, or extra text. User prompt Listen to this English speech and classify the speakers accent cluster. Output only the class_id (0-12). L2-so: L2 Proficiency (Speechocean762) System prompt ### Role You are an expert rater of non-native English speech. You assign sentence-level accuracy score based on the overall pronunciation quality of the sentence. ### Task Listen to the provided audio clip and rate the sentencelevel accuracy on an integer scale from 0 to 10. ### Sentence-level Accuracy Scoring (010) 9-10: The overall pronunciation of the sentence is excellent without obvious mispronunciation 7-8: The overall pronunciation of the sentence is good, with few mispronunciations 5-6: The pronunciation of the sentence has many mispronunciations but it is still understandable 3-4: Awkward pronunciation with many serious misL1-eda: L1 Classification (EdAcc) pronunciations System prompt ### Role You are an expert phonetician specializing in inferring speakers linguistic background from English speech. You use segmental and prosodic cues (systematic sound substitutions, vowel/consonant quality, rhythm, stress, 0-2: The pronunciation of the whole sentence is unable to understand or there is no voice ### Output Format Return JSON with single field score containing an INTEGER from 0 to 10. Do NOT include any explanation, markdown, or extra text. 17 If uncertain, give your best estimate within the valid range. User prompt Rate the sentence-level pronunciation accuracy on an integer scale of 0-10. Output only the integer score. LID-fl: LID (FLEURS) System prompt ### Role You are an expert phonetician specializing in inferring linguistic background from speech. You use segmental cues (vowel/consonant inventories and realizations) and suprasegmental cues (rhythm, stress, intonation) to choose the most likely class. ### Task Listen to the provided audio clip and identify which of the 24 languages is being spoken. ### Class Mapping (class_id language) Output EXACTLY ONE class_id from the fixed mapping below. 0: Assamese 1: Asturian 2: Persian 3: Filipino 4: Gujarati 5: Hebrew 6: Armenian 7: Igbo 8: Kamba 9: Kabuverdianu 10: Khmer 11: Kannada 12: Sorani-Kurdish 13: Luxembourgish 14: Ganda 15: Lingala 16: Luo 17: Latvian 18: Nepali 19: Northern-Sotho 20: Occitan 21: Pashto 22: Umbundu 23: Wolof ### Guidelines Focus on segmental cues (vowels/consonants) and systematic realizations. Focus on suprasegmental cues (rhythm, stress, intonation) and phonotactics. Use ONLY the class mapping above; output EXACTLY ONE class_id. If uncertain, output your single best class_id (no hedging). ### Output Format Return JSON with single field class_id containing an integer from 0 to 23. Do NOT include any explanation, markdown, or extra text. User prompt Identify the language. Use ONLY the class mapping above and output exactly one class_id (0-23). GEO-v: Speech Geolocation (Vaani) System prompt ### Role You are an expert dialectologist. You infer speakers geographic origin from dialectal cues in speech. ### Task Listen to the provided audio clip and predict speakers geographic location from dialectal features. ### Output Return JSON only with latitude/longitude in decimal degrees: the - { \"lat\": NUMBER, \"lon\": NUMBER } - lat in [-90, 90], lon in [-180, 180] - Do NOT include any explanation or extra text. User prompt Predict the geographic location. Output JSON only: { \"lat\": <deg>, \"lon\": <deg> }. L1 to accent cluster mapping for"
        },
        {
            "title": "EdAcc",
            "content": "The EdAcc corpus contains 41 distinct L1 labels, which we consolidate into 13 accent clusters based on phonological and typological similarity. Grouping criteria include language family (e.g., Sino-Tibetan, Austronesian), vowel inventory size (e.g., 5-vowel Romance languages), prosodic patterns (e.g., syllable-timed vs. stress-timed), and shared phonetic transfer patterns to English (e.g., rhoticity, vowel reduction). Table 6 lists the complete mapping. Algorithm for Vaani-Hi For the GEO-v task, we construct Vaani-Hi, Hindi-belt subset of the Vaani corpus (Ghosh et al., 2025), and release it on Hugging Face. Sampling We focus on 12 Hindi-belt states: Chandigarh, Himachal Pradesh, Delhi, Madhya Pradesh, Jharkhand, Uttarakhand, Bihar, Chhattisgarh, Haryana, Rajasthan, Punjab, and Uttar Pradesh. From each state, we randomly sample up to 4 districts; for each district, we use up to 4 audio shards and take up to 600 utterances per shard (seed 42). Filtering and Labeling We retain only pincodes with more than 450 utterances to ensure sufficient density per location. Each pincode is mapped to latitude/longitude using pincode metadata table; we assign mean coordinates per pincode as the geolocation target. Splitting and Preprocessing Splits are created within each pincode (75%/10%/15% train/- val/test) to avoid location leakage. Audio is re18 EdAcc L1 label Accent cluster Hindi Indian English Urdu Sinhalese SOUTH_ASIAN SOUTH_ASIAN SOUTH_ASIAN SOUTH_ASIAN English Southern British English Mainstream US English South African English INNER_CIRCLE_ENGLISH INNER_CIRCLE_ENGLISH INNER_CIRCLE_ENGLISH INNER_CIRCLE_ENGLISH Scottish English Irish English Spanish Spanish (Mexican) Catalan Italian Portoguese Maltese French Indonesian Bahasa Filipino Tagalog Vietnamese Mandarin Japanese Korean CELTIC_ENGLISH CELTIC_ENGLISH ROMANCE ROMANCE ROMANCE ROMANCE ROMANCE ROMANCE GALLO_ROMANCE INSULAR_SEA INSULAR_SEA INSULAR_SEA INSULAR_SEA MAINLAND_SEA EAST_ASIAN EAST_ASIAN EAST_ASIAN Nigerian English Kenyan English Ghanain English AFRICAN_ENGLISH AFRICAN_ENGLISH AFRICAN_ENGLISH Russian Polish Bulgarian Macedonian Montenegrin Lithuanian Romanian German Dutch Icelandic Arabic Hebrew SLAVIC_BALKAN SLAVIC_BALKAN SLAVIC_BALKAN SLAVIC_BALKAN SLAVIC_BALKAN SLAVIC_BALKAN SLAVIC_BALKAN GERMANIC GERMANIC GERMANIC AFROASIATIC_SEMITIC AFROASIATIC_SEMITIC Jamaican English CARIBBEAN Table 6: Mapping from EdAcc L1 labels (41) to 13 accent clusters used in L1-eda. sampled to 16 kHz and clipped to maximum of 20 seconds. Effect of Thinking Mode on L1-eda"
        },
        {
            "title": "Classification",
            "content": "Figure 5 provides the full confusion matrices for the LALM bias analysis discussed in 6.4. (a) Baseline (F1-score=32.7%) (b) Thinking Mode (F1-score=24.9%) Figure 5: Normalized confusion matrices for Gemini 2.5 Flash on L1-eda (13 accent clusters). Rows denote true labels; columns denote predictions."
        }
    ],
    "affiliations": [
        "CMU",
        "Gwangju Institute of Science and Technology",
        "LMU Munich",
        "NVIDIA",
        "UBC",
        "UC Berkeley",
        "UT Austin"
    ]
}