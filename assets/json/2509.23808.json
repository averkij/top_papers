{
    "paper_title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR",
    "authors": [
        "Fanding Huang",
        "Guanbo Huang",
        "Xiao Fan",
        "Yi He",
        "Xiao Liang",
        "Xiao Chen",
        "Qinting Jiang",
        "Faisal Nadeem Khan",
        "Jingyan Jiang",
        "Zhi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our analysis reveals that at the hidden-state level, exploration and exploitation could be decoupled (Sec. 4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 0 8 3 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "BEYOND THE EXPLORATION-EXPLOITATION TRADEOFF: HIDDEN STATE APPROACH FOR LLM REASONING IN RLVR Fanding Huang1, Guanbo Huang1, Xiao Fan1, Yi He1, Xiao Liang2, Xiao Chen1, Qinting Jiang1, Faisal Nadeem Khan1, Jingyan Jiang3:, Zhi Wang1: 1Tsinghua Shenzhen International Graduate School, Tsinghua University 2University of California, Los Angeles 3Shenzhen Technology University"
        },
        {
            "title": "ABSTRACT",
            "content": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel firstand second-order derivatives, named Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our analysis reveals that at the hidden-state level, exploration and exploitation could be decoupled (Sec. 4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as predictive meta-controller to create synergistic, dualchannel incentive structure. Instead of forcing trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset. Code (cid:128) Project https://github.com/hf618/VERL.git https://hf618.github.io/VERL.github.io"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in Reinforcement Learning with Verifiable Rewards (RLVR) have significantly improved the reasoning abilities of Large Language Models (LLMs). dominant narrative emerging from these recent works (Chen et al., 2025b; Yue et al., 2025; Deng et al., 2025) interprets this progress through the lens of balancing exploration (searching for diverse reasoning paths) and exploitation (refining the most promising known strategies). However, this paradigm is almost exclusively rooted in token-level analysis, where exploration is captured by high-entropy token distributions and exploitation by high-confidence, low-entropy ones. This has inevitably led to the widespread assumption of an inherent trade-off between the two, as models output distribution is seen as unable to be simultaneously uniform and sharp (Agarwal et al., 2025). This token-centric viewpoint, while convenient, introduces significant limitations. Equating exploration with mere token-level entropy faces an intrinsic dilemma (Fu et al., 2025; Qiao et al., 2025; Agarwal et al., 2025): excessively high entropy risks generating incoherent noise, while overly low entropy stifles exploration it aims to encourage. Similarly, defining exploitation via hand-crafted *Equal contribution. Corresponding authors: Jingyan Jiang and Zhi Wang. # jiangjingyan@sztu.edu.cn;wangzhi@sz.tsinghua.edu.cn"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Comparative analysis with the responses of DeepSeek-R1-Distill-Qwen-7B in simpleRL test dataset (Zeng et al., 2025). (a) Traditional metrics for exploitation & exploration are constrained by negative coupling, leading to meandering progress for both capabilities. (b) Our metrics are mutually independent. (c) Training regularization with our metrics demonstrates stronger performance in both exploitation (small K) and exploration (large K). heuristic rewards (Chen et al., 2025a; Li et al., 2025a; Bensal et al., 2025) produces brittle models with poor generalizability as they simply learn to chase surface-level proxies. More related works in Sec. 7. While many works are aware to consider both exploration and exploitation as in Fig. 1a, their continued reliance on token-level metrics invariably traps them in cycle of balancing the tradeoff, instead of doubting its existence. This raises critical question: Is the explorationexploitation trade-off intrinsic to reasoning, or merely an artifact of token-level measurement? To answer this, we move beyond token-level statistics to investigate exploration and exploitation at the more granular hidden states level. To analyze these dynamics, we are the first to apply Effective Rank (ER) in an RL context, and take it to quantify exploration by measuring the semantic diversity of hidden-state representations. To capture exploitation, which we define as the efficient gain of information along reasoning path, we propose two novel derivatives of ER. Effective Rank Velocity (ERV), the first-order change, measures the velocity of this information gain, while Effective Rank Acceleration (ERA), the second-order change, captures the trend of velocity, indicating whether reasoning is accelerating or saturating. Equipped with these tools, we uncover striking result: at the hidden-state level, exploration and exploitation show near-zero correlation (Fig. 1b, bottom). This contrast provides strong evidence that the trade-off is not an inherent property of RLVR for reasoning but an artifact of biased token-level measurements. It reveals that these two capacities are not antagonistic but can, in fact, be decoupled and enhanced simultaneously (Fig. 1c). Furthermore, by grouping questions based on correctness score, Fig. 2 demonstrates that the relationship between exploration and exploitation exhibits consistent pattern, regardless of the modeling granularity (e.g., token-level vs. hidden states level). This result underscores the effectiveness of our proposed method for modeling exploration and exploitation. Building on this core insight, we propose Velocity-Exploiting Rank-Learning (VERL), method that moves beyond the trade-off between the two capacities by directly shaping the RL advantage function using ER and ERV. Instead of acting as switch between the two capacities in lower dimension, VERL functions as tuner synergistically enhances both capacities in higer dimension. Its key innovation is leveraging ERA as meta-control variable, choice justified by our theoretical proof of its remarkable Op1q growth stability (Sec. 3). Specifically, VERL uses ERA as dynamic signal to enhance the training incentives; Specifically, VERL uses ERA to create synergistic, dualchannel incentive structure. Instead of switching between modes, it prospectively shapes the reward to simultaneously encourage exploration (via ER) to preempt overconfidence, while also reinforcing exploitative gains (via ERV) to consolidate the reasoning path. This unique stability makes ERA robust signal to guide training, allowing VERL to simultaneously encourage exploration from"
        },
        {
            "title": "Preprint",
            "content": "(a) Token-level exploration & exploitation (b) Response-level exploration & exploitation Figure 2: We conduct comparative analysis against DeepSeek-R1-Distill-Qwen-7B on the simpleRL test dataset (Zeng et al., 2025), generating four distinct responses for each question via 4 separate rollouts. In (a) and (b), the horizontal axis represents the average correctness, quantifying the models mastery of the question. Notably, both the conventional token-level (a) and our proposed response-level (b) methods for modeling exploration and exploitation exhibit consistent trend. productive-potential states while preventing overfitting to local optima. As result, VERL delivers significant performance gains across diverse models and tasks, achieving up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 benchmark. Contributions. (i) We are the first to probe the exploration-exploitation relationship in the semantically rich hidden-state space. By adopting ER to quantify exploration and proposing novel metrics ERV and ERA for exploitation, we empirically demonstrate that these two capacities are decoupled, moving beyond the conventional token-level trade-off. (ii) We present VERL, method that leverages ERA to manage exploration and exploitation in unified framework, enabling the simultaneous enhancement of both capabilities. (iii) Our extensive experiments demonstrate the efficiency, generality, and versatility of VERL across different RL architectures."
        },
        {
            "title": "2.1 PROBLEM FORMULATION AND NOTATIONS",
            "content": "We frame the training of LLMs for reasoning tasks as an RL problem. The LLM is modeled as policy πθpxq, parameterized by θ, which generates reasoning trajectory for given prompt sampled from distribution Px. The models output is sequence of reasoning steps y0:T py0, y1, . . . , yT q, constrained to maximum length Lmaxpi.e., ă Lmaxq. The quality of this trajectory is evaluated by scalar reward function rpx, yq. The objective is to find the optimal policy πϕ that maximizes the expected reward: ϕ argmax θ ExPx Eyπθpxq rrpx, yqs s.t. ď Lmax. (1) Conventionally, optimizing this objective at the token level is framed as fundamental explorationexploitation trade-off. The policy must explore diverse and potentially novel reasoning pathways to discover high-reward solutions. Concurrently, it must exploit known strategies by reinforcing correct and reliable reasoning patterns that consistently yield high rewards."
        },
        {
            "title": "2.2 REINFORCEMENT LEARNING BASELINE FRAMEWORKS",
            "content": "Proximal Policy Optimization (PPO) from Schulman et al. (2017) is standard RL algorithm that seeks to maximize clipped surrogate objective function. This objective prevents excessively large changes that would destabilize training, which is defined as: LPPOpθq : ExPx,yπθold pyxq min rρtpθqAt, clippρtpθq, 1 ϵlow, 1 ` ϵhighqAts (2) where ρtpθq : πθpytx,yătq πθold pytx,yătq is the probability ratio between the current and old policies, and At is the estimated advantage, often calculated using Generalized Advantage Estimation (GAE) from Schulman et al. (2015), with clipping (hyperparameter ϵ) to mitigate excessive deviation. $ & yÿ % t1 , . - ,"
        },
        {
            "title": "Preprint",
            "content": "Group Relative Policy Optimization (GRPO) from Shao et al. (2024) computes baseline directly from the rewards of multiple trajectories. For given prompt, it samples group of responses, obtains their corresponding rewards tr1, ..., rGu, and normalizes these rewards to compute the advantage for each response: Ai,t : ri meanptrjuG stdptrjuG j1q j1q . (3) GRPO would assign single rule-based reward to the entire output sequence, and the resulting group-relative advantage is uniformly propagated to all tokens, then updated as in Eq. 2."
        },
        {
            "title": "2.3 CHARACTERIZING HIDDEN STATE REPRESENTATIONS",
            "content": "Response Hidden States. LLM would generate responses token by token in an autoregressive manner. The token yt output at step represents the current explicit state, while the corresponding output in the intermediate layers is referred to as the hidden state zt. As the sequence of explicit states forms the final response, simultaneously, the hidden states tztuT t1, ordered by their output sequence, collectively form the hidden states matrix RT ˆD, where is the output length and is the feature dimension, representing the semantic trajectory. To align with the semantic space, we focus on the hidden state of the final layer in this paper. Dataset Hidden States. Following the definition in Skean et al. (2025), for dataset containing prompts, after obtaining single vector representation for the i-th response by averaging its token Ti hidden states zi 1 t1 zi,t, we stack these mean embeddings in the dataset hidden states Ti matrix RN ˆD, to represent the overall semantic distribution of the entire dataset. ř"
        },
        {
            "title": "3.1 STATIC METRIC: EFFECTIVE RANK (ER)",
            "content": "According to Roy & Vetterli (2007), the Effective Rank (ER), which is denoted by erankpZq for response, is computed based on the normalized singular values of its non-padding hidden states Z. Let σj be the j-th singular value of Z, and pj σjř be the normalized singular values. The ER is σk then given by: ff ÿ ER : erankpZq exp pj logppjq . (4) To quantify models reasoning breadth, we treat exploration as measurable semantic property. Our primary metric for this is ER, which measures the effective dimensionality of the hidden-state space model occupies during response. high ER signals that the model is leveraging rich and diverse set of internal features, which is direct signature of exploratory behavior. low ER, in contrast, points to collapsed, simpler representation, indicating the model is not exploring widely. By capturing this dispersion of embeddings, ER provides more nuanced view of exploration than conventional rank, which merely counts dimensions without considering their diversity. Theorem 3.1. Suppose we have matrix of embeddings RT ˆD. Then the ER of is lower bound of conventional rank of Z: 1 ď erankpZq ď rankpZq ď mintT, Du. (5) Remark 3.2. Conventional rank offers discrete count of available dimensions but fails to capture the geometric complexity essential for true exploration. In contrast, ER provides nuanced, continuous measure of effective dimensionality. In reasoning, this distinction is critical: conventional rank may count many potential paths, but ER reveals how uniformly the model is actually exploring them. high ER reflects more uniform distribution, signaling broader and more effective exploration of the solution space."
        },
        {
            "title": "3.2 DYNAMIC METRICS: EFFECTIVE RANK VELOCITY (ERV) AND ACCELERATION (ERA)",
            "content": "In this section, we develop temporal higher-order metrics, termed ERV and ERA, to characterize the dynamics of policys information gain together. Corresponding to the first and second-order temporal differences of representational metric, these tools measure how the quality of hidden states evolves, revealing whether the reasoning process is expanding, stabilizing, or saturating."
        },
        {
            "title": "Preprint",
            "content": "Definition 3.3. (First-Order Temporal Difference: ERV) To quantify the rate of change for given metric , such as the ER or conventional rank of the hidden states matrix, we define the firstorder temporal difference, denoted p1q . This metric captures the velocity of information gain by measuring how the metrics value at given step deviates from its historical average. Let mt be the value of metric computed on the token sub-sequence from the start to position t. For sequence of length and difference stride s, let the set of evaluation time steps be ts, 2s, . . . , Ksu, where tpT 1q{su. The overall first-order difference is defined as: p1q :"
        },
        {
            "title": "1\nK ´ 1",
            "content": "Kÿ j2 δjs, where δjs : mjs 1 1 j1ÿ k1 mks. (6) Our primary metric for exploitation is ERV, which is designed to capture the rate of information gain. It is the average of instantaneous differences (δjs), where each difference contrasts the complexity of the current token chunk with the cumulative average of all preceding ones. This formulation directly operationalizes our definition of exploitation: large ERV demonstrates that the model is successfully enriching its representation at rate that outpaces its historical trend, signifying deepening and productive line of inquiry. Conversely, small ERV signals that exploitation of the current path is becoming less effective. Definition 3.4. (Second-Order Temporal Difference: ERA) To measure the rate of change of the velocity, we define the second-order temporal difference p2q , which represents the acceleration of the metrics evolution. It reveals whether the process of representation formation is speeding up or stabilizing. It is computed as the average change between consecutive instantaneous differences: p2q :"
        },
        {
            "title": "1\nK ´ 2",
            "content": "Kÿ rδjs δpj1qss. j3 (7) positive p2q signifies an accelerating growth rate, indicating that the diversification of the representation is speeding up. negative value suggests this growth is decelerating, implying that the representations quality is approaching stability or saturation."
        },
        {
            "title": "3.3 SCALING PROPERTIES OF REPRESENTATIONAL DYNAMICS",
            "content": "In the preceding sections (Sec. 3.1 and 3.2), we introduced metrics for analyzing the hidden states of individual responses. We now analyze the scaling properties of these dynamics at two distinct levels of granularity: across an entire dataset as function of its size (N ), and within single reasoning trajectory as function of its length (T ). The following proposition provides unified theoretical model for both scenarios. Proposition 3.5. Assume hidden-state matrix is composed of approximately orthogonal row vectors. The Effective Rank (ER) and its first-order difference (ERV) scale linearly with k, such that ER Opkq and p1q ER Opkq. The second-order difference (ERA) is independent of k, with scaling order of p2q Remark 3.6. This proposition offers dual interpretation of how our metrics scale under ideal conditions: At the dataset level, represents the number of questions . The proposition implies that as dataset grows with semantically distinct responses (approaching orthogonality), its overall representational diversity (ER) should increase proportionally. The constant acceleration (ERA) suggests stable, predictable growth pattern for the datasets semantic volume. At the response level, represents the sequence length . The proposition suggests that for an ideal reasoning process where each step contributes novel information (making token embeddings approach orthogonality), the trajectorys semantic complexity (ER) and information-gain velocity (ERV) should also grow linearly with its length. In this context, constant ERA becomes signature of robust and non-saturating reasoning process. ER Op1q."
        },
        {
            "title": "4 DECOUPLING EXPLORATION AND EXPLOITATION IN REASONING",
            "content": "In this section, we first investigate the changing trends of the hidden states matrix rank (both ER and conventional rank) during regular RL training. Specifically, we utilized the Qwen (Hui et al., 2024) and Llama (Dubey et al., 2024) models for our experiments, employing GRPO (Shao et al.,"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Response-level metrics during GRPO post-training, smoothed with 10-step rolling window. Metrics are shown for the Overall batch, as well as for subsets of Correct and Incorrect samples. The rightmost column displays the average Critic Score (reward) and Response Length per batch. Figure 4: Visualization of dataset-level metrics during GRPO post-training. The figure compares Traditional metrics with our proposed metrics. Also shown are the Validation Score and sample Correctness, both averaged over the validation dataset. 2024) reinforcement learning paradigm. The training dataset followed the configuration in Zeng et al. (2025), which comprises 8k hard-level 3 to 5 mathematical problems from MATH datasets, each accompanied by verifiable reference answer."
        },
        {
            "title": "4.1 ANALYSIS OF RESPONSE-LEVEL METRICS",
            "content": "During each training step, we quantitatively analyzed the representational dynamics of hidden states within that batch as depicted in Fig. 3, and provided more and diverse details in App. E.1. Semantic space of hidden states move beyond the exploration-exploitation trade-off towards stable enhancements. While RL consistently improves performance, it interacts differently with distinct base models, evidenced in the divergent trends of the ER (first column in Fig. 3), which measures the total information within response. For instance, the Qwen model exhibits an increasing ER, suggesting more exploratory reasoning, whereas the one of Llama model decreases, indicating more concise, exploitative reasoning. Despite these differences in exploratory behavior, the ERV (second column) demonstrates consistent upward scaling trend for all models, which suggests while the models intrinsic exploratory tendencies differ, RL fine-tuning universally enhances their exploitation capabilities by consistently accelerating the rate of information gain. ERA distinguishes correct reasoning. For both the zero-order metric (ER) and the first-order metric (ERV), incorrect reasoning paths consistently score higher than correct ones. This suggests that excessive exploration (high ER) with new excessive information (high ERV) will potentially derail the reasoning process and lead to incorrect answers. Conversely, for the second-order metric (ERA), correct reasoning trajectories consistently exhibit higher values, which implies the acceleration of information gainthe ability to increasingly speed up the representational evolutionis the key to guide the policy towards correct solution, distinguishing robust reasoning from flawed exploration."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Overview of VERL. Exploration is quantified by computing the ER of the rolling-done hidden states via SVD, while exploitation is captured through EMA-smoothed first-order difference (ERV) on per-step rolling hidden state and extended to second-order difference (ERA). Finally, exploration and exploitation are adaptively integrated to derive the auxiliary advantage."
        },
        {
            "title": "4.2 ANALYSIS OF DATASET-LEVEL METRICS",
            "content": "Following the framework established in Sec. 2, we extend our analysis from the response level to the entire validation dataset. By computing the dataset hidden states matrix Z, we examine its zero-, first-, and second-order rank dynamics to understand how the policys overall representational space evolves. The trends are visualized in Fig. 4. While key experiments are shown here, we refer the reader to App. E.2 for more diverse range of studies. Policy optimization correlates with expanding dataset-level diversity. Across the training process, we observe strong positive correlation between performance metrics (accuracy and critic score on the validation set) and the dynamics of dataset-level ER. As the model improves, the zeroorder erankpZq and its firstand second-order differences consistently scale up. This indicates that as the policy is updated, it develops more diverse and complex repertoire of reasoning strategies for the same set of problems. The increasing ERV and ERA suggest the model becomes progressively more efficient at navigating and expanding this richer semantic space to discover correct solutions. ER reveals refinement beyond the limits of conventional rank. During late-stage training, plateauing conventional rank suggests the model has settled on fixed number of linearly independent reasoning directions. Yet, simultaneously rising ER points to more subtle optimization. This trend reveals that the model is improving the quality of its existing solution space by making the magnitudes of these directions more uniform. In essence, instead of finding new pathways, the model learns to utilize its established ones more equitably, reducing representational redundancy and fostering more sophisticated and distributed reasoning capability."
        },
        {
            "title": "5 VELOCITY-EXPLOITING RANK-LEARNING (VERL)",
            "content": "Building upon the insights from Sec. 3 and the empirical observations in Sec. 4, we find that conventional RL objectives overlook the intrinsic hidden-state dynamics that more faithfully characterize exploration and exploitation. This oversight may lead to inefficient training, where policies either wander in unproductive exploration or collapse prematurely to suboptimal reasoning paths. To address this, we propose novel method named Velocity-Exploiting Rank-Learning (VERL), which refines advantage by incorporating the nuanced dynamics of hidden states, enabling simultaneous enhancement of exploration and exploitation capacities."
        },
        {
            "title": "5.1 STABLE REPRESENTATION DEVIATION INDICATOR",
            "content": "Concretely, we first formalize the representational metrics, letting tM0, M1, M2u denote the set of metrics derived from the hidden states, where M0 is the 0-order ER, M1 : p1q is its firstorder temporal difference (ERV), and M2 : p2q is its second-order temporal difference (ERA). To create stable guidance signal, having already computed scalar values tm0, m1, m2u for each trajectory, we normalize these values against their historical trends by maintaining an Exponential Moving Average (EMA) µk for each metric Mk. The relative deviation for each metric is: mk µk µk ` ϵ t0, 1, 2u, dk : (8) ,"
        },
        {
            "title": "Preprint",
            "content": "where ϵ is small constant for numerical stability. This deviation dk quantifies how the current trajectorys representational structure diverges from the policys recent average behavior."
        },
        {
            "title": "5.2 BEYOND TRADE-OFF FOR EFFICIENT TRAINING VIA ERA",
            "content": "Our analysis in Sec. 3.1 and 3.2 reveals that ER (M0) and ERV (M1) are effective proxies for exploration and exploitation, respectively. Crucially, these two metrics are also almost independent of each other, as shown in Fig. 1c bottom. This decoupling is key, as it allows us to combine them into single objective to enhance both capabilities simultaneously. M1 measures the confidence of the current step as analysis above, so the subtraction of M1, namely ERA (M2), would predict the evolution of confidence in subsequent steps. Meanwhile, theoretical analysis (Prop. 3.5) and empirical observations  (Fig. 4)  indicate that M2 remains approximately constant across trajectories. Thus, M2 can serve as meta-level signal to guide training. An increasing ERV indicates that the model is acquiring progressively more informative evidence, reflecting its growing confidence. However, our preliminary experiments reveal that trajectories exhibiting excessively high confidence often achieve high in-domain performance (as evidenced by the results on the MATH dataset in Tab. 1, where performance suffers without ERAs dynamic unification or when using simple 50/50 sum β = 0.5.) while compromising out-of-domain generalization (across most datasets), suggesting severe overfitting. This implies that overconfident trajectories reduce the opportunity to learn from less confident yet potentially informative samples. To mitigate this, we employ M2 as the predictive signal to combine exploration and exploitation, strategically encouraging exploring lower-confidence samples when trajectories exhibit excessively high future confidence, thereby enhancing training efficiency and robustness. Specifically, we define two orthogonal unit vectors of the weights first, an exploration-focused vector wexplore r1, 0s, which targets M0, and an exploitation-focused one wexploit r0, 1s of M1. The dynamic weight wdyn is interpolated by the relative deviation of the second-order metric d2: wdyn : β wexplore ` p1 βq wexploit, where β : sigmoidpd2q. (9) The interpolation coefficient β is adaptively determined by the second-order metric d2 through sigmoid mapping. high M2 pd2 \" 0q means overconfidence in the future, risking overfitting to in-domain patterns; thus, VERL increases β to favor the exploration profile M0. In contrast, when M2 is low pd2 ď 0q, namely limited confidence and reasoning saturation, VERL decreases β to emphasize the exploitation profile M1. As M2 typically fluctuates around zero, VERL jointly enhances exploration and exploitation. The final auxiliary advantage Φ is defined as: Φ : wdyn,0 tanhpd0q ` wdyn,1 tanhpd1q, (10) where wdyn,0 and wdyn,1 are the first and second entries of the dynamic weight vector wdyn, respectively. The tanh function bounds the magnitude of Φ while preserving its sign, thereby stabilizing training. This formulation rewards trajectories that exceed the historical average and penalizes those that fall short, guiding the policy with adaptive reasoning dynamics while mitigating risks of stagnation and overconfidence."
        },
        {
            "title": "5.3 ADVANTAGE SHAPING VIA REPRESENTATIONAL DYNAMICS",
            "content": "Our core idea is to refine the learning signal by injecting an auxiliary term directly into the advantage function (termed as Advantage Shaping). Let Ap0q be an original advantage estimate from policy gradient algorithm (e.g., GRPO or PPO with GAE), and let Φi be the sequence-level auxiliary term for trajectory as defined in Sec. 5.2. We define the shaped advantage ˆA as follows: ˆ ˆA : Ap0q ` min maxp0, Φiq, , Ap0q κ (11) where κ is clipping factor for stable training. This shaping mechanism can apply seamlessly to different advantage structures. For GRPO, Ap0q is the sequence-level advantage, and the shaping is applied once per trajectory. For PPO with GAE, Ap0q is the per-token advantage estimation Ap0q , and the sequence-level term Φi is applied to each tokens advantage within that trajectory. In both cases, this reshaped advantage ˆA replaces the original estimation in the clipped surrogate objective function, directly influencing the gradient to guide policy optimization. More details in Algorithm 1."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Pass@1 performance with variant β. Adapted β denotes β : sigmoidpd2q. In this paper, all results in the table are reported in percentage (%), with Bold indicating the best performance. Training Strategy Score Avg In Domain Out of Domain Hard Problems MATH MATH500 Avg Gaokao CN Middle School CMATH Avg AIME24 AIME25 AMC23 AMC24 Avg GRPO GRPO+VERL (β 0.5) GRPO+VERL (Adapted β) 0.36 0.38 0.38 51.4 51.2 50.9 46.2 47.2 51.2 48.80 49.20 51.05 23.7 21.2 22. 28.7 36.6 38.6 28.3 38.7 46.2 26.90 32.17 35.90 3.3 10.0 13.3 0.0 0.0 6.7 27.5 27.5 20. 8.9 8.9 11.1 9.93 11.60 12.78 Table 2: Performance comparison of models on mathematical reasoning benchmarks (Pass@1). + GRPO and + PPO denote RL fine-tuning by GRPO and PPO framework respectively. w/ VERL. indicates incorporating our VERL with original RL type. represents the performance contrast between original RL method and its VERL variant. See App. E.3 for full details. Model AIME24 AIME25 AMC23 AMC24 ASDiv Carp En CMATH Gaokao 2024 Gaokao 2024 Mix Gaokao MathCloze GSM8K MAWPS Olympiad Bench SVAMP TabMWP Avg. Llama-3.2-3B-Instruct + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Qwen2.5-7B + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO 0.0 3.3 13.3 +10.0 10.0 10.0 +0.0 6.7 10.0 13.3 +3.3 6.7 10.0 +3.3 0.0 0.0 6.7 +6.7 3.3 3.3 +0.0 0.0 6.7 10.0 +3.3 3.3 6.7 +3.3 25.0 27.5 25.0 -2.5 22.5 25.0 +2.5 45.0 55.0 50.0 -5.0 50.0 52.5 +2. 11.1 8.9 11.1 +2.2 13.3 11.1 -2.2 15.6 26.7 28.9 +2.2 33.3 33.3 +0.0 74.6 88.8 89.3 +0.5 87.9 88.7 +0.8 91.4 94.8 95.0 +0.2 94.9 94.8 -0.1 26.5 45.0 45.4 +0.4 46.4 46.0 -0.4 55.8 60.2 60.8 +0.6 59.6 60.0 +0. 10.2 28.3 46.2 +17.9 21.2 30.7 +9.5 86.7 91.7 90.7 -1.0 89.8 90.3 +0.5 14.3 21.4 14.3 -7.1 7.1 14.3 +7.2 42.9 14.3 35.7 +21.4 28.6 28.6 +0.0 14.3 20.9 22.0 +1.1 16.5 19.8 +3.3 33.0 34.1 35.2 +1.1 31.9 34.1 +2. 6.8 23.7 22.9 -0.8 20.3 27.1 +6.8 49.2 64.4 69.5 +5.1 63.6 66.9 +3.3 66.6 80.7 81.7 +1.0 81.4 82.9 +1.5 85.8 90.2 89.2 -1.0 89.1 90.2 +1.1 86.9 96.0 96.0 +0.0 95.5 95.7 +0.2 95.4 97.6 97.7 +0.1 97.3 97.8 +0. 12.7 16.7 17.6 +0.9 17.8 17.3 -0.5 25.8 36.1 35.4 -0.7 36.1 36.1 +0.0 74.1 87.7 87.8 +0.1 86.8 85.8 -1.0 88.5 92.8 92.9 +0.1 92.8 92.5 -0.3 41.4 71.7 72.3 +0.6 71.0 71.3 +0.3 82.8 91.3 91.9 +0.6 90.8 90.6 -0. 31.0 41.4 43.4 +2.0 40.1 41.9 +1.9 53.6 57.7 59.8 +2.1 57.9 59.0 +1.1 Table 3: Performance comparison of instruction-tuned models under diverse decoding settings (Pass@k). For full details, please refer to App. E.4. Model MATH500 (Pass@16) AMC23 (Pass@128) AMC24 (Pass@128) AIME24 (Pass@256) AIME25 (Pass@256) Llama-3.2-3B-Instruct + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Qwen2.5-7B + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO 79.8 80.2 80.6 +0.4 82.2 82.4 +0.2 90.6 90.8 91.4 +0.6 91.2 91.4 +0. 93.5 95.4 95.7 +0.3 94.5 94.7 +0.2 98.4 97.8 98.3 +0.5 98.6 98.0 0.6 51.5 60.6 59.0 1.6 57.0 57.8 +0.8 73.7 78.3 79.0 +0.7 74.3 74.4 +0.1 40.0 40.0 50.0 +10.0 46.7 46.7 +0.0 60.0 56.7 63.3 +6.6 53.3 56.7 +3. 30.0 30.0 36.7 +6.7 36.7 40.0 +3.3 60.0 50.0 60.0 +10.0 56.7 66.7 +10.0 Avg. 58.96 61.24 64.40 +3.16 63.42 64.32 +0.90 76.54 74.72 78.40 +3.68 74.82 77.44 +2."
        },
        {
            "title": "6.1 EXPERIMENT SETTINGS",
            "content": "(i) Dataset. The same datasets as in Sec. 4 are used. (ii) Reward. Our rule-based reward function assesses mathematical correctness and boxed{} formatting. Correct answers receive +1.0 reward if formatted, and +0.5 if not. Incorrect answers are penalized with -0.5 if formatted and -1.0 otherwise. (iii) Training. Based on verl (Sheng et al., 2025) and vLLM (Kwon et al., 2023) framework, we set batch size as 48, generating 4 rollouts per prompt for GRPO and 1 rollout for PPO, and set the maximum length Lmax to 1536. More details of the experiments are provided in App. D."
        },
        {
            "title": "6.2 MAIN RESULTS",
            "content": "VERL Generalizes across Multiple Benchmarks of Varying Difficulty. As summarized in Tab. 2 (Full details in App. E.3), VERL leads to consistent performance gains across benchmarks of different difficulty levels, ranging from elementary school problems (e.g., ASDiv) to Olympiad-level challenges (e.g., OlympiadBench). The improvements are particularly pronounced on benchmarks that demand multi-step reasoning rather than simple arithmetic. VERL achieves up to 21.4% and 10.0% absolute accuracy improvements on Gaokao 2024 and AIME24 (in Tab. 2), respectively. VERL Generalizes across RL Algorithms and Base Models. VERL is plug-and-play method that can be integrated with different RL algorithms to enhance their performance. As shown in Tab. 2, applying VERL to GRPO and PPO improves the average results on 15 benchmarks for both the Llama and Qwen series, demonstrating its strong generalization ability. Gains in Both Exploration and Exploitation. As shown in Tab. 3 (Full details in App. E.4), VERL yields larger improvements on Pass@k (a measure of exploration) than on Pass@1 (a measure of exploitation), particularly on more challenging benchmarks. Since Pass@1 reflects exploitation and"
        },
        {
            "title": "Preprint",
            "content": "(a) Different Strides (s). (b) Different advantage clip (κ). (c) Different shaping signals. Figure 6: Comparison of various hyperparameters with Llama-3.2-3B-Instruct. Pass@k reflects exploration, the combined results of Tab. 2 and Tab. 3 demonstrate that VERL effectively enhances both abilities. For detailed case study, see Sec. F."
        },
        {
            "title": "6.3 ABLATION ANALYSES",
            "content": "We conduct ablation studies on VERLs key hyperparameters: the stride length (s) for temporal dynamics, the advantage clipping factor (κ), and the composition of the auxiliary shaping signal. Analysis of Stride psq. The stride determines the granularity for calculating temporal difference metrics. As depicted in Fig. 6 (a), VERLs performance improvement is robust across various stride values, indicating the underlying signal is not overly sensitive to sampling frequency. We find that 40 yields optimal validation rewards, striking an effective balance between capturing significant temporal shifts and avoiding noise from minor token-level fluctuations. Analysis of Advantage Clip pκq. The advantage clipping factor κ stabilizes training by ensuring our auxiliary term acts as refinement rather than dominant signal. It constrains the shaping bonus to fraction of the original advantage, preventing it from overpowering the primary task reward. The results in Fig. 6 (right) show that VERL consistently enhances performance for all tested values of κ underscoring its stability. Optimal performance is achieved at κ 2, which provides sufficiently strong and well-proportioned signal to guide the policy without destabilizing the learning process. Analysis of Shaping Signals pΦq. Compared to the full formulation in Eq. 10, using only the exploration-related term prevents the model from exploiting high-reward trajectories, leading to earlier bottlenecks and inferior final performance. In contrast, using only the exploitation-related term yields higher initial returns but quickly plateaus due to insufficient exploration. When combining both terms, the model achieves more stable training and superior final performance."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Reinforcement Learning with Verifiable Rewards. To unlock the full reasoning potential of Large Language Models (LLMs), Reinforcement Learning with Verifiable Rewards (RLVR) has become prominent training paradigm. This approach was notably employed by DeepSeek-R1Zero (Guo et al., 2025) and , which executes complex reasoning processes through actions such as reflection and validation. Following the success of DeepSeek-R1, significant body of research has investigated the efficacy of RLVR on popular open-source LLMs, including Qwen (Yang et al., 2024), Mistral (Jiang et al., 2024), and LLaMA (Dubey et al., 2024). This has fostered an optimistic view that RLVR can not only enhance existing model capabilities but also enable the acquisition of novel reasoning knowledge, facilitating path toward continuous self-improvement (Zeng et al., 2025; Yu et al., 2025). RLVR training has been shown to grant LLMs controllable output length for efficient inference (Yan et al., 2025; Cheng et al., 2025b), deepen their reasoning pathways (Bensal et al., 2025), mitigate their weaknesses (Liang et al., 2025a;b), enable the use of external tools (Rainone et al., 2025; Jin et al., 2025), and even facilitate unsupervised reasoning (Zuo et al., 2025). However, Some studies (Yue et al., 2025) argue that while RLVR significantly improves the confidence and reliability of model reasoning, it may inadvertently constrain the models exploratory capacity. The core of this issue lies in RLVRs optimization objective: maximizing expected rewards. This objective function inherently biases the policy gradient toward reinforcing known trajectories that lead to high rewards (i.e., exploitation), while suppressing the exploration of unknown paths that may offer potentially higher returns but also carry greater risk (i.e., exploration). Consequently, the outputs of RLVR-optimized models often remain confined within the sampling distribution of the base model, suggesting the paradigm excels at refining existing knowledge rather than generating new knowledge. This trade-off between exploration and exploitation constitutes central challenge in the contemporary RLVR landscape."
        },
        {
            "title": "Preprint",
            "content": "Exploration and Exploitation in Reinforcement Learning. Recent perspectives (Wang et al., 2025; Cui et al., 2025) on the exploration-exploitation dilemma have predominantly been shaped by analyses at the token level, focusing on the prediction distribution over the vocabulary. From this viewpoint, higher entropy in the token-level predictionindicating more uniform distribution over the next tokenis interpreted as sign of greater exploratory behavior, as it suggests capacity for more diverse responses. This has led to the adoption of techniques (Deng et al., 2025; Cheng et al., 2025a) such as entropy regularization to explicitly encourage the policy to explore novel reasoning paths. Conversely, lower entropy in the token-level prediction is taken to signify higher model confidence in its reasoning chain, thus representing strong exploitation. Subsequent work (Fu et al., 2025) has also utilized metrics derived from the top-k probabilities of the token prediction to quantify confidence. For instance, some approaches (Damani et al., 2025; Qiao et al., 2025) leverage the models internal confidence signals to dynamically evaluate and filter the quality of reasoning steps, while others have employed high confidence as feedback signal to enable unsupervised reinforcement learning (Li et al., 2025b). Ultimately, however, these confidence-based metrics are not fundamentally different from entropy. This token-level standard of measurement introduces an endogenous contradiction: classifying behavior as either exploratory or exploitative requires the introduction of prior assumptions, practice that is disadvantageous for LLM research. In this paper, we depart from this paradigm. We shift the analysis from the token level to the semantic space at the response level. This approach allows us to decouple the intertwined elements of exploration and exploitation, aiming to achieve simultaneous enhancement of both during reinforcement learning."
        },
        {
            "title": "8 CONCLUSION",
            "content": "We challenge the conventional exploration-exploitation capacities trade-off in LLM reasoning blamed on token-level analysis and shift focus to their decoupled relation in hidden-state representations (measured as semantic diversity and information gain velocity, respectively). We introduce ER, ERV and ERA to quantify the dynamics of semantic complexity, with ERA as stable indicator distinguishing correct from flawed reasoning. We further propose Velocity-Exploiting Rank-Learning (VERL) method, which uses ERA as meta-controller to adaptively shape the advantage function, moving beyond the trade-off for simultaneous enhancement. Extensive experiments validate VERLs superior out-of-domain generalization and performance on complex reasoning tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, and Waseem AlShikh. Reflect, retry, reward: Self-improving llms via reinforcement learning. arXiv preprint arXiv:2505.24726, 2025. Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu Cheng. Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning. arXiv preprint arXiv:2506.04207, 2025a. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025b. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025a. Zhengxiang Cheng, Dongping Chen, Mingyang Fu, and Tianyi Zhou. Optimizing length compression in large reasoning models. arXiv preprint arXiv:2506.14755, 2025b. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021."
        },
        {
            "title": "Preprint",
            "content": "Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806, 2025. Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng Gao, Wayne Xin Zhao, and Ji-Rong Wen. From trial-and-error to improvement: systematic analysis of llm exploration mechanisms in rlvr. arXiv preprint arXiv:2508.07534, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting In Proceedings of the 62nd agi with olympiad-level bilingual multimodal scientific problems. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pp. 11521157, 2016. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pp. 611626, 2023. Junyan Li, Wenshuo Zhao, Yang Zhang, and Chuang Gan. Steering llm thinking with budget guidance. arXiv preprint arXiv:2506.13752, 2025a. Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan Oseledets. Confidence is all you need: Few-shot rl fine-tuning of language models. arXiv preprint arXiv:2506.06395, 2025b."
        },
        {
            "title": "Preprint",
            "content": "Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, and Weizhu Chen. Sws: Self-aware weakness-driven problem synthesis in reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.08989, 2025a. Xiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, and Weizhu Chen. Beyond pass@ 1: Self-play with variational problem synthesis sustains rlvr. arXiv preprint arXiv:2508.14029, 2025b. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In ICLR, 2023. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975984, 2020. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 20802094, 2021. Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, Fandong Meng, Jie Zhou, Ju Ren, and Yaoxue Zhang. Concise: Confidence-guided compression in step-by-step efficient reasoning. arXiv preprint arXiv:2505.04881, 2025. Corrado Rainone, Tim Bakker, and Roland Memisevic. Replacing thinking with tool usage enables reasoning in small language models. arXiv preprint arXiv:2507.05065, 2025. Olivier Roy and Martin Vetterli. The effective rank: measure of effective dimensionality. In 2007 15th European signal processing conference, pp. 606610. IEEE, 2007. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint dimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by layer: Uncovering hidden representations in language models. arXiv preprint arXiv:2502.02013, 2025. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model pass chinese elementary school math test? CoRR, 2023. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Hang Yan, Fangzhi Xu, Rongman Xu, Yifei Li, Jian Zhang, Haoran Luo, Xiaobao Wu, Luu Anh Tuan, Haiteng Zhao, Qika Lin, et al. Mur: Momentum uncertainty guided reasoning for large language models. arXiv preprint arXiv:2507.14958, 2025."
        },
        {
            "title": "Preprint",
            "content": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. CoRR, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. Beichen Zhang, Kun Zhou, Xilin Wei, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive math reasoning. Advances in Neural Information Processing Systems, 36:2357023589, 2023. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "C Details of Theorems",
            "content": ". C.1 Proof of Theorem 3.1 . C.2 Proof of Proposition 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Implementation Details",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . D.1 Training and Evaluation Details D.2 Efficient Incremental Computation of Higher-Order Metrics . . . . . . . . . . . . D.3 Time Overhead of VERL Training . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E More Experiments",
            "content": "E.1 Analysis of Response-Level Hidden States . . . . . . . . . . . . . . . . . . . . . . E.2 Analysis of Dataset-Level Hidden States . . . . . . . . . . . . . . . . . . . . . . . E.3 Detailed Analysis of Pass@1 Performance . . . . . . . . . . . . . . . . . . . . . . E.4 Detailed Analysis of Pass@k Performance . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Case Study",
            "content": "F.1 Case Study For Pass@1 Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Case Study For Pass@16 Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 17 17 17 18 20 20 20 21 22 22 23 26 28 28"
        },
        {
            "title": "A NOTATIONS",
            "content": "Symbol π Ai,t Ap0q ˆA zt zi Zc Z1:n piq Mi mt ϵhigh{low ϵ rj wexplore{exploit{dyn wdyn,i rankpq{erankpq SVDpq δpiq yt yi:j µk λjpq β θ ϕ Φ Lmax Px ρt pj LPPOpq σj dk κ Hpq qi Ut Description Large language model policy Group-relative advantage for the t-th token in the i-th response in group Original advantage estimation Reshaped advantage value Hidden state corresponding to the t-th step of the output token Single vector representation for the i-th response by averaging its token hidden states Mean-centered hidden state matrix Dataset-level hidden states matrix formed by the first prompts Response-level hidden states matrix The i-order temporal difference for metric Set of metrics derived from the hidden states Ti-order temporal difference of ER, exactly the different metrics Value of metric computed on the token sub-sequence from the start to position Hyperparameter for the upper/lower bound used for clipping Small constant for numerical stability Reward of the j-th response Exploration-focused profile/Exploitation-focused profile/Dynamic-weighted profile The i-th scalar of wdynamic Conventional rank/Effective rank function function to calculate the singular values Instantaneous i-Order Difference for step The stride for effective rank velocity calculation The t-th step (token) of the models response Sequence of reasoning steps from to Exponential Moving Average for metric Mk The j-th eigenvalues of the given matrix Interpolation coefficient for VERL training Size of sampled group in GRPO Large language model policys parameter The parameter corresponding to the optimal policy Auxiliary advantage Time step Output length The maximum length of models output Prompt Sample times per prompt Distribution of prompts Probability ratio between the current and old policies for t-step of the output The j-th normalized singular values The optimization objective for PPO applied to policy Feature dimension of hidden states The size of the dataset The j-th singular value of matrix Singular value distribution Set of time steps Deviation for metric Mk Advantage clipping factor Performance difference between the baseline RL method and its VERL variant Shannon entropy function The i-th row of the dataset matrix Gram matrix of dataset matrix Uncentered Gram matrix"
        },
        {
            "title": "B ALGORITHM",
            "content": "Algorithm 1 VERL: Training 1: Input: txiuN prompt S. i1, prompt xi, policy model πθ, hidden-state dimension D, sample times per 2: Parameters: EMA factor γ, relative deviation stabilizing factor ϵ ! 1, RL fine-tuning stabilizing factor κ. 3: Initialize: Randomly initialize policy parameters πθ, historical averages of metrics µER µERV µERA 0, exploration capacity profilewexplore r1, 0s, exploitation capacity profile wexploit r0, 1s. for xi do: 0 Ð H, Ð 1 4: Output: well-trained policy model πθ. 5: repeat 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: for 1 to do: yi 0 Ð xi, Zi repeat πθpyi t, zi yi t1q t1; yi yi yi Ð ts RtˆD t1; zi Ð rZi Zi Ð SVDpZi σi tq Ð σi ř σi j,t Ð σi pi j,t{ j,t ř eranki j,t log pi Ð exp j,t If ą 1 then: δERV,t Ð eranki 1 t1 If ą 2 then: δERA,t Ð δERV,t δERV,t1 Ð ` 1 pi ř t1 k1 eranki // Pick sample from dataset // Rolling times for one sample // Generation process // Concatenate token sequence // 1 is the final timestep while rolling done // Calculating ER metric // Calculating ERV metric // Calculating ERA metric k, tER, ERV, ERAu ERV t1 t1 t2 δt t1 t3 δt until rolling done the sentence; Ai origin Ð base RL evaluating on yi ER Ð eranki mi ř t1 ERV Ð 1 mi ř t2 ERA Ð 1 mi ERA t3 µk Ð γ µk ` p1 γqmi Ð mi kµk di µk`ϵ , tER, ERV, ERAu βi Ð sigmoidpdi wi wi wi Φi Ð wi ˆAi Ð Ai ERAq dyn Ð βiwexplore ` p1 βiqwexploit dyn,ER Ð the first scalar value of wi dyn,ERV Ð the second scalar value of wi iq ` wi dyn,ER tanhpdER maxp0, Φiq, Ai origin ` min dyn κ dyn dyn,ERV tanhpdERV iq origin end for 33: 34: 35: 36: until θ converges; end for Update θ via base RL objective with ˆAi"
        },
        {
            "title": "C DETAILS OF THEOREMS",
            "content": "C.1 PROOF OF THEOREM 3.1 Suppose we have matrix of embeddings RT ˆD. Then the effective rank of is lower bound of rankpZq: 1 ď erankpZq ď rankpZq ď mintT, Du (12) 17 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32:"
        },
        {
            "title": "Preprint",
            "content": "Proof. Let the singular value distribution of the matrix be pp1, p2, . . . , pmintT,Duq. The Shannon entropy of this distribution Hppq is bounded. Its minimum is 0, which occurs when only one element of is 1 and all others are 0. Its maximum is log k, where is the number of non-zero singular values, and this occurs when the distribution is uniform (pj 1{k for all non-zero values). The lower bound is established from the minimum entropy value: erankpZq exppHppqq ě expp0q 1 (13) Equality holds if and only if the singular value distribution is p1, 0, . . . , 0q, meaning has only one non-zero singular value. For the upper bound, let rankpZq be the number of non-zero singular values of Z. The entropy of the distribution is calculated only over these values and is maximized when they are uniform. Therefore Applying the exponential function to this inequality gives: Hppq ď log erankpZq exppHppqq ď expplog kq rankpZq (14) (15) This establishes that the effective rank is upper-bounded by the conventional rank. The final inequality, rankpZq ď mintT, Du, is standard property of matrix rank. Equality for erankpZq rankpZq holds if and only if the non-zero singular values are all equal, corresponding to uniform singular value distribution over its support. C.2 PROOF OF PROPOSITION 3.5 OpN q, p1q The zero-order metric and first-order difference of the effective rank scales linearly with the number of responses, p0q OpN q. The second-order difference of the effective rank is constant and does not depend on , yielding scaling order of p2q Proof. Without loss of generality, we take the effective rank for example. We adopt the provided definition of effective rank for representation matrix with singular values tσiu: Op1q. ÿ erankpZq exp pj logppjq , where pj σjř σk (16) Our analysis focuses on the dataset matrix RN ˆD, whose rows tqiuN i1 are the mean token embeddings of responses. The singular values σipZq of are the square roots of the eigenvalues of the Gram matrix ZZ λjpKq. Given that the rows of are nearly orthogonal, the Gram matrix is strongly diagonal-dominant. Its eigenvalues can be approximated by its diagonal entries: ; i.e., σjpZq λjpKq Kjj }qj}"
        },
        {
            "title": "1\nT",
            "content": "for 1, . . . , (17) The matrix has significant eigenvalues, each approximately equal to 1{T . The singular values of are the square roots of the eigenvalues of K: σjpZq λjpKq"
        },
        {
            "title": "1\nT",
            "content": "1 ? for 1, . . . , (18) To calculate the effective rank, we first normalize these singular values to form probability distribution tpju. The sum of singular values is: Nÿ k1 σkpZq Nÿ k1 1 ? ? (19) The individual probabilities are therefore:"
        },
        {
            "title": "Preprint",
            "content": "pj σjř σk ? ? 1{ {"
        },
        {
            "title": "1\nN",
            "content": "(20) The distribution tp1, p2, . . . , pN is uniform distribution over states. The Shannon entropy of this distribution is maximal: Nÿ Hppq pj logppjq Nÿ j"
        },
        {
            "title": "1\nN",
            "content": "ˆ"
        },
        {
            "title": "1\nN",
            "content": "log ˆ logpN N logpN (21) The effective rank is the exponential of this entropy: erankpZq exppHppqq expplogpN qq . In the maximal prompt entropy regime, the effective rank of the dataset matrix scales as OpN q. We adapt them to our context by defining the metrics value at time as the Effective Rank computed on the dataset matrix formed by the first prompts, denoted Z1:n. Let mn erankpZ1:nq. From our previous analysis, we established crucial result that forms the basis of this derivation: for maximal cases, the effective rank of dataset with prompts scales linearly with n. mn erankpZ1:nq (22) We will use this linear approximation to derive the scaling orders of the difference metrics, assuming stride of 1 for simplicity. The first-order difference quantifies the average velocity of change in the metric relative to its historical mean. Instantaneous First-Order Difference pδp1q is the value at step minus the average of all preceding values. δp1q mn Substituting our approximation mk k: 1 1 n1ÿ k1 mk δp1q 1 1 δp1q ˆ 1 1 pn 1qn 2 n1ÿ k1 (23) (24) (25) 2 The instantaneous difference grows linearly with n. Overall First-Order Difference p1q the average of the instantaneous differences over the entire dataset of size . erank: This is p1q erank p1q erank 1 2pN 1q Nÿ n"
        },
        {
            "title": "1\nN ´ 1\n¸",
            "content": "Nÿ δp1q"
        },
        {
            "title": "1\nN ´ 1",
            "content": "2 Nÿ n2 ˆ n1 1 2pN 1q pN ` 1q 2 For large , the expression is dominated by the highest power of : p1q erank 2{4 4 (26) (27) (28) The first-order difference of the effective rank scales linearly with the number of prompts, p1q erank OpN q. As for second-order difference, we compute the change in Instantaneous Differences between consecutive values of δp1q . δp1q δp1q n1 2 1 2 1 2 (29)"
        },
        {
            "title": "Preprint",
            "content": "This change is constant, indicating linear increase in the first-order difference. Overall SecondOrder Difference p2q erank: p2q erank"
        },
        {
            "title": "1\nN ´ 2",
            "content": "Nÿ n3 dp1q dp1q n"
        },
        {
            "title": "1\nN ´ 2",
            "content": "Nÿ n3 1 2 p2q erank"
        },
        {
            "title": "1\nN ´ 2",
            "content": "pN 2q 1 2 1 2 (30) (31) The second-order difference of the effective rank is constant and does not depend on , yielding scaling order of p2q erank Op1q."
        },
        {
            "title": "D IMPLEMENTATION DETAILS",
            "content": "D.1 TRAINING AND EVALUATION DETAILS We typically use the same set of hyperparameters to train and evaluate all models in the SimpleRLZoo series (Zeng et al., 2025) in the default main experiment setting. Training. We conduct all experiments with 4 A800-PCIE-80GB GPUs. For GRPO and PPO, we use prompt batch size of 48 with maximum rollout length of 1536 tokens. Training is performed using mini-batch size of 24. For GRPO, we generate 4 rollouts per prompt. For PPO, we use DeepSeek-R1-Distill-Qwen-1.5B (Guo et al., 2025) as the value model and generate 1 rollout per prompt. The default sampling temperature is set to 1.0, and the clip ratio is 0.2. For all actor models ranging from 3B to 8B parameters, we use learning rate of 1e-6 and KL loss coefficient of 1e-4. For critic models in PPO, we use learning rate of 1e-5. For our training datasets, we follow the same setup as in Zeng et al. (2025), where the data is filtered from GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) configured with different difficulty levels for models of varying capabilities. We tested using the checkpoint model trained up to 120 steps. Evaluation. We build our evaluation script based on that of Zeng et al. (2025), using temperature of 0.6 and maximum generation length of 2048 tokens. To ensure consistency, we adopt the same prompt template used during training. For most benchmarks, we report Pass@1 results. However, for benchmarks like AIME 2024, which contains fewer problems, we report both Pass@1 and average accuracy (Pass@256), computed over 256 generated samples per problem. Base Models. To demonstrate the universality of our insights and methods, we conduct zero RL training experiments on Llama-3.2 (3B), Llama-3.1 (8B) (Dubey et al., 2024), Mistral-v0.37B (Jiang et al., 2024), and Qwen-2.5 (1.5B, 3B, 7B) (Hui et al., 2024). For value model in PPO, we use DeepSeek-R1-Distill-Qwen-1.5B (Guo et al., 2025) for all experiments. These models were loaded and handled using the Hugging Face transformers library (Wolf et al., 2019), which provided robust framework for our training and inference pipeline. Benchmark. We evaluate on diverse suite of mathematical reasoning benchmarks. These include standard benchmarks such as GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), ASDiv (Miao et al., 2020), Carp (English Version) (Zhang et al., 2023), MAWPS (Koncel-Kedziorski et al., 2016), SVAMP (Patel et al., 2021), TabMWP (Lu et al., 2023), and OlympiadBench (He et al., 2024); Chinese mathematics collections like CMATH (Wei et al., 2023) and Gaokao 2024; and benchmarks from mathematics competitions, including the 2024/2025 AIME and the 2023/2024 AMC. D.2 EFFICIENT INCREMENTAL COMPUTATION OF HIGHER-ORDER METRICS naive computation of the temporal difference metrics would be computationally prohibitive. Our methods feasibility hinges on an efficient, incremental algorithm that computes the required metrics without redundant operations on the growing hidden state matrix RT ˆD. The effective rank is derived from the singular values of the mean-centered hidden state matrix Zc. These are equivalent to the square roots of the eigenvalues of the centered Gram matrix ZcZJ . Instead of recomputing Gt from scratch at each time step t, our algorithm incrementally updates two sufficient statistics: the uncentered Gram matrix Ut Z1:tZ 1:t and the sum of hidden state i1 zi.. When extending the analysis window, the new uncentered Gram matrix Ut is vectors st ř"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Time overhead of the main computation of RL Training. constructed from the prior matrix Uts and the new chunk of hidden states Zt Zts`1:t. This update follows recursive block matrix structure: ˆ Ut Uts pZtqZ 1:ts Z1:tspZtqJ pZtqpZtqJ (32) From the efficiently updated Ut and st, we can directly construct the centered Gram matrix Gt. Letting µt st{t be the mean vector and 1t be column vector of ones, Gt can be expressed as: Gt Ut pZ1:tµtq1J 1tpZ1:tµtqJ ` pµJ µtq p1t1J (33) ř ř pj log pj This allows for the calculation of Gt without re-accessing the full history of hidden states. The eigenvalues tλju of Gt are then used to derive the effective rank. First, the singular values of the λj. These are normalized to form probability distribucentered matrix are obtained, σj tion, pj σj{ σk. The effective rank is then the exponential of the Shannon entropy of this distribution:erankpZc,tq exp . This pipeline efficiently yields sequence of effective rank values, mjs erankpZc,jsq, at each stride s. From this sequence, we compute the instantaneous first-order difference, δ, which compares the current value to the running average of all preceding values. This is defined recursively as:δjs mjs 1 j1 The computational advantage of this incremental approach is substantial. While the total cost for the series of eigenvalue decompositions OpT 4{sq, is common to both methods, the cost of matrix construction differs significantly. The naive methods recalculation totals OpDT 3{sq, whereas our incremental update method reduces this to OpDT 2q. This reduction of the polynomial dependency on sequence length from cubic to quadratic is critical, as this term is multiplied by the large hidden dimension D, making it the dominant factor in practical performance and rendering the dense calculation of temporal dynamics feasible. j1 k1 mks. ř D.3 TIME OVERHEAD OF VERL TRAINING We conducted post-training with Zero RL on several base models. The Fig. 7 illustrates the time associated with each computational stage. The metrics calculation component, which represents the cost of computing metrics for hidden states, accounts for an insignificant portion of the total processing time. This demonstrates that our method does not introduce substantial time overhead."
        },
        {
            "title": "E MORE EXPERIMENTS",
            "content": "E.1 ANALYSIS OF RESPONSE-LEVEL HIDDEN STATES As shown in Figs. 8 and 9, our analysis of response-level hidden states across additional LLMs confirms that the insights presented in Sec. 4.1 hold true for various base models and RL paradigms. Figure 8: Visualization of response-level metrics for GRPO post-training. Overall (blue) represents the metric across the entire data batch, while Correct (green) and Incorrect (red) show the metrics specifically for correctly and incorrectly classified samples, respectively. Data is smoothed using rolling window of 10 steps to highlight underlying trends."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Visualization of response-level metrics for PPO post-training. Overall (blue) represents the metric across the entire data batch, while Correct (green) and Incorrect (red) show the metrics specifically for correctly and incorrectly classified samples, respectively. Data is smoothed using rolling window of 10 steps to highlight underlying trends. E.2 ANALYSIS OF DATASET-LEVEL HIDDEN STATES As shown in Figs. 10 and 11, our analysis of dataset-level hidden states across additional LLMs confirms that the insights presented in Sec. 4.2 hold for various base models and RL paradigms."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Visualization of dataset-level metrics for GRPO post-training"
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Visualization of dataset-level metrics for PPO post-training"
        },
        {
            "title": "Preprint",
            "content": "E.3 DETAILED ANALYSIS OF PASS@1 PERFORMANCE As shown in Tab. 4, Pass@1 measures the models ability to generate correct answer in single attempt, which directly reflects its exploitation ability. We fine-tune the base model by integrating our VERL-based Advantage method into two reinforcement learning paradigms, GRPO and PPO. Table 4: Performance comparison of instruction-tuned models on mathematical reasoning benchmarks (Pass@1). + GRPO and + PPO denote reinforcement learning fine-tuning from the base model using GRPO and PPO, respectively. w/ VERL. indicates the application of our VERL-based advantage to the corresponding RL algorithm. represents the performance difference between the baseline RL method and its VERL-advanced variant. All results are reported in percentage (%). Model AIME24 AIME25 AMC23 AMC24 ASDiv Carp En CMATH Gaokao 2024 Gaokao 2024 Mix Gaokao MathCloze GSM8K MAWPS Olympiad Bench SVAMP TabMWP Avg. Llama-3.2-3B-Instruct + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Llama-3.1-8B-Instruct + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Qwen2.5-3B + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Qwen2.5-7B + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Mathstral-7B-v0.1 + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Mistral-7B-v0.3 + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO 0.0 3.3 13.3 +10.0 10.0 10.0 +0.0 0.0 6.7 10.0 +3.3 6.7 10.0 +3.3 6.7 3.3 6.7 +3.4 3.3 6.7 +3.4 6.7 10.0 13.3 +3.3 6.7 10.0 +3.3 0.0 0.0 6.7 +6.7 6.7 10.0 +3.3 0.0 0.0 0.0 +0.0 0.0 0.0 +0. 0.0 0.0 6.7 +6.7 3.3 3.3 +0.0 3.3 0.0 3.3 +3.3 0.0 0.0 +0.0 0.0 0.0 0.0 +0.0 0.0 0.0 +0.0 0.0 6.7 10.0 +3.3 3.3 6.7 +3.3 0.0 0.0 0.0 +0.0 3.3 0.0 -3.3 0.0 0.0 0.0 +0.0 0.0 0.0 +0. 25.0 27.5 25.0 -2.5 22.5 25.0 +2.5 17.5 22.5 32.5 +10.0 30.0 35.0 +5.0 20.0 40.0 30.0 -10.0 32.5 32.5 +0.0 45.0 55.0 50.0 -5.0 50.0 52.5 +2.5 12.5 47.5 45.0 -2.5 32.5 27.5 -5.0 10.0 2.5 7.5 +5.0 0.0 2.5 +2. 11.1 8.9 11.1 +2.2 13.3 11.1 -2.2 8.9 15.6 15.6 +0.0 17.8 13.3 -4.5 24.4 22.2 17.8 -4.4 15.6 17.8 +2.2 15.6 26.7 28.9 +2.2 33.3 33.3 +0.0 8.9 17.8 20.0 +2.2 20.0 22.2 +2.2 0.0 4.4 2.2 -2.2 0.0 0.0 +0. 74.6 88.8 89.3 +0.5 87.9 88.7 +0.8 48.0 90.3 90.7 +0.4 89.8 90.7 +0.9 90.7 92.6 92.6 +0.0 92.8 92.6 -0.2 91.4 94.8 95.0 +0.2 94.9 94.8 -0.1 87.1 92.9 93.3 +0.4 90.9 93.0 +2.1 40.5 58.2 59.1 +0.9 8.9 44.7 +35. 26.5 45.0 45.4 +0.4 46.4 46.0 -0.4 34.1 42.4 45.0 +2.6 42.0 42.6 +0.6 54.7 56.0 56.9 +0.9 56.5 57.0 +0.5 55.8 60.2 60.8 +0.6 59.6 60.0 +0.4 51.1 55.9 55.5 -0.4 51.8 53.8 +2.0 12.4 11.1 15.0 +3.9 6.6 10.6 +4. 10.2 28.3 46.2 +17.9 21.2 30.7 +9.5 18.5 60.7 72.7 +12.0 60.0 62.0 +2.0 76.7 82.7 84.8 +2.1 83.2 84.3 +1.1 86.7 91.7 90.7 -1.0 89.8 90.3 +0.5 74.2 81.3 81.5 +0.2 78.3 78.2 -0.1 21.8 42.3 43.0 +0.7 7.7 35.7 +28. 14.3 21.4 14.3 -7.1 7.1 14.3 +7.2 0.0 7.1 14.3 +7.2 0.0 14.3 +14.3 0.0 7.1 21.4 +14.3 0.0 21.4 +21.4 42.9 14.3 35.7 +21.4 28.6 28.6 +0.0 28.6 35.7 50.0 +14.3 42.9 42.9 +0.0 14.3 0.0 0.0 +0.0 7.1 7.1 +0. 14.3 20.9 22.0 +1.1 16.5 19.8 +3.3 15.4 14.3 14.3 +0.0 14.3 22.0 +7.7 22.0 27.5 33.0 +5.5 28.6 29.7 +1.1 33.0 34.1 35.2 +1.1 31.9 34.1 +2.2 33.0 44.0 40.7 -3.3 37.4 51.6 +14.2 13.2 15.4 6.6 -8.8 11.0 16.5 +5. 6.8 23.7 22.9 -0.8 20.3 27.1 +6.8 16.9 32.2 30.5 -1.7 25.4 28.8 +3.4 41.5 42.4 49.2 +6.8 50.0 47.5 -2.5 49.2 64.4 69.5 +5.1 63.6 66.9 +3.3 31.4 49.2 46.6 -2.6 49.2 48.3 -0.9 3.4 5.1 4.2 -0.9 2.5 5.1 +2. 66.6 80.7 81.7 +1.0 81.4 82.9 +1.5 47.4 88.4 88.6 +0.2 86.4 87.3 +0.9 80.7 82.8 82.2 -0.6 81.7 81.8 +0.1 85.8 90.2 89.2 -1.0 89.1 90.2 +1.1 81.6 88.1 89.5 +1.4 87.0 87.4 +0.4 86.9 96.0 96.0 +0.0 95.5 95.7 +0. 43.5 96.4 96.9 +0.5 95.7 96.6 +0.9 95.1 96.6 96.4 -0.2 96.6 96.5 -0.1 95.4 97.6 97.7 +0.1 97.3 97.8 +0.5 93.8 97.6 97.2 -0.4 96.0 96.7 +0.7 24.0 52.4 40.3 -12.1 3.3 28.8 +25.5 50.8 79.2 69.5 -9.7 8.6 70.7 +62. 12.7 16.7 17.6 +0.9 17.8 17.3 -0.5 10.4 19.7 21.3 +1.6 18.2 19.1 +0.9 23.0 23.6 24.4 +0.8 24.4 24.6 +0.2 25.8 36.1 35.4 -0.7 36.1 36.1 +0.0 17.9 25.6 29.3 +3.7 28.4 26.1 -2.3 1.6 3.0 2.8 -0.2 2.1 2.4 +0. 74.1 87.7 87.8 +0.1 86.8 85.8 -1.0 48.5 88.5 88.4 -0.1 88.6 88.1 -0.5 84.3 89.0 88.5 -0.5 86.0 88.3 +2.3 88.5 92.8 92.9 +0.1 92.8 92.5 -0.3 87.7 93.0 90.7 -2.3 89.9 89.6 -0.3 39.1 47.6 57.5 +9.9 6.9 57.5 +50. 41.4 71.7 72.3 +0.6 71.0 71.3 +0.3 34.3 82.7 83.1 +0.4 82.3 83.0 +0.7 71.3 81.4 81.0 -0.4 80.8 81.4 +0.6 82.8 91.3 91.9 +0.6 90.8 90.6 -0.2 54.7 81.5 83.5 +2.0 70.7 84.1 +13.4 30.6 37.7 53.0 +15.3 12.0 35.1 +23. 31.0 41.4 43.4 +2.0 40.1 41.9 +1.9 23.1 44.5 47.2 +2.7 43.8 46.2 +2.4 46.1 49.8 51.0 +1.2 48.8 50.8 +2.0 53.6 57.7 59.8 +2.1 57.9 59.0 +1.1 44.2 54.0 55.3 +1.3 52.3 54.1 +1.8 17.4 23.9 24.0 +0.1 5.1 21.1 +16."
        },
        {
            "title": "Preprint",
            "content": "E.4 DETAILED ANALYSIS OF PASS@k PERFORMANCE As shown in Tab. 5, which provides comprehensive analysis of the models performance on the Pass@k metric, which is direct measure of the models exploration ability. As supplement to the main papers discussion, it presents the detailed performance of our VERL-based models across variety of mathematical reasoning benchmarks. These results demonstrate that our method consistently leads to significant improvements, confirming its effectiveness in enhancing the models exploration capabilities. Table 5: Performance comparison of instruction-tuned models under diverse decoding settings (Pass@k). All results are reported in percentage (%)."
        },
        {
            "title": "Model",
            "content": "Llama-3.2-3B-Instruct + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Llama-3.1-8B-Instruct + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Qwen2.5-3B + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Qwen2.5-7B + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Mathstral-7B-v0.1 + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Mistral-7B-v0.3 + GRPO + GRPO w/ VERL. GRPO + PPO + PPO w/ VERL. PPO Avg. 58.96 61.24 64.40 +3.16 63.42 64.32 +0.90 63.04 65.04 65.66 +0.62 62.80 64.86 +2.06 69.68 66.80 69.58 +2.78 67.76 69.78 +2.02 76.54 74.72 78.40 +3.68 74.82 77.44 +2. 61.96 63.60 72.18 +8.58 67.62 69.70 +2.08 37.16 30.44 33.38 +2.94 21.34 22.48 +1.14 MATH500 (Pass@16) AMC23 (Pass@128) AMC24 (Pass@128) AIME24 (Pass@256) AIME25 (Pass@256) 40.0 40.0 50.0 +10.0 46.7 46.7 +0.0 46.7 53.3 50.0 3.3 46.7 53.3 +6.6 56.7 46.7 53.3 +6.6 43.3 53.3 +10.0 60.0 56.7 63.3 +6.6 53.3 56.7 +3.4 43.3 36.7 50.0 +13.3 53.3 53.3 +0. 20.0 10.0 16.7 +6.7 6.7 3.3 3.4 30.0 30.0 36.7 +6.7 36.7 40.0 +3.3 36.7 36.7 36.7 +0.0 36.7 36.7 +0.0 40.0 40.0 43.3 +3.3 43.3 43.3 +0.0 60.0 50.0 60.0 +10.0 56.7 66.7 +10.0 36.7 40.0 50.0 +10.0 40.0 46.7 +6. 16.7 10.0 13.3 +3.3 6.7 13.3 +6.6 79.8 80.2 80.6 +0.4 82.2 82.4 +0.2 79.8 83.4 83.4 +0.0 79.2 82.4 +3.2 86.0 86.6 87.6 +1.0 87.8 88.2 +0.4 90.6 90.8 91.4 +0.6 91.2 91.4 +0.2 80.4 84.8 87.0 +2.2 82.4 84.8 +2. 36.0 33.0 34.4 +1.4 21.8 19.2 2.6 51.5 60.6 59.0 1.6 57.0 57.8 +0.8 57.4 56.9 63.1 +6.2 59.0 60.0 +1.0 69.0 68.5 67.8 0.7 67.9 67.3 0.6 73.7 78.3 79.0 +0.7 74.3 74.4 +0.1 60.9 69.2 76.9 +7.7 70.7 69.9 0. 39.6 36.0 38.0 +2.0 25.1 30.1 +5.0 93.5 95.4 95.7 +0.3 94.5 94.7 +0.2 94.6 94.9 95.1 +0.2 92.4 91.9 0.5 96.7 92.2 95.9 +3.7 96.5 96.8 +0.3 98.4 97.8 98.3 +0.5 98.6 98.0 0.6 88.5 87.3 97.0 +9.7 91.7 93.8 +2. 73.5 63.2 64.5 +1.3 46.4 46.5 +0."
        },
        {
            "title": "F CASE STUDY",
            "content": "For all case studies presented in this section, we employ Qwen2.5-7B as the base model. The answers shown in the gray boxes are generated by Qwen2.5-7B trained with vanilla GRPO for 120 steps, while those in the purple boxes are produced by Qwen2.5-7B trained with GRPO augmented with the auxiliary shaping termour proposed method VERLfor 120 steps. F.1 CASE STUDY FOR PASS@1 SETTING Case Study I. For the case shown in Fig. 12, the model trained with vanilla GRPO incorrectly judges 1 ă 13, whereas the model trained with GRPO augmented with the auxiliary shaping term gives the correct answer. This example highlights the limitation of vanilla GRPO in exploitation: it fails to consistently apply basic numerical commonsensenamely, that larger absolute values correspond to smaller magnitudes for negative numbers. By contrast, introducing the Auxiliary Shaping Term strengthens exploitation capacity, enabling the model to make such comparisons more reliably. Question What is the smallest possible integer value of if 12 px ` 1q is an integer? Ground Truth -13 Vanilla GRPO To find the smallest possible integer value of such that 12 12 x`1 will be an integer if ` 1 is divisor of 12. Lets list the divisors of 12: 1. The divisors of 12 are: 1, 2, 3, 4, 6, 12. 2. We need to find the smallest x, so we start with the smallest positive divisor, which is 1: x`1 is an integer, we need to consider the factors of 12. The expression 3. To confirm, we substitute 0 back into the original expression: ` 1 1 ùñ 1 1 ùñ 0 12 0 ` 1 12 12 Since 12 is an integer, 0 is indeed valid solution. 4. We should also consider the negative divisors to ensure we have the smallest possible integer value. The next smallest divisor is -1: ` 1 1 ùñ 1 1 ùñ 2 5. Substituting 2 back into the original expression: 12 2 ` 1 12 1 12 Since -12 is an integer, 2 is also valid solution, but it is not smaller than 0. Therefore, the smallest possible integer value of is 0 GRPO with Auxiliary Shaping Term To find the smallest possible integer value of such that 12 of 12 are 1, 2, 3, 4, 6, 12. For 12 possible x, so we should consider the smallest positive divisor, which is 1. If ` 1 1, then 0. We should also consider the negative divisors to ensure we find the smallest possible integer value of x. The smallest negative divisor is 12. If ` 1 12, then 13. Comparing 0 and 13, the smallest possible integer value of is 13. Therefore, the final answer is x`1 to be an integer, ` 1 must be one of these divisors. We want the smallest x`1 is an integer, we need to determine the divisors of 12. The divisors 13 Figure 12: Case study: Vanilla GRPO fails to exploit numerical commonsense when comparing negative values (-1 vs. -13), while our method gives the correct answer."
        },
        {
            "title": "Preprint",
            "content": "Case Study II. For the case in Fig. 13, the crucial distinction lies in how the models interpret the condition No house faces two different streets. The vanilla GRPOtrained model incorrectly infers that each street contains only 10 houses, overlooking the structural implication of the condition. In contrast, the model trained with GRPO augmented by the auxiliary shaping term exploits this commonsense constraint more effectively: it decomposes the problem step by step, correctly deduces that each street has 20 houses, and thereby arrives at the correct total. Question In my neighborhood, there are six streets. There are 10 houses on each side of each street. No house faces two different streets. How many houses are in my neighborhood? Ground Truth 120 Vanilla GRPO To determine the total number of houses in the neighborhood, we can follow these steps: 1. Identify the number of streets and houses per street: - There are 6 streets. - Each street has 10 houses on each side. 2. Calculate the number of houses on one street: Since there are 10 houses on each side of street, and each house faces only one street, the total number of houses on one street is 10 (since they are all on the same street). 3. Calculate the total number of houses in the neighborhood: There are 6 streets, and each street has 10 houses. Therefore, the total number of houses is 6 ˆ 10 60. Putting the final answer within , we get: GRPO with Auxiliary Shaping Term To find the total number of houses in the neighborhood, lets break down the problem step by step. 1. There are 6 streets in the neighborhood. 2. Each street has houses on both sides. 3. There are 10 houses on each side of every street. 4. No house faces two different streets, meaning each house is counted only once. First, calculate the number of houses on one side of single street: Since there are houses on both sides of each street, multiply by 2: 10 houses/side Now, since there are 6 streets, multiply the number of houses per street by the number of streets: 10 houses/side ˆ 2 sides/street 20 houses/street 20 houses/street ˆ 6 streets 120 houses So, the total number of houses in the neighborhood is 120 Figure 13: Case study: Vanilla GRPO overlooks the constraint and infers 10 houses per street, while our method exploits it correctly and infers 20. Case Study III. As shown in Fig. 14, the task concerns solving differential equation under the constraint of finding purely exponential solution rather than general one. The model trained with vanilla GRPO overlooks this constraint and defaults to the generic procedure for obtaining the full solution. In contrast, the model trained with GRPO augmented by the auxiliary shaping term respects the constraint and produces the correct purely exponential answer. This case illustrates how the auxiliary shaping term enhances exploitation by enforcing adherence to problem-specific constraints. F.2 CASE STUDY FOR PASS@16 SETTING Case Study I. As shown in Fig. 15, the question is simple elementary-level math problem requiring the integer solution ě 1, 2. While the vanilla GRPO model consistently outputs 3, which is incorrect, the model trained with the auxiliary shaping term produces 1 in 5 out of 16 rollouts, the correct answer. This case highlights that the auxiliary shaping term enhances the models exploration ability, enabling it to consider wider range of possibly valid solutions."
        },
        {
            "title": "Preprint",
            "content": "Question Find purely exponential solution of d4x dt4 e2t. Ground Truth e2t 15 Vanilla GRPO To find purely exponential solution of the differential equation ( d4 1. Find the complementary solution by solving the homogeneous equation: dt4 e2t), we will follow these steps: The characteristic equation is: Solving for r, we get: Therefore, the complementary solution is: d4x dt4 0 r4 1 0 r4 1 ùñ 1, xcptq C1et ` C2et ` C3 cosptq ` C4 sinptq 2. Find particular solution using the method of undetermined coefficients: Since the right-hand side of the non-homogeneous equation is e2t, we assume particular solution of the form: Taking the fourth derivative of xpptq: xpptq Ae2t d4 dt4 pAe2tq Ap2q4e2t 16Ae2t Substituting xpptq and its fourth derivative into the original differential equation: Simplifying, we get: Therefore: So, the particular solution is: 16Ae2t Ae2t e2t p16A Aqe2t e2t ùñ 15Ae2t e2t 15A 1 ùñ 1 xpptq 1 15 e2t 3. Combine the complementary and particular solutions: The general solution to the differential equation is: xptq xcptq ` xpptq C1et ` C2et ` C3 cosptq ` C4 sinptq ` 1 e2t Thus, the final answer is: C1et ` C2et ` C3 cosptq ` C4 sinptq ` 1 15 e2t Figure 14: Case study: Vanilla GRPO ignores the constraint of purely exponential solution, while our method enforces it and yields the correct result."
        },
        {
            "title": "GRPO with Auxiliary Shaping Term\nTo find a purely exponential solution to the differential equation",
            "content": ", we will follow these steps: 1. Find the complementary solution by solving the homogeneous equation: d4x dt4 e2t The characteristic equation is: Solving for r, we get: Therefore, the complementary solution is: d4x dt4 r4 1 0 r4 1 ùñ 1, xcptq C1et ` C2et ` C3 cosptq ` C4 sinptq 2. Find particular solution using the method of undetermined coefficients: Since the right-hand side of the nonhomogeneous equation is e2t, we assume particular solution of the form: Taking the fourth derivative of xpptq: xpptq Ae2t d4xp dt4 16Ae2t Substituting xpptq and its fourth derivative into the original differential equation: Simplifying, we get: Solving for A: Therefore, the particular solution is: 16Ae2t Ae2t e2t 15Ae2t e2t 15A 1 ùñ 1 15 xpptq 1 15 e2t 3. Combine the complementary and particular solutions: The general solution to the differential equation is: xptq xcptq ` xpptq C1et ` C2et ` C3 cosptq ` C4 sinptq ` 1 15 e2t Thus, the purely exponential solution is: 1 e2t Figure 14: Case study: Vanilla GRPO ignores the constraint of purely exponential solution, while our method enforces it and yields the correct result (continued)."
        },
        {
            "title": "Question",
            "content": "Given pxq ? x1 x2 , what is the smallest possible integer value for such that pxq has real number value? Ground Truth 1 Figure 15: Case study demonstrating improved exploration: the auxiliary shaping term enables the model to explore and reach the correct solution."
        },
        {
            "title": "Preprint",
            "content": "Vallina GRPO#1-#8 ? (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: 1. The numerator is For the square root to be defined and result in real number, the expression inside the square root must be non-negative. 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator is 2. For the function to be defined, the denominator must not be zero. 2 0 ùñ Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . To determine the smallest possible integer value for such that pxq ? x1 x2 has real number value, we need to ensure that both the numerator and the denominator satisfy their respective conditions for real numbers. 1. Numerator Condition: The expression inside the square root must be nonnegative. 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator must not be zero to avoid division by zero. 2 0 ùñ 2 Combining these conditions, we need ě 1 and 2. The smallest integer that satisfies these conditions is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . To determine the smallest possible integer value for such that pxq ? x1 x2 has real number value, we need to ensure that both the numerator and the denominator satisfy certain conditions. 1. Numerator Condition: The expression inside the square root must be non-negative because the square root of negative number is not real number. 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator must not be zero because division by zero is undefined. 2 0 ùñ Combining these conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. (Details omitted as they are not crucial for this case study.) 1. For the square root to be defined, the expression inside it must be greater than or equal to zero: 1 ě 0 (Details omitted as they are not crucial for this case study.) 1. For the square root to be defined, the expression inside it must be greater than or equal to zero: (Details omitted as they are not crucial for this case study.) 1. For the square root to be defined, we need: 1 ě 0 1 ě 0 Solving this inequality: Solving this inequality: Solving this inequality: ě 1 ě 1 ě 1 2. For the denominator to be non-zero: 2 0 Solving this equation: 2 (Details omitted as they are not crucial for this case study.) Therefore, the smallest possible integer value for such that pxq has real number value is: 3 2. For the function to be defined, the denominator must not be zero: 2 0 Solving this equation: 2 (Details omitted as they are not crucial for this case study.) Therefore, the smallest possible integer value for such that pxq has real number value is 3 . 2. For the denominator to be non-zero, we need: 2 0 Solving this equation: Combining these two conditions, must be greater than or equal to 1 but not equal to 2. The smallest integer that satisfies this condition is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . (Details omitted as they are not crucial for this case study.) 1. Square Root Condition: The expression inside the square root, 1, must be greater than or equal to zero for the square root to be real number. 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator, 2, must not be zero to avoid division by zero. 2 0 ùñ 2 Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . (Details omitted as they are not crucial for this case study.) 1. Square Root Condition: The expression inside the square root, 1, must be greater than or equal to zero for the square root to be real number. 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator, 2, must not be zero to avoid division by zero. 2 0 ùñ Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . Figure 15: Case study demonstrating improved exploration: the auxiliary shaping term enables the model to explore and reach the correct solution (continued)."
        },
        {
            "title": "Preprint",
            "content": "Vanilla GRPO#9-#16 (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression inside the square root, x1, must be non-negative because the square root of negative number is not real number. Therefore, we need: (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression inside the square root, x1, must be non-negative because the square root of negative number is not real number. Therefore, we need: 1 ě 0 ùñ ě 1 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator, 2, must not be zero because division by zero is undefined. Therefore, we need: 2. Denominator Condition: The denominator, 2, must not be zero because division by zero is undefined. Therefore, we need: 2 0 ùñ 2 2 0 ùñ 2 Combining these two conditions, we get: Combining these two conditions, we get: ě 1 and ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is: 3 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . ? (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: 1. The numerator is For the square root to be defined and result in real number, the expression inside the square root must be non-negative. 1 ě 0 ùñ ě 2. Denominator Condition: The denominator is 2. For the function to be defined, the denominator must not be zero. 2 0 ùñ 2 Combining these two conditions, we need to be greater than or equal to 1, but not equal to 2. Therefore, the smallest possible integer value for that satisfies both conditions is 3. 3 (Details omitted as they are not crucial for this case study.) 1. Square Root Condition: The expression inside the square root, 1, must be greater than or equal to zero for the square root to be real number. 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator, 2, must not be zero to avoid division by zero. 2 0 ùñ 2 Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . (Details omitted as they are not crucial for this case study.) 1. For the square root to be defined, we need: 1 ě ě 1 2. For the denominator to be non-zero, we need: 2 0 2 Combining these two conditions, we get: ě 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . To determine the smallest possible integer value for such that pxq ? x1 x2 has real number value, we need to ensure that both the numerator and the denominator satisfy their respective conditions for real numbers. 1. Numerator Condition: The expression inside the square root must be nonnegative. 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator must not be zero. 2 0 ùñ Combining these conditions, we need to be greater than or equal to 1 but not equal to 2. Therefore, the smallest possible integer value for that satisfies both conditions is 3. 3 (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression under the square root, x1, must be non-negative because the square root of negative number is not real number. Therefore, we need: 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator, 2, must not be zero because division by zero is undefined. Therefore, we need: 2 0 ùñ 2 Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is: 3 (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: 1. The numerator is For the square root to be real number, the expression inside the square root must be non-negative. ? 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator is 2. For the function to be defined, the denominator must not be zero. 2 0 ùñ 2 Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . Figure 15: Case study demonstrating improved exploration: the auxiliary shaping term enables the model to explore and reach the correct solution (continued)."
        },
        {
            "title": "Preprint",
            "content": "GRPO with Auxiliary Shaping Term#1-#8 (Details omitted as they are not crucial for this case study.) 1. For the square root to be defined in the real numbers, the expression inside it must be greater than or equal to zero: 1 ě 0 Solving this inequality: ě 1 2. For the function to be defined, the denominator must not be zero: 2 0 Solving this equation: 2 (Details omitted as they are not crucial for this case study.) Thus, the smallest possible integer value for such that pxq has real number value is 3 . (Details omitted as they are not crucial for this case study.) ### Combining the Conditions From Condition 1, we know ě 1. From Condition 2, we know 2. The smallest integer that satisfies ě 1 but not 2 is 1. (Details omitted as they are not crucial for this case study.) Since p1q is real number, 1 is indeed valid solution. Therefore, the smallest possible integer value for such that pxq has real number value is 1 . (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: 1. The numerator is For the square root to be defined and real, the expression inside the square root must be non-negative. ? 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator is 2. For the function to be defined, the denominator must not be zero. 2 0 ùñ 2 Combining these conditions, we need to be greater than or equal to 1, but not equal to 2. The smallest integer that satisfies these conditions is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression under the square root, x1, must be non-negative because the square root of negative number is not real number. Therefore, we need: 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator, 2, must not be zero because division by zero is undefined. Therefore, we need: 2 0 ùñ 2 Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 but not 2 is 1. Therefore, the smallest possible integer value for such that pxq has real number value is 1 . (Details omitted as they are not crucial for this case study.) ### Combining the Conditions From Condition 1, we have ě 1. From Condition 2, we have 2. Combining these, the smallest integer value for that satisfies both conditions is 3. ### Verification Lets verify that 3 works: - For 3, ? ? 1 3 1 2, which is real num- ? ber. - The denominator 2 3 2 1, which is not zero. Since both conditions are satisfied, the smallest possible integer value for is indeed 3. Thus, the final answer is: 3 (Details omitted as they are not crucial for this case study.) 1 to be de1. For fined in the real numbers, we need: ? 1 ě 0 ùñ ě 1 2. For the denominator 2 to not be zero, we need: 2 0 ùñ 2 Combining these two conditions, must be greater than or equal to 1 but not equal to 2. The smallest integer that satisfies this condition is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . (Details omitted as they are not crucial for this case study.) 1. The expression inside the square root must be non-negative. 2. The denominator must not be zero. (Details omitted as they are not crucial for this case study.) ### Step 3: Combine the conditions From Step 1, we know ě 1. From Step 2, we know 2. Therefore, the smallest integer value for that satisfies both conditions is the smallest integer greater than 1, which is 3. ### Final Answer The smallest possible integer value for such that pxq has real number value is 3 . (Details omitted as they are not crucial for this case study.) 1. For fined, we need: 1 to be de- ? 1 ě 0 ùñ ě 1 2. For the denominator 2 to be non-zero, we need: 2 0 ùñ Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 1. (Details omitted as they are not crucial for this case study.) Since p1q 0 is real number, our solution is confirmed. Therefore, the smallest possible integer value for such that pxq has real number value is 1 . Figure 15: Case study demonstrating improved exploration: the auxiliary shaping term enables the model to explore and reach the correct solution (continued)."
        },
        {
            "title": "Preprint",
            "content": "GRPO with Auxiliary Shaping Term#9-#16 (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression inside the square root, x1, must be non-negative because the square root of negative number is not real number. Therefore, we need: (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression under the square root must be nonnegative. (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression inside the square root must be nonnegative. 1 ě 0 ùñ ě 1 1 ě 0 ùñ ě 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator, 2, must not be zero because division by zero is undefined. Therefore, we need: 2 0 ùñ 2 Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 1. Therefore, the smallest possible integer value for such that pxq has real number value is 1 . (Details omitted as they are not crucial for this case study.) 1. The expression inside the square root must be non-negative. 2. The denominator must not be zero. Lets address these conditions one by one: 1. For the square root to be defined, the expression inside it must be greater than or equal to zero: 1 ě 0 ùñ ě 1 2. For the denominator to be non-zero: 2 0 ùñ 2 Combining these conditions, we need to be greater than or equal to 1 but not equal to 2. The smallest integer that satisfies this is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . 2. Denominator Condition: The denominator must not be zero. ? 1 This ensures that is real number. 2. Denominator Condition: The denominator must not be zero. 2 0 ùñ 2 2 0 ùñ 2 Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . This ensures that the function is defined. Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression inside the square root must be non-negative because the square root of negative number is not real number. (Details omitted as they are not crucial for this case study.) 1. Numerator Condition: The expression under the square root, 1, must be non-negative because the square root of negative number is not real number. 1 ě 0 ùñ ě 1 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator must not be zero because division by zero is undefined. 2. Denominator Condition: The denominator, 2, must not be zero because division by zero is undefined. 2 0 ùñ 2 0 ùñ 2 Combining these two conditions, we get: ě 1 and 2 The smallest integer that satisfies ě 1 and 2 is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . Combining these conditions, we need ě 1 and 2. The smallest integer that satisfies these conditions is 1. (Details omitted as they are not crucial for this case study.) Therefore, the smallest possible integer value for such that pxq has real number value is 1 . (Details omitted as they are not crucial for this case study.) 1. For the square root to be defined, we need: 1 ě 0 Solving this inequality: ě 1 2. For the denominator to be non-zero, we need: 2 0 Solving this equation: 2 Combining these two conditions, must be greater than or equal to 1 but not equal to 2. The smallest integer that satisfies this condition is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . To determine the smallest possible integer value for such that pxq ? x1 x2 has real number value, we need to ensure that both the numerator and the denominator satisfy their respective conditions for real numbers. 1. Numerator Condition: The expression inside the square root must be nonnegative. 1 ě 0 ùñ ě 1 2. Denominator Condition: The denominator must not be zero. 2 0 ùñ Combining these conditions, we need to be greater than or equal to 1 but not equal to 2. The smallest integer that satisfies this condition is 3. Therefore, the smallest possible integer value for such that pxq has real number value is 3 . Figure 15: Case study demonstrating improved exploration: the auxiliary shaping term enables the model to explore and reach the correct solution (continued)."
        },
        {
            "title": "Preprint",
            "content": "Case Study II. As shown in Fig. 16, this high-school level geometry problem illustrates the contrast between vanilla GRPO and GRPO with the auxiliary shaping term. The vanilla GRPO model fails to produce the correct answer in any rollout. In contrast, the model trained with the auxiliary shaping term succeeds in 5 out of 16 rollouts. Moreover, its reasoning traces display higher exploratory diversity: it applies the law of sines 9 times, the law of cosines 4 times, and other general geometric properties 3 times. By comparison, the vanilla GRPO model relies more narrowly on the law of sines (11 times), with only limited use of other strategies. This case highlights that the auxiliary shaping term enhances the models exploration capacity, enabling it to leverage broader range of problem-solving strategies rather than over-committing to single path."
        },
        {
            "title": "Preprint",
            "content": "Question Abdul and Chiang are standing 48 feet apart in field. Bharat is standing in the same field as far from Abdul as possible so that the angle formed by his lines of sight to Abdul and Chiang measures 60. What is the square of the distance (in feet) between Abdul and Bharat? Ground Truth 3072 Vanilla GRPO#1-#8 (Details omitted as they are not crucial for this case study.) 1. Identify the triangle and its properties: (Details omitted as they are not crucial for this case study.) 2. Use the Law of Sines: (Details omitted as they are not crucial for this case study.) 3. Apply the Law of Sines: (Details omitted as they are not crucial for this case study.) 4. Find the square of the distance: (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 2304 . To solve this problem, we can use the Law of Sines and some geometric properties. Lets break it down step by step. 1. Identify the triangle and its properties: (Details omitted as they are not crucial for this case study.) 2. Apply the Law of Sines: (Details omitted as they are not crucial for this case study.) 3. Calculate the distances: (Details omitted as they are not crucial for this case study.) 4. Find the square of the distance: (Details omitted as they are not crucial for this case study.) So, the square of the distance between Abdul and Bharat is 768 . To solve this problem, we can use the Law of Sines in trigonometry. Lets denote the distance between Abdul and Bharat as and the distance between Chiang and Bharat as y. We know that the angle between Abdul and Chiang as seen from Bharat is 60, and the distance between Abdul and Chiang is 48 feet. Given: - =ABC 60 - AB 48 feet - We need to find x2 First, we apply the Law of Sines in triangle ABC: (Details omitted as they are not crucial for this case study.) Thus, the square of the distance between Abdul and Bharat is 9216 . (Details omitted as they are not crucial for this case study.) 1. Recognize that the triangle ABC is right triangle with =BAC 90 and =ABC 60. Therefore, =ACB 30. (Details omitted as they are not crucial for this case study.) 3. Since AC 48 feet, and in 30-60-90 triangle, the side opposite the 60 3 times the side angle is opposite the 30 angle, we can find AB: ? ? AB 48 3 (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 6912 feet. To solve this problem, we can use the Law of Sines in trigonometry. Lets denote the distance between Abdul and Bharat as AB, and the distance between Chiang and Bharat as BC. Since Bharat is as far from Abdul as possible to form 60 angle, triangle ABC is right triangle with the right angle at Chiang. This means =ACB 90 and =BAC 60. (Details omitted as they are not crucial for this case study.) Thus, the square of the distance between Abdul and Bharat is 9216 . To solve this problem, we need to use some geometric principles, specifically involving the Law of Sines and properties of triangles. 1. Identify the triangle and the given information: (Details omitted as they are not crucial for this case study.) 2. Use the Law of Sines to find the distance BC (the distance from Bharat to Chiang): (Details omitted as they are not crucial for this case study.) 3. Find the distance AC (the distance from Abdul to Bharat): (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 11520 . First, we need to find the distance AB using the Law of Sines. In triangle ABC: (Details omitted as they are not crucial for this case study.) Now, we need to find the square of the distance AB: (Details omitted as they are not crucial for this case study.) Thus, the square of the distance between Abdul and Bharat is 1728 . To verify the accuracy of this result, we can use Python to perform the calculations. (Details omitted as they are not crucial for this case study.) (Details omitted as they are not crucial for this case study.) 1. Identify the triangle and the given information: (Details omitted as they are not crucial for this case study.) 2. Use the Law of Sines: (Details omitted as they are not crucial for this case study.) 3. Maximize AB: (Details omitted as they are not crucial for this case study.) 4. Apply the Pythagorean theorem: (Details omitted as they are not crucial for this case study.) Thus, the square of the distance between Abdul and Bharat is 9216 . Figure 16: Case study: Geometry problem. GRPO with auxiliary shaping term not only achieves higher accuracy but also explores more diverse reasoning strategies than vanilla GRPO."
        },
        {
            "title": "Preprint",
            "content": "Vanilla GRPO#9-#16 To solve this problem, we can use the Law of Sines and some geometric properties. Lets break it down step by step. 1. Identify the triangle and the given information: (Details omitted as they are not crucial for this case study.) 2. Determine the largest possible angle at A: (Details omitted as they are not crucial for this case study.) 3. Use the Pythagorean theorem: (Details omitted as they are not crucial for this case study.) 4. Find the square of the distance between Abdul and Bharat: (Details omitted as they are not crucial for this case study.) So, the square of the distance between Abdul and Bharat is 9216 . To solve this problem, we need to use some geometric properties and trigonometry. Lets break it down step by step. 1. Understanding the Problem: (Details omitted as they are not crucial for this case study.) 2. Geometric Insight: (Details omitted as they are not crucial for this case study.) 3. Using Trigonometry: (Details omitted as they are not crucial for this case study.) 4. Calculating the Distance: - Let the distance AB be x. Since =ABC is 60 and we need to maximize AB, we use the Law of Cosines in triangle ABC: (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 432 . To solve this problem, we can use some geometric properties and trigonometry. Lets break it down step by step: 1. Identify the triangle formed: (Details omitted as they are not crucial for this case study.) 2. Use the Law of Sines: (Details omitted as they are not crucial for this case study.) 3. Maximize the distance: (Details omitted as they are not crucial for this case study.) 4. Square the distance: (Details omitted as they are not crucial for this case study.) Lets calculate it: 482 2304 So, the square of the distance between Abdul and Bharat is 2304 . (Details omitted as they are not crucial for this case study.) 1. Identify the given information: (Details omitted as they are not crucial for this case study.) 2. Visualize the problem: (Details omitted as they are not crucial for this case study.) 3. Use the Law of Sines: (Details omitted as they are not crucial for this case study.) 4. Properties of an equilateral triangle: (Details omitted as they are not crucial for this case study.) 5. Calculate the square of the distance: (Details omitted as they are not crucial for this case study.) Thus, the square of the distance between Abdul and Bharat is 2304 . Providethefinalanswerwithin . To solve the problem, we need to determine the distance between Abdul and Bharat, given that the angle between their lines of sight to Chiang is 60 and that Bharat is positioned to be as far from Abdul as possible. (Details omitted as they are not crucial for this case study.) Using the Law of Cosines in triangle ABC: (Details omitted as they are not crucial for this case study.) BC2 d2 `230448d (Details omitted as they are not crucial for this case study.) The square of the distance between Abdul and Bharat is: d2 962 9216 Thus, the square of the distance between Abdul and Bharat is 9216 . To solve this problem, we need to use some geometric properties and trigonometry. Lets break it down step by step. 1. Understand the Problem: (Details omitted as they are not crucial for this case study.) 2. Visualize the Problem: (Details omitted as they are not crucial for this case study.) 3. Use Geometric Properties: (Details omitted as they are not crucial for this case study.) 4. Apply Trigonometry: (Details omitted as they are not crucial for this case study.) 5. Calculate the Distance: - AB = 2 * AC = 2 * 48 = 96 feet. - The square of the distance AB is 962. 6. Final Calculation: - 962 9216. So, the square of the distance between Abdul and Bharat is 9216 . (Details omitted as they are not crucial for this case study.) 1. Identify the known values: (Details omitted as they are not crucial for this case study.) 2. Set up the triangle: (Details omitted as they are not crucial for this case study.) 3. Apply the Law of Sines: (Details omitted as they are not crucial for this case study.) 4. Determine the angles: (Details omitted as they are not crucial for this case study.) 5. Simplify the Law of Sines equation: (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 2304 . To solve this problem, we can use the Law of Sines in trigonometry. Lets break it down step by step: 1. Identify the given information: (Details omitted as they are not crucial for this case study.) 2. Set up the triangle: (Details omitted as they are not crucial for this case study.) 3. Use the Law of Sines: (Details omitted as they are not crucial for this case study.) 4. Find the square of the distance: The distance between Abdul and Bharat is 24 feet. The square of this distance is: 242 576 Therefore, the square of the distance between Abdul and Bharat is 576 . Figure 16: Case study: Geometry problem. GRPO with auxiliary shaping term not only achieves higher accuracy but also explores more diverse reasoning strategies than vanilla GRPO (continued)."
        },
        {
            "title": "Preprint",
            "content": "GRPO with Auxiliary Shaping Term#1-#8 To solve this problem, we can use the Law of Sines in trigonometry. (Details omitted as they are not crucial for this case study.) Given that AC 48 feet, we can use the Law of Sines to find AB: AB sinp60q AC sinp30q (Details omitted as they are not crucial for this case study.) Solving for AB: (Details omitted as they are not crucial for this case study.) Given that =ABC 60 and AC 48 feet, we can use the Law of Sines to find AB: (Details omitted as they are not crucial for this case study.) The square of the distance AB is: ? p32 3q2 322ˆ3 1024ˆ3 3072 AB 96ˆ ? 3 2 ? 3 48 (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 6912 . To find the square of the distance between Abdul and Bharat, we can use the Law of Sines in trigonometry. Lets denote the distance between Abdul and Bharat as AB, and the distance between Chiang and Bharat as BC. Since Bharat is as far from Abdul as possible, triangle ABC is right triangle with =ACB 90. Given: - =ABC 60 - AC 48 feet We want to find AB2. First, we apply the Law of Sines in triangle ABC: (Details omitted as they are not crucial for this case study.) Thus, the square of the distance between Abdul and Bharat is 3072 . Thus, the square of the distance between Abdul and Bharat is 3072 . To solve this problem, we need to use some geometric principles. Lets denote the positions of Abdul, Chiang, and Bharat as points A, C, and respectively. We are given that the distance between and (AC) is 48 feet, and the angle ABC is 60. We need to find the distance between and (AB), and then square it. (Details omitted as they are not crucial for this case study.) Step 3: Recognize that if triangle ABC is equilateral, then all sides are equal. Therefore, AB = BC = AC = 48 feet. Step 4: Calculate the square of the distance AB. Since AB = 48 feet, we square this value to get AB². AB2 482 2304 So, the square of the distance between Abdul and Bharat is 2304 . (Details omitted as they are not crucial for this case study.) 1. Identify the given information: (Details omitted as they are not crucial for this case study.) 2. Visualize the problem: (Details omitted as they are not crucial for this case study.) 3. Determine the type of triangle: - Since angle ABC and angle ACB are both 60, triangle ABC is an equilateral triangle. In an equilateral triangle, all sides are equal. 4. Calculate the side length of the equilateral triangle: - Since AC = 48 feet and triangle ABC is equilateral, AB (the distance from Abdul to Bharat) is also 48 feet. (Details omitted as they are not crucial for this case study.) So, the square of the distance between Abdul and Bharat is 2304 . (Details omitted as they are not crucial for this case study.) In right triangle, if one of the angles is 60, the triangle is 30-60-90 triangle. In 30-60-90 triangle, the sides are in the 3 : 2. Here, ratio 1 : AC is the hypotenuse, so we can set up the relationship as follows: ? AC 2 ˆ AB 48 2 ˆ AB AB 48 2 24 feet We need to find the square of the distance AB: AB2 242 576 Thus, the square of the distance between Abdul and Bharat is 576 . To solve this problem, we can use the Law of Sines and some geometric properties. Lets denote the positions of Abdul, Chiang, and Bharat as points A, C, and respectively. The distance between and is given as 48 feet. The angle =ACB is 60. We need to find the square of the distance AB, which is the farthest distance Bharat can be from Abdul. (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 6912 . (Details omitted as they are not crucial for this case study.) Using the properties of 30-60-90 triangle, we know that the side oppo- ? 3 site the 60 angle is times the side opposite the 30 angle. Lets denote the side opposite the 30 angle (which is half the distance between Abdul and Chiang) as x. Therefore, AB BC 2x. (Details omitted as they are not crucial for this case study.) 3 and ? AB2 2304p2112 ? 3q Since we are asked to find the square of the distance, and we have already calculated it, we can box the final answer: Figure 16: Case study: Geometry problem. GRPO with auxiliary shaping term not only achieves higher accuracy but also explores more diverse reasoning strategies than vanilla GRPO (continued)."
        },
        {
            "title": "Preprint",
            "content": "GRPO with Auxiliary Shaping Term#9-#16 To solve this problem, we need to use some geometric properties and trigonometric relationships. Lets break it down step by step. 1. Identify the given information: (Details omitted as they are not crucial for this case study.) 2. Visualize the problem: (Details omitted as they are not crucial for this case study.) 3. Use the Law of Cosines: (Details omitted as they are not crucial for this case study.) 4. Apply the Law of Cosines: (Details omitted as they are not crucial for this case study.) 5. Find the square of the distance: - The square of the distance between Abdul and Bharat is d2 2304. Therefore, the square of the distance between Abdul and Bharat is 2304 . To find the square of the distance between Abdul and Bharat, lets denote the distance between Abdul and Bharat as d, and the distance between Chiang and Bharat as d1. Since the angle formed by the lines of sight to Abdul and Chiang is 60, we can use the Law of Cosines in the triangle formed by Abdul, Chiang, and Bharat. The Law of Cosines states that for any triangle with sides a, b, and c, and the angle θ opposite side c: c2 a2`b22ab cospθq (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 2304 . To solve this problem, we can use the Law of Sines and some geometric properties. (Details omitted as they are not crucial for this case study.) Given that the angle ABC is 60 and AB is maximized, the triangle ABC is right triangle with angle ACB being 90 (since the angle at Bharat is 60 and the sum of angles in triangle is 180, the remaining angle at must be 30, making AB the hypotenuse of 30-60-90 triangle). (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 3072 . To solve this problem, well use some geometric principles, particularly the Law of Sines and properties of triangles. Lets break down the problem step-by-step: 1. Identify the given information: (Details omitted as they are not crucial for this case study.) 2. Visualize the problem: (Details omitted as they are not crucial for this case study.) 3. Use the Law of Sines: (Details omitted as they are not crucial for this case study.) 4. Apply the Law of Sines: (Details omitted as they are not crucial for this case study.) 5. Use trigonometry in the right triangle: (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 1728 . To solve this problem, we need to use some geometric principles. Lets denote the positions of Abdul, Chiang, and Bharat as points A, C, and respectively. The distance between and is 48 feet, and we need to find the distance AB when the angle =ABC is 60 and is as far from as possible. 1. Identify the configuration: (Details omitted as they are not crucial for this case study.) 2. Use the Law of Cosines: (Details omitted as they are not crucial for this case study.) 3. Substitute into the Law of Cosines: (Details omitted as they are not crucial for this case study.) So, the square of the distance between Abdul and Bharat is 3840 square feet. To solve this problem, we need to use some geometric properties and trigonometry. Lets denote the positions of Abdul, Chiang, and Bharat as points A, C, and respectively in the field. We know that AC = 48 feet and the angle =ACB = 60. We want to find the distance AB, which well denote as d, and then find d2. 1. Identify the type of triangle: (Details omitted as they are not crucial for this case study.) 2. Apply the Law of Cosines: (Details omitted as they are not crucial for this case study.) 3. Maximize the distance d: (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 1728 . To solve this problem, we can use the Law of Sines in trigonometry. (Details omitted as they are not crucial for this case study.) Using the Law of Sines: AB sinp120q AC sinp60q (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 3072 . To solve this problem, we need to use some geometric principles, specifically involving the Law of Sines and properties of triangles. Lets break it down step by step: 1. Identify the given information: (Details omitted as they are not crucial for this case study.) 2. Visualize the problem: (Details omitted as they are not crucial for this case study.) 3. Use the Law of Sines: (Details omitted as they are not crucial for this case study.) 4. Determine the angles: (Details omitted as they are not crucial for this case study.) 5. Calculate the distance AB: (Details omitted as they are not crucial for this case study.) Therefore, the square of the distance between Abdul and Bharat is 3072 . Figure 16: Case study: Geometry problem. GRPO with auxiliary shaping term not only achieves higher accuracy but also explores more diverse reasoning strategies than vanilla GRPO (continued)."
        }
    ],
    "affiliations": [
        "Shenzhen Technology University",
        "Tsinghua Shenzhen International Graduate School, Tsinghua University",
        "University of California, Los Angeles"
    ]
}