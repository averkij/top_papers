{
    "paper_title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "authors": [
        "Chengwen Liu",
        "Xiaomin Yu",
        "Zhuoyue Chang",
        "Zhe Huang",
        "Shuo Zhang",
        "Heng Lian",
        "Kunyi Wang",
        "Rui Xu",
        "Sen Hu",
        "Jianheng Hou",
        "Hao Peng",
        "Chengwei Qin",
        "Xiaobin Hu",
        "Hong Peng",
        "Ronghao Chen",
        "Huacan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents."
        },
        {
            "title": "Start",
            "content": "2026-01-13 Watching, Reasoning and Searching: Video Deep Research Benchmark on Open Web for Agentic Video Reasoning Chengwen Liu1*, Xiaomin Yu2*, Zhuoyue Chang1*, Zhe Huang10*, Shuo Zhang10*, Heng Lian10, Kunyi Wang3,10, Rui Xu4, Sen Hu5,10, Jianheng Hou6, Hao Peng1, Chengwei Qin2,9, Xiaobin Hu7, Hong Peng1, Ronghao Chen5,10, Huacan Wang8,10 1LZU, 2HKUST(GZ), 3UBC, 4FDU, 5PKU, 6USC, 7NUS, 8UCAS, 9HKUST, 10QuantaAlpha *These authors contributed equally to this work. Correspondence: pengh@lzu.edu.cn, chenronghao@alumni.pku.edu.cn, wanghuacan17@mails.ucas.ac.cn Github: https://github.com/QuantaAlpha/VideoDR-Benchmark"
        },
        {
            "title": "Abstract",
            "content": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoningbased verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint videoweb evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on models ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents. 6 2 0 2 1 1 ] . [ 1 3 4 9 6 0 . 1 0 6 2 : r Figure 1: Overview of the VideoDR construction pipeline."
        },
        {
            "title": "Introduction",
            "content": "In existing multimodal evaluations, video remains significant weakness (Li et al., 2024a; Fang et al., 2024; Wu et al., 2024; Jang et al., 2024). On the one hand, video reasoning inherently requires crosstemporal cue tracking and spatiotemporal modeling (Wu et al., 2024; Yang et al., 2025b; Chen et al., 2025a); on the other hand, most existing evaluations adopt closed-evidence setting, where models typically only need to answer questions within the given video, without interacting with evidence on the open web (Fu et al., 2025; Zhou et al., 2025; Wang et al., 2025a; Yang et al., 2025a). As result, the capability of using videos as clues and completing fact verification and reasoning synthesis on open webpages has not been systematically characterized. Meanwhile, deep research agents are pushing question answering from static contexts toward active evidence exploration on the open web: instead of answering directly from given context, systems must conduct multiple rounds of search, filtering, and cross-checking in real web environments, ultimately producing conclusions grounded in evidence (Chen et al., 2025b; Zheng et al., 2025). large number of deep research benchmarks have emerged around this capability (Li et al., 2025a; Jin et al., 2025). However, overall, these benchmarks still mostly start from textual queries (Wei et al., 2025; Wu et al., 2025; Li et al., 2025b); even when multimodal information is introduced, visual content is often treated as static auxiliary information rather than key evidence that must be precisely tracked and propagated (Jiang et al., 2024). However, in real use, videos often carry decisive clues. User questions about videos are typically opendomain factoid questions: the knowledge to be verified does not directly appear in the video or its title, but is distributed across large and dynamically changing web corpora; meanwhile, the key clues relevant to the question lie along the video timeline and must be extracted through cross-frame association. In this research pattern, videos provide localized visual cues, and webpages provide verifiable answersyet it is not directly covered by existing deep research benchmarks that take text as input (Wei et al., 2025), nor by video benchmarks that assume evidence is closed within the video (Li et al., 2024a; Fang et al., 2024; Zhou et al., 2025). Based on these gaps, we propose VideoDR, the first open-domain benchmark that systematically evaluates video deep research. As shown in Figure 2, we extend deep research to an open-domain factoid question answering setting conditioned on video: models must extract and compose visual anchors from multiple frames (Yang et al., 2025b; Chen et al., 2025a; Zhang et al., 2025), use browser-based search to locate candidate evidence on the open web (Wei et al., 2025; Wu et al., 2025; Li et al., 2025b; Chen et al., 2025b; Zheng et al., 2025), and perform multi-hop reasoning in the joint evidence space of videos and webpages to output unique and verifiable answer (Liang et al., 2025). To support this setting, we explicitly incorporate an interactive web search process into the task definition and adopt annotation and strict quality control over diverse real-world scenarios to systematically remove samples that can be answered by the video alone or by webpages alone, making the combined capability of video understanding, web search, and evidence-based reasoning the core evaluation target (Yao et al., 2022; Gao et al., 2023). Based on VideoDR, we evaluate mainstream models under both the Workflow and Agentic paradigms (Liu et al., 2025), including closed-source models GPT-4o (Hurst et al., 2024) and Gemini-3-pro-preview (Team et al., 2023), as well as open-source models MiniCPM-V 4.5 (Yu et al., 2025b), Qwen3-Omni-30B-A3B (Xu et al., 2025), and InternVL3.5-14B (Wang et al., 2025b), and conduct comprehensive performance and error analyses along three dimensions: difficulty, video duration, and semantic domain. Our main contributions are as follows: ❶ Video Deep Research Task: We first define the Video Deep Research task, shifting video understanding from closed-context perception to active, multi-hop search and reasoning on the open web anchored by video cues. ❷ VideoDR Benchmark: We construct high-quality VideoDR benchmark through rigorous human annotation and quality control. VideoDR benchmark ensures that the multi-step evidence gathering process maintains strong dependency on multi-frame visual cues within the video. 2 Figure 2: An example of the VideoDR task: identifying museum via video visual cues, then using multi-hop search to find the closest \"dont miss\" exhibit to the entrance and outputting its accession number WB.67. ❸ Agent Capability Boundaries: By benchmarking leading MLLMs across Workflow and Agentic paradigms, we delimit the capability boundaries of these two agentic approaches. Leveraging the diverse distribution of VideoDR across semantic domains, question lengths, and video durations, we systematically analyze the performance and error patterns under different paradigms. Our findings reveal that Goal Drift and Long-horizon Consistency are the core bottlenecks constraining the development of next-generation video deep research agents."
        },
        {
            "title": "2 Related Work",
            "content": "Deep Research Benchmarks. Existing deep research evaluations typically assess search, reasoning, and tool use as unified process (Wei et al., 2025; Wu et al., 2025; Li et al., 2025b; Zheng et al., 2025): one line of work tests multi-step query planning and information localization in live web environments (Wei et al., 2025; Wu et al., 2025), while another studies more controllable settings to improve reliability and stability (Xue et al., 2025; Chen et al., 2025b; Jin et al., 2025; Li et al., 2025a). Overall, however, most benchmarks still start from textual queries (Wei et al., 2025; Wu et al., 2025), and visual content is often downplayed as static auxiliary information rather than first-class evidence in the retrieval and verification loop (Jiang et al., 2024; Liang et al., 2025; Liu et al., 2025). Video Reasoning Benchmarks. Existing video reasoning evaluations are conducted under closedevidence setting, where questions are designed to be answerable using only the video itself, and they primarily stress long-video temporal understanding and long-context reasoning (Fu et al., 2025; Li et al., 2024a; Wu et al., 2024; Zhou et al., 2025; Wang et al., 2025a; Fang et al., 2024; Yang et al., 2025a; Li et al., 2024b; Nagrani et al., 2024; Yu et al., 2025a). Recent agentic video efforts also explore multi-round interaction and tool calling within the video-understanding loop (Zhang et al., 2025; Yang et al., 2025b; Chen et al., 2025a). In summary, systematic evaluations of multi-step evidence gathering and reasoning integration on the open web anchored to video cues remain relatively scarce."
        },
        {
            "title": "3.1 Task Definition",
            "content": "We propose Video Deep Research (VideoDR): an open-domain factoid question answering benchmark conditioned on given video, designed to evaluate models ability to perform complex reasoning anchored in the video while leveraging the open web. Given video and natural language question Q, the model can interactively call browser search tool S, iteratively searching between video cues and web-page evidence, and finally output factual answer A. Each VideoDR sample is constructed such that the model must exploit multi-frame cues from the video to locate candidate evidence on the open web, and then perform multi-hop reasoning over the joint evidence space of the video and the open web in order to obtain unique answer. Formally, the VideoDR task can be expressed as: : (V, Q; S) A."
        },
        {
            "title": "3.2 Data Annotation Process",
            "content": "In the data construction stage, we recruited three annotators with experience in video understanding and web search to create questions and annotate answers. Each annotator must actively locate several multi-frame visual cues in the video and design corresponding multi-hop questions and answers around these cues. To ensure annotation consistency, we provided unified annotation guidelines and examples before the annotation phase. As shown in Figure 1, the overall annotation pipeline consists of three steps. Candidate Video Pool. Annotators first select videos from different platforms and then perform stratified sampling along three dimensions: source, domain, and duration, to cover diverse real-world scenarios. To ensure data quality, we apply strict negative filtering strategy to remove the following three types of videos: ❶ single-scene clips with highly redundant visual semantics; ❷ popular topics whose information is overly explicit and can be obtained via text search without watching the video; ❸ isolated content on the open web for which no verifiable chain of evidence can be found. Initial Filtering. At the initial screening stage, we manually remove clips that lack prominent visual anchors. We only retain videos that exhibit coherent visual cues at multiple time points and are suitable for cross-frame association, as candidates for subsequent annotation. For longer videos, annotators are allowed to extract multiple semantically focused segments from the same video and construct separate questions for each segment. Question Design. As shown in Figure 2, in this stage, annotators are required to design high-complexity questions for the retained video segments, under two strict constraints: ❶ Multi-frame reasoning: Each question must be grounded in video cues spanning multiple frames; it should be impossible to answer the question from single-frame screenshot. ❷ Multi-hop reasoning: Each question must implicitly admit decomposable multi-step reasoning path, forcing the model to perform at least one round of information exchange between video perception and external search. To ensure verifiability, we archived the web pages containing the key evidence supporting each answer."
        },
        {
            "title": "3.3 Quality Control",
            "content": "To ensure that VideoDR is rigorous and unambiguous, we apply two-stage quality verification procedure to the annotated samples."
        },
        {
            "title": "3.3.1 Video & Web Dependency Testing",
            "content": "To verify that the annotated samples simultaneously depend on both the video and search, we design two ablation settings and conduct tests on all samples. Only samples that fail under both of the following conditions are retained as valid: Web-only Test. The annotator is only given the textual question and can conduct the web search. If unique and unambiguous answer can be obtained purely through web search, the question is deemed to lack dependence on visual anchors and is discarded. 4 Figure 3: Human solvability across benchmark difficulty levels. Video-only Test. The annotator answers the question by watching only the video . If the answer can be directly obtained from the video information alone, the question degenerates into pure video understanding task and is discarded."
        },
        {
            "title": "3.3.2 Human Testing",
            "content": "To verify the correctness of annotated questions and characterize their difficulty, we adopt human evaluation protocol with five independent participants solving the tasks in blind manner. For each sample i, five subjects independently produce answers under the condition that they can only access the annotated video and the textual question, without being given any reference answers, forming the answer set {ai1, . . . , ai5}. Subjects are required to complete the task in browser environment and must, for each question, submit final answer obtained via video browsing and autonomous web search. This process serves two purposes: ❶ to validate the solvability of the questions and the correctness of the annotations under conditions close to real-world usage; and ❷ to provide empirical evidence for subsequent difficulty stratification and estimation of the human upper bound. Difficulty. To quantify question difficulty at the sample level, we define the difficulty score of sample based on the human success rate from the five-subject blind evaluation: si = 1 5 5 (cid:88) j= 1[aij Ai], where aij denotes the answer provided by the j-th subject for sample i, Ai is the gold answer for that sample, and 1[] is the indicator function (1 if the prediction is semantically equivalent to the gold answer, and 0 otherwise). We treat the distribution of {si} as proxy for question difficulty and stratify samples by the number of correct human answers: if only 01 out of 5 subjects answer correctly, the sample is labeled High; if 23 subjects answer correctly, it is labeled Mid; and if 45 subjects answer correctly, it is labeled Low. This difficulty partition is used in subsequent experiments to analyze how different models perform across human difficulty levels. Figure 3(a) shows the distribution of samples across three difficulty levels. Human performance. Under the above setting, we estimate the human upper bound by the average accuracy of the 5 subjects over all samples: ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 si. Figure 3(b) shows the human test results. Across all 100 samples, the mean sample-level success rate is = 0.504 with standard deviation of 0.346. When results are stratified by difficulty, the mean 5 Figure 4: Data statistics of VideoDR, including (a) video category, (b) question length, and (c)video duration. si is 0.900 0.190 for Low, 0.506 0.101 for Mid, and 0.106 0.101 for High. Overall, human participants can reliably solve the majority of samples, and we do not observe systematic pattern of universal failure, which indirectly supports the consistency of the annotated answers. In addition, during dataset construction, we manually re-examine samples with clear disagreement or samples where multiple mutually inconsistent yet seemingly reasonable answers are provided; if ambiguity remains after multiple rounds of verification, the sample is discarded. The final retained dataset ensures that each question corresponds to unique and verifiable gold answer, given the available evidence, thereby reducing evaluation noise."
        },
        {
            "title": "3.4 Data Statistics",
            "content": "Finally, we construct 100 VideoDR samples. To characterize the benchmarks structural properties, and to provide basis for stratified analyses in subsequent experiments, we summarize the dataset from three perspectives: ❶ video category distribution, ❷ question length distribution, and ❸ video duration distribution. Video category Distribution. As shown in Figure 4(a), VideoDR spans six semantic domains: Daily Life, Economics, Technology, Culture, History, and Geography. The distribution is relatively balanced: Daily Life accounts for 33%, Economics 16%, Technology and Culture 15% each, History 11%, and Geography 10%. This distribution ensures broad coverage of diverse content types in open-domain settings. Question Length. As shown in Figure 4(b), we compute the token-length distribution of the naturallanguage questions. The average question length is 25.54 tokens, and the lengths are concentrated: the 95th percentile is 54 tokens. This indicates that most questions are compact in phrasing, maintaining concise language while preserving necessary constraints. As result, evaluation can focus more on the core process of starting from video cues and combining web evidence to perform multi-hop reasoning, rather than on extra comprehension burden induced by overly long inputs. Video duration. As shown in Figure 4(c), on the video side, durations exhibit clear long-tailed pattern: most videos cluster in short-duration range, while small number of videos longer than 10 minutes form the tail. This duration structure covers both rapid cue capture in short-video scenarios and cross-segment association in long-video scenarios, enabling more comprehensive test of models cross-frame understanding and evidence localization across different time scales."
        },
        {
            "title": "4.1 Setting",
            "content": "Baselines. We compare two common paradigms: Workflow and Agentic. Workflow adopts two-stage design: multimodal model first extracts question-relevant cross-frame visual cues from the video and produces structured intermediate text, which is then provided together with the question as input for subsequent reasoning. Without further access to the original video, the system uses the search tool to 6 Table 1: Performance comparison across difficulty levels under the Workflow and the Agentic settings."
        },
        {
            "title": "Agentic",
            "content": "Low Mid High Ave. Low Mid High Ave. #Samples 32 36 100 32 36 32 100 Qwen3-Omni-30B-A3B 59.38 37.50 InternVL3.5-14B 50.00 MiniCPM-V 4.5 90.62 Gemini-3-pro-preview 50.00 GPT-4o 84.38 GPT-5. 30.56 22.22 13.89 61.11 30.56 61.11 21.88 21.88 12.50 56.25 46.88 62.50 37.00 27.00 25.00 69.00 42.00 69.00 65.62 46.88 18.75 93.75 62.50 84.38 27.78 22.22 19.44 69.44 38.89 58.33 18.75 21.88 9.38 65.62 28.12 65. 37.00 30.00 16.00 76.00 43.00 69."
        },
        {
            "title": "Human",
            "content": "90.00 50.56 10.63 50.40 90.00 50. 10.63 50.40 Table 2: Performance comparison across different video durations under the Workflow and the Agentic settings."
        },
        {
            "title": "Agentic",
            "content": "Short Medium Long Ave. Short Medium Long Ave. #Samples 52 38 100 52 38 10 100 Qwen3-Omni-30B-A3B 34.62 36.54 InternVL3.5-14B 30.77 MiniCPM-V 4.5 75.00 Gemini-3-pro-preview 46.15 GPT-4o 75.00 GPT-5. 36.84 15.79 15.79 65.79 36.84 60.53 50.00 20.00 30.00 50.00 40.00 70.00 37.00 27.00 25.00 69.00 42.00 69.00 38.46 32.69 17.31 71.15 51.92 76.92 39.47 28.95 15.79 84.21 31.58 63.16 20.00 20.00 10.00 70.00 40.00 50. 37.00 30.00 16.00 76.00 43.00 69."
        },
        {
            "title": "Human",
            "content": "51.92 50.53 42.00 50.40 51.92 50. 42.00 50.40 retrieve candidate evidence from the open web, and uses the think tool to reflect on and filter evidence and to generate the next-round query, finally aggregating multi-round search results to produce an answer. In contrast, Agentic adopts stronger end-to-end setting: it feeds the raw video and the question directly into single multimodal agent, which performs video understanding, query generation, web retrieval, and evidence integration within the same execution loop. The agent likewise uses only the think tool and the search tool, and autonomously decides when to initiate search based on video cues, when to reflect, and how to organize multi-round evidence into the final conclusion. MLLMs. We select mainstream multimodal models spanning both closed-source and open-source families as the core research agents. Closed-source models include Gemini-3-pro-preview (Team et al., 2023), GPT-4o (Hurst et al., 2024), GPT-5(Hurst et al., 2024); open-source models include MiniCPM-V 4.5 (Yu et al., 2025b), InternVL3.5-14B (Wang et al., 2025b), and Qwen3-Omni-30B-A3B (Xu et al., 2025) . All models are tested under both Workflow and Agentic paradigms to analyze their capability boundaries under different system organizations. LLM as Judge. VideoDR is an open-domain factual question answering task. To avoid false mismatches, we adopt an LLM-as-judge protocol (Zheng et al., 2023) using DeepSeek-V3-0324 (Liu et al., 2024) to assess semantic equivalence between the model prediction and the reference answer. The judge outputs binary correctness label, which we use to compute overall Accuracy."
        },
        {
            "title": "4.2 Main Results",
            "content": "From the overall averages, our VideoDR setting yields clear capability stratification among existing models: Gemini-3-pro-preview leads under both paradigms (Workflow 69% / Agentic 76%), closely followed by GPT-5.2 which demonstrates comparable top-tier performance (Workflow 69% / Agentic 69%), while GPT-4o consistently forms the second tier (42% / 43%), and the three open-source models 7 Table 3: Performance comparison across different domains under the Workflow and the Agentic settings. Model Setting Domain (%) History Geography Culture Economy Technology Daily Life Ave. #Samples Qwen3-Omni-30B-A3B InternVL3.5-14B MiniCPM-V 4. Gemini-3-pro-preview GPT-4o GPT-5.2 Human Workflow Agentic Workflow Agentic Workflow Agentic Workflow Agentic Workflow Agentic Workflow Agentic 11 36.36 54. 9.09 36.36 27.27 9.09 72.73 81.82 63.64 63.64 72.73 90.91 36. 10 30.00 40.00 50.00 40.00 10.00 10.00 70.00 50.00 40.00 20. 70.00 70.00 34.00 15 26.67 26.67 20.00 26.67 46.67 26. 80.00 86.67 33.33 53.33 80.00 73.33 49.33 16 43.75 43. 25.00 31.25 25.00 12.50 62.50 68.75 43.75 50.00 56.25 56.25 56. 14 50.00 35.71 21.43 28.57 14.29 14.29 64.29 85.71 42.86 35. 64.29 71.43 60.00 33 36.36 33.33 30.30 24.24 24.24 18. 69.70 78.79 39.39 39.39 72.73 66.67 52.73 100 37.00 37. 27.00 30.00 25.00 16.00 69.00 76.00 42.00 43.00 69.00 69.00 50. are overall weaker (Qwen3-Omni-30B-A3B 37% / 37%, InternVL3.5-14B 27% / 30%, MiniCPM-V 4.5 25% / 16%). Difficulty stratification reveals the essential differences between the two paradigms. As shown in Table 1, all evaluated models exhibit highly consistent decline in performance as the difficulty level increases from Low to Mid and then to High. Human performance decreases from 90.00% to 50.56% and then to 10.63%, and models also generally degrade as difficulty increases, suggesting that our difficulty definition based on human success rate indeed corresponds to longer and more fragile evidence chains. Gains from Agentic mainly appear in models with stronger state maintenance capabilities, and become more pronounced toward Mid/High; conversely, mid-tier models can suffer clear backlash on High. For example, Gemini-3-pro-preview continues to improve on Mid/High from 61.11%69.44% and 56.25%65.62%, and GPT-5.2 similarly achieves gains on High difficulty (62.50%65.62%); but while GPT-4o improves on Low/Mid (50.00%62.50%, 30.56%38.89%), it drops sharply on High (46.88%28.12%). This phenomenon is highly consistent with our implementation constraints: in Agentic, the model cannot re-watch the video during research, so the subsequent process must rely solely on cues obtained from the initial viewing to guide multi-round search and reasoning, lacking opportunities for online correction via revisiting the video. Once early visual anchors drift, web noise can amplify the deviation in later searches, making High-difficulty questions more prone to goal drift; in contrast, Workflow externalizes video cues into an intermediate textual representation, effectively providing repeatedly accessible external memory that reduces drift risk in long-horizon decision-making. Video-duration stratification further reinforces this explanation: as videos get longer, cues become more dispersed, and the state space grows, so Agentic places higher demands on models ability to retain and continuously leverage initial video cues over the long run, leading to strong polarization. As shown in Table 2, Gemini-3-pro-preview achieves substantial gains on Medium/Long (65.79%84.21%, 50.00%70.00%), indicating that strong models can preserve enough discriminative details from one viewing and keep using them in subsequent search; in contrast, Qwen3-Omni-30B-A3B and MiniCPMV 4.5 drop markedly on Long (50.00%20.00%, 30.00%10.00%), suggesting that when model struggles to stably preserve and continuously utilize key video anchors across multiple rounds of search and reasoning, directly using the video does not translate into better search decisions and instead makes the system more likely to destabilize over long chains. In other words, long videos amplify the key trade-off between the two paradigms: Workflow may lose details but is more controllable, whereas Agentic preserves details more faithfully but relies more on long-term consistency. Domain stratification highlights the respective advantages of the two paradigms across domains. As shown in Table 3, in Technology, Gemini-3-pro-preview shows the largest Agentic-over-Workflow 8 Table 4: Tool-use statistics under the Workflow and Agentic paradigms."
        },
        {
            "title": "Agentic",
            "content": "think search time (s) think search time (s) Qwen3-Omni-30B-A3B 1.82 1.58 InternVL3.5-14B 0.94 MiniCPM-V 4.5 2.40 Gemini-3-pro-preview 1.90 GPT-4o 2.90 GPT-5.2 0.95 1.48 1.13 1.86 1.11 2.94 222.86 214.28 242.21 422.48 136.15 569.13 1.80 1.20 1.97 2.89 1.31 3.06 1.21 1.24 2.07 2.52 1.17 2. 367.19 228.05 139.07 449.44 181.87 1375.83 Table 5: Error type distribution across different models under both the Workflow and the Agentic. Model Setting Error Type Count Categorical Incomplete Not Found Numerical Context Semantic Reasoning Others Total Qwen3-Omni-30B-A3B InternVL3.5-14B MiniCPM-V 4.5 Gemini-3-pro-preview GPT-4o GPT-5.2 Workflow Agentic Workflow Agentic Workflow Agentic Workflow Agentic Workflow Agentic Workflow Agentic 22 16 25 29 25 36 5 7 22 8 7 16 20 18 17 23 13 10 8 11 11 8 6 11 20 15 13 14 4 1 8 7 10 7 11 9 7 10 12 9 6 10 3 5 11 3 1 0 2 4 0 1 2 0 1 0 1 0 1 1 1 2 0 2 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 3 1 0 3 0 0 63 63 73 70 75 84 31 24 58 31 31 improvement (64.29%85.71%), indicating that such questions often require translating fine-grained visual cues into high-precision queries, and Workflows one-shot summary is more likely to miss crucial search triggers. However, in Geography, both Gemini-3-pro-preview (70.00%50.00) and GPT-4o (40.00%20.00) degrade noticeably, suggesting that geographic search is more ambiguous and therefore depends more on clear and consistent visual anchors to keep the search target stable. Across all three stratifications, we conclude that under current mainstream model capabilities, Agentic is not necessarily superior to Workflow: its effectiveness depends on whether the model can continually rely on initial video cues throughout multi-round search and reasoning; meanwhile, Workflow provides stable anchors for downstream search and reasoning through an explicit intermediate text, thereby more effectively reducing the risk that the search process drifts away from the correct target."
        },
        {
            "title": "4.3 Tool-use Analysis",
            "content": "As shown in Table 4, the number of tool calls is not monotonically correlated with accuracy. What determines performance is not using tools more, but whether model can turn limited number of search calls into high-quality evidence chain. Concretely, Gemini-3-pro-preview demonstrates stronger tool-use effectiveness: under the Agentic setting, it averages 2.89/2.52 think/search calls (449s runtime) and achieves the best overall score (76%), indicating that additional retrieval and reflection are indeed converted into more reliable evidence integration. In contrast, Qwen3-Omni-30B-A3Bs extra search overhead does not yield corresponding performance gains: its think usage is nearly identical between Agentic and Workflow (1.80 vs. 1.82), but Agentic has higher search usage and substantially longer runtime (1.21, 367s vs. 0.95, 223s), while accuracy remains unchanged (both 37%), suggesting that the extra searches mostly fall into low-yield exploration or failed evidence filtering. An even sharper contrast is MiniCPM-V 4.5: under Agentic it uses tools more frequently (think/search 1.97/2.07) and runs faster (139s), yet accuracy drops from 25% (Workflow) to 16%; combined with its across-the-board decline on High difficulty in Table 1 (12.50%9.38%), this indicates that it provides stable reference for downstream research via an explicit intermediate text representation, making search and reasoning more focused on relevant evidence and thus less likely to drift."
        },
        {
            "title": "4.4 Error Analysis",
            "content": "To obtain more fine-grained understanding of MLLMs behaviors under different paradigms, we conduct detailed analysis of their error trajectories across all experiments. We further break down the errors into eight categories, as shown in Table 5. Perception Grounding Limitation. Table 5 indicates that Reasoning Error is nearly absent under both paradigms, with counts ranging from 0 to 1 in Workflow and likewise 0 to 1 in Agentic. In contrast, Categorical Error remains the dominant failure mode across all models, spanning 525 cases in Workflow and 736 cases in Agentic, and it increases for some models when switching to Agentic (MiniCPM-V 4.5 rises from 25 to 36). This pattern is closely tied to our setting: once the research phase cannot revisit the video, downstream retrieval and reasoning depend entirely on the visual anchors extracted from the first pass. When early perception deviates, there is no mechanism to re-localize key frames and correct the anchor, making error propagation along the evidence chain more likely. Numerical Error Bottleneck. Table 5 shows that Geminis overall error count is substantially lower in the Agentic setting (24 total errors, compared with 57 for GPT-4o), yet this advantage does not carry over to Numerical Error. Under Workflow, Gemini-3-pro-preview has 9 numerical errors, while Qwen3-Omni30B-A3B, InternVL3.5-14B, MiniCPM-V 4.5, and GPT-4o have 7, 9, 10, and 10, respectively. Under Agentic, Gemini-3-pro-preview records 6 numerical errors, compared with 11, 7, 12, and 8 for the same four models. The numerical-error counts, therefore, remain tightly clustered across models, suggesting that numerical reliability constitutes distinct and persistent weakness for current MLLMs."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce VideoDR, video deep research benchmark for evaluating multimodal deep research on the open web. It requires models to extract multi-frame visual anchors, turn them into searchable queries, retrieve open-web evidence, and perform multi-hop reasoning to produce verifiable factual answers. We benchmark mainstream multimodal models under Workflow and Agentic paradigms, with stratified analyses by difficulty, video duration, and semantic domain to characterize capability boundaries on video deep search."
        },
        {
            "title": "6 Limitations",
            "content": "During the annotation process, while we enforced strict quality control protocol and verified the uniqueness of final answers, the intermediate search queries and reasoning paths were derived from the subjective search behaviors of our expert annotators. In real-world scenarios, different users might employ diverse keywords or browsing strategies to locate the same evidence. Although this does not affect the objective verifiability of the VideoDR samples, the current benchmark primarily reflects specific set of high-efficiency research trajectories. Future work could involve collecting wider variety of human search logs to better model the diversity of user-agent interactions."
        },
        {
            "title": "References",
            "content": "Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li, and Yali Wang. 2025a. Lvagent: Long video understanding by multi-round dynamical collaboration of mllm agents. arXiv preprint arXiv:2503.10200. Mingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, and 1 others. 2025b. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470. Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. 2024. Mmbenchvideo: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, and 1 others. 2025. Video-mme: The first-ever comprehensive evaluation benchmark of 10 multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1). Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Lawrence Jang, Yinheng Li, Dan Zhao, Charles Ding, Justin Lin, Paul Pu Liang, Rogerio Bonatti, and Kazuhito Koishida. 2024. Videowebarena: Evaluating long context multimodal agents with video understanding web tasks. arXiv preprint arXiv:2410.19100. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, and 1 others. 2024. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, and 1 others. 2024a. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yongkang Wu, Ji-Rong Wen, Yutao Zhu, and Zhicheng Dou. 2025b. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776. Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. 2024b. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303. Zhengyang Liang, Yan Shu, Xiangrui Liu, Minghao Qin, Kaixin Liang, Paolo Rota, Nicu Sebe, Zheng Liu, and Lizi Liao. 2025. Video-browsecomp: Benchmarking agentic video research on open web. arXiv preprint arXiv:2512.23044. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, and 1 others. 2025. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990. Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, and 1 others. 2024. Neptune: The long orbit to benchmarking long video understanding. arXiv preprint arXiv:2412.09582. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Ming Ding, Xiaotao Gu, Shiyu Huang, Bin Xu, and 1 others. 2025a. Lvbench: An extreme long video understanding benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2295822967. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Internvl3.5: Advancing open-source multimodal models in Shenglong Ye, Jie Shao, and 1 others. 2025b. versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516. 11 Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. 2024. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:28828 28857. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and 1 others. 2025. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, and 19 others. 2025. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. 2025. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479. Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, and Changsheng Xu. 2025a. Svbench: benchmark with temporal multi-turn dialogues for streaming video understanding. arXiv preprint arXiv:2502.10810. Zuhao Yang, Sudong Wang, Kaichen Zhang, Keming Wu, Sicong Leng, Yifan Zhang, Chengwei Qin, Shijian Lu, Xingxuan Li, and Lidong Bing. 2025b. Longvt: Incentivizing\" thinking with long videos\" via native tool calling. arXiv preprint arXiv:2511.20785. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. Jiashuo Yu, Yue Wu, Meng Chu, Zhifei Ren, Zizheng Huang, Pei Chu, Ruijie Zhang, Yinan He, Qirui Li, Songze Li, and 1 others. 2025a. Vrbench: benchmark for multi-step reasoning in long narrative videos. arXiv preprint arXiv:2506.10857. Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, and 1 others. 2025b. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. arXiv preprint arXiv:2509.18154. Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. 2025. Deep video discovery: Agentic search with tool use for long-form video understanding. arXiv preprint arXiv:2505.18079. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, and 1 others. 2025. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1369113701."
        }
    ],
    "affiliations": [
        "FDU",
        "HKUST",
        "HKUST(GZ)",
        "LZU",
        "NUS",
        "PKU",
        "QuantaAlpha",
        "UBC",
        "UCAS",
        "USC"
    ]
}