{
    "paper_title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
    "authors": [
        "Hanxu Hu",
        "Xingxing Zhang",
        "Jannis Vamvas",
        "Rico Sennrich",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have achieved strong performance on reasoning tasks, solving competition-level coding and math problems. However, their scalability is limited by human-labeled datasets and the lack of large-scale, challenging coding problem training data. Existing competitive coding datasets contain only thousands to tens of thousands of problems. Previous synthetic data generation methods rely on either augmenting existing instruction datasets or selecting challenging problems from human-labeled data. In this paper, we propose QueST, a novel framework which combines difficulty-aware graph sampling and difficulty-aware rejection fine-tuning that directly optimizes specialized generators to create challenging coding problems. Our trained generators demonstrate superior capability compared to even GPT-4o at creating challenging problems that benefit downstream performance. We leverage QueST to generate large-scale synthetic coding problems, which we then use to distill from strong teacher models with long chain-of-thought or to conduct reinforcement learning for smaller models, proving effective in both scenarios. Our distillation experiments demonstrate significant performance gains. Specifically, after fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we surpass the performance of the original Qwen3-8B on LiveCodeBench. With an additional 112K examples (i.e., 28K human-written problems paired with multiple synthetic solutions), our 8B model matches the performance of the much larger DeepSeek-R1-671B. These findings indicate that generating complex problems via QueST offers an effective and scalable approach to advancing the frontiers of competitive coding and reasoning for large language models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 5 1 7 7 1 . 0 1 5 2 : r Preprint. QUEST: INCENTIVIZING LLMS TO GENERATE DIFFICULT PROBLEMS Hanxu Hu1 Xingxing Zhang2 Jannis Vamvas1 Rico Sennrich1 Furu Wei2 1University of Zurich 2Microsoft Research"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models have achieved strong performance on reasoning tasks, solving competition-level coding and math problems. However, their scalability is limited by human-labeled datasets and the lack of large-scale, challenging coding problem training data. Existing competitive coding datasets contain only thousands to tens of thousands of problems. Previous synthetic data generation methods rely on either augmenting existing instruction datasets or selecting challenging problems from human-labeled data. In this paper, we propose QueST, novel framework which combines difficulty-aware graph sampling and difficulty-aware rejection finetuning that directly optimizes specialized generators to create challenging coding problems. Our trained generators demonstrate superior capability compared to even GPT-4o at creating challenging problems that benefit downstream performance. We leverage QueST to generate large-scale synthetic coding problems, which we then use to distill from strong teacher models with long chain-of-thought or to conduct reinforcement learning for smaller models, proving effective in both scenarios. Our distillation experiments demonstrate significant performance gains. Specifically, after fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we surpass the performance of the original Qwen3-8B on LiveCodeBench. With an additional 112K examples (i.e., 28K human-written problems paired with multiple synthetic solutions), our 8B model matches the performance of the much larger DeepSeek-R1-671B. These findings indicate that generating complex problems via QueST offers an effective and scalable approach to advancing the frontiers of competitive coding and reasoning for large language models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Test-time scaling through long chain-of-thought and large-scale reinforcement learning has dramatically boosted the reasoning ability of large language models, enabling LLMs to solve competitionlevel coding and math problems that were previously beyond their reach. Models like OpenAI o1 (OpenAI, 2024) and DeepSeek-R1 (Guo et al., 2025) have demonstrated remarkable performance on challenging benchmarks such as Codeforces, AIME, and IOI, achieving expert-level problem-solving capabilities through extensive reasoning traces that can span thousands of tokens. However, the problems used for training these models even require expert-level human annotations, which severely limits the scalability of LLM training. Current competitive coding datasets (Li et al., 2022; 2023) contain only thousands to tens of thousands of problems. As LLMs become more capable, the requirements for training data grow increasingly demandingoften requiring PhD-level experts in mathematics, computer science, and algorithm design to propose novel problems that can genuinely challenge these models. This process is not only extremely costly for requiring experts to propose difficult problems but also fundamentally cannot scale in terms of both dataset size and problem difficulty, creating critical bottleneck in the development of next-generation reasoning models. To mitigate this problem, methods for synthetic data generation and augmentation have been proposed. Previous works have focused either on paraphrasing-based augmentation (Luo et al., 2025a; Yu et al., 2024) or extracting concepts and recombining them based on co-occurrence probabilities (Tang et al., 2024; Zhao et al., 2025). Some recent works have proposed leveraging model weaknesses Work done during internship at Microsoft Research. 1 Preprint. Figure 1: Comparisons of Livecodebench scores and model parameters between LLMs trained using various methods. Our model (QueST-8B) achieves new Pareto optimum. and extracting concepts to create new problems (Liang et al., 2025). More recently, reasoning-based LLMs have presented the next paradigm in advancing large language model reasoning capabilities (OpenAI, 2024; Guo et al., 2025). Works like Guo et al. (2025); Guha et al. (2025) created long chain-of-thought responses from reasoning models and curated synthetic SFT datasets, effectively helping small open-weight LLMs achieve superior performance in code and math tasks. Ahmad et al. (2025) curated the largest open-source dataset by obtaining long CoT responses from DeepSeek-R1 multiple times for each problems, though the problems themselves are still sourced from humanlabeled competition coding problems. These methods have narrowed the performance gap between open-weight models and closed-weight reasoning models. However, existing methods described above either focus on leveraging existing human-annotated problems and curating synthetic responses from existing reasoning models, or rely on fixed LLM to generate new problems by prompting. In this paper, we are the first to propose method that directly trains an LLM generator to create challenging competitive code reasoning problems. We call our method QueST, embarking on quest to generate increasingly challenging code problems through the combination of difficulty-aware graph sampling and difficulty-aware rejection fine-tuning. This approach is more scalable and flexible compared to previous methods that used fixed generator or fixed human-labeled problems. Our proposed method makes the generator specialized and stronger than even proprietary models in creating problems that are challenging and useful for training of downstream tasks. We leverage this to generate the largest-scale code problem training set compared to previous synthetic data approaches, and the statistics of our synthetic data with previous data are shown in Table 1. We obtained responses from long chain-of-thought reasoning models, then leveraged the generated datasets to SFT small models. As illustrated in Figure 1, our 8B model (QueST-8B), trained on combination of 100K QueST-generated examples and an additional 112K examples derived from human-written problems, achieves state-of-the-art performance among models of similar size on code reasoning benchmarks. Notably, its results closely approach those of the much larger DeepSeek-R1 671B. Our contributions can be summarized as follows: We introduce novel difficulty-aware coding problems generation framework that combines both difficulty-aware graph sampling and difficulty-aware rejection fine-tuning, which trains specialized generators to create challenging coding problems. We create the largest synthetic code reasoning problem set to date, comprising over 100K challenging coding problems paired with detailed chain-of-thought solutions from reasoning models. Preprint. Table 1: Comparison between representative code reasoning datasets. Code Datasets #Problems CodeContest (Li et al., 2022) TACO (Li et al., 2023) Bespoke-Stratos (Labs, 2025) Open-R1 Codeforces-cots (Face, 2025) OpenCodeReasoning (Ahmad et al., 2025) Ours (QueST) 13K 26K 17K 10K 28K 100K Long CoT Responses Synthetic Problems We train 8B base model using our synthetic data combined with original data and achieved state-of-the-art performance with only 212K samples. Our model reaches the new Pareto optimum as shown in Figure 1. We conduct comprehensive ablation studies and analyses of our proposed method and the distribution of the generated coding problems."
        },
        {
            "title": "2 QUEST",
            "content": "In this section, we present QUEST, our proposed method for generating difficult problems. We focus our investigation on the generation of coding problems, as other forms of reasoning tasks (e.g., mathematical reasoning) can be regarded as special cases of coding tasks (Jiao et al., 2025). We begin by introducing our scaffolding framework for problem generation, which builds upon MathScale (Tang et al., 2024). Next, we detail our strategies for incentivizing LLMs to produce more difficult problems. Finally, we demonstrate how our scaffolding can be adapted to further enhance the generation of challenging problems. 2.1 PRELIMINARY: PROBLEM GENERATION THROUGH CONCEPT GRAPH Our scaffolding for problem generation is based on (Tang et al., 2024), which generates new problems based on existing seed problems by prompting an LLM in three steps (i.e., concept extraction, graph construction and problem generation). Concept Extraction For each problem in the seed problem set Qseed, we prompt an LLM to extract concepts (topics and knowledge points) from it. We follow the setting of Tang et al. (2024): Topics refers to general directions, knowledge points refers to more fine-grained concepts; an example can be found in Appendix Table 5. Note that problem generation can be guided later using the concepts that we extracted in this step. The process can be defined formally as follows: where pextract is the prompt used to extract concepts, GQ is our problem generator and is the set of concepts extracted. Detail prompts of pextract are in Appendix Table 7. = GQ((pextract, Qseed)) (1) Graph Construction Once we obtain the concepts C, we proceed to identify plausible combinations of these concepts. Two concepts are considered to form reasonable combination if they have frequently co-occurred within the same problem in the seed dataset. To capture these relationships, we construct concept graph in which nodes represent individual concepts, and edge weights encode the strength of co-occurrence between concept pairs. The edge weight w(u, v) is defined as follows: w(u, v) = log (freq(u, v) + ϵ) (2) where and denote concept nodes, and freq(u, v) represents the observed co-occurrence frequency of these concepts. small constant ϵ is added to ensure numerical stability by preventing zero counts. Given the constructed graph, we proceed to sample concept combinations, which are then utilized for the generation of new problems. We start from uniformly random sampling from all the topics and subsequently perform up to six steps of random walk on the graph (Tang et al., 2024). At each step, 3 Preprint. Figure 2: The pipeline of QueST. We first extract concepts based on seed problems, then use difficultyaware sampling method described in Equation 10 to create prompts for problem generation. We generate 8 problems for each prompt, calculate the difficulty δ of the generated problem based on Equation 6, and use the most difficult problem as rejection fine-tuning data to train our generator. the transition probability from node to node is defined as: pu,v = exp (w(u, v)) vN (u) exp (w(u, v)) (cid:80) (3) where (u) denotes the set of nodes adjacent to u. After each random walk episode, we obtain sampled concept combination s, which is subsequently used for problem generation. Problem Generation Given the sampled concept set s, we leverage an LLM to generate new problems. We incorporate few-shot examples to guide the LLM in formulating problems. These examples are selected from the pool of seed problems based on the Jaccard distance between their respective sets of concepts. Formally, this process can be described as: Qnew = GQ(pgenerate, S(C), Qseed) where S(C) denotes the set of sampled concepts, and pgenerate represents the prompt template utilized for problem generation (see Appendix Table 5 for additional details). (4) At this stage, our problem generator is designed to produce new problems, rather than explicitly targeting increased difficulty. The generation of more challenging problems will be addressed in the subsequent sections. 2.2 DIFFICULTY-AWARE REJECTION FINETUNING We focus primarily on the generation of challenging coding problems, though our approach is readily extensible to other forms of reasoning tasks. We first present our method for measuring problem difficulty, and then illustrate how this measure is employed to guide LLMs in producing more difficult problems. Difficulty Estimation natural way to assess the difficulty of generated problem is to examine the consistency of the models multiple outputs. Wang et al. (2023a) finds that self-consistency is highly correlated with accuracy, which reflects the uncertainty of the models, and also the difficulty of the problem. When most solutions converge to single outcome, the problem is likely straightforward. Conversely, if the solutions diverge and produce inconsistent outputs, this indicates model uncertainty, suggesting the problem is more difficult. Building on this intuition, we define the difficulty of problem using the average majority voting rate of its solutions. We illustrate our metric using coding problems as case study, noting that other verifiable reasoning problems (e.g. math) can be regarded as special case of this setting. Let GQ(pgenerate, s, Qseed) denote generated coding problem (see Equation (4)), where S(C) Preprint. is sampled concept combination (see Section 2.1). The estimation proceeds in three steps. First, we prompt gpt-4o to generate test inputs, forming the set = {i1, i2, . . . , iT } (details of the prompt in Appendix Figure 6). Second, we obtain candidate solutions = {y1, y2, . . . , yM } from gpt-4o. Third, we execute each ym on all inputs it I, producing output sets Ot = {g(y1, it), g(y2, it), . . . , g(yM , it)}, where g(ym, it) denotes extracting the code from ym, running it on input it, and recording the output. For each test input, the most likely output ot is identified as the most frequent element in Ot: ot = arg max oOt (o, Ot) (5) where (o, Ot) = (cid:12) problem difficulty as (cid:12){x Ot = o}(cid:12) (cid:12) counts the occurrences of in Ot. Finally, we quantify the δ(q) = 1 1 (cid:88) t= (ot, Ot) (6) Intuitively, δ(q) measures the degree of disagreement among candidate solutions: the lower the majority voting rate, the higher the difficulty. Thus, larger values of δ(q) correspond to more challenging problems. To further enhance the probability of generating valid synthetic problems, we filter out problems where over half of the test case outputs from generated responses return None, indicating unsuccessful code execution. Rejection Fine-tuning Having introduced the difficulty measure δ(q), we now describe how it is employed to construct dataset of promptproblem pairs for training LLMs to generate difficult problems. The key idea is to sample multiple candidate problems from the same prompt and retain only the most difficult one. As discussed in Section 2.1, for each concept combination s, problem can be generated via GQ(pgenerate, s, Qseed) (7) where GQ denotes the LLM-based generator. More generally, let Mθ be the LLM parameterized by θ, and let denote the actual prompt (pgenerate instantiated with concept set and seed problems Qseed) used to query Mθ. By sampling times, we obtain set of candidate problems: qk Mθ(p) for = 1, . . . , (8) We denote this set by = {q1, q2, . . . , qK}. We then select the most difficult problem according to our measure δ(): = arg max qkQ δ(qk) (9) Only is retained, while the remaining candidates are discarded. The resulting pair (p, q) is added to the training set Dhard, which is used to fine-tune the problem generator Mθ. 2.3 DIFFICULTY-AWARE GRAPH CONSTRUCTION This section extends our problem generation scaffolding (Section 2.1) to be difficulty-aware. In the baseline setup, the initial edge weights of the concept graph are determined primarily by the co-occurrence statistics of concepts within the same problems. Here, we further incorporate difficulty by modeling the hardness of concepts with respect to the difficulty levels of the problems in which they appear. Since each problem in the seed dataset (e.g., TACO; Li et al. (2023)) is annotated with human-curated difficulty labels, we leverage this information when constructing the concept graph for problem generation prompts. Specifically, beyond using co-occurrence counts as edge weights for random walk sampling, we also incorporate the average difficulty of all problems that involve both concepts connected by an edge. The new edge weights are defined as w(u, v) = log (α freq(u, v) + (1 α) diff(u, v) + ϵ) where diff(u, v) = 1 Qu,v (cid:88) qQu,v 5 d(q), Qu,v = { q, }. (10) Preprint. Here, α is hyperparameter that balances the contribution of co-occurrence frequency and difficulty and we set α = 0.2 in our experiments. The constant ϵ is included to avoid taking the logarithm of zero. The set Qu,v consists of all problems containing both concepts and v; its cardinality is denoted by Qu,v. Finally, d(q) represents the human-annotated difficulty level of problem q, given as an integer from 1 to 5."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In this section, we present the detail of our experiments. We first use our proposed difficulty measure method for data selection. Then we show the long CoT SFT results using datasets distilled from Qwen3-8B compared with previous strong baselines, and we show our generated datasets can also be effective when used in RL training. We further present an ablation study to investigate the effect of each role in our proposed method. Finally we have contamination analysis and statistics about our generated data. 3.1 IMPLEMENTATION DETAILS Seed data: We use TACO (Li et al., 2023) as seed data, which has human-annotated labels for difficult. TACO has 25.4K training samples and 1K test samples. Each problems is annotated with difficulty, test cases, and list of topics. Samples in this dataset are collected from open-access sites where programmers share problems with each other, including Aizu, AtCoder, CodeChef, Codeforces, and LeetCode. Benchmarks We use LiveCodeBench-V5 (Jain et al., 2025) and USACO as our evaluation benchmarks. We use LiveCodeBench-V5 for direct comparison with strong baseline (Ahmad et al., 2025); USACO (Shi et al., 2024) is used because it is representative code competition which contains difficult problems and has already been curated as benchmark for evaluation. Models: We use Qwen3-8B as our teacher model in distillation experiments, as it is efficient and has competitive reasoning performance. For the final large scale distillation experiments  (Table 3)  , we employ Qwen3-235B-A22B (Yang et al., 2025) as our teacher. We use Qwen2.5-Coder-7B-Instruct (Hui et al., 2024) and Qwen3-8B-Base as our student model, respectively. For the RL experiments, we use Qwen2.5-7B-Instruct (Yang et al., 2025) model as starting checkpoint for small-scale verification. We use Qwen2.5-14B-Instruct and GPT-4o as generators, as they can follow instructions relatively well compared to smaller models. Hyperparameters: We use vLLM 1 as our inference framework for both distillation and evaluation experiments. We set temperature to 0.6 for all experiments. We set the batch size to 128 and the learning rate to 5e-5 for our SFT experiments, including the fine-tuning of the generator models. We use VeRL 2 for our RL experiments, and use 128 as the rollout batch size, 64 as the mini-batch size, and 16 as the rollout sample size. For all evaluation, we calculate averaged pass@1 across 16 runs. 3.2 USING ESTIMATED DIFFICULTY FOR DATA SELECTION Before training the generator to produce difficult coding problems, we first need trustworthy signal that can serve as proxy for difficulty when gold labels are unavailable for generated problems. As mentioned above, we propose using δ we defined in Section 2.2 based on model responses. To verify the usefulness of this signal, we conduct preliminary experiment that selects subsets of generated problems based on this signal for controlled comparison. We use our baseline graph random walking process to generate 50K problems using TACO as seed data. For each problem, we generate 8 responses and compute δ. We then select 3K samples with the highest δ, 3K with the lowest δ, 3K with δ closest to 0.5, and an additional 3K randomly sampled for comparison. We also use response token length as another difficulty proxy and select 3K samples with the longest responses. Table 2 shows the results of using different selection methods and the performance of models trained on the selected problems, with 8 responses generated for each problem to ensure the scale and significance of our experiments. We observe that problems with the highest δ achieve the best performance, even 1https://github.com/vllm-project/vllm 2https://github.com/volcengine/verl 6 Preprint. Table 2: Effect of different strata of synthetically generated coding problems on downstream performance. δ refers our estimated difficulty defined in Section 2.2 . Response length is determined based on responses generated by Qwen3-8B. Selection of problems LiveCodeBench-V5 score Avg. response length in tokens Random 3K Highest δ 3K Median δ 3K Lowest δ 3K Longest response 3K 36.29 39.28 36.35 32.37 38.35 11.9K 14.2K 14.1K 6.8K 22.6K surpassing those with the longest token responses, and using significant less tokens. We can also observe that for the problems with highest δ, the token length is higher than problems with median and lowest δ, which indicates there are some positive correlations between token length and δ, but δ is still more effective and efficient signal compared to response length. 3.3 TRAINED GENERATOR FOR DISTILLATION We then use our trained generator to generate problems and leverage these problems to obtain responses from long chain-of-thought models (Qwen3-235B-A22B in our experiments) for training student models. In Table 3, we conduct comprehensive comparison between previous Long CoT SFT datasets and our generated datasets on representative code reasoning benchmarks (i.e., LiveCodeBench-V5 and USACO). As detailed in Section 3.1, we employ Qwen2.5-14B-Instruct to train specialized generator using our rejection fine-tuning and difficulty-aware graph sampling techniques. This approach enables us to generate set of 100K challenging coding problems, each paired with response from Qwen3-235B-A22B. We adopt Qwen3-8B-Base as the student model and benchmark its performance against other models of comparable size trained on prior synthetic datasets (see the second block in Table 3). Our model, QueST-100K-8B, consistently surpasses all similarly sized baselines. Among the comparison group, OCR-Qwen-7B-Instruct (Ahmad et al., 2025) stands out as the strongest competitor, leveraging DeepSeek-R1 as the teacher model and generating up to 32 responses for each of the 28K humanwritten coding problems. To ensure fair comparison, we re-implement the OCR method using Qwen3-8B-Base as the student model and generate 4 responses per problem (yielding total of 112K examples) using Qwen3-235B-A22B as the teacher (see OCR-8B in the third block). Even under these conditions, QueST-100K-8B outperforms OCR-8B across both benchmarks, suggesting that large volume of diverse, challenging synthetic problems provides greater benefit than simply repeating existing human-written ones. Furthermore, by combining the 112K organic examples with our 100K generated examples, we achieve even stronger results (QueST-8B), attaining performance on par with the much larger DeepSeek-R1-671B. Table 3: Performance on LiveCodeBench-V5 (from 2409 to 2502) and USACO. Note: We use Qwen3-235B-A22B as the teacher model and Qwen3-8B-Base as the student model. In our methods, we use our trained Qwen2.5-14B-Instruct as generator. 100K means the number of training samples. Model LiveCodeBench-V5 USACO Easy Medium Hard Avg. Easy Medium Hard Avg. DeepSeek-R1-671B (Guo et al., 2025) Qwen3-8B (Yang et al., 2025) R1-0528-Qwen3-8B (Guo et al., 2025) OpenThinker-7B (114K) (Guha et al., 2025) R1-Distill-Qwen-7B (800K) (Guo et al., 2025) OlympicCoder-7B (100K) (Face, 2025) OCR-Qwen-7B-Instruct (700K) (Ahmad et al., 2025) OCR-8B (112K, Our Impl.) QueST-100K-8B (100K, Ours) QueST-8B (212K, Ours) 98. 94.0 94.4 80.6 86.6 82.1 95.4 96.0 97.1 97.6 7 79.8 74.1 73.5 16.9 43.8 49.4 64.0 70.2 74.8 81. 37.4 28.9 27.7 1.6 7.0 12.2 18.0 26.2 28.4 36.6 65.6 58.7 58.1 25.5 38.0 40.9 51.3 56.5 59.4 65. 72.5 58.5 57.0 11.0 22.9 31.4 41.5 54.5 55.9 65.5 54.6 42.8 33.6 2.1 9.7 12.5 26.0 40.5 44.0 48. 34.3 22.3 17.2 0.0 3.8 1.3 7.5 22.9 24.7 28.7 56.2 43.5 38.5 5.0 13.4 17.0 27.2 41.3 43.5 49. Preprint. Figure 3: Training rewards comparison in the training process of RL under different datasets. Table 4: RL results on LiveCodeBench-V5 Model LiveCodeBench-V5 Easy Medium Hard Avg. Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct TACO RL Qwen2.5-7B-Instruct Baseline RL Qwen2.5-7B-Instruct QueST RL 47.4 56.7 56.0 56.4 8.4 10.8 9.6 9.6 0.1 1.1 3.2 4.8 14.3 17.3 17.6 18.6 3.4 REINFORCEMENT LEARNING Our generated data can also be used for RLVR (Reinforcement Learning with Verifiable Reward). We use majority voting results produced by Qwen3-8B as pseudo output labels for each test case of each generated problem. Since our generated test cases are not guaranteed to be valid, we filter out test cases where over half of the outputs are none (indicating failed execution for generated solutions), then keep the remainder for RLVR. We use the GRPO (Shao et al., 2024) algorithm to train Qwen2.5-7B-Instruct on 12K problems sampled from TACO, 6K data from our baseline synthetic method (mathscale) (Tang et al., 2024), and 6K data from QueST. We report our results in Table 4, which shows effectiveness of our proposed method. We report the training reward curve during the training process in Figure 3. It shows that the model trained on TACO datasets gains the highest reward score during the whole training stage, our baseline synthetic method gains lower score, and the model trained on dataset generated by the QueST method gains the lowest score. The training reward can serve as proxy of the inverse difficulty of these three different datasets. 3.5 ABLATION STUDY We conducted an ablation study for fair comparison across different settings, as shown in Table 5. In the first two rows of the table, we examine whether using difficulty-aware graph random walking improves performance when using GPT-4o as the generator. The results demonstrate that the difficulty-aware graph achieves clear improvement. In the third and fourth rows, we compare performance when using the difficulty-aware graph with different generators: Qwen2.5-14B-Instruct without further training and Qwen2.5-14B-Instruct under our rejection fine-tuning method (QueST). The results show that when using difficulty-aware random sampling prompts, our fine-tuned generator can bring better performance than the model without using our fine-tuning method. Therefore, Table 8 Preprint. 5 indicate both difficulty-aware sampling and rejection fine-tuning have positive effect and lead to generating difficult problems. We conducted controlled comparisons using stronger teacher model (Qwen3-235B-A22B) and base model (Qwen3-8B-Base) at larger scale in Table 6. Models trained solely on our synthetic problems achieve higher performance than those trained exclusively on OCR problems, albeit with significantly longer response lengths, indicating that our generated problems are more challenging. In contrast, combining OCR problems with our synthetic problems yields the best overall performance while also reducing response lengths. Table 5: Ablation study on LiveCodeBench-V5. Baseline here represents the our baseline problem generation pipeline (Tang et al., 2024) which we discussed in Section 2.1. Here we generate 20K questions for all settings to fair comparison, and the base model we used to train is Qwen2.5-Coder7B-Instruct. RFT is abbreviation of our rejection fine-tuning method. Methods LiveCodeBench-V5 Easy Medium Hard Avg. Problem Generator: GPT-4o Baseline Baseline w/ difficulty-aware graph 82.8 83.6 36.6 41.1 8.2 10.9 34.9 37.5 Problem Generator Qwen2.5-14B-Instruct Baseline w/ difficulty-aware graph Baseline w/ difficulty-aware graph w/ RFT (QueST) 85.0 84.9 39.2 41.4 8.0 10.4 36.1 37.9 Table 6: Performance and average response length of models trained on our 100K synthetic problems, 112K OCR problems, and merged one (100K + 112K). Models QueST (merged) QueST (synthetic only) OCR 112K Performance on LCB Avg. response length in tokens 65.2 30503 59.4 34347 56.5 3.6 ADDITIONAL ANALYSIS We visualize and compare the 25 most sampled knowledge points with and without difficulty-aware sampling in Appendix Figure 4. The figure shows that knowledge points sampled more frequently by naive sampling than by difficulty-aware sampling tend to be more common overall, while knowledge points sampled less frequently by naive sampling tend to be less common. In other words, difficultyaware sampling upweights infrequent knowledge points and downweights frequent knowledge points compared to naive sampling. The infrequent knowledge points are visualized in the left figure and are generally more difficult, including topics such as the knapsack problem, Optimal Play Strategies, and prime factorization, compared to the basic concepts shown in the right figure. We also conduct case study on generated problems from both original model and model trained by QueST framework in Appendix Table 7. It shows that the problem generated by our trained model requires more complex operations and more knowledge compared the question generated by original model. To prove that our results are not affected by data contamination, we conduct contamination detection experiments on our generated datasets to exclude the effects of data contamination on benchmark performance. Specifically, we compute token-based 50-gram Jaccard similarity scores and the scores across all datasets and benchmarks we used are 0 which indicates there is no contamination in our generated data. 9 Preprint."
        },
        {
            "title": "4 RELATED WORK",
            "content": "4.1 SYNTHETIC DATA FOR LANGUAGE MODELS Synthetic data has been widely used in training language models. Previous works have mainly focused on using small sets of seed data and leveraging LLMs to augment them and generate larger datasets. Some works (Honovich et al., 2023; Li et al., 2024a; Toshniwal et al., 2025; Wang et al., 2023b; Tang et al., 2024) focus on sampling seed data as in-context learning exemplars to generate new ones. Ge et al. (2025) proposed using personas to augment previous in-context learning synthetic data generation methods. Xu et al. (2024); Luo et al. (2025a); Hu et al. (2025) focus on augmenting existing samples to create more complex ones. Some methods have also explored how to generate synthetic data from scratch (Li et al., 2024b; Xu et al., 2025). More recently, Qin et al. (2025) investigated whether synthetic data follows similar scaling laws as real data. PromptCoT (Zhao et al., 2025) also generates challenging problems based on mathematical concepts and rationale. Tong et al. (2024) also proposed difficulty-aware method but focuses on synthetic responses for challenging problems. Liang et al. (2025) extract concepts from failure cases and synthesize new problems during RL training. Additionally, there is research focused on leveraging pretraining or web data to generate reasoning data in general domains (Yuan et al., 2025; Yue et al., 2024). Our QueST framework focuses on new perspective that aims to train difficulty-aware generator to generate difficult problems. 4.2 CODE REASONING Code reasoning is an important capability of large language models. The reasoning ability of language models can be enhanced using chain-of-thought (Wei et al., 2022), RLVR (OpenAI, 2024; Guo et al., 2025; Lambert et al., 2025), and self-consistency (Wang et al., 2023a), in math (Hendrycks et al., 2021) and code (Jain et al., 2025; Shi et al., 2024) domains. Muennighoff et al. (2025) and Ye et al. (2025) focus on manually curating small-scale reasoning problems, which is sufficient to boost models reasoning ability. More recently, Face (2025), Ahmad et al. (2025), and Guha et al. (2025) have developed large-scale distillation methods from reasoning models to obtain high-quality long CoT SFT datasets that can be used to train student models effectively. Nvidia et al. (2024) curate reasoning datasets throughout the entire training pipeline. Li et al. (2025) introduce an innovative paradigm that transforms traditional code reasoning tasks from their original format into given code + test cases / input-output prediction structure. Complementing these supervised learning approaches, Luo et al. (2025b) demonstrate the effectiveness of reinforcement learning techniques applied to verified code reasoning problems. However, how to generate difficult synthetic coding problems and use them for training remains relatively underexplored."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose method for generating difficult code problems at scale. Specifically, we investigated pipeline that uses majority voting to compute proxy of difficulty and employs this as signal for rejection fine-tuning of the problem generator, and combined it with novel difficulty-aware graph sampling prompts. This enables the trained generator to produce challenging problems at scale. We then use these generated problems for supervised fine-tuning (SFT) and reinforcement learning (RL) to verify their effectiveness. As novel synthetic data generation method, we compared our approach with previous baselines at similar scales on code reasoning benchmarks and show that our method achieves better performance even when using less SFT data, particularly for hard problems. LIMITATIONS AND FUTURE WORK Although our method shows promise for rejection fine-tuning generator, we still face limitations as the generator hasnt been trained using RL. One primary reason is that our current difficulty calculation is computationally expensive and challenging to implement in real-time to provide difficulty rewards in an RL pipeline, considering that we need to generate 8 responses and 20 test cases for each problem on the fly, execute them, and generate problems for each prompt. In future work, it would be worthwhile to explore methods that can provide rewards in real time, such as directly training reward model to predict difficulty, or investigating other efficient approaches. 10 Preprint."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was done during the internship of HX at Microsoft Research. HX, JV, and RS acknowledge funding by the Swiss National Science Foundation (project InvestigaDiff; no. 10000503)."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To help community reproduce our work, we described details of implementation in Section 3.1, which reports the details of data, benchmark, models, and hyperparameters we use in our experiments. We also report the framework we use for training and inference. In Appendix Figure 5 6 7, we report the prompt template we use."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "In the paper, all the data we use is open-sourced. TACO (Li et al., 2023) has Apache-2.0 license. LiveCodeBench (Jain et al., 2025) and USACO (Shi et al., 2024) are collected from open part of common competition websites. 11 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data disIn Second Conference on Language Modeling, 2025. URL tillation for competitive coding. https://openreview.net/forum?id=aykM7KUVJZ. Hugging Face. Open R1: fully open reproduction of DeepSeek-R1, January 2025. URL https: //github.com/huggingface/open-r1. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas, 2025. URL https://arxiv.org/abs/2406.20094. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506.04178. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang Chen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, Sep 2025. ISSN 1476-4687. doi: 10.1038/s41586-025-09422-z. URL https://doi.org/10.1038/s41586-025-09422-z. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning In Anna Rogers, Jordan Boyd-Graber, and language models with (almost) no human labor. 12 Preprint. Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1440914428, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.806. URL https://aclanthology.org/2023.acl-long.806/. Hanxu Hu, Simon Yu, Pinzhen Chen, and Edoardo Ponti. Fine-tuning large language modIn Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proels with sequential instructions. ceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 55895610, Albuquerque, New Mexico, April 2025. Association for Computational LinISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.288. URL https: guistics. //aclanthology.org/2025.naacl-long.288/. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. Fangkai Jiao, Geyang Guo, Xingxing Zhang, Nancy F. Chen, Shafiq Joty, and Furu Wei. Preference optimization for reasoning with pseudo feedback. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= jkUp3lybXf. Bespoke Labs. reasoning distillation. www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoningdistillation, 2025. Accessed: 2025-01-22. Bespoke-stratos: The unreasonable effectiveness of Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Christopher Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. In Second Conference on Language Modeling, 2025. URL https://openreview.net/forum?id=i1uGbfHHpH. Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities, 2024a. URL https://arxiv.org/abs/2403.04706. Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, and Furu Wei. Synthetic data (almost) from scratch: Generalized instruction tuning for language models, 2024b. URL https://arxiv.org/abs/2402.13064. Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. CodeIO: Condensing reasoning patterns via code input-output prediction. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=feIaF6vYFl. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset, 2023. URL https://arxiv.org/ abs/2312.14852. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):10921097, December 2022. ISSN 1095-9203. doi: 10.1126/science.abq1158. URL http://dx.doi.org/10.1126/science.abq1158. 13 Preprint. Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, and Weizhu Chen. SwS: Self-aware weakness-driven problem synthesis in reinforcement learning for llm reasoning, 2025. URL https://arxiv.org/abs/2506.08989. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview. net/forum?id=mMPMHWOdOy. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, fully open-source https://pretty-radio-b75.notion.site/ 14b coder DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level1cf81902c14680b3bee5eb349a512a51, 2025b. Notion Blog. and Ion Stoica. at o3-mini Deepcoder: level. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Nvidia, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, and Chen Zhu. Nemotron-4 340b technical report, 2024. URL https://arxiv.org/abs/2406.11704. OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Zeyu Qin, Qingxiu Dong, Xingxing Zhang, Li Dong, Xiaolong Huang, Ziyi Yang, MAHMOUD KHADEMI, Dongdong Zhang, Hany Hassan Awadalla, Yi R. Fung, Weizhu Chen, Minhao Cheng, and Furu Wei. Scaling laws of synthetic data for language model. In Second Conference on Language Modeling, 2025. URL https://openreview.net/forum?id=UmUXPXHtdl. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Ben Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. Can language models solve In First Conference on Language Modeling, 2024. URL https: olympiad programming? //openreview.net/forum?id=kGa4fMtP9l. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=Kjww7ZN47M. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. DART-math: Difficulty-aware rejection tuning for mathematical problem-solving. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=zLU21oQjD5. 14 Preprint. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating AI for math with massive open-source instruction In The Thirteenth International Conference on Learning Representations, 2025. URL data. https://openreview.net/forum?id=mTCbq2QssD. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=1PL1NIMMrw. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484 13508, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long.754/. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=_VjQlMeSB_J. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=CfXh93NDgH. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned LLMs with nothing. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=Pnk7vMbznK. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. LIMO: Less is more for reasoning. In Second Conference on Language Modeling, 2025. URL https://openreview. net/forum?id=T2TZ0RY4Zk. Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=N8N0hgNDRt. Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Ilia Kulikov, Kyunghyun Cho, Dong Wang, Yuandong Tian, Jason Weston, and Xian Li. Naturalreasoning: Reasoning in the wild with 2.8m challenging questions, 2025. URL https://arxiv.org/abs/2502.13124. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 2024. Xueliang Zhao, Wei Wu, Jian Guan, and Lingpeng Kong. PromptCoT: Synthesizing olympiadlevel problems for mathematical reasoning in large language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1816718188, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.935. URL https://aclanthology.org/2025.findings-acl.935/. 15 Preprint."
        },
        {
            "title": "A APPENDIX",
            "content": "Figure 4: Examples of knowledge points where the count is increased (left) or decreased (right) when using difficulty-aware sampling. A.1 USAGE OF LLM We used LLMs to polish the writing and assist in the implementation of parts of the codebase. All the methodologies, ideas, conceptual contributions, experiment designs, analyses were developed solely by the authors. 16 Preprint. Table 7: Case Study: Topic and knowledge points that were provided to the generator, and two generated questions. Left is generated by our trained model, right is generated by original Qwen2.514B-Instruct. Topic String Analysis Knowledge points Problem 1. Hamming Distance Calculation; 2. Time Complexity Optimization; 3. Conditional Logic and State Management; 4. Event Handling and Simulation; 5. Priority Queue In messaging system, messages are sent as strings of binary digits. Sometimes, due to technical issues, certain bits in the messages might get flipped (0 becomes 1, and 1 becomes 0). To ensure the integrity of the messages, special protocol is used where each message is sent alongside its flipped version. Upon receiving both versions, the receiver calculates the Hamming distance between the original and the flipped message to detect any errors. Given two strings and of equal length representing the original and flipped versions of message, your task is to write function the Hamming distance that calculates between them. The Hamming distance is defined as the number of positions at which the corresponding bits are different. Additionally, the function should handle up to 105 queries efficiently, optimizing for time complexity. You are given string of length consisting of binary characters (0 and 1). You need to perform several queries on this string. Each query can be one of the following types: 1. set(l, r, c): Set all characters in the substring s[l:r+1] to the character (either 0 or 1). 2. flip(l, r): Flip all characters in the substring s[l:r+1] (change 0 to 1 and 1 to 0). 3. hamming(l1, r1, l2, r2): Calculate the Hamming distance between the substrings s[l1:r1+1] and s[l2:r2+1]. Your task is to handle these queries efficiently. Input: - The first line contains the string of length (1 200,000). - The second line contains an integer (1 400,000) representing the number of queries. - The next lines describe the queries. Each query is in one of the above formats. - For each hamming query, output the Hamming distance between the specified substrings. 17 Preprint. Figure 5: 1-shot prompt example for problem generation. It is simplified for visualization, in real prompt, we have 8-shot for in-context learning. 18 Preprint. Figure 6: 1-shot example prompt for testcase generation. 19 Preprint. Figure 7: Prompt demonstration for concept extraction."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "University of Zurich"
    ]
}