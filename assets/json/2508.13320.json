{
    "paper_title": "Rapidly Adapting to New Voice Spoofing: Few-Shot Detection of Synthesized Speech Under Distribution Shifts",
    "authors": [
        "Ashi Garg",
        "Zexin Cai",
        "Henry Li Xinyuan",
        "Leibny Paola García-Perera",
        "Kevin Duh",
        "Sanjeev Khudanpur",
        "Matthew Wiesner",
        "Nicholas Andrews"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We address the challenge of detecting synthesized speech under distribution shifts -- arising from unseen synthesis methods, speakers, languages, or audio conditions -- relative to the training data. Few-shot learning methods are a promising way to tackle distribution shifts by rapidly adapting on the basis of a few in-distribution samples. We propose a self-attentive prototypical network to enable more robust few-shot adaptation. To evaluate our approach, we systematically compare the performance of traditional zero-shot detectors and the proposed few-shot detectors, carefully controlling training conditions to introduce distribution shifts at evaluation time. In conditions where distribution shifts hamper the zero-shot performance, our proposed few-shot adaptation technique can quickly adapt using as few as 10 in-distribution samples -- achieving upto 32% relative EER reduction on deepfakes in Japanese language and 20% relative reduction on ASVspoof 2021 Deepfake dataset."
        },
        {
            "title": "Start",
            "content": "Rapidly Adapting to New Voice Spoofing: Few-Shot Detection of Synthesized Speech Under Distribution Shifts Ashi Garg, Zexin Cai, Henry Li Xinyuan, Leibny Paola Garcıa-Perera, Kevin Duh Sanjeev Khudanpur, Matthew Wiesner, Nicholas Andrews Human Language Technology Center of Excellence, Johns Hopkins University 5 2 0 2 8 1 ] . e [ 1 0 2 3 3 1 . 8 0 5 2 : r AbstractWe address the challenge of detecting synthesized speech under distribution shiftsarising from unseen synthesis methods, speakers, languages, or audio conditionsrelative to the training data. Fewshot learning methods are promising way to tackle distribution shifts by rapidly adapting on the basis of few in-distribution samples. We propose self-attentive prototypical network to enable more robust fewshot adaptation. To evaluate our approach, we systematically compare the performance of traditional zero-shot detectors and the proposed fewshot detectors, carefully controlling training conditions to introduce distribution shifts at evaluation time. In conditions where distribution shifts hamper the zero-shot performance, our proposed few-shot adaptation technique can quickly adapt using as few as 10 in-distribution samples achieving upto 32% relative EER reduction on deepfakes in Japanese language and 20% relative reduction on ASVspoof 2021 Deepfake dataset. Index TermsSynthetic speech detection, few-shot learning, speech anti-spoofing I. INTRODUCTION Recent advances in speech synthesis capabilities, including text-tospeech (TTS) and voice conversion (VC), have resulted in systems that can produce speech that is, in many cases, indistinguishable from actual human speech to layperson [1][7]. However, such advancements in audio generation increase the risk of abuse. Watermarking techniques [8][10] may be effective when applicable, but watermarking is not always feasible defense and may be circumvented [11]. Therefore, it is critical to look into detection methods that make minimal assumptions about the synthesis methods. Indeed, number of supervised detectors have been proposed, which typically fit neural network-based classifier to dataset of real and fake utterances. While effective in certain settings, such detectors are vulnerable to real-world distribution shifts relative to the training data, such as the introduction of new speech synthesis methods, recording conditions, languages, and noise conditions [12], [13]."
        },
        {
            "title": "Since it",
            "content": "is not possible to anticipate all possible distribution shifts and they are unavoidable in practice (i.e., outside of academic benchmarks), more adaptable detection approaches are needed. To this end, we remark that is often feasible to collect it handful of presumed-fake speech in downstream applications, even if building complete training set with thousands of samples is prohibitive. For example, spoofing attempt could be detected using other information such as caller metadata. Alternatively, it is often possible to proactively synthesize small amounts of applicationspecific speech, as new synthesis methods are released, to simulate what an adversary may attempt. This being the case, in this paper, we ask: based on small sample of in-distribution speechboth synthetic and realand possibly larger set of out-of-distribution speech, can we build an accurate synthetic speech detector? This problem is inherently difficult since it involves learning at test time, and there are risks of both overfitting and underfitting the small in-distribution sample. We therefore turn to specialized methods designed for few-shot learning. Few-shot learning has been applied mainly for image classification tasks [14][16], and there is limited work in the speech domain (we discuss relevant work in section IV). In this paper, we take closer look at few-shot detection for synthetic speech detection. Specifically, we consider broader set of experimental settings than previous work, and focus on settings where there is controlled distribution shift from training-time to testtime. Furthermore, we consider state-of-the-art synthesis methods, evaluating across 12 different vocoders [13] as well as multiple TTS and VC systems from the ASVspoof dataset [17], [18]. We demonstrate the importance of the feature aggregation strategy for few-shot learning by proposing new self-attentive architecture for few-shot learning. In detail, our primary contributions are: We perform systematic study of few-shot learning for synthetic speech detection across wide range of evaluation conditions, including controlled distribution shifts [13]. We introduce new few-shot learning method for synthetic speech detectionself-attentive prototypical networkswhich significantly improves performance over state-of-the-art baselines in few-shot conditions. We show that supervised fine-tuning of SSL-based detectors can exhibit strong performance in medium-shot settings involving larger adaptation sets. To our knowledge, our work is the first to demonstrate the consistent benefit of few-shot learning for synthetic speech detection. II. PROPOSED METHOD A. Preliminaries a) Data: We assume access to two training datasets. The first is potentially quite large set of real and synthesized speech, but is assumed to be out-of-distribution relative to the test conditions. We assume that real-or-fake labels are available for each speech sample and that fake data include speech generated from variety of synthesis methods, for diverse set of speakers. The first dataset is sufficiently large to train neural network-based detectors using gradient descent as baselines (subsection III-B). The second datasetthe few-shot samplecomprises small dataset of real and synthesized speech samples that is in-distribution relative to the test data. The challenge is to use this small sample to adapt to the test conditions, hopefully improving performance relative to models trained using only the first out-of-distribution dataset. b) Pre-trained models: We leverage pre-trained speech representations fθ obtained using self-supervised learning (SSL) models. These features have been shown to yield state-of-the-art zero-shot detectors [19] and therefore present strong baseline for our proposed 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. This work is accepted by the IEEE ASRU 2025. few-shot methods. Our approach aims to adapt these initial representations to novel test-time conditions. Although large-scale pre-training using SSL typically employs large datasets of real speech under diverse audio conditions, we assume that it is primarily capturing properties of genuine speech and any exposure to synthetic speech is incidental. However, SSL features are particularly well-suited in few-shot adaptation settings, since we hypothesize that they may be more readily fine-tuned using small samples of in-distribution data than representations trained from scratch, which is borne out in our experiments (see Table IV). B. Attention-based Prototype Aggregation a) Prototypical Networks: We begin by introducing prototypical networks [16]a simple but effective few-shot learning method which has seen various applications in image, text [20], and audio [21] classification problems. The approach involves learning mapping from set of examples to single vector prototype of class. For binary classification task of real and fake (spoof) speech detection, if we denote the prototype of the fake class cfake and learned few-shot embedding gϕ, it is obtained via cfake = 1 Sfake (cid:88) xSfake gϕ(z), (1) where = fθ(x) and Sy is the support set for class y. Given trained embedding and test sample x, classification is obtained via the softmax over the prototypes for each class (in our case, real-vs.-fake): p(fake x) = exp(d(gϕ(z), cfake) y{real,fake} exp(d(gϕ(z), cy) (cid:80) (2) The parameters ϕ of embedding comprising the prototypical network are trained via episodic learning, where episodes of few-shot samples from each class are sampled repeatedly from larger (outof-distribution) training set. Here, the assumption is that there exists an embedding space where samples belonging to particular class can be clustered around its class prototype. Further, the unlabeled query examples are classified based on their distance from the class prototypes. For the purpose of this work, we utilize Euclidean distance as the distance function d(, ) in Equation 2. b) Self-Attentive Prototypes: Note that the embedding function gϕ in Equation 1, used in prototypical network, learns representation of each data point independently. However, for synthetic speech detection, we hypothesize that the features to separate both classes may be more subtle and require more expressive fewshot learning model. Inspired by attentive pooling used in tasks like speaker verification [22], we propose an attention-based aggregation method to compute more discriminative class prototypes. The new gϕ(x1, x2, . . .) jointly embeds the entire support sample x1, x2, . . . Sy, enabling learning higher-order features of the set of examples rather than considering simple mean. Specifically, given set of support samples, to capture inter-sample dependencies and enhance each representation with global context, multi-head self-attention (MHA) mechanism is applied over the support embeddings [23]. We use single layer of MHA with 2 attention heads. We then compute weighted sum over these embeddings using learnable attention mechanism to create the class prototype cy as: cy = (cid:88) i=1 αi zi for attention scores α and transformed outputs = MHA(z), both parametrized by ϕ. Finally, cy is ℓ2-normalized. We note that our approach bears some resemblance to matching networks [14], which also consider full context embeddings, but in the case of matching networks, these are obtained using an LSTM which suffers from ordering biasesthe ordering is arbitrary in the case of few-shot samplesand vanishing gradients which could be problematic with larger support sets. C. Binary vs Multi-class Synthetic speech detection is inherently binary task at test time, where the goal is to distinguish between bonafide and spoofed audio. Nevertheless, we hypothesize that training with multi-class classification objective, by treating each spoofing attack as distinct class, could potentially help the model learn more fine-grained representations, thereby improving generalization to unseen attack types. To investigate this, we examine the role of binary and multi-class classification training strategies in improving generalization to unseen spoofing attacks. For the multi-class setting, we use the attack labels provided in the ASVspoof 2019 training set, treating each attack type as separate class. In the few-shot setting, we randomly sample three spoof classes per episode during training. For the binary setting, all spoof classes are merged into single FAKE label to align with the test-time evaluation strategy. III. EXPERIMENTS In this section, we examine the adaptation ability of few-shot models to newly synthesized utterances. Our proposed models are compared against several baseline systems, including zero-shot spoof speech detection models and few-shot methods that utilize simple mean-pooling of speech representations. A. Model Implementation Details We utilize the state-of-the-art SSL-AASIST [19] model as the backbone architecture for developing the few-shot detector. SSLAASIST integrates the Wav2Vec 2.0 XLSR [24] as front-end feature extractor to obtain frame-level features with spectro-temporal graph attention network as the back-end. The spectral and temporal graphs are combined using two heterogeneous stacking graph attention layers (HSGAL) and two graph pooling layers for each of the branches. This layer helps take into account how artifacts from both spectral and temporal domains are related. B. Baselines a) Anomaly detection: In non-stationary settings where testtime conditions are unknown, one effective approach is to concentrate modeling capacity on the known classin our case, real speech. We then cast fake speech detection as an anomaly detection problem and identify deviations in feature space from the features of known real speech. Specifically, we employ the Mahalanobis distance-based approach described in [25], fitting Gaussian to features extracted from in-distribution real speech samples. While this approach requires no samples of in-distribution fake speech, an important downside of the approach is that real speech that deviates from the training data may be falsely detected as fake. b) Zero-shot: We include number of state-of-the-art baseline including SSL-AASIST and AASIST. These zero-shot detectors, methods are often highly competitive, even in challenging settings like those considered in this paper, which involve distribution shifts between training and test time. c) Few-shot: We compare against several ablations of our own method to validate the effectiveness of the proposed approach. Specifically, we consider the impact of varying the number of support samples for few-shot detection, the inclusion of the selfattentive pooling mechanism, and the choice of binary or multi-class classification objectives. We additionally include comparisons to [26], which report results on subset of the datasets we consider, which we reproduce here.1 d) Supervised adaptation: We speculated in section that the use of pre-trained SSL features may enable effective supervised adaptation using gradient descent, even with moderately sized indistribution fine-tuning sets of 100 examples. We compare the performance of few-shot detectors with fine-tuned SSL-AASIST model trained under the same data constraints. In the fine-tuning setup, we update the parameters of the zero-shot model using small number of labeled samples per class through supervised learning. In contrast, the few-shot approach uses the same amount of data to compute class prototypes without updating the model weights. This allows us to evaluate whether prototype-based adaptation offers improved generalization with limited supervision. key risk of the above baseline approach is overfitting to the small in-distribution sample. Therefore, we hold-out 30% of the support sample for early stopping, and use the rest for supervised adaptation. C. Dataset We utilize ASVspoof 2019 dataset [17] to train the detection systems. In order to assess generalization performance, we evaluate our models on four diverse and challenging datasets: ASVspoof 2021 [18], ShiftySpeech [13], In-the-Wild (ITW) [27] and CodecFake [28]. These benchmarks collectively cover at least 10 distinctive evaluation conditions. ASVspoof 2021 extends the 2019 dataset with more advanced and varied spoofing attacks, including state-of-the-art TTS and VC systems. ShiftySpeech benchmark is specifically designed to evaluate model robustness under distributional shift, encompassing variations in vocoder type, language, and recording conditions. In contrast, the ITW dataset consists of deepfake samples sourced from various online platforms like YouTube, introducing uncontrolled acoustic variability and spoofing artifacts generated in the wild. Finally, CodecFake targets robustness against compression-induced artifacts by including spoofed speech compressed with diverse codecs and bitrates. Together, these datasets enable comprehensive evaluation of generalization to previously unseen attacks and domain shifts. D. Experimental Setting The proposed few-shot models are trained with episodic learning [16] for 100 epochs, with each epoch comprising of 100 episodes. Each episode is mini-batch designed to mimic the test-time scenario, where only few examples from novel class are available. During training, we set the number of support samples to create prototype as 5 and the number of queries as 15 for each class. similar setup is used for validation. The model is trained using prototypical loss with learning rate of 1e-3 and optimized using the Adam optimizer. StepLR learning rate scheduler is used, which decays the learning rate by factor of 0.5 every 20 epochs. We adopt the SSL-AASIST model as our backbone and, in our experiments, we freeze the Wav2Vec 2.0 XLSR-53 front-end while fine-tuning all remaining parameters. Model selection is based on accuracy measured on the validation set. For zero-shot detectors, we utilize publicly available pre-trained checkpoints of SSL-AASIST2 and AASIST3 models, both trained on ASVSpoof 2019 dataset. Additionally, we also train SSL-AASIST in multi-class classification setting for 100 epochs, considering each attack type in ASVspoof 2019 as distinct spoof class. This model is trained using learning rate of 1e-5 and batch size of 64. To investigate the effect of class granularity, we also train multi-class variant of the few-shot model. In this setting, each episode samples two spoof classes randomly, with the bonafide class always included. The rest of the training setup remains consistent with the binary fewshot model. For anomaly detection, we utilize x-vector [29] based speaker embedding model pretrained on VoxCeleb [30] as the feature extractor4. We fit multivariate Gaussian distribution on embeddings extracted from bonafide (real) samples in the ASVspoof 2019 dataset, estimating the mean vector µ and covariance matrix Σ. During inference, given sample with embedding is scored based on the Mahalanobis distance. The supervised adaptation detector with 10 samples per class is trained with batch size of 4, learning rate of 1e-4 and for 2 epochs.5 For 100 samples per class, the model is trained with batch size of 64, learning rate of 1e-4 and for 3 epochs. Evaluation Metric To evaluate performance, we utilize Equal Error Rate (EER). EER is defined as the value where the false acceptance rate (FAR) equals the false rejection rate (FRR). To account for variability introduced by the random selection of support samples used to build prototypes, we evaluate the model over 100 runs with different randomly sampled support sets and report the mean EER. In zero-shot multi-class classification setting, we average the scores of all spoof classes to produce single aggregated spoof score. E. Results and Discussion We evaluate zero-shot, few-shot, and fine-tuned models for synthetic speech detection across multiple datasets. The goal is to assess detection performance under limited supervision and distribution shift. Average Equal Error Rate (aEER) is used as the primary evaluation metric. Results are organized to reflect differences across domains and adaptation strategies. Table reports aEER on ShiftySpeech dataset, while Table II reports results on in-domain (ASVspoof 2019) and more challenging out-of-domain datasets, including ASVspoof 2021 DF and In-theWild (ITW). Zero-shot detection As shown in Table and Table II, the SSLAASIST model consistently outperforms the baseline AASIST model across all datasets. This highlights the natural benefit of incorporating self-supervised representations for spoof detection. For instance, on challenging dataset such as YouTube, SSL-AASIST achieves relative EER reduction of 28% compared to AASIST. We further compare the generalization performance of zero-shot models with few-shot methods below. Few-shot detection Few-shot models demonstrate substantial performance gains over zero-shot baselines on average across the ShiftySpeech subsets. For instance, on Japanese (Ja) subset, the EER drops from 22.15% (SSL-AASIST) using zero-shot model to 18.84% 2https://github.com/TakHemlata/SSL Anti-spoofing 3https://github.com/clovaai/aasist 4https://huggingface.co/speechbrain/spkrec-xvect-voxceleb. We also explored WavLM based speaker verification model as feature extractor https://huggingface.co/microsoft/wavlm-base-sv 1At the time of this writing, code is not available to reproduce the results 5To account for variability due to random sampling in 10-shot fine-tuning from [26]. adaptation, we train each model independently 15 times per dataset TABLE I: Average EER (% ) for zero-shot and few-shot methods on ShiftySpeech [13]. Method Zero-shot Few-shot ProtoNet (Binary) Few-shot ProtoNet + Attn Pool (Binary) Few-shot ProtoNet + Attn Pool (Multi-class) Setting aEER (%) Ja Zh EN-Spk EN-Audiobook EN-Podcast EN-YouTube AASIST SSL-AASIST SSL-AASIST (multi-class) 38.90 22.15 67.61 26.25 26.06 29.93 5-shot 10-shot 100-shot 5-shot 10-shot 100-shot 5-shot 10-shot 100-shot 18.84 18.26 17.98 15.53 15.03 14.85 16.18 15.69 15.56 31.12 29.48 28. 26.32 25.29 24.52 39.10 36.01 31.29 52.39 32.40 38.40 32.52 30.67 29.24 27.99 26.61 26. 32.44 30.05 28.32 49.49 34.58 37.37 30.19 28.40 27.58 27.15 26.20 25.59 30.78 28.77 27. 51.17 34.63 39.71 37.55 35.58 33.38 35.94 34.23 33.15 37.35 34.89 32.65 53.67 38. 43.28 37.80 36.13 33.62 37.12 35.11 32.60 43.91 39.67 34.04 TABLE II: Average EER (% ) for zero-shot and few-shot methods on in-domain and out-of-domain data (ITW, ASVspoof2021 DF). Method Setting aEER (%) ASV19:LA ASV21:LA ASV21:DF ITW One-Class Learning Mahalanobis 19.42 Zero-shot Few-shot [26] Few-shot ProtoNet (SSL-AASIST, Binary) Few-shot ProtoNet + Attn Pool (SSL-AASIST, Binary) Few-shot ProtoNet + Attn Pool (SSL-AASIST, Multi-class) AASIST SSL-AASIST SSL-AASIST (multi-class) ProtoNet-256 ProtoNet-96 5-shot 10-shot 100-shot 5-shot 10-shot 100-shot 5-shot 10-shot 100-shot 0.99 0.22 0.36 1.68 1.68 2.66 2.43 2.12 6.24 6.19 6. 0.72 0.62 0.58 24.93 13.94 8.10 11.41 4.55 4.56 13.21 13.09 12. 11.52 11.37 10.68 11.57 10.67 10.26 43.26 17.67 8.33 18.51 7.65 7. 6.65 6.65 6.65 7.12 6.95 6.52 12.68 11.13 9.10 42.89 44.57 11.19 13. 20.94 21.04 25.78 23.66 23.36 18.56 18.56 18.56 21.77 17.09 15.22 with 5-shot learning. Similar trends are observed in the Audiobook and YouTube subsets, demonstrating that even limited supervision can yield substantial improvements. These gains are particularly pronounced under domain shift, where the spoofing artifacts differ significantly from those seen during training on ASVspoof. cally, attention aggregation yields 6.65% EER on this set as compared to 7.65% in prior work. Similar is observed for ITW, where 256 samples used in [26] yields an EER of 20.94%, whereas utilizing only 5 support samples with an attention aggregation mechanism reduces the EER by 11.36% relative to that baseline. We also observe diminishing returns as the number of support samples increases from 10 to 100, indicating that only small number of labeled examples are sufficient to achieve near-optimal results (see Figure 1a). Specifically, increasing the number of support samples from 10 to 100 yields an 7% relative EER reduction on the Podcast and YouTube datasets. However, there is less than 5% relative reduction for other subsets of ShiftySpeech. Additionally, incorporating attention pooling further enhances performance compared to standard mean-based prototype aggregation. For example, on Japanese subset, the 5-shot EER improves from 18.84% (ProtoNet) to 15.53% (ProtoNet + Attn Pool), and on Mandarin, from 31.12% to 26.32%approximately 15% relative reduction in both cases. These results suggest that attention-based prototype aggregation helps refine class representations, particularly under low-shot and cross-lingual conditions. For some challenging datasets such as ASVspoof DF, attention aggregation further improves performance over prior work [26], utilizing over 200 samples. SpecifiA similar trend is observed for codec-related shifts, as shown in the codec subsets (C1-C7) of the CodecFake dataset (Table III). The zero-shot SSL-AASIST model achieves an average EER of 38.27%. Notably, incorporating few-shot approach with as few as 5 support samples reduces the aEER to 31.98%, highlighting strong generalization to previously unseen codec distortions. In order to further quantify the performance gain from attentionbased prototype aggregation, we compute the average difference in EER across datasets for each support set (See Figure 1a), defined as: EER(k) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (EERProto(i, k) EERProto+Attn(i, k)) where is the number of support samples, and represents number of datasets. In similar manner, we also plot the performance gain over the best-performing zero-shot baseline (SSL-AASIST) and the few-shot detector trained with attention-based prototype aggregation. (a) (b) Fig. 1: Average EER (ProtoNet ProtoNet+Attn) across datasets, computed per support shot (x-axis). Left (a): positive value indicates performance improvement using the proposed attention-based prototype aggregation over the ProtoNet baseline. Right (b): negative value indicates an improvement in performance over the zero-shot baseline, which we see for 5. TABLE III: Mean EER (% ) on CodecFake C1C7 subsets for different support set sizes. Method SSL-AASIST ProtoNet ProtoNet + Attn Pool Support Zero-shot 5-shot 10-shot 100-shot 5-shot 10-shot 100-shot C1 23.59 23.60 22.19 19.98 9.33 9.27 9.14 C2 42.61 40.38 40.70 37.17 37.50 35.53 34.62 C3 46.67 47.32 46.54 43.70 37.96 34.88 34.57 C4 30.41 38.46 35.63 34.03 27.45 26.03 26.03 C5 31.47 31.50 28.59 27.58 19.69 19.69 19.70 C6 46.80 49.54 49.88 48.53 45.00 43.58 41. C7 46.35 48.30 48.27 45.85 46.99 45.07 42.66 aEER 38.27 39.87 38.82 36.69 31.98 30.57 29.69 Particularly, we compute the average EER difference across all datasets for each support size k, defined as: EER(k) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (EERProto+Attn(i, k) EERZeroShot(i)) negative EER(k) indicates that the few-shot model achieves lower error rate than the zero-shot baseline. As shown in Figure 1b, the performance improves consistently with increasing support size, but even small support size yield substantial gains highlighting the effectiveness of attention-based adaptation under limited supervision. Binary vs Multi-class The zero-shot multi-class variant of SSLAASIST improves over AASIST on most datasets; however, it performs worse than AASIST on ASVspoof 2021 DF (18.51 % vs. 17.67 %), and exhibits notable degradation compared to the binary SSL-AASIST model (18.51 % vs. 8.33 %). In the few-shot setting, the multi-class ProtoNet + Attn Pool (SSL-AASIST) model achieves strong performance on in-domain ASVspoof 2019 LA (0.72 % with 5-shot) and competitive results on ASVspoof 2021 LA, but underperforms its binary counterpart in the more challenging out-of-domain datasets such as ASVspoof 2021, Zh and YouTube. This trend is evident across datasets in both Table and Table II, suggesting that multi-class supervision may reduce generalization to unseen spoofing conditions. Fine-tuning adaptation vs few-shot adaptation Table IV reports the EER on out-of-domain ShiftySpeech and ITW datasets. Across all datasets, few-shot adaptation consistently outperforms fine-tuning adaptation, when the number of samples per class is limited to 10. When the number of samples is increased to 100, fine-tuning model performance surpasses few-shot model. Notably, in the YouTube dataset, the fine-tuned model with 100 samples per class achieves an EER of 33.04%, while the 10-shot few-shot model reaches 35.11%, and the 100-shot few-shot model remains competitive at 32.60%. These results indicate that few-shot learning provides strong alternative to full-model fine-tuning under low-data conditions, while remaining competitive even when more data is available. However, we note that fine-tuning all model parameters at test-time may be impractical for large backbones (fθ) or when it is necessary to continually adapt to different test time conditions. Therefore, the 100sample FT model may prove to be an upper bound on meta-learning approaches, which have the ostensibly harder job of adapting without changing the underlying model parameters. Anomaly detection We evaluate the one-class learning approach on both in-domain and the more challenging ITW dataset. This method yields relatively high EER of 19.42% even on the in-domain setting, and an EER of 42.89% on ITW. Notably, the EER on ITW is comparable to that of the zero-shot AASIST model (44.57%). Although performance is limited compared to supervised methods, these result suggest the potential of utilizing only bonafide samples for detection. IV. RELATED WORK Prior work in deepfake detection has been driven by number of key benchmarks [17], [18], [31]. However, for detection research to translate into real-world effectiveness, cross-domain generalization to real-world is crucial for developing detectors that are robust distribution shifts. Few-shot learning has been employed to enhance performance in low-resource settings and to improve generalization to unseen classes or domains. Such methods have found applications in variety of audio classification tasks, including speaker identification its application to synthetic speech detection [32][34]. However, remains underexplored. Only few recent studies have investigated meta-learning approaches [26], [35]. [26] focuses on utilizing various limits evaluation to loss functions to improve generalization but TABLE IV: Comparison of model fine-tuning (FT) adaptation and few-shot adaptation under few-shot (N = 10) and medium-shot (N = 100) conditions. For = 100, the pre-trained SSL features enabled quick adaptation to the test conditions without significant overfitting of the small training sample. However, in the few-shot setting (N = 10), this approach breaks down. Model Ja Zh EN-Spk EN-Audiobook EN-Podcast EN-YouTube ITW SSL-AASIST (FT, 10-sample) SSL-AASIST (FT, 100-sample) Few-shot (10-shot) Few-shot (100-shot) 26.60 4.33 15.03 14.84 41.53 13.58 25.29 24.52 39.31 13.31 26.61 26.24 45.97 17.61 26.20 25.59 45.69 24.18 34.23 33. 46.19 33.04 35.11 32.60 24.34 4.71 18.56 18.56 the ASVspoof dataset. [35] further investigates by utilizing selfsupervised features to study the generalization performance of vanilla Prototypical Networks on more challenging datasets such as ITW. In this work, we take closer look at few-shot adaptation for synthetic speech detection, and propose an improvement over standard prototypical networks which incorporates attention-based prototype aggregation. V. CONCLUSION Summary We have shown that few-shot methods are able to significantly improve detection performance relative to state-of-theart zero-shot detectors, given only small in-distribution samples (N = 5 or = 10). Specifically, we find that learning set-level embeddings using self-attention is key ingredient in unlocking the potential of few-shot detection in challenging test conditions. Overall, in the few-shot setting, the proposed few-shot adaptation approach greatly outperforms alternatives based on supervised finetuning or anomaly detection as demonstrated using datasets like ShiftySpeech [13], which facilitated systematic evaluation under wide range of distribution shifts.. For larger numbers of samples (N = 100), we find that full fine-tuning of SSL-based detectors obtains strong performance, albeit at much larger computational cost and by producing specialized models. Limitations & Future Work Our work focuses on single SSL backbone architecture (SSL-AASIST). While this enabled controlled comparisons between zero-shot and few-shot learning methods, it is possible that other pre-trained speech representations could improve our results further. Exploring other meta-learning strategies, such as Model-Agnostic Meta-Learning (MAML) [36] and model architectures, may yield further improvement; however, these metalearning methods incur much greater computation burden, making them difficult to evaluate across all the conditions we consider here. Additionally, our work focuses on features derived from neural networks; however, low-level features such as pitch, subband features, spectral features, and harmonic-to-noise ratio could also be discriminative and useful in few-shot settings [31], [37][39]."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work was supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the ARTS Program under contract D20232308110001. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. Yang, Y. Kartynnik, Y. Li, J. Tang, X. Li, G. Sung, and M. Grundmann, StreamVC: Real-Time Low-Latency Voice Conversion, in IEEE International Conference on Acoustics, Speech and Signal Processing, 2024, pp. 11 01611 020. [2] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, StarGAN-VC: NonParallel Many-to-Many Voice Conversion Using Star Generative Adversarial Networks, in IEEE Spoken Language Technology Workshop, 2018, pp. 266273. [3] H.-Y. Choi, S.-H. Lee, and S.-W. Lee, DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 86217 870. [4] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. Golge, and M. A. Ponti, YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone, in International Conference on Machine Learning, 2022, pp. 27092720. [5] K. Qian, Y. Zhang, S. Chang, X. Yang, and M. Hasegawa-Johnson, AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss, in International Conference on Machine Learning, vol. 97, 2019, pp. 52105219. [6] T. Kaneko, H. Kameoka, K. Tanaka, and N. Hojo, CycleGAN-VC2: Improved CycleGAN-Based Non-Parallel Voice Conversion, in IEEE International Conference on Acoustics, Speech and Signal Processing, 2019, pp. 68206824. [7] A. R. Bargum, S. Serafin, and C. Erkut, Reimagining Speech: Scoping Review of Deep Learning-Powered Voice Conversion, Frontiers in signal processing, vol. 4, p. 1339159, 2024. [8] Q. Cheng and J. Sorensen, Spread Spectrum Signaling for Speech Watermarking, in IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings, vol. 3, 2001, pp. 13371340. [9] M. Faundez-Zanuy, M. Hagmuller, and G. Kubin, Speaker verification security improvement by means of speech watermarking, Speech communication, vol. 48, no. 12, pp. 16081619, 2006. [10] S. Dathathri, A. See, S. Ghaisas, P.-S. Huang, R. McAdam, J. Welbl, V. Bachani, A. Kaskasoli, R. Stanforth, T. Matejovicova et al., Scalable Watermarking for Identifying Large Language Model Outputs, Nature, vol. 634, no. 8035, pp. 818823, 2024. [11] H. Liu, M. Guo, Z. Jiang, L. Wang, and N. Gong, Audiomarkbench: Benchmarking Robustness of Audio Watermarking, Advances in Neural Information Processing Systems, vol. 37, pp. 52 24152 265, 2024. [12] A. DAmour, K. Heller, D. Moldovan, B. Adlam, B. Alipanahi, A. Beutel, C. Chen, J. Deaton, J. Eisenstein, M. D. Hoffman et al., Underspecification presents challenges for credibility in modern machine learning, Journal of Machine Learning Research, vol. 23, no. 226, pp. 161, 2022. [13] A. Garg, Z. Cai, L. Zhang, H. L. Xinyuan, L. P. Garcıa-Perera, K. Duh, S. Khudanpur, M. Wiesner, and N. Andrews, ShiftySpeech: Large-Scale Synthetic Speech Dataset with Distribution Shifts, 2025. [Online]. Available: https://arxiv.org/abs/2502.05674 [14] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al., Matching Networks for One Shot Learning, Advances in Neural Information Processing Systems, vol. 29, 2016. [15] C. Finn, P. Abbeel, and S. Levine, Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, in International Conference on Machine Learning, 2017, pp. 11261135. [16] J. Snell, K. Swersky, and R. Zemel, Prototypical Networks for Few-shot Learning, Advances in Neural Information Processing Systems, vol. 30, 2017. [17] X. Wang, J. Yamagishi, M. Todisco, H. Delgado, A. Nautsch, N. Evans, M. Sahidullah, V. Vestman, T. Kinnunen, K. A. Lee et al., ASVspoof Detection of Natural vs. Spoofed Speech, in Interspeech, 2015, pp. 20622066. 2019: Large-Scale Public Database of Synthesized, Converted and Replayed Speech, Computer Speech & Language, vol. 64, p. 101114, 2020. [18] X. Liu, X. Wang, M. Sahidullah, J. Patino, H. Delgado, T. Kinnunen, M. Todisco, J. Yamagishi, N. Evans, A. Nautsch, and K. A. Lee, ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 25072522, 2023. [19] H. Tak, M. Todisco, X. Wang, J. weon Jung, J. Yamagishi, and N. Evans, Automatic Speaker Verification Spoofing and Deepfake Detection Using Wav2vec 2.0 and Data Augmentation, in Speaker and Language Recognition Workshop, 2022, pp. 112119. [20] R. A. R. Soto, K. Koch, A. Khan, B. Y. Chen, M. Bishop, and N. Andrews, Few-Shot Detection of Machine-Generated Text using Style Representations, in The Twelfth International Conference on Learning Representations, 2024. [21] C. Heggan, S. Budgett, T. Hospedales, and M. Yaghoobi, MetaAudio: Few-Shot Audio Classification Benchmark, in International Conference on Artificial Neural Networks, 2022, pp. 219230. [22] K. Okabe, T. Koshinaka, and K. Shinoda, Attentive Statistics Pooling for Deep Speaker Embedding, in Interspeech 2018, 2018, pp. 2252 2256. [23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention Is All You Need, Advances in neural information processing systems, vol. 30, 2017. [24] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, Unsupervised Cross-Lingual Representation Learning for Speech Recognition, in Interspeech 2021, 2021, pp. 24262430. [25] J. Ren, S. Fort, J. Liu, A. G. Roy, S. Padhy, and B. Lakshminarayanan, Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection, arXiv preprint arXiv:2106.09022, 2021. [26] I. Kukanov, J. Laakkonen, T. Kinnunen, and V. Hautamaki, MetaLearning Approaches For Improving Detection of Unseen Speech Deepfakes, in IEEE Spoken Language Technology Workshop (SLT), 2024, pp. 11731178. [27] Nicolas Muller and Pavel Czempin and Franziska Diekmann and Adam Froghyar and Konstantin Bottinger, Does Audio Deepfake Detection Generalize? in Interspeech 2022, 2022, pp. 27832787. [28] H. Wu, Y. Tseng, and H. yi Lee, CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems, in Interspeech 2024, 2024, pp. 17701774. [29] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, XVectors: Robust DNN Embeddings for Speaker Recognition, in IEEE International Conference on Acoustics, Speech and Signal Processing, 2018, pp. 53295333. [30] J. S. Chung, A. Nagrani, and A. Zisserman, VoxCeleb2: Deep Speaker Recognition, in Interspeech 2018, 2018, pp. 10861090. [31] X. Xiao, X. Tian, S. Du, H. Xu, E. S. Chng, and H. Li, Spoofing Speech Detection Using High Dimensional Magnitude and Phase Features: the NTU Approach for ASVspoof 2015 Challenge, in Interspeech 2015, 2015, pp. 20522056. [32] P. Wolters, C. Careaga, B. Hutchinson, and L. Phillips, Study of FewShot Audio Classification, arXiv preprint arXiv:2012.01573, 2020. [33] Y. Chen, T. Ko, L. Shang, X. Chen, X. Jiang, and Q. Li, An Investigation of Few-Shot Learning in Spoken Term Classification, in Interspeech 2020, 2020, pp. 25822586. [34] J. Liang, B. Meyer, I. N. Lee, and T.-T. Do, Self-Supervised Learning for Acoustic Few-Shot Classification, in IEEE International Conference on Acoustics, Speech and Signal Processing, 2025, pp. 15. [35] M. Pal, A. Raikar, A. Panda, and S. K. Kopparapu, Synthetic Speech Detection Using Meta-Learning with Prototypical Loss, arXiv preprint arXiv:2201.09470, 2022. [36] C. Finn, P. Abbeel, and S. Levine, Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, in International conference on machine learning, 2017, pp. 11261135. [37] J. Yang, R. K. Das, and H. Li, Significance of Subband Features for Synthetic Speech Detection, IEEE Transactions on Information Forensics and Security, vol. 15, pp. 21602170, 2019. [38] D. Paul, M. Pal, and G. Saha, Spectral Features for Synthetic Speech Detection, IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 4, pp. 605617, 2017. [39] T. B. Patel and H. A. Patil, Combining Evidences from Mel Cepstral, Cochlear Filter Cepstral and Instantaneous Frequency Features for"
        }
    ],
    "affiliations": [
        "Human Language Technology Center of Excellence, Johns Hopkins University"
    ]
}