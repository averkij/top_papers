{
    "paper_title": "Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation",
    "authors": [
        "Nicolas Dufour",
        "David Picard",
        "Vicky Kalogeiton",
        "Loic Landrieu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Global visual geolocation predicts where an image was captured on Earth. Since images vary in how precisely they can be localized, this task inherently involves a significant degree of ambiguity. However, existing approaches are deterministic and overlook this aspect. In this paper, we aim to close the gap between traditional geolocalization and modern generative methods. We propose the first generative geolocation approach based on diffusion and Riemannian flow matching, where the denoising process operates directly on the Earth's surface. Our model achieves state-of-the-art performance on three visual geolocation benchmarks: OpenStreetView-5M, YFCC-100M, and iNat21. In addition, we introduce the task of probabilistic visual geolocation, where the model predicts a probability distribution over all possible locations instead of a single point. We introduce new metrics and baselines for this task, demonstrating the advantages of our diffusion-based approach. Codes and models will be made available."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 1 8 7 6 0 . 2 1 4 2 : r Around the World in 80 Timesteps:"
        },
        {
            "title": "A Generative Approach to Global Visual Geolocation",
            "content": "Nicolas Dufour 1,2 David Picard 1 Vicky Kalogeiton 2 Loic Landrieu1 1 LIGM, Ecole des Ponts, IP Paris, CNRS, UGE 2 LIX, Ecole Polytechnique, IP Paris iNat-21 [74] YFCC-100M [1] OSV-5M [2] Figure 1. Geolocation as Generative Process. We explore diffusion and flow matching for visual geolocation by sampling and denoising random locations. This process generates trajectories onto the Earths surface, whose endpoints provide location estimates. Our models also provide probability densities for every possible image locations. We illustrate these trajectories and the log-densities for three images from different datasets: an Andean condor from iNat21 [74], an African open-air market from YFCC-100M [1], and dashcam snapshot from OSV-5M [2]. The predicted image locations are indicated by and the true ones by ."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Global visual geolocation consists in predicting where an image was captured anywhere on Earth. Since not all images can be localized with the same precision, this task inherently involves degree of ambiguity. However, existing approaches are deterministic and overlook this aspect. In this paper, we propose the first generative approach for visual geolocation based on diffusion and flow matching, and an extension to Riemannian flow matching, where the denoising process operates directly on the Earths surface. Our model achieves state-of-the-art performance on three visual geolocation benchmarks: OpenStreetView-5M, YFCC100M, and iNat21. In addition, we introduce the task of probabilistic visual geolocation, where the model predicts probability distribution over all possible locations instead of single point. We implement new metrics and baselines for this task, demonstrating the advantages of our generative approach. Codes and models are available here. The world has shrunk; today, we travel it at ten times the speed of hundred years past. Around the World in 80 Days, Jules Verne Knowing where an image was captured is crucial for numerous applications, and yet most images lack geolocation metadata [23]. In archaeology and cultural heritage, location data help catalog and interpret historical artifacts [13, 67], enabling better preservation and contextual understanding. In fields like forensics and investigative journalism, recovering intentionally removed GPS data can have significant implications [3, 80], such as verifying the authenticity of news images and reconstructing crime scenes or missing persons last known locations. Moreover, geolocation helps organizing multimedia archives for efficient retrieval [16, 55]. These applications motivate the long-standing computer vision challenge of global visual geolocation: inferring the location of an image purely from its visual content [28, 76]. Modeling Spatial Ambiguity. As illustrated in Fig. 1, the precision with which images can be localizedtheir localizability [2, 36]varies significantly. featureless beach could have been photographed almost anywhere, while landmark like the Eiffel Tower can be pinpointed with meterlevel accuracy. In intermediate cases, such as close-up photo of kangaroo, the location can be narrowed down to Australia but specifying its exact spot is challenging. This inherent ambiguity should be reflected in geolocation methods and metrics. However, most existing approaches produce deterministic predictions using regression [2, 26], classification [11, 72, 77], or retrieval-based techniques [47, 56, 76], thus disregarding the varying localizability of images. Modeling spatial ambiguity in computer vision tasks, such as object localization, has improved their robustness and interpretability [17, 48, 79]. Furthermore, generative models like diffusion [32, 71] and flow matching [45] have been successfully applied to complex tasks with noisy supervision, including image [33], video [6], speech [62], and music [50] generation. Inspired by these advances, we propose to bridge the gap between traditional geolocation and modern generative methods. Generative Geolocation. In this work, we present novel generative approach to global visual geolocation by using diffusion or flow-matching to denoise random locations into accurate estimates conditioned on image features. We extend recent manifold-based flow matching techniques [9] such that the denoising operate directly on geographic coordinates. This allows our model to take into account the Earths spherical geometry when learning the relationship between the content of images and their location. Additionally, we extend recent developments in density estimation for flow matching [45] to our setting, enabling our models to compute the likelihood of any location given an image and provide quantifiable estimate of its localizability. Our approach achieves higher accuracy than state-ofthe-art geolocation methods on three standard large-scale datasets: OpenStreetView-5M [2], iNat21 [74], and YFCC100M [1]. Moreover, we introduce the task of probabilistic visual geolocation, where the model predicts probability distribution over all possible locations rather than single point. We implement new metrics and baselines for this task, demonstrating the advantages of our generative approach in capturing ambiguous yet informative visual cues. Our contributions are as follows: We introduce the first application of diffusion and Riemannian flow matching methods for visual geolocation by directly denoising spatial coordinates, using manifoldbased methods to respect the Earths spherical geometry. We extend recent density estimation methods to our geolocalization setting, thus modeling the conditional distribution over locations and quantifying localizability. We demonstrate that modeling the ambiguity in geolocation leads to improved performance, achieving state-ofthe-art results on three public datasets. We propose the task of probabilistic visual geolocation, along with associated metrics and baselines. 2. Related Work Global Visual Geolocation. Visual geolocation consists in predicting an images geographic coordinates, focusing on large-scale and generalizability to unseen areas [29]. Existing methods are categorized into image retrieval-based, classification-based, and hybrid approaches. Retrieval-based methods locate an image by finding the most similar one in database using handcrafted [28, 47, 56] or deep features [76], but they require dense databases and may struggle in sparse or dynamic environments. Classification-based methods partition the globe into discrete cells, such as regular grids [77], adaptive cells [11], semantic regions [72], or administrative boundaries [26, 63] and treat geolocation as classification task. Hybrid approaches combine classification with regression [2] or retrieval to mitigate discretization issues, employing contrastive losses [39, 76] or prototype networks [26]. Izbicki et al. [36] propose model that predicts distribution of probability anywhere on Earth, but only evaluates its performance in terms of geolocation performance. Uncertainty-Aware Localization. Estimating uncertainty in neural networks is long-standing problem in computer vision [38]. This is particularly important for finegrained localization tasks, especially in robotic applications [15, 17, 41]. In 6DOF or human body pose estimation [48], uncertainty is often modeled by predicting localization heatmaps [58, 73]. This challenge is typically addressed using Bayesian statistics [51] and variational inference [82], which have been adapted to deep learning models [37]. Generative approaches, such as diffusion models [5] and normalizing flows [24], have shown promise in explicating uncertainty. These methods have been applied for uncertainty estimation for tasks such as image segmentation [78], source localization [35], and LiDAR localization [43]. Generative Models. Diffusion models have emerged as transformative force in generative modeling [32, 69, 71], demonstrating remarkable success across diverse applications including image synthesis [64, 65], video generation [31, 61], and human-centric tasks [12, 60]. Flow matching models [44] have further advanced the field by offering simplified training objective. Recent research has also explored learning directly on data distribution manifolds [10]. Generative models show particular robustness in handling data with irreducible uncertainty [20, 46, 54]. While these models have been adapted for discriminative tasks [42], bridging the performance gap with traditional discriminative models remains an active research challenge. In our work, we demonstrate that generative models can effectively tackle the geolocation task by learning the manifold of the underlying data distribution, ultimately achieving superior performance compared to discriminative approaches. 3. Method We first present our diffusion-based approach (Sec. 3.1) and extend it to the Riemannian flow matching framework (Sec. 3.2), see Fig. 2 for visual summary of the difference between these techniques. We then describe how to predict location distribution (Sec. 3.3). Finally, we detail implementation choices in Sec. C. Notations. Given an image c, we aim to predict the most likely location x0 where it was taken. More broadly, we model the conditional probability distribution p(y c), where can be any point on Earth, modeled as the unit sphere S2 in R3. Throughout the paper, we will denote pure random noise as ϵ, the noisy coordinates as xt for timestep t, and the network to optimize as ψ. 3.1. Geographic Diffusion In this section, we describe our diffusion-based generative approach to image geolocation. Traditional diffusion models progressively add Gaussian noise to data and train neural network to reverse this noising process [32, 71]. Once trained, the model can generate new data samples by starting from pure noise and performing iterative denoising. In our setting, we operate in the Euclidean space R3. Given coordinate-image pair (x0, c) from dataset Ω of geotagged images, we add noise to the true coordinates x0 and train neural network ψ to predict this noise conditioned on the image c, thus learning the relationship between visual content and geographic locations. We can then predict the location of an unseen image by iteratively denoising random initial coordinate ϵ. Training. We sample coordinate-image pair (x0, c) from Ω, and random coordinates ϵ from (0, I), where the identity matrix in R3. We randomly select time variable [0, 1] representing the diffusion time step and use scheduling function κ(t) : [0, 1] [0, 1] with κ(0) = 0 and κ(1) = 1 to control the noise level added to the coordinates. The noisy coordinates xt are defined as xt = (cid:112)1 κ(t)x0 + (cid:112)κ(t)ϵ . (1) Our network ψ takes as input the noisy coordinate xt, the noise level κ(t), and the image embedding c, and is tasked with predicting the corresponding pure noise ϵ. For ease of notation, we will omit the conditional dependence of ψ v(xt) xt xt x0 ϵ Diffusion xt = (cid:112)1 κ(t)x0 + (cid:112)κ(t)ϵ LD = ψ(xt c) ϵ2 Ψ(xt c) Flow Matching xt = (1 κ(t))x0 + κ(t)ϵ LFM = ψ(xt c) v(xt)2 x0: true location ϵ: sampled noise xt: noisy location ψ(xt c): prediction v(xt): velocity field Riemannian Flow Matching (ϵ)(cid:1) LRFM = ψ(xt c) v(xt)2 xt (cid:0)κ(t) logx0 xt = expx0 κ(t): noise scheduler Figure 2. Generative Framework. We implement three generative approaches for geolocation: diffusion in R3, flow matching in R3, and Riemannian flow matching directly on S2. This figure provides the formulas for the noising processes and the loss functions for each approach. on κ(t) in the rest of the paper. The model is trained to minimize the diffusion loss function: LD = Ex0,c,ϵ,t (cid:104) ψ(xt c) ϵ2(cid:105) , (2) where the expectation is over (x0, c) Ω, ϵ (0, I), and U[0, 1], the uniform distribution over [0, 1]. Inference. To predict the likely locations for new image c, we start by sampling random coordinate ϵ (0, I) and initialize x1 = ϵ. We then iteratively refine the coordinate xt over timesteps from = 1 to = 0 using the Denoising Diffusion Implicit Models (DDIM) sampling procedure [70]. The update equations are xtdt = (cid:112)1 κ(t)ˆxt + (cid:112)κ(t)ψ(xtc) , (cid:17) (cid:16) xt (cid:112)κ(t)ψ(xtc) ˆxt = 1 (cid:112)1 κ(t) (3) (4) , where dt is the time step size, and ˆxt is the estimate of the denoised coordinate at time t. At the end of the denoising process (t = 0), we project ˆx0 to S2 to ensure that it is valid location on the Earths surface. See Fig. 3 for an illustration of the inference process. 3.2. Extension to Riemannian Flow Matching Flow matching generalizes diffusion models with increased performance and versatility [45]. We extend our approach to this setting, and leverage Riemannian flow matching to directly work on the sphere 2. In each setting, we still denote our network ψ but redefine an alternative noising process (Eq. (1)), loss function (Eq. (2)), and denoising procedure (Eq. (3)). ϕ Ψ Ψ"
        },
        {
            "title": "ODE Solver",
            "content": "t = 1 = 0.99 = 0 predicted coordinates Figure 3. Inference Pipeline. We start by embedding the image to be localized into vector using frozen image encoder. We then sample random noise ϵ in R3 or on S2, projected here onto the sphere. We iteratively remove the noise using either the reverse diffusion or flow-matching equations for = 1 to 0. The final point of this trajectory is our predicted location. Additionally, our model be queried to predict probability distribution at any point on the sphere by solving an Ordinary Differential Equation (ODE) system. Flow Matching in R3. ping from the true coordinates x0 to random noise ϵ: In flow matching, we define mapcoordinates along the geodesic between the true coordinate x0 and the noise sample ϵ, parameterized by κ(t): xt = (1 κ(t))x0 + κ(t)ϵ . This defines the following velocity field: v(xt) = dxt dt = κ(t)(ϵ x0) , (5) (6) where κ the derivative of κ with respect to t. We train our model ψ to predict this velocity field conditionally to the image c: LFM = Ex0,c,ϵ,t ψ(xt c) v(xt)2(cid:105) (cid:104) , (7) with the expectation taken over the same distributions as in Eq. (1). During inference, we solve the Ordinary Differential Equation (ODE) initialized at random coordinate ϵ, integrating backward from = 1 to = 0 using the predicted velocity field ψ(xt c): xtdt = xt ψ(xtc)dt . (8) At the end of the integration, we project x0 onto the sphere. Riemannian Flow Matching on the Sphere. Since our data lies on the sphere 2, it is natural to constrain the flow matching process to this manifold. The Riemannian flow matching approach [9] extends flow matching to Riemannian manifolds and requires three conditions: (i) all true coordinates x0 lie on 2, (ii) the noise samples ϵ lie on 2, and (iii) the noisy coordinates xt remain on 2. Condition (i) is naturally satisfied since we are working with coordinates on the Earths surface. For condition (ii), we sample ϵ uniformly at random on 2. Unlike diffusion models, flow matching does not require the noise distribution to be Gaussian. For condition (iii), we define the noisy xt = expx0 (cid:0)κ(t) logx0(ϵ)(cid:1) , (9) where logx0 is the logarithmic map mapping point of S2 to the tangent space at x0, and expx0 is the exponential map, mapping tangent vectors back to the manifold (see appendix for detailed expressions). This parametrization induces velocity field v(xt) defined on the tangent space of xt: v(xt) = κ(t) D(xt) , (10) where D(xt) is the tangent vector at xt pointing along the geodesic from x0 to ϵ, with magnitude equal to the geodesic distance between x0 and ϵ. We train our model ψ to approximate this velocity field by minimizing LRFM = Ex0,c,ϵ,t (cid:104) ψ(xtc) v(xt)2 xt (cid:105) , (11) with (x0, c) Ω, ϵ U(S2) U[0, 1], and xt denotes the norm induced by the Riemannian metric on the tangent space at xt. During inference, we solve the ODE starting from random point ϵ 2 and integrating backward from = 1 to = 0 using the predicted velocity and projecting the iterates on the manifold at each step: xtdt = expxt (dtψ(xt c)) . (12) This ensures that the trajectory remains on the sphere 2 throughout the integration process. 3.3. Guidance and Density Prediction We can incorporate guidance to our models to improve their accuracy, and compute the spatial distribution of locations p(y c) for an image c. ) ( κ 1 0. 0.6 0.4 0.2 0 0 our scheduler sigmoid scheduler linear scheduler 0.2 0.4 0.6 0.8 1 Figure 4. Scheduler. We chose noise scheduler that assigns more weights to the beginning of the diffusion process. Guided Geolocation. We adapt the idea of classifier-free guidance [34] to our setting. We train the network ψ to learn both the conditional distribution p(y c) and the unconditional distribution p(y ) by randomly dropping the conditioning on the image for fraction of the training samples (e.g., 10%). During inference, we replace ψ in the ODE of Eqs. (3), (8) and (12) by ˆψ defined as follows: ˆψ(xt c) = ψ(xt c) + ω (ψ(xt c) ψ(xt )) , (13) where ψ(xt, ) is the prediction without conditioning, and ω 0 is the guidance scale. guidance scale of ω = 0 corresponds to the standard approach, while higher values of ω place more emphasis on the conditioning, leading to sharper distributions. Note that changing the guidance scale does not require to retrain the model. Predicting Distributions. After training ψ, we can compute the likelihood p(y c) of any coordinate corresponding to the image c. We provide here the derivation in the Euclidean flow matching setting, where it is the most straightforward. Our derivations are inspired by [45, Appendix C] and rely on the logarithmic mass conservation theorem [4, 75]. We provide more details in the appendix. Proposition 1. Given location 2 and an image c, consider solving the following ordinary differential equation system for from 0 to 1: (cid:21) (cid:20) xt (t) dt = (cid:20) ψ(x(t) c) div ψ(xt c) (cid:21) with (cid:21) (cid:20) x0 (0) (cid:21) (cid:20)y 0 , = (14) Then the log-probability density of given is: log p(y c) = log pϵ(x(1) c) (1) where pϵ is the known distribution of the pure noise ϵ, and (t) accumulates the negative divergence of the velocity field along the trajectory xt. We solve this system numerically using the fifth-order Dormand-Prince-Shampine variant of the Runge-Kutta stepping scheme [7, 18], as implemented in TorchDiffEq [8]. 3.4. Implementation We detail here our choice of scheduler and model architecture, which are shared across all implementations. Scheduler. We observed better results with schedulers κ(t) that assign more time to the beginning of the noising process i.e. when the coordinates remain close to the true location. Our intuition is that this encourages the network to focus on learning fine-grained location cues in images rather than the easier, continent-level information. As illustrated in Fig. 4, we set κ(t) as skewed sigmoid function: κ(t) = σ(α) σ(α + t(β α)) σ(α) σ(β) , (15) where σ(t) = 1/(1 + exp(t)) is the sigmoid function, and α, β control the skewness of the sigmoid. In practice, we use α = 3 and β = 7. Model Architecture. The network ψ used for all methods is composed of 6 residual blocks which take as inputthe current noisy coordinate xt, the embedding of image c, and the current noise level κ(t). The image is embedded using pre-trained and frozen image encoder ϕ into d-dimensional vector. Additionally, we compute d-dimensional Fourier features of κ(t) to capture fine-grained temporal information. Each block of ψ follows similar architecture to the DiT model [59], consisting of Multi-Layer Perceptron (MLP) with GELU activations [30]. We modulate the coordinate embeddings according to the conditioning using adaptive layer normalization (AdaLN). The network concludes with an AdaLN layer and linear layer that outputs the predicted noise. See the appendix for more details. 4. Experiments We evaluate our models on two tasks: global visual geolocation and probabilistic visual geolocation. In the first task, the model predicts the most likely location where an image was taken (Sec. 4.1), while in the second, the model estimates distribution over all possible locations (Sec. 4.2). Since probabilistic visual geolocation is novel task, we introduce new metrics and baselines for evaluation. We consider three datasets of geolocated images: OpenStreetView-5M [2] (OSV-5M) contains 5 million street view training images from 225 countries and over 70K cities worldwide. The test set includes 200K images and is built with 1km buffer with the train set. iNat21 [74] includes 2.7 million images of animals from 10K species, collected and annotated by community scientists. We use the public validation set that contains 10 images for each of the 10K species featured. Table 1. Geolocation Performance. We compare the geolocation precision of traditional and generative visual geolocation methods, and three implementation of our generative approaches. OSV-5M [2] iNat21 [74] geos. dist accuracy (in %) / (km) country region city dist (km) 2273 SC 0-shot [25] 3028 Regression [2] 3331 ISNs [52] Hybrid [2] 3361 SC Retrieval [25] 3597 i r d Uniform vMF vMFMix [36] Diff R3 (ours) FM R3 (ours) RFM S2 (ours) t n 131 2776 1746 3762 3688 3767 2854 1481 2308 1814 10052 2439 5662 1123 1149 1069 38.4 56.5 66.8 68.0 73.4 2.4 52.7 34.2 75.9 74.9 76.2 20.8 16.3 39.4 39.4 45.8 0.1 17.2 11.1 40.9 40.0 44.2 14.8 0.7 4.2 5.9 19. 0.0 0.6 0.3 3.6 4.2 5.4 10,010 6270 4701 3057 2942 2500 YFCC-4k [1, 76] geos. dist accuracy (in %) /5000 (km) 25km 200km 750km 2500km PlaNet [77] i r d t n CPlaNet [66] ISNs [52] Translocator [63] GeoDecoder [11] PIGEON [26] Uniform 131.2 vMF 1847 vMFMix [36] 1356 Diff R3 (ours) 2845 FM R3 (ours) 2838 RFM S2 (ours) 2889 RFM10M S2 (ours) 3210 10052 3563 4394 2461 2514 2461 2058 14.3 14.8 16.5 18.6 24.4 24.4 0.0 4.8 0.4 11.1 22.1 23.7 33.5 22.2 21.9 24.2 27.0 33.9 40. 0.0 15.0 8.8 37.7 35.0 36.4 45.3 36.4 36.4 37.5 41.1 50.0 62.2 0.3 30.9 20.9 54.7 53.2 54.5 61.1 55.8 55.5 54.9 60.4 68.7 77.7 3.8 53.4 41.0 71.9 73.1 73.6 77.7 YFCC [1] The Yahoo Flickr Creative Commons dataset comprises 100 million highly diverse media objects, of which we use the subset of 48 million images with precise geotags. To allow comparison with other methods, we evaluate all methods on the public subset YFCC4k of 4000 images introduced in [76]. Baselines. We implement several generative baselines to contextualize our results: Uniform. This baseline assigns constant density probability of 1/(4π) steradian1 to any point on Earth. von Mises-Fisher Regression [22, 27]. We modify our model to map the image feature to parameters (µ, κ) of von Mises-Fisher (vMF) distribution on the sphere, where µ R3, µ = 1, and κ > 0. The network is trained to minimize the negative log-likelihood at the true location x0: 4,000 3,000 2,000 1, o e 0 1 2 1 0. 0.6 0.4 0.2 0 r A 4 16 Number of Timesteps 32 64 128 256 GeoScore Country Region City Figure 5. Impact of Number of Timesteps. We represent different metrics on OpenStreetView-5M with different numbers of timesteps for the Riemannian Flow matching model. vMF distributions (vMFMix) with mixture weights w1, . . . , wK > 0 and (cid:80)K k=1 wk = 1, and distribution parameters (µ1, . . . , µK, κ1, . . . , κK). The loss is defined as: ℓvMFMix(x, c) = log2 (cid:33) wi vMF(x µk, κk) . (17) (cid:32) (cid:88) Model Parameterization. We evaluate our three generative approaches: diffusion and flow matching in R3 (Diff R3 and FM R3), and Riemannian Flow-Matching on the sphere (RFM S2). All models and baselines are trained on the training set of the dataset they are evaluated on. All models are trained for one million iterations, except RFM10M S2 which undergoes 10M iterations. All models and baselines share the same backbone ϕ : DINOv2-L [57] with registers [14], except when training on OpenStreetView-5M, where we employ ViT-L model [19] fine-tuned with StreetCLIP (SC) [25]. All models use the same configuration for the network ψ with 36 parameters, except for iNat21, where we use smaller version with 9.2 parameters (details in the appendix). We set the guidance scale to 2 when predicting locations and to 0 when computing distributions, as justified in Section 4.2. 4.1. Visual Geolocation Performance We first evaluate our models ability to predict the location where an image was taken, comparing its performance to existing geolocation methods from the literature. ℓvMF(x0, c) = log2 (vMF(x0 µ, κ)) (16) = log2 (cid:18) κ 4π sinh(κ) (cid:19) κµx0 . Metrics. We use the following geolocation metrics, averaged across the test sets: Distance: The Haversine distance (in km) between the true and predicted locations. Mixture of vMF [36]. To handle multimodal distributions, we extend the model to predict mixture of GeoScore: score inspired by the game GeoGuessr, defined as 5000 exp(δ/1492.7) [26] where δ is the Table 2. Probabilistic Visual Geolocation. We evaluate the quality of the predicted distributions. Note that the likelihoods of distributions defined in R3 and S2 are not directly comparable, as they are based on different metrics. Moreover, contrary to the discrete case, log-likelihoods and entropies of continuous distribution can be negative. To save space, we only provide the generation metrics for iNat21. OSV-5M YFCC iNat21 NLL NLL NLL precision recall density coverage Uniform vMF Regression vMFMix RFlowMatch S2 (ours) Diffusion R3 (ours) FlowMatch R3 (ours) 1.22 10.13 0.06 -1.51 0.58 -5.01 1.22 0.01 -0.04 -3.71 0.63 -7.15 1.22 1.99 -0.23 -1. 0.68 -4.00 0.58 0.52 0.63 0.88 0.76 0.76 0.98 0.98 0.98 0.95 0.98 0.97 0.38 0.37 0.47 0. 0.60 0.61 0.22 0.24 0.29 0.59 0.44 0.47 Haversine distance. This score ranges from 0 to 5000, with higher scores indicating better accuracy. Accuracy: The proportion of predictions that fall within the right countries, regions, or cities, or set distance to their true location. Results. Table 1 compares our models against established geolocation methodsincluding classification, regression, and retrieval-based approachesas well as our own generative baselines introduced in Sec. 4.2. On all three datasets, our models achieve state-of-the-art geolocation performance, beating not only discriminative methods but also retrievalbased approaches that rely on large, million-image databases. On the large-scale YFCC dataset, extending the training of our best model (RFM S2) to 9 million iterations yields consistent improvements. Overall, our generative approach surpasses all methods not based on retrieval or prototypes by considerable margin. Compared to the specialized hybrid approach of Astruc et al. [2], we increase the GeoScore by 406 points, reduce the average distance by 745 km, and improve country-level accuracy by 8.2%. While our methods display excellent results at various scales (from countrylevel down to 25 km), retrieval-based techniques maintain an advantage at extremely fine-grained resolutions, thanks to their extensive image databases. Among the generative strategies, flow matching consistently outperforms diffusion, and the Riemannian variant on the sphere outperforms the Euclidean counterpart, highlighting the benefit of incorporating the Earths geometry into the model. The single-component vMF model performs similarly to discriminative regression baseline, which aligns with the fact that predicting single direction on the sphere is essentially location regression. In contrast, the mixture of vMF distributions overfits the training set, leading to weaker performance. Analysis. We represent in Figure 5 the influence of the number of timesteps on the RFM models performance. The GeoScore improves from 591 (1 step) to 3744 (16 steps), after which it plateaus around 3746. Similarly, country-level accuracy increases from 9.4% to 76%, and city-level accuracy from 0.02% to 4.8%. This demonstrates that iterative refinement benefits our model up to certain point, after which additional steps yield diminishing returns. 4.2. Probabilistic Visual Geolocation Beyond predicting single location, our model can estimate distribution over all possible locations, capturing the inherent uncertainty in visual geolocation. Metrics. We evaluate the quality of the predicted distributions p(y c), where is an image and 2 represents any location on the Earths surface, with the following metrics: Negative Log-Likelihood (NLL): We compute the average negative log-likelihood per-dimension (see [9, F]) of the true locations under the predicted distributions: NLL = 1 3N (cid:88) i=1 log2 p(xi ci) , (18) where (xi, ci) are the true location and image pairs in the test set. This metric quantifies how well the predicted distributions align with the true locations. Localizability: We quantify the localizability of an image as the negative entropy of the predicted distribution: Localizability(c) = (cid:90) 2 p(y c) log2 p(y c)dy . (19) We estimate this integral with Monte-Carlo sampling [49] with 10,000 samples. Generative Metrics. we report the classic Precision and Recall metrics [40], as well as the more recent Density and Coverage [53]. See the appendix for more details. Results. Table 2 reports the performance of all models for the probabilistic visual geolocation task. Our models achieve significantly lower NLL than the baselines, clearly showing that the predicted distribution is more consistent with the test image locations. Although we cannot directly compare the likelihoods of models defined in R3 and on the OSV-5M [2] INat21 [74] YFCC [1] (a) Localizability = 1. (b) Localizability = 0.68 (c) Localizability = 1.75 (d) Localizability = 0.57 (e) Localizability = 0.51 (f) Localizability = 0.94 H i w (g) Localizability = 0.49 (h) Localizability = 0.41 (i) Localizability = 0.47 Figure 6. Estimating Localizability. We use the entropy of the predicted distribution as proxy for the localizability of images. For each dataset, we present examples of high, medium, and low localizability, which correlate well with human perception. sphere 2 due to different underlying metrics, we observe that flow matching performed in R3 yields better NLL than diffusion. The mixture of vMF distributions improves upon the single vMF model across all metrics. This indicates that while mixtures may not enhance geolocation accuracy, they may better capture the inherent ambiguity of the task as many images have multimodal distributions with several reasonable guesses, for example, Ireland vs. New Zeland. In terms of generative metrics, our Riemannian Flow Matching model outperforms all baselines and models operating in R3, demonstrating the effectiveness of modeling distributions on the Earths surface. We hypothesize that our Riemannian flow matching approach leads to better performances because the results are directly output by the c G 3,800 3,700 3, 3,500 3,400 0.9 0.85 e 2 + p GeoScore F1-score 0 2 4 6 10 0.8 Guidance Scale Figure 7. Impact of Classifier-Free Guidance. We plot the evolution of the GeoScore and generative metrics depending on the guidance scale ω for the OSV-5M dataset. generation process, compared to R3 where the output of the generation process has to be projected onto 2 which can add subtle errors. Localizability. Figure 6 displays examples of images with low, medium, and high localizability as measured by the negative entropy of the distributions predicted by the Riemannian Flow Matching approach. The model can detect subtle hints, such as road signage (a) or vegetation (d), to locate street-view images with relatively good confidence. However, rural road in India (g) has low localizability score, as it could have been taken anywhere in the country. The localizability of animal images (b,e,h) is lower than that of human-centric or street-view images and correlates with the rarity of the species depicted. Impressively, some images can be pinpointed to meter-level accuracy, such as the picture of the Eiffel Tower (c). An image captured inside an NFL stadium (f) produces multimodal distribution centered around major American cities with prominent NFL teams. picture of featureless beach (i) results in highly spread-out distribution along most of the Earths coastlines, resulting in low localizability. Guidance. Figure 7 shows the impact of the guidance scale ω on both GeoScore and generative metricsusing the F1-score to combine precision and recall. At higher guidance scales, the predicted distributions become sharper, concentrating probability density around the predicted locations and assigning less density elsewhere. This increased focus on the modes enhances geolocation accuracy but leads to poorer coverage of the true distribution, as the model collapses onto the most probable areas. Consequently, metrics that evaluate discrepancies between the predicted and true densitiessuch as precision and recallworsen at higher guidance levels. This trade-off highlights the balance between achieving high geolocation accuracy and capturing the full diversity of the data distribution. 5. Conclusion We introduced novel generative approach to global visual geolocation based on diffusion models and Riemannian flow matching on the Earths surface. Our method effectively captures the inherent ambiguity in geolocating imagesan aspect often overlooked by deterministic models. Experiments on three standard benchmarks demonstrated state-of-theart geolocation performance. Additionally, we introduced the task of probabilistic visual geolocation, along with its metrics and baselines. Our generative approach predicts probability distributions that fits more closely to the data despite its high ambiguity. Our approach is especially valuable for applications involving images with vague or ambiguous location cues, where traditional methods struggle to provide meaningful predictions. 6. Acknowledgements This work was supported by ANR project TOSAI ANR-20IADJ-0009, and was granted access to the HPC resources of IDRIS under the allocation 2024-AD011015664 made by GENCI. We would like to thank Julie Mordacq, Elliot Vincent, and Yohann Perron for their helpful feedback."
        },
        {
            "title": "References",
            "content": "[1] YFCC100m. https: // gitlab. com/jfolz / yfcc100m, accessed: 2023-10-10 1, 2, 6, 8 [2] Astruc, G., Dufour, N., Siglidis, I., Aronssohn, C., Bouia, N., Fu, S., Loiseau, R., Nguyen, V.N., Raude, C., Vincent, E., et al.: OpenStreetView-5M: The many roads to global visual geolocation. In: CVPR (2024) 1, 2, 5, 6, 7, 8 [3] Bamigbade, O., Sheppard, J., Scanlon, M.: Computer vision for multimedia geolocation in human trafficking investigation: systematic literature review. In: arXiv preprint arXiv:2402.15448 (2024) 1 [4] Ben-Hamu, H., Cohen, S., Bose, J., Amos, B., Nickel, M., Grover, A., Chen, R.T., Lipman, Y.: Matching normalizing flows and probability paths on manifolds. In: ICML (2022) 5, 15 [5] Berry, L., Brando, A., Meger, D.: Shedding light on large generative networks: Estimating epistemic uncertainty in diffusion models. In: UAI (2024) [6] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023) 2 [7] Butcher, J.: Runge-Kutta methods for ordinary differential equations. Numerical Analysis and Optimization (2015) 5 [8] Chen, R.T.Q.: torchdiffeq (2018), https : / / github.com/rtqichen/torchdiffeq 5 [9] Chen, R.T., Lipman, Y.: Riemannian flow matching on general geometries. In: ICLR (2024) 2, 4, 7 [10] Chen, R.T., Lipman, Y.: Riemannian flow matching on general geometries. In: ICLR (2024) 2 [11] Clark, B., Kerrigan, A., Kulkarni, P.P., Cepeda, V.V., Shah, M.: Where we are and what were looking at: Query based worldwide image geo-localization using hierarchies and scenes. In: CVPR (2023) 2, 6 [12] Courant, R., Dufour, N., Wang, X., Christie, M., Kalogeiton, V.: ET the exceptional trajectories: Text-tocamera-trajectory generation with character awareness. In: ECCV (2024) 2 [13] Daoud, M., Huang, J.X.: Mining query-driven contexts for geographic and temporal search. International Journal of Geographical Information Science (2013) 1 [14] Darcet, T., Oquab, M., Mairal, J., Bojanowski, P.: Vision transformers need registers. ICLR (2024) 6 [15] Dellaert, F., Fox, D., Burgard, W., Thrun, S.: Monte Carlo localization for mobile robots. In: ICRA (1999) [16] DeLozier, G., Wing, B., Baldridge, J., Nesbit, S.: Creating novel geolocation corpus from historical texts. In: ACL Linguistic Annotation Workshop (2016) 1 [17] Deng, H., Bui, M., Navab, N., Guibas, L., Ilic, S., Birdal, T.: Deep Bingham networks: Dealing with uncertainty and ambiguity in pose estimation. International Journal of Computer Vision (2022) 2 [18] Dormand, J.R., Prince, P.J.: family of embedded Runge-Kutta formulae. Journal of computational and applied mathematics (1980) 5 [19] Dosovitskiy, A.: An image is worth 16x16 words: Transformers for image recognition at scale. ICLR (2021) 6 [20] Dufour, N., Besnier, V., Kalogeiton, V., Picard, D.: Dont drop your samples! Coherence-aware training benefits conditional diffusion. In: CVPR (2024) 2 [21] Durrett, R., Durrett, R.: Probability: Theory and examples. Cambridge university press (2019) [22] Fisher, R.A.: Dispersion on sphere. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences (1953) 6 [23] Flatow, D., Naaman, M., Xie, K.E., Volkovich, Y., Kanza, Y.: On the accuracy of hyper-local geotagging of social media content. In: International Conference on Web Search and Data Mining (2015) 1 [24] Grathwohl, W., Chen, R.T., Bettencourt, J., Sutskever, I., Duvenaud, D.: FFJORD: Free-form continuous dynamics for scalable reversible generative models. In: ICLR (2019) 2 [25] Haas, L., Alberti, S., Skreta, M.: Learning generalized zero-shot learners for open-domain image geolocalization. In: arXiv preprint arXiv:2302.00275 (2023) 6 [26] Haas, L., Alberti, S., Skreta, M.: PIGEON: Predicting image geolocations. In: CVPR (2023) 2, [27] Hasnat, M.A., Bohne, J., Milgram, J., Gentric, S., Chen, L.: von Mises-Fisher mixture model-based deep learning: Application to face verification. In: arXiv preprint arXiv:1706.04264 (2017) 6 [28] Hays, J., Efros, A.A.: Im2GPSs: Estimating geographic information from single image. In: CVPR (2008) 1, 2 [29] Hays, J., Efros, A.A.: Large-scale image geolocalization. Multimodal location estimation of videos and images (2015) 2 [30] Hendrycks, D., Gimpel, K.: Gaussian error linear units (Gelus). In: arXiv preprint arXiv:1606.08415 (2016) 5 [31] Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P., Poole, B., Norouzi, M., Fleet, D.J., et al.: Imagen video: High definition video generation with diffusion models. arXiv (2022) 2 [32] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS (2020) 2, [33] Ho, J., Saharia, C., Chan, W., Fleet, D.J., Norouzi, M., Salimans, T.: Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research (2022) 2 [34] Ho, J., Salimans, T.: Classifier-free diffusion guidance. In: NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications (2021) 5 [35] Huang, B., Yu, W., Xie, R., Xiao, J., Huang, J.: Twostage denoising diffusion model for source localization in graph inverse problems. In: ECML-PKDD. Springer (2023) 2 [36] Izbicki, M., Papalexakis, E.E., Tsotras, V.J.: Exploiting the Earths spherical geometry to geolocate images. In: MLKDD (2020) 2, 6 [37] Kendall, A., Cipolla, R.: Modelling uncertainty in deep learning for camera relocalization. In: ICRA (2016) 2 [38] Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for computer vision? In: NeurIPS (2017) 2 [39] Kordopatis-Zilos, G., Galopoulos, P., Papadopoulos, S., Kompatsiaris, I.: Leveraging EfficientNet and contrastive learning for accurate global-scale location estimation. In: International Conference on Multimedia Retrieval (2021) [40] Kynkaanniemi, T., Karras, T., Laine, S., Lehtinen, J., Aila, T.: Improved precision and recall metric for assessing generative models. NeurIPS (2019) 7, 14 [41] Levinson, J., Thrun, S.: Robust vehicle localization in urban environments using probabilistic maps. In: ICRA (2010) 2 [42] Li, A.C., Prabhudesai, M., Duggal, S., Brown, E., Pathak, D.: Your diffusion model is secretly zeroshot classifier. In: CVPR (2023) 3 [43] Li, W., Yang, Y., Yu, S., Hu, G., Wen, C., Cheng, M., Wang, C.: Diffloc: Diffusion model for outdoor lidar localization. In: CVPR (2024) 2 [44] Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.: Flow matching for generative modeling. In: ICLR (2023) 2, 15 [45] Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.: Flow matching for generative modeling. In: The Eleventh International Conference on Learning Representations (2024) 2, 3, 5 [46] Mackowiak, R., Ardizzone, L., Kothe, U., Rother, C.: Generative classifiers as basis for trustworthy image classification. In: CVPR (2021) [47] Martin, D., Fowlkes, C., Tal, D., Malik, J.: database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In: ICCV (2001) 2 [48] Merrill, N., Guo, Y., Zuo, X., Huang, X., Leutenegger, S., Peng, X., Ren, L., Huang, G.: Symmetry and uncertainty-aware object SLAM for 6DOF object pose estimation. In: CVPR (2022) 2 [49] Metropolis, N., Ulam, S.: The Monte Carlo method. Journal of the American statistical association (1949) 7 [50] Mittal, G., Engel, J., Hawthorne, C., Simon, I.: Symbolic music generation with diffusion models. ISMIR (2021) 2 [51] Mullane, J., Vo, B.N., Adams, M.D., Vo, B.T.: random-finite-set approach to Bayesian SLAM. IEEE transactions on robotics (2011) 2 [52] Muller-Budack, E., Pustu-Iren, K., Ewerth, R.: Geolocation estimation of photos using hierarchical model and scene classification. In: ECCV (2018) [53] Naeem, M.F., Oh, S.J., Uh, Y., Choi, Y., Yoo, J.: Reliable fidelity and diversity metrics for generative models. In: ICML (2020) 7, 14 [54] Nicolas Dufour, David Picard, V.K.: SCAM! Transferring humans between images with semantic cross attention modulation. In: ECCV (2022) 2 [55] Nikolaidou, K., Seuret, M., Mokayed, H., Liwicki, M.: survey of historical document image datasets. International Journal on Document Analysis and Recognition (2022) 1 [56] Oliva, A., Torralba, A.: Building the gist of scene: The role of global image features in recognition. Progress in brain research (2006) 2 [57] Oquab, M., Darcet, T., Moutakanni, T., Vo, H.V., Szafraniec, M., Khalidov, V., Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., et al.: DINOv2: Learning robust visual features without supervision. TMLR 6 [58] Pavlakos, G., Zhou, X., Derpanis, K.G., Daniilidis, K.: Coarse-to-fine volumetric prediction for single-image 3D human pose. In: CVPR (2017) [59] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: ICCV (2023) 5 [60] Petrovich, M., Litany, O., Iqbal, U., Black, M.J., Varol, G., Peng, X.B., Rempe, D.: Multi-track timeline control for text-driven 3D human motion generation. In: CVPR Workshop on Human Motion Generation (2024) 2 [61] Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.Y., Chuang, C.Y., et al.: Movie Gen: cast of media foundation models. arXiv (2024) 2 [62] Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., Kudinov, M.: Grad-TTS: diffusion probabilistic model for text-to-speech. In: ICML. PMLR (2021) 2 [63] Pramanick, S., Nowara, E.M., Gleason, J., Castillo, C.D., Chellappa, R.: Where in the world is this image? Transformer-based geo-localization in the wild. In: ECCV (2022) 2, [64] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022) 2 [65] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. In: NeurIPS (2022) 2 [66] Seo, P.H., Weyand, T., Sim, J., Han, B.: Cplanet: Enhancing image geolocalization by combinatorial partitioning of maps. In: ECCV (2018) 6 [67] Smith, D.A., Crane, G.: Disambiguating geographic names in historical digital library. In: International Conference on Theory and Practice of Digital Libraries. Springer Berlin Heidelberg, Berlin, Heidelberg (2001) 1 [68] Sommer, S., Fletcher, T., Pennec, X.: Introduction to differential and riemannian geometry. In: Riemannian Geometric Statistics in Medical Image Analysis. Elsevier (2020) 15 [69] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR (2021) 2 [70] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR (2021) 3, 15 [71] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-based generative modeling through stochastic differential equations. In: ICLR (2021) 2, 3, 15 [72] Theiner, J., Muller-Budack, E., Ewerth, R.: Interpretable semantic photo geolocation. In: WACV (2022) [73] Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint training of convolutional network and graphical model for human pose estimation. In: NeurIPS (2014) 2 [74] Van Horn, G., Cole, E., Beery, S., Wilber, K., Belongie, S., Mac Aodha, O.: Benchmarking representation learning for natural world image collections. In: CVPR (2021) 1, 2, 5, 6, 8 [75] Villani, C.: Optimal transport: Old and new. Berlin: Springer (2009) 5, 15 [76] Vo, N., Jacobs, N., Hays, J.: Revisiting IMG2GPS in the deep learning era. In: ICCV (2017) 1, 2, 6 [77] Weyand, T., Kostrikov, I., Philbin, J.: Planet-photo geolocation with convolutional neural networks. In: ECCV (2016) 2, 6 [78] Wolleb, J., Sandkuhler, R., Bieder, F., Valmaggia, P., Cattin, P.C.: Diffusion models for implicit image segmentation ensembles. In: International Conference on Medical Imaging with Deep Learning. PMLR (2022) 2 [79] Xu, L., Qu, H., Cai, Y., Liu, J.: 6D-diff: keypoint diffusion framework for 6d object pose estimation. In: CVPR (2024) [80] Yokota, R., Hawai, Y., Tsuchiya, K., Imoto, D., Hirabayashi, M., Akiba, N., Kakuda, H., Tanabe, K., Honma, M., Kurosawa, K.: revisited visual-based geolocalization framework for forensic investigation support tools. Forensic Science International: Digital Investigation (2020) 1 [81] You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., Hsieh, C.J.: Large batch optimization for deep learning: Training bert in 76 minutes. ICLR (2020) 13 [82] Zangeneh, F., Bruns, L., Dekel, A., Pieropan, A., Jensfelt, P.: probabilistic framework for visual localization in ambiguous scenes. In: ICRA (2023) 2 Around the World in 80 Timesteps:"
        },
        {
            "title": "Supplementary Material",
            "content": "In this appendix, we present our ablation study in Sec. A, and provide additional results and qualitative illustrations in Sec. B. We then provide implementation and technical details in Sec. C, and some technical elements and proofs in Sec. D. A. Ablation Study We conduct an ablation study on the Riemannian Flow Matching approach to evaluate the impact of our design choices, and report the results in Tab. A. Guided Sampling. Guided sampling improves the geoscore, but as shown in Figure 7 of the main paper, leads to low likelihood scores due to overconfident predictions. Single sampling without guidance. We do not add any guidance (ω = 0 in Eq. 13). We observe loss of geoscore of 182 GeoScore point (3485 vs 3767) , but the NLL is better (-1.8 vs 33.1). Guidance improves the geolocation performance but significantly worsen the probabilistic prediction. Ensemble sampling. We sample and denoise 32 random points and select the prediction with highest likelihood. While this approach yields the best performance for the distribution estimation metrics, it is significantly more computationally expensive due to the necessity of generating and evaluating multiple samples. In practice, this inflates the prediction time per image from approximately 2 milliseconds to 72 milliseconds.. Standard Sigmoid Scheduler. We replace our proposed scheduler defined in Eq.15 of the main paper by the standard not skewed sigmoid scheduler with α = 3 and β = 3. This modification increases the geoscore but decreases the quality of the predicted densities as measurs by the generative metrics. The standard sigmoid does not allocate sufficient emphasis to the earlier stages of the diffusion process (t close to 0: low noose regime), which are crucial for fine-grained localization. Linear Sigmoid Scheduler. We replace our proposed scheduler defined in Eq.15 of the main paper by linear scheduler. This modification decreases both the geoscore and the quality of the predicted densities. B. Qualitative Illustration Qualitative Illustrations. We provide detailed illustration of our network in Fig. A. We observe that the parametric methods vMF and vMF mixture fail to capture highly multimodal distributions. In contrast, our distributions are non-parametric and can predict highly complex spatial distributions. The vMF mixture is collapse to single vMF, as we observed for majority of the prediction. We observe that both flow matching approaches give results that visually close. Note however that the value of the likelihoods are not comparable as both models are not embedded in the same metric space. The generative metrics detailed in Tab. show that the Riemannian model fits the unconditioned distribution better at fine-grained scale. Detailed Quantitative Results. We provide in Tab. the full generative metrics for the OSV-5M and YFCC datasets. Similarly to what we observed for iNat21 in the main paper, flow matching and particularly Riemannian flow matching leads to the most faithful predicted distributions of samples. C. Implementation Details Baseline Details. We use the same backbone and image encoder as in our model for all baselines. We adapt them to the baselines with two modifications: (i) The missing inputs (noisy coordinates and scheduler) are replaced by learnable parameters. (ii) We replace the final prediction head with MLPs that predict the parameters of the von Mises-Fisher (vMF) distribution: the mean direction µ 2 (using L2 normalization) and the concentration parameter κ > 0 (using softplus activation). For the mixture of vMF model, we use = 3 vMF distributions. The µ and κ heads now predict three sets of parameters, and the mixture weights are predicted by another dedicated head (with softmax activation). Architecture Details. Our model architecture, illustrated in Fig. B, consists of several key components: Input Processing: The model takes three inputs: the current coordinate xt, an image embedding c, and the noise level κ(t). Initial Transformation: The coordinate xt first passes through linear layer that expands the dimension from 3 to d, followed by an ADA-LN layer that conditions on parameters α, β. Main Processing Block: The core of the network (shown in gray) is repeated times and consists of: linear layer that expands dimension from to 4d GELU activation function linear layer that reduces dimension from 4d to (a) Image from YFCC (b) Diffusion in R3 (c) Flow Matching in (d) Riemannian Flow Matching in S2 (e) von Mishes-Fisher (f) von Mishes-Fisher Mixture Figure A. Qualitative Illustration. We represent the predicted distributions predicted by different models for the same image, taken in an NFL stadium in Maryland, USA. Table A. Ablation Study. We estimate the impact of different designs. We consider Riemannian diffusion model and evaluate on OpenStreetView-5M. Geoscore NLL precision recall density coverage Guided sampling Single sampling Ensemble sampling Linear sigmoid Standard sigmoid 3746.79 3485.88 3588.25 3734.84 3767.21 33.1 -1.81 -4. -1.28 -1.51 0.841 0.844 0.899 0.775 0.827 0.896 0.924 0. 0.931 0.913 0.797 0.790 0.881 0.687 0.765 0.590 0.560 0. 0.536 0.565 An ADA-LN layer conditioned on α, β block. AdaLN: The AdaLN layer is conditional layer normalization that scales and shifts the input based on the image features: AdaLN(x) = γ µ σ + β (A) where µ, σ are the mean and standard deviation of on the feature dimension, and γ, β are learnable parameters. Skip Connections: Each processing block has skip connection path that: Skips the processing block and directly connects the input to the output to allow better gradient flow. Is modulated by gating parameter γ that controls how much of the block output is added to the main path. This gated skip connection allows the network to adaptively control information flow around each processing Output Head: The final prediction is obtained through linear layer that maps to the target dimension (cid:55) 3. Time step Conditioning: The noise level κ(t) is incorporated through addition to the conditioning of the AdaLN layers. We use = 12 blocks of dimension = 512 for OSV5M and YFCC-100M and blocks of dimension = 256 for iNat21. Optimization. We train our models for 1M steps with batch size of 1024, using the Lamb optimizer [81] with learning rate of 8 104. We use warmup of 500 steps and cosine decay learning rate schedule. We use an EMA of 0.999 for the model weights. For OSV-5M and YFCC-100M, we use weight decay of 0.05 and for iNaturalist we use 0.1. We drop out 10% of the time the conditioning image embedding to allow classifier free guidance. Table B. Generative Metrics. We evaluate the quality of the predicted distributions with generated metrics for OSV-5M and YFCC for the unconditional distribution. OSV-5M YFCC precision recall density coverage precision recall density coverage Uniform vMF Regression vMF Mixture RFlowMatch S2 (ours) Diffusion R3 (ours) FlowMatch R3 (ours) 0.29 0.598 0.513 0.841 0.822 0.845 0.98 0.982 0.980 0. 0.916 0.907 0.21 0.499 0.422 0.797 0.752 0.799 0.21 0.446 0.358 0.590 0.568 0.575 0.59 0.667 0.626 0. 0.938 0.953 0.99 0.993 0.988 0.952 0.959 0.959 0.38 0.542 0.474 1.060 0.959 1.037 0.22 0.599 0.498 0. 0.837 0.920 (cid:55)"
        },
        {
            "title": "Linear",
            "content": "d (cid:55) 2d ADA-LN α, β n L 4d (cid:55) N Linear GELU (cid:55) 4d Linear ADA-LN α, β 3 (cid:55) Linear γ n (cid:55) 3d I Linear (cid:55) xt κ(t) Figure B. Architecture. Our model takes as input the current coordinate xt, the image embedding ϕ(c), and the noise level κ(t). We use this architecture for all our formulations, including deterministic baselines. Metrics. Precision and Recall: We adapt the classic generation metrics of precision and recall [40] to our spatial setting by considering geographic proximity. We consider set of true locations, and set of locations sampled from the unconditional distribution predicted by our model. For set of locations (X or ) and Z, we define B(z, Z) the ball of S2 centered on and with radius equal to the k-th nearest neighbour of in Z. We can then define the approximated manifold of ta et of locations: manifold(Z) := (cid:91) zZ B(z, Z) . (B) We now define the precision and recall as the proportion of predicted (resp. true) locations within the manifold of true (resp. predicted) locations: precision := recall := 1 1 (cid:88) [y manifold(X)] (C) yY (cid:88) xX [x manifold(Y )] , (D) where [P ] is the Iverson bracket, equal to one is the statement is true and 0 otherwise. Throughout this paper, we select the number of neighbours to = 3. Density and Coverage: Naeem et al. [53] introduce more reliable versions of the precision and recall metrics, particularly for distributions containing outliers. We propose to adapt these metrics to our setting. The density measures how closely the predicted locations cluster around the true location : density := 1 (cid:88) (cid:88) yY xX [y B(x, X)] . (E) The recall metrics can be misleading high for predicted manifolds that cover uniformly the embeddings space, which is particularly problematic on lowdimensional space such as S2: the uniform distribution has recall of 0.98 on OSV-5M. Coverage better captures how well the generated distribution spans the true data modes without rewarding such overestimation by assessing how well the predicted distributions span the true data: coverage := 1 (cid:88) xX [y B(x, X)] . (F) D. Technical Details In this section, we present details on Riemannian geometry on the sphere, and proof sketch of Proposition 1 and elements on its generalization. Spherical Geometry. The logarithmic map logx maps point S2 onto Tx, the tangent space at point [68]: logx(y) = θ sin θ (y cos θx) , (G) (cid:21) (cid:20)x(0) (0) = (cid:21) (cid:20)y 0 . (N) Where accumulates the divergence of the velocity field along the trajectory: (t) = (cid:82) 0 div ψ(x(t) c) and hence (0) = 0. The system in Eq. (M) admits only one solution for all [0, 1]. Equation (O) gives us that: log p(x0 c) = log p(x(1) c) (1) . (O) where θ = arccos(x, y) is the angle between and y. The exponential map expx of point S2 maps tangent vector Tx back onto th sphere: The probability log p(x(1) c) is given directly by the distribution of the initial noise,a nd (1) is the solution of the system for at = 1. expx(v) = cos(v)x + sin(v) , (H) where is the Euclidean norm of v. Proof of Prop 1. Please find here the corrected proposition and its proof. We now propose short proof of Proposition 1, inspired by [44, Appendix C] Proposition 2. Given location 2 and an image c, consider solving the following ordinary differential equation system for from 0 to 1: (cid:21) (cid:20)x(t) (t) dt = (cid:20) ψ(x(t) c) div ψ(x(t) c) (cid:21) with (cid:21) (cid:20)x(0) (0) = (cid:21) (cid:20)y 0 , (I) Then the log-probability density of given is: log p(y c) = log pϵ(x(1) c) + (1) where pϵ is the distribution of the noise ϵ, and (t) accumulates the divergence of the velocity field along the trajectory. Proof. The logarithmic mass conservation theorem [4, 75] writes: dt log p(xt c) + div v(xt) = 0 . (J) After training the network ψ to regress v(xt), we can substitute ψ(xt c) to v(xt) and obtain: dt log p(x(t) c) + div ψ(x(t) c) = 0 . (K) We integrate from 0 to 1: log p(x1 c) log p(x(0) c) = (cid:90) 1 0 We thus have the following system: (cid:20) (cid:21) (cid:20)x(t) (t) dt = with initial condition: ψ(x(t) c) div ψ(x(t) c) (cid:21) Extending Prop 1. Prop 1 can be extended to Riemenian Flow Matching simply by projecting the iterate onto the sphere at each step when iteratively solving the ODE Eq. (M). For diffusion models, we do not have direct access to the velocity field. However, according to Song et al. [71, Section D.2], for stochastic differential equation of the form: dx = (x, t)dt + G(x, t)dω (P) where dω is Wiener process [21], the velocity field Ψ(x, t) can be expressed as: v(x, t) = (x, t) 1 [G(x, t)G(x, t)T ] 1 2 G(x, t)G(x, t)T log pt(xt x0, c) (Q) In our case, we defined our forward noising process as: xt = (cid:112)1 κ(t)x0 + (cid:112)κ(t)ϵ, ϵ (0, I) . (R) This leads us to choose: 1 2 (x, t) = xβ(t) G(x, t) = (cid:112)β(t) , (S) (T) where β(t) represents the infinitesimal change in xt variation between and + δt: β(t) = xt+δt xt. According to [71, Eq 29], this process yields: xt (cid:16) x0e 1 (cid:82) 0 β(s)ds, (cid:16) 1 (cid:82) 0 β(s)ds(cid:17) (cid:17) (U) div ψ(x(t) c) . which implies that [70, X]: (L) (M) β(t) = log(κ(t)) dt (V) Finally, we can replace log pt(xt x0, c) with ϵθ(xt, t, c) in Eq. (Q), as our model learns to predict the noise added to the data. This yields the following velocity field: ψ(x, t) = β(t)(x ϵθ(x, t, c)) . (W)"
        }
    ],
    "affiliations": [
        "LIGM, Ecole des Ponts, IP Paris, CNRS, UGE",
        "LIX, Ecole Polytechnique, IP Paris"
    ]
}