{
    "paper_title": "CAD-Editor: A Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing",
    "authors": [
        "Yu Yuan",
        "Shizhao Sun",
        "Qi Liu",
        "Jiang Bian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Computer Aided Design (CAD) is indispensable across various industries. \\emph{Text-based CAD editing}, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce \\emph{CAD-Editor}, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively."
        },
        {
            "title": "Start",
            "content": "CAD-Editor: Locate-then-Infill Framework with Automated Training Data Synthesis for Text-Based CAD Editing Yu Yuan 1 Shizhao Sun 2 Qi Liu 1 Jiang Bian"
        },
        {
            "title": "Abstract",
            "content": "Computer Aided Design (CAD) is indispensable across various industries. Text-based CAD editing, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce CAD-Editor, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose locatethen-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively. 5 2 0 2 6 ] . [ 1 7 9 9 3 0 . 2 0 5 2 : r 1. Introduction In the modern digital era, Computer-Aided Design (CAD) has become indispensable across industries. Most modern CAD tools follow the Sketch-and-Extrude (SE) Operations paradigm (Shahin, 2008; Camba et al., 2016), where Work done during the internship at Microsoft Research Asia. The paper is the result of an open-source research project starting 1University of Science and Technology of from March 2024. China 2Microsoft Research Asia. Correspondence to: Shizhao Sun <shizsu@microsoft.com>. Preprint. Under review. 1 designers sketch 2D curves to define the outer and inner boundaries of profiles, extrude them into 3D shapes, and combine these shapes to create complex models. CAD model creation is an iterative process, where an initial draft undergoes multiple modifications until it aligns with user requirements. Natural language plays crucial role throughout this process, serving as key medium of communication. For non-experts, it offers the most intuitive way to express their needs, while for professionals, it enables fast, detailed, and precise instructions. Consequently, system capable of automatically editing CAD models based on textual instructions known as text-based CAD editing (Figure 1) has the potential to revolutionize the entire CAD design workflow. Such system could significantly accelerate CAD model development and empower broader range of individuals, especially those with limited design expertise, to create CAD models more effectively. While important, text-based CAD editing receives limited attention. Some studies explore design variation generation, where new CAD models are generated by randomly altering components of an existing model (Wu et al., 2021; Xu et al., 2022; 2023; Zhang et al., 2024b). However, these approaches lack support for text-based control over the appearance of the generated CAD models, limiting their practical usability. Another line of research makes initial attempts at text-based CAD generation, focusing on generating new CAD models directly from textual descriptions (Khan et al., 2024b). Nonetheless, these methods do not incorporate an existing CAD model as input, which prevents them from leveraging the original designs context and constraints. Text-based CAD editing presents several distinct challenges. First, training for this task requires triplet data with accurate correspondence among an original CAD model, an editing instruction and an edited CAD model. However, such data does not naturally exist, and manually collection is both costly and difficult to scale. Second, text-based CAD editing is inherently composite problem. It demands comprehensive understanding of the textual description, the ability to locate the corresponding parts within the intricate structure of the CADs SE operations, and the capability to generate concrete modifications to these SE operations. CAD-Editor Figure 1. Text-based CAD editing achieved by CAD-Editor. Each sub-figure shows the editing instruction at the top, the original CAD model on the left, and the edited CAD model on the right. The rendered image is shown for better comprehension. The actual editing occurs on sketch-and-extrusion (SE) operations of CAD model to provide editability and reusability. In this work, we introduce CAD-Editor, the first framework for text-based CAD editing. We frame the task as sequence-to-sequence (seq2seq) generation problem, where the input combines an editing instruction and the sequence representation of the original CAD model, and the output is the sequence of the edited CAD model (Figure 2). To address the need for triplet data with accurate correspondence, we propose an automated data synthesis pipeline that leverages existing design variation models and Large Vision-Language Models (LVLMs) (Figure 3). Starting from an existing CAD model, design variation models generate edited CAD models by randomly altering parts while keeping others unchanged, producing pairs of original and edited CAD models. LVLMs then summarize the differences between these CAD models into editing instructions, resulting in triplets with strong correspondence. To tackle the composite nature of text-based CAD editing, we decompose the task into specialized sub-tasks: locating and infilling, each addressing specific aspect of the editing process (Figure 4). For both sub-tasks, Large Language Models (LLMs) serve as the backbone, leveraging their natural language understanding and CAD design knowledge (Makatura et al., 2023). In the locating stage, LLMs identify regions requiring modification by generating masked CAD sequence, where special tokens <mask> indicate the regions to be modified. In the infilling stage, LLMs generate appropriate edits for these masked regions, using the masked CAD sequence from the locating stage as context. The contributions of this work are summarized as follows: We introduce new task, text-based CAD editing, enabling precise edits through textual instructions to better align with real-world user needs. We propose an automated data synthesis pipeline that combines design variation models and LVLMs to generate triplet data with accurate correspondence, addressing critical challenge in training. We develop locate-then-infill framework that decomposes the task into focused sub-tasks, effectively handling the composite nature of text-based CAD editing. We conduct extensive experiments, demonstrating that our approach outperforms baselines in generation validity, text-CAD alignment, and overall quality. 2. Related Works CAD Generation. Parametric CAD, defined by its sketchand-extrude operations, is central to mechanical design due to its ability to retain modeling history, which facilitates both editing and manufacturing. Recent large-scale CAD datasets (Wu et al., 2021) have fueled the development of generative models. Wu et al. (2021) explores unconditional generation, where random latent vector is used as input to generate CAD models. Xu et al. (2022; 2023); Zhang et al. (2024b) focus on design variation generation, which randomly modifies parts of an existing CAD model. Khan et al. (2024a) studies text-based CAD generation, which transforms textual descriptions into CAD models. Our work differs in two key aspects. First, we target distinct task, unlike prior work, which either lacks text-based control (Wu et al., 2021; Xu et al., 2022; 2023; Zhang et al., 2024b) or disregards existing CAD models as constraints (Khan et al., 2024a). Second, we introduce novel locate-theninfill framework based on LLMs to handle the composite nature of text-based CAD editing. Previous approaches either rely on VAE-based framework (Wu et al., 2021; Xu et al., 2022; 2023) or use LLMs without accounting for task-specific needs (Zhang et al., 2024b; Khan et al., 2024a). Large Language Models (LLMs). Recently, LLMs like GPT (Achiam et al., 2023; OpenAI, 2023) and LLaMA (Touvron et al., 2023) have distinguished themselves from smaller models through remarkable emergent capabilities. Beyond excelling in natural language processing, LLMs have transforms generative tasks in other domains, e.g., motion generation (Zhang et al., 2024a) and material generation (Gruver et al., 2024). These advancements inspire us to adopt LLMs as the backbone for sub-tasks in our locate-then-infill framework. Moreover, LLMs and Large Vision-Language Models (LVLMs) are increasingly utilized for data synthesis to enhance training (Xu et al., 2024; Yu et al., 2024). Specifically, Khan et al. (2024b) leverage LLMs/LVLMs to synthesize data for text-based CAD generation. However, our task of text-based CAD editing presents 2 CAD-Editor Figure 2. Left: Example input and output for CAD-Editor. The input combines the original CAD sequence with the editing instruction, and the output is the edited CAD sequence. The specific CAD sequence is shortened to [Original (or Edited) CAD Sequence] to save space. Right: An illustration for specific CAD sequence and its rendered CAD model. distinct challenges. First, unlike Khan et al. (2024a), who generate two-tuple data (a text prompt and CAD model), our task involves creating triplet data: an editing instruction, an original CAD model, and an edited CAD model. We address this by combining design variation models with LLMs/LVLMs. Second, while Khan et al. (2024a) focus on captioning single CAD models, our task requires summarizing differences between two CAD models. We handle this by introducing stepwise captioning strategy. Text-based Editing in Other Domains. Text-based editing has been widely explored across various domains, e.g., 3D editing (Mikaeili et al., 2023), image editing (Meng et al., 2021; Brooks et al., 2023), and video editing (Chai et al., 2023; Ceylan et al., 2023). It enables users to specify and modify particular objects or attributes with precision and flexibility. Inspired by these advancements, we introduce text-based editing in the CAD domain for the first time. 3. Approach Overview Let denotes the editing instruction, Corig the original CAD model and Cedit the edited CAD model. Here, the CAD model is represented using Sketch-and-Extrude (SE) operations, as this representation preserves the modeling history, making it easier to edit. The goal of text-based CAD editing is to learn function that takes and Corig as inputs and generates Cedit as output, i.e., Cedit = (I, Corig). We formulate text-based CAD editing as sequence-tosequence (seq2seq) generation problem. To achieve this, both the editing instruction and the CAD models are represented as sequences of textual tokens. The editing instruction naturally consists of textual tokens. For the CAD models Corig and Cedit, we adopt the sequence format introduced by Zhang et al. (2024b), which abstracts all primitives in SE operations, including numerical and categorical parameters, into textual tokens (Figure 2). To address the need for training data with good correspondence among I, Corig, and Cedit, denoted as = {(I, Corig, Cedit)}N 1 (where is the data size), we introduce an automated data synthesis pipeline (Section 4). First, we obtain Cedit from Corig by leveraging existing design variation models (Xu et al., 2023) to randomly modify certain parts of the original CAD model while keeping others unchanged. Next, we generate the corresponding by utilizing LVLMs to summarize the differences between the paired CAD models. Finally, we assemble these components into triplets. Based on seq2seq formulation and triplet-correspondence dataset, we propose locate-then-infill framework to tackle the composite nature of text-based CAD editing (Section 5). Specifically, we decompose the problem into two focused sub-tasks: locating and infilling, each handling more manageable aspect of the overall problem. In the locating stage, we predict masked CAD sequence Cmask by inserting special <mask> tokens into Corig, indicating the regions that require modification, i.e., Cmask = flocate(I, Corig). In the infilling stage, We generate Cedit by filling in precise modifications within the masked regions, i.e., Cedit = finfill(I, Corig, Cmask). In both stages, LLMs serves as the backbone, leveraging their natural lauguage understanding and CAD knowledge acquired during the pre-training. 4. Automated Data Synthesis Pipeline Figure 3 illustrates our data synthesis pipeline, comprising three key steps introduced below. Paired CAD Models Generation. In this step, we create paired CAD models, Corig and Cedit, by starting with an existing CAD model and applying design variation models to create its variations. We source CAD models from the DeepCAD dataset (Wu et al., 2021) and use Hnc-CAD (Xu et al., 2023) as our design variation model1. Given an existing CAD model C0, we use Hnc-CADs auto-completion to generate variants C1, . . . , CK. We then create paired CAD models by: 1) treating C0 as the original and Ck (k [1, K]) as the edited CAD model; 2) reversing the roles, considering Ck as the original and C0 as the edited 1We choose Hnc-CAD for its well-developed open-source implementation. Other design variation models (Xu et al., 2022; Zhang et al., 2024b) are also applicable. CAD-Editor Figure 3. Illustration of automated data synthesis pipeline. CAD model; and 3) using two generated variants, Ck1 and Ck2 (k1, k2 [1, K], k1 = k2), as the original and edited CAD models. This approach captures common CAD editing operations, including addition, deletion, and replacement. Editing Instruction Generation. In this step, we generate editing instructions, I, by summarizing the difference between Corig and Cedit using LVLMs. To ensure both diversity and accuracy, we introduce two key techniques. First, we represent CAD models in multiple modalities: the visual and sequence modalities. The intuition is that high-level structural changes (e.g., changing cylinder to cube) are more easily observed in the visual modality, while low-level numerical changes (e.g., doubling the height) are better captured in the sequence modality. Additionally, the sequence modality provides detailed representation of all operations, ensuring that no information is lost. In our implementation, we use rendered images for the visual modality and SE operations for the sequence modality. Second, we propose stepwise captioning strategy to break down the complex task of difference summarization into three sub-tasks, enhancing generation quality of LVLMs. For each representation (visual or sequence) of CAD model pair, we follow these steps: 1) describing each CAD model this involves analyzing geometric properties such as component types, quantities, sizes, and spatial relationships; 2) identifying differences using the detailed descriptions from the previous step alongside CAD models, this stage extracts the necessary modifications between CAD models; and 3) compressing instructions the final step refines the editing instructions into concise yet precise form. Assembling. Finally, we assemble CAD pairs (Corig and Cedit) from the first step and editing instruction (I) from the second step into triplets, constructing the training dataset = {(I, Corig, Cedit)}N 1 . Note that the visual modality is only used in the second stage to generate diverse editing instructions. In the final dataset, all CAD models are represented as SE operations, aligning with the focus of this work text-based CAD editing in the SE domain. 5. Locate-then-Infill Framework Figure 4 illustrates our framework. We decompose textbased CAD editing by explicitly introducing masked CAD sequence, Cmask, to indicate potential modification regions: (Cedit Corig, I) (Cmask Corig, I) (Cedit Corig, I, Cmask), (1) where () denotes the probability. Here, (Cmask Corig, I) represents locating stage, while (Cedit Corig, I, Cmask) corresponds to the infilling stage. 5.1. Locating Stage This stage aims to generate masked CAD sequence, Cmask, where regions requiring modification are marked by special tokens <mask> while unchanged parts are copied from the original CAD sequence Corig. We adopt LLMs as the backbone and autoregressively predict tokens in Cmask using Corig and the editing instruction as context: (Cmask Corig, I) = (cid:89) t= (ct mask Corig, I, c<t mask), (2) where is the sequence length. Creating Ground-Truth Masked CAD Sequence via LCS. To finetune LLMs for the locating task, we require groundtruth masked CAD sequences, denoted as Cgt-mask as supervision signals. We obtain them using the Longest Common Subsequence (LCS) algorithm. Let Corig = {c1, . . . , cT } and Cedit = {c1, . . . , cT } denote the tokenized original and edited CAD sequences respectively. The LCS algorithm computes the longest subsequence CLCS = {ci1, . . . , cik } 4 CAD-Editor Figure 4. (a)-(b): Overview of Locate-then-Infill framework. (c): Examples of input and output, where the left column shows abstracted representations using legends, the middle column displays concrete sequences and the right column presents rendered visual objects. such that CLCS Corig and CLCS Cedit, where the indices i1, i2, . . . , ik represent the positions of matching tokens in Corig. Using CLCS, we construct Cgt-mask as follows: 1) for each token ci Corig, if ci CLCS, retain ci in Cgt-mask and if ci / CLCS, replace ci with the placeholder token <mask>; 2) for tokens in Cedit that do not appear in CLCS (representing insertions), insert <mask> tokens at the corresponding positions in Cgt-mask; 3) consecutive <mask> tokens are merged into single <mask> token for simplicity. 5.2. Infilling Stage This stage focuses on generating the final edited sequence Cedit by precisely filling in the masked regions while preserving the unmodified parts. We employ LLMs as the backbone, autoregressively predicting tokens in Cedit using Cmask from the locating stage along with Corig and as inputs: (Cedit Cmask, Corig, I) (cid:89) (ct Cmask, Corig, I, c<t). = t=1 (3) Improving Performance with Selective Data. To fine-tune LLMs for the infilling task, we use the dataset synthesized through the pipeline introduced in Section 4. While we strive to ensure high-quality synthetic data, achieving absolute accuracy is impossible. To further improve performance, we introduce selective dataset curated with human annotations. Rather than having human annotators create editing instructions or CAD models from scratch, we adopt more efficient approach inviting them to select the best option from generated candidates. This significantly reduces human effort and accelerates dataset construction. Specifically, we first fine-tune LLMs on synthetic data and use them to generate multiple edited CAD sequences. These sequences are then rendered into visual objects, and human annotators select the best one. The chosen sequence, along with its corresponding original CAD model and textual instruction, is added to the selective dataset. Finally, we further fine-tune LLMs using this selective dataset. 5.3. Training and Inference Training. In the locating stage, we fine-tune LLMs using Low-Rank Adapters (LoRA) (Hu et al., 2022) with the ground-truth masked CAD sequence constructed via LCS. For the infilling stage, we first fine-tune LLMs using LoRA with the synthetic dataset introduced in Section 4. We then further refine the model by fine-tuning it with LoRA on the selective dataset introduced in Section 5.2. Inference. The locating and the infilling stage operates sequentially. The locating stage generates Cmask using Corig and as input. The infilling stage generates Cedit by using Cmask from the locating stage along with Corig and as input. 6. Experiments 6.1. Experimental Setup Datasets. We use the DeepCAD dataset (Wu et al., 2021) which contains 178k CAD models. We split it into 90% training, 5% validation, and 5% testing segments. We follow the same strategy in existing work (Xu et al., 2022; 2023) to remove duplication. Besides, we also exclude the long-tail data with more than 3 sketch-extrusion pairs. For the synthetic dataset used for training, we generate 120k examples using the method introduced in Section 4. For the test set, we randomly sample 2k examples from the original test segment, generate the initial version following Section 4, and manually examine them to ensure the correctness. To compare the performance of different methods, we generate 5 outputs for each example in the test set, yielding 10k CAD models for evaluation. Implementation Details. During data synthesis, GPT-4o is utilized for the visual modality, while LLaMA-3-70B handles the sequence modality. For training, we adopt Llama3-8b-Instruct as the backbone model, fine-tuning it over 70 epochs using PyTorchs Distributed Data Parallel (DDP) framework on 4 NVIDIA A800-80GB SMX GPUs. The initial learning rate is set to 1e-4 with maximum token length of 1024. We employ LoRA with rank of 32. During 5 CAD-Editor Figure 5. Qualitative results from CAD-Editor, GPT-4o-Basic and GPT-4o-IC . The text below shows the editing instruction. the inference, we set the temperature as 0.9 and top-p as 0.9 to generate varied results in each trial. Baselines. We compare our results with three types of baselines: 1) design variation generation models, including SkexGen (Xu et al., 2022), Hnc-CAD (Xu et al., 2023) and FlexCAD (Zhang et al., 2024b); 2) text-based CAD generation models such as Text2CAD (Khan et al., 2024a) and 3) foundation models that are not specifically designed for CAD generation but have acquired some CAD knowledge during pre-training. For the third category, we select one of the most powerful foundation models, GPT-4o, as our baseline. We use two prompting strategies: 1) GPT-4oBasic, which provides only an explanation of CAD operation sequences; and 2) GPT-4o-IC: which includes three in-context (IC) examples retrieved based on cosine similarity between editing instructions, in addition to the basic explanation. Notably, existing CAD design variation models and text-based CAD generation models do not support textbased CAD editing. We include them as baselines to show that our model can generate CAD models with comparable or superior quality while addressing more complex task. Metrics. As this work is the first to address text-based CAD editing, we propose evaluating the task from three key aspects. 1) Validity. The generated CAD sequence must be valid, meaning it can be successfully parsed and rendered into visual object. We denote this as Valid Ratio (VR). 2) Realism. The generated CAD models should be realistic and similar to ground-truth CAD models. To measure Table 1. Quantitative evaluations. SkexGen, Hnc-CAD, FlexCAD and Text2CAD do not support text-based editing, so only their generation quality is compared. JSD and D-CLIP values are scaled by 102. : the higher the better, : the lower the better. Method JSD D-CLIP VR H-Eval SkexGen Hnc-CAD FlexCAD Text2CAD GPT-4o-Basic GPT-4o-IC CAD-Editor 1. 1.77 1.72 2.39 1.10 0.70 0. - - - - - 1.08 - 0. 0.27 74.3 77.4 82.1 84.8 63. 84.5 95.6 - - - - 7.22 15.6 43.2 this, we adopt the Jensen-Shannon Divergence (JSD) from prior work (Wu et al., 2021; Xu et al., 2022; 2023). JSD quantifies the similarity between two probability distributions, evaluating how often the ground-truth point clouds occupy similar positions as the generated point clouds. 3) Edit Consistency. The modifications in the generated CAD model should align with the given editing instructions. To assess this, we adapt Directional CLIP Score (D-CLIP) from the image generation domain (Gal et al., 2022) as the metric. This metric evaluates how well the change between the image for the edited CAD model and the image for the 6 CAD-Editor Figure 6. Additional results from CAD-Editor with various editing instructions . In each sub-figure, the left image shows the original CAD model, the right image displays the edited CAD model and the text below provides the editing instruction. Figure 7. Given one CAD model and various instructions, CADEditor produces different outcomes. Figure 8. Given the same CAD model and instruction, CAD-Editor produces diverse outcomes. original CAD model aligns with the editing instruction: D-CLIP = T IT , the results indicate that CAD-Editor not only enable precise text-based CAD editing, which better alignment with user instructions, but also achieve higher validity and better quality of the generated CAD designs. (4) = ET (tedit) ET (torig) , = EI (iedit) EI (iorig) . EI and ET are CLIPs image and text encoders, torig is neutral text (e.g.,This is 3D shape.), tedit is the concatenation of torig and the textual instruction. iedit and iorig are the images for the edited and original CAD model. 6.2. Main Results Quantitative Evaluation. Table 1 reports the average scores across 3 runs. Notably, CAD-Editor achieves high Valid Ratio of 95.6%, significantly surpassing other methods and indicating greater proportion of valid and high-quality CAD generations. In terms of D-CLIP, which measures alignment with editing instructions, CAD-Editor scores 0.27, substantial improvement over GPT-4o-Basic and GPT-4o-IC. This underscores CAD-Editors effectiveness in adhering to user instructions. Additionally, CAD-Editor outperforms all baselines on the point cloud evaluation metric JSD, demonstrating good generation quality. Overall, Qualitative Evaluation. In Figure 5, we qualitatively compare our method with GPT-4o-Basic and GPT-4o-IC. We observe that GPT-4o-Basic often generates irrelevant edits (case 6), unrealistic shapes (case 3), or fails to make any changes (cases 1 ,4 and 7). Additionally, it struggles with distinguishing shape types (case 2) and locating specific positions (case 5). It performs better with dynamic few-shot prompting (i.e., GPT-4o-IC), highlighting the high quality of our synthetic data. GPT-4o-IC can detect specific shapes reasonably well but still struggles with precise localization (case 2 and 4) and object count (case 8). In contrast, our model successfully executes many challenging edits, including modifying sizes, shapes, and positions, as well as replacing, adding, and removing objects. Besides, we present more results from CAD-Editor. Figure 6 shows how CAD-Editor handles variety of editing instructions. Figure 7 illustrates that, given the same original CAD model, CAD-Editor applies different modifications based on the provided editing instructions. Figure 8 shows 7 CAD-Editor Figure 9. Apply CAD-Editor iteratively to edit CAD model until it meets user requirements. that when the editing instruction is vague, CAD-Editor generates multiple CAD models, all aligning with the users intent. Figure 9 highlights the iterative editing capability of CAD-Editor, allowing users to refine CAD model through successive instructions until satisfactory result is achieved. Human Evaluation. We randomly sampled 2,000 CAD models from the full set of generated results. Each pair of original CAD model and edited CAD model was independently rated by five crowd workers. For each pair, score of 1 is assigned if the generated data is deemed successful, and 0 otherwise. Success is defined by two criteria: alignment with the text and sufficiently high visual quality. The results, denoted as H-Eval, are presented in Table 1. CAD-Editor outperforms baselines, indicating that crowd workers frequently found CAD models generated by baselines to be misaligned with the instructions or of lower quality, whereas our method demonstrated superior performance. 6.3. Ablation Studies Stepwise Captioning Strategy. As introduced in Section 4, we propose stepwise captioning strategy to decompose the complex task of generating editing instructions. To evaluate its effectiveness, we conduct an experiment where LVLMs are directly queried to generate editing instructions, denoted as CAD-Editor w/ Basic. Our approach, which incorporates stepwise captioning, is referred to as CAD-Editor w/ Step. Due to resource constraints, we compare these methods on subset of 10k examples. Table 2 shows results. CADEditor w/ Step outperforms CAD-Editor w/ Basic across all metrics, highlighting the importance of stepwise captioning strategy in ensuring accurate editing instruction generation. Locate-then-Infill Framework. Our approach consists of two stages: the locating and infilling stages, each focused on specialized sub-task of text-based CAD editing. more straightforward approach would be to treat the task as whole, without decomposing it, and directly fine-tune Table 2. Ablation studies. The CAD-Editor-mini is trained on small set with 10k examples. Method JSD D-CLIP VR CAD-Editor-mini w/ Basic CAD-Editor-mini w/ Step CAD-Editor w/ Direct CAD-Editor w/ L-I CAD-Editor w/ L-I & HS 0.78 0.70 0.77 0.67 0. - 0.12 - 0.07 - 0.16 0.21 0.27 88. 90.1 86.1 96.5 95.6 LLMs. We refer to this as CAD-Editor w/ Direct and compare it with our method, CAD-Editor w/L-I. As shown in Table 2, CAD-Editor w/L-I outperforms CAD-Editor w/ Direct, showing the effectiveness of our two-stage approach in addressing the composite nature of text-based CAD editing. Selective Dataset. In Section 5.2, we propose improving the performance of the infilling stage with selective data curated with human annotation. We conduct experiments both with or without such human selection, denoted as CAD-Editor w/ L-I & HS and CAD-Editor w/ L-I. Table 2 shows that this strategy results in the greatest improvement in D-CLIP, highlighting its effectiveness in aligning editing instructions with the edited CAD model. 7. Limitation While CAD-Editor shows promising results, it has limitations. First, its data synthesis pipeline relies on LVLMs, which are costly and struggle with processing multiple images simultaneously. Second, as an LLM-based system, it faces challenges with long contexts and generating extended sequences, limiting its ability to handle highly complex CAD models and their corresponding edits. 8 CAD-Editor 8. Conclusion We introduced CAD-Editor, the first framework for textbased CAD editing. We proposed data synthesis pipeline to address the need of triplet data with accurate correspondence, and locate-then-infill framework to handle the composite nature of the task. Experiments showed that CADEditor outperforms other methods. In the future, we aim to enhance our data synthesis pipeline to make it more costeffective and efficient. We also plan to develop an advanced benchmark that better reflects practical user scenarios."
        },
        {
            "title": "Impact Statement",
            "content": "This work presents novel task: text-based CAD editing, along with an automated data annotation pipeline and locate-then-infill framework to effectively address it. By streamlining the CAD model development process, our approach has the potential to significantly accelerate design workflows and make CAD modeling more accessible to broader audience, especially those with limited design expertise. Rather than replacing human designers, our goal is to augment creativity and productivity, empowering individuals to bring their ideas to life more efficiently and effectively."
        },
        {
            "title": "Ethics Statement",
            "content": "The data used in this work is tailored for creating and modifying CAD models. Due to its specialized nature, the misuse risk is naturally minimized, ensuring that the developed methods primarily benefit design and engineering tasks."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Camba, J. D., Contero, M., and Company, P. Parametric cad modeling: An analysis of strategies for design reusability. Computer-Aided Design, 74:1831, 2016. Ceylan, D., Huang, C.-H. P., and Mitra, N. J. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2320623217, 2023. Chai, W., Guo, X., Wang, G., and Lu, Y. Stablevideo: Text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2304023050, 2023. Gal, R., Patashnik, O., Maron, H., Bermano, A. H., Chechik, G., and Cohen-Or, D. Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):113, 2022. Gruver, N., Sriram, A., Madotto, A., Wilson, A. G., Zitnick, C. L., and Ulissi, Z. W. Fine-tuned language models generate stable inorganic materials as text. In The Twelfth International Conference on Learning Representations, 2024. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Khan, M. S., Dupont, E., Ali, S. A., Cherenkova, K., Kacem, A., and Aouada, D. Cad-signet: Cad language inference from point clouds using layer-wise sketch instance guided attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4713 4722, 2024a. Khan, M. S., Sinha, S., Uddin, S. T., Stricker, D., Ali, S. A., and Afzal, M. Z. Text2cad: Generating sequential cad designs from beginner-to-expert level text prompts. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. Makatura, L., Foshey, M., Wang, B., HahnLein, F., Ma, P., Deng, B., Tjandrasuwita, M., Spielberg, A., Owens, C. E., Chen, P. Y., et al. How can large language models help humans in design and manufacturing? arXiv preprint arXiv:2307.14377, 2023. Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Mikaeili, A., Perel, O., Safaee, M., Cohen-Or, D., and Mahdavi-Amiri, A. Sked: Sketch-guided text-based 3d editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1460714619, 2023. OpenAI. Introducing chatgpt. OpenAI Blog, 2023. Available: https://openai.com/blog/chatgpt. Shahin, T. M. Feature-based designan overview. ComputerAided Design and Applications, 5(5):639653, 2008. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. CAD-Editor Wu, R., Xiao, C., and Zheng, C. Deepcad: deep generative network for computer-aided design models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 67726782, 2021. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., and Jiang, D. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024. Xu, X., Willis, K. D., Lambourne, J. G., Cheng, C.-Y., Jayaraman, P. K., and Furukawa, Y. Skexgen: Autoregressive generation of cad construction sequences with disentangled codebooks. In International Conference on Machine Learning, pp. 2469824724. PMLR, 2022. Xu, X., Jayaraman, P. K., Lambourne, J. G., Willis, K. D., and Furukawa, Y. Hierarchical neural coding for controllable cad model generation. In International Conference on Machine Learning, pp. 3844338461. PMLR, 2023. Yu, L., Jiang, W., Shi, H., Jincheng, Y., Liu, Z., Zhang, Y., Kwok, J., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. Zhang, Y., Huang, D., Liu, B., Tang, S., Lu, Y., Chen, L., Bai, L., Chu, Q., Yu, N., and Ouyang, W. Motiongpt: Finetuned llms are general-purpose motion generators. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 73687376, 2024a. Zhang, Z., Sun, S., Wang, W., Cai, D., and Bian, J. Flexcad: Unified and versatile controllable cad generation with fine-tuned large language models. arXiv preprint arXiv:2411.05823, 2024b. 10 A. Additional Results CAD-Editor We present qualitative comparison between the directly fine-tuned LLM and our Locate-then-Infill framework in Figure 10. Compared to the directly fine-tuned approach, our framework enhances generation quality, improves text-CAD alignment and ensures greater output stability. Figure 10. The qualitative comparison between the directly fine-tuned LLM and our Locate-then-Infill framework. Further qualitative comparisons between CAD-Editor and baseline methods are presented in Figure 11, highlighting the superior performance of CAD-Editor across diverse editing conditions. Figure 11. Additional qualitative comparison results between CAD-Editor and baseline methods. B. Stepwise Captioning Strategy For stepwise captioning, we employ GPT-4o four times per CAD pair to generate the final editing instruction at the image level, while LLaMA-3-70B is used for the sequence level. The detailed prompt is shown in Figure 12. CAD-Editor Figure 12. The detailed prompt for stepwise captioning. Moreover, we provide comparison between basic captioning and our stepwise captioning results in Figure 13. Figure 13. Comparisons between basic captioning method and our stepwise captioning method. C. Locate-then-Infill Framework We use task-specific prompts for locating task and infilling task, with details shown in Table C. D. Baseline Prompting Here, we provide the detailed prompt for the baseline method in Figure 14. 12 CAD-Editor"
        },
        {
            "title": "Infilling Prompt",
            "content": "Below is Computer-Aided Design (CAD) operation sequence. Replace the parts that need to be modified with the string <mask> according to the editing instruction. Original CAD Operation Sequence: [Original CAD sequence] Editing Instruction: [Textual editing instruction] Masked CAD Operation Sequence: [Masked CAD sequence] Below is the original Computer-Aided Design (CAD) operation sequence. Original CAD Operation Sequence: [Original CAD sequence] The parts that need to be modified according to the editing instruction have been replaced by the string <mask>. Editing Instruction: [Textual editing instruction] Masked CAD Operation Sequence: [CAD sequence with <mask> ] Generate the edited CAD sequence that could replace <mask> in the CAD model: Table 3. Prompt for Locate-then-Infill framework. Figure 14. Prompt for GPT-4o-IC."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "University of Science and Technology of China"
    ]
}