{
    "paper_title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
    "authors": [
        "Senyu Fei",
        "Siyin Wang",
        "Junhao Shi",
        "Zihao Dai",
        "Jikun Cai",
        "Pengfang Qian",
        "Li Ji",
        "Xinzhe He",
        "Shiduo Zhang",
        "Zhaoye Fei",
        "Jinlan Fu",
        "Jingjing Gong",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 2 6 3 1 . 0 1 5 2 : r LIBERO-Plus: Action Models In-depth Robustness Analysis of Vision-LanguageSenyu Fei2,3, Siyin Wang1,3,, Junhao Shi1,3, Zihao Dai1, Jikun Cai1, Pengfang Qian1,3, Li Ji1 Xinzhe He1 Shiduo Zhang1 Zhaoye Fei1 Jinlan Fu4 Jingjing Gong3,(cid:66) Xipeng Qiu1,3,(cid:66) 1Fudan University 2Tongji University 3Shanghai Innovation Institute 4National University of Singapore https://sylvestf.github.io/LIBERO-plus/ https://github.com/sylvestf/LIBERO-plus https://huggingface.co/datasets/Sylvest/LIBERO-plus"
        },
        {
            "title": "Abstract",
            "content": "VisualLanguageAction (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in VisualLanguageAction (VLA) models have led to impressive performance on standardized benchmarks, with many systems achieving near-perfect success rates on tasks in controlled simulation environments (Kim et al., 2024; 2025; Li et al., 2025; Black et al.; Pertsch et al., 2025; Hung et al., 2025; Cen et al., 2025; Tan et al., 2025). However, these headline numbers often conceal critical deficiencies in the underlying models. In fact, closer inspection reveals that contemporary VLA systems tend to exhibit fragile robustness, struggling to maintain performance when faced with even minor variations in environmental conditions or task parameters. The prevailing evaluation methodologies (Liu et al., 2023; Li et al., 2024c) focus on aggregate success rates under static, ideal conditions. While such metrics provide valuable baselines for comparing different approaches, they fail to capture the stability and reliability of learned policies under realistic variations. This approach tends to obscure the models inability to handle subtle variations that are intrinsic to any realistic task setting (Wang et al., 2025; Müller, 2019; Zhang et al., 2024)even if those tasks remain within the realm of simulation. For example, models trained to excel under fixed camera angles or consistent illumination often fail to generalize when confronted with slight shifts in viewpoint or minor changes in the robots initial configuration. This gap is especially problematic for VLA models, which must integrate information across multiple modalities and maintain coherent behavior despite perturbations in any of these input channels. Joint First Authors, Joint Second Authors, Project Lead. (cid:66)Corresponding Authors. 1 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models To uncover these hidden vulnerabilities, we conduct comprehensive analysis of contemporary VLA models using the LIBERO (Liu et al., 2023) benchmark as diagnostic tool. By systematically varying key factors such as camera viewpoints, robot initial states, language instructions, light conditions, background textures, sensor noise, and object layout, we expose the brittle nature of these models. Our analysis shows that even nominal modifications can lead to steep drops in performance. This indicates that, rather than achieving true multimodal understanding, current VLA architectures rely on overfitting to specific, narrowly defined cues provided during training. Our study highlights several core weaknesses in contemporary VLA models: Vulnerability to Visual Shifts: an over-reliance on fixed visual features leads to failure under variations in camera angle or illumination; Inadequate Kinematic Reasoning: limited generalization across different initial robot configurations reflects lack of deep kinematic understanding; Superficial Language Interaction: linguistic inputs are often underutilized or even completely ignored, as shown by the minimal impact of instruction variation. Through this work, we provide: 1. detailed vulnerability analysis of current VLA models through systematic parameter variation. 2. diagnostic framework for identifying and quantifying the impact of perturbations on model performance. 3. Critical insights into the mismatch between apparent multimodal competence and actual robust understanding. Our findings challenge the assumption that high benchmark scores equate to true competency, urging the community to re-evaluate current evaluation practices and focus on building models that are robust in the face of inherent variability. This work is step toward developing VLA systems that are not only high-performing but also genuinely reliable and adaptable."
        },
        {
            "title": "2 How Do Single-Dimension Perturbations Affect VLA Models?",
            "content": "2.1 Perturbation Factors We systematically evaluate how different perturbation factors affect VLA performance and study seven common single-dimension perturbations applied to the evaluation episodes: (1) Objects Layout: add confounding objects and/or shift the target objects position. (2) Camera Viewpoints: change the viewpoint/pose and field-of-view of the third-person camera. (3) Robot Initial States: change the manipulators initial pose. (4) Language Instructions: rewrite task instructions to increase linguistic richness and complexity. (5) Light Conditions: vary illumination intensity, direction, color, and shadow patterns. (6) Background Textures: modify table/scene textures and materials. (7) Sensor Noise: inject photometric distortions (e.g., jitter, Gaussian blur) into input images. Full per-factor specifications are provided in Appendix A. 2.2 Models We analyze series of representative open-checkpoint models spanning diverse architectures (autoregressive vs. diffusion-based) and training paradigms (web-data co-training, world modeling, reinforcement learning, etc): (1) OpenVLA (Kim et al., 2024) and its variants (2) OpenVLA-OFT (Kim et al., 2025), (3) OpenVLA-OFT_w (third-view-only version), (4) OpenVLA-OFT_m (mix-sft version, trained on all 4 suites), (5) π0 (Black et al.), (6) π0-fast (Pertsch et al., 2025), (7) Nora (Hung et al., 2025), (8) WorldVLA (Cen et al., 2025), (9) UniVLA (Bu et al., 2025) and (10) RIPT-VLA (Brohan et al., 2022). Please refer to Appendix for further details. 2.3 Results We present the main experimental results in Table 1 and Figure 1, which collectively reveal significant fragility in the generalization capabilities of current VLAs. As shown, even minor perturbations can lead to drastic performance degradation. Below we analyze the specific robustness patterns across perturbation dimensions, models, and tasks. 2 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Table 1: Model performance under different perturbations. For each model, the first row reports the task success rate (%) under each perturbation dimension, with the \"Original\" column indicating the performance on unperturbed inputs. The second row (denoted by ) shows the corresponding absolute performance drop. The results highlight significant variations in robustness across models and perturbation types. Original Camera Robot Language Light Background Noise Layout OpenVLA OpenVLA-OFT 76.5 97.1 OpenVLA-OFT_w 95.3 OpenVLA-OFT_m 97.6 π0 π0-fast Nora WorldVLA UniVLA RIPT-VLA 94.2 85.5 87. 79.1 95.2 97.5 1.1 75.4 59.7 37.4 16.8 78. 57.9 39.7 15.8 78.4 66.4 19.1 4.0 83.9 0.3 78.8 4.3 90. 58.3 39.2 4.1 72.4 37.2 59.9 43.7 51.6 30.6 67.0 6.6 87. 24.8 60.7 41.1 46.8 30.2 48.9 50.3 44.9 36.7 60.8 26.8 49. 81.5 15.6 73.2 22.1 83.6 14.0 61.0 33.2 63.3 22.2 67.0 20. 44.2 34.9 71.8 23.4 80.1 17.4 4.4 72.1 85.8 11.3 68.2 27. 91.6 6.0 79.6 14.6 73.0 12.5 31.0 56.9 29.4 49.7 59.1 36. 87.9 9.6 25.3 51.2 92.4 4.7 92.5 2.8 83.6 14.0 78.5 15. 67.7 17.8 50.5 37.4 14.5 64.6 80.0 15.2 90.4 7.1 19.3 57. 76.7 20.4 51.4 43.9 76.3 21.3 79.4 14.8 75.8 9.7 17.6 70. 12.2 66.9 25.3 69.9 73.8 23.7 31.6 44.9 77.1 20.0 72.3 23. 73.2 24.4 70.4 23.8 70.3 15.2 63.9 24.0 39.4 39.7 34.3 60. 76.5 21.0 Finding 1: Significant Overall Fragility to Perturbations Across all perturbation factors, current VLAs exhibit brittle generalization. Performance degrades significantly under various input perturbations, particularly with changes in camera viewpoint and robot initial state. Finding 2: Robustness varies considerably by perturbation type. Models are most vulnerable to changes in camera viewpoint and robot initial state, which require high-level understanding of spatial geometry and proprioception. In contrast, they show relative resilience to lighting and background variations, which constitute more superficial, low-level visual changes. Finding 3: Minor Impact of Language Perturbation. Contrary to expectations, language perturbations result in the second smallest average performance drop (-25.3) across most models. This apparent robustness is counter-intuitive and merits deeper investigation. As we explore in Section 4, this phenomenon is unlikely to stem from superior linguistic generalization. more plausible hypothesis, which we have proven empirically, is that models may be relying less on the language instruction than anticipated, potentially leveraging task cues from the visual context. Finding 4: Model robustness is dictated by architecture and training paradigm. Specifically, models incorporating first-person wrist camera (e.g., OpenVLA-OFT) demonstrate superior generalization, especially to camera viewpoint changes, compared to those reliant solely on third-person view (e.g., OpenVLA-OFT_w). Furthermore, training strategies that emphasize diversity and co-training (e.g., π0,π0-fast ) consistently yield more robust models across multiple perturbation types, highlighting the importance of exposure to varied data distributions."
        },
        {
            "title": "3 Do contemporary VLA Models truly pay attention to visual inputs?",
            "content": "While the overall trends reveal substantial fragility, we observe two particularly interesting patterns in the data: (1) models exhibit surprising resilience to background changes, and (2) several models show limited 3 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models sensitivity to light variations. These observations raise important questions about what representations the models are actually learning. Do they genuinely understand task-relevant object semantics, or are they relying on superficial visual cues? To answer these questions, we conduct finer-grained analyses of object layout and illumination robustness. Do Models Genuinely Attend to Task-Relevant Objects? We are pleasantly surprised to observe that the models are relatively insensitive to changes in the Background setting. To further investigate whether the models truly focus on the core interactive objects and genuinely understand the high-level semantics and spatial information relevant to the task, we decomposed the Object Layout perturbation into two subcategories: (1) adding confounding objects, and (2) changing the placement and pose of the target objects. We then evaluated all models under these conditions, and the results are shown in Figure 1. It can be seen that for π0, π0-Fast, RIPT-VLA, UniVLA, and WorldVLA, the success rate decreases only marginally when confounding objects are added, indicating that these models, through training, indeed manage to focus their attention on the target objects. However, when the target objects placement is altered, the performance of the models drops significantly, suggesting that the current models may have merely learned the positional information of the target objects rather than truly capturing the high-level task-relevant semantics. Figure 1: Robustness to object layout perturbations. Comparison of different models under confounding and displacement perturbations, as well as their overall robustness. How Do Models Maintain Performance Under Illumination Changes? We observe that for several models, the performance drop under light perturbations is limited to around 10 points, suggesting surprising insensitivity to illumination changes. To investigate this phenomenon, we design an extreme ablation test: (i) all-black, where all camera inputs are replaced with black frames, and (ii) 3rd-black, where only the third-person view is masked while the wrist camera is preserved. In the all-black condition, performance collapses to nearly zero across models, confirming strong reliance on visual input. In contrast, under the 3rd-black setting, the same models still achieve accuracies of 43.6, 43.0, and 67.3, respectively, demonstrating that the wrist view alone provides critical and stable close-range geometric and contact cues. This explains why standard light perturbations cause only minor degradation: illumination changes primarily affect the third-person view and global appearance, whereas the wrist view remains relatively stable. Consistently, models such as OpenVLA, Nora, and WorldVLAwhich depend exclusively on third-person observationssuffer severe drops under light perturbations (often exceeding 60 points). Figure 2: Illumination robustness and extreme ablation tests. The term Light denotes the condition with light perturbation applied. 3rd Black and All Black represent conditions where only the third-view image is masked and where images from both views are masked, respectively. Based on our deeper investigation into object layout and illumination robustness, we can conclude: Finding 5: Current VLAs exhibit positional bias rather than genuine semantic understanding of objects. While models demonstrate an ability to ignore distracting objects, they fail to generalize when target objects are displaced, indicating that they rely on memorized positional cues rather than learning invariant object semantics. 4 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 3: Accuracy of different models on instruction removed (a) and target modified (b) tasks. Light bars: original success rate with language instruction; (a) dark bars: success rate after removing the instruction; (b) Dark bars: success rate under altered task goal and instruction (task substitution). Finding 6: Wrist cameras provide critical robustness to illumination changes. The relative stability of performance under light perturbations is largely attributable to the wrist cameras close-range perspective, which provides illumination-invariant geometric cues. Models lacking wrist-camera inputs show significantly greater vulnerability to lighting variations."
        },
        {
            "title": "4 Do Contemporary VLA Models Truly Follow Language Instructions?",
            "content": "In the experiments presented in Section 2, we observed an intriguing phenomenon: when introducing language perturbations, the overall performance of the OpenVLA-OFT model was barely affected and remained close to the baseline level. To further investigate the potential underlying reasons, we propose the following three hypotheses: (1) The model may possess strong generalization capabilities in the language domain, allowing it to remain robust even when instructions are perturbed. (2) The model may extract limited keywords from the input instruction for matching and decision-making, rather than genuinely understanding the full semantic structure. However, this is unlikely because our perturbations include commonsense subclass that performs keyword commonsense rewrite, yet the performance drop remains nearly negligible. (3) The model may not fully utilize the language modality, instead relying primarily on visual or other nonlinguistic signals to complete tasks. In such scenario, language inputs would be functionally redundant, and even significant perturbations would have minimal impact. To verify which of the above hypotheses is more plausible, we conducted additional analysis experiments. 4.1 What If We Remove Language, Does Performance Drop? We introduced blank instruction experiment. In this setting, the language input provided to the model was entirely replaced with an empty value, i.e., no linguistic information was supplied during inference. This approach directly tests whether the absence of language leads to substantial performance degradation. We conducted experiments on all four suites of LIBERO, and the results are shown in Figure 3(a). Surprisingly, even without any valid language input, the performance of OpenVLA-OFT on the object suite remained largely unchanged, with significant degradation observed only on the long suite. We attribute this to the greater reliance on instruction guidance in long-horizon tasks, which forces the model to attend to the language modality. This finding is highly revealing: although the model is nominally designed as Vision-Language-Action (VLA) framework, in practice it degenerates into form that disregards language, behaving more like Vision-Action (VA) model. 5 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models 4.2 What If We Replace Goals with OOD Objects, Do Models Fail? We further designed goal replacement task to directly examine whether models genuinely possess language instruction-following ability. Specifically, for the layout suite, where the issue appeared most pronounced, we replaced the target object in the instruction and the task goal with alternatives within the same scene. For instance, the original task instruction pick up the alphabet soup was replaced with pick up the tomato sauce, and similarly, series of new goal instructions were constructed. As shown in Figure 3(b), the experimental results revealed two key findings: Finding 7: VLA models do not possess strong cross-object instruction-following generalization. In tasks with replaced targets, the models success rate dropped nearly to zero, with the degradation particularly severe for OpenVLA-OFT. The apparent robustness observed in prior language perturbation experiments did not stem from deep modeling of language but rather from ignoring linguistic inputs altogether, leading to superficially stable performance under perturbations. Finding 8: VLA models appear to rely more on fixed visionaction mappings than on fully exploiting language signals in task decision-making. By analyzing rollout cases, we observed that even when the target in the instruction was explicitly changed, the model still tended to execute the original target action rather than adjust its behavior according to the new instruction. More details can be found in Appendix E."
        },
        {
            "title": "5 Does There Exist Compositional Generalization Gap Across Multi-Dimensional",
            "content": "Perturbations? Generalization results under single-dimension perturbations demonstrate the models robustness against isolated factors. However, these dimensions may not be independent, and different types of perturbations are likely to exhibit complex dependencies. In this study, we refer to such performance as compositional generalization. To ensure scientific rigor, we define the problem from statistical perspective as follows. 5.1 Statistical Definition of the Compositional Generalization Gap We define the random variables Di as (cid:26)1, 0, Di = if the i-th type of perturbation is applied, otherwise, and similarly for Dj. For single trial, we define the success indicator variable (cid:26)1, 0, if the task is successfully executed, otherwise. = The success rate can be defined in terms of conditional probability as s(Di = di, Dj = dj) = P(Y = 1 Di = di, Dj = dj), di, dj {0, 1}. We further estimate the joint probability between Di and Dj conditioned on = 1, p(Di = di, Dj = dj = 1) = s(Di = di, Dj = dj) a,b{0,1} s(Di = a, Dj = b) (1) (2) (3) (4) which represents the probability that the combination Di = di and Dj = dj occurs among all successful cases. Similarly, the marginal probabilities are p(Di = 1 = 1) = p(Di = 1, Dj = 0 = 1) + p(Di = 1, Dj = 1 = 1) = s(Di = 1, Dj = 0) + s(Di = 1, Dj = 1) a,b{0,1} s(Di = a, Dj = b) , p(Dj = 1 = 1) = p(Di = 0, Dj = 1 = 1) + p(Di = 1, Dj = 1 = 1) = s(Di = 0, Dj = 1) + s(Di = 1, Dj = 1) a,b{0,1} s(Di = a, Dj = b) . (5) (6) 6 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Intuitively, p(Di = 1 = 1) reflects the probability that the i-th perturbation occurs among all successful cases. It measures the contribution of the i-th perturbation to the overall successful outcomes. high value indicates that the perturbation frequently co-occurs with successful trials, suggesting the model is robust to this perturbation, while low value indicates sensitivity to this perturbation. Similarly, p(Di = 1, Dj = 1 = 1) = s(Di = 1, Dj = 1) a,b{0,1} s(Di = a, Dj = b) (7) represents the proportion of successful cases under the \"double perturbation\" scenario. high probability suggests the model maintains performance under joint perturbations, whereas low probability indicates that the combination severely affects success. In this study, we focus on the Compositionality Gap which is also the covariance between variable Di and Dj given that = 1: ij Cov(Di, Dj = 1) = E[DiDj = 1] E[Di = 1] E[Dj = 1] = p(Di = 1, Dj = 1 = 1) p(Di = 1 = 1) p(Dj = 1 = 1). (8) The sign of outcomes. Specifically: turbations. beyond independent effects. ij correctly reflects the correlation of the contributions of the two perturbations to successful ij > 0 indicates positive correlation, meaning the model can jointly handle both perij < 0 indicates negative interaction, meaning that the combination introduces additional difficulty ij = 0 indicates no interaction, satisfying the independence assumption. 5.2 Experimental Setup and Results Analysis We perform 2000 independent repeated experiments to ensure high statistical significance. As noted in the previous section, the performance of the VLA model on LLMBased Language Rewrites is somewhat limited by the models language-following ability, and its scores may be somewhat deceptive. Therefore, when analyzing compositional generalization, we select single-dimension perturbations objects spanning, environment sampling, Illumination Variations, camera-sphere shifts, Robot Initialization perturbations, sensor noise and use the OpenVLA-OFT model for testing. In the experiments, we perform independent tests for each type of single-dimension perturbation and pairwise perturbations, recording the success rate over 2000 repeated trials, which can be found in Appendix F. The final experimental results are presented in heatmap shown in Figure 4. The values in the upper-triangular matrix Aij (1 < 6) are the product of the conditional probabilities of two single-dimension perturbations. The values in the lower-triangular matrix Aij (1 < 6) represent the actual probabilities when applying joint perturbations. Additionally, we calculate the compositional generalization gap Figure 4: Heatmap of conditional probabilities under pairwise perturbations. Upper triangular entries represent independence-based products of single-dimension probabilities, while lower triangular entries show actual joint outcomes. ij = Aij Aji (1 < 6) and verify the statistical significance of the results using chi-squared test, as shown in Appendix F. Finding 9: Generalization is intrinsically non-decomposable. The consistent negative compositionality gap reflects interaction effects among perturbations, where co-occurring shifts act as coupled noise sources in feature space and expose entanglement in the learned representations. The findings indicate that current models lack mechanisms to capture higher-order dependencies, leading to pronounced robustness degradation under complex perturbation combinations. 7 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 5: Model performance trends across perturbation difficulty levels. The line plots show the success rate of each model as the intensity of four different perturbation dimensions increases."
        },
        {
            "title": "6 LIBERO-Plus",
            "content": "6.1 Benchmark Construction Building on the analysis in Section 2, we introduce LIBERO-Plus, benchmark designed to rigorously evaluate generalization capabilities along the key dimensions identified in our study. Our construction process consists of two main steps: (1) systematically expanding and enriching the original LIBERO benchmark by applying seven distinct perturbation factors, followed by filtering and balancing task categories based on the findings from Section 2; and (2) evaluating the resulting tasks using four representative models, then stratifying them into five difficulty levels (Level-1Level-5) according to the accuracy distribution observed across these models. Figure 5 presents the corresponding accuracy of each model across the five difficulty levels under four representative perturbation factors. The resulting benchmark comprises 10,030 tasks spanning seven perturbation factors with twenty-one low-level components, as illustrated in Figure 6. Detailed generation specifications and level-wise statistics are provided in Appendix D. Figure 6: Architecture of the LIBERO-Plu benchmark, comprising 10,030 tasks organized across seven perturbation factors and twenty-one underlying components. 6.2 Does Training on Generalized Sets Improve Generalization? Leveraging our highly automated generalization pipeline, we constructed an extensive training dataset comprising over 20,000 successful trajectories. This dataset was constructed through substantial expansion of the original LIBERO benchmark, greatly increasing the number of trajectories and scene diversity, enabling systematic evaluation of how generalization-oriented training affects model performance. Further details on dataset construction are available in Appendix D. Using this dataset, we conducted mixed fine-tuning starting from the official OpenVLA-OFT weights. The corresponding results on the LIBERO-plus benchmark are presented in Table 2. As shown in Table 2, our method achieves the highest overall success rate (79.6%), outperforming all baseline models across nearly all perturbation types. Most notably, it exhibits dramatic improvement in camera view robustness (92.8%), surpassing the next best model by 37.2 percentage points. Significant gains are also observed under noise (89.3%) and layout (77.6%) perturbations. These results demonstrate that training with 8 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Table 2: Robustness evaluation across perturbation dimensions, with bold values denoting the highest scores. The bottom row (+ PT) shows the performance of our post-training method, with absolute improvements over the baseline (OpenVLA-OFT_m) indicated by upward arrows."
        },
        {
            "title": "Total",
            "content": "OpenVLA OpenVLA-OFT OpenVLA-OFT_w NORA WorldVLA UniVLA π0 π0-Fast RIPT-VLA Openvla-OFT_m"
        },
        {
            "title": "Ours",
            "content": "0.8 56.4 10.4 2.2 0.1 1.8 13.8 65.1 55.2 55.6 92.8 37.2 3.5 31.9 38.7 37.0 27.9 46.2 6.0 21.6 31.2 21.7 30.3 8.6 23.0 79.5 70.5 65.1 41.6 69.6 58.8 61.0 77.6 81.0 85.8 4. 8.1 88.7 76.8 45.7 43.7 69.0 85.0 73.2 88.4 92.7 94.9 2.2 50.4 97.3 99.2 65.5 19.8 90.7 90.7 97.7 100.0 92.3 93.9 1.6 15.2 75.8 49.9 12.8 10.9 21.2 79.0 74.4 73.5 78.6 89.3 10. 28.5 74.2 69.9 62.1 38.0 31.9 68.9 68.8 74.2 68.7 77.6 8.9 17.3 70.0 56.4 39.8 25.3 43.9 54.6 64.2 69.3 68.1 79.6 11.5 our generalized dataset substantially enhances model robustness to wide range of unseen environmental variations."
        },
        {
            "title": "7 Related Work",
            "content": "7.1 Vision-Language-Action Models Recent advancements in Vision-Language-Action (VLA) models have expanded the paradigm of foundation models from language and vision into robotics, motivating unified architectures. Autoregressive approaches (Brohan et al., 2022; Kim et al., 2024; Pertsch et al., 2025; Li et al., 2025; Wen et al., 2025a; Li et al., 2024b) discretize robot actions into tokens and train end-to-end policies on large-scale demonstrations, while diffusionbased models (Black et al.; Bjorck et al., 2025; Li et al., 2024a; Wen et al., 2025b) generate continuous trajectories via generative diffusion experts. More recently, reinforcement learning methods (Tan et al., 2025; Liu et al., 2025; Lu et al., 2025; Guo et al., 2025) move beyond supervised fine-tuning, emphasizing robustness and downstream adaptability through reinforcement learning objectives. However, while these models show zero-shot performance in familiar settings, their success often reflects interpolation rather than true generalization. As result, existing benchmarks lack comprehensive insights into model performance under distribution shifts, highlighting the need for systematic and fine-grained robustness evaluations. 7.2 Generalization Robotic Manipulation Evaluations Table 3: Comparison of different simulated generalization evaluation benchmarks for VLA models. Method Automation Simulator Fine-grained AGNOSTOS (Zhou et al., 2025) RL4VLA (Liu et al., 2025) INT-ACT (Fang et al., 2025) Gembench (Garcia et al., 2025) VLATest (Wang et al., 2025) COLOSSEUM (Pumacay et al., 2024) LIBERO-Plus (Ours) RLBench ManiSkill ManiSkill RLBench ManiSkill RLBench LIBERO Perturbation Dimensions Layout Background Light Camera Robot Language Noise Evaluating the generalization ability of robotic manipulation models has remained significant challenge. Early efforts (James et al., 2020; Mu et al., 2021; Liu et al., 2023) established reproducible environments for model testing. However, these benchmarks primarily focused on specific tasks, with limited scope for evaluating robustness across diverse conditions. Some studies (Zhou et al., 2025; Liu et al., 2025; Fang et al., 9 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models 2025; Garcia et al., 2025), relied on manually designed tasks to cover several perturbation dimensions. These studies typically used small sample sizes, often with fewer than 100 test scenarios. This limited the ability to perform large-scale, systematic evaluations of model robustness across broad range of conditions. Other benchmarks (Wang et al., 2025; Pumacay et al., 2024), addressed this issue by automating task generation. These frameworks significantly increased the number of tasks, enabling broader coverage of perturbation dimensions, including lighting and camera viewpoints. However, these automated approaches still lack fine-grained analysis within each perturbation dimension, limiting the depth of insights into model performance under varying conditions. In contrast, our LIBERO-plus offers three key advancements: (1) Comprehensive: Evaluation across seven robustness dimensions and 21 sub-dimensions, ensuring thorough assessment of model generalization. (2) Automation: Automated task generation, enabling the creation of large number of diverse training and test samples. (3) Fine-grained: Incorporation of difficulty levels (L1L5), providing detailed insights into when and how VLA models fail."
        },
        {
            "title": "8 Conclusion",
            "content": "This work systematically analyzes modern VLA models, exposing significant generalization problem in contrast to their almost saturated performance on benchmarks such as LIBERO. Our findings reveal that most of the contemporary VLA models remain brittle, showing particular vulnerability to camera and robot state changes, almost all models ignore the language instructions, and some of the models execute with bare memorization of the trajectory instead of relying on visual feedback. We also identify positional bias and negative combinatorial generalization gaps under combined perturbations. We urge the community to prioritize the true diversity of embodied tasks in evaluation and develop architectures capable of robust generalization beyond limited benchmark environments."
        },
        {
            "title": "References",
            "content": "Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550. arXiv preprint ARXIV.2410.24164. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Anand Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Ho Vuong, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. ArXiv, abs/2212.06817, 2022. URL https://api.semanticscholar.org/CorpusID:254591260. Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. arXiv preprint arXiv:2505.06111, 2025. Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. Irving Fang, Juexiao Zhang, Shengbang Tong, and Chen Feng. From intention to execution: Probing the generalization boundaries of vision-language-action models. arXiv preprint arXiv:2506.09930, 2025. Ricardo Garcia, Shizhe Chen, and Cordelia Schmid. Towards generalizable vision-language robotic manipulation: benchmark and llm-guided 3d policy. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 89969002. IEEE, 2025. 10 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. Improving vision-language-action model with online reinforcement learning. arXiv preprint arXiv:2501.16664, 2025. Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, Tan, Navonil Majumder, Soujanya Poria, et al. Nora: small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854, 2025. Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):30193026, 2020. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag R. Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. ArXiv, abs/2406.09246, 2024. URL https://api.semanticscholar.org/ CorpusID:270440391. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. ArXiv, abs/2502.19645, 2025. URL https://api.semanticscholar.org/CorpusID: 276647709. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024a. Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. arXiv preprint arXiv:2503.00200, 2025. Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1806118070, 2024b. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024c. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36: 4477644791, 2023. Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. What can rl bring to vla generalization? an empirical study. arXiv preprint arXiv:2505.19789, 2025. Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. arXiv preprint arXiv:2107.14483, 2021. Vincent Müller. Measuring progress in robotics: Benchmarking and the measure-target confusion. In Metrics of Sensory Motor Coordination and Integration in Robots and Animals: How to Measure the Success of Bioinspired Solutions with Respect to their Natural Models, and Against More ArtificialSolutions?, pp. 169179. Springer, 2019. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. ArXiv, abs/2501.09747, 2025. URL https://api.semanticscholar.org/CorpusID:275570494. Wilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, and Dieter Fox. The colosseum: benchmark for evaluating generalization for robotic manipulation. arXiv preprint arXiv:2402.08191, 2024. 11 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Shuhan Tan, Kairan Dou, Yue Zhao, and Philipp Krähenbühl. Interactive post-training for vision-languageaction models. arXiv preprint arXiv:2505.17016, 2025. Zhijie Wang, Zhehua Zhou, Jiayang Song, Yuheng Huang, Zhan Shu, and Lei Ma. Vlatest: Testing and evaluating vision-language-action models for robotic manipulation. Proceedings of the ACM on Software Engineering, 2(FSE):16151638, 2025. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 2025a. Junjie Wen, Yichen Zhu, Minjie Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Xiaoyu Liu, Chaomin Shen, Yaxin Peng, and Feifei Feng. Diffusionvla: Scaling robot foundation models via unified diffusion and autoregression. In Forty-second International Conference on Machine Learning, 2025b. Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, et al. Vlabench: large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks. arXiv preprint arXiv:2412.18194, 2024. Jiaming Zhou, Ke Ye, Jiayi Liu, Teli Ma, Zifan Wang, Ronghe Qiu, Kun-Yu Lin, Zhilin Zhao, and Junwei Liang. Exploring the limits of vision-language-action manipulations in cross-task generalization. arXiv preprint arXiv:2505.15660, 2025. 12 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models"
        },
        {
            "title": "A Perturbation Dimensions",
            "content": "We conducted comprehensive review of existing studies aimed at evaluating the generalization performance of VLA models, particularly those introducing new test suites such as COLOSSEUM (Pumacay et al., 2024), RL4VLA (Liu et al., 2025), AGNOSTOS (Zhou et al., 2025), etc. The comparison of these methods is summarized in Table 3. Based on systematic analysis of their task paradigms, environment construction, data collection pipelines, and evaluation dimension designs, this study ultimately identified seven core dimensions of perturbation: objects layout, environment background sampling, light variations, camera-view shifts, robotarm initialization perturbations, LLM-based language rewrites, and image noise, with the goal of testing model robustness and generalization ability across all modalities of input (vision, state, language). Each dimension contains multiple quantifiable sub-dimensions defined to enable fine-grained evaluation of model performance. The perturbed examples are shown in Figures 1116. A.1 Objects Layout This dimension is designed to test model robustness against object-level disturbances. It is further divided into two sub-dimensions: O1: Confounding Objects. Randomly add additional unseen objects into the task scene. The object categories are drawn from predefined set of 416 distractor objects. This perturbation is implemented by modifying the task description files (BDDL). In the benchmark, related perturbations are stored in BDDL files with an add suffix. O2: Target Object Pose. Apply random perturbations to the target objects initial position (x, y, z) and orientation (pitch, yaw, roll). This perturbation does not alter the target object itself and ensures that essential semantic relations to other objects remain unchanged (e.g., in the task pick_up_the_black_bowl_next_to_the_cookie_box_and_place_it_on_the_plate, the relation to the cookie box determines the target object, and our modifications do not alter this constraint). A.2 Background Textures This dimension evaluates the models ability to generalize to different background textures of the scene. It contains two sub-dimensions: B1: Scene Theme. Change the scene texture of the environment (e.g., from painted wall to brick wall). The new textures are sourced from curated collection of 950 textures. This perturbation is implemented by modifying the scene XML definition files and registering new scene classes. B2: Surface Appearance. Randomly alter the texture of the working surface (e.g., tabletop or floor). A.3 Light Conditions This dimension evaluates the models visual understanding under different lighting conditions. It includes four sub-dimensions, all implemented by modifying scene XML definition files: L1: Diffuse. The diffuse color, which defines the light color uniformly reflected by object surfaces (adjusted via RGB channels; e.g., 1 0 0 indicates red diffuse light, making objects appear reddish). L2: Direction. Change the direction of the parallel light source, which significantly affects color rendering and shading. L3: Specular. The intensity of the specular highlight on object surfaces (e.g., the bright spot reflected on metals). Larger values yield more distinct highlights, strongly influencing scene style. L4: Shadows. Boolean variable (true/false) indicating whether shadows of the robot arm and objects are cast in the scene. 13 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models A.4 Camera Viewpoints This dimension tests the models view-free representation and generalization ability by changing camera viewpoints. All perturbations are implemented by modifying the Problem class interface, with parameters derived from task filenames: C1: Camera Distance. Move the camera along its optical axis, changing the distance to the scene center. Camera distances are valued among 1.01 to 2.00 the original value. C2: Spherical Position. Perturb camera position on sphere centered at the scene, altering azimuth (θ) and elevation (ϕ) within 1575 cones. C3: Camera Orientation. Fix the camera position but perturb its orientation (yaw, pitch, roll), valued within 2 to 10. A.5 Robot Initial States Initial Joint Angle. Random perturbations are applied to the robot arms initial joint positions (qpos). Perturbation magnitudes are valued from 0.1 to 0.5. This perturbation is implemented by modifying the Problem class interface. A.6 Language Instructions This dimension employs large language models (LLMs) to rewrite original task instructions, testing model generalization and reasoning ability in natural language: R1: Distraction. Task instructions are rewritten into longer and more conversational forms that contain additional but task-irrelevant contextual cues. R2: Common Sense. Replacing the existing object descriptions with commonsense-based descriptions to test information extraction and filtering. R3: Reasoning Chain. For multi-step reasoning instructions, perturbations involve altering reasoning complexity. Table 4: Examples of Language Instruction Rewriting Sub-category Examples Original R1 R3 push the plate to the front of the stove before turning on the burner, push the plate to the front of the stove propel the flat surface used for holding food toward the area designated for cooking heat adjustment make sure the plate ends up at the front of the stove A.7 Sensor Noise This dimension simulates real-world sensor imperfections to evaluate robustness under degraded input quality: N1: Motion Blur. Simulates blur caused by relative motion between camera and scene. Higher levels correspond to larger blur kernels, longer trajectories, and more severe blur. N2: Gaussian Blur. Simulates optical blur caused by defocus. Higher levels correspond to larger kernel size and standard deviation, resulting in smoother images with greater detail loss. N3: Zoom Blur. Simulates radial blur caused by rapid zoom during exposure. Higher levels increase zoom center and blur intensity, producing strong vignetting. N4: Fog. Simulates atmospheric interference such as fog or haze. Higher levels increase fog density and brightness, lowering image contrast and saturation. 14 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models N5: Glass Blur. Simulates distortions and refractions caused by viewing through textured glass. Higher levels increase distortion amplitude and range, resulting in severe local pixel displacements. Perturbation parameters are shown in Table 5. Table 5: Noise perturbation parameters. ID Noise Type Key Parameters Description of L1L5 1 2"
        },
        {
            "title": "Motion Blur",
            "content": "Radius r, Gaussian kernel σ, angle θ"
        },
        {
            "title": "Gaussian Blur",
            "content": "Standard deviation σ Zoom Blur Scaling factors [smin, smax, step] Fog Density α, decay rate β Glass Blur Gaussian blur σ, pixel displacement δ, iteration count and σ control blur strength (kernel size and spread). From weak blur (r = 5, σ = 2) to strong blur (r = 35, σ = 20). θ U(45, 45) determines blur direction. σ controls the amount of smoothing. Small σ produces slight blur (σ = 1), large σ produces heavy blur (σ = 10). Successive rescaling creates zoom-like blur. Weak effect at ([1, 1.11, 0.01]) and strong effect at ([1, 1.56, 0.03]). α controls fog thickness, β controls how quickly fog attenuates. Light fog (α = 0.5, β = 3.0) Dense fog with slow decay (α = 5.0, β = 1.3). σ defines baseline blur, δ controls the displacement of pixels, and iterations determine the accumulation of distortions. Light blur with small displacements (σ = 0.5, δ = 1, iters = 3) Strong blur with large displacements (σ = 2.5, δ = 5, iters = 1)."
        },
        {
            "title": "B Model Details",
            "content": "This appendix provides comprehensive descriptions of all models evaluated in our study, covering their architectural designs, training data sources, and key implementation specifications. We aim to offer sufficient transparency such that the reported results can be faithfully reproduced and compared against future work. B.1 Model Overview We evaluate diverse set of vision-language-action (VLA) models that represent different design choices in terms of architecture and training strategy, enabling us to systematically analyze how different factors contribute to task performance and robustness. For each model, we summarize its backbone, modality encoders, fusion mechanisms, and decision heads. B.2 OpenVLA (Kim et al., 2024) and OpenVLA-OFTs (Kim et al., 2025) Base Architecture. OpenVLA adopts modular vision-language architecture built on the Prismatic-7B VLM. The visual encoder is 600M-parameter dual-backbone composed of SigLIP and DINOv2, whose outputs are concatenated along the channel dimension to enhance spatial reasoning capabilities crucial for robotic control. lightweight two-layer MLP projector maps the fused visual features into the input space of Llama2-7B language backbone, which integrates visual and textual inputs through cross-attention. This design enables OpenVLA to leverage both semantic understanding and spatial grounding for action prediction. To adapt the VLM backbone for robotic control, continuous robot actions are discretized into 256 bins per dimension and represented as tokens within the LLM vocabulary. The 256 least frequently used tokens of the Llama tokenizer LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Table 6: Model HuggingFace repository addresses. Checkpoint Address https://huggingface.co/openvla https://huggingface.co/moojink ID Model Name OpenVLA 1 OpenVLA-OFT 2 OpenVLA-OFT_m https://huggingface.co/moojink/openvla-7b-oft-finetuned-libero-spatial 3 4 NORA 5 WorldVLA 6 7 8 9 https://huggingface.co/declare-lab https://huggingface.co/Alibaba-DAMO-Academy/WorldVLA https://huggingface.co/qwbu/univla-7b-224-sft-libero https://storage.googleapis.com/openpi-assets/checkpoints/pi0_libero https://storage.googleapis.com/openpi-assets/checkpoints/pi0_fast_libero https://huggingface.co/tanshh97/RIPT_VLA UniVLA π0 π0-Fast RIPT-VLA are replaced by action tokens, and training proceeds with the standard next-token prediction objective applied to action sequences. Training Strategy. The training pipeline consists of two stages: an initial pre-training followed by supervised fine-tuning. OpenVLA is pre-trained on the Open X-Embodiment (OpenX) dataset, which includes over 970k robot trajectories across multiple embodiments and tasks. The model is trained end-to-end with cross-entropy loss applied exclusively to the action tokens. Unlike typical VLM practices, the vision encoder is fine-tuned rather than frozen, enabling the model to capture fine-grained spatial details crucial for robotic control. Variants. In addition to the baseline OpenVLA models, we consider the OpenVLA-OFT family of variants: OpenVLA-OFT: parallel decoding variant enabling simultaneous prediction of all actions in single forward pass. It employs continuous action representations through multi-layer MLP head and is trained with an L1 regression objective, resulting in faster inference and more precise action generation, and it incorporates Feature-wise Linear Modulation (FiLM) to enhance language grounding. OpenVLA-OFT_w: variant of OpenVLA-OFT that removes the first-person wrist camera input and retains only the third-person view. This model is trained from OpenVLA with the official OFT hyperparameters on four LIBERO benchmark suites for 150K steps using 8A100 GPUs. OpenVLA-OFT_m: mixed-training variant that adopts the official mix-SFT weights. Unlike suite-specific training, this model is jointly trained across all four LIBERO suites, enabling it to learn from broader distribution of tasks and environments. B.3 π0 (Black et al.) and π0-fast (Pertsch et al., 2025) Base Architecture. The π0 architecture is inspired by the Transfusion framework, which trains single Transformer with multiple objective functions: flow-matching loss for continuous output tokens and cross-entropy loss for discrete tokens. Building upon this, π0 implements two sets of transformer weights (one initialized from the VLM and smaller action expert). The core model comprises VLM base (PaliGemma) for semantic understanding of multimodal inputs (multiple RGB images, language instructions, and proprioceptive state qt), and action tokens are projected and routed to smaller action-expert. Training Strategy. The training follows two-stage paradigm: (i) large-scale, diverse pre-training on mixture dataset to learn broad capabilities and recovery behaviors; (ii) post-training on smaller, high-quality curated datasets to induce dexterity and fluent task execution. The pre-training mixture is carefully reweighted to avoid over-representation. π0-fast: Efficient Action Tokenization. The π0-fast variant introduces the FAST tokenization method to compress action sequences. FAST combines Discrete Cosine Transform (DCT) for converting temporal action trajectories into sparse frequency-domain representation, followed by Byte-Pair Encoding (BPE) to losslessly compress the sparse DCT coefficient matrix into dense tokens. B.4 Nora (Hung et al., 2025) Base Architecture. NORA is 3B-parameter general-purpose VLA model optimized for robotic tasks. It adopts the Qwen-2.5-VL-3B multimodal model as its backbone, chosen for its strong visual-semantic LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models understanding capabilities, which enhance visual reasoning and action grounding. The model processes natural language task instructions and single-frame (as per its implementation) visual observations as input. It outputs discrete action sequences by employing the FAST+ tokenizer to discretize continuous action tokens. Training Strategy. NORA is pre-trained on the Open X-Embodiment dataset, which includes trajectories from various robots performing diverse tasks. This phase aims to equip the model with broad robotic capabilities and strong generalization. Training was conducted on 8H100 GPUs for approximately three weeks (totaling 4000 GPU hours), using the AdamW optimizer with batch size of 256 over 1.1 million gradient steps. linear warmup followed by cosine decay was applied to the learning rate. B.5 WorldVLA (Cen et al., 2025) Base Architecture. WorldVLA is an autoregressive action-world model that unifies visual-language-action (VLA) modeling and world modeling within single, integrated framework. The core idea is to jointly learn policy model for action generation and world model for future state prediction, allowing the two components to mutually enhance each other. The model is initialized from Chameleon, unified image understanding and generation model. It employs three tokenizers: VQ-GAN-based image tokenizer, BPE-based text tokenizer, and an action tokenizer that discretizes each dimension of the continuous robot action into 256 bins. All modalities (text, image, action) are discretized into tokens and modeled autoregressively within unified sequence. key architectural innovation is customized attention mask for action generation that prevents the current action from attending to previous actions, thereby mitigating error propagation and enabling more robust parallel action chunk prediction. Training Strategy. The model is trained on mixture of action-modeling data and world-modeling data. The action-modeling data trains the model to generate action chunks given language instruction and history of image observations, using loss computed only on the action tokens. The world-modeling data trains the model to predict the next image frame given the current image and action, using loss computed only on the image tokens. This joint training strategy encourages the learning of shared representations: the world model acquires an understanding of environmental physics to aid task-relevant action generation, while the action model enhances visual understanding to support accurate frame prediction. The model is evaluated on the LIBERO benchmark suite, with training leveraging 90% of the successful trajectories for training and 10% for validation. B.6 UniVLA (Li et al., 2025) Base Architecture. UniVLA is universal visual-language-action model that operates in discrete, taskcentric latent action space to achieve cross-embodiment generalization. The architecture is built upon pre-trained Prismatic-7B VLM, which integrates fused visual encoder (SigLip and DINOv2), projection layer, and an LLaMA-2 LLM. key innovation is the extension of the LLMs vocabulary with special action tokens to represent quantized latent actions. The model takes visual observation and language instruction as input and autoregressively predicts sequence of these latent action tokens. For deployment on specific robots, lightweight action decoder head is added, which uses multi-head attention pooling to map the predicted latent actions into the robots executable low-level control space. Training Strategy. The training process involves three stages. First, latent action model is trained in self-supervised manner on large-scale video datasets to learn discrete codebook of task-centric actions. This model uses DINOv2-based reconstruction objective and conditions on language instructions to disentangle task-relevant dynamics from irrelevant visual changes. Second, the universal policy is pre-trained to predict these latent action tokens from observations and instructions, leveraging the generalizable representations of the pre-trained VLM. This approach compresses the action space dramatically, leading to significantly faster convergence compared to methods operating in raw action spaces. Finally, for downstream adaptation, the entire model is fine-tuned end-to-end with combined loss for latent action prediction and low-level action regression, often using parameter-efficient methods like LoRA. history-augmented input scheme, where past latent actions are fed back as context, is employed to enhance performance in long-horizon tasks. B.7 RIPT-VLA (Brohan et al., 2022) Base Architecture. The base model for RIPT-VLA is OpenVLA-OFT, continuous-action VLA model where the action head is typically trained with an L1 regression loss. To make this architecture compatible with LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models reinforcement learning, which requires probabilistic policy output, RIPT-VLA augments the model with lightweight auxiliary head that predicts the scale parameter σθ for the action distribution. The policy is then treated as factorized Laplace distribution (for L1 loss) with the original model output as the mean µθ and the new heads output as the scale. This allows for sampling actions and computing the log-probability log πθ(ata<t, c) in closed form, which is essential for policy gradient updates. Training Strategy. RIPT-VLA introduces third stage of Reinforcement Interactive Post-Training (RIPT) following the standard pre-training and supervised fine-tuning (SFT) stages. The strategy is centered on the Dynamic Sampling Leave-One-Out PPO (LOOP) framework. In the rollout collection phase, for given context ci, trajectories are sampled from the current policy. The RLOO advantage estimation method is used to compute advantages from the sparse binary rewards. key innovation is dynamic rejection mechanism that filters out context samples where all rollouts receive identical rewards (all successes or all failures), thus ensuring that the training batch contains meaningful learning signals. During the policy optimization phase, the PPO algorithm is applied to the collected rollouts to maximize the expected task success rate, with the policy update constrained by the probability ratio ri = πθ(aici)/πψ(aici) to ensure stable training. This iterative process of data collection and optimization allows the model to improve its performance through environment interaction, specifically targeting and overcoming failure modes encountered during deployment."
        },
        {
            "title": "C Perturbations and Benchmark Construction",
            "content": "C.1 Data Generation and Filtering We began with the 40 evaluation tasks from LIBERO and generated 500 instances for each of the four generalization sub-tasks (Spatial, Object, Goal, Long) across the seven generalization dimensions, resulting in an initial set of 14,000 candidate tasks. These tasks were evaluated using several widely adopted baseline models to assess performance distributions, as summarized in Section 2. Tasks that were solved by all models, or by large majority, were removed to avoid ceiling effects. We further balanced the remaining tasks across augmentation sub-dimensions to prevent bias. The final test-only benchmark consists of 10,030 tasks spanning all seven dimensions. C.2 Dataset Composition Table 7 presents the final distribution of evaluation tasks across generalization dimensions and sub-task categories. Table 7: Distribution of the evaluation dataset across dimensions and different categories. Spatial Object Goal Long Total Camera Robot Language Light Background Noise Layout 292 297 279 274 1142 258 248 281 289 1076 376 396 408 419 1599 350 398 409 393 1550 354 390 410 383 312 425 403 385 1525 351 422 379 449 1601 Total 2293 2576 2569 2592 10030 C.3 Difficulty Assessment We evaluated the 10,030 tasks using four representative modelsOpenVLA-OFT, π0, π0-fast, and UniVLAand stratified task difficulty based on how many of these models succeeded on each instance: Level 1 (L1): solved by all four models; Level 2 (L2): solved by exactly three models; Level 3 (L3): solved by exactly two models; Level 4 (L4): solved by exactly one model; Level 5 (L5): solved by none. Figure 7 illustrates the proportion of tasks at each difficulty level for every dimension. 18 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 7: Proportion of tasks per difficulty level across the seven generalization dimensions. C.4 Model Performance by Difficulty Level We further analyzed how model accuracy varies with task difficulty. Figure 8 shows the success rates of each model across the five difficulty levels."
        },
        {
            "title": "D Training Dataset Construction",
            "content": "D.1 Dataset Overview The generalized training dataset consists of over 20,000 successful trajectories, covering wide range of task variations and environment configurations. Figure 9 shows the distribution of the 7-dimensional robot actions in the dataset. The plots are arranged from top to bottom and left to right, corresponding to the seven action dimensions, respectively. This visualization demonstrates the diversity and coverage of the actions captured in the generalized dataset. The dataset includes six types of task variants and environment modifications: objects spanning, environment sampling, light variations, camera-view shifts, LLM-based language rewrites, and sensor noise. Among these, the objects spanning variant contains only compounding objects, which are generated by executing existing trajectories and selecting only the successful ones. Variants involving pose changes were not added due to the limited reliability of automatically generated trajectories. D.2 Data Generation Process The generalized dataset was constructed using the same automated generalization pipeline, with variations in parameters to produce diverse scenarios: Objects Positioning: For compounding objects, distractor objects and their poses were varied while ensuring no overlap with the test set. Background Environment Sampling: Additional textures for tables, walls, and floors were automatically sampled to avoid overlap with the test environments. Light Variations: Different lighting parameters were applied to the scenes. Camera-view Shifts: Camera angles differed by 5 on the spherical coordinate system compared to the test set. 19 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models LLM-based Language Rewrites: New language instructions were generated to provide additional linguistic diversity. Image Noise: Sensor noise parameters differed from those listed in Table 5. D.3 Trajectory Collection Trajectory collection was performed using the original LIBERO datasets (state, action) pairs, executed in the newly generated environments. Only successful trajectories were retained, and any actions corresponding to no-ops were filtered out. Specifically, 2,400 trajectories were collected for the compounding object variant, while 4,000 trajectories were collected for each of the other variants, resulting in total of 22,400 trajectories. After filtering, over 20,000 high-quality trajectories were retained for training. Figure 8: Model performance trends across perturbation difficulty levels. The line plots show the success rate of each model as the intensity of all seven different perturbation dimensions increases. D.4 Training Configuration Using this dataset, we performed mixed fine-tuning based on the official OpenVLA-OFT weights. The training was conducted on 8 A100 GPUs with learning rate of 5 104 for 100,000 steps. The batch size was set to 64 per GPU, resulting in an effective batch size of 512. We employed the AdamW optimizer with weight decay of 0.1 and used cosine learning rate schedule with warmup. The training results on LIBERO-plus are shown in Table 2. D.5 Storage Format All trajectories are stored in the rlds format, consistent with standard practices for robotics datasets and ensuring compatibility with existing training pipelines."
        },
        {
            "title": "E Goal Replacement Rollout Cases Analysis",
            "content": "To further probe whether Vision-Language-Action (VLA) models genuinely understand and act upon natural language instructions, we designed goal replacement evaluation. In this task, the target object specified in both the instruction and the task goal was replaced with an alternative object from the same scene, while keeping the rest of the environment unchanged. For example, an original instruction such as pick up the alphabet soup and place it in the basket could be modified to pick up the tomato sauce and place it in the 20 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 9: Distribution of the 7-dimensional robot actions in the generalized dataset. Plots are arranged from top to bottom and left to right, corresponding to action dimensions 17. basket. We performed this manipulation on the object suite, where misalignment between model actions and instructions was most pronounced. Figure 5(b) summarizes the performance drop across models, while the rollout cases in Figure 10 reveal how these degradations manifest in execution. From these results, we observed two key patterns: 1. Lack of cross-object generalization in instruction following. Across all tested instances, models failed to adapt to the new target specified in the instruction, with success rates in replaced-target tasks dropping nearly to zero. This drop was particularly dramatic for OpenVLA-OFT, whose accuracy in the modified target setting diminished from high baseline values to almost complete failure. This confirms that the robustness observed in earlier language perturbation experiments did not originate from true linguistic comprehensionthe models appear to ignore linguistic signals and rely instead on fixed, learned perceptionaction associations. 2. Over-reliance on fixed visionaction mappings rather than dynamic instruction-based planning. In nearly all rollout cases (Figure 10), the model performed the original action for the original target even when the instruction had explicitly changed. For example: In case (a), the new instruction specified picking up the butter, yet the model still picked up the alphabet soup as in the original task. In case (c), the model was instructed to pick up tomato sauce, but executed the original butter action. Similar behavior was observed in (d) and (e), where the model persisted with the original target (e.g., chocolate pudding, cream cheese) rather than adjusting to the new goal. These behavioral patterns indicate that the VLA models in our study function more like visual pattern matchers mapping scene configurations to predetermined action sequences, rather than integrating task-relevant 21 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 10: Behavioral Analysis of Goal Replacement Failures. Case studies showing model responses to modified instructions. For each pair: originalnew instruction (above); actually executed behavior (below). The consistent execution of original tasks despite changed targets indicates shallow language processing and strong bias toward memorized visual-action associations. 22 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Table 8: Pairwise evaluation results across different perturbation dimensions. Layout Background Light Camera Robot Noise Object Background Light Camera Robot Noise 71.75 57.00 57.20 35.95 24.40 44. 85.75 67.10 37.70 29.95 51.05 82.10 39.65 29.65 54.00 57.30 19.05 36.70 39.10 22.15 71."
        },
        {
            "title": "F Details of the Compositional Generalization Experiments",
            "content": "F.1 Success Rate Record Table 8 reports the pairwise success rates across different perturbation dimensions. Each diagonal entry corresponds to the performance under single perturbation dimension, while the off-diagonal entries represent joint perturbations of two dimensions. This analysis allows us to examine not only the robustness of models to isolated disturbances, but also the interaction effects between multiple perturbations, which are critical for assessing compositional generalization in realistic robotic scenarios. F.2 Significance Experiments for Compositional Generalization To ensure that the observed deviations between the expected product-based success rates and the actual joint success rates are not due to random chance, we conduct significance experiments. Specifically, we aim to statistically validate whether the negative compositional gaps indeed reflect systematic interaction effects between perturbations, rather than sampling noise arising from finite trials. For this purpose, we adopt the classical chi-square test for independence. Let n00 be the number of samples succeeding under neither of the two perturbations, n01 the number succeeding under perturbation 2, n10 the number succeeding under perturbation 1, and n11 the number succeeding under both perturbations. Chi-square test for independence: To statistically assess whether the deviation is significant, we consider the 2 2 contingency table of success counts under perturbations Di and Dj: Di = 0 Di = 1 Total Dj = 0 Dj = 1 Total n0 n01 n1 n11 n1 n00 n10 n0 The chi-square statistic is then given by χ2 = r,c (Orc Erc)2 Erc , where Orc denotes the observed count and Erc = (row total)(column total) the independence hypothesis. is the expected count under p-value: Given the chi-square statistic χ2 and the corresponding degrees of freedom (here dof = 1 for 2 2 table), the p-value is the probability of observing test statistic at least as extreme as χ2 under the null hypothesis of independence: = (cid:16) dof=1 χ2(cid:17) χ2 , where χ2 < 0.05) indicates strong evidence against the independence assumption. dof=1 denotes chi-square distribution with 1 degree of freedom. small p-value (e.g., 23 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models large χ2 value (with small p-value) indicates that the joint success/failure distribution under perturbations di and dj deviates significantly from the independence assumption, implying interaction effects between the two perturbations. Conversely, small χ2 (large p-value) suggests no evidence against independence. Table 9: Chi-square test results for perturbation pairs Perturbation Perturbation Chi-square p-value"
        },
        {
            "title": "Env\nLight\nCamera\nRobo init\nNoise\nLight\nCamera\nRobo init\nNoise\nCamera\nRobo init\nNoise\nRobo init\nNoise\nNoise",
            "content": "4.09 1.23 7.55 6.13 9.42 2.37 26.1 4.87 16.1 12.1 2.79 4.53 6.76 5.51 14.3 4.32e-02 2.68e-01 6.01e-03 1.33e-02 2.14e-03 1.24e-01 3.33e-07 2.74e-02 6.07e-05 4.92e-04 9.48e-02 3.34e-02 9.31e-03 1.90e-02 1.59e-04 From Table 9, it can be observed that most perturbation pairs yield large χ2 values, with correspondingly tiny p-values, below conventional significance thresholds (0.05). This indicates that the joint distribution under different perturbations deviates strongly from the independence assumption, implying clear interaction effects between perturbations. Overall, the results consistently demonstrate that perturbation interactions are significant and cannot be ignored when evaluating compositional generalization."
        },
        {
            "title": "G Failure Cases Study",
            "content": "To gain deeper insights into the models failure mechanisms beyond aggregate performance metrics, we conduct qualitative analysis of characteristic error patterns across different perturbation types. This case study reveals how each perturbation dimension induces distinct failure modes in object localization, task understanding, and action execution, providing explanatory context for the quantitative results presented in previous sections. Typical failure cases can be seen in Figures 17 to 19. 24 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 11: Rendering results with background texture perturbations. The top-left image is the original; the others show results with the textures as labeled. Figure 12: Rendering results under background texture perturbations, comparing the original image (top-left) with transformed versions. The labels denote the following transformation parameters: hr (horizontal rotation angle), vr (vertical rotation angle), dis (distance pulled away), chr (in-place horizontal rotation angle), and cvr (in-place vertical rotation angle). Detailed results of LIBERO-Plus This section presents comprehensive analysis of generalization performance under diverse perturbations on the LIBERO-Plus benchmark. Table 10 provides detailed success rates across seven perturbation categories (Camera, Robot Initialization, Language Instruction, Lighting, Background, Sensor Noise, and Scene Layout) for various VLA methods, with results further broken down by task suite (Spatial, Object, Goal, and Long). 25 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 13: Rendering results with robot initial state perturbations. The top-left image is the original; the others show results with the norm of the change in the robots joint angles as labeled. Figure 14: Rendering results with light perturbations. The top-left image is the original; the others show results with the relative change as labeled. The comparative analysis reveals significant differences in robustness patterns across methods and perturbation types, offering valuable insights for understanding model generalization capabilities. 26 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 15: Rendering results with sensor noise perturbations. The top-left image is the original; the others show results corresponding to the type and severity of the applied noise, as indicated by the labels. Figure 16: Rendering results with object layout perturbations. The top-left image is the original; the others show results with the number of added objects as labeled. 27 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 17: Failure Mode Analysis Across Perturbation Types. Visualization of characteristic failure patterns induced by each perturbation dimension, revealing distinct vulnerability profiles: camera shifts cause viewpointdependent localization errors; language modifications lead to semantic misinterpretations; lighting variations introduce shadow artifacts; sensor noise produces feature corruption; initial state changes affect trajectory planning; and object distractors trigger recognition confusion. 28 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 18: Failure Mode Analysis Across Perturbation Types. Visualization of characteristic failure patterns induced by each perturbation dimension, revealing distinct vulnerability profiles: camera shifts cause viewpointdependent localization errors; language modifications lead to semantic misinterpretations; lighting variations introduce shadow artifacts; sensor noise produces feature corruption; initial state changes affect trajectory planning; and object distractors trigger recognition confusion. 29 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Figure 19: Failure Mode Analysis Across Perturbation Types. Visualization of characteristic failure patterns induced by each perturbation dimension, revealing distinct vulnerability profiles: camera shifts cause viewpointdependent object localization inaccuracy; object distractors provoke recognition confusion and mislocalization of the target, in some cases leading to incorrect collision-prone trajectories when arm motion flexibility is insufficient. 30 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Table 10: Detailed generalization performance comparison across different perturbation types on the LIBEROPlus benchmark. The table reports success rates (%) for various VLA methods under seven distinct perturbation categories and their average (Total). Results are further broken down by task suite to provide fine-grained insights into each methods robustness capabilities."
        },
        {
            "title": "Spatial\nObject\nGoal\nLong\nAvg",
            "content": "Spatial Object Goal Long Avg Spatial Object Goal Long Avg Spatial Object Goal Long Avg Spatial Object Goal Long Avg Spatial Object Goal Long Avg 0.0 0.5 2.5 0.0 0. 88.3 38.9 62.0 38.7 56.4 8.8 10.1 16.4 6.2 10.4 55.3 70.2 56.6 41.0 55.6 4.3 0.5 2.9 1.2 2.2 0.0 0.0 0.3 0.0 0.1 1.1 0.0 3.9 1.9 1. 3.7 4.5 2.7 3.0 3.5 40.0 25.4 25.2 38.2 31.9 39.7 31.4 39.9 43.8 38.6 19.7 18.1 17.1 31.8 21.7 50.9 28.4 31.1 39.4 37.0 44.3 26.4 30.6 12.2 27. 52.6 42.2 37.9 53.2 46.2 50.4 45.2 27.1 19.4 34.8 97.3 97.6 92.5 86.8 93.3 99.2 96.4 89.0 90.7 93.6 92.3 91.9 94.7 85.5 91.0 65.5 54.8 60.5 54.0 58. 19.8 17.3 30.3 1.7 17.1 90.7 81.5 78.3 74.4 81."
        },
        {
            "title": "OpenVLA",
            "content": "12.3 1.0 9.0 10.6 8.1 OpenVLA-OFT 98.3 73.7 93.9 89.4 88.7 27.7 21.0 21.5 22.2 23.0 80.5 99.0 53.2 87.0 79.5 OpenVLA-OFT_w 83.6 76.4 47.1 77.3 70.5 88.4 85.9 85.3 46.0 76.8 OpenVLA-OFT_m 92.7 98.5 47.6 88.3 81.0 63.8 76.4 56.6 64.0 65.1 46.3 57.2 42.2 20.6 41. 83.9 86.9 45.6 64.2 69.5 100.0 100.0 87.8 82.1 92.7 NORA 66.8 25.3 60.6 30.3 45.7 WorldVLA 65.1 20.5 68.8 20.4 43. UniVLA 96.6 25.6 89.6 65.7 69.0 π0 31 12.0 11.4 19.5 17.6 15.2 96.3 72.3 75.2 63.5 75. 55.3 48.3 54.9 43.0 49.9 85.2 94.1 65.7 69.9 78.6 12.5 5.7 18.2 15.1 12.8 11.7 18.0 13.5 1.6 10.9 15.7 10.4 33.5 25.4 79.0 40.7 22.4 25.6 28.3 28. 93.9 71.8 59.1 76.9 74.2 82.7 66.3 61.8 72.0 69.9 94.5 77.4 46.6 61.0 68.7 84.6 55.8 53.9 59.5 62.1 46.1 53.6 47.4 4.4 38.0 69.5 27.3 22.6 16.4 31. 19.4 14.0 15.1 14.3 15.6 84.0 66.5 63.0 66.4 69.6 62.5 56.0 53.3 52.2 55.8 75.4 77.1 56.2 63.9 67.9 47.6 34.4 38.8 36.3 39.0 32.5 28.6 31.8 8.2 25. 55.5 36.7 40.7 39.9 52.1 Continued on next page LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models"
        },
        {
            "title": "Light Background Noise Layout Total",
            "content": "Table 10 (continued)"
        },
        {
            "title": "Spatial\nObject\nGoal\nLong\nAvg",
            "content": "Spatial Object Goal Long Avg Spatial Object Goal Long Avg 17.8 22.2 12.3 3.8 13.8 87.2 72.0 70.8 33.2 65.1 85.4 37.9 65.7 34.1 55.2 98.4 97.0 93.9 82.6 92. 6.6 8.3 5.6 3.6 6.0 26.9 27.6 20.5 12.0 21.6 38.0 26.4 23.2 38.4 31.2 31.7 24.6 24.7 40.7 30.3 58.8 70.0 39.3 68.4 58.8 84.2 71.5 47.3 43.6 61. 99.7 80.8 45.4 88.3 77.5 96.0 100.0 55.1 94.8 85.8 89.7 90.9 84.2 74.5 85.0 π0_Fast 37.0 71.0 95.3 91.6 73.2 RIPT-VLA 99.7 85.9 74.2 93.4 88. Ours 99.3 99.7 96.8 83.2 94.9 90.7 91.1 76.5 69.5 81.4 97.7 95.2 60.9 44.6 73.2 100.0 99.2 79.7 89.3 91.6 98.8 98.8 94.0 85.1 93. 90.9 87.0 76.5 64.4 79.0 93.2 93.1 69.7 46.1 74.4 92.0 68.0 71.0 66.4 73.5 86.3 97.4 93.4 80.6 89.3 89.1 76.2 44.7 69.6 68.8 95.5 84.5 51.6 47.8 68. 92.3 70.1 59.8 79.2 74.2 97.8 82.8 53.9 80.3 77.6 60.7 61.4 44.9 48.4 53.6 74.4 72.7 57.5 43.4 61.6 85.8 64.3 58.0 67.5 68.4 86.1 84.5 70.7 77.7 79."
        }
    ],
    "affiliations": [
        "Fudan University",
        "National University of Singapore",
        "Shanghai Innovation Institute",
        "Tongji University"
    ]
}