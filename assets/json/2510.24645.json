{
    "paper_title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling",
    "authors": [
        "Zengzhuang Xu",
        "Bingguang Hao",
        "Zechuan Wang",
        "Yuntao Wen",
        "Maolin Wang",
        "Yang Liu",
        "Long Chen",
        "Dong Wang",
        "Yicheng Chen",
        "Cunyin Peng",
        "Chenyi Zhuang",
        "Jinjie Gu",
        "Leilei Gan",
        "Xiangyu Zhao",
        "Shi Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning."
        },
        {
            "title": "Start",
            "content": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling Zengzhuang Xu1, Bingguang Hao1, Zechuan Wang2, Yuntao Wen1, Maolin Wang3, Yang Liu2, Long Chen1, Dong Wang1, Yicheng Chen1, Cunyin Peng1, Chenyi Zhuang1, Jinjie Gu1, Leilei Gan2, Xiangyu Zhao3, Shi Gu2 1AWorld Team, Inclusion AI FunReason-MT Dataset 2Zhejiang University 3City University of Hong Kong FunReason-MT Model Project FunReason-MT"
        },
        {
            "title": "Abstract",
            "content": "Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multiturn logical dependency. To address these structural deficiencies, we present FunReason-MT, novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides reliable and robust source for agentic learning. This is part of the open-source Project model weights are released. AWorld, Inclusion AI. The training data and 5 2 0 2 8 2 ] A . [ 1 5 4 6 4 2 . 0 1 5 2 : r Figure 1: Performance on BFCL Single-Turn, Multi-Turn and Agentic Evaluation. Equal Contribution. The two authors designed the system. Core Contributor. These authors participated in algorithm development, data engineering, and infrastructure. Corresponding Author. chenyi.zcy@antgroup.com, gus@zju.edu.cn Work was done during Bingguangs internship at Ant Group. bingguanghao7@gmail.com 1 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling Figure 2: We summarize the core challenges in multi-turn function-calling data generation into three aspects from left to right: Targeted Model Training, Isolation of Tool Architecture, Multi-Turn Logical Dependency."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are profoundly transforming human-computer interaction, largely through Function Calling (FC), the pivotal technology enabling LLMs to interface with external tools WANG et al. (2025). Although contemporary foundation models possess inherent tool-use capability, unlocking more advanced applications in this domain is hindered by critical bottleneck: The construction of effective, high-quality data, particularly for multi-turn trajectories. Existing data generation research predominantly relies on constrained methodologies, such as random sampling within environments or Multi-Agent System (MAS) role-playing simulations Huang et al. (2025); Prabhakar et al. (2025); Liu et al. (2024); Zeng et al. (2025). Crucially, random sampling fails to capture rare, complex events, while MAS role-playing often defaults to simplistic happy-path scenarios or suffers from lack of true diversity Han et al. (2024). Thus these approaches inherently produce data that is limited in both effectiveness and diversity, resulting in substantial training performance ceiling. As illustrated in Figure 2, we categorize the challenge in multi-turn FC data generation into three critical, interconnected issues that diminish data quality and model stability. 1) Targeted Model Training. Existing random sampling techniques fail to controllably and targetedly construct model training scenarios requiring the collaborative use of target complex tool with others across diverse, multi-turn trajectories. 2) Isolation of Tool Architecture. Due to the modular inputs of individual tools rather than holistic integration, direct environmental sampling and MAS role-playing struggle to synthesize hard, logical-jump queries. 3) MultiTurn Logical Dependency. In complex multi-turn dialogues, each step of the Chain-of-Thought (CoT) is validation and reconstruction of the reliance on prior logic. Current Reasoning Large Language Models (RLLMs) often falter when generating CoT in unexplored environments, preventing the acquisition of complete and accurate multi-turn trajectories. To fundamentally resolve these structural limitations, we introduce the Function Call Reasoning MultiTurn (FunReason-MT) data synthesis framework. Our framework prioritizes robustness and efficiency, and incorporates three specific core components to tackle the outlined challenges. Environment-API Graph Interactions. We establish an API graph based on interdependencies between tools and environments, enabling the collection of multi-turn trajectories centered around target complex API within varied scenarios, ultimately training models for targeted mastery and collaborative tool use. Advanced Tool-Query Synthesis. We use the tool-state pairs collected during Environment-API Graph Interactions to construct an advanced tool that eliminates the need to input the modularized sub-tools when queries are generated in reverse. This inherently makes the execution of prerequisite tools implicit. Guided Iterative Chain. We input the hard query, the set of sub-tools, and the advanced tool description to obtain CoT. We then implement an iterative feedback loop: The analysis identifying the specific reasons for any failure to match the ground truth is continuously integrated as corrective hint until the CoT successfully leads to the right FC or meets the maximum attempts. We validate the FunReason-MT framework on the challenging Berkeley Function-Calling Leaderboard (BFCLv3), encompassing both single-turn and multi-turn tests Patil et al.. Experiments demonstrate that FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling our data construction method consistently outperforms approaches based on MAS role-playing or random environmental sampling, and achieves state-of-the-art performance among comparable-sized models. This success affirms that FunReason-MT effectively overcomes the complexity barrier in multi-turn FC data generation, establishing an essential knowledge supplement for future LLM environment exploration. We also evaluate our models on BFCLv4, an out-of-distribution (OOD) benchmark, to further assess their agentic ability. Although our data is unrelated to the tasks evaluated by BFCLv4 (Web Search and Memory), our models still show improvement in these specific areas. We believe our study is well-positioned to lay robust and enduring foundation that facilitates the future progression of agentic learning Zhang et al. (2025); Ke et al. (2025)."
        },
        {
            "title": "2 Motivation",
            "content": "Our motivation stems from the insight that the identified \"performance ceiling\" is not merely data quantity issue, but fundamental failure of the existing data generation paradigm. We recognized that \"bottom-up\" approaches like random sampling and MAS role-playing are structurally incapable of producing the highcomplexity, long-tail trajectories needed for true mastery. These methods merely hope for complexity to emerge from simple interactions, suffer from low controllability and poor reliability. This drove us to devise new, \"top-down\" construction methodology that can explicitly direct the model to master specific complex tool within targeted complex scenario. Our approach is motivated by the \"why\" and \"how\" of this shift: instead of hoping for targeted complex tool involved trajectories, we engineer them by modeling the environments structural dependencies (the API graph). Instead of sampling simple tool combinations, we guarantee complex, logical-jump queries by generating them in reverse. And finally, instead of accepting flawed reasoning, we enforce CoT accuracy through guided feedback loop, thus directly targeting the three core challenges that plague existing methods and corresponding to solving the issues of reliability and complexity respectively."
        },
        {
            "title": "3 Notation",
            "content": "In order to describe the generation process of the FunReason-MT framework, we introduce the key symbols and concepts used to model the systems components and operations. The core concepts and models that manage the system are defined in Table 1. Symbol = (T, R, P) AT, AQ, AR, AC Ta Kmax Traj Turni"
        },
        {
            "title": "Concept",
            "content": "Multi-Environment Simulation Space. Tool Set. API Relation Graph (Tools, Dependencies R, Parameters P). LLM Agents (Tooling, Querying, Reasoning, Critiquing). Target Tool. Number of tool calls per round. Number of trajectory rounds. Maximum self-correction attempts. Multi-turn trajectory. i-th single-turn content. Table 1: Core Concepts and Models of FunReason-MT."
        },
        {
            "title": "4 Methodology",
            "content": "As shown in Figure 3, our proposed FunReason-MT framework comprises three core phases: (1) Phase I: Environment-API Graph Interactions constructs valid multi-step execution trace by sampling tool calls from the API Relation Graph (4.1); (2) Phase II: Advanced Tool-Query Synthesis reverse-engineers hard-to-solve data sample and synthesizing corresponding challenging Hard Query (Qhard) (4.2); (3) Phase III: Guided Iterative Chain utilizes an iterative, self-correction loop to refine the CoT and ensure its logical consistency with the ground truth answer (4.3)."
        },
        {
            "title": "4.1 Phase I: Environment-API Graph Interactions\nA central challenge in generating multi-turn tool-use trajectories is to devise a sampling strategy that is both\nexecutable and purposefully directed. To achieve this, FunReason-MT framework is designed to satisfy two",
            "content": "3 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling Figure 3: Three Phases of FunReason-MT. critical constraints simultaneously: execution correctness (i.e., respecting tool dependencies) and goal-directed efficiency (i.e., prioritizing Ta). We detail these two components below. Formalizing Tool Call Legality To ensure valid execution sequence, we introduce strict dependency constraint: Tool Ti is only callable if all its necessary preconditions, Prerequisite(Ti), are satisfied by the set of already executed tools, called. We formalize this essential constraint using the tool legality check I: I(Ti, called) = 1{Prerequisite(Ti)T called} where 1{} is the indicator function, returning 1 if the condition is true and 0 otherwise. Tool Sampling with Priority While random sampling over the tool graph can satisfy execution correctness, it often fails to reach the target tool Ta efficiently. To address this, we introduce Directed Sampler, denoted as SampleTool, which explicitly biases the sampling process toward Ta. Specifically, SampleTool adopts greedy heuristic strategy that operates over the set of legal tools legal and prioritizes those minimizing the graph distance dist(, ) to Ta. The sampling policy of SampleTool is formally defined as follows: Ts = SampleTool(Ta, called) = rand(T Ta argmin Tk legal) if Ta if I(Ta, {dist(Tk, Ta)} otherwise called called) = 1 Ta / called (1) The chosen tool Ts is executed using sampled parameters Ps as the call Cs = (Ts, Ps). This action yields environmental feedback Ei, triggers an update to the system state j, and consequently updates the API Graph G, legal tools legal and executed tools called. The resultant single-turn execution sequence is recorded as the trace: Turni = (C1, E1, C2, E2, . . . , CM, EM)."
        },
        {
            "title": "4.2 Phase II: Advanced Tool-Query Synthesis\nWhile the execution trace Turni ensures correct tool execution, it does not directly capture the abstract intent\nbehind those operations. To address this, Phase II reverses the process by synthesizing a challenging query",
            "content": "4 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling Qhard that compels the model to reason over the entire trace as single high-level abstraction. 1. Advanced Tool Generation. Given the execution trace Turni, the Tooling Agent (AT) synthesizes single composite abstraction, denoted as the advanced tool Tadv: Tadv = AT (Turni) (2) where AT abstracts the multi-step execution trace into unified high-level operation that encapsulates the functionality and interdependencies of all sub-tools within Turni. 2. Hard Query Construction. Conditioned on the synthesized advanced tool Tadv, the Querying Agent (AQ) generates challenging hard query Qhard that explicitly requires the use of Tadv for resolution. Formally, the construction process is expressed as: Qhard = AQ (Tadv, ϵ) (3) where ϵ denotes synthesis noise that increases task difficulty and encourages generalization."
        },
        {
            "title": "4.3 Phase III: Guided Iterative Chain\nWhile the construction of Qhard promotes higher-level reasoning, it also increases the risk of logical inconsis-\ntency or incomplete Chain-of-Thought (CoT) generation. To address this, Phase III introduces the Guided\nIterative Chain, a feedback-driven refinement process that iteratively improves CoT quality and coherence\nuntil the reasoning converges to a reliable solution or reaches the maximum attempts.",
            "content": "Initial Reasoning Attempt and Correction Loop The Reasoning Agent AR begins by attempting to resolve the hard query, Qhard, utilizing both the advanced tool description Des(Tadv) and its primitive components sub. This first attempt yields the initial output O(0) = {CoT, FC}, comprising CoT and final Function Call. The generated function call (FC(k)) is subsequently validated against the known ground truth ( = (C1, C2, . . . , CM) ) using verification function Validate : FC {Pass, Fail}. 1. Error Analysis (Upon Failure): If the current attempt fails, the Critiquing Agent (AC) analyzes the incorrect function call FC(k) with respect to the ground truth A, identifies the specific error, and generates targeted corrective feedback: Error(k) = AC(FC(k), G). 2. Prompt Augmentation: The corrective error feedback is then integrated into the Reasoning Agents context as Prompt(k+1). This addition implicitly creates feedback loop, effectively guiding the subsequent attempt. 3. Self-Correction: The Reasoning Agent AR is re-invoked with the augmented prompt to generate refined solution, O(k+1) = AR(. . . , Prompt(k+1)). The maximum allowed limit for this iterative correction process is Kmax attempts. data sample is only retained if the validation function Validate(FC(k), G) returns Pass before the iteration cap is reached."
        },
        {
            "title": "4.4 Scaling from Single-Turn Trace to Multi-Turn Trajectory\nThe full pipeline is iterated N times to scale the single-turn generation into a comprehensive, high-quality\nmulti-turn trajectory, Traj.",
            "content": "The overall data generation process starts by initializing Traj as an empty set. The framework then sequentially executes the three phases for each of the required data rounds: Step 1: Environment API-Graph Interactions: target tool Ta is chosen from the primitive tool set T. This choice dictates the generation of complete execution trace Turn1 within the simulation environment, following the procedure outlined in Phase 1. Step 2: Advanced Tool-Query Synthesis: The successful trace is then gave to the LLM agents AT and AQ. These agents collaborate to synthesize the Advanced Tool (Tadv) and its corresponding Hard Query (Qhard), thereby forming the core of the new data sample. 5 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling Step 3: Guided Iterative Chain: Immediately following synthesis, the data sample undergoes the crucial Guided Iterative Chain verification process. This step employs the reasoning and critiquing agents (AR, AC) to attempt solving the hard query and validate results against the ground truth answer G. To ensure the resulting multi-turn trajectory is reliable and complex, the entire process detailed above will be repeated times."
        },
        {
            "title": "5.1 Experimental Setup\nTraining Details. Considering the length of multi-turn conversation and CoT, we use Qwen3-4B-Instruct-2507\nwith 256K long-context understanding as our base model Yang et al. (2025). The collected trajectories are\nsplit at each assistant response and the model is only trained on the CoT and answer at current turn. We\ncollect 10000 multi-turn samples, and to enhance the diversity of the dataset, we also jointly train model with\nfunction calling data from APIGen. We use LLama-Factory Zheng et al. (2024) and Verl Sheng et al. (2024)\nlibrary to train models with implementations from Hao et al. (2025b) and Hao et al. (2025a).",
            "content": "Benchmarks. Our primary evaluation is performed on the BFCLv3, which is designed to assess the models performance on both Single-Turn and Multi-Turn tool-calling tasks. To further evaluate the data influence on the models agentic capability, we extend our assessment to the Held-out (OOD) BFCLv4 benchmark. BFCLv4 is engineered for the evaluation of agentic performance with Web Search and Memory. We present all the results in terms of accuracy (%)."
        },
        {
            "title": "5.2 Main Results",
            "content": "Multi-Turn Single-Turn Overall Base Miss Func Miss Param Long Context Overall Non-Live Live Model Close Source Model GPT-5-2025-08-07 GPT-4o-2024-11-20 Claude-Sonnet-4-20250514 Gemini-2.5-Pro o3-2025-04-16 Grok-4-0709 Open Source Model Moonshotai-Kimi-K2-Inst DeepSeek-R1-0528 Qwen3-235B-A22B-Inst-2507 Llama-4-Maverick ToolACE-2-8B BitAgent-8B watt-tool-8B xLAM-2-3b-fc-r ToolACE-MT 28.50 42.50 54.75 25.00 38.38 36.12 41.25 44.50 39.62 17.88 37.00 37.75 37.88 57.12 40.25 Qwen3-4B-Inst-2507 +FunReason-MT (SFT) +FunReason-MT (RL) 15.75 46.90+31.15 56.50+40.75 33.50 55.50 64.00 25.50 44.00 44.00 51.00 54.50 53.50 23.50 47.00 46.50 45.50 73.50 57.50 19.00 53.20 61.50 29.50 34.50 54.00 26.00 40.50 31.00 43.00 41.00 34.50 18.00 31.00 37.50 39.00 55.00 31. 15.50 47.10 60.00 23.00 29.00 47.50 24.50 31.50 26.00 31.00 36.50 27.50 14.00 28.00 24.00 24.00 54.50 34.00 12.50 40.40 48.00 28.00 51.00 53.50 24.00 37.50 43.50 40.00 46.00 43.00 16.00 42.00 43.00 43.00 45.50 38. 16.00 46.90 56.50 65.59 77.21 84.72 74.50 53.01 79.80 80.80 78.22 83.37 80.90 82.54 81.71 81.71 73.82 78.23 78.19 81.97+3.78 85.02+6.83 72.92 83.88 88.38 85.04 39.98 85.21 84.02 75.73 90.12 88.15 87.87 87.33 87.54 83.31 84. 86.35 83.36 88.38 58.25 70.54 81.05 63.95 66.03 74.39 77.57 80.90 76.61 73.65 77.20 76.09 75.87 64.32 71.52 70.02 80.57 81.65 Table 2: Performance on BFCLv3 (last updated August 26, 2025). The best result within each category is highlighted in bold. The second best result is underlined. As shown in Table 2, on the BFCLv3 benchmark, FunReason-MT yields notable improvements on Qwen3-4BInstruct, raising the Multi-Turn score from 15.75 to 46.90 (+31.15) after SFT and to 56.50 (+40.75) after RL. Despite its 4B size, the FunReason-MT RL-trained model surpasses both strong open-source models (e.g., Kimi-K2-Inst, DeepSeek-R1) and leading close-source models (e.g., GPT-5, Gemini-2.5-Pro, Claude-Sonnet-4), achieving state-of-the-art results. In addition, FunReason-MT demonstrates balanced performance across all sub-metrics, indicating strong generalization and stability across reasoning dimensions. Overall, these results demonstrate that FunReason-MT effectively enhances multi-turn reasoning and tool-use 6 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling capabilities by utilizing high-quality synthesized data, resulting in consistent and reliable improvements for real-world function-calling tasks."
        },
        {
            "title": "5.3 Agentic Results\nTo investigate the transference of agentic capabilities inherent in the FunReason-MT, we evaluate our models\non the OOD BFCLv4 benchmark. Specifically, we assess models on Search and Memory subsets, showing the\nresults in Table 3.",
            "content": "Web Search Memory Overall Overall Base No Snippet Overall Models ToolACE-2-8B BitAgent-8B watt-tool-8B xLAM-2-3b-fc-r 14.83 8.24 6.30 7.42 9.00 4.00 4.00 1.50 Qwen3-4B-Instruct-2507 +FunReason-MT (SFT) +FunReason-MT (RL) 8.85 9.99+1.14 15.10+6. 5.00 9.00+4.00 16.00+11.00 13.00 4.00 7.00 1.00 4.00 12.00 20.00 5.00 4.00 1.00 2.00 6.00 6.00 12.00 KV 7.74 4.52 3.87 4.52 20.65 12.47 8.60 13.33 12.69 10.971.72 14.19+1.50 11.61 1.29 5.81 Vector Recursive Sum 18.06 20.65 13.55 10. 11.61 0.00 0.00 36.13 12.26 8.39 24.52 14.84 31.61 36.77 Table 3: Performance of on BFCLv4 (last updated August 26, 2025). On the BFCLv4 benchmark, the FunReason-MT model demonstrates significant performance gains over its base model, Qwen3-4B-Instruct-2507. SFT increases the Overall performance score from 8.85 to 9.99 (+1.14), while RL further boosts it to 15.10 (+6.25). The RL models strength is particularly evident in the Web Search Overall subset, where its score raises from 5.00 to 16.00 (+11.00), alongside an increase in the Memory Overall score from 12.69 to 14.19. Notably, the RL-trained FunReason-MT outperforms all other competing specialized models in the main Overall metric. The FunReason-MT model, particularly after being trained with reinforcement learning, demonstrates significant performance improvements on the BFCLv4 benchmark. This proves that data generated by this paradigm consistently enhances the models agentic capabilities. We believe this paradigm of procedural data generation process can lay strong foundation and provide robust initial knowledge base for future large-scale, environment-feedback based, agentic reinforcement learning."
        },
        {
            "title": "6 Conclusion",
            "content": "In this technical report, we introduce FunReason-MT, novel data synthesis framework designed to address critical bottlenecks in multi-turn function calling data generation. We identified that existing methods, such as random sampling and MAS role-playing, are insufficient, producing data that lacks diversity and fails to capture complex, logically-dependent trajectories. Our framework overcomes these limitations through three core components: Environment-API Graph Interactions for targeted scenario generation, Advanced Tool-Query Synthesis for creating logically complex queries, and Guided Iterative Chain for robust CoT generation. Empirical validation on the BFCLv3 benchmark demonstrates that our approach achieves state-of-the-art performance among comparable-sized models. Furthermore, our model shows promising out-of-distribution generalization on BFCLv4, underscoring the robustness of our method. This research paves the way for developing more robust and stable models capable of complex reasoning and environment exploration. 7 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling"
        },
        {
            "title": "References",
            "content": "Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, and Zhaozhuo Xu. Llm multi-agent systems: Challenges and open problems. arXiv preprint arXiv:2402.03578, 2024. Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie Gu, and Chenyi Zhuang. Exploring superior function calls via reinforcement learning. arXiv preprint arXiv:2508.05118, 2025a. Bingguang Hao, Maolin Wang, Zengzhuang Xu, Cunyin Peng, Yicheng Chen, Xiangyu Zhao, Jinjie Gu, and Chenyi Zhuang. Funreason: Enhancing large language models function calling via self-refinement multiscale loss and automated data refinement. arXiv preprint arXiv:2505.20192, 2025b. Chengrui Huang, Shen Gao, Zhengliang Shi, Dongsheng Wang, and Shuo Shang. Ttpa: Token-level tool-use preference alignment training framework with fine-grained evaluation. arXiv preprint arXiv:2505.20016, 2025. Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei Qin, Peifeng Wang, Silvio Savarese, et al. survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems. arXiv preprint arXiv:2504.09037, 2025. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482, 2024. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, et al. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. MAOLIN WANG, YINGYI ZHANG, CUNYIN PENG, YICHENG CHEN, WEI ZHOU, JINJIE GU, CHENYI ZHUANG, RUOCHENG GUO, BOWEN YU, WANYU WANG, et al. Function calling in large language models: Industrial practices, challenges, and future directions. 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Xingshan Zeng, Weiwen Liu, Lingzhi Wang, Liangyou Li, Fei Mi, Yasheng Wang, Lifeng Shang, Xin Jiang, and Qun Liu. Toolace-mt: Non-autoregressive generation for agentic multi-turn interaction. arXiv preprint arXiv:2508.12685, 2025. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for llms: survey. arXiv preprint arXiv:2509.02547, 2025. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling"
        },
        {
            "title": "A Supplementary Case",
            "content": "This Section details the case study for FunReason-MT. We describe the synthesis process and complete trajectory. Case: We showcase the construction of Hard Query with logical jump that directly enables ticket purchase, which involves jump for consulting the city zip code. Trace: get_zipcode(city=\"Rivermist\") output: \"83214\" get_zipcode(city=\"Stonebrook\") output: \"74532\" buy_tickets(zipcodeA=\"83214\", zipcodeB=\"74532\") output: \"ticket_id: 14589\" Advanced Tool: buy_tickets_adv(input_param: cityA, cityB, output: ticket_id, desc: purchase air tickets between two cities by city names, returning the purchased ticket information.) Hard Query: Please purchase air tickets between the city \"Rivermist\" and the city \"Stonebrook\". Hint: \"purchase air tickets between two cities by city names, returning the purchased ticket information\" is equal to \"retrieve the zipcode for given city\" and \"buy tickets from cityA to cityB with zipcode.\" Assistant: <think> Okay, so the user wants me to purchase air tickets between Rivermist and Stonebrook. Let me see how can approach this using the tools provided. First, need to understand the tools available. Theres get_zipcode which retrieves zipcode for given city, and buy_tickets which buys tickets using the zipcodes of the start and target cities. Hmm, the users query is about purchasing tickets between two cities, but the tools require zipcodes. So, cant directly buy tickets with city names; need to get their zipcodes first. Alright, step one: use get_zipcode for Rivermist. Ill call it with the city parameter as \"Rivermist\". This should give me the zipcode for Rivermist. Next, Ill do the same for Stonebrook. Another call to get_zipcode with \"Stonebrook\" as the city parameter. That should give me the zipcode for Stonebrook. Once have both zipcodes, can use the buy_tickets tool. Ill input the zipcodes of Rivermist and Stonebrook into buy_tickets to purchase the tickets. Wait, the output format the user provided is list of function calls in specific format. So need to structure my response with the function names and their parameters correctly. should make sure that each function call is properly formatted, especially the parameters. Since the city names are strings, Ill enclose them in quotes as per the users instructions. Let me double-check the parameters for each function. For get_zipcode, its just the city name. For buy_tickets, its both cityA_zipcode and cityB_zipcode. need to ensure that pass these correctly in the order specified. think Ive got the sequence right. First, get the zipcodes for both cities, then use those zipcodes to buy the tickets. That way, the tools are used in the correct order to achieve the users goal. should also consider if there are any edge cases, like if city doesnt have zipcode. But since the user provided specific cities, Ill assume the tools can handle them. Alright, putting it all together, Ill list the function calls in the required format, making sure each parameter is correctly specified with quotes around the string values. </think> <tool_call> [get_zipcode(city=\"Rivermist\"), get_zipcode(city=\"Stonebrook\"), buy_tickets(cityA_zipcode=\"12345\", cityB_zipcode=\"67890\")] </tool_call> 9 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling Turn: Query: Please purchase air tickets between the city \"Rivermist\" and the city \"Stonebrook\". Tools: get_zipcode, buy_ticket, . . . Assistant: <think> Okay, so the user wants me to purchase air tickets between Rivermist and Stonebrook. Let me see how can approach this using the tools provided. First, need to understand the tools available. Theres get_zipcode which retrieves zipcode for given city, and buy_tickets which buys tickets using the zipcodes of the start and target cities. Hmm, the users query is about purchasing tickets between two cities, but the tools require zipcodes. So, cant directly buy tickets with city names; need to get their zipcodes first. Alright, step one: use get_zipcode for Rivermist. Ill call it with the city parameter as \"Rivermist\". This should give me the zipcode for Rivermist. Next, Ill do the same for Stonebrook. Another call to get_zipcode with \"Stonebrook\" as the city parameter. That should give me the zipcode for Stonebrook. Once have both zipcodes, can use the buy_tickets tool. Ill input the zipcodes of Rivermist and Stonebrook into buy_tickets to purchase the tickets. Wait, the output format the user provided is list of function calls in specific format. So need to structure my response with the function names and their parameters correctly. should make sure that each function call is properly formatted, especially the parameters. Since the city names are strings, Ill enclose them in quotes as per the users instructions. Let me double-check the parameters for each function. For get_zipcode, its just the city name. For buy_tickets, its both cityA_zipcode and cityB_zipcode. need to ensure that pass these correctly in the order specified. think Ive got the sequence right. First, get the zipcodes for both cities, then use those zipcodes to buy the tickets. That way, the tools are used in the correct order to achieve the users goal. should also consider if there are any edge cases, like if city doesnt have zipcode. But since the user provided specific cities, Ill assume the tools can handle them. Alright, putting it all together, Ill list the function calls in the required format, making sure each parameter is correctly specified with quotes around the string values. </think> <tool_call> [get_zipcode(city=\"Rivermist\"), get_zipcode(city=\"Stonebrook\"), buy_tickets(cityA_zipcode=\"12345\", cityB_zipcode=\"67890\")] </tool_call> 10 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling Trajectory: We showcase two-turn complete trajectory. User: Determine the current working directory path, and then search for all files and directories in the current directory and its subdirectories. Assistant: <think> Okay, lets tackle the users request step by step. The user wants to determine the current working directory path and then search for all files and directories in the current directory and its subdirectories. First, need to figure out the current working directory. The pwd command is perfect for that. It doesnt require any parameters, so Ill call it first. That should give the user the path theyre in. Next, the user wants to search all files and directories in the current directory and subdirectories. The find function can do this. The find tool has parameters path and name. The default for path is \".\", which is the current directory. Since they want everything, the name parameter should be None (or omitted) to return all items. The default for name is already None, so just need to call find with the default parameters. Wait, the user specified \"its subdirectories\" so the recursive search is covered by the find tools description. So using find with path \".\" and no name should work. dont need to specify any parameters except maybe path, but since its default, its okay. So the function calls should be first pwd, then find. Let me check the parameters again. The find functions path defaults to \".\", which is correct. The name is optional and defaults to None. So the correct calls are [pwd(), find(path=.)]. need to make sure that the parameters are correctly formatted. Since path is set to \".\", even though its the default, including it explicitly might be better to ensure clarity. The user might expect the search to start at the current directory, so specifying path=\".\" is safe. No other functions are needed here. The ls command only lists the current directory, not subdirectories, so find is the right choice. Alright, that should cover the users needs. </think> <tool_call> [pwd(), find(path=\".\")] </tool_call> Tool: <tool_response> [pwd(): [\"./processed\", \"./processed/vqrqs3933.md\", \"./processed/akqff.md\", \"./processed/documents\", \"./requirements.txt\", \"./dqyiu_68828.json\", \"./data/metadata.json\", \"./data\", \"./data/vwccl.md\", \"./data/unggv.json\", \"./data/code\", \"./data/project\"]] </tool_response> Assistant: <think> Okay, lets see what the user needs here. The user asked to determine the current working directory and then search all files and directories in the current directory and subdirectories. First, the assistant already called pwd() and find(path=\".\") in the previous turn. The tool responses show that pwd() returned \"/workspace\" as the current directory. The find function with path \".\" gave list of matches including various files and directories under the current directory and its subdirectories. Now, the users requirement is to address their query in the next step. Since the user wanted both the current directory and search, the assistant already executed both functions successfully. The tool_response includes the path and the matches. The thought process should confirm that both functions were executed and their results are available. The reply should describe the results. Since the users needs were met, theres no need for further function calls. The assistant should present the findings, like stating the current directory and listing the found items. But according to the output format, the reply_to_the_user should either be function calls or description. Since the functions are already done, the reply should be description summarizing the results. Wait, the output format says if you need to give description, output sentence. So the assistant should say something like \"The current working directory is /workspace. The search found the following items: [list of matches].\" But the matches are in the tool_responses find result. However, the user might just need the info presented. But looking back at the instructions, in the reply_to_the_user section, if you need to give description, output sentence. Since the functions have been executed, the next step is to describe the results. So the assistant should format the answer with the current directory and the matches from find. Wait, but the example given in the instructions for reply_to_the_user when giving description is like \"the cd function is executed success, and current workdirctory is src.\" So in this case, the assistant should combine both results. So the thought process would note that both functions were executed successfully. The reply should state the current directory and list the found items. But since the matches are many, maybe just mention that the search found items \"/workspace\", find(path=\".\"): \"current_working_directory\": \"./data/raw/dtpsp.json\", \"./ulyuo.json\", \"./data/raw\", \"matches\": 11 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling The search found the following items: like those listed. Alternatively, the exact answer should reference the tool_response data. The current_working_directory is /workspace, and the matches array includes all those paths. So the assistant can present that info concisely. </think> <tool_call> The current working directory is /workspace. [\"./processed\", \"./processed/vqrqs3933.md\", \"./processed/akqff.md\", \"./processed/documents\", \"./requirements.txt\", \"./dqyiu_68828.json\", \"./ulyuo.json\", \"./data\", \"./data/raw\", \"./data/raw/dtpsp.json\", \"./data/metadata.json\", \"./data/vwccl.md\", \"./data/unggv.json\", \"./data/code\", \"./data/project\"].\" </tool_call> User: Display the contents of the file named unggv.json in the data directory, and then display the last 7 lines of the file named dtpsp.json, which is in the data directory of the raw directory. Assistant: <think> Okay, lets tackle the users latest request. They want to display the contents of \"unggv.json\" in the data directory and the last 7 lines of \"dtpsp.json\" located in the data/raw directory. First, need to check the current directory. From the previous tool response, the current working directory is \"/workspace\". The user mentioned \"data directory\", so \"unggv.json\" is directly under \"/workspace/- data/unggv.json\". But to access it, should use the cat command. However, cat requires the file to be in the current directory. Since the current directory is \"/workspace\", need to check if \"unggv.json\" is there. Wait, looking back at the find results, \"unggv.json\" is listed under \"./data/unggv.json\". So its in the data subdirectory. To access it, need to either change into the data directory first or specify the path, but the cat tool only allows the file name in the current directory. Therefore, should first cd into the data directory. Wait, the cd tool changes the directory, so first step: cd into data. Then, use cat unggv.json. After that, for the second part, the user wants the last 7 lines of dtpsp.json in data/raw. Since we are now in the data directory, the raw directory is subdirectory. So we need to cd raw to get into that directory, then use tail with lines=7 on dtpsp.json. But after that, we might need to go back, but the user didnt ask for that. Alternatively, maybe we can do it without changing directories again by specifying the path, but according to the tools parameters, tail only takes the file name in the current directory. Wait, the tail parameters say \"No path is allowed and you can only perform on file at local directory.\" So to access data/raw/dtpsp.json, we need to be in the data/raw directory. So the steps would be: 1. Current directory is /workspace. First, to access unggv.json in data/, do cd data, then cat unggv.json. 2. Then, to get into data/raw, do cd raw, then tail(file_name=\"dtpsp.json\", lines=7). But after the first cd data, the current directory becomes /workspace/data. Then after cd raw, it becomes /workspace/data/raw. Then after those commands, the user might want to return, but since its multi-step process, the assistant should just proceed step by step. However, the users request is to first display unggv.json and then dtpsp.json. So the function calls would be: First, cd into data: [cd(folder=\"data\")] Then, cat unggv.json: [cat(file_name=\"unggv.json\")] Then, cd into raw: [cd(folder=\"raw\")] Then, tail dtpsp.json with 7 lines: [tail(file_name=\"dtpsp.json\", lines=7)] But after these steps, the current directory is now data/raw. The user might need to go back, but since the user didnt ask, maybe its okay. Alternatively, maybe theres another way without changing directories multiple times, but according to the tool constraints, its necessary. Wait, but the find result shows that dtpsp.json is in \"./data/raw/dtpsp.json\". So the path from the current directory (which is /workspace) would require navigating into data/raw. So the steps are necessary. Therefore, the function calls should be in the order: 1. cd into data 2. cat unggv.json 3. cd into raw 4. tail dtpsp.json with lines=7 So the function calls would be: [cd(folder=\"data\"), cat(file_name=\"unggv.json\"), cd(folder=\"raw\"), tail(file_name=\"dtpsp.json\", lines=7)] But need to make sure each step is correct. Let me check each functions parameters again. For cd, the parameter is folder, which is the directory to change to. So first cd to \"data\", then from there cd to \"raw\" (since current is now data/). The cat requires the file name in current directory. After cd data, unggv.json is there. Then, after cd raw, dtpsp.json is in current directory. So yes, that works. Therefore, the correct function calls are as above. </think> <tool_call> [cd(folder=\"data\"), cat(file_name=\"unggv.json\"), cd(folder=\"raw\"), tail(file_name=\"dtpsp.json\", lines=7)] </tool_call> Tool: <tool_response> 12 FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling [cd(folder=\"data\"): \"current_working_directory\": \"data\", cat(file_name=\"unggv.json\"): \"file_content\": \" \"data\": \"id\": \"94\", \"type\": \"product\" \", cd(folder=\"raw\"): \"current_working_directory\": \"raw\", tail(file_name=\"dtpsp.json\", lines=7): \"last_lines\": \" \"data\": \"id\": \"27\", \"type\": \"user\" \"] </tool_response> Assistant: <think> Okay, lets see what the user is asking for now. The users latest query was to display the contents of \"unggv.json\" in the data directory and the last 7 lines of \"dtpsp.json\" located in the data/raw directory. Looking at the previous interactions, the assistant first used pwd() and find() to get the current directory and list all files. Then, the users next request required navigating into the data directory with cd(\"data\"), then using cat on unggv.json. After that, they changed to the raw directory with cd(\"raw\") and used tail on dtpsp.json with lines=7. The tool responses show that all these commands were executed successfully. The current working directory was first changed to \"data\", then to \"raw\". The cat and tail commands both returned the expected file contents. Since all the functions called in the assistants response have been executed without errors, theres no need to repeat any commands. The users requirements have been fully met. Now, the assistant should present the results of these commands to the user in clear manner, mentioning the contents of unggv.json and the last lines of dtpsp.json as per the tool responses. </think> <answer> The contents of \"unggv.json\" in the data directory are: \" \"data\": \"id\": \"94\", \"type\": \"product\" \". The last 7 lines of \"dtpsp.json\" in the data/raw directory are: \" \"data\": \"id\": \"27\", \"type\": \"user\" \".\" </answer>"
        }
    ],
    "affiliations": [
        "AWorld Team, Inclusion AI",
        "City University of Hong Kong",
        "Zhejiang University"
    ]
}