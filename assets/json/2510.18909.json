{
    "paper_title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection",
    "authors": [
        "Hongyi He",
        "Xiao Liu",
        "Zhenghao Lin",
        "Mingni Tang",
        "Yi Cheng",
        "Jintao Wang",
        "Wenjie Li",
        "Peng Cheng",
        "Yeyun Gong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "High-quality pre-training data is crutial for large language models, where quality captures factual reliability and semantic value, and diversity ensures broad coverage and distributional heterogeneity. Existing approaches typically rely on single or multiple-dimensional score-based selection. However, directly selecting top-scored data often degrades performance, and sampling from a broader range is required to recover results. The above non-monotonicity between dataset scores and downstream benchmark results reveals a fundamental bias: score-based methods collapse correlated dimensions, causing top-scored data to appear high-quality while systematically overlooking diversity. We argue that ensuring diversity requires decomposing correlated metrics into orthogonal feature dimensions, from which the top-scored data can be directly selected. Therefore, we proposed the Orthogonal Diversity-Aware Selection (ODiS) algorithm, which preserves both quality and diversity during data selection. First, ODiS evaluates data from multiple dimensions, covering language quality, knowledge quality, and comprehension difficulty. The multi-dimensional scores are then decorrelated via Principal Component Analysis (PCA), yielding orthogonal evaluation dimensions. For each dimension, a Roberta-based scorer is trained to regress the data onto PCA-projected scores, enabling scalable inference on large corpora. Finally, ODiS constructs the training dataset by selecting top-scored data within each orthogonal dimension, thereby ensuring both quality and diversity. Empirical results show that ODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming orthogonality between dimensions. More importantly, models trained with ODiS-selected data significantly outperform other baselines on downstream benchmarks, highlighting the necessity of orthogonal, diversity-aware data selection for LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 0 9 8 1 . 0 1 5 2 : r Preprint LEARNING FROM THE BEST, DIFFERENTLY: DIVERSITY-DRIVEN RETHINKING ON DATA SELECTION Hongyi He1* Xiao Liu2 Zhenghao Lin2 Mingni Tang3* Yi Cheng3* Jintao Wang1 Wenjie Li3 Peng Cheng2 Yeyun Gong2 1 Tsinghua University, 2 Microsoft Research, 3 The Hong Kong Polytechnic University hehy22@mails.tsinghua.edu.cn, {xiaoliu2,zhenghaolin,pengc,yegong}@microsoft.com, wangjitao@tsinghua.edu.cn, {minnie17.tang,alyssa.cheng}@connect.polyu.hk, cswjli@comp.polyu.edu.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "High-quality pre-training data is decisive factor for large language models, where quality captures factual reliability and semantic value, and diversity ensures broad coverage and distributional heterogeneity. Existing approaches typically rely on single or multiple-dimensional score-based selection. However, empirical studies have shown that directly selecting top-scored data often degrades downstream performance, and sampling from broader range is required to recover results. The above non-monotonicity between the dataset scores and the downstream benchmark results reveals fundamental bias: score-based methods collapse correlated dimensions, causing top-scored data to appear high-quality while systematically overlooking diversity. We argue that ensuring diversity requires decomposing correlated evaluation metrics into orthogonal feature dimensions, from which the top-scored data can be directly selected. To this end, we proposed the Orthogonal Diversity-Aware Selection (ODiS) algorithm, method to preserve both quality and diversity during high-quality data selection. First, ODiS evaluates data from multiple dimensions, covering language quality, knowledge quality, and comprehension difficulty. The resulting multi-dimensional scores are then decorrelated via Principal Component Analysis (PCA), yielding orthogonal evaluation dimensions. For each dimension, Roberta-based scorer is trained to regress the data onto PCA-projected scores, enabling scalable inference on large corpora. Finally, ODiS constructs the training dataset by selecting top-scored data within each orthogonal dimension, thereby ensuring both quality and diversity. Empirical results show that ODiS-selected data exhibit less than 2% inter-dimension overlap, confirming the orthogonality between dimensions. More importantly, models trained with ODiS-selected data significantly outperform other baselines on multiple downstream benchmarks, highlighting the necessity of orthogonal, diversity-aware data selection for LLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Pretraining is the primary stage for models to acquire fundamental abilities, such as language understanding, text generation and information extraction (Brown et al. (2020); Chowdhery et al. (2023); Roberts et al. (2020)). These capabilities are largely determined by the quality and diversity of the training data. Quality captures authenticity, reliability, and semantic integrity, ensuring that models learn accurate and well-structured knowledge. Diversity, on the other hand, emphasizes coverage and comprehensiveness, enabling models to generalize across domains and tasks. With the increase of both model and corpus sizes, designing efficient data selection methods that jointly account for these two aspects has become critical challenge for advancing model performance. *Work done during their internships at Microsoft Research. Corresponding authors. 1 Preprint Existing works have proposed diverse methods for selecting data based on quality and diversity. Quality-based methods typically utilize rule-based heuristics (Laurencon et al. (2022); Weber et al. (2024); Penedo et al. (2023); Raffel et al. (2020); Lee et al. (2021)), such as document length constraint and content deduplication, or score-based techniques (Wenzek et al. (2019); Touvron et al. (2023); Wettig et al. (2024); Penedo et al. (2024); Su et al. (2024)), where classifiers or perplexity models assign single quality score to filter noisy or irrelevant data. Diversity-based methods (He et al. (2024); Zhang et al. (2024); Tirumala et al. (2023); Yang et al. (2025)) instead focus on broadening coverage by mixing data across domains or clustering in the embedding space, thereby reducing redundancy and expanding the distribution. Recent works also attempt to combine quality and diversity to select data (Zhuang et al. (2025); Liu et al. (2025); Bai et al. (2025)), typically formulating both diversity and quality into multi-dimensional score or mixing selected data from different domains. However, the intrinsic correlation between dimensions makes weight tuning challenging, and naive aggregation often results in overlapping signals. In summary, current works encounter three key challenges: (1) The data selected with top scores are not always optimal, and sampling is necessary to achieve satisfactory performance, but the cause remains unexplored. (2) Scored-based methods combine multiple aspects into one-dimensional signal, making them unable to capture both quality and diversity. (3) Delicate hyper-parameter tuning is required to balance the influence from different dimensions, undermining generality and practical deployment of the methods. To address these challenges, we first revisit the problem of data selection through the lens of bias and correlation, identifying the neglect of diversity as the fundamental cause. Guided by the insight, we propose Orthogonal Diversity-Aware Selection (ODiS) algorithm, method to effectively construct dataset with both quality and diversity. Specifically, drawing inspiration from (Zhuang et al. (2025); Wettig et al. (2024)), we label reference dataset from 11 dimensions, covering four main categories: language quality, knowledge quality, comprehension difficulty, and information quality. After analyzing the correlation across the dimensions, we reveal strong entanglement between them, which will introduce redundancy and bias the score-based selection. To mitigate this, we apply Principal Component Analysis (PCA) to scores and derive orthogonal evaluation dimensions. For acceleration and scalability, Roberta-based scorers are trained to predict scores along each PC dimension on the target dataset. Finally, ODiS constructs the training set by selecting the top samples from orthogonal PC dimensions, ensuring both quality and diversity of the dataset. Empirical validations demonstrate that the model trained on data selected by the proposed ODiS methods achieves the best in various downstream benchmarks, compared with existing baselines DSIR, PPL, and Nemotron-CC. We analyze the source of performance gains by comparing topscored data with samples drawn from broader ranges of each dimension, and find that top-scored subsets consistently underperform, whereas combining data across dimensions substantially improves performance. Data analysis further confirms the strong dimension correlations before PCA and demonstrates that orthogonal principal components capture distinct aspects of the data, thereby validating the data quality and diversity in the selected dataset. Finally, the ablation study suggests that increasing the number of dimensions will marginally improve performance, indicating an efficiency-performance trade-off. The main contributions of this work are as follows: (1) We provide the first analysis about performance degradation of top-scored data, identifying neglected diversity as the underlying cause. (2) We propose the ODiS algorithm, score-based data selection algorithm that explicitly ensures both quality and diversity through dimension decomposition. (3) The analysis results reveal that ODiS benefits from dimension decomposition and enhanced data diversity, which sheds light on future data selection methods for reducing inter-dimensional correlations to improve data diversity."
        },
        {
            "title": "2 METHOD",
            "content": "2.1 SCORE-BASED BIAS IN DATA SELECTION We begin by analyzing the causes of selection bias and the role of data sampling through examining model performance with varying data sizes. Following Wettig et al. (2024), we establish multiple metrics targeting different semantic features. Using these metrics, we utilize score-based method to select subset of data from the Nemotron-CC dataset (Su et al. (2024)). Specifically, we filter the top-scored data at different scales, ranging from 100B to 900B tokens. After that, we train 1.5B-parameter model from scratch on these subsets, with 100B tokens training data budget. Fi2 Preprint nally, model performance is evaluated across multiple benchmarks. The details of the evaluation metrics, training details, and benchmark selection are provided in Sections 2.3, 3.1, and 3.1, respectively. From Figure 1, we can observe that the data with the highest score performs the worst, (a) Arc-Easy (b) Arc-Challenge (c) Hellaswag (d) PIQA (e) SCIQ Figure 1: Performances across downstream tasks. while sampling data from broader range leads to improvements. This reveals non-monotonic relationship between the data score and model performance, which complicates data selection: the sampling range and other potential hyperparameters should be optimized accordingly. Since the topscored data has the highest quality, we further investigate its diversity through an embedding-based visualization to determine the reason behind the non-monotonicity. We sample the data from different sizes of data pools, and visualize the UMAP projection of text embeddings together with the distribution of pairwise distances. As shown in Figure 2a, larger data subset spreads more broadly in the compressed dimension. Moreover, Figure 2b demonstrates that the pairwise distance increases with the data size. These results suggest that the increase in data size widens the distinction between data and amplifies data diversity, whereas top-scored data is relatively homogeneous, which explains the performance degradation of the top-scored data. (a) UMAP projection of text embeddings. The embeddings are reduced to two dimensions using UMAP. (b) Pairwise distance distribution of text embeddings. The distances are computed with cosine distances. Figure 2: Visualization of data distribution from different data pool sizes. The embeddings are generated using 1000 randomly sampled texts with the m3e-base model, followed by L2 normalization and removal of the top 3 principal components to suppress dominant common directions. 2.2 PROBLEM FORMULATION Motivated by the previous analysis in Section 2.1, we focus on enhancing data diversity while ensuring data quality during data selection. Our objective is to select the most valuable subset of large corpus to facilitate the model pre-training. Instead of directly processing the target dataset Dt, we first take smaller reference dataset Dr to generate the scoring strategy. Specifically, each data xi in the reference dataset Dr = {xi}N i=1 will be labeled from dimensions, whose score vector can be expressed as α(i) = (α(i) ). The goal is to design mapping function () that transforms the scores α(i) into ranking, through which we can directly select the top-scored data. The function should be designed such that training on the top-scored data maximizes the performance of downstream tasks. The problem can be written as follows: 2 , , α(i) 1 , α(i) Ds = arg max DsDt G(Ds), (1) 3 Preprint where G() denotes evaluation performance on benchmarks tasks. 2.3 MULTI-DIMENSION DATA EVALUATION Instead of directly optimizing the data selection for downstream tasks, which may be biased toward task-specific signals, we propose method that focuses on enhancing data quality and diversity for general purposes. To comprehensively evaluate each data document, we set up 11 dimensions, i.e., = 11, for four general aspects: Language quality, Knowledge quality, Comprehension difficulty, and Information Quality. Without loss of generality, we still use to denote the number of dimensions in the algorithm. We briefly describe the dimensions as follows, and the details are in Appendix B: Language quality. We prefer data that is (i) coherent in structure, (ii) concise without redundancy, and (iii) correct in spelling/grammar and word choice (Penedo et al. (2023)). Knowledge quality. We value content with (i) sufficient coverage and (ii) depth, (iii) useful reasoning signals, (iv) clear educational, and (v) practical value (Gunasekar et al. (2023); Guo et al. (2025)). Comprehension difficulty. We assess the difficulty level, i.e., conceptual complexity and domain professionalism, as higher difficulty can improve generalization (Agrawal & Singh (2023)). Information quality. We require (i) factual accuracy and (ii) sufficient completeness so models can learn reliable, fully specified facts (Chang et al. (2024)). Each document is assigned score from 0 to 5 on every dimension using the OpenAI GPT API. Detailed definitions of the metrics and prompt are provided in Appendix C. The resulting scores constitute matrix X: = α(1) 1 ... α(N ) 1 α(1) ... . . . α(N ) RN m. (2) 2.4 ORTHOGONAL DIVERSITY-AWARE SELECTION Dimension decomposition via PCA. Previous studies have shown that different dimensions often exhibit correlations(Zhuang et al. (2025)), i.e., the data with higher knowledge depth may have less knowledge richness, while the data with high educational value usually achieves high information quality. Such redundancy in the raw labeled score reduces effective data diversity and hinders data selection. Therefore, instead of directly using the raw labeled scores, we will transform the scores to eliminate the potential correlation between different dimensions, which is done through principal component analysis (PCA). To eliminate the scale difference, we first calculate the mean of each dimension µ, and obtain the data matrix Xc centered with the mean: µ = 1 (cid:88) i=1 α(i), Xc = µ. After that, we compute the covariance matrix Σ and adopt eigen decomposition to Σ: Σ = ΛV , Σ = 1 1 Xc Xc Rmm, (3) (4) where = [v1, , vm] are orthogonal eigenvectors and Λ = diag(λ1 λm 0) are eigenvalues. Each eigenvector represents an orthogonal combination of the original metrics, with the eigenvalue quantifying the variance contribution, i.e., the proportion of the feature representation that the eigenvector accounts for. The higher eigenvalue indicates that the data spreads more along this direction, and the eigenvector contains more representative feature. Score transformation. To reduce cost and improve efficiency, we take the first principal comτ, where τ ponents (PC) to satisfy the explained-variance ratio, rather than all the PCs: (cid:80)K (cid:80)m k=1 λk k=1 λk 4 Preprint Algorithm 1 Orthogonal Diversity-Aware Selection Input: Reference dataset Dr, target dataset Dt, dimensions for evaluation, data budget Output: Selected dataset Ds 1: For each xi Dr, obtain the dimensional score vector α(i) = (α(i) 2: Compute the mean vector µ and construct the centered matrix Xc as equation 3; 3: Compute the covariance matrix Σ, and perform eigendecomposition to obtain eigenvalues λi 2 , , α(i) ); 1 , α(i) and eigenvectors vi; 4: Determine the number of principal components (PCs) with the threshold τ ; 5: Construct the project matrix WK, and project scores into the orthogonal space β(i) = WT K(α(i))T RK; 6: Allocate budget sk to each PC dimension such that (cid:80)K 7: for = 1, . . . , do 8: k=1 sk = s; Train RoBERTa-based scorer rk() [0, 5] by regressing text xi Dr to the PCAtransformed scores β(i) ; 9: Apply rk() to obtain predicted scores {θ(i) Given the budget sk, determine threshold tk and select Dk } for all xi Dt; 10: 11: end for 12: Construct the final selected dataset Ds = k=1Dk . = {xi θ(i) > tk, xi Dt}; is the threshold. Then, we can obtain the project matrix: WK = [v1, , vK] RmK. Through K(α(i))T RK. Note projection, the compressed score vector for xi is calculated as: β(i) = WT that the principal component is linear combination of the original dimensions, and it is difficult to interpret its meaning. Since the dimensions are orthogonal, we have decomposed each metric, and the score in each principal component represents the quality in this new dimension. Roberta-enabled model-based scorer. To enhance labeling accuracy and capture the semantic feature of the principal component, we train Roberta-based scorer rk() to map the original text to the PCA-derived score for each PC dimension. The scorer rk() will regress from the data xi from the reference dataset and the transformed score β(i) . The Roberta-based scorer enables efficient inference on unseen data, preserves semantic richness, and provides unified and noise-robust scoring framework for large-scale data selection. We then label the target dataset Dt with scorer rk() to obtain the scores θ(i) , 1, , K, {1, , Dt} Dataset construction based on scores. We allocate data budget sk to each PC dimension considering its contribution and the total data budget: = (cid:80)K k=1 sk. For each dimension, score threshold tk is set based on sk, and the corresponding subset is denoted as Dk . Since labeling and scorer training inevitably introduce noise, the top-scored data across each orthogonal dimension may still overlap, and directly merging the subsets will result in duplication. To address this, we apply joint score threshold = (t1, , tK) and select data within the target dataset xi Dt if > tk, 1, , K. The final selected data is obtained as the union: Ds = (cid:80)K θ(i) . This construction ensures that each dimension contributes its highest-quality data, while the orthogonality of PC dimensions guarantees enhanced diversity in the resulting dataset. k=1 Dk"
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EXPERIMENT SETUP Dataset. We use Nemotron-CC dataset (Su et al. (2024)) as the data pool for selection. It is largescale Chinese dataset for pretraining large language models. The dataset comprises both real-world and synthetic data, covering major domains, such as web knowledge and question answering. For tokenization, we use the LLaMA-3-8B tokenizer, which has vocabulary size of 128,256, and set the maximum sequence length to 4096. Evaluation. We evaluate the model performance with lm-eval-harness framework (Gao et al. (2024)). We first monitor task-level performance fluctuations across training steps, with detailed re5 Preprint sults presented in Appendix D. Based on the above result, we follow the fine task metric (Kydlıˇcek et al.) to select downstream tasks with performance that varies significantly as training progresses. We select five tasks covering main categories: General Knowledge (including Arc-Easy/Challenge (Clark et al. (2018))), Commonsense Reasoning (including Hellaswag (Zellers et al. (2019)), SIQA (Sap et al. (2019))), and Physical Reasoning (PIQA (Bisk et al. (2020))). Training. Each experiment is conducted under data budget of 100B tokens. Unless specified in the caption, the top selection represents directly selecting 100B token dataset for training, while sample selection represents selecting 700B token dataset and sampling training data from it. We employ decoder-only model with 1.5 billion parameters and train it from scratch with the selected data. Training uses global batch size of 512 and the AdamW optimizer (Loshchilov & Hutter (2019)), with peak learning rate of 3e-4, cosine decay scheduling, and linear warmup. Unless specified, the proposed ODiS method selects data from the first 4 PC dimensions and allocates the data budget evenly to each dimension. 3.2 MAIN RESULTS The main results are summarized in Table 1. We compare the proposed ODiS method with existing baselines, including DSIR (Xie et al. (2023)), PPL (Ankner et al. (2024)), and Nemotron-CC (Su et al. (2024)). The results show that the model trained on data selected by ODiS achieves the highest performance compared with the baselines. Specifically, ODiS achieves generally 3-point marginal improvement compared with the random sampling in average accuracy. Notably, it surpasses all the methods across all task categories, highlighting its versatility in addressing wide range of downstream tasks. The performance gains are typically obvious in Arc-E and Arc-C, indicating their enhancement in general knowledge and content diversity. In contrast, baseline methods such as PPL and DSIR emphasize only data quality, which limits diversity and hampers overall performance. These results demonstrate the importance of data diversity during data selection, and decomposition is direction for efficient diversity improvement. Method Random Selection Nemotron-HQ PPL-Top PPL-Sample DSIR PC Average-Top PC Average-Sample ODiS Arc-C 0.3503 0.3725 0.3788 0.3609 0.2782 0.3532 0.3916 0.4155 Arc-E Hellaswag SIQA 0.6273 0.6463 0.6284 0.6431 0.4848 0.6481 0.5791 0.6694 0.5825 0.5774 0.5469 0.5837 0.5457 0.5332 0.5892 0.5842 0.855 0.839 0.834 0.858 0.785 0.719 0.806 0. PIQA 0.7448 0.7356 0.7465 0.7481 0.7095 0.7448 0.7514 0.7740 Average 0.6320 0.6341 0.6269 0.6388 0.5606 0.5859 0.6373 0. Table 1: Performance across data selection methods. The random selection method samples data from the whole Nemotron-CC dataset, while the Nemotron-HQ method samples from the NemotronCC HQ subset. The PC Average baseline utilizes the averaged scores from PC1 to PC4. 3.3 ANALYSIS RESULT 3.3.1 INSPECTION OF DATA BIAS ODiS mitigates data bias. Figure 3a shows that models trained with top-scored data within single dimension consistently underperform, while sampling from broader score range yields significant performance gains. This observation highlights the inherent bias in relying exclusively on single-dimension scoring, where overemphasis on one metric neglects complementary aspects of data quality and diversity. Furthermore, Figure 3b illustrates that averaging scores from multiple dimensions only partially alleviates the data bias issue, as correlation across dimensions discussed in Section 3.3.2 continues to bias the data selection. In contrast, ODiS decomposes the dimensions into orthogonal components and selects high-quality data from each dimension, effectively mitigating data bias and enhancing diversity. Notably, ODiS achieves superior performance with the smallest subset of data, avoiding both excessive sampling ranges and unnecessary data waste. Preprint (a) Performance with data selected from PCs (b) Performance across selected data pools Figure 3: Model performance under different selection methods and ranges. ODiS effectively enhances data diversity. To validate the diversity gain from ODiS, we compare the ODiS-selected data against score-based baselines. Figure 4 shows that the data selected by ODiS has significantly larger average pairwise distance, even compared with larger selected data pool, indicating enhanced data diversity. Similarly, UMAP visualization reveals that the ODiS-selected data spans wider region in the compressed space, while the data selected with baseline methods tend to cluster around narrower region. These results suggest that ODiS reduces redundancy and captures wider range of semantic features during data selection. (a) Pairwise distance distribution (b) UMAP projection of text embeddings Figure 4: Data diversity visualization from different selection methods. 3.3.2 CORRELATION BETWEEN DIMENSIONS Correlation of original dimensions. Figure 5a describes the correlation coefficient across 11 dimensions with scores using 460k examples from Fineweb-Edu dataset. We observe that most dimensions exhibit weak correlation (correlation coefficient < 0.5), suggesting that the metrics capture distinct aspects of the data. Nonetheless, we can still discover moderate correlation between the dimensions, such as knowledge depth vs. knowledge richness, and completeness vs. knowledge richness, indicating partial overlaps in their coverage. Moreover, nearly all the dimensions exhibit at least some degree of correlation with one another. These results suggest that although different metrics may appear conceptually orthogonal from human perspective, they are not strictly independent in practice or from the models viewpoint, validating the necessity of dimension decomposition. Correlation between original dimensions and principal components. After applying PCA to transform dimensions into orthogonal principal components, Figure 5b describes the correlations between PC dimensions and original dimensions. Each PC exhibits strong correlation with subset of the original dimensions, indicating that they can represent meaningful information or semantic features. For example, PC1 aligns strongly with knowledge quality and comprehension difficulty, highlighting its central role in characterizing overall data quality. By contrast, language-related dimensions exhibit weaker direct alignment with the leading PCs. This observation indicates that the linguistic factors may already be embedded within other correlated dimensions, e.g., knowledge quality, and thus they do not emerge as dominant signals in the first few components. More generally, the orthogonal principal components are linear combinations of the original dimensions. They should not be interpreted as single semantic dimensions, but rather as abstract features that combine Preprint multiple correlated attributes and capture salient aspects of the dataset from the models perspective. (a) Correlation matrix between different dimensions (b) Correlation matrix between original and PC dimensions Figure 5: Correlation analysis of evaluation dimensions. The correlations are calculated using 460k samples from the Fineweb-Edu dataset. Pairwise correlation coefficients are visualized as heatmap. 3.3.3 ORTHOGONALITY BETWEEN DIMENSIONS To assess the distinctiveness of different PC dimensions, we visualize the embeddings of top-scored data from each dimension using UMAP projection and an Upset plot, as shown in Figure 6. The UMAP visualization in Figure 4b reveals that samples from different dimensions occupy separable regions, which validates the orthogonality between dimensions and suggests that combining data from different dimensions can yield complementary data subsets. Figure 6b illustrates the marginal intersection of the data from different PC dimensions, with an overlapping ratio of 2% after tokenization, further confirming the effectiveness of the dimension decomposition. (a) UMAP for embeddings. The embeddings are generated through 1000 sampled top-scored data from each PC dimension. (b) UpSet plot of top-scored subsets. Data selected with top scores of each dimension are shown, with intersections indicating data that simultaneously belong to multiple top-scored subsets. Figure 6: Data orthogonality from different PC dimensions. The data is top-scored tokens from each PC dimension, which constitutes 100B tokens dataset. 3.3.4 SCALING WITH MORE DIMENSIONS To examine whether increasing the number of PCs improves performance, we conduct an ablation study with varying PC dimensions, as summarized in Table 2. The results show that the performance improves steadily up to four dimensions, after which the performance gain becomes saturated. Beyond four PCs, additional dimensions contribute marginally to the model performance, likely because lower-variance PCs contain less information or noisy signals. Additionally, selecting more PCs will introduce increased computational cost, as labeling large corpus is both computationally and time-consuming. These findings suggest that small set of carefully selected orthogonal dimensions is sufficient for robust data selection, striking balance between efficiency and effectiveness. 8 Preprint Method PC1 PC1-2 PC1-3 PC1-4 PC1-5 PC1-6 Arc-C 0.3635 0.3575 0.3635 0.4053 0.3938 0. Arc-E 0.6035 0.6107 0.6090 0.6570 0.6692 0.7101 Hellaswag SIQA 0.4309 0.5372 0.5936 0.5741 0.5689 0.5606 0.811 0.830 0.791 0.872 0.880 0. PIQA 0.6436 0.7514 0.7748 0.7715 0.7669 0.7598 Average 0.5705 0.6174 0.6264 0.6559 0.6557 0.6654 Table 2: Performance across varying PC numbers. The data budget is set at 100B tokens for all experiments, and we evenly distribute the budget across each dimension."
        },
        {
            "title": "4 RELATED WORKS",
            "content": "With the increasing scale of both model and corpora size, there is growing demand for efficient methods to select high-quality pretraining data. The quality and diversity are two key considerations during data selection. Existing data selection methods have been proposed to enhance data quality and diversity through three main directions: non-classifier-based methods, single-classifier-based methods, and multi-classifier-based methods. Non-classifier-based methods. Various works have relied on rule-based filtering with explicit heuristics or deterministic criteria (Laurencon et al. (2022); Weber et al. (2024); Penedo et al. (2023); Raffel et al. (2020); Lee et al. (2021)), including language identification, URL blocks, content de-duplication, and document length thresholds. Moreover, these approaches are combined into multi-stage pipelines that sequentially perform cleaning, deduplication, quality, and safety filtering (Nguyen et al. (2023)). While rule-based methods effectively improve data quality and reduce noisy data, they fail to inspect semantic-level information and will introduce distribution bias. Single-classifier-based methods. In contrast to the rule-based method, it utilized learned scoring function or discriminator to label and filter out high-quality data (Wenzek et al. (2019); Touvron et al. (2023); Wettig et al. (2024); Penedo et al. (2024); Su et al. (2024); Wang et al. (2025)). Among them, language modeling perplexity has been adopted to identify data with high quality (Wenzek et al. (2019); Touvron et al. (2023)). Methods like QuRating (Wettig et al. (2024)) and FineWebEdu (Penedo et al. (2024)) utilize classifiers that focus on specific aspects of LLM capabilities, such as reading comprehension and knowledge acquisition. Moreover, works like Ultra-FineWeb (Wang et al. (2025)) and DSIR (Xie et al. (2023)) utilize target dataset to guide the classifier in predicting the quality of the data. Although methods with single classifier can effectively filter out data with certain desirable features, their reliance on single evaluation dimension limits data diversity and often leads to imbalanced capabilities in the trained models. Multi-classifier-based methods. More recent works have attempted to incorporate multidimensional evaluation during data selection. Compared with single-dimensional methods, the combination of multi-dimensional classifiers can provide more comprehensive evaluation of data. However, it remains challenge to balance the influence from different dimensions. One line of the research typically assigns weights to each dimension through performance tests on small proxy models (Zhuang et al. (2025); Bai et al. (2025)). However, the dimensional correlations are not well-addressed, resulting in bias in the combined scores and reduced data diversity. Another line of the research design sampling ratios for different domains (Liu et al. (2025)) to ensure data diversity. However, the inherent overlapping of the domains will still lead to bias in the selected data. As result, the traditional top-k method is ineffective in the multi-dimensional setting, leaving the question of integrating multi-dimensional evaluations open."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we investigated the underlying cause of bias in score-based data selection and identified that neglecting diversity leads to non-monotonicity between the dataset scores and model performance. To address this issue, we proposed ODiS, method that explicitly mitigates the correlation between different data features while ensuring data diversity and retaining high-quality data. The experiment results demonstrated that ODiS can effectively mitigate inter-dimensional correlation, enhance data diversity, and consistently improve model performance across various downstream 9 Preprint tasks compared to several baselines. These findings highlight that effective data selection for pretraining models should consider quality and diversity jointly, and the correlation between different dimensions must be addressed appropriately. Looking forward, we encourage future data selection works to consider the neglected diversity as cause of performance degradation and adopt appropriate measures to enhance data diversity. 10 Preprint"
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The authors confirm that this work adheres to the ICLR Code of Ethics. Our research was conducted in accordance with recognized ethical standards, and we have carefully examined the societal, environmental, and potential misuse implications of our contributions."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "The authors have made extensive efforts to ensure the works Reproducibility, including datasets, evaluation metrics, methodology, models. The details of the datasets used in this work are all open-sourced, and we have described them in the Section 3.1. The evaluation metrics and prompt for the dimensions can be found in Appendix and Appendix C. The details of the evaluation benchmarks are described in Section 3.1. The proposed method is described in detail, and the pseudocode is provided. All the implementation details are provided during the description. We utilized the LLaMA-3 model as the base model and adjusted the parameters to obtain 1.5B-parameter model for training. Besides, the Roberta model can be obtained on the HuggingFace website. All the experimental results are reproducible, and we have averaged the results from multiple experiments to ensure an accurate result. We believe these detailed descriptions are sufficient to reproduce our results."
        },
        {
            "title": "REFERENCES",
            "content": "Ameeta Agrawal and Suresh Singh. Corpus complexity matters in pretraining language models. In Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP), pp. 257263, 2023. Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew Leavitt, and Mansheej Paul. Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprint arXiv:2405.20541, 2024. Tianyi Bai, Ling Yang, Zhen Hao Wong, Fupeng Sun, Xinlin Zhuang, Jiahui Peng, Chi Zhang, Lijun Wu, Qiu Jiantao, Wentao Zhang, et al. Efficient pretraining data selection for language models via multi-actor collaboration. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94659491, 2025. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, and Minjoon Seo. How do large language models acquire factual knowledge during pretraining? Advances in neural information processing systems, 37:6062660668, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang 11 Preprint Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Nan He, Weichen Xiong, Hanwen Liu, Yi Liao, Lei Ding, Kai Zhang, Guohua Tang, Xiao Han, and Wei Yang. Softdedup: an efficient data reweighting method for speeding up language model pre-training. arXiv preprint arXiv:2407.06654, 2024. Hynek Kydlıˇcek, Guilherme Penedo, Clementine Fourier, Nathan Habib, and Thomas Wolf. tasks. URL https:// Finetasks: Finding signal huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks. in haystack of 200+ multilingual Hugo Laurencon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonzalez Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:3180931826, 2022. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021. Fengze Liu, Weidong Zhou, Binbin Liu, Zhimiao Yu, Yifan Zhang, Haobin Lin, Yifeng Yu, Bingni Zhang, Xiaohuan Zhou, Taifeng Wang, et al. Quadmix: Quality-diversity balanced data selection for efficient llm pretraining. arXiv preprint arXiv:2504.16511, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan Rossi, and Thien Huu Nguyen. Culturax: cleaned, enormous, and multilingual dataset for large language models in 167 languages. arXiv preprint arXiv:2309.09400, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Advances in Neural Information Processing Systems, 36:7915579172, 2023. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of language model? arXiv preprint arXiv:2002.08910, 2020. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454/. Preprint Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining via document de-duplication and diversification. Advances in Neural Information Processing Systems, 36:5398353995, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng, Chaojun Xiao, et al. Ultra-fineweb: Efficient data filtering and verification for high-quality llm training data. arXiv preprint arXiv:2505.05427, 2025. Maurice Weber, Dan Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, et al. Redpajama: an open dataset for training large language models. Advances in neural information processing systems, 37:116462 116492, 2024. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. Qurating: Selecting high-quality data for training language models. arXiv preprint arXiv:2402.09739, 2024. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36: 3420134227, 2023. Xianjun Yang, Shaoliang Nie, Lijuan Liu, Suchin Gururangan, Ujjwal Karn, Rui Hou, Madian Khabsa, and Yuning Mao. Diversity-driven data selection for language model tuning through sparse autoencoder. arXiv preprint arXiv:2502.14050, 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can In Anna Korhonen, David Traum, and Lluıs M`arquez machine really finish your sentence? (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10. 18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/. Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ju Fan, et al. Harnessing diversity for important data selection in pretraining large language models. arXiv preprint arXiv:2409.16986, 2024. Xinlin Zhuang, Jiahui Peng, Ren Ma, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, and Conghui He. Meta-rater: multi-dimensional data selection method for pre-training language models. arXiv preprint arXiv:2504.14194, 2025. 13 Preprint"
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "During manuscript preparation, we occasionally utilized large language models (LLMs) to refine language expression, such as improving sentence fluency, and enhancing readability. The model was not involved in generating original research contributions, including research direction formulation, methodologies selection, experiment designs, results analysis. All the core intellectual work, such as idea development, experiment execution, and results interpretation, was carried out independently by the authors. Any linguistic suggestions offered by the LLM were carefully reviewed and selectively incorporated, ensuring that accuracy, originality, and scholarly integrity were fully maintained. The authors alone take responsibility for the research content and conclusions, and the LLM is not listed as contribution or author. 11 EVALUATION DIMENSIONS To comprehensively evaluate the quality and usefulness of training data, we design an 11dimensional evaluation framework. These dimensions aim to capture complementary aspects of data that jointly determine its contribution to pretraining large language models (LLMs). Specifically, the dimensions are grouped into four general categories: language quality, which reflects the clarity and fluency of expression; knowledge quality, which measures the depth, diversity, and utility of information; comprehension difficulty, which reflects the complexity of content and its potential to improve generalization; and information quality, which ensures factual correctness and completeness. Together, these dimensions provide multi-perspective evaluation of data, enabling balanced assessment of both quality and diversity. Below, we provide the details of each category: 1. Language quality: LLMs fundamentally rely on languages to understand the content and interact with humans. High-quality data contributes directly to improving models comprehension and generation abilities. high-quality document should present ideas in logical and rational organization, avoid redundant and irrelevant content, and use accurate spelling and grammar to convey meaning (Penedo et al. (2023)). To capture these properties, we evaluate language quality in three dimensions: (i) coherence, which reflects whether the text follows logical and consistent structure; (ii) conciseness, which measures whether the information is conveyed efficiently without unnecessary repetition; and (iii) spelling/- grammar accuracy, which ensures correctness in word usage and sentence construction. 2. Knowledge quality: Beyond linguistic clarity, high-quality data must provide comprehensive knowledge to enrich an LLMs understanding of the world. This dimension mainly measures whether document contains valuable, diverse, and practical information that can improve the models reasoning ability and enhance the factual knowledge base (Gunasekar et al. (2023)). Recent work has also shown that the small models can approach the performance of larger ones if trained with reasoning data (Guo et al. (2025)). To comprehensively capture knowledge quality, we define five dimensions: (i) knowledge depth, assessing the extent to which document explores concepts beyond superficial descriptions; (ii) knowledge richness, measuring the breadth of covered topics and perspectives; (iii) reasoning, capturing the presence of explicit logical inference, argumentation, or stepby-step derivations; (iv) educational value, evaluating whether the content provides clear, structured explanations suitable for learning or instruction; and (v) practical helpfulness, which assesses the applicability of the knowledge to real-world problems or everyday use. 3. Comprehension difficulty: Challenging and complex data can enhance better generalization and adaptability in LLMs. Texts with higher conceptual complexity, specialized domain knowledge, or multi-step expository structures push models to develop deeper comprehension abilities and to generate more professional and domain-specific responses Agrawal & Singh (2023). This dimension therefore evaluates the difficulty of the data, considering factors such as the abstractness of concepts, the level of technicality or professionalism, and the requirement for multi-stage reasoning. 4. Information Quality: For models to learn accurate and reliable representations, it is crucial that the training data provides factual, complete, and unambiguous information. Documents with inaccurate or incomplete facts risk propagating errors and degrading downstream performance (Chang et al. (2024)). To evaluate this aspect, we define two evalua14 Preprint tion dimensions: (i) factual accuracy, which measures the degree to which the document presents correct and verifiable information; and (ii) completeness, which reflects whether the information is sufficiently detailed and covers all key aspects of the described topic, thereby reducing the risk of the model learning partial or misleading facts."
        },
        {
            "title": "C METRICS AND PROMPT FOR EACH DIMENSION",
            "content": "To evaluate data quality and diversity across multiple dimensions, we design set of prompts that guide large language models to score documents on 11 distinct dimensions, as described in Appendix B. Each dimension is assessed on tailored Likert scale ranging from 0 to 3/4/5 points depending on the property being measured, with detailed criteria provided for each score level. The prompts are designed to generate both quantitative score and brief qualitative justification, ensuring transparency in the evaluation process. These scores are then aggregated into the score matrix used in our PCA-based data selection framework (see Section 2.3). 1 Below is an extract from web page. Evaluate whether the text demonstrates high coherence in terms of language quality. Please follow the following guideline to assess the language quality of the given extract on 4 likert scale: 2 3 0 Point: Incomprehensible 4 - The text is grammatically chaotic and difficult to understand. 5 - Severe errors in structure, agreement, and tense prevent understanding. 6 7 1 Point: Partially Readable 8 - Some sentences are clear, but overall clarity is lacking. 9 - Noticeable grammatical errors and inconsistency disrupt smooth reading. 10 11 2 Points: Moderately Coherent 12 - Occasional language issues but overall understandable. 13 - Logical flow is maintained with some awkward phrasing. 14 15 3 Points: Generally Coherent with Minor Errors 16 - Paragraphs progress logically with minor, infrequent language errors. 17 - Sentences are generally well-formed with consistent tense and clear subject-verb agreement. 18 19 4 Points: Exceptionally Coherent 20 - The text is grammatically flawless, with precise subject-verb agreement and tense usage. 21 - Sentence and paragraph structure is logically ordered and fluid. 22 - Punctuation and syntax enhance the clarity and flow of ideas. 23 24 The extract: {text} 25 After examining the extract: 26 - Briefly justify your total score, up to 50 words. - Conclude begin with the score using the format: \"Language 27 Coherence Score: <total points>\" Listing 1: Prompt for Coherence 1 Below is an extract from web page. Evaluate whether the text demonstrates high level of conciseness. Please follow the following guideline to assess the conciseness of the given extract on 4 likert scale: 2 3 0 Point: Excessively Wordy 4 - The extract is filled with redundant, unrelated, or repetitive language. 15 Preprint 5 - Nearly every sentence could be significantly shortened or removed without loss of meaning. 6 - Core ideas are obscured or lost in verbosity. 7 8 1 Point: Somewhat Wordy 9 - The text is clear but contains noticeable repetition or unnecessary words. 10 - Some sentences are overly elaborate. 11 12 2 Points: Moderately Concise 13 - The extract avoids major redundancy but may include some unnecessary elaboration. 14 - Most sentences convey meaning efficiently, though small improvements in brevity are possible. 15 - The main points are clear and not lost in superfluous language. 16 17 3 Points: Concise and Effective 18 - Ideas are expressed clearly and directly, with minor redundancy or unnecessary details. 19 - Minimal to no repetition or fluff. 20 21 4 Points: Exceptionally Concise 22 - Every word is essential and contributes directly to the meaning. 23 - No repetition, filler, or unnecessary elaboration. 24 - The writing is focused, impactful, and efficient. 25 26 27 - The extract: {text} 28 29 After examining the extract: 30 - Briefly justify your total score, up to 50 words. - Conclude begin with the score using the format: \"Language 31 Conciseness Score: <total points>\" Listing 2: Prompt for Conciseness 1 Below is an extract from web page. Evaluate whether the text demonstrates high accuracy of word usage, which contributes to the as overall language quality. Please follow the following guideline to assess the accuracy of word usage in the given extract on 4 likert scale: 2 3 0 Points: Severe Inaccuracy 4 - The extract contains frequent incorrect word usages. 5 - Frequent typos, incorrect word forms, or misuse of words make the text almost unreadable. 6 - Errors severely hinder understanding. 7 8 1 Points: Limited Accuracy 9 - Spelling mistakes appear regularly but are not overwhelming. 10 - Occasional misuse of words or minor typos affect clarity. 11 - The overall message is still understandable but occasionally unclear. 12 13 2 Points: Moderate Accuracy 14 - Most of the text is correctly spelled, with some minor errors or infrequent typos. 15 - Occasional confusion between similar-sounding words may appear but does not significantly affect meaning. 16 - The extract remains mostly readable and understandable. 17 18 3 Points: Strong Accuracy 19 - Spelling is generally correct throughout. 20 - Only rare, minor typos or homophone errors are present, and they do not interfere with comprehension. 16 Preprint 21 - The extract demonstrates clear attention to written accuracy. 22 23 4 Points: Perfect Accuracy 24 - The extract is free from any spelling errors, typos, or homophone confusion. 25 - All words are used appropriately and are correctly spelled. 26 - The writing is polished and precise, reflecting excellent language control. 27 28 The extract: {text} 29 30 After examining the extract: 31 - Briefly justify your total score, up to 50 words. - Conclude begin with the score using the format: \"Language Spelling Accuracy Score: <total points>\" Listing 3: Prompt for Spelling Accuracy 1 Below is an extract from web page. Evaluate whether the text demonstrates an appropriate depth of knowledge, particularly with regard to the grade level it targets. The following gudeline is used to assess whether text has high knowledge depth on 5 likert scale: 2 3 0 Points: No Knowledge Depth 4 - The extract contains no meaningful or accurate knowledge. 5 - It lacks substance entirely and offers no educational value at any grade level. 6 7 1 Point: Shallow and Common Knowledge for Pre-K to Grade 1 8 - The content is understandable even to early primary grades (Pre-K to Grade 1). 9 - Contain simple, basic facts or common knowledge (e.g., basic facts like \"grass is green\" or \"2 + 2 = 4\"). 10 11 2 Points: Basic Knowledge for Lower Grades (Grades 2-4) 12 - The content is at lower elementary levels. 13 - Introduces simple concepts and provides very short, basic explanations. 14 - Requires understanding of simple definitions and explicit information. 15 16 3 Points: Introductory Knowledge for Middle Grades (Grades 5-7) 17 - Understandable for upper elementary to early middle school. 18 - Explains foundational concepts with some detail and structure. 19 - Some depth is present. It may require understanding of cause-and-effect relationships and ability to follow multi-step explanations. 20 21 4 Points: Substantive Knowledge for Secondary Levels (Grades 8-12) 22 - Content is well-developed and appropriate for high school. 23 - Explores concepts in depth, including underlying principles, reasoning, and potential implications. 24 - Characterized by complex sentence structures, theoretical concepts, evidence or examples to support points; resembles textbook content. 25 26 5 Points: Advanced Knowledge Depth (college-level or graduate-level) 27 - The extract reflects college-level or graduate-level understanding. 28 - The knowledge is usually only known to the professional people in certain field. 29 - May presents complex information, including detailed analysis, theoretical frameworks, multiple perspectives, and nuanced arguments. 17 Preprint 31 The extract: {text} 32 33 After examining the extract: 34 - Briefly justify your total score, up to 50 words. - Conclude begin with the score using the format: \"Knowledge Depth 35 Score: <total points>\" Listing 4: Prompt for Knowledge Depth 1 Below is an extract from web page. Evaluate whether the text demonstrates high degree of knowledge density in its content. The following curriculum is used to assess whether text has dense knowledge on 4 likert scale: 2 3 0 Point: No Meaningful Knowledge 4 - The extract lacks any meaningful or specific content. 5 - No concrete facts, data, or identifiable concepts 6 7 1 Point: Minimal Knowledge Content 8 - Contains only 1-2 disjointed factual statements 9 - No context, sourcing, or explanation 10 11 2 Points: Moderately Knowledge Density 12 - The extract includes several points of useful knowledge. 13 - Support with some details, examples, or explanations. 14 15 3 Points: Substantially Rich in Knowledge 16 - The content provides well-rounded and informative discussion. 17 - Ideas are explained with clarity and supported by relevant details or examples. 18 19 4 Points: Exceptionally Knowledge-Rich 20 - The extract offers dense, nuanced, and well-connected presentation of knowledge. 21 - The content shows breadth and depth, encouraging comprehensive understanding. 22 23 The extract: {text} 24 25 After examining the extract: 26 - Briefly justify your total score, up to 50 words. - Conclude begin with the score using the format: \"Knowledge Richness Score: <total points>\" Listing 5: Prompt for Knowledge Richness 1 Below is an extract from web page. Evaluate whether the text demonstrates high level of reasoning level. The following curriculum is used to assess whether text has high reasoning level: 2 3 0 Points: No Reasoning Present 4 - The text lacks any evidence of thinking or reasoning from the writer. 5 6 1 Point: Minimal Reasoning 7 - Some claims are made, but reasoning is largely absent or extremely shallow. 8 - No causal relationships or inferential steps are evident. 9 - Readers are not encouraged to reflect or engage intellectually. 10 11 2 Points: Limited Reasoning 12 The text demonstrates some basic thinking and reasoning, such as: 13 - straightforward application of known technique 18 Preprint 14 - simple analysis of problem. 15 16 3 Points: Moderate Reasoning 17 The text demonstrates adequate level thinking and reasoning, such as 18 - consideration of multiple approaches to problem. 19 - discussion of the trade-offs between different solutions. 20 21 4 Points: Strong Reasoning 22 The text demonstrates significant thinking and reasoning, such as: 23 - Multi-step reasoning chains to solve complex problem. 24 - Advanced reasoning patterns often used in specialized science domains 25 26 5 Points: Exceptional Reasoning Quality 27 The text exemplifies exceptional thinking and reasoning, such as: 28 - highly innovative and creative approach to solving complex problem in specialized domains. 29 - Combining multiple reasoning and thinking techniques, with novel abstraction of the problem. 30 31 The extract: {text} 32 33 After examining the extract: 34 - Briefly justify your total score, up to 50 words. - Conclude begin with the score using the format: \"Knowledge 35 Reasoning Score: <total points>\" Listing 6: Prompt for Reasoning Level 1 Below is an extract from web page. Evaluate whether the page has high educational value for teaching from kindergarten to graduate education. The following curriculum is used to assess whether text has high educational value on 3 point scale: 2 3 **0 Point: No Educational Value** 4 - Not even single bit of information is worth learning. 5 - Note that if there is even single bit of information that is worth learning, the score should be at least 1 point. 6 7 **1 Point: Minimal Educational Relevance** 8 - The extract provides some useful information pertinent that is worth learning or teaching, but does not align closely with educational standards. 9 - It may include large amount of non-educational content (e.g., advertisements, promotional material) that detracts from its usefulness. 10 11 **2 Points: Suitable for Educational Use** 12 - The extract provides lot of useful information that is worth learning or teaching. The content is fluent and coherent. 13 - It may include small amount of non-educational content. It may have limitations, such as incomplete coverage or extraneous information. 14 15 **3 Points: Highly Relevant and Beneficial** 16 - The extract has very high educational value. It contains high density of information that is worth learning or teaching, either for any level of education. 17 - Content is clear, consistent, and focused, with minimal irrelevant information. 18 - May resemble snippet from textbook, tutorial, exercises, solutions, or any structured learning materials. 19 20 The extract: 21 {text} 19 Preprint 22 23 After examining the extract: 24 - Briefly justify your total score, up to 50 words. 25 - Conclude begin with the score using the format: \"Educational score: <the assigned score>\" Listing 7: Prompt for Educational Value 1 Below is an extract from web page. Evaluate whether the content demonstrates high degree of practical helpfulness, particularly in terms of offering applicable knowledge for real-world utility. The following curriculum is used to assess whether text has high practical helpfulness on 4 likert scale: 2 3 0 Points: No Practical Helpfulness 4 - The extract contains no useful or applicable knowledge. 5 - May be purely entertainment or advertisement with zero actionable takeaways 6 - May contain misinformation or harmful suggestions 7 8 1 Point: Minor Utility 9 - The text may hint at applicable ideas but lacks clarity, specificity, or guidance. 10 - It is too general or abstract to be put into use. 11 12 2 Points: Moderately Helpfulness 13 - The knowledge can be applicable in some uncommon scenarios (targets <1% audience) that only relate to small portion of people. 14 15 3 Points: Broadly Helpful 16 - The extract includes practical information that could be applied in common contexts. 17 - Offers validated strategies for common needs 18 19 4 Points: Substantially Helpful 20 - The extract offers clear, applicable knowledge or skills that are useful in real-world scenarios that frequently occur. 21 - Addresses frequent pain points (>10% audience) 23 24 The extract: {text} 25 26 After examining the extract: 27 - Briefly justify your total score, up to 50 words. - Conclude begin with the score using the format: \"Knowledge 28 Practical Helpfulness Score: <total points>\" Listing 8: Prompt for Practical Helpfulness 1 Here is an extract from webpage. Please evaluate the percentage of the global population that is likely to be able to comprehend the knowledge text. The following scale is used to assess the comprehension difficulty, with 5-point Likert scale: 2 3 0 Points: No value to understand 4 - The content is incomprehensible due to its low language quality. 5 - Contains gibberish, severe grammar errors, or formatting problems. 6 - Examples: Advertisement, machine-translated nonsense, corrupted text 7 8 1 Point: Universal Comprehension 9 - The content is very simple and direct, easily understood by the vast majority of people. 10 - Requies basic vocabulary (<4th grade level), commonsense knowledge, with no jargon. 20 Preprint 11 - Examples: Weather reports, simple recipes, basic safety instructions 12 13 2 Points: Majority Effortless 14 The content is clear and easily understandable for almost everyone, with only very small percentage finding it difficult. 15 - Requires conversational language level and general world knowledge 16 - Examples: social media posts, most new articles 17 18 3 Points: Educated Majority 19 - The content is accessible to the majority of people, with some difficulty, but most people should be able to understand and comprehend it after some effort. 20 - Requires high school reading level and secondary education concepts 21 - Examples: Government pamphlets, workplace training manuals, simple financial advice. 22 23 4 Points: Specialized Audience 24 - The content is understood by small portion of people, but it remains challenging for the majority. 25 - The content may require some expertise. 26 - Requires undergraduate-level training in field 27 - Examples: College textbooks, legal contracts, financial advice 28 29 5 Points: Expertise 30 - The content may be very professional or academic. 31 - Requires graduate-level expertise. 32 - Examples: Quantum physics proofs, AI architecture patents, genomic research 33 34 Extract: 35 {text} 36 37 After reviewing the text: 38 Briefly justify your total score in up to 50 words. 39 Conclude begin with the score using the format: \"Comprehension Difficulty Score: \" Listing 9: Prompt for Comprehension Difficulty 1 Here is an extract from webpage. Evaluate whether the content demonstrates high level of factual accuracy as part of its overall information quality. 2 Note that: 3 - the text may include some facts that are unknown to you. In these cases, you can ignore these unknown or uncertain facts and only focus on identify those obvious factual errors that are known to you. 4 - In some special contexts, such as fictions, it is allowed to contain some imaginary facts. 5 6 7 The following guideline is used to assess the factual accuracy, with 3-point Likert scale: 8 9 0 Point: Evidently Inaccurate 10 - The extract is filled with incorrect information. 11 - Key claims are demonstrably wrong or contradict well-established facts. 12 13 1 Point: Highly Unreliable 14 - The extract contains multiple factual inaccuracies or distortions. 15 - Misleading phrasing or vague statements obscure the truth. 16 - While not entirely false, it cannot be trusted as reliable source of information. 21 Preprint 18 2 Points: Generally Accurate with Minor Issues 19 - <2 minor errors in peripheral details 20 - Occasional imprecise language without distorting meaning 21 - Preserves core truth despite technical imperfections 22 23 3 Points: Accurate and Trustworthy 24 - No detectable errors in verifiable claims. 25 26 27 Extract: 28 {text} 29 30 After reviewing the text: 31 - Briefly justify your total score in up to 50 words. 32 - Conclude begin with the score using the format: \"Information Factual Accuracy Score:\" Listing 10: Prompt for Factual Accuracy 1 Here is an extract from webpage. Evaluate whether the content demonstrates high degree of completeness, specifically in terms of how fully the topic is covered and whether the information is presented with sufficient context. The following scale is used to assess the information completeness, with 4-point Likert scale: 2 3 4 0 Point: Severely Incomplete 5 The extract offers only fragments of information or vague references to the topic. 6 Key background, definitions, or context are missing. 7 The presentation leaves readers with more questions than answers. 8 9 1 Point: Limited Completeness 10 The extract touches on parts of the topic but leaves significant gaps. 11 It may assume prior knowledge or skip necessary context. 12 Information is partial or unevenly distributed. 13 14 2 Points: Moderately Complete 15 The extract introduces the main topic and provides sufficient context to follow the discussion. 16 Some areas may be underdeveloped or missing, but overall understanding is possible. 17 It resembles summary or introductory passage. 18 19 3 Points: Substantially Complete 20 The extract covers the topic in well-rounded and balanced manner. 21 Most relevant aspects are addressed, with clear and sufficient context. 22 There may be minor omissions, but they do not disrupt comprehension. 23 24 4 Points: Exceptionally Complete 25 The extract thoroughly explores the topic with comprehensive coverage. 26 All necessary context is included, with no critical gaps. 27 It reflects deep and well-structured presentation that anticipates and answers potential reader questions. 28 29 Extract: 30 {text} 31 32 After reviewing the text: 33 Briefly justify your total score in up to 50 words. 34 Conclude begin with the score using the format: \"Information Completeness Score: \" Listing 11: Prompt for Completeness 22 Preprint"
        },
        {
            "title": "D BENCHMARKS SELECTION",
            "content": "To select appropriate downstream benchmarks that can effectively reflect the model performance, we observe the accuracy fluctuation as the trained data increases, with results reported in Figure 7. Arc-C, Arc-E, hellaswag, piqa, and sciq have obvious variation as the training progresses, while the rest of the benchmarks have smaller performance improvement. Since our model and the trained data budget are relatively small, some benchmarks can not obviously reflect the training outcomes of the model. Therefore, we select Arc-C, Arc-E, hellaswag, piqa, and sciq as our benchmarks. Figure 7: Performance across downstream tasks"
        },
        {
            "title": "E SCORE DISTRIBUTION ACROSS DIFFERENT PC",
            "content": "Figure 8 demonstrates the score distribution over different PC dimensions on different domains. The domains are pre-devided by the Nemotron-CC dataset. We can observe that different PC dimensions emphasize distinct aspects, and joint selection across dimensions enhances data diversity. Figure 8: Score distribution over different domains. 23 Preprint"
        },
        {
            "title": "F RESULTS WITH SINGLE PC",
            "content": "Table 3 demonstrates the results of data selected with the single PC scorer. We can observe that each PC exhibits strength in certain area: PC1 and PC4 perform better on Arc-C and Arc-E, indicating better ability at general knowledge, while PC2 and PC3 perform better on Hellaswag and PIQA, indicating better ability for commonsense and physical reasoning. Moreover, models trained with top-scored data from each PC dimension consistently underperform, while sampling from larger score range enhances the performance. These results highlight that different PC scorers focus on distinct data features and using one of them alone can not achieve the best performance. Method PC1-top PC2-top PC3-top PC4-top PC1-Sample PC2-Sample PC3-Sample PC4-Sample Arc-C 0.3635 0.3072 0.3311 0.4053 0.3951 0.3686 0.3686 0.4087 Arc-E 0.6035 0.5097 0.5551 0.7041 0.6519 0.6103 0.6557 0.6860 Hellaswag SIQA 0.4309 0.5567 0.6178 0.4484 0.5464 0.5759 0.6112 0.5356 0.811 0.727 0.741 0.879 0.863 0.765 0.846 0.861 PIQA 0.4289 0.4483 0.4386 0.4350 0.7116 0.7448 0.7579 0.7318 Average 0.5705 0.5707 0.6059 0.6245 0.6336 0.6129 0.6479 0.6446 Table 3: Performance across PC dimensions."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "The Hong Kong Polytechnic University",
        "Tsinghua University"
    ]
}