{
    "paper_title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
    "authors": [
        "Bowei Zhang",
        "Lei Ke",
        "Adam W. Harley",
        "Katerina Fragkiadaki"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera motion is effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion estimates within this stabilized representation, enabling robust tracking over extended periods. To manage the inherent irregularities of 3D point distributions, we propose a Local Pair Attention mechanism. This 3D contextualization strategy effectively exploits spatial relationships in 3D, forming informative feature neighborhoods for precise 3D trajectory estimation. Our 3D-centric approach significantly outperforms existing 3D point tracking methods and even enhances 2D tracking accuracy compared to conventional 2D pixel trackers when accurate depth is available. It supports inference in both camera coordinates (i.e., unstabilized) and world coordinates, and our results demonstrate that compensating for camera motion improves tracking performance. Our approach replaces the conventional 2D square correlation neighborhoods used in prior 2D and 3D trackers, leading to more robust and accurate results across various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io"
        },
        {
            "title": "Start",
            "content": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry Bowei Zhang1,2* 1Carnegie Mellon University Lei Ke1* Adam W. Harley3 2Peking University Katerina Fragkiadaki1 3Stanford University 5 2 0 2 0 ] . [ 1 7 1 7 4 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "feature clouds, We introduce TAPIP3D, novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatiotemporal leveraging depth and camera motion information to lift 2D video features into 3D world space where camera motion is effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion estimates within this stabilized representation, enabling robust tracking over extended periods. To manage the inherent irregularities of 3D point distributions, we propose Local Pair Attention mechanism. This 3D contextualization strategy effectively exploits spatial relationships in 3D, forming informative feature neighborhoods for precise 3D trajectory estimation. Our 3D-centric approach significantly outperforms existing 3D point tracking methods and even enhances 2D tracking accuracy compared to conventional 2D pixel trackers when accurate depth is available. It supports inference in both camera coordinates (i.e., unstabilized) and world coordinates, and our results demonstrate that compensating for camera motion improves tracking performance. Our approach replaces the conventional 2D square correlation neighborhoods used in prior 2D and 3D trackers, leading to more robust and accurate results across various 3D point tracking benchmarks. Project Page:tapip3d.github.io 1. Introduction Tracking every pixel in video is effortless for humans but remains challenging for machines. Fine-grained motion estimation at the particle level provides unified interface for modelling temporal changes in objects, their components, deformable structures, and granular materials. The task is challenging due to occlusions caused by camera motion and dynamic scene changes. Yet, it is fundamental problem in computer vision, with broad applications in augmented reality, and robotics [37]. For example, point tracking have recently used as subgoal representations for robot policies learning [2, 46] with depth-sensing cameras. *Equal contribution. Early motion estimation approaches, such as optical flow, focused on estimating motion between adjacent frames. Recent methods such as PIPs [13] and CoTracker [17, 18] address multiframe tracking of pixels, by predicting 2D point trajectories that track points even under occlusions. At the heart of these methods is inferring 2D correlation maps of query trajectory points with 2D image features computed with convolutional network, and then iteratively retrieving local patches from these correlation maps to provide directional hints on how trajectories might be improved, alongside latent information retrieved from crosstrajectory and within-trajectory attention operations. While significant progress has been made, existing solutions continue to struggle in challenging scenarios, particularly when dealing with complex deformations and large camera movements. We argue that one potential cause for this difficulty stems from estimating pixel motion in the 2D image space, instead of in 3D [44]. As the real motion of physical entities takes place in 3D, it may be easier for points to be represented and tracked in 3D space instead of the image plane. Moreover, representing features in 3D space offers advantages over 2D featurization. In image space, camera projection brings spatially distant regions into close proximity, leading to misleading feature correlations. In contrast, 3D representation preserves true spatial relationships, reducing ambiguity and improving tracking accuracy. We advance this approach further than prior work by focusing specifically on stabilized world-space coordinates for 3D tracking, which provides distinct advantages over methods [28, 44] that rely on per-frame 3D coordinate systems. For instance, static 3D point in the real world may appear to follow complex trajectory in image spaceor even exit the cameras field of viewpurely due to camera motion. However, in world-space coordinates, the same point remains fixed at single (X, Y, Z) location, making tracking more stable and reliable. We propose TAPIP3D for Tracking Any Point in Persistent 3D Geometry, 3D point tracking method that represents and iteratively updates multi-frame 3D pixel motion trajectories through spatio-temporal attentions in an 1 Figure 1. TAPIP3D performs long-term 3D point tracking in persistent 3D world space of 3D feature clouds, which exceeds prior 3D point tracking methods [28, 44] operating in camera-dependent UV pixels + Depth (UVD) spaces. We leverage the given / estimated depth map and camera pose from MegaSaM [23] to compute 3D space where camera motion is cancelled. TAPIP3D designs Local Pair Attention for featurization and iterative motion estimation. The 3D motion trajectories for the sampled dynamic points in 3D XYZ world space are significantly smoother and more linear then UVD space. RGB-D video. Our method represents video as spatiotemporal 3D feature cloud. Each point in the cloud represents 2D feature vector lifted to corresponding 3D coordinate (X, Y, Z) using sensed or estimated depth. Under known camera motion, we use camera extrinsics to infer world-centric 3D feature graph, by removing camera motion from the video observations, as shown in Figure 1. Our method performs tracking in either the camera or world 3D feature space by directly featurizing scene coordinates using 3D attentions. Specifically, it identifies the local 3D neighborhood for each query trajectory coordinate as query group and replaces traditional 2D CNNs with novel local pair cross-attention design. This mechanism enables the deep feature representation of query point group at given timestep to attend to the features of key point group, i.e., its neighboring 3D points at future timesteps. Moreover, within these cross attentions, we incorporate the 3D relative offset between the query trajectory point and the neighboring target point into the attention values, alongside appearance features, thereby enhancing spatial context awareness. We test TAPIP3D in established 3D point tracking benchmarks of TAPVid3D [19], LSFOdyssey [38], Dynamic Replica [16] and DexYCB [5] for tracking points in 2D and 3D under ground-truth, sensor or estimated depth, in camera and world 3D coordinate frames. We show TAPIP3D outperforms all previous methods by big margin in 3D point tracking metrics when accurate depth is available. Our experiments show that point tracking in 3D world coordinate frame estimated from state-of-theart learnt camera and depth estimation methods such as MegaSAM [23] outperforms tracking in camera frame, pursued by existing 3D point trackers [28, 44]. We ablate design choices of our model, to shed light to the contribution in performance of our 3D-centric featurization, augmentations and quality of depth estimation. In summary, we present TAPIP3D for 3D point tracking from RGB-D and RGB videos that uses 3D feature clouds for video to estimate 3D point tracks, which demonstrates state-of-the-art performance when ground-truth / sensor depth is available and competitive performance with estimated depth [23]. It can exploit recent advances in depth and camera pose estimation and deliver both camera and world-centric 3D tracks. To our knowledge, TAPIP3D is the only 3D tracking method that can track in 3D worldcentric space, where camera motion is factored out. We will make our code available upon acceptance. 2. Related Work 2D and 3D Point Tracking Recent advances in point tracking formulate the task as the estimation of multi-frame point trajectories, moving beyond traditional pairwise optical flow estimation. Inspired by earlier work on particle video [33], PIPs [13] introduces an iterative refinement approach with dense cost maps. This work along with the 2 Tracking Any Point (TAP) benchmark [8] have catalyzed series of advancements [21, 22] including the use of multipoint context [17, 18], enhanced initialization schemes [9], broader correlation contexts [3, 6], and post-hoc densification techniques [20]. Beyond 2D, recent works such as SpatialTracker [44] have extended point tracking to 3D by incorporating depth information. SpatialTracker employs Triplane representation [4], where 3D featured point clouds per frame from pixel (UV) and depth (D) coordinates are projected onto onto three orthogonal planes. Such Triplane featurization is faster than our k-NN attention, but this speedup comes at the cost of decreased performance. The works of DELTA [28] and SceneTracker [38] also operate on UVD coordinate system to separately compute appearance and depth correlation features, directly extending the 2D method of CoTracker [18]. DELTA [28] proposes coarseto-fine trajectory estimation that manages to estimate dense point tracks across the whole image plane, instead of on set of sparse locations. In contrast to previous 3D point trackers, TAPIP3D leverages an explicit spatio-temporal 3D Point-Feature Graph video representation, instead of 2.5D one of the image-space and depth, for feature extraction and tracking. By fully utilizing the underlying geometric structure of the scene, our method achieves better point tracking accuracy and consistency. Moreover, as an important novelty over previous works, it can estimate trajectories in 3D world space, by canceling inferred camera motion from the scene image pixels during lifting. Point Tracking via Reconstruction 3D point tracks can also be extracted from monocular videos through test-time optimization by fitting dynamic radiance fields, such as dynamic NeRFs [31, 39] or Gaussians [7, 27, 34, 40] to the observed video frames supervised from reprojection RGB, depth and motion error. These approaches require per-video optimization and are thus computationally expensive. In contrast, our TAPIP3D is learning-based feed-forward approach for 3D point tracking. Scene Flow Estimation Scene flow estimation extends optical flow into three dimensions by considering pairs of point clouds and estimating the 3D motion between them [11, 12, 14, 25, 29, 32, 36, 43]. Recent works incorporate rigid motion priors, explicitly [36] or implicitly [45] or leveraging diffusion models [24]. Linking 3D scene flow estimates terminates at pixel occlusions by design, similarly to linking 2D flow estimates. Instead our work focuses on inferring multiframe point trajectories through occlusions. Learning-Based 3D Foundation Models Recent methods in learning based camera motion and depth estimation, such as MoGe [41], DUSt3R [42], MonST3R [47], and MegaSaM [23], focus on predicting dense 3D reconstructions from single images, image pairs, or videos, bringing the vision of video to 4D translation closer. MegaSAM [23] delivers highly accurate camera motion and depth by combining learning based initialization and updates with second order optimization, extending earlier work of DROIDSLAM [35]. None of the methods above addresses the problem of estimating the 3D point motion, which is the focus of our work. TAPIP3D builds upon the progress of recent learning-based 3D camera motion and depth estimation methods to explore their use in 3D point tracking, by lifting videos to 3D world space which makes the scene points easier to track than in the original 2D image plane. 3. Method Overview. The architecture of TAPIP3D is illustrated in Figure 2. TAPIP3D can track any pixel in RGB or RGBD video in camera or world 3D space. It is 3D transformer that iteratively refines set of 3D point trajectories through spatio-temporal attentions into 4D (3D + time) video point feature graph. Specifically, it takes as input an RGB video = {It RHW 3}T t=1, sequence of depth maps = {Dt RHW }T t=1, camera intrinsics in the form of constant over time focal length , and, optionally, sequence of camera poses = {Camt R6}T t=1, relative to the camera pose in the first frame I0. It then lifts video pixel coordinates into 3D point coordinate through reversing the camera projection equation. The input depth maps can originate from off-the-shelf monocular depth estimator [41], depth sensors, or GT depth provided by simulation environments. Camera intrinsics and extrinsics can either be estimated algorithmically by video 3D reconstruction models [23] or directly provided by the dataset. , tq Given set of 3D query points = {(X tq ), = 1...Q} specified in their respective frames tq, = 1 . . . Q, our model produces occlusion-aware 3D point traq , jectories τ = {τq = (X t=1, = 1...Q} and corresponding visibility trajectories = {(ot t=1, = 1...Q} which represent the 3D locations and visibility of the query It achieves this through iterative inferpoints over time. ence, where, at iteration m, the model takes as input trajectories τ , extracts features by contextualizing the trajectories with the video input, and predicts updated 3D locations and visibilities τ m+1 , as shown in Figure 2. We will use τq to denote both 2D and 3D trajectory, and we let context indicate which one is implied. We will use τ to denote the pixel or point location of trajectory q. We will omit superscript indicating the iteration the quantities of interested refer to, as this will be implied from the context, in an attempt to declutter notation. , tq q, q)T q)T Tracking in World vs. Camera space. Previous 3D point trackers [28, 38, 44] represent 3D point trajectories using pixel coordinates (u, v) combined with depth information, forming an intermediate 2.5D representation, 3 which we refer to as space. In contrast, TAPIP3D tracks points directly in XY coordinates obtained via inverse camera projection. Tracking in XY space allows TAPIP3D to seamlessly support two distinct modes of 3D tracking within unified framework: 1. World-Frame Tracking: When camera extrinsics are available, each pixel (u, v) with depth value in the video is mapped to 3D world coordinate (X, Y, Z) via camera-to-world transformation, ensuring trajectories are expressed in global reference frame where camera motion is removed. 2. Camera-Frame Tracking: When camera extrinsics are unavailable, each pixel (u, v) with depth is mapped to 3D camera coordinate (X, Y, Z) using the pin-hole camera unprojection equations: = ud , = d. This formulation maintains tracking in the cameras local coordinate space. , = vd By supporting both world-frame and camera-frame tracking, TAPIP3D provides greater flexibility, enabling robust tracking across diverse scenarios with or without explicit camera pose information. 3.1. Preliminaries: Point Tracking as Iterative Trajectory Refinement TAPIP3D builds upon iterative update architectures for 2D point tracking, most notably CoTracker3 [17]. CoTracker3 overview. CoTracker3 tracks 2D points across time by iteratively refining their coordinates and visibilities through transformer. Given sequence of frames and set of query points, it maintains and updates q), per-frame confidence ct q, vt = (ut per-frame locations τ q, for each track q. Initially, τ and visibility indicator ot is set to the same query coordinate for all frames (zero motion assumption) and ct are set to one. The model then performs iterations of refinement, each time predicting offsets τ q, which are then added to the current estimates. q, and updates to ot q, updates to ct q, ot Token construction and correlation features. At each RC iteration m, CoTracker3 resamples 2D features from CNN around the current location τ t,m . It also extracts 2D features around the query frames reference location, and computes 4D correlation between these two local feature patches. This correlation serves as strong signal for how well the local image content matches the query. Several other inputs (embeddings) are also assembled to form token for frame of track q, denoted Gt q. In particular: Correlation Corrt is one-dimensional feature vector obtained by encoding all-pairs feature similarities between the local patch around τ in frame and the patch around the query location in the reference frame τ tq with an MLP. 4 This is re-computed at each iteration using the updated track locations τ t,m . Confidence and visibility, (ct q), from the previous iteration are concatenated as part of the token, allowing the transformer to refine these estimates. q, ot τ t1 Positional embeddings include Fourier embeddings ) (the relative displacement between adjacent γ(τ frames), and time-index embedding γtime(t). Putting these together, each trajectory at each frame is tokenized as: Gt = [Corrt q, ct q, ot q, γ(τ τ t1 ), γ(τ t+1 τ q)], (1) where we omit the additional temporal embedding γtime(t) for simplicity. The trajectory tokens are then updated through spatio-temporal attentions, which predict updated point positions, confidences and visibilities. At test time, to track points in video longer than frames, CoTracker3 shifts the inference window by 2 frames, and initializes the first half of the trajectories with the estimates of the previous window. 3.2. Tracking in Persistent 3D Geometric Space ℓ ℓ ℓ (3+C)}T Video Encoding as 4D Point-Feature Cloud. Our key innovation is to represent the input video as sequence of multi-scale 3D Point-Feature maps, where each map represents the 3D coordinate (X, Y, Z) of each pixel and its C-dimensional feature vector. We first encode each video frame with 2D image encoder with downsample rate ℓ, following [17], producing sequence of feature maps at reduced resolution ℓ , where and the original width and height of the image. We then unproject each pixel in the feature maps to its 3D coordinate, forming sequence of 3D point-feature maps {Ft t=1. Depending on whether we use camera frame tracking or world frame tracking, the coordinates are represented either in camera local coordinates or in shared world frame. We further downsample the pointfeature maps into levels of multi-resolution maps: Fs = {Fs,t t=1, = 1, . . . , S. We apply nearest-neighbor interpolation for coordinates and average pooling for features during downsampling. The multiresolution point feature maps are later used for extracting correlation features. The proposed 3D point-feature map encode 3D geometry more explicitly and without distortions or approximations, comparing to other 3D scene representations, such as tri-plane encoders [44] and UVD coordinates. Feature Extraction through Local Pair Attentions. Previous methods [6, 18] compute correlation features for each trajectory point using 2D CNNs or MLPs on fixed image grids. Our 3D Point-Feature maps do not have fixed grid structure. We use Local Pair Attentions for each trajectory point to attend to its 3D spatially adjacent points at ℓ2s1 (3+C)}T ℓ2s1 Figure 2. Architecture of our TAPIP3D. The model takes RGB frames and corresponding 3D point maps as input, computes features from the RGB frames, and transfers them to the 3D points, forming feature cloud for each timestep. Using either provided or estimated camera poses, these feature clouds can be arranged in either world space or camera space. We then apply our Local Pair Attention module (Figure 3) to extract correlation features, followed by our 3D Trajectory Transformer, which iteratively updates the estimated trajectories. Top right: Illustration of the difference between 3D k-NN (used in our approach) and fixed 2D neighborhoods (used in prior works [6, 17]). = (X tq in LocoTrack [6], we take 3D structures around the query point into consideration and extract nearest neighbors of the query point τ tq ) (the reference point of the trajectory) in the point-feature map Ps,tq , denoting them as {(f k), = 1, . . . , K}. We then add the features with relative positional encoding to form context features {xcn , = 1, . . . , K} and query features {xq , tq , tq k, = 1, . . . , K}: , pq = γθ(pcn xcn p) + cn , xq = γθ(pq q) + (2) Figure 3. Local Pair Attention. Given 3D query point at specific timestep, we first identify its local 3D context using kNN to form query group. Then, within the point cloud at another timestep, we find nearest 3D neighbors to construct key group. We apply bi-directional cross-attention between the query and key groups to capture spatio-temporal correspondences. the same timestep. Specifically, we propose an attention based correlation encoder, which encodes the relationship between each point on in input trajectories and its neighboring points in the point-feature graph, as shown in Figure 3. We extend the traditional point-to-region cross attention to more expressive bi-directional local pair attention scheme. In this design, we consider both the query points and their neighboring support points during the attention process. Formally, as shown in Figure 3, for each point τ q, q = (X q) in the input trajectory τq, we compute its correlation feature Corrt at each resolution level = 1, . . . , as follows. First, we extract the nearest 3D neighbors of the trajectory point τ from the point-feature cloud Fs,t, which we refer to as context neighbors. We denote the , pcn context neighbors as {(f cn ), = 1, . . . , K}, where R3 represent features and posik Rc and pcn cn Inspired by the 4D correlation design tions respectively. , where γ is an MLP and θ are the learnable parameters of our model. The context tokens and query tokens are fused standard 1-head cross-attention layer, followed by attention pooling to obtain the correlation Corrs,t of the current resolution level . The final correlation feature, {Corrt i}, is simply the concatenation of Corrs,t 3D Trajectory updating Transformer The 3D-aware correlation features, together with additional inputs, form the trajectory tokens features Gt over all resolution levels. as: Gt q)), ot i, γ(τ τ ), η(τ t+1 qτ t1 q), η(πt(τ = [Corrt q, ct q], (3) where γ denotes the Fourier encoding function and πt : R3 R2 denotes the projection function that maps the 3D coordinates of the τ to pixel coordinates. Pixel coordinates are explicitly included to assist the model in identifying points located outside the image boundaries. These trajectory tokens are further augmented with temporal positional encodings and processed by transformer utilizing proxy tokens following CoTracker3 [17], i.e., virtual trajectories (i.e., proxy tokens) that do no correspond to any query points, to efficiently perform spatial and temporal 5 attentions. The transformer outputs incremental updates for positions and visibility scores: = τ τ + τ q, = ot ot + ot q, = 1..Q, = 1..T, (4) which serve as inputs for subsequent iterations. Training During training, we run our model over two windows and refine the trajectories for four iterations per window. To avoid distribution shift between training and testing trajectories, at training time, we train iteratively using predictions from the model as conditioning trajectories. Following [17], we supervise each iterations output. Let {τq}Q q=1 and {τq}Q q=1 denote the predicted and ground truth 3D trajectories, and {oq}Q q=1 be the predicted and ground truth visibility probabilities. We define the loss for this iteration as follows: q=1 and {oq}Q = (cid:88) (cid:88) q=1 t= 1 dt (cid:13) (cid:13)τ τ (cid:13) (cid:13)2 + αvisCE(ot q, ot q) (5) denotes the depth of τ where dt q, CE denotes the binary cross entropy loss and αvis is weighting factor balancing trajectory loss and visibility loss. We scale the L2 distance loss by the inverse of the ground truth depth, as tracking distant points becomes more challenging due to the increased sparsity of the point cloud in farther regions. During training, we augment the training data by applying random rigid transforms to each frames point-feature map. Implementation Details Our model is trained on the Kubric MOVi-F dataset [10]. We initialize the image encoder with CoTracker3s pre-trained weights [17]. The training runs on 8 L40S GPUs with batch size of 1 for 200K iterations. We set the sliding window length to 16 and sample 24-frame video with 384 point trajectories per batch. The model is optimized using AdamW [26] with both the base learning rate and weight decay set to 5e-4. The learning rate starts with 5% warm-up phase, then follows cosine annealing schedule. Following the practice of CoTracker [18], we always use fixed video length for refinement to reduce memory usage, and perform windowed inference to deal with longer videos. We additionally scale all the coordinates in the point maps to unit variance before processing each window to simplify learning. 4. Experiments We evaluate TAPIP3D on both 3D and 2D point tracking benchmarks, on videos from simulated and real-world scenes with various depth modalities, including sensor depth, estimated depth, and simulator depth. The 2D point trajectories are obtained by projecting the inferred 3D point tracks onto the image plane. Our experiments aim to answer the following questions: 1. How does TAPIP3D compare with the state-of-the-art in 3D and 2D tracking? 2. How does depth quality (estimated versus sensed depth) affect performance of TAPIP3D? 3. How does camera vs. world space affect performance? 4. How does 3D k-nn attention compare against other tracking space representations, such as UVD (UV+Depth)? Evaluation metrics We adopt the 3D point tracking metrics in TAPVid3D-3D [19] for datasets evaluation, where APD3D (< δavg) quantifies the average percentage of points whose errors fall within the threshold δ. Occlusion Accuracy (OA) assesses the precision of occlusion predictions, while Average Jaccard (AJ) jointly measures accuracy in both position and occlusion estimation. Datasets We evaluate the performance of our model and baselines in the following synthetic and real world datasets: TAPVid3D [19] integrates videos from three distinct real-world datasets covering diverse scenarios: DriveTrack [1], PStudio [15], and Aria [30]. Together, these datasets provide total of 4,569 evaluation videos, with video lengths ranging from 25 to 300 frames. DexYCB-Pt: dataset we introduce by annotating 8,000 real-world RGB-D videos depicting manipulation scenarios from DexYCB [5]. We leverage ground-truth object and hand poses to generate accurate 3D point tracks. LSFOdyssey [38]: synthetic dataset that contains 90 videos with complex but realistic long-range motion using humanoids, robots, and animals. Each video is 40 frames long and has 3D point trajectories and occlusion labels, sourced from PointOdyssey [48]. DynamicReplica [16]: synthetic 3D reconstruction dataset that contains 500 videos of articulated human and animal models. Each video is 300 frames long. Baselines We compare TAPIP3D against the following 2D and 3D point trackers: 1. DELTA [28]: very recent state-of-the-art 3D point tracker that uses the depth map as an additional image channel during computation of query 2D crosscorrelation features, extending CoTracker [18]. It uses UVD space for tracking, i.e., image plane plus depth. 2. SpatialTracker [44]: 3D point tracker that builds upon cotrackers architecture and uses tri-plane representation to featurize (u, v, d) coordinates, it uses UVD space for tracking. 3. CoTracker3 [17] + depth: method that obtains 3D point trajectories by lifting 2D trajectories to 3D using estimated or GT depth of the trajectory points. We consider the following versions of our model: 1. TAPIP3D-world: It is TAPIP3D tracking in world coordinate frame by using information of camera pose trajectory to compute global 3D world space where camera 6 Table 1. Comparison of long-term 3D point tracking methods on the large-scale real-world TAPVid-3D [19] benchmark. We take the estimated depth from MegaSaM [23]. For our TAPIP3D-world, we also leverage the camera pose estimation from MegaSaM [23]. Note that ADT [30] has significant camera motions while PStudio [15] is with the static camera. We evaluate on the full split of TAPVid-3D."
        },
        {
            "title": "Average",
            "content": "AJ3D APD3D OA AJ3D APD3D OA AJ3D APD3D OA AJ3D APD3D OA CoTracker3 [17] + MegaSaM [23] SpatialTracker [44] + MegaSaM [23] DELTA [28] + MegaSaM [23] TAPIP3D-camera + MegaSaM [23] TAPIP3D-world + MegaSaM [23] 20.4 15.9 21.0 21.6 23.5 30.1 23.8 29. 31.0 32.8 89.8 90.1 89.7 90.4 91.2 14.1 7.7 14.6 14.6 14.9 20.3 13.5 22. 21.3 21.8 88.5 85.2 88.1 82.2 82.6 17.4 15.3 17.7 18.1 18.1 27.2 25.2 27. 27.7 27.7 85.0 78.1 81.4 85.5 85.5 17.3 13.0 17.8 18.1 18.8 25.9 20.8 22. 26.7 27.4 87.8 84.5 86.4 86.0 86.4 motion is cancelled. 2. TAPIP3D-camera: It is TAPIP3D tracking in camera coordinate frame, where information of camera pose is not used, and thus the 3D coordinated of pixels are result of combination of camera and scene motion. In all the evaluations, our model is trained on the Kubric MOVi-F dataset [10]. TAPIP3D-world and TAPIP3Dcamera adopt the same trained checkpoint but conduct inference in the two different 3D coordinate systems. 4.1. Point Tracking Evaluations We first evaluate our TAPIP3D against strong baseline methods [17, 28, 44] on the challenging real-world TAPVid3D [19] benchmark. Table 1 summarizes the comparative 3D tracking performance, utilizing depth and camera poses obtained from state-of-the-art 3D reconstruction models [23] for each frame. Our TAPIP3D consistently outperforms recent state-of-the-art methods, including SpatialTracker [28] and DELTA [44], across all three subsets in terms of AJ3D. Notably, on the ADT dataset [30], characterized by particularly challenging camera motions, our TAPIP3D-world significantly surpasses DELTA [28], achieving improvements of 2.5 points in AJ3D and 3.5 points in APD3D. Furthermore, TAPIP3D-world demonstrates clear advantage over TAPIP3D-camera, highlighting the benefits of conducting tracking in persistent 3D world space rather than in camera-centric coordinates. Next, we present results for 3D point tracking on the real-world manipulation dataset DexYCB [5], as shown in Table 3. Our TAPIP3D notably boosts performance over the previous best method [28], improving AJ3D from 26.4 to 30.3. Additionally, we observe that SpaTracker [44] achieves limited performance, even falling below the depthlifted CoTracker3 [17]. This suggests that SpaTrackers triplane representation struggles to leverage the accurate sensor depth provided by DexYCB effectively. Lastly, we evaluate our models 3D point tracking performance on the synthetic LSFOdyssey [38] and DynamicReplica [16] benchmarks, as shown in Table 2. Comparisons are conducted between our method and baselines under different depth sources: ground-truth depth from simulation, and estimated depth from MegaSaM [23]. We report two notable findings: 1) TAPIP3D-world outperforms all competing 3D point trackers in AJ3D when provided with depth estimation from MegaSaM [23] or ground-truth depth. Moreover, utilizing ground-truth depth, our approach even outperforms CoTracker3 [17] on 2D point tracking accuracy. 2) As the depth quality improves (MegaSaM [23] GT Depth), the performance enhancement of our method in APD3D is substantially greater compared to DELTA [28] and SpaTracker [44], underscoring the effectiveness of our 3D point feature representation in world coordinates. 4.2. Ablations In Tables 4 and 5 we ablate our models design choices regarding the selection of nearest neighbors in 3D vs. 2D space, UVD vs. XYZ space for point tracking and Local Pair Attention vs. conventional point-to-region cross attention for feature extraction. We draw following conclusions: Computing nearest neighbors in 3D space is better than fixed 2D. In Table 4, we see that our 3D k-NN design improves the AJ3D metric from 27.7 to 29.8. This indicates that our 3D k-NN mechanism leverages 3D geometric information to effectively filter out irrelevant 2D neighbors during correlation, as illustrated in 2. Tracking in world space helps. In Table 5a, TAPIP3D that tracks in XYZ (world) and XYZ (Cam) space outperform versions that track in UV+D and UV+log(D) space, the latter two commonly adopted in existing state-of-the-art methods [18, 28]. Specifically, tracking in world XYZ coordinates performs best (AJ3D metric). Tracking in world coordinates generalizes effectively to scenarios with significant camera motion, such as those in LSFOdyssey [38]. Considering support points during attentions helps. In Table 5b, we compare the proposed Local Pair Attention mechanism against conventional point-to-region cross attention. Specifically, we replace the Local Pair attention, which groups each query point with its neighboring context points in both spatial dimensions, with standard 2D atten7 Table 2. Comparison of long-term 3D and 2D point tracking results in LSFOdyssey [38] and Dynamic Replica [16] benchmarks with improved input depth quality ( MegaSaM [23] GT Depth). The best results per column are highlighted in bold. GT C&D denotes using GT depth and camera poses provided by the dataset."
        },
        {
            "title": "Dynamic Replica",
            "content": "AJ3D APD3D AJ2D APD2D OA AJ3D APD3D AJ2D APD2D OA CoTracker3 [17] + MegaSaM [23] SpatialTracker [44] + MegaSaM [23] DELTA [28] + MegaSaM [23] TAPIP3D-camera + MegaSaM [23] TAPIP3D-world + MegaSaM [23] CoTracker3 [17] + GT C&D SpatialTracker [44] + GT C&D DELTA [28] + GT C&D TAPIP3D-camera + GT C&D TAPIP3D-world + GT C&D 19.0 14.8 18.9 20.2 20.5 28.4 8.2 37.7 68.3 72.2 28.1 22.8 28.0 28.6 29. 35.0 13.3 50.1 83.2 85.8 75.0 73.8 75.2 70.4 72.3 75.0 70.6 72.4 76.0 78.5 88.0 88.5 90.2 86.2 87.5 88.0 85.9 88.4 91.2 92.8 88.5 85.7 83.7 83.8 83. 88.5 84.9 82.3 86.2 86.9 20.0 6.6 18.0 20.6 20.6 27.4 7.6 27.4 53.7 55.5 30.2 12.0 27.2 30.3 30.2 38.0 13.6 37.7 70.8 72.8 61.4 54.1 61.3 55.3 56. 61.4 56.4 65.6 64.6 66.2 80.0 72.8 77.3 77.1 77.7 80.0 74.7 80.5 84.7 85.7 86.2 81.7 81.0 80.2 78.6 86.2 83.8 83.8 84.7 85.3 Table 3. 3D point tracking comparison on DexYCB-Pt consisting of 8,000 real-world manipulation videos using Sensor Depth (SD)."
        },
        {
            "title": "Methods",
            "content": "DexYCB-Pt AJ3D APD3D OA CoTracker3 + SD [17] SpatialTracker [44] DELTA [28] TAPIP3D (Ours) 14.9 5.5 26.4 30.3 26.1 11.4 43. 52.4 70.9 66.8 72.8 71.3 Table 4. Ablation on the searching neighbors methods when performing correlation. We compare 3D k-NN vs. Fixed 2D Neighbors in DexYCB-Pt benchmark using Sensor Depth (SD)."
        },
        {
            "title": "Methods",
            "content": "AJ3D APD3D OA Fixed Neighbors in 2D 27.7 k-NN in 3D 29.8 50.0 51.6 67.7 70.9 Table 5. Ablation experiments of our TAPIP3D. To isolate the influence of depth and camera pose, we use the annotations provided by the LSFOdyssey dataset [38]. LP Att.: Local Pair Attention. Cam: 3D camera space. World: 3D world space. Coord. Systems LSFOdyssey AJ3D APD3D OA 87.0 77.0 63.4 84.1 77.9 62.9 85.8 81.6 67.1 70.7 84.1 86.6 (a) 3D Coordinates Systems UV + UV + log(D) XYZ (Cam) XYZ (world) Methods Cam + Att. Cam + LP Att. LSFOdyssey AJ3D APD3D OA 88.0 72.7 59.4 81.6 67.1 85.8 World + 2D Att. World + Att. 62.1 70.7 (b) Ablation on LP Attention 75.1 84.1 88.2 86.6 tion that considers only the individual query points. Our experiments show that Local Pair attention significantly improves the APD3D metric from 75.1 to 84.1. This region-toregion cross-attention approach notably mitigates matching 8 ambiguities by incorporating richer context compared to the simpler point-to-region matching baseline. Similar conclusion was reached in LocoTrack [6] but for 2D image plane using CNNs, which we extend to 3D feature clouds using cross-attention. Limitations Despite its state-fo-the-art performance, our model currently has the following limitations: 1) Its performance degrades when provided depth inputs are lowresolution or of lower quality, indicating sensitivity to input depth fidelity. 2) It is around 1.5 slower and SpatialTracker [44]. With ongoing advancements in 3D vision reconstruction models providing increasingly accurate reconstruction priors, we anticipate continued improvements in the robustness of our model. 5. Conclusion We introduced TAPIP3D, novel approach for multi-frame 3D point tracking that represents videos as spatio-temporal 3D feature clouds in either camera-centric and worldcentric coordinate frame, and employs Local Pair Attention to featurize and iteratively update 3D point trajectories. By lifting 2D video features into structured 3D space using depth and camera motion information, TAPIP3D addresses fundamental limitations of previous 2D and 3D tracking methods, particularly in handling large camera motion. We validated TAPIP3D on established synthetic and real world tracking benchmarks demonstrating state-of-theart performance in 3D point tracking when ground-truth depth is available and competitive performance with estimated depth. By leveraging advances in depth estimation and camera pose prediction, TAPIP3D paves the way for more robust and accurate multi-frame tracking in both RGB and RGB-D videos."
        },
        {
            "title": "References",
            "content": "[1] Arjun Balasingam, Joseph Chandler, Chenning Li, Zhoutong Zhang, and Hari Balakrishnan. Drivetrack: benchmark for In CVPR, long-range point tracking in real-world videos. 2024. 6 [2] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2act: Predicting point tracks from internet videos enables generalizable robot manipulation. In ECCV, 2024. 1 [3] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yitong Dong, Yijin Li, and Hongsheng Li. Context-pips: persistent independent particles demands spatial context features. NeurIPS, 36, 2023. 3 [4] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, 2022. 3 [5] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. Dexycb: benchmark for capturing hand grasping of objects. In CVPR, pages 90449053, 2021. 2, 6, 7 [6] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seungryong Kim, and Joon-Young Lee. Local all-pair correspondence for point tracking. In ECCV, 2024. 3, 4, 5, [7] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene generation from monocular videos. In NeurIPS, 2024. 3 [8] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: benchmark for tracking any point in video. In NeurIPS, 2022. 3 [9] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In ICCV, 2023. 3 [10] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, HsuehTi (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: scalable dataset generator. In CVPR, 2022. 6, 7 [11] Xiuye Gu, Yijie Wang, Chongruo Wu, Yong Jae Lee, and Panqu Wang. Hplflownet: Hierarchical permutohedral lattice flownet for scene flow estimation on large-scale point clouds. In CVPR, pages 32543263, 2019. 3 [12] Simon Hadfield and Richard Bowden. Kinecting the dots: Particle based scene flow from depth sensors. In ICCV, 2011. [13] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In ECCV, 2022. 1, 2 [14] Michael Hornacek, Andrew Fitzgibbon, and Carsten Rother. In CVPR, Sphereflow: 6 dof scene flow from rgb-d pairs. 2014. 3 [15] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: massively multiview system for social motion capture. In ICCV, 2015. 6, 7 [16] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. In CVPR, 2023. 2, 6, 7, [17] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudoarXiv preprint arXiv:2410.11831, labelling real videos. 2024. 1, 3, 4, 5, 6, 7, 8 [18] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In ECCV, 2024. 1, 3, 4, 6, 7 [19] Skanda Koppula, Ignacio Rocco, Yi Yang, joseph heyward, Joao Carreira, Andrew Zisserman, Gabriel Brostow, and Carl Doersch. Tapvid-3d: benchmark for tracking any point in 3d. In NeurIPS, 2024. 2, 6, 7 [20] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Dense optical tracking: Connecting the dots. In CVPR, 2024. 3 [21] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, and Lei Zhang. Taptr: Tracking any point with transformers as detection. In ECCV, 2024. 3 [22] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Feng Li, Bohan Li, Tianhe Ren, and Lei Zhang. Taptrv2: Attention-based position update improves tracking any In Advances in Neural Information Processing Syspoint. tems, 2025. [23] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. MegaSaM: Accurate, fast and robust structure and motion from casual dynamic videos. arXiv preprint, 2024. 2, 3, 7, 8 [24] Jiuming Liu, Guangming Wang, Weicai Ye, Chaokang Jiang, Jinru Han, Zhe Liu, Guofeng Zhang, Dalong Du, and Hesheng Wang. Difflow3d: Toward robust uncertainty-aware scene flow estimation with iterative diffusion-based refinement. In CVPR, 2024. 3 [25] Xingyu Liu, Charles Qi, and Leonidas Guibas. Flownet3d: Learning scene flow in 3d point clouds. In CVPR, 2019. 3 [26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [27] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024. [28] Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, and 9 [44] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In CVPR, 2024. 1, 2, 3, 4, 6, 7, 8 [45] Gengshan Yang and Deva Ramanan. Learning to segment rigid motions from two frames. In CVPR, 2021. 3 [46] Chengbo Yuan, Chuan Wen, Tong Zhang, and Yang Gao. General flow as foundation affordance for scalable robot learning. arXiv preprint arXiv:2401.11439, 2024. [47] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. In ICLR, 2025. 3 [48] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J. Guibas. Pointodyssey: large-scale In ICCV, synthetic dataset for long-term point tracking. 2023. 6 Chaoyang Wang. Delta: Dense efficient long-range 3d tracking for any video. In ICLR, 2025. 1, 2, 3, 6, 7, 8 [29] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Occupancy flow: 4d reconstruction by learning particle dynamics. In ICCV, 2019. 3 [30] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng Carl Ren. Aria digital twin: new benchmark dataset for egocentric 3d machine perception. In ICCV, 2023. 6, 7 [31] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021. 3 [32] Julian Quiroga, Thomas Brox, Frederic Devernay, and James Crowley. Dense semi-rigid scene flow estimation from rgbd images. In ECCV, 2014. [33] Peter Sand and Seth Teller. Particle video: Long-range moIJCV, 80:7291, tion estimation using point trajectories. 2008. 2 [34] Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, and Laura Leal-Taixe. Dynomo: Online point tracking by dynamic online monocular gaussian reconstruction. arXiv preprint arXiv:2409.02104, 2024. 3 [35] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. In NeurIPS, 2021. 3 [36] Zachary Teed and Jia Deng. Raft-3d: Scene flow using rigidmotion embeddings. In CVPR, 2021. 3 [37] Mel Vecerik, Carl Doersch, Yi Yang, Todor Davchev, Yusuf Aytar, Guangyao Zhou, Raia Hadsell, Lourdes Agapito, and Jon Scholz. Robotap: Tracking arbitrary points for few-shot visual imitation. In ICRA, 2024. [38] Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, and Dewen Hu. Scenetracker: Long-term scene flow estimation network. arXiv preprint arXiv:2403.19924, 2024. 2, 3, 6, 7, 8 [39] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In ICCV, 2023. 3 [40] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024. 3 [41] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. arXiv preprint arXiv:2410.19115, 2024. 3 [42] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 3 [43] Zirui Wang, Shuda Li, Henry Howard-Jenkins, Victor Prisacariu, and Min Chen. Flownet3d++: Geometric losses for deep scene flow estimation. In WACV, 2020."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Peking University",
        "Stanford University"
    ]
}