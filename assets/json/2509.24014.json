{
    "paper_title": "SparseD: Sparse Attention for Diffusion Language Models",
    "authors": [
        "Zeqing Wang",
        "Gongfan Fang",
        "Xinyin Ma",
        "Xingyi Yang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While diffusion language models (DLMs) offer a promising alternative to autoregressive models (ARs), existing open-source DLMs suffer from high inference latency. This bottleneck is mainly due to the attention's quadratic complexity with respect to context length in computing all query-key pairs. Intuitively, to reduce this complexity, a natural strategy is to restrict attention to sparse patterns that retain only the most relevant connections. Such approaches are well-established in ARs, where attention follows fixed and clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity behaviors: (1) attention patterns vary across heads, (2) attention patterns in each head remain highly similar across denoising steps, and (3) early denoising steps are critical for generation. These findings render sparse attention methods designed for ARs largely incompatible with DLMs, as they fail to capture head-specific structures and risk degrading generation when applied in early denoising steps. To address these challenges, we propose SparseD, a novel sparse attention method for DLMs. Leveraging the observations, SparseD only requires pre-computing head-specific sparse patterns one time, and reuses them across all steps. This prevents recomputing sparse patterns at each denoising step. Meanwhile, SparseD uses full attention in the early steps, then switches to sparse attention later to maintain generation quality. Together, these establish SparseD as a practical and efficient solution for deploying DLMs in long-context applications. Experimental results demonstrate that SparseD achieves lossless acceleration, delivering up to $1.50\\times$ speedup over FlashAttention at a 64k context length with 1,024 denoising steps."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 1 0 4 2 . 9 0 5 2 : r SPARSED: SPARSE ATTENTION FOR DIFFUSION LANGUAGE MODELS Zeqing Wang1, Gongfan Fang1, Xinyin Ma1, Xingyi Yang2, Xinchao Wang1 1National University of Singapore, 2The Hong Kong Polytechnic University zeqing.wang@u.nus.edu, xingyi.yang@polyu.edu.hk, xinchao@nus.edu.sg"
        },
        {
            "title": "ABSTRACT",
            "content": "While diffusion language models (DLMs) offer promising alternative to autoregressive models (ARs), existing open-source DLMs suffer from high inference latency. This bottleneck is mainly due to the attentions quadratic complexity with respect to context length in computing all querykey pairs. Intuitively, to reduce this complexity, natural strategy is to restrict attention to sparse patterns that retain only the most relevant connections. Such approaches are well-established in ARs, where attention follows fixed and clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity behaviors: (1) attention patterns vary across heads, (2) attention patterns in each head remain highly similar across denoising steps, and (3) early denoising steps are critical for generation. These findings render sparse attention methods designed for ARs largely incompatible with DLMs, as they fail to capture head-specific structures and risk degrading generation when applied in early denoising steps. To address these challenges, we propose SparseD, novel sparse attention method for DLMs. Leveraging the observations, SparseD only requires pre-computing head-specific sparse patterns one time, and reuses them across all steps. This prevents recomputing sparse patterns at each denoising step. Meanwhile, SparseD uses full attention in the early steps, then switches to sparse attention later to maintain generation quality. Together, these establish SparseD as practical and efficient solution for deploying DLMs in long-context applications. Experimental results demonstrate that SparseD achieves lossless acceleration, delivering up to 1.50 speedup over FlashAttention at 64k context length with 1,024 denoising steps. Code is available at https://github.com/INV-WZQ/SparseD."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, diffusion language models (DLMs) have achieved significant progress in the area of natural language processing (Nie et al., 2025; Ye et al., 2025). Unlike traditional autoregressive models (ARs) (Touvron et al., 2023; Yang et al., 2025), which generate tokens sequentially from left to right, DLMs generate the entire context in parallel. Leveraging this capability, DLMs achieve strong performance in language generation and represent promising alternative to ARs. Despite the advantages of parallel decoding, DLMs suffer from high-generation latency (Ma et al., 2025; Wu et al., 2025). This bottleneck arises mainly from the bidirectional attention mechanism (Vaswani et al., 2017), which is central to DLMs. This mechanism computes attention over all querykey token pairs simultaneously, including both prefill (prompt) and all generation tokens. As context length increases, the complexity of this mechanism grows quadratically, leading to high latency in generation and limiting the efficiency of DLMs in real-world applications. To reduce this complexity, sparse attention methods (Xiao et al., 2023; Lai et al., 2025) have emerged as an effective solution. These methods lower the cost of standard attention by restricting computations to sparse patterns that include only subset of important querykey pairs, i.e., important attention scores. Such approaches have been widely adopted in ARs, as attention in ARs exhibits Corresponding Author. 1 prominent and fixed sparse patterns (Xiao et al., 2023). Therefore, applying such methods to DLMs first requires verifying whether sparse patterns also exist in their attention mechanisms. In this paper, we investigate attention patterns in DLMs and find that they also exhibit clear sparse patterns, making sparse attention feasible in theory. However, we make three unique observations in DLMs: (1) attention patterns vary significantly across attention heads, showing head-specific patterns, (2) attention patterns within each head remain highly consistent across denoising steps, (3) early diffusion steps are critical for generation, rendering sparse attention unsuitable at this stage. These unique observations make sparse attention methods designed for ARs largely incompatible with DLMs. Widely used fixed patterns in ARs, such as the sliding-window scheme (Jiang et al., 2023) and sink attention (Xiao et al., 2023), fail to capture head-specific patterns of DLMs. Moreover, applying sparse attention to DLMs in the early steps leads to degradation in generation quality. To tackle these problems, we introduce SparseD, novel sparse attention approach tailored for DLMs. Its core principle is to efficiently handle the unique attention patterns of DLMs without degrading generation quality. To the best of our knowledge, SparseD is the first sparse attention method designed to accelerate DLMs. To achieve this goal, we leverage the three empirical observations above to reduce redundant computation and maintain generation quality. Specifically, SparseD pre-computes and selects important querykey pairs for each head to construct head-specific sparse patterns. These sparse patterns are then reused for sparse attention across denoising steps without the need to recompute. To enable hardware-friendly acceleration, we select important pairs as block-wise querykey pairs (Dao, 2023) rather than individual pairs. Besides, SparseD applies full attention in the early steps to prevent significant degradation in generation quality. These designs enable SparseD to capture head-specific dynamics without incurring significant latency in recomputing sparse patterns at every denoising step, while also preventing the generation degradation caused by sparse attention in the early steps. To further preserve accuracy, we adopt an isolated selection strategy in computing sparse patterns. Specifically, we observe that attention scores for generation tokens are relatively low during the early steps but gradually increase in later steps. Since SparseD computes sparse patterns in the early steps and reuses them in subsequent steps, this will cause the selection to concentrate primarily on prefill tokens with high attention scores. To address this issue, we separately select important scores for prefill and generation tokens, ensuring that both receive sufficient attention in selection. Together, these establish SparseD as practical and efficient solution for deploying DLMs, particularly in long-context applications. Experiments on recent DLMs, including Dream-7B-Instruct (Ye et al., 2025) and LLaDA-1.5 Zhu et al. (2025), demonstrate that SparseD greatly preserves the original accuracy with negligible loss while achieving up to 1.50 speedup over FlashAttention (Dao, 2023) at 64k context length with 1,024 diffusion steps. In summary, our main contributions are as follows: We identify three key attention patterns in DLMs: (1) attention scores vary across heads, (2) attention remains highly consistent across denoising steps, and (3) early diffusion steps are crucial for language generation. We propose SparseD, sparse attention method that accelerates DLM. It uses full attention and computes sparse patterns during early denoising steps, then reuses these patterns in later steps to restrict computation and improve efficiency. Extensive experiments show that SparseD greatly maintains accuracy on the evaluated benchmarks while achieving up to 1.50 speedup at 64k context length with 1,024 steps."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Diffusion Language Models (DLMs) Diffusion models (Ho et al., 2020; Rombach et al., 2022) have emerged as powerful paradigm in generative modeling, framing data generation as the inversion of forward-noise process. They have achieved remarkable success in continuous domains such as images (Peebles & Xie, 2023) and videos (Kong et al., 2024). More recently, diffusion models have also advanced the natural language processing area. DLMs (Ye et al., 2025; Nie et al., 2025) extend diffusion to discrete sequences by redefining noise injection and denoising(Ou et al., 2025; 2 Zheng et al., 2023). Unlike conventional autoregressive models (ARs) (Touvron et al., 2023; Yang et al., 2025) that generate tokens sequentially, DLMs denoise all tokens jointly in bidirectional manner. This parallel, bidirectional generation enables DLMs to achieve strong performance in both language understanding and generation, establishing them as promising alternative to ARs. Sparse Attention Despite their success, DLMs suffer from high inference latency, which remains major bottleneck. This issue is primarily due to the quadratic complexity of the core attention mechanism (Vaswani et al., 2017). This challenge has been extensively studied in traditional ARs. In ARs, many To address it, sparse attention has emerged as promising and mature solution. methods restrict attention computation to fixed patterns, such as sink attention (Xiao et al., 2023) and the sliding-window approach (Jiang et al., 2023). Other approaches (Zhang et al., 2025; Lai et al., 2025) identify distinct fixed patterns in ARs and dynamically select them for each head by computing approximate attention scores during inference. However, these methods still rely on patterns from ARs, and sparse attention in DLMs remains largely unexplored. Efficient DLMs Prior works on accelerating DLMs inference primarily focuses on cache-based approaches (Ma et al., 2025; Wu et al., 2025). For example, dKV-Cache (Ma et al., 2025) exploits the stability of activations in decoded tokens by caching their keyvalue states to reduce redundant computation. Fast-dLLM (Wu et al., 2025) further introduces block-wise caching scheme that caches both prefix and suffix tokens for improved efficiency. While these methods achieve substantial latency reduction, they suffer from noticeable accuracy degradation, especially in long-context scenarios. In this paper, instead of relying on cache-based techniques, we propose new sparse attention method that reduces inference latency with lossless accuracy."
        },
        {
            "title": "3.1 PRELIMINARY",
            "content": "Diffusion language models (DLMs) generate text via an iterative unmasking process over discrete denoising steps, gradually transforming masked sequence into the final output. Formally, let :l Vl denote the sequence state of length at step t, where denote the vocabulary, and let xt = 0, . . . , . The initial state is defined as xT :l = (c1, . . . , cp, [M ASK], . . . , [M ASK]), where (c1, . . . , cp) represents the prompt (prefill tokens), and the remaining positions are occupied by mask tokens to be generated (generation tokens). Through iterative denoising of both prefill and all generation tokens, DLMs achieve strong performance on language understanding and generation. However, denoising all tokens across all diffusion steps incurs substantial computational overhead due to the quadratic complexity of the attention mechanism with respect to sequence length l. This challenge becomes even more severe in the long-context setting. To tackle this challenge, sparse attention becomes promising solution. The sparse attention mechanism reduces redundant computation by focusing only on the most important query-key pairs, i.e., important attention scores. The attention score Rll is computed as the scaled dot product between the query matrix Rld and the key matrix Rld, normalized by the square root of the head dimension d. Formally, the attention score is defined as: = A(Q, K) = Sof tmax( 1 (Q )). (1) Each Ai,j can be viewed as the dot product between querykey pair Qi,: and Kj,:. To improve computational efficiency, sparse attention restricts computation to subset of querykey pairs. simple strategy is to retain only the top-ρ% pairs with the highest scores Ai,j for each query as sparse patterns. The overall index set for selected query-key pairs is defined as = (cid:91) iZ Si = (cid:91) iZ Topρ%{(i, j)j Z, ranked by Ai,j}. Then, the sparse attention mechanism is defined as: = A(Q, K, MS) = Sof tmax( 1 (Q + MS)). (2) (3) 3 Figure 1: Attention score across denoising steps using LLaDA-1.5 (l = 78, = 32, block length = 32). Rows correspond to different attention heads. Red lines divide key tokens in prefill and generation tokens. The result shows pronounced similarity across denoising steps. More visualized attention patterns from different DLMs are provided in the Appendix A.1. Here, MS is sparse attention pattern based on S, defined as: MS[i, j] = (cid:40) 0, , if (i, j) S, otherwise. (4) The goal of sparse attention is to minimize the discrepancy between A(Q, K, MS) and A(Q, K) , where Rld is the value matrix in the attention mechanism. Existing approaches in ARs achieve this goal either by using fixed sparse patterns (Xiao et al., 2023) or by dynamically selecting suitable patterns for each attention head (Lai et al., 2025; Zhang et al., 2025). The commonality among these methods is that they all rely on attention patterns observed in ARs. Although the attention mechanism in DLMs also exhibits clear sparse patterns (Figure 1), strategies developed for ARs are not well suited to DLMs. This incompatibility primarily arises from the distinct attention patterns observed in DLMs, as discussed in Section 3.2. To address this issue, we build on these unique observations and propose our method in Section 3.3."
        },
        {
            "title": "3.2 OBSERVATIONS",
            "content": "To enable sparse attention to accelerate DLMs inference while preserving accuracy, we conduct systematic analysis of attention patterns, which reveals three fundamental properties: Head-Specific Attention Patterns In the attention mechanism of DLMs, attention scores vary across heads, as shown in Figure 1(ac). For example, the second row exhibits column-wise pattern, while the third row shows sliding-window pattern. In the first row, the upper part follows sliding-window structure, whereas the lower part displays column-wise pattern. Such inconsistencies render widely used sparse attention methods in ARs, such as sliding-window (Jiang et al., 2023) and sink attention (Xiao et al., 2023), unsuitable for DLMs. Attention Similarity Across Time Although attention scores differ across heads, each of them remain highly consistent across denoising steps. As shown in Figure 1(d), the attention scores in each head exhibit high similarity across steps. Since sparse attention patterns are directly derived from attention scores, this consistency suggests that the sparse attention patterns for each head are also largely stable across steps, motivating the sparse reusing strategy in SparseD, detailed in Section 3.3. Significant Impact of Early Steps on Generation Unlike traditional AR models that generate each token sequentially from scratch, DLMs decode all tokens simultaneously. In this process, all 4 (a) 32 steps (b) 128 steps Figure 2: Influence of sparse attention across denoising steps. Experiments are conducted on LLaDA-1.5 (l = 256 and block length = 256) with denoising steps 32 and 128. FullSparse denotes applying full attention in the first steps and sparse attention in the remaining steps, while SparseFull is the opposite. Sparse attention retains only the top 30% of attention scores per query token (Equation 2). Results highlight the importance of early denoising steps. tokens undergo denoising across every denoising step. This raises natural question: do all denoising steps contribute equally? In other words, from sparse attention perspective, which diffusion steps can apply sparse attention with minimal impact on generation quality? To investigate this, we evaluate loss changes under different sparse attention configurations (Figure 2). As shown by the green and gray dashed lines, applying sparse attention in the early steps results in significant loss increase (left side of the gray dashed line), while extending it to additional steps causes only marginal further degradation (right side of the gray dashed line). This indicates that early steps are particularly sensitive to sparse attention. Conversely, the blue line shows that gradually transitioning from sparse to full attention in the early steps substantially reduces loss, further underscoring the critical role of early denoising steps in DLM text generation. These findings demonstrate that directly applying sparse attention methods from ARs in early steps leads to severe degradation in generation quality. In summary, the above findings show that widely used sparse patterns in ARs fail to capture the head-specific patterns of DLMs, and applying sparse attention to DLMs in the early steps leads to degradation in generation quality. To address these challenges, we propose SparseD for DLMs, based on these unique observations, as discussed in Section 3.3."
        },
        {
            "title": "3.3 SPARSED",
            "content": "At high level, SparseD is able to efficiently handle the unique attention patterns of DLMs without degrading generation quality. Specifically, SparseD uses full attention in early steps, and then pre-computes and reuses head-specific sparse patterns for sparse attention in subsequent steps. An overview of SparseD is shown in Figure 3, and this section details each of its components. Isolated Selection As discussed in Section 3.2, DLMs exhibit head-specific attention patterns. This makes fixed sparse patterns, e.g., the sliding-window scheme, insufficient to capture important attention scores in DLMs. To address this problem, we compute and select important attention scores for each attention head to form head-specific sparse patterns using Equation 2 and 4. Moreover, since some heads gradually increase attention score on generation tokens in the key dimension (right side of the red line in Figure 1(ac)), selecting dominant attention scores in the early stages may overlook the contribution from generation tokens. To address this issue, we separately select attention scores for prefill and generation tokens in the key dimension, applying the same selection ratio ρ% to both. Then, the index set for selected indices can be formulated as: = (cid:91) iZ Si = (cid:91) (Spre (cid:91) Sgen ), iZ (5) and Sgen where Spre refer to the index set in prefill and generation tokens, respectively. Considering hardware-friendly acceleration, we select important attention scores in block-wise manner. Specifically, we first apply the average pooling to A, formally = avgpool(A, block size) Rl//block sizel//block size. Then the selecting sets can be formulated as Spre = Topρ% Sgen = Topρ% (cid:8)(i, j) 1 p, ranked by (cid:8)(i, j) < l, ranked by i//block size, j//block size i//block size, j//block size (cid:9), (cid:9). (6) Figure 3: Overview of SparseD. SparseD first applies full attention during the early diffusion steps. It then pre-computes attention scores and selects the important scores using block-wise scheme, while performing isolated selection for prefill and generation tokens. The resulting sparse patterns are reused in the subsequent steps. However, computing the full A(Q, K) incurs substantial memory overhead. To address this, we partition Rld into smaller blocks Rblock sized and sequentially compute = avgpool(A(Q, K), blokc size) for each block, thereby reducing memory usage. Sparse Reusing As shown in Section 3.2, attention scores within each head show strong similarity over denoising steps. Leveraging this, we can only compute the sparse patterns once and reuse them in subsequent steps. Specifically, we calculate attention scores and select the top-ρ% important attention score (Equation 5). The resulting selection defines the sparse pattern Ms in Equation 4, which is then reused across denoising steps as shown in Equation 3. Skipping Sparse As discussed in Section 3.2, the early denoising steps are critical for language generation in DLMs, and applying sparse attention at this stage leads to substantial degradation in quality. To address this issue, we apply full attention during the initial skip% of denoising steps, thereby preserving generation performance. Specifically, full attention is applied during the first skip% steps. At step skip%, SparseD computes and selects head-specific sparse patterns, which are then reused for sparse attention throughout the remaining (1 skip%) steps. In summary, the overview of SparseD is shown in Figure 1 and the pseudo code is shown in Algorithm 1. Attention Output Full Attention(Q, K, ) if current step = skip% then for in range(l/block size) do Algorithm 1 SparseD 1: Input: Q, K, Rld, current step, , ρ%, skip%, block size 2: if current step <= skip% then 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: else 14: 15: end if 16: return Attention Output start = block size, end = (i + 1) block size = Qstart:end,: avgpool(A(Q, K), block size) IndexSelection(A, ρ%) MS[start : end, :] Attention Output A(Q, K, MS) end for end if 6 Skipping Sparse Isolated Selection Equation 5 and 6 Equation 4 Sparse Reusing"
        },
        {
            "title": "4.1 EXPERIMENTAL SETTING",
            "content": "Models: We evaluate all comparing methods on recent DLMs, including both LLaDA-1.5 (Zhu et al., 2025) and Dream-7B-Instruct (Ye et al., 2025) models. Baselines: We compare SparseD against the original models, the widely used sparse attention methods from ARs (Slide Window and StreamingLLM (Xiao et al., 2023)), and efficient DLM methods (dKV-Cache (Ma et al., 2025) and Fast-dLLM (Wu et al., 2025)). Datasets: Experiments are conducted on diverse set of benchmarks, including general language understanding (MMLU (Hendrycks et al., 2021)), mathematical reasoning (GSM8K (Cobbe et al., 2021)), code generation (HumanEval (Chen et al., 2021)), and long-context evaluation (RULER (Hsieh et al., 2024)). We use 5-shot for MMLU, 4-shot for GSM8K, and 0-shot for the other datasets. Implementation Details All experiments were conducted on NVIDIA A800 (80 GB) GPUs. The original DLMs were accelerated with FlashAttention (Dao, 2023). For the sliding-window method, we set the window size (ws) to 256 for short-context evaluations (MMLU, GSM8K, HumanEval) and to 2048 or 4096 for RULER with 4k and 8k contexts, respectively. For StreamingLLM, we set the same ws with the sliding-window method and initial 10% key tokens as sink tokens. For dKV-Cache, we set the cache refresh interval to 2 for the LLaDA-1.5 and to 4 for the Dream-7BInstruct. For Fast-dLLM, we set the threshold of 0.9, and block size to 8 for MMLU and 32 for other datasets. For SparseD, we set block size = 32 and ρ = 50% for short-context tasks, and block size = 128 with ρ = 30% for RULER. In all SparseD settings, we use skip = 20%. During the early steps employing full attention, we use FlashAttention as the accelerator, and afterward switch to FlexAttention (Dong et al., 2024), which supports customized sparse patterns. We evaluate accuracy across all datasets and measure latency by processing individual input samples of varying lengths from RULER. Details for datasets, models, and methods are provided in the Appendix A.2."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "Table 1: Comprehensive benchmark results on LLaDA-1.5 and Dream-7B-Instruct. MMLU GSM8k HE RULER-4k RULER-8k Avg. Dream-7B-Instruct + dKV-Cache + Fast-dLLM + Slide Window + StreamingLLM + SparseD LLaDA-1.5 + dKV-Cache + Fast-dLLM + Slide Window + StreamingLLM + SparseD 66.42 66.32 65.51 63.45 64.19 66.34 64.24 63.45 63.17 63.72 63.52 64.14 80.74 80.67 78.17 70.20 72.86 80.29 80.38 79.98 82.64 57.77 52.01 79.80 53.05 54.88 48.78 34.76 33.54 53. 40.85 40.85 40.24 27.44 37.20 40.85 90.13 81.41 81.68 41.46 43.94 89.76 90.45 88.18 86.64 39.20 40.39 90.89 71.79 55.08 55.64 34.36 36.36 72.47 60.73 57.11 47.76 36.32 36.62 62.44 72.42 67.67 65.95 48.84 50.17 72. 67.33 65.91 64.09 44.89 45.94 67.62 This section presents comparative evaluation of SparseD from accuracy and latency perspectives. Accuracy As shown in Table 1, SparseD achieves lossless performance compared with the original models. On average, it incurs only 0.04% accuracy drop on Dream-7B-Instruct and even yields 0.29% improvement on LLaDA-1.5. In contrast, compared with sparse attention methods in ARs, the sliding-window method and StreamingLLM struggle to handle head-specific attention patterns in DLMs, whereas SparseD delivers great performance in maintaining original capacity. Compared with efficient DLM approaches such as dKV-Cache and Fast-dLLM, SparseD shows clear advantages in long-context scenarios. Although cache-based methods perform well on short-context tasks, they experience significant accuracy degradation with long contexts. Compared with SparseD on RULER-8k, both dKV-Cache and Fast-dLLM show approximately 16% accuracy reduction on 7 Dream-7B-Instruct. Additionally, they exhibit 5.3% and 14.6% accuracy reductions on LLaDA1.5, respectively, highlighting their limitations. Detailed accuracy comparisons on the long-context RULER dataset are provided in Table 4 of Appendix A.3. Figure 4: Latency comparison (T =128) for Dream-7B-Instruct and LLaDA-1.5, evaluated on single sample from the RULER dataset with varying sequence lengths. Latency To evaluate the inference latency of SparseD, we compare it with the sparse attention method (StreamingLLM) and FlashAttention across different context lengths with 128 steps. For StreamingLLM, the window size is set to ws = 2 , and sink token length is set to sink = 10% l. As shown in Figure 4, SparseD matches FlashAttention at 4k and 8k length, and demonstrates clear advantages beyond 16k length. In particular, at 64k, SparseD achieves 1.23 and 1.25 speedups over FlashAttention on Dream-7B-Instruct and LLaDA-1.5 model, respectively. Although SparseD achieves similar acceleration compared with StreamingLLM, our method maintain accuracy with lossless loss while StreamingLLM lead to great degradation in generation. Beyond evaluation at 128 diffusion steps, we further assess SparseD under varying numbers of diffusion steps. As shown in Figure 5, the results demonstrate gradual increase in acceleration compared to FlashAttention. At 128 steps, SparseD achieves 1.23 and 1.25 speedups over FlashAttention on Dream-7B-Instruct and LLaDA-1.5, respectively. At 1024 steps, the speedups increase to 1.50 and 1.48, respectively. This efficiency gain arises because sparse attention patterns are pre-computed only once and reused across all denoising steps. Consequently, in scenarios with long contexts and many diffusion steps, SparseD effectively amortizes the computational cost. Figure 5: Latency comparison of SparseD on Dream-7B-Instruct and LLaDA-1.5 across varying diffusion steps, evaluated on single RULER sample with 64k context length."
        },
        {
            "title": "4.3 ABLATIONS",
            "content": "In this section, we conduct extensive ablation studies to evaluate the components of SparseD and their effects under different configurations. We first validate the effectiveness of its core componentsskipping sparse, sparse reusing, and isolated selection. Next, we analyze the two key hyperparameters of SparseD: the skipping ratio (skip) and the selection ratio (ρ). 8 Table 2: Ablation study of SparseD on LLaDA1.5. Each component is excluded individually to assess its contribution. Accuracy is measured on RULER at 4k length, and latency is evaluated on 64k-length RULER sample. The relative changes compared to SparseD are highlighted in gray. Effectiveness of Each Component As shown in Table 2, we conduct an ablation study to evaluate the effectiveness of each component in SparseD. Removing skipping sparse attention (third row) causes severe accuracy drop, highlighting its importance in preventing degradation during early steps. Recomputing sparse patterns at every denoising step (fourth latency due to row) repeated computations, whereas reusing sparse patterns (second row) achieves comparable accuracy with far lower latency. Furthermore, excluding isolated selection (last row) decreases accuracy, while enabling it (second row) improves accuracy with negligible latency overhead. In summary, these results collectively confirm that all three components are crucial for making SparseD both effective and efficient for DLMs. FlashAttention SparseD 87.91 (-3.07%) 1552 (-8.43%) - Skipping Sparse - Sparse Reusing 90.82 (-0.07%) 30020 (+1671%) - Isolated Selection 90.53 (-0.36%) 1687 (-0.47%) introduces substantial RULER (%) LLaDA-1.5 90.45 90.89 Latency (s) 2127 1695 (a) Analysis of skip%. (b) Analysis of ρ% on Dream. (c) Analysis of ρ% on LLaDA. Figure 6: Hyper-parameter analysis in Dream-7B-Instruct and LLaDA-1.5 with RULER-4k dataset. Skipping Ratio As shown in Figure 6a, for both LLaDA-1.5 and Dream-Instruct, increasing the skipping ratio in SparseD improves accuracy. LLaDA-1.5 shows stronger gain, reaching above 90.89% accuracy at skip = 20%. Dream-Instruct steadily increases and plateaus at 90.00% accuracy once the skipping ratio exceeds skip = 30%. This suggests moderate skipping ratio (2030%) achieves the best balance. This result further verifies the observation in Figure 2. For an optimal balance between accuracy and efficiency, we set skip = 20% in all experiments for both models. Selecting Ratio For the Dream-7B-Instruct model, as shown in Figure 6b, accuracy rises sharply from around 40% at ρ = 5% to nearly 89.76% at ρ = 30%, after which it saturates. In contrast, latency increases steadily with higher ρ. similar trend is observed for LLaDA-1.5 in Figure 6c, with accuracy increasing until ρ = 20% and then saturating. To achieve balanced trade-off between accuracy and efficiency, we set ρ = 30% for long-context experiments."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "In this paper, we propose novel sparse attention method, SparseD, for DLMs. The design of SparseD is based on three key observations in DLMs: (1) attention patterns vary across attention heads, showing head-specific patterns, (2) attention patterns remain highly consistent across denoising steps, and (3) early diffusion steps are crucial for effective language generation. Leveraging these insights, SparseD pre-computes sparse attention patterns for each head once and reuses them across diffusion steps. Additionally, SparseD applies full attention in the early steps and skips sparse attention to preserve generation quality. These designs enable SparseD to efficiently handle head-specific patterns while avoiding degradation in generation quality, making it practical and effective solution for deploying DLMs in long-context applications. Extensive experiments demonstrate that SparseD achieves lossless performance on all tested benchmarks while delivering up to 1.50 speedup over FlashAttention at 64k context length with 1,024 diffusion steps."
        },
        {
            "title": "REFERENCES",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference. In The Thirteenth International Conference on Learning Representations, 2025. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. In The Thirteenth International Conference on Learning Representations, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 10 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding, 2025. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Yu Zhang, Dong Guo, Fang Wu, Guoliang Zhu, Dian Ding, and Yiming Zhang. Anchorattention: Difference-aware sparse attention with stripe granularity. arXiv, 2025. Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. reparameterized discrete diffusion model for text generation. arXiv preprint arXiv:2302.05737, 2023. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ATTENTION PATTERNS In this section, we visualize broader range of attention patterns in DLMs to further support the observations discussed in Section 3.2. Specifically, the attention patterns of LLaDA-8B-Base and Dream-7B-Instruct are shown in Figures 7 and 8, respectively. As illustrated in Figures 7(ac) and 8(ac), both models display distinct head-specific attention patterns. Moreover, Figures 7(d) and 8(d) show strong consistency across denoising steps. These findings align well with the observations in Section 3.2. Admittedly, certain corner cases exist. For example, the last rows of Figures 7(d) and 8(d) reveal instances with reduced similarity across steps. In Figure 7(d), although the early steps deviate, the later steps still maintain strong similarity. In Figure 8(d), the similarity appears in block-wise manner; nevertheless, it still remains above 60% similarity across steps. Moreover, such cases are rare among all attention heads. Figure 7: Attention score across denoising steps using LLaDA-8B-Base (l = 102, = 32, block length = 32). Rows correspond to different attention heads. Red lines divide key tokens in prefill and generation tokens. The result shows pronounced similarity across denoising steps. A.2 EXPERIMENTAL DETAILS In this section, we provide the detailed evaluation settings described in Section 4. Specifically, we report the parameters used for different models, methods, and datasets. Datasets Experiments are conducted on diverse set of benchmarks, including general language understanding (MMLU (Hendrycks et al., 2021)), mathematical reasoning (GSM8K (Cobbe et al., 2021)), code generation (HumanEval (Chen et al., 2021)), and long-context evaluation (RULER (Hsieh et al., 2024)). The RULER dataset consists of 13 subtasks for comprehensive evaluation, including niah single 1, niah single 2, niah single 3, niah multikey 1, niah multikey 2, niah multikey 3, niah multiquery, niah multivalue, ruler vt, ruler fwe, ruler qa squad, and ruler qa hotpot. Depending on the specific subtask, we set different evaluation parametersparticularly and lto enable efficient evaluation. The details are shown in the Table 3. 12 Figure 8: Attention score across denoising steps using Dream-7B-Instruct (l = 83, = 32). Rows correspond to different attention heads. Red lines divide key tokens in prefill and generation tokens. The result shows pronounced similarity across denoising steps. Methods We compare SparseD with the slide-window approach, StreamingLLM, dKV-Cache, and Fast-dLLM. For SparseD, the slide-window method and StreamingLLM. Details of SparseD, the slide-window approach, and StreamingLLM are shown in Table 3. Note that in StreamingLLM, sink refers to the ratio of initial tokens designated as sink tokens. For the slide-window approach and StreamingLLM, we use FlexAttention for acceleration. For dKV-Cache, we employ the dKV-Cache-PD variant on the Dream-7B-Instruct model, setting the cache refresh interval to 4. For the LLaDA-1.5 model, we adopt the dKV-Cache-Greedy variant, with cache refresh interval of 2 and window size of 4. For Fast-dLLM, we use the Prefix KV Cache version, setting the threshold to 0.9, with block size of 8 for MMLU and 32 for the other datasets. A.3 EVALUATION DETAILS ON RULER DATASET Since we use different configurations for evaluating the subtasks of the RULER dataset, in this section, we present the detailed accuracy results for each subtask. Specifically, Table 4 provides the detailed breakdown corresponding to Table 1, while Table 5 presents the detailed results corresponding to Table 2. A.4 LIMITATIONS This work presents novel sparse attention method for DLMs that efficiently adapts to head-specific patterns and avoids generation degradation in early denoising steps. However, several limitations remain. One primary limitation of this work lies in its focus on algorithmic design. Future work could explore further system-level optimizations, both for computing sparse attention patterns and for accelerating head-specific sparse patterns. Another aspect is combining our method, sparse attention-based approach, with cache-based methods. The key to implementation is how to retain the advantages of both the lossless sparse attention method and the fast cache-based method simultaneously. 13 Table 3: Parameters of Evaluation. BS represents the batch size. For the RULER dataset with varying lengths, certain parameters (ws, BS) are configured differently, with specific changes indicated in parentheses to denote the version being used."
        },
        {
            "title": "MMLU",
            "content": "GSM8k HE Dream-7B -Instruct few shot=5 BS=1 =8 l=8 temperature=0.1 top p=0.9 few shot=4 BS=1 =256 l=256 temperature=0.1 top p=0.9 few shot=0 BS=1 =512 l=512 temperature=0.1 top p=0. + Slide Window ws=256 ws=256 ws=256 + StreamingLLM ws= ws=256 ws=256 sink=20%ws sink=10%l sink=10%l niah single 1 niah single 2 niah multikey 1 niah multikey 2 ruler vt ruler qa squad ruler qa hotpot few shot=0 BS=16(4k)/8(8k) =32 l=32 temperature=0.1 top p=0.9 ws=2048(4k) ws=4096(8k) ws=2048(4k) ws=4096(8k) sink=10%l niah single 3 niah multikey 3 niah multiquery niah multivalue ruler fwe few shot=0 BS=16(4k)/8(8k) =64 l=64 temperature=0.1 top p=0.9 ws=2048(4k) ws=4096(8k) ws=2048(4k) ws=4096(8k) sink=10%l ruler cwe few shot=0 BS=16(4k)/8(8k) =128 l=128 temperature=0.1 top p=0.9 ws=2048(4k) ws=4096(8k) ws=2048(4k) ws=4096(8k) sink=10%l block size=32 ρ = 0.5 skip = 20% block size=32 ρ = 0.5 skip = 20% block size=32 ρ = 0.5 skip = 20% block size=128 ρ = 0.3 skip = 20% block size=128 ρ = 0.3 skip = 20% block size=128 ρ = 0.3 skip = 20% + SparseD LLaDA-1.5 few shot=5 BS=1 =8 l=8 block length=8 few shot=4 BS=1 =256 l=256 block length=32 few shot=0 BS=1 =512 l=512 block length=32 + Slide Window ws= ws=256 ws=256 + StreamingLLM ws=256 ws=256 ws= sink=20%ws sink=10%l sink=10%l few shot=0 BS=1 =32 l=32 block length=32 ws=2048(4k) ws=4096(8k) ws=2048(4k) ws=4096(8k) sink=10%l few shot=0 BS=1 =64 l=64 block length=64 ws=2048(4k) ws=4096(8k) ws=2048(4k) ws=4096(8k) sink=10%l few shot=0 BS=1 =128 l=128 block length=128 ws=2048(4k) ws=4096(8k) ws=2048(4k) ws=4096(8k) sink=10%l + SparseD block size=32 ρ = 0.5 skip = 20% block size=32 ρ = 0.5 skip = 20% block size=32 ρ = 0.5 skip = 20% block size=128 ρ = 0.3 skip = 20% block size=128 ρ = 0.3 skip = 20% block size=128 ρ = 0.3 skip = 20% Table 4: Evaluation details of RULER in main results  (Table 1)  . 1 2 3 MK 1 MK 2 MK MQ MV VT CWE niah Dream-7B-Instruct + dKV-Cache + Fast-dLLM + Slide Window + StreamingLLM + SparseD LLaDA-1.5 + dKV-Cache + Fast-dLLM + Slide Window + StreamingLLM + SparseD Dream-7B-Instruct + dKV-Cache + Fast-dLLM + Slide Window + StreamingLLM + SparseD LLaDA-1.5 + dKV-Cache + Fast-dLLM + Slide Window + StreamingLLM + SparseD 100.00 100.00 100.00 26.00 32.00 100.00 100.00 100.00 100.00 25.60 25.80 100.00 99.80 89.20 89.00 26.80 37.00 99. 63.00 63.20 53.80 26.20 26.20 74.00 100.00 98.88 98.20 29.40 29.60 100.00 100.00 100.00 100.00 29.40 29.40 100.00 90.80 71.20 69.80 29.60 29.60 91.40 71.40 74.40 66.60 29.60 29.60 75.20 93.00 81.40 78.00 28.20 28.80 93. 100.00 96.60 88.40 28.20 28.20 100.00 67.20 40.00 39.20 28.60 28.60 71.60 58.20 54.60 40.80 28.60 28.60 58.40 98.60 83.40 83.40 30.40 31.00 97.40 100.00 100.00 100.00 30.40 30.40 100.00 75.40 48.40 47.60 31.40 33.00 76. 64.40 64.60 58.20 31.80 31.80 67.20 97.45 87.10 88.25 26.80 27.90 97.00 95.05 99.40 99.95 24.35 27.70 100.00 93.10 62.45 67.25 29.45 29.95 93.55 69.00 59.15 54.50 30.40 30.45 69.50 98.75 82.10 88.75 27.30 28.45 97. 99.80 94.20 98.05 24.05 27.30 98.60 92.70 47.90 58.65 28.05 28.70 93.75 64.55 59.60 45.25 28.15 28.15 62.35 96.36 91.12 91.00 27.28 36.84 96.80 100.00 96.60 88.16 38.64 38.68 100.00 63.08 32.00 30.68 30.84 31.48 65. 63.96 54.84 29.96 38.60 38.64 66.60 87.12 72.48 73.58 86.60 86.78 83.50 56.24 50.86 45.32 55.98 38.62 53.74 57.50 49.08 45.64 61.34 61.04 57.42 67.12 64.92 26.50 57.94 56.62 57.44 4K-Length 99.80 99.60 99.60 28.40 28.80 99.60 100.00 100.00 100.00 28.20 28.20 100.00 79.80 44.00 44.20 22.00 22.20 79.60 100.00 96.20 95.60 42.00 42.00 100.00 8K-Length 71.80 71.00 71.40 20.40 20.40 72. 55.80 56.60 53.80 26.00 26.00 60.60 28.40 16.20 15.60 21.90 21.20 28.40 41.40 27.60 35.40 20.20 20.20 41.60 14 ruler FWE 78.73 75.40 74.47 80.33 88.13 80. 63.80 56.67 51.13 52.13 73.87 68.87 79.27 76.53 76.27 61.93 72.53 79.53 61.60 55.13 45.47 71.47 76.93 68.60 QS QH AVG 78.68 78.75 78.62 79.53 81.98 79.22 83.17 80.47 83.00 80.30 82.30 83.40 58.45 56.55 56.68 27.97 29.70 56.88 49.95 48.87 50.92 27.27 27.37 50.68 63.40 64.20 63.80 46.80 48.80 63.00 77.80 75.40 76.80 50.40 52.60 77. 55.80 55.60 55.60 48.40 49.60 56.20 59.20 59.00 59.80 56.00 55.60 59.60 90.13 81.41 81.68 41.46 43.94 89.76 90.45 88.18 86.64 39.20 40.39 90.89 71.79 55.08 55.64 34.36 36.36 72.47 60.73 57.11 47.76 36.32 36.62 62. Table 5: Evaluation details of RULER-4k in ablation study  (Table 2)  . niah LLaDA-1.5 1 2 MK 1 MK 2 MK 3 MQ MV VT CWE FlashAttention SparseD - Skipping Sparse - Sparse Reusing - Isolated Selection 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 99. 95.05 100.00 99.95 100.00 99.95 99.80 98.60 95.95 98.50 97.65 100.00 100.00 97.20 100.00 98.88 56.24 53.74 34.02 53.26 53.88 ruler FWE 63.80 68.87 59.80 68.60 68. QS QH AVG 83.17 83.40 81.43 83.60 82.47 77.80 77.00 74.60 76.80 76.60 90.45 90.89 87.91 90.82 90."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "The Hong Kong Polytechnic University"
    ]
}