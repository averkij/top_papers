{
    "paper_title": "Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels",
    "authors": [
        "Julius Pesonen",
        "Stefan Rua",
        "Josef Taher",
        "Niko Koivumäki",
        "Xiaowei Yu",
        "Eija Honkavaara"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 2 2 0 3 1 . 2 0 6 2 : r Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels Julius Pesonena,b,c,, Stefan Ruaa, Josef Tahera, Niko Koivumäkia, Xiaowei Yua, Eija Honkavaaraa aDepartment of Remote Sensing and Photogrammetry, Finnish Geospatial Research Institute, Espoo, 02150, Finland bDepartment of Computer Science, Aalto University, Espoo, 02150, Finland cDOTA, ONERA, Université de Toulouse, Toulouse, 31000, France Abstract Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task. Keywords: Remote sensing, Deep learning, Sensor fusion, Weakly supervised learning, Instance segmentation, Individual tree crown delineation 1. Introduction Mapping individual tree crowns helps us take care of our environment by enabling critical tasks such as urban tree inventorying and forest health monitoring. High-resolution maps on urban trees help us, for example, to understand the effect of the trees on the health of people living in cities [1] and to quantify their carbon storage [2, 3]. Mapping forests, while essential for forestry [4], also serves other important purposes, such as to aid us protect them from growing number of pests [5, 6] and to assess the risk of forest fires [7]. Tree inventories can be conducted and maintained using various data sources, ranging from ground-based expert measurements [8] to automated satellite image analysis [9, 10]. Aerial data collection lies between these methods, offering quicker mapping of larger areas than field measurements and higher spatial resolution than satellite imaging. However, automatically separating individual trees from each other and the background in such data, to create high-resolution Corresponding author Email address: julius.pesonen@nls.fi (Julius Pesonen) tree maps, is non-trivial. In recent years, the best results have mostly been obtained using machine learning-based methods [11], which rely on large amounts of, typically human-annotated, training data. The imagery from which the trees are detected also varies drastically depending on the geographic locations, the used sensors, the weather, and the times of the year or day. Thus, to obtain methods that work reliably in these various possible scenarios, efficient training data collection methods are required. The process of separating individual trees is called tree crown instance segmentation, and it is also referred to and used interchangeably in this study as individual tree crown delineation. The works where automated methods for the task have been studied can be most clearly divided between lidarand camera-based methods. In both, recent development focus has been on leveraging learning-based methods. The intersection of the methods based on the two distinct sensor types has, however, been left largely unexplored, besides some works directly using the fused sensor data to generate the tree shape estimates [12, 13]. It is major challenge to obtain sufficient annotated training data for learning-based methods, as it is unfeasible to create hand-annotated datasets for all possible types of locations and tree species. This work shows that RGB and multispectral image models can be trained on the task using canopy height model (CHM)-based segments derived from low-resolution aerial laser scanning (ALS) surveys. We compared the models to alternative image-based methods, which showed that training the image-based models, even with coarse or noisy labels, provides more robust results than the current zero-shot model options. This presents new solution for obtaining both tree segment information for areas where ALS surveys and aerial image collection have been conducted, as well as new way to obtain training data for tree crown instance segmentation models, which can be deployed in areas where only image data is available. In addition, our results show major benefit of using zero-shot instance segmentation model, SAM 2 [14], as processing step when generating image-labels of tree canopies from the coarse CHM-based segments. The use of SAM 2 preprocessing for this specific task was first introduced by Stefan Rua in his masters thesis [15], which this study extends. 2. Related work In recent years, individual-tree-based approaches have gained increasing attention, making tree crown segmentation crucial topic in forest monitoring and analysis. Tree crowns are usually detected using either lidar, optical sensors, or both. Lidar data has the problem of being less readily available than optical camera images, due to the relative cost and rarity of the sensors, while the best performing methods using optical sensor data are typically based on supervised learning, meaning that they require, usually hand-crafted, labels to train. Various supervised models have been trained for the task on different datasets. Models which were publicly available at the date of this study, and thus available for testing on our data, have been described in Section 3.6. The metric results obtained using such models vary heavily based on the data. The variation is caused by many factors, such as the resolution of the data, the similarity between test and training data, and the number of training samples. Relevant metrics compiled in survey, also considering lidar data and semantic segmentation, include F1 scores ranging from 0.82 to 0.98, precision from 0.85 to 0.91, recall from 0.88 to 0.91, and mean intersection over union (mIoU) from 0.49 to 0.94 [16]. The test metrics in the paper introducing one of the baseline methods, Detectree2 [17], include precision ranging from 0.60 to 0.71, recall from 0.54 to 0.66, and F1 score from 0.57 to 0.69, depending on the test area. Similarly, the results from study using supervised U-net on aerial imagery [18], also tested as 2 baseline method in this study, yielded precision of 0.71 and recall of 0.66 on aerial images. On satellite images, the model performed slightly worse with precision of 0.63 and recall of 0.64. Using lidar-derived tree segments to train image-based models is way to leverage the benefits of both modalities. This has been studied on data from the taiga-tundra ecotone with low tree density [19] and in Germany with high-density lidar data [20]. Unfortunately, acquiring high-density lidar data is costly compared to similar resolution optical imagery. Using lowerresolution lidar data to generate tree crown segments, however, results in noisy labels. To enhance the labels using information from optical images, zero-shot segmentation models such as the Segment Anything model (SAM) [21], or its more recent variants SAM 2 or 3 [22] can be used. The use of SAM for tree segmentation has been studied using learning-based prompts [23] or digital surface model-derived prompts [24]. SAM 2 has also been deployed on the task by combining it with the general tree detection model DeepForest [25, 26]. Finally, Grounded SAM [27], combination of Grounding DINO [28] and Segment Anything Model [21], has been used for tree mapping in the context of disaster management [29]. Different ways of prompting SAM 2, using either lidar, manual and model-based prompts, have also been evaluated on their direct individual tree crown delineation performance [30]. Prior works have not explored the possibility of using SAM-enhanced lidar-based segments to train optical image-based models for segmenting individual tree crowns. This presents unique opportunity to obtain higher-quality, domain-specific, annotated data for training the models. 3. Materials and methods 3.1. Data We studied the methods using multispectral imagery and point clouds. The used image data consisted of an orthophoto with spatial resolution of 5 cm collected in July 2023. The size of the complete orthophoto was 31 254 by 36 871 pixels, spanning an area of approximately 2 km2. The multispectral imagery was captured from helicopter with MicaSense Altum-PT at an altitude of 120 m. Micasense Altum-PT acquires five multispectral bands at 3.2 MP each, 12.4 MP panchromatic band, and thermal band. Agisoft Metashape Professional [31] was used for photogrammetric processing of the imagery. total of 66 ground control points extracted from lidar data were used during the bundle block adjustment step to improve alignment of the imagery. For this study, only the five multispectral bands were used. The bands were 475 nm (blue), 560 nm (green), 668 nm (red), 717 nm (red edge, RE), and 842 nm (near infrared, NIR). The respective bandwidths were 32 nm, 27 nm, 16 nm, 12 nm, and 57 nm. The multispectral airborne laser scanning (ALS) point clouds were acquired on 14 June 2016, during leaf-on conditions, using the Optech Titan sensor (Teledyne Optech, Ontario, Canada). TerraTec Oy (Helsinki, Finland) operated fixed-wing data acquisition flight at an altitude of 700 over the study area, generating three distinct point clouds corresponding to the 1550 nm (infrared), 1064 nm (near-infrared), and 532 nm (green) laser wavelength channels. The raw point clouds underwent initial radiometric and geometric calibration, which included intensity range-normalisation (further described in work by Matikainen et al. [32]) and the removal of redundant flight-line overlaps to ensure spatial homogeneity. The resulting point clouds exhibited mean point densities of 11, 13, and 11 points/m2 for channels 1, 2, and 3, respectively. All data were georeferenced to the ETRS-TM35FIN coordinate system, with vertical values referenced to the N2000 orthometric height system. 3 We reserved part of the data for testing. The part was selected as single area of the orthophoto with size of 4471 by 4893 pixels. The trees in this area were manually annotated according to human perception of the same orthophoto. The main results consist of typical segmentation metrics between the model-generated tree segments and the human-drawn annotations. The orthophoto of the area and the separated test area are shown in Figure 1. The test area contained 362 manually segmented trees, and the rest of the orthophoto, used for training and validation, contained approximately 24 000 trees. Figure 1: RGB visualisation of the orthophoto spanning the whole dataset region on the left, with the manually annotated test region delineated in light green and shown separately on the right. The manual test annotations are delineated in white. The dataset was captured from Espoonlahti district of Espoo, Finland (centred at approximately 60.1462N and 24.6587E), location which is characterised by its diversity of forest types, ranging from typical Fennoscandian coastal landscape to managed Finnish boreal forests. Based on the statistics from the study of Taher et al. [33], in the region, the tree species distribution consists of approximately 37% pine (Pinus sylvestris), 25% birch (Betula sp.), 11% maple (Acer platanoides), 11% spruce (Picea sp.), 8% aspen (Populus tremula), 3% rowan (Sorbus sp.), 3% linden (Tilia sp.), 2% alder (Alnus sp.), and 1% Oak (Quercus robur). 3.2. Coarse segments After the initial processing, individual tree crown labels were derived from the Optech Titan point clouds. Classification of returns into ground, vegetation, building, and noise classes was first performed using the LAStools software suite [34] . Digital Terrain Model (DTM) was then interpolated from the ground-classified points, enabling normalisation of vegetation returns to height above ground level. ITC delineation was carried out using variable-window watershed segmentation approach by Kaartinen et al. [35]. 0.5 resolution Canopy Height Model was generated from the combined first returns of Channels 1 and 2 (1550 nm and 1064 nm) to capture canopy structure in detail. After smoothing the CHM with Gaussian filter, treetops were identified using local 4 maximum filter. These treetop locations served as markers for marker-controlled watershed algorithm, which was applied to delineate individual crown boundaries. The resulting individual tree crown segments were subsequently used as pseudo-labels in the downstream processing steps. more detailed description of the LiDAR data acquisition and processing pipeline has been provided in the studies by Karila et al. [36] and Taher et al. [33]. As the point cloud was collected seven years earlier than the multispectral imagery, there were quite few false positive segments in regions replaced by built environments. The false positive segments were filtered using threshold of the normalised difference vegetation index NDVI = (RE Red)/(RE + Red). (1) The index was computed from the multispectral orthophoto. The threshold was set to 0.2, where segments with an average NDVI less than that were discarded. The threshold was set based on manual observations of the data. histogram of the NDVI distribution of the segments, shown in Figure A.1 of the appendix, highlights that most segments fall either clearly over or clearly under the set threshold. 3.3. SAM 2 enhancement Due to the low resolution of the available lidar point clouds, the CHM-based segments were very blocky. To generate more precise annotations, we leveraged the SAM 2. SAM 2 is model that takes an image and queries, such as points or bounding boxes, as input, and outputs segments corresponding to said queries. To generate masks of the tree crowns, we used bounding boxes of the coarse labels (the smallest rectangle containing all coarse label pixels) as the query inputs for SAM 2. To avoid segmenting only partly visible trees, the orthophoto was sampled in patches with size of 1024 by 1024 pixels using stride of 512 pixels. This meant that all trees could be captured by only pseudo-labelling the trees, whose centroids were found in the 512 by 512 pixel square in the middle of each patch. This specific configuration was chosen, as from the distribution of tree crown sizes, it was found that only tiny fraction of the trees had large enough crown that they could extend from the middle area to outside of the image. The SAM 2-generated pseudo-labels are visualised on five test set samples in Figure 2 alongside the manual and coarse segments. 3.4. Pseudo-supervised model We used the CHM and SAM 2-derived labels to train downstream model which directly predicts individual tree crown masks from RGB or multispectral images. An overview of the model training and evaluation scheme is shown in Figure 3. As our model architecture, we chose an improved version of the Mask R-CNN [37, 38] with an ImageNet [39] pretrained ResNet-50FPN backbone [40, 41]. The specific configuration was chosen due to its prominence in both general and tree crown instance segmentation literature. For multispectral variants using RGB and red edge or near-infrared bands, we adapted the model by repeating the parameters of the first image layers. The models were optimise based on the simple sum loss function = Lcls + Lbox + Lmask, where Lcls is classification loss, Lbox is box prediction loss, and Lmask is mask loss. The loss is described in detail in the original work introducing the Mask R-CNN architecture [37]. The models were trained using the Adam optimiser [42] with decoupled weight decay [43, 44]. Both the training and validation loss were computed between the automatically generated coarse 5 Figure 2: Visualisation of the different labels on the test set. labels and the SAM 2-enhanced pseudo-labels. During the training process, pre-sampled set of image patches, sized 1024 by 1024 pixels, was used for both training and validation. The validation set was sampled with pseudo-random algorithm, and the same validation split was used for all model variations. The validation set contained 209 (19%) images, and the training set 886 (81%). The training images were augmented with horizontal and vertical flipping, random rotations [-180, 180], and random IoU crops, as introduced in work by Liu et al [45]. The random IoU crops varied between scales of 0.3 to 1.0, and aspect ratios of 0.5 to 2.0. Each augmentation was performed independently and sampled uniformly in the corresponding range. The validation set was used for choosing the best model obtained during training process of 400 epochs, and to optimise the training and inference hyperparameters through trial and error. The final test set evaluated models were trained with learning rate of 1e-6, batch size of 8, and weight decay of 1e-2. For inference, we used mask threshold of 0.5, score threshold of 0.3, and non-maximum suppression (NMS) IoU of 0.3. NMS removes overlapping output masks based on the set threshold, keeping the mask with the higher score. 3.5. Evaluation We evaluated all methods on the geographically separated and manually labelled test area. For most experiments, the area was split into 70 patches of size 1024 by 1024 pixels. As one of the baseline models was trained with smaller patches, we used patches of size 512 by 512 as they produced better results. The patches were used to compute precision, recall, F1 score, and the Jaccard Index (mIoU) for the predictions by each model. For the precision, recall, and F1 computations, an overlap threshold of 50% between the predictions and ground truth segments was used. Due to having such small test set, we estimated the 95% confidence intervals of our results using test set bootstrapping. The bootstrapping was performed at the individual tree level with 6 Figure 3: Overview of the proposed data fusion method for the pseudo-supervised model training. 1000 resampled sets. The confidence intervals provide more representative estimate of how well the method holds for other data, discounting significant domain shifts. We used the same evaluation metrics for all methods, including the coarse and pseudo-labels generated using the CHM and SAM 2 enhancement, respectively. 3.6. Baselines We compared the pseudo-supervised Mask R-CNN models to multiple different baselines. As our method can be deployed without the requirement of locally collected, manually annotated training data, we dismissed alternatives which required such. Thus, the baseline methods represent alternative solutions to obtaining tree crown segment predictions, without leveraging local, manually annotated data for training or fine-tuning. The tested baselines are presented below. Besides them, we also considered both the coarse and SAM 2-enhanced masks as alternative tree crown segmentation methods, as they can be deployed directly in locations with ALS data available, such as the test location of this study. We only considered methods for which the source code was publicly available. Detectree2. family of fully supervised Mask R-CNN models trained using other available tree crown instance segmentation data, called Detectree2, is one of the most prominent models used for the task [17]. At the date of this study, seven pretrained Detectree2 models were available, and we evaluated the performance of each. Each of the models used the same Mask R-CNN architecture with ResNet 101 FPN backbone, and they differed from each other in their training data compositions and training strategies. The pretrained model checkpoints were available on Zenodo [46]. Following the authors naming convention, the models are called withParacouUAV, model trained with UAV and aeroplane captured imagery from tropical areas, randresize_full, model trained with the same data but also using random resize augment, urban_trees_Cambridge, model trained with urban 7 data from Cambridge, UK, base, model trained with aeroplane captured tropical data and introduced as the base model in an earlier work by Ball et al. [47], 05dates, copy of the base model further tuned with 5 cm resolution UAV-captured RGB imagery from five different dates and referred to as the 5 date model in the same prior work by Ball et al., flexi, model trained with data from both urban and forested areas, and tropical_closed_canopy, later version of model focused on tropical forest areas. As all the models were trained with image patches of approximately 1000 by 1000 pixels with spatial resolutions varying from approximately 5 to 10 cm, we deployed the models directly on the same 1024 by 1024 pixel patches as our proposed methods. Supervised U-net. publicly available, fully supervised tree crown instance segmentation model has also been developed based on the U-net architecture [18]. The model has been tested with 5cm resolution aerial imagery, and we deployed the pretrained model on our data as is. Besides the RGB image channels used by the other baseline models, the U-net also uses the red edge channel. The preprocessing also contains step to generate an NDVI image channel based on the red and red edge bands as shown in Equation 1. The model has been developed for inference on image patches of size 224 by 224 pixels. As such, we investigated the accuracy of few options with different image patching and resizing configurations. In the results, we report the best obtained metrics with the configuration of patching the ortho into images of size 512 by 512 pixels and resizing them to 224 by 224 pixels before passing them to the neural network. Grounded SAM (2). fusion of Grounded DINO [28] and Segment anything model [21], and the latter version leveraging SAM 2 [14]. The model leverages zero-shot bounding boxes from the Grounded DINO model to prompt SAM (2) [27]. The method has been applied in the context of tree crown segmentation to aid in the construction of the FMARS dataset [29]. The authors of the FMARS dataset stated that the prompt \"bushes\" worked the best for detecting trees in aerial images. We tested this alongside few other prompts and arrived at the same conclusion. DeepForest + SAM 2. DeepForest is bounding box detection model for tree crowns. It has been trained with various tree crown data, promising at least some generalisability across datasets [25]. To leverage the model for segmentation, we combined it with SAM 2, which was used by prompting it with the bounding boxes generated by DeepForest. SAM 3. The latest version, 3, of the Segment Anything Model comes alongside native feature for text prompting, meaning that it can be directly deployed on images without finetuning or leveraging external object detection models [22]. We trialled SAM 3 on our data with few different prompts, including \"bushes\" and \"tree crown\", and noted that the simple prompt \"tree\" performed the best. 4. Experiments and results The results show that our method outperforms all out-of-the-box alternatives on the Espoonlahti dataset. Intuitively, this is not surprising result as our model leveraged training data from the same location. However, it is important to note that none of the data was fully annotated, but instead, the process to generate the pseudo-labels used to train the model could be fully automated for areas in which ALS data is available. The results from our best performing model, using RGB and near infrared inputs (RGB NIR), the one using only RGB inputs (RGB), and all of the baseline methods, are collected in Table 1. Method F1 Precision Recall mIoU mIoU 95% CI 0.556 Detectree2* U-net 0.306 Grounded SAM 0.166 0.501 DeepForest 0.558 SAM 3 0.571 Coarse masks 0.597 Pseudo masks 0.758 RGB 0.778 RGB NIR 0.911 0.214 0.430 0.544 0.822 0.550 0.584 0.783 0. 0.400 0.536 0.103 0.464 0.422 0.594 0.611 0.733 0.772 0.324 0.497 0.094 0.396 0.352 0.477 0.513 0.599 0.621 [0.281, 0.364] [0.472, 0.520] [0.070, 0.121] [0.363, 0.428] [0.311, 0.397] [0.447, 0.505] [0.478, 0.545] [0.566, 0.632] [0.587, 0.652] Table 1: Comparison between our proposed methods (bottom) and the applicable baselines (top). *Detectree2 results correspond to the best results obtained with the Flexi checkpoint. Full results from all seven checkpoints are shown in Appendix Table A.1. The best performing of the purely image-based baseline models, the supervised U-net, manages to exceed the mIoU of the coarse CHM-based segments but falls short of the proposed methods. Still, the accuracy of the model is impressive for dataset where other models are unable to produce robust results. This might be because it is the only model that also leveraged the available red edge band, while the other models have only been trained for RGB images. Another notable success in the baseline methods are those of SAM 3 and Detectree2. While the mIoU and F1 scores were clearly worse than those obtained with our final models, the precision of the models, corresponding to few false positives, was impressive. However, this is in part caused by the model being overly cautious and only finding few of the visible tree crowns in the image, as highlighted by the low recalls and qualitative results in Figure 4. While there remains clear room for improvement in the metrics of even our best performing model, the results indicate that the automated training label collection method, alongside the zero-shot segmentation model enhancement bring us closer to obtaining sufficient data for training broadly generalisable tree crown instance segmentation models. All the metrics fall in the ranges that have been observed in prior literature for models trained with fully supervised data, which is significant achievement for method trained using only weak, automatically collected labels. In Figure 4, we have visualised outputs from models we deemed most important to highlight based on the quantitative results. The missing baselines are visualised in Figure A.2 of the appendix. The visualised test samples were chosen to highlight the variety of scenarios present in the data, purely based on the input and ground truth masks. The frames are cropped to 3/4 of the original width and height of 1024 pixels for the visualisation to save space, as only the masks in the middle 512 by 512 pixel area were evaluated and thus also visualised for each individual patch. The shown results in Figure 4 support the quantitative results. They highlight how the Detectree2 and SAM 3 models seem too conservative in their predictions, while the supervised U-net model, on the other hand, produces more false positives and overly large segments. These correspond to the presented precision and recall values in Table 1. The proposed method produces more balanced outputs. The slight improvement of the predictions between the proposed model using only RGB inputs and those of the model using RGB+NIR inputs can also be observed in the same figure. 9 Figure 4: Visualisation of the outputs of selected models. The Detectree2 results are from the best performing Flexi checkpoint. 10 4.1. Ablation We performed ablation on the SAM 2 label smoothing and the use of different channels of the multispectral images, which are both shown in Table 2. As seen in Table 2 and Figure 5, the SAM 2 label enhancement improves the downstream model performance across the board. The effect of different input channel configurations is less clear. Notably, the different metrics highlight the usefulness of the near infrared channel, but the model using it in combination with other input channels is unfortunately unable to capture its full value. Methods to improve this should be investigated further, but the problem might also be data-specific and related to, for example, channel normalisation. Supervision Input F1 Precision Recall mIoU 95% CI Coarse Coarse Coarse Coarse Coarse Coarse Coarse Pseudo Pseudo Pseudo Pseudo Pseudo Pseudo Pseudo RGB RE NIR RGB+RE RGB+NIR RE+NIR RGB+RE+NIR RGB RE NIR RGB+RE RGB+NIR RE+NIR RGB+RE+NIR 0.743 0.775 0.739 0.740 0.746 0.760 0.748 0.758 0.767 0.767 0.774 0.778 0.772 0.761 0.784 0.805 0.777 0.766 0.762 0.792 0.786 0.783 0.776 0.785 0.793 0.783 0.787 0.771 0.706 0.747 0.706 0.717 0.731 0.731 0.714 0.733 0.761 0.750 0.756 0.772 0.758 0. 0.548 0.576 0.539 0.562 0.565 0.559 0.549 0.599 0.597 0.586 0.610 0.621 0.610 0.602 [0.515, 0.579] [0.541, 0.607] [0.505, 0.568] [0.528, 0.591] [0.532, 0.595] [0.525, 0.590] [0.515, 0.579] [0.566, 0.632] [0.565, 0.631] [0.554, 0.617] [0.577, 0.643] [0.587, 0.652] [0.576, 0.640] [0.567, 0.635] Table 2: Ablation results of the different trained Mask-RCNN models. The confidence interval 95% CI is that of the mIoU. The ablation provides concrete proof of the usefulness of the SAM 2 label enhancement and clear indication that fusing RGB images with near infrared or infrared channels is likely to yield better results. These results are most visible in the boxplot of Figure 5. The visualised confidence intervals highlight that the improvement is stable across different input modalities. To confidently determine the optimal input channel configuration for the task, more experiments with more varied data should be carried out. The qualitative results of different input modalities and training label configurations can be observed from Figure 6 and Figure A.3 of the appendix. 4.2. Failure points While our method shows great improvement over the baselines in our test dataset, we observed some clear failure points which should be taken into account when deploying the methods directly in practical setting or when developing new models based on the study. These weak points of the models include their transferability to new datasets, predictions near image edge areas, and the tendency to create overlapping masks that are not filtered by NMS. Overlapping predictions. The main weakness of the proposed methods, which can be observed on the used test samples, is the tendency to produce outputs in which some masks are entirely contained by larger predicted mask. For masks which are of similar size, this is not problem as they are removed by the NMS post-processing step, but in scenarios where one of the masks is significantly larger than the other, the NMS does not capture the overlap, as the filtering is done based on simple intersection over union threshold. However, for practical inference, another 11 Figure 5: Standard quartile box plot showing the prediction quality (mIoU) between different input modalities and coarse vs. pseudo-supervision. filtering step could be included based on the intersection and the area of single mask. The problem also highlights the difficulty of the dataset caused by the large differences in the size of the presented trees. Transferability. Our final RGB image-based method fails when applied to some publicly available tree crown images. It most likely results from the unique spatial resolution and colours of our dataset compared to the other available data. Heavier augmentations could likely alleviate the problem, but this had yet to be explored further. Predictions near image edges. As the model has been trained on labels which were only present in the middle part of the image, the predictions deteriorate in the edges of the image. This means that for practical inference on larger areas, the rasters should be split into patches with an overlap of 1/4 of the image size between images in each dimension. The metrics resulting from an evaluation with full image stride are shown in Table 3. Input mIoU Change Relative(%) RGB RE NIR RGB + RE RGB + NIR RE + NIR RGB + RE + NIR 0.580 0.598 0.553 0.587 0.594 0.584 0.579 -0.020 +0.001 -0.035 -0.024 -0.027 -0.016 -0. -3.33 1.70 -5.97 -3.93 -4.35 -2.62 -3.82 Table 3: Results when evaluating models based on full image predictions (including partial crowns closer to the image edges). All results are from pseudo-supervised models. 12 Figure 6: Visualisation of the outputs from models using different input modalities and training labels. C. is short for coarse and P. for pseudo-labels, respectively. 13 5. Conclusions The work shows that CHM-generated annotations can be used to train image-based tree crown segmentation models and that using SAM 2 as preprocessing step clearly enhances these labels. Our method presents way to obtain relatively high-quality training annotations for optical image-based models without any manual annotation cost. This allows models to be cheaply trained for specific locations, outperforming any state-of-the-art models that have been trained for the same task on different data. Unfortunately, most of the work on camera-based tree crown prediction has been more focused on bounding box detection, which is reflected in the amount of available data. While we provide an alternative method for generating segmentation data, we lack extensive data from more varied locations, which could be used to train or test more generalisable model. In addition, most specifically between different input modalities, the observed differences were small, and its not guaranteed that slightly better segmentation metrics compared to human annotations necessarily correlate perfectly with real-world performance [48]. To further leverage this method to produce tree crown instance segmentation models that would perform better in any given scenario, we believe the data should be combined with similar data from more varied geographic areas and time periods. Additionally, the model could be improved, for example, by changing from Mask R-CNN to something which has been shown to work better on tree crowns specifically, such as the StarDist model [49], by simply using more recent backbone architecture, or by leveraging model with larger-scale pretraining. Acknowledgements We thank Teemu Hakala for preparing the sensor systems used to capture the multispectral image data of the study. We also wish to acknowledge CSC IT Center for Science, Finland, for computational resources Funding sources This research was funded by the Research Council of Finland (RCF) within projects Learning techniques for autonomous drone based hyperspectral analysis of forest vegetation (decision no. 357380) and RCF research Infrastructure Measuring Spatiotemporal Changes in Forest Ecosystem (346382), as well as European Union NextGenerationEU instrument through the Research Council of Finland (grant number 353263 for Multirisk Project). This study has been performed with affiliation to the RCF Flagship ForestHumanMachine InterplayBuilding Resilience, Redefining Value Networks and Enabling Meaningful Experiences (UNITE) (decision no. 357908). References [1] Y. Lai, C. E. Kontokosta, The impact of urban street tree species on air quality and respiratory illness: spatial analysis of large-scale, high-resolution urban data, Health & place 56 (2019) 8087. doi:https://doi.org/10.1016/j.healthplace.2019.01.016. [2] C. Zhao, H. A. Sander, Quantifying and mapping the supply of and demand for carbon storage and sequestration service from urban trees, PLoS One 10 (8) (2015) e0136392. doi:https://doi.org/10.1371/journal.pone.0136392. 14 [3] T. Yang, Y. Ryu, R. Kwon, C. Choi, Z. Zhong, Y. Nam, S. Jo, Mapping carbon stock and growth of trees using lidar-camera fusion-based mobile mapping system, Remote Sensing of Environment 328 (2025) 114895. doi:https://doi.org/10.1016/j.rse.2025.114895. individual street [4] K. D. Brosofske, R. E. Froese, M. J. Falkowski, A. Banskota, review of methods for mapping and prediction of inventory attributes for operational forest management, Forest Science 60 (4) (2013) 733756. doi:https://doi.org/10.5849/forsci.12-134. [5] T. Hlásny, L. König, P. Krokene, M. Lindner, C. Montagné-Huck, J. Müller, H. Qin, K. F. Raffa, M.-J. Schelhaas, M. Svoboda, et al., Bark beetle outbreaks in europe: state of knowledge and ways forward for management, Current Forestry Reports 7 (3) (2021) 138165. doi:https://doi.org/10.1007/s40725-021-00142-x. [6] M. Patacca, M. Lindner, M. E. Lucas-Borja, T. Cordonnier, G. Fidej, B. Gardiner, Y. Hauf, G. Jasineviˇcius, S. Labonne, E. Linkeviˇcius, et al., Significant increase in natural disturbance impacts on european forests since 1950, Global change biology 29 (5) (2023) 1359 1376. doi:https://doi.org/10.1111/gcb.16531. [7] W. You, L. Lin, L. Wu, Z. Ji, J. Yu, J. Zhu, Y. Fan, D. He, Geographical information system-based forest fire risk assessment integrating national forest inventory data and analysis of its spatiotemporal variability, Ecological Indicators 77 (2017) 176184. doi:https://doi.org/10.1016/j.ecolind.2017.01.042. [8] Forest Service, U.S. Department of Agriculture, Urban forest inventory and analysis program, available at: https://research.fs.usda.gov/programs/urbanfia, (accessed 12 January 2026) (2025). [9] J. Zhou, C. Proisy, X. Descombes, G. Le Maire, Y. Nouvellon, J.-L. Stape, G. Viennois, J. Zerubia, P. Couteron, Mapping local density of young eucalyptus plantations by individual tree detection in high spatial resolution satellite images, Forest Ecology and Management 301 (2013) 129141. doi:https://doi.org/10.1016/j.foreco.2012.10.007. [10] S. Zhang, Y. Cui, Y. Zhou, J. Dong, W. Li, B. Liu, J. Dong, mapping approach for eucalyptus plantations canopy and single tree using high-resolution satellite images in Liuzhou, China, IEEE Transactions on Geoscience and Remote Sensing 61 (2023) 113. doi:https://doi.org/10.1109/TGRS.2023.3327128. [11] J. Zheng, S. Yuan, W. Li, H. Fu, L. Yu, individual tree crown detection and delineation from optical remote sensing images: Current progress and future, IEEE Geoscience and Remote Sensing Magazine (2024). doi:https://doi.org/10.1109/MGRS.2024.3479871. J. Huang, review of [12] M. Favorskaya, A. Tkacheva, I. M. Danilin, E. M. Medvedev, Fusion of airborne lidar and digital photography data for tree crowns segmentation and measurement, in: Intelligent interactive multimedia systems and services, Springer, 2015, pp. 191201. doi:https://doi.org/10.1007/978-3-319-19830-9_18. [13] M. Aubry-Kientz, A. Laybros, B. Weinstein, J. G. Ball, T. Jackson, D. Coomes, G. Vincent, Multisensor data fusion for improved segmentation of individual tree crowns in dense tropical forests, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14 (2021) 39273936. doi:https://doi.org/10.1109/JSTARS.2021.3069159. 15 [14] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. Dollar, C. Feichtenhofer, SAM 2: Segment anything in images and videos, in: The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=Ha6RTeWMd0 [15] S. Rua, SAM2 pseudolabeling for instance segmentation of tree crowns from aerial imagery, Masters thesis, Aalto University (2025). URL https://urn.fi/URN:NBN:fi:aalto-202512179400 [16] K. Wołk, M. S. Tatara, review of semantic segmentation and instance segmentation techniques in forestry using lidar and imagery data, Electronics 13 (20) (2024) 4139. doi:https://doi.org/10.3390/electronics13204139. [17] J. G. Ball, S. H. Hickman, T. D. Jackson, X. J. Koay, J. Hirst, W. Jay, M. Archer, M. AubryKientz, G. Vincent, D. A. Coomes, Accurate delineation of individual tree crowns in tropical forests from aerial RGB imagery using Mask R-CNN, Remote Sensing in Ecology and Conservation 9 (5) (2023) 641655. doi:https://doi.org/10.1002/rse2.332. [18] M. Freudenberg, P. Magdon, N. Nölke, Individual tree crown delineation in high-resolution remote sensing images based on U-Net, NCAA (2022). doi:https://doi.org/10.1007/s00521022-07640-4. [19] Y. Lin, H. Li, L. Jing, H. Ding, S. Tian, Individual tree crown delineation using airborne lidar data and aerial imagery in the taigatundra ecotone, Remote Sensing 16 (21) (2024) 3920. doi:https://doi.org/10.3390/rs16213920. [20] S. Dersch, A. Schöttl, P. Krzystek, M. Heurich, Semi-supervised multi-class tree crown delineation using aerial multispectral ISPRS Journal of Photogrammetry and Remote Sensing 216 (2024) 154167. doi:https://doi.org/10.1016/j.isprsjprs.2024.07.032. imagery and lidar data, [21] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollár, R. Girshick, Segment anything, arXiv:2304.02643[preprint] (2023). doi:https://doi.org/10.48550/arXiv.2304.02643. [22] N. Carion, L. Gustafson, Y.-T. Hu, S. Debnath, R. Hu, D. Suris, C. Ryali, K. V. Alwala, H. Khedr, A. Huang, J. Lei, T. Ma, B. Guo, A. Kalla, M. Marks, J. Greer, M. Wang, P. Sun, R. Rädle, T. Afouras, E. Mavroudi, K. Xu, T.-H. Wu, Y. Zhou, L. Momeni, R. Hazra, S. Ding, S. Vaze, F. Porcher, F. Li, S. Li, A. Kamath, H. K. Cheng, P. Dollár, N. Ravi, K. Saenko, P. Zhang, C. Feichtenhofer, SAM 3: Segment anything with concepts, [preprint] (2025). arXiv:2511.16719, doi:https://doi.org/10.48550/arXiv.2511.16719. [23] K. Chen, C. Liu, H. Chen, H. Zhang, W. Li, Z. Zou, Z. Shi, RSPrompter: Learning to prompt for remote sensing instance segmentation based on visual foundation model, IEEE Transactions on Geoscience and Remote Sensing 62 (2024) 117. doi:https://doi.org/10.1109/TGRS.2024.3356074. [24] M. Teng, A. Ouaknine, E. Laliberté, Y. Bengio, D. Rolnick, H. Larochelle, Assessing SAM for tree crown instance segmentation from drone imagery, arXiv:2503.20199[preprint] (2025). doi:https://doi.org/10.48550/arXiv.2503.20199. 16 [25] B. G. Weinstein, S. Marconi, M. Aubry-Kientz, G. Vincent, H. Senyondo, E. P. White, DeepForest: python package for RGB deep learning tree crown delineation, Methods in Ecology and Evolution 11 (12) (2020) 17431751. doi:https://doi.org/10.1111/2041210X.13472. [26] M. Chen, D. Russell, A. Pallavoor, D. Young, tree detection and segmentation from aerial forest imagery, arXiv:2506.03114[preprint] (2025). doi:https://doi.org/10.48550/arXiv.2506.03114. J. Wu, Zero-shot [27] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan, Z. Zeng, H. Zhang, F. Li, J. Yang, H. Li, Q. Jiang, L. Zhang, Grounded SAM: Assembling open-world models for diverse visual tasks, [preprint] (2024). arXiv:2401.14159, doi:https://doi.org/10.48550/arXiv.2401.14159. [28] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, et al., Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection, arXiv:2303.05499[preprint] (2023). doi:https://doi.org/10.48550/arXiv.2303.05499. [29] E. Arnaudo, J. L. Vaschetti, L. Innocenti, L. Barco, D. Lisi, V. Fissore, C. Rossi, Fmars: Annotating remote sensing images for disaster management using foundation models, in: IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium, IEEE, 2024, pp. 39203924. doi:https://doi.org/10.1109/IGARSS53475.2024.10641130. [30] Y. Zhu, W. Locke, J. Yuan, Y. Zhang, Q. Ma, L. Liang, Leveraging sam 2 and lidar for automated individual tree crown delineation: comparative evaluation of prompting methods, Information Geography (2025) 100025doi:https://doi.org/10.1016/j.infgeo.2025.100025. [31] Agisoft, Agisoft Metashape Professional, (v.2.0.2). [software], https://www.agisoft.com/. [32] L. Matikainen, K. Karila, J. Hyyppä, P. Litkey, E. Puttonen, E. Ahokas, Object-based analysis of multispectral airborne laser scanner data for land cover classification and map updating, ISPRS Journal of Photogrammetry and Remote Sensing 128 (2017) 298313. doi:https://doi.org/10.1016/j.isprsjprs.2017.04.005. [33] J. Taher, E. Hyyppä, M. Hyyppä, K. Salolahti, X. Yu, L. Matikainen, A. Kukko, M. Lehtomäki, H. Kaartinen, S. Thurachen, et al., Multispectral airborne laser scanning for tree species classification: benchmark of machine learning and deep learning algorithms, ISPRS Journal of Photogrammetry and Remote Sensing 233 (2026) 278309. doi:https://doi.org/10.1016/j.isprsjprs.2026.01.031. [34] rapidlasso GmbH, LAStools software suite, [software], https://rapidlasso.de/. [35] H. Kaartinen, J. Hyyppä, X. Yu, M. Vastaranta, H. Hyyppä, A. Kukko, M. Holopainen, C. Heipke, M. Hirschmugl, F. Morsdorf, et al., An international comparison of individual tree detection and extraction using airborne laser scanning, Remote Sensing 4 (4) (2012) 950974. doi:https://doi.org/10.3390/rs4040950. [36] K. Karila, L. Matikainen, P. Litkey, J. Hyyppä, E. Puttonen, The effect of seasonal variation on automated land cover mapping from multispectral airborne laser International Journal of Remote Sensing 40 (9) (2019) 32893307. scanning data, doi:https://doi.org/10.1080/01431161.2018.1528023. [37] K. He, G. Gkioxari, P. Dollár, R. Girshick, Mask R-CNN, Proceedings of the IEEE international conference on computer vision, 2017, pp. 29612969. doi:https://doi.org/10.1109/ICCV.2017.322. in: [38] Y. Li, S. Xie, X. Chen, P. Dollar, K. He, R. Girshick, Benchmarking detec- (2021). learning with vision transformers, arXiv:2111.11429[preprint] tion transfer doi:https://doi.org/10.48550/arXiv.2111.11429. [39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, ImageNet: large-scale hierarchical image database, in: 2009 IEEE conference on computer vision and pattern recognition, Ieee, 2009, pp. 248255. doi:https://doi.org/10.1109/CVPR.2009.5206848. [40] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. doi:https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90. [41] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie, Feature pyramid networks for object detection, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 21172125. doi:https://doi.org/10.1109/CVPR.2017.106. [42] D. P. Kingma, Adam: method for stochastic optimization, arXiv:1412.6980[preprint] (2014). doi:https://doi.org/10.48550/arXiv.1412.6980. [43] S. Hanson, L. Pratt, Comparing biases for minimal network construction with backpropagation, in: D. Touretzky (Ed.), Advances in Neural Information Processing Systems, Vol. 1, Morgan-Kaufmann, 1988. [44] I. Loshchilov, F. Hutter, Decoupled weight decay regularization, in: International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 [45] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg, SSD: Single shot multibox detector, in: European conference on computer vision, Springer, 2016, pp. 2137. doi:https://doi.org/10.1007/978-3-319-46448-0_2. [46] J. Ball, C. Kotthoff, detectree2 trained models (2025). doi:10.5281/zenodo.15863800. URL https://doi.org/10.5281/zenodo.15863800 [47] J. G. C. Ball, S. Jaffer, A. Laybros, C. Prieur, T. Jackson, A. Madhavapeddy, N. Barbier, G. Vincent, D. A. Coomes, Towards comprehensive individual tree species mapping in diverse tropical forests by harnessing temporal and spectral dimensions, bioRxiv[preprint] (2025). doi:https://doi.org/10.1101/2024.06.24.600405. [48] T. Khan, J. Krebs, S. K. Gupta, J. Renkel, C. Arnold, N. Nölke, Validation challenges in large-scale tree crown segmentations from remote sensing imagery using deep learning: case study in germany, in: International Conference on Theory and Practice of Digital Libraries, Springer, 2025, pp. 311323. doi:https://doi.org/10.1007/978-3-032-06136-2_30. [49] F. Tong, Y. Zhang, Individual tree crown delineation in high resolution aerial RGB imagery using StarDist-based model, Remote Sensing of Environment 319 (2025) 114618. doi:https://doi.org/10.1016/j.rse.2025.114618. 18 Appendix Figure A.1: The NDVI distribution of initial tree crown segments with the cut-off threshold indicated with the black dashed vertical line. Detectree2 checkpoint F1 Precision Recall mIoU mIoU 95% CI base 05dates flexi randresize_full tropical_closed_canopy urban_trees_Cambridge withParacouUAV 0.298 0.423 0.556 0.447 0.064 0.393 0.415 0.540 0.606 0.911 0.718 0.177 0.600 0. 0.206 0.325 0.400 0.325 0.039 0.292 0.336 0.182 0.261 0.324 0.269 0.039 0.243 0.280 [0.150, 0.215] [0.227, 0.298] [0.281, 0.364] [0.228, 0.308] [0.025, 0.056] [0.211, 0.276] [0.242, 0.319] Table A.1: Test set metrics from all different Detectree2 model checkpoints. 19 Figure A.2: Visualisation of the outputs from the Grounded SAM (denoted G. SAM) and DeepForest models. 20 Figure A.3: Visualisation of the outputs from models using different input modalities and training labels. C. is short for coarse, and P. for pseudo-labels, respectively. All refers to models using all three input modalities (RGB, RE, and NIR)."
        }
    ],
    "affiliations": [
        "DOTA, ONERA, Université de Toulouse, Toulouse, 31000, France",
        "Department of Computer Science, Aalto University, Espoo, 02150, Finland",
        "Department of Remote Sensing and Photogrammetry, Finnish Geospatial Research Institute, Espoo, 02150, Finland"
    ]
}