{
    "paper_title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
    "authors": [
        "Hsi-Che Lin",
        "Yu-Chu Yu",
        "Kai-Po Chang",
        "Yu-Chiang Frank Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users."
        },
        {
            "title": "Start",
            "content": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction Hsi-Che Lin1 Yu-Chu Yu1 Kai-Po Chang1,2 Yu-Chiang Frank Wang1,2 1National Taiwan University 2NVIDIA frankwang@nvidia.com r13942079@ntu.edu.tw 5 2 0 J 3 1 ] . [ 1 5 1 0 2 1 . 6 0 5 2 : r hsi-che-lin.github.io/EMLoC"
        },
        {
            "title": "Abstract",
            "content": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of 38B model on single 24GB consumer GPUbringing efficient and practical model adaptation to individual users."
        },
        {
            "title": "Introduction",
            "content": "General-purpose foundation models have demonstrated impressive zero-shot capabilities across wide range of benchmarks [2, 6, 8, 11, 35]. For real-world deployment, such as domain-specific tasks [21, 42] or personalized user behavior [26, 30] , further customization by fine-tuning is still required. However, fine-tuning typically incurs significantly more memory overhead than inference [28]. Consequently, if users have fixed amount of available computing resources, they will be forced to choose between two unfavorable options. First, they can use small model that fits within their memory budget for fine-tuning as shown in Fig. 1(a), but this sacrifices the emergent capabilities [38] of larger models and underutilizes hardware during inference. Alternatively, they can opt for large model that fully utilizes resources during inference but exceeds memory limits for fine-tuning as shown in Fig. 1(b), making user-specific adaptation infeasible and potentially limiting performance in specialized applications. This paper addresses central research question: Is it possible to design fine-tuning strategy such that users can fine-tune model under the same memory budget as inference? The memory cost of fine-tuning can be broadly attributed to three components: optimizer states, intermediate activations, and the model parameters themselves, as marked in Fig. 1 with different colors. Initial efforts to reduce the memory usage of fine-tuning concentrated on the first two components. The first component, optimizer states, stores auxiliary information such as momentum and variance in the Adam optimizer [15] for each trainable parameter. This overhead can be mitigated using parameter-efficient fine-tuning (PEFT) methods [13, 20, 41], such as LoRA [13], which reduces trainable parameters by only updating lightweight low-rank matrices. The second Preprint. Under review. Figure 1: The dilemma caused by additional memory overhead during fine-tuning. (a) Users opt for smaller 8B model, sacrificing emergent capabilities and underutilizing available hardware. (b) Use of larger 26B model requiring memory exceeding the hardware limit even with LoRA [13] and gradient checkpointing [4] techniques. (c) Our EMLoC utilizes smaller model during fine-tuning, allowing the same budget for both training and inference. component, intermediate activations, refers to the values retained during the forward pass for use in backpropagation. Previous works reduced it by modifying model architectures [10, 19, 34] or gradient checkpointing [4]. However, since the components reduced by both strategies still presented during fine-tuning, reducing them alone, without addressing the memory usage of model parameters, cannot fully close the memory gap between fine-tuning and inference (Fig. 1(b)). Few recent works have taken initial steps toward addressing the third component, the memory cost of model parameters. For example, Offsite-Tuning [40] proposes to drop some of the intermediate layers entirely during fine-tuning and to update only the top and bottom layers, thereby saving some memory occupied by model parameters. LORAM [45] introduces row-pruning as method to reduce model parameters during fine-tuning, allowing the updating of the unpruned rows in the intermediate layers. Despite their promise, these methods typically rely on memory-intensive full-model training. This entails strong and often impractical assumption that well-resourced institutions must be involved, making them unfeasible for individual users. In addition, they impose strict constraints on which subsets of weights can be updated, limiting their flexibility and general applicability across tasks and architectures. Moreover, they overlook the fact that different base models are used during fine-tuning and inference, introducing potential mismatches that degrade performance. In this paper, we propose EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction. EMLoC completely eliminates the memory cost gap between inference and training and can be carried out solely by individual users. The central idea is to perform finetuning on low-rank model which we call the emulator so that the memory taken up by model parameters can be reduced as depicted in Fig. 1(c). To make this approach effective, we introduce an efficient, downstream-aware emulator construction procedure based on activation-aware singular value decomposition (SVD), using small subset of downstream data for calibration. This yields lightweight emulator tailored to the downstream data, without requiring costly full-model training. Fine-tuning is then conducted entirely on the emulator without any assumptions about the fine-tuning processany standard training pipeline can be used. However, since the LoRA modules are finetuned on the emulator rather than the original model, naively merging the LoRA modules into the original model will suffer from the misalignment between the full model and the compressed emulator. To mitigate this issue, we further introduce novel LoRA correction algorithm, which adjusts the parameters to compensate such discrepancy during inference. Our contributions can be summarized as follows: We introduce pipeline that closes the memory cost gap between inference and fine-tuning, enabling users to fine-tune model under the same memory budge as inference. We propose novel emulator-based fine-tuning framework that significantly reduces memory consumption by operating on downstream-aware low-rank approximation of the model. We develop LoRA correction algorithm that addresses the misalignment between emulatorbased fine-tuning and full-model inference, improving final performance and transferability."
        },
        {
            "title": "2.1 Reducing Optimizer State",
            "content": "There are mainly two ways to reduce the memory required for optimizer states: by reducing the number of trainable parameters [1214, 20, 41, 43, 46], or by reducing auxiliary information required per parameter [18, 31, 49]. prominent example of the first approach is LoRA [13], which introduces trainable low-rank adapters into frozen pre-trained models to achieve parameter-efficient fine-tuning. For the second approach, Adafactor [31] eliminates the need to store full-rank second-moment estimators by using factored approximation. While these methods are effective, non-trivial number of trainable parameters are required to maintain acceptable performance. As result, the optimizer states associated with those parameters continue to contribute non-negligible portion of the overall memory footprint."
        },
        {
            "title": "2.2 Reducing Intermediate Activation",
            "content": "Another way to fine-tune with less memory overhead is to reduce memory from intermediate activations. Two explored strategies are shortening the backpropagation path [25, 34, 41, 44] and recomputing activations during the backward pass [4, 10, 19]. An example of the first strategy is Ladder Side-Tuning [34], where trainable modules are placed outside the backbone to avoid backpropagation through the backbone. [19] carefully design the position and initialization of the adapter modules so that the pre-trained model becomes reversible, which enables recomputing intermediate activation from the output during backpropagation. However, these methods cannot completely eliminate activation memory due to the fundamental need for gradients, and they typically incur additional computational overhead."
        },
        {
            "title": "2.3 Reducing Model Parameters",
            "content": "Beyond optimizing optimizer states and intermediate activations, very recently reducing the memory footprint of the model parameters during fine-tuning has been explored. It is worth noting that quantization-based methods [9, 48] are largely orthogonal to our focus: they do not reduce fine-tuning memory relative to inference, and can therefore be combined with our method. An approach to reducing parameter memory during fine-tuning involves using pruned model for training while maintaining full model for inference [40, 45]. For example, LORAM [45] assumes that the model publisher provides row-pruned and continually pre-trained variant of the original model. Users fine-tune this reduced model and then transfer the learned LoRA modules back to the full model. However, their setting requires continual pre-training of the pruned model, making it inaccessible to individual users. Furthermore, only the unpruned weights can be fine-tuned, limiting flexibility in selecting which parameters to adapt. In contrast, our work introduces pipeline that can be performed by users individually and allows flexible selection of tunable parameters."
        },
        {
            "title": "3 Method",
            "content": "Our proposed method, EMLoC, is memory-efficient fine-tuning framework that enables individual users to fine-tune model with the same memory cost as inference. An overview of EMLoC is shown in Fig. 2. EMLoC consists of three main stages. First, we construct downstream-aware lightweight emulator by performing activation-aware singular value decomposition (SVD) [37] on small calibration set drawn from the downstream data. Second, we fine-tune the emulator with LoRA modules using any standard training pipeline. The reduced parameter size of the emulator is crucial for enabling memory-efficient fine-tuning. Finally, to address the misalignment issue between the original model and the emulator, we present novel LoRA correction algorithm to correct the learned LoRA modules, and transfer them back to the original model before inference."
        },
        {
            "title": "3.1 Memory-Efficient Fine-Tuning: Problem Formulation",
            "content": "Given pre-trained model and downstream dataset Ddown, the goal is to fine-tune with the same memory constraint as inference. We denote the collection of linear weights in as = {Wi}. In EMLoC, we introduce an lightweight emulator, denoted as E, constructed by replacing each 3 Figure 2: Overview of EMLoC. Stage 1: Construct downstream-aware lightweight emulator. Stage 2: Fine-tune the emulator via LoRA, allowing reduced memory costs. Stage 3: Update the LoRA module to compensate the misalignment between the full model and emulator. WE . The subscript here is retained to emphasize the positional Wi with its counterpart correspondence between Wi and in the architecture. For simpleness, we may omit the index when the context is clear. Fine-tuning is then performed on the emulator using any standard training pipeline. During training, set of LoRA modules Λ = {Λi} is learned on top of , resulting in the fine-tuned model weights + Λ. At inference time, the learned LoRA modules are transferred back to the original model M, with the corresponding weights + Λ, which is used for inference. Note that conventional parameter-efficient fine-tuning (PEFT) methods are special case of this framework, where WE = and hence = M."
        },
        {
            "title": "3.2 Downstream-aware Emulator Construction and Fine-tuning",
            "content": "Downstream-aware Emulator Construction. The goal of emulator construction is to create lightweight replacement for the original model to be used during fine-tuning. To ensure that EMLoC is memory-efficient, generalizable, and effective, the emulator must satisfy three key criteria. First, it should have fewer parameters than the original model to reduce memory consumption during fine-tuning. Second, it should support flexible placement of LoRA modules, enabling all weights to be fine-tunable. In other words, if LoRA module Λ can be used to fine-tune weight matrix in the original model, then EMLoC must support placing and training Λ beside the corresponding in the emulator, thereby enabling effective fine-tuning of . Third, it should preserve knowledge relevant to the downstream task to ensure the fine-tuning process is effective. To meet all three criteria simultaneously, we propose constructing the emulator using activation-aware SVD, which naturally balances parameter reduction, flexibility, and task relevance. Specifically, we construct the set of emulator weights WE by applying the activation-aware SVD method proposed in SVD-LLM [37] to each weight matrix WE = (cid:8)W = WU WV = SVD-LLM(W, n) W(cid:9) , where is the number of kept singular values, chosen to control the parameter count of the emulator. The emulator is then constructed by replacing each in with its corresponding low-rank approximation . Although we denote each as single matrix, it is stored in its factored form WU WV , which significantly reduces the memory footprint of model parameters, thus satisfying the first criterion. Importantly, each retains the same position, and the input/output dimensions as , differing only in the rank of the transformation. As result, any LoRA module Λ that could be applied to can equivalently be applied to , enabling all weights to be fine-tunable, the second criterion. Finally, SVD-LLM minimizes the output reconstruction error (1) W F , (2) where is the intermediate activation computed from calibration data DC. This ensures that the emulator preserves task-relevant knowledge, making fine-tuning on it effective, thereby fulfilling the third criterion. Fine-tuning. Owing to the specific design choices in the emulator, we can apply any standard PEFT techniques [13, 41, 43] to fine-tune the significantly smaller emulator, achieving memory-efficient fine-tuning. In particular, if we aim to fine-tune weight matrix Wi in the original model, we 4 Figure 3: LoRA correction to compensate model misalignment. To alleviate the misalignment that arises from fine-tuning the lightweight emulator but running inference on the original model, LoRA parameters are corrected via feature spaces between the emulator and the original model. instead insert LoRA module next to to fine-tune the emulator as in conventional frameworks. This allows us to use any existing training pipeline without modification, making our method plug-and-play replacement for conventional LoRA fine-tuning, while further reducing memory usage."
        },
        {
            "title": "3.3 LoRA Correction for Compression Misalignment",
            "content": "To fully utilize the capacity of the original model during inference, we aim to transfer the LoRA modules Λ fine-tuned on back to the original model M. However, the emulator construction step that replaces with its low-rank counterpart inevitably introduces misalignment between these two models. As result, the base model outputs seen by the LoRA modules are different during fine-tuning and inference. The difference is unexpected by the LoRA modules since they were optimized in the context of . To address this issue, we propose novel LoRA correction algorithm and correct the misalignment explicitly, as shown in Fig. 3. Algorithm 1 LoRA_correction A, B: Corrected LoRA. Input: WA, WB: LoRA weights; W, : Linear weights from and E; λ: An hyperparameter for regularization. Output: 1. Apply SVD to WA, obtaining U, Σ, 2. 3. 4. 5. clamp(, λ) return ΣV WB (W ) , A, Objective of LoRA correction. The goal of our LoRA correction procedure is to correct the LoRA module Λ fine-tuned with the emulator E, and derive new LoRA module Λc suitable for the original model M. In other word, for each input that activates the LoRA, the merged output should remain consistent between training and inference. Formally, we enforce the following condition: x(W + Λc) = x(W + Λ) VΛ, (3) where VΛ denotes the subspace on which Λ produces none-zero outputsi.e., the region where it is active. It is worth noting that we enforce Eq. (3) only within VΛ, so that the correction has effects exclusively when the LoRA module is active. This design prevents unintended side effects in parts of the model unrelated to the LoRA module when doing correction, thereby preserving the integrity of the original models behavior. Rewriting Eq. (3), we obtain an equivalent condition, x(Λc Λ) = x(W ) = x VΛ. (4) This formulation highlights the explicit correction objective. The desired Λc aims to compensate for the misalignment x, which exists between the outputs of and . This can be achieved through offsetting the original LoRA module by the correction term x. Mitigating compression misalignment. Although this idea is conceptually straightforward, two practical challenges arise. First, the condition in Eq. (4) must hold for all inputs VΛ. Second, there is no direct way to incorporate the correction term into the LoRA weights Λ = WAWB. 5 Table 1: Performance comparisons of finetuning approaches on VQA with different memory costs. With InternVL2.5-8B as the original model, and 50% indicates finetuning on 4B models is conducted. Note that direct finetuning on InternVL2.5-8B (i.e., rows in gray) exceeds the memory budgets of all other finetuning methods and thus is not feasible under the same constraints. Ratio ChartQA DocVQA InfoVQA TextVQA PMC-VQA WebSRC WC-VQA Method 50% 25% 25%+ QLoRA Original InternVL 4B Offsite [40] UPop [32] EMLoC Original InternVL 2B Offsite [40] UPop [32] EMLoC Original InternVL 2B Offsite [40] UPop [32] EMLoC 84.5 82.9 84.3 84.4 84.6 84.5 78.8 84.0 84.3 84. 84.8 78.2 83.9 84.2 84.2 92.2 90.4 91.3 92.0 92.3 92.2 87.4 92.3 92.2 92.3 92.0 85.7 91.8 91.9 91.9 69.8 64.6 69.0 69.7 69.9 69.8 56.0 69.6 69.6 70. 68.0 53.5 67.0 67.1 67.5 79.6 75.8 77.7 78.5 78.8 79.6 73.2 77.2 78.2 79.0 79.5 71.7 77.1 78.1 79.0 52.9 50.6 51.0 50.7 52.3 52.9 44.6 50.6 50.9 51. 52.3 44.2 50.1 50.3 50.9 87.4 84.8 76.1 76.4 85.2 87.4 78.1 76.6 76.6 79.6 84.8 77.4 75.2 75.8 78.1 53.4 48.6 45.9 42.1 48.8 53.4 34.4 45.4 44.1 46. 51.9 33.0 44.2 43.9 44.8 The first challenge is resolved by leveraging two facts: the condition in Eq. (4) is linear, and VΛ forms subspace of the input feature space. As result, it suffices to enforce the condition only on basis of VΛ, denoted by βΛ x(Λc Λ) = x(W ) = x βΛ. (5) Furthermore, natural choice for βΛ is the set of column vectors of WA, the input projection matrix of the original LoRA module. To address the second challenge, we introduce preprocessing step that reparameterizes the LoRA module while preserving its functionality. Specifically, we perform SVD on the input projection WA, yielding WA = ΣV . We then redefine the LoRA factors as = U, = ΣV WB, (6) AW such that the overall transformation remains unchanged Λ = WAWB = B. After this reparameterization, the columns of A, denoted by {a1, . . . , ar}, are orthonormal. This orthogonality makes the behavior of the LoRA module on each basis vector ai governed directly by the corresponding bi, i-th row of B. = i Λ = (cid:0)a (cid:1) where ei is the standard basis vector. In other words, modifying the output of Λ on each basis vector ai reduces to adjusting the corresponding row bi in B. With the challenges addressed, we now formally describe the LoRA correction algorithm, as presented in Algorithm 1. We begin by preprocessing the original LoRA module Λ = WAWB to obtain and as the basis βΛ in Eq. (3). The correction term is then computed by measuring the output difference between the original weight and the emulator weight under this basis B. Next, we select the columns of = bi, i (7) = (cid:0)W (cid:1) . (8) Each row of represents the correction ai corresponding to basis vector ai. Finally, we adjust to satisfy the condition in Eq. (5), compensating for the misalignment between and = clamp(, λ), = A, c"
        },
        {
            "title": "W c",
            "content": "(9) where the clamp function limits the norm of each correction vector ai to be no more than multiple λ of the corresponding bi. This hyperparameter λ acts as safeguard against excessive deviation, and we find it improves robustness in practice. The resulting corrected LoRA module, Λc = B, is thus adapted to account for the misalignment introduced by training on the emulator and inferring on the original model. AW 6 Table 2: Applying EMLoC to 26B and 38B models. Note that 5B-sized emulator is considered, and all experiments are conducted under 24GB memory budget. Size Method 26B 38B Zero-shot EMLoC Zero-shot EMLoC PMC-VQA WebSRC WC-VQA 77.1 80.9 79.0 82.1 49.9 52.5 52.5 57.0 51.0 52.6 53.6 56. Table 3: Applying EMLoC to NLP tasks. Following LORAM [45], LLaMA2 13B is used as the full model. All experiments are conducted under the same budget. MATHQA GSM8K w/o FT LORAM-RAND LORAM-STRU EMLoC 32.6 33.8 33.8 33.9 24.3 27.2 24.6 29.8 Table 4: Comparisons of emulator construction settings. Note that, LORAM [45] requires external data and additional continual pretraining for memory efficient finetuning. We validate that both row pruning and SVD can be applied for our emulator construction with supperior performances on various tasks using only downstream data. Compression Method Add. Data for compression Require Add. Cont. Pre-train Overhead () (GPU-hours) PMC-VQA WebSRC WC-VQA Row-pruning"
        },
        {
            "title": "SVD",
            "content": "Yes No"
        },
        {
            "title": "Yes\nNo",
            "content": "Yes No"
        },
        {
            "title": "Yes\nNo",
            "content": "214 1.7 230 0.3 51.0 51.1 51.3 51.6 78.7 76.6 74.6 79. 43.6 44.1 42.9 46."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Datasets. In our main experiments, we evaluate EMLoC on seven visual question answering (VQA) datasets. Four of these are widely used benchmarks: ChartQA [22], DocVQA [23], InfoVQA [24], and TextVQA [33]. To reflect realistic deployment scenarios that require domain adaptation, we also include three datasets from specialized domains: PMC-VQA [47] (medical imaging), WebSRC [5] (web page understanding), and WC-VQA [39] (multicultural knowledge). Ablation studies and additional analyses are primarily conducted on these three domain-specific datasets. For language modeling, we evaluate on two challenging mathematical reasoning benchmarks: MathQA [1] and GSM8K [7]. All reported metrics across datasets are based on accuracy. Comparison. For the main experiments, we compare EMLoC against four baselines. Original: We directly fine-tune the original large model, InternVL2.5-8B [6], with LoRA. This baseline incurs significantly higher memory cost compared to other methods and serves as an upper bound in performance. Small: We fine-tune smaller variants of InternVL (4B and 2B), representing the low-resource alternative illustrated in Fig. 1(a). Offsite: We follow the Offsite-tuning [40] framework using the variant that omits full-model distillation to ensure fair comparison under the same compute constraints. UPop: row-pruning-based method [32] is used to construct the emulator, followed by LoRA transfer using the procedure described in LORAM [45]. Except for the Small baseline, all methodsincluding oursperform inference using the original InternVL2.5-8B model. Implementation details. For EMLoC, we use LoRA with rank 8 and train with 500 iterations. The learning rate is set to 4 105 with cosine annealing. The number of calibration data is set to 64 and λ for LoRA correction is 3. The other baselines use the same hyperparameters setting except we adjust the LoRA rank so that the number of trainable parameters of all baselines are the same."
        },
        {
            "title": "4.2 Main Results",
            "content": "Main Results. We evaluate EMLoC using InternVL2.5-8B as the base model under three realistic memory-constrained fine-tuning scenarios. These settings simulate user environments with training memory budgets equivalent to: fine-tuning 4B model (denoted as 50% compression ratio in the table), fine-tuning 2B model (25%), and fine-tuning 2B model using QLoRA [9] quantization (25%+QLoRA). As shown in Table 1, EMLoC consistently outperforms all baselines across both 7 Table 5: Ablation study on activation-aware SVD and LoRA correction. Activation-Aware SVD LoRA Correction PMC-VQA WebSRC WC-VQA 51.0 51.5 51.2 51.6 74.4 79.0 74.4 79.6 44.7 45.8 44.8 46.2 standard and domain-specific VQA benchmarks. The improvements are especially significant on specialized datasets, where adaptation to the target domain is essential. Notably, EMLoC achieves performance closest to the upper bound set by full-model fine-tuning. These results also underscore the adaptability of our approach under varying memory constraints, maintaining strong performance even under the tightest budgets. In contrast to the common strategy of fine-tuning smaller models, EMLoC provides more effective alternative by better leveraging the capacity of the full model. In addition, comparisons with UPop and Offsite-tuning demonstrate the advantages of our downstream-aware emulator construction. Unlike row-pruning or layer-dropping approaches that restrict architectural flexibility, our method tailors the emulator to downstream data without altering model structure, enabling more effective and generalizable adaptation."
        },
        {
            "title": "4.3 Analysis",
            "content": "Scalability on Model Size. We further evaluate EMLoC on larger-scale models, including the 26B and 38B variants of InternVL2.5. For this experiment, we construct 5B-sized emulator, making the fine-tuning cost roughly equivalent to training 5B model. We use LoRA rank 6 for the 26B model and rank 4 for the 38B model. As shown in Table 2, EMLoC consistently improves upon the zero-shot performance of these large models, demonstrating its effectiveness even at greater model scales. Importantly, all fine-tuning were conducted with less than 24GB of GPU memorysubstantially lower than the 95GB typically required for directly fine-tuning the 38B model. This means that users capable of running inference can also perform fine-tuning using our method, without additional hardware requirements. These results underscore the practicality and scalability of EMLoC for resource-constrained users. Comparison of Emulator Construction Settings. We compare two emulator construction settings. The first is our proposed downstream-aware construction, in which the emulator is derived directly from task-specific data without any additional training. The second follows the approach adopted in LORAM, where the emulator is first constructed using general-purpose data, followed by continual pretraining. For general data, we use Laion-COCO [17]a commonly used dataset for visionlanguage pretrainingand perform continual pretraining using 8 V100 GPUs. As shown in Table 4, when row pruning is used as the emulator construction method, both settings achieve comparable downstream performance. However, the continual pretraining stage required in LORAM incurs substantial computational overhead, making it impractical for individual users. In contrast, when SVD is used for emulator construction, our downstream-aware method not only eliminates the need for continual pretraining but also achieves superior performance. Moreover, it incurs the lowest construction overhead, demonstrating the efficiency and effectiveness of our downstream-aware SVD. Robustness Across Modalities. To evaluate the generality of our approach beyond vision-language tasks, we apply EMLoC to two challenging NLP datasets, MATHQA and GSM8K. We follow the experimental setup of LORAM and use LLaMA 2 13B [36] as the base model. As shown in Table 3, our method achieves comparable performance on MATHQA and outperforms LORAM on GSM8K, demonstrating its effectiveness in language-only settings. These results confirm that EMLoC is not limited to multimodal models and can be readily extended to other modalities. Furthermore, our method retains its core advantagesminimal memory overhead and no need for continual pretrainingmaking it more accessible to users with limited computational resources. Effectiveness of Transferring LoRA From Emulator to Original Model. To better understand the role of the emulator and the effectiveness of our LoRA correction strategy, we compare model performance before and after transferring the fine-tuned LoRA modules from the emulator to the Table 6: Performance comparisons of EMLoC before and after transferring the LoRA modules from the emulator to the original model. Note that running inference using emulator does not fully utilize the capacity of the original model and thus is not desirable. Ratio Method ChartQA DocVQA InfoVQA TextVQA PMC-VQA WebSRC WC-VQA 50% 25% Emulator EMLoC Emulator EMLoC 25%+ Emulator QLoRA EMLoC 57.1 84.6 19.3 84.2 14.8 84. 62.3 92.3 6.3 92.3 4.5 91.9 29.3 69.9 10.8 70.0 10.5 67. 15.0 78.8 6.9 79.0 7.3 79.0 40.9 52.3 31.2 51.6 32.2 50. 42.1 85.2 4.0 79.6 4.3 78.1 30.7 48.8 19.1 46.2 19.0 44. original model. As shown in Table 6, performance on the emulator itself is significantly lower the final model after LoRA transfer. This gap underscores the importance of using the full model for inference and justifies our design choice of transferring LoRA back. Interestingly, despite the poor performance of the emulator, the LoRA modules trained on it improve the original models downstream performance once transferred. This indicates that the fine-tuning process successfully extracts domain-specific knowledge, which is embedded in the LoRA weights and effectively transferred to the original model. These results highlight the effectiveness of our emulator-based training. Ablation Study. We conduct ablation experiments to evaluate the contribution of two key components in the EMLoC framework: activationaware SVD for downstream-aware emulator construction, and the LoRA correction algorithm for mitigating training-inference misalignment. As shown in Table 5, removing either component results in drop in performance. This highlights the importance of preserving downstream-relevant knowledge during emulator construction, and the necessity of correcting for discrepancies introduced by training and inference on different models. Figure 4: Sensitivity analysis of λ in the LoRA correction algorithm. We plot performance on WC-VQA under different λ values. Impact of Hyperparameter λ. We investigate the effect of the hyperparameter λ, which constrains the norm of the correction term in our LoRA correction algorithm. As shown in Fig. 4, performance varies with different choices of λ. In particular, when no constraint is applied (i.e., λ ), performance degrades noticeably. We observe that, in such cases, the correction term may have significantly larger norm than the original LoRA output. We hypothesize that correction term with large norm will dominate and distort the LoRA modules, ultimately harming downstream performance. In contrast, applying an appropriate constraint on the correction magnitude helps preserve the learned knowledge in the LoRA modules while still compensating for model misalignment."
        },
        {
            "title": "5 Conclusion",
            "content": "We present EMLoC, novel framework that makes fine-tuning large foundation models as memoryefficient as inference. By introducing downstream-aware emulator constructed via activation-aware SVD and performing all training on this low-rank approximation, EMLoC enables individual users to fine-tune large models without exceeding the memory budget required for inference. Our LoRA correction algorithm further improve the performance by addressing the mismatch introduced by training and inference on different model bases. Extensive experiments across diverse datasets and modalities demonstrate that EMLoC not only outperforms other baselines but also scales to models as large as 38B parameters on commodity hardware. These results open up new opportunities for accessible and scalable model customization, empowering broader adoption of large language models in specialized, resource-constrained settings."
        },
        {
            "title": "References",
            "content": "[1] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23572367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [4] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [5] Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. WebSRC: dataset for web-based structural reading comprehension. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 41734185, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. [9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. [10] Aidan Gomez, Mengye Ren, Raquel Urtasun, and Roger Grosse. The reversible residual network: Backpropagation without storing activations. Advances in neural information processing systems, 30, 2017. [11] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision, pages 10261034, 2015. [13] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [14] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 565576, 2021. [15] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6980. [16] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [17] LAION. Laion coco: 600m synthetic captions from laion2b-en. https://laion.ai/blog/ laion-coco/, 2022. [18] Bingrui Li, Jianfei Chen, and Jun Zhu. Memory efficient optimizers with 4-bit states. Advances in Neural Information Processing Systems, 36:1513615171, 2023. [19] Baohao Liao, Shaomu Tan, and Christof Monz. Make pre-trained model reversible: From parameter to memory efficient fine-tuning. Advances in Neural Information Processing Systems, 36:1518615209, 2023. [20] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024. [21] Ming Lu, Bowen Chen, Drew FK Williamson, Richard Chen, Melissa Zhao, Aaron Chow, Kenji Ikemura, Ahrong Kim, Dimitra Pouli, Ankush Patel, et al. multimodal generative ai copilot for human pathology. Nature, 634(8033):466473, 2024. [22] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. [23] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [24] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [25] Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, and Anurag Arnab. Time-memory-and parameter-efficient visual adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55365545, 2024. [26] Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, and Yong Jae Lee. Yo'llava: Your personalized language and vision assistant. In Advances in Neural Information Processing Systems, 2024. [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [28] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [30] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: trainable agent for role-playing. In EMNLP, 2023. [31] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 45964604. PMLR, 2018. [32] Dachuan Shi, Chaofan Tao, Ying Jin, Zhendong Yang, Chun Yuan, and Jiaqi Wang. UPop: Unified and progressive pruning for compressing vision-language transformers. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 3129231311. PMLR, 2023. [33] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [34] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:1299113005, 2022. [35] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. 11 [36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [37] Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. SVD-LLM: Truncation-aware singular value decomposition for large language model compression. In International Conference on Learning Representations (ICLR), 2025. [38] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022, 2022. [39] Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, et al. Worldcuisines: massive-scale benchmark for multilingual and multicultural visual question answering on global cuisines. arXiv preprint arXiv:2410.12705, 2024. [40] Guangxuan Xiao, Ji Lin, and Song Han. Offsite-tuning: Transfer learning without full model. arXiv preprint arXiv:2302.04870, 2023. [41] Dongshuo Yin, Xueting Han, Bin Li, Hao Feng, and Jing Bai. Parameter-efficient is not sufficient: Exploring parameter, memory, and time efficient adapter tuning for dense predictions. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 13981406, 2024. [42] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, and Zhongyu Wei. Disc-lawllm: Fine-tuning large language models for intelligent legal services, 2023. [43] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. [44] Jeffrey Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: baseline for network adaptation via additive side networks. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 698714. Springer, 2020. [45] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, and Kunlong Zhou. Train small, infer large: Memory-efficient lora training for large language models. arXiv preprint arXiv:2502.13533, 2025. [46] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo In The Eleventh International Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. Conference on Learning Representations, 2023. [47] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmcvqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. [48] Zhengxin Zhang, Dan Zhao, Xupeng Miao, Gabriele Oliaro, Qing Li, Yong Jiang, and Zhihao Jia. Quantized side tuning: Fast and memory-efficient tuning of quantized large language models. arXiv preprint arXiv:2401.07159, 2024. [49] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. GaLore: Memory-efficient llm training by gradient low-rank projection. In International Conference on Machine Learning, Vienna, Austria, 2024. PMLR. URL http://arxiv.org/abs/2403.03507. 12 EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary document provides additional context, analysis, and results to support the main paper. We begin with discussion of the limitations of our approach and potential directions for future work, followed by brief examination of the broader social impact. We then present supplementary proofs and derivations to justify components of our method. Additional implementation details are included to facilitate reproducibility. Finally, we provide extended experimental results that further validate the effectiveness and versatility of EMLoC."
        },
        {
            "title": "6 Limitation and Future Work",
            "content": "While our emulator construction method enables significant memory savings and offers flexibility in placing LoRA modules, it relies on off-the-shelf SVD methods. These methods are primarily designed to preserve the inference-time behavior of the model, such as maintaining output logits, rather than the fine-tuning dynamics. As result, the emulator may not fully capture the training characteristics of the original model, which could introduce suboptimalities during fine-tuning. This limitation is shared by all baselines. promising direction for future work is to develop emulator construction strategies explicitly tailored to preserve fine-tuning behavior, such as gradient directions or parameter update trajectories. Such an approach could more faithfully mimic the training dynamics of the original model, potentially narrowing the performance gap between EMLoC and directing LoRA fine-tuning the original model."
        },
        {
            "title": "7 Social Impact",
            "content": "EMLoC lowers the memory barrier for fine-tuning large foundation models, broadening access for users with limited resources and promoting more inclusive participation in AI development. While this democratization has clear benefits, it may also increase the risk of misuse, such as facilitating the generation of harmful or biased content. Additionally, although EMLoC reduces memory usage, widespread fine-tuning of large models may still contribute to environmental concerns. We encourage responsible use and transparency in deployment to mitigate these risks."
        },
        {
            "title": "8.1 Verification of Corrected LoRA",
            "content": "In the main text, we proposed that the corrected LoRA module Λc = AW can be constructed by (10) (cid:0)W (cid:1). We now verify that substituting Λc into the original model indeed , A, B = ="
        },
        {
            "title": "W c",
            "content": "where = satisfies the desired condition x(W + Λc) = x(W + Λ) VΛ. Note that we can write any VΛ as linear combination of the columns of γ Rd such that = Aγ VΛ. (11) (12) 13 We now substitute this into the left-hand side of Eq. (11) and simplify: Aγ)(W + Λc) x(W + Λc) = (W AW Aγ)(W + = (W B) AW Aγ)(W + = (W AW Aγ)(W + Λ = (W Λ (W + = γ(W Λ IW + = γ(W + = γ(W Λ) = (W Aγ)(W + Λ) = x(W + Λ) VΛ, A) (W )) A)W (W )) (W )) (13) which is exactly the right-hand side of Eq. (11)."
        },
        {
            "title": "8.2 Justification for Using SVD in LoRA Correction Preprocessing",
            "content": "A and In our LoRA correction algorithm, we introduce preprocessing step where we apply SVD to the LoRA weights to obtain B. It is important to note that, in the subsequent derivation of the correction procedure, the only essential property of is that its columns form an orthonormal basis. Any other pair (W A, B) that satisfies this orthonormality condition can also be used in the correction algorithm. In this section, we show that although SVD may appear to be an arbitrary choice, it is valid and convenient one and critically, the final corrected LoRA result remains the same for any orthonormal decomposition. Formally, suppose we have where both LoRA modules are identical: and (14) have orthonormal columns. Our goal is to show that the resulting corrected = WAWB, = AW AW (cid:0)W (cid:0)W (W )(cid:1) = Given Eq. (14), this reduces to prove that are orthonormal and span the same subspace VΛ, (16) AW Since the columns of both and are identicalthey both represent the projection onto VΛ. Therefore, the resulting corrected LoRA modules are equivalent, regardless of the specific orthonormal basis chosen. This justifies the use of SVD in our preprocessing step: while not strictly necessary, it provides one convenient and consistent way to obtain the required orthonormal basis. and = (W )(cid:1) . AW . AW AW (15)"
        },
        {
            "title": "9.1 Usage of Existing Assets",
            "content": "In our experiments, we make use of publicly available datasets, models, and codebases in accordance with their respective licenses. Specifically, we use the InfoVQA and TextVQA datasets, which are released under the CC-BY license; the PMC-VQA and WC-VQA datasets, available under the CC BY-SA license; and the WebSRC and LAION-COCO datasets, released under the MIT license; and the ChartQA data set, release under the GPL-3.0 license. The model backbones and training code are based on InternVL2.5, which is licensed under MIT. All assets were used strictly for academic, non-commercial research in accordance with their intended terms. We thank the respective authors and communities for making these valuable resources publicly available. 9."
        },
        {
            "title": "Implementation Details",
            "content": "Main Results. Detailed information, including the number of base model parameters, LoRA ranks, and the number of trainable parameters, for the experiments in our main results  (Table 1)  can be found in Table 7. Note that we use different LoRA ranks for each baseline to ensure that the number of trainable parameters is approximately the same across methods. 14 Table 7: Information of experiments for main results. Base Model Parameters Trainable Parameters Ratio Method LoRA Rank Table 8: Information of the experiments for 26B and 38B large models. InternVL-26B InternVL2.5-38B 100% Original 50% 25% InternVL 4B Offsite UPop EMLoC InternVL 2B Offsite UPop EMLoC 8.1B 3.7B 3.8B 3.8B 3.7B 2.2B 2.2B 2.3B 2.3B 8 10 64 14 19 64 19 8 18.9M 18.7M 18.9M 18.7M 18.9M 18.7M 18.9M 18.9M 18.9M Original Parmaeters Emulator Parameters LoRA Rank Trainable Parameters 25.5B 38.4B 5.0B 5.3B 4 27.1M 33.5M Table 9: Performance comparison between EMLoC and zero-shot inference. Note that finetuning with EMLoC requires less memory than performing inference. This enables users who can run inference to also benefit from fine-tuning, gaining additional performance improvements without increased memory cost. Quantization Method ChartQA DocVQA InfoVQA TextVQA PMC-VQA WebSRC WC-VQA Zero-shot 83.8 EMLoC 50% 84.6 EMLoC 25% 84.2 Zero-shot 83.7 EMLoC 25% 84.2 92.0 92.3 92.3 91.3 91. 69.6 69.9 70.0 67.1 67.5 78.5 78.8 79.0 78.6 79.0 48.6 52.3 51.6 48.1 50. 76.5 85.2 79.6 67.4 78.1 43.1 48.8 46.2 42.0 44.8 Large Models. Details of the experiments involving the 26B and 38B models  (Table 2)  are provided in Table 7. Note that for these experiments, we report performance on subset of the test set due to limited computational resources, which made full-set inference infeasible within reasonable time."
        },
        {
            "title": "10.1 Zero-shot Results",
            "content": "In Table 9, we report the zero-shot performance of the backbone models used in our experiments. As expected, fine-tuning with EMLoC leads to consistent improvements over the zero-shot baselines. While surpassing zero-shot performance is not surprising, it is important to highlight that EMLoC achieves these gains under the same memory requirements as inference. This means that users who are able to run inference on these models can also benefit from task-specific fine-tuningwithout needing additional memory resourcesmaking model adaptation more accessible in practice."
        },
        {
            "title": "10.2 Computation Resource",
            "content": "We report the computational resource usage of our main experiments in Table 10, including memory consumption during both fine-tuning and inference. Additionally, we present the FLOPs per forward pass and the throughput, measured as the time taken for full forward and backward pass on single training sample. All metrics are evaluated on the same machine with sequence length of 2048 to ensure fair comparison. Notably, Offsite-tuning exhibits slightly higher throughput due to its emulator using significantly fewer layers than other baselines. Despite this, EMLoC achieves superior fine-tuning performance while maintaining comparable computational resource, demonstrating its efficiency and effectiveness."
        },
        {
            "title": "10.3 Comparison with QLoRA",
            "content": "Although quantization-based methods like QLoRA are orthogonal to our focusthey do not reduce fine-tuning memory relative to inference, we include comparison to highlight the potential 15 Table 10: Computational resource usage and performance comparisons of finetuning approaches. Ratios follow the definitions in Table 1. Note that EMLoC achieves stronger performance than other baselines while maintaining comparable computation resource. Ratio Method 100% Original 50% 25% InternVL 4B Offsite UPop EMLoC InternVL 2B Offsite UPop EMLoC Fine-tuning memory (GB) Inference memory (GB) TFLOPs Throughput (sample/sec) PMC WebSRC WC-VQA 22.3 15.4 13.8 14.0 14.2 10.9 10.7 11.3 11.5 16. 8.1 16.1 16.1 16.1 4.9 16.1 16.1 16.1 37.4 18.9 16.1 16.5 17.8 12.9 9.3 10.3 11.3 0. 0.17 0.20 0.16 0.16 0.29 0.33 0.22 0.21 52.9 50.6 51.0 50.7 52.3 44.6 50.6 50.9 51.6 87. 84.8 76.1 76.4 85.2 78.1 76.6 76.6 79.6 53.4 48.6 45.9 42.1 48.8 34.4 45.4 44.1 46.2 Table 11: Comparison between EMLoC and QLoRA. Quantization-based methods offer limited flexibility in memory reduction and can degrade inference performance, while EMLoC enables fine-tuning on consumer-grade 24GB GPUs without affecting the base models inference quality."
        },
        {
            "title": "Method",
            "content": "Zero-shot LoRA QLoRA EMLoC InternVL2.5-26B InternVL2.5-38B Fine-tuning memory (GB)"
        },
        {
            "title": "WC VQA",
            "content": "Fine-tuning memory (GB)"
        },
        {
            "title": "WC VQA",
            "content": "- 72.9 38.1 18.6 53.6 - 39.0 56.8 - 94.8 43.4 20.1 51 - 43.6 52.6 advantages of our emulator-based approach. Results are presented in Table 11. Memory usage is measured in the same way as Table 10, with the exception that methods other than EMLoC are run across multiple GPUs without distributed data parallelism (DDP), and model weights are split across devices. Due to limited computing resources, we omit standard LoRA fine-tuning results for these large models. By design, quantization methods offer limited flexibility and fixed upper bound on memory savings. As shown in Table 11, even with 4-bit quantization, QLoRA cannot support fine-tuning of InternVL2.5-26B or InternVL2.5-38B on 24GB consumer GPU. In contrast, EMLoC enables flexible memory reduction via emulator construction, allowing fine-tuning under memory budgets. Furthermore, while quantization performs well in most cases, it can degrade the original models performanceeven during inference. In our experiments, quantizing InternVL2.5-26B and 38B significantly reduced accuracy, to the extent that QLoRA underperformed compared to the halfprecision zero-shot baseline. These results underscore the practical benefits of emulator-based fine-tuning, offering greater memory flexibility without compromising inference performance."
        },
        {
            "title": "10.4 Ablation Study on the Number of Calibration Data",
            "content": "EMLoC relies on small calibration set DC to perform activation-aware SVD during emulator construction. In our main experiments, we use 64 samples as the default size for DC. To evaluate the sensitivity to this choice, we vary the number of calibration examples and report the results in Fig. 5. As shown, model performance plateaus after using 3264 calibration samples, indicating that only small amount of task-specific data is sufficient to construct an effective emulator. This highlights the practicality of EMLoC in low-resource scenarios. Figure 5: We plot performance on WC-VQA under different number of calibration data."
        },
        {
            "title": "10.5 Applying to Diffusion Model Personalization",
            "content": "Table 12: Quantitative results of applying EMLoC to diffusion model. EMLoC achieves comparable, sometimes slightly lower, performance than direct fine-tuning while significantly reducing memory usage. To further assess the generalizability of EMLoC beyond text generation, we apply it to the task of personalizing large textto-image diffusion model. Specifically, we use DreamBooth [29]a well-established personalization methodcombined with LoRA to fine-tune FLUX.1-dev [16], 12B state-of-the-art diffusion model. We conduct experiments on 8 subjects from the DreamBooth dataset, fine-tuning each for 500 steps without prior preservation. For evaluation, we report standard metrics used in prior works: DINO [3], CLIP-I [27], and CLIP-T scores. Due to the short fine-tuning duration in this experiment, we adopt standard SVD for emulator construction instead of activation-aware SVD, which would be less cost-effective in this setting. Fine-tuning memory (GB) w/o EMLoC w/ EMLoC DINO CLIP-I CLIP-T 0.851 0. 0.652 0.615 0.306 0.321 35.1 22.9 Method Quantitative results are presented in Table 12. Note that normal fine-tuning requires over 35GB of memory, whereas EMLoC enables fine-tuning within 24GB GPU budget. While EMLoC achieves slightly lower DINO and CLIP-I scores, it yields higher CLIP-T scorelikely due to slower convergence. We also show qualitative examples in Fig. 6. These results demonstrate that EMLoC can be effectively applied to image generation tasks, reinforcing its potential as plug-and-play replacement for conventional LoRA fine-tuning with reduced memory requirements. Figure 6: Qualitative results of applying EMLoC to diffusion model personalization. DreamBooth with LoRA is used to personalize the 12B FLUX.1-dev diffusion model, illustrating that EMLoC can be effectively extended to generative tasks beyond text."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "National Taiwan University"
    ]
}