{
    "paper_title": "Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt",
    "authors": [
        "Bin-Bin Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 4 6 2 9 0 . 5 0 5 2 1 : r Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt Bin-Bin Gao Tencent YouTu Lab csgaobb@gmail.com Code and Models: https://github.com/gaobb/OneNIP Abstract. Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with single model. However, these selfattention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. Keywords: Unsupervised Reconstruction Unified AD Image Prompt"
        },
        {
            "title": "Introduction",
            "content": "Unsupervised visual anomaly detection aims to learn models only on normal training samples and expects these learned models to be capable of detecting anomalies at the image level and even localizing anomaly regions at the pixel level for both normal and anomaly testing samples. Anomaly detection (AD) has wide range of applications, including video surveillance [13,28,40], medical image diagnosis [19, 48], industrial defect inspection in manufacturing [3, 4, 26], and more. Most AD methods [1, 10, 11, 20, 25, 35, 37, 38, 54] mainly focus on training separated models for different objects or textures. However, this separated paradigm (one model for one class) may be not practical, as it requires B.-B. Gao is the corresponding author. 2 B.-B. Gao (a) Qualitative comparisons on selected common (left three columns) and camouflaged (right three columns) anomaly images. Fig. 1: Comparisons of state-of-the-art UinAD and our OneNIP. The proposed image OneNIP detects anomalies through learning comparison with one normal as visual prompt. Compared to UniAD, OneNIP enjoys more accurate anomaly localization (a) and faster convergence (b). (b) Testing metrics (I-ROC, P-ROC and P-PR) comparisons as function of training epoch on MVTec testing set. high memory consumption and storage burden, especially when the number of classes increases. In contrast, unified AD (one model for all classes) attempts to detect various anomalies for all categories using single model. Compared to the separated training mode, the unified AD paradigm is more challenging as it requires handling more complex data distributions. Therefore, most AD methods often suffer from significant performance drop when extending them from separated paradigms to unified ones. Furthermore, it seems necessary to study unified AD from foundational model perspective. recent work (UniAD) [52] attempts to detect multiple anomalies for all categories with unified model using transformer reconstruction network. However, the pure transformer suffers from over-fitting because of identity shortcut issue, which appears as returning direct copy of input disregarding its content. This implies that even anomalous samples can be well recovered with the learned model and hence fail to be detected. To address this issue, UniAD [52] proposed layer-wised query decoder and neighbor-masked attention (NMA) to prevent model learning from the shortcut. Similar to NMA in UniAD, SSPCAB [35] learns to reconstruct the masked area using contextual information implemented by dilated convolutional. Despite UniAD and SSPCAB employing different architectures and implementation strategies, they share the same spirit of reconstruction with context. In this way, the performance of AD can be ensured as most objects inherently possess specific physical structures or geometric characteristics as shown in Fig. 1a (left three columns). However, for some complex scenarios, e.g., camouflaged anomalies (right three columns in Fig. 1b), which refer to abnormal regions that are seamlessly embedded in their context in an image, it is hard to effectively detect them only using contextual information of themselves. In order to explore more general anomaly detection, lets first recall how we humans recognize anomalies. Generally, people are able to perceive anomalies OneNIP 3 when an input significantly deviates from those normal or expected patterns stored in the human brain. There actually, in fact, exists evidence to support this point in neural science. For example, predictive coding theory states that the human brain compares its expectations with the data it receives, and sends discrepancies (prediction errors) to higher levels [32]. This process allows the brain to perceive anomalies based on memory and contextual information. PatchCore [37] indeed captures normal local patch features, stores them in memory bank, and then recognizes anomalies by comparing input features with the memory bank. In addition, some distribution-based methods [1, 10] model multivariate Gaussian distribution for normal local features, then utilize distance metric to measure anomalies. However, these memoryand distributionbased methods still struggle with detecting camouflaged anomalies because of ignoring global structuration information. Naturally, we raise question: how to elegantly leverage both contextual and global structural information to enhance the performance of anomaly detection? In this paper, we propose simple yet effective anomaly detection framework that utilizes normal image as global prompt to guide the feature reconstruction, which is inspired by predictive coding theory [32]. Under the guidance of normal image prompt, feature reconstruction network can leverage selfattention mechanisms to model contextual information, while also conveniently facilitating interaction between target feature and global image prompt using cross-attention. Therefore, our approach can effectively detect both common and camouflaged anomalies by utilizing normal image prompt as shown in Fig. 1a. Compared to state-of-the-art UniAD, our method exhibits faster convergence as shown in Fig. 1b. Our contributions are summarized as follows: We propose novel unified anomaly detection framework, that unsupervised reconstructs normal features utilizing both contextual information themselves and corresponding global information from normal image prompt. To enhance the guidance of the normal image prompt, we introduce pseudoanomalous samples and propose an unsupervised restoration stream that pushes these pseudo features to recover to their corresponding normal ones. We propose supervised refiner that regresses reconstruction errors from low to high resolution with both real normal samples and pseudo-anomalous samples, which greatly boosts the performance of anomaly segmentation. Our method achieves state-of-the-art performance with unified setting on three industry anomaly detection benchmarks, MVTec, BTAD, and VisA."
        },
        {
            "title": "2 Related Work",
            "content": "Embedding-based AD methods leverage offline features extracted from pretrained models for anomaly detection. It assumes that these offline features preserve discriminative information and thus help to separate anomalies from normal samples. PaDiM [10], MDND [34], and DFM [1] model normal distribution based on normal features, then utilize distance metric to measure anomalies. PatchCore [37] captures normal features and stores them in memory bank, 4 B.-B. Gao and calculates anomaly scores by comparing all patch features and the memory bank. However, computing the inverse of covariance in the normal distribution or searching in the memory bank brings large memory-consuming. In addition, there is domain gap between target (industrial images) and source distribution (e.g., ImageNet) if directly using offline features. CS-Flow [38] proposes to transform normal feature distribution into Gaussian distribution via normalizing flow. Further, PyramidFlow [22] combines latent templates and normalizing flow for high-resolution anomaly localization. CFA [20] and PADA [33] propose feature adaptation for adapting targeted datasets. Knowledge distillation methods [5, 5, 11, 39, 42, 45, 46] train student network to match fixed pre-trained teacher network. However, they always are limited by designing structural differences between teacher and student. Discriminator-based methods typically convert unsupervised anomaly detection to supervised anomaly detection by introducing pseudo (synthesized) anomaly samples. CutPaste [23] proposes simple strategy to generate synthetic anomalies, which cuts small rectangular area of variable sizes and aspect ratios from normal training images and pastes this patch back to the image at random location. Similar to CutPaste, DRAEM [54] generates pseudo anomaly images using Perlin [30] and obtains binarized anomaly maps. CutPaste [23] learns an image-level classifier for enhancing discrimination between normal and anomaly features, while DRAEM [54] learns an additional pixel-level segmentation model with pseudo-mask. PRN [56] presents variety of anomaly generation strategies for more accurate anomaly localization. DeSTSeg [57] proposes denoising knowledge distillation and employs segmentation network for accurate anomaly localization with synthetic samples. BGAD [51] proposes boundary-guided semi-push-pull loss for learning more discriminative features with normal and synthetic samples. Instead of synthesizing anomalies on images, SimpleNet [25] generates anomaly features by adding Gaussian noise to normal features and then learns binary discriminator to distinguish anomaly features from normal ones. [8] proposes self-supervised normalizing flow-based density estimation model, which is trained by normal images and synthetic anomalous images. Reconstruction-based AD methods assume that anomalous image regions or features should not be able to be properly reconstructed since they do not exist in normal training samples. Some works use generative models such as auto-encoders [2, 6, 14, 16, 21, 44] and generative adversarial networks [29, 49, 53] to reconstruct normal images. RGI [27] proposes robust GAN-inversion that can restore any input image (even with gross corruptions) to clean image and identify the corrupted region mask by solving the optimization problems thereof. Some works frame anomaly detection as an inpainting problem, where patches from images are partly masked. RIAD [55] randomly removes partial image regions and reconstructs the image from partial inpaintings with convolutional neural network. SSPCAB [35] learns to reconstruct masked regions using contextual information with masked convolutional kernel. To enhance reconstruction diversity while avoiding the undesired generalization of anomalies, pyramid deformation module is proposed to model diverse normal and measure OneNIP 5 the severity of anomaly in [24]. These methods tend to be computationally expensive because they involve reconstruction in image space. The recent UniAD [52], omniNAL [58] and FOD [50] reconstruct features extracted from pre-trained model and achieve state-of-the-art performance for unified anomaly detection. However, pixel-level anomaly segmentation is still unsatisfactory. Prompt-based AD methods using large pre-trained vision-language models, e.g., CLIP [31], have shown unprecedented generality, and achieve impressive performance on various tasks, such as zeroand few-shot image classification, open-vocabulary object detection [12], and text-to-image generation [36]. Recent studies, WinCLIP [18], SAA+ [7], AnomalyCLIP [59] and MVFA [17], have demonstrated that utilizing multiple fixed textual prompts or learning dynamic textual prompt on powerful CLIP model [31] can yield excellent performance for zeroand few-shot anomaly detection. Furthermore, AnomalyGPT [15] applying multi-turn dialogues not only indicates the presence and location of the anomaly but also provides detailed description of the anomaly in testing image. However, these methods primarily rely on textual prompts to identify anomalies. Different from them, we explore to detect anomalies via normal image as visual prompt."
        },
        {
            "title": "3 Methods",
            "content": "Our OneNIP is built on state-of-the-art UniAD and is mainly composed of an unsupervised reconstruction, an unsupervised restoration, and supervised refiner as shown in Fig. 2. The unsupervised reconstruction and unsupervised restoration share the same encoder-decoder architecture. The encoder models contextual information with self-attention transformer, while the decoder models the relationship between target features and normal prompt with bidirectional cross-attention transformer. The supervised refiner regresses the reconstruction errors from low to high resolution for more accurate anomaly localization. Concretely, for normal input image and its corresponding prompt image (I and RHW 3), we parallelly extract their offline features (F and Rhwc) using pre-trained backbone, e.g., EfficientNet-b4 [41]. Then, the self-attention encoder independently processes them with added positional embeddings for modeling contextual dependencies. Next, these encoded features and prompt tokens are dynamically updated in two directions (promptto-features and features-to-prompt) with bidirectional decoder consisting of multiple two-way cross-attention blocks. We expect the original feature to be well reconstructed by fully exploring both the context and relationship of the target feature and normal image prompt. To further enhance the guidance of the normal image prompt, we introduce pseudo-anomalous image synthesizing from and propose an unsupervised feature restoration stream that pushes the pseudo-anomalous feature to recover to its corresponding normal feature by refusing the encoder-decoder network. At inference, the unsupervised restoration stream can be flexibly removed. Finally, the reconstruction errors between the 6 B.-B. Gao Fig. 2: Overview of OneNIP for unified anomaly detection. In the training stage, both normal and synthetic images are fed pre-trained backbone for extracting multi-level representation. Under the guidance of normal image prompt, the normal features are reconstructed in an unsupervised reconstruction stream (Sec. 3.1), and the synthetic anomaly features are restored in an unsupervised restoration stream (Sec. 3.2). Furthermore, supervised refiner (Sec. 3.3) is used to regress reconstruction errors for both normal and synthetic anomaly images. The unsupervised restoration stream will be removed at inference. original and reconstructed ones are refined by lightweight supervised refiner module with real normal and pseudo anomaly samples and their pixel-level anomaly masks. Next, we elaborately introduce them in this section."
        },
        {
            "title": "3.1 Reconstruction with Normal Image Prompt",
            "content": "Revisiting UniAD. We focus on unified anomaly detection which requires model to handle more complex data distribution and thus is more challenging. As we know, reconstruction-based UniAD [52] using an encoder-decoder transformer is powerful and state-of-the-art solution for unified anomaly detection and is composed of neighbor-masked attention (NMA) and layer-wise query decoder (LQD). The NMA limits that one feature cant see itself and its neighborhoods and thus takes its contextual features (long dependencies) for reconstruction. On the other hand, learnable query embedding qi is first fused with the encoder embedding xe and then integrated with the outputs xi of the i-th decoder layer. Experiments have proven that the learned query embedding can alleviate over-fitting. For simplicity, we here omit MLP, residual connection, layer normalization, and dropout in LQD, and then formulate two important steps in the i-th block of LQD as follows: = softmax(qixe xi+1 = softmax(qxi / / c)xe, c)xi d, (1) where is the dimension of xe and x0 LQD. In actual experiments, Eq. 1 is implemented by multi-head self-attention [43]. In each block of LQD, it is important to note that qi is individual and learnable, while xe remains fixed and unchanged. The LQD reconstructs features only by themselves, which may lead to failure when facing challenging anomalies. is initialized by xe at the first block in OneNIP 7 Unidirectional decoder with static prompt. We expect the feature reconstruction not only to rely on its structure and characteristics but also to be guided by normal prompt, aiming to reduce the difficulty of reconstruction and improve the performance of anomaly detection. simple and naive manner is to directly replace the query embedding qi in the LQD with the encoder output pe of normal image prompt p, thereby enabling the interaction between the prompt and target features. Therefore, we convert Eq. 1 as follows: / = softmax(pexe xi+1 = softmax(qxi / c)xe, c)xi d. (2) The change is simple but has completely different implications and boosts the performance of anomaly detection (Tab. 4). In Eq. 2, the prompt encoding statically interacts with the target feature in unidirectional manner, hence we call it unidirectional decoder. However, this unidirectional mode may not be flexible enough and may fail to align with the target feature especially when the target feature is continuously updated. Bidirectional decoder with dynamic prompt. Unlike the static prompt in the unidirectional decoder, we dynamically update both prompt and target features using pair of bidirectional cross-attention as follows: pi+1 = softmax(pi xi+1 = softmax(xi c)xi d, c)pi+1 dxi dpi+1 / (3) / , and x0 where p0 is initialized by pe and xe from the encoder module. The above bidirectional decoder models two-directional feature interactions including prompt-to-features and features-to-prompt. The first interaction performs cross-attention from prompt tokens (as queries) to the target features, and the second interaction performs another cross-attention from the target features (as queries) to prompt tokens. The next decoder block takes updated prompt tokens and target features from the previous block. In this way, the target feature reconstruction not only utilizes its contextual information but also leverages the corresponding normal prompt dynamically. It is worth noting that this bidirectional modeling way also enhances the flexibility of the prompt features and can adapt to the distribution shift of target features to some extent. Last, final cross-attention is used to update prompt tokens on the outputs of the bidirectional decoder, and its output is taken as the reconstructed one ( ˆF n) of the original feature (F n). The reconstruction loss function computes the mean squared error (MSE) between the reconstructed and original features as Lrec = 1 (cid:88) (cid:88) (cid:88) (F i,j,k ˆF i,j,k)2. i=1 j= k=1 (4)"
        },
        {
            "title": "3.2 Restoration with Normal Image Prompt",
            "content": "It can be observed that the unsupervised reconstruction learning is solely performed on normal training images, which may lead the model to rely more on 8 B.-B. Gao its contextual information and weaken the involvement of the image prompt in the reconstruction process. To further enhance the guidance of the image prompt, an expected manner is to increase the difficulty of the reconstruction task, forcing the network to rely not only on contextual information from itself but also on prompt information from the normal image prompt. To achieve this, we introduce artificially synthesized pseudo anomaly image a, which can be easily generated by adding corruptions or disruptions to normal training image n, such as CutPaste [23] and DRAEM [54]. Based on the pseudo anomaly image, we can convert the previous reconstruction into restoration problem that expects to restore the anomaly feature to the normal one with normal image prompt p. This restoration manner is consistent with the expectation of reconstruction models during the testing phases. Similar to the reconstruction process, we first feed pair of images (I and p) into pre-trained backbone for extracting offline features (F and p) and then obtain the ultimately repaired features ˆF sequentially applying the offline paired features into self-attention encoder and bidirectional cross-attention decoder. Specifically, the self-attention encoder parallelly takes and as inputs and outputs as xe and pe. Next, the bidirectional decoder is initialized by xe and pe, and dynamically updated with Eq. 3. In the i-th block of the bidirectional decoder, we denote dynamic feature and prompt as xi and pi d, which is easily obtained by simply replacing xi in Eq. 3. Different from the objective function Eq. 4 in unsupervised reconstruction, the restoration loss function computes the MSE between the restored feature ( ˆF a) and the corresponding original normal feature (F n) as with xi Lres = 1 w"
        },
        {
            "title": "3.3 Supervised Refiner",
            "content": "c (cid:88) (cid:88) (cid:88) (F i,j,k ˆF i,j,k)2. i=1 j=1 k=1 (5) Given normal training image and the corresponding anomaly mask (all elements are zero), we can synthesize an anomaly image and denote its anomaly mask as a. Then, we feed the normal and synthesized images {I t}t={n,a} into pre-trained backbone and derive their offline representation as {F t}t={n,a}. Then, we reconstruct the normal as ˆF with the proposed reconstruction stream and restore anomaly as ˆF with the proposed restoration stream, respectively. Here, we use the absolute element-wise subtraction of original and reconstructed (restored) features to measure their difference, that is Et = ˆF t, = {n, a}. (6) In fact, the L2 norm of Et in Eq. 6 can be used to roughly localize anomaly regions. In this way, however, it is hard to accurately locate abnormal regions since feature reconstruction or restoration is performed in low-resolution (i.e., 1/16 of original input) latent space. OneNIP 9 Note that the synthesized anomaly image naturally carries pixel-level anomaly mask a, we expect to fully utilize to further refine reconstruction errors from low to high resolution. To end this, we design lightweight and pixellevel refiner based on reconstruction errors for performing anomaly segmentation. The refiner consists of several transposed convolution blocks following 11 convolution layer. Here, each transposed convolution block upsamples the reconstruction error Et by 2, and it is composed of 33 convolution, BatchNorm, ReLU, and 22 deconvolution. In our experiment, we employ two transposed convolutional blocks and thus upscale the reconstruction error from 1/16 to 1/4 relative to the input image. Finally, the 11 convolution layer transforms the channel number of upscaled reconstruction error to 1 and obtains an estimated anomaly map as ˆM t. For compute loss between ˆM and the ground-truth t, we further resize ˆM to the size of t. Considering that anomaly pixels are typically in the minority in anomaly detection, we utilize Dice loss [47], which is effective for learning from extremely imbalanced data, that is Lseg = 1 (cid:80)H i=1 2 (cid:80)H i=1 j=1( ˆM (cid:80)W ˆM i,j)2 + (cid:80)H j=1 i,j i,j (cid:80)W j=1(M i=1 (cid:80)W i,j)2 , (7) where (i, j) represents spatial location in or ˆM t."
        },
        {
            "title": "3.4 Training Loss",
            "content": "During training, given an image of specific class, we randomly sample normal image among all training images of this class to serve as its prompt by default. In addition, we also explore other prompt strategies, e.g., fixed mode, which means that only one fixed image prompt is used for each category during training. Considering all three objectives including unsupervised reconstruction, unsupervised restoration, and supervised refiner in our OneNIP, the total training loss is = Lrec + Lres + λLseg, (8) where λ > 0 is weight that balances the importance of the two types of loss functions Lrec + Lres and Lseg. 3."
        },
        {
            "title": "Inference",
            "content": "At inference, we first randomly select normal training image for each class and then pre-extract offline prompt features for constructing class-aware prompt pool {Pi} i=1. Given testing image and its feature , we can derive an appropriate prompt by computing the cosine similarity between the testing feature and the prompt pool because the class of the testing image is agnostic. Pixel-Level Anomaly Segmentation: For unsupervised reconstruction, the anomaly score map is calculated as the L2 norm of the reconstruction error as Srec = ˆF 2 Rhw. (9) 10 B.-B. Gao For supervised refiner, the anomaly score map is predicted as ˆM RHW . Finally, we combine Srec (resizing into original resolution) and ˆM together and take it as the final anomaly segmentation map, that is = (1 α) Srec + α ˆM , (10) where α [0, 1] is weight. Image-Level Anomaly Classification: Anomaly classification aims to detect whether an image contains anomalous regions. Following the previous work, we take the maximum value of as the image-level anomaly score."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Following the previous works, we comprehensively evaluate our method on three industry anomaly detection benchmarks, including MVTec [4], BTAD [26], and VisA [60]. Protocol: We train single model for detecting all categories following UniAD. For fair comparisons, we use the original training/testing splits given in previous works [4,26,60]. In our experiments, all images are resized to 224224 for training and testing unless otherwise specified. Metric: We compare the state-of-the-art anomaly detection methods with our OneNIP using ROC and PR metrics in imageand pixel levels. We argue that the PR metric is better for anomaly segmentation, where the imbalance issue is very extreme between normal and anomaly pixels [9, 60]. Comparison Methods: We compare our method with diverse state-of-theart anomaly detection methods including CS-Flow [38], PaDiM [10], DFM [10], PatchCore [37], CFA [20], DRAEM [54], SimpleNet [25], and UniAD [52]. Here, most methods are run with the publicly available Anomalib except for DRAEM [54], SimpleNet [25], and UniAD [52] using official code."
        },
        {
            "title": "4.2 Comparisons with State-of-the-Arts",
            "content": "Main Results. We report the results of image-level classification and pixel-level segmentation on three industry AD datasets (MVTec, BTAD and VisA) and compare our OneNIP with state-of-the-art methods in Tab. 1. Some important observations are summarised as follows: Most state-of-the-art methods suffer from significant performance drop in both image-level classification and pixel-level segmentation when extending onemodel-one-class setting to one-model-all-classes one, which is also consistent with observations in UniAD. For example, state-of-the-art SimpleNet [25] drops about 21.4% (from 99.6% to 78.2%) in I-ROC and 17.1% (from 98.1% to 81.0%) in P-ROC, respectively; Our method beats all competitors and outperforms the state-of-the-art UniAD by large margin for pixel-level anomaly segmentation on all three datasets, e.g., from 44.7% to 63.7% on MVTec, from 50.9% to 56.8% on OneNIP 11 Table 1: Image-level anomaly classification and pixel-level anomaly segmentation comparisons with ROC/PR metric on MVTec, BTAD and VisA. All methods are evaluated under the unified setting. The best and second-best results are highlighted in red and blue, respectively. Note that the results are averaged over multiple categories and the full results of each category are presented in supplementary material. Datasets Metric CS-Flow [38] PaDiM [10] DFM [1] PatchCore [37] Embedding-based MVTec [4] I-ROC/PR P-ROC/PR 81.4 / 90.2 93.8 / 33.8 87.5 / 92.8 95.5 / 37.8 69.7 / 89.8 96.5 / 42.4 BTAD [26] I-ROC/PR P-ROC/PR 91.8 / 96.3 95.9 / 34. 95.7 / 97.4 96.7 / 48.7 68.8 / 82.8 96.3 / 48.0 VisA [60] I-ROC/PR P-ROC/PR 75.8 / 80.0 95.6 / 18.6 78.1 / 78.3 95.9 / 17. 51.6 / 77.8 96.5 / 25.2 89.8 / 96.3 96.4 / 50.1 89.2 / 96.4 96.3 / 48.4 90.3 / 92.0 96.8 / 38.2 Discriminator-based Reconstruction-based CFA [20] DRAEM [54] SimpleNet [25] UniAD [52] OneNIP 97.9 / 99.3 78.2 / 90.0 80.4 / 91.0 97.9 / 63.7 81.0 / 24.8 90.7 / 37. 96.5 / 98.9 96.8 / 44.7 91.4 / 95.3 85.2 / 49.6 87.5 / 87.7 95.6 / 40.4 84.7 / 95.0 74.2 / 12.3 69.0 / 73.8 91.4 / 16.8 81.8 / 85.8 78.1 / 15. 90.3 / 95.0 78.8 / 36.2 89.2 / 92.2 95.3 / 33.1 92.2 / 97.9 97.1 / 50.9 92.6 / 98.5 97.4 / 56.8 90.8 / 93.0 98.4 / 33.6 92.5 / 94.5 98.7 / 43. BTAD, and 33.6% to 43.3% on VisA; Some methods are not robust to different application scenarios while our method consistently outperforms state-of-theart methods. For example, DRAEM achieves 49.6% P-PR on MVTec, but only 12.3% on BTAD and 15.1% on VisA; For image-level anomaly classification, our method also surpasses UniAD in most cases, e.g., improving I-ROC performance from 96.5% to 97.9% on MVTec, and 90.8% to 92.5% on VisA. for Table 2: Comparisons with state-of-the-art UniAD on more complex data distribution (one model for multiple datasets). Furthermore, we also compared the trend of testing metrics (I-ROC, PROC and P-PR) for UniAD and our OneNIP with the number of training epochs increased, as shown in Fig. 1b. It can be observed that our method only requires significantly fewer epochs to achieve the same performance as UniAD, especially for P-PR. This reveals that the introduction of normal image prompt and supervised refiner indeed accelerates the convergence of the reconstruction model. Results on More Complex Distribution. In Tab. 1, we train unified anomaly detection model each dataset following the previous UniAD. To further demonstrate the effectiveness of the proposed OneNIP when facing more complex data distribution, we merge MVTec, BTAD, and VisA into larger scale and more categories dataset, then train UniAD and OneNIP on the merging dataset. We report the image-level classification and pixel-level segmentation results including the average over all 30 categories, and the mean results of each dataset in Tab. 2. Datasets #Classes Metric UniAD [52] OneNIP I-ROC/PR 94.8/98.0 97.1/99.0 P-ROC/PR 96.2/42.1 97.6/61.1 I-ROC/PR 89.9/92.4 91.9/93.9 P-ROC/PR 98.3/33.2 98.6/40. I-ROC/PR 92.0/97.1 92.0/97.5 P-ROC/PR 97.1/48.0 97.9/59.0 I-ROC/PR 92.6/95.7 94.5/96.8 P-ROC/PR 97.1/39.1 98.0/52.4 BTAD [26] MVTec [4] VisA [60] All 15 30 12 3 Our OneNIP still significantly outperforms the state-of-the-art UniAD in both image-level classification (94.5% vs. 92.6% in I-ROC) and pixel-level segmentation (52.4% vs. 39.1% in P-PR) when evaluating on more complex data B.-B. Gao Fig. 3: Qualitative comparisons of UniAD (second and sixth rows) and our OneNIP (third and seventh rows) on MVTec (15 classes), BTAD (3 classes) and VisA (12 classes). Here, the first and fifth rows are original testing images, and the fourth and eighth rows are their corresponding anomaly masks highlighted with red color. MVTec [4] Datasets Metric 224224 I-ROC/PR 97.9/99.3 P-ROC/PR 97.9/63.7 Table 3: Results comparisons of OneNIP with different resolutions on MVTec, BTAD and VisA. distribution (i.e., one model for multiple datasets). Furthermore, there is no significant performance drop from the unified case (one model for multi-class) in Tab. 1 to more unified case (one model for multi-dataset) in Tab. 2, while most existing methods suffer from significant performance drop when they are extended to complex distributions (i.e., one model for all classes). Results on Different Resolutions. We conduct OneNIP with varying input resolutions considering different defect area distributions on different datasets, and the results are reported in Tab. 3. For pixel-level anomaly segmentation, the performance tends to consistently improve when the input resolution increases in certain range (i.e., from 224224 to 320320). For image-level anomaly classification, accuracy can be significantly boosted when increasing input resolution on BTAD and VisA, while almost constant on MVTec. This is not surprising as we know that anomaly regions are typically smaller on BTAD and VisA compared to MVTec. The low resolution makes it challenging for pretrained models to capture anomaly characters, thus resulting in difficulties in small anomaly detection. Qualitative Comparisons. We present representative examples to qualitatively compare UniAD and our OneNIP for each object on MVTec, BTAD and VisA in Fig. 3. It can be observed that both UniAD and OneNIP are able I-ROC/PR 92.5/94.5 P-ROC/PR 98.7/43.3 I-ROC/PR 92.6/98.5 P-ROC/PR 97.4/56.8 320320 97.9/99.3 97.9/65.9 256256 97.6/99.2 97.8/64.7 94.2/95.7 98.8/46. 95.3/98.9 97.8/57.6 93.3/94.3 98.8/44.1 94.9/99.0 97.6/57.0 BTAD [26] VisA [60] Table 4: Ablation studies on MVTec. Default settings are marked in blue. (b) Effects of the number of Encoder, and Decoder (a) Prompt strategy in Reconstruction, Restoration, and Refiner No. Prompt Res. Ref. I-ROC P-ROC I-PR P-PR Enc Dec I-ROC P-ROC I-PR P-PR OneNIP 13 96.8 98.9 44.7 96.5 0 97.0 98.9 45.8 96.8 static 1 97.1 99.2 46.0 97.5 2 dynamic 97.0 98.9 46.5 96.7 3 4 dynamic 97.3 99.1 48.4 97.4 5 dynamic 97.9 97.9 99.3 63. 1 2 4 6 2 4 94.8 96.7 97.9 97.0 97.9 56.0 1 97.4 98.9 59.4 2 4 97.9 99.3 63.7 6 98.1 98.0 99.4 64.6 97.6 99.0 61.2 4 97.6 99.0 62.1 2 97.0 97.1 (c) Effects of weight α α I-ROC P-ROC I-PR P-PR 97.3 99.2 48.3 0.00 97.6 97.7 99.3 59.3 0.25 97.8 97.9 99.3 63.7 0.50 97.9 96.7 98.9 63.7 1.00 96.7 (d) Different prompt modes of the same category Train Test I-ROC P-ROC I-PR P-PR rand fixed rand 97.850.01 97.860.00 99.270.01 63.710.01 fixed 97.850.02 97.860.00 99.270.01 63.710.02 fixed 97.91 rand 96.050.24 97.490.03 98.340.19 60.650. 97.86 63.66 99.30 to recognize anomalies at image level, but OneNIP often does more precise segmentation at pixel level."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "To verify the effectiveness of all proposed components and the effects of hyperparameters, we implement extensive ablation studies on MVTec with unified setting as shown in Tab. 4. Static or Dynamic Prompt in Reconstruction. We first simply replace the learned query embedding with normal image prompt feature in the LQD of UniAD, which brings 1.1 points improvement in P-PR for anomaly segmentation and also improves image-level anomaly classification in I-ROC and P-ROC (Lines 0 and 1 in Tab. 4a). Further, there is significant improvement in both imagelevel classification and pixel-level segmentation when we take the static prompt as an initial value and dynamically update the prompt and target feature in our bidirectional decoder (Lines 0 and 2 in Tab. 4a). This demonstrates that the dynamic prompt manner takes an important role in unsupervised feature reconstruction. Effectivness of Restoration. To enhance the guidance of prompt in unsupervised reconstruction, we introduce synthesized anomaly images and restore their features to normal ones and thus form an unsupervised restoration stream. In fact, this is what we expect at the inference stage. We can see that the restoration stream is effective in improving pixel-level anomaly segmentation (from 46.0% to 48.4% in P-PR, Lines 2 and 4 in Tab. 4a). For fair comparisons, we directly introduce the restoration stream into UniAD without normal prompt, but the corresponding improvement is less than ours (Lines 3 and 4 in Tab. 4a). This further implies the importance of normal prompts in the restoration stream. 14 B.-B. Gao Effectivness of Refiner. We improve anomaly localization by regressing reconstruction errors from low to high resolution with supervised refiner. It is simple and lightweight but greatly boosts pixel-level anomaly segmentation (from 48.4% to 63.7% in P-PR, Lines 4 and 5 in Tab. 4a). This can be attributed to two facts: reconstruction errors themselves can roughly localize anomalies, and pseudo anomalies carry accurate pixel-level masks. Effects of Hyper-parameter. We carefully study the effects of some hyperparameters, such as the number of encoder and decoder, coefficient α between reconstruction and refiner, and prompt mode in training and testing as shown in Tab. 4b, c, and d. It can be seen that more encoders and decoders are helpful for better performance. Furthermore, it is important to choose reasonable α, which will be dependent on the refiner when the coefficient is too large, and on the reconstruction when its too small. To demonstrate the robustness of our method to different normal prompts of the same category, we compare different prompt modes, i.e., random and fixed in training and testing, and report averaged testing metric and standard deviation based on 10 random seeds in Tab. 4d. It can be seen that our method is robust for different normal prompts of the same category when training in random mode. However, incorrect image prompts will significantly decrease the performance. For example, if one MetaNut image as the prompt for testing the Screw images, the corresponding I-ROC drops from 91.4% to 67.3%, and P-PR drops from 39.8% to 2.3%. Furthermore, it will weaken performance if training the model with fixed image prompt while testing in random manner. This performance degradation mainly happens for large positional changes, such as Srew on MVTec, and the performance of most categories is maintained because the geometric appearance of MVTec for most categories is roughly aligned."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose simple yet effective anomaly detection framework that learns to detect anomalies with normal image prompt. To adequately leverage the prompt information in unsupervised feature reconstruction, we first propose bidirectional decoder to dynamically update the prompt and target features and promote their interaction. To further enhance the guidance of the prompt, we introduce pseudo-anomalous images and propose restoration stream that restores these pseudo-anomalous features to the corresponding normal ones. Furthermore, we propose lightweight refiner that regresses the reconstruction errors for both real normal and pseudo-anomalous samples from low to high resolution in supervised manner, which greatly boosts anomaly segmentation performance. Limitation: In our OneIP, the proposed restoration stream introduces additional training costs, although it can be completely removed at inference. Furthermore, the bidirectional decoder and supervised refiner are only simply designed, leaving ample room for improvement. OneNIP"
        },
        {
            "title": "References",
            "content": "1. Ahuja, N.A., Ndiour, I.J., Kalyanpur, T., Tickoo, O.: Probabilistic modeling of deep features for out-of-distribution and adversarial detection. In: NeurIPSW (2019) 2. Bengio, Y., Yao, L., Alain, G., Vincent, P.: Generalized denoising auto-encoders as generative models. In: NeurIPS (2013) 3. Bergmann, P., Batzner, K., Fauser, M., Sattlegger, D., Steger, C.: Beyond dents and scratches: Logical constraints in unsupervised anomaly detection and localization. IJCV 130(4) (2022) 4. Bergmann, P., Fauser, M., Sattlegger, D., Steger, C.: MVTec AD: comprehensive real-world dataset for unsupervised anomaly detection. In: CVPR (2019) 5. Bergmann, P., Fauser, M., Sattlegger, D., Steger, C.: Uninformed Students: Student-teacher anomaly detection with discriminative latent embeddings. In: CVPR (2020) 6. Bergmann, P., Löwe, S., Fauser, M., Sattlegger, D., Steger, C.: Improving unsupervised defect segmentation by applying structural similarity to autoencoders. In: VISIGRAPP (2019) 7. Cao, Y., Xu, X., Sun, C., Cheng, Y., Du, Z., Gao, L., Shen, W.: Segment any anomaly without training via hybrid prompt regularization. arXiv:2305.10724 (2023) 8. Chiu, L.L., Lai, S.H.: Self-supervised normalizing flows for image anomaly detection and localization. In: ICCV (2023) 9. Davis, J., Goadrich, M.: The relationship between precision-recall and roc curves. In: ICML (2006) 10. Defard, T., Setkov, A., Loesch, A., Audigier, R.: PaDim: patch distribution modeling framework for anomaly detection and localization. In: ICPR (2021) 11. Deng, H., Li, X.: Anomaly detection via reverse distillation from one-class embedding. In: CVPR (2022) 12. Du, Y., Wei, F., Zhang, Z., Shi, M., Gao, Y., Li, G.: Learning to prompt for openvocabulary object detection with vision-language model. In: CVPR (2022) 13. Georgescu, M.I., Barbalau, A., Ionescu, R.T., Khan, F.S., Popescu, M., Shah, M.: Anomaly detection in video via self-supervised and multi-task learning. In: CVPR (2021) 14. Gong, D., Liu, L., Le, V., Saha, B., Mansour, M.R., Venkatesh, S., Hengel, A.v.d.: Memorizing Normality to Detect Anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In: ICCV (2019) 15. Gu, Z., Zhu, B., Zhu, G., Chen, Y., Tang, M., Wang, J.: AnomalyGPT: detecting industrial anomalies using large vision-language models. In: AAAI (2024) 16. Hou, J., Zhang, Y., Zhong, Q., Xie, D., Pu, S., Zhou, H.: Divide-and-Assemble: Learning block-wise memory for unsupervised anomaly detection. In: ICCV (2021) 17. Huang, C., Jiang, A., Feng, J., Zhang, Y., Wang, X., Wang, Y.: Adapting visuallanguage models for generalizable anomaly detection in medical images. In: CVPR (2024) 18. Jeong, J., Zou, Y., Kim, T., Zhang, D., Ravichandran, A., Dabeer, O.: WinCLIP: Zero-/few-shot anomaly classification and segmentation. In: CVPR (2023) 19. Kim, D.Y., Lee, S.J., Kim, E.K., Kang, E., Heo, C.Y., Jeong, J.H., Myung, Y., Kim, I.A., Jang, B.S.: Feasibility of anomaly score detected with deep learning in irradiated breast cancer patients with reconstruction. npj Digit. Med. 5(1) (2022) 16 B.-B. Gao 20. Lee, S., Lee, S., Song, B.C.: CFA: Coupled-hypersphere-based feature adaptation for target-oriented anomaly localization. IEEE Access 10 (2022) 21. Lee, Y., Kang, P.: AnoViT: Unsupervised anomaly detection and localization with vision transformer-based encoder-decoder. IEEE Access 10 (2022) 22. Lei, J., Hu, X., Wang, Y., Liu, D.: PyramidFlow: High-resolution defect contrastive localization using pyramid normalizing flow. In: CVPR (2023) 23. Li, C.L., Sohn, K., Yoon, J., Pfister, T.: CutPaste: Self-supervised learning for anomaly detection and localization. In: CVPR (2021) 24. Liu, W., Chang, H., Ma, B., Shan, S., Chen, X.: Diversity-measurable anomaly detection. In: CVPR (2023) 25. Liu, Z., Zhou, Y., Xu, Y., Wang, Z.: SimpleNet: simple network for image anomaly detection and localization. In: CVPR (2023) 26. Mishra, P., Verk, R., Fornasier, D., Piciarelli, C., Foresti, G.L.: VT-ADL: vision transformer network for image anomaly detection and localization. In: SIE (2021) 27. Mou, S., Gu, X., Cao, M., Bai, H., Huang, P., Shan, J., Shi, J.: RGI: Robust gan-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection. In: ICLR (2023) 28. Park, H., Noh, J., Ham, B.: Learning memory-guided normality for anomaly detection. In: CVPR (2020) 29. Perera, P., Nallapati, R., Xiang, B.: OCGAN: One-class novelty detection using GANs with constrained latent representations. In: CVPR (2019) 30. Perlin, K.: An image synthesizer. ACMSCG 19(3) (2005) 31. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021) 32. Rao, R.P., Ballard, D.H.: Predictive coding in the visual cortex: functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience 2(1) (1999) 33. Reiss, T., Cohen, N., Bergman, L., Hoshen, Y.: PANDA: Adapting pretrained features for anomaly detection and segmentation. In: CVPR (2021) 34. Rippel, O., Mertens, P., Merhof, D.: Modeling the distribution of normal data in pretrained deep features for anomaly detection. In: ICPR (2021) 35. Ristea, N.C., Madan, N., Ionescu, R.T., Nasrollahi, K., Khan, F.S., Moeslund, T.B., Shah, M.: Self-supervised predictive convolutional attentive block for anomaly detection. In: CVPR (2022) 36. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022) 37. Roth, K., Pemula, L., Zepeda, J., Schölkopf, B., Brox, T., Gehler, P.: Towards total recall in industrial anomaly detection. In: CVPR (2022) 38. Rudolph, M., Wehrbein, T., Rosenhahn, B., Wandt, B.: Fully convolutional crossscale-flows for image-based defect detection. In: WACV (2022) 39. Salehi, M., Sadjadi, N., Baselizadeh, S., Rohban, M.H., Rabiee, H.R.: Multiresolution knowledge distillation for anomaly detection. In: CVPR (2021) 40. Sultani, W., Chen, C., Shah, M.: Real-world anomaly detection in surveillance videos. In: CVPR (2018) 41. Tan, M., Le, Q.: EfficientNet: Rethinking model scaling for convolutional neural networks. In: ICML (2019) 42. Tien, T.D., Nguyen, A.T., Tran, N.H., Huy, T.D., Duong, S., Nguyen, C.D.T., Truong, S.Q.: Revisiting reverse distillation for anomaly detection. In: CVPR (2023) OneNIP 43. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 44. Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing robust features with denoising autoencoders. In: ICML (2008) 45. Wang, G., Han, S., Ding, E., Huang, D.: Student-teacher feature pyramid matching for anomaly detection. BMVC (2021) 46. Wang, S., Wu, L., Cui, L., Shen, Y.: Glancing at the patch: Anomaly localization with global and local feature comparison. In: CVPR (2021) 47. Wei, Q., Li, X., Yu, W., Zhang, X., Zhang, Y., Hu, B., Mo, B., Gong, D., Chen, N., Ding, D., et al.: Learn to segment retinal lesions and beyond. In: ICPR (2021) 48. Xiang, T., Lu, Y., Yuille, A.L., Zhang, C., Cai, W., Zhou, Z.: SQUID: Deep feature in-painting for unsupervised anomaly detection. In: CVPR (2023) 49. Yan, X., Zhang, H., Xu, X., Hu, X., Heng, P.A.: Learning semantic context from normal samples for unsupervised anomaly detection. In: AAAI (2021) 50. Yao, X., Li, R., Qian, Z., Luo, Y., Zhang, C.: Focus the Discrepancy: Intra-and inter-correlation learning for image anomaly detection. In: ICCV (2023) 51. Yao, X., Li, R., Zhang, J., Sun, J., Zhang, C.: Explicit boundary guided semipush-pull contrastive learning for supervised anomaly detection. In: CVPR. pp. 2449024499 (2023) 52. You, Z., Cui, L., Shen, Y., Yang, K., Lu, X., Zheng, Y., Le, X.: unified model for multi-class anomaly detection. In: NeurIPS (2022) 53. Zaheer, M.Z., Lee, J.h., Astrid, M., Lee, S.I.: Old is Gold: Redefining the adversarially learned one-class classifier training paradigm. In: CVPR (2020) 54. Zavrtanik, V., Kristan, M., Skočaj, D.: DRAEM: discriminatively trained reconstruction embedding for surface anomaly detection. In: ICCV (2021) 55. Zavrtanik, V., Kristan, M., Skočaj, D.: Reconstruction by inpainting for visual anomaly detection. PR 112 (2021) 56. Zhang, H., Wu, Z., Wang, Z., Chen, Z., Jiang, Y.G.: Prototypical residual networks for anomaly detection and localization. In: CVPR (2023) 57. Zhang, X., Li, S., Li, X., Huang, P., Shan, J., Chen, T.: Destseg: Segmentation guided denoising student-teacher for anomaly detection. In: CVPR (2023) 58. Zhao, Y.: OmniAL: unified cnn framework for unsupervised anomaly localization. In: CVPR (2023) 59. Zhou, Q., Pang, G., Tian, Y., He, S., Chen, J.: AnomalyCLIP: Object-agnostic prompt learning for zero-shot anomaly detection. In: ICLR (2024) 60. Zou, Y., Jeong, J., Pemula, L., Zhang, D., Dabeer, O.: Spot-the-difference selfsupervised pre-training for anomaly detection and segmentation. In: ECCV (2022) OneNIP"
        },
        {
            "title": "A Implementation Details",
            "content": "For fair comparisons, we maintain the same hyper-parameters as in UniAD [52]. All input images are resized to 224224 resolution for all methods both training phase and inference time. The 4 staged features extracted from stages 1 to 4 of EfficientNet-b4 are first resized to spatial size of 1414 and then concatenated together to finally form 272-channel feature map. For unsupervised reconstruction or restoration, the layer numbers of the encoder and decoder are set to 4 to balance performance and computation costs. For supervised refiner, we employ two transposed convolutional blocks, and the channels of each convolution block are set to 128. For synthesized anomaly generation, we employ CutPaste [23] and DRAEM [54] with probability of 0.5. The loss weight λ is set to 0.5. The model is trained with total of 1000 epochs on 8 Tesla V100 GPUs with batch size 64. AdamW optimizer with weight decay 1 104 is used. The learning rate is 1104 initially and dropped by 0.1 after 800 epochs. We conduct experiments based on the open-source framework PyTorch and NVIDIA V100 GPU. We use the official codes for DRAEM [54], SimpleNet [25] and UniAD [52], and the publicly available Anomalib for other methods."
        },
        {
            "title": "B Industry Anomaly Detection Benchmarks",
            "content": "Following previous works, we comprehensively evaluate our method on three industry anomaly detection benchmarks, MVTec [4], BTAD [26], and VisA [60]. MVTec [4] is highly popular dataset used for industrial anomaly detection. It encompasses 15 categories (10 objects and 5 textures) from real-world manufacturing. The whole dataset is split into training and testing sets. The training set includes 3,629 normal images, and the testing set contains 1,258 anomaly images and 467 normal images. All anomaly images are annotated by pixel-level mask, which is very convenient for pixel-level evaluation. BTAD [26] is another real-world industrial anomaly detection dataset. It contains total of 2,830 images, showcasing 3 industrial products with body and surface defects. The training set comprises 1,799 normal images while the testing set consists of 290 anomaly images and 451 normal images. Similar to the MVTec, pixel-wise annotations are given for anomaly images in the testing set. VisA [60] is currently larger and more challenging anomaly detection dataset. This dataset contains 12 objects spanning 3 domains, complex structures, multiple instances, and multiple anomaly classes. There are 10,821 high-resolution color images with 9,621 normal (8,659 for training and 962 for testing) and 1,200 anomaly images (all for testing) carrying both imageand pixel-level annotations. Complete Multi-class Anomaly Detection Results In our main paper, we reported only the averaged results of all categories for each dataset. Here, we provide more comprehensive report in Tabs. 5 and 6, detailing both image-level anomaly classification and pixel-level anomaly segmentation for each category on MVTec, BTAD, and VisA. 2 B.-B. Gao Table 5: Pixel-level anomaly segmentation comparisons with ROC/PR on MVTec, BTAD and VisA. All methods are evaluated under the unified setting. The best and second-best results are highlighted in red and blue, respectively. Datasets Catergories Embedding-based CS-Flow [38] PaDiM [10] DFM [1] PatchCore [37] CFA [20] 97.5 / 61.7 84.5 / 15.7 97.8 / 30.4 96.0 / 32.2 87.9 / 41.3 94.1 / 38.0 95.2 / 7.0 97.6 / 38.6 84.5 / 38.7 96.1 / 37.4 98.0 / 37.4 92.9 / 17.4 98.9 / 37.5 92.4 / 34.0 93.9 / 40.1 97.7 / 64.2 95.9 / 36.6 98.1 / 31.0 98.1 / 48.0 95.3 / 66.9 95.1 / 37.6 97.0 / 9.1 98.1 / 39.6 96.1 / 49.0 92.9 / 21.6 98.5 / 49.2 86.2 / 11.5 98.7 / 29.4 93.4 / 42.0 92.0 / 31.1 97.1 / 58.2 97.4 / 51.8 97.1 / 30.8 98.1 / 44.4 97.8 / 81.4 97.3 / 55.9 95.8 / 5.9 98.2 / 51.4 98.2 / 69.6 95.2 / 24.5 98.3 / 48.3 93.8 / 14.2 98.3 / 25.1 94.2 / 45.5 90.5 / 28.6 97.3 / 72.1 98.2 / 56.0 97.5 / 36.7 98.5 / 49.6 98.6 / 80.8 97.7 / 59.0 96.4 / 8.9 98.2 / 47.3 96.1 / 69.7 95.4 / 56.9 97.5 / 49.4 87.3 / 12.0 98.9 / 41.3 95.7 / 59.9 93.3 / 52.1 94.4 / 49.1 90.8 / 22.6 97.7 / 39.7 97.7 / 48.5 95.2 / 66.9 94.1 / 58.5 94.9 / 3.6 97.5 / 49.4 82.9 / 24.0 85.8 / 19.5 92.3 / 36.3 53.2 / 0.7 98.8 / 43.5 93.3 / 53.1 91.4 / 40.6 Discriminator-based Reconstruction-based DRAEM [54] SimpleNet [25] UniAD [52] OneNIP 98.5 / 81.6 96.1 / 75.9 98.2 / 67.5 52.4 / 5.7 98.6 / 49.9 7.7 66.5 / 98.7 / 70.2 97.5 / 73.9 96.5 / 74.1 56.5 / 22.8 96.0 / 47.6 75.4 / 37.4 98.9 / 39.8 97.3 / 55.8 98.8 / 53.4 97.6 / 68.9 98.8 / 84.1 74.1 / 27.9 97.6 / 59.1 96.3 / 63.5 99.0 / 69.5 94.4 / 60.7 98.4 / 45.6 98.7 / 54.9 99.6 / 71.7 96.2 / 51.1 95.3 / 76.6 85.0 / 68.2 94.9 / 65.3 93.9 / 69.6 79.8 / 21.0 82.8 / 24.4 89.4 / 12.5 88.5 / 20.3 89.5 / 53.3 83.1 / 42.3 69.9 / 0.6 92.1 / 31.8 69.0 / 19.1 88.4 / 25.5 94.7 / 31.4 0.3 26.6 / 85.3 / 7.8 90.1 / 56.0 85.8 / 25.4 98.0 / 68.0 97.5 / 53.1 98.6 / 46.5 98.1 / 53.6 93.3 / 49.5 95.4 / 40.7 98.4 / 24.5 98.4 / 39.7 98.3 / 73.2 96.6 / 33.2 98.5 / 51.4 96.6 / 22.2 98.8 / 33.7 92.1 / 44.2 93.1 / 37.8 93.8 / 33.8 95.5 / 37.8 96.5 / 42. 96.4 / 50.1 90.7 / 37.1 85.2 / 49.6 81.0 / 24.8 96.8 / 44.7 97.9 / 63. 95.0 / 41.8 93.5 / 33.1 99.4 / 28.9 95.9 / 43.5 94.6 / 45.7 99.7 / 56.8 94.3 / 38.4 94.9 / 57.6 99.6 / 47.9 94.3 / 44.6 95.0 / 50.4 99.6 / 50.2 92.8 / 26.6 94.7 / 52.2 99.4 / 42.3 87.8 / 14.4 41.3 / 4.1 93.4 / 18. 90.3 / 30.2 48.9 / 38.1 97.2 / 40.4 97.0 / 53.4 94.8 / 42.9 99.6 / 56.4 97.3 / 58.7 95.2 / 46.9 99.8 / 64.7 95.9 / 34.6 96.7 / 48.7 96.3 / 48. 96.3 / 48.4 95.6 / 40.4 74.2 / 12.3 78.8 / 36.2 97.1 / 50.9 97.4 / 56. Bottle Cable Capsule Hazelnut Metal Nut Pill Screw Toothbrush Transistor Zipper Carpet Grid Leather Tile Wood Mean 01 02 03 Mean Candle Capsules Cashew 97.1 / 11.6 86.0 / 7.0 96.7 / 35.2 Chewinggum 99.0 / 37.1 94.2 / 22.3 2.5 95.3 / 92.2 / 0.2 98.0 / 41.1 4.6 91.6 / 5.8 93.2 / 93.4 / 8.2 98.1 / 47. Fryum Macaroni1 Macaroni2 Pcb1 Pcb2 Pcb3 Pcb4 Pipe Fryum 8.7 97.5 / 89.6 / 3.4 97.7 / 42.0 96.3 / 25.1 96.9 / 42.4 0.9 97.1 / 91.9 / 0.2 97.8 / 15.4 7.1 95.3 / 96.1 / 7.3 95.9 / 12.1 98.7 / 40.0 97.9 / 8.0 93.5 / 13.8 99.2 / 77.1 97.5 / 28.6 97.6 / 50.1 0.8 94.4 / 91.8 / 0.3 98.7 / 25.0 5.3 96.1 / 94.8 / 7.8 96.9 / 19.6 99.3 / 65.8 95.6 / 45.1 98.0 / 13.0 99.1 / 72.0 98.1 / 23.4 92.4 / 42.2 97.9 / 5.1 85.9 / 0.2 99.7 / 91.8 97.5 / 10.0 99.0 / 43.7 97.2 / 39.6 99.1 / 71.8 87.1 / 0.7 80.6 / 1.0 97.7 / 54.2 96.1 / 14.6 93.9 / 24.1 89.0 / 0.2 81.7 / 0.1 97.8 / 41.6 92.9 / 3.9 95.5 / 6.7 87.1 / 5.7 97.7 / 48.7 92.3 / 12.9 67.4 / 16.3 62.6 / 2.5 94.2 / 49.7 74.8 / 20.0 80.7 / 12.9 5.6 82.6 / 7.5 69.7 / 63.0 / 1.5 79.3 / 13.4 78.1 / 12.9 92.3 / 26. 97.7 / 9.4 94.6 / 44.9 99.4 / 65.9 97.0 / 19.5 93.5 / 47.3 1.5 95.4 / 83.8 / 0.2 99.1 / 85.6 94.8 / 12.6 98.2 / 12.6 94.5 / 28.0 95.3 / 69.6 99.1 / 20.4 97.9 / 47.4 99.0 / 50.1 99.1 / 57.5 97.7 / 48.2 99.1 / 7.6 97.6 / 3.1 99.3 / 57.4 97.6 / 7.7 98.1 / 15.0 97.6 / 34.0 99.2 / 55.1 99.2 / 33.7 98.4 / 55.6 99.2 / 74.6 99.1 / 61.1 97.7 / 49.5 99.2 / 21.3 97.9 / 7.6 99.6 / 70.0 98.1 / 11.0 98.2 / 17.7 98.1 / 41.6 99.5 / 76.5 Mean 95.6 / 18.6 95.9 / 17. 96.5 / 25.2 96.8 / 38.2 91.4 / 16.8 78.1 / 15.1 95.3 / 33.1 98.4 / 33. 98.7 / 43.3 Table 6: Image-level anomaly classification comparisons with ROC/PR on MVTec, BTAD and VisA. All methods are evaluated under the unified setting. The best and second-best results are highlighted in red and blue, respectively. Bottle Cable Capsule Hazelnut Metal Nut Pill Screw Toothbrush Transistor Zipper Carpet Grid Leather Tile Wood Mean 01 02 03 Mean Datasets Catergories CS-Flow [38] PaDiM [10] DFM [1] PatchCore [37] Embedding-based 100 / 100 40.2 / 61.0 84.2 / 96.5 96.0 / 97.5 95.8 / 99.1 40.6 / 82.4 65.3 / 81.7 75.0 / 89.9 63.2 / 60.2 91.7 / 97.5 93.6 / 98.2 77.2 / 90.5 100 / 100 98.0 / 99.2 99.5 / 99. 99.3 / 99.8 83.9 / 87.3 77.4 / 91.0 89.8 / 91.6 96.2 / 99.2 80.6 / 95.4 81.9 / 90.9 71.7 / 80.2 88.9 / 85.1 85.0 / 94.5 98.6 / 99.6 61.4 / 78.0 100 / 100 99.7 / 99.9 98.3 / 99.4 99.8 / 100 50.0 / 80.7 90.1 / 97.4 50.0 / 81.8 50.0 / 90.4 70.4 / 94.7 72.2 / 91.3 84.2 / 89.5 50.0 / 70.0 91.9 / 97.7 86.9 / 95.9 85.9 / 94.3 64.1 / 90.0 50.0 / 85.9 50.0 / 88.0 97.6 / 99.4 98.3 / 99.1 82.6 / 96.1 99.9 / 100 92.8 / 98.3 64.5 / 92.4 55.9 / 76.3 83.9 / 94.1 99.3 / 99.1 97.2 / 99.3 88.1 / 96.1 89.9 / 96.2 98.2 / 99.4 98.5 / 99.5 99.9 / 100 Discriminator-based Reconstruction-based CFA [20] DRAEM [54] SimpleNet [25] UniAD [52] OneNIP 100 / 100 87.1 / 95.3 97.5 / 99.3 99.0 / 99.4 76.9 / 86.0 68.5 / 80.6 91.1 / 97.8 70.6 / 92.3 78.7 / 94.6 100 / 100 86.9 / 93.1 95.7 / 97.8 99.8 / 100 84.1 / 95.9 80.6 / 95.1 96.9 / 99.5 63.4 / 91.3 68.8 / 93.4 91.4 / 96.6 52.7 / 76.6 47.1 / 73.1 93.3 / 97.3 85.3 / 94.2 75.0 / 90.7 99.8 / 99.7 73.0 / 72.6 79.5 / 75.9 99.0 / 99.7 74.7 / 92.7 79.9 / 94.0 99.9 / 100 88.1 / 96.7 83.2 / 95.0 99.0 / 99.7 45.1 / 68.1 56.2 / 77.4 100 / 100 95.8 / 98.6 99.9 / 100 100 / 100 91.9 / 97.3 98.4 / 99.5 98.8 / 99.6 98.2 / 99.4 97.5 / 99.1 99.6 / 99.9 63.3 / 73.0 80.9 / 95.1 98.2 / 98.7 90.5 / 97.8 81.0 / 96.2 96.3 / 98.7 89.7 / 96.1 83.0 / 78.0 99.0 / 99.7 97.6 / 99.2 99.5 / 99.8 96.0 / 98.7 98.7 / 99.5 97.6 / 99. 99.8 / 99.9 95.5 / 97.3 88.1 / 97.1 99.9 / 100 98.9 / 99.7 94.0 / 98.9 88.8 / 95.6 95.8 / 98.3 99.8 / 99.6 94.9 / 98.5 99.8 / 99.9 97.1 / 99.1 100 / 100 99.3 / 99.8 98.5 / 99.6 81.4 / 90.2 87.5 / 92.8 69.7 / 89.8 89.8 / 96.3 80.4 / 91. 91.4 / 95.3 78.2 / 90.0 96.5 / 98.9 97.9 / 99.3 95.1 / 98.0 81.0 / 96.7 99.4 / 94.3 99.8 / 99.9 87.9 / 98.1 99.4 / 94. 98.7 / 99.5 50.0 / 93.5 57.5 / 55.4 98.2 / 99.2 70.2 / 95.8 99.3 / 94.0 96.0 / 98.6 70.8 / 95.0 95.6 / 69.6 93.1 / 97.5 61.4 / 90.9 99.5 / 96.7 96.4 / 98.3 75.2 / 96.2 99.3 / 90.6 92.2 / 97.9 78.8 / 96.4 99.8 / 98. 98.8 / 99.6 79.0 / 96.5 100 / 99.5 91.8 / 96.3 95.7 / 97.4 68.8 / 82.8 89.2 / 96.4 87.5 / 87. 84.7 / 95.0 90.3 / 95.0 92.2 / 97.9 92.6 / 98.5 Candle Capsules Cashew 91.0 / 92.9 58.3 / 71.5 91.8 / 95.7 Chewinggum 98.9 / 99.5 88.0 / 94.7 67.0 / 65.6 26.4 / 38.1 91.2 / 90.1 70.8 / 73.4 54.0 / 61.8 89.6 / 87.5 82.1 / 89. Fryum Macaroni1 Macaroni2 Pcb1 Pcb2 Pcb3 Pcb4 Pipe Fryum 81.7 / 76.3 59.2 / 69.6 83.0 / 91.2 87.2 / 94.2 84.8 / 89.2 79.9 / 71.6 56.4 / 52.7 77.0 / 72.6 75.9 / 74.8 69.5 / 63.3 89.7 / 87.5 93.0 / 96.6 50.0 / 75.0 53.0 / 81.0 53.0 / 84.3 53.5 / 84.5 51.5 / 83.8 50.0 / 75.0 50.5 / 62.9 50.0 / 75.0 52.5 / 76.3 53.0 / 76.4 50.0 / 74.9 52.0 / 84.0 66.4 / 80.3 95.4 / 96.0 96.0 / 98.0 98.3 / 99.3 94.0 / 97.3 85.5 / 87.2 66.5 / 61.7 94.5 / 94.7 95.3 / 96.4 95.3 / 95.5 99.4 / 99.3 97.3 / 98.8 54.9 / 54.9 52.9 / 65.6 82.2 / 89.6 90.2 / 95.5 65.9 / 82.1 56.0 / 58.1 41.4 / 43.0 88.9 / 86.6 62.5 / 66.5 72.4 / 73.6 82.5 / 81.5 78.3 / 88.2 88.0 / 88.5 82.5 / 90.7 65.4 / 81.0 94.0 / 97.5 86.3 / 93.8 81.6 / 81.6 68.5 / 73.0 69.6 / 71.2 85.1 / 85.1 84.9 / 85.8 92.4 / 92.5 83.0 / 88. 92.3 / 93.4 76.2 / 85.3 94.1 / 97.0 97.1 / 98.8 88.0 / 94.1 84.7 / 87.3 75.0 / 77.0 93.4 / 94.4 90.0 / 91.7 91.3 / 93.4 99.1 / 99.2 89.0 / 94.9 96.6 / 97.0 73.8 / 85.5 93.6 / 96.8 99.0 / 99.5 88.4 / 94.5 89.3 / 90.0 82.1 / 82.3 94.3 / 93.6 91.9 / 93.0 85.2 / 86.2 99.3 / 99.2 96.4 / 98.2 96.8 / 97.1 79.0 / 89.0 93.7 / 96.7 99.3 / 99.7 86.9 / 93.8 91.9 / 91.6 84.1 / 86.5 95.8 / 94.8 94.1 / 94.6 91.9 / 92.6 99.5 / 99.5 97.3 / 98.5 Mean 75.8 / 80.0 78.1 / 78. 51.6 / 77.8 90.3 / 92.0 69.0 / 73.8 81.8 / 85.8 89.2 / 92.2 90.8 / 93. 92.5 / 94.5 ] 4 [ V ] 6 2 [ B ] 0 6 [ i ] 4 [ V ] 6 2 [ B ] 0 6 [ i V"
        }
    ],
    "affiliations": [
        "Tencent YouTu Lab"
    ]
}