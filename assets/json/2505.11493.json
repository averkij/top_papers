{
    "paper_title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing",
    "authors": [
        "Yusu Qian",
        "Jiasen Lu",
        "Tsu-Jui Fu",
        "Xinze Wang",
        "Chen Chen",
        "Yinfei Yang",
        "Wenze Hu",
        "Zhe Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Editing images using natural language instructions has become a natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on image-text similarity metrics like CLIP, which lack precision. In this work, we introduce a new benchmark designed to evaluate text-guided image editing models in a more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, and spatial object masks. We conduct a large-scale study comparing GPT-Image-1, the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting a key trade-off in the current model behavior. GIE-Bench provides a scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 3 9 4 1 1 . 5 0 5 2 : r GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing Yusu Qian, Jiasen Lu, Tsu-Jui Fu, Xinze Wang, Chen Chen, Yinfei Yang, Wenze Hu, Zhe Gan Apple Figure 1: Overview of the proposed GIE-Bench pipeline for grounded and fine-grained evaluation of text-guided image editing models. GIE-Bench consists of two components: (i) functional correctness evaluation; and (ii) content preservation evaluation."
        },
        {
            "title": "Abstract",
            "content": "Editing images using natural language instructions has become natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on imagetext similarity metrics like CLIP, which lack precision. In this work, we introduce new benchmark designed to evaluate text-guided image editing models in more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, 1Meta. Work done while at Apple. 2Senior authors. Preprint. and spatial object masks. We conduct large-scale study comparing GPT-Image1 [1], the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting key trade-off in the current model behavior. GIE-Bench provides scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing."
        },
        {
            "title": "Introduction",
            "content": "Text-guided image editing has become powerful and accessible way for users to modify images by simply describing the desired change in natural language, eliminating the need for complex tools or interfaces. Recent advances in diffusion models and multimodal large language models (MLLMs) have significantly improved the visual quality and instruction-following capability of these systems. Among them, GPT-Image-1 [1], recently released model from OpenAI, has gained significant attention for its impressive performance in image editing. However, fine-grained evaluation of its editing performance remains limited. Most prior works evaluate text-guided image editing performance using CLIP-based [2] similarity metrics or human preference scores. While useful, these approaches have key limitations. CLIP similarity offers only global alignment and does not reflect whether model correctly executed the intended semantic change. Human evaluations, on the other hand, are costly, non-reproducible, and difficult to scale across thousands of examples. To address this gap, we introduce GIE-Bench,1 new benchmark for evaluating text-guided image editing in grounded manner, with precise and interpretable metrics. Our evaluation framework is twofold: (i) functional correctness evaluation, and (ii) content preservation evaluation. First, we assess functional correctness using VQA-style protocol. For each edit, we formulate multiple-choice question grounded in the edited image, designed to verify whether the intended semantic change was applied. Unlike previous VQA-style evaluations that use binary yes/no questions (e.g., in I2E-Bench [3]), our multiple-choice format includes two to five answer options, increasing the difficulty and reducing the chance of success by guessing. This enables automatic, objective, and more grounded assessment of instruction execution. Second, we evaluate image content preservation, the models ability to leave irrelevant areas of the image unchanged. Unlike prior work that applies global similarity metrics, we introduce an object-aware preservation score. Our benchmark provides masks for edited objects and computes similarity over the remaining area that is intended to be left untouched. This localized approach yields more sensitive and precise measurement of unintended changes. To support robust evaluation, our benchmark covers over 800 diverse images across 20 categories (e.g., human faces, animals, architecture, text, cartoon, art, food, and electronics), with over 1000 editing tasks spanning 9 functional edit types. Each sample includes natural language instructions, edit type metadata, object masks, multiple-choice question for functional correctness evaluation, and human-annotated ground truth. Finally, we benchmark and compare wide array of state-of-the-art image editing models, including MGIE [4], OmniGen [5], and the most recent GPT-Image-1 [1]. Our results highlight GPT-Image-1s strong performance in functional correctness, while also revealing its tendency to over-edit irrelevant regions, as captured by our preservation metrics. Our contributions are summarized as follows. high-quality image editing benchmark covering diverse domains and instruction types; precise, fully automated evaluation protocol using VQA-style questions and object-aware masking to assess both functional correctness and preservation; standardized empirical comparison of leading image editing models, with detailed focus on GPT-Image-1s strengths and weaknesses. 1short for Grounded Image Editing Evaluation Benchmark. Benchmark data and evaluation code will be released soon. 2 Benchmark Instruction Eval Preserv. Eval Mask Usage in Eval Edit Types Covered I2E-Bench [3] IE-Bench [6] Edit-Bench [7] DiffEdit [8] (VQA) (Human-rated) (Retrieval) (CLIP/FID curves) (MOS) (LPIPS) Diverse Diverse Caption-driven Diverse GIE-Bench (Ours) (VQA) (SAM + DINO) Diverse Table 1: Comparison of text-guided image editing evaluation benchmarks. Our benchmark uniquely evaluates both functional correctness and content preservation with precise mask-based protocols."
        },
        {
            "title": "2 Related Work",
            "content": "Text-Guided Image Editing. Early methods such as Prompt2Prompt [9] and Imagic [10] introduced ways to edit images with text prompts by modifying either the attention layers or the latent space of diffusion models. DiffEdit [8] further refined image editing by detecting editable regions and generating masks. Text-guided image editing models have since emerged to simplify user interaction by directly following natural language instructions. InstructPix2Pix [11] fine-tuned diffusion models on synthetic image editing pairs guided by instructions. More recent models [5, 12, 4, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24] have advanced the semantic and compositional understanding of edits. InstructAny2Pix [17] uses multimodal inputs (e.g., audio, image) for editing guidance. SmartEdit [16] focuses on complex scenes and incorporates MLLMs for better instruction comprehension. MGIE [4] learns to derive expressive instructions from user input and provides explicit guided visual editing. Most recently, OneDiffusion [14] and UniVG [13] presents generalist diffusion models for unified image generation and editing. Benchmarks for Text-Guided Image Editing. To address the need for image editing evaluation, several benchmarks [25, 3, 6, 8] have been proposed. EditVal [25] assesses the fidelity of generated images across various edit types. It evaluates models based on their ability to perform specific attribute edits. I2E-Bench [3] emphasizes human perception alignment. IE-Bench [6] introduces database containing diverse source images, editing prompts, and results from different editing methods, along with Mean Opinion Scores from human subjects. These benchmarks have advanced image editing evaluation, but often focus on either high-level text-image alignment or human perceptual scores without integrating functional correctness and content preservation in unified framework. More related work on text-to-image generation benchmarks is provided in Appendix A.1. Evaluation of Content Preservation. Evaluating content preservation is crucial to ensure that edits are confined to specified regions without unintended alterations. Traditional preservation metrics that compute CLIP [2] similarity over the entire image are flawed because they conflate intended edits with unintended changes, failing to isolate regions that should remain untouched. Evaluation of Functional Correctness via VQA. Incorporating VQA into image editing evaluation offers promising avenue for assessing functional correctness. By formulating the evaluation as VQA task, models can be tested on their understanding of the edits in relation to the instructions. For example, TIFA [26] and Davidsonian Scene Graph [27] use VQA-based approaches to evaluate finegrained alignment between text and images. I2E-Bench [3] also adopts VQA-style evaluation by generating binary (yes/no) questions that query whether particular edit was made. While effective, such binary formulations may oversimplify the evaluation and are easier to answer by chance. In contrast, our benchmark formulates each evaluation as multiple-choice question with 25 carefully constructed options, requiring more nuanced reasoning and disambiguation. Our Contributions. As shown in Table 1, most image editing benchmarks do not precisely evaluate whether the background is preserved while assessing instruction correctness. The use of segmentation masks to precisely isolate editable and non-editable regions is largely absent from these prior works. GIE-Bench addresses these gaps by providing: (i) precise preservation evaluation by separating areas intended to edit from areas not intended to be edited by providing object masks, (ii) VQA-style multiple-choice questions to assess edits without requiring access to the original instruction. The dual-axis evaluation of GIE-Bench enables us to differentiate between models that make accurate edits but introduce unnecessary changes, and those that preserve image integrity but fail to satisfy the 3 Figure 2: Example image editing instructions and edited results by GPT-Image-1 [1], OmniGen [5], and MGIE [4]. instruction. Our use of automated, mask-aware metrics makes our benchmark uniquely suited for large-scale, fine-grained evaluation of instruction following behavior in image editing models."
        },
        {
            "title": "3 Benchmark Construction",
            "content": "3.1 Dataset Collection Source of Images. To ensure diversity in visual content and evaluate text-based image editing across wide range of domains, we hand-selected 2000 images from Pexels, repository of high-quality, royalty-free images. The images are of various resolution. They are evenly distributed across 20 distinct categories, with 100 images per category. These categories span broad spectrum of real-world and synthetic visual content. The selected categories are: (i) Objects & Environments: furniture, architecture, electronics, home appliance, city, nature, plant; (ii) People & Clothing: human, human face, cloth, accessories, jewellery; (iii) Animals & Creatures: animal, cartoon; (iv) Food & Lifestyle: food, musical instrument, art; (v) Text & Symbols: text, sign; (vi) Vehicles: transportation. This collection was curated to represent both structured and unstructured scenes, photographic and illustrated styles, and static and dynamic subjects. Generation of Editing Instructions. Each image in our dataset is randomly paired with 3 editing tasks, from the 9 editing types: Color Change, Add Object, Remove Object, Attribute Change, Object Replacement, Layout Modification, Scene/Background Change, Size Change, and Textual Edit (only for images containing text). This variety is designed to comprehensively evaluate an image editing models ability to handle both low-level appearance modifications and high-level semantic changes. We then prompt GPT-4o to generate edit instructions based on the image and edit type. We provide the detailed prompt in Appendix A.3. Figure 2 shows 4 examples of editing instructions and edited results. 3.2 Generation of Multiple-Choice Questions for Functional Correctness Evaluation To assess whether model has correctly executed given instruction, we introduce VQA-style evaluation framework focused on functional correctness. For each instruction-image pair, GPT-4o generates multiple-choice question that can only be answered correctly by inspecting the edited image, along with 2 to 5 answer choices including plausible distractors. The correct answer serves as ground-truth for automated evaluation. The quality of these questions is validated by testing them on original (unedited) images, yielding only 17% accuracy, confirming that successful answers require the correct edit. Prompt templates and examples are provided in Appendix A.3 and A.2. 4 3.3 Object Extraction and Mask Generation for Image Content Preservation Evaluation Traditional approaches to evaluating content preservation in text-based image editing often rely on computing similarity metrics across the entire edited image compared to the original. However, this strategy is fundamentally flawed. For example, if model makes no edits at all, these metrics would yield perfect similarity score, even though the editing instruction has been ignored. This makes it incorrect to assume that higher similarity scores necessarily indicate better performance. To address this, we propose more precise and targeted evaluation method that focuses on the regions of the image that are not supposed to be edited. Our key idea is to isolate the editable region using segmentation masks, and then invert the mask to define the rest of the image: the portion that should remain unchanged. By comparing these preserved regions between the original and edited images, we obtain much more faithful estimate of content preservation. Extracting and Segmenting Edit Targets. To identify the region to be edited, we first extract the target object to be edited from each instruction using GPT-4, prompting it to return concise phrases (e.g., the green bus, dogs tail). These phrases are then passed to Grounded SAM [28] to produce binary segmentation mask of the edit region. All masks are verified through human validation to ensure accuracy. This approach enables open-vocabulary grounding and supports precise, instruction-specific evaluation. The prompt used for GPT-4 is included in Appendix A.3. Object Detection. We use the object label predicted by GPT-4 (e.g., tree trunk) to perform open-vocabulary object detection using the GroundingDINO-Tiny model [28]. GroundingDINO outputs bounding boxes along with confidence scores and associated class names. When multiple detections are returned for the same label, we select the one with the highest confidence score. Figure 3: Distribution of object mask size ratios. Mask Generation. Given the bounding box returned by GroundingDINO, we apply the Segment Anything Model (SAM-ViT-Base) [29] to produce pixel-level segmentation mask for the object. This mask defines the editable region of the image and is stored as the object mask. We then compute and store the rest-of-image mask by inverting the object mask; this represents all parts of the image not intended for editing. The distribution of object mask size ratio is shown in Figure 3. Most object masks occupy less than 20% of the image area, indicating the prevalence of fine-grained edits. This combination of using (i) GPT-4 for understanding open-domain natural language, (ii) GroundingDINO for open-vocabulary detection, and (iii) SAM for high-fidelity segmentation, allows us to robustly localize editable regions even when instructions involve abstract, uncommon, or compositional object references. 3.4 Human Filtering for High-Quality Annotations To ensure high-quality evaluation data, we applied human filtering step to our initially generated benchmark. We initially sampled 3 edit instructions for each image, resulting in raw pool of 6,000 image-instruction pairs. However, we observed that some automatically generated object masks were inaccurate, and some edit instructions were ambiguous or invalid. To address these issues, we manually reviewed the entire dataset to only keep high-quality examples. The final curated benchmark consists of 1,080 high-quality image-instruction pairs with 856 unique images. We also made sure that these examples are carefully balanced across 9 edit types (120 edit instructions in each edit type) and 20 image categories. 3.5 Evaluation Metrics We report functional correctness and content preservation as separate metrics because combining them into single score would obscure the trade-off between instruction following and image consistency. Functional Correctness. To evaluate functional correctness, we adopt the VQA-style multiple-choice format described in Section 3.2. We use GPT-4o to answer each question, using only the edited image 5 Figure 4: Left: Functional correctness per edit type. Right: Model preservation rankings based on masked MSE (inverted), PSNR, and CLIP scores. We empirically observe that model ranking evaluated by masked CLIP differs from model ranking evaluated by CLIP. Model Add Object Attribute Change Color Change Layout Modif. Object Replace Remove Object StableDiffusion HQ-Edit OneDiffusion InsPix2Pix-CLIP InsPix2Pix InsDiffusion MGIE MagicBrush OmniGen GPT-Image-1* GPT-Image-1 4.17 4.17 49.17 35.83 38.33 40.00 60.00 70.83 78.33 92.98 94. 0.83 6.67 44.17 40.83 47.50 41.67 40.00 57.50 70.83 94.59 95.50 0.00 8.33 65.83 40.83 43.33 56.67 48.33 75.00 84.17 92.04 95.58 8.33 10.83 10.00 10.83 12.50 8.33 15.00 13.33 39.17 64.08 63.11 0.83 6.67 35.83 46.67 48.33 53.33 53.33 72.50 66.67 97.32 99.11 22.50 11.67 14.17 40.00 39.17 50.00 46.67 48.33 46.67 73.45 83.19 Scene / Bkgd Change 3.33 14.17 35.00 51.67 43.33 60.83 58.33 82.50 85.83 96.36 98.18 Size Change Textual Edit Overall 7.50 6.67 7.50 6.67 15.83 4.17 7.50 9.17 30.00 45.95 45.05 0.00 0.00 6.67 2.50 2.50 1.67 0.00 5.83 45.00 83.33 88. 5.28 7.69 29.81 30.65 32.31 35.19 36.57 48.33 60.74 82.42 85.00 Table 2: Functional correctness (multiple-choice accuracy) per edit type evaluated by GPT-4o. Values are raw percentages. * denotes judgment by Gemini-2-Flash [32] as the second judge. Full Gemini-2-Flash results are in Appendix A.4. unless the question asks for comparison between the original and edited images, and the question itself. We report accuracy of each edit type and overall accuracy across all benchmark entries. Image Content Preservation. In addition to evaluating instruction fidelity, our benchmark assesses how well models preserve unedited content. To this end, we introduce calibrated metric based on masked, aligned mean squared error (MSE) difference. Specifically, we compute the mean squared error between the original and edited images over unmasked regions, that is, regions that should remain unchanged, after applying geometric alignment. To mitigate the impact of spatial shifts introduced during generation, we first align the edited image to the original using SIFT keypoints [30], FLANN-based matching [31], and affine transformation. This yields an partially aligned edited image to the original image I. We then invert and min-max normalize the MSE values across all benchmark entries to produce calibrated preservation score in the range [0, 1], where higher values indicate better preservation. In addition to masked MSE, we also compute masked SSIM, masked CLIP, and masked PSNR to evaluate both semantic and pixel-level preservation. Together, they provide more comprehensive view of unintended changes introduced during editing and correlate strongly with human-annotated preservation scores (see Section 4.3). 6 Figure 5: Examples showing three failure modes: functional failure, preservation failure, and combined failure, paired with correct editing. Model SSIM CLIP Unmasked CLIP PSNR MSE OneDiffusion MagicBrush GPT-Image-1 InsPix2Pix-CLIP InsPix2Pix MGIE InsDiffusion HQ-Edit OmniGen StableDiffusion 0.9585 0.8703 0.5704 0.7557 0.7537 0.7561 0.7239 0.4779 0.4569 0.6690 0.9805 0.9489 0.9485 0.9375 0.9364 0.9189 0.9099 0.8949 0.8943 0.8790 0.9434 0.8399 0.8536 0.7845 0.7859 0.7843 0.7812 0.7014 0.7421 0.6898 26.24 20.48 13.36 16.91 16.78 17.75 16.51 9.89 12.71 14.33 521.57 1529.45 3952.34 2498.30 2582.85 2917.03 3799.92 8035.06 4752.33 3219.74 Table 3: Preservation scores across five automatic metrics. All scores are computed over regions outside the object mask to evaluate unintended changes, except Unmasked CLIP which is widely used in previous work on image editing to evaluate preservation."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Main Results Functional Correctness. Figure 4 (left) reports functional correctness evaluation results across various edit types, evaluated via multiple-choice QA with GPT-4o. Detailed numbers are reported in Table 2. Interestingly, GPT-image-1 maintains consistent high accuracy across all categories, while other models tend to vary more significantly depending on edit type, indicating disparity in generalization capabilities. These findings highlight the importance of model architecture and instruction-following ability in achieving faithful functional edits. In Figure 5, we show examples of both correct and incorrect edits, some failing in executing the instruction, and others failing to preserve areas not meant to be modified, emphasizing why dual-axis evaluation is necessary. Image Content Preservation. Figure 4 (right) presents normalized preservation scores across models using both CLIP-based and pixel-based metrics. Detailed results are reported in Table 3. We observe that OneDiffusion and MagicBrush consistently achieve the highest preservation scores across all metrics. Interestingly, some models (e.g., GPT-Image-1) score better on masked CLIP than pixel-based metrics, implying they preserve semantic structure better than low-level appearance. Depending on whether semantic preservation or exact pixel-level preservation is more important for the application, users can choose to evaluate preservation using CLIP-based or pixel-based metrics, respectively. Figure 6: Examples of GPT-Image-1s failure modes. 4.2 How Well Does GPT-Image-1 Edit Images? GPT-Image-1 demonstrates strong overall performance across wide range of text-guided editing instructions. It excels particularly in appearance-level edits, such as changing colors, modifying textures, adding or replacing objects, and editing text. For example, the model accurately changes the color of clothing items and signage, introduces new objects like potted plants or balloons with realistic rendering, and performs text edits with high spatial and semantic fidelity. These strengths highlight its powerful visual grounding and instruction-following ability. However, GPT-Image-1 shows noticeable weaknesses in spatial and layout-related edits. For layout modifications (e.g., move the table to the left side or move the person to the right), the model often interprets the direction loosely or inconsistently (example in Figure 6 top left). Similarly, in sizechange instructions (e.g., make the vase twice as large as the lamp), the modified objects sometimes appear disproportionate or fail to satisfy the relative sizing constraints (example in Figure 6 top right). These issues suggest that GPT-Image-1 struggles with precise spatial reasoning and proportional scaling. In object removal tasks, GPT-Image-1 occasionally leaves residual artifacts or incompletely erases the object (example in Figure 6 bottom left), especially when the background is complex. Another common failure is imprecision in preserving non-targeted regions. GPT-Image-1 frequently introduces unintended changes in areas unrelated to the instruction (example in Figure 6 bottom right), such as altering background textures, shifting lighting, or distorting peripheral objects. In contrast, models like MGIE tend to preserve these untouched regions more faithfully, leading to higher preservation scores and more stable image content. Overall, while GPT-Image-1 is highly capable in executing core edits, it lacks fine-grained control over spatial relationships and content preservation, leaving room for improvement for tasks that demand high precision or minimal collateral changes. 4.3 Human Study One of the central goals of our benchmark is to faithfully reflect human preferences when evaluating textguided image editing. To assess this alignment, we conduct human ranking study over representative subset of 100 editing examples. Each example consists of an original image, an edit instruction, and the corresponding outputs generated by nine models2: StableDiffusion, HQ-Edit, OneDiffusion, InsPix2PixCLIP, InsPix2Pix, InsDiffusion, MGIE, MagicBrush, and OmniGen. Each image-edit pair is independently evaluated by four human annotators, who are inMetric Spearman Masked SSIM Masked CLIP Masked PSNR Masked Inverted MSE 0.881 0.952 0.905 0.833 Table 4: Spearman correlation between human ratings on content preservation and automatic metrics. 2GPT-Image-1 API was not released at the time thus not included. 8 Figure 7: Left: Correlation between human and GPT-4o evaluated functional correctness scores. Right: Correlation between human-evaluated preservation and inverted calibrated MSE rank. structed to provide two scores: one for instruction adherence and another for preservation of regions that are not supposed to be edited. Both scores used 4-point scale, where 0 denotes complete failure, 1 indicates weak rejection, 2 represents weak acceptance, and 3 signals complete success. We then compute the average rank for each model across all 100 examples and annotators. Figure 7 (left) visualizes the relationship between GPT-4o evaluated functional correctness scores and human-annotated correctness, showing clear positive trend. Table 4 reports the Spearman correlation coefficients between masked SSIM, masked CLIP, masked PSNR, and masked inverted MSE scores and human rankings (inverted so that higher values indicate better human preference alignment). Figure 7 (right) shows strong correspondence between human preservation scores and inverted MSE rank, with models like OneDiffusion and MagicBrush scoring highly on both. Together, these two metrics provide complementary views of model quality. Functional correctness captures whether the desired edit was made, while preservation evaluates whether it was done in controlled and localized way. The strong alignment of both metrics with human preferences supports their joint use for comprehensive evaluation in text-guided image editing."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce benchmark for evaluating text-based image editing models along two critical axes: functional correctness and content preservation. Our evaluation combines VQA-style multiplechoice questions and object-aware preservation score to ensure edits are both instruction-faithful and minimally invasive. With over 1000 annotated examples across 20 diverse domains, the benchmark offers scalable and interpretable framework. We also present comparative analysis of leading editing models, revealing key strengths and weaknesses. We hope our benchmark drives progress toward developing text-based image editing systems that are not only visually compelling but also faithful to instructions and minimally invasive to unedited regions."
        },
        {
            "title": "Limitation",
            "content": "Our benchmark focuses on evaluating single-turn text-guided image edits where instructions are relatively atomic and localized. While this setting captures wide range of edit types, it does not yet address more complex scenarios such as multi-step editing, interactive refinement, or longform compositional instructions. In real-world applications, users often use sequential or iterative commands that require maintaining visual and semantic coherence over time. These scenarios pose additional challenges for instruction interpretation, memory, and image consistency, demanding more advanced model capabilities and evaluation methods. In future work, we plan to extend text-guided image-editing evaluation to these more dynamic settings."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Gpt image 1: State-of-the-art image generation model. https://platform.openai. com/docs/models/gpt-image-1, 2025. [2] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [3] Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, Rongrong Ji, et al. I2ebench: comprehensive benchmark for instruction-based image editing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [4] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding Instruction-based Image Editing via Multimodal Large Language Models. In International Conference on Learning Representations (ICLR), 2024. [5] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation, 2024. [6] Shangkun Sun, Bowen Qu, Xiaoyu Liang, Songlin Fan, and Wei Gao. Ie-bench: Advancing the measurement of text-driven image editing for human perception alignment, 2025. [7] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In CVPR, 2023. [8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusionbased semantic image editing with mask guidance, 2022. [9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022. [10] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, In 2023 and Michal Irani. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 60076017. IEEE, June 2023. Imagic: Text-based real image editing with diffusion models. [11] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 1839218402. IEEE, June 2023. [12] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, and Baining Guo. Instructdiffusion: generalist modeling interface for vision tasks, 2023. [13] Tsu-Jui Fu, Yusu Qian, Chen Chen, Wenze Hu, Zhe Gan, and Yinfei Yang. Univg: generalist diffusion model for unified image generation and editing, 2025. [14] Duong H. Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all, 2024. [15] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. In Advances in Neural Information Processing Systems, 2023. [16] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, and Ying Shan. Smartedit: Exploring complex instruction-based image editing with multimodal large language models, 2023. [17] Shufan Li, Harkanwar Singh, and Aditya Grover. Instructany2pix: Flexible visual editing via multimodal instruction following, 2024. 10 [18] Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu Jiang, and Ming-Hsuan Yang. Text-driven image editing via learnable regions, 2024. [19] Zichong Meng, Changdi Yang, Jun Liu, Hao Tang, Pu Zhao, and Yanzhi Wang. Instructgie: Towards generalizable image editing, 2024. [20] Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, and Jingfeng Zhang. Stylebooth: Image style editing with multimodal instruction, 2024. [21] Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xuhui Liu, Jiaming Liu, Li Lin, Xu Tang, Yao Hu, Jianzhuang Liu, and Baochang Zhang. Zone: Zero-shot instruction-guided local editing, 2024. [22] Qin Guo and Tianwei Lin. Focus on your instruction: Fine-grained and multi-instruction image editing by attention modulation, 2023. [23] Rodrigo Santos, João Silva, and António Branco. Leveraging llms for on-the-fly instruction guided image editing, 2024. [24] Ruoyu Zhao, Qingnan Fan, Fei Kou, Shuai Qin, Hong Gu, Wei Wu, Pengcheng Xu, Mingrui Zhu, Nannan Wang, and Xinbo Gao. Instructbrush: Learning attention-based instruction optimization for image editing, 2024. [25] Samyadeep Basu, Mehrdad Saberi, Shweta Bhardwaj, Atoosa Malemir Chegini, Daniela Massiceti, Maziar Sanjabi, Shell Xu Hu, and Soheil Feizi. Editval: Benchmarking diffusion based text-guided image editing methods, 2023. [26] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering, 2023. [27] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation, 2024. [28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. [30] David Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60:91110, 2004. [31] Marius Muja and David Lowe. Fast approximate nearest neighbors with automatic algorithm configuration. VISAPP (1), 2(331-340):2, 2009. [32] Google. Introducing gemini 2.0: our new ai model for the agentic era. https://blog.google/ technology/google-deepmind/google-gemini-ai-update-december-2024/, 2024. [33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. [34] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation, 2022. [35] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. [36] Shuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Chunle Guo, and Chongyi Li. Evalmuse-40k: reliable and fine-grained benchmark with comprehensive human annotations for text-to-image generation model evaluation, 2024. [37] Chunyi Li, Zicheng Zhang, Haoning Wu, Wei Sun, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, and Weisi Lin. Agiqa-3k: An open database for ai-generated image quality assessment, 2023. [38] Jiarui Wang, Huiyu Duan, Yu Zhao, Juntong Wang, Guangtao Zhai, and Xiongkuo Min. Lmm4lmm: Benchmarking and evaluating large-multimodal image generation with lmms, 2025. [39] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Related Work on Benchmarks for Text-to-Image Generation Several benchmarks [33, 34, 16, 35, 36, 37, 38, 26, 27, 39] have been proposed to evaluate text-toimage generation models from different aspects. DrawBench [33] and PartiPrompt [34] assess basic compositional abilities like attributes and spatial relationships. T2I-CompBench [16] introduces more complex reasoning skills such as comparison and logic. Pick-a-Pic [35] and EvalMuse-40K [36] collect large-scale human preferences for ranking outputs. Meanwhile, AGIQA-3K [37] and EvalMi50K [38] incorporate detailed perceptual and correspondence annotations, providing fine-grained and scalable evaluation of both image quality and instruction adherence. GenEval [39] offers an objectcentric evaluation framework that assesses compositional properties. TIFA [26] and Davidsonian Scene Graph [27] adopt VQA-based methods to assess fine-grained text-image alignment. A.2 Examples for Functional Correctness Evaluation Table 5 provides representative examples of the nine edit types included in GIE-Bench, covering both low-level appearance changes and high-level semantic modifications. For each edit type, we show the input image content, the natural language instruction provided to the model, the corresponding multiple-choice question used for functional correctness assessment, and the expected answer. Edit Type Color Change Add Object Image Content Example Instruction Evaluation Question and Options Expected Answer black coffee mug on wooden table child standing in grassy field Add yellow kite in the sky Change the color of the mug to red. above the child. What color is the mug? [Black, Green, Red, Blue] What is flying in the sky above the child? [A red balloon, Nothing, yellow kite, drone] Red yellow kite Remove Object dog and ball on lawn Remove the ball from the lawn. What objects are visible on the lawn? [A dog Scene Change Background Change Attribute Change Object Replacement Layout Modification Size Change Textual Edit tree with green leaves in sunny park man standing in front of beach woman with neutral expression table with an apple on it Change the season to winter. Change the background to snowy mountain. Make the woman smile. ball, Nothing, dog, cat] What season is shown in the image? [Spring, Autumn, Winter, Summer] What is in the background? [Beach, Forest, Snowy mountain, City street] Is the woman smiling? [No, Not clear, Yes, Maybe] Winter Snowy mountain Yes Replace the apple with banana. What fruit is on the table? [Apple, Banana dining table with vase in the center Move the vase to the left side of the table. single red apple on white plate Make the apple significantly larger to 2 times the size of the plate. notebook with poem written on it Change the first line of the poem to In love we believe Pear, Banana, Orange] Where is the vase located on the table? [Right side, On the floor, Left side, Center] How big is the apple relative to the plate? [Smaller than the plate, Slightly larger than the plate, Three times larger than the plate, Twice the size] What is the first line of the poem? [In love we believe, In peace we believe, To be or not to be , Not sure] Left side Twice the size In love we believe Table 5: Examples of edit instructions, evaluation questions and options, and expected answer of 9 edit types (Scene Change and Background Change are in the same category). A.3 Prompt Templates to Generate Benchmark The prompts we used to generate editing instructions using GPT-4o are shown in Figure 8. The prompt we used to summarize object to edit based on the editing instruction is shown in Figure 9. A.4 Robustness of GPT-4o-Based Evaluation To evaluate the stability of using GPT-4o for automatic functional correctness assessment in our VQA-style setup, we conducted three independent runs of the evaluation process. For each run, GPT-4o was provided with the same sets of multiple-choice questions and corresponding edited images for each model. We set the decoding temperature to 0 in all runs to ensure deterministic outputs. The results show strong consistency across runs, with negligible fluctuations (typically under 0.3 percentage points). The per-model scores are stable. This confirms that GPT-4o, when used deterministically, provides robust evaluation, which strengthens the benchmarks reproducibility and fairness in model comparison. 13 Figure 8: Prompts used to generate editing instructions, multiple-choice questions, and true answers. Figure 9: Prompt used to summarize object to edit in the editing instructions. Model GPT-4o Run 1 GPT-4o Run 2 GPT-4o Run 3 StableDiffusion HQ-Edit InsDiffusion OmniGen 5.19 7.78 35.46 61.02 5.46 8.06 35.37 61.48 5.28 7.69 35.19 60.74 Table 6: Functional correctness accuracy (%) across three evaluation runs using GPT-4o with temperature set to 0. To assess the robustness of our functional correctness evaluation, we employed Gemini-2-Flash [32] as secondary judge alongside GPT-4o. For each edited image and its associated multiple-choice question, we recorded the answer selected by Gemini-2-Flash using the same VQA-style evaluation protocol. As shown in Table 3, the scores from Gemini closely track those of GPT-4o, confirming the reliability of our automatic evaluation framework across different models. Model Add Object Attribute Change Color Change Layout Modif. Object Replace Remove Object StableDiffusion HQ-Edit OneDiffusion InsPix2Pix-CLIP InsPix2Pix MGIE InsDiffusion MagicBrush OmniGen GPT-Image-1 6.67% 11.67% 44.17% 38.33% 42.50% 49.17% 48.33% 67.50% 78.33% 92.98% 5.83% 10.00% 43.33% 42.50% 47.50% 36.67% 45.00% 57.50% 75.00% 94.59% 4.17% 16.67% 64.17% 39.17% 44.17% 50.00% 67.50% 76.67% 84.17% 92.04% 13.33% 11.67% 10.83% 15.83% 19.17% 19.17% 15.83% 15.83% 49.17% 65.05% 5.00% 10.00% 35.83% 48.33% 55.83% 50.83% 59.17% 70.00% 70.83% 97.32% 21.67% 15.00% 11.67% 30.00% 30.83% 40.00% 44.17% 39.17% 38.33% 73.45% Scene / Bkgd Change 7.50% 17.50% 37.50% 47.50% 45.00% 61.67% 67.50% 77.50% 80.83% 96.36% Size Change Textual Edit Overall 15.83% 17.50% 22.50% 20.83% 17.50% 22.50% 13.33% 19.17% 35.00% 46.85% 3.33% 3.33% 9.17% 7.50% 8.33% 2.50% 2.50% 7.50% 50.83% 84.17% 9.26% 12.59% 31.02% 32.22% 34.54% 36.94% 40.37% 47.87% 62.50% 82.72% Table 7: Functional correctness (multiple-choice accuracy) per edit type as judged by Gemini-2-flash. A.5 Human Study Figure 10: The annotation guidance we provided to human annotators on how to assign scores. To validate the alignment of our automatic evaluation metrics with human perception, we conducted human study over randomly chosen subset of 100 edited examples from GIE-Bench. Annotation Setup. For each example, participants were presented with: (i) one original image (top row), (ii) set of nine edited images, each generated by different image editing model (arranged in rows below), (iii) the editing instruction (e.g., Remove the photo of the circular window.), and the edit type (e.g., Remove Object) displayed at the bottom. The annotation guidance we provided on how to assign scores is shown in Figure 10. The human annotation task in this study involved scoring visual outputs of image editing models and did not collect any personal or sensitive information from participants. Hourly wage is 25 USD."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}