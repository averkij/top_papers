{
    "paper_title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers",
    "authors": [
        "Xingjian Leng",
        "Jaskirat Singh",
        "Yunzhong Hou",
        "Zhenchang Xing",
        "Saining Xie",
        "Liang Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper we tackle a fundamental question: \"Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?\" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https://end2end-diffusion.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 8 4 0 1 . 4 0 5 2 : r REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers Xingjian Lengα Jaskirat Singhα Yunzhong Houα Zhenchang Xingβ Saining Xieχ Liang Zhengα αAustralian National University βData61 CSIRO χNew York University Figure 1. Can we unlock VAE for end-to-end tuning with latent-diffusion models? Traditional deep learning wisdom dictates that end-to-end training is often preferable when possible. However, latent diffusion models usually only update the generator network while keeping the variational auto-encoder (VAE) fixed (a). This is because directly using the diffusion loss to update the VAE (b) causes the latent space to collapse. We show that while direct diffusion-loss is ineffective, end-to-end training can be unlocked through the representationalignment (REPA) loss allowing both encoder and diffusion model to be jointly tuned during the training process (c). Notably, this allows for significantly accelerated training; speeding up training by over 17 and 45 over REPA and vanilla training recipes, respectively (d)."
        },
        {
            "title": "Abstract",
            "content": "In this paper we tackle fundamental question: Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner? Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-toend training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17 and 45 over REPA and Project Leads vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure In terms of fiand downstream generation performance. nal performance, our approach sets new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifierfree guidance on ImageNet 256 256. Code is available at https://end2end-diffusion.github.io. 1. Introduction End-to-end training has propelled the field forward for the past decade. It is understood that incorporating more components into end-to-end training can lead to increased performance, as evidenced by the evolution of the RCNN family [14, 15, 38]. With that said, training schemes of latent diffusion models (LDMs) [39] remain two-stage: first, the variational auto-encoder (VAE) [23] is trained with the re- (a) PCA Analysis on VAE Latent Space Structure (b) Performance Improvements with REPA-E (400K Steps) Figure 2. End-to-End Training Automatically Improves VAE Latent-Space Structure. (a) Following [25], we visualize latent space structure from different VAEs before and after end-to-end training using principal component analysis (PCA) that projects them to three channels colored by RGB. We consider SD-VAE [39], and IN-VAE1, 16 downsampling, 32-channel VAE trained on ImageNet [6]. For SD-VAE we find that latent representations have high-frequency noise. Applying end-to-end tuning helps learning more smooth and less noisy latent representation. Interestingly to the contrast, the latent space for IN-VAE is over-smoothed (e.g., row-2). Applying end-to-end tuning automatically helps learn more detailed latent space structure to best support final generation performance. (b) Jointly tuning both VAE and latent diffusion model (LDM) significantly improves final generation performance (gFID) across different VAE architectures. construction loss; then, the diffusion model is trained with the diffusion loss while keeping the VAE fixed (see Fig. 1a). The above two-stage division of the LDM training process, though popular, leads to challenging optimization task: How to best optimize the representation from first stage (VAE) for optimal performance while training the second stage (diffusion model)? While recent works study the interplay between the performance of the two stages [25, 43], they are often limited to empirical analysis, which may vary depending on the architecture and training setting for both the VAE and the diffusion model. For instance, in concurrent work [43] show that the latent space of popular autoencoders e.g., SD-VAE [39] suffer from high-frequency noise / components. However, as seen in Fig. 2 & 6, while the same holds for some VAEs (e.g. SD-VAE), it might not be true for other VAE architectures which instead might suffer from an over-smoothed latent space (Fig. 2, 6). In this paper, we therefore ask fundamental question: Can we jointly tune both VAE and LDM in an end-to-end manner to best optimize final generation performance? Technically, it is straightforward to do end-to-end LDM training by simply back-propagating the diffusion loss to the VAE tokenizer. However, experiments (3) reveal that this naive approach for end-to-end training is ineffective. The diffusion loss encourages learning simpler latent space structure which is easier for denoising objective (refer 3.1), but leads to reduced generation performance (Fig. 1d). To address this, we propose REPA-E; an end-to-end training recipe using representation alignment loss [53]. We show that while the diffusion loss is ineffective, end-toend tuning can be unlocked through the recently proposed representation-alignment (REPA) loss - allowing both VAE and diffusion model to be jointly tuned during training process. Through extensive evaluations, we demonstrate that end-to-end tuning with REPA-E offers several advantages; End-to-End Training Leads to Accelerated Generation Performance; speeding up diffusion training by over 17 and 45 over REPA and vanilla training recipes (Fig. 1d). Furthermore, it also helps significantly improve the final generation performance. For instance as seen in Fig. 1d, we find that when using the popular SiT-XL [30] architecture, REPA-E reaches an FID of 4.07 within 400K steps, significantly boosting final performance over even REPA which only only reaches final FID for 5.9 after 4M steps [53]. End-to-End Training improves VAE latent-space structure. As seen in Fig. 2 and 4.4, we find that jointly tuning the VAE and latent diffusion model during training , automatically improves the latent space structure across different VAE architectures. For instance, for SD-VAE [39], it is observed that the original latent space suffers from highfrequency noise  (Fig. 2)  . Applying end-to-end tuning helps learn more smooth latent space representation. In contrast, the latent space for IN-VAE1 is over-smoothed. Applying REPA-E automatically helps learn more detailed latent space structure to best support generation performance. End-to-End Tuning Improves VAE Performance. Finally, we find that once tuned using REPA-E, the endto-end tuned VAE can be used as drop-in replacement for their original counterparts (e.g. SD-VAE) showing im1trained on imagenet at f16d32 using official training code from [39]. 2 proved generation performance across diverse training settings and model architectures (refer 4.4). To summarize, key contributions of this paper are: 1) We propose REPA-E; an end-to-end training recipe for jointly tuning both VAE and LDM using representation alignment loss (3). 2) We find that despite its simplicity, REPA-E leads to accelerated generation performance; speeding up diffusion training by over 17 and 45 over REPA and vanilla training recipes, respectively (4.2). 3) We show that end-to-end training is able to adaptively improve the latent space structure across diverse VAE architectures. 4) We demonstrate that once tuned using REPA-E, the end-to-end tuned VAE can be used as drop-in replacement for their original counterparts (e.g., SD-VAE), exhibiting significantly better downstream generation performance (4.4). 2. Related Work Tokenizers for image generation are typically formulated as autoencoders (AE) [3], which are trained to encode images into latent representation and decode back to pixels. Usually, those autoencoders use either the variational objective [23] for continuous tokenization or vector quantization objective [9, 47] for discrete tokenization. Modern tokenizers are trained not only to minimize reconstruction errors but also potentially for perceptual similarity and adversarial feedback from discriminators [9]. Most tokenizers are built on convolutional neural network (CNN) architectures [9, 10, 23, 36, 39, 47], such as ResNet [16]. Recent research [22, 52, 54] explores vision transformer (ViT) architectures [8] for more flexible control over the latent space compression rate by limiting the number of latent tokens. Diffusion models for image generation learn transformation from the noise distribution to the real data distribution. Early approaches, such as DDPM [19] and DDIM [44] operate directly in image pixel space. Latent diffusion models (LDM) leverage pre-trained image tokenizers to compress images into lower-dimensional latent space, in which diffusion models are trained [10, 36, 39]. Earlier methods [36, 39] typically use the U-Net [40] architecture for noise prediction. Recent ones explore transformer-based architectures [48] due to their superior scalability [5, 10, 11, 27, 32]. Despite their effectiveness, existing tokenizers and diffusion models are trained separately [10, 36, 39]. This confines diffusion model training to fixed latent space. In this paper, we explore the potential of jointly optimizing tokenizers and diffusion models to achieve faster convergence and improved image generation performance. Representation alignment for generative models. Recent research explore the role of visual representations in improving diffusion model training. Pernias et al. [35] split text-to-image generation into two diffusion models. One generates compressed semantic maps from text prompts, which serve as the condition for image generation of the second model. More recently, REPA [53] and VAVAE [49] explicitly align model features with visual representations from vision foundation models to bootstrap diffusion training. In particular, REPA [53] regularizes diffusion model training by aligning early-layer features of DiT [34]/SiT [30] models with clean image features from pre-trained vision encoders, such as DINOv2 [33] and CLIP [37]. Differently, VA-VAE [49] aligns latent space of the tokenizer (VAE) with pre-trained foundation models and then freezes it to train diffusion model. This paper builds on these existing findings and reports that the REPA loss benefits end-to-end training of diffusion models by providing useful and stable optimization target. Additionally, the VAE trained from REPA-E can be frozen and then used to train diffusion models. 3. REPA-E: Unlocking VAE for Joint Training Overview. Given variational autoencoder (VAE) and latent diffusion transformer (e.g., SiT [30]), we wish to jointly tune the VAE latent representation and diffusion model features in an end-to-end manner to best optimize the final generation performance. To this end, we first make three key insights in 3.1: 1) Naive end-to-end tuning - directly back-propagating the diffusion loss to the VAE is ineffective. The diffusion loss encourages learning more simpler latent space structure (Fig. 3a) which is easier for minimizing the denoising objective [39], but degrades the final generation performance. We next analyze the recently proposed representation-alignment loss [53] showing that; 2) Higher representation-alignment score [53] correlates with improved generation performance (Fig. 3b). This offers an alternate path for improving final generation performance using representation-alignment score as proxy. 3) The maximum achievable alignment score with vanilla-REPA is bottlenecked by the VAE latent space features. We further show that backpropagating the REPA loss to the VAE during training can help address this limitation, significantly improving final representation-alignment score (Fig. 3c). Given the above insights, we finally propose REPA-E (3.2); an end-to-end tuning recipe for both VAE and LDM features. Our key idea is simple: instead of directly using diffusion loss for end-to-end tuning, we can use the representation alignment score as proxy for the final generation performance. This motivates our final approach, where instead of the diffusion loss, we propose to perform end-toend training using the representation-alignment loss. The end-to-end training with REPA loss helps better improve the final representation-alignment score (Fig. 3b), which in turn leads to improved final generation performance (3.1). 3.1. Motivating End-to-End Training with REPA Naive End-to-End Tuning is Ineffective. We first analyze the naive approach for end-to-end tuning; directly 3 (a) PCA Visualization of Latent Spaces (b) Correlation: gFID & CKNNA Score (c) E2E tuning with REPA improves CKNNA Score Figure 3. Motivating End-to-End Tuning using Representation Alignment (REPA) Loss. We make three key insights: 1) Naive endto-end (E2E) tuning using diffusion loss is ineffective. The diffusion encourages learning more simpler latent space structure (a) which is easier for denoising objective (refer 3.1) but degrades final generation performance  (Fig. 1)  . We next analyze the recently proposed representation alignment (REPA) loss [53] showing: 2) Higher representation alignment (CKNNA) leads to better generation performance. This suggests an alternate path for improving performance by using representation-alignment (CKNNA) as proxy for generation performance. 3) The maximum achievable CKNNA score with vanilla-REPA is bottlenecked by the VAE features (c) saturating around 0.42. Back-propagating the REPA-loss to the VAE helps address this limitation and improve the final CKNNA score. Given the above insights: we propose REPA-E (3.2) for end-to-end LDM training. The key idea is simple: instead of using the diffusion loss, we perform end-to-end training using the REPA loss. The end-to-end training with REPA loss helps improve the final representation-alignment (CKNNA), which in turn leads to improved generation performance (4). Training Strategy Spatial Variance Total Variation w/o E2E Tuning E2E w/ REPA Loss E2E w/ Diff. Loss 17.06 18.02 0. 6627.35 5516.14 89.80 Table 1. Impact of Naive End-to-End Training with Diffusion Loss. We report total variation [41] and mean variance along each VAE latent channel for three training settings: 1) Standard LDM training (w/o end-to-end (E2E) tuning), 2) Naive E2E tuning with Diffusion loss, 3) E2E tuning with REPA loss [53]. All experiments use SDVAE for VAE initialization. We observe that using diffusion loss for end-to-end tuning encourages learning simpler latent space with lower variance along the spatial dimensions (Fig. 3a). The simpler latent space is easier for denoising objective (3.1), but degrages final generation performance  (Fig. 1)  . All results are reported at 400K iterations with SiT-XL/2 [30] as LDM. backpropagating the diffusion loss to the VAE tokenizer. As shown in Fig. 3a, we observe that directly backpropagating the diffusion loss encourages learning more simpler latent space structure with lower variance along the spatial dimensions (Tab. 1). The simpler latent-space structure poses an easier problem for the denoising objective [39], but leads to reduced generation performance  (Fig. 1)  . Consider an intermediate latent zt = αtzVAE + σtϵorig for any timestep t. The denoising objective [34] mainly aims to predict ϵpred; estimating the originally added noise ϵorig from VAE features zVAE and timestep t. As the variance along the spatial dimensions for VAE latent zVAE goes down, the denoising objective effectively reduces to predicting bias term for recovering back the originally added noise ϵorig. Thus, backpropagation the diffusion loss effectively hacks the latent space structure to create an easier denoising problem, but leads to reduced generation performance  (Fig. 1)  . Higher Representation Alignment Correlates with Better Generation Performance. Similar to the findings of [53], we also measure representation alignment using CKNNA scores [20] across different model sizes and training iterations. As seen in Fig. 3b, we observe that higher representation alignment during the training process leads to improved generation performance. This suggests an alternate path for improving generation performance by using the representation alignment objective instead of the diffusion loss for end-to-end training (refer 3.2). Representation Alignment is Bottlenecked by the VAE Features. Fig. 3c shows that while the naive application of REPA loss [53] leads to improved representationalignment (CKNNA) score, the maximum achievable alignment score is still bottlenecked the VAE features saturating around value of 0.4 (maximum value of 1). Furthermore, we find that backpropagating the representation-alignment loss to the VAE helps address this limitation; allowing endto-end optimization of the VAE features to best support representation-alignment objective [53]. 3.2. End-to-End Training with REPA Given the above insights, we next propose REPA-E (3.2); an end-to-end tuning recipe for jointly training both VAE 4 Instead of directly using diffusion and LDM features. loss, we propose to perform end-to-end training using the representation-alignment loss. The end-to-end training with REPA loss helps better improve the final representationalignment score (Fig. 3c), which in turn leads to improved final generation performance (refer 4.2). We next discuss key details for implementation of REPA-E for training. Batch-Norm Layer for VAE Latent Normalization. To enable end-to-end training, we first introduce batchnorm layer between the VAE and latent diffusion model  (Fig. 1)  . Typical LDM training involves normalizing the VAE features using precomputed latent statistics (e.g., std = 1/ 0.1825 for SD-VAE [39]). This helps normalize the VAE latent outputs to zero mean and unit variance for more efficient training for the diffusion model. However, with end-to-end training the statistics need to be recomputed whenever the VAE model is updated - which is expensive. To address this, we propose the use of batchnorm layer [21] which uses the exponential moving average (EMA) mean and variance as surrogate for dataset-level statistics. The batch-norm layer thus acts as differentiable normalization operator without the need for recomputing dataset level statistics after each optimization step. End-to-End Representation-Alignment Loss. We next enable end-to-end training, by using the REPA loss [53] for updating the parameters for both VAE and LDM during training. Formally, let Vϕ represent the VAE, Dθ be the diffusion model, be the fixed pretrained perceptual model (e.g., DINO-v2 [33]) for REPA [53] and be clean image. Also similar to REPA, consider hω(ht) be the projection of diffusion transformer output ht through trainable projection layer hω. We then perform end-to-end training by applying the REPA loss over both LDM and VAE as, LREPA(θ, ϕ, ω) = Ex,ϵ,t (cid:34)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) n=1 sim(y[n], hω(h[n] )) , (cid:35) where = (x) is the output of the pretrained perceptual model (e.g., DINO-v2 [33]), is number of patches, sim(< . , . >) computes the patch-wise cosine similarities between pretrained representation from perceptual model (e.g., DINO-v2) and diffusion transformer hidden state ht. Diffusion Loss with Stop-Gradient. As discussed in Fig. 3a and 3.1, backpropagating the diffusion loss to the VAE causes degradation of latent-space structure. To avoid this, we introduce simple stopgrad operation which limits the application of diffusion loss LDIFF to only the parameters θ of the latent diffusion model Dθ. VAE Regularization Losses. Finally, we introduce regularization losses LREG for VAE Vϕ, to ensure that the endto-end training process does not impact the reconstruction performance (rFID) of the original VAE. In particular, following [1], we use three losses, 1) Reconstruction Losses (LMSE, LLPIPS), 2) GAN Loss (LGAN), 3) KL divergence loss (LKL) as regularization loss LREG for the VAE Vϕ. Overall Training. The overall training is then performed in an end-to-end manner using the following loss, L(θ, ϕ, ω) = LDIFF(θ) + λLREPA(θ, ϕ, ω) + ηLREG(ϕ), where θ, ϕ, ω refer to the parameters for the LDM, VAE and trainable REPA projection layer [53], respectively. Further implementation details are provided in 4.1 and Appendix. 4. Experiments We next validate the performance of REPA-E and the effect of proposed components through extensive evaluation. In particular, we investigate three key research questions: 1. Can REPA-E significantly improve generation performance and training speed? (Sec. 4.2, Tab. 2, Fig. 1, 4) 2. Does REPA-E generalize across variations in training settings including model-scale, architecture, encoder model for REPA etc.? (Sec. 4.3, Tab. 3, 4, 5, 6, 7, 8) 3. Analyze the impact of end-to-end tuning (REPA-E) on VAE latent-space structure and downstream generation performance. (please refer Sec. 4.4, Fig. 6, Tab. 9, 10) 4.1. Setup Implementation Details. We follow the same setup as in SiT [30] and REPA [53] unless otherwise specified. All training is conducted on the ImageNet [6] training split. We adopt the same data preprocessing protocol as in ADM [7], where original images are center-cropped and resized to 256 256 resolution. We experiment with publicly available VAEs, including SD-VAE (f8d4) [39], VA-VAE (f16d32) [39], and our own f16d32 VAE trained on ImageNet, referred to as IN-VAE. Depending on the VAE downsampling rate, we adopt SiT-XL/1 and SiT-XL/2 for 4 and 16 downsampling rates, respectively, where 1 and 2 denote the patch sizes in the transformer embedding layer. We disable affine transformations in the BN [21] layer between the VAE and SiT, relying solely on the running mean and standard deviation. The VAE regularization loss combines multiple objectives and is defined as: LREG = LKL + LMSE + LLPIPS + LGAN. For alignment loss, we use DINOv2 [33] as external visual features and apply alignment to the eighth layer of the SiT model. Empirically, we set the alignment loss coefficient to λREPAg = 0.5 for updating SiT and λREPAv = 1.5 for VAE. For optimization, we use AdamW [24, 29] with constant learning rate of 1104, and global batch size of 256. During training, we apply gradient clipping and exponential moving average (EMA) to the generative model for stable optimization. All experiments are conducted on 8 NVIDIA H100 GPUs. Evaluation. For image generation evaluation, we strictly follow the ADM setup [7]. We report generation qual5 Figure 4. End-to-End tuning (REPA-E) improves visual scaling. We observe that REPA-E produces higher-quality images at 400K steps compared with the vanilla-REPA and generates more structurally meaningful images even in the early stages of training. Results for both methods are sampled using the same seed, noise and class label. We use classifier-free guidance scale of 4.0 during sampling. ity using Frechet inception distance (gFID) [17], structural FID (sFID) [31], inception score (IS) [42], precision (Prec.) and recall (Rec.) [26], measured on 50K generated images. For sampling, we follow the approach in SiT [30] and REPA [53], using the SDE Euler-Maruyama sampler with 250 steps. In terms of VAE benchmark, we measure the reconstruction FID (rFID) on 50K images from the ImageNet [6] validation set at resolution of 256 256. 4.2. Impact on Training Performance and Speed We first analyze the impact of end-to-end tuning using REPA-E (Sec. 3.2) for improving generation performance and speed when training latent-diffusion transformers. Quantitative Evaluation. We compare REPA-E against various latent diffusion model (LDM) baselines in Tab. 2. We evaluate models of similar sizes (675M parameters) on ImageNet 256 256 generation task. All results are reported without classifier-free guidance [18] using popular SiT-XL [30] model for training. We make two observations; 1) End-to-End tuning leads to faster training: consistently improving generation FID (gFID) from 19.40 12.83 (20 epochs), 11.10 7.17 (40 epochs), and 7.90 4.07 (80 epochs), even when comparing with REPA [53]. 2) End-toEnd training leads to better final performance: REPA-E at 80 epochs surpasses FasterDiT [50] (gFID=7.91) trained for 400 epochs and even MaskDiT [55], DiT [34], and SiT [30] which are trained over 1400 epochs. For instance, REPAE reaches an FID of 4.07 within 400K steps, significantly boosting final performance over even REPA which only reaches final FID for 5.9 after 4M steps [53]. Qualitative Evaluation. We provide qualitative comparisons between REPA [53] and REPA-E in Fig. 4. We generate images from the same noise and label using checkpoints at 50K, 100K, and 400K training iterations, respectively. As seen in Fig. 4, we observe that REPA-E demonstrates superior image generation quality compared to the REPA baseline, while also generating more structurally meaningful images during early stages of training process. 4.3. Generalization and Scalability of REPA-E We next analyze the generalization of the proposed approach to variation in training settings including modelsize, tokenizer architecture, representation encoder, alignment depth [53] etc. Unless otherwise specified, all analysis and ablations use SiT-L [30] as the generative model, SD-VAE as the VAE, and DINOv2-B [33] as the pretrained vision model for REPA loss [53]. Default REPA alignmentdepth of 8 is used. We train each variant for 100K iterations and report results without classifier-free guidance [18]. All baseline numbers are reported using vanilla REPA and compared with end-to-end training using REPA-E. Impact of Model Size. Tab. 3 compares SiT-B, SiTL, and SiT-XL to evaluate the effect of model size. We make two key observations. First, across all configurations, Figure 5. Qualitative Results on Imagenet 256 256 using E2E-VAE and SiT-XL. We use classifier-free guidance scale αcfg = 4.0. Method Tokenizer Epochs gFID sFID IS Diff. Model gFID sFID IS Prec. Rec. Without End-to-End Tuning MaskDiT [55] DiT [34] SiT [30] FasterDiT [50] SD-VAE REPA [53] SD-VAE 1600 1400 1400 400 20 40 80 800 5.69 9.62 8.61 7.91 19.40 11.10 7.90 5. 10.34 6.85 6.32 5.45 6.06 6.06 5.06 5.73 177.9 121.5 131.7 131.3 67.4 67.4 122.6 157.8 With End-to-End Tuning (Ours) REPA-E SD-VAE 20 40 80 12.83 7.17 4.07 5.04 4.39 4.60 88.8 123.7 161.8 Table 2. REPA-E for Accelerated Generation Performance. End-to-End training with REPA-E achieves significantly better performance (lower gFID) while using fewer epochs. Notably, REPA-E with only 80 epochs surpasses vanilla REPA using 10 epochs. indicates that VAE is updated during end-to-end training. All results are w/o classifier-free guidance on ImageNet 256 256. Additional system-level comparisons with classifier-free guidance and state-of-the-art results are provided in Tab. 10. REPA-E consistently improves performance over the REPA baseline. Specifically, it reduces gFID from 49.5 34.8 for SiT-B, 24.1 16.3 for SiT-L, and 19.4 12.8 for SiT-XL, demonstrating the effectiveness. Second, surprisSiT-B (130M) +REPA-E (Ours) SiT-L (458M) +REPA-E (Ours) SiT-XL (675M) +REPA-E (Ours) 49.5 34.8 24.1 16. 19.4 12.8 7.00 6.31 6.25 5.69 6.06 5.04 27.5 39.1 55.7 75. 67.4 88.8 0.46 0.57 0.62 0.68 0.64 0.71 0.59 0.59 0.60 0. 0.61 0.58 Table 3. Variation in Model-Scale. We find that REPA-E brings substantial performance improvements across all model-scales. All baselines are reported using vanilla-REPA [53] for training. ingly the percentage gains in gFID achieved with REPA-E (over REPA) improve with increasing model size. For instance, for SiT-B model REPA-E leads to 29.6% improvement in gFID over REPA. Surprisingly even more gains are achieved for bigger models improving gFID by 32.3% and 34.0% for SiT-L and SiT-XL models respectively. This trend highlights the scalability of REPA-E; larger models achieve better percentage gains over vanilla-REPA. Variation in Representation Encoder. We report results across different perception model encoders (CLIP-L, I-JEPA-H, DINOv2-B, and DINOv2-L) Tab. 4. We observe that REPA-E gives consistent performance improvements over REPA, across different choices of the perceptual encoder model. In particular, with DINOv2-B and DINOv2L, REPA-E significantly reduces gFID from 24.1 16.3 and from 23.3 16.0, respectively. 7 (a) PCA Visualization of Latent Space Structure [25] (b) Impact of End-to-End Tuning for Automatically Improving Latent Space Structure Figure 6. End-to-End Training Improves Latent Space Structure. (a) We observe that the latent space of pretrained VAEs can suffer either high noise components (e.g., SDXL-VAE, SD-VAE [39]), or, be over-smoothed and lack details (e.g., VA-VAE [49]). (b) The use of end-to-end tuning (3.2) automatically helps improve the latent space structure in model-agnostic manner across different VAE architectures. For instance, similar to findings of concurrent work [43], we observe that SD-VAE suffers from high noise components in the latent space. Applying end-to-end training automatically helps adjust the latent space to reduce noise. In contrast, other VAEs such as recently proposed VA-VAE [49] suffer from an over-smoothed latent space. The use of end-to-end tuning with REPA-E automatically helps learn more detailed latent-space structure to best support generation performance. Target Repr. gFID sFID IS Prec. Rec. Aln. Depth gFID sFID IS Prec. Rec. I-JEPA-H [2] +REPA-E (Ours) CLIP-L [37] +REPA-E (Ours) DINOv2-B [33] +REPA-E (Ours) DINOv2-L [33] +REPA-E (Ours) 23.0 16.5 29.2 23.4 24.1 16. 23.3 16.0 5.81 5.18 5.98 6.44 6.25 5.69 5.89 5.59 60.3 73. 46.4 57.1 55.7 75.0 59.9 77.7 0.62 0.68 0.59 0.62 0.62 0. 0.61 0.68 0.60 0.60 0.61 0.60 0.60 0.60 0.60 0.58 Table 4. Variation in Representation Encoder. REPA-E yields consistent performance improvements across different choices for the representation-encoder used for representation-alignment [53]. All baselines are reported using vanilla-REPA [53] for training. Autoencoder gFID sFID IS Prec. Rec. SD-VAE [39] +REPA-E (Ours) IN-VAE (f16d32) +REPA-E (Ours) VA-VAE [49] +REPA-E (Ours) 24.1 16.3 22.7 12.7 12.8 11.1 6.25 5. 5.47 5.57 6.47 5.31 55.7 75.0 56.0 84.0 83.8 88.8 0.62 0. 0.62 0.69 0.71 0.72 0.60 0.60 0.62 0.62 0.58 0.61 Table 5. Variation in VAE Architecture. REPA-E improves generation-performance across different choices for the VAE. All baselines are reported using vanilla-REPA [53] for training. Variation in VAE. Tab. 5 evaluates the impact of difIn particular, we ferent VAEs on REPA-E performance. 6th layer +REPA-E (Ours) 8th layer +REPA-E (Ours) 10th layer +REPA-E (Ours) 23.0 16. 24.1 16.3 23.7 16.2 5.72 6.64 6.25 5.69 5.91 5.22 59.2 74. 55.7 75.0 56.9 74.7 0.62 0.67 0.62 0.68 0.62 0.68 0.60 0. 0.60 0.60 0.60 0.58 Table 6. Variation in Alignment Depth. End-to-End tuning (REPA-E) gives consistent performance imrpovements over original REPA [53] across varying alignment-depths. All baselines are reported using vanilla-REPA [53] for training. report results using three different VAEs 1) SD-VAE [1], 2) VA-VAE [49] and 3) IN-VAE (a 16 downsampling, 32channel VAE trained on ImageNet [6] using official training code from [39]). Across all variations, REPA-E consistently improves performance over the REPA baseline. REPA-E reduces gFID from 24.1 16.3, from 22.7 12.7, and 12.8 11.1, for SD-VAE, IN-VAE and VA-VAE, respectively. The results demonstrate that REPA-E robustly improves generative quality across diverse variations in architecture, pretraining dataset and training setting of the VAE. Variation in Alignment Depth. Tab. 6 investigates the effect of applying the alignment loss at different layers the diffusion model. We observe that REPA-E consistently enhances generation quality over the REPA baseline across variation in choice of alignment depth; with gFID improving from 23.0 16.4 (6th layer), 24.1 16.3 (8th layer), Component gFID sFID IS Prec. Rec. VAE Diffusion model REPA gFID-50K w/o stopgrad w/o batch-norm w/o LREG REPA-E (Ours) 444.1 18.1 19.2 16.3 460.3 5.32 6.47 5.69 1.49 72.4 68.2 75.0 0.00 0.67 0.64 0.68 0.00 0.59 0.58 0. Table 7. Ablation Study on Role of Different Components. Method gFID sFID IS Prec. Rec. 100K Iterations (20 Epochs) REPA [53] REPA-E (scratch) REPA-E (VAE init.) 19.40 14.12 12.83 6.06 7.87 5.04 67.4 83.5 88.8 200K Iterations (40 Epochs) REPA [53] REPA-E (scratch) REPA-E (VAE init.) 11.10 7.54 7.17 5.05 6.17 4.39 100.4 120.4 123.7 400K Iterations (80 Epochs) REPA [53] REPA-E (scratch) REPA-E (VAE init.) 7.90 4.34 4. 5.06 4.44 4.60 122.6 154.3 161.8 0.64 0.70 0.71 0.69 0.74 0.74 0.70 0.75 0.76 0.61 0.59 0. 0.64 0.61 0.62 0.65 0.63 0.62 Table 8. End-to-End Training from Scratch. We find that while initializing the VAE with pretrained weights (SD-VAE [39]) helps slightly improve performance, REPA-E can be used to train both VAE and LDM from scratch in an end-to-end manner; still achieving significantly superior performance over REPA which requires separate stage for training VAE in addition to LDM training. and 23.7 16.2 (10th layer). Ablation on Design Components. We also perform ablation studies analyzing the importance of each component discussed in Sec. 3.2. Results are shown in Tab. 7. We observe that each component plays key role in the final performance for REPA-E. In particular, we observe that the stop-grad operation on the diffusion loss helps prevent degradation of the latent-space structure. Similarly, the use of batch norm is useful adaptively normalizing the latent-statistics and helps improve the gFID from 18.09 16.3. Similarly, the regularization losses play key role in maintaining the reconstruction performance of the finetuned VAE, thereby improving the gFID from 19.07 16.3. End-to-End Training from Scratch. We next analyze the impact of VAE initialization on end-to-end training. As shown in Tab. 8, we find that while initializing the VAE from pretrained weights helps slightly improve performance, REPA-E can be used to train both VAE and LDM from scratch still achieving superior performance over REPA, which technically requires separate stage for VAE training in addition to LDM training. For instance, while REPA achieves FID of 5.90 after 4M iterations, REPA-E while training entirely from scratch (for both VAE SD-VAE [39] VA-VAE [49] E2E-VAE (Ours) SD-VAE [39] VA-VAE [49] E2E-VAE (Ours) SD-VAE [39] VA-VAE [49] E2E-VAE (Ours) SD-VAE [39] VA-VAE [49] E2E-VAE (Ours) DiT-XL [34] DiT-XL [34] DiT-XL [34] SiT-XL [30] SiT-XL [30] SiT-XL [30] DiT-XL [34] DiT-XL [34] DiT-XL [34] SiT-XL [30] SiT-XL [30] SiT-XL [30] 19.82 6.74 6.75 17.20 5.93 5.26 12.29 4.71 4.20 7.90 4.88 3.46 Table 9. Impact of End-to-End tuning on VAE performance. We find that once tuned using REPA-E, the finetuned VAEs can be used as drop-in replacement for their original counterparts offering significantly accelerated generation performance. We fix all the VAEs and only train the diffusion models (with and w/o REPA). E2E-VAE is obtained from REPA-E fine-tuning (VA-VAE + SiT-XL). All results are reported at 80 epochs (400K iterations). and LDM) achieves much faster and better generation FID of 4.34 within just 400K iterations. 4.4. Impact of End-to-End Tuning on VAE We next analyze the impact of end-to-end tuning on the VAE. In particular, we first show that end-to-end tuning improves the latent-space structure  (Fig. 6)  . We next show that once tuned using REPA-E, the finetuned VAEs can be used as drop-in replacement for their original counterparts offering significantly improved generation performance. End-to-End Training improves Latent Space Structure. Results are shown in Fig. 6. Following [25], we visualize latent space structure using principal component analysis (PCA) that projects them to three channels colored by RGB. We consider three different VAEs: 1) SD-VAE [39], 2) IN-VAE (a 16 downsampling, 32-channel VAE trained on ImageNet [6]). 3) VA-VAE from recent work from [49]. We observe that end-to-end tuning using REPAE automatically improves the latent space structure of the original VAE. For instance, similar to findings of concurrent work [43], we observe that SD-VAE suffers from high noise components in the latent space. Applying end-to-end training automatically helps adjust the latent space to learn reduce noise. In contrast, other VAEs such as recently proposed VA-VAE [49] suffer from over-smoother latent space. Application of end-to-end tuning with REPA-E automatically helps learn more detailed latent-space structure to best support generation performance. End-to-End Training Improves VAE Performance. We next evaluate the impact of end-to-end tuning on downstream generation performance of the VAE. To this end, we 9 Tokenizer Method Training Epoches #params rFID Generation w/o CFG Generation w/ CFG gFID sFID IS Prec. Rec. gFID sFID IS Prec. Rec. MaskGiT VQGAN VQVAE LFQ tokenizers LDM MaskGIT [4] LlamaGen [45] VAR [46] MagViT-v2 [51] MAR [28] SD-VAE [39] MaskDiT [55] DiT [34] SiT [30] FasterDiT [50] MDT [12] MDTv2 [13] VA-VAE [49] LightningDiT [49] SD-VAE REPA [53] E2E-VAE (Ours) REPA 555 300 350 1080 1600 1400 1400 400 1300 1080 80 800 80 800 80 800 AutoRegressive (AR) 2.28 0.59 - 1.50 0. 6.18 9.38 - 3.65 2.35 - 8.24 - - - 182.1 112.9 - 200.5 227.8 Latent Diffusion Models (LDM) 0.61 5.69 9.62 8.61 7.91 6.23 - 10.34 6.85 6.32 5.45 5.23 - 177.9 121.5 131.7 131.3 143.0 - Representation Alignment Methods 0.28 0.61 0. 4.29 2.17 7.90 5.90 3.46 1.83 - 4.36 5.06 5.73 4.17 4. - 205.6 122.6 157.8 159.8 217.3 227M 3.1B 2.0B 307M 945M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 0.80 0.69 - - 0.79 0.74 0.67 0.68 0.67 0.71 - - 0.77 0.70 0. 0.77 0.77 0.51 0.67 - - 0.62 0.60 0.67 0.67 0.69 0.65 - - 0.65 0.65 0.69 0.63 0. - 2.18 1.80 1.78 1.55 2.28 2.27 2.06 2.03 1.79 1.58 - 1.35 - 1.42 1.67 1.26 - 5.97 - - - 5.67 4.60 4.50 4.63 4.57 4.52 - 4.15 - 4.70 4.12 4.11 - 263.3 365.4 319.4 303.7 276.6 278.2 270.3 264.0 283.0 314. - 295.3 - 305.7 266.3 314.9 - 0.81 0.83 - 0.81 0.80 0.83 0.82 0.81 0.81 0.79 - 0. - 0.80 0.80 0.79 - 0.58 0.57 - 0.62 0.61 0.57 0.59 0.60 0.61 0.65 - 0.65 - 0. 0.63 0.66 Table 10. System-Level Performance on ImageNet 256 256 comparing our end-to-end tuned VAE (E2E-VAE) with other VAEs for traditional LDM training. We observe that in addition to improving VAE latent space structure  (Fig. 6)  , end-to-end tuning significantly improves VAE downstream generation performance. Once tuned using REPA-E, the improved VAE can be used as drop-in replacement for their original counterparts for accelerated generation performance. Overall, our approach helps improve both LDM and VAE performance achieving new state-of-the-art FID of 1.26 and 0.28, respectively for LDM generation and VAE reconstruction performance. first use end-to-end tuning for finetuning the recently proposed VA-VAE [49]. We then use the resulting end-to-end finetuned-VAE (named E2E-VAE), and compare its downstream generation performance with current state-of-the-art VAEs; including SDVAE [39] and VA-VAE [49]. To do this, we conduct traditional latent diffusion model training (w/o REPA-E), where only the generator network is updated while keeping the VAE frozen. Tab. 9 shows the comparison of VAE downstream generation across diverse training settings. We observe that end-to-end tuned VAEs consistently outperform their original counterparts for downstream generation tasks across variations in LDM architecture and training settings. Interestingly, we observe that VAE tuned using SiT-XL yields performance improvements even when using different LDM architecture such as DiTXL; thereby demonstrating the robustness of our approach. 5. Conclusion In this paper, we tackle fundamental question: Can we unlock VAEs for performing end-to-end training with latent diffusion transformers? In particular, we observe that directly backpropagating the diffusion loss to the VAE is ineffective and even degrages final generation performance. We show that while diffusion loss is ineffective, end-to-end training can be performed using the recently proposed representational alignment loss. The proposed end-to-end training recipe (REPA-E), significantly improves latentspace structure shows remarkable performance; speeding up diffusion model training by over 17 and 45 over REPA and vanilla training recipes, respectively. REPA-E not only shows consistent improvements across variations in training settings, but also improves the original latent-space structure across different VAE architectures. Overall, our approach achieves new state-of-the-art results with generation FID of 1.26 and 1.83 with and without use of classifier-free guidance. We hope that our work can help foster further research for enabling end-to-end training with latent diffusion transformers."
        },
        {
            "title": "References",
            "content": "[1] Stability AI. https : / / Improved autoencoders ... huggingface . co / stabilityai / sd - vae - ft - mse, n.d. Accessed: April 11, 2025. 5, 8 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. 8 [3] Dana Ballard. Modular learning in neural networks. In Proceedings of the sixth National conference on Artificial intelligence-Volume 1, pages 279284, 1987. 3 [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 10 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 3 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 2, 5, 6, 8, 9 [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 5 [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 3 [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 3 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3 [11] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 3 [12] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2316423173, 2023. 10 [13] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 10 [14] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 14401448, 2015. 1 [15] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580587, 2014. 1 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [20] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip In InternaIsola. The platonic representation hypothesis. tional Conference on Machine Learning, 2024. 4 [21] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448456. pmlr, 2015. 5 [22] Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-Chieh Chen. Democratizing text-to-image masked generative models with comarXiv preprint pact text-aware one-dimensional tokens. arXiv:2501.07730, 2025. [23] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 1, 3 [24] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 5 [25] Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Eq-vae: Equivariance regularized latent space for improved generative image modeling. arXiv preprint arXiv:2502.09509, 2025. 2, 8, 9 [26] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. [27] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3 [28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2025. 10 [29] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5 [30] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 2, 3, 4, 5, 6, 7, 9, [31] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. In International Conference on Machine Learning, pages 79587968. PMLR, 2021. 6 [32] OpenAI. Sora. https://openai.com/sora, 2024. 3 [33] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. 11 generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2025. 10 [47] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [49] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. 3, 8, 9, 10 [50] Jingfeng Yao, Wang Cheng, Wenyu Liu, and Xinggang Wang. Fasterdit: Towards faster diffusion transformers training without architecture modification. arXiv preprint arXiv:2410.10356, 2024. 6, 7, 10 [51] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 10 [52] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth Advances 32 tokens for reconstruction and generation. in Neural Information Processing Systems, 37:128940 128966, 2025. 3 [53] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 2, 3, 4, 5, 6, 7, 8, 9, 10 [54] Kaiwen Zha, Lijun Yu, Alireza Fathi, David Ross, Cordelia Schmid, Dina Katabi, and Xiuye Gu. Languageguided image tokenization for generation. arXiv preprint arXiv:2412.05796, 2024. 3 [55] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 6, 7, Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pages 131, 2024. 3, 5, 6, 8 [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 3, 4, 6, 7, 9, 10 [35] Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 3 [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 3 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 8 [38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region IEEE transactions on pattern analysis proposal networks. and machine intelligence, 39(6):11371149, 2016. 1 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3, 4, 5, 8, 9, 10 [40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [41] Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena, 60(1):259268, 1992. [42] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6 [43] Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin. Improving the diffusability of autoencoders. arXiv preprint arXiv:2502.14831, 2025. 2, 8, 9 [44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. 3 [45] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 10 [46] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image"
        }
    ],
    "affiliations": [
        "Australian National University",
        "Data61 CSIRO",
        "New York University"
    ]
}