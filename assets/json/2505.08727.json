{
    "paper_title": "Memorization-Compression Cycles Improve Generalization",
    "authors": [
        "Fangyuan Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation."
        },
        {
            "title": "Start",
            "content": "Memorization-Compression Cycles Improve Generalization Fangyuan Yu Temus"
        },
        {
            "title": "Abstract",
            "content": "We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorizationcompression cycle during LLM pretraining, evidenced by oscillating positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), measure for representation entropy. This pattern closely mirrors the predictivecompressive trade-off prescribed by IBLM and also parallels the biological alternation between active learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalization by 35% in pretraining task on arithmetic multiplication. In setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving 97% improvement in separation paralleling the functional role of sleep consolidation in biological learning process."
        },
        {
            "title": "Introduction",
            "content": "Generalization occurs when learning from one task improves performance on another. The pursuit of generalization in pre-training LLM [11] has historically focused on scaling up data and parameter size [4], in post-training, Reinforcement Learning with Verifiable Reward (RLVR) [7] has gained attention. However, high quality data has been exhausted after years of tireless scraping, and RLVR is shown to only trims knowledge from baseline model [10] instead of incentivize new reasoning pattern. We establish theoretical upper bound on generalization error for deep learning models, indicating representation entropy as another dimension for improving generalization besides scaling data. Theorem 1. Upper Bound on Generalization Error. Let and be random variables with an unknown joint distribution (X, ), and suppose is discrete with finite cardinality. Let be neural network with intermediate representations forming Markov chain: 5 2 0 2 3 1 ] . [ 1 7 2 7 8 0 . 5 0 5 2 : r R1 RL ˆY Where Rl is internal representations, ˆY is the prediction of the network. Then, for any dataset DN = {(xi, yi)}N i=1 sampled i.i.d. from (X, ), the generalization error satisfies for α [1, +): LP (X,Y )(f, ℓ) (cid:124) (cid:125) (cid:123)(cid:122) Generalization Error ˆLP (X,Y )(f, ℓ, DN ) (cid:125) (cid:123)(cid:122) Empirical Error (cid:124) + (cid:124) Preprint. Under review. (cid:18) 2αH(Rl) / log min 1lL (cid:123)(cid:122) Upper Bound on Generalization Gap (cid:19) (cid:125) (1) Intuitively, generalization performance of neural network can be improved by either increasing training data size, or by reducing entropy of its internal representations. We refer minimization of H(Rl) as compression, and minimizing empirical loss as memorization. The Information Bottleneck (IB) framework [8] characterizes the optimal representation as one that discards as much information from the input as possible, while preserving all information relevant to the output. Motivated by this, we introduce the Information Bottleneck Language Modeling (IBLM) objective, along with theorem showing equivalence of IBLM with IB framework under language modeling. Definition 1. Information Bottleneck Language Modeling (IBLM). Given language model with internal representations R1:L and output token variable , the IBLM objective is: min s.t. (cid:88) H(Rl) l=1 ˆY = arg min ˆY H(Y ˆY ) (2) Theorem 2. The IBLM objective defined in Equation 2 is equivalent to the classical Information Bottleneck formulation under the language modeling setting. Note that H(Y ˆY ) is the cross-entropy (CE) loss in conventional language modeling task [11]. In short, LBLM requires maximal compressing internal representation under optimal cross-entropy. To explicitly calculate H(R), we adopt Matrix-based entropy (MBE), first proposed in [5], given matrix Rsd concatenating token representations, MBE is given by Sα(T ) = 1 1 α (cid:16) (cid:88) ( log λi(K) Tr(K) )α(cid:17) where = RRT is the Gram matrix. MBE essentially provide continuous measure of matrix rank, by calculating entropy on distribution of singular values. It has been shown to exhibit strong correlation with embedding quality [14], where its also observed that later checkpoints in pre-trained base models has lower entropy, suggesting pre-training with cross-entropy loss alone leads to decrease in MBE. Previous work [15] observed that in deterministic models (where I(R; X) = H(R)), training with single loss target leads to two-phase trend: short initial memorization phase with rapid empirical loss reduction, followed by monotonic decrease in H(R), interpreted as compression phase. This observation suggests clean separation between learning and compression in deep networks. However, our empirical analysis of GPT pretraining reveals richer structure. By tracking the cosine similarity between gradients of CE and MBE, we observe that their alignment oscillates between positive and negative throughout training. This suggests that instead of proceeding through distinct phases, learning unfolds as cyclic process, continuously alternating between expanding representations to capture data (memorization) and reorganizing them into more compact forms (compression). Importantly, this local oscillation occurs alongside global trend of decreasing MBE, indicating that compression accumulates over time even as the network periodically re-enters memorization-like states. We refer to this phenomenon as the memorizationcompression cycle. In biological neural systems, learning is inherently cyclic, alternating between awake learning and sleep consolidation. In seminal study [18], sleep was shown to resolve interference between conflicting memories by encouraging competition between them. When two conflicting tasks were presented sequentially, awake learning alone led to catastrophic forgetting, as new synaptic updates overwrote previously learned associations. Sleep consolidation overcame this by reorganizing synaptic weightsallocating distinct subsets to store each memoryachieving better retention than even interleaved learning. Inspired by biolocial learning cycle, we introduce Gated Phase Transition (GAPT), training algorithm that explicitly alternates between memorization (minimizing CE) and compression (minimizing CE and MBE) phases. GAPT is designed to approximate the optimal solution to the IBLM objective by dynamically switching phases based on learning signals, mirroring the role of consolidation in resolving representational conflict. GAPT delivers consistent improvements across domains: First, in LLM pre-training on FineWeb dataset, GAPT reduces Matrix-Based Entropy (MBE) by an average of 70.5% across layers while improving cross-entropy by 4.8%, outperforming standard language modeling and approaching the IBLM trade-off. Second, in arithmetic generalization, GAPT reduces test cross-entropy by 35% and MBE by 47% when trained on 13 digit multiplication and tested on 46 digits, supporting Theorem 1. Third, in synthetic task with gradient interference, GAPT improves representational separation by 97% and reduces MBE by 91% relative to mixed baselineclosely mirroring the conflict resolution behavior observed in biological sleep. Our work makes several key contributions: Theoretical Foundation: We derive an upper-bound on generalization error showing that reducing representation entropy can improve generalization, alongside scaling data. IBLM Objective: We formulate Information Bottleneck Language Modeling (IBLM) as constrained optimization problem, unifying representation entropy and cross entropy targets. GAPT Algorithm: We propose Gated Phase Transition (GAPT), algorithm to solve IBLM alternates between memorization and compression phases based on dynamic training signals. Empirical Results: We show that GAPT improves LLM pre-training, arithmetic generalization, and conflict resolution. Biological Analogy: We relate the memorizationcompression cycle in LLMs to the awakesleep consolidation cycle in biological systems and validate compressions similarity to consolidation. The remainder of the paper expands on these contributions: Section 2 presents our theoretical framework and generalization bound; Section 3 details the GAPT algorithm; Section 4 presents empirical results, including the memorizationcompression cycle and GAPTs effectiveness; Section 5 discusses relevant work; Section 6 discusses broader implications."
        },
        {
            "title": "2 Theory",
            "content": "Corollary 1 (Entropy Lower Bound for Finite Discrete Random Variables). Let be discrete random variable with finite support Ω, where Ω = n, and assume that (X = x) > 0 for all Ω. Then there exists constant β (0, 1] such that: H(X) β log2 Ω. Proof of Corollary 1 can be found in Appendix A. Theorem 1. Upper Bound on Generalization Error. Let and be random variables with unknown joint distribution (X, ), and suppose is discrete with finite cardinality. Let be neural network with intermediate representations forming Markov chain: R1 RL ˆY Then, for any dataset DN = {(xi, yi)}N satisfies i=1, there exists α [1, +) s.t. the generalization error LP (X,Y )(f, ℓ) ˆLP (X,Y )(f, ℓ, DN ) + (cid:18) log min1lL 2αH(Rl) (cid:19) Proof. We begin by recalling the standard formulation of the generalization gap: GenP (X,Y )(f, ℓ, DN ) := LP (X,Y )(f, ℓ) ˆLP (X,Y )(f, ℓ, DN ) For discrete input variables X, it has been shown in [6] that the generalization gap is upper-bounded by: LP (X,Y )(f, ℓ) ˆLP (X,Y )(f, ℓ, DN ) 3 (cid:18) log (cid:19) where denotes the cardinality of the input space. By Corollory 1, we have = O(2αH(X)) for α [1, +) , this suggests the upper bound could be reformulated via entropy LP (X,Y )(f, ℓ) ˆLP (X,Y )(f, ℓ, DN ) O( 2αH(X) log ) Let be neural network with intermediate representations forming Markov chain: R1 RL ˆY Fix any intermediate representation Rl, and decompose the network into = de, where : Rl maps inputs to Rl = e(X), and : Rl predicts the output. Applying the generalization bound to the representation Rl, we obtain: LP (X,Y )(f, ℓ) = LP (Rl,Y )(d, ℓe) ˆLP (Rl,Y )(d, ℓe, DN ) + (cid:18) 2αH(Rl) log (cid:19) ˆLP (X,Y )(f, ℓ, DN ) + (cid:18) 2αH(Rl) log (cid:19) (3) (4) (5) Since the Markov structure ensures that each Rl is valid bottleneck in the information flow, we can take the tightest such bound across all layers, yielding: LP (X,Y )(f, ℓ) ˆLP (X,Y )(f, ℓ, DN ) + (cid:18) log min1lL 2αH(Rl) (cid:19) This concludes the proof. Theorem 2. The Information Bottleneck Language Modeling (IBLM) objective defined in Equation 2 is equivalent to the classical Information Bottleneck formulation under the language modeling setting. Proof. The Information Bottleneck (IB) framework [8] defines the optimal representation as the solution to: min p(rx) I(R; X) subject to I(Y ; R) = I(Y ; X) (6) (7) Intuitively, the optimal is one that discards as much information from the input as possible, while retaining all information necessary to predict the output . This representation is referred to as the minimal sufficient statistic. In the case of large language models (LLMs), the network forms deterministic Markov chain: where denotes an intermediate hidden representation and ˆY is the predicted output. Because is deterministically computed from X, we have: ˆY I(X; R) = H(R) H(RX) = H(R) I(Y ; X) I(Y ; R) I(Y ; ˆY ) I(Y ; ˆY ) = H(Y ) H( ˆY ) (8) (9) (10) The first equality follows from the fact that H(RX) = 0 for deterministic functions. The second line follows from the Data Processing Inequality (DPI) [3], which ensures that information cannot increase along Markov chain. Rewriting the loss of predictive information: I(Y ; X) I(Y ; R) I(Y ; X) I(Y ; ˆY ) = H(Y ˆY ) H(Y X) (11) (12) 4 Since H(Y X) is fixed (defined by the true distribution), minimizing H(Y ˆY ) increases I(Y ; ˆY ) and pushes it closer to I(Y ; X). In practice, minimizing H(Y ˆY ) corresponds to minimizing the cross-entropy loss used in language modeling. On the compression side, we know H(RX) = 0 since neural network propagation is deterministic, therefore we have I(X; R) = H(R). Thus, minimizing H(R) directly minimizes the IB compression term I(X; R), giving us tractable surrogate for representation compression. Together, this shows that minimizing cross-entropy corresponds to satisfying the predictive constraint in IB, while minimizing representation entropy implements the compression objectivejustifying the IBLM formulation."
        },
        {
            "title": "3 Approach",
            "content": "While applying Lagrangian objective (CE + λMBE) is natural approach to solving the constrained optimization in IBLM, we find it often leads to representation collapse: MBE converges to near-zero, but CE worsens as the model loses structure in its internal representations. Inspired by the alternation between learning and consolidation in biological systems, we divide training into two phases: memorization, where the model minimizes cross-entropy (CE) loss, and compression, where it minimizes weighted sum of CE and Matrix-Based Entropy (MBE). We propose Gated Phase Transition (GAPT), training algorithm that dynamically alternates between these phases. GAPT tracks global minimum CE loss and per-layer MBE histories, and uses patience-based gating to switch phases. Compression is exited early if CE degrades. GAPT encourages localized compressionreorganizing existing knowledge without interfering with the acquisition of new informationand ensures that entropy reduction occurs only when it does not hinder memorization. end if if Lce > Emin (1 + τ ) then ϕ 2, sc 0, Emin , MBEmin[i] sm 0 if > δ else sm+ = 1 if sm pm then Algorithm 1 Gated Phase Transition (GAPT) 1: Input: losses L, thresholds δ, τ , patience pm, pc 2: State: ϕ {1, 2} (1 = mem, 2 = comp), counters sm, sc, Emin, MBEmin[i] 3: Extract Lce, {MBEi}; update Emin Lce, Emin min(Emin, Lce) 4: if ϕ = 1 then 5: 6: 7: 8: 9: else 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end if end for sc 0 if > δ else sc+ = 1 if sc pc then maxi(MBEmin[i] MBEi) for each do MBEmin[i] min(MBEmin[i], MBEi) ϕ 1, sm ϕ 1, sm 0 end if end if else Memorization Compression"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Natural Compression-Memorization Cycle Our experimental setup follows the Modded-NanoGPT framework [23], We remove FP8 matmul (due to hardware incompatibility with Hopper GPUs) and use simplified block causal attention mask. 5 Figure 1: Left: CE and MBE loss curves during pretraining with CE loss only, showing implicit momentum for representation compression. Right: final per-layer MBE values. Later layers show lower MBE, indicating representation compression. Figure 2: Cosine similarity between CE gradients across batches. CE gradients become increasingly decorrelated over time, reflecting diminishing shared signal. All experiments are conducted on 8L40 GPUs, training 12-layer GPT model on 0.73B-token FineWeb training set, evaluated on its corresponding validation split. We train on CE loss only. We log cross-entropy (CE) and Matrix-Based Entropy (MBE) gradients at every training step. For each iteration, we record both CE and MBE gradients across multiple batches and compute the average cosine similarity both (1) across batches (CE vs. CE), and (2) between CE and MBE gradients, within and across batches. In Figure 1, we observe that training with CE loss alone leads to consistent decrease in MBE across layers, confirming observations from [14]. However, unlike the \"compression valley\" phenomenon described in that work, our results show that layers 712 have lower MBE than layers 16. This difference is likely due to our architectural design: we adopt U-Net-like skip connection pattern in which layer 1 is connected to layer 12, layer 2 to layer 11, and so on. As result, the lower MBE observed in later layers likely reflects more compact representations in the decoding stages. We further analyze CE gradient behavior. Figure 2. shows that gradient consistency (measured by cosine similarity across batches) declines over time across all layers. This indicates an increasing signal-to-noise ratio in CE gradients as training progresses. We next inspect cosine similarity between CE and MBE gradients. As shown in Figure 3, we observe recurring sign flips that indicate an implicit alternation between memorization (negative similarity) and compression (positive similarity) phases, even without explicitly optimizing for entropy. To characterize this oscillation, we analyze the gradient signal across training using three measures: (1) standard deviation (oscillation strength), (2) zero-crossing rate (oscillation frequency), and (3) peak-to-average power ratio from the power spectral density (periodicity strength). Figure 4 summarizes these metrics across layers and parameter types. We find that attention parameters exhibit 6 Figure 3: Cosine similarity between CE and MBE gradients over training. Alternating positive and negative phases indicate emergent memorizationcompression cycles. Figure 4: Oscillation metrics between CE and MBE gradients across layers and parameter groups. Left: standard deviation; center: zero-crossing rate; right: periodic strength (peak-to-mean PSD ratio). stronger and more frequent oscillations than MLP parameters. Interestingly, earlier layers show higher oscillation frequency, while no layer demonstrates strong rhythmic periodicitysuggesting that oscillation is irregular and state-driven, rather than strictly periodic. This suggests an intriguing perspective on learning: while training on fixed dataset induces global momentum toward representation compression, the local dynamics alternate between phases of memorization and compression. 4.2 Pre-training with GAPT In our second experiment, we retain the same GPT pre-training setup but incorporate the GAPT algorithm to test whether it offers better solution to IBLM objective defined in Equation 2 , compared to baseline model trained solely on the cross-entropy (CE) loss. MBE regularization during compression phase in GAPT is done from layer 2 to 9. Model CE Loss Baseline GAPT (Ours) 3.31 3.15 (4.8%) Table 1: Cross-entropy loss on the FineWeb validation set. As shown in Table 1, GAPT reduces cross-entropy on the FineWeb validation set by 4.8% compared to the CE-only baseline. In addition to reducing test CE loss, GAPT also significantly compresses internal representations. Figure 5 (left) shows the layer-wise MBE for both models. We observe consistent reductions across all regularized layers. The per-layer MBE values and their relative improvements are summarized in Figure 5 (right). GAPT reduces MBE by an average of 70.5% across layers 29 while improving validation crossentropy by 4.8%. This suggests that explicitly alternating between memorization and compression phases offers an effective solution to the constrained optimization objective in IBLM (Equation 2), and can exceed baseline training with cross-entropy target alone. Layer Base GAPT Reduction 2 3 4 5 6 7 8 0.5313 0.5039 0.6094 0.5273 0.2676 0.3105 0.2471 0.1270 0.2285 0.2217 0.1465 0.1279 0.0728 0.0344 0.0483 0.0542 56.99% 56.01% 75.96% 75.74% 72.81% 88.91% 80.44% 57.32% Avg -70.52% Figure 5: Left: Layer-wise MBE for baseline vs. GAPT. Right: Per-layer MBE reduction with GAPT. 4.3 Conflicting Memory Resolution Inspired by [18], which showed that sleep consolidation helps resolve memory conflicts, we design synthetic experiment to test whether GAPT can mitigate representation interference between conflicting experiences. We use 2-layer MLP fθ with randomly initialized parameters and define symmetric shift θ. Inputs X1, X2 R10 are sampled from Gaussians with shared variance but distinct means: X1 ([1, 1, 1, 1, 1, 0, 0, 0, 0, 0], σ2I), X2 ([0, 0, 0, 0, 0, 1, 1, 1, 1, 1], σ2I) The targets are defined as Y1 = fθ+θ(X1) and Y2 = fθθ(X2), producing two tasks with negatively aligned gradients. We compare GAPT against four baselines: single-task training, ordered training, mixed training, and GAPT applied to mixed batches with MBE regularization. Table 2 summarizes results. Ordered learning, where the model is trained on one experience and then on the other, suffers from catastrophic forgetting: performance on the first task degrades significantly after exposure to the second. Mixed training alleviates this issue, achieving low L1 loss on both tasks and moderate representation separation. However, GAPT improves over mixed training in both respects: it maintains the same L1 accuracy while achieving 97% increase in separation ratio and 91% reduction in MBE. These results suggest that the compression encouraged by GAPT not only preserves generalization performance but also promotes the disentanglement of conflicting memories. Moreover, compression 8 Strategy L1 (Pos) L1 (Neg) MBE (Pos) MBE (Neg) Dist. Sep. Ratio Pos-only Neg-only Pos Neg Neg Pos Mixed GAPT + MBE (Ours) 0.02 0.92 0.43 0.02 0.03 0.03 0.71 0.02 0.04 0.57 0.03 0.03 0.18 0.36 0.19 0.19 0.10 0.02(80%) 0.45 0.36 0.15 0.43 0.22 0.02(91%) 2.43 1.86 3.66 6.64(+81%) 2.84 2.33 4.11 8.08(+97%) Table 2: Performance on conflicting experience learning. Lower L1/MBE and higher separation indicate better generalization and disentanglement. Metric Baseline (1085) GAPT (Ours, 898) Change Entropy (ID) Entropy (OOD) Avg MBE (011) 0.010 4.334 0.218 0.011 2.817 0.115 +10% 35% 47% Figure 6: Up: Comparison of baseline vs. GAPT on arithmetic generalization. Bottom: Arithmetic generalization performance summary. GAPT improves OOD generalization and yields more compact representations. and separation emerge in tandem during training, closely resembling the consolidation behavior observed in biological neural systems during sleep. This supports the view that compression serves not only as generalization mechanism, but also as functional tool for resolving interference in memory [18]. 4.4 Arithmetic Generalization To evaluate whether GAPT improves generalization in pre-training language models, we conduct controlled experiment using synthetic arithmetic dataset. We pre-train GPT-2 model from scratch to perform integer multiplication. The training dataset contains 10 million multiplication equations between integers with 13 digits. For evaluation, we prepare two test sets: an in-domain (ID) set with 10,000 examples also from the 13 digit range, and an out-of-domain (OOD) set with 10,000 examples involving 46 digit multiplications. An additional OOD validation set with 1,000 examples is used for early stopping. We tokenize the input and output sequences at the per-digit level and train the model for 1,750 iterations with batch size of 16. Due to observed instability in OOD entropy, we adopt an early stopping strategy that halts training if validation loss increases by more than 20% after iteration 800. We expect future work to further stabilize GAPT in OOD settings. As shown in Figure 6, GAPT improves generalization substantially. It reduces OOD entropy by 35% and average representation entropy (MBE) by 47%, while maintaining similar performance on the 9 in-domain set. This supports our theoretical prediction that minimizing representation entropy leads to stronger generalization under the Information Bottleneck Language Modeling (IBLM) framework. Interestingly, while MBE regularization is only applied to subset of layers, we observe that GAPT achieves lower MBE even in unregularized layers (e.g., layers 0, 1, and 11), suggesting degree of entropy compression generalization across the network. Notably, MBE in layer 1 is reduced by 92%, and in layer 11 by 45%."
        },
        {
            "title": "5 Relevant Work",
            "content": "The Information Bottleneck (IB) method was introduced in [2] to formalize the goal of retaining only task-relevant information in representations by minimizing entropy. theoretical connection between generalization and the cardinality of discrete inputs was established in [6], and later applied to deep networks in [8]. However, the entropygeneralization link remained incomplete due to the gap between cardinality and entropy measures. Empirical evidence of two-phase learning dynamicearly memorization followed by entropy compressionwas presented in [15]. To quantify entropy in neural networks, matrix-based entropy (MBE) was proposed in [5], and later applied to LLMs in [14], where it correlated with embedding quality and revealed compression trends across checkpoints. LLMs such as GPT-2 [11] generalize across tasks by scaling training data and parameters [4], effectively compressing the training corpus [13, 17]. While post-training methods like instruction tuning [9] improve usability, LLMs still struggle on out-of-distribution tasks such as reverse reasoning [22] and multi-hop inference [20]. Recent advancements in RL with verifiable rewards (RLVR) have improved mathematical and coding performance [7], though often by narrowing base model behavior [10]. Finally, vocabulary design has seen progress through curriculum-based tokenization, where dynamic vocabulary growth based on modeling entropy leads to improved pretraining efficiency [21]."
        },
        {
            "title": "Implication",
            "content": "Generalization occurs when learning from one task improves performance on another. Our findings suggest that generalization can emerge from compressing internal representations under predictive constraintswithout relying on massive amounts of demonstrative data. This ability may allow artificial systems to discover novel patterns and principles, reducing dependence on large-scale supervision. However, it also raises important safety concerns: system that generalizes well but lacks transparency or interpretability may pose greater risks than one that mostly memorizes. As we move toward AGI, it is essential to understand and govern the mechanisms behind representation compression and generalization to ensure safe, aligned, and accountable behavior."
        },
        {
            "title": "References",
            "content": "[1] Shannon, C.E. Mathematical Theory of Communication. Bell System Technical Journal, 27(3), 379423, 1948. [2] Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. [3] Cover, T.M. and Thomas, J.A. Elements of Information Theory (2nd ed.). Wiley Series in Telecommunications and Signal Processing. Wiley-Interscience, 2006. [4] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. arXiv preprint arXiv:2001.08361, 2020. [5] Giraldo, L.G.S., Rao, M., and Principe, J.C. Measures of entropy from data using infinitely divisible kernels. IEEE Transactions on Information Theory, 2014. 10 [6] Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information bottleneck. Theoretical Computer Science, 411(29):26962711, 2010. [7] DeepSeek AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. [8] Tishby, N. and Zaslavsky, N. Deep Learning and the Information Bottleneck Principle. arXiv preprint arXiv:1503.02406. [9] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. [10] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model? arXiv preprint arXiv:2504.13837, 2025. [11] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models are Unsupervised Multitask Learners. OpenAI Technical Report, 2019. [12] Skean, O., Osorio, J.K.H., Brockmeier, A.J., and Giraldo, L.G.S. DiME: Maximizing Mutual Information by Difference of Matrix-Based Entropies. arXiv preprint arXiv:2302.13949, 2023. [13] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness. Language modeling is compression. arXiv preprint arXiv:2309.10668, 2024. [14] Oscar Skean and Md Rifat Arefin and Dan Zhao and Niket Patel and Jalal Naghiyev and Yann LeCun and Ravid Shwartz-Ziv Layer by Layer: Uncovering Hidden Representations in Language Models arXiv preprint arXiv:2502:02013. [15] Ravid Shwartz-Ziv and Naftali Tishby Opening the Black Box of Deep Neural Networks via Information arXiv preprint arXiv:1703.00810 [16] Zeyuan Allen-Zhu and Yuanzhi Li Physics of Language Models: Part 3.1, Knowledge Storage and Extraction arXiv preprint arXiv:2309.14316 [17] Yuzhen Huang and Jinghan Zhang and Zifei Shan and Junxian He Compression Represents Intelligence Linearly arXiv preprint arXiv: 2404. [18] Oscar González, Yury Sokolov, Giri Krishnan, Jean Erik Delanois, Maxim Bazhenov Can sleep protect memorise from catastrophic forgetting? eLife 9:e51005 [19] Tadros, T., Krishnan, G.P., Ramyaa, R., Bazhenov, Sleep-like unsupervised replay reduces catastrophic forgetting in artificial neural networks Nature Communications, 13, 7742 (2022) [20] Yu, F., Arora, H.S., and Johnson, M. Iterative Graph Alignment. arXiv preprint arXiv:2408.16667 (2024). [21] Fangyuan Yu. Scaling LLM pre-training with vocabulary curriculum. arXiv preprint arXiv:2502.17910, 2025. [22] Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A.C., Korbak, T., and Evans, O. The Reversal Curse: LLMs Trained on \"A is B\" Fail to Learn \"B is A\". arXiv preprint arXiv:2309.12288 (2024). 11 [23] Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Vlado Boza, Jiacheng You, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the NanoGPT baseline, 2024. https://github.com/KellerJordan/modded-nanogpt."
        },
        {
            "title": "A Appendix",
            "content": "Theorem 3 (Minimum Probability Entropy Bound). Let be discrete random variable with sample space Ω, where Ω = n. Suppose there exists constant α (cid:0)0, 1 (cid:3) such that (X = x) α for all Ω. Then the entropy of is bounded below by: H(X) (1 α(n 1)) log2 (1 α(n 1)) (n 1)α log2(α). Furthermore, for sufficiently large and small α such that β = αn 1, this bound approximates to: H(X) β log2(n) Proof. Under the constraint that (X = x) α for all Ω, the entropy H(X) = (cid:88) xΩ (x) log2 (x) is minimized when the distribution is as imbalanced as possible: one outcome has the highest allowed probability, and all others are assigned the minimum α. The worst-case distribution is: (x1) = 1 α(n 1), (xi) = α for = 2, . . . , n. The resulting entropy is: H(X) = (1 α(n 1)) log2 (1 α(n 1)) (n 1)α log2(α). For small α and large such that αn 1, we approximate have: 1 α(n 1) 1 αn. Substituting this into the expression: H(X) (1 αn) log2(1 αn) αn log2(α) = (1 αn) log2(1 αn) αn log2(αn) + αn log2(n) = h(αn) + αn log2(n) where h(p) = log2 (1 p) log2(1 p) is the binary entropy function. Since h(αn) 0 and log2(1/α) log2(n) for α 1/n, we get: H(X) αn log2(n) = β log2(n) which completes the proof. Corrolary 1 (Entropy Lower Bound for Finite Discrete Random Variables). Let be discrete random variable with finite support Ω, where Ω = n, and assume that (X = x) > 0 for all Ω. Then there exists constant β (0, 1] such that: H(X) β log2 Ω. Proof. Let ε := minxΩ (X = x), which exists and is strictly positive since is discrete with finite support. By the Minimum Probability Entropy Bound (Theorem 3), we have: H(X) (1 ε(n 1)) log2 (1 ε(n 1)) (n 1)ε log2(ε). For small ε and large n, this approximates to: H(X) εn log2 (cid:19) , (cid:18) 1 ε which is linear in = Ω. Setting β := εn, the result follows: H(X) β log2 Ω."
        }
    ],
    "affiliations": []
}