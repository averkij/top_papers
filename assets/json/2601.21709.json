{
    "paper_title": "Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis",
    "authors": [
        "Qingyue Yang",
        "Jie Wang",
        "Xing Li",
        "Yinqi Bai",
        "Xialiang Tong",
        "Huiling Zhen",
        "Jianye Hao",
        "Mingxuan Yuan",
        "Bin Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce \\textbf{Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations} from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 9 0 7 1 2 . 1 0 6 2 : r Published as conference paper at ICLR WHY ATTENTION PATTERNS EXIST: UNIFYING TEMPORAL PERSPECTIVE ANALYSIS Qingyue Yang1*, Jie Wang1, Xing Li2, Yinqi Bai1, Xialiang Tong2, Huiling Zhen2, Jianye Hao3, Mingxuan Yuan2, Bin Li1 1MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China 2Huawei Technologies Co., Ltd. 3College of Intelligence and Computing, Tianjin University yangqingyue@mail.ustc.edu.cn, li.xing2@huawei.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Attention patterns play crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack unifying explanation. To bridge this gap, we introduce Temporal Attention Pattern Predictability Analysis (TAPPA), unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations from temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https: //github.com/MIRALab-USTC/LLM-TAPPA."
        },
        {
            "title": "INTRODUCTION",
            "content": "Attention patterns matter for both LLM training and inference (Jiang et al., 2024; Li et al., 2025; Yang et al., 2025). Prior studies have shown that attention heads exhibit structured and reusable forms, such as streaming heads, retrieval heads, and sink heads (Xiao et al., 2023; 2024). Understanding why such patterns emerge is critical for deeper conceptual understanding of the attention mechanism and can directly inform the design of architectures and inference strategies that improve efficiency and robustness, for example, cache compression and pruning. substantial body of recent research has empirically analyzed attention behavior. Prior analyses typically focus on single phenomenon, for example, the attention sink at the first token (Gu et al., 2024) or diagonal traces linked to high-frequency components of RoPE (Barbero et al., 2025). Other studies categorize heads by functional roles, such as retrieval and streaming (Xiao et al., 2023; 2024). Despite these advances, it remains unclear what factors determine which attention pattern head will adopt under the same attention formulation. Our goal is to uncover unifying underlying mechanism that explains the emergence of these diverse patterns. *This work was done when Qingyue Yang was an intern at Huawei. Project lead. Corresponding author. Email: jiewangx@ustc.edu.cn. 1 Published as conference paper at ICLR 2026 Figure 1: Overview of Temporal Attention Pattern Predictability Analysis (TAPPA) Framework. Left: Theoretical discoveries. Query self-similarity (q-similarity) affects the predictable and unpredictable patterns. Within the periodic sequential pattern, the slash interval is affected by the joint effect of queries, keys and RoPE. Right: Q-similarity is applied to downstream tasks and achieves consistant improvements. To address this gap, we adapt temporal view of auto-regressive inference and analyze how attention evolves over time. During inference, transformer LLM generates each token from the previously generated sequence, so the hidden states and attention scores across steps can be regarded as temporal series. We then isolate the source of temporal variation in attention along the time axis. Conditioned on the past keys being fixed, changes in the attention distribution across steps are determined by the evolution of the query vector. In this interaction, few embedding channels may dominate the inner product (Sun et al., 2024; Liu et al., 2024), which determines the shape of the attention pattern. Figure 1 provides an illustration of how changes in queries and dominant embedding channels reshape the attention pattern. Guided by the temporal view, we propose Temporal Attention Pattern Predictability Analysis (TAPPA) , unified framework that interprets attention patterns through the temporal behavior of queries and the response of the RoPE channels. We view the sequence of query vectors and the associated attention distributions as time series and characterize them using the notion of continuity. We mathematically show that temporal continuity of queries, measured by their self-similarity, is the key factor distinguishing predictable patterns, characterized by clear regularities, and unpredictable patterns that lack stable regularities. Within the predictable regime, we further provide theoretical conditions for three representative patterns with the joint effect of queries, keys, and RoPE. Reaccess patterns, where an attention head repeatedly focuses on small set of tokens, require high query self-similarity and favorable initial query-key geometry. Sequential patterns, which appear as diagonals, are driven by high self-similarity in both queries and keys. In this case, we provide sufficient condition that does not require attributing the phenomenon exclusively to high-frequency components (Barbero et al., 2025). Seasonal patterns, which periodically repeat focus, arise when input periodicity combines with the periodic nature of dominant embedding channels. Since the computing attention from queries, keys, and RoPE is common design in transformer-based models, TAPPA both unifies diverse attention patterns and is broadly applicable across LLMs. To validate TAPPA, we evaluate it on downstream tasks. Prior works have shown that attention patterns are closely linked to models representational capacity (Li et al., 2025; Xiao et al., 2024) and can guide compression. Stable temporal behavior indicates redundant or predictable attention allocation, which can be exploited for selective retention or pruning. Building on this view, we focus on two complementary compression settings: KV cache compression for stored states and LLM pruning for model weights. In both cases, simple metric q-similarity consistently outperforms baselines, demonstrating that these principles are practically useful. In summary, our contributions are as follows: (1) We introduce TAPPA, which provides the first systematic analysis of the shapes of attention patterns from unifying temporal perspective, analyz2 Published as conference paper at ICLR Figure 2: TAPPA explains the formation of sparse attention patterns from temporal continuity perspective. We first establish the fundamental Predictable and Unpredictable patterns in Sec. 4. We then detail the conditions that form the Re-access (Sec. 5.1), Sequential (Sec. 5.2), Seasonal (Sec. 5.4), and Periodical Sequential (Sec. 5.3) patterns in their dedicated sections. ing unpredictable patterns alongside three predictable types: re-access, sequential, and seasonal. (2) Theoretically, we demonstrate that stable patterns emerge from the continuity of queries and keys combined with the RoPE mechanism. (3) We identify periodic sequential diagonals and explain them as consequence of the RoPE rotation period of the dominant channel. (4) We apply insights derived from TAPPA to downstream tasks, including KV cache compression and LLM pruning, achieving accuracy improvements."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 ATTENTION PATTERNS The sparse nature of attention mechanisms in Large Language Models (LLMs) is well-documented, giving rise to distinct, recurring patterns. Prior work has largely focused on identifying these patterns and using them for inference optimization. For instance, one widely discussed pattern is the attention sink, where high attention scores are consistently assigned to the initial tokens (Xiao et al., 2023), attracting significant research interest and analysis from various perspectives (Gu et al., 2024; Yu et al., 2024; Cancedda, 2024). Xiao et al. (2023) also highlighted the importance of attention to recent tokens, which form distinct diagonal trace in the attention map. The structured nature of these patterns has been widely exploited for KV cache compression and inference optimization by various methods, such as Minference (Jiang et al., 2024), H2O (Zhang et al., 2024), SnapKV (Li et al., 2024), DuoAttention (Xiao et al., 2024), and KVTuner (Li et al., 2025). Alongside these structured patterns, other works have identified retrieval heads (Wu et al., 2024; Xiao et al., 2024). These heads appear to scan the entire context for semantically relevant information, resulting in seemingly random attention maps that are crucial for long-context reasoning and factuality (Xiao et al., 2024). However, these observations have remained largely fragmented, lacking unifying theory to explain the co-existence and emergence of these diverse patterns. 2.2 THE ROLE OF POSITIONAL ENCODING growing body of work has sought mechanistic explanation for these patterns by examining the role of Rotary Positional Embeddings (RoPE) (Su et al., 2024). Research has shown direct link between RoPEs frequency components and specific pattern shapes. For instance, high-frequency components in RoPE have been demonstrated to be responsible for the formation of diagonal or previous-token patterns (Barbero et al., 2025). Conversely, other studies suggest that low-frequency components, or specific outlier channels with large magnitudes, may contribute to the emergence of attention sinks by creating rotational offset that favors certain positions (Jonasson, 2025). While these studies provide crucial insights into how positional encoding shapes attention, they often analyze RoPEs effects in isolation, without fully modeling its interaction with the dynamic content of the query and key vectors. 3 Published as conference paper at ICLR"
        },
        {
            "title": "2.3 THE INFLUENCE OF INPUT DYNAMICS",
            "content": "A parallel line of research investigates how the properties of the input tokens themselves influence attention patterns. AttentionPredictor (Yang et al., 2025) proposed that the temporal continuity of queries is key driver for pattern formation, though it did not provide deep mathematical analysis or consider the interplay with RoPE. Other works have corroborated the importance of input features, suggesting that attention sinks may arise from specific query-key angular relationships that are independent of position (Gu et al., 2024). Similarly, the continuity of queries between layers and constant massive channels of keys has also been noted (Lee et al., 2024; Liu et al., 2024), hinting at the inherent temporal consistency within the model. However, this line of inquiry has yet to be formally connected with the rotational effects of RoPE to provide complete picture. In this work, we bridge the gap between these latter two perspectives. We propose TAPPA, unifying theoretical framework that explains how input dynamics and positional encoding together influence attention patterns. Specifically, we demonstrate that variations in query self-similarity over time, when coupled with the rotational mechanics of RoPE, can mathematically account for the diverse patterns observed in prior works."
        },
        {
            "title": "3 BACKGROUND",
            "content": "Attention Mechanism. At the decoding step t, let the query be qt Rd, the key matrix = [k1, . . . , kT ] RT with kj Rd, and the unnormalized logits at RT , (1) where Rtj is the Rotary Positional Embedding (RoPE) operator that rotates vector kj by relative phase proportional to (t j). at,j = Rtjkj, The attention distribution is then At = softmax(at). (2) Since the softmax function is monotonic with respect to the logits, it preserves their relative order across positions. Therefore, for clarity, our discussion focuses on the logits at , and the resulting conclusions directly extend to the final attention distribution At. RoPE. RoPE encodes relative position information by applying channel-wise 2D rotations to pairs of embedding dimensions. For feature pair (m, + d//2) at position m, the rotation is Rn,m = (cid:19) (cid:18)cos(nθm) sin(nθm) cos(nθm) sin(nθm) , (3) where θm = c2m/d is the frequency of the m-th channel, is the hidden dimension, and is hyperparameter. While the original RoPE paper (Su et al., 2024) proposed pairing adjacent dimensions, this half-split pairing scheme is adopted by large-scale models like Llama and Qwen2 for greater computational efficiency. Thus, for query qt and key ki, the RoPE-augmented attention score on channel is t,i = q(m) a(m) Rti,m k(m) , (4) where q(m) = (qt,2m, qt,2m+1). Decomposition View of Attention. Using the RoPE formulation, the attention logits at,i between qt and ki can be decomposed channel-wise. Let qt = (cid:76)M m=1 k(m) , where each pair q(m) R2 corresponds to frequency channel with angular frequency θm = c2m/d. Then and ki = (cid:76)M m=1 q(m) , k(m) at,i = (cid:88) m= q(m) k(m) (cid:16) cos ϕ(m) t,i + (i t)θm (cid:17) , (5) t,i denotes the angle between q(m) where ϕ(m) and k(m) . This decomposition highlights how each frequency channel contributes additively to the overall attention score, and how temporal shifts (i t) are modulated by channel-dependent phases θm. Published as conference paper at ICLR 2026 RoPE Key Property. RoPE satisfies relative-position identity: which ensures that attention depends only on the relative distance (t i), not absolute positions. mRn = Rmn, (6) Attention Patterns. It is well-established that the attention mechanism is sparse and shows various patterns. In this work, we focus on these sparse attention patterns, especially in Llama-3.18B (Dubey et al., 2024) and Qwen-2.5-7B (Yang et al., 2024a) with GSM8K (Cobbe et al., 2021) and AIGC (SoftAge-AI, 2024) datasets."
        },
        {
            "title": "4 WHY PREDICTABLE AND UNPREDICTABLE ATTENTION PATTERNS EXIST",
            "content": "Previous works mainly analyze and utilize attention patterns, including retrieval/streaming heads or A-shape/vertical-slash/block-sparse patterns, from the functionality or geometric morphology view. In contrast, TAPPA provides new and unifying time-series analysis perspective to theoretically understand the existence of diverse attention patterns (Figure 2), utilizing the underlying attention mechanisms. Under TAPPA, attention patterns fall into two temporal categories: predictable and unpredictable. Predictable patterns exhibit temporal continuity across decoding steps or along the temporal dimension, where the indices of high attention scores evolve smoothly over time. Unpredictable patterns display irregular jumps with little temporal consistency. This distinction matters because temporal stability enables inference optimization: stable patterns can be anticipated and efficiently compressed in the KV cache, while unpredictable ones resist such treatment. Empirically, retrieval attention heads exemplify the unpredictable case. Their attention often jumps across the entire context in seemingly random fashion (Wu et al., 2024; Xiao et al., 2024; Li et al., 2025), which is crucial for retrieving semantically relevant information but undermines predictability. Predictable patterns, by contrast, correspond to heads that consistently attend to locally structured or repeatedly accessed tokens, reflecting stable model behaviors that are exploitable for compression and acceleration. In TAPPA, the key differentiator behind these two regimes is query self-similarity, namely the similarity between queries at different time indices. When successive queries remain close in representation space, attention indices change smoothly and produce predictable attention maps. When queries drift strongly, the inequalities that define structured patterns are violated. Even with RoPE relative rotations, attention can jump unpredictably. To capture this distinction, we introduce quantitative measure of query continuity, termed q-similarity. In Appendix F.3, we further study the distribution of q-similarity across layers, heads, models, and datasets, and show that high-continuity heads are common but not universal. High q-similarity correlates with stable, predictable heads, while low q-similarity leads to retrieval-like, unpredictable behavior. Figure 3 shows the attention patterns of the two models with high and low q-similarity scores. It can be seen that patterns with high q-similarity are more stable, while patterns with low q-similarity are more random. Proposition 4.1. Let qt, qt+1 Rd be consecutive queries, = [k1, . . . , kT ] the key matrix, and define the logits at,j = Rtjkj, at+1,j = t+1Rt+1jkj. If qt+1 qt has large norm and is not orthogonal to all rotated keys {Rt+1jkj}, then the difference between the logit vectors at and at+1 is necessarily large. In particular, there exist constants c1, c2 > 0 such that at+1 at c1qt+1 qt c2. Proposition 4.1 demonstrates that while low q-similarity leads to more random patterns, high qsimilarity is necessary condition for predictable ones. In summary, q-similarity provides quantitative indicator of whether an attention head behaves in predictable or unpredictable manner. In the following sections, the theoretical analysis focuses on the predictable heads. Published as conference paper at ICLR 2026 Figure 3: Attention patterns at high and low Query similarity on the Llama and Qwen models. Stable patterns emerge under high similarity, whereas low similarity results in random patterns. There are random bright dots of critical keys in the second and fourth figures."
        },
        {
            "title": "5 PREDICTABLE ATTENTION PATTERNS",
            "content": "In this section, we provide temporal perspective analysis on predictable attention patterns, which rely on the temporal continuity of queries. The re-access pattern occurs when queries are highly self-similar, with low-frequency RoPE components helping to maintain alignment with fixed keys. We also discuss how this analysis relates to the conditions described in prior work (Gu et al., 2024). Sequential patterns arise from the combination of high query and key similarity and the relative position property of RoPE. In some cases, periodic sequential patterns appear. We provide clear calculation for the spacing between adjacent periods and verify it experimentally by varying the location of the dominant RoPE channel and the RoPE base parameter. Finally, we analyze seasonal pattern with periodical queries and keys. These predictable patterns are useful for LLM inference acceleration. Methods that exploit such temporal regularities, including Minference (Jiang et al., 2024), H2O (Zhang et al., 2024) and SnapKV (Li et al., 2024) can compress the KV cache with little loss in LLM performance, which empirically supports the claim that temporal stability is an important signal for effective KV compression (Jiang et al., 2024; Zhang et al., 2024; Li et al., 2024). 5.1 RE-ACCESS PATTERN The re-access pattern describes repeated attention to small set of key tokens, appearing as vertical lines in the attention map and often referred to as attention sink (Xiao et al., 2023). Prior work has attributed this phenomenon to query continuity (Yang et al., 2025) or to the small angle between the first key and all queries (Gu et al., 2024), while others observed its correlation with low-frequency RoPE rotations (Jonasson, 2025). However, these explanations are partial, and TAPPA provides unified account of why they align under the same mechanism. We propose that the stability of reaccess pattern relies on two factors: (1) high self-similarity of consecutive queries, which prevents attention scores from drifting, and (2) the low-frequency components of RoPE, which preserve alignment between queries and fixed keys even as time increases. Theorem 5.1 (Vertical Stability of Attention). Suppose the channel-wise decomposition (Background, Eq. 5) holds for the attention logits at,i. Assume that the queries evolve continuously in the sense that qt+1 qt ε, while all keys ki remain fixed between steps and + 1. Further assume the existence of dominant low-frequency channel whose weight wm dominates the other channels, and whose RoPE frequency θm is small. Then the per-key differences at+1,i at,i are uniformly small, and the attention logits are vertically stable. When queries vary little over time or decoding steps, the only source of temporal change in equation 5 is the RoPE-induced phase (i t)θm. If dominant channel with small θm controls the sum, then shifting (cid:55) + 1 changes the cosine term only marginally, hence at+1,i at,i. This yields vertically aligned attention weights. The empirical validation of the dominant-channel assumption for the re-access head is in Appendix F.1. Connection to Attention Sink in the First Token. well-known empirical phenomenon is the attention sink, which typically appears at the first token position. Prior work Gu et al. (2024) observed that queries and keys at the initial position tend to have very small angle, and attributed this alignment as the cause of the sink. TAPPA analysis provides complementary explanation: from 6 Published as conference paper at ICLR 2026 Figure 4: High self-similarity in Query (Q) and Key (K) matrices results in sequential attention patterns. An example from Qwen-2.5 head (left) with high and self-similarity (0.99 and 0.96) produces strong diagonal pattern in the attention map (far right). This phenomenon is also observed in Llama-3.1 (center right). the decomposition in Equation 5, when the angle ϕ(m) is small, the cosine term cos(ϕ(m) t,i + (i t)θm) is close to 1. Consequently, the logit contribution from that channel approaches its maximum possible value q(m) k(m) , making the overall attention score at,i large. This alignment effect explains why high attention scores often emerge at positions where and are nearly aligned, particularly at the first token. t,i between q(m) and k(m) 5.2 SEQUENTIAL PATTERN Sequential patterns exhibit shifting focus across tokens, typically progressing step by step along the sequence. The diagonal slash often observed near the main diagonal is commonly attributed to positional heads, which attend to tokens at fixed relative offsets. We argue that the sequential pattern arises from the combined effect of both high q-similarity and k-similarity and the relative-position property of RoPE. Theorem 5.2 (Sequential Patterns under High Self-similarity). Under the RoPE relative-position encoding, suppose queries and keys both exhibit high self-similarity, in the sense that for sufficiently small ε > 0. Then the attention logits satisfy qt+1 qt ε, ki+1 ki ε at+1,i+1 at,i Cε, for some constant > 0. Consequently, the attention logits exhibit approximate shift-invariance along the (+1, +1) diagonal, giving rise to sequential patterns in the attention map. RoPE encodes relative positions through rotations. When queries and keys vary little across steps, this rotation structure preserves their interactions under simultaneous shift. As result, attention scores propagate along the (+1, +1) diagonal, producing sequential (slash-like) patterns. Empirical Results. High self-similarity in both query and key representations is sufficient condition for the emergence of Sequential patterns. Figure 4 illustrates the patterns of heads with high query similarity and high key similarity, all of which clearly exhibit diagonal structures. Empirical results for separating the roles of input dynamics and RoPE are in Appendix F.2. 5.3 PERIODICITY OF SEQUENTIAL PATTERNS Empirically, we sometimes observe multiple parallel diagonal lines in attention maps, with roughly constant spacing between adjacent lines (periodic sequential pattern). We attribute this periodicity to the rotation angle of the dominant RoPE channel. Theorem 5.3 (Periodic Sequential Pattern from Dominant RoPE Channel). If sequential pattern arises and the corresponding key exhibits massive channel at index m, then the spacing between adjacent diagonals is determined by the rotation frequency of that channel: = 2π θm = 2π 2m/d. (7) Published as conference paper at ICLR 2026 (a) (b) (c) (d) (e) Figure 5: An illustration of how RoPE configuration affects attention patterns. (a) and (b) show sequential pattern with dominant channel at = 124. In (c) and (d), we manually change the dominant channel to higher frequencies (m = 2 and = 5), which causes periodic diagonals to emerge. In (e), we change the RoPE base from = 1, 000, 000 to = 100, 000 with = 5. Intuition. When the massive channel is m, the attention score is dominated by that component: at,j q(m) k(m) (cid:16) cos ϕ(m) t,j + (j t)θm (cid:17) . This term is cosine function of the relative offset (jt) with angular frequency θm . Consequently, the diagonal lines in the attention map exhibit regular repetition with period = 2π/θm , as given in equation 7. Since θm = c2m/d, higher channel indices correspond to lower angular frequencies and therefore to greater spacing between adjacent diagonals. We validate the theoretical mechanism with controlled manipulations on learned key vectors. We consider two axes of intervention: (i) relocating the massive channel across different indices, and (ii) varying the RoPE base hyperparameter c. Relocating the massive channel. We first analyze key vector kj whose attention map exhibits single diagonal, as shown in Figure 5 (b). We identify its massive channel at index = 124 as shown in Figure 5 (a). Given the Qwen2.5 RoPE hyperparameters (base = 1, 000, 000, dimension = 128), this high-index channel corresponds to an extremely low angular frequency. Its theoretical period is = 2πc2m/d 2.4 106, value so large that no repetition can be observed within practical context window. To demonstrate the relationship between channel frequency and periodicity, we experimentally relocate this massive channel to different target indices m, recomputing the RoPE-augmented attention for each case. The resulting attention maps with = 2 and = 3, visualized in Figure 5 (c) and (d), show that periodic diagonals emerge as the massive channel is moved to lower-index, higher-frequency positions. Specifically, as the channel index decreases, the angular frequency θm increases, shortening the period and making the diagonals denser. This confirms the first finding of TAPPA: observable periodic diagonals require the keys massive channel to reside in high-frequency (low-index) position. Furthermore, we observe that even for high-frequency channels, the diagonal patterns fade over long distances. This occurs because the self-similarity between queries and keys naturally diminishes as their relative distance increases, which disrupts the continuity required to sustain the pattern. Varying the RoPE base c. Independent of the channel index, the choice of RoPE base also controls the periodicity. To isolate this effect, we keep the same dominant channel = 5 and repeat the above procedure for different values of the base (e.g., = 1, 000, 000 and 100,000 in Figure 5 (d) and (e)). Since the channel frequency is given by θm = c2m/d, decreasing directly increases θm and hence reduces the diagonal period = 2π/θm. 5.4 SEASONAL PATTERN Seasonal patterns arise when attention maps repeat with fixed periodicity. This periodicity can manifest along either the temporal axis or the spatial axis. Due to the periodicity of the hidden states, the periodicity of queries and keys is often aligned, so temporal and spatial repetitions typically occur simultaneously and share the same period. We argue that the underlying cause of the seasonal pattern is that queries and keys exhibit periodicity, which is preserved and sometimes amplified by 8 Published as conference paper at ICLR 2026 Table 1: The evaluation results on the LongBench dataset across different KV cache budgets. Single-DocumentQA Multi-DocumentQA Summary Few-shot Learning Synthetic Code Budget Method NrtvQA Qasper MF-en HotpotQA 2WikiMQA Musique GovReport QMSum MultiNews TREC TriviaQA SAMSum PCount PRe Lcc RB-P Average Full Full 31.06 45.43 53.78 1024 2048 StreamingLLM 25.64 27.76 30.76 30.47 31.82 29.47 H2O SnapKV PyramidKV CAKE TAPPA StreamingLLM 26.64 29.57 30.95 30.54 30.88 30.77 H2O SnapKV PyramidKV CAKE TAPPA StreamingLLM 27.40 29.65 30.99 31.13 30.79 30.70 H2O SnapKV PyramidKV CAKE TAPPA 27.48 29.01 42.03 42.15 42.99 42.66 30.77 36.15 44.74 43.64 44.95 44.94 36.91 39.53 45.06 45.06 45.83 45.69 33.30 44.75 52.13 52.17 51.65 51. 35.59 45.94 52.58 52.73 52.38 52.14 37.85 48.64 53.15 53.80 53.57 53.06 Full Full 29.05 43. 52.52 512 1024 2048 StreamingLLM 19.82 26.83 28.94 27.33 28.97 28.97 H2O SnapKV PyramidKV CAKE TAPPA StreamingLLM 22.72 26.45 29.24 29.34 29.47 29.64 H2O SnapKV PyramidKV CAKE TAPPA StreamingLLM 23.18 28.55 29.11 28.39 29.08 29.18 H2O SnapKV PyramidKV CAKE TAPPA 25.40 34.17 40.70 38.04 39.46 39.40 29.42 34.94 41.61 38.60 42.71 43. 36.93 40.20 41.53 43.36 43.35 44.03 35.57 41.43 50.40 50.38 50.40 50.46 31.47 40.49 50.93 50.17 52.12 51.53 45.64 47.45 52.05 51.83 51.92 52.24 55.04 47.36 52.78 54.15 54.67 54.37 54. 47.31 54.43 55.09 55.29 55.49 55.43 49.23 54.23 55.25 55.78 55.50 55.49 57.59 43.24 50.80 55.80 55.73 54.80 55.48 43.57 48.63 57.60 55.67 56.11 56.90 45.30 53.49 57.17 56.75 57.20 57. 47.14 40.06 44.31 46.14 45.25 46.89 46.64 42.03 44.81 46.83 46.29 46.99 46.99 44.66 46.50 46.56 46.59 46.60 46.68 47.05 39.18 41.83 44.21 44.28 44.70 44. 38.18 42.02 45.50 45.12 46.41 45.86 40.10 44.44 46.26 45.60 45.77 45.77 31.29 24.80 29.22 30.51 30.60 30.73 30.81 24.17 29.04 30.37 31.28 30.82 31.16 24.31 29.28 30.78 30.89 30.47 30. 30.24 18.59 22.82 27.83 27.12 28.02 28.02 17.99 22.27 29.39 27.82 29.13 29.43 19.74 27.00 30.69 30.50 30.26 30.19 Llama-3.1-8B 34. 23.16 24.71 24.98 25.00 26.36 25.48 25.81 27.64 27.87 27.53 28.68 28.72 28.57 29.97 30.24 30.25 31.12 30.54 25.33 20.80 23.11 24.24 24.33 24.94 24.57 21.31 23.31 24.57 24.50 24.91 24. 21.67 23.68 24.63 24.82 24.67 24.65 Qwen2.5-7B 31.78 25.45 25.57 24.42 22.24 23.90 23.99 24.33 25.67 25.63 23.26 26.86 26.64 28.49 28.93 29.49 26.90 29.35 29. 23.64 19.07 21.35 22.74 21.86 22.35 22.87 19.47 20.90 23.06 22.16 23.12 22.57 20.57 22.66 23.23 23.03 23.46 23.31 27.49 22.85 24.56 24.65 24.51 25.27 24. 25.66 26.47 25.99 26.00 26.39 26.65 27.12 27.21 27.32 27.35 27.16 27.12 23.96 22.33 22.03 21.07 19.54 20.74 20.72 22.46 22.41 22.26 20.55 22.72 23.00 23.50 23.79 23.64 23.38 23.59 23. 72.50 57.50 54.50 64.00 62.50 63.50 62.50 63.50 62.00 68.50 68.00 69.00 69.50 67.50 68.50 70.50 71.00 70.50 71.00 72.50 58.50 60.50 66.50 66.00 55.00 55. 61.00 59.00 65.50 62.50 67.50 65.50 68.00 63.50 71.50 71.00 69.00 69.00 91.25 87.60 91.38 92.05 91.24 91.54 92.35 88.84 91.43 92.03 92.09 91.94 91.95 90.98 91.48 91.48 91.65 91.48 91. 89.47 71.13 84.67 86.56 86.36 86.91 87.02 87.53 87.83 88.92 86.85 89.23 89.48 74.41 88.50 89.17 88.22 89.37 89.47 43.81 42.08 42.10 42.04 41.67 42.52 42. 42.76 43.14 42.60 41.75 42.60 42.38 42.49 43.06 42.37 42.62 43.48 43.00 45.61 32.29 45.86 44.14 43.69 44.92 44.66 43.79 45.07 44.65 43.26 45.46 45.24 33.06 46.08 45.49 45.01 45.37 44. 6.00 6.50 6.36 6.08 5.95 6.33 6.25 6.50 6.36 6.50 6.05 6.00 6.00 6.12 6.11 6.00 6.00 6.00 6.00 8.50 8.00 8.00 8.00 8.00 8.00 8. 8.50 8.50 8.50 8.50 8.50 8.00 8.00 8.00 8.00 8.00 8.00 8.00 99.50 97.00 99.00 99.50 99.50 99.50 99.50 88.00 99.00 99.50 99.50 99.50 99.50 87.00 99.50 99.50 99.50 99.50 99. 63.36 56.65 60.51 62.30 62.62 61.58 62.30 64.56 61.31 62.24 63.00 62.35 62.65 64.99 63.06 63.06 63.28 63.27 63.23 64.93 51.28 54.33 54.90 53.89 54.30 57. 53.47 55.74 56.50 55.44 56.89 58.84 55.32 56.91 56.86 56.44 56.64 58.80 100.00 59.61 67.12 23.00 95.50 99.50 99.00 99.50 100. 34.00 98.48 100.00 100.00 100.00 100.00 18.00 100.00 100.00 100.00 100.00 100.00 46.18 59.11 59.17 57.59 57.06 59.04 55.17 59.77 58.16 57.76 59.11 60.04 54.50 61.06 60.92 61.36 59.35 60.95 49.01 64.66 64.22 62.09 64.26 64. 58.43 63.88 65.30 61.99 64.79 66.06 53.73 67.50 67.94 67.37 67.88 68.06 49.06 41.75 44.39 46.92 46.59 47.19 47.21 42.73 46.11 47.95 47.69 48.13 48.43 44.39 47.30 48.32 48.51 48.43 48. 48.87 33.55 44.07 45.51 45.58 45.56 45.78 37.38 44.14 47.27 45.85 47.70 47.72 37.07 46.95 48.12 48.17 48.31 48.46 RoPE through its relative-position encoding. Although the query condition does not exhibit temporal continuity, the pattern remains predictable over time and is therefore predictable pattern. Theorem 5.4 (Seasonal Attention Pattern from Periodic Keys and Dominant RoPE Channel). Suppose the query and key vectors are approximately periodic with interval L, in the sense that qt+L qt εq, ki+L ki εk for sufficiently small εq, εk > 0, and that this interval is in near resonance with the dominant RoPE frequency, i.e., (cid:12) (cid:12) δ for some positive integer and sufficiently small δ > 0. Then the attention logits satisfy (cid:12)L θm 2kπ(cid:12) at+L,i at,i C1(εq + εk) + C2δ, at,i+L at,i C3(εq + εk) + C4δ for some constants C1, C2, C3, C4 > 0, and therefore exhibit seasonal pattern with period along both query and key dimensions. The seasonal pattern arises from two combined effects. First, the approximate periodicity of the input queries and keys induces corresponding periodicity in the attention map. This type of periodicity is common in structured data, such as looking at corresponding elements in consecutive lines of code or data records. Second, when the interval is in resonance with the dominant RoPE frequency, the relative-position rotations align with the input periodicity, reinforcing the repetition and producing stronger, more regular pattern. This dual conditionperiodic keys amplified by RoPE resonanceexplains the emergence of clean, regularly spaced attention pattern. The observed interval is therefore determined primarily by the period of the input data itself."
        },
        {
            "title": "6 DOWNSTREAM TASKS\n6.1 KV CACHE COMPRESSION",
            "content": "To demonstrate the practical value of TAPPA, we apply q-similarity, metric derived from TAPPA, to the KV cache compression task, which aims to reduce the memory footprint of key-value caches during large language model inference while maintaining model accuracy. Based on TAPPA, lower 9 Published as conference paper at ICLR 2026 Table 2: Comparison of TAPPA with ShortGPT under the same pruning ratios. Model Method Pruned Piqa Hellaswag Winogrande Arc Easy Average (%) Llama-2-7B Llama-3.1-8B Qwen-2.5-7B ShortGPT with TAPPA ShortGPT with TAPPA ShortGPT with TAPPA ShortGPT with TAPPA ShortGPT with TAPPA ShortGPT with TAPPA 31% 31% 34% 34% 28% 28% 31% 31% 39% 39% 43% 43% 63.33 63. 60.83 60.45 66.65 64.69 64.96 65.51 63.17 62.89 60.83 60.88 45.94 50. 42.11 48.53 42.41 55.09 37.69 42.22 41.83 41.80 36.13 39.87 61.40 63. 60.38 62.43 58.72 63.77 58.41 62.51 50.59 51.93 47.43 49.72 47.26 45. 44.15 42.55 46.25 52.90 42.76 46.59 44.32 45.03 39.77 43.94 54.48 55. 51.87 53.49 53.51 59.11 50.96 54.21 49.98 50.42 46.04 48.60 query similarity indicates higher likelihood of retrieval patterns. Since retrieval patterns attend to scattered and unpredictable key positions, they generally require larger cache budget to preserve critical information (Xiao et al., 2024; Li et al., 2025). Therefore, we leverage q-similarity as proxy signal to dynamically guide the per-layer cache budget allocation under limited memory resources, improving inference efficiency while maintaining model accuracy. We provide the experiment details in Appendix G.1, and additional studies on the sensitivity to α and alternative similarity formulations for q-similarity are reported in Appendix H. Results. As shown in Table 1, our method consistently outperforms CAKE and the other four baselines across three different budget settings. These results confirm that q-similarity derived from TAPPA effectively reflects the likelihood of retrieval patterns, and by allocating more cache budget to layers exhibiting lower query similarity, we are able to preserve critical information more effectively, enabling efficient KV cache compression. More results comparing with DuoAttention (Xiao et al., 2024) and Expected Attention (Devoto et al., 2025) are in Appendix and J. 6.2 LLM PRUNING To reduce the parameter size of LLMs and accelerate inference, structured pruning, which removes entire components such as layers, has emerged as promising approach. Our specific goal is to design more effective proxy metrics to guide whole-layer pruning, so as to achieve higher accuracy under the same compression ratio. Based on TAPPA, higher q-similarity indicates more stable and predictable patterns. Such stability suggests that the layer extracts less novel information, making it more dispensable. Consequently, layers with higher query similarity can be pruned with less impact on model performance, while low similarity layers, which are more likely to host retrieval-oriented and task-critical behaviors, are preserved. We provide the experiment details in Appendix G.2, and the sensitivity study of β is reported in Appendix H. Results. As shown in Table 2, our method consistently outperforms ShortGPT across different pruning ratios and models, validating the effectiveness of combining Block Influence with q-similarity as proxy signal for structured layer pruning. These results on LLM pruning validate our hypothesis regarding the connection between q-similarity and stable, predictable patterns. Layers with higher qsimilarity exhibit greater redundancy due to their stability, and can therefore be pruned with minimal impact on overall model performance. Results about more baselines are in Appendix G.2.1."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we introduced TAPPA, unifying framework to systematically analyze the diverse attention patterns within large language models. We demonstrated that the distinction between predictable and unpredictable patterns can be explained by the temporal self-similarity of queries. Our theoretical analysis further elucidated that stable, predictable patterns arise from the combined effects of query-key continuity and Rotary Positional Embeddings (RoPE), providing clear explanation for phenomena like periodic sequential diagonals. The practical value of TAPPA is confirmed by applying its insights to downstream tasks. simple metric inspired by TAPPA successfully improved performance in both KV cache compression and LLM pruning, validating our framework. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research does not involve any personally identifiable information. All datasets used are publicly available and widely adopted in the community, and we have verified that their licenses permit research use. In accordance with the ICLR Code of Ethics, we ensure that our work adheres to principles of fairness, transparency, and responsible AI research. We also disclose that LLMs were used for text polishing, while all conceptual contributions and validation remain the responsibility of the authors in Appendix K."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We will provide open access to all source code, configuration files, and preprocessing scripts, together with detailed instructions to reproduce the main experimental results. All datasets employed are publicly available, and we specify the exact versions and preprocessing steps. Collectively, these resources and specifications enable reliable and faithful reproduction of our results."
        },
        {
            "title": "REFERENCES",
            "content": "Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=vXxardq6db. arXiv:2401.15024. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veliˇckovic. Round and round we go! what makes rotary positional encodings useful? In The Thirteenth International Conference on Learning Representations, 2025. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. URL https://arxiv.org/abs/1911. 11641. Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, and Wen Xiao. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling, 2025. Nicola Cancedda. Spectral filters, dark signals, and attention sinks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 47924808, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Alessio Devoto, Maximilian Jeblick, and Simon Jegou. Expected attention: Kv cache compression by estimating attention from future queries distribution. arXiv preprint arXiv:2510.00636, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Published as conference paper at ICLR 2026 Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. When attention sink emerges in language models: An empirical view. arXiv preprint arXiv:2410.10781, 2024. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024. Andre Jonasson. Rotary outliers and rotary offset features in large language models. arXiv preprint arXiv:2503.01832, 2025. Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. {InfiniGen}: Efficient generative inference of large language models with dynamic {KV} cache management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pp. 155172, 2024. Xing Li, Zeyu Xing, Yiming Li, Linping Qu, Hui-Ling Zhen, Wulong Liu, Yiwu Yao, Sinno Jialin Pan, and Mingxuan Yuan. Kvtuner: Sensitivity-aware layer-wise mixed precision kv cache quantization for efficient and nearly lossless llm inference, 2025. URL https://arxiv.org/ abs/2502.04420. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. Xin Men, Mingyu Xu, Qingyu Zhang, Qianhao Yuan, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. ShortGPT: Layers in large language models are more redundant than you expect. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 2019220204, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. Ziran Qin, Yuchen Cao, Mingbao Lin, Wen Hu, Shixuan Fan, Ke Cheng, Weiyao Lin, and Jianguo Li. CAKE: Cascading and adaptive KV cache eviction with layer preferences. In The Thirteenth International Conference on Learning Representations, 2025. Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. URL https://arxiv.org/abs/1907. 10641. SoftAge-AI, 2024. URL https://huggingface.co/datasets/SoftAge-AI/ multi-turn_dataset. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Published as conference paper at ICLR 2026 Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality, 2024. URL https://arxiv.org/abs/2404. 15574. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv:2412.15115, 2024a. Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, et al. Attentionpredictor: Temporal pattern matters for efficient llm inference. arXiv preprint arXiv:2502.04077, 2025. Yifei Yang, Zouying Cao, and Hai Zhao. LaCo: Large language model pruning via layer collapse. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 6401 6417, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.372. URL https://aclanthology.org/2024. findings-emnlp.372/. Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid Shaikh, and Yingyan Celine Lin. Unveiling and harnessing hidden attention sinks: Enhancing large language models without training In International Conference on Machine Learning, pp. 57659 through attention calibration. 57677. PMLR, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024. 13 Published as conference paper at ICLR"
        },
        {
            "title": "A PROOF OF UNPREDICTABLE PATTERN",
            "content": "Proposition 4.1. Let qt, qt+1 Rd be consecutive queries, = [k1, . . . , kT ] the key matrix, and define the logits at,j = Rtjkj, at+1,j = t+1Rt+1jkj. If qt+1 qt has large norm and is not orthogonal to all rotated keys {Rt+1jkj}, then the difference between the logit vectors at and at+1 is necessarily large. In particular, there exist constants c1, c2 > 0 such that at+1 at c1qt+1 qt c2. Proof. Let := qt+1 qt. For each position j, the change in the logit is aj = at+1,j at,j = (q)Rt+1jkj + (cid:0)Rt+1j Rtj (cid:1)kj. Denote the first term by T1,j and the second by T2,j. Step 1: Bounding the RoPE difference term. Since Rm is an orthogonal rotation, its operator norm is 1, so by the triangle inequality, Rt+1j Rtjop Rt+1jop + Rtjop 2. If we assume the keys are bounded such that kj BK for all j, then T2,j qt Rt+1j Rtjop kj 2qtBK. Step 2: Lower-bounding the query difference term. The first term can be written as T1,j = (cid:12) , Rt+1jkj(cid:12) (cid:12) (cid:12). The condition that is not orthogonal to all rotated keys implies that the inner product is not always zero. We formalize this by assuming there exists an index and constant α > 0 such that the normalized vectors have significant projection: (cid:12) , Rt+1j kj /kj (cid:12) (cid:12) (cid:12) α. This condition essentially states that the direction of the query change aligns with at least one rotated key. Under this condition, and assuming minimum key norm kj Bk,min, we get T1,j αBk,minq. Step 3: Combining both terms. Using the bounds for the two terms at index j, the reverse triangle inequality gives aj T1,j T2,j αBk,minq 2qtBK. Since the infinity norm of vector is the maximum of the absolute values of its components, we have at+1 at = max aj aj αBk,minq 2qtBK. This establishes the proposition with constants c1 = αBk,min and c2 = 2qtBK. This completes the proof. PROOF OF RE-ACCESS PATTERN Theorem 5.1(Vertical Stability of Attention): Suppose the channel-wise decomposition (Eq. equation 5) holds for the attention logits at,i. Assume that the queries evolve continuously in the sense that qt+1 qt ε, while all keys ki remain fixed between steps and t+1. Further assume the existence of dominant low-frequency channel whose weight wm dominates the other channels, and whose RoPE frequency θm is small. Then the per-key differences at+1,i at,i are uniformly small, and the attention logits are vertically stable. Proof. We derive an explicit uniform bound for the per-key logit difference and show how it depends on the query increment and channel parameters. 14 Published as conference paper at ICLR 2026 Using the channel decomposition from Eq. equation 5, write for each channel wm := q(m) k(m) , := q(m) t+1 k(m) , and ψm := ϕ(m) t,i + (i t)θm, := ϕ(m) ψ t+1,i + (i (t + 1))θm."
        },
        {
            "title": "Define the logit difference",
            "content": "t,i := at+1,i at,i."
        },
        {
            "title": "Direct subtraction yields the exact identity",
            "content": "t,i = (cid:88) m=1 (w wm) cos ψ + (cid:88) m=1 wm (cid:0) cos ψ cos ψm (cid:1). (8) We bound the two sums on the right-hand side separately. Let ε := qt+1 qt. First sum. By the triangle inequality and the definition of wm, (cid:12) (cid:12) (cid:12) (cid:88) m=1 (w wm) cos ψ (cid:12) (cid:12) (cid:12) (cid:88) m= wm = (cid:88) m=1 k(m) (cid:12) (cid:12)q(m) t+1 q(m) (cid:12) (cid:12). Since the Euclidean norm is 1-Lipchitz, (cid:12) (cid:12) q(m) t+1 q(m) (cid:12) (cid:12)q(m) t+1 q(m) qt+1 qt = ε. Hence (cid:12) (cid:12) (cid:12) (cid:88) (w wm) cos ψ m=1 (cid:12) (cid:12) (cid:12) ε (cid:88) m=1 k(m) . (9) Second sum. Use the inequality cos cos u v, so (cid:12) (cid:12) cos ψ cos ψm (cid:12) (cid:12) ψ ψm = (cid:12) (cid:12)ϕ(m) t+1,i ϕ(m) t,i θm (cid:12) (cid:12) ϕ(m) t+1,i ϕ(m) t,i + θm. t+1 q(m) , q(m) . In the 2D RoPE subspace, write q(m) To control the angular difference, let rm := min{q(m) t+1 } and assume rm > 0, and denote = rtut and q(m) ε(m) := q(m) t+1 = rt+1ut+1 with ut = ut+1 = 1 and let ϕ(m) := ϕ(m) and q(m) t+1 . Projecting both vectors onto the circle of radius rm can only decrease their Euclidean distance while preserving the angle, so by elementary planar geometry, we have (cid:18) ϕ(m) 2 t,i be the angle between q(m) t+1,iϕ(m) ε(m). 2rm sin (cid:19) Therefore (cid:18) ε(m) 2rm where we used ε(m) qt+1 qt = ε in the last inequality. t,i = ϕ(m) 2 arcsin t+1,i ϕ(m) ϕ(m) (cid:19) π 2 ε(m) rm π 2 ε rm , Therefore Summing over yields (cid:12) (cid:12)wm (cid:0) cos ψ cos ψm (cid:1)(cid:12) (cid:12) wm (cid:16) π 2 ε rm + θm (cid:17) . (cid:12) (cid:12) (cid:12) (cid:88) m=1 wm (cid:0) cos ψ cos ψm (cid:1)(cid:12) (cid:12) (cid:12) π 2 ε (cid:88) m=1 wm rm + (cid:88) m=1 wmθm. (10) 15 Published as conference paper at ICLR 2026 Combine bounds. bound"
        },
        {
            "title": "Define",
            "content": "Inserting equation 9 and equation 10 into equation 8 gives the explicit uniform t,i ε (cid:88) m=1 k(m) + π 2 ε (cid:88) m=1 wm rm + (cid:88) m=1 wmθm. (11) δ := ε (cid:88) m=1 k(m) + π 2 ε (cid:88) m=1 wm rm + (cid:88) m=1 wmθm. Thus t,i δ for every token index i. Conclusion and asymptotics. Under the theorem hypotheses the keys are bounded and there exists dominant channel with wm much larger than the remaining {wm}m=m , while rm is bounded away from zero and θm is small. In that regime, the two terms proportional to ε in δ vanish as ε 0, and the last term is small because the dominant channels frequency θm is small and the remaining channels carry only small total weight. Consequently δ can be made arbitrarily small by taking ε 0, θm 0, and by increasing the dominance of wm over other channel weights. Therefore the per-key differences t,i = at+1,i at,i are uniformly small, which proves vertical stability."
        },
        {
            "title": "C PROOF OF SEQUENTIAL PATTERN",
            "content": "Theorem 5.2(Sequential Patterns under High Self-similarity): Under the RoPE relative-position encoding, suppose queries and keys both exhibit high self-similarity, in the sense that for sufficiently small ε > 0. Then the attention logits satisfy qt+1 qt ε, ki+1 ki ε at+1,i+1 at,i Cε, for some constant > 0. Consequently, the attention logits exhibit approximate shift-invariance along the (+1, +1) diagonal, giving rise to sequential patterns in the attention map. Proof. Recall the attention logit at,i := Rtiki, where is the RoPE rotation for relative offset . By the RoPE identity we have R(t+1)(i+1) = Rti, hence at+1,i+1 = t+1Rtiki+1. Therefore, the difference can be written as at+1,i+1 at,i = (qt+1 qt)Rtiki+1 + Rti(ki+1 ki). Taking absolute values and applying the CauchySchwarz inequality gives (cid:12) (cid:12)at+1,i+1 at,i (cid:12) (cid:12) qt+1 qt Rtiki+1 + qt Rti(ki+1 ki) = qt+1 qt ki+1 + qt ki+1 ki, where the last equality uses that each is orthogonal (rotation), hence Rv = v. Now impose the high self-similarity hypothesis in the rigorous form qt+1 qt ε, ki+1 ki ε for some ε > 0. Further assume the query/key vectors are uniformly norm-bounded, i.e. there exist constants Q, > 0 with qt and ki for all relevant t, i. Then (cid:12) (cid:12) ε ki+1 + qt ε ε(K + Q). (cid:12) (cid:12)at+1,i+1 at,i Setting := + yields the claimed bound (cid:12) (cid:12)at+1,i+1 at,i (cid:12) (cid:12) ε. Thus, under the stated assumptions, the attention logits are approximately shift-invariant along the (+1, +1) diagonal (with error at most Cε), which produces the sequential diagonal structure in the logit map. 16 Published as conference paper at ICLR"
        },
        {
            "title": "D PROOF OF PERIODIC SEQUENTIAL PATTERN",
            "content": "Theorem 5.3 (Periodic Sequential Pattern from Dominant RoPE Channel): If sequential pattern arises and the corresponding key exhibits massive channel at index m, then the spacing between adjacent diagonals is determined by the rotation frequency of that channel: = 2π θm = 2π 2m/d. (12) Proof. From the decomposition view of attention, the attention logits can be written as sum over channels: at,i = (cid:88) m=1 q(m) k(m) cos (cid:0)ϕ(m) t,i + (i t)θm (cid:1). By assumption, channel is massive, meaning its contribution to at,i dominates all other channels: q(m) k(m) q(m) k(m) for all = m. Hence, the logits are approximately at,i q(m) k(m) cos (cid:0)ϕ(m) t,i + (i t)θm (cid:1). Consider positions and i+T . Assuming that the magnitudes k(m) across consecutive tokens forming the sequential pattern, the attention pattern repeats whenever and angles ϕ(m) vary slowly t,i (i t)θm (i + t)θm (mod 2π), which yields = 2π θm . By the definition of RoPE, θm = c2m/d, and substituting = gives = 2π θm = 2π c2m/d. Therefore, the interval between adjacent diagonals in the attention map is exactly determined by the rotation frequency of the dominant channel, as claimed."
        },
        {
            "title": "E PROOF OF SEASONAL PATTERN",
            "content": "Theorem 5.4 (Seasonal Attention Pattern from Periodic Keys and Dominant RoPE Channel): Suppose the query and key vectors are approximately periodic with interval L, in the sense that qt+L qt εq, ki+L ki εk for sufficiently small εq, εk > 0, and that this interval is in near resonance with the dominant RoPE frequency, i.e., (cid:12) (cid:12)L θm 2kπ(cid:12) (cid:12) δ for some positive integer and sufficiently small δ > 0. Then the attention logits satisfy at+L,i at,i C1(εq + εk) + C2δ, at,i+L at,i C3(εq + εk) + C4δ for some constants C1, C2, C3, C4 > 0, and therefore exhibit seasonal pattern with period along both query and key dimensions. 17 Published as conference paper at ICLR and denote the 2 2 rotation matrices induced by RoPE at positions and with angular frequency Proof. We again use the channel-wise RoPE decomposition. For each channel m, let R(m) R(m) θm. We define the post-RoPE query and key components as q(m) := R(m) := R(m) By construction, RoPE is an orthogonal transformation, so q(m) The logit contributed by channel can be written as dot product and k(m) = k(m) . k(m) = q(m) k(m) q(m) , . t,i = q(m) a(m) , k(m) , at,i = (cid:88) m= a(m) t,i . t,i () (cid:12) u + v v. We first bound the variation of the dominant channel along the query dimension. For arbitrary vectors u, u, v, v, we use the standard dot-product inequality (cid:12)uv uv(cid:12) (cid:12) and = = k(m) t+L , = q(m) Applying () with = q(m) gives = (cid:12) t+L , k(m) (cid:12)q(m) t+L,i a(m) q(m) a(m) t+L q(m) q(m) k(m) . . Using the definition of q(m) t+L R(m) t+L q(m) t+L q(m) (cid:0)q(m) Taking norms and using orthogonality of R(m) t+L yields t+L q(m) It remains to control q(m) t+L q(m) q(m) q(m) The first term is controlled by the assumed L-periodicity of the queries: q(m) (cid:1) + (cid:0)R(m) = R(m) = R(m) t+L R(m) t+L R(m) t+L q(m) t+L q(m) + (cid:13) (cid:0)R(m) (cid:13) , k(m) q(m) (cid:1)q(m) (cid:1)q(m) we have (15) (13) (14) (cid:13) (cid:13). (cid:12) (cid:12) t+L . t t t t i εq. q(m) t+L q(m) For the second term, we use the near-resonance condition. By the definition of RoPE, R(m) , where R(m) R(m) R(m) Lθm 2kπ δ means that R(m) the identity. For planar rotation by angle γ, we have R(γ) = 2 sin(γ/2) γ, so (cid:13) = (cid:13) (cid:13) t+L = is rotation by angle Lθm in the channel-m plane. The hypothesis is in fact rotation by an angle of magnitude at most δ around I(cid:1)q(m) t+L R(m) (cid:13) (cid:0)R(m) (cid:13) (cid:1)q(m) L t (cid:13)R(m) R(m) (cid:0)R(m) q(m) (cid:13) (cid:13) δ q(m) . (16) Combining equation 15 and equation 16 gives t+L q(m) q(m) Substituting this into equation 13 and recalling k(m) εq + k(m) t+L,i a(m) a(m) k(m) t,i εq + δ q(m) . = k(m) q(m) yields δ =: () 1 εq + () 2 δ. An entirely symmetric argument, exchanging the roles of and and using the L-periodicity of the keys k(m) εk, shows that i+L k(m) t,i+L a(m) a(m) () 4 > 0 depending only on the norms of q(m) 3 εk + () 4 δ t,i 3 , () for some constants () Finally, recall that channel is assumed to be massive: its contribution q(m) dominates the contributions of all other channels. The residual variation coming from non-dominant channels {m = m} is therefore uniformly bounded and can be absorbed into the constants C1, . . . , C4. Renaming the constants and noting that εq + εk εq and εq + εk εk, we obtain the bounds stated in Theorem 5.4: . k(m) i and k(m) at+L,i at,i C1(εq + εk) + C2δ, at,i+L at,i C3(εq + εk) + C4δ. This shows that the dominant component of the attention logits approximately repeats every steps along both query and key dimensions, giving rise to seasonal pattern with period L. 18 Published as conference paper at ICLR"
        },
        {
            "title": "F EMPIRICAL SUPPORT",
            "content": "F.1 EMPIRICAL VALIDATION OF THE DOMINANT-CHANNEL ASSUMPTION OF RE-ACCESS PATTERN (a) (b) Figure 6: Empirical validation of the dominant-channel assumption for re-access head. (a) is an attention heatmap of the re-access pattern. (b) plots the RoPE-channel weights of attention at the sink position (dark vertical stripe), showing that single low-frequency channel accounts for most of the total weight. Theorem 5.1 assumes that the attention logits of re-access heads are dominated by single lowfrequency channel. To directly examine this assumption, we perform simple spectrum analysis on head whose attention map exhibits clear re-access pattern in Figure 6(a). For this head, we focus on the key position corresponding to the re-access stripe, namely the attention sink. We decompose the query and key vectors into = D/2 RoPE channels, where each channel groups the two feature dimensions that share the same RoPE frequency. For every channel m, we aggregate its contribution over the decoding steps and then normalize the resulting values so that they sum to 1. This gives one-dimensional spectrum {pm}M 1 m=0 . Figure 6(b) plots the weight of each attention channel. The horizontal axis is the RoPE channel index (0 < ), and the vertical axis is the normalized channel weight pm, i.e., the relative contribution of each channel to the attention logits at the sink position. We observe highly concentrated pattern: single channel carries about pm 51% of the total mass, while the remaining channels form long tail with much smaller weights. The dominant channel lies in the low-frequency half of the RoPE spectrum, consistent with the dominant low-frequency channel assumption used in Theorem 5.1. Together, these observations provide direct empirical evidence that, for the re-access heads we analyze, the attention logits are indeed governed by single low-frequency channel. F.2 DISENTANGLING QUERY DYNAMICS AND ROPE IN SEQUENTIAL PATTERN. To empirically separate the roles of input dynamics and RoPE, we conduct controlled ablation on single attention head that exhibits clear sequential pattern. For this head, the average cosine similarity between consecutive queries is approximately 0.99, and the full model (with RoPE enabled) produces an almost perfectly smooth diagonal attention pattern. We construct three variants using the same head and the same input sequence (Figure 7): 1. High q-similarity with RoPE (full model). In the original model, both the queries and keys have high temporal self-similarity, and RoPE is applied as usual. The resulting attention map shows clean, nearly translation-invariant diagonal stripe: as increases, the high-attention region shifts along the (+1, +1) direction with very little distortion. This behavior is consistent with the analysis of TAPPA, which predicts that when both qt and ki vary smoothly in time, RoPE induces approximate shift-invariance along the main diagonal. 19 Published as conference paper at ICLR 2026 Figure 7: Ablation of query dynamics and RoPE on head with strong sequential pattern. Left: original head with high q-similarity and RoPE enabled. Middle: high q-similarity without RoPE, which retains rough, broken diagonal with additional vertical streaks. Right: RoPE with perturbed q, where the diagonal tendency is overlaid with scattered, unpredictable activation spikes. 2. High q-similarity without RoPE. In the second variant, we disable RoPE for this head by replacing the rotation matrices with identity, while keeping the original queries and keys unchanged. The attention map still exhibits diagonal bias, reflecting the strong local similarity in the queries and keys. However, the diagonal becomes noticeably rough: it is broken into segments and is superposed with vertical streaks. This indicates that high q-similarity alone is sufficient to encourage local, near-diagonal attention, but it does not guarantee the smooth, globally shift-invariant diagonal pattern observed in the full model. 3. Perturbed q-dynamics with RoPE. In the third variant, we keep RoPE enabled but mildly perturb the temporal dynamics of the queries by randomly resampling their time indices within the same sequence. This reduces the average cosine similarity between consecutive queries from 0.99 to 0.97, while leaving the keys and RoPE parameters unchanged. The resulting attention map still contains visible diagonal tendency, but it is now overlaid with many scattered, seemingly random activation spots. In other words, the attention pattern becomes mixture of predictable diagonal component and unpredictable spikes. Across these three conditions, we observe that: (i) high q-similarity without RoPE yields coarse, locally diagonal pattern, (ii) RoPE with perturbed q-dynamics produces partially diagonal but noticeably more unpredictable attention, and (iii) only when smooth q-dynamics and RoPE are both present do we obtain the clean, stable sequential pattern seen in the full model. This ablation supports the mechanism identified by TAPPA that sequential attention patterns arise from the joint effect of smooth input dynamics and RoPE, and that these two factors play complementary roles: input dynamics control whether the pattern is predictable or unpredictable, while RoPE shapes the predictable component into regular, shift-invariant diagonal structure. F.3 Q-SIMILARITY DISTRIBUTION To better understand the behavior of q-similarity, we compute per-head q-similarity scores across all layers for two models (Llama-3.1 and Qwen2.5-7B) on two representative datasets (GSM8K and AIGC). As shown in Figure 8, we have following observations: Overall high q-similarity supporting temporal continuity. Across all heads and layers, the average q-similarity is high for both models (around 0.80 for Llama-3.1 and 0.86 for Qwen2.5-7B). This empirically supports the assumption of TAPPA that queries tend to evolve in temporally continuous manner in large portion of the network. Model-specific but layer-structured distributions. Each model exhibits its own characteristic distribution of q-similarity values, indicating that the q-similarity distribution reflects model-specific properties and thus naturally calls for per-model calibration. At the same time, within given model, we observe clear and consistent structure: heads in the same layer have very similar q-similarity scores (forming tight clusters), whereas the average q-similarity differs significantly across layers. 20 Published as conference paper at ICLR Figure 8: Head-wise q-similarity heatmaps for Llama-3.1 and Qwen2.5 on GSM8K and AIGC. For readability, we show only the two decimal digits of each q-similarity value (e.g., 83 denotes q-similarity of 0.83). 21 Published as conference paper at ICLR 2026 This justifies the design choice in TAPPA of operating at the layer level by using layer-wise averages when building downstream metrics and policies. Stable across datasets for the same model, enabling lightweight calibration. For fixed model, the q-similarity distribution is highly consistent across datasets. For Llama-3.1, the average qsimilarity on GSM8K and AIGC differs by only about 0.01. For Qwen2.5-7B, the absolute mean difference between the two datasets is about 0.07, but the overall shape and ranking of layers/heads are very similar. In particular, the relative ordering of heads is largely preserved, so percentile-based selection strategies such as selecting the top x% most continuous heads are unaffected. This indicates that q-similarity has good stability and generalization across datasets, and that only small amount of data is needed to calibrate q-similarity for given model, without requiring separate tuning for each task."
        },
        {
            "title": "G EXPERIMENT DETAILS",
            "content": "G.1 DETAILS FOR KV CACHE COMPRESSION Implementation details. Following CAKE (Qin et al., 2025), we introduce an adjusted per-layer performance score that incorporates q-similarity derived from TAPPA: = Pl + α(1 Sl), where Pl denotes the original layer preference score based on entropy and variance of attention patterns (as defined in Equation (6) of CAKE), Sl is the cosine similarity among queries within recent window, which instantiates q-similarity in TAPPA, and α is hyperparameter controlling the contribution of q-similarity. Formally, (17) Sl = sim(Q[Sw:]), (18) The intuition is that lower q-similarity indicates more random and dispersed attention pattern, which generally requires allocating larger budget. By adjusting Pl with (1 Sl), we bias the score toward layers exhibiting retrieval-like behaviors. Finally, following the allocation rule in CAKE, we normalize the adjusted scores to distribute the total budget across layers: Bl = (cid:80)L1 k=0 Btotal. (19) LLMs, benchmark and baselines. We evaluate our method on Llama-3.1-8B (Dubey et al., 2024) and Qwen2.5-7B (Yang et al., 2024a), using the LongBench (Bai et al., 2024) benchmark, which covers 16 long-context understanding tasks. G.1.1 BASELINES OF KV CACHE COMPRESSION Baselines include StreamingLLM (Xiao et al., 2023), H2O (Zhang et al., 2024), SnapKV (Li et al., 2024), PyramidKV (Cai et al., 2025), and CAKE (Qin et al., 2025). We provide detailed descriptions of these baselines in Appendix G.1.1. In the KV cache compression task, we evaluate our method against five representative baselines. Based on whether the budget allocation across layers is uniform, these baselines can be categorized into Uniform Allocation, represented by StreamingLLM (Xiao et al., 2023), H2O (Zhang et al., 2024), and SnapKV (Li et al., 2024), and Non-Uniform Allocation, represented by PyramidKV (Cai et al., 2025) and CAKE (Qin et al., 2025). StreamingLLM: retains the first and most recent tokens. H2O: prioritizes tokens with high cumulative attention. SnapKV: leverages an observation window at the end of the input to cluster and preserve important KV positions for each head. PyramidKV: allocates larger budgets to lower layers and smaller ones to higher layers with SnapKVs eviction indicator. CAKE: introduces preference-prioritized adaptive allocation strategy, dynamically adjusting budgets across layers. 22 Published as conference paper at ICLR Table 3: Comparison with additional structured pruning baselines on Llama-2-7B. Results of LLMPruner, SliceGPT, LaCo, and ShortGPT are quoted from ShortGPT, and the LLMPruner setting excludes post-training as in ShortGPT. Method Pruning Ratio PIQA HellaSwag WSC BoolQ RACE-H Avg. LLMPruner SliceGPT LaCo ShortGPT TAPPA 27.00% 26.40% 27.10% 27.10% 28.10% 71.22 66.21 69.80 66.43 66. 56.46 50.27 55.69 53.02 55.97 36.54 36.54 40.38 52.46 68.13 55.20 38.32 64.07 74.71 62.17 22.56 21.07 22.61 32.25 33.88 48.40 42.48 50.51 55.77 57.38 G.2 DETAILS FOR LLM PRUNING Implementation details. Building on the Block Influence (BI) metric proposed by ShortGPT (Men et al., 2025), we design an adjusted proxy score: BI = BI + β(1 q), (20) where β is hyperparameter and 1 is an importance score derived from q-similarity in TAPPA. Following ShortGPTs pruning pipeline, we use the PG19 dataset (Rae et al., 2019) as calibration set. First, we collect hidden states and queries from each layer while running inference on the calibration data. Next, we compute the proxy scores for all layers based on the adjusted BI score. Finally, we sort the layers in ascending order of scores and remove those with the lowest scores. The number of pruned layers can be adjusted to balance efficiency gains and accuracy preservation. LLMs, benchmark, and baselines. We evaluate our method on Llama-2-7B (Touvron et al., 2023), Llama-3.1-8B (Dubey et al., 2024) and Qwen2.5-7B (Yang et al., 2024a). Using the procedure described above, we first evaluate how redundant each layer is and decide which layers are to be pruned. Then we perform zero-shot task classification on common sense reasoning datasets: PIQA (Bisk et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019) and ARC-easy (Clark et al., 2018) at different pruning ratios. In the main experiments, we compare our method with ShortGPT as the primary baseline. We further include additional structured pruning baselines in Appendix G.2.1. We list the removed layers in Table 4 of Appendix G.2.2. G.2.1 COMPARISON WITH ADDITIONAL STRUCTURED PRUNING BASELINES To provide more comprehensive comparison for structured LLM pruning, we additionally consider three representative baselines, namely LLMPruner (Ma et al., 2023), SliceGPT (Ashkboos et al., 2024), and LaCo (Yang et al., 2024b). LLMPruner is gradient-based structured pruning method that removes non-critical coupled structures to preserve model functionality. SliceGPT is posttraining pruning scheme that compresses the model by applying principal component analysis to hidden states in each layer and reducing the embedding dimension. LaCo is structured pruning approach based on layer collapse, which gradually merges similar layers while using threshold to avoid excessive collapsing. Following the evaluation protocol in ShortGPT (Men et al., 2025), we report results on PIQA, HellaSwag, WSC, BoolQ, and RACE-H. The results of LLMPruner, SliceGPT, LaCo, and ShortGPT are quoted from ShortGPT, and the LLMPruner setting excludes post-training as in ShortGPT for fair comparison. As shown in Table 3, our method achieves the best average performance under higher pruning ratio. In particular, it improves the average score over ShortGPT while maintaining strong accuracy on WSC and RACE-H, indicating that the proposed pruning criterion remains effective in more challenging structured pruning regimes. G.2.2 LIST OF REMOVED LAYERS In the LLM Pruning downstream task, we evaluated our pruning method on different LLMs and pruning ratios. we list the removed layers in Table 4. Published as conference paper at ICLR 2026 Table 4: Removed layers for different benchmark models, using PG19 as calibration dataset. Model Method Pruning Ratio Removed Layers Llama-2-7B Llama-3.1-8B Qwen2.5-7B"
        },
        {
            "title": "ShortGPT\nTAPPA",
            "content": "31% 31% 34% 34% 28% 28% 31% 31% 39% 39% 43% 43% 21, 22, 23, 24, 25, 26, 27, 28, 29, 30 19, 21, 22, 23, 24, 25, 26, 27, 28, 29 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29 20, 22, 23, 24, 25, 26, 27, 28, 29 21, 22, 23, 24, 25, 26, 27, 28, 29 20, 21, 22, 23, 24, 25, 26, 27, 28, 29 19, 21, 22, 23, 24, 25, 26, 27, 28, 29 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 20 4, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23 4, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, Table 5: Comparison of eight q-similarity formulations on Llama-3.1-8B under the 1024 KV budget on LongBench subsets. Method MF-en HotpotQA QMSum TriviaQA Lcc Avg. sim cosine sim dot sim pearson sim euclidean sim l1 sim angular sim rbf sim kl 52.63 52.57 52.68 52.58 52.57 52.59 52.40 52.54 55.45 55.12 54.84 54.85 54.76 55.05 54.63 55.56 24.59 24.71 24.80 24.76 24.69 24.87 24.43 24.56 92.04 91.87 92.03 91.62 91.79 91.86 91.61 91.48 64.58 64.52 64.91 64.59 64.56 64.82 64.82 64.54 57.86 57.76 57.85 57.68 57.67 57.84 57.58 57."
        },
        {
            "title": "H ADDITIONAL ABLATIONS AND HYPERPARAMETER SENSITIVITY",
            "content": "H.1 ALTERNATIVE SIMILARITY FORMULATIONS FOR Q-SIMILARITY Throughout the paper, we instantiate q-similarity using cosine similarity, which is used to compute the layer-wise score for KV cache compression. To assess whether the downstream gains rely on particular similarity formulation, we replace cosine similarity with seven alternatives and re-evaluate KV cache compression on representative LongBench subsets. Specifically, we consider dot-product similarity, Pearson correlation coefficient, Euclidean distance, L1 distance, angular distance, radial basis function (RBF) kernel similarity, and Kullback-Leibler (KL) divergence. For distance-based and divergence-based measures, lower values indicate higher similarity and are used accordingly in the layer scoring. The performance variation across similarity formulations is small, indicating that the method does not critically depend on particular metric choice. Cosine similarity achieves the best average performance and is therefore used as the default throughout the main experiments. H.2 SENSITIVITY TO α IN KV CACHE COMPRESSION We study the sensitivity to the weighting coefficient α in the adjusted layer score used for KV cache compression. Table 6 reports results on two LongBench subsets under fixed 1024 KV budget. The performance remains stable across wide range of α values, and larger α generally yields slightly better results, consistent with the usefulness of the q-similarity signal. Hyperparameter selection follows lightweight per-model strategy. For each model, we evaluate α {0.1, 1, } on small validation split and select the best α. The selected α is then fixed for all 24 Published as conference paper at ICLR 2026 Table 6: Sensitivity to α on Llama-3.1-8B under the 1024 KV budget on LongBench subsets. The setting α = 0 corresponds to CAKE without q-similarity. The setting α = corresponds to using only q-similarity. α MF-en HotpotQA Avg. 0 (CAKE) 0.1 0.2 0.5 0.8 1 1.5 2 5 10 52.16 52.19 52.17 52.17 52.22 52.14 52.19 52.27 52.11 52.24 52.41 55.43 55.43 55.39 55.39 55.43 55.43 55.42 55.42 55.56 55.54 55.44 53.80 53.81 53.78 53.78 53.83 53.79 53.80 53.84 53.84 53.89 53.93 Table 7: Per-layer computational overhead comparison between q-similarity computation and CAKE under different context lengths. Latency is measured in ms, and memory is measured in MB. Length CAKE Latency Memory q-sim Latency Memory Improvement Latency Memory 4K 8K 16K 32K 0.198 0.308 0.506 0.874 72.12 136.12 264.12 520.12 0.195 0.197 0.199 0. 8.69 8.69 8.69 8.69 2% 36% 61% 78% 88% 94% 97% 98% datasets and KV budgets for that model in the full evaluation. The setting α = corresponds to using only the q-similarity term to rank layers. H.3 COMPUTATIONAL OVERHEAD OF Q-SIMILARITY COMPUTATION This subsection evaluates the runtime and memory overhead of computing q-similarity scores during inference. We measure the per-layer overhead of our q-similarity computation and compare it with CAKE, which maintains attention score-based statistics for eviction. All measurements are conducted on Llama-3.1-8B with window size of 32, under different context lengths up to 32K tokens. As shown in Table 7, the overhead of q-similarity is small and stable across all context lengths. In particular, the per-layer latency stays below 0.2 ms, and the additional memory consumption remains about 8.69 MB. This behavior is expected because q-similarity only computes query similarities within fixed window, so its complexity is effectively independent of the total sequence length. In contrast, attention score-based methods like CAKE update and store per-token statistics to derive eviction signals, so both runtime and memory overhead increase with the context length. At 32K tokens, q-similarity reduces the per-layer latency overhead by 78% and the additional memory consumption by 98% compared to CAKE, highlighting the efficiency advantage of query-based signal for budget allocation. H.4 SENSITIVITY TO β IN LAYER PRUNING We conduct sensitivity study for the weighting coefficient β used in layer pruning. As shown in Table 8, the performance varies moderately across β, with plateau for β [0.2, 0.4]. Identical results for nearby β values occur when the induced pruned layer sets are unchanged. 25 Published as conference paper at ICLR 2026 Table 8: Sensitivity to β on Llama-2-7B under 34% pruning ratio. Identical results for some β values arise when the corresponding pruned layer sets are the same. β 0 0.1 0.2 0.3 0.4 0."
        },
        {
            "title": "Piqa Hellaswag Winogrande Arc Easy Average",
            "content": "60.83 60.83 60.45 60.45 60.45 61.10 42.11 42.11 48.53 48.53 48.53 46.47 60.38 60.38 62.43 62.43 62.43 57.77 44.15 44.15 42.55 42.55 42.55 39.69 51.87 51.87 53.49 53.49 53.49 51.26 For each model, we evaluate β Hyperparameter selection follows per-model strategy. {0.1, 0.3, 0.5} on small validation split and select the best β. The selected β is then fixed for all datasets and pruning ratios for that model in the full evaluation."
        },
        {
            "title": "I COMPARISON WITH DUOATTENTION",
            "content": "In this section, we provide detailed comparison between the q-similarity based method derived from TAPPA and DuoAttention (Xiao et al., 2024), recent baseline that explicitly distinguishes retrieval heads and streaming heads for KV cache compression. I.1 BASELINES AND METHODOLOGY ADAPTATION DuoAttention is an optimization-based method that explicitly identifies retrieval heads via training. It assigns learnable scalar, which we denote as αduo, to each attention head to represent its retrieval importance. To conduct direct comparison between the q-similarity metric derived from TAPPA and DuoAttentions learned importance for the layer-wise budget allocation task, we adapted their scoring mechanism into the same layer-wise budget allocation scheme. Specifically, we calculate the importance score for each layer by averaging the αduo values across all heads in that layer. We then compute the allocated budget Bl for layer using formulation analogous to Eq. 19: Bl = α(l) duo (cid:80)L1 k=0 α(k) duo Btotal, (21) where α(l) the two metrics in identifying layers that require higher KV cache budgets. duo is the average score of layer l. This setup allows us to fairly evaluate the effectiveness of Table 9: Performance comparison with DuoAttention on LongBench using Llama-3.1-8B. For each budget, the best score on each subset is highlighted in bold. Single-DocumentQA Multi-DocumentQA Summary Few-shot Learning Synthetic Code Budget Method NrtvQA Qasper MF-en HotpotQA 2WikiMQA Musique GovReport QMSum MultiNews Full 512 1024 2048 Full DuoAttention TAPPA DuoAttention TAPPA DuoAttention TAPPA 31.06 30.60 29.47 30.13 30.77 30.63 30.70 45. 53.78 43.09 42.66 44.57 44.94 45.39 45.69 51.70 51.63 52.72 52. 53.62 53.06 55.04 54.15 54.53 54.58 55.43 55.49 55.49 47. 45.98 46.64 46.95 46.99 46.52 46.68 31.29 30.51 30.81 31.01 31. 30.32 30.94 34.87 25.62 25.48 28.16 28.72 30.61 30.54 25. 24.50 24.57 24.62 24.90 24.98 24.65 27.49 24.88 24.71 26.22 26. 27.25 27.12 TREC 72.50 62.50 62.50 67.00 69.50 70.50 71. TriviaQA SAMSum PCount PRe Lcc RB-P Average 91.25 92.35 92.35 91.89 91.95 91.49 91.65 43. 41.70 42.42 42.22 42.38 42.74 43.00 6.00 6.08 6.25 6.50 6. 6.00 6.00 99.50 63.36 56.65 99.50 99.50 99.50 99. 99.50 99.50 64.55 64.56 64.78 64.99 64.96 64.93 56.87 57.35 58.84 58. 58.82 58.80 49.06 47.16 47.21 48.11 48.43 48.68 48.73 I.2 POTENTIAL FOR HIGHER COMPRESSION RATIO It is crucial to highlight the fundamental difference in how the two methods categorize attention patterns and the resulting impact on the compression scope. DuoAttention operates on binary premise where it differentiates Streaming Heads that necessitate only sink and recent tokens from 26 Published as conference paper at ICLR 2026 Table 10: Performance comparison of Expected Attention (EA) with and without our q-similarity based budget allocation on LongBench. For each budget, the best score on each subset is highlighted in bold. Single-DocumentQA Multi-DocumentQA Summary Few-shot Learning Synthetic Code Budget Method NrtvQA Qasper MF-en HotpotQA 2WikiMQA Musique GovReport QMSum MultiNews TREC TriviaQA SAMSum PCount PRe Lcc RB-P Average 1024 2048 512 1024 2048 EA 21.53 EA with TAPPA 21.72 EA 23.36 EA with TAPPA 29.17 EA 25.68 EA with TAPPA 30.13 EA 19.30 EA with TAPPA 17.40 EA 18.57 EA with TAPPA 21.39 EA 20.28 EA with TAPPA 23.47 37.72 34. 40.79 38.58 42.58 43.96 24.62 31.09 35.71 35.21 41.08 39.95 40.01 41. 42.72 43.13 47.13 50.20 27.85 34.51 38.93 42.03 44.97 46.55 47.59 46. 52.30 50.30 49.91 50.57 24.63 26.86 36.81 36.92 45.46 45.38 40.99 40. 44.53 45.91 43.40 44.54 23.73 31.39 38.43 36.31 39.47 38.35 Llama-3.1-8B 27.77 27.99 29.66 29.37 31.70 31.25 22.99 22.62 23.90 23.85 23.69 23. Qwen2.5-7B 26.81 28.41 30.07 29.87 31.45 31.47 21.27 20.76 20.68 21. 21.64 21.93 17.09 19.50 18.19 23.78 18.88 23.26 12.20 14.90 18.70 17. 25.36 25.11 26.43 26.68 27.17 27.21 27.37 27.35 23.70 24.39 25.07 25. 25.66 25.42 47.50 45.00 55.50 53.50 62.00 58.50 14.00 30.75 31.50 45. 55.05 55.00 72.82 71.83 88.70 87.30 86.25 86.71 71.42 71.87 82.48 76. 86.91 82.59 35.60 36.94 36.23 36.86 36.96 38.22 35.29 43.00 37.06 43. 38.38 44.39 7.17 7.85 4.79 4.88 7.25 6.92 4.17 6.03 6.25 5. 7.65 6.08 81.50 84.50 88.00 90.50 95.50 95.00 7.50 89.08 94.33 97. 98.25 97.33 52.80 54.18 52.97 55.78 52.00 54.16 19.68 54.49 32.66 52. 34.76 41.84 50.39 49.32 53.42 54.25 51.67 54.52 31.79 44.46 37.93 52. 41.55 55.01 39.37 39.46 42.64 43.40 43.87 44.95 24.25 35.59 36.57 39. 41.12 42.49 Retrieval Heads requiring full history retention. Consequently, its compression efforts primarily focus on heads exhibiting streaming behavior. In contrast, TAPPA provides more detailed categorization. The q-similarity metric derived from TAPPA distinguishes complex Retrieval patterns from variety of regular attention patterns, including Re-access, Sequential, and Seasonal patterns. Crucially, TAPPA identifies these regular patterns as compressible. This effectively expands the scope of compressible heads beyond just streaming heads. By compressing these additional heads that might otherwise be preserved, this strategy could achieve higher compression ratio while maintaining model performance. I.3 EXPERIMENTAL SETUP We compare DuoAttention and TAPPA under strict KV cache budgets. For DuoAttention, the coefficient αduo is set to the official value trained on Llama-3.1-8B and released by the authors.1 All results are evaluated on the LongBench benchmark with 16 subsets. We report performance at KV cache budgets of 512, 1024, and 2048 tokens, and also include the full context baseline. I.4 RESULTS AND ANALYSIS The results are in Table 9. Across all three budgets, the q-similarity based method achieves higher average performance than the DuoAttention based allocation. At the subset level, the advantage is most visible on multihop reasoning and retrieval-oriented tasks. For example, at the 1024 budget, TAPPA improves HotpotQA from 54.58 to 55.43 and TREC from 67.00 to 69.50. At the 2048 budget, the q-similarity based method reaches an average score of 48.73, which is close to the full context baseline of 49.06."
        },
        {
            "title": "J COMPARISON WITH EXPECTED ATTENTION",
            "content": "Expected Attention (Devoto et al., 2025) is training-free KV cache compression method that ranks and prunes KV pairs by analytically estimating how future queries are expected to attend to cached keys. Expected Attention uses uniform layerwise budget allocation and then applies its expected attention-based importance score within each layer to perform KV pruning. To evaluate the compatibility of the budget adjustment strategy derived from TAPPA with other compression frameworks, we integrate the q-similarity based layerwise budget allocation into Expected Attention by replacing the uniform allocation while keeping the original Expected Attention scoring and pruning mechanism unchanged. We use the official hyperparameter settings of Expected Attention from the released implementation and do not tune them for our integration.2 1https://github.com/mit-han-lab/duo-attention.git 2https://github.com/NVIDIA/kvpress 27 Published as conference paper at ICLR 2026 Table 10 reports results on LongBench with 16 subsets under KV budgets of 512, 1024, and 2048. Across both backbones and all budgets, adding TAPPA budget allocation improves the average performance over Expected Attention. Specifically, the improvement is approximately 46.8% on Qwen-2.5 with the 512 KV budget, increasing the average score from 24.25 to 35.59. These results indicate that the proposed q-similarity based temporal signal can serve as plug-in budget allocation component that strengthens Expected Attention without modifying its core expected attention estimation. THE USE OF LARGE LANGUAGE MODELS (LLMS) Large Language Models (LLMs) were employed solely for the purpose of enhancing the linguistic clarity and stylistic refinement of this manuscript."
        }
    ],
    "affiliations": [
        "Huawei Technologies Co., Ltd.",
        "Tianjin University",
        "University of Science and Technology of China"
    ]
}