{
    "paper_title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
    "authors": [
        "Zhengshen Zhang",
        "Hao Li",
        "Yalun Dai",
        "Zhengbang Zhu",
        "Lei Zhou",
        "Chenchen Liu",
        "Dong Wang",
        "Francis E. H. Tay",
        "Sijin Chen",
        "Ziwei Liu",
        "Yuxiao Liu",
        "Xinghang Li",
        "Pan Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height."
        },
        {
            "title": "Start",
            "content": "From Spatial to Actions: Grounding Vision-LanguageAction Model in Spatial Foundation Priors Zhengshen Zhang1,2 Hao Li3 Yalun Dai3 Zhengbang Zhu1 Lei Zhou2 Chenchen Liu2 Dong Wang1 Francis E. H. Tay2 Sijin Chen1 Ziwei Liu3 Yuxiao Liu1,, Xinghang Li4, Pan Zhou5, 1ByteDance Seed, 2NUS, 3NTU, 4THU, 5SMU Corresponding Author, Project Lead"
        },
        {
            "title": "Abstract",
            "content": "Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height. Date: October 21, 2025 Correspondence: liuyuxiao.876@bytedance.com, lixingha23@mails.tsinghua.edu.cn, panzhou@smu.edu.sg Project Page: https://falcon-vla.github.io/ 5 2 0 2 0 2 ] . [ 1 9 3 4 7 1 . 0 1 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in vision-language-action models (VLAs) have significantly advanced the pursuit of generalist robotics, enabling robots to interpret natural language instructions and execute intricate action sequences [3, 5, 13, 17]. Most advanced VLAs are built on 2D foundation models like Vision Language Models (VLMs) [1, 21, 22, 28, 30, 32], aligning 2D images with text and leveraging the strong language understanding of the Large Language Models (LLMs) for information processing. This design provides strong semantic understanding and supports manipulation tasks conditioned on language and camera inputs. 1 Figure 1 We propose FALCON, vision-language-action model that achieves robust 3D spatial understanding by seamless integrating spatially rich tokens and sematic features. FALCON demonstrates exceptional modality transferability by excelling with both RGB-only and multi-modal inputs, superior spatial understanding in tasks involving unseen object sizes, heights and abstract spatial instructions, and strong few-shot generalizability in real-world scenes. The model achieves state-of-the-art performance across diverse range of benchmark evaluations. However, while VLMs operate purely in the 2D domain, VLAs must interact with the 3D physical world. This discrepancy results in critical gap: current VLAs lack reliable 3D spatial understanding, leading to persistent challenges in generalization and adaptability. Specifically, the absence of explicit 3D awareness causes VLAs to struggle in scenarios that require reasoning about geometry, depth, or spatial relations. First, they exhibit limited generalization, failing to transfer robustly to novel scenes, backgrounds, or object variations [42]. Second, they lack adaptability to environmental changes, such as height variations or object scale differences [16]. These limitations now form major bottleneck to developing reliable generalist robotic policies. To address this gap, recent works incorporate 3D information into VLAs [16, 33, 44], most often by providing explicit 3D inputs (e.g., point clouds or depth maps). While effective under ideal conditions, these methods suffer from low modality transferabilitythe ability to function and improve under different input modalities (RGB-only, RGB-D, point clouds, camera pose) without retraining or collapsing. This stems from two fundamental issues. First, acquiring high-quality 3D inputs requires specialized sensors that are expensive and difficult to deploy in practice. Second, many large-scale manipulation datasets (e.g., Open X-Embodiment dataset [27]) lack aligned 3D annotations, limiting scalability. As result, such methods are tied to specific input modalities and break down when those inputs are unavailable. An alternative direction introduces weak 3D cues, e.g., pseudo-depth estimates (like ZoeDepth [2]) or learnable spatial embeddings [29]. However, these approaches face three fundamental limitations. (1) Limited spatial representation. The learnable spatial embeddings provide only weak geometric signals within the highdimensional space of LLMs, failing to capture robust 3D priors necessary for tasks like reasoning about height differences or object sizes in grasping. (2) Lack of modality transferability. While encoding some 3D cues, they cannot effectively exploit higher-quality 3D inputs when available. (3) Challenges in alignment. Concatenating spatial embeddings with text tokens risks disrupting the original visionlanguage alignment. The scarcity of 3D data makes it difficult to properly align modalities, causing embedding drift that degrades zero-shot generalization, especially in tasks requiring high-level reasoning like spatial prompts [29]. Contribution. We propose FALCON (From Spatial to Actions), novel paradigm that injects richer and more representative 3D spatial tokens into VLAs through an improved injection scheme. 2 To solve limitation (1) i.e., weak 3D spatial representation, we leverage insights from spatial foundation models that encode scenes into token sequences for holistic 3D reconstruction. So FALCON adopts broader and richer tokens from these foundation models, and delivers comprehensive spatial information from RGB signals alone, improving robustness when explicit 3D sensors are absent. For limitation (2) of poor modality transferability, we introduce an Embodied Spatial Model that can optionally integrate extra 3D modalities (e.g., depth, poses). Unlike prior methods that mandate specific 3D inputs [16, 33, 44], our design is flexible: when RGB-D cameras or calibrated scenes are available, the model gains additional accuracy; when not, it still performs effectively with RGB-only input. This optional pathway substantially improves transferability by natively absorbing 3D priors from sensors without requiring architectural changes. To overcome limitation (3) of alignment challenges, we draw inspiration from the brains division of labor. The VLM (cerebrum) handles high-level reasoning and semantics, while the action head (cerebellum) manages fine-grained motor control and sensorimotor integration [10, 31]. Motivated by this, we design SpatialEnhanced Action Head that directly incorporates spatial tokens into action decisionsa more natural fit, since precise control depends on detailed spatial cues. This departs from prior approaches that forcibly align spatial and text tokens within VLMs [9, 39]. In this way, FALCON provides (i) robust spatial reasoning, (ii) strong modality transferability, and (iii) principled integration of 3D priors into VLAs. As shown in Fig. 1, extensive experiments across three simulation benchmarks and 11 real-world tasksincluding cluttered-scene manipulation, few-shot adaptation, and spatial capability evaluationdemonstrate that FALCON consistently outperforms existing baselines, achieving state-of-the-art performance with strong robustness and generalization (e.g., spatial-prompt conditioning, height-aware manipulation, object-scale variation)."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 3D-Enhanced Vision-Language-Action Models The development of generalist robot policies has been significantly advanced by VLAs [3, 57, 13, 14, 17, 18, 43], which leverage large-scale pre-trained VLMs to connect visual and linguistic understanding with action generation. For instance, RT-2 [5] and OpenVLA [13] fine-tune VLMs on robot data, representing actions as language tokens. While demonstrating impressive instruction-following capabilities, these methods operate primarily in the 2D domain, lacking explicit mechanisms for 3D geometric perception, which is critical limitation in tasks requiring precise spatial understanding. To address this, recent works have integrated 3D perceptual cues into policy learning. One line of research directly consumes explicit 3D representations like point clouds for action prediction, as seen in PointVLA [16] and GeoVLA [33]. While enhancing geometric awareness, these methods rely on specific 3D sensor configurations, limiting modality transferability when such inputs are unavailable. An alternative strategy embeds 3D features into the VLMs input space, exemplified by 3D-VLA [44], SpatialVLA [29], and Evo-0 [20], but this often disrupts pre-trained vision-language alignment, necessitating costly embodied instruction tuning to recover performance. FALCON overcomes these issues by decoupling spatial processing from the VLM, preserving its semantic integrity while achieving strong modality transferability and robust performance across diverse input conditions."
        },
        {
            "title": "2.2 Spatial Foundation Models",
            "content": "Recent advancements in deep learning have introduced novel alternatives to traditional SfM methods. DUSt3R [38] represents significant deviation from conventional SfM pipelines by predicting point clouds from image pairs without relying on geometric constraints or inductive biases. Unlike traditional SfM, which depends on keypoint matching and geometric optimization, DUSt3R generates predictions in shared coordinate frame, enabling robust reconstruction across diverse scenes. This approach addresses several challenges inherent in classical methods, such as sensitivity to initialization and sparse correspondences. Building on this paradigm, several works have proposed variations with distinct architectural innovations. MASt3R [15] improves the estimation of the pixel-wise correspondence between image pairs, strengthening the efficacy of unconstrained feed-forward models for SfM tasks. CUT3R [37] introduces recurrent formulation of DUSt3R, achieving 3 computational efficiency at the expense of marginal accuracy degradation. More recently, VGGT [36] proposes multi-view architecture that processes multiple images simultaneously, moving beyond pairwise processing to improve reconstruction consistency and robustness. However, their integration into generalist robot policies remains challenging, often requiring complex feature alignment or suffering from information loss when adapted to action spaces."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first formulate the problem of task-oriented, language-guided robot control (Sec. 3.1). We then introduce the overall architecture of FALCON (Sec. 3.2), and detail its two key components: the Embodied Spatial Model (ESM) for 3D geometric representation (Sec. 3.3), and the Spatial-Enhanced Action Head for multimodal fusion and action generation (Sec. 3.4). , . . . , n"
        },
        {
            "title": "3.1 Problem Definition\nWe study the problem of task-oriented robot control, where a robot must interpret visual observations\nt } at time step t and a natural language instruction L to generate an action sequence\nOt = {I 1\nAt = [at, . . . , at+C−1] through a mapping function F(·). Each action ai is a 7D vector encoding the 6-DoF\ngripper pose (e.g., Euler angles) and a binary open/close state, with C denoting the action horizon. Our\nfocus is on table-top manipulation with a robot arm, using inputs from a static side camera I 3rd\n(global\nscene context), a wrist-mounted camera I hand\n(fine-grained object details), or both. Optional depth maps\nDt ∈ RH×W and camera poses P ∈ R7 can further enhance spatial grounding when available but are not\nstrictly required, ensuring robustness across different sensing setups. Formally,",
            "content": "t At = F(Ot, L, Dt, ). (1) This setting spans diverse applications from service robots following language commands to industrial manipulators performing instruction-driven assembly, where robust performance in unstructured environments requires tight integration of semantic understanding and geometric reasoning. To this end, we propose FALCON, generalist robot policy that overcomes limitations of prior VLAs by integrating rich geometric priors from spatial foundation models while flexibly leveraging optional 3D modalities. The result is spatially enhanced VLA that unifies semantic reasoning with fine-grained geometric grounding for robust manipulation across diverse conditions."
        },
        {
            "title": "3.2 Overall Architecture",
            "content": "As illustrated in Fig. 2, FALCON is an end-to-end VLA consists of three core components: (1) 2D VLM for multimodal semantic representation, (2) an ESM for extracting 3D structural features, and (3) Spatial-Enhanced Action Head that combines both streams to generate precise robot actions. At each timestep t, given image observations Ot and language instruction L, FALCON employs pre-trained 2D VLM (e.g., Kosmos-2 [28]) to extract contextualized representation of the scene and task. The visual and textual inputs are tokenized and formed into unified multi-modal sequence. learnable action token tact is appended to it, and the corresponding output hidden state ˆtact RDact , where Dact represents the feature dimension, is extracted as the semantic action representation, encapsulating task-oriented behavior grounded in multi-modal context. In parallel, the ESM processes third-view image 3rd , along with optional geometric inputs such as depth Dt and camera poses , to extract spatial structure representations. Through spatial encoder Espl(), it outputs set of spatial tokens Tspl, encoding global 3D geometric priors essential for scene understanding. Further details are provided in Sec. 3.3. The extracted semantic action token ˆtact and spatial tokens Tspl are then integrated in the Spatial-Enhanced Action Head, collectively guide action generation. We introduce lightweight fusion mechanism that aligns and combines these complementary representations (see Sec. 3.4 for more details), serving as input to an action predictor that outputs action sequences At. This novel design ensures that both high-level semantic Figure 2 Overview of FALCON framework. FALCON integrates 2D VLM (e.g., Kosmos-2), an Embodied Spatial Model, and Spatial-Enhanced Action Head. At timestep t, the VLM processes visual observations Ot and language instructions to produce semantic action token ˆtact. Concurrently, the Embodied Spatial Model encodes third-view image 3rd and optional geometric inputs into spatial tokens Tspl. These are fused by the Spatial-Enhanced Action Head to generate precise robot actions At, enabling robust manipulation through joint semantic and spatial reasoning. context and 3D structural representation are directly incorporated into the policy, significantly improving precision and generalization in manipulation tasks. We provide detailed training paradigms of the FALCON model in Appendix A."
        },
        {
            "title": "3.3 Embodied Spatial Model",
            "content": "Although recently proposed Spatial Foundation Models [36, 38] have shown promising results in image-only reconstruction, they cannot exploit additional 3D modalities commonly available in robotics, such as RGB-D cameras and calibrated poses. To address this limitation, we propose an Embodied Spatial Model that injects 3D conditions (i.e., depth, pose) to build more accurate spatial representations, enabling our action head to predict precise trajectories in space. Specifically, given an input image It at time t, we follow VGGT [36] to encode it into spatial tokens Tspl RM Ds, where is the token number per image and Ds is the token dimension. The image is first tokenized into visual tokens Tvis via DINO [26]. These are then concatenated with learnable camera token tcam RDs and fed into Spatial Encoder Espl(), which consists of cross-attention and self-attention blocks: (Tspl, ˆtcam) = Espl(Tvis, tcam). (2) The resulting spatial tokens Tspl and refined camera tokens ˆtcam are passed to depth predictor and camera predictor, respectively, to achieve accurate scene reconstruction. Notably, the spatial tokens Tspl, which encapsulate rich spatial priors, have shown significant benefits for spatial understanding when integrated into VLMs [9, 39]. 3D Conditions Encoding. First, as shown in Fig. 2, given camera pose R7 input, we encode the intrinsic and normalized extrinsic of the side camera into the GT camera token tgt-cam RDs using an MLP-based camera encoder Ecam(). Given depth Dt RHW and its valid map Mdpt RHW input, we first normalize depth = Dt/Norm(Dt) to handle any depth ranges at train and test time. The normalized depth map is concatenated with its corresponding validity mask Mdpt, enabling us to capture frame-wise incomplete depth information. The resulting tensor is then passed through depth encoder Edpt(), which consists of stack of convolutional layers with kernel size 14 14, effectively partitioning it into sequence of tokens Tdpt that are aligned in size with the image tokens Tspl. The formulation is shown below: 5 tgt-cam = Ecam(tcam) , Tdpt = Edpt([D Mdpt]) , Tdpt RM Ds, (3) where [] denotes channel-wise concatenation. 3D Conditions Injection and Training Strategy. After obtaining the GT pose token tgt-cam and depth tokens Tdpt, our objective is not only to achieve accurate reconstruction through the reconstruction head under 3D-conditioned settings, but also to enhance the geometric grounding ability of the spatial tokens Tspl generated by the Spatial Encoder. To this end, we replace the learnable camera token tcam with the GT camera token tgt-cam, and fuse the depth tokens Tdpt with the image tokens Tvis via element-wise addition. Meanwhile, in robotic applications, depth sensors or accurate camera poses may not always be available (e.g., Open X-Embodiment dataset). To preserve the models ability to reason about spatial structure even without 3D conditions, we design stochastic conditioning strategy. Specifically, we randomly decide whether to inject depth and/or pose during training, formulated as: (cid:16) (Tspl, ˆtcam) = Espl Tvis + bdTdpt, bptgt-cam + (1 bp)tcam (cid:17) , (4) where bd, bp Bernoulli(p). This strategy ensures the model can exploit depth and pose cues when available, while retaining strong image-only spatial reasoning when they are absent. As for supervision, we follow VGGT [36] to adopt depth, point map, and pose losses to formulate multi-task supervision."
        },
        {
            "title": "3.4 Spatial-Enhanced Action Head\nAs illustrated in Fig. 2, the proposed Spatial-Enhanced Action Head integrates geometric representations\nTspl from the ESM with semantic features ˆtact from the VLM, enabling more accurate and spatially-aware\npolicy learning.",
            "content": "Modality Fusion Strategies. To combine these complementary representations, we first compress the spatial tokens Tspl into unified vector tspl RDs through max-pooling operation, then project it into the VLMs feature space to obtain (cid:101)tspl using lightweight MLP adapter D. The aligned spatial feature (cid:101)tspl is then fused with the semantic action token ˆtact through: 1) Cross-Attention Fusion: The action token ˆtact serves as the query, while the projected spatial feature (cid:101)tspl provide key and value inputs. Multi-head attention enables adaptive feature recalibration based on cross-modal relevance. 2) FiLM-Gated Modulation: This method uses the spatial feature (cid:101)tspl to generate affine parameters (γ, β) for feature-wise linear modulation of the action token ˆtact, followed by gating mechanism that learns to blend the modulated semantic and original spatial features. 3) Element-wise Addition: direct, non-parametric combination of the two feature vectors. As illustrated in Fig. 3, empirical evaluation on the manipulation benchmark demonstrates that while cross-attention and FiLM-gated offer richer interaction mechanisms, element-wise addition achieves the highest performance in both seen and unseen environment generalization settings (see Sec. 4.3 for detailed quantitative results and analysis). This approach not only yields superior success rates but also provides greater training stability and computational efficiency. Thus, element-wise addition is adopted to fuse the aligned spatial feature (cid:101)tspl with the semantic action token ˆtact. The overall fusion process is formulated as: Figure 3 Different modality fusion strategies between spatial and semantic action tokens. This dedicated fusion mechanism preserves the pre-trained representation space and generalizable capabilities of the VLM while enriching VLA with geometrically grounded structural awareness. (cid:101)tspl RDact = (tspl) , ffused = ˆtact + (cid:101)tspl. (5) 6 Action Predictor. The fused feature vector is forwarded to an action predictor π to generate robot actions. We explore two distinct architectures for this predictor: An MLP-based predictor directly maps the current fused feature vector to an action output: At = π(f fused). For long-horizon robotic tasks that involve sequential decision-making, we employ predictor based on the long short-term memory (LSTM) network [8, 11] that utilizes history of feature representations. This approach processes the sequence tH+1 fused through the LSTM network, followed by an MLP that produces the final action chunk prediction: At = is obtained through the same feature π(f tH+1 fused fusion process described previously. fused), where denotes the history length. Each , . . . , , . . . , fused fused By integrating spatially rich tokens from the ESM with semantically grounded features from VLM, our model achieves enhanced spatial perception capabilities while retaining strong semantic alignment with language instructions. Moreover, the proposed fusion strategy significantly enhances the performance of the real-world spatial awareness tasks of the policy, as demonstrated in Sec. 4.2."
        },
        {
            "title": "4 Experiments",
            "content": "Benchmarks. For simulation, we evaluate on the widely used benchmarks CALVIN [24] and SimplerEnv [19]. For real-world tasks, we design settings that span from simple interactions (e.g., lifting yellow pepper) to long-horizon, spatially demanding activities (e.g., placing red coke can on the bottom shelf), thereby thoroughly testing robustness and spatial reasoning. Further benchmark details are provided in Appendix D. Implementation Details. FALCON is built on Kosmos-2 [28] VLM backbone (1.6B parameters), combined with 1.0B-parameter Embodied Spatial Model and Spatial-Enhanced Action Head, totaling 2.9B parameters. Training was conducted on cluster of 32 A100 GPUs. Additional training and deployment details are available in Appendix and C. Table 1 Long-horizon robotic manipulation evaluation on the CALVIN benchmark. Method Task Tasks Completed in Row (%) 5 3 2 4 Avg. Len. MCIL [23] RT-1 [4] Robo-Flamingo [18] GR-1 [40] UP-VLA [43] RoboVLM [17] ABCDD 37.3 ABCDD 84.4 ABCDD 96.4 ABCDD 94.9 ABCDD 96.2 ABCDD 96.7 ABCDD 97.2 FALCON (ours) 28.3 ABCD 3DDP [41] 30.4 ABCD MCIL [23] 53.3 ABCD RT-1 [4] 82.4 ABCD Robo-Flamingo [18] 85.4 GR-1 [40] ABCD 93.8 3D Diffuser Actor [12] ABCD ABCD 92.8 UP-VLA [43] 98.0 ABCD RoboVLM [17] ABCD 96.3 Seer-Large [34] ABCD FALCON (ours) 98.4 2.7 61.7 89.6 89.6 92.1 93.0 93.3 2.3 1.3 22.2 61.9 71.2 80.3 86.5 93.6 91.6 94.5 0.2 43.8 82.4 84.4 87.9 89.9 90.3 0.0 0.2 9.4 46.6 59.6 66.2 81.5 85.4 86. 88.6 0.0 32.3 74.0 78.9 84.2 86.5 88.0 0.0 0.0 3.8 33.1 49.7 53.3 76.9 77.8 80.3 82.5 0.0 22.7 66.0 73.1 81.2 82.6 84.0 0.0 0.0 1.3 23.5 40.1 41.2 69.9 70.4 74. 75.5 0.40 2.45 4.09 4.21 4.42 4.49 4.53 0.27 0.31 0.90 2.47 3.06 3.35 4.08 4.25 4.28 4."
        },
        {
            "title": "4.1 Simulation Experiments\nCALVIN Evaluations. Tab. 1 presents the evaluation results on the CALVIN benchmark. Our method\nachieves SOTA performance in both the ABC→D and ABCD→D settings, significantly outperforming all\nprior approaches. These results highlight FALCON’s strong ability to tackle diverse tasks and execute\nlong-horizon, language-conditioned manipulation. Notably, in the challenging zero-shot ABC→D setting,\nFALCON surpasses previous methods that rely on ground-truth point clouds (e.g., 3DDP [41] and 3D Diffuser",
            "content": "7 Table 2 SimplerEnv evaluation across different policies on WidowX Robot tasks. Put Spoon: Put Spoon on Towel. Put Carrot: Put Carrot on Plate. Stack Block: Stack Green Block on Yellow Block. Put Eggplant: Put Eggplant in Yellow Basket. Method RT-1-X [27] OpenVLA [13] Octo-Base [25] RoboVLM [17] SpatialVLA [29] Put Spoon 0.0% 0.0% 12.5% 45.8% 16.7% Put Carrot 4.2% 0.0% 8.3% 20.8% 25.0% FALCON (ours) 62.5% 41.7% Stack Block 0.0% 0.0% 0.0% 4.2% 29.2% 20.8% Put Eggplant Average 0.0% 4.1% 43.1% 79.2% 100.0% 100.0% 1.1% 1.0% 16.0% 37.5% 42.7% 56.3% Table 3 SimplerEnv evaluation across different policies on Google Robot tasks. Open/Close: Open / Close Drawer. Drawer Apple: Open Top Drawer and Place Apple. Method RT-1-X [27] RT-2-X [27] Octo-Base [25] OpenVLA [13] TraceVLA [45] RoboVLM [17] SpatialVLA [29] Pick Coke Can Move Near Open/Close Drawer Apple Average 56.7% 78.7% 17.0% 16.3% 28.0% 77.3% 86.0% 42.4% 46.3% 11.0% 24.5% 34.7% 51.7% 55.3% 21.3% 3.7% 0.0% 0.0% 0.0% 24.1% 0.0% 31.7% 77.9% 4.2% 46.2% 53.7% 61.7% 77.9% FALCON (ours) 90.7% 79.2% 41.7% 62.9% 59.7% 25.0% 22.7% 35.6% 57.0% 43.5% 57.4% 39.8% Actor [12]), improving the Avg. Len. by 4.13 and 1.05, respectively. This provides clear evidence of the effectiveness of our implicit spatial information integration strategy. SimplerEnv Evaluations. Tab. 2 reports the results on the Bridge-WidowX setup, where FALCON consistently outperforms all baselines and achieves best performance. The notable improvements are observed in challenging tasks like Put Spoon on Towel (16.7% vs. 62.5%) and Put Carrot on Plate (25.0% vs. 41.7%), demonstrating FALCONs superior adaptability and effectiveness. Tab. 3 summarizes the performance of various generalist policies on the Google Robot setup. FALCON achieves an overall success rate of 62.9%, surpassing all baseline methods. Notably, on the challenging task Open Top Drawer and Place Apple, most baselines show near-zero success rates. Even the large-scale closed-source model RT-2-X [27] with 55B parameters achieves only 3.7% success, while FALCON delivers an impressive 41.7%, highlighting its exceptional generalization and spatial perception capabilities. Figure 4 Evaluation of base tasks in cluttered scene. Base Tasks contains total of nine distinct task suites, encompassing language grounding (cluttered scenes with random distractors) and semantic understanding (unseen object poses). Each task is evaluated over 10 different scene layouts with 10 trials, resulting in total of 90 rollouts."
        },
        {
            "title": "4.2 Real-World Experiments",
            "content": "To enable more comprehensive evaluation, we conduct series of carefully designed real-world experiments covering diverse object manipulation scenarios with varying task variations. The experiments are organized into three distinct settings: Base Tasks, Few-shot Adaptation, and Spatial Understanding Capability Evaluations. All models are initially pre-trained on mixture of the Open X-Embodiment dataset [27] and then fine-tuned with multi-task real-robot data. Relevant qualitative results are provided in Appendix I. Base Tasks. As shown in Fig. 4, FALCON achieves the highest average success rate of 70.0% across all nine task suites, outperforming the advanced method SpatialVLA (44.4%) by 25.6%. Moreover, in the task pick banana and place on red plate, while RoboVLM and OpenVLA-OFT often erroneously place the banana onto yellow plate, FALCON consistently places it on the correct red plate, demonstrating precise instruction following capability and superior scene understanding. Few-shot Adaptation. As shown in Fig. 5, FALCON achieves the highest performance across all settings, significantly outperforming the second-best model by 27.5% in Simple and 27% in Unseen Average. Notably, in the Unseen Object variation of the task open drawer and place bread  (Fig. 1)  , FALCON achieves an impressive success rate of 80%, while other models demonstrate near-zero success. Success rates for individual variants and sub-tasks are provided in Appendix I.2. Spatial Understanding Capability Evaluations. As illustrated in Fig. 6, FALCON demonstrates superior spatial understanding, outperforming all existing policies across the evaluated tasks. Notably, baseline methods such as RoboVLM often struggle with objects of varying sizes. For larger blocks, collisions frequently occur during the placement of the blue block, while smaller blocks are prematurely released before placement, leading to task failure. In contrast, our method exhibits strong robustness to scale variations, avoiding these issues and achieving the highest success rates in both scenarios. Figure 5 Performance comparison of different methods. Simple setting (left): includeing four challenging tasks selected from Base Tasks. Unseen scenarios (right): containing three unseen variations: Unseen Object, Background, and Task Description to evaluate the robustness and generalization of all models. Figure 6 Spatial Understanding Capability Evaluations consist of four tasks with varying levels of spatial complexity, designed to further investigate the spatial perception capabilities of FALCON."
        },
        {
            "title": "4.3 In-Depth Analysis\nModality Transferability. To evaluate the modality transferability of FALCON, we conduct extensive\nexperiments on both the CALVIN benchmark and real-world tasks to demonstrate the benefits of additional\nmodality inputs for our approach. As shown in Tab. 4, under identical input conditions, FALCON outperforms",
            "content": "9 Table 4 Performance comparison of different modality input on CALVIN benchmark. Kosmos-VLA (w/ rgb-d ) is point cloudbased variant where the ESM is replaced by lightweight point cloud encoder [42] while retaining other components. Method Task Tasks Completed in Row (%) 5 3 1 4 Avg. Len. Kosmos-VLA (w/ rgb-d ) ABCDD 92.4 ABCDD 94.0 FALCON (w/ rgb) ABCDD 94.0 FALCON (w/ rgb-d ) 93.6 Kosmos-VLA (w/ rgb-d ) ABCD ABCD FALCON (w/ rgb) 93.7 ABCD FALCON (w/ rgb-d ) 94.7 85.3 86.7 87.0 86. 86.9 86.7 80.0 80.8 81.3 78.6 77.9 79.1 76.5 76.4 76. 73.3 70.3 72.4 70.5 70.9 70.3 66.3 62.2 64.4 4.05 4.08 4. 3.98 3.91 3.97 Figure 7 Performance comparison of different modality input on real-world tasks. Left: lift yellow pepper. Right: put white cup on pink cloth-cup height change. Kosmos-VLA in the ABCDD setting. In the zero-shot ABCD setting, both methods achieve comparable performance. Furthermore, FALCON using only RGB input achieves performance comparable to Kosmos-VLA with RGB-D input. Real-world experiments further validate that incorporating depth and camera poses significantly enhances FALCONs robustness  (Fig. 7)  , increasing task success rates from 60% to 80% in scenarios involving objects of varying heights. These findings highlight FALCONs effective utilization of additional geometric information and its adaptability across different sensory modalities. Embodied Spatial Model. To further investigate the role of ESM, we conducted additional experiments on CALVIN to evaluate the monocular depth results of ESM. These experiments further validate why ESM effectively leverages additional modality inputs to achieve performance gains. As shown in Tab. 5, our ESM achieves performance comparable to VGGT when using only RGB input. Moreover, its performance improves significantly when additional depth or camera pose information is accessible. This demonstrates the inherent strength of FALCONs modality transferability, which stems from the ability of our proposed ESM to seamlessly benefit from diverse 3D modality inputs. Fig. 8 presents ESM predicted depth maps and corresponding error maps (darker colors indicate smaller errors) under different input modalities. We provide further ablation studies in Appendix to validate key design choices in FALCON, including spatial token injection strategies and fusion mechanisms. Table 5 Ablation study on modality inputs for ESM on the CALVIN benchmark. Method Depth Camera δ < 1.25 (%) Abs. Rel VGGT [36] Ours Ours Ours - - 91.33 90.91 99.79 99.47 8.53 8.61 0.91 0.87 Figure 8 Depth Visualization."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce FALCON, vision-language-action model that augments generalist robot policies with robust 3D spatial understanding. FALCON makes three main contributions: (1) integration of spatial tokens from foundation models to provide strong geometric priors; (2) an Embodied Spatial Model that optionally incorporates 3D modalities (e.g., depth, camera poses) while preserving RGB-only functionality; and (3) Spatial-Enhanced Action Head that injects spatial tokens directly into the control policy, avoiding disruptive alignment within the VLM. Experiments across both simulation and real-world tasks show that FALCON consistently surpasses existing VLA methods, achieving state-of-the-art performance and robustness on spatially demanding tasks."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In Proceedings of the Conference on Neural Information Processing System (NeurIPS), 2022. [2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [6] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. [7] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al. Gr-3 technical report. arXiv preprint arXiv:2507.15493, 2025. [8] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. [9] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. [10] AI Figure. Helix: vision-language-action model for generalist humanoid control. Figure AI News, 2024. [11] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997. doi: 10.1162/neco.1997.9.8.1735. [12] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations, 2024. URL https://arxiv.org/abs/2402.10885. [13] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [14] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. [15] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [16] Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, and Yichen Zhu. Pointvla: Injecting the 3d world into vision-language-action models. arXiv preprint arXiv:2503.07511, 2025. [17] Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2024. [18] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. In ICLR, 2024. [19] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 11 [20] Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, and Bo Zhao. Evo-0: Vision-language-action model with implicit spatial understanding. arXiv preprint arXiv:2507.00416, 2025. [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Proceedings of the Conference on Neural Information Processing System (NeurIPS), 2024. [22] Huaping Liu, Di Guo, and Angelo Cangelosi. Embodied intelligence: synergy of morphology, action, perception and learning. ACM Computing Surveys, 57(7):136, 2025. [23] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. arXiv preprint arXiv:2005.07648, 2020. [24] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters (RA-L), 7(3):73277334, 2022. [25] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems (RSS), 2024. [26] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pages 131, 2024. [27] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. [28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [29] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), 2021. [31] Christelle Rochefort, Arnaud Arabo, Marion André, Bruno Poucet, Etienne Save, and Laure Rondi-Reig. Cerebellum shapes hippocampal spatial code. Science, 334(6054):385389, 2011. [32] Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, and Liqiang Nie. Large vlm-based vision-language-action models for robotic manipulation: survey. arXiv preprint arXiv:2508.13073, 2025. [33] Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, and Jiale Cao. Geovla: Empowering 3d representations in vision-language-action models. arXiv preprint arXiv:2508.09071, 2025. [34] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. arXiv preprint arXiv:2412.15109, 2024. [35] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe HansenEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023. [36] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. [37] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1051010522, 2025. 12 [38] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [39] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. [40] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In The Twelfth International Conference on Learning Representations, 2024. [41] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations, 2024. URL https://arxiv.org/abs/ 2403.03954. [42] Yanjie Ze, Zixuan Chen, Wenhao Wang, Tianyi Chen, Xialin He, Ying Yuan, Xue Bin Peng, and Jiajun Wu. Generalizable humanoid manipulation with 3d diffusion policies, 2025. URL https://arxiv.org/abs/2410.10803. [43] Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. Up-vla: unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867, 2025. [44] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. In Proceedings of the International Conference on Machine Learning (ICML), 2024. [45] Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank all members at ByteDance Seed Robotics team for their support throughout this project. We also want to extend our gratitude to Wenke Xia for his essential assistance with the robotic hardware setup and to Hang Li for his leadership of this team."
        },
        {
            "title": "A Training Paradigm",
            "content": "A.1 FALCON Training Paradigm During the training process of FALCON, the objective for action sequence generation is formulated as the minimization of composite loss function over the predicted action horizon. Specifically, we compute the discrepancy between the predicted action sequence at:t+C1 and the corresponding ground truth sequence ˆat:t+C1 using two complementary loss components: the Mean Squared Error (MSE) and the Binary CrossEntropy (BCE). The overall loss function is defined as: = t+C1 (cid:88) i=t MSE(ˆai,pose, ai,pose) + λ BCE(ˆai,gripper, ai,gripper), (6) where the MSE term penalizes errors in the first six dimensions of the action vector and the BCE loss is applied to the last gripper dimension. The weighting factor λ balances the contributions of the two loss terms, ensuring stable and representative learning across heterogeneous action components. To integrate 3D spatial awareness into the pre-trained VLA model while preserving its generalizable action capabilities acquired during pre-training, we carefully design two-stage post-training pipeline that effectively incorporates spatial tokens from the ESM G. Rather than incorporating spatial information during pretrainingwhich would significantly increase computational cost and complicate optimizationwe adopt post-training approach that preserves the original pre-training efficiency while enabling seamless integration of 3D geometric cues. Let ΘV , ΘA, ΘG, and ΘD denote the parameter sets of the VLM V, action head π, ESM G, and lightweight adapter D, respectively. The overall training objective is to minimize the expected action prediction loss (refer to Eq. 6) over the target data distribution: Stage 1: Feature Space Alignment. In this initial stage, we freeze all pre-trained components (ΘV , ΘA, ΘG) and optimize only the adapter parameters ΘD. The adapter architecture employs zero-initialized final linear layer, which ensures the spatial tokens Tspl initially contribute minimally to the fused representation, thereby preserving the pre-trained feature space and ensuring stable optimization. The training objective is: min ΘD denotes the expectation over the dataset of image-language-action tuples. This stage where facilitates gradual alignment of spatial tokens with the semantic feature space without disrupting pre-trained representations. (Ot,L, ˆAt)S (Ot,L, ˆAt)S , (cid:16) ˆAt, π (cid:0)V(Ot, L) + D(MaxPooling(G(I 3rd )))(cid:1)(cid:17)(cid:105) (7) (cid:104) Stage 2: Joint Feature Refinement. Building upon the aligned features from Stage 1, we unfreeze both the VLM parameters ΘV and adapter parameters ΘD, while keeping ΘA and ΘG frozen. This allows the VLM to adapt its feature representation to effectively incorporate spatial information while maintaining linguistic understanding. The optimization objective becomes: min ΘV ,ΘD (Ot,L, ˆAt)S (cid:104) (cid:16) ˆAt, π (cid:0)V(Ot, L) + D(MaxPooling(G(I 3rd )))(cid:1)(cid:17)(cid:105) . (8) This phased approach ensures stable convergence and prevents the spatial features from overwhelming the semantic representations during initial learning phases. 14 The proposed training strategy offers several advantages: (1) it maintains the integrity of the pre-trained VLAs knowledge while incorporating new spatial capabilities; (2) zero-initialization in Stage 1 ensures training stability and avoids disruptive feature shifts; (3) gradual unfreezing in Stage 2 enables balanced feature adaptation. This paradigm allows FALCON to achieve robust 3D spatial perception while maintaining strong performance across diverse tasks and scenarios. A.2 Embodied Spatial Model Training Paradigm ESM is trained following the dataset and preprocessing procedures of VGGT [36]. For each training batch, we randomly sample 112 frames from randomly selected scene, resulting in total of 24 images per batch. Training is performed using the AdamW optimizer with differentiated learning rates: 1e-6 for the large unified transformer backbone and 1e-5 for the depth, camera, and point heads. The complete training process requires 16 A100 GPUs and runs for approximately 2 days. Hyper-Parameters and Training Details Simulation Benchmarks. As FALCON employs two-stage post-training strategy, we first pre-train 2D VLA (denotes as Kosmos-VLA-2D) on the target datasets (CALVIN [24], OXE dataset [27]) without involving the ESM. The pre-training uses learning rate of 2e-5, global batch size of 128, and warmup ratio of 0.25 epochs for CALVIN and 2.5k steps for OXE. Subsequently, we conduct post-training based on the pre-trained weights of Kosmos-VLA-2D in two stages: Stage 1 uses learning rate of 1e-4, global batch size of 128, and no warmup. Stage 2 hyper-parameters are detailed in Tab. 6. Real-World Tasks. For real-world evaluation, we initialize the model with pre-trained weights from KosmosVLA-2D (OXE) and ESM. Stage 1 training uses learning rate of 1e-4, global batch size of 512, and no warmup. Stage 2 hyper-parameters are provided in Tab. 6. We evaluate all VLAs under the following training settings: (1) Base Tasks: models trained separately on three scenarios. (2) Few-shot Adaptation: multi-task training across all four tasks. (3) Spatial-Prompts: efficient fine-tuning on two tasks. Baseline models (SpatialVLA [29], RoboVLM [17]) use the same hyper-parameters as FALCON. For OpenVLA-OFT [14] (LoRA-only), we use learning rate of 5e-4, global batch size 128, LoRA rank 32, chunk size 5, and Base Tasks train for 150k iterations, other two settings for 100k iterations. All VLAs trained on real-world datasets use only the side camera input. Across all experiments, both training stages for FALCON use the same number of epochs/iterations, Constant learning rate scheduler, and the AdamW optimizer. All input images (including depth maps) are resized to resolution of 224224 Checkpoint Selection. Following the findings of [17], we note that policy performance does not fully correlate with offline evaluation metrics (e.g., validation loss) due to compounding errors in long-horizon rollouts, making checkpoint selection challenging. To ensure fair comparisons, all VLAs are trained for fixed number of epochs or iterations. Specifically: On CALVIN, models are trained for 5 epochs with batch size of 128 truncated trajectories, and the final checkpoint is evaluated. On SimplerEnv, models are trained for 150K iterations (batch size 128), with evaluation at 10K-interval checkpoints, and the best-performing model is reported. In Real-World experiments, all models are trained for 30 epochs or same iterations for OpenVLA-OFT with batch size of 512 truncated trajectories, and only the final checkpoint is evaluated. This consistent protocol ensures comparable evaluation across all baselines."
        },
        {
            "title": "C Implementation Details",
            "content": "As demonstrated in Tab. 6, on the CALVIN benchmark, the VLM receives side and wrist camera images with history length of 16 frames, while the ESM processes third-view images also with same length historical 15 context. Predictions are made using an LSTM-based action predictor that outputs chunk of = 10 future actions. For SimplerEnv and the real-world benchmark, both the VLM and ESM receive single-step third-view image, and an MLP-based action predictor predicts an action chunk of length = 5. During inference, we employ two different action execution strategies: for CALVIN and SimplerEnv, the policy executes the ensemble actions before generating the next chunk. In the real-world setup, the entire action chunk is executed at once. FALCON requires 12.8 GB of GPU memory and runs at approximately 57 Hz on single NVIDIA RTX 4090 GPU during real-world evaluation. Table 6 Hyper-parameters setup of FALCON for different experiments. Abbreviations: Ep: Epochs, Iters: Iterations Experiment Name Action Predictor Window Size Chunk Size VLM Input View ESM Input View Batch Size CALVIN Performance (Tab. 1) SimplerEnv Performance (Tab. 2-3) Real-World Performance (Fig. 4-7) CALVIN Ablation (Tab. 4, Tab. 7) LSTM MLP MLP MLP 16 1 1 1 10 5 5 Side+Wrist Side Side Side+Wrist Side Side Side Side 128 128 512 128 Learning Rate 2e-5 (ABCD) 5e-5 (ABCDD) 2e-5 2e-5 5e-5 (ABCD) 2e-5 (ABCDD) Total 5 Ep 150K Iters 30 Ep 5 Ep"
        },
        {
            "title": "D Benchmark Details",
            "content": "CALVIN [24] is simulation benchmark designed for evaluating long-horizon, language-conditioned robotic manipulation. It consists of four scene splits (A, B, C, and D), each representing distinct environment configuration and featuring 24k human-teleoperated demonstrations annotated with language instruction. Each trajectory is less than 64-time steps, which includes 34 pre-defined basic skills: rotate blue block right, move slider right, lift red block slider, place slider, turn off light bulb, turn off led light, push in drawer, lift blue block drawer, close drawer, lift pink block slider, lift pink block table, move slider left, open drawer, turn on light bulb, rotate blue block left, push blue block left, rotate red block right, turn on led light, push pink block right, push red block left, lift blue block table, place in drawer, rotate red block left, push pink block left, lift stacked blocks, lift blue block slider, push red block right. We train and test FALCON and VLA baselines on different training/test splits to fully analyze the capabilities. Standard evaluation protocols such as ABCD and ABCDD are employed to assess the models generalization capability to unseen environments and its robustness in long-horizon task compositions. During evaluation, the robot is required to complete set of 5 consecutive tasks. The metrics are the success rates of finishing these sequential tasks and the average length of achieved tasks (Avg. Len.). All evaluations are implemented on split, with 1000 rollouts and 5 consecutive sub-tasks for each rollout. SimplerEnv [19] provides benchmark for evaluating the transfer and generalization capabilities of models trained on large-scale real-world video data. It supports diverse manipulation setups across both the WidowX and Google Robot platforms, incorporating variations in lighting conditions, object textures, color distributions, and camera viewpoints. By faithfully replicating real-world conditions in simulated environment, SimplerEnv enables reproducible and controlled evaluation of robot policies, facilitating rigorous benchmarking under settings that closely mirror private real-world systems such as Bridge V2 [35] and Google Robot [4, 5]. We adopt the following tasks in the WidowX + Bridge setting: put the spoon on the towel. In this setup, the spoon is positioned at one corner of square on the tabletop, with the towel placed at different corner. The square has sides measuring 15 cm in length. The orientation of the spoon alternates between horizontal and vertical, requiring the robot to adjust the orientation of its gripper accordingly. This configuration results in total of 24 trials. put carrot on plate. This setup is similar to put the spoon on the towel, but the spoon is replaced with carrot and the towel is substituted with plate. stack the green block on the yellow block. In this experiment, green block is positioned at one corner of square on the tabletop, while yellow block is placed at different corner. Both blocks measure 16 3 cm in size. Two square configurations with 10 cm and 20 cm side lengths are used. This setup results in total of 24 trials. put eggplant into yellow basket. An eggplant is positioned randomly within the right basin of sink, while yellow basket is placed in the left basin. The eggplants placement varies in both location and orientation but is carefully arranged to remain easily graspable, avoiding proximity to the sinks edges. total of 24 trials are conducted under this setup. For the Google Robot setting, we test the following tasks: pick coke can. The task assigned to the robot is to pick up an empty Coke can from the table and lift it. Under the standard configuration, the environment is kept free of any distracting elements. The Coke can is arranged in three distinct positions: lying flat horizontally, lying flat vertically, and standing upright. For each of these positions, the can is placed at 25 specific grid points within defined rectangular area on the table. This setup results in 25 experiments per position, totaling 75 trials across all orientations. move {obj1} near {obj2}. In the experiment, set of three objects was arranged on the table in triangular formation. For each trial, one object was assigned the role of the source, another was designated as the target, and the third served as distractor. This setup resulted in six distinct trials for each triplet and triangular configuration. From total of eight objectsblue plastic bottle, Pepsi can, orange, 7up can, apple, sponge, Coke can, and Redbull canfive triplets were randomly selected. Additionally, two triangular patterns, upright and inverted, were employed. This design produced total of 60 trials. (open/close) (top / middle / bottom) drawer. In this setup, the robot is placed facing cabinet equipped with three drawers and tasked with opening or closing specific drawer. This experiment evaluates the robots capability to handle articulated objects. The robot is positioned at nine distinct locations on predefined grid within rectangular area on the floor. With three drawers and two possible actions (opening or closing), the setup results in total of 54 trials. open top drawer; place apple into top drawer. In this experiment, the robot is tasked with opening the top drawer and transferring an apple from the surface of the cabinet into the drawer. This setup evaluates the robots ability to execute tasks that require multiple sequential actions. The robot is positioned in three distinct locations on the floor, while the apple is placed at nine specific grid points on the cabinet surface, resulting in total of 27 trials. At the start, the robot operates under the instruction to open the top drawer. Once the robot either signals task completion with terminate token or reaches the midpoint of the allotted time, the instruction transitions to directing the robot to place the apple into the drawer. Real World Benchmark comprises 1,030 expert trajectories collected through human teleoperation, spanning five distinct robot learning scenarios and 11 individual tasks. These range from simple object interactions, such as lift the yellow pepper, to long-horizon sequential activities, such as place the red coke can on the bottom shelf, comprehensively assessing the models robustness and spatial perception capabilities. Base Tasks are organized into three scenarios (Dining Table, Bedroom, and Kitchen) containing total of nine distinct task suites, encompassing language grounding (cluttered scenes with random distractors) and semantic understanding (unseen object poses). Each task is evaluated over 10 different scene layouts with 10 trials, resulting in total of 90 rollouts. Besides, for each task, we collected 100 demonstration trajectories (except lift the yellow pepper for 50 trajectories), resulting in total of 850 trajectories for training. Few-shot Adaptation includes four challenging tasks selected from the Base Tasks that require more spatial perception capabilities. For each task, we collected 20 demonstration trajectories, resulting in total of 80 trajectories for training. In addition to this base setting (denoted as Simple in Fig. 13), we introduce three unseen variations: Unseen Object, Unseen Background (by changing two different colored tablecloths), and Unseen Task Description, to evaluate the robustness and generalization of all models in low-data regimes. Each task is evaluated across 5 different layouts with 2 trials per layout. Spatial Understanding Capability Evaluations consist of four tasks with varying levels of spatial complexity: 17 two spatial-prompt tasks adapted via efficient fine-tuning (each task we collected 50 demonstrations), two zero-shot tasks, one from Base Tasks involving explicit height variation (put white cup on pink cloth with two 3cm blocks below cup), and the other from Few-shot Adaptation featuring objects of different sizes (stack blue block on red block with larger block size: 5cm, and smaller block size: 3cm. Regular size for training is 4cm). This suite of tasks is designed to further investigate the spatial perception capabilities of the FALCON model. Each task is evaluated across 5 different layouts with 2 trials per layout. The physical setup consists of an xArm 6 robot arm equipped with Robotiq parallel gripper and an Intel RealSense D435i depth camera positioned approximately 0.6 meters away to provide third-person view, as illustrated in Fig. 9. All fine-tuning datasets are collected via human teleoperation using Spacemouse device, sampled at 10Hz. We use absolute Cartesian control as the action space for policy training and deployment. Figure 9 Real-world setup of the xArm 6 robotic system used in the experiments. The system is equipped with side camera that provides both RGB and depth images for visual observation and spatial perception."
        },
        {
            "title": "E Ablation Study",
            "content": "In this section, we perform ablation studies on CALVIN benchmark to assess key design choices in FALCON, focusing on the impact of spatial token injection positions and modality fusion strategies on its performance. Where to inject spatial tokens? To verify the effectiveness of our strategy for injecting 3D information into the action head, we evaluate variant following the approach of most 3D-based VLAs, where spatial tokens from the ESM are directly injected into the VLMs (denoted as FALCONVLM-tokens). As shown in Tab. 7, this approach results in significant performance degradation compared to the standard FALCON paradigm. Notably, in the zero-shot ABCD setting, the Avg. Len. drops significantly from 3.91 to 3.79. These results indicate that introducing fine-grained spatial features into the VLM disrupts its pre-trained semantic representation space, negatively impacting its generalization capability. In contrast, injecting spatial tokens directly into the action head preserves the VLMs integrity while effectively utilizing geometric cue, making it the superior strategy. How to fusion spatial tokens? We evaluate three modality fusion approaches detailed in Sec. 3.3: crossattention, FiLM-gated modulation, and element-wise addition (our standard FALCON). As shown in Tab. 7, 18 Table 7 Ablation studies on spatial token injection strategies and fusion methods. Results confirm that the standard FALCON paradigm achieves the best performance, validating it as the optimal design choice. Method Task Tasks Completed in Row (%) 5 3 1 4 2 Avg. Len. FALCONVLM-tokens ABCDD 92.9 ABCDD 93.7 Cross-Attention ABCDD 93.8 FiLM-Gated ABCDD 94.0 94.2 91.3 92. FALCON (ours) FALCONVLM-tokens ABCD ABCD Cross-Attention ABCD FiLM-Gated ABCD FALCON (ours) 93.7 85.4 85.5 85.7 86.7 85.2 81.9 83.7 86. 79.4 78.2 80.2 80.8 75.6 72.9 74.5 77.9 74.4 73.0 75.4 76.4 66.1 64.9 66.9 70. 68.1 67.5 69.6 70.9 57.6 57.2 58.4 62.2 4.00 3.98 4.04 4.08 3.79 3.68 3.76 3. element-wise addition consistently delivers the best performance across all experimental settings. This method achieves the highest task success rates while remaining both simple and computationally efficient, as it introduces no additional parameters. The results underscore the effectiveness of straightforward, parameter-free fusion strategy for seamlessly integrating spatial and semantic representations in VLA models."
        },
        {
            "title": "F Potential Future Works",
            "content": "Table 8 Performance comparison of wrist camera input for ESM on the CALVIN benchmark. Method Task Tasks Completed in Row (%) 5 3 1 4 Avg. Len. w/o wrist ABCDD 94.0 ABCDD 94.1 w/ wrist 86.7 87.2 80.8 81. 76.4 77.0 70.9 70.6 4.08 4.10 The integration of wrist camera images into the ESM further enhances FALCONs performance, as evidenced in Table 8. For instance, in the CALVIN ABCDD setting, the Avg. Len. increases from 4.08 to 4.10 when wrist images are incorporated. This improvement suggests that multi-view inputs can provide complementary geometric cues. Future work could investigate multi-view camera systems that offer more consistent geometric perspectives, potentially further boosting robustness in diverse sensor configurations."
        },
        {
            "title": "G Rollout Examples in CALVIN",
            "content": "Figure 10 Rollouts on the ABCD split of the CALVIN benchmark."
        },
        {
            "title": "H Rollout Examples in SimplerEnv",
            "content": "Figure 11 Qualitative results for SimplerEnv tasks. 21 Rollout Examples in Real-World Tasks I.1 Base Tasks Rollouts Figure 12 Qualitative results for Base Tasks. I.2 Few-Shot Adaptation Rollouts and Detailed Results Figure 13 Qualitative results for Few-Shot Adaptation tasks. Figure 14 Performance comparison of different methods in the Simple setting and four variants. 23 Table 9 Few-shot Adaptation performance under Simple Settings. We report both the final success rates (Success) along with the sub-task success rates (e.g., Grasp Block). Stack Block: Stack Blue Block on Red Block. Place Bread: Open Drawer and Place Bread. Place Coke Can: Place the Red Coke Can on the Bottom Shelf. Place Sprite Can: Place the Green Sprite Can on the Top Shelf. Method Stack Block Place Bread Place Coke Can Place Sprite Can Grasp Block Success Open Drawer Success Grasp Can Success Grasp Can Success Average OpenVLA-OFT [14] RoboVLM [17] SpatialVLA [29] FALCON (ours) 30.0% 60.0% 60.0% 90.0% 30.0% 40.0% 50.0% 80.0% 30.0% 100.0% 90.0% 100.0% 30.0% 80.0% 70.0% 100.0% 40.0% 80.0% 50.0% 80.0% 30.0% 60.0% 40.0% 80.0% 60.0% 100.0% 80.0% 90.0% 20.0% 60.0% 50.0% 90.0% 27.5% 60.0% 50.0% 87.5% Table 10 Few-shot Adaptation performance under Unseen Objects variants. We report both the final success rates (Success) along with the sub-task success rates (e.g., Grasp Block). Stack Block: Stack Orange Block on Green Block. Place Tennis Ball: Open Drawer and Place Tennis Ball. Place Strawberry Can: Place the Strawberry Juice Can on the Bottom Shelf. Place Grape Can: Place the Grape Juice Can on the Top Shelf. Method Stack Block Place Tennis Ball Place Strawberry Can Place Grape Can Grasp Block Success Open Drawer Success Grasp Can Success Grasp Can Success Average OpenVLA-OFT [14] RoboVLM [17] SpatialVLA [29] FALCON (ours) 20.0% 40.0% 30.0% 40.0% 0.0% 0.0% 10.0% 40.0% 60.0% 60.0% 70.0% 100.0% 0.0% 0.0% 20.0% 80.0% 30.0% 60.0% 70.0% 60.0% 10.0% 20.0% 30.0% 60.0% 60.0% 80.0% 60.0% 60.0% 20.0% 40.0% 50.0% 60.0% 7.5% 15.0% 27.5% 60.0% Table 11 Few-shot Adaptation performance under Unseen Background 1 variants. We report both the final success rates (Success) along with the sub-task success rates (e.g., Grasp Block). Stack Block: Stack Blue Block on Red Block. Place Bread: Open Drawer and Place Bread. Place Coke Can: Place the Red Coke Can on the Bottom Shelf. Place Sprite Can: Place the Green Sprite Can on the Top Shelf. Method Stack Block Place Bread Place Coke Can Place Sprite Can Grasp Block Success Open Drawer Success Grasp Can Success Grasp Can Success Average OpenVLA-OFT [14] RoboVLM [17] SpatialVLA [29] FALCON (ours) 20.0% 20.0% 20.0% 40.0% 10.0% 0.0% 20.0% 40.0% 20.0% 20.0% 40.0% 60.0% 20.0% 20.0% 30.0% 60.0% 0.0% 20.0% 20.0% 40.0% 0.0% 0.0% 20.0% 40.0% 20.0% 20.0% 30.0% 40.0% 20.0% 20.0% 30.0% 20.0% 12.5% 10.0% 25.0% 40.0% Table 12 Few-shot Adaptation performance under Unseen Background 2 variants. We report both the final success rates (Success) along with the sub-task success rates (e.g., Grasp Block). Stack Block: Stack Blue Block on Red Block. Place Bread: Open Drawer and Place Bread. Place Coke Can: Place the Red Coke Can on the Bottom Shelf. Place Sprite Can: Place the Green Sprite Can on the Top Shelf. Method Stack Block Place Bread Place Coke Can Place Sprite Can Grasp Block Success Open Drawer Success Grasp Can Success Grasp Can Success Average OpenVLA-OFT [14] RoboVLM [17] SpatialVLA [29] FALCON (ours) 40.0% 40.0% 30.0% 40.0% 10.0% 0.0% 30.0% 40.0% 20.0% 20.0% 40.0% 60.0% 20.0% 20.0% 30.0% 60.0% 20.0% 20.0% 50.0% 60.0% 10.0% 20.0% 40.0% 60.0% 40.0% 60.0% 50.0% 60.0% 20.0% 40.0% 30.0% 40.0% 15.0% 20.0% 32.5% 50.0% Table 13 Few-shot Adaptation performance under Unseen Task Description variants. We report both the final success rates (Success) along with the sub-task success rates (e.g., Put Cube). Put Cube: Put the Blue Cube on Top of the Red Cube. Put Bread: Unlock the Drawer and Put the Bread Inside. Put Coke Can: Put the Red Coke Can on the Lower Shelf. Position Sprite Can: Position the Green Sprite Can on the Upper Shelf. Method Put Cube Put Bread Put Coke Can Grasp Block Success Open Drawer Success Grasp Can Success Grasp Can Position Sprite Can Success Average OpenVLA-OFT [14] RoboVLM [17] SpatialVLA [29] FALCON (ours) 40.0% 60.0% 50.0% 70.0% 40.0% 40.0% 40.0% 70.0% 20.0% 100.0% 80.0% 100.0% 20.0% 60.0% 60.0% 100.0% 40.0% 100.0% 40.0% 80.0% 20.0% 80.0% 40.0% 80.0% 40.0% 100.0% 70.0% 80.0% 10.0% 40.0% 40.0% 60.0% 22.5% 55.0% 45.0% 77.5% 24 I.3 Spatial Understanding Capability Evaluation Rollouts Figure 15 Qualitative results for Spatial Understanding Capability Evaluations."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "NTU",
        "NUS",
        "SMU",
        "THU"
    ]
}