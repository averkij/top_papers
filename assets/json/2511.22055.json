{
    "paper_title": "OralGPT-Omni: A Versatile Dental Multimodal Large Language Model",
    "authors": [
        "Jing Hao",
        "Yuci Liang",
        "Lizhuo Lin",
        "Yuxuan Fan",
        "Wenkai Zhou",
        "Kaixin Guo",
        "Zanting Ye",
        "Yanpeng Sun",
        "Xinyu Zhang",
        "Yanqi Yang",
        "Qiankun Li",
        "Hao Tang",
        "James Kit-Hon Tsoi",
        "Linlin Shen",
        "Kuo Feng Hung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 5 0 2 2 . 1 1 5 2 : r OralGPT-Omni: Versatile Dental Multimodal Large Language Model Jing Hao1 Yuci Liang2 Lizhuo Lin1 Yuxuan Fan3 Wenkai Zhou1 Kaixin Guo1 Zanting Ye4 Yanpeng Sun5 Xinyu Zhang6 Yanqi Yang1 Qiankun Li7 Hao Tang8 James Kit-Hon Tsoi1 Linlin Shen9 Kuo Feng Hung1 1Faculty of Dentistry, The University of Hong Kong 2College of Computer Science and Software Engineering, Shenzhen University 3The Hong Kong University of Science and Technology (GZ) 4School of Biomedical Engineering, Southern Medical University 5Singapore University of Technology and Design 6University of Auckland 7University of Science and Technology of China 8School of Computer Science, Peking University 9College of Artificial Intelligence, Shenzhen University"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPTOmni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists diagnostic reasoning, we construct TRACE-CoT, clinically grounded chain-of-thought dataset that mirrors dental radiologists decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the models capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended questionanswer pairs spanning five modalities and five tasks, offering comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available. Equal Contribution. Project Leader. Corresponding Authors: hungkfg@hku.hk, llshen@szu.edu.cn Multimodal Large Language Models (MLLMs) have exhibited remarkable capabilities in open-world visual understanding and reasoning in natural domains [1, 4, 5, 13, 21, 29, 35, 44, 51], and they also demonstrate immense potential across medical specialties, including dermatology [64], ophthalmology [24], chest radiology [19], pathology [26], and pediatrics [54]. However, recent studies conclude that existing MLLMs still face notable limitations in dentistry, including insufficient consistency, completeness, and clarity of generated outputs, as well as the observation of hallucinated responses, which prevents their application in real clinical applications [30, 31]. Due to the lack of deep modeling of dentistry-specific expertise and modalityspecific features, existing general-purpose and medicalpurpose MLLMs struggle to provide reliable support for complex dental imaging analysis and highly specialized clinical requirements. Although some preliminary studies [34, 60] have explored the application of MLLMs to dental imaging, their performance remains far from satisfactory. These limitations largely stem from the substantial heterogeneity across dental imaging modalities, the intrinsic complexity of clinical diagnostic workflows, and the lack of transparency and reliability in model responses. Progress in this domain is further constrained by the scarcity and inconsistent quality of dental imaging datasets, which result from strict privacy concerns, limited data sharing, and the high cost of expert annotation [11, 12, 14, 16]. At the same time, explainable decision-making is indispensable in the medical field, as clinicians and patients must understand not only the final diagnostic conclusion but also the reason1 Figure 1. Overview of diverse dental-specialized corpus. (a) Eight types of widely used dental imaging modalities. (b) Introduction of our proposed TRACE-CoT reasoning pattern that enhances the reliability of MLLMs response. (c) The composition of the training corpus for OralGPT-Omni. The bar chart shows the distribution of various dental modalities. ing process that leads to it. Yet, this critical aspect has been largely overlooked in existing research. To bridge these gaps in digital and intelligent dentistry, we aim to develop dental-specialized MLLM capable of robust and comprehensive multimodal imaging analysis. To this end, we introduce OralGPT-Omni, versatile MLLM that facilitates comprehensive analysis across eight dental imaging modalities and five dental-related tasks, as illustrated in Figure 1(a). Importantly, for the diagnosis of abnormalities, OralGPT-Omni does more than produce final answers. It reveals its diagnostic process by producing explicit chain-of-thought (CoT) rationales that mirror real clinical workflows. This capability substantially improves the models transparency and interpretability. To build the OralGPT-Omni, we systematically prepare the recipe around three key aspects: diverse dental imaging curation, TRACE-CoT reasoning data construction, and four-stage training strategy. First, we curate comprehensive multimodal dental imaging dataset by aggregating data from 31 public sources and one dental hospital. Next, we design the TRACE-CoT (Transparent Radiologic Analysis with Clinical Evidence), reasoning pattern that mirrors radiologists diagnostic decision-making, which is demonstrated in Figure 1(b). Each CoT instance explicitly outlines the intermediate reasoning processes involved in the diagnosis of abnormalities, encompassing detailed descriptions of visual appearances, the rationale behind diagnostic hypotheses, the integration and verification of domain2 Figure 2. (a) The dental imaging data curation and TRACE-CoT data generation pipeline. It involves curating diverse imaging modalities from public datasets and dental hospitals. TRACE-CoT data is then generated using GPT, Wikipedia, and various annotations. Finally, the data is split into training set and benchmark, with professional dentists assessing the training samples and thorough manual correction conducted on the benchmark. (b) Results from two dentists evaluating the quality of 300 TRACE-CoT data from the training set. specific knowledge, and final evidence-informed diagnosis. Lastly, leveraging our meticulously curated large-scale, multimodal, dental-specific dataset, we adopt four-stage training paradigm that enhances OralGPT-Omnis integration of visual comprehension, explicit reasoning, and the ability to follow complex instructions. Currently, there is only one public dental-related benchmark, MMOral-OPG, for panoramic X-ray analysis [15]. The absence of benchmarks that include various dental imaging modalities hampers the systematic evaluation of MLLMs in dentistry. To fill this gap and guide future optimization, we present MMOral-Uni, the first unified benchmark dedicated to dental multimodal imaging analysis. It comprises 2,809 open-ended questionanswer pairs that emulate realistic user interactions. All pairs are validated and refined by two experienced dentists to ensure clinical correctness. The benchmark spans five modalities and five tasks, enabling comprehensive and rigorous evaluation of MLLMs in the dental domain. We assess the performance of OralGPT-Omni on the MMOral-OPG [15] and MMOral-Uni benchmarks to thoroughly evaluate its capability for dental applications. OralGPT-Omni achieves an impressive overall score of 51.84 on the Moral-Omni benchmark and 45.31 on the MMOral-OPG benchmark, significantly surpassing the scores of GPT-5. It also markedly outperforms existing medical MLLMs, emphasizing its unique contributions to the dental field. clinical validity assessment conducted by radiologist with over ten years of experience on three leading MLLMs indicates that our OralGPT-Omni demonstrates outstanding accuracy and potential clinical utility. Overall, our contributions are summarized as follows: We propose OralGPT-Omni, the first dental-specialized MLLM for comprehensive dental imaging analysis across diverse imaging modalities and tasks. We present MMOral-Uni, the first unified benchmark for dental multimodal imaging analysis, covering five imaging modalities and five tasks, with 2,809 open-ended VQA pairs that together offer comprehensive evaluation suite for existing MLLMs. Extensive experiments demonstrate that OralGPT-Omni delivers superior performance across diverse dental imaging modalities, achieving the superior overall scores on both the MMOral-OPG and Moral-Omni benchmarks, highlighting its potential in digital dentistry. 2. OralGPT-Omni 2.1. Training Corpus Construction curate We meticulously dental-specific multimodal datasets, including text-only corpora and imaging datasets, sourced from trusted public data platforms and dental hospitals. For the text corpora, we compile materials from undergraduate dental textbooks, dialogue data from dental community forums, and QA pairs on oral diseases and diagnoses derived from clinical guidelines. For imaging datasets, we prioritize image quality and label reliability, incorporating 31 public datasets with low applicability concerns, as defined by an authoritative systematic review [16]. The dataset having low applicability concern indicates it reports both ethical approval as well as licensing requirements for its reuse. Additionally, we include one expert-annotated dataset sourced from dental hospital in Hong Kong. comprehensive list of all collected datasets is provided in the Appendix. To further ensure the accuracy and reliability of annotations in the public datasets, we conduct manual post-processing guided by the risk-of-bias rating assigned in the systematic review [16]. This rating quantifies the reliability of the ground truth and is widely used in diagnostic imaging research [49]. For datasets with 3 Figure 3. There are four stages for training OralGPT-Omni, and only the first stage is training in the single modality. moderate or high risk-of-bias ratings, professional dentists manually inspect and refine the annotations, correcting any identified errors. By contrast, datasets with low risk-of-bias rating do not undergo further manual validation due to high confidence in their annotation quality. The curated training dataset comprises approximately 3.21 million text tokens, 59,658 images, and 90 videos. It supports five primary tasks: abnormality diagnosis, CVM stage prediction, recommended treatment planning, video understanding, and tooth localization and counting. The dataset encompasses eight dental imaging modalities, as illustrated in Figure 1(c). It includes intraoral photographs and videos, panoramic, periapical, and cephalometric radiographs, pathology images, 3D model scans, and interleaved image-text data. In addition, it provides broad demographic coverage, with samples from at least ten countries. It also offers rich structured annotations, such as abnormality labels, bounding boxes, segmentation masks, keypoints, and diagnostic descriptions. Building on this high-quality, heterogeneous resource, we convert the original annotations into chain-of-thought reasoning data, as detailed in Sec. 2.2. 2.2. TRACE-CoT Data Generation Unlike black-box predictions [2], CoT explicitly reveals the intermediate reasoning steps, thereby enhancing the transparency and trustworthiness of MLLMs. However, two mainstream approaches for generating reasoning chains, CoT prompting strategies [20, 28, 36, 45] and humanverified annotation [8], exhibit notable limitations. The former relies heavily on the reasoning capability of the base model, while the latter is difficult to scale up due to intensive expert involvement. To overcome these challenges, we propose TRACE-CoT (Transparent Radiologic Analysis with Clinical Evidence), reasoning pattern that mirrors the diagnostic decisionmaking process of radiologists. This process is clinically coherent and closely aligns with routine radiologic practice, involving image inspection, hypothesis generation, reference to medical expertise, and verification to reach final diagnosis. The reasoning chains proceed as follows: (1) Image inspection: conduct thorough examination of the image, carefully describing salient structures, visual appearances, and notable patterns. (2) Hypothesis generation: propose plausible abnormalities based on the observed features. (3) Medical expertise reference: refer to authoritative clinical guidelines and well-established knowledge regarding the suspected abnormalities and their characteristic imaging signatures. (4) Feature-based verification: compare the observed image features against knowledge-based standards to identify and resolve inconsistencies. (5) Evidence-informed conclusion: aggregate the accumulated evidence and finalize the diagnostic findings. We construct the TRACE-CoT data for abnormality diagnosis across three widely used clinical dental imaging intraoral images, periapical radiographs, and modalities: pathology images. The pipeline can be found in Figure 2(a). We first prompt GPT-5-mini to generate detailed descriptions of visual appearances and patterns, then we treat the sparse annotations for each image as initial diagnostic hypotheses. Guided by these hypotheses, we retrieve characteristic imaging patterns, expressed in natural language, from authoritative clinical guidelines and Wikipedia. Next, we instruct GPT-5-mini to organize these elements into complete five-step reasoning chains, and we ultimately generate 36,777 TRACE-CoT reasoning chains. Because abnormality taxonomies differ across datasets, we design dataset-specific prompts tailored to each taxonomy to ensure the quality of the chains; the full prompts are provided in the Appendix. To validate the quality of the generated TRACE-CoT data, we invited two dentists to evaluate 300 instances from seven perspectives, and the results are shown in Figure 2(b), demonstrating its high quality and reliability. Clinically grounded reasoning chains make MLLMs more transparent, interpretable, and reliable. They also mitigate the black-box nature of MLLMs, bolster clinician trust, and facilitate adoption in high-stakes medical settings. 2.3. Model & Training Strategy OralGPT-Omni is built upon the Qwen2.5-VL-7B model [5] due to its superior generalization and instruction-following capability. To further adapt the models capacity to dental scenarios, we employ four-stage training strategy that 4 progressively strengthens its multimodal understanding and reasoning abilities, as shown in Figure 3. In the first stage, we perform dental knowledge injection using the corpus composed of 16 professional dental textbooks. This stage aims to inject and consolidate fundamental dental knowledge into the model, during which only the language model is updated. In the second stage, to guide the initial alignment between dental concepts and visual representations, we employ 6,318 dental imagecaption pairs extracted from dental textbooks to optimize only the visionlanguage projector. In the third stage, we conduct supervised fine-tuning (SFT) on the entire OralGPT-Omni architecture to further enhance its instruction-following, multimodal comprehension, and explicit reasoning capabilities. The fine-tuning process uses 52,725 high-quality dental instruction pairs, comprising 31,777 CoT reasoning pairs and additional pairs without explicit reasoning patterns. The training corpus spans eight imaging modalities as well as text-only dialogue data, comprehensively covering variety of dental imaging and diagnostic scenarios. In the final stage, we apply reinforcement learning tuning (RLT) within the Group Relative Policy Optimization (GRPO) [41] framework to further incentivize OralGPTOmnis reasoning ability. We introduce two components: difficulty-aware data selection strategy and TRACEbased reward closely aligned with the TRACE-CoT reasoning pattern. For data selection, we assess the difficulty of each instance relative to the preceding SFT model without the TRACE-CoT pattern and retain only medium-difficulty cases, as shown in Figure 4. This is because extremely easy or extremely hard instances provide limited learning signals [56, 59]. The rationale for selecting model without the TRACE-CoT pattern is that we anticipate RLT can further enhance the models performance and generalization by stimulating TRACE-CoT reasoning patterns in these medium-difficulty cases. Concretely, we perform rollouts per instance, compute the score = {S1, S2, ..., SN }, (1) 0.2 and retain only those that meet two criteria: Savg 0.8, (2) ax(S) in(S) 0.4. We set = 5 and select 2,000 medium-difficulty cases from 5,000 instruction data. We also introduce TRACE-based reward Rtrace that comprehensively evaluates reasoning quality with LLM-based judge models. This reward is integrated into RLT to guide models toward producing higherquality and more reliable reasoning paths across three aspects, including factual knowledge soundness, logical coherence, and answer consistency. Following conventional RTL reward computation, both the answer reward Ranswer and the format reward Rformat are considered. The Rtrace and Ranswer are provided by GPT-5-nano, acting as rewardjudger. The values for these three reward components range from 0 to 1. The overall reward signal unifies these compoFigure 4. The difficulty-aware data selection strategy for RLT. nents into single formulation: Rtotal = α Ranswer + β IRanswer>0 Rtrace + γ Rformat, (1) where α + β + γ = 1. Note that the Rtrace is ineffective when the Ranswer is completely wrong in order to ensure the consistent correctness of the reasoning and answers. More details of TRACE-based reward and RLT are illustrated in the Appendix. 3. MMOral-Uni Benchmark 3.1. Benchmark Composition We introduce MMOral-Uni, the first unified benchmark for dental multimodal imaging analysis, spanning five modalities and five tasks. It includes 2,809 open-ended QA pairs to simulate realistic user interactions. The modalities cover intraoral photographs, periapical and cephalometric radiographs, pathological images, intraoral videos, and interleaved imagetext inputs. The tasks include abnormality diagnosis, cervical vertebral maturation (CVM) stage prediction, treatment planning, tooth localization and counting, and dental treatment video comprehension. The combination of diverse imaging modalities and task types allows for thorough evaluation of MLLMs in the dental field. The distribution of modalities and tasks is shown in Fig. 5(a). To ensure the quality and reliability of the benchmark, we implement strict data selection and extensive manual validation. All images in the MMOral-uni benchmark are sourced from publicly available datasets that had been assessed as low applicability concern [16]. We design the questions to align with specific task types and anticipated user intents. For example, the question is This is an intraoral photograph of the oral cavity. Identify any oral diseases present. for abnormality diagnosis, and What is the estimated CVM stage based on this cephalometric radiograph? for CVM staging. Answers are generated by converting sparse annotations into clear textual descriptions with assistance from GPT-5-mini, after which two experienced dentists validated and refined all QA pairs to further 5 Figure 5. Performance comparison on the MMOral-Uni benchmark. (c) Performance comparison on the MMOral-OPG benchmark. (a) The distribution of the MMOral-Uni benchmark, spanning five dental imaging modalities and covering five tasks. (b) ensure correctness. More examples can be found in the Appendix. The MMOral-Uni benchmark will be released publicly to facilitate comprehensive assessment of current MLLMs and to inform future research in multimodal AI for digital and intelligent dentistry. 3.2. Evaluation Metrics Following well-established benchmarks [15, 57, 58], we meticulously design few-shot prompt and use GPT-5-mini to conduct the open-ended evaluation. The prompt incorporates five in-context examples with free-form answers, covering fully correct, partially correct, and incorrect cases. GPT-5-mini assigns score ranging from 0 to 1 for each sample based on the input question, ground truth, and model prediction. We report the evaluation scores for each modality as well as the overall performance. Comprehensive ablation experiments have validated the feasibility and stability of using LLMs as judges; the full few-shot prompt and further details are provided in the Appendix. We have integrated the MMOral-Uni evaluation into the VLMEvalKit framework [9] to facilitate subsequent capability assessments of newly developed MLLMs. 4. Experiments 4.1. Experimental Setup Benchmarked MLLMs. We systematically evaluate OralGPT-Omni on the MMOral-Uni benchmark to provide comprehensive assessment of its performance on multimodal dental imaging analysis. We conduct evaluations of 27 representative MLLMs: 7 proprietary systems accessed via API [3, 6, 10, 37, 38, 46, 50], 12 general-purpose models [1, 4, 5, 21, 29, 33, 35, 47, 48, 51, 53, 55], and 8 medical MLLMs [7, 20, 22, 27, 39, 43, 52]. We also evaluate OralGPT-Omni on the MMOral-OPG benchmark [15] to assess its capability for panoramic X-ray analysis. Implementation Details. We utilize the LLaMA-Factory framework [63] to train OralGPT-Omni for the first three stages: DKI, DCA, and SFT, followed by the RLT stage using the ms-swift framework [62]. OralGPT-Omni is initialized with the Qwen2.5-VL-7B-Instruct pre-trained model [5] due to its exceptional instruction-following capabilities. The training of OralGPT-Omni is conducted on 2 NVIDIA A100 80G GPUs over approximately 90 hours. For the RLT stage, the GRPO policy generates six candidate rationales per sample, with sampling temperature of τ = 0.8. For further details on the hyperparameters employed in model training, please refer to the Appendix. 4.2. Comparisons on MMOral-Uni Benchmark The performance of various MLLMs on the MMOral-Uni benchmark is summarized in Table 1 and Fig. 5(b). In general, our OralGPT-Omni achieves the highest overall score of 51.84, significantly surpassing GPT-5s score of 15.42, attesting to its strong capabilities in multimodal imaging analysis within the dental domain. However, it is noteworthy that OralGPT-Omni underperforms compared to proprietary MLLMs in the recommended treatment planning task. We attribute this discrepancy to differences in task objectives. The treatment planning task requires the formulation of subsequent strategies based on final diagnosis. This necessitates deeper understanding of medical expertise related to surgical procedures and postoperative recovery, which is an area where proprietary MLLMs particularly excel. In contrast, OralGPT-Omni is primarily specialized in dental imaging analysis and abnormality diagnosis, with treatment planning data comprising only 0.006% of the whole training dataset. This limitation contributes to its lower performance in this specific task. Nevertheless, effective treatment planning depends critically on 6 Table 1. Results on the MMOral-Uni for various existing LVLMs across five dental imaging modalities. The abbreviations are defined as follows: II - Intraoral Image, PA - Periapical Radiograph, CE - Cephalometric Radiograph, PI - Pathological Image, TP - Treatment Planning, IV - Intraoral Video. The best-performing model in each category is highlighted in-bold, while the second-best is underlined. Model Proprietary LVLMs GPT-5 [37] o3 [38] Grok-4 [50] Claude-Sonnet-4-5-20250929 [3] Doubao-1.5-vision-pro-32k [6] Gemini-2.5-flash [46] GLM-4.5v [47] Open-Source LVLMs Qwen2.5-VL-7B [5] Qwen3-VL-8B-Instruct [53] Qwen3-VL-235B-A22B [53] GLM-4.1V-9B-Thinking [47] InternVL3.5-8B [48] LLaVA-v1.6-Mistral-7B [29] LLaVA-OneVision [21] MiMo-VL-7B [51] Phi-4-multimodal-instruct [1] Mistral-Small-3.1-24B [35] R-4B [55] Ovis2.5-9B [33] Medical Specific LVLMs LLaVA-Med [22] HuatuoGPT-Vision-7B [7] Lingshu-7B [52] MedVLM-R1 [39] Med-R1-Diagnosis [20] Chiron-o1-8B [43] MedGemma-27B [40] HealthGPT [27] OralGPT-Omni (Ours) Loc 44.60 45.00 11.80 36.30 26.90 31.00 21.70 11.20 14.30 18.70 15.50 10.10 4.80 7.50 25.40 3.70 16.30 3.80 15.40 5.40 11.90 12.00 10.10 6.30 4.60 11.90 18.00 66.80 II Dx-I 45.24 53.49 35.71 38.85 43.33 51.10 45.10 25.40 27.17 40.00 37.78 30.85 20.57 23.13 36.28 18.32 28.14 32.29 37.47 3.99 30.19 30.58 23.88 22.93 20.94 20.44 20.43 56.60 Dx-R 25.16 28.19 20.27 11.28 16.97 19.70 14.37 23.84 16.95 12.53 18.14 11.02 14.03 17.59 22.51 7.86 18.77 19.16 19. 5.81 23.10 25.77 14.30 8.91 17.37 17.09 8.84 39.99 PA CE PI TP IV Overall 31.43 37.48 30.89 38.03 40.48 37.01 38.37 19.72 33.54 33.62 37.57 32.95 11.61 14.34 34.62 18.31 29.15 28.96 36.70 25.31 25.79 27.48 16.25 20.13 34.95 21.65 19.44 48.11 41.27 28.60 22.60 33.33 18.20 36.30 31.00 13.70 4.97 26.70 13.53 14.80 2.50 0.00 18.70 1.67 11.50 16.60 18. 1.50 19.57 20.50 0.00 2.00 2.03 22.17 7.50 65.90 40.52 37.52 34.73 34.52 28.49 35.09 27.60 30.73 30.68 28.75 27.13 28.67 12.06 11.49 31.91 9.63 27.02 27.89 34.33 16.08 28.02 30.94 21.28 19.01 31.20 32.69 31.02 56.01 80.67 75.33 72.67 79.33 68.00 73.33 70.00 36.67 54.00 56.00 62.00 64.67 34.67 40.67 62.00 34.67 62.00 50.67 66. 28.67 41.33 48.00 24.00 16.00 42.00 68.67 54.67 47.33 56.00 50.00 23.00 29.00 45.00 46.00 33.00 12.00 18.00 23.00 43.00 14.00 18.00 5.00 43.00 7.00 28.00 19.00 61.00 14.00 19.00 20.00 26.00 22.00 23.00 11.00 14.00 65.00 36.42 38.70 28.47 30.32 30.67 35.72 31.05 22.88 23.45 27.83 27.86 23.39 13.54 15.39 29.62 12.12 23.69 24.94 29. 10.16 25.41 27.08 16.50 15.29 21.61 21.56 17.32 51.84 accurate diagnostic findings, underscoring the importance of OralGPT-Omni. Furthermore, we observe that existing medical MLLMs exhibit no substantial advantage over general MLLMs in the field of dentistry, further emphasizing the unique contribution of OralGPT-Omni in multimodal dental image analysis and reasoning. 4.3. Comparisons on MMOral-OPG Benchmark We evaluate OralGPT-Omni on the MMOral-OPG benchmark [15], specifically focusing on open-ended VQA for panoramic X-ray analysis. We assess performance across six clinically grounded dimensions, as shown in Table 2 and Fig. 5(c). OralGPT-Omni outperforms existing medical MLLMs, achieving an overall score of 45.31. Its scores on the the Teeth, Patho, His, Jaw, and Summ dimensions notably exceed those of GPT-4V, underscoring its strength in panoramic X-ray interpretation. By contrast, its report generation performance lags behind GPT-4V. We hypothesize that this gap arises from the intrinsic complexity of panoramic radiographs, which contain dense anatomical Table 2. Performance on the MMOral-OPG benchmark for various LVLMs on open-ended VQA tasks. The best-performing model is highlighted in-bold, while the second-best is underlined. Model General-purpose LVLMs GPT-5 [37] GPT-4V [18] Gemini-2.5-Flash [46] Qwen-Max-VL [4] Deepseek-VL-7b-chat [32] GLM-4V-9B [10] Qwen2.5-VL-72B [5] Medical Specific LVLMs LLaVA-Med [22] HealthGPT-XL32 [27] MedVLM-R1 [39] MedDr [17] OralGPT-Omni (Ours) Teeth Patho His Jaw Summ Report Overall Open-ended VQA 39.77 31.46 28.04 2.10 16.48 20.94 13.90 0.91 30.64 22.42 22.99 37. 29.32 23.79 24.77 4.47 7.50 9.70 15.83 1.52 25.83 13.71 32.58 43.94 44.05 39.51 31.90 7.06 13.44 18.77 15.40 0.00 27.98 24.42 29.57 55.34 78.56 69.81 47.81 11.62 34.56 26.62 27.12 0.00 51.12 43.88 52.44 70. 40.12 34.29 12.98 7.98 9.52 12.74 7.38 0.00 17.02 13.57 20.95 38.57 28.20 43.70 16.70 5.50 9.60 21.30 11.50 24.50 8.00 25.80 8.70 37.90 42.42 39.38 27.84 5.29 15.95 19.74 15.38 4.76 27.80 24.70 26.20 45. structures and multi-dimensional conditions that OralGPTOmni may not fully capture when generating comprehensive reports. 4.4. Ablation and In-Depth Study Effect of Four-Stage Training Strategy. We conduct ablation studies on MMOral-Uni to evaluate the effectiveness of 7 Figure 6. Three modalities of case studies on the MMOral-Uni benchmark are presented, with correct responses highlighted in green and obvious incorrect responses highlighted in red. Table 3. Ablation experiments of the four-stage training strategy and the TRACE-CoT reasoning pattern. No. Model Baseline (Qwen2.5-VL-7B) [5] +1st stage (DKI) +2nd stage (DCA) +3rd stage (SFT) +4th stage (RLT) SFT wo/ TRACT-CoT SFT w/ TRACT-CoT 1 2 3 4 5 6 7 - II Dx-I 25.40 24.43 25.44 52.61 56.60 44.26 52.61 +8.35 Loc 11.20 12.50 11.90 64.40 66.80 60.60 64.40 +3. Dx-R 23.84 22.96 23.22 38.02 39.99 33.73 38.02 +4.29 PA CE PI TP IV Overall 19.72 21.32 24.38 43.97 48.11 48.00 43.97 -4.03 13.70 14.37 17.70 63.30 65.90 58.93 63.30 +4.37 30.73 36.63 29.19 53.89 56.01 44.67 53.89 +9.22 36.67 38.67 50.00 35.33 47.33 36.00 35.33 -0.67 12.00 18.00 31.00 33.00 65.00 31.00 33.00 +2. 22.88 23.66 24.00 48.67 51.84 44.31 48.67 +4.36 the four-stage training strategy, with results presented in Table 3. Overall, this strategy progressively improves performance in dental image analysis. The first two stages, DKI and DCT, achieve modest improvement over the baseline model (Qwen2.5-VL-7B). By incorporating domainspecific dental knowledge corpus and concise descriptions of diverse dental imaging modalities, the overall score on MMOral-Uni increases from 22.88 to 24.00. The SFT stage, with carefully curated high-quality instruction data, yields substantial improvement, raising the overall score from 24.00 to 48.67. SFT markedly strengthens instruction following and provides solid foundation for explicit, transparent reasoning in dental image analysis, thereby preparing the model for the subsequent RLT stage. RLT further improves the overall score by 3.17 points and incentivizes stronger reasoning capabilities. Effect of TRACE-CoT Reasoning Data. Our proposed TRACE-CoT pattern mirrors the diagnostic decisionmaking process of radiologists and is designed to explicitly improve the transparency and trustworthiness of MLLMs. To assess whether explicit reasoning chains can enhance diagnostic accuracy, we perform an ablation study during the SFT phase. We compare the model trained with TRACE-CoT data against the model trained without it, and the results are shown in Table 3 (rows 6-7). Incorporating TRACE-CoT data during SFT increases the overall score by 4.36 points relative to training on answer-only data that excludes reasoning chains. Notably, in our instruction data, TRACE-CoT annotations are included only for the II-Dx-I, II-Dx-R, PA, and PI modalities, and the most significant improvements appear in II-Dx-I, II-Dx-R, and PI. These findings provide concrete evidence that TRACE-CoT data could strengthen diagnostic performance. Case Study & Clinical Feedback. We conduct an indepth case study of the OralGPT-Omni model, focusing on its performance across various dental imaging modalities, as illustrated in Fig. 6. In comparison to GPT-5 and the medical-specific MLLM Lingshu-7B [52], our OralGPTOmni demonstrates superior abnormality diagnosis capabilities in three modalities: intraoral images, periapical X-rays, and histopathological images. Additionally, it provided explicit TRACE-CoT reasoning patterns, which enhance its transparency and reliability. To further evaluate the clinical validity of OralGPT-Omni, we invite radiologist with over ten years of experience to perform comprehensive clinical assessments of the responses from three top-performing models, the details of which can be found in the Appendix. 5. Conclusion In this paper, we introduce OralGPT-Omni, the first dentalspecialized MLLM for comprehensive analysis across diverse dental imaging modalities and tasks. The success of our OralGPT-Omni relies on high-quality, large-scale, dental-specific multimodal datasets; the explicit TRACECoT reasoning pattern and data construction pipeline; and the four-stage training strategy. In addition, the proposed MMOral-Uni is the first unified benchmark dedicated to multimodal dental imaging analysis, which offers comprehensive evaluation suite for digital dentistry. Experiments show that OralGPT-Omni outperforms current state-of-the-art MLLMs on the MMOral-Uni and MMOral-OPG benchmarks by large margin. It also provides explicit reasoning chains that explain how the final diagnosis is reached. This work paves the way for future advancements in digital and intelligent dentistry by enabling natural language interaction, multimodal dental imaging analysis, and enhanced explainability in next-generation dental artificial intelligence."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. 1, 6, 7 [2] Manar Aljohani, Jun Hou, Sindhura Kommu, and Xuan Wang. comprehensive survey on the trustworthiness arXiv preprint of large language models in healthcare. arXiv:2502.15871, 2025. 4 [3] Anthropic. https://www.anthropic.com/news/claude-sonnet-4-5, 6, 7 Claude-sonnet-4-5. 2025. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1, 6, [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 4, 6, 7, 8 [6] ByteDance. https://www.volcengine.com/product/doubao, 2025. 7 Doubao-1.5-vision-pro. 6, [7] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. 6, 7, 1, 4, 5 [8] Chao Ding, Mouxiao Bian, Pengcheng Chen, Hongliang Zhang, Tianbin Li, Lihao Liu, Jiayuan Chen, Zhuoran Li, Yabei Zhong, Yongqi Liu, et al. Building human-verified clinical reasoning dataset via human llm hybrid pipeline for trustworthy medical ai. arXiv preprint arXiv:2505.06912, 2025. 4 [9] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. 6 [10] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. 6, 7 [11] Jing Hao, Moyun Liu, Lei He, Lei Yao, James Kit Hon Tsoi, and Kuo Feng Hung. Semit-sam: Building visual foundation model for tooth instance segmentation on panoramic radiographs. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 110 121. Springer, 2024. 1 [12] Jing Hao, Lun Wong, Zhiyi Shan, Qi Yong Ai, Xieqi Shi, James Kit Hon Tsoi, and Kuo Feng Hung. semi-supervised transformer-based deep learning framework for automated tooth segmentation and identification on panoramic radiographs. Diagnostics, 14(17):1948, 2024. 1 [13] Jing Hao, Yuxiang Zhao, Song Chen, Yanpeng Sun, Qiang Chen, Gang Zhang, Kun Yao, Errui Ding, and Jingdong Wang. Fullanno: data engine for enhancing image comarXiv preprint arXiv:2409.13540, prehension of mllms. 2024. [14] Jing Hao, Yonghui Zhu, Lei He, Moyun Liu, James Kit Hon Tsoi, and Kuo Feng Hung. T-mamba: unified framework with long-range dependency in dual-domain for 2d & 3d tooth segmentation. arXiv preprint arXiv:2404.01065, 2024. 1 [15] Jing Hao, Yuxuan Fan, Yanpeng Sun, Kaixin Guo, Lizhuo Lin, Jinrong Yang, Qi Yong Ai, Lun Wong, Hao Tang, and Kuo Feng Hung. Towards better dental ai: multimodal benchmark and instruction dataset for panoramic x-ray analysis. arXiv preprint arXiv:2509.09254, 2025. 3, 6, 7, 1 [16] Jing Hao, Andrew Nalley, Andy Wai Kan Yeung, Ray Tanaka, Qi Yong Ai, Walter Yu Hang Lam, Zhiyi Shan, Yiu Yan Leung, Abeer AlHadidi, Michael Bornstein, et al. Characteristics, licensing, and ethical considerations of openly accessible oral-maxillofacial imaging datasets: systematic review. npj Digital Medicine, 8(1):412, 2025. 1, 3, 5 [17] Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: large-scale medical Diagnosis-guided bootstrapping for vision-language learning. CoRR, 2024. 7, 1 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7 [19] Shehroz Khan, Petar Przulj, Ahmed Ashraf, and Ali Abedi. Chestgpt: Integrating large language models and vision transformers for disease detection and localization in chest x-rays. arXiv preprint arXiv:2507.03739, 2025. [20] Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. 4, 6, 7, 1 [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 6, 7 [22] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. 6, 7, 1 [23] Qingqiu Li, Zihang Cui, Seongsu Bae, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Junjun He, et al. Aor: Anatomical ontology-guided reasoning for medical large multimodal model in chest x-ray interpretation. arXiv preprint arXiv:2505.02830, 2025. 1 [24] Sijing Li, Tianwei Lin, Lingshuai Lin, Wenqiao Zhang, Jiang Liu, Xiaoda Yang, Juncheng Li, Yucheng He, Xiaohui Song, Jun Xiao, et al. Eyecaregpt: Boosting comprehensive ophthalmology understanding with tailored dataset, benchmark and model. arXiv preprint arXiv:2504.13650, 2025. 1 [25] Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, Guoan Wang, Chenglong Ma, Ying Chen, Ming Hu, et al. Gmai-vl & gmai-vl-5.5 m: large vision-language model and comprehensive multimodal dataset towards general medical ai. arXiv preprint arXiv:2411.14522, 2024. 1 [26] Yuci Liang, Xinheng Lyu, Wenting Chen, Meidan Ding, Jipeng Zhang, Xiangjian He, Song Wu, Xiaohan Xing, Sen Yang, Xiyue Wang, et al. Wsi-llava: multimodal large language model for whole slide image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2271822727, 2025. 1 [27] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large visionlanguage model for unifying comprehension and generation arXiv preprint via heterogeneous knowledge adaptation. arXiv:2502.09838, 2025. 6, [28] Chi Liu, Derek Li, Yan Shu, Robin Chen, Derek Duan, Teng Fang, and Bryan Dai. Fleming-r1: Toward expert-level medical reasoning via reinforcement learning. arXiv preprint arXiv:2509.15279, 2025. 4 [29] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1, 6, 7 [30] Zekai Liu, Qi Yong Ai, Andy Wai Kan Yeung, Ray Tanaka, Andrew Nalley, and Kuo Feng Hung. Performance of vision-language model in detecting common dental conditions on panoramic radiographs using different tooth numbering systems. Diagnostics, 15(18):2315, 2025. 1 [31] Zekai Liu, Andrew Nalley, Jing Hao, Qi Yong Ai, Andy Wai Kan Yeung, Ray Tanaka, and Kuo Feng Hung. The performance of large language models in dentomaxillofacial radiology: systematic review. Dentomaxillofacial Radiology, page twaf060, 2025. 1 [32] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. [33] Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, et al. Ovis2. 5 technical report. arXiv preprint arXiv:2508.11737, 2025. 6, 7 [34] Zijie Meng, Jin Hao, Xiwei Dai, Yang Feng, Jiaxiang Liu, Bin Feng, Huikai Wu, Xiaotang Gai, Hengchuan Zhu, Tianxiang Hu, et al. Dentvlm: multimodal vision-language model for comprehensive dental diagnosis and enhanced clinical practice. arXiv preprint arXiv:2509.23344, 2025. 1 Mistral-small-3.1-24b-instruct-2503. [35] mistralai. https://huggingface.co/mistralai/Mistral-Small-3.1-24BInstruct-2503, 2025. 1, 6, 7 [36] Chee Ng, Liliang Sun, and Shaoqing Tang. X-ray-cot: Interpretable chest x-ray diagnosis with vision-language arXiv preprint models via chain-of-thought reasoning. arXiv:2508.12455, 2025. 4, 1 [37] OpenAI. Gpt-5. https://openai.com/zh-HansCN/index/introducing-gpt-5, 2025. 6, 7, 4, 5, 10 [38] OpenAI. o3. https://openai.com/zh-HansCN/index/introducing-o3-and-o4-mini, 2025. 6, 7 [39] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement In International Conference on Medical Image learning. Computing and Computer-Assisted Intervention, pages 337 347. Springer, 2025. 6, 7, 1 [40] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, Cıan Hughes, Charles Lau, arXiv preprint et al. Medgemma technical report. arXiv:2507.05201, 2025. 7 [41] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 5, 2 10 [42] Yanzhou Su, Tianbin Li, Jiyao Liu, Chenglong Ma, Junzhi Ning, Cheng Tang, Sibo Ju, Jin Ye, Pengcheng Chen, Ming Hu, et al. Gmai-vl-r1: Harnessing reinforcement learning for multimodal medical reasoning. arXiv preprint arXiv:2504.01886, 2025. [43] Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, and Xiaosong Wang. Chiron-o1: Igniting multimodal large language models towards generalizable medical reasoning via mentorintern collaborative search. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. 6, 7 [44] Yanpeng Sun, Jing Hao, Ke Zhu, Jiang-Jiang Liu, Yuxiang Zhao, Xiaofan Li, Gang Zhang, Zechao Li, and Jingdong Wang. Descriptive caption enhancement with visual specialists for multimodal perception. arXiv preprint arXiv:2412.14233, 2024. 1 [45] Yu Sun, Xingyu Qian, Weiwen Xu, Hao Zhang, Chenghao Xiao, Long Li, Yu Rong, Wenbing Huang, Qifeng Bai, and Tingyang Xu. Reasonmed: 370k multi-agent generated arXiv preprint dataset for advancing medical reasoning. arXiv:2506.09513, 2025. 4 [46] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 6, 7 [47] Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, et al. Glm-4.5 and glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. https://arxiv.org/abs/2507.01006, 2025. 6, 7 [48] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 6, 7 [49] Penny Whiting, Anne WS Rutjes, Marie Westwood, Susan Mallett, Jonathan Deeks, Johannes Reitsma, Mariska MG Leeflang, Jonathan AC Sterne, Patrick MM Bossuyt, and QUADAS-2 Group*. Quadas-2: revised tool for the quality assessment of diagnostic accuracy studies. Annals of internal medicine, 155(8):529536, 2011. 3 [50] xAI. grok4. https://x.ai/grok, 2025. 6, 7 [51] LLM Xiaomi, Bingquan Xia, Bowen Shen, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, et al. Mimo: Unlocking the reasoning potential of language modelfrom pretraining to posttraining. arXiv preprint arXiv:2505.07608, 2025. 1, 6, [52] Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, et al. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning. arXiv preprint arXiv:2506.07044, 2025. 6, 7, 8, 4, 5, 10 [53] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 6, 7, 4, 5 11 [54] Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, et al. Pediatricsgpt: Large language models as chinese medical assistants for pediatric applications. Advances in Neural Information Processing Systems, 37:138632138662, 2024. 1 [55] Qi Yang, Bolin Ni, Shiming Xiang, Han Hu, Houwen Peng, and Jie Jiang. R-4b: Incentivizing general-purpose autothinking capability in mllms via bi-mode annealing and reinforce learning. arXiv preprint arXiv:2508.21113, 2025. 6, 7 [56] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [57] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 6 [58] Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024. 6 [59] Yuheng Zha, Kun Zhou, Yujia Wu, Yushu Wang, Jie Feng, Zhi Xu, Shibo Hao, Zhengzhong Liu, Eric Xing, and Zhiting Hu. Vision-g1: Towards general vision language reasoning with multi-domain data curation. arXiv preprint arXiv:2508.12680, 2025. 5 [60] Jia Zhang, Bodong Du, Yitong Miao, Dongwei Sun, and Xiangyong Cao. Oralgpt: two-stage vision-language model for oral mucosal disease diagnosis and description. arXiv preprint arXiv:2510.13911, 2025. 1 [61] Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, and Hoifung Poon. Med-rlvr: Emerging medical reasoning from 3b base model via reinforcement learning. arXiv preprint arXiv:2502.19655, 2025. 1 [62] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, et al. Swift: scalable lightweight infrastructure for fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2973329735, 2025. 6, 2 [63] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. 6, 2 [64] Juexiao Zhou, Xiaonan He, Liyuan Sun, Jiannan Xu, Xiuying Chen, Yuetan Chu, Longxi Zhou, Xingyu Liao, Bin Zhang, Shawn Afvari, et al. Pre-trained multimodal large language model enhances dermatological diagnosis using skingpt-4. Nature Communications, 15(1):5649, 2024. OralGPT-Omni: Versatile Dental Multimodal Large Language Model"
        },
        {
            "title": "Contents",
            "content": "1. Related Works 2. Details on Training Data Construction 3. Implementation Details of Model Training 4. MMOral-Uni Benchmark 5. LLM as the judges for MMOral-Omni: Feasibility Analysis 6. Case Study and Clinical Validity of OralGPT-"
        },
        {
            "title": "Omni",
            "content": "1. Related Works 1 1 2 4 5 Medical Large Vision-Language Models. Medical MLLMs have shown great potential to serve as valuable assistants for clinicians, researchers, and trainees by providing an interactive natural language interface for analyzing medical images across diverse modalities. Recent efforts [7, 17, 22, 25] aim to develop medical general-purpose MLLMs that are capable of simultaneously analyzing and responding to images and instructions from multiple medical disciplines. Meanwhile, more researchers are dedicated to building discipline-specialized medical MLLMs, such as dermatology [64], ophthalmology [24], chest [19], pathology [26], and pediatrics [54]. These specialized models demonstrate superior imaging diagnostic performance within their respective fields compared to those medical general-purpose MLLMs. However, dentistry-specific MLLMs remain largely underexplored. OralGPT [15], trained on large-scale instruction dataset, represents the first vision-language model specialized for panoramic Xray analysis. Jia Z. et al. [60] finetuned the Qwen2.5-VL model using less than 2k intraoral images to achieve diagnosis of four types of oral mucosal diseases. DentVLM [34] supports basic oral disease diagnosis on three imaging modalitiespanoramic, lateral, and intraoral imagesbut lacks the capability to provide detailed explanations. In contrast, our OralGPT-Omni offers comprehensive analysis across seven dental imaging-based modalities as well as text-only interactions. It also generates explicit chain-ofthoughts, significantly enhancing the models transparency and trustworthiness. Medical Chain-of-Thought Reasoning. Existing medical MLLMs have made significant progress through supervised fine-tuning (SFT) on specific instruction datasets. However, these models [17, 22, 25, 34, 60] primarily generate final answers without revealing the underlying reasoning processes, due to the bias towards memorizing task-specific shortcuts rather than learning generalizable reasoning in the SFT stage. Prior studies, including MedVLM-R1 [39], Med-R1 [20], and MED-RLVR [61], have attempted to enhance the reasoning abilities of medical MLLMs via reinforcement learning. Nevertheless, the quality of the generated reasoning chains heavily relies on the base policy model. To address this limitation, GMAI-VL-R1 [42] designs reasoning data synthesis approach that employs GPT-4o for step-by-step reasoning generation through rejection sampling. However, it still suffers from the hallucination sourced from GPT-4o. To alleviate hallucination issues, Q. Li et al. [23] constructed the AOR-Instruction reasoning dataset by incorporating explainable region-level visual content based on anatomical regions and ontologies. Similarly, X-Ray-CoT [36] created reasoning data for chest X-rays by integrating general medical knowledge and visual concept descriptions using the CoT prompting strategy. In contrast, we propose the TRACE-CoT reasoning pattern that closely mirrors the diagnostic decision-making process of radiologists, which is presented in Section 2.2. 2. Details on Training Data Construction comprehensive list of all data sources utilized during the training phase is presented in Table 4. The training corpus consists of approximately 3.21 million text toIt includes varikens, 59,658 images, and 90 videos. ous types of dental-specific data, such as intraoral images and videos, panoramic radiographs, periapical radiographs, histopathological images, cephalometric radiographs, 3D text-only data, and interleaved image-text model scans, data. Additionally, we provide detailed descriptions of abnormalities and clinically relevant tasks associated with each modality, highlighting the extensive coverage of our Regarding OralGPT-Omni in dental imaging analysis. TRACE-CoT data generation, we construct an automatic pipeline utilizing sparse label categories, the visual appearance of abnormalities, and dental knowledge from academic publications and Wikipedia, instructing GPT-5-mini to generate 36,777 TRACE-CoT reasoning chains. Since abnormality taxonomies differ across datasets, we craft datasetspecific prompts tailored to each taxonomy to ensure the quality of the reasoning chains. The details of these prompt designs are presented in Figures 9, 10, 11, 12, 13. The dental expertise used in the prompts is summarized in Table 7. Besides, we also provide some data used in the training Table 4. Comprehensive list of dental-specific data sources used in the training phase, including corresponding abnormalities and tasks. No 1.1 1.2 1.3 2.1 Modality Text-only data Text-only data Text-only data Abnormality / Subject 16 Undergraduate textbooks in dentistry Dialogue data from dental forums Open-domain oral disease QA dataset Intraoral image Teeth location and counting 2.2 Intraoral image 2.3 Intraoral image Image-level analysis: Calculus, Caries, Gingivitis, Ulcer Tooth discoloration, Defective dentition Cancer, Normality, Orthodontics Region-level analysis: Abrasion, Filling, Crown, Caries, Gingivitis, Deep Overjet, Fenestration and Dehiscence, Tooth Torsion, Tooth emergence, Invisible orthodontic attachment, Fixed orthodontic device, Case fixed orthodontic appliances, Tooth misalignment, Mandibular retrusion, Orthodontic Brace, Dental plaque 3.0 Periapical radiograph Impacted tooth, Pulpitis, Caries, Periodontitis, Crown, Apical periodontitis, Bone loss, Root canal treatment, Restoration, Mixed dentition 4.0 Cephalometric radiograph 5.0 Histopathological image 6.0 7.0 8.0 9. Intraoral video Interleaved image-text data 3D model scan Panoramic radiograph 29 cephalometric landmarks detection, Cervical vertebral maturation (CVM) stage prediction Leukoplakia without dysplasia, Leukoplakia with dysplasia, Oral squamous cell carcinoma, Oral submucous fibrosis, Healthy epithelial nucleus, Abnormal epithelial nucleus, Blood cell nucleus, Reactive cell nucleus, Dividing nucleus Intraoral videos of real dental treatments 6 Clinical Cases Guidelines in Dentistry for treatment planning Image description Open-Ended VQA for panoramic X-ray analysis Source / URL URL URL URL URL URL URL URL URL URL URL URL URL URL One in-house dataset URL URL URL URL URL URL URL URL URL URL URL URL URL URL URL URL URL URL / stage for an intuitive understanding, which can be found in Figure 14 - Figure 22. vide the word clouds of training datasets used in DKI, DCA, and SFT stages, which are depicted in Figure 8. 3. Implementation Details of Model Training 3.1. Hyperparameters of four-stage training paradigm We employ the four-stage training strategy that progressively strengthens the multimodal understanding and reasoning ability of the OralGPT-Omni model. The hyperparameters used in each training stage are summarized in Table 5. We utilize the LLaMA-Factory framework [63] to train OralGPT-Omni for the first three stages: DKI, DCA, and SFT, followed by the RLT stage using the ms-swift framework [62]. The GPU Hour for each stage is estimated based on the NVIDIA A100 80G GPU. We also pro3.2. Reinforcement Learning Tuning To further incentivize the reasoning capability of OralGPTOmni, we implement reinforcement learning tuning within the Group Relative Policy Optimization (GRPO) [41] algorithm on the OralGPT-Omni model. GRPO calculates group-relative advantage from multiple samples of the same prompt instead of learning separate value function, making it particularly effective for training tasks where correctness can be verified. During the training, we optimize the OralGPT-Omni with the GRPO loss JGRPO(θ): 2 Table 5. The hyperparameters used in the four-stage training strategy. The GPU Hours are estimated based on the NVIDIA A100 80G GPU."
        },
        {
            "title": "DCA",
            "content": "1 4 Batch Size Gradient Accumulation Step Trainable Component LoRA Rank # Generations Learning Rate Warmup Ratio bf16 # Epoch / Step GPU Hours 1 4 Language model 8 / 1.0e-5 0.1 True 1 epoch"
        },
        {
            "title": "SFT",
            "content": ""
        },
        {
            "title": "RLT",
            "content": "4 3 Projector Visual encoder, Projector, Language model Language model 8 / 1.0e-5 0.1 True 1 epoch 20 8 / 1.0e-4 0.1 True 1 epoch 50 8 6 1.0e-6 0.05 True 2000 steps 100 JGRPO(θ) = (Oq) {oi}G qP(Q), i=1πθold πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) ˆAi,t, (cid:34) 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:32) min , 1 ϵ, 1 + ϵ (cid:33) (cid:17) ˆAi,t clip (cid:16) πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) β DKL[πθ πref ] (cid:35) , (1) where πθ and πθold are the current and old policy. πref is the reference model, which, in this case, is the preceding SFT model with the TRACE-COT pattern. The and are the questions and outputs sampled from our dataset and the old policy πθold, respectively. The ϵ and β are hyperparameters for stabilizing training. ˆAi,t is the advantage of the relative rewards of the outputs in each group. For each response, we use an LLM-based judge model to evaluate its correctness reward and our proposed TRACE-based reasoning reward. The detailed reward computation will be discussed in Sec. 3.3. The reward ri is defined within the range [0, 1]. We use the normalized reward as the advantage: ˆAi,t = ri mean(r) std(r) . (2) Besides, we use an unbiased estimator to estimate the KL divergence DKL: DKL[πθ πref] = πref(oi,t q, oi,<t) πθ(oi,t q, oi,<t) log πref(oi,t q, oi,<t) πθ(oi,t q, oi,<t) 1. (3) 3.3. Reward Computation for GRPO In conventional RL-based tuning paradigms, supervision is applied only to the final <answer>, while the intermedi3 ate reasoning process <think> remains unregulated. This often leads to cases where the final output appears correct, yet the underlying reasoning is flawed or logically unsound. Such discrepancies pose substantial safety risks in medical AI, where unreliable or hallucinated reasoning may still produce superficially plausible answer but lead to harmful or unsafe clinical interpretations. To address this issue, we introduce the TRACE-based reward Rtrace , medically aligned reward framework that uses an LLM-based judge model to comprehensively evaluate the quality of the reasoning trace. This framework provides fine-grained supervision over the models diagnostic reasoning process, ensuring that the generated clinical rationale is logically coherent, medically accurate, and consistent with the final prediction, rather than supervising only the end result. To operationalize this framework, the evaluation is decomposed into three key dimensions: Factual Knowledge Soundness. This dimension measures the correctness and clinical reliability of domain knowledge referenced in the <Think> section, including oral medicine expertise and pathological criteria. Incorrect medical statements receive d1 = 0; partially correct or incomplete knowledge receives d1 = 1; and fully accurate, evidence-based knowledge receives d1 = 2. Logical Coherence. This aspect assesses whether the diagnostic reasoning path presented in the <Think> section is logically consistent with the visual appearance descriptions in the <Caption>. The judge model evaluates whether the reasoning contains logical gaps, unsupported inferences, or clinically implausible conclusions. Fully coherent and clinically sound reasoning receives score of d2 = 1, whereas reasoning that is logically inconsistent or contradictory receives score of d2 = 0. Answer Consistency. This reward evaluates whether the gold-standard answer can be directly justified by the reasoning trace and the feature-based verification. The judge model ensures that the conclusion is supported by stated evidence without contradictions, hallucinated features, or reliance on unstated assumptions. Unsupported conclusions receive d3 = 0; partially supported conclusions receive d3 = 1; and fully justified, clinically consistent conclusions receive d3 = 2. The TRACE-based reward Rtrace is then computed as the average of these three dimensions to get the normalized reward. Following traditional RTL reward computation, both the answer reward Ranswer and the format reward Rformat are considered. The Rtrace and Ranswer are provided by GPT-5nano, acting as reward-judger. The prompts for TRACEbased reward and answer reward are provided in Figure 23 and Figure 24, respectively. The values for these three reward components range from 0 to 1. The overall reward signal unifies these components into single formulation: Rtotal = α Ranswer + β IRanswer>0 Rtrace + γ Rformat, (4) where α + β + γ = 1. Note that the Rtrace is ineffective when the Ranswer is completely wrong in order to ensure the consistent correctness of the reasoning and answers. 4. MMOral-Uni Benchmark Our MMOral-Uni benchmark is the first unified benchmark for dental multimodal imaging analysis, spanning five modalities and five tasks. It includes 2,809 open-ended QA pairs to simulate realistic user interactions. The modalities cover intraoral photographs, periapical and cephalometric radiographs, pathological images, intraoral videos, and interleaved imagetext inputs. The tasks include abnormality diagnosis, cervical vertebral maturation (CVM) stage prediction, treatment planning, tooth localization and counting, and dental treatment video comprehension. The specific quantities for each task and modality are presented in Table 10. Regarding the abnormality diagnosis, the MMOralUni benchmark includes 40 categories of abnormalities, which are summarized in Table 8. We also provide some examples in the MMOral-Uni benchmark for an intuitive understanding of this benchmark, which can be found in Figure 25 - Figure 29. 5. LLM as the judges for MMOral-Omni: A"
        },
        {
            "title": "Feasibility Analysis",
            "content": "Effectiveness. To verify the effectiveness of LLM-based evaluation for the MMOral-Omni benchmark, we invite two professional dentists to objectively score the outputs of different LVLMs. We calculate the absolute difference between the evaluators scores and the human-annotated scores. Specifically, the few-shot prompts designed for LLM-based evaluation are presented to the dentists to guide the evaluation criteria. The two dentists then independently scored the predictions of GPT-5 and Lingshu-7B on 300 cases from the MMOral-Omni benchmark based on these criteria. The absolute differences between human scores and evaluators scores are shown in Table 9, represented as . Overall, the absolute differences of the Overall metric given by dentists fluctuate by approximately 2 points in comparison to the LLM-based evaluations for the predictions of both LVLMs (GPT-5 and Lingshu-7B). This observation indicates that human scoring preferences generally align with the trends observed in LLM-based evaluations. However, it also suggests that there are subjective differences in the dentists interpretations of the evaluation criteria outlined in the few-shot prompts. For each subcategory, Dentist shows smaller differences in scores compared to the LLM-based evaluation for questions in the II-Loc, IIDx-I, II-Dx-R, PA, CE, and PI, whereas the differences are larger for the TP and IV categories. Although Dentist exhibits slightly larger differences with LLMbased scoring across all subcategories, their overall score difference is only 2.43 points. This indicates that LLMbased scoring aligns well with human preferences in reflecting the overall performance of LVLMs on the MMOralOmni benchmark. At the same time, we speculate that the score fluctuations in each subcategory are strongly associated with the subjective perceptions of human evaluators. Stability. Since using LLMs as judges inevitably introduces randomness, even with the temperature hyperparameter set to 0, we conduct multiple repeated experiments to verify the stability of LLMs as judges. Specifically, we evaluate the prediction results of GPT-5 [37], Qwen3-VL8B [53], Lingshu-7B [52], and HuatuoGPT-Vision-7B [7] on the MMOral-Omni benchmark using GPT-5-mini [37] with the same prompt five times. The obtained mean, standard deviation, and coefficient of variation (CV) of the metric overall are shown in Table 6. For proprietary models, medical-specific models, and general-purpose LVLMs, the standard deviation of the metric overall is no more than 0.212 when evaluated 5 times using GPT-5-mini with our designed few-shot prompt. Specifically, for the prediction results of GPT-5, the standard deviation of the scores is 0.179, while for Qwen3-VL-8B, the standard deviation is as low as 0.039. Meanwhile, CV (Coefficient of Variation), as standardized measure of dispersion of probability distribution, can be used to assess the stability of scores across multiple experiments. The CV values for the prediction results of these four models, after being scored 5 times, are all around 0.5%, which demonstrates the evaluation stability of using LLMs as evaluators. The detailed results across each specific category are demonstrated in Figure 7. 4 Figure 7. The means and standard deviations of each category on 5 repeated evaluations across four LVLMs predictions. 6. Case Study and Clinical Validity of OralGPT-Omni Table 6. Stability verification of using LLMs as judges: Standard deviation and coefficient of variation (CV) are reported across four LVLMs from five repeated evaluations. Model GPT-5 [37] Qwen3-VL-8B [53] Lingshu-7B [52] Mean StdDev CV % 36.446 23. 26.910 0.179 0.039 0.127 0.212 0. 0.164 0.472 0.838 HuatuoGPT-Vision-7B [7] 25.258 tential clinical value of our OralGPT-Omni. We provide more case studies to demonstrate the superiority of our OralGPT-Omni, as illustrated in Figure 30, Figure 31, and Figure 32. From clinical perspective, the ability to accurately identify common dental diseases and conditions is fundamental prerequisite for diagnosis and treatment planning. To explore the clinical validity of the OralGPT-Omni, we invite professoriate faculty member in oral-maxillofacial radiology with over ten years of experience in diagnostic imaging studies to conduct in-depth clinical assessments on the outputs of the top-performing models (GPT-5, Lingshu-7B, and our OralGPT-Omni). The clinical validation of the model outputs on four representative modalities is demonstrated in Figure 33, Figure 34, Figure 35, and Figure 36. The cases and clinical feedback effectively demonstrate the superior performance and po5 Table 7. The category, visual appearance, and relevant dental knowledge utilized in prompts for the TRACE-CoT data construction pipeline across multiple datasets."
        },
        {
            "title": "Installed crown",
            "content": "Caries 1 class Caries 2 class Caries 3 class Caries 4 class Caries 5 class Caries 6 class Primary tooth decay"
        },
        {
            "title": "Permanent\ndecay",
            "content": "tooth Caries in fissures and blind pits of teeth (occlusal surfaces of molars and premolars, buccal surfaces of molars, lingual surfaces of upper incisors) Caries of the contact surfaces of molars and premolars. Caries of the contact surfaces of incisors and canines without damage to the cutting edge. Caries of the contact surfaces of incisors and canines with damage to the cutting edge. Cervical caries of the vestibular and lingual surfaces. Caries of the cutting edges of the front teeth and the cusps of the chewing teeth. Occlusal surface of primary molar shows small, dark brown spot in the central pit, with surrounding enamel appearing slightly chalky. Minor enamel surface irregularities are visible along the grooves. Occlusal surface of permanent molar shows large, dark brown cavitated lesion extending into the central pit and fissures. Enamel margins appear undermined, and surface texture is roughened. 6 Abrasion typically appears as flat, smooth, and well-defined areas of hard tissue loss, often wedge-shaped at the cervical margins, with shiny polished surface. filling is visually recognized as distinct restorative material embedded in the tooth, usually with regular geometric outline and surface texture or color different from natural enamel or dentin. crown presents as full-coverage artificial cap over the tooth, with smooth and uniform surfaces, clear margins near the gingiva, and contours that may look slightly different from natural tooth anatomy depending on the material. Caries are seen as irregular areas of demineralization or cavitation, with rough surfaces and discoloration ranging from chalky white to brown or black, often with undermined enamel edges. The affected primary tooth shows localized brownish or dark spot on the occlusal or smooth surfaces, with enamel appearing softened or chalky. The child may exhibit mild sensitivity to sweet or cold foods, occasional discomfort when chewing, and sometimes visible plaque accumulation near the lesion. Gingival tissue surrounding the tooth may appear slightly inflamed. Permanent teeth with decay display larger cavitated lesions, often dark brown or black in color, with enamel margins appearing undermined. Patients may complain of prolonged sensitivity to cold, sweet, or acidic foods, intermittent pain while chewing, and visible plaque or calculus accumulation at the affected site."
        },
        {
            "title": "URL",
            "content": "Localized gingival recession with partially exposed root surfaces, thin labial mucosa allowing clear visualization of root contours, cementoenamel junction, and subtle convexities, and small fenestration windows or linear dehiscence defects present in the alveolar bone beneath. The region shows marked redness, edema, and/or marginal gingival hypertrophy of the unit or spontaneous bleeding, papillary, congestion, or ulceration. soft, adherent film on the tooth surface, typically pale yellow to light brown, often along the gingival margin or in pits and fissures."
        },
        {
            "title": "Tooth torsion",
            "content": "A condition where tooth is rotated or twisted around its longitudinal axis, resulting in an abnormal orientation within the dental arch."
        },
        {
            "title": "Deep overjet",
            "content": "A dental malocclusion where the upper front teeth (maxillary incisors) horizontally overlap the lower front teeth (mandibular incisors) to an excessive degree. Invisible orthodontic attachment"
        },
        {
            "title": "Tooth emergence",
            "content": "These attachments, part of clear aligner or aesthetic orthodontic treatments, are tooth-colored or transparent devices bonded to teeth to aid in controlled tooth movement without the visibility of traditional braces. The process by which tooth moves through the alveolar bone and soft tissue to appear in the oral cavity. This is natural phase of dental development. Anterior teeth with fenestration and dehiscence exhibit localized gingival recession with exposed root surfaces. The thin labial mucosa allows visualization of the underlying root contour, and alveolar bone may be partially or completely deficient in the affected regions. Clinically, patients are often asymptomatic or may report mild sensitivity at the exposed sites. Severe inflammation of gingivitis presents with intense redness and deep erythema of the gingival margins and interdental papillae, pronounced swelling, shiny and moist surface, spontaneous bleeding in some areas, hypertrophy and deformation of papillae, and occasional ulcerations. Dental plaque appears as soft, adherent film on tooth surfaces, typically pale yellow to light brown, often along the gingival margin or in pits and fissures. Clinically, patients are usually asymptomatic or may report mild sensitivity to sweet foods, with slight gingival redness and minimal inflammation. Tooth torsion results in tooth noticeably rotated around its longitudinal axis, causing misalignment within the dental arch; this rotation creates uneven spacing with adjacent teeth, sometimes overlapping or crowding neighboring teeth, which can lead to difficulty in chewing, localized food impaction, accumulation of plaque, and mild inflammation of the surrounding gingival tissue. Deep overjet occurs when the upper front teeth are prominently protruded over the lower front teeth, producing pronounced horizontal overlap; this may impair lip closure, cause difficulty in biting or chewing, increase the risk of trauma to the anterior teeth or lips, and create aesthetic concerns due to facial profile changes. Invisible orthodontic attachment involves small, tooth-colored or transparent attachments bonded to the teeth to guide clear aligner therapy, blending almost seamlessly with the natural tooth surface; patients may experience minor gum irritation, localized plaque accumulation, or slight sensitivity during initial treatment stages. Tooth emergence occurs when tooth is erupting through the gingival tissue, causing visible swelling and redness around the emerging crown; this process may result in mild tenderness, discomfort during chewing, and localized inflammation, sometimes accompanied by increased salivation or irritability."
        },
        {
            "title": "Dental Knowledge",
            "content": "fixed"
        },
        {
            "title": "Cast\northodontic\nappliance",
            "content": "Tooth misalignment Custom-made orthodontic devices, such as fixed space maintainers or lingual braces, which are fabricated using dental impressions (casts) of the patients teeth and are permanently attached to the teeth to correct or maintain dental alignment. Improper positioning of teeth within the dental arch, leading to malocclusion. This includes various forms of malpositions such as crowding, spacing, and deviations from the ideal arch form. Mandibular retrusion skeletal condition characterized by the posterior positioning of the mandible relative to the maxilla, often contributing to the class II malocclusion."
        },
        {
            "title": "Fixed orthodontic\ndevice",
            "content": "A comprehensive orthodontic appliance consisting of brackets, wires, and sometimes bands, which is fixed to the teeth. This device applies continuous pressure to move teeth into the desired position. Any orthodontic apparatus that is bonded to the teeth and remains in place throughout the treatment period. These devices, which include traditional braces and certain types of retainers, are used to correct dental and skeletal discrepancies by exerting controlled forces over time. Cast fixed orthodontic appliance consists of custom-made fixed devices bonded to teeth, precisely matching the dental arch; these appliances may be metallic or tooth-colored, producing initial soreness, localized gum irritation, and requiring meticulous oral hygiene to prevent plaque accumulation. Tooth misalignment occurs when teeth are irregularly positioned in the dental arch, showing crowding, spacing, or tilting; this misalignment can interfere with normal bite function, create difficulty in chewing, increase plaque accumulation, and elevate the risk of gingival inflammation or caries. Mandibular retrusion presents with the lower jaw positioned posteriorly relative to the upper jaw, visible in profile view; this skeletal discrepancy can cause Class II malocclusion, impaired occlusion, difficulty chewing, and altered facial aesthetics with receding chin. Orthodontic brace consists of fixed braces with metal or ceramic brackets bonded to teeth with connecting archwires, sometimes including colored elastics; these appliances apply continuous force to correct alignment, causing mild soreness, gum irritation, and requiring careful oral hygiene. Fixed orthodontic device remains permanently bonded to teeth throughout treatment, including they may traditional braces or fixed retainers; cause mild soreness, gum irritation, difficulty in maintaining oral hygiene, and localized plaque accumulation. Figure 8. The word cloud maps for training datasets used in the DKI, DCA, and SFT stages, respectively. 8 Figure 9. The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for image-level diagnosis of intraoral images. Table 8. Categories of analysis for abnormal conditions and diseases."
        },
        {
            "title": "Tooth discoloration\nOrthodontics\nCaries\nInvisible orthodontic attachment\nMandibular retrusion\nImpacted tooth\nBone loss\nLeukoplakia without dysplasia\nhealthy epithelial nucleus\nDividing nucleus",
            "content": "9 Figure 10. The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for region-level diagnosis of intraoral images. Table 9. Average absolute differences () between the evaluation scores of the LLM-based evaluator and the dentist-annotated scores on the MMOral-Omni benchmark. Model Evaluators II-Loc II-Dx-I II-Dx-R PA CE PI TP IV Overall GPT-5 [37] Lingshu7B [52] GPT-5-mini Dentist () GPT-5-mini Dentist 44.60 45.82 +1.22 12.00 18.20 45.24 43.24 -2.00 30.58 32.96 25.16 27.58 +2.42 25.77 28. 31.43 35.30 +3.87 27.48 23.34 41.27 39.50 -1.77 20.50 26.83 40.52 38.17 -2.35 30.94 28. 80.67 60.56 -20.11 48.00 42.24 56.00 49.80 -6.20 20.00 14.50 36.42 34.94 -1.48 27.08 29. () +6.20 +2.38 +3.18 -4.14 +6. -2.68 -5.76 -5.50 +2.43 10 Figure 11. The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for diagnosis of periapical radiographs. Table 10. Detailed compositions of tasks and modalities in the MMOral-Omni benchmark."
        },
        {
            "title": "Diagnosis of Abnormality",
            "content": "Intraoral Image Periapical X-ray Pathological Image"
        },
        {
            "title": "CVM Stage Prediction",
            "content": "Cephalometric X-ray Tooth Location & Counting"
        },
        {
            "title": "Treatment Planning",
            "content": "Interleaved Image-Text Data"
        },
        {
            "title": "Intraoral Video",
            "content": "1462 539 383 300 100 15 10 Figure 12. The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for diagnosis of pathological images. 12 Figure 13. The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for comprehension of intraoral videos. 13 Figure 14. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. Figure 15. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. 15 Figure 16. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. 16 Figure 17. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. Figure 18. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. 18 Figure 19. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. 19 Figure 20. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. Figure 21. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. 21 Figure 22. Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth. 22 Figure 23. The prompt for computing the TRACE-based reward using GPT-5-nano. Figure 24. The prompt for computing the answer reward using GPT-5-nano. 24 Figure 25. Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists. 25 Figure 26. Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists. Figure 27. Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists. 27 Figure 28. Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists. 28 Figure 29. Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists. Figure 30. Case study on the intraoral image modality, with correct responses highlighted in green and obvious incorrect responses highlighted in red. 30 Figure 31. Case study on the intraoral image modality, with correct responses highlighted in green and obvious incorrect responses highlighted in red. 31 Figure 32. Case study on the periapical X-ray modality, with correct responses highlighted in green and obvious incorrect responses highlighted in red. Figure 33. Case study and clinical validity on the intraoral image modality, with correct responses highlighted in green and obvious incorrect responses highlighted in red. 33 Figure 34. Case study and clinical validity on the periapical X-ray modality, with correct responses highlighted in green and obvious incorrect responses highlighted in red. 34 Figure 35. Case study and clinical validity on the histopathological image modality, with correct responses highlighted in green and obvious incorrect responses highlighted in red. Figure 36. Case study and clinical validity on the cephalometric radiograph modality, with correct responses highlighted in green and obvious incorrect responses highlighted in red."
        }
    ],
    "affiliations": [
        "College of Artificial Intelligence, Shenzhen University",
        "College of Computer Science and Software Engineering, Shenzhen University",
        "Faculty of Dentistry, The University of Hong Kong",
        "School of Biomedical Engineering, Southern Medical University",
        "School of Computer Science, Peking University",
        "Singapore University of Technology and Design",
        "The Hong Kong University of Science and Technology (GZ)",
        "University of Auckland",
        "University of Science and Technology of China"
    ]
}