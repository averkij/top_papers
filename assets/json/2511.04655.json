{
    "paper_title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable Non-Visual Shortcuts",
    "authors": [
        "Ellis Brown",
        "Jihan Yang",
        "Shusheng Yang",
        "Rob Fergus",
        "Saining Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to ``game'' their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly ``training on the test set'' -- probing the released test set for its intrinsic, exploitable patterns. We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via $k$-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score $s(x)$. We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original."
        },
        {
            "title": "Start",
            "content": "Benchmark Designers Should Train on the Test Set to Expose Exploitable Non-Visual Shortcuts"
        },
        {
            "title": "Abstract",
            "content": "Robust benchmarks are crucial for accurately evaluating Multimodal Large Language Models (MLLMs). However, we find that models can ace many multimodal benchmarks without strong visual understanding by exploiting biases, linguistic priors, and superficial patterns. This is particularly problematic for vision-centric benchmarks, which explicitly aim to require visual inputs to be solved. We introduce diagnostic principle for robust benchmark design: if benchmark can be gamed, it will be. Therefore, designers should proactively try to game their own benchmarks first as key step in the development lifecycleadopting rigorous diagnostic and debiasing procedures to systematically identify, quantify, and mitigate non-visual biases. We demonstrate that effective diagnosis of these issues must involve directly training on the test seti.e., probing the specific test set being released for its intrinsic, exploitable patterns. To demonstrate an effective realization of this standard, we propose systematic approach involving two core components: First, we diagnose benchmark susceptibility using Test-set Stress-Test (T T) methodology. The primary diagnostic tool involves fine-tuning powerful Large Language Model (LLM) via 洧녲-fold cross-validation on exclusively the non-visual, textual inputs of the test set to unveil shortcut performance and derive quantitative, sample-level bias score, 洧(洧논). We complement this with lightweight Random Forest-based diagnostic trained on hand-crafted features, enabling rapid auditing and interpretable bias analysis. Second, we debias benchmarks by systematically filtering samples identified as highly biased according to 洧(洧논) using an Iterative Bias Pruning (I P) procedure. Applying this framework to four prominent benchmarksVSIBench, CV-Bench, MMMU, and VideoMMEwe uncover substantial and pervasive non-visual biases. As case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating marked reduction in non-visual solvability and significantly wider vision-blind performance gap compared to the original. Project Page: https://cambrian-mllm.github.io 5 2 0 2 6 ] . [ 1 5 5 6 4 0 . 1 1 5 2 : r Figure 1 The Evolving Landscape of Visual Understanding Benchmarks. As benchmarks evolved from controlled, narrow tasks to open-ended VQA, they gained expressivity but became vulnerable to non-visual shortcuts. Language-driven evaluation enables flexible querying but risks models exploiting linguistic patterns rather than visual understanding. 1. Introduction Despite significant advances in visual understanding, evaluating progress remains challenging. From MNIST [17] to ImageNet [11], from COCO [19] to VQA [4], benchmarks have served as the backbone of progress in computer vision. As tasks grew more complex and models more capable, benchmarks evolved accordinglyshifting from clean, domain-specific datasets to open-ended, real-world evaluations  (Fig. 1)  . Today, with multimodal LLMs, we can ask models anything about an imagebut this expressive power comes at hidden cost: weve lost control over whats being measured. The uncomfortable truth is that models can ace multimodal benchmarks without strong visual understanding. They exploit biases, linguistic priors, and superficial patterns. model can answer visual question without looking at the image. It can ground language without grounding perception, yet still achieve high score. Although concerns about language bias were raised years ago in the VQA era (see Sec. 5.1), the rise of LLMs as both knowledge engines and sources of shortcuts makes it necessary to reexamine evaluation methodology and benchmark design with greater attention to nuance and complexity [30]. This observation motivates the central argument of this paper: multimodal benchmark designers should proactively stress-test their creations for exploitable non-visual shortcuts. While it has become common sanity check, the blind test [30]where multimodal benchmark is evaluated with vision disabledonly reveals when vision is unnecessary. It offers no insight into why specific samples are exploitable or how to fix the non-visual shortcuts. We demonstrate that the most rigorous and useful stress test involves directly training on the test setnot to overfit, but to adversarially probe the test set for such intrinsic vulnerabilities. This approach elevates robust benchmark design from simple static dataset curation to an iterative and adversarial refinement process. This train on the test set framing is critical. While training diagnostic models on held-out in-distribution data can reveal biases that generalize across domain, such an approach may miss idiosyncratic vulnerabilities unique to the specific test set artifact, e.g., those arising from the sampling process, templated questions, 2 or human filtering decisions. Our proposed methodology directly targets these test-set-specific shortcuts, helping to ensure that reported benchmark scores reflect genuine multimodal capabilities rather than the exploitation of unique statistical artifacts present in the evaluation instrument. To demonstrate an effective realization of these principles, we introduce systematic approaches for both diagnosing and mitigating non-visual shortcuts. We first diagnose benchmark susceptibility using Test-set Stress-Test (T T) methodology that applies 洧녲-fold cross-validation to train diagnostic models exclusively on non-visual test-set features, deriving both an overall exploitability measure and sample-level bias scores, 洧(洧논). We realize this diagnostic through two complementary approaches: powerful LLM-based variant (TsT-LLM) that LoRA-tunes language model on question-only inputs to capture complex shortcuts, and lightweight Random Forest variant (TsT-RF) trained on hand-crafted features that offers efficiency and interpretability for rapid auditing. We then debias benchmarks by systematically filtering samples identified as highly biased according to 洧(洧논) using an Iterative Bias Pruning (I P) procedure. Applying our diagnostic framework to prominent vision-centric multimodal benchmarks such as VSIBench [31] and CV-Bench [30], we uncover substantial non-visual biases, highlighting the prevalence of this issue (Sec. 3). As primary case study, we apply our framework to VSI-Bench, resulting in VSI-BenchDebiased (Sec. 4.2). Our experiments demonstrate that effectively quantifies these shortcuts, and that debiasing markedly reduces non-visual solvability, leading to significantly wider performance gap between vision-enabled and blind MLLM configurations on VSI-Bench-Debiased compared to the original. In summary, our contributions are fourfold: (1) principled stance on what constitutes an exploitable non-visual shortcut (Sec. 2.1); (2) generalizable diagnostic framework realized through both highefficacy LLM-based variant and an interpretable Random Forest variant (Sec. 3); (3) Empirical validation across four major multimodal benchmarksVSI-Bench, CV-Bench, MMMU, and VideoMMErevealing pervasive non-visual shortcuts (Sec. 3.4); and (4) The mitigation methodology and its application to create VSI-Bench-Debiased, demonstrably less compromised subset (Sec. 4). In the remainder of this paper, we detail the challenge of non-visual shortcuts (Sec. 2), present our diagnostic and mitigation methodologies (Secs. 3 and 4), discuss related work (Sec. 5), and conclude with call for more rigorous benchmark design practices (Sec. 6). 2. The Challenge: Non-Visual Shortcuts Undermine Multimodal Evaluation In multimodal evaluation, non-visual shortcuts occur when questions can be solved without utilizing the visual input. This can happen when MLLMs exploit world knowledge acquired during linguistic pretraining or leverage statistical correlations within the question-answer pairs themselves. The prevalence of such shortcuts leads to inflated performance metrics, misrepresents true visual understanding capabilities, and can misguide research by rewarding pattern matching over genuine multimodal reasoning. In this section, we first establish principled definition of what constitutes an exploitable shortcut (Sec. 2.1). We then examine two categories through this lens: knowledge-based shortcuts arising from LLM pretraining (Sec. 2.2), and statistical shortcuts embedded in benchmark structure (Sec. 2.3). 2.1. What Constitutes Non-Visual Shortcut? In the context of multimodal benchmarks, not all patterns that correlate with correct answers are equally problematic for evaluation integrity. critical question arises: when does learnable pattern become an exploitable non-visual shortcut, undermining measurement of visual understanding? We argue that shortcuts should be defined by their effect on the evaluation task, not their origin. patternwhether reflecting natural world knowledge, real-world statistical regularities, or procedural generation artifactsbecomes an exploitable non-visual shortcut if and only if it renders the visual input redundant for task designed to measure visual understanding. While some patterns reflect natural distributions (analogous to Zipfs Law in language [37]), their presence in multimodal benchmark becomes problematic when models can leverage these patterns to bypass the visual altogether. For vision-centric 3 Figure 2 Knowledge-based shortcuts in multimodal benchmarks. Blind performance across LLaVA-OneVision model scales. MMMU shows substantial gains from scaling the LLM backbone (x-axis) but minimal improvement from enabling vision (y-axis), indicating reliance on linguistic knowledge. VSI-Bench demonstrates the opposite patternlarge vision gains with negligible blind scalingconfirming robustness to knowledge-based shortcuts. VideoMME shows roughly equal gains from both sources, while CV-Bench benefits more from vision but still exhibits significant gains from LLM scaling. vs. vision-enabled benchmarks, the litmus test is clear: if model can correctly answer question using parametric knowledge or statistical priors without consulting the visual input, the benchmark has failed to isolate visual understanding for that sample, regardless of whether the pattern itself is natural or artificial. This definition has important implications for how we interpret world knowledge in visual benchmarks. Consider visual size estimation task that asks How tall is the refrigerator in this image? Standard refrigerators have relatively standardized heights (around 150180 cm), which constitutes natural world knowledge that models might reasonably possess. However, this knowledge can function as non-visual shortcut: model can achieve high accuracy on this task by recalling the typical refrigerator height without performing any visual measurement of the specific refrigerator shown. While the pattern reflects genuine world knowledge, its effect is to allow models to bypass the visual reasoning the benchmark intends to measure. Statistical correlations present more nuanced case. Consider benchmark where questions like Which is closer to the desk: the __ or the lamp? have lamp as the correct answer disproportionately often. If this correlation reflects real-world spatial organization (lamps are indeed often near desks), it represents potentially useful knowledge, yet still functions as shortcut in vision-centric evaluation. More problematically, if the correlation arises from idiosyncrasies in how the specific test set was sampled or generated (e.g., procedural biases in scene selection), models that use this pattern are merely exploiting spurious correlations within the benchmark artifact rather than acquiring the generalizable visual capabilities intended to be tested. Regardless of origin, be it world knowledge or spurious statistical artifacts, the framework above clarifies our diagnostic objective: identifying samples where non-visual patterns render visual input unnecessary to guide targeted mitigation and debiasing. 2.2. Non-visual Shortcuts from Knowledge The first category of exploitable shortcuts arises from the extensive world knowledge embedded in LLMs during pretraining [30]. As shown in Fig. 2, benchmarks like MMMU [34] and VideoMME [12] exhibit clear evidence of this vulnerability: models benefit more from scaling up the LLM backbone than from enabling visual inputs, suggesting they rely heavily on linguistic knowledge rather than visual understanding. In contrast, VSI-Bench [31] shows negligible gains from LLM scaling in blind settings but substantial improvements when vision is enabled, demonstrating greater robustness to knowledge-based shortcuts. Because knowledge-based shortcuts have been extensively documented in prior work, this paper focuses on complementary and less-explored challenge: statistical shortcuts embedded in benchmark structure, which can persist even in benchmarks designed to be robust against world-knowledge exploitation. (a) Counting (b) Spatial relation (c) Appearance order (d) Size estimation Figure 3 Statistical biases create non-visual shortcuts across diverse multimodal benchmarks. (a) Counting tasks exhibit severe long-tailed answer distribution skews; (b) Spatial relation tasks show imbalanced answer frequencies, where certain object categories appear as correct answers disproportionately often; (c) Appearance order tasks have strong category-position correlations; and (d) Size estimation tasks follow predictable log-normal distributions. Such patterns enable achieving high accuracy without the visual input. 2.3. Non-visual Shortcuts from Statistical Correlations As illustrated in Fig. 3, statistical biases manifest across diverse task types and benchmarks. Counting tasks often exhibit severe long-tailed answer distributions; in VSI-Bench, over 50% of such questions have ground truth answer 3 or fewer, enabling simple diagnostic model to achieve high score by consistently predicting 2 (Fig. 3a). Spatial relation tasks can show imbalanced answer frequencies, where certain object categories disproportionately appear as correct answers. In CV-Bench depth, categories like keyboard and clothes appear as the correct answer 100% of the time they are queried (Fig. 3b). Appearance order tasks can exhibit strong category-position correlations. In VSI-Bench, clock appears in the fourth slot in 100% of GT answers it is involved in (n=50) and ceiling light appears in the first slot in >80% (Fig. 3c). Many size estimation tasks naturally follow predictable log-normal distributions; the VSI-Bench room size task is heavily concentrated around typical room dimensions (log 洧랞 17洧녴2, log 洧랥 2), enabling accurate predictions without seeing the room (Fig. 3d). Critically, these statistical biases persist even in benchmarks explicitly designed to avoid knowledge-based shortcuts, such as VSI-Bench [31], highlighting that statistical and knowledge-based shortcuts are orthogonal vulnerabilities requiring independent diagnosis. The Harm: MLLMs Readily Exploit Statistical Shortcuts. The existence of these non-visual statistical regularities becomes particularly detrimental since modern MLLMs are highly adept at identifying and 5 Table 1 MLLMs readily exploit statistical shortcuts. LLaVA-Video-7B performance on VSI-Bench before and after fine-tuning on VSI-Train-10k. Fine-tuning on held-out in-distribution data with similar statistical biases boosts blind accuracy (+18.8 points) nearly as much as vision-enabled accuracy (+20.4 points), demonstrating that MLLMs can learn and exploit non-visual shortcuts from limited training data. Configuration Vision Blind LLaVA-Video 7B (Base) + VSI-Train-10k FT 풊 due to FT Chance (frequency) 36.7 57.1 +20.4 25.9 44.7 +18.8 34.0 풊VB 10.8 12.4 +1.6 exploiting such patterns, even from relatively small amounts of data. To demonstrate how readily MLLMs can learn to exploit these patterns, we conduct an adversarial stress-test on VSI-Bench [31]. We first curate small, in-distribution training set, VSI-Train-10k, comprising 10,000 samples generated using the same procedural logic as the VSI-Bench test set and sourced from the corresponding training splits of the datasets used in VSI-Bench (details in Appendix A). We then fine-tune representative MLLM model (LLaVA-Video7B [36]) on this VSI-Train-10k set. Tab. 1 presents the performance of LLaVA-Video-7B [36] on the original VSI-Bench test set, both before and after fine-tuning on VSI-Train-10k. We report accuracy for the standard vision-enabled model and blind evaluation configuration where the model only receives non-visual (textual) inputs. Before finetuning, the models blind performance is above chance, indicating some inherent exploitability. However, after fine-tuning on VSI-Train-10k, we observe dramatic increase in blind accuracy from 25.9% to 44.7% (+18.8 points). Critically, the vision-enabled models performance improves by nearly identical margin (+20.4 points), resulting in only minimal widening of the vision-blind gap (+1.6 points). This demonstrates that the MLLM learns statistical shortcuts that benefit both configurations equally, confirming that these patterns fundamentally bypass the need for visual reasoning. These results reveal critical vulnerability: overall accuracy scores can be misleading when nonvisual shortcuts significantly contribute to performance. The ease with which MLLMs learn these patternsachieving +18.8 point blind accuracy gain from just 10K examplesdemonstrates that statistical shortcuts are both pervasive and readily exploitable. This motivates the need for systematic diagnostic methodology that can quantify such vulnerabilities at both the benchmark and sample levels, enabling targeted mitigation. In the following section, we introduce our Test-set Stress-Test (TsT) framework to address this challenge. 3. Diagnosing Non-visual Shortcuts via Test-Set Stress-Testing Benchmarks are pivotal in driving progress in multimodal research. Once test set becomes an established yardstick, the community endeavors to optimize models, methodologies, and training datasets to enhance performance on it [25]. However, as demonstrated in Sec. 2, these evaluations can be deceptive, inadvertently harboring non-visual shortcuts that allow models to achieve high scores without engaging in genuine multimodal reasoning, thereby creating an illusion of progress. critical question then arises: How can we reliably detect and quantify these non-visual shortcuts within benchmarks test set itself? To clarify our approach, we note that benchmarks can fail in two fundamentally different ways. training failure occurs when biased training data prevents models from learning robust features, typically assessed through out-of-distribution evaluation (e.g., VQA-CP [3]). An evaluation failure occurs when the test set itself contains exploitable artifacts that allow high scores for wrong reasons, regardless of training quality. Our TsT framework specifically targets the latter by auditing the test sets intrinsic vulnerabilities. Though it is logical to use available in-distribution training data to diagnose learnable biases, as we demonstrated with VSI-Train-10k (Tab. 1), such an approach reveals biases that generalize across domain but may miss idiosyncratic vulnerabilities specific to the test sets particular composition (see Fig. 4a). 6 (a) Bias space (b) Diagnostic pipeline Figure 4 Test-set Training (TsT) targets intrinsic test-set vulnerabilities. (a) TsT directly probes biases intrinsic to the specific test set (pink region), rather than approximating them via external training data. (b) The test set is split into 洧녲 folds; blind diagnostic model trains on 洧녲1 folds and evaluates on the held-out fold, repeated 洧녲 times to yield (i) overall non-visual solvability and (ii) per-sample bias scores 洧(洧논). To directly probe the intrinsic exploitability of given test set, we advocate for more rigorous approach: 洧녲-fold cross-validation performed directly on non-visual features of the test set. Our Test-set Stress-Test (T T) diagnostic, which embodies this principle, is often preferable to relying on separate training data, even when such data is readily available. The primary advantage is its ability to uncover exploitable biases, statistical artifacts, and unintended regularities that are specific to the particular test set under examination e.g., arising from its unique sampling process, procedural generation logic, or subtleties of human filtering. In this section, we first present the core framework (Sec. 3.1). We then detail its two complementary realizations: powerful LLM-based diagnostic (Sec. 3.2) and an efficient, interpretable RF-based diagnostic (Sec. 3.3). Finally, we present empirical validation across four benchmarks (Sec. 3.4), demonstrating widespread non-visual shortcut susceptibility and providing sample-level bias scores 洧(洧논) that form the foundation for targeted mitigation (Sec. 4). 3.1. The TsT Framework The foundational principle of TsT is to quantitatively estimate the extent to which benchmark questions can be answered using exclusively non-visual information present in the test set itself. We apply 洧녲-fold cross-validation [27, 14] directly on the test set: partition into 洧녲 disjoint folds (typically 洧녲 = 5), train diagnostic model on non-visual features from 洧녲1 folds, and evaluate on the held-out fold. This process repeats 洧녲 times, ensuring predictions for every sample come from model not exposed to that sample during training. The TsT framework, illustrated in Fig. 4, provides direct measure of benchmarks non-visual solvability. The TsT methodology yields two critical outputs: 1. Overall TsT Accuracy: The aggregated accuracy across all 洧녲 folds provides global estimate of the benchmarks non-visual solvability. High TsT accuracy indicates that significant portion can be solved without the visual input by exploiting non-visual patternspotentially inflating reported MLLM performance. We consider this pragmatic lower bound on true exploitability, as more powerful diagnostics could uncover subtler cues. 2. Sample-Level Bias Score 洧(洧논): For each sample 洧논, we derive bias score 洧(洧논) representing the diagnostic models confidence in the ground truth answer when 洧논 was in the validation fold. High 洧(洧논) indicates the sample is likely answerable without visual information, marking it as candidate for shortcut learning. These scores enable targeted mitigation (Sec. 4). 7 3.2. LLM-Based TsT Diagnostic To diagnose shortcuts in modern MLLM benchmarks, we propose using language model as the diagnostic. This approach leverages the same model class being evaluated, ensuring the diagnostic can capture the full spectrum of shortcuts that MLLMs might exploitfrom simple statistical correlations to complex knowledgebased patterns. Critically, TsT-LLM requires no manual feature engineering: the model directly processes question and answer text to learn exploitable patterns, making it broadly applicable to any VQA benchmark regardless of question structure or generation methodology. Method. Our TsT-LLM diagnostic applies the TsT framework using powerful language model (e.g., Qwen2.5-7B [28]) as the diagnostic model. For each fold in the 洧녲-fold procedure, we parameter-efficiently fine-tune the LLM using Low-Rank Adaptation (LoRA) [15] on the question and answer-choice text (ignoring all visual inputs) from the 洧녲1 training folds. The tuned model then predicts answers for samples in the held-out validation fold. This process repeats 洧녲 times, yielding predictions for every sample from model that was not exposed to that sample during training. Advantages. TsT-LLM offers three key advantages: (1) Zero feature engineeringdirectly applicable to any text-based benchmark without manual feature design or domain expertise; (2) Comprehensive detection capable of capturing both simple statistical patterns and complex knowledge-based shortcuts; and (3) Aligned diagnosismatching the sophistication of the MLLMs being evaluated ensures realistic assessment of exploitability. The use of LoRA [15] enables computational efficiency, significantly reducing the time and storage required for 洧녲-fold fine-tuning. For the dataset sizes typical in benchmark diagnostics, LoRA performs equivalently to full fine-tuning [26], though our method generalizes to full fine-tuning if desired. Implementation details are provided in Appendix B.1. Limitations. TsT-LLM requires GPU resources and longer training time (approximately 20 minutes per benchmark on 4A100 GPUs for 洧녲 = 5 folds), making it less accessible for rapid iteration compared to CPU-based methods (Sec. 3.3). Additionally, while TsT-LLM provides strong detection of shortcuts, it offers limited interpretability regarding which specific non-visual cues drive exploitability, potentially hindering root-cause analysis for targeted benchmark improvements. 3.3. RF-Based TsT Diagnostic While TsT-LLM provides powerful detection capability, benchmark designers often need to understand why specific samples are exploitable to guide mitigation. We therefore develop Random Forest-based diagnostic (TsT-RF) that trades the zero-engineering convenience of TsT-LLM for interpretabilityrevealing which specific non-visual features drive exploitability. TsT-RF also offers extreme computational efficiency (completing in seconds without GPUs), enabling rapid iteration during benchmark refinement. Method. TsT-RF follows the same 洧녲-fold cross-validation procedure but uses Random Forest classifier [6] trained on hand-crafted non-visual features. For each sample 洧논, we extract features 洧녭洧녵洧녺(洧논) designed to capture any information present at test time (excluding visual input) that might correlate with the correct answer, including textual information (TF-IDF vectors, keywords, question length), answer space characteristics (e.g., multiple-choice option properties), and metadata (question type, object categories). See Appendix B.2 for full details on feature extraction. Advantages. TsT-RF offers two primary advantages: (1) Computational efficiencytraining on CPU in minutes without GPU requirements enables rapid iteration during benchmark refinement; and (2) Interpretabilityfeature importance analysis (e.g., Gini importance [21]) reveals which specific non-visual cues drive exploitability, providing actionable insights for targeted benchmark improvements. For instance, analyzing VSI-Benchs size estimation task revealed that single feature (average object size) had Gini importance of 0.968, directly informing mitigation strategy to remove low-variance object categories (full analysis in Appendix D). Notably, when carefully engineered features target known bias sources, TsT-RF can match or even exceed TsT-LLM accuracy (e.g., 75.5% vs. 73.4% on CV-Bench), though at the cost of manual effort. 8 Table 2 TsT-LLM reveals pervasive non-visual shortcuts across benchmarks. Performance of blind Qwen2.5-7B in zero-shot evaluation vs. after 洧녲-fold LoRA fine-tuning on test-set text. The improvement 풊TsT quantifies learnable non-visual shortcuts intrinsic to each benchmark. Benchmark Blind ZS Acc. TsT-LLM CV-Acc. CV-Bench VSI-Bench MMMU (val) VideoMME 40.1 25.0 34.9 35. 73.4 56.4 43.5 41.7 풊TsT +33.3 +31.4 +8.6 +6.4 Table 3 TsT-RF provides efficient, interpretable diagnostics. Random Forest performance via 洧녲-fold cross-validation on hand-crafted features. Chance and majority baselines provided for context. Benchmark Chance Acc. Majority Acc. TsT-RF CV-Acc. CV-Bench VSI-Bench 33.3 - 43.1 34.0 75.5 43.5 Limitations. While TsT-RF excels on benchmarks with templated or structured questions, it is challenging to apply to non-templated benchmarks (e.g., MMMU, VideoMME) where programmatic feature extraction from is infeasible. For such benchmarks, TsT-LLM provides more practical alternative. 3.4. Empirical Validation: TsT Reveals Widespread Shortcut Susceptibility Applying our TsT diagnostics to four prominent multimodal benchmarks reveals that non-visual shortcut susceptibility is both widespread and significant. TsT-LLM Results. Tab. 2 presents TsT-LLM diagnostic results across VSI-Bench, CV-Bench, MMMU, and VideoMME. We compare the performance of blind Qwen2.5-7B model in zero-shot (ZS) evaluation against its accuracy after 洧녲-fold LoRA fine-tuning directly on the test set. The improvement, 풊TsT, quantifies the degree of learnable non-visual shortcuts intrinsic to each test set. For template-based benchmarks CV-Bench and VSI-Bench, TsT-LLM achieves dramatic gains of +33.3 and +31.4 points respectively, indicating that substantial fractions of these benchmarks can be solved by learning patterns in non-visual data alone. Even for the more complex, non-templated benchmarks MMMU and VideoMME featuring humanand LLM-authored questions, we observe significant gains of +8.6 and +6.4 points, confirming that exploitable regularities exist across diverse question generation methodologies. TsT-RF Results. To demonstrate the complementarity of our two diagnostic approaches, Tab. 3 presents TsT-RF results on VSI-Bench and CV-Bench. On VSI-Bench, TsT-RF achieves 43.5% accuracy, lower than TsT-LLMs 56.4%, as expected for simpler model. Notably, on CV-Bench, TsT-RF achieves 75.5%slightly exceeding TsT-LLMs 73.4%. This result highlights an important insight: when benchmark designers invest effort in carefully engineering features that target specific, known bias patterns (as we did for CV-Benchs template-based structure), TsT-RF can match or exceed the performance of zero-feature-engineering LLM approaches. However, this manual feature engineering requires significant domain expertise and is infeasible for non-templated benchmarks, where TsT-LLMs generalizability becomes essential. The two approaches are thus truly complementary: TsT-LLM provides strong, effort-free baseline applicable to any benchmark, while TsT-RF can achieve superior detection when interpretability justifies the engineering investment. These findings demonstrate that non-visual shortcut vulnerability is pervasive across benchmarks of different modalities (video vs. image), generation methodologies (template-based vs. human/LLMauthored), and diagnostic approaches. The fact that both TsT-LLM and TsT-RF independently detect substantial exploitability underscores both the severity of the problem and the robustness of our diagnostic framework. The sample-level bias scores 洧(洧논) generated by these diagnostics provide concrete, data-driven foundation for targeted mitigation, which we explore in Sec. 4. 9 4. Mitigating Non-Visual Shortcuts Guided by TsT Insights Having quantified the pervasive nature of non-visual shortcuts across four benchmarks (Sec. 3.4), we now demonstrate how TsT-derived bias scores enable systematic benchmark refinement. While our diagnostic framework applies broadly, we focus mitigation efforts on VSI-Bench as an in-depth case study, where the combination of template-based structure and rich metadata enabled comprehensive TsT-RF analysis and derivation of sample-level bias scores 洧(洧논). We first detail the Iterative Bias Pruning (I P) procedure (Sec. 4.1), general iterative framework that leverages 洧(洧논) scores to guide data-driven filtering. We then present VSI-Bench-Debiased (Sec. 4.2), demonstrating tangible improvements in benchmark quality and ability to compel visual reasoning. 4.1. The Iterative Bias Pruning (I P) Procedure The primary contribution of our work is the TsT diagnostic framework and the sample-level bias scores 洧(洧논) it produces. serves as straightforward proof-of-concept application of these scores, demonstrating their utility for targeted benchmark refinement through systematic, data-driven pruning. While both TsT-LLM and TsT-RF can provide 洧(洧논) scores, we apply the procedure using TsT-RF (Sec. 3.3) for our VSI-Bench case study, as its interpretability facilitated targeted mitigation strategies for different question types. The procedure generalizes to any source of bias scores, any mitigation approach (pruning, rewriting, etc.), and any benchmark structure. We detail the unified iterative procedure in Algorithm 1, which aims to yield debiased benchmark version that more effectively compels genuine visual reasoning from evaluated models. Note: the biasdiagnosis function ComputeSampleBiasScores() represents any TsT diagnostic (TsT-LLM or TsT-RF) that produces 洧(洧논) scores via 洧녲-fold cross-validation. Algorithm 1: Iterative Bias Pruning (I P) Input: Dataset D; bias-diagnosis function ComputeSampleBiasScores(); removal budget 洧냣; batch size 洧녪; early-stopping threshold 洧랦 Output: Debiased dataset 洧녠 0 ; while 洧녠 < 洧냣 do { 洧멇롐둏洧논洧녰 ComputeSampleBiasScores(D) ; if max洧녰 洧멇롐 洧랦 then // samples removed so far // re-diagnose break ; 洧녲 min(cid:0)洧녪, 洧냣 洧녠(cid:1); SelectBatch(cid:0){ 洧멇롐둏洧논洧녰 D, 洧녲, D(cid:1) ; I; 洧녠 洧녠 + I; return // early stop: all biases below threshold // select batch for removal The core idea behind is to iteratively remove small batches of the most biased samples (as indicated by their 洧(洧논) non-visual bias scores) and then re-diagnose the remaining set by re-computing all 洧(洧논) scores. This iterative re-computation is crucial because the removal of some highly biased samples can alter the statistical landscape of the remaining data. Consequently, the relative exploitability of other samples, or even the predictive power of different non-visual features, might change. This adaptive approach helps ensure that the debiasing process doesnt inadvertently shift the bias under the rugfor example, by addressing one dominant shortcut only to leave secondary ones untouched or even amplified. Implementation Parameters. The procedure is primarily governed by two global parameters: the total removal budget 洧냣 and the batch size 洧녪. The budget 洧냣 controls the trade-off between bias reduction and dataset size/coverage, while the batch size 洧녪 influences the granularity of the iterative process. Smaller 洧녪 values enable more frequent re-diagnosis and adaptive debiasing (at higher computational cost), while larger 洧녪 approaches single-pass filtering. Early-stopping criteria, based on the maximum residual bias score 洧랦 or near-chance TsT accuracy, provide data-driven termination points to prevent unnecessary filtering once desired debiasing is achieved. Alternative Mitigation Strategies. While we focus on pruning as our primary mitigation strategy due to its simplicity, reproducibility, and unambiguous impact, the framework could accommodate alternative approaches. For instance, reparative methods such as question rewriting or answer rebalancing could be employed during the iterative loop in place of pruning. However, such approaches risk introducing new, unquantified biases and require careful validation to ensure they genuinely improve benchmark quality rather than merely transform one bias into another. We focus on pruning in this work and leave exploration of reparative strategies to future work. 4.2. Case Study: Creating VSI-Bench-Debiased with We apply the full algorithm to the original VSI-Bench test set [31], using the sample-level bias scores 洧(洧논) derived from its analysis (presented in Sec. 3.4), to create VSI-Bench-Debiased. This serves to demonstrate how identified biases can be systematically mitigated to produce more reliable benchmark. The application of to VSI-Bench yields two primary categories of improvements. First, we observe notable shifts in the ground truth answer distributions for several representative question types, which makes the questions in VSI-Bench-Debiased less predictable from non-visual statistical priors alone. Second, and more critically for the fair evaluation of genuine multimodal reasoning, VSI-Bench-Debiased elicits markedly different behavior from MLLMs compared to the original benchmark. Tab. 4 presents the performance of the LLaVA-Video-7B MLLM (both before and after fine-tuning on VSI-Train-10k, as evaluated in Tab. 1) on both the original VSI-Bench and the newly created VSI-Bench-Debiased. The results clearly show substantial reduction in the performance of the blind (non-visual input only) model configuration on VSI-BenchDebiased compared to its performance on the original VSI-Bench. For instance, the blind model fine-tuned on VSI-Train-10k achieves 44.7% on the original VSI-Bench but drops to 32.0% on VSI-Bench-Debiased. Consequently, the Vision-Blind Gap (풊)the performance difference between the vision-enabled and blind configurationsis significantly wider on VSI-Bench-Debiased, particularly after fine-tuning (e.g., increasing from 12.4% to 16.6%). This outcome strongly indicates that VSI-Bench-Debiased is more reliant on visual input and is less susceptible to the non-visual shortcuts that the MLLM had readily learned. The detailed analysis in the caption of Tab. 4 further explores how in-distribution training impacts vision-enabled versus blind scores differently across the original and robust benchmarks. This case study on VSI-Bench demonstrates that our proposed framework, combining diagnosis with P-based mitigation, provides an effective pathway to creating more robust benchmark versions that better isolate and assess genuine multimodal understanding. While we focus the full application on VSI-Bench in this paper due to the intensive nature of tailoring and evaluating specific debiasing strategies for each question type, the diagnostic insights from Sec. 3.4 suggest that applying similar mitigation approaches, guided by scores, would be beneficial for other benchmarks exhibiting non-visual shortcuts. Table 4 VSI-Bench-Debiased better isolates visual reasoning from statistical shortcuts. The visionblind gap (풊VB) widens significantly on VSI-Bench-Debiased, especially after fine-tuning (+16.6 vs. +12.4), indicating reduced non-visual solvability. Critically, fine-tuning improves vision and blind scores nearly equally on the original (+20.4 vs. +18.8), but vision improves much more than blind on the robust version (+17.4 vs. +11.7), confirming that VSI-Bench-Debiased better isolates visual reasoning improvements. Model Configuration LLaVA-Video 7B (Base) + VSI-Train-10k FT Increase in 풊 due to FT Chance (frequency) Vis. Blind VSI-Bench (Original) VSI-Bench-Debiased 풊VB 11.0 16.6 5.6 풊VB 10.8 12.4 1.6 36.7 57.1 20.4 31.3 48.7 17.4 Blind Vis. 20.3 32.0 11.7 34.0 25.9 44.7 18.8 34.0 11 5. Related Work Our work sits at the intersection of benchmark auditing and debiasing methodologies for multimodal evaluation. In this section, we position TsT within the landscape of prior approaches, clarifying how it complements existing methods by targeting distinct failure mode: intrinsic test-set artifacts. 5.1. VQA Debiasing: From Model Training to Benchmark Auditing The challenge of non-visual shortcuts in multimodal evaluation has deep roots in the Visual Question Answering (VQA) literature [4]. Early work identified that models could achieve high accuracy by exploiting language priors learned from training data rather than performing genuine visual reasoning [13]. This motivated the creation of VQA-CP [3], which deliberately introduces distribution shift between training and test sets to expose models that rely on question-type priorsa canonical example of testing for training failures where biased training data prevents robust generalization. Subsequent research developed sophisticated training-time interventions to mitigate these biases. Methods include adversarial regularization to penalize language-only branches [3, 24], unimodal bias suppression techniques like RUBi [7] that down-weight examples where question-only models are confident, and feedback-based objectives [18] that encourage visual grounding. More recent approaches employ counterfactual data augmentation [22, 1], synthesizing modified samples to force models to attend to visual evidence, and generative bias modeling [9], which explicitly learns and suppresses bias distributions. While these methods effectively improve model robustness, they operate under the assumption that the evaluation benchmark itself is soundthat the test set, if stripped of training-induced biases, accurately measures the intended capability. Our work addresses complementary challenge: evaluation failures where the test set itself harbors intrinsic exploitable patterns. Unlike model-debiasing approaches that modify training procedures, TsT audits the benchmark artifact directly, identifying samples where non-visual patternswhether from natural distributions or procedural generationrender visual input redundant. This diagnostic focus on test-set quality rather than model training represents shift from improving models to improving evaluation instruments themselves. 5.2. Modern Benchmark Auditing and Diagnostic Tools Beyond training-time debiasing, recent work has developed diagnostic tools to assess benchmark quality and modality reliance. The simplest diagnostic is the blind test: evaluating models with one modality removed (typically vision) [30]. While widely used as sanity check, this provides only coarse, dataset-level signals without identifying which specific samples are problematic or guiding systematic improvement limitations that motivated our development of TsT. More sophisticated auditing methods have emerged to quantify specific dimensions of benchmark quality. Park et al. [23] propose the Modality Importance Score (MIS), which assesses which modalities (visual, auditory, textual) contain necessary information by comparing performance across different modality ablations in video QA. Agarwal et al. [2] introduce the Region Comprehension Index (RCI), which audits whether benchmarks require local versus global visual reasoning by comparing model performance on full images versus image patches. Chen et al. [8] develop causal framework to quantify and mitigate unimodal biases in MLLMs, creating the MORE dataset to stress-test models reliance on single-modality shortcuts. These methods provide valuable complementary perspectives on benchmark quality. TsT distinguishes itself by specifically quantifying learnable non-visual patterns intrinsic to the test set through 洧녲-fold cross-validation, producing not only global exploitability measures but also sample-level bias scores 洧(洧논) that enable targeted refinement. Tab. 5 situates TsT within this landscape, clarifying its unique focus on test-set artifacts versus training priors or modality contributions. 5.3. Benchmarks Designed for Robustness Complementing diagnostic and debiasing approaches, several benchmarks have been designed from the ground up to resist specific shortcuts. Winoground [29] and its video extension Vinoground [35] use carefully 12 Table 5 TsT complements existing approaches by auditing test-set intrinsic artifacts. In comparison to prior benchmark-auditing and model-debiasing methods which target training biases or modality contributions, TsT quantifies exploitable patterns within the test set itself. Method Goal Target Methodology Output Model Robustness VQA-CP [3] RUBi [7] Model Debiasing Counterfactual [22] Model Debiasing Model Debiasing GenB [9] Benchmark Audit Modality Use MIS [23] Benchmark Audit RCI [2] Train Priors Model Behavior Model Behavior Model Behavior Spatial Bias TsT (Ours) Benchmark Audit Test Set Artifacts OOD Acc. Train-Test Shift Debiased Model Bias Suppression Data Synthesis Debiased Model Generative Modeling Debiased Model Importance Score Modality Ablation RCI Score Patch-based Eval 洧(洧논), Accuracy 洧녲-fold CV on Text paired image-caption sets that differ only in word order, requiring fine-grained compositional understanding that cannot be solved through bag-of-words matching. The Hateful Memes Challenge [16] employs benign confounders where hateful content requires combining image and text, as each modality alone provides misleading signals. MMBench [20] aims for robustness through comprehensive skill coverage and careful curation. While these benchmarks demonstrate the value of adversarial design, creating new benchmarks from scratch is resource-intensive and impractical for the many existing evaluation sets already in widespread use. Our TsT framework provides complementary pathway: systematic auditing and refinement of existing benchmarks to improve their robustness post-hoc, enabling the community to strengthen current evaluation standards without rebuilding from scratch. 6. Conclusion Multimodal benchmarks are the foundation for measuring progress in multimodal AI, yet they remain vulnerable to critical flaw: models can achieve high scores by exploiting non-visual shortcuts rather than demonstrating genuine visual understanding. This phenomenon risks creating an illusion of progress that misdirects research efforts toward pattern matching rather than true multimodal reasoning. We argue that benchmark designers must proactively train on the test setnot to overfit, but to adversarially audit for intrinsic, exploitable patterns. To enable this, we introduce Test-set Stress-Test (T T), diagnostic framework that quantifies non-visual exploitability through 洧녲-fold cross-validation on test-set text, producing sample-level bias scores 洧(洧논) that guide targeted mitigation. Applying to four major benchmarks reveals pervasive shortcuts, with blind models achieving dramatic improvements of up to +33 points by learning test-set patterns alone. Our Iterative Bias Pruning (I P) procedure demonstrates that 洧(洧논)-guided refinement produces meaningfully more robust benchmarks: VSI-Bench-Debiased exhibits 34% wider vision-blind gap after fine-tuning compared to the original. Rigorous test-set stress-testing should become standard practice in the robust design of multimodal benchmarks. Only through adversarial evaluation of our evaluation instruments can we ensure benchmarks measure genuine multimodal understanding rather than statistical pattern matching."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Shengbang Tong and Anjali Gupta for reviewing this manuscript, Chris Hoang for helpful discussions, and Zaid Khan for advice on LoRA frameworks. E.B. is supported by the DoD NDSEG Fellowship Program. S.X. acknowledges support from the MSIT IITP grant (RS-2024-00457882) and the NSF award IIS-2443404."
        },
        {
            "title": "References",
            "content": "[1] Ehsan Abbasnejad, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van den Hengel. Counterfactual vision and language learning. In CVPR, 2020. [2] Amit Agarwal, Hitesh Laxmichand Patel, Srikant Panda, Hansa Meghwani, Jyotika Singh, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, and Dan Roth. Rci: score for evaluating global and local reasoning in multimodal benchmarks. In EMNLP, 2025. [3] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Dont just assume; look and answer: Overcoming priors for visual question answering. In CVPR, 2018. [4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In ICCV, 2015. [5] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In NeurIPS, 2021. [6] Leo Breiman. Random forests. Machine learning, 2001. [7] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. RUBi: Reducing unimodal biases for visual question answering. NeurIPS, 2019. [8] Meiqi Chen, Yixin Cao, Yan Zhang, and Chaochao Lu. Quantifying and mitigating unimodal biases in multimodal large language models: causal perspective, 2024. [9] Jae Won Cho, Dong-Jin Kim, Hyeonggon Ryu, and In So Kweon. Generative bias for robust visual question answering. In CVPR, 2023. [10] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie릁er. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. [12] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. [14] Trevor Hastie, Robert Tibshirani, Jerome Friedman, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction, volume 2. Springer, 2009. [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2022. [16] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In NeurIPS, 2020. [17] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998. [18] Zujie Liang, Haifeng Hu, and Jiaying Zhu. Lpf: language-prior feedback objective function for de-biased visual question answering. In SIGIR, 2021. [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll치r, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 14 [20] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. [21] Stefano Nembrini, Inke K칬nig, and Marvin Wright. The revival of the gini importance? Bioinformatics, 2018. [22] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: cause-effect look at language bias. In CVPR, 2021. [23] Jean Park, Kuk Jin Jang, Basam Alasaly, Sriharsha Mopidevi, Andrew Zolensky, Eric Eaton, Insup Lee, and Kevin Johnson. Assessing modality bias in video question answering benchmarks with multimodal large language models. In AAAI, 2025. [24] Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee. Overcoming language priors in visual question answering with adversarial regularization. In NeurIPS, 2018. [25] Rylan Schaeffer. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632, 2023. [26] John Schulman and Thinking Machines Lab. LoRA Without Regret. Thinking Machines Lab: Connectionism, 2025. https://thinkingmachines.ai/blog/lora/. [27] Mervyn Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the royal statistical society, 1974. [28] Qwen Team. Qwen2.5: party of foundation models, September 2024. [29] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In CVPR, 2022. [30] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: Fully Open, Vision-Centric Exploration of Multimodal LLMs. NeurIPS, 2024. [31] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. CVPR, 2024. [32] Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Danhao Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, and Saining Xie. Cambrian-S: Towards Spatial Supersensing in Video. arXiv preprint, 2025. [33] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie릁er, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In ICCV, 2023. [34] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. [35] Jianrui Zhang, Mu Cai, and Yong Jae Lee. Vinoground: Scrutinizing lmms over dense temporal reasoning with short videos. arXiv preprint arXiv:2410.02763, 2024. [36] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [37] George Kingsley Zipf. Selected Studies of the Principle of Relative Frequency in Language. Harvard University Press, Cambridge, MA and London, England, 1932."
        },
        {
            "title": "Appendix",
            "content": "This appendix provides implementation details and supplementary analyses supporting the main paper: describes the VSI-Train-10k dataset generation used in 2 to demonstrate MLLM shortcut learning. provides technical specifications for both LLM-TsT and RF-TsT diagnostics, including hyperparameters, feature extraction procedures, and computational requirements. details the Iterative Bias Pruning (IBP) procedure and its application to create VSI-Bench-Debiased. presents comprehensive interpretability analysis of RF-TsT on VSI-Benchs size estimation task, demonstrating how feature importance analysis guides mitigation strategy. A. VSI-Train-10k Generation To create the in-distribution training set for demonstrating MLLM shortcut learning (Tab. 1), we follow the VSI-Bench [31] benchmark curation pipeline. First, we extract object numbers, bounding boxes, and room sizes from the training splits of ScanNet [10], ScanNet++ [33], and arkitScenes [5]. We then use the exact same question templates from VSI-Bench to generate question-answer pairs for seven rule-based question types (excluding route planning). Finally, we randomly sample 10K questions, setting maximum of 20 questions per question type per scene. See [32] for more details on the procedure followed. B. TsT Diagnostic Details B.1. TsT-LLM Implementation Details For our LLM-based TsT diagnostic (Sec. 3.2), we use Qwen2.5-7B-Instruct as the base model. For each fold in the 洧녲-fold cross-validation (we use 洧녲 = 5), we fine-tune the model using LoRA [15] with rank 洧 = 128 and 洧띺 = 256. Training uses learning rate of 5 105 with cosine scheduling, batch size of 32, and 3 epochs per fold. The model receives only the question text and answer choices (for multiple-choice questions) or the question alone (for open-ended questions), with no visual input. We use the models predicted probability for the ground truth answer as the bias score 洧(洧논) for each sample. Total training time for 洧녲 = 5 folds across benchmark like VSI-Bench (3K samples) is approximately 20 minutes on 4A100 GPUs. B.2. TsT-RF Feature Extraction For RF-based TsT diagnostic (Sec. 3.3), we extract non-visual features 洧녭洧녵洧녺(洧논) tailored to each benchmarks structure. For template-based benchmarks (VSI-Bench, CV-Bench), features include: (1) textual features (TF-IDF vectors on question text, question length, keyword presence); (2) answer space features (answer format, option count for MC questions); (3) metadata (question type, object categories mentioned); and (4) task-specific features (e.g., for size estimation tasks, the average size of mentioned objects computed from dataset statistics). For non-templated benchmarks, features are limited to generic textual representations and question-level metadata. We use scikit-learns RandomForestClassifier with 1000 estimators and max depth of 20. See Appendix B.3 for more details on the feature engineering process. B.3. TsT-RF Feature Engineering The following tables detail the non-visual features engineered for each benchmarks template-based question types. These features capture textual, statistical, and metadata patterns that might correlate with ground truth answers without requiring visual input. VSI-Bench Features. Based on the VSI-Bench question templates and dataset structure, we extract taskspecific features for each question type, shown in Tab. 6. VSI-Bench uses templated questions with predictable structure, enabling efficient feature extraction via regular expressions that parse object categories, distances, directions, and other task-specific elements directly from the question text. 16 Table 6 VSI-Bench TsT-RF features by task type. Features are extracted from the question text and answer choices (for Multiple-Choice) or the question alone (for Open-Ended), with no visual input. All statistical features are computed exclusively from training folds during cross-validation. Task Format Feature Type Feature Name Description Categorical object Object Counting OE Numerical obj_count obj_val_log_mean obj_val_log_std global_mean_log global_std_log Object Size Estimation OE Object Abs. Distance OE Room Size Estimation OE Object Rel. Distance MC Categorical object Numerical obj_count obj_val_log_mean obj_val_log_std Categorical object_pair Numerical Numerical Categorical Numerical pair_count pair_val_mean_log pair_val_std_log global_mean_log global_std_log target_object object_{i} opt_{i}_obj_freq opt_{i}_pair_freq max_opt_obj_freq max_opt_pair_freq difficulty positioning_object orienting_object querying_object Object Rel. Direction MC Categorical Object category Count of this object Mean of log GT values Std dev of log GT values Global mean (log space) Global std dev (log space) Object category Count of this object Mean of log GT sizes Std dev of log GT sizes Sorted pair of objects Count of this pair Mean of log distances Std dev of log distances Global mean (log space) Global std dev (log space) Category of target object Object category of option 洧녰, 洧녰 [4] Frequency of option 洧녰s object being correct Frequency of option 洧녰s object-target pair being correct Max option frequency Max object-target pair frequency Subtype (easy/med/hard) Object that is being stood by Object that is being faced toward Query target object Numerical obj_freq_score Combined frequency of all 3 objects being in question Route Planning MC Appearance Order MC Categorical Numerical beginning_object facing_object target_object opt_{i} num_steps num_choices opt_{i}_freq_score obj_freq_score Categorical opt_seq_{i} seq_{i}_pos_score Numerical seq_{i}_adj_pair_score seq_{i}_comb_pair_score seq_{i}_score 17 Starting position object Initial orientation object Destination object Route string option 洧녰 Number of steps in route options (all the same) Number of MC options Route option 洧녰 frequency Combined frequency of all 3 objects being in question Object sequence for option 洧녰: [洧녶洧녰 Sum of frequencies of object 洧녶洧녰 in GT sequence, 洧녱 [洧녵] Sum of frequencies of adjacent pairs 洧녶洧녰 sequence, 洧녱 [洧녵 1] Sum of frequencies of all combinatorial pairs 洧녶洧녰 GT sequence, 洧녱, 洧녲 [洧녵], Average of the above three scores for option 洧녰 1, 洧녶洧녰 洧녱 appearing in position 洧녱 2, . . . , 洧녶洧녰 洧녵 ] 洧녱+1 in GT 洧녱, 洧녶洧녰 洧녱, 洧녶洧녰 洧녱 < 洧녲 洧녲 in Table 7 CV-Bench TsT-RF features by task type. Features are extracted from the question text and answer choices with no visual input. All CV-Bench tasks are Multiple-Choice. Statistical features are computed exclusively from training folds during cross-validation. Task Feature Type Feature Name Description Categorical object Object category 2D Count Numerical n_options Number of MC choices obj_count Count of this object obj_val_log_mean Mean of log GT values obj_val_log_std Std dev of log GT values global_mean_log Global mean (log space) global_std_log Global std dev (log space) opt_{i}_dist_from_obj_mean Per-option distance scores opt_{i}_dist_from_global_mean Per-option distance scores Categorical object_{i} Category of object 洧녰, 洧녰 [2] 2D Spatial Relation Numerical n_options pair_freq_score question_length contains_{dir} spatial_keyword_count 3D Depth 3D Distance Categorical object_{i} Numerical n_options pair_freq_score Categorical object_{i} Numerical n_options pair_freq_score Number of MC choices Frequency of object pair in questions Question character length Contains spatial keyword dir, for dir {left, right, above, below, front, behind} Total spatial direction keywords Category of object 洧녰, 洧녰 [2] Number of MC choices Frequency of object pair in questions Category of object 洧녰, 洧녰 [2] Number of MC choices Frequency of object pair in questions CV-Bench Features. Based on the CV-Bench question templates and task structure, we extract features for each visual reasoning task, shown in Tab. 7. All statistical features are computed exclusively from the training folds during 洧녲-fold cross-validation, ensuring no leakage from validation samples. 18 C. Debiasing Details C.1. Iterative Bias Pruning (I P) Details The algorithm, presented in Algorithm 1, operationalizes this iterative debiasing philosophy. It begins with the full dataset and progressively refines it. In each iteration, current bias scores { 洧멇롐둏洧논洧녰 are computed for all remaining samples using the ComputeSampleBiasScores() function (which encapsulates our methodology). Based on these scores, and respecting an overall removal budget 洧냣 and periteration batch size 洧녪, subset of samples is selected for removal by the SelectBatch() function. This selection can employ various strategies to target different types of biases effectivelyfrom direct removal of the highest 洧(洧논) scoring samples, to weighted sampling for numerical tasks, or group-aware balancing for categorical imbalances, always using 洧(洧논) as the primary guiding metric. The process repeats until the budget 洧냣 is exhausted or an early-stopping criterion, such as the maximum remaining bias score max洧녰 洧멇롐 falling below threshold 洧랦, is met. C.2. VSI-Bench-Debiased Dataset Statistics The VSI-Bench-Debiased dataset was created by applying the Iterative Bias Pruning (IBP) procedure to the original VSI-Bench test set, guided by the sample-level bias scores 洧(洧논) from the TsT-RF diagnostic. The process began with the full set of 3,056 questions and concluded after removing 937 samples identified as highly susceptible to non-visual shortcuts, resulting in final dataset of 2,119 questionsa 30.7% reduction in total size. Tab. 8 details the number of samples removed from each of the four primary question categories. The object counting category experienced the highest pruning rate (36.8%), reflecting the severe answer distribution skew documented in Fig. 3a. Size estimation also required substantial filtering (31.8%) to remove questions about low-variance objects like dishwashers and beds, as discussed in Appendix D. Spatial relation and appearance order tasks, while still exhibiting exploitable patterns, proved less susceptible overall, requiring 27.9% and 26.2% removal respectively. Table 8 VSI-Bench-Debiased pruning statistics by question category. The IBP procedure removed 937 samples (30.7%) from the original 3,056-question test set, with removal rates varying by task type based on measured non-visual exploitability. Question Category Original Count Removed Percent Removed Final Count Object Counting Object Size Estimation Spatial Relation Appearance Order Total 764 764 764 3,056 281 243 213 200 937 36.8% 31.8% 27.9% 26.2% 30.7% 483 521 551 2,119 These statistics demonstrate that the IBP procedure, guided by TsT-derived bias scores, enables targeted refinement proportional to measured exploitabilityremoving higher fractions from more biased categories while preserving relatively more samples from less exploitable tasks. 19 D. TsT-RF Interpretability Analysis To demonstrate the interpretability value of TsT-RF and provide concrete evidence of how feature importance analysis translates into actionable insights for benchmark designers, we present detailed case study of VSI-Benchs object_size_estimation task. Task and Features. This task asks: What is the length of the longest dimension (length, width, or height) of the {object}, measured in centimeters? For each question, we extract five features: one categorical (object_category_name), and four numerical features derived from the training fold data (object_count, object_frequency_score, obj_val_log_mean, and obj_val_log_std for the object category). All statistical features are computed exclusively from the training folds during 洧녲-fold cross-validation, ensuring no leakage from validation samples. Diagnostic Performance. Applying TsT-RF via 5-fold cross-validation, the diagnostic model achieved an overall accuracy of 61.4% 2.4%, substantially above the majority baseline of 34.0%, indicating strong non-visual exploitability. Feature Importance Analysis. Tab. 9 presents the Gini feature importances. The obj_val_log_mean feature (object categorys average size) dominates with importance 0.968, while all other features combined contribute less than 0.04. This reveals that the RF diagnostic essentially ignores question structure, object frequency, and size variance, instead simply memorizing and predicting the typical size of each object category. High-洧(洧논) Examples and Root Cause. Tab. 10 shows the topranked samples by bias score 洧(洧논). All involve object categories with extremely low size variation in the dataset: dishwashers (903cm, coefficient of variation 0.037), beds (21617cm, CV 0.080), and washers (875cm, CV 0.058). For comparison, objects with high size variation like ceiling lights (7256cm, CV 0.778) or radiators (146110cm, CV 0.755) received much lower 洧(洧논) scores. Table 9 Gini Importances of RF features for VSI-Bench size estimation task with TsT-RF diagnostic. See Tab. 6 for full feature set. Feature Importance obj_val_log_mean object (category) obj_val_log_std obj_val_log_ratio obj_count obj_freq_score 0.968 0.009 0.008 0.007 0.004 0.004 Table 10 High-洧(洧논) examples and corresponding object size statistics from VSI-Bench size estimation. Object Mean Size (cm) Std Dev (cm) Coef. of Var. Rank by 洧(洧논) Dishwasher Bed Washer Kettle Mouse 90.4 216.1 87.1 23.8 11.6 Low-variance objects (easily exploitable) Ceiling Light Radiator 71.8 146.3 3.4 17.2 5.0 1.5 1. 55.8 110.4 0.037 0.080 0.058 0.062 0.091 0.778 0.755 1st 2nd 3rd 4th 5th Low Low Actionable Insight for Benchmark Design. This analysis provides concrete, quantitative prescription for improving the benchmark: the diagnostic model learned to exploit the fact that certain object categories have near-constant sizes, allowing it to bypass visual estimation entirely. Benchmark designers can address this by either (1) removing questions about low-variance objects, or (2) ensuring that sampled instances of each object category exhibit diverse sizes. This exemplifies how TsT-RFs interpretability translates statistical patterns into specific, implementable design improvementsa capability that complements TsT-LLMs superior detection of complex shortcuts."
        }
    ],
    "affiliations": []
}