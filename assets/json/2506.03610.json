{
    "paper_title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games",
    "authors": [
        "Dongmin Park",
        "Minkyu Kim",
        "Beongjun Choi",
        "Junhyuck Kim",
        "Keon Lee",
        "Jonghyun Lee",
        "Inkyu Park",
        "Byeong-Uk Lee",
        "Jaeyoung Hwang",
        "Jaewoo Ahn",
        "Ameya S. Mahabaleshwarkar",
        "Bilal Kartal",
        "Pritam Biswas",
        "Yoshi Suhara",
        "Kangwook Lee",
        "Jaewoong Cho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present \\textbf{\\benchname{}}, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 1 6 3 0 . 6 0 5 2 : r Orak: Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games Dongmin Park1, Minkyu Kim1, Beongjun Choi1, Junhyuck Kim1, Keon Lee1, Jonghyun Lee1, Inkyu Park1, Byeong-Uk Lee1, Jaeyoung Hwang1, Jaewoo Ahn1,2, Ameya S. Mahabaleshwarkar3, Bilal Kartal3, Pritam Biswas3, Yoshi Suhara3, Kangwook Lee1,4, Jaewoong Cho1 1KRAFTON, 2Seoul National University, 3NVIDIA, 4University of Wisconsin-Madison"
        },
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present Orak, foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and finetuning effects, establishing foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak."
        },
        {
            "title": "Introduction",
            "content": "Large Language Model (LLM) agents are revolutionizing various industries [1], and well-established benchmarks play key role in advancing their capabilities on complex tasks, e.g., coding [2, 3, 4], web search [5, 6, 7], and scientific research [8, 9]. Similarly, in the game industry, there is growing interest in leveraging LLM agents to enhance users game experiences, e.g., introducing more intelligent non-player characters (NPCs), monsters, and companions [10]. In response, several benchmarks have been proposed to assess the capability of LLMs in playing games [11]. While these benchmarks have effectively utilized games to evaluate LLMs general capabilities, they exhibit three major limitations: 1) they mostly rely on text-only games or 2D-grid simulators rather than complex real video games, 2) they offer insufficient assessment of agentic modules, such as self-reflection, memory, and tool use, which are essential to complex gameplay, and 3) they lack fine-tuning datasets necessary to adapt pre-trained LLMs into effective gameplay agents, which significantly hinder the adoption of LLM agents in real-world video games. To this end, we present Orak, foundational benchmark designed to evaluate LLM agents across diverse video games. As shown in Figure 1, Orak includes 12 video games played by millions to billions of users worldwide: Street Fighter III, Super Mario, Ace Attorney, Her Story, Pokémon Red, Darkest Dungeon, Minecraft, Stardew Valley, StarCraft II, Slay the Spire, Baba Is You, and 2048. Equal contribution. Core contribution. Preprint. Figure 1: Overview of Orak, benchmark designed to evaluate LLM agents across 12 real-world video games from various genres. Using Model Context Protocol (MCP) as plug-and-play interface, it ensures efficient and reliable assessment, supporting gameplay leaderboards, battle arenas, and in-depth studies on agentic modules and fine-tuning. These games span 6 major game genres, i.e., action, adventure, role-playing, simulation, strategy, and puzzle, enabling comprehensive assessment of key abilities required for general gameplay; action games enable testing fine-grained player control, adventure games challenge long-term memory and error handling, and strategy/puzzle games require complex logical reasoning and multi-step planning. In addition, with the use of real video games, Orak ensures evaluation on rich, dynamic environments with varying stages, levels, and story-driven quests, which are even challenging for humans. To enable consistent evaluation of rapidly evolving LLMs, we introduce plug-and-play interface for interaction with game environments and agentic modules using Model Context Protocol (MCP) [12]. Each game environment and agentic module package operates as an independent MCP server, providing game mechanics (e.g., retrieving game states, executing game steps) or agentic strategies (e.g., reflection, planning) as callable functions to LLMs. During gameplay evaluation, the LLM interacts with these servers by sequentially retrieving game states, performing action inference using agentic modules, and executing game steps. This interface enables streamlined evaluation across diverse games and supports controlled studies of various agentic modules. Furthermore, we propose fine-tuning dataset that aims to transform pre-trained LLMs into effective gaming agents. The dataset consists of game interaction trajectories generated by expert LLMs, e.g., GPT-4o, using various agentic strategies on all games in Orak. These trajectories encapsulate meta-knowledge on how and when to use the agentic strategies to play various genres of games, leading to more resource-efficient and effective gaming LLM agents. Our benchmark offers comprehensive evaluation dimensions, including general game scores with leaderboards, competitive LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects. Extensive experiments on Orak with 12 LLMs reveal that (1) proprietary LLMs generally achieve superior performance across games, with significant gaps compared to open-source LLMs, (2) their performance gap becomes narrow in battle games, (3) proprietary LLMs benefit from extended agentic workflows, while open-source LLMs show limited gains, (4) visual states often hinder gameplay performance, and (5) fine-tuning on gameplay trajectories enables effective transfer of gameplay meta-knowledge from larger LLMs to smaller ones, leading to generalization in intra-game and out-of-distribution (OOD) game scenarios. We believe that Orak not only establishes foundation for developing gaming LLM agents but also serves as critical benchmark for evaluating general LLMs on realistic, long-horizon decision-making tasks."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Playing Games by LLMs Many recent works have explored the use of LLMs for gameplay. Early efforts focused on text-based games such as Jericho [13], Zork [14], and TextCraft [15], where LLMs leveraged their abilities in exploration and reasoning to navigate textual environments. Subsequent work shifted toward 2D-grid games, including Chess [16], NetHack [17], and Crafter [18], where spatial reasoning and puzzle-solving skills became more important for successful gameplay. More recently, several studies have applied LLMs, combined with agentic workflows, to play more complex video games. Notably, 2 Benchmarks Game Domain Full Genre # Games State Type Agent Study Fine-tuning Set GAMA-bench [25] GameBench [26] GameArena [27] SmartPlay [28] Balrog [29] LVLM-Playground [30] Cradle [31] V-MAGE [32] DSGBench [33] Orak (Ours) Text Text Text Text/2D-grid Text/2D-grid 2D-grid Video Video Video Video 8 9 6 6 6 6 4 5 6 Text Text Text Text Text/Image Image Image Image Text Text/Image Table 1: Game Benchmark Comparison. Full Genre indicates whether six common game genres are fully covered (i.e., action, adventure, role-playing, simulation, strategy, and puzzle). State Type refers to the modality of game state provided to LLMs or VLMs. Unlike prior benchmarks, Orak is the only benchmark that features diverse set of real video games, fully covers all major genres, supports both text and image state inputs for LLMs/VLMs, provides ablation studies for agent modules, and releases fine-tuning dataset. LLMs have been used for item crafting in Minecraft [19, 20], city building in Civilization [21], Pokémon battles [22], and strategic planning in StarCraft [23]. However, these approaches rely on manually designing agentic workflows customized for each specific game given proprietary LLMs, e.g., GPT [24], limiting their usability toward developing general gaming agent. 2.2 Evaluation Benchmarks for LLMs with Games As gameplay requires complex cognitive abilities, e.g., context understanding, logical reasoning, and error handling, several recent benchmarks have sought to evaluate LLMs or Vision Language Models (VLMs) on games [11]. GAMA-Bench [25], GameBench [26], GameArena [27], and SmartPlay [28] focus primarily on text-based games, assessing LLMs ability to navigate and reason within textual environments. Barlog [29] and LVLM-Playground [30] are mainly based on 2D-grid games, such as TicTacToe and Chess, to evaluate the spatial and visual reasoning capabilities of LLMs/VLMs. Using video games, Cradle [31] evaluates VLMs through 3 simulation games and 1 adventure game, V-MAGE [32] assesses VLMs on 5 action video games, and DSGBench [33] validates LLMs on 6 strategic games. Despite their contributions, existing benchmarks have several limitations: they lack coverage of diverse game genres, omit in-depth studies on agentic modules essential for complex gameplay, and rely solely on visual inputs, despite VLMs current limitations in spatial perception and reasoning [30, 32]. Also, aligning pre-trained LLMs with agentic workflows for effective gameplay remains largely unexplored. Table 1 summarizes the key characteristics of game benchmarks. 2.3 Fine-tuning LLMs toward Agents Recent advances in LLM agents have enabled applications in diverse interactive settings, including web navigation, API usage, and game environments. To enhance agent capability, recent research efforts have proposed agentic strategies such as chain-of-thought reasoning [34, 35], selfreflection [36, 37], hierarchical task planning [38, 39, 40, 41], and self-generated skill libraries [20]. Complementing these advancements, efforts have also focused on fine-tuning strategies tailored to LLM agents. These strategies span two main directions: data-centric approaches, which involve supervised fine-tuning on curated expert demonstrations [42, 43, 44, 45, 46, 47]; and frameworkoriented approaches, which focus on learning from agentic interactions [48, 49, 50, 51, 52]. Notably, FireAct [42] emphasizes unified data formatting and CodeAct [47] highlights the importance of high-quality curation of training trajectories. However, fine-tuning methods for game-playing agents remain insufficiently explored. Unlike structured tasks in web, programming, or math domains, games involve large, dynamic, and partially observable state spaces. This requires agents to generalize across wide variety of situations and learn diverse behavior patterns, posing unique challenges."
        },
        {
            "title": "3 Orak",
            "content": "We propose Orak, benchmark designed to evaluate LLM agents across diverse video games. Figure 2 illustrates the evaluation pipeline of Orak, which is self-explanatory with code structures. By integrating the MCP interface with game environments and agentic modules, Orak enables systematic and plug-and-play evaluation of backbone LLMs with agentic strategies across various games. For 3 Figure 2: Evaluation pipeline of Orak. Game scores are computed via eval.py by simply configuring game, LLM backbone, and agentic strategy. Orak supports two types of submissions: (1) customizing llm.py with new backbone LLMs, and (2) customizing agent.py with new agentic strategies. The agentic strategies are callable by the LLM through the MCP interface in eval.py (in grey box). Figure 3: LLM capabilities required to play 12 games in Orak. The color theme (red, yellow, etc) represents game genres. See Appendix for genre categorization details. evaluation, the game score is obtained by simply configuring the game, LLM backbone, and agentic strategy in eval.py. At each game step, the game observation is retrieved, the specified agent strategy is executed by the LLM, and the resulting action is applied to the game. This loop continues until the game ends or reaches the maximum step limit, after which the game score is recorded. Note that, with MCP interface, users can readily customize their agentic strategy, i.e., calling single agentic module or multiple agentic modules sequentially. In the following section, we describe the 12 game environments in Orak, along with the submission guidelines for the benchmark leaderboard. 3.1 Game Environments LLM Capabilities Required. LLM agents require various capabilities to play video games. Figure 3 summarizes the level of LLM capabilities required to play each game in Orak, which is measured on scale of 1 to 3, following the principled criteria adopted from [28]. Rule Following (RF): LLM agents should be able to follow game-specific rules. The level is measured by the extent to which adherence to rules is required for gameplay (1: single rule, 2: fewer than 5 rules, 3: 5 or more rules). Logical Reasoning (LR): The number of LLMs reasoning hops required to determine an in-game action (1: 1 hop, 2: 1 to 3 hops, 3: 3 or more hops). Spatial Reasoning (SR): The level of spatial understanding required for gameplay (1: not necessary, 2: required in specific situations, 3: critical to core gameplay). Long-text Understanding (LTU): The extent of long-context comprehension required for gameplay (1: few lines, 2: few paragraphs, 3: longer than one page with 500+ words). Long-term Planning (LP): The extent to which strategic planning is required (1: not necessary, 2: planning for up to 3 sequential actions, 3: essential to plan more than 3 sequential actions). Error Handling (EH): The extent to which error correction is required during gameplay (1: not necessary, 2: requires one-step rollback, 3: requires multi-step rollback and re-planning). Odds Handling (OH): The extent to which understanding randomness is required for gameplay (1: not necessary, 2: randomness exists in game, 3: randomness is critical to core gameplay). 4 The level for each game is measured by 8 human participants, and the moderate value is reported. Since most video games are designed to require various cognitive abilities even for humans, they tend to require high levels of LLM capabilities in many dimensions. For example, action games, Street Fighter III and Super Mario in red, require spatial reasoning and rule following, more than long-context understanding and planning, while adventure games, Ace Attorney and Her Story in yellow, emphasize long-text understanding and logical reasoning due to the need to comprehend long storylines. More detailed analysis of required LLM capabilities for each game is in Appendix B. Game Description. For each game, we provide brief description of (1) the game state, (2) the action space given to LLMs, and (3) the evaluation task and metric. More detailed explanations of each environment are elaborated in Appendices C-N. (a) Street Fighter III [53] is 2D side-scrolling fighting action game with 20 unique characters, each equipped with distinctive skills. (1) Game state: The player character, opponent character, remaining time, each players health, super-bar, stun-bar gauges, and the distance between the two characters. (2) Action space: 15-20 discrete actions: move closer, low punch, high kick, etc. (3) Evaluation task: Beating the game bot; performance is measured by the number of stages cleared. (b) Super Mario [54] is side-scrolling game where the player controls Mario to avoid obstacles, defeat enemies, and reach the flag. (1) Game state: The positions (x,y) and sizes of obstacles and enemies extracted from the current game state. (2) Action space: Mario keeps moving to the right, and LLM decides the jump level, discretized in 6 bins. (3) Evaluation task: Reaching out to the final flagpole; performance is measured by the horizontal distance that Mario travels before dying. (c) Ace Attorney [55] is courtroom adventure game where players act as defense attorneys, gathering evidence and cross-examining witnesses to prove their clients innocence. (1) Game state: Dialogue history, collected evidence, court records profiles, etc. (2) Action space: Players courtroom actions: advancing dialogue, accessing court records, pressing witnesses, and presenting evidence. (3) Evaluation task: Performance is measured by response correctness and total steps taken. (d) Her Story [56] is an interactive adventure game where players explore police interview clips to uncover hidden truth. (1) Game state: History of queries and search results with metadata for the first 5 clips (visual description, date, viewing status, and transcript if played). (2) Action space: Searching for clips with keywords, or selecting video to play. (3) Evaluation task: Uncover the truth; Performance is measured by the number of distinct video clips viewed to complete the game. (e) Pokémon Red [57] is turn-based role-playing game where player explores, collects Pokémon, and battles other trainers to progress the storyline. (1) Game state: Players location, party Pokémon (species, level, HP, status), inventory, battle state, and screen text. (2) Action space: Choosing high-level tools or low-level joypad actions. (3) Evaluation task: Defeat Brock, the first gym leader; Progress measured by how many of 12 predefined storyline flags are triggered within step limit. (f) Darkest Dungeon [58] is turn-based role-playing game where heroes explore dungeons while managing stress and resources. (1) Game state: Party status (character stats, health, stress, and status effects), available skills, and enemy encounters. (2) Action space: Combat actions like attack, heal, and swap. (3) Evaluation task: Complete the first expedition; Performance is measured by the sum of the successful combats, the survived heroes, and their remaining stress capacities. (g) Minecraft [59] is an open-ended sandbox game where players explore world, gather resources, and survive by placing and breaking blocks. (1) Game state: The players position, inventory, health status, the nearby blocks and biome, etc. (2) Action space: Executable JavaScript code within the Mineflayer environment [60]. (3) Evaluation task: Crafting target item; performance is measured by whether the item is collected in the inventory. (h) Stardew Valley [61] is an open-ended life simulation game where players farm, fish, mine, and explore. (1) Game state: Players location, energy, inventory, crop status, soil status, date, and weather. (2) Action space: Leaving the house, entering the house, sleeping, buying seeds, tilling soil, watering, harvesting, and selling crops. (3) Evaluation task: Earning the most money by harvesting crops within the first 13 in-game days; performance is measured by total profit. (i) StarCraft II [62] is real-time strategy game where players gather resources, construct buildings, train units, and command armies to defeat opponents. (1) Game state: Resource levels, unit/building counts, production queues, research progress, and observed enemy info. (2) Action space: 72 discrete actions, including unit training, building, research, and strategic operations. (3) Evaluation task: Beating built-in AI bots; performance is measured by the win rate. 5 (j) Slay the Spire [63] is deck-building roguelike game where players ascend multi-floor tower, battling enemies and building decks. (1) Game state: Players class, deck, hand, health, relics, energy, enemies intents and statuses, and current floor. (2) Action space: Playing card during combat, ending the turn, and selecting card reward after combat. (3) Evaluation task: Defeating the final boss at the top floor; performance is measured by the number of floors reached. (k) Baba Is You [64] is puzzle game where player manipulates the rules by moving word tiles on board. (1) Game state: Coordinates of text and object tiles, and active rules. (2) Action space: single movement up, down, left, and right, or sequence of such moves. (3) Evaluation task: Solving the first stage; if the stage is not cleared, partial credit is awarded based on sub-goals (e.g., breaking the Wall Is Stop rule). (l) 2048 [65] is sliding tile puzzle game that aims to combine numbered tiles on 44 grid board to create tile of the value 2048. (1) Game state: The current configuration of the 44 grid, where each cell contains either number (power of 2) or is empty. (2) Action space: Four discrete actions; up, down, left, and right. (3) Evaluation task: Creating the 2048 tile; performance is measured by the normalized progress toward creating the 2048 tile. 3.2 Submission Guideline to Benchmark Leaderboard Models. Participants can submit new pre-trained LLMs, VLMs, or their fine-tuned versions. Unless otherwise specified, all models will be evaluated under the default agentic strategies for each game specified in Section 5.1. Agentic Strategies. Participants can also submit new agentic modules and strategies. All submitted strategies should follow consistent structure across games, e.g., by maintaining fixed sequence such as reasoning, planning, and using specific tool."
        },
        {
            "title": "4 Fine-tuning: Aligning Pre-trained LLMs into Game Agents",
            "content": "Data example from Reflection module Data example from Action module ref Analyze Marios past action using state difference and provide critiques for improving his action. You should only respond in the format as below: ### Self-reflection (Describe self-reflection here) ### Past Game State Mario at (100,100), Bricks at (120, 100), (120, 150) ### Current Game State Mario at (100,100), Bricks at (120, 100), (120, 150) act (Retrieve the self-reflection from memory) Analyze the current game state, and decide the best action. You should only respond in the format as below: ### Action Jump Level: (where is an integer from 0 to 6) ### Game State Mario at (100,100) Position of all objects: - Bricks at (120, 100), (120, 150) . . . ref ### Self-reflection Mario is blocked by bricks. Jump higher to get past. act ### Action Jump Level: 6. Table 2: Fine-tuning data examples when playing Supermario with reflection agent. We collect the fine-tuning dataset from expert LLMs, e.g., GPT-4o and o3-mini, playing 12 games in Orak using several agentic modules. This dataset with environment interaction trajectories encapsulates meta-knowledge on how to use the agentic strategies to solve diverse game genres. Data Format. The LLMs gameplay trajectory is denoted as = {τ1, . . . , τT }, where is the number of game steps, and τt denotes the sequence of LLM inferences executed via agentic strategies at game step t. Each LLM inference sequence τ is represented as τ = {(X ai , S, ai)}n i=1, where ai {reflection, planning, . . . , action} is the i-th agentic module in sequence, is the prompt for agentic module a, is the game state, and is the corresponding response of LLM. Table 2 shows detailed data examples of τ . Data Selection. For each game in Orak, we first collect gameplay trajectories , where varies depending on the game. To ensure high-quality data, we select the trajectories with high game scores until the number of LLM inferences exceeds 900. All selected trajectories follow the reflection-planning-action sequence, so that we have around 300 samples for each agent module. By performing data selection on all 12 games, our fine-tuning set consists of approximately 10k samples. Data Augmentation. To enhance the linguistic diversity, we augment each data sample τ by paraphrasing. We prompt GPT-4o to rephrase the game prompt while preserving all game-related information, generating 10 augmented samples for each sample τ . See Appendix for details. 6 Genre Games Action Adventure RPG Simulation Strategy Puzzle SF3 SuperMario AceAttorney HerStory Pokémon DarkestD Minecraft Stardew StarCraft2 SlaySpire BabaIsYou 0.00.0 Llama-3.2-1B 13.35.8 Llama-3.2-3B Qwen-2.5-3B 20.00.0 Qwen-2.5-7B 16.711.5 16.711.5 Minitron-4B 23.35.8 Minitron-8B GPT-4o-mini GPT-4o o3-mini 16.711.5 29.714.3 33.315.3 Gemini-2.5-pro 13.311.5 16.711.5 20.00.0 Claude-3.7 Deepseek-R1 18.78.6 31.810.1 23.414.1 27.29.6 24.46.0 31.312.8 28.88.8 34.114.2 34.914.6 38.014.4 31.78.2 28.713. 1.32.2 4.61.3 20.017.4 9.30.2 35.74.5 29.93.6 28.42.8 85.31.5 91.71.5 55.73.4 81.91.6 83.31.5 2.11.2 4.21.1 1.21.1 8.51.9 4.52.2 8.21.8 21.15.5 64.05.1 66.33.6 67.33.3 62.62.6 66.93.9 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 38.99.6 0.00.0 83.30.0 63.919.2 75.00. 0.00.0 47.539.2 44.822.2 88.82.0 0.00 63.830.4 81.35.8 93.41.5 89.02.1 93.71.6 89.92.5 91.71.1 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 46.07.0 71.07.0 75.00.0 75.00.0 75.00.0 41.70.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 18.932.7 95.75.7 64.718.8 69.611.9 63.024.6 77.713. 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 75.050.0 100.00.0 25.050.0 100.00.0 50.057.7 50.057.7 0.00.0 0.00.0 0.00.0 5.00.0 0.00.0 0.00.0 3.32.9 23.622.1 15.00.0 51.931.9 15.00.0 24.917.1 6.711.5 20.00.0 13.311.5 20.00.0 20.00.0 20.00.0 13.311.5 20.00.0 73.346.2 73.346.2 46.746.2 20.00. 0.00.1 0.30.2 0.10.1 0.60.4 0.10.0 0.70.7 1.11.0 5.61.5 25.37.3 5.12.5 5.32.7 11.53.4 Avg Rank 10.8 8.9 9.5 8.1 9.0 7.7 6.8 2.9 3.3 2.8 4.2 4. Table 3: Performance of LLMs on Orak with default agentic strategies. The best scores for each game are highlighted in bold, and the average ranking for each LLM across all games is reported. Our fine-tuning dataset is mainly for supervised fine-tuning (SFT). While dynamic data extraction from the environment could enable reinforcement learning fine-tuning, we leave this for future work."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experiment Setup Models. On all games in Orak, we validate the performance of 12 LLMs using states provided in text format. The models include 6 open-source LLMs: LLaMA-3.2-1B/3B [66], Qwen-2.5-3B/7B [67], and Minitron-4B/8B [68], and 6 proprietary LLMs: GPT-4o-mini, GPT-4o [69], o3-mini, Gemini2.5-pro [70], Claude-3.7-sonnet [71], and DeepSeek-R1 [72]. In addition, we study the effects of incorporating image inputseither alongside text or as the sole inputon multi-modal LLMs: GPT-4o, Gemini-2.5-pro, and Claude-3.7. Default Agentic Strategies. For each game in Orak, we select the most effective agentic strategy over GPT-4o and o3-mini as the default agent strategy. Specifically, For Street Fighter III, HerStory, Darkest Dungeon, and 2048, we use zero-shot action inference agent. For Super Mario, Pokémon Red, Stardew Valley, StarCraft II, Slay the Spire, and Baba Is You, we use reflection-planning agent that sequentially performs self-reflection, subtask planning, and action inference, integrated with memory at each game step. For AceAttorney, we use reflection agent. For Minecraft, we use skill-management agent that further includes knowledge retrieval and skill management in the reflection-planning-action agent, following [20]. Metrics and Implementation Details. For each game, we report the normalization score rather than the absolute score, i.e., the game score is normalized by the maximum game score. We report the average score of 3 to 20 trials for each game. More detailed metrics and LLM hyperparameter configurations are in Appendix P. 5.2 LLM Gameplay Performance Table 3 shows the gameplay performance of LLMs on Orak with default agentic strategies. Overall, proprietary LLMs outperform open-source LLMs across all games in most cases. Gemini-2.5-pro performs the best on average, ranking first in 8 out of 12 games, with the best average ranking of 2.7. o3-mini shows the second-best performance with the average ranking of 2.7, while showing the best in puzzle games, i.e., Baba Is You and 2048, which require strong mathematical and logical reasoning, and spatial understanding abilities. Most open-source LLMs show almost zero score on complex games such as Pokémon-red, Minecraft, Stardew Valley, StarCraft II, and Slay the Spire. Among open-source LLMs, Minitron-8B achieves the best average ranking of 7.7, and Qwen-2.5-7B shows the second-best average ranking of 8.1. See Appendices C-N for more detailed results of each game. 5.3 LLM Arena Among 12 games in Orak, Street Fighter III and StarCraft II support two-player competitive modes. For Street Fighter III, we conduct pairwise battles among 8 LLMs with zero-shot agent. Each pair competed in three rounds, and the agent winning 2 out of 3 rounds was declared the winner. To ensure fair comparison, both agents were assigned the same character, Ken, in all matches. Figure 4(a) shows the relative win rates and Elo ratings. Interestingly, as opposed to the result in Section 5.2, Minitron-8B consistently outperforms all other LLMs and achieves the best Elo rating. This may 7 (a) Street Fighter III Arena. (b) StarCraft II Arena. Figure 4: Match outcomes and Elo ratings for LLMs in two competitive environments. Models LLaMA-3B GPT-4o Agent Action Adventure RPG Simulation Strategy Puzzle Avg Strategies SF3 SuperMario AceAttorney HerStory Pokémon DarkestD Minecraft Stardew StarCraft2 SlaySpire BabaIsYou Rank Zeroshot 13.35.8 Reflection 30.017.3 20.00.0 Planning Ref-Plan 16.720.8 Zeroshot 29.714.3 Reflection 23.320.8 30.026.5 Planning 23.320.8 Ref-Plan 21.28.2 32.48.6 27.08.4 31.810.1 29.69.2 32.315.5 29.412.5 34.114.2 5.73.2 4.61.3 4.61.3 3.80. 49.91.3 85.31.5 52.70.5 52.80.5 4.21.1 4.41.3 5.11.0 5.40.4 64.05.1 61.30.4 59.25.7 61.84.9 0.00.0 0.00.0 0.00.0 0.00.0 33.30.0 36.14.8 33.30.0 38.99.6 47.539.2 47.339.0 56.323.6 57.031. 93.41.5 85.210.3 82.08.6 91.62.5 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 50.00.0 13.00.0 50.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00. 40.535.2 18.35.2 64.623.7 95.75.7 50.057.7 0.00.0 50.057.7 100.00.0 24.79.2 41.319.6 35.39.2 36.025.5 20.00.0 20.00.0 20.00.0 20.00.0 20.00.0 20.00.0 20.00.0 20.00.0 0.30.2 0.00.0 0.10.1 0.10. 5.61.5 3.52.9 6.05.5 7.05.7 6.4 5.6 6.2 6.1 3.3 3.0 3.1 2.2 Table 4: Ablation study for agentic modules. Ref-Plan refers to the Reflection-Planning agent. imply that when multiple agents are involved in the environment, adversarial actions can significantly change the game dynamics. For StarCraft II, we conduct pairwise battles among 7 LLMs, with each pair competing in single round. Both agents were assigned the same race, Protoss, in all matches. As shown in Figure 4(b), Claude-3.7-Sonnet performs the best with the highest Elo rating, while GPT-4o and Minitron-8B form the second group. 5.4 Ablation Study for Agentic Modules Table 4 shows the ablation results of LLaMA-3.2-3B and GPT-4o across 4 agent strategies. Interestingly, the impact of adding agentic modules to gameplay performance differs between the two LLMs. For GPT-4o, the inclusion of agentic modules consistently improves gameplay performance; reflection-planning agent achieves the best average ranking of 2.2, followed by reflection and planning agents with rankings of 3.1 and 3.3, and zero-shot agent with the lowest ranking of 3.4. However, LLaMA-3.2-3B does not follow this trend. The reflection agent shows the highest average ranking of 5.6, while reflection-planning follows with 6.1, although it adds the planning module. This indicates that, for relatively smaller LLMs like LLaMA-3.2-3B, adding agentic modules may increase the complexity of the prompt, hindering their decision-making accuracy. These results suggest that the optimal agentic strategy may depend on the inherent capability of the LLM. 5.5 Effect of Visual Input We studied the visual input effect on GPT-4o, Gemini-2.5-pro, and Claude-3.7, as suitable open-source multimodal models were unavailable during experiments. Games were divided into two categories: Group 1  (Table 5)  includes games where the provided textual game state can be derived from single visual screenshot; for these, we evaluated Text-only, Image-only, and Both (text and image) input modalities. Group 2  (Table 6)  comprises games where the textual state contains information beyond what is visible in the current frame (e.g., abstracted inventory lists or off-screen character/item details); for this group, we evaluated Text-only and Both inputs. Minecraft was excluded from this vision-based analysis due to challenges in programmatic screenshot capture within our setup. As shown in Table 5, relying solely on Image-only input led to substantial drop in performance. This was consistently reflected across all models, with average ranks deteriorating significantly compared to Text-only input. In contrast, as shown in Table 5& 6, utilizing Both text and visual inputs produced mixed effects on game scores and model ranks. For instance, in games like Street Fighter III (Group 1), since on-screen visual details are challenging to fully convey textually, adding visual context significantly benefited Claude, increasing its score by 16.6. Conversely, in narrative-heavy games such as Ace Attorney (Group 2), the same approach often proved detrimental; GPT-4os score, for example, dropped by 31.8, and its average rank in that group fell from 2.9 (Text-only) to 4.7. This highlights that the impact of combining modalities varied considerably, with some scenarios showing improved ranks or scores while others demonstrated decline. 8 Models Input SF3 SuperMario Stardew BabaIsYou 2048 Rank GPT-4o Gemini Claude 29.714.3 Text Image 23.715.9 24.314.5 Both Text 13.311.5 Image 16.711.5 20.010.0 Both Text 16.711.5 Image 23.311.5 33.35.8 Both 34.114.1 27.113.7 27.110.2 38.013.4 28.510.7 40.99. 28.713.2 25.66.4 22.66.3 95.75.7 0.0 0.0 49.226.0 69.611.9 8.910.5 70.67.1 63.024.6 0.00.0 58.51.2 20.00.0 6.711.5 20.00.0 73.346.2 20.00.0 86.723. 46.746.2 20.00.0 20.00.0 5.61.5 1.81.1 5.44.5 5.12.5 5.52.4 3.12.6 5.32.7 8.44.0 6.70.9 3.0 7.4 5.3 4.6 5.9 3. 4.9 5.7 4.6 Table 5: Comparison across modality (Group 1). Models Input AceAttorney HerStory Pokémon DarkestD StarCraft2 SlaySpire Rank GPT-4o Gemini Claude Text Both Text Both Text Both 85.31.5 53.51.7 55.73.4 52.60. 81.91.6 71.317.3 64.05.1 40.429.4 67.33.3 64.72.4 62.62.6 63.43.1 38.99.6 41.78.3 83.30.0 83.30. 63.919.2 72.24.8 93.41.5 92.23.0 93.71.6 92.21.8 89.92.5 90.15.7 100.00.0 50.057.7 100.00.0 100.00. 50.057.7 50.057.7 23.622.1 23.622.1 51.931.9 26.219.4 15.00.0 9.74.6 2.9 4.7 1.8 2. 4.5 4.3 Table 6: Comparison across modality (Group 2). Model Llama-3.2-1B Finetune Llama-3.2-3B GPT-4o SF3 SuperMario DarkestD StarCraft2 SlaySpire BabaIsYou Model Finetune SuperMario 2048 0.00.0 42.016.4 12.011.0 40.07.1 15.67.2 15.79.0 13.74.3 17.25. 0.00.0 93.42.6 87.29.5 92.00.2 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 8.03.5 0.00.0 10.71. 20.00.0 20.00.0 20.00.0 20.00.0 Llama-3.2-1B Llama-3.2-3B 10.00.0 18.93. 94.70.7 75.050.0 48.70.0 20.00.0 GPT-4o 18.78.6 26.712.3 31.810.1 34.47.0 0.10.1 2.81.8 0.10.2 3.12. 34.114.2 7.05.7 Table 7: Intra-game generalization accuracy. Table 8: OOD-game accuracy. 5.6 Effect of Fine-tuning Setup. We study the generalization capabilities of LLMs fine-tuned on expert trajectories described in Section 4. We consider two types of generalization: intra-game and OOD-game. Intra-game generalization evaluates whether an LLM can adapt to unseen scenarios within the same game, e.g., new stages, characters, or maps, that were not part of the training data. For this, we fine-tune the LLM on expert trajectories from all 12 games and evaluate it on 6 games that naturally provide unseen scenarios. Note that the scores are not directly comparable with those in Table 3, as the evaluation scenarios differ. In contrast, OOD-game generalization evaluates whether fine-tuning on specific set of game trajectories enables the model to perform better on held-out set of unseen games that were not encountered during training. Specifically, we hold out Super Mario and 2048, and fine-tune the model on trajectories from the remaining 10 games. We used the same evaluation scenario as Table 3. See Appendix for details on the unseen scenarios used in intra-game generalization experiments. Intra-game Generalization. Table 7 shows performance of fine-tuned Llama-3.2-1B/3B models on unseen scenarios, compared to the pretrained ones. Despite their relatively smaller scale, we find that these models demonstrate strong generalization to unseen scenarios; in 4 out of 6 games, both fine-tuned models outperform their pretrained counterparts. The performance gain largely comes from the model learning to generate valid actions more reliably after fine-tuning, especially in environments where the pretrained model frequently failed to act meaningfully. However, this approach appears insufficient to significantly enhance the spatial reasoning abilities of smaller models. This is evident in Baba Is You, where fine-tuned models struggle to construct the winning condition required to score above 20.0a task that remains challenging even for GPT-4o. OOD-game Generalization. Table 8 shows OOD-game generalization performance of Llama3.2-1B/3B models. Despite being trained exclusively on trajectories from different games, the performance of these models on Super Mario and 2048 improves significantly. This suggests that fine-tuning on trajectories shaped by reflection and planning enables models to learn transferable decision-making routines, as the underlying capabilities required for these behaviors are shared across games, even when the games themselves differ. Specifically, the improvement in 2048 likely benefits from its structural similarity to Baba Is You, training game that also uses 2D grid layout and discrete directional actions (up, down, left, right), which may have facilitated generalization."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce Orak, benchmark designed to train and evaluate LLM agents across diverse real-world video games. Our benchmark enables comprehensive assessments of LLM capabilities required to play most game genres. Through plug-and-play interface powered by MCP, it further allows consistent evaluation of rapidly evolving LLMs over various agentic modules. In addition, we release fine-tuning dataset, consisting of game interaction trajectories of topperforming LLMs, which can effectively transform pre-trained LLMs into gaming agents. With the comprehensive game set and user-friendly interface, Orak sets new foundation for game-based LLM evaluation, driving progress towards versatile and high-performing gaming agents."
        },
        {
            "title": "References",
            "content": "[1] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [2] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [3] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. [4] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. [5] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. [6] Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373, 2024. [7] Ido Levy, Ben Wiesel, Sami Marreed, Alon Oved, Avi Yaeli, and Segev Shlomov. Stwebagentbench: benchmark for evaluating safety and trustworthiness in web agents. arXiv preprint arXiv:2410.06703, 2024. [8] Peter Mühlbacher, Nikos Bosse, and Lawrence Phillips. Towards realistic long-term benchmark for open-web research agents. arXiv preprint arXiv:2409.14913, 2024. [9] Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, and Yisong Yue. Datascibench: An llm agent benchmark for data science. arXiv preprint arXiv:2502.13897, 2025. [10] NVIDIA. Introducing nvidia ace for games generative spark life into virtual charachttps://www.nvidia.com/en-us/geforce/news/ ters with nvidia-ace-for-games-generative-ai-npcs/, 2025. Accessed: 2025-05-13. ai. - [11] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu. survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024. [12] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278, 2025. [13] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 79037910, 2020. [14] Chen Feng Tsai, Xiaochen Zhou, Sierra Liu, Jing Li, Mo Yu, and Hongyuan Mei. Can large language models play text games well? current state-of-the-art and open questions. arXiv preprint arXiv:2304.02868, 2023. [15] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning with language models. arXiv preprint arXiv:2311.05772, 2023. [16] Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. Chessgpt: Bridging policy learning and language modeling. Advances in Neural Information Processing Systems, 36:72167262, 2023. 10 [17] Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktäschel. The nethack learning environment. Advances in Neural Information Processing Systems, 33:76717684, 2020. [18] Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780, 2021. [19] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:1834318362, 2022. [20] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [21] Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, et al. Civrealm: learning and reasoning odyssey in civilization for decision-making agents. arXiv preprint arXiv:2401.10568, 2024. [22] Sihao Hu, Tiansheng Huang, and Ling Liu. Pokéllmon: human-parity agent for pokémon battles with large language models. arXiv preprint arXiv:2402.01118, 2024. [23] Weiyu Ma, Qirui Mi, Yongcheng Zeng, Xue Yan, Runji Lin, Yuqiao Wu, Jun Wang, and Haifeng Zhang. Large language models play starcraft ii: Benchmarks and chain of summarization approach. Advances in Neural Information Processing Systems, 37:133386133442, 2024. [24] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [25] Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael Lyu. How far are we on the decisionmaking of llms? evaluating llms gaming ability in multi-agent environments. arXiv preprint arXiv:2403.11807, 2024. [26] Anthony Costarelli, Mat Allen, Roman Hauksson, Grace Sodunke, Suhas Hariharan, Carlson Cheng, Wenjie Li, Joshua Clymer, and Arjun Yadav. Gamebench: Evaluating strategic reasoning abilities of llm agents. arXiv preprint arXiv:2406.06613, 2024. [27] Lanxiang Hu, Qiyu Li, Anze Xie, Nan Jiang, Ion Stoica, Haojian Jin, and Hao Zhang. arXiv preprint Gamearena: Evaluating llm reasoning through live computer games. arXiv:2412.06394, 2024. [28] Yue Wu, Xuan Tang, Tom Mitchell, and Yuanzhi Li. Smartplay: benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557, 2023. [29] Davide Paglieri, Bartłomiej Cupiał, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kucinski, Lerrel Pinto, Rob Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543, 2024. [30] Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players? arXiv preprint arXiv:2503.02358, 2025. [31] Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. [32] Xiangxi Zheng, Linjie Li, Zhengyuan Yang, Ping Yu, Alex Jinpeng Wang, Rui Yan, Yuan Yao, and Lijuan Wang. V-mage: game evaluation framework for assessing visual-centric capabilities in multimodal large language models. arXiv preprint arXiv:2504.06148, 2025. [33] Wenjie Tang, Yuan Zhou, Erqiang Xu, Keyan Cheng, Minne Li, and Liquan Xiao. Dsgbench: diverse strategic game benchmark for evaluating llm-based agents in complex decision-making environments. arXiv preprint arXiv:2503.06047, 2025. [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [35] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [36] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [37] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. [38] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Inner monologue: Embodied Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. [39] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pages 91189147. PMLR, 2022. [40] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29983009, 2023. [41] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. Advances in Neural Information Processing Systems, 36:31967 31987, 2023. [42] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023. [43] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023. [44] Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881, 2024. [45] Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction trajectories. arXiv preprint arXiv:2410.07706, 2024. [46] Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Ö Arık. Learn-byinteract: data-centric framework for self-adaptive agents in realistic environments. arXiv preprint arXiv:2501.10893, 2025. [47] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024. [48] Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang, and Hang Li. Agile: novel reinforcement learning framework of llm agents. arXiv preprint arXiv:2405.14751, 2024. [49] Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, and Philipp Krähenbühl. Reinforcement learning for long-horizon interactive llm agents. arXiv preprint arXiv:2502.01600, 2025. 12 [50] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. [51] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. [52] Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao Wang, Akshay Malik, Graham Neubig, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-v0: Train real-world long-horizon agents via reinforcement learning, 2025. [53] Capcom. Street Fighter III: 3rd Strike. https://streetfighter.fandom.com/wiki/ Street_Fighter_III:_3rd_Strike, 1997. Accessed: 2025-05-12. [54] Christian Kauten. Super Mario Bros for OpenAI Gym. https://github.com/Kautenja/ gym-super-mario-bros, 2018. Accessed: 2025-05-12. [55] Capcom. Phoenix Wright: Ace Attorney. https://aceattorney.fandom.com/wiki/ Phoenix_Wright:_Ace_Attorney, 2001. Accessed: 2025-05-12. [56] Sam Barlow. Her Story. https://www.herstorygame.com, 2015. Accessed: 2025-05-12. [57] Game Freak. Pokémon Red Version. https://pokemon.fandom.com/wiki/Pok%C3% A9mon_Red_and_Blue_Versions, 1996. Accessed: 2025-05-12. [58] Red Hook Studios. Darkest Dungeon. https://www.darkestdungeon.com, 2016. Accessed: 2025-05-12. [59] Mojang Studios. Minecraft. https://www.minecraft.net, 2011. Accessed: 2025-05-12. [60] PrismarineJS contributors. PrismarineJS/mineflayer: Create Minecraft bots with powerful, stable, and high-level JavaScript API. https://github.com/PrismarineJS/mineflayer, 2013. Accessed: 2025-05-01. [61] ConcernedApe. Stardew Valley. https://www.stardewvalley.net, 2016. Accessed: 202505-12. [62] Blizzard Entertainment. StarCraft II. https://starcraft2.com, 2010. Accessed: 2025-0512. [63] MegaCrit. Slay the Spire. https://www.megacrit.com, 2017. Accessed: 2025-05-12. [64] Hempuli. Baba is you. https://hempuli.com/baba/, 2019. Accessed: 2025-05-12. [65] Gabriele Cirulli. 2048. https://play2048.co/, 2014. Accessed: 2025-05-12. [66] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [67] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [68] Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil Mahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, et al. Llm pruning and distillation in practice: The minitron approach. arXiv preprint arXiv:2408.11796, 2024. [69] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 13 [70] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [71] Anthropic. Claude 3.7 sonnet: Our most capable model yet. https://www.anthropic.com/ news/claude-3-7-sonnet, 2025. Accessed: 2025-05-08. [72] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [73] Wikipedia contributors. List of video game genres. https://en.wikipedia.org/wiki/ List_of_video_game_genres, 2025. Accessed: 2025-05-22. [74] DIAMBRA. DIAMBRA: Reinforcement Learning Platform for Competitive Video Games. https://www.diambra.ai/, 2025. Accessed: 2025-05-22. [75] Rahima Khanam and Muhammad Hussain. Yolov11: An overview of the key architectural enhancements. arXiv preprint arXiv:2410.17725, 2024. [76] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3-4):324345, 1952. [77] Christian Kauten. Super Mario Bros for OpenAI Gym. https://github.com/Kautenja/ gym-super-mario-bros, 2018. Accessed: 2025-05-21. [78] Andreas Pardeike. Harmony: library for patching, replacing and decorating .net and mono methods during runtime. https://github.com/pardeike/Harmony, 2025. Accessed: 202505-21. [79] BepInEx Contributors. Bepinex: Unity / xna game patcher and plugin framework. https: //github.com/BepInEx/BepInEx, 2025. Accessed: 2025-05-21. [80] Unity Doorstop Contributors. Unity doorstop: tool to execute managed .net assemblies inside unity. https://github.com/NeighTools/UnityDoorstop, 2025. Accessed: 2025-05-21. [81] Baekalfen. Pyboy: Game boy emulator written in python. https://github.com/Baekalfen/ PyBoy. Accessed: 2025-05-23. [82] Kgleken. Darkestdungeonbot. https://github.com/kgleken/DarkestDungeonBot, 2023. Accessed: 2025-05-21. [83] Robojumper. Darkest dungeon save editor. https://github.com/robojumper/ DarkestDungeonSaveEditor, 2023. Accessed: 2025-05-21. [84] Pathoschild. SMAPI - Stardew Modding API. https://smapi.io/, 2025. Accessed: 202505-21. [85] BurnySc2. Starcraft II Bot API Client Library for Python 3. https://github.com/ BurnySc2/python-sc2, 2017. Accessed: 2025-05-21. [86] Bug Kiooeht. BaseMod. https://steamcommunity.com/sharedfiles/filedetails/ ?id=1605833019, 2018. Accessed: 2025-05-23. [87] Bug Kiooeht. Mod the Spire. https://steamcommunity.com/sharedfiles/ filedetails/?id=1605060445, 2018. Accessed: 2025-05-23. [88] Forgotten Arbiter. Communication Mod. https://github.com/ForgottenArbiter/ CommunicationMod, 2019. Accessed: 2025-05-23. [89] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 14 [90] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [91] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as mastermind: survey of strategic reasoning with large language models. arXiv preprint arXiv:2404.01230, 2024. [92] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. [93] Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, and Irwin King. Recent advances in speech language models: survey. arXiv preprint arXiv:2410.03751, 2024. 15 Orak: Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games (Supplementary Material)"
        },
        {
            "title": "A Game Genre Categorization",
            "content": "Video games are generally categorized into six widely recognized genres, i.e., Action, Adventure, Role-Playing, Simulation, Strategy, and Puzzle, each characterized by distinct gameplay structures [73]. Action games emphasize responsiveness and precise physical control. Adventure games focus on narrative exploration, interactive dialogues, and clue-based progression grounded in logical inference. Role-Playing games focus on character progression, stat-driven combat mechanics, and quest-based narrative development. Simulation games present system-driven environments in which players manage complex and interdependent variables such as time, resources, and procedural systems. Strategy games are genre that emphasizes planning, resource management, decision-making, and tactical execution. Puzzle games revolve around rule-based problem solving, pattern recognition, and logical or spatial reasoning, typically within clearly defined system of constraints. In Table 9, we categorize each of the 12 video games in Orak into one of the six major genres based on the primary gameplay characteristics and the genre information provided by the respective game publishers. Some games belong to one or more genres due to hybrid gameplay mechanics. Game Action Adventure Role-Playing Simulation Strategy Puzzle Street Fighter III Super Mario Ace Attorney Her Story Pokémon Red Darkest Dungeon Minecraft Stardew Valley StarCraft II Slay the Spire Baba Is You 2048 Table 9: Genre categorization of the 12 games in Orak. denotes the main genre and indicates the secondary genre. (a) Street Fighter III [53] is classified as an Action game, as it primarily relies on responsiveness, frame-precise input, and physical dexterity. Its gameplay requires fast reflexes and mastery of complex input sequences. (b) Super Mario [54] is categorized as Action game. The core gameplay emphasizes precise timing in jumping and movement, demanding moment-to-moment control in response to environmental hazards and enemy placements. (c) Ace Attorney [55] is labeled as an Adventure game due to its narrative-driven structure, reliance on clue collection, and logical deduction. Players progress by interacting with characters and uncovering story elements through investigative mechanics, with minimal emphasis on reflex-based input. (d) Her Story [56] similarly fits within the Adventure category. Though more experimental in form, it shares strong focus on narrative discovery through search-based interface, requiring players to piece together fragmented story using non-linear exploration and deductive reasoning. (e) Pokémon Red [57] is classified as Role-Playing game, as its primary mechanics involve turnbased combat, character progression, stat management, and quest-driven exploration. Additionally, Adventure was assigned as secondary genre to reflect the games emphasis on world exploration, interaction with non-player characters, and sequential progression through narrative landmarks. 16 (f) Darkest Dungeon [58] is similarly assigned Role-Playing as the main genre, supported by stat-driven character development and progression systems. However, it also includes significant Strategy elements, as players must carefully manage resources, form party compositions, and make tactical decisions in turn-based combat. Adventure was further added as secondary genre due to its dungeon-crawling structure and emphasis on risk-driven exploration. (g) Minecraft [59] is categorized as Simulation game due to its open-ended, system-driven mechanics, including resource gathering, crafting, and environmental manipulation. It also exhibits Role-Playing traits through its progression systems and player-driven narrative development, warranting secondary classification. (h) Stardew Valley [61] is also assigned to the Simulation genre based on its emphasis on time management, farming systems, and interrelated mechanics spanning multiple in-game variables. It incorporates Role-Playing through relationship-building and character progression, and Adventure through dungeon exploration, seasonal events, and quest-based interactions. (i) StarCraft II [62] is classified as Strategy game, consistent with its real-time strategic planning, resource allocation, and micromanagement mechanics. Given the importance of unit-level control, Action was added as secondary genre to reflect the real-time, reflex-driven demands during gameplay. (j) Slay the Spire [63] is categorized solely as Strategy game. The gameplay centers around deck-building, route optimization, and turn-based combat, requiring players to plan several moves. (k) Baba Is You [64] is clearly identified as Puzzle game, as its mechanics are centered on solving logic-based problems through the manipulation of in-game rules represented by words. The core loop involves constrained, rule-based problem solving and spatial reasoning. (l) 2048 [65] is also classified as Puzzle game, characterized by deterministic mechanics, arithmetic pattern recognition, and constraint-based spatial logic within fixed grid. This genre classification provides structured foundation for analyzing agent cognition and gameplay dynamics across diverse games."
        },
        {
            "title": "B Required LLM Capabilities for Gameplay",
            "content": "Looking at the Figure 3, most games demand advanced LLM capabilities across multiple dimensions, (1) Action games, as they are designed to challenge wide range of human cognitive skills. Street Fighter III and Super Mario in red, require spatial reasoning and rule following, more than long-context understanding and planning. (2) Adventure games, Ace Attorney and Her Story in yellow, emphasize long-text understanding and logical reasoning due to the need to comprehend long storylines. (3) Role-playing games, Pokémon Red and Darkest Dungeon in brown, require strong long-term planning, logical reasoning, and rule-following abilities to understand game-specific rules and complete milestones of game tasks. (4) Simulation games, Minecraft and Stardew Valley in green, also require high levels of long-term planning and rule-following abilities. While Minecraft requires strong spatial reasoning and error handling, which are generally essential for simulation games, Stardew Valley gets lower scores for them because these abilities are not critical for its evaluation task; earning money by harvesting crops. (5) Strategic games, StarCraft II and Slay the Spire in blue, require various LLM abilities for gameplay. Notably, these two games are the only ones in Orak that require 5 different LLM capabilities rated at level 3, highlighting that recent strategic video games increasingly demand wide range of cognitive skills for effective gameplay. (6) Puzzle games, Baba Is You and 2048 in purple, require high levels of spatial reasoning, logical reasoning, and long-term planning because puzzle games are typically designed to require complex problem-solving through multiple reasoning hops and spatial understanding."
        },
        {
            "title": "C Street Fighter III",
            "content": "C.1 Game Description for Street Fighter III Environment. Street Fighter III [53] is 2D competitive fighting game, known for precise controls, deep mechanics, and diverse roster of characters. Each character features unique moves, combos, and super arts, requiring precise timing and strategic decision-making. Players aim to defeat their opponent through mix of normal attacks, special moves, and advanced mechanics like parries and cancels. For implementation, we use Diambra Arena environment [74], Dockerbased platform designed for RL research. Street Fighter III is one of the environments supported by Diambra, which not only enables seamless extraction of the game statesuch as screenshots, health, timer, and super bar valuesbut also provides straightforward interface for sending controller inputs. In this setting, the agent operates in discrete action space that directly corresponds to various controller actions, including directional movement, attack buttons, and their combinations, enabling intuitive and fine-grained control of the in-game character. Figure 5: Two of playable characters in Street Fighter III. (b) Chun-Li. (a) Ken. The game supports both single-player and multi-player modes. In single-player mode, the player progresses through ten increasingly difficult stages, facing stronger opponents at each level. Each stage follows best-of-three format, where the player must win two out of three matches to advance to the next stage. The game ends upon either completion of the final stage or defeat. In multi-player mode, two players compete in best-of-three match, and the game is over once winner is determined. For our default evaluation setting, agents play Ken in both modes (see Figure 5(a)). However, since the environment supports variety of characters, we also conduct evaluations using Chun-Li to demonstrate intra-game generalization capabilities (see Figure 5(b)). Observation-to-Text Conversion. The Diambra environment offers convenient interface for extracting the game state from Street Fighter III. Through this interface, we obtain the latest game frame at resolution of 224384, along with key state information such as remaining time, player and opponent health, super bar gauge, super count, stun bar gauge, and stun status. However, critical aspect in fighting games is understanding the relative positions of the characters, which is not directly provided by the Diambra environment. To address this limitation, we employ lightweight YOLOv11 object detection model [75] to extract the relative positions of the two characters from the game frame. Based on the computed distance, we classify the spatial relationship into three discrete categoriesvery close, close, and farand incorporate this information into the agents user prompt. Figure 6: Character detection using YOLOv11 model [75] in Street Fighter III. Action Space. In the Diambra environment, the native action space is defined on per-frame basis and consists of 18 discrete actions. These are composed of: Idle action (1 total): Idle (No action) Movement actions (8 total): Left, Left+Up, Up, Up+Right, Right, Right+Down, Down, Down+Left Attack actions (9 total): Low Punch, Medium Punch, High Punch, Low Kick, Medium Kick, High Kick, Low Punch+Low Kick, Medium Punch+Medium Kick, High Punch+High Kick To enable more strategic and temporally consistent behavior, we use higher-level action space that abstracts these frame-level controls into semantically meaningful commands. Each high-level action is mapped to predefined sequence of low-level controller inputs, often spanning multiple frames. The high-level action space is divided into two categories: Character-agnostic actions (14 total): Move Closer, Move Away, Jump Closer, Jump Away, Super Attack, Low Punch, Medium Punch, High Punch, Low Kick, Medium Kick, High Kick, Low Punch+Low Kick, Medium Punch+Medium Kick, High Punch+High Kick 18 Figure 7: Planning prompt for reflection-planning agent playing Street Fighter III. Character-specific actions: Ken: Fireball (Hadouken), Hurricane Kick, etc. Chun-Li: Kikkoken, Hyakuretsukyaku, etc. For example, if the character is positioned on the left side of the screen and the high-level action is Move Closer, the system issues Right movement commands over four frames. If the action is Fireball, the corresponding low-level sequence would be DownDown+RightRightMedium Punch. Since the number of character-specific actions varies, the total size of the high-level action space differs depending on character, typically ranging around 20 actions. C.2 Gameplay Prompt for Street Fighter III Our implementation of Street Fighter III supports four types of agents: reflection, planning, reflectionplanning, and zero-shot. Among these, we introduce prompts for the reflection-planning agent in this subsection. Figures 79 present the prompts used by the reflection-planning agent for planning, action inference, and reflection, respectively. At each step, the agent plans to determine the subtask, using the planning module. Based on this subtask, the action inference module infers the optimal action to execute. Finally, the reflection module evaluates whether the executed action was successful. Planning prompt. As shown in Figure 7, the system prompt provides detailed instructions for an agent to support strategic planning. It defines the assistants role in proposing suitable subtasks based on the target task and the current game state. The user prompt includes: (1) the main goal of the game, (2) the previous subtask (generated by the recent planning module), (3) the last executed action, (4) self-reflection on the last action (generated by the recent reflection module), (5) the current state, and (6) the expected output format for the subtask reasoning task. Figure 8: Action inference prompt for reflection-planning agent playing Street Fighter III. Action inference prompt. As shown in Figure 8, the system prompt outlines strategic guidelines for playing Ken, predefined set of valid actions, and the required output format. The user prompt contains: (1) the current subtask (provided by the recent planning module), (2) the last executed action, (3) the corresponding self-reflection (generated by the recent reflection module), (4) the current state, and (5) the expected output format for the action inference task. Reflection prompt. As illustrated in Figure 9, the system prompt provides detailed instructions for an agent to perform reflection. The agent is required to analyze whether the last action was successful based on state transitions. The user prompt includes: (1) the target task, (2) the current subtask (generated by the recent planning module), (3) the last executed action, (4) the previous state, (5) the current state, and (6) the expected output format for the reflection task. C.3 Evaluation Metric for Street Fighter III Single-Agent Play. In the single-player mode, the agent faces series of 10 stages against in-game rule-based bots. The game ends either when the player loses stage or successfully clears all 10 stages. Therefore, the evaluation metric can be straightforwardly defined as Score = Number of stages cleared by the agent 10. Multi-Agent Play. To evaluate models in competitive multi-agent environment, we conduct pairwise matches between all agents and compute Elo ratings based on their win rates. For each pair, three games are played to obtain reliable estimate of relative performance. The resulting win rate matrix and Elo scores are presented in Figure 4(a). 20 Figure 9: Reflection prompt for reflection-planning agent playing Street Fighter III. We adopt Bradley-Terry model formulation [76] for Elo estimation, where each models rating is iteratively optimized using gradient ascent on the log-likelihood of observed outcomes. The gradient is computed based on the expected win probabilities derived from current ratings, using the standard Elo transformation: (i beats j) = 1 1 + 10(Rj Ri)/ , where Ri and Rj denote the Elo ratings of agent and agent j, respectively. The expected probability of defeating increases as the rating difference Ri Rj becomes larger. After optimization, we shift all Elo ratings so that their mean equals 1,500 for intuitive interpretation. If two models receive identical ratings, the one that won their head-to-head match is ranked higher. C.4 Experimental Configuration for Street Fighter III For all 6 open-source LLMs, including Llama-3.2-1B/3B, Qwen-2.5-3B/7B, and Minitron-4B/8B, we use temperature of 0.0 and repetition penalty of 1.0. During LLM inference, we pause the Street Fighter III game environment. We set the maximum number of game steps to 10,000. Due to the in-game time constraints, episodes do not reach the maximum of 10,000 steps. single stage (best-of-three matches) is usually resolved within 100 to 200 steps, as rounds either timeout or end early when one players health reaches zero. C.5 Result for Street Fighter III All reported results in Tables 1013 are computed as the mean and standard deviation over three to five independent runs, using zero-shot agent for each model configuration. To establish baseline 21 Models SF3 Rank Models Agent SF3 Rank Models Input SF3 Rank Random Agent 10.06.4 Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 0.00.0 13.35.8 20.00.0 16.711.5 16.711.5 23.35.8 16.711.5 29.714.3 33.315.3 13.311.5 16.711.5 20.00. 12 13 10.5 4.5 7.5 7.5 3 7.5 2 1 10.5 7.5 4.5 Table 10: Gameplay score on Street Fighter III. Random Agent - Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 10.06.4 13.35.8 30.017.3 20.00.0 16.720. 29.714.3 23.320.8 30.026.5 23.320.8 9 8 1.5 6 7 3 4.5 1.5 4.5 Table 11: Ablation study for agentic modules on Street Fighter III. Random Agent - 10.06.4 10 GPT-4o Gemini Claude Text Image Both Text Image Both Text Image Both 29.714.3 23.715.9 24.314.5 13.311.5 16.711.5 20.010.0 16.711.5 23.311.5 33.35. 2 4 3 9 7 6 7 5 1 Table 12: Comparison across modalities on Street Fighter III. Model Llama-3.2-1B Finetune Llama-3.2-3B GPT-4o SF 0.00.0 42.016.4 12.011.0 40.07.1 10.00.0 Table 13: Intra game generalization score of Street Fighter III. for comparison, we additionally included random agent that selects an arbitrary action uniformly at random. This agent was evaluated over 30 episodes, and its performance is summarized in tables. Single-Agent Play. The performance across various LLMs was evaluated as presented in Table 10. The smallest model, Llama-3.2-1B, completely failed to comprehend the current game context and consistently ignore the required output format, performing even worse than random agent. On the other hand, all other evaluated models surpassed the random agent, demonstrating varying levels of competence. Commercial LLMs generally outperformed their open-source counterparts, with GPT-4o and o3-mini standing out as notable examples. To investigate the effectiveness of agentic modules, we conducted an ablation study shown in Table 11. For Llama-3.2-3B, the Reflection agent showed notably superior performance, indicating that reflective reasoning significantly aids in aligning actions to game dynamics, possibly by allowing the model to reassess and correct previous outputs based on feedback. Conversely, for GPT-4o, both the zero-shot and Planning agents performed remarkably well. This may suggest GPT-4os inherent capability for generalization (zero-shot) and structured sequential reasoning (planning), enabling efficient decision-making without iterative reflection. Input modality may significantly influence agent performance, since spatial information in fighting games is expecially important for gameplay. Since our implementation simplify character distance into three sparse levels, we expected image inputs would enhance spatial understanding and consequently improve performance. However, as shown in Table 12, GPT-4o surprisingly demonstrated decreased performance when using image inputs, possibly due to limited visual comprehension capabilities. In contrast, Gemini and Claude effectively leveraged visual data, improving their gameplay scores. Lastly, Table 13 demonstrates intra-game generalization capabilities when fine-tuning models on Ken-specific gameplay data and evaluating on Chun-Li scenarios, which represent out-of-distribution conditions due to differences in action spaces (as previously detailed in Section C.1). Remarkably, the previously format-incompliant pretrained Llama-3.2-1B learned to follow the required output format effectively and exhibited excellent gameplay performance after fine-tuning, even in Chun-Li gameplay. similar improvement was observed in Llama-3.2-3B, which even surpassed GPT-4o in performance. These results highlight the significant impact of targeted fine-tuning, demonstrating that relatively small-scale LLMs can achieve substantial gains in task-specific capability. Multi-Agent Play. We evaluate agent performances in multi-agent environment by conducting pairwise matches among 8 LLMs, all operating in zero-shot setting. The results are provided in Figure 4(a). Each pairwise matchup consisted of three independent games, with each game played in 22 best-of-three format to determine the winner. To ensure fair performance comparison, both agents used the same characterKenin all matches. Notably, unlike the single-agent evaluation results in Table 10, Minitron-8B consistently outperforms all other models in the multi-agent arena and achieves the highest Elo rating. This divergence raises the possibility that the involvement of other intelligent agents could alter the game dynamics, perhaps due to increased strategic diversity or emergent adversarial behavior."
        },
        {
            "title": "D Super Mario",
            "content": "D.1 Game Description for Super Mario Environment. Super Mario (1985 Super Mario Bros) [54] is side-scrolling game where the player controls Mario to avoid obstacles, defeat monsters, and reach the flag. In this environment, Mario progresses through the game using directional key controls (e.g., left and right keys) and jump actions. Mario should either destroy or traverse obstacles (e.g., bricks, stairs, pipes), avoid or defeat monsters (e.g., Goombas, Koopas) by jumping on them and avoid Figure 10: Screenshot and assets of Super Mario. falling into pits. For implementation, we adopt the gym_super_mario_bros environment [77], which is widely used in the reinforcement learning (RL) community. Specifically, we use the SuperMarioBros-v1 environment, where Mario plays within 256240-pixel screen with black background, as shown in Figure 10(a). The environment consists of 8 worlds, each containing 4 stages. We use World 1, Stage 1 as our default evaluation setting (for Table 3). However, the environment supports evaluation across any world and stage. For example, we use World 3, Stage 1 for the evaluation of intra-game generalization (for Table 7). (b) Object Patterns. (a) Screenshot. Observation-to-Text Conversion. The environment only provides RGB image frames as observations. To convert visual game states into text input suitable for LLMs, we apply visual pattern matching to parse the exact location of each object on the frame. As shown in Figure 10(b), in the SuperMarioBros-v1 environment, the pixel-level visual patterns of objects remain stable across frames. By maintaining set of pixel templates for all game objects as game assets, we perform 2D visual pattern matching to parse the presence and exact location of each object in the scene. These parsed object locations are converted to 2D coordinates (x, y) and formatted as text, which is then passed to the LLM as part of its observation input. Action Space. We constrain Mario to only move in the right direction to simplify the control space. Mario can jump at varying heights, and by constraining action spans 4 game frames (i.e., frame skipping), Marios jumping ability is discretized into 7 levels. Jump Level 0 corresponds to walking forward without jumping, while Jump Levels 1 through 6 represent increasing jump heights, with Level 6 being the highest possible jump. At each game step, the LLM chooses jump level from 0 to 6, determining the jump height as Mario moves to the right. D.2 Gameplay Prompt for Super Mario Figure 11 shows the action inference prompt used by the zero-shot agent for playing Super Mario. The system prompt contains most of the gameplay-specific knowledge. It includes (1) the main goal of the game, (2) detailed descriptions and sizes of each object, (3) explanations and safety notes for each action, and (4) the expected input-output format between the LLM and the environment. The user prompt provides the current game state as list of all objects detected in the frame, represented by their top-left corner (x, y) coordinates obtained by visual pattern matching. Given this prompt, the LLM agent infers the appropriate jump level to advance safely toward the flag by avoiding obstacles and monsters. 23 Figure 11: Action inference prompt for zero-shot agent playing Super Mario. D.3 Evaluation Metric for Super Mario The goal of Super Mario is to reach the flag located at the right end of the stage. Since the gym_super_mario_bros environment provides Marios current position on the map, we define the evaluation metric as the proportion of the distance traversed toward the flag before Mario dies. Formally, the normalized score is defined as: Score = dist(xM ario, xstart)/dist(xf lag, xstart) 100, where xM ario, xf lag, and xstart are the coordinate of Mario traversed before die, that of the flag, and that of the starting position on the map, respectively. D.4 Experimental Configuration for Super Mario For all 6 open-source LLMs, including Llama-3.2-1B/3B, Qwen-2.5-3B/7B, and Minitron-4B/8B, we use temperature of 1 and repetition penalty of 1. During LLM inference, we pause the Super Mario game environment. We set the maximum number of game steps to 100. We run all experiments with 20 trials and report the average score with the standard deviation. D.5 Result for Super Mario As shown in Table 14, Gemini-2.5-pro achieves the highest score of 38.0 on Super Mario, followed by o3-mini with the score of 34.9. Among open-source LLMs, Llama-3.2-3B and Minitron-8B perform competitively, achieving scores of 31.8 and 31.3 respectively, which are comparable to the 24 Models SuperMario Rank Models Agent SuperMario Rank Models Input SuperMario Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 18.78.6 31.810.1 23.414.1 27.29.6 24.46.0 31.312. 28.88.8 34.114.2 34.914.6 38.014.6 31.78.2 28.713.2 12 4 11 9 10 6 7 3 2 1 5 8 Table 14: Gameplay score on Super Mario. Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 21.28.2 32.48.6 27.08.4 31.810.1 29.69.2 32.315.5 29.412.5 34.114.2 8 2 7 4 5 3 6 Table 15: Ablation study for agentic modules on Super Mario. GPT-4o Gemini Claude Text Image Both Text Image Both Text Image Both 34.114.1 27.113.7 27.110.2 38.013.4 28.510.7 40.99.6 28.713.2 25.66.4 22.66.3 3 6.5 6.5 2 5 4 8 9 Table 16: Comparison across modalities on Super Mario. 31.7 score of Claude-3.7-sonnet. Qualitatively, most LLMs, including Gemini-2.5-pro, frequently fail to correctly estimate the parabolic jump trajectory when Mario is mid-air. This change of moves often results in Mario colliding with obstacles or being killed by monsters. Table 15 shows the effect of reflection and planning modules on gameplay performance. Among these, the reflection module has more pronounced impact. Specifically, when Mario is stuck in front of high obstacles such as warp pipes, the reflection module enables the agent to revise its previous low jump level decisions and select higher jump level, allowing it to overcome the obstacle and proceed, thereby improving the final score. GPT-4o achieves the best score of 34.1 when both reflection and planning modules are used. In contrast, Llama-3.2-3B performs best when only the reflection module is used with score of 32.4, indicating that Llama-3.2-3B may not benefit from the additional planning module or be easily disturbed by its response. As shown in Table 16, using image-only input observations consistently underperforms compared to text-only inputs across all models, including GPT-4o, Gemini-2.5-pro, and Claude-3.7-sonnet. This suggests that relying solely on visual input makes it more challenging for models to extract detailed information from the game scene or to perform spatial reasoning, i.e., estimating distances between objects. In contrast, when both text and image inputs are provided, GPT-4o and Claude-3.7-sonnet show performance drop, while Gemini-2.5-pro shows improved performance. This indicates that multimodal input can be beneficial when the model effectively integrates complementary information from both modalities."
        },
        {
            "title": "E Ace Attorney",
            "content": "E.1 Game Description for Ace Attorney Environment. Ace Attorney [55] is courtroom adventure game where players act as defense attorneys, gather evidence, and cross-examine witnesses. We target the first episode of Phoenix Wright: Ace Attorney Trilogy on Steam (see Figure 12). We define four subtasks: one threequestion multiple-choice quiz where the player selects the correct answer, and three cross-examination tasks where the player presses witnesses for more details or presents evidence to expose contradictions. We use Harmony [78] with BepInEx plugin [79] to hook the games source code at launch, capturing states such as dialogue text, arrowbutton visibility, keyboard inputs, and Court Record entries. The hooks save states as .txt or .json files and monitor command .txt file for inputs, injecting them into the game in real time. Figure 12: Screenshot of Episode 1: The First Turnabout. Observation-to-Text Conversion. All states remain in text form and require only minimal postprocessing. We map speaker indices to character names (e.g., index 2 Phoenix Wright) using predefined mapping and replace original names with arbitrary aliases to prevent contamination (e.g., Phoenix Wright Alias). We also convert Court Records and multiple-choice candidate optionsoriginally stored in .jsoninto continuous descriptive text when they exist. 25 Figure 13: Action inference prompt for zero-shot agent playing Ace Attorney. Action Space. The basic actions include pressing Ok to progress the dialogue and Tab to access the Court Records. For multiple-choice questions, the action space expands to include candidate options. During cross-examinations, additional actions become available: Left to return to the previous dialogue, Press to question the witness further, selecting relevant evidence from the Court Records, and Present to introduce it during the examination. Each option carries an index, and the model returns only the corresponding integer. E.2 Gameplay Prompt for Ace Attorney Figure 13 presents the zero-shot agents action-inference prompt for Ace Attorney. The system prompt specifies (1) the games goal, (2) procedures for dialogue, multiple-choice questions, and cross-examinations, (3) rules for accessing Court Records and presenting evidence, and (4) the expected I/O format between the LLM and the environment. The user prompt lists recent conversationshighlighting the conversation currently visible on the screenand provides timestamped Court Record entries. Using this prompt, the agent detects contradictions, selects the correct actions, and manages dialogue and evidence to advance the trial. E.3 Evaluation Metric for Ace Attorney Each subtask begins and ends at fixed points, with screenshots at both start and end and the preceding conversation history provided, and yields reward ri and step count ti. In the multiple-choice task (MC), ri is the number of correct answers out of three, while in cross-examination tasks CE1, CE2, and CE3, ri is 1 for pass and 0 for fail; any failure incurs maximum step count of 50. We normalize each reward and step count against fixed benchmarks ri and ti, namely (3, 25) for MC, Models AceAttorney Rank Models Agent AceAttorney Rank Models Input AceAttorney Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 1.32.2 4.61.3 20.017.4 9.30.2 35.74.5 29.93.6 28.42.8 85.31.5 91.71.5 55.73.4 81.91.6 83.31.5 12 11 9 10 6 8 2 1 5 4 3 Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 5.73.2 4.61.3 4.61.3 3.80. 49.91.3 85.31.5 52.70.5 52.80.5 5 6.5 6.5 8 4 1 3 2 Table 18: Ablation study for agentic modules on Ace Attorney. GPT-4o Gemini Claude Text Both Text Both Text Both 85.31.5 53.51.7 55.73.4 52.60. 81.91.6 71.317.3 1 5 4 6 2 3 Table 19: Comparison across modalities on Ace Attorney. Table 17: Gameplay score on Ace Attorney. (1, 11) for CE1, (1, 3) for CE2, and (1, 4) for CE3, by computing pi = ri ri , si = ti ti , and assign each task difficulty weight wi {1, 4, 2, 3} reflecting MC, CE1, CE2, and CE3 respectively. These weights were set proportional to each tasks difficulty and the minimum number of steps required (the benchmarks). We then form weighted averages = (cid:80) wi pi (cid:80) wi , = (cid:80) wi si (cid:80) wi , and combine them into composite score Score = 100(cid:0)α + (1 α) B(cid:1), with α = 0.7 (70% for accuracy, 30% for efficiency). All experiments were repeated three times, and we report the sample mean and standard deviation of score. E.4 Experimental Configuration for Ace Attorney For all six open-source LLMs (Llama-3.2-1B/3B, Qwen-2.5-3B/7B, and Minitron-4B/8B), we set the temperature to 0.7, apply repetition penalty of 1, cap game steps at 50, and limit conversation history to the most recent 20 exchanges. E.5 Result for Ace Attorney As shown in Table 17, o3-mini tops the leaderboard with score of 91.7, followed by GPT-4o with score of 85.3, Deepseek-R1 with score of 83.3, and Claude-3.7-sonnet with score of 81.9. Among open-source models, Gemini-2.5-pro and Minitron-4B score 55.7 and 35.7, respectively, while smaller Llama and Qwen variants remain below 20. Notably, Qwen-2.5-3B completes the first cross-examination in one out of three trials, where other open-source models struggle. Models scoring around 30 often misinterpret key details and repeat the same mistakes until they reach the maximum number of steps. Models below 20 sometimes reason correctly but mostly select irrelevant options, whereas the lowest performers fail to follow the required format and cannot advance further. Table 18 compares our reflection and planning modules. The base Llama-3.2-3B model scores 5.7; adding reflection or planning alone yields approximately 4.6, and combining both drops the score to 3.8. Smaller models like Llama-3.2-3B often focus on incorrect details and produce flawed reasoning when reflection or planning is faulty. GPT-4o scores highest with reflection alone (85.3), since reflection prevents repeated errors and helps spot contradictions. Planning alone adds little, and combining it with reflection lowers the score to approximately 52.7, as it attempts to resolve contradictions not visible on-screen. Table 19 compares input modalities. GPT-4o and Gemini-2.5-pro score 85.3 vs. 53.5 and 55.7 vs. 52.6 for text-only vs. multimodal inputs, respectively, while Claude-3.7-sonnet falls from 81.9 to 71.3. Because we supply complete text descriptions of every on-screen element and its context, adding visual input brings no improvementconfirming that text alone suffices to play Ace Attorney."
        },
        {
            "title": "F Her Story",
            "content": "F.1 Game Description for Her Story Environment. Her Story [56] is an interactive adventure game where players explore police interview clips to uncover hidden truth. The player begins the game by accessing an old desktop interface, where program called L.O.G.I.C. Database is open. The player can enter keywords into the database to retrieve up to five video clips whose transcripts contain the searched word. By watching these clips and gathering clues, the player repeatedly formulates new queries to reconstruct the underlying story. To interface the game with our code, we use Harmony [78] together with Unity Doorstop [80] to log internal game states to .txt file. Specifically, each line in the file is JSON object representing snapshot of the games state at given moment. Each object contains an event type and its associated metadata. The following examples illustrate key elements of the logged game state. For clarity, we omit auxiliary metadata that are present in the actual logs but are only used for debugging or UI-related purposes, such as video IDs, screen resolution, and UI element positions. Load title screen: {\"status\": \"title\"} Load L.O.G.I.C. Database: {\"status\": \"start_game\"} Query: {\"status\": \"query\", \"keyword\": keyword} Get query result: {\"status\": \"query_result\", \"num_total\": number of clips containing the keyword, \"num_visible\": number of clips shown, \"video\": list of {\"new\": 1 if not viewed, otherwise 0, \"session\": recording date, \"outfit\": visual description of the thumbnail}} Open video panel: {\"status\": \"open_detail\"} Close video panel: {\"status\": \"close_detail\"} Play video: {\"status\": \"play_video\", \"script\": transcript of the video} Close video: {\"status\": \"close_video\"} Observation-to-Text Conversion. We aggregate the game states and convert them into textual observations. Each observation includes summary information such as the number of clips containing the keyword and the number of clips shown. It also contains per-clip metadata, including the recording date, thumbnail description, viewing status, and the transcript if the clip was viewed after the query. Action Space. The original Her Story game is designed as point-and-click interface, where the player interacts with the game by typing keywords into search bar, clicking on retrieved clip thumbnails, navigating panels, and controlling playback. These interactions rely on low-level input mechanisms such as mouse movements and keyboard input. To reduce complexity, we abstract these low-level interactions into two high-level actions: searching with keyword and playing the retrieved video clip. Search [keyword]: Returns all video clips whose transcripts contain the exact word. It consists of three low-level GUI actions: (1) clicking the search bar, (2) typing the keyword, and (3) pressing Enter to submit the query. Play Video [i]: Plays the i-th video from the current search result list. It consists of four low-level GUI actions: (1) clicking the thumbnail of the video clip to open the panel, (2) clicking the thumbnail within the panel to start playback, (3) either waiting until the video finishes or pressing the ESC key to exit playback, and (4) clicking the Exit button to close the panel. F.2 Gameplay Prompt for Her Story Figure 14 shows the action inference prompts used by the zero-shot agent to play Her Story. The system prompt provides instructions covering: (1) the main goal of the game, (2) the type of information each video clip may contain, (3) behavioral rules the agent should followsuch as avoiding repeated keywords in the search historyand (4) the expected input-output format between the LLM and the environment. The user prompt includes: (1) the last executed action, (2) the current game state, and (3) the search history. 28 Figure 14: Action inference prompt for zero-shot agent playing Her Story. F.3 Evaluation Metric for Her Story To uncover the truth behind the case, the player actively explores the video archive by issuing queries and watching clips. We evaluate this behavior by counting how many unique clips the player has viewed from the full archive of 272 clips. This aligns with the original game design, where unlocking achievements and reaching the ending depend on the number of clips watched. Specifically, we define the score as: Score = (cid:18) Number of distinct video clips viewed 272 (cid:19) 100. F.4 Experimental Configuration for Her Story For all LLMs, we use temperature of 0.3 and repetition penalty of 1. We limit the maximum number of interactions with the game environment to 400 steps. We run each experiment three times and report the average score along with the standard deviation. F.5 Result for Her Story Table 20 summarizes the gameplay scores of different LLMs on Her Story. Commercial LLMs outperform open-source models in this task. Gemini-2.5-pro achieves the highest score of 67.3, followed by Deepseek-R1 (66.9), o3-mini (66.3), GPT-4o (64.0), Claude-3.7-sonnet (62.6), and GPT-4o-mini (21.1). The low score of GPT-4o-mini is due to its repeated search of keywords such as alibi, evidence, and witness, even after those queries have already been issued. This redundancy reduces its ability to discover new clips. Among open-source LLMs, all models score below 10. Most exhibited similar failure patterns: they fail to play unseen clips, repeatedly issue the same keywords, or search using entire sentences (e.g., for the name \"Simon\", for keywords related to Lunas) rather than meaningful single words. 29 Models HerStory Rank Models Agent HerStory Rank Models Input HerStory Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 2.11.2 4.21.1 1.21.1 8.51.9 4.52.2 8.21.8 21.15.5 64.05.1 66.33.6 67.33.3 62.62.6 66.93.9 11 10 12 7 9 8 6 4 3 1 5 Table 20: Gameplay score on Her Story. Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 4.21.1 4.41.3 5.11.0 5.40. 64.05.1 61.30.4 59.25.7 61.84.9 8 7 6 5 1 3 4 2 Table 21: Ablation study for agentic modules on Her Story. GPT-4o Gemini Claude Text Both Text Both Text Both 64.05.1 40.429.4 67.33.3 64.72. 62.62.6 63.43.1 3 6 1 2 5 4 Table 22: Comparison across modalities on Her Story. Table 21 reports the effect of incorporating reflection and planning modules on gameplay performance. For Llama-3.2-3B, the reflection-planning agent achieves the best performance, followed by planning, reflection, and zero-shot agents. This suggests that such modules can improve performance for weaker base models. In contrast, for GPT-4o, the zero-shot agent achieves the highest score, and adding reflection or planning modules does not lead to further improvements. These modules tend to produce information that may be redundant for stronger models like GPT-4o, which can already generate effective queries without additional reasoning support. Table 22 compares gameplay performance across different input modalities. In our multimodal setup, the visual input corresponds to the main screen of the L.O.G.I.C. Database interface, as shown in Figure 14. For GPT-4o and Gemini-2.5-pro, using both text and image inputs underperforms compared to the text-only setting. In particular, for GPT-4o, performance varies significantly depending on the random seed when using multimodal inputs. We observe issues such as repeated use of the same keywords and the inclusion of quotation marks around keywords (e.g., \"murder\"), which often lead to failed queries. In contrast, Claude-3.7-sonnet shows slightly better performance in the multimodal setting. We speculate two reasons why the visual input fails to improve performance in most cases: (1) most of the useful visual elements (e.g., clip lists, thumbnails) are already represented in the text observation, and (2) additional information that visual input could providesuch as the interviewees gestures or expressionsis only available when the video is actually played, and thus not present in the static visual input used in our setup. Pokémon Red G.1 Game Description for Pokémon Red Environment. Pokémon Red [57] is role-playing game where the player navigates the Kanto region to catch and train creatures called Pokémon, battle other trainers, and ultimately defeat the Elite Four and the Champion. The player explores various environments, including towns, routes, caves, and buildings, encountering wild Pokémon and other characters. The gameplay loop involves exploring these areas, engaging in turn-based battles with Pokémon, and managing team of up to six Pokémon. For implementation, we utilize the PyBoy [81] emulator to run the game. Specifically, our evaluation focuses on segment where the player starts in Pallet Town and progresses towards Viridian City, encountering wild Pokémon and trainers. The game screen resolution is 160144 pixels. Observation-to-Text Conversion. Instead of relying on visual pattern matching, we directly access the games internal memory via the PyBoy emulator to extract relevant game state information. This includes detailed map information, the players current coordinates, information about the players party (e.g., Pokémon, their HP), encountered opponent Pokémon information, and the players inventory of items. This rich set of information is then formatted as text to serve as the observation input for the LLM. Action Space. The fundamental action space consists of the Game Boy buttons: up, down, left, right, a, b, start, and select. Additionally, we define set of higher-level tools to facilitate more complex interactions: move_to(x, y): Finds and executes path to the specified map coordinates (x, y). 30 interact_with_object(object_name): Interacts with specified object in the environment. warp_with_warp_point(warp_point_coord): Uses specified warp point to move to different location. overworld_map_transition(direction): Transitions to an adjacent map in the given direction. continue_dialog(): Advances the current dialogue. select_move_in_battle(move_name): Selects and uses specific move in Pokémon battle. switch_pkmn_in_battle(pkmn_name): Switches to different Pokémon in the players party during battle. run_away(): Attempts to flee from wild Pokémon battle. use_item_in_battle(item_name): Uses specified item during Pokémon battle. At each step, the LLM can choose up to five consecutive fundamental actions or invoke one of the provided tools. G.2 Gameplay Prompt for Pokémon Red Figure 15 shows the system prompt used by the agent for playing Pokémon Red, which provides the LLM with the necessary game rules, action space details (including basic controls and available tools), information about different game states, and the expected input/output format. The system prompt guides the LLM on how to interpret the game state and decide on the next action or tool to use to achieve the overarching goals of becoming the Champion and completing the Pokédex. Figure 16 illustrates an example of the user prompt provided to the LLM. This includes the recent history of actions and their outcomes, the current game state (map information, player position, inventory, party, screen text, etc.), any recent critique on the agents actions, the current sub-task (if any), and relevant memory entries. Based on this information, the LLM infers the next action or tool to use, following the guidelines set in the system prompt. G.3 Evaluation Metric for Pokémon Red The goal in our defined segment of Pokémon Red is to progress through series of key storyline milestones. We define 12 predefined storyline flags, and our evaluation metric is the percentage of these flags achieved by the agent. The 12 flags are: Exit Reds House, Encounter Professor Oak, Choose starter Pokémon, Finish the first battle with the Rival, Arrive in Viridian City, Receive Oaks parcel, Deliver Oaks parcel to Professor Oak, Obtain the Town Map, Purchase Poké Ball, Catch new Pokémon, Arrive in Pewter City, Defeat Pewter Gym Leader Brock. The final score is calculated as the percentage of these 12 flags that have been successfully triggered within given episode or evaluation period. Formally, Score = (cid:0) Number of flags achieved 12 (cid:1) 100. G.4 Experimental Configuration for Pokémon Red We configure all six open-source LLMs (Llama-3.2-1B/3B, Qwen-2.5-3B/7B, and Minitron-4B/8B) with temperature of 0.1, repetition penalty of 1, and maximum of 1000 game steps. We conduct all experiments over three trials and report the mean score along with its standard deviation. G.5 Result for Pokémon Red Table 23 presents the gameplay scores on Pokémon Red. Gemini-2.5-pro achieves the highest score of 83.3, followed by Deepseek-R1 (75.0) and Claude-3.7 (63.9). GPT-4o achieves score of 38.9. Notably, all the open-source LLMs evaluated (Llama-3.2-1B/3B, Qwen-2.5-3B/7B, and Minitron-4B/8B) recorded score of 0.0, indicating significant challenges in playing Pokémon Red. Interestingly, o3-mini, which is expected to perform well due to its reasoning capabilities, also achieved score of 0.0. We observed that o3-mini exhibited tendency to rely on its pre-existing knowledge or intuition rather than adapting to the game environment, such as consistently moving downwards based on likely incorrect assumption about the exits location, leading to unproductive 31 Figure 15: Action inference system prompt for zero-shot agent playing Pokémon Red. 32 Figure 16: Action inference user prompt for zero-shot agent playing Pokémon Red. repeated actions. This highlights the difficulty some models face in grounding their reasoning within the specific context of the game. Table 24 shows the impact of reflection and planning modules. For GPT-4o, the reflection-planning agent achieved the highest score (38.9), followed by the reflection agent (36.1), and then the planning and zero-shot agents (both at 33.3). This suggests that reflection plays crucial role in improving performance in Pokémon Red. In contrast, Llama-3.2-3B consistently scored 0.0 across all agent configurations. 33 Models Pokémon Red Rank Models Agent Pokémon Red Rank Models Input Pokémon Red Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 38.99.6 0.00.0 83.30.0 63.99.2 75.00.0 8.5 8.5 8.5 8.5 8.5 8. 8.5 4 8.5 1 3 2 Table 23: Gameplay score on Pokémon Red. Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 0.00.0 0.00.0 0.00.0 0.00.0 33.30.0 36.14.8 33.30.0 38.99.6 6.5 6.5 6.5 6.5 3.5 2 3.5 1 Table 24: Ablation study for agentic modules on Pokémon Red. GPT-4o Gemini Claude Text Both Text Both Text Both 38.99.6 41.78. 83.30.0 83.30.0 63.919.2 72.24.8 6 5 1.5 1.5 4 3 Table 25: Comparison across modalities on Pokémon Red. The comparison across input modalities is presented in Table 25. For Gemini-2.5-pro and Claude-3.7, using both text and image inputs resulted in performance equal to or better than using text input alone. This indicates that visual information can be beneficial in this environment. GPT-4o also showed performance increase when both modalities were used (41.7) compared to text-only input (38.9), suggesting that incorporating image data aids the agents decision-making."
        },
        {
            "title": "H Darkest Dungeon",
            "content": "H.1 Game Description for Darkest Dungeon Environment. Darkest Dungeon is turn-based roguelike role-playing game where the player manages roster of heroes as they explore procedurally generated dungeons filled with monsters, traps, and treasures [58]. Each hero has unique abilities and stress level that influences their behavior during combat and exploration. Stress accumulates through continued exploration and battle, and heroes who reach high stress thresholds may develop afflictions that hinder or occasionally enhance their performance. The game emphasizes tactical positioning, turn-order strategy, and long-term roster management. Elements of randomness, such as attack accuracy, critical hits, and affliction outcomes, introduce uncertainty and require players to adapt their strategies dynamically. For implementation, we build our environment on top of rule-based bot [82], replacing its rule-based combat logic with the decisions made by LLM agents. To access internal game states, we utilize the Darkest Dungeon Save Editor [83]. Since the game does not support complete control via keyboard input, we employ the Xbox 360 controller emulator to inject actions from the LLM agent. We evaluate the agents performance during the first embarkation mission after the tutorial, which we designate as our default evaluation setting. For consistency, we fix the party roster to include the Plague Doctor, Vestal, Highwayman, and Crusader, in that order, and equip the inventory with an additional 8 Food and 8 Torches. To ensure reproducibility, we provide save file with this setup preconfigured. During the mission, dungeon exploration is handled by rule-based logic, while all combat decisions are delegated to the LLM agent. Observation-to-Text Conversion. We convert the internal game state of Darkest Dungeon into structured textual description suitable for LLM input. The observation includes combat-relevant details such as the active heros stats, available skills, party composition, and enemy formation. For each hero, we format key attributes (e.g., HP, stress, position, status effects) along with skill availability and target constraints, using symbolic descriptors extracted from parsed skill configuration file. Enemy information is similarly structured, including HP, rank, resistances, and threat indicators. The final text is composed of three parts: detailed hero description with skill information, party summary with basic stats, and an enemy formation breakdown. Action Space. At each decision point, the agent chooses one of four action types: Attack, Heal, or Swap. For Attack and Heal actions, the agent specifies skill slot index and target index corresponding to specific enemy or ally. Swap actions require the agent to provide the current heros rank and swap distance. To allow for complex plans (e.g., swapping then healing), the agent may output up to two such structured command lines in sequence. 34 Figure 17: Action inference prompt for zero-shot agent playing Darkest Dungeon. H.2 Gameplay Prompt for Darkest Dungeon Figure 17 shows the action inference prompt used by the zero-shot agent for playing Darkest Dungeon. The system prompt encodes task-specific knowledge, including (1) the primary objective of completing expeditions while minimizing party stress, (2) strict targeting constraints for combat skills based on hero and enemy rank positions, and (3) the expected format for issuing valid commands. The user prompt provides the current game state, including the acting heros stats and available skills, summary of the party, and the enemy formation. Some fields are omitted for brevity, but the format mirrors the actual prompt used during inference. H.3 Evaluation Metric for Darkest Dungeon To evaluate the performance of an LLM agent in Darkest Dungeon, we define composite scoring metric that reflects progress, survivability, and stress management throughout the expedition. The score consists of three components: (1) the proportion of room combats successfully cleared, weighted at 40 points, (2) the fraction of heroes who survive the entire mission (out of four), weighted at 30 points, and (3) the remaining stress capacity of the team, also weighted at 30 points. However, the latter two components are only counted if the stage is successfully cleared (i.e., if the first term reaches its full 40 points); otherwise, they are set to zero. To ensure fairness, the stress of any hero who dies before the end of the run is treated as the maximum stress (200) when computing the average 35 Models DarkestD Rank Models Agent DarkestD Rank Models Input DarkestD Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 0.00.0 47.539.2 44.822.2 88.82.0 0.00 63.830.4 81.35.8 93.41.5 89.02.1 93.71.6 89.92.5 91.71.1 11.5 9 10 6 11.5 8 7 2 5 1 4 Table 26: Gameplay score on Darkest Dungeon. Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 47.539.2 47.339.0 56.323.6 57.031. 93.41.5 85.210.3 82.08.6 91.62.5 7 8 6 5 1 3 4 2 Table 27: Ablation study for agentic modules on Darkest Dungeon. GPT-4o Gemini Claude Text Image Text Image Text Image 93.41.5 92.23.0 93.71.6 92.21. 89.92.5 90.15.7 2 3.5 1 3.5 6 5 Table 28: Comparison across modalities on Darkest Dungeon. team stress. The final score is computed as: (cid:26)40 (cid:0) # combats cleared 40 (cid:0) # combats cleared (cid:1) + 30 (cid:0) # heroes survived (cid:1) , Score = # total combats 4 # total combats (cid:1) + 30 (cid:0)1 total stress 800 (cid:1) , if stage is cleared otherwise H.4 Experimental Configuration for Darkest Dungeon For all 6 open-source LLMs, we use temperature of 0.7 and repetition penalty of 1, and set the maximum number of game steps to 200. We run all experiments with 3 trials and report the average score with the standard deviation. H.5 Result for Darkest Dungeon Table 26 reports the gameplay scores of various models on Darkest Dungeon. We show that Gemini2.5-pro achieves the highest score of 93.7, closely followed by GPT-4o (93.4) and Deepseek-R1 (91.7), demonstrating strong capabilities across combat decisions and roster management. Among open-source models, Qwen-2.5-7B performs best, achieving score of 88.8. In contrast, smaller models such as Llama-3.2-1B and Minitron-4B fail to make meaningful progress, often producing invalid outputs and scoring 0.0. closer analysis reveals that Llama-3.2-1B frequently fails to follow the correct action format, while Llama-3.2-3B and Qwen-2.5-3B tend to issue invalid commands, such as using unavailable skills from incorrect hero positions or targeting unreachable enemies. These models also overuse Swap actions, leading to inefficient combat sequences. Notably, many small models become stuck in loop when the Crusader hero is affected by the Surprised! status and repositioned to the back row. Since most of the Crusaders skills are unusable from that position, the models repeatedly attempt invalid actions or swap ineffectively, wasting turns and failing to recover from the disrupted formation. Table 27 and Table 28 present ablation studies on agentic modules and input modalities in Darkest Dungeon. For agentic components, GPT-4o performs best in the zero-shot setting, with reflection and planning offering marginal or even negative impact on its performance. Llama-3.2-3B shows small but consistent gains when equipped with the planning module, though overall improvements remain limited. This suggests that large models like GPT-4o already possess sufficient planning capabilities for this task, while smaller models benefit only modestly from explicit agentic prompting. In terms of modality  (Table 28)  , we find that providing image input in addition to text yields minimal improvement. For all models, performance remains similar or only slightly better when image data is included, indicating that the agents primarily rely on the structured text input to make decisions."
        },
        {
            "title": "I Minecraft",
            "content": "I.1 Game Description for Minecraft Environment. Minecraft [59] is an open-ended sandbox game where players explore world, gather resources, and survive by placing and breaking blocks. This environment is based on the Mineflayer JavaScript API [60]. Using Mineflayer, the agent can control Minecraft bot through high-level JavaScript commands. We use Minecraft version 1.19 for compatibility, and to ensure consistent 36 evaluation, we fix the world seed to 42 and initialize the bot at coordinates (604, 100, -823). The bot starts in survival mode with no items and progressively crafts target item using its in-game observations and JavaScript-based actions generated by the LLM. We select 8 target items with varying levels of crafting difficulty: crafting table, stone pickaxe, furnace, bucket, golden sword, diamond pickaxe, enchanting table, and nether portal. Observation-to-Text Conversion. The Mineflayer API provides the bot with its state and contextual information from the surrounding environment. Specifically, the bot receives textual observations in the form of: {Current biome, DayTime, Nearby blocks, Health status, Hunger status, Position, Equipped items, Inventory contents}. Action Space. The action space consists of JavaScript code that interfaces with the Mineflayer API. Following Voyager [20], we expose the following set of control primitives to guide the LLM in generating valid and effective code actions for the bot using in-context learning. exploreUntil(bot, direction, maxTime, callback): Moves the agent in fixed direction for up to maxTime seconds, or until custom stopping condition (defined in callback) is satisfied. mineBlock(bot, name, count): Mines and collects up to count number of blocks with the specified name, within 32-block radius. craftItem(bot, name, count): Crafts the specified item using nearby crafting table. placeItem(bot, name, position): Places block of the specified type at the given position. smeltItem(bot, itemName, fuelName, count): Smelts the specified item using the provided fuel. Requires access to nearby furnace. KillMob(bot, mobName, timeout): Hunts and eliminates the specified mob within the time limit, and collects any resulting drops. getItemFromChest(bot, chestPosition, itemsToGet): Navigates to the chest at the given location and retrieves the requested items. depositItemIntoChest(bot, chestPosition, itemsToDeposit): Navigates to the given chest and deposits specified items into it. I.2 Gameplay Prompt for Minecraft Figure 18 shows the action inference prompt used by the zero-shot agent for playing Minecraft. The system prompt contains gameplay-specific knowledge and guidance for action inference using Mineflayer APIs. It includes (1) the main task of the game, (2) control primitives, which is Javascript code template that should be referred to, (3) game observation in text format, and (4) the expected output response format with reasoning and code. The user prompt provides the current game state provided by Mineflayer APIs. Given this prompt, the LLM agent infers the appropriate Javascript code for action to complete the target task. I.3 Evaluation Metric for Minecraft We evaluate agent performance using the success rate of crafting target item. Since Mineflayer APIs provide access to the agents inventory, we compute the success score by checking whether the target item appears in the inventory at each game step. If the item is successfully crafted and presented, the episode is marked as successful with score of 100; otherwise, 0. I.4 Experimental Configuration for Minecraft For all LLMs, we use temperature of 1 and repetition penalty of 1. The interaction with the game environment is limited to maximum of 100 steps. We repeat all experiments 3 times to craft each target item, and report the average score and standard deviation. I.5 Result for Minecraft As shown in Table 29, o3-mini, Gemini-2.5-pro, and Claude-3.7-sonnet performed the best on Minecraft, each obtaining score of 75.0. All three models successfully crafted the following six items: crafting table, stone pickaxe, furnace, bucket, golden sword, and diamond pickaxe. However, they all failed to craft the two most difficult items: enchanting table and nether portal. In contrast, all six open-source LLMs failed to craft any of the target items, resulting in score of 0.0. 37 Figure 18: Action inference prompt for zero-shot agent playing Minecraft. Models Minecraft Rank Models Agent Minecraft Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00. 46.07.0 71.07.0 75.00.0 75.00.0 75.00.0 41.70.0 9.5 9.5 9.5 9.5 9.5 9.5 5 4 2 2 2 6 Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 50.00.0 13.00.0 50.00.0 6 6 6 6 6 1.5 3 1.5 Table 30: Ablation study for agentic modules on Minecraft. Table 29: Gameplay score on Minecraft. Notably, five of the models, except Minitron-8B, failed to generate any executable JavaScript code compatible with the Mineflayer API. While Minitron-8B was able to generate valid code sometimes to move the bot and mine wood, it failed to craft even the simplest item, the crafting table. As shown in Table 30, the reflection module, which encourages the model to generate improved code actions based on past failed attempts, significantly improved the performance of GPT-4o. However, the reflection-planning agent achieved score of 50.0, which is lower than the default skill-management agent score of 71.0 in Table 29. This suggests that the skill management module, which is responsible for storing previously successful code actions and retrieving them when needed, plays more substantial role in enhancing the performance of Minecraft. 38 Seed Buy Price Sell Price Growth Days Notes Parsnip Seeds Bean Starter Cauliflower Seeds Potato Seeds 20 60 80 50 35 40 175 4 10 12 6 Regrows every 3 days after first harvest 20% chance of extra yield Table 31: Comparison of available seeds in Stardew Valley."
        },
        {
            "title": "J Stardew Valley",
            "content": "J.1 Game Description for Stardew Valley Environment. Stardew Valley [61] is life simulation and farming role-playing game. The player can engage in variety of daily activities such as farming, fishing, mining, foraging, and socializing with villagers. Our objective is to evaluate an LLM agents ability to autonomously perform farming-related tasks that maximize monetary gain within the first 13 in-game days (i.e., until the Egg Festival on Spring 13). Specifically, we focus on harvesting crops and strategically earning money by predicting highprofit crops and interacting with the in-game environment. The character begins on Day 1 with 200 gold. Four types of seeds are available for purchase: parsnip seeds, bean starter, cauliflower seeds, and potato seeds. Each seed type differs in cost, selling price, days to harvest, and other characteristics. Table 31 summarizes the properties of each seeds. To make the task challenging and enforce planning under resource constraints, we manually set the players maximum energy to 50 (default: 200). Using tools such as the hoe or watering can consumes 2 energy per use, limiting the number of tiles the agent can till or water in single day. If the agents energy drops to 0 or below, they start the next day with only 26 energy (instead of 50). Furthermore, if energy falls -15 or below, the player loses 10% of their current gold and still begins the following day with only 26 energy. This constraint encourages the agent to prioritize actions and manage resources efficiently. We run the game on the Steam platform and use the modding tool SMAPI [84] to extract in-game states and implement custom actions. To send keyboard and mouse inputs, we use the pyautogui library on macOS and the AutoHotkey (AHK) library on Windows, following prior work [31]. Since many in-game actions such as planting or watering crops consist of multiple low-level actions (e.g., move up/down/right/left, switch tool, use tool), we define set of high-level actions to abstract these into semantically meaningful units. Each high-level action is mapped to predefined sequence of keyboard inputs and implemented via SMAPI scripts with custom keyboard bindings. We provide the save point used for our experiment to ensure reproducibility. Observation-to-Text Conversion. After each high-level action is executed, we extract JSONformatted game state via SMAPI, which includes information such as player location, current inventory, crop states in the field, remaining energy, and money. This state is then serialized into natural language description and passed to the LLM. Action Space. We define compact action space consisting of 8 high-level actions essential for solving the task. These actions abstract away low-level controls and are defined as follows: till_soil(num_tiles): Tills num_tiles soil tiles to prepare them for planting. The tiles are selected in fixed order starting from the pre-defined position. plant_seeds(): Plants all available seeds from the inventory into empty, tilled soil tiles. If the number of tilled tiles is less than the number of seeds, only the available plots are used. water_seeds(): Waters all planted crops that have not yet been watered on the current day. harvest_crops(): Harvests all crops that are fully grown and ready to be collected. sell_item(): Sells all harvested crops currently in the inventory. buy_item(item_name, item_count): Opens the shop interface, selects the specified item, and attempts to purchase the specified quantity. If there is insufficient money, the agent buys as many units as possible. get_out_of_house(): Moves the character out of the house. go_house_and_sleep(): Navigates the character back to the house, enters it, moves to the bed, and interacts with it to end the day. 39 J.2 Gameplay Prompt for Stardew Valley Figure 19: Action inference prompt for zero-shot agent playing Stardew Valley. Figure 19 shows the action inference prompt used by the zero-shot agent for playing Stardew Valley. The system prompt defines the agents role as an in-game assistant tasked with selecting the best next action based on the current situation and target task. It specifies strict behavioral rules, such as only using actions from predefined set, avoiding repeated failed actions, and formatting outputs as Python code (up to two actions). The valid action set includes available actions with clear descriptions and constraints. The user prompt provides the goal and detailed context, including crop stats, energy rules, and the current game state (location, inventory, weather, etc.). It also includes the last executed actions and requires the agent to valid action output. J.3 Evaluation Metric for Stardew Valley The objective in the Stardew Valley task is to maximize the amount of money earned during the first 13 in-game days, starting from Day 1. This period is natural milestone in the game, as the Egg Festival takes place on Spring 13, where players can purchase high-reward crops such as Strawberry seeds. We evaluate performance based on the net profit earned by the end of Spring 13, calculated as the difference between the final gold amount and the initial amount of 200 gold. We normalize the score using human expert baseline of 1013 200 = 800 gold, which we assign normalized score 40 Models Stardew Rank Models Agent Stardew Rank Models Input Stardew Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 18.932.7 95.75.7 64.718.8 69.611.9 77.713.6 63.024.6 9.5 9.5 9.5 9.5 9.5 9.5 6 1 4 3 2 5 Table 32: Gameplay score on Stardew Valley. Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 0.00.0 0.00.0 0.00.0 0.00.0 40.535.2 18.35.2 64.623.7 95.75.7 6.5 6.5 6.5 6. 3 4 2 1 Table 33: Ablation study for agentic modules on Stardew Valley. GPT-4o Gemini Claude Text Image Both Text Image Both Text Image Both 95.75.7 0.00.0 49.226.0 69.611.9 8.910.5 70.67.1 63.024.6 0.00.0 58.51.2 1 8.5 3 7 2 4 8.5 5 Table 34: Comparison across modalities on Stardew Valley. of 100. Formally, the normalized score is defined as Score = (xfinal xstart)/(xoracle xstart) = (xfinal xstart)/813, where xfinal is the agents final gold amount, xstart = 200 is the starting gold, and xoracle = 1013 is the human expert score. J.4 Experimental Configuration for Stardew Valley For all LLMs, we use temperature of 0.0 and repetition penalty of 1. The interaction with the game environment is limited to maximum of 150 steps. For GPT-4o, we used GPT-4o-2024-05-13 model for Stardew Valley. During LLM inference, the game is paused to prevent in-game time from progressing, which could otherwise alter the environment (e.g., day transition). We perform each experiments three times and and present the average score with the standard deviation. J.5 Result for Stardew Valley Table 32 presents comparison of LLM performance in Stardew Valley. Among the models, GPT-4o presents the best performance, followed by Gemini-2.5-pro. Open-sourced LLMs fails to earn money, primarily for two reasons; (1) failure to perform valid actions (all models excepts Qwen-2.5-7B) (2) poor crop scheduling, resulting in crops not being ready for harvest on Day 13 (Qwen-2.5-7B). Under the imposed energy constraints, the optimal strategy is to plant Parsnip Seeds every four days, as they yield the highest profit. The player should purchase and plant as many seeds as possible on Days 1 and 5, and exactly 24 seeds on Day 9. Planting more than 24 seeds on Day 9 depletes the players energy during watering, leading to insufficient energy the next day and ultimately failure to harvest on Day 13. None of the LLMs, including the API-based models, fully followed this optimal strategy. In particular, most models frequently selected suboptimal crops such as potato seeds, which contributed to their lower performance. Table 33 shows the effect of different agentic modules on performance. In the case of GPT-4o, the planning module has significant effect, as the task requires accurate seed selection and scheduling. In contrast, Llama-3.2-3B model fails to make profit across four agent configuration, indicating that the underlying models capabilities are limiting factor regardless of agent design. Table 34 summarizes game performance under different input modalities. Notably, all three proprietary LLMs fails to achieve strong performance when only vision input is available. This underperformance is mainly due to the difficulty of extracting structured information from single screenshot (see figure 19 for an example). Although the screenshot contains rich contextual information, it also contains lot of redundant content, and critical information occupies only small portion of the image. For instance, watered plants appears slightly darker than dry soil, making it difficult to distinguish visually. Similarly, the current day, crucial cue for planning, is indicated in small font in the upper-right corner of screenshot. When both text and image inputs are given, GPT-4o and Claude-3.7-Sonnet exhibit performance drop, while Gemini-2.5-pro shows improved performance. This suggests that only sufficiently capable models can effectively integrate multimodal information, while others may struggle with modality fusion or become distracted by noisy visual inputs."
        },
        {
            "title": "K StarCraft II",
            "content": "K.1 Game Description for StarCraft II Environment. StarCraft II is real-time strategy game where players gather resources, construct buildings, train units, and command armies to defeat opponents. The environment features partially observable map, requiring the agent to explore and gather information about the opponents actions. For implementation, we adopt the BurnySc2/python-sc2 environment [85], Python interface widely used in the reinforcement learning (RL) community. Note that the library supports the raw scripted interface without graphics-based interface. The environment supports various official maps and game modes. For our default evaluation (as in Table 3), we use the Ancient Cistern LE map with the agent playing as Protoss against the built-in AI bot (Zerg, Hard difficulty; employing timing build order strategy). The environment allows testing across different races, maps, and difficulty settings. For instance, the Babylon LE is used to assess intra-game generalization in Table 7. Observation-to-Text Conversion. The BurnySc2/python-sc2 environment provides the agent with observations capturing the current game state and context. Specifically, the observations include: {Resources, Buildings, Units, Research Progress, In-progress Actions, Enemy Information, Game Time}. We convert these observations into concise text summary; an example summary is shown in Figure 20. Action Space. Following the BurnySc2/python-sc2 implementation, We define the action space as discrete set of 72 high-level commands specifically for the Protoss race. These include unit training (e.g., Probes, Zealots, and Stalkers), building construction (e.g., Pylons and Gateways), research upgrades, scouting, multi-unit attacks or retreats, and special abilities (e.g., Chrono Boost). complete list of these commands is provided in Figure 20. At each game step, the agent generates list of five actions, which are executed in order. K.2 Gameplay Prompt for StarCraft II Figure 20 shows the action inference prompt used by the zero-shot agent for playing StarCraft II. The system prompt contains gameplay-specific knowledge and detailed instructions for action inference tailored to the Protoss race. It includes (1) the main task and game context, (2) comprehensive action dictionary listing all possible unit production, building construction, and research actions, (3) the current game status summary including resources, buildings, units, and ongoing actions, and (4) the expected output response format that guides the agent to provide step-by-step analysis, reasoning, and 5 concrete actionable commands with cost and resource availability considerations. The user prompt provides the current game state with detailed information on resources, supply, buildings, units, and ongoing unit production. Given this prompt, the agent infers the appropriate Protoss-specific actions to optimize the gameplay strategy against Zerg opponent, focusing on resource management, army composition, and tech progression to counter the enemy effectively. K.3 Evaluation Metric for StarCraft II Single-Agent Play. In the single-player mode, the agent competes in series of matches against the AI bot opponent. It plays up to 4 matches, continuing until it either wins or loses. The evaluation metric is the win rate, calculated as: Score = Number of Wins Total Matches Played Multi-Agent Play. In the multi-player mode, we do not use win rate as the performance metric, since only single run is conducted per match. Instead, we measure the performance by the army supply difference between agents at the end of the match. The difference is calculated as the sum of each units count multiplied by its consumed resource cost, reflecting the effective army strength. The winning agent receives positive score equal to this army supply difference, while the losing agent is assigned the negative of this value. This scoring method captures not only victory but also the margin of the win. 42 Figure 20: Action inference prompt for zero-shot agent playing StarCraft II. Using these difference-based scores, we then compute Elo ratings for all agents following the Bradley-Terry model [76], following the approach used in StreetFighter III multi-agent evaluation in Section C.3. The resulting army supply difference matrix and Elo scores are presented in Figure 4b. K.4 Experimental Configuration for StarCraft II For all LLMs, we use temperature of 0.1 and repetition penalty of 1.0. During LLM inference, we pause the StarCraft II game environment. Interactions with the game environment are limited to maximum of 1,000 steps. For single-agent play, we repeat the experiments 4 times and report the average score along with the standard deviation. For multi-agent play, we run single experiment and report the Elo score. K.5 Result for StarCraft II Single-Agent Play. We evaluate agent performances in single-agent environment, all operating in ref-plan setting. Table 35 shows significant performance gap between open-source LLMs and proprietary LLMs. Among them, Llama-3.2-1B/3B models mostly repeat simple actions like scouting and mining minerals, without showing much strategic planning. On the other hand, proprietary LLMs, except for o3-mini, achieve over 50% win rate. Notably, GPT-4o and Gemini-2.5-pro won all four matches against the AI bot. They demonstrate strategic behavior by appropriately allocating resources over time in line with the games progression. Table 36 presents ablation studies on agentic modules. As previously mentioned, Llama-3.2-3B models fail to manage matches effectively regardless of ablation settings, resulting in 0% win rate. In contrast, GPT-4o demonstrates remarkable planning abilities, which are critical in StarCraft II given its real-time strategy nature. In other words, long-term planning to sustain strategies over time plays 43 Models StarCraft II Rank Models Agent StarCraft II Rank Models Input StarCraft II Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 75.050.0 100.00.0 25.050.0 100.00.0 50.057.7 50.057.7 9.5 9.5 9.5 9.5 9.5 9. 3 1.5 6 1.5 4.5 4.5 Table 35: Gameplay score on StarCraft II. Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 0.00.0 0.00.0 0.00.0 0.00.0 50.057.7 0.00.0 50.057.7 100.00.0 6 6 6 6 2.5 6 2.5 1 Table 36: Ablation study for agentic modules on StarCraft II. GPT-4o Gemini Claude Text Both Text Both Text Both 100.00.0 50.057. 100.00.0 100.00.0 50.057.7 50.057.7 2 5 2 2 5 5 Table 37: Comparison across modalities on StarCraft II. pivotal role in securing victories. Interestingly, models relying solely on reflection perform worse than zero-shot, suggesting that reflection without proper planning may actually degrade performance. Table 37 presents the results of modality experiments. Surprisingly, for GPT-4o, using both text and image inputs results in decreased performance, and Gemini and Claude also show no improvement. This implies that agents can make sufficiently accurate decisions based on textual observations alone, and the addition of image input may introduce challenges in multimodal reasoning. Multi-Agent Play. We evaluate agent performances in multi-agent setting by conducting pairwise matches among seven LLMs (excluding Gemini-2.5-pro), all operating under ref-plan configuration. The results are summarized in Figure 4(b). To ensure fair comparison, both competing agents consistently use the Protoss race in every match. Interestingly, unlike the single-agent evaluation where Claude achieved only 50% win rate  (Table 35)  , Claude outperforms GPT-4o and attains the highest Elo rating in the multi-agent arena. Even more surprising is that Minitron-8b, which had 0% win rate in single-agent play, defeated GPT-4o, o3-mini, and GPT-4o-mini, earning the second highest Elo rating. This discrepancy suggests that the presence of multiple intelligent agents can significantly alter game dynamics, potentially due to increased strategic diversity or emergent adversarial behaviors. We also acknowledge that single evaluation episode may have introduced some bias in the observed rankings."
        },
        {
            "title": "L Slay the Spire",
            "content": "L.1 Game Description for Slay the Spire Environment. Slay the Spire is deck-building rogue-like game where the player ascends procedurally generated three-act tower. Each act consists of branching map with various room types such as combat encounters, shops, treasure rooms, rest sites, and random events, ending in boss fight. Our goal is to evaluate an LLM agents ability to reason over strategic choices in stochastic, multi-step environment. Specifically, we task the agent with playing as the Ironclad character under standard rules (no ascension levels) and aim to defeat the final boss at floor 50. The LLM agent is responsible for two key decision types: (1) choosing which cards to play during combat, and (2) selecting rewards after battles. All other game decisions, such as map navigation, non-combat events, and potion usages, are handled by simple rule-based policy. The game runs on the Steam platform, and we use modding tools to enable communication between the game and an external agent. In particular, we use BaseMod [86] and ModTheSpire [87], along with modified version of CommunicationMod [88], which enables state extraction and action input via standard input/output streams. Our modified version also extracts detailed in-game information, including full card and relic descriptions. To ensure consistency and reproducibility, we fix the game seed for all runs. This guarantees the same map layout, encounters, and card offerings. Additionally, since the original game unlocks card pools progressively as the player completes runs, we pre-unlock all cards using the Unlock Everything mod to make the full pool available from the start. Observation-to-Text Conversion. After each action, we extract JSON representation of the current game state, including player status, opponent status, current cards in hand, and relics. These states are serialized into concise natural language summary that is passed to the LLM. Action Space. Our environment defines two categories of actions: combat and card selection. During combat, the agent can either play card (PLAY) or end its turn (END). In the card selection stage, the agent can choose card reward (CHOOSE) or skip the reward (SKIP). PLAY CARD_INDEX: Play non-target card from the hand at position CARD_INDEX. PLAY CARD_INDEX TARGET_INDEX: Play targeted card from the hand at position CARD_INDEX, targeting opponent at TARGET_INDEX. END: End the current turn. CHOOSE CARD_INDEX: Select the card reward at position CARD_INDEX. SKIP: Skip the card reward. L.2 Gameplay Prompt for Slay the Spire Figure 21: Action inference prompt for zero-shot agent playing Slay the Spire. 45 Models SlaySpire Rank Models Agent SlaySpire Rank Models Input SlaySpire Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 0.00.0 0.00.0 0.00.0 5.00.0 0.00.0 0.00.0 3.32.9 23.622.1 15.00.0 51.931.9 15.00.0 24.917.1 10 10 10 6 10 7 3 4.5 1 4.5 2 Table 38: Gameplay score on Slay the Spire. Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 0.00.0 0.00.0 0.00.0 0.00.0 12.34.6 26.219.4 23.214.2 23.622.1 6.5 6.5 6.5 6.5 4 1 3 2 Table 39: Ablation study for agentic modules on Slay the Spire. GPT-4o Gemini Claude Text Both Text Both Text Both 23.622.1 23.622. 51.931.9 26.219.4 15.00.0 9.74.6 3.5 3.5 1 2 5 6 Table 40: Comparison across modalities on Slay the Spire. Figure 21 presents the action inference prompt utilized by the zero-shot agent to play Slay the Spire. The system prompt describes the agents role as strategic player in the game and outlines key game mechanics, including block, energy, card draw, and enemy intents. Detailed game states, such as player status, relic, card, and opponents are provided in the user prompt. L.3 Evaluation Metric for Slay the Spire The primary objective is to reach and defeat the final boss located on floor 50. Accordingly, the baseline score is determined by the highest floor cleared. For example, if the character dies on floor 43, the score would be 42. Since each floor has varying difficulty levels, and bossesappearing at the end of each actpose significant challenges, we assign bonus points to boss defeats. There are three major bosses and we grant an additional 50/3 points for each boss defeated. Formally, the total score is defined as: Score = (# of Cleared Floors) + 50 3 (# of Bosses Defeated) (1) L.4 Experimental Configuration for Slay the Spire For all LLMs, we use temperature of 0.0 and repetition penalty of 1. Interaction with the game environment is limited to maximum of 200 steps. The game is not paused during LLM inference, as the game state does not change over time. All experiments carried out in three runs. L.5 Result for Slay the Spire Table 38 shows comparison in gameplay performance between open-source LLMs and proprietary LLMs in Slay the Spire. Most open-sourced models fail to make meaningful progress in the game, resulting in near-zero scores. In contrast, API-based LLMs demonstrate significantly stronger performance: Gemini-2.5-pro achieves the highest score, defeating the second boss in two out of three runs. The inherent stochasticity of the game contributes to high variance across all proprietary models. Table 39 presents an ablation study evaluating the impact of agentic modules, reflection and planning, on performance in Slay the Spire. For the weaker model (Llama-3.2-3B), none of the agent variants achieve any meaningful progress, indicating that architectural changes alone are insufficient without strong base capabilities. In contrast, GPT-4o benefits substantially from added reasoning modules: the reflection variant achieves the highest score, while both planning and reflection-planning agents also outperform the zero-shot baseline. Notably, the planning module is relatively less effective, which may stem from the fact that optimal actions are highly sensitive to the opponents intent, information that is only partially observable and difficult to predict multiple turns ahead. These results highlight that while base model capacity is prerequisite, structured reasoning routines further enhance gameplay performance in complex decision-making environments. Table 40 compares model performance in Slay the Spire when using either text-only input or combination of text and vision inputs. In all three proprietary models, adding visual input does not improve performanceand in fact, often leads to degradation. This outcome is not entirely surprising, as crucial gameplay information, such as detailed card effects, relic descriptions, and 46 power mechanics, is often absent in the game screenshot. As result, the image input fails to provide meaningful utility and instead introduces ambiguity or redundancy, effectively acting as noise rather than useful context. These findings suggest that for structured, information-dense environments like Slay the Spire, high-quality textual representations remain the most reliable modality for LLM agents."
        },
        {
            "title": "M Baba Is You",
            "content": "M.1 Game Description for Baba Is You Environment. Baba Is You is puzzle game in which players must discover and understand every rule and mechanic on their own, apart from the basic movement keys (left, right, up, and down) [64]. The games defining feature is that the text tiles forming the rules can be pushed around, allowing the player to rewrite those rules on the fly. Every valid rule sentence must contain verb (e.g., Is and Has), and text tiles with colored background (e.g., You, Push and Win) cannot serve as subjects. level is cleared when the object designated by You touches the object designated by Win. For instance, with the rules Baba Is You and Flag Is Win, the player wins as soon as Baba touches the flag. For implementation, we instrument the Steam edition with lightweight Lua mod that, after every player move, dumps the full internal game state, i.e., the coordinates of every object, to JSON file. The Lua modding script uses mod hook functions provided by the game developer, which are triggered at specific points in the games code. The agent outputs are delivered to the game via simulated key presses using the pyautogui library. We use Level 1 - Where do go?, shown in Figure 22, as our default evaluation setting (for Table 3). Figure 22: Level 1 of Baba Is You. Observation-to-Text Conversion. We present the LLM with textual description of the game state in two parts. First, we list the current (x, y) coordinates of every object in the level. Second, we manually parse the state to extract all active rules and append those rules to the prompt. The combined description forms the models observation. Action Space. In the 2D grid environment, the agent can move left, right, up, or down. At each step, the agent outputs finite sequence of these directions, which we translate into consecutive key presses and send to the game. M.2 Gameplay Prompt for Baba Is You Figure 23 shows the action inference prompt used by the zero-shot agent for playing Baba Is You. The system prompt encodes key game-specific knowledge, including (1) the primary objective of touching Win object with controllable (You) object, (2) an explanation of how rules are formed and manipulated via text block arrangements, (3) crucial pushing mechanics with coordinate-based guidance to avoid unintended interactions, and (4) the expected input-output format for the LLM. The user prompt provides the current puzzle state, including the map dimensions, object locations, and active rules. Given this prompt, the LLM agent outputs sequence of directional actions. M.3 Evaluation Metric for Baba Is You To evaluate the performance of an LLM agent, we define hierarchical scoring metric that rewards meaningful progress toward solving the puzzle. As shown in Figure 22, key prerequisite is breaking the rule Wall Is Stop, which enables movement out of the closed area. The second subgoal is creating winning condition, such as forming the rule Flag Is Win, but this is only possible once the wall constraint is removed. Each subgoal provides 20 points. If the agent clears the level by having the You object touch Win object, it receives full score of 100, overriding subgoal rewards. The 47 Figure 23: Action inference prompt for zero-shot agent playing Baba Is You. final score is computed as: Score = 100, 40, 20, if level is cleared if Wall Is Stop is broken and Win rule is created if Wall Is Stop is broken only M.4 Experimental Configuration for Baba Is You For all 6 open-source LLMs, we use temperature of 0.7 and repetition penalty of 1, and set the maximum number of game steps to 30. We run all experiments with 3 trials and report the average score with the standard deviation. M.5 Result for Baba Is You The performance of different models on Level 1 of Baba Is You using the reflection-planning agent is shown in Table 41. We observe that o3-mini and Gemini-2.5-pro achieve the highest score of 73.3, significantly outperforming all other models. Apart from these two reasoning models, only Claude-3.7-sonnet, hybrid reasoning model, scores above 20.0, indicating that it is the only other model capable of constructing valid winning condition. In contrast, all non-reasoning models, including every open-source small language model we tested, typically only managed to break the Wall Is Stop rule, consistently earning score of 20.0. However, qualitative analysis of the models self-defined subtasks and reasoning traces suggests that these models often fail to infer that breaking 48 Models BabaIsYou Rank Models Agent BabaIsYou Rank Models Input BabaIsYou Rank Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini Gemini-2.5-pro Claude-3.7 Deepseek-R1 6.711.5 20.00.0 13.311.5 20.00.0 20.00.0 20.00.0 13.311.5 20.00.0 73.346.2 73.346.2 46.746.2 20.00.0 12 6.5 10.5 6.5 6.5 6.5 10.5 6.5 1.5 1.5 3 6. Table 41: Gameplay score on Baba Is You. Llama-3B GPT-4o Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 20.00.0 20.00.0 20.00.0 20.00. 20.00.0 20.00.0 20.00.0 20.00.0 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 Table 42: Ablation study for agentic modules on Baba Is You. GPT-4o Gemini Claude Text Image Both Text Image Both Text Image Both 20.00.0 6.713.7 20.00.0 73.346.2 20.00.0 86.723. 46.746.2 20.00.0 20.00.0 6 9 6 2 6 1 3 6 6 Table 43: Comparison across modalities on Baba Is You. Wall Is Stop is necessary prerequisite for constructing the winning condition. This implies that their success in breaking the rule was largely unintentional. Table 42 presents an ablation study evaluating the impact of agentic modules on performance in Baba Is You. Across both Llama-3.2-3B and GPT-4o, we observe no measurable improvement over the zero-shot baseline, with all configurations achieving the same score of 20.0. This indicates that the agentic components do not significantly enhance the agents ability to reach the winning condition for these two models. The task remains challenging, largely due to the models limited spatial reasoning capabilities. While the models occasionally produce valid high-level plans, such as to form the rule Flag Is Win at specific coordinates, they frequently fail to account for the games pushing mechanics. As result, they often push text tiles in unintended directions, breaking or misaligning the intended rule formation. Table 43 presents an ablation study on input modalities across several multimodal models. We observe that relying solely on image input significantly degrades performance for all models. Adding image input on top of text yields only marginal improvements, if any, suggesting that the agents primarily rely on text-based representations to make decisions. Notably, Gemini-2.5-pro benefits slightly from the combined input, achieving the highest score of 86.7. 2048 N.1 Game Description for 2048 Environment. 2048 [65] is single-player sliding tile puzzle game played on 44 grid. The objective is to combine numbered tiles by sliding them in one of four directions (i.e., up, down, left, or right) to create tile with the value 2048. In this environment, the agent observes the current board state, represented as 44 matrix of integers (each cell contains 0 for empty or power of 2 for active tiles), and selects one of four discrete actions corresponding to directional moves. While the original game ends when no moves are available (i.e., the board is full and no adjacent tiles can be merged), we additionally terminate the episode if the agent performs five consecutive invalid moves (i.e., actions that result in no change to the board state). For implementation, we use an open-source, Pygame-based game environment. The logic module manages the board state, tile movements, merging, and win/loss conditions, while the interface leverages Pygame to render the board and handle user input. The implementation supports dynamic resizing and configurable parameters, and is designed to facilitate both human play and automated experiments. Observation-to-Text Conversion. The environments board state can be directly transformed into textual description, formatting it as 44 array of integers in which each element indicates the value of the corresponding tile. Action Space. The action space comprises four discrete actions: up, down, left, and right, each representing possible direction in which the agent can slide the tiles on the board. N.2 Gameplay Prompt for 2048 Figure 24 shows the action inference prompt and the corresponding game screenshot used by the zero-shot agent to play 2048. The system prompt includes (1) the main objective of the game, (2) 49 Figure 24: Action inference prompt for zero-shot agent playing 2048. detailed game rules, and (3) the expected input-output format between the LLM and the environment. The user prompt provides (1) the specific task for the 2048 game, (2) the previous board state and game score, (3) the last executed action, (4) the current board state and game score, and (5) the expected output format. Based on this information, the agent determines the next action to take. N.3 Evaluation Metric for 2048 The goal of 2048 is to create tile with the value 2048. The game score increases as tiles are merged, with the value of the merged tile added to the total score. Although the score when the 2048 tile is created can slightly vary depending on the gameplay, it is generally estimated that the score is around 20,000 points. Therefore, we define the evaluation metric as the progress to the target score of 20,000, normalized to 100. Formally, the normalized score is defined as: Score = min (cid:0) Final Game Score 100, 100(cid:1). 20,000 where Final Game Score denotes the total score at the end of the game. This metric reflects how close the agent came to achieving the primary objective of creating the 2048 tile. N.4 Experimental Configuration for For all 6 open-source LLMs, including Llama-3.2-1B/3B, Qwen-2.5-3B/7B, and Minitron-4B/8B, we use temperature of 0.0 and repetition penalty of 1.0. We set the maximum number of game steps to 10,000. However, the maximum number of game steps (10,000) was never reached in the experiments. The game typically terminated either when the board was full and no adjacent tiles could be merged, or when the agent failed to take an action that changed the board state for more 50 Models 2048 Rank Models Agent 2048 Rank Models Input Rank Random Agent Llama-3.2-1B Llama-3.2-3B Qwen-2.5-3B Qwen-2.5-7B Minitron-4B Minitron-8B GPT-4o-mini GPT-4o o3-mini o4-mini o3 Gemini-2.5-pro Claude-3.7 Deepseek-R1 5.52.3 0.00.1 0.30.2 0.10.1 0.60.4 0.10.0 0.70. 1.11.0 5.61.5 25.37.3 15.76.7 34.923.4 5.12.5 5.32.7 11.53.4 6 15 12 13 11 14 10 9 5 2 3 1 8 7 4 Table 44: Gameplay score on 2048. Random Agent - Zero-shot Reflection Planning Ref-Plan Zero-shot Reflection Planning Ref-Plan 5.52.3 0.30.2 0.00.0 0.10.1 0.10.2 5.61.5 3.52.9 6.05.5 7.05. Zero-shot Ref-Plan 25.37.3 17.69.5 6 8 11 10 9 5 7 4 3 1 Random Agent - GPT-4o Gemini Claude Text Image Both Text Image Both Text Image Both 5.52.3 5.61.5 1.81.1 5.44.5 5.12.5 5.52.4 3.12.6 5.32.7 8.44.0 6.70. 5 3 10 6 8 4 9 7 1 2 Llama-3B GPT-4o o3-mini Table 45: Ablation study for agentic modules on 2048. Table 46: Comparison across modalities on 2048. than five consecutive steps. In the best gameplay episode of o3 zero-shot agent, which achieved score of 57.32, total of 685 steps were taken to reach this result. N.5 Result for 2048 All reported results in Tables 4446 are computed as the mean and standard deviation over five independent runs, using zero-shot agent for each model configuration. To establish baseline for comparison, we additionally included random agent that selects one of the four possible actions (up, down, left, right) uniformly at random. This agent was evaluated over 50 episodes, and its performance is summarized in Tables. Gameplay score on 2048. Beyond the 12 models presented in the main paper, we also include results for OpenAIs more recent models, o4-mini and o3 in Table 44. Interestingly, none of the open-source models (e.g., Llama, Qwen, Minitron) were able to correctly interpret the 2D array prompt representing the game board. Consequently, they repeatedly issued the same action even when no tiles could be merged, leading to premature termination of the game. In contrast, GPT-4o, Gemini-2.5-pro, and Claude-3.7 were able to detect invalid moves and avoid them to some extent. However, these models failed to manage merged tiles into their planning, often repeating superficially valid actions that quickly led to board lock-up. As result, their gameplay performance was almost indistinguishable from that of the random agent. Notably, the reasoning-capable modelso3-mini, o4-mini, o3, and DeepSeek-R1exhibited meaningful gameplay performance. While the open-source and non-reasoning models were typically limited to producing maximum tile of 64 or 128, o3-mini successfully generated the 512 tile in 4 out of 5 runs, and o3 achieved the 1024 tile in 2 out of 5 runs. particularly interesting observation is that, even without explicit strategic instructions (e.g., cornering high-value tiles or arranging tiles in staircase pattern), the reasoning-based models implicitly discovered human-like strategies based solely on the basic game rules provided via system prompts. This behavior is illustrated in Figure 25, where o3 exhibits an emergent form of spatial organization akin to that used by experienced human players. Ablation study for agentic modules on 2048. In addition to the models evaluated in the main paper, we conducted an extended ablation study using the o3-mini model to assess the impact of agentic modulesReflection and Planningon gameplay performance in the 2048 environment. Table 45 presents the results of this study across three representative models: Llama-3.2-3B, GPT-4o, and o3-mini, under various agent configurations. For Llama-3.2-3B and GPT-4o, enabling the Reflection or Planning modulesindividually or togetherdid not yield significant gains, suggesting that external agentic scaffolding alone is insufficient for improving performance, likely due to limited reasoning or architectural constraints. In contrast, o3-mini exhibited counter-intuitive trend: the zero-shot agent significantly outperformed both the Reflection and the Reflection-Planning agents. plausible explanation for this phenomenon is that reasoning-centric models such as o3-mini may already perform internal reflection and planning 51 Figure 25: Three different runs for o3 zero-shot agent playing 2048. Figure 26: Output of OpenAIs o3 model right before producing the 1024 tile in 2048. when generating response. As such, the introduction of additional agentic modules may introduce redundancy or even propagate errors in intermediate reasoning steps, degrading performance. Supporting this hypothesis, Figure 26 shows the inference output of o3 shortly before achieving 1024-tile. The output reveals that the model implicitly forecasts and sequences the next three actions, essentially performing internal planning without explicit agent modules. This emergent behavior highlights the potential of foundation models with strong reasoning capabilities to handle multi-step decision problems autonomously without external agentic augmentation. Comparison across modalities on 2048. To assess the impact of input modality on gameplay performance, we evaluated models using either textual or visual (image-based) representations of the 2048 board state. The results are summarized in Table 46. Interestingly, GPT-4o demonstrated weaker performance with image inputs compared to text, suggesting that its image understanding capabilitiesat least within the structured context of 2048may lag behind its text comprehension. This is consistent with prior findings that GPT-4o, while multimodal, exhibits varying levels of alignment across modalities depending on task complexity and structure. In contrast, both Gemini 2.5 Pro and Claude 3.7 achieved better performance with image-based inputs than with text. This indicates stronger visual reasoning capability in these models, particularly when parsing structured 2D spatial layouts such as the 2048 board. Their ability to interpret and act on visual patterns appears 52 Figure 27: Example of original system prompt and augmented system prompt in Slay the Spire. to be more robust than their capacity to process raw 2D arrays expressed as textual input.These findings highlight that multimodal models exhibit non-uniform modality strengths, and task-specific evaluations are crucial for selecting the appropriate input format to maximize agent performance."
        },
        {
            "title": "O Details for Data Augmentation",
            "content": "To increase the diversity of our training dataset and mitigate overfitting risks, we applied data augmentation strategy focused on the system prompt. The original dataset was constructed by rolling out the same game prompt multiple times in the environment to collect assistant responses. The system prompt remained static, and the user prompt exhibited some variation due to changes in game state, though these were constrained to fixed format where only specific values changed. To address this issue, we performed augmentation on the system prompt, which contains general information such as the LLMs role, game rules, and behavioral guidelines. Since the user prompt was dependent on dynamic game states, modifying it risked introducing inconsistencies or hallucinations. Therefore, we chose to keep the user prompt fixed during augmentation. We use GPT-4o to generate paraphrased versions of the original system prompt. Specifically, we prompted the model to produce 10 alternative phrasings of the original system prompt that preserved its semantics while varying its linguistic expression. Each paraphrased system prompt was then paired with the original user prompt and assistant response, resulting in 10 augmented versions of each original data point. Including the original, this expands the dataset by factor of 11. Figure 27 presents an example of the original and augmented prompt in Slay the Spire. This augmentation strategy helped increase the syntactic and lexical diversity of the dataset while preserving semantic fidelity and coherence, thereby supporting more robust fine-tuning of the LLM."
        },
        {
            "title": "P Implementation Details",
            "content": "P.1 Asynchronous Inferences for Multi-Agent Environment Here we provide implementation details for multi-agent game environment that enables efficient, scalable, and realistic agent interaction. Overview. The core design principle is to execute the game loop on frame-by-frame basis while allowing each agent to asynchronously infer and initiate its next high-level action upon completion of the previous one. This approach is particularly well-suited for real-time, frame-based games such as Street Fighter III and StarCraft II, where individual actions may span variable number of frames (e.g., Move Away action takes 4 frames, whereas Super Attack requires 7 frames to complete). In this system, each agent operates independently, without waiting for other agents to complete their actions. As soon as an agent completes its current action, it observes the current game state and determines its next high-level action. This asynchronous execution allows the game to progress fluidly and continuously, closely mimicking the dynamics of real-time multiplayer games. Illustrative Example. Consider multi-agent scenario in Street Fighter III. Agent 1 selects Super Attack that spans 7 frames, while Agent 2 chooses Move Away action that lasts 4 frames. The 53 game orchestrator initiates both actions simultaneously. After 4 frames, Player 2s action concludes, triggering the agent to observe the updated state and select new action, e.g., Medium Punch lasting 2 frames. The orchestrator then integrates this new action into the ongoing simulation, even as Agent 1s \"Super Attack\" continues. This frame-by-frame orchestration proceeds iteratively. Each agent re-enters the decision-making process immediately upon completing its current action, independent of the progress or status of other agents. Advantages. This asynchronous, non-blocking scheduling model provides several key advantages. It enables agents to operate with varying action durations without artificial synchronization barriers, facilitating more natural and responsive interaction dynamic. The resulting system supports overlapping actions, better reflects the timing complexities of real-time games, and can be readily extended to support environments with more than two agents. P.2 Fine-tuning We conducted supervised fine-tuning using collected gameplay data to adapt the LLM agent to game-specific reasoning and interactions with environments. Training Configuration. We fine-tune two models: Llama-3.2-1B and Llama-3.2-3B. Training is conducted using 4 NVIDIA A100 GPUs with 80GB of memory each. We use learning rate of 1e-6, per-device batch size of 4 with gradient accumulation steps set to 4, resulting in an effective batch size of 64. The models were trained for 1 epoch with 100 warm-up steps. Data Statistics. In total, we use 105,502 data points for fine-tuning. Only data points containing fewer than 4,096 tokens are used for training to ensure compatibility with model input length limitations. All data points from Pokémon and 329 out of 9900 data points from Minecraft are discarded due to length constraint. For the out-of-distribution (OOD) game generalization experiments, we exclude the games 2048 and Super Mario, resulting in 87,660 data points used. Apart from the number of data points, the training configuration remained identical. P.3 Unseen Scenarios To evaluate the intra-game generalization capability of fine-tuned LLM agents, we define separate scenario that is not used during the training dataset collection. Unseen scenarios differs from the seen ones by featuring different character, map, or stage. Figure 28 presents example screenshots of the seen and unseen scenarios across six games. Street Fighter III. While the character Ken was used during training data collection, different character, Chun-Li, was introduced at evaluation time to assess intra-game generalization. Chun-Li features distinct set of moves and hitboxes, posing significantly different control and tactical challenge compared to Ken. The game environment and match settings remained unchanged. Super Mario. For Supermario, we used Stage 3-1 as an unseen scenario, while we used Stage 1-1 for both the main evaluation and fine-tuning dataset. Unlike Stage 1-1, Stage 3-1 features different obstacle placements and introduces flying monster Koopas, making it more challenging task. The game mechanics and control scheme remained unchanged. Darkest Dungeon. While the first expedition used during training data collection featured party composed of Plague Doctor, Vestal, Highwayman, and Crusader, the unseen scenario involved different party composition: one Man-At-Arms, two Grave Robbers, and one Vestal. This change introduced significantly different combat dynamics, synergies, and positional requirements. Additionally, the enemy pool in this dungeon included the Madmana monster not encountered during trainingknown for its stress-inducing attacks and erratic behavior. All other gameplay parameters, including the dungeon type (short dungeon in Ruins area), remained unchanged. StarCraft II. During training, agents were exposed to the Ancient Cistern LE map, while evaluation was conducted on different map, Babylon LE to assess intra-game generalization. All other game settings, including player race (Protoss), opponent race (Zerg), opponents build order strategy (Timing), and difficulty level (Hard), were kept constant. Slay the Spire. We used game runs with different random seeds as unseen scenarios. Each game seed determines the layout of the map, including the sequence of opponents, bosses, events, and card 54 reward options. All other game parameters, such as character choice (IronClad) and starting deck, were held constant. Baba Is You. In the case of Baba Is You, the second stage, Level 2 - Now what is this?, is used as an unseen scenario. This stage features new rule combinations and object interactions that are absent from the training set, thereby testing the agents ability to generalize to novel logic structures. The game mechanics and control scheme remained unchanged."
        },
        {
            "title": "Q Limitations and Future Work",
            "content": "Cost Consideration. Among all games in Orak, six games, i.e., Ace Attorney, Her Story, Darkest Dungeon, Stardew Valley, Slay the Spire, and Baba is You, require one-time purchase, typically priced ranging from $9.99 to $24.99. While this represents non-negligible upfront cost, it is relatively minor compared to the recurring cost associated with proprietary LLM API calls. From cost-efficiency perspective, the benchmark remains accessible and practical for sustained research. Real-time Gameplay. Street Fighter III, Super Mario, and StarCraft II inherently require real-time gaming, unlike other turn-based or simulation games in Orak. However, in our current evaluation setup, the game is paused during LLM inference to remove the impact of real-time constraints on agent performance. While this allows for more stable evaluation of reasoning capabilities, real-time responsiveness is critical in many gaming contexts, so it should be handled for practical needs. We leave latency-aware evaluation protocols for building real-time gaming LLM agents as future work. Study on RL-based Fine-tuning. Although RL-based fine-tuning has demonstrated strong performance in many domains, such as mathematics and programming, we did not explore it in this study. Given the interactive nature of the Orak environments, it would be natural to derive dynamic, context-aware rewards from in-game feedback and apply RL fine-tuning methods such as DPO [89] or GRPO [90]. Unlike domains such as mathematics or programming, where problems typically have static correct solutions derived through logical reasoning, gameplay requires strategic reasoning that adapts dynamically to the actions of other agents. In games, the optimal action is often contingent on the evolving behavior of the player or opponents, reflecting the complex, interactive nature of multi-agent environments. This distinction is particularly relevant to real-world domains such as business, economics, and negotiation, where strategic decision-making is frequently modeled using game-theoretic frameworks [91]. Therefore, leveraging Orak as an environment for RL-based finetuning may significantly enhance an LLMs capacity to reason strategically and operate effectively in multi-agent settings. This line of research holds promise for improving LLM performance in broad range of real-world applications where understanding multi-agent dynamics is essential. Support for Diverse Modalities. Orak supports evaluation of LLMs and VLMs, but it does not extend to other modalities often essential in real-world gameplay. One key example is sound. For instance, in Minecraft, players rely on zombie audio cues to avoid danger, while in Street Fighter III and StarCraft II, alert sounds indicate some specific attacks by the enemy. Also, beyond games in Orak, First-Person Shooter (FPS) games usually use gunfire sounds for spatial awareness, and horror games often rely on audio to signal the proximity of threats. Consequently, incorporating other modalities such as audio remains an open challenge, and benchmarking the performance of emerging Speech-Language Models [92, 93] in gaming scenarios could be an important step toward broader multimodal agent development."
        },
        {
            "title": "R Broader Societal Impacts",
            "content": "Simulation games such as Minecraft and Stardew Valley offer rich environments where players can explore, mine resources, and craft items, enabling life-like simulations of human behavior. These games offer valuable testbed for analyzing long-horizon behavior of LLM agents by systematically comparing gameplay trajectories of humans and LLM agents, which enables rigorous assessment of whether LLMs exhibit human-like decision-making patterns. Moreover, introducing multiple agents into these environments allows for the study of emergent social behaviors among LLM agents. We believe such settings are particularly well-suited for precisely measuring the social impact of complex agent behaviors, offering valuable insights into the dynamics of LLM-based agents. 55 (a) Street Fighter III (b) Super Mario (c) Darkest Dungeon (d) Starcraft II (e) Slay the Spire (f) Baba Is You Figure 28: Comparison of seen (left) and unseen (right) scenarios for six games."
        }
    ],
    "affiliations": [
        "KRAFTON",
        "NVIDIA",
        "Seoul National University",
        "University of Wisconsin-Madison"
    ]
}