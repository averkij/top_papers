{
    "paper_title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks",
    "authors": [
        "Xu Zheng",
        "Zihao Dongfang",
        "Lutao Jiang",
        "Boyuan Zheng",
        "Yulong Guo",
        "Zhenquan Zhang",
        "Giuliano Albanese",
        "Runyi Yang",
        "Mengjiao Ma",
        "Zixin Zhang",
        "Chenfei Liao",
        "Dingcheng Zhen",
        "Yuanhuiyi Lyu",
        "Yuqian Fu",
        "Bin Ren",
        "Linfeng Zhang",
        "Danda Pani Paudel",
        "Nicu Sebe",
        "Luc Van Gool",
        "Xuming Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning."
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 1 Multimodal Spatial Reasoning in the Large Model Era: Survey and Benchmarks Xu Zheng1,2, Zihao Dongfang1, Lutao Jiang1, Boyuan Zheng1, Yulong Guo1, Zhenquan Zhang4, Giuliano Albanese2, Runyi Yang2, Mengjiao Ma2, Zixin Zhang1, Chenfei Liao1,5, Dingcheng Zhen, Yuanhuiyi Lyu1, Yuqian Fu2 Bin Ren6,7, Linfeng Zhang5, Danda Paudel2, Nicu Sebe7, Luc Van Gool2, Xuming Hu1,3 4South China University of Technology 5Shanghai Jiao Tong University 6University of Pisa 7University of Trento 1HKUST(GZ) 2INSAIT, Sofia University St. Kliment Ohridski 3HKUST Co-first Author; Core Contributors; Corresponding Author. 5 2 0 2 9 2 ] . [ 1 0 6 7 5 2 . 0 1 5 2 : r Fig. 1: (a) Various multimodal inputs for advanced spatial reasoning with MLLMs, such as 2D images [1], 3D scenes [2] and videos [3]. (b) Downstream tasks base or rely on spatial reasoning, such as VLA [4], 3D layout generation [5], and vision-language action [6]. AbstractHumans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on posttraining techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning. Index TermsSpatial Reasoning, Multimodal Large Language Model, Survey, Benchmark I. INTRODUCTION A. Background Spatial reasoning is fundamental human ability that allows individuals to understand and interact with the world through multimodal inputs, such as vision, sound, and other senses. It supports navigation, comprehension of object relationships, and problem-solving in spatial contexts, as shown in Figure 1. While large language models (LLMs) have made significant strides in text processing and generation [56], their spatial reasoning is limited by their primarily unimodal design [57]. Integrating multimodal informationsuch as images, audio, and videointo language models offers new opportunities to enhance spatial reasoning, particularly for tasks requiring deep understanding of complex real-world scenarios [5864]. Large multimodal reasoning models have emerged as promising solution, as they are trained to perceive and reason across multiple modalities simultaneously [6569]. These models have shown remarkable performance in wide range of spatial tasks, from understanding 2D spatial relationships to more complex 3D reasoning. However, despite these advancements, there remains notable gap in systematically reviewing IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 2 Prompt Engineering Spatial-MM [7], VSI-Bench [8], VoT [9], etc Test-Time Scaling Tool Use SpatialScore [10], SpatialPIN [11], etc General MLLM Post-Training Others VisuoThink [12], Logic-RAG [13], etc SFT RL Multi-SpatialMLLM [14], SpatialVLM [15], etc Video-R1 [16], Spatial-R1 [17], etc Model Design Spatial-MLLM [18], SpatialRGPT [19], Spatial-ORMLLM [20], etc Explainability Beyond Semantics [21], ADAPTVIS [22], RelatiViT [23], etc 3D Visual Grounding Multi-view Input VLM-Grounder [26], 3DAxisPrompt [2], etc 3D Input LLM-Grounder [24], Grounded 3D-LLM [25], etc n e i S o l 3D Vision 3D Scene Reasoning and QA 3D Generation Hybrid of 3D and 2D SeeGround [27], ReasonGrounder [28], etc Training-required LLaVA-3D [29], 3DGraphLLM [30], etc Training-free SpatialPIN [11], Agent3D-Zero [31], etc 3D Layout Generation LayoutGPT [5], Layout-your-3D [32], etc 3DGen as Program 3D-GPT [33], CAD-Recode [34], etc Scene Understanding Spartun3D [35], GSA-VLN [36], etc Vision-Language Navigation Intention Interpretation AutoSpatial [37], LL3DA [38], etc Planning & Navigation NavVLM [39], NavCoT [40], etc Embodied AI Embodied Question Answering OpenEQA [41], EMBOSR [42], etc Embodied Grasping ThinkGrasp [43], FreeGrasp [44], etc Vision-Language Action 3D-VLA [45], Ï€0.5[46], Chat-VLA2 [47], etc Embodied World Model TesserAct [48], EVA [49], etc Novel Modalities Video-based VideoLLaMA2 [50], VideoINSTA [51], Video-R1 [16], SpaceR [3], etc Audio-based STARSS23 [52], SpatialSoundQA [53], ACORN [54], SAVVY [55], etc Fig. 2: Taxonomy for multimodal spatial reasoning with large models. and evaluating the performance of these emerging models, especially in the context of multimodal spatial reasoning. B. Contributions This survey aims to fill that gap by providing comprehensive review of the current state of multimodal spatial reasoning with large models, as shown in Figure 2. We begin by reviewing the general landscape of spatial reasoning, focusing on key aspects such as post-training techniques [15, 16], model explainability [21], and architecture design [19]. Moving beyond traditional 2D tasks [10], we delve into more advanced forms of spatial reasoning, including spatial relationship reasoning [40], scene and layout understanding [5], and grounding visual information in 3D space [28]. Furthermore, this paper also explores the intersection of spatial reasoning and embodied AI tasks [41], including vision-language navigation and action models [45], where models are required to perform tasks in dynamic environments based on multimodal inputs. We extend the discussion to incorporate the use of emerging modalities such as audio and ego-centric video, which offer distinct opportunities for spatial understanding, particularly in novel sensor environments [70, 71]. In addition to reviewing the existing literature, we introduce open benchmarks for evaluating the performance of MLLMs in spatial reasoning tasks. These benchmarks aim to standardize the evaluation of these models and provide reliable foundation for future research. The introduction of these benchmarks will also facilitate comparisons across different models and drive advancements in the field by offering standardized testing protocols. We believe this survey serves as an essential resource for researchers and practitioners in the field of multimodal spatial reasoning, establishing solid foundation for future work in this critical area. Additionally, we provide access to the codes, implementations, and up-to-date information about the open benchmarks at https://github.com/zhengxuJosh/ Awesome-Spatial-Reasnoning, which can help further advance IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 3 Authors Zhou et al. [72] Wang et al. [73] Ke et al. [74] Zha et al. [57] Bi et al. [75] Chen et al. [76] Chen et al. [77] Ali et al. [78] Wang et al. [79] Plaat et al. [80] Qu et al. [81] Lin et al. [82] Sui et al. [83] TABLE I: Recent related survey papers on Reasoning in MLLMs. Venue/Date Main Focus/Analysis Arxiv 2025 (May) RL-based reasoning Arxiv 2025 (Apr) Explores small reasoning models, training, inference, and applications Arxiv 2025 (Apr) Discusses inference scaling, learning-to-reason, and agentic systems in LLMs Arxiv 2025 (Apr) Focuses on enabling LLMs with 3D spatial reasoning capabilities Arxiv 2025 (Apr) Reviews advancements in multimodal reasoning in LLMs Arxiv 2025 (Apr) Investigates scaling challenges and techniques in LLM reasoning Arxiv 2025 (Apr) Discusses long chain-of-thought approaches for enhancing LLM reasoning Arxiv 2025 (Mar) Focuses on mathematical reasoning and optimization tasks within LLMs Arxiv 2025 (Mar) Reviews efficient reasoning techniques for large-scale LLMs Arxiv 2025 (Mar) Explores efficient inference techniques for large reasoning models Arxiv 2025 (Mar) Discusses language and multimodal techniques for efficient reasoning in LLMs Arxiv 2025 (Mar) Focuses on transitioning from language reasoning to multimodal reasoning Arxiv 2025 (Mar) Reviews techniques for reducing inefficiencies in LLM reasoning Wang et al. [84] Arxiv 2025 (Mar) Examines the integration of chain-of-thought reasoning with multimodal LLMs Bandyopadhyay et al. [85] Arxiv 2025 (Mar) Discusses various reasoning strategies implemented in LLMs Li et al. [86] Yan et al. [87] Yang et al. [88] Li et al. [89] Arxiv 2025 (Mar) Focuses on methods to improve causal reasoning abilities in LLMs Arxiv 2025 (Feb) Reviews mathematical reasoning benchmarks and methods in LLMs Arxiv 2025 (Feb) Explores code-enhanced reasoning in LLMs, and reasoning-driven code tasks Arxiv 2025 (Feb) Focuses on cognitive reasoning models and LLMs (System 1 vs System 2) Cheng et al. [90] Arxiv 2025 (Feb) Discusses integrating logical reasoning in LLMs for more structured outputs Srivastava et al. [91] Arxiv 2025 (Feb) Investigates small language models reasoning abilities and improvements Xu et al. [92] Wang et al. [93] Ours Arxiv 2025 (Jan) Focuses on reinforced reasoning techniques for LLMs Arxiv 2024 (Jan) Explores the emerging trends and challenges in multimodal reasoning for LLMs Arxiv 2025 (Aug) Multimodal spatial reasoning in the large model era Link link link link link link link link link link link link link link link link link link link link link link link link link research in this domain. Through this work, we aim to provide valuable insights into the current challenges and future opportunities in multimodal spatial reasoning with large models, encouraging further exploration and development in this rapidly evolving field. 3D visual grounding), incorporate emerging modalities (audio, egocentric video), and present open benchmarks and evaluation protocols absent from prior work. This focused review aims to provide concise foundation for advancing research and practical evaluation in multimodal spatial reasoning. C. Related Works Significant progress has integrated vision, audio, and other modalities with text models, enabling richer spatial reasoning in 2D and 3D. Prior surveys examine related directions but leave gaps relevant to multimodal spatial tasks. For example, Wang et al. [73] study small reasoning models but focus on unimodal, low-complexity tasks; Ke et al. [74] analyze inference scaling and agentic systems without deeply addressing multimodal spatial reasoning; and Zha et al. [57] emphasize 3D capabilities but concentrate on implementation details rather than cross-modal evaluation. Broad reviews such as Bi et al. [75] summarize multimodal advances but do not propose systematic benchmarks or evaluation frameworks for spatial understanding in dynamic, real-world settings. Our survey fills this gap by concentrating on multimodal spatial reasoning in the large-model era. We categorize spatial tasks (e.g., relationship reasoning, scene understanding, II. PROBLEM SETUP: MULTIMODAL SPATIAL REASONING Definition. Multimodal spatial reasoning aims to infer spatial relations, locations, and actions from heterogeneous inputs and to produce verifiable outputs grounded in space. Formally, given inputs = {ximg, xvid, xpc, xaud, xtext, . . .} (e.g., RGB images, videos, point clouds, audio, and language) under specified reference frame (2D/3D/ego/allo), model predicts such as (i) textual answers/rationales, (ii) geometric quantities (boxes, poses, trajectories), or (iii) executable actions/plans for embodied settings. This unifies classic VQAstyle queries, 3D grounding, navigation, and layout/scene generation [19, 35, 37, 94, 95]. A. Types of Spatial Reasoning in MLLMs Spatial reasoning in MLLMs spans basic localization to advanced scene modeling. Key types include: â‘  Localization & IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 4 Category Details 1. Localization: Locate objects in 2D/3D. 2. Relation: Reason about spatial relations. 3. Navigation: Plan paths and optimize actions. 4. Pattern: Detect patterns/symmetries. 5. Scaling: Resize while preserving proportions. 6. Transformation: Apply spatial changes. 7. Context: Interpret positions in context. 8. 3D Generation: Synthesize 3D scenes. 9. Modeling: Build scene models for predictions. 10. Interaction: Support real-time spatial interaction. 1. Multimodal Integration: Test modality combinations. 2. Task Coverage: VQA, 3D localization, navigation. 3. Transparency: Trace decisions with maps or probes. 4. Generalization: Test adaptability in new environments. 5. Embodied Testing: Measure real-time performance. 6. Benchmarking: Provide reproducible tasks. Types Eval Roadmap 1. 2D Tasks: Spatial reasoning in images/videos. 2. 3D Reasoning: Grounding, QA, navigation. 3. Embodied Reasoning: Navigation and world models. 4. Novel Modalities: Cross-domain spatial reasoning. TABLE II: Overview of Spatial Reasoning in MLLMs: Types, Evaluation Protocols, and Roadmap Memory: Locate objects in 2D/3D relative to others/observer and track their states over time. â‘¡ Relation & Geometry: Reason about spatial relations (above/below/left/right) and metrics (distance, angle, area, volume). â‘¢ Navigation & Problem Solving: Plan paths and optimize actions (e.g., shortest routes, spatial puzzles). â‘£ Pattern & Perspective: Detect patterns/symmetries and reason across viewpoints. â‘¤ Scaling & Resizing: Model size changes while preserving proportions. â‘¥ Transformation: Apply rotation, translation, and scaling while maintaining relationships. â‘¦ Contextualization: Interpret positions under environmental context (e.g., room vs. spacecraft). â‘§ 3D Model Generation: Synthesize 3D shapes/scenes from spatial cues. â‘¨ Environmental Modeling: Build scene/world models for prediction and decision making. â‘© Sensing & Interaction: Support real-time spatial interaction (e.g., AR) via sensors/vision. These abilities underpin applications from navigation to simulation and interactive systems. B. Evaluation Protocols for Spatial Reasoning Evaluating MLLMs spatial reasoning should probe acinterpretability, and generalization. Key curacy, robustness, dimensions: â‘  Multimodal Integration: Test diverse modality combos (images, text, audio, depth/point clouds, sensors) to assess cross-modal fusion beyond unimodal cues. â‘¡ Task Coverage: Include VQA, 3D localization, map-based navigation, embodied planning, and scene/image generation to span lowand high-level reasoning. â‘¢ Process Transparency: Trace decisions via attention maps, intermediate states, or rationale probes to reveal how spatial relations are encoded/manipulated. â‘£ Generalization & Robustness: Evaluate out-of-distribution settings (novel layouts, unseen environments, perturbations) to test adaptability. â‘¤ Interactive/Embodied Testing: Measure real-time performance for navigation/manipulation and AR/VR, including responsiveness and online updates. â‘¥ Benchmark Standardization: Provide Fig. 3: Typical MLLM architecture and strategies. reproducible suites spanning controlled synthetic tasks and real-world scenarios. Addressing these facets enables comprehensive, comparable assessment of MLLMs spatial reasoning and clarifies strengths/weaknesses across applications. Roadmap. We next instantiate this setup across application strata: (1) general 2D image/video tasks with MLLMs, (2) 3D spatial reasoning (grounding, QA, navigation), and (2) embodied spatial reasoning (VLN, VLA, world model), and (3) novel modalities & cross-domain settings. Each section maps back to the taxonomy above and adopts the evaluation dimensions outlined here. III. GENERAL MULTIMODAL SPATIAL REASONING General multimodal spatial reasoning refers to MLLMs ability to understand and reason about spatial relationships across visual and textual inputs. It encompasses tasks such as visual question answering (VQA) on spatial relations, object localization, perspective understanding, 3D comprehension, and navigation. These tasks require aligning visual perception with linguistic expressions of spatial concepts like above, behind, and to the left of. As shown in Figure 3, current research enhances spatial reasoning in multimodal models along four main directions: â‘  Test-time scaling to boost inferencetime capability; â‘¡ Post-training methods such as supervised fine-tuning and reinforcement learning on spatial datasets; â‘¢ Architectural improvements for richer spatial encoding; and â‘£ Explainability studies to reveal limitations and failure modes in spatial reasoning. A. Test-Time Scaling Methods Test-time scaling methods offer training-free strategies to enhance MLLMs spatial reasoning during inference. Instead of retraining or fine-tuning, these approaches leverage improved prompting, tool-assisted reasoning, and external modality integration. Existing works can be broadly grouped into three categories based on their methodological focus. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 5 TABLE III: Comparison of prompt engineering methods for multimodal spatial reasoning. We summarize key ideas and prompt types of representative approaches. Method Prompt Type Key Idea / Mechanism TopViewRS [96] Textual Uses simple top-view templates as baseline prompts for spatial reasoning. VSI-Bench [8] Textual (graph-structured) Guides models to build and use cognitive graphs for spatial distance reasoning. OmniSpatial [97] Textual (CoT) Applies Chain-of-Thought reasoning to spatial VQA tasks. SCABenchmark [98] Textual (structured cues) Adds coordinates and reference frames to improve spatial understanding. Spatial-MM [7] Visual Minds Eye (VoT) [9] Visual / Hybrid SpatialPIN [11] Progressive SpatialPrompt [99] Quantitative / Textual SpatialMind [100] Structured / Multi-modal Uses bounding boxes or scene graphs to enhance spatial reasoning accuracy. Visualizes reasoning steps as spatial traces to aid understanding. Decomposes spatial queries into multi-stage sub-tasks. complex Establishes anchors stepwise geometric reasoning. spatial for Integrates representations with task-specific reasoning plans. scene 1) Prompt Engineering: Prompt engineering is the most direct and lightweight approach to enhance spatial reasoning in MLLMs without external tools or fine-tuning. Recent work explores how carefully crafted prompts can better elicit models latent spatial reasoning abilities. Although Chain-of-Thought (CoT) prompting has achieved notable success in general reasoning, its direct application to spatial tasks yields limited gains. To address this, researchers have proposed specialized prompting strategies tailored for spatial understanding, as shown in Table III. Early methods, such as TopViewRS [96], introduce simple templates but show only marginal improvements. VSIBench [8] demonstrates that explicitly instructing MLLMs to build cognitive graphs enhances spatial question answering, whereas standard CoT fails. Similarly, OmniSpatial [97] finds textual CoT ineffective for complex perspectivetaking. SCABenchmark [98] further analyzes prompt formats and frames of reference, showing that explicit geometric and relational cueslike coordinates and reference framesoutperform long, free-form CoT reasoning. Beyond text, visual prompting has proven complementary. Spatial-MM [7] shows that supplying bounding boxes or scene graphseither annotated or self-generatedgreatly improves multi-hop spatial reasoning, where CoT alone fails. Minds Eye [9] extends this with the Visualization-of-Thought paradigm, where the model visualizes reasoning traces during inference, significantly boosting 2D spatial reasoning accuracy. Additionally, progressive prompting frameworks decompose complex queries into manageable steps. SpatialPIN [11] employs multi-stage prompting with dense visual priors from multiple vision foundation models, demonstrating the benefits of structured, incremental reasoning. For quantitative spatial TABLE IV: Summary of tool-usage methods for multimodal spatial reasoning. indicates the method supports the feature. a / a n A B / k z i s v o d R ) / c ( I i c D 2 e D 3 t c e t x P e Method IoT [101] Struct2D [102] Lee et al. [103] ZeroVLM [104, 105] SpatialPIN [11] VADAR [106] SpatialAgent [10] reasoning, SpatialPrompt [99] improves performance by establishing explicit spatial anchors and prompting stepwise transformations relative to them. SpatialMind [100] integrates scene representationsmodeled as object-centric text, 2D grids, or 3D mapswith question-typespecific reasoning plans (e.g., locate transform compare), guiding more systematic inference at test time. Insights & Discussion. The evolution from simple CoT prompting to spatially structured prompting reveals key distinction between linguistic and spatial reasoning in MLLMs. While textual CoT assumes that verbalizing intermediate steps improves reasoning, spatial reasoning requires explicit modeling of visual relationsthrough visual traces, structured graphs, or reference-based transformations. This indicates that effective spatial prompting depends less on longer reasoning chains and more on aligning prompt representations with the inherently visual and relational nature of spatial cognition. Future work may explore adaptive prompting frameworks that automatically select the most suitable representational formattextual, visual, or hybridbased on the type of spatial query and reasoning context. test 2) Tool Usage: time enhances Integrating tools at MLLMs spatial reasoning by providing explicit geometric or structural priors without modifying the base model. As in Table IV, three main tool families have emerged. First, UI-style visual operations (e.g., crop, zoom, mark, edge, and segmentation) expose fine-grained spatial cues often missed by MLLMs. For instance, Image-of-Thought [101] directs the model to plan and execute short visual operation sequences, generating visual rationales that are fed back alongside text reasoning. Second, 2D perception modulessuch as object detection, orientation, depth, and pose estimationconvert pixels into structured, object-centric facts. Struct2D [102] renders BEV canvas with filtered object marks and metadata (IDs, categories, coordinates), while Lee et al. [103] constructed abstract scene layouts to support perspective transformations. Third, 3D reconstruction tools lift images into view-consistent geometry for perspective-based reasoning. ZeroVLM [104] employs Zero-1-to-3 [105] to synthesize novel views and pair them with view prompts that anchor camera relations, and IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 SpatialPIN [11] partially reconstructs lightweight 3D objects for downstream spatial queries. Given these tools, inference-time integration generally follows three escalating patterns. First, some methods append tool-generated images or traces to the input: IoT concatenates cropped or segmented snippets as visual evidence, while ZeroVLM stitches multi-view mosaics with view-aware prompts [101, 104]. Second, others serialize perception into structured tokens or sketches: Struct2D supplies BEV bitmap with concise object metadata, and Lee et al. inject numeric orientations and perspective descriptors to convert allocentric queries into egocentric ones [102, 103]. Finally, 3D-aware approaches render novel views from reconstructed geometry: ZeroVLM generates left/right/random perspectives to test viewpoint sensitivity, while SpatialPINs partial 3D lifting enables virtual viewpoints that re-ground spatial relations [11, 104]. Beyond single-shot prompting, modern systems increasingly control tools through agentic policies at inference. VADAR (Visual Agentic AI) [106] designs dynamic API and synthesizes short programs that call specialized modules (detector, depth, pose) on demandillustrating plan-to-execute tool use via code generation for reliable multi-step reasoning. SpatialScores SpatialAgent [10] provides standardized multiagent framework with nine spatial tools and two control paradigms: hierarchical PlanExecute pipeline and an interleaved ReAct mode that alternates reasoning and action, enabling consistent cross-method evaluation. Diagnostics further reveal which tool outputs matter most. Ravi et al. [107] show in Disjoint-3DQA that trajectories or BEV features offer limited gains across non-co-visible frames, while oracle 3D coordinates yield substantial improvementshighlighting metrically faithful 3D states or persistent scene memory as the most effective feedback signals. Insights & Discussion. Test-time tool use works by externalizing geometry into inputs MLLMs already consumevisual traces, structured tokens, and novel viewsrather than elongating textual CoT. Gains are largest when signals are metrically grounded (poses, coordinates, calibrated depth) and agentic controllers compose tools into reusable subroutines, improving perspective shifts, occlusions, and multi-object relations without retraining. â‘  Remaining issues: perception and view-synthesis errors propagate without uncertainty handling; trajectories) poorly approximate metric 2D proxies (BEV, 3D state; temporal persistence is weakno durable, objectcentric world memory; and tool outputs lack standardized units/frames, harming alignment and reproducibility. Multitool pipelines also add cost and latency for open-world, longhorizon tasks. â‘¡ Promising directions: maintain persistent object-centric scene memory with cross-view/time checks and lightweight geometric self-verification; standardize tool outputs (schemas for objects/cameras/constraints with calibrated uncertainty) to enable evidence weighting and conflict resolution; and develop budget-aware controllers that switch between PlanExecute and ReAct, add verifyreflect loops, and distill heavy chains into compact prompts/pluginsevaluated with utilitycostrobustness metrics in long-horizon, non-covisible, open-world regimes. 3) Others: Beyond prompt engineering and tool use, several training-free inference strategies improve spatial reasoning. The first category is self-consistency voting. Sample multiple reasoning chains and take consensus to stabilize answers under perspective shifts and multi-object relations. Secondly, multimodal search explores and prunes visualspatial reasoning paths at test time; e.g., VISUOTHINK performs look-ahead tree search over interleaved visual-textual steps and selects the best-scoring solution under spatial constraints [12]. There are also retrieval-augmented generation (RAG) methods. Inject external spatial knowledge at inference. LOGIC-RAG [13] builds dynamic first-order logic knowledge base (object positions/relations) from visual input and feeds these facts to the model, increasing driving-scene spatial accuracy from 5575% to >8090%. Grounding in retrieved maps/KBs or computed facts reduces hallucinations and sharpens spatial relations. Insights & Discussion. Enhancing spatial reasoning in MLLMs often requires more than static prompts or single-pass outputs. Exploring multiple reasoning paths, retrieving external spatial knowledge, performing light test-time adaptation, and preserving spatial context collectively scale inferencetime capability and complement prompt/tool methods. These approaches carry trade-offse.g., multi-sampling and adaptation increase compute, while retrieval depends on knowledge qualitybut they point toward MLLMs that dynamically and reliably reason about space with higher accuracy. B. Post-Training Methods Post-Training methods enhance spatial reasoning by adapting MLLMs after pre-training, mainly through supervised finetuning and reinforcement learning (RL). These approaches rely on spatially targeted datasets, rewards, and curricula to strengthen model understanding of geometry and motion. 1) Supervised Fine-tuning (SFT): SFT advances spatial reasoning by progressively broadening supervision from domainspecific static scenes to dynamic, temporally grounded reasoning. On the data side, domain-grounded QA continues to seed robust priors. CITYGPT [108] injects urban navigation and landmark knowledge through structured instructions, while MULTI-SPATIALMLLM [14] moves from single images to multi-frame settings, annotating frame-level relations (e.g., depth, camera/object motion) to capture persistence and occlusion. Extending this trend, LLAVA-ST [109] aligns finegrained spatio-temporal understanding by coupling language with explicit coordinates and temporal anchors, and STTHINK [110] focuses the lens on egocentric 4D reasoning to expose viewpoint changes and long-horizon temporal cues missing from static corpora. Synthetic pipelines complement real data: SAT [111] generates interactive, motion-centric tasks in simulation to cover self-motion and object-motion factors, and SPARE [112] automatically distills spatial QA from long-form descriptions to relieve the long-tail sparsity of rare relations. In between these regimes, SPATIALVLM [15] augments instruction tuning with region tags and relativein-front-of, between, etc), pairing position tokens (left-of, layout-driven QA and referring expressions so that textual IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 7 TABLE V: Comparison of reinforcement learning methods for spatial reasoning in MLLMs. indicates the presence of feature, indicates absence. w l l - c n a u i C s r R i o E / P - S e s l p T r l a 3 Method Video-R1 [16] Spatial-R1 [17] MetaSpatial [117] R1-Zero [3] ST-Think [110] M2-Reasoning [118] predicates are explicitly bound to coordinates/regions rather than inferred implicitly. Training strategy then ties these sources together. Curricula that progress from perception to composition remain effective: SPARKLE [113] stages supervision from detection/localization toward multi-hop spatial reasoning. In parallel, motion-aware instruction tuning such as ST-VLM [114] makes kinematics explicit with trajectory-style hints. Multi-stage alignment further stabilizes learning: LLAVA-ST [109] couples semanticto-coordinate alignment with video-aware objectives, whereas SAT [111] interleaves dynamic spatial tasks as higher-level sub-curricula to encourage transfer from static to viewpointshifting scenarios. Looking beyond plain instruction tuning, thinking overlays also matter: VISUALIZATION-OFTHOUGHT [115] and VISUAL+TEXTUAL THINKING [116] introduce multimodal reasoning traces (textual steps with region/coordinate cues), nudging the model to externalize intermediate spatial inferences rather than collapsing them into single answer token. Insights & Discussion. SFT highlights the value of taskspecific data and structured curricula for strengthening spatial reasoning in MLLMs. Compared with pre-training alone, spatially grounded supervision enables models to internalize explicit spatial relations, motion cues, and temporal dependencies often missing in general multimodal data. Methodologically, SFT studies show that gradual exposure to increasing spatial complexitystarting from low-level perception (e.g., object localization) to higher-order reasoning (e.g., trajectory prediction, multi-hop inference)consistently improves model performance. Incorporating temporally annotated or motionaware datasets further allows models to reason over both static configurations and dynamic evolution. Nonetheless, current SFT methods depend heavily on human-labeled or synthetic data, limiting scale and diversity. Future work could focus on automatically generating spatial annotations, leveraging selfsupervised pretexts, or designing adaptive multi-task curricula that balance static and dynamic reasoning. Ultimately, effective SFT should align supervision with the cognitive structure of spatial reasoning, bridging perception and high-level spatial understanding. 2) Reinforcement Learning (RL): RL enhances spatial reasoning by optimizing models through reward-driven feedback rather than explicit supervision. On rewards, as in Table VIDEO-R1 [16] introduces time-orderaware signal (e.g., preferring correct answers on ordered vs. shuffled clips) to explicitly reward temporal use, while SPATIAL-R1/SPACER [17] extends beyond outcome rewards to process-aware credit for intermediate steps (e.g., partial route/landmark correctness, local relation checks) to improve reward stability. For 3D layout and interaction, METASPATIAL [117] blends format checks, physical feasibility, and rendering-based validationtogether with object-level modulationfor consistent spatial plans. Unifying general and spatial reasoning, M2-REASONING [118] adopts task-specific RLVR signals (e.g., coordinate/ordering correctness) so that spatial subtasks contribute targeted feedback without derailing broader multimodal skills. Training strategies typically follow staged recipewarm up with SFT, then refine with RL, and finally stabilize with self-improvement. VIDEO-R1 [16] uses SFT to initialize video reasoning and then applies temporally sensitive RL to consolidate it. Similarly, ST-THINK [110] employs Long-CoT SFT followed by GRPO; meanwhile, reverse thinking is used as the explicit thought style in RL, strengthening bidirectional spatial recall. METASPATIAL [117] employs curriculum-style increases in scene difficulty and multi-round refinement so that rewards stay informative as tasks grow more complex. Selfplay closes the loop: R1-Zerolike training [119] generates and solves spatial puzzles autonomously, reducing dependence on human labels and converting search over solutions into search over training data. In broader multi-task settings, M2REASONING [118] interleaves spatial RLVR with generalpurpose tasks and dynamic scheduling, mitigating interference while retaining cross-task transfer. Overall, these approaches illustrate how RL advances spatial reasoning from two complementary angles: (1) reward design, which explicitly encodes geometric and temporal correctness; and (2) self-improvement, where models iteratively refine reasoning through autonomous exploration. Compared with supervised fine-tuning, RL offers more flexible framework for post-training adaptationenhancing spatial consistency, dynamic reasoning, and generalization without modifying the base architecture. Insights & Discussion. Reinforcement learning (RL) provides powerful framework for improving spatial reasoning in MLLMs by optimizing beyond static supervision. The reviewed methods reveal clear evolution: from composite tasklevel rewards (VIDEO-R1) to process-level and curriculumbased optimization (SPATIAL-R1, METASPATIAL), and finally to autonomous self-play learning (R1-ZERO). This progression reflects shift from externally guided training toward self-improving spatial cognition. Two primary insights emerge. First, reward granularity mattersintegrating intermediate reasoning rewards and geometric correctness encourages stable and interpretable spatial learning. Second, autonomous exploration enables continual improvement without reliance on labeled data, promising direction for scalable spatial intelligence. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 8 However, current RL frameworks remain constrained by high computational cost, reward sparsity, and limited generalization across 2D3Dtemporal domains. Future research could develop hybrid paradigms that combine RL with supervised fine-tuning or self-distillation, using automatically generated spatial feedback signals. Advancing toward richer, selfsupervised spatial rewards and cross-domain generalization will be key to achieving more human-like spatial reasoning in multimodal large language models. C. MLLM Architectural Modifications Beyond post-training, architectural changes are essential for enabling MLLMs to reason about space effectively. Most MLLMs adopt standard three-part structurea pre-trained LLM, visual encoder, and modality alignment interface [65, 120124]. However, spatial reasoning demands explicit preservation of positional and geometric information, which these components alone cannot ensure. Recent studies have thus proposed modifications to inject spatial knowledge either at the input level or via specialized model components. 1) Enhancing Input Representations: One strategy is to augment the model inputs with additional spatial cues so that the LLM can infer geometric relations without changing the core architecture. The most straightforward one, SPATIALLLM [125], adopts composite 3D information design, where the vision front-end mixes features from language-supervised encoder (CLIP) with features from self-supervised encoder (DINOv2 or MAE) to improve the 3D perception capability at the input level. Going further, MPDRIVE [126] adds an extra to each video frame, overlaying simple marker channel glyphs or numeric labels at detected object centers. The model processes the original RGB frame and this marker map in parallel (dual-stream), effectively bridging visual coordinates with language; this yields improved spatial understanding on autonomous driving VQA tasks. Similarly, LOCVLM [127] appends normalized (x, y) location coordinates of salient objects directly into the text prompt (treating location as part of the language input). By doing so, the LLM is encouraged to reason about spatial relations (e.g., left of, inside of) using these coordinate tokens, all without altering the pretrained vision encoder or adding new visual branches. Both methods inject explicit spatial information into the models context, which in turn guides the language model to produce spatially-aware descriptions and answers. Another direction is to incorporate depth and 3D cues as part of the input. SPATIALBOT [128] feeds the model with both an RGB image and its corresponding depth image (e.g., from monocular depth estimator), essentially giving the MLLM pseudo-3D view of the scene. This simple input-level fusion of color and depth significantly boosts the models depth perception and spatial QA performance, as evidenced by improvements on the SpatialQA benchmark and embodied AI tasks. Rather than images, SSR [129] leverages depth information in textual form: it converts raw depth maps into structured natural-language rationales describing the 3D layout (e.g., relative distances, sizes, and occlusions). These intermediate text descriptions are provided to the LLM (as chain-of-thought prompt) to guide its reasoning and are later distilled into latent embeddings for efficiency. This rationale-guided approach enables the model to utilize depth cues for higher-order spatial reasoning without requiring special sensors at inference. In similar vein, other works enrich the models visual context by supplying multiple views or an explicit 3D scene representation. For instance, the SPATIO-TEMPORAL LLM framework [130] can input an entire point cloud of the environment alongside an egocentric video clip, allowing the LLM to consider the global 3D scene while also tracking temporal events. Experiments show that feeding both the holistic point cloud and video frames (plus text) enables better spatial understanding of environments and improves temporal grounding of actions. Likewise, MMSPATIAL [131] explores training MLLMs with multi-view images of scene and their associated metric depth values. By exposing the model to multiple perspectives and precise depth measurements during fine-tuning (via the CA-VQA dataset), MM-Spatial achieves state-of-the-art 3D spatial understanding; notably, it can estimate object sizes and distances with accuracy on par with dedicated monocular depth estimators. In summary, these input-centric approaches enhance spatial reasoning by explicitly encoding geometry into the models inputs (either as augmented images or as location/depth tokens in text). This mitigates the loss of spatial information in standard vision backbones and provides the LLM with richer basis for spatial inference. Insights & Discussion. Input-centric augmentation remains minimally invasive: marker channels or coordinate tokens guide the LLM toward geometry without altering backbones, while depth, multi-view, or point-cloud evidence supplies 3D context that strengthens grounding. Yet performance is tightly coupled to detector/depth fidelity, and longer contexts strain alignment and attention memory. Uncertainty-aware spatial tokenizers and differentiable 2D3D projectors that compress geometry, paired with curricula that progress from single-view to spatio-temporal inputs, are likely to curb shortcut reliance and improve cross-domain generalization. 2) Redesigning Spatial Reasoning Modules: An alternative (and complementary) approach introduces dedicated architectural modules that are tailored for spatial and relational reasoning. Here, the base MLLM architecture is extended with new components (or entire sub-networks) that preserve spatial structure through the models internal representations. For example, SPATIAL-MLLM [18] introduces dedicated spatial encoder built on lightweight VGGT backbone. Given sampled video frames, this encoder produces 3D-aware features that retain scene geometry. These features are then linearly projected to match both the dimensionality and the effective batch size of features from conventional 2D visual encoder. The two streams are concatenated and passed through modality bridgea lightweight MLPthat converts them tokens, which are consumed alongside into unified visual text tokens by shared LLM backbone. This geometrypreserving, spatio-temporal pathway yields consistent gains on spatial benchmarks, reporting 35-45% relative improvements over strong baselines. Similarly, SPATIAL-ORMLLM [20] incorporates Spatial-Enhanced Feature Fusion block within IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 9 the vision tower to inject 3D understanding. In this design, 2D image features are combined with rich 3D cues (e.g., depth or volumetric estimates obtained via an external algorithm) inside fusion module, and the resulting 2D+3D feature is fed into the LLMs visual encoder. This end-to-end architecture effectively endows the model with volumetric spatial reasoning using only monocular RGB input, achieving robust 3D scene understanding in complex environments (like surgical operating rooms) without additional sensors. Another notable system, SPATIALRGPT [19], integrates spatial reasoning capabilities by adding plug-in depth module and leveraging region-level training signals. In particular, SpatialRGPT uses flexible depth-integration module that attaches to the existing visual encoder, enabling it to process inferred depth maps alongside RGB features. Moreover, it is trained with curated pipeline of 3D scene-graph data to learn detailed regional representations, which allows the model to interpret user-provided region proposals and accurately judge their relative directions and distances during inference. This yields marked improvements in spatial question-answering, both with and without explicit region prompts. Yet another architectural innovation is found in CAMBRIAN-1 [132], vision-centric multimodal model that introduces Spatial Vision Aggregator (SVA). The SVA is dynamic, spatially-aware connector module that fuses highresolution visual feature maps into the LLM while intelligently reducing the number of visual tokens required. By preserving fine-grained spatial information from the vision encoder and feeding it more efficiently to the language model, Cambrian1 achieves better visual grounding and overall multimodal performance (it served as an open-source testbed that reached state-of-the-art results on new CV-Bench benchmark). Across these designs, the common theme is the addition of structural bias for space: by introducing new layers or networks devoted to geometric processing (be it via explicit spatial feature fusion, graph relationships, or high-res feature aggregation), the models can maintain spatial layouts through the reasoning process, instead of relying solely on implicit signals in the image embeddings. Insights & Discussion. Dedicated modules inject geometric inductive bias: multi-scale encoders, relation graphs, and spatial cross-attention preserve layout/topology; domain-tailored 2D+3D fusion and depth-integrated connectors enhance robustness under occlusion and clutter. Furthermore, visioncentric aggregators retain fine spatial detail with fewer tokens, and aligning static 3D context with video stabilizes temporal grounding. Nevertheless, added complexity, latency, and reliance on pseudo-3D labels motivate intent-aware routing between spatial modules and the LLM, unified 2D/3D/temporal consistency objectives, and lightweight hardware-friendly spatial layers for deployment. D. Explainability of Multimodal Spatial Reasoning Understanding why MLLMs struggle with spatial reasoning is essential for advancing their design and interpretability. Recent studies have provided valuable insights into these limitations and suggested strategies for improvement. From mechanistic perspective, Rajabi et al. [133] reveal through attention visualization that current MLLMs often rely on object co-occurrence rather than genuine geometric grounding. To address this, they propose decomposing spatial descriptions into grounded subjectobjectrelation triplets, linking detection and positional features through lightweight relational bridge. Following this thread, Qi et al. [21] identify representational imbalance in multimodal Transformers where dominant vision embeddings suppress positional encodings, erasing spatial order. Using interpretability metrics, they attribute this to cross-modal norm disparities and propose normalizing vision token magnitudes and injecting mid-layer geometric features to recover spatial sensitivity without altering the backbone. Chen et al. [22] further analyze attention maps and found that only 1520% of attention weights target regions enindicating that MLLMs focus coding spatial relationships, on isolated objects instead of inter-object relations. They propose ADAPTVIS, training-free inference strategy that dynamically adjusts attention based on confidence, helping the model refocus on relevant spatial regions. This process-level modulation highlights attention control as an effective route to better spatial grounding. In parallel, Wen et al. [23] show that even large MLLMs often depend on bounding-box heuristics instead of genuine relational cues. They recast spatial relation prediction as global interaction problem and introduce RelatiViT, objectobject transformer that integrates relation-awareness directly into self-attention, embedding structural bias for spatial reasoning into the encoder itself. Finally, Zhang et al. [134] take broader view, showing that simply scaling multimodal data yields diminishing gains on spatial reasoning tasks. Their analysis indicates that spatial competence relies more on the positional fidelity of the vision encoder than on the LLMs textual positional signals. They advocate embedding explicit 3D-aware modules and crossview fusion layers to ensure spatial understanding emerges from structure rather than scale. Insights & Discussion. Together, these studies converge on shared diagnosis: MLLMs exhibit strong semantic reasoning but weak spatial grounding due to representational imbalance, attention bias, and lack of geometric priors, which emphasize the need for models that balance semantic and spatial representations. Future research should focus on integrating these complementary insightsexplicit spatial grounding, balanced cross-modal encoding, relation-aware attention, and geometryinformed architectural priorsto enhance the accuracy and robustness of MLLMs in reasoning about spatial configurations. IV. MULTIMODAL SPATIAL REASONING IN 3D SPACE Multimodal spatial reasoning in 3D space is key area of research, with significant implications for downstream applications such as navigation [39, 40], vision-language-action tasks [140, 141], and more. This section focuses on foundational tasks with multimodal spatial reasoning, including 3D grounding, 3D scene reasoning, and 3D generation. As illustrated in Figure 4, we provide an overview of these core tasks, highlighting their roles within the broader landscape of 3D spatial understanding. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 10 Year Method Input Backbone Highlights 2023 LLM-Grounder [24] Point Cloud GPT-4 Uses LLM as an agent for 3D closed-loop, feedback-driven visual grounding, which is fully zero-shot and open-vocabulary 2023 Grounded 3D-LLM [25] Point Cloud Tiny-Vicuna-1B Unifies 3D task modeling with LLM Vigor [135] Point Cloud GPT-3.5-Turbo Introduces referential order modeling for language-obj structure 2023 ViewRefer [136] Multi-View Image GPT-3 Multi-view modeling improves spatial perception 2023 3DAxiesPrompts [2] Multi-View Image GPT-4V First to encode 3D coordinates into prompt input 2024 VLM-Grounder [26] Multi-View Image GPT-4V Utilizes dynamic stitching strategy that dynamically uses the optimal layouts to stitch images, enhancing VLMs performance 2024 SpatialRGPT [19] 2024 ZSVG3D [137] RGB-D RGB-D LLaMA2-7B Modular design enables flexible integration GPT3.5 First use of program generation in 3DVG SeeGround [27] Text+RGB+3D Qwen2-VL-72B Dynamically adjusts perspectives to capture essential details 2025 ReasonGrounder [28] RGB+3DGS LLaVA 1.5 Integrates LVLM, 3DGS, and hierarchical features, enables amodal perception under occlusion TABLE VI: Comparison of recent multimodal spatial reasoning methods in 3D Grounding. Fig. 4: An overview of core spatial reasoning tasks in 3D space, including 3D visual grounding[19, 135], 3D scene reasoning[11, 38], and 3D generation[138, 139]. grounding methods are fully supervised on limited 3D datasets with predefined object captions [142], but they struggle to generalize to unseen objects and handle complex texts. Unlike traditional methods, researchers are developing approaches based on MLLMs, significantly enhancing generalizability by leveraging large-scale priors. However, integrating MLLMs into 3D grounding remains challenging [143]. Existing approaches for embedding MLLMs into 3D grounding systems can be broadly categorized based on the input data modality: â‘  direct utilization of 3D representations and spatial information; â‘¡ generation of multi-view 2D images rendered from 3D scenes; â‘¢ hybrid methods combining both 2D and 3D modalities, as shown in Table VI. 1) 3D Input: Some methods perform spatial reasoning by embedding 3D formatssuch as point clouds, voxels, or learned volumetric featuresinto MLLMs [24, 25, 135]. LLM-Grounder [24] adopts coarse-to-fine approach, first using an MLLM to parse complex linguistic concepts and an open-vocabulary 3D vision module to generate candidate proposals, then evaluating their semantic alignment with the query. Grounded 3D-LLM [25] integrates scene-referent toFig. 5: 3D visual grounding with MLLM [24]. A. 3D Visual Grounding As in Figure 5, given natural language description, 3D grounding involves localizing an object in 3D scene. This task requires strong spatial reasoning to handle complex instructions and is crucial for robotics and AR, combining language understanding and 3D spatial reasoning. Traditional 3D IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 11 kens into the MLLM and employs alignment training to enable 3D input, leveraging the MLLMs reasoning capabilities. Vigor [135] focuses on interpreting spatial language by using an LLM to infer the referential order of entities, enhancing fine-grained spatial reasoning. Insights & Discussion. In summary, these approaches focus on 3D visual grounding by embedding 3D representations into MLLMs and utilizing their spatial reasoning ability. However, while embedding 3D modalities holds great potential, it presents challenges. The complexity of 3D data structures can hinder model interpretability, and the limited availability of labeled 3D datasets constrains the development of robust, generalizable models for open-world applications. 2) Multi-view Input: While 3D point clouds provide explicit scene representation, they present challenges for models due to the complexity of spatial information. To address this, researchers are increasingly adopting multi-view 2D representations as promising alternative. This approach leverages the spatial reasoning capabilities of existing 2D MLLMs with minimal modifications. Representative methods include ViewRefer [136], VLM-Grounder [26], and 3DAxisPrompt [2]. key challenge in multi-view 3D visual grounding is view discrepancy, which arises from the misalignment between the models perspective and the source of the grounding instruction. Several methods have been proposed to mitigate this issue. For example, ViewRefer [136] introduces learnable multi-view prototypes to capture inter-view relationships and enable knowledge transfer. VLM-Grounder [26] dynamically stitches image sequences and incorporates grounding-andfeedback mechanism. 3DAxisPrompt [2] enhances the realworld scene by inserting 3D coordinate axes. Insights & Discussion. These works leverage powerful MLLMs to align with 3D scenes using 2D multi-view inputs. However, key challenges remain [19]: First, MLLMs designed for global image understanding struggle with parsing specific object regions. Second, spatial perception extends beyond RGB data and requires geometric information like depth or spatial coordinates. 3) Hybrid of 2D and 3D: To combine the advantages of both 3D and multi-view representations, recent methods utilize hybrid inputs, including [19, 27, 137, 144]. SpatialRGPT [19] highlights the limitations of MLLMs relying solely on RGB pixels for 3D tasks. It proposes integrating relative depth maps from depth prediction models with RGB images to enhance spatial perception and reasoning. ZSVG3D [137] defines visual program interface to standardize spatial relationships, enabling reasoning plans for grounding. SeeGround [27] integrates 2D visuals with explicit 3D spatial descriptions localization. 3D-MOOD [145] achieves to improve object monocular open-set 3D object detection via lifting the open-set 2D detection into 3D space. ReasonGrounder [28] introduces 3D Gaussian splatting features as intermediate representations from SAM [146] and CLIP [120]. Insights & Discussion. These methods demonstrate the limitations of using only 2D or 3D representations and propose strategies for integrating both modalities. Combining multiview images and 3D structures enhances performance and robustness in 3D visual grounding systems. Year 2023 2023 2023 2023 2024 2024 2024 2025 Method Alignment Technique Chat-3D [151] Multi-modal Transformer Chat-Scene [152] Multi-modal Transformer 3D-LLM [153] Q-Former-liked module GPT4Point [147] Q-Former-liked module LL3DA [38] LEO [148] Q-Former-liked module LLaVA-liked module Scene-LLM [149] LLaVA-liked module LLaVA-3D [29] LLaVA-liked module 3D-LLaVA [154] LLaVA-liked module TABLE VII: Comparison in alingment methods. B. 3D Scene Reasoning and Question Answering (QA) 3D scene reasoning and QA require models capable of processing 3D representationssuch as point clouds, meshes, neural radiance fields, or multi-view RGB-D inputsand generating natural language responses grounded in the spatial and semantic structure of the environment. Current research falls into two paradigms: training-required and training-free. Training-required methods fine-tune MLLMs, typically via Q-Former [38, 147] or projection-layer modules [148, 149]. Training-free methods use frozen MLLMs with progressive prompting [11] and chain-of-thought reasoning [11, 150]. 1) Training-required: Training-required studies can be classified into three categories: â‘  Alignment approach: These methods focus on aligning 3D features with language modalities. â‘¡ Training efficiency: Aiming to reduce complexity and improve convergence. â‘¢ 3D Representation: Expanding beyond conventional 3D representations to scene graphs, 3DGS [155, 156], etc. The next sections elaborate on each category, summarizing current advancements in multimodal spatial reasoning for 3D. â‘  Recent methods focus on aligning 3D scene features with MLLM feature spaces. Early works [151, 152] use 3D detectors to extract object-level representations, which are aligned with text features using 3D-text paired data, enabling MLLMs to leverage prior knowledge. However, reliance on 3D detectors can be bottleneck. To address this, inspired by Q-Former [157], recent works [38, 147, 153, 158] integrate similar designs into 3D MLLMs for more complex reasoning. For example, 3UR-LLM [158] uses 3D compressor to condense 3D features into compact vision tokens and 3D query fusion mechanism to select high-confidence queries, improving reasoning robustness. Besides Q-Former, several methods [29, 148, 149, 154] are inspired by LLaVA. These approaches use projection layer to align the feature space with LLMs, enabling them to process 3D inputs and leverage their spatial reasoning capabilities. For example, Scene-LLM [149] employs two-stage strategy, training projection layer with conceptual annotations while keeping the LLM frozen. An overview of these alignment techniques is presented in Table VII. â‘¡ Beyond improving alignment quality, recent studies [29, 154, 159, 160] note that aligning 3D features with language is time-consuming. To improve efficiency, 3DMIT [159] removes IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 12 Year 2024 2025 Method Representation Training 3DGraphLLM [30] Scene Graph Full Training SplatTalk [161] GPT4Scene [162] 3DGS BEV Fine-tuning Zero-shot / Fine-tuning TABLE VIII: Comparison of multimodal spatial reasoning methods with diverse 3D representations. the alignment step by focusing on instruction tuning for spatial understanding. LLaVA-3D [29] retains LLaVAs 2D multimodal capabilities by constructing 3D patches and using 3D-aware positional encoding. Inst3D-LMM [160] introduces multi-task instruction tuning, enabling adaptation to various spatial reasoning tasks without task-specific fine-tuning. â‘¢ Recent works [30, 161, 162] focus on diverse 3D representations, including 3D scene graphs, 3DGS, and BEV. 3DGraphLLM [30] creates learnable 3D scene graph to enhance spatial reasoning by utilizing richer structural information. SplatTalk [161] integrates language features from RGB images into unified 3DGS [155] representation, supporting spatial reasoning. GPT4Scene [162] improves reasoning by reconstructing BEV images from 3D scene videos and establishing consistent mapping between local views and global scene structure. comparison of these 3D representations is provided in Table VIII. Insights & Discussion. Efforts to enhance 3D spatial reasoning in MLLMs focus on modality alignment, training efficiency, and exploring alternative 3D representations. However, challenges remain: â‘  Training 3D-aware models is computationally intensive due to complex data and architectures. â‘¡ The lack of large, diverse, and well-annotated 3D datasets limits the effectiveness of supervised training. â‘¢ The absence of transparent reasoning mechanisms hinders interpretability and understanding of model decisions. Addressing these limitations could further advance MLLMs for spatial reasoning. 2) Training-free Methods: Training-free methods [11, 31, 150, 166] leverage the prior knowledge in MLLMs for multimodal spatial reasoning without the need for fine-tuning. These methods explore various prompting strategies to facilitate interpretable spatial reasoning. Some works [11, 150] use MLLMs to extract semantic object attributes and apply the chain-of-thought mechanism, prompting sequential reasoning. SpatialPIN [11] is modular framework that employs progressive prompting to decompose and reconstruct explicit 3D representations, enhancing spatial reasoning. Agent3DZero [31] introduces Set-of-Line strategy for selecting and analyzing multiple viewpoints, improving spatial reasoning while reducing memory and computation. LLM-TPC [166] employs Think-Program-reCtify loop to bridge 3D visual perception and reasoning, improving reliability through iterative self-correction. Insights & Discussion. These training-free methods utilize MLLMs to summarize and refine spatial information through diverse prompting strategies. Despite their success, they have limitations: â‘  They depend on the quality of the MLLMs used, and deficiencies in these models may hinder performance on some tasks. â‘¡ Some methods involve complex inference steps, reducing processing speed and making them less suitable for real-time applications. C. 3D Generation with Spatial Reasoning 3D generation [167, 168] has advanced rapidly, particularly with the integration of LLMs and multimodal reasoning systems. Scene-level and program-level generation demand strong spatial reasoning capabilities. These tasks can be categorized into two aspects: â‘  3D Layout Generation: Generating spatially reasonable indoor layouts from natural language or multi-turn dialogues. â‘¡ 3D Generation as Program: Treating 3D content generation as programmatic task, where spatial reasoning is framed as executable program generation. 1) 3D Layout Generation: Given the complexity of 3D scene generation [169171], researchers often use MLLMs for initial 3D layout generation, followed by scene-level synthesis. Figure 6 presents qualitative comparison of representative 3D scene generation approaches, showcasing variations in geometric fidelity, texture quality, and semantic consistency across different methods. Approaches can be broadly categorized based on how MLLMs are integrated into the layout pipeline: â‘  Direct Guidance for Scene Synthesis via LLMs: MLLMs directly generate spatial configurations or layout instructions, translating high-level descriptions into structured commands for scene elements, such as furniture arrangement and room dimensions. However, this direct mapping can lead to implausible configurations, like overlapping objects. Methods like LayoutGPT [5] and HOLODECK [164] address this by incorporating optimization-based solvers or inferring spatial relational constraints. â‘¡ Indirect Guidance for Scene Synthesis via LLMs: Indirect guidance uses MLLMs to extract semantic knowledge (e.g., object relationships or contextual constraints) to guide subsequent 3D modeling. For instance, Diorama [139] generates scene graph defining object relationships, while the MLLM retrieves multimodal 3D shapes. Approaches like LayoutGPT [5] use programmatic reasoning to generate spatial layout specifications, while HOLODECK [164] enhances this with optimization techniques for physical realism. Iterative methods, such as I-Design [165] and Generation Agents [172], introduce multi-agent systems for step-by-step refinement. LLPlace [173] supports real-time interactive layout refinement through conversational interface, and Chat2Layout [174] combines VQA with visual prompting, enhancing spatial layout reasoning. Insights & Discussions. The primary approaches either generate positions directly or create intermediate representations like scene graphs. Both paradigms leverage MLLMs for semantically coherent and physically feasible 3D environments. Future advancements in MLLMs could enhance both numerical accuracy and formatting capabilities. 2) 3D Generation as Program: Building on advances in MLLM-based code generation (e.g., Cursor [175] and GitHub Copilot [176]), recent work treats 3D synthesis as procedural program generation, where geometry and layout are specified by code. As shown in Fig. 7, 3D model can be described by code snippet, leveraging MLLMs structured reasoning and IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 13 Fig. 6: Some comparative examples of 3D generations, such as input conditions (e.g., text or image), and outputs from different approaches[139, 163165], showcasing variations in geometry, texture, and semantic coherence. Fig. 7: demo for programming 3D object representation. The left is CAD model, and the right is the corresponding code segment of CAD Query [34]. constraints. Current approaches target three output formats: â‘  Blender scripts, â‘¡ CAD parametric programs, and â‘¢ meshgeneration pipelines. â‘  Blender is the most common software in 3D modeling and animation, supporting operations via its API and Python code. The following methods utilize MLLMs spatial reasoning for programming outputs. 3D-GPT [33] introduces training-free framework where an LLM interprets natural language commands and generates Blender scripts to construct 3D scenes, unlocking the potential of MLLMs in spatial programming. SceneCraft [177] proposes dualloop optimization system: an inner loop refines scenes using MLLM feedback, while the outer loop accumulates spatial knowledge across iterations, enabling self-evolving capabilities. SceneMotifCoder [178] introduces visual programs structured code representations extracted from example-based demonstrations. â‘¡ In addition to Blender, other works extend spatial reasoning into CAD modeling. CAD-GPT [179] enhances spatial reasoning by integrating spatial tokens and positional embeddings, enabling accurate generation of CAD sequences from images or text. CAD2PROGRAM [180] converts 2D engineering drawings into executable Python scripts using MLLMs. CAD-Recode [34] maps point cloud data into CadQuery scripts via lightweight encoder and pre-trained MLLM backbone. CAD-LLaMA [181] designs parametric language to better utilize MLLMs spatial knowledge. â‘¢ Other work focuses on general mesh generation using programmatic approach. ShapeLib [182] guides LLMs in constructing libraries through hybrid human-AI workflow. Insights & Discussions. These works reflect the expanding scope of MLLMs in tackling complex, real-world tasks that require deep spatial reasoning, precise geometric control, and integration with downstream tools. While directly generating 3D representations is challenging, using MLLMs for 3D content generation via programming harnesses their full spatial reasoning potential. Programmatic generation is also more controllable, making it better suited for real applications. V. MULTIMODAL SPATIAL REASONING IN EMBODIED AI is Embodied AI regarded as crucial path toward AGI [186]. The rapid progress of MLLMs positions them as promising candidates for the core reasoning module of embodied agents. Many of the core intelligences expected of embodied agentssuch as geometric reasoning, navigation, and perspective-takingfundamentally rely on spatial reasoning capabilities as their foundation [187189]. As demonstrated in Fig. 8, in this section, we focus on the multimodal spatial reasoning capabilities of MLLM-based embodied agents within the context of current mainstream tasks, including VisionLanguage Action (VLA), Vision-and-Language Navigation (VLN), and other embodied AI tasks. A. Multimodal Spatial Reasoning in VLA Models VLA models generate executable actions from multimodal inputstypically visual observations and language instructionsusing vision-language foundation models as their backbone. These systems often involve intermediate reasoning steps, either implicit within the architecture or explicit through modular design. Pioneering works such as OpenVLA [190] and Ï€0 [191] adopt an end-to-end paradigm, training VLMs as reactive policies to predict low-level control actions from large-scale demonstrations. Others [46, 192] decompose tasks into natural language sub-tasks executed by reactive controllers or lower-level VLAs, while some frameworks introduce intermediate stages like affordance or goal-state prediction followed by motion planning for action generation. Regardless of the control representation, spatial reasoning to these systems. Research efforts to imremains central prove spatial understanding in VLAs generally follow three directions: â‘  integrating spatially informative sensor modalities (e.g., depth, point clouds) to enrich spatial context; â‘¡ adopting multi-task pre-training or co-training schemes that implicitly encourage spatial reasoning; and â‘¢ incorporating IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 14 Fig. 8: Spatial reasoning in embodied tasks, such as VLA [6, 45, 183], VLN [184, 185] and other embodied tasks [48]. TABLE IX: Comparison of 3D-enhanced VLA methods. indicates the feature is present, and indicates it is absent. t r s h D o n n a e o i n l a Method Training Strategy 3D-VLA [45] PointVLA [193] SpatialVLA [194] BridgeVLA [195] Dual-phase (2D pre-train + 3D fine-tune) Diffusion-aligned Action expert fusion Monocular depth-based encoding explicit reasoning steps. The following subsections review representative methods in each direction and discuss their respective advantages and limitations. 1) Spatially informative input modalities: Several studies enhance spatial understanding in VLA models by incorporating spatially informative modalities such as depth maps and 3D point clouds, as shown in Table IX. These additional inputs compensate for the limitations of 2D visual data, which often lack the geometric cues needed for reasoning about physical interactions in 3D space. 3D-VLA [45] enhances language model with 3D perception and goal generation by introducing interaction tokens for objects, locations, scenes, and actions. It aligns the language model with diffusion models that generate goal images, depth maps, and point clouds from instructions. PointVLA [193] combines 2D image features from VLM and 3D point cloud features from point encoder as inputs to an action expert for prediction. SpatialVLA [194] encodes 3D information into 2D observations using 3D-aware positional encodings derived from monocular depth predictions. BridgeVLA [195] employs dual-phase training: pre-training VLM for 2D heatmap-based object localization and fine-tuning with multi-view orthographic projections of 3D point clouds to generate action trajectories. Insights & Discussion. These approaches show promise for action prediction with richer spatial perception, but challenges remain. key limitation is the scarcity of large-scale datasets compared to visionlanguage corpora [193, 195], motivating synthetic data [45] or imputing missing modalities with pretrained models (e.g., SpatialVLA [194]). Yet such approximations often underperform. Moreover, models trained at scale on 2D visionlanguage data still lead overall [6, 46, 191], indicating that fully leveraging extra modalities will require targeted pre-training and more data-efficient architectures. 2) Multi-task Preand Co-training: Another major approach to enhance spatial understanding in VLA models is to modify the training regime to include auxiliary tasks that implicitly encourage spatial reasoning, such as embodied question answering or 3D bounding box detection, as in Table X. This is typically achieved through pre-training or co-training frameworks that share representations across related spatial tasks. The concept is first explored in RT-2 [196], which jointly trained VLM on visual question answering and robot IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 TABLE X: Comparison of multi-task preand co-training strategies for VLA models. indicates the feature is present, and indicates it is absent. Method RT-2 [196] Gemini Robotics [6] Ï€0.5 [46] ChatVLA [183] Magma [197] e b . r o j k D 3 a - s - u t / l r action prediction within shared token space. Building on this idea, recent large-scale models like GEMINI ROBOTICS [6] and Ï€0.5 [46] employ multi-stage co-training pipelines. GEMINI ROBOTICS [6] adopts two-stage procedure: the base VLM is first pre-trained on tasks including trajectory prediction, multi-view correspondence, and 3D bounding box detection, yielding the embodied reasoning model Gemini-ER capable of few-shot control through in-context learning. The model is then fine-tuned with an action decoder that outputs low-level control commands for complex manipulation tasks. Similarly, Ï€0.5 [46] pre-trains its VLM backbone on mixture of tasks such as visual question answering, object localization, sub-task prediction, and discrete action generation. During post-training, an additional action head is introduced for continuous control prediction, followed by fine-tuning for both continuous control and sub-task reasoning. ChatVLA [183] introduces two-stage curriculum where the model first learns control from robot data, then training examples from other tasks, such as VQA, are gradually introduces to preserve alignment with pre-trained VLM representations. It also adopts Mixture of Experts architecture with task-specific heads to avoid task interference. Magma [197] proposes to bridge the gap between vision-language and action data via surrogate tasks that require predicting actionable 2D annotationsSet-of-Mark and Trace-of-Mark. This enables joint training on diverse datasets across digital and physical domains using the same output representation. Insights & Discussion. Preand cotraining on spatial reasoning tasks is an effective way to enhance the generalization capabilities of VLA models. However, this approach doesnt come without its challenges. It requires access to large and diverse datasets, and carefully balancing multiple training objectives. Still, when these challenges are addressed, it remains core strategy for building capable VLA models. 3) Explicit Reasoning: third line of research enhances spatial reasoning in VLA models by introducing explicit reasoning steps during action generation. Unlike reactive policies [140, 190, 191] that directly map inputs to actions, these models incorporate structured intermediate representations and multi-step reasoning to interpret spatial relations and plan subtasks before executing actions. ECoT [198] trains VLA models to generate step-by-step reasoning chains grounded in the scene and robot state prior to action prediction. These chains include high-level plans, subtasks, object locations, and low-level motions, improving both generalization and interpretability. Chat-VLA2 [47] builds on ChatVLA by adding reasoning-following module that aligns generated actions with the backbones internal reasoning, yielding better performance on multi-step spatial tasks. Chainof-Affordance [199] introduces an affordance-based reasoning process that decomposes tasks into four stages: identifying target objects, selecting grasp points, locating placement regions, and planning trajectories. These affordances, generated at inference time, guide the policy models action selection. Similarly, RT-Affordance [200] proposes hierarchical VLA where action generation is conditioned on affordance plans. An affordance prediction model first generates key poses from images and task descriptions, which then guide reactive VLA to produce low-level control actions. Insights & Discussion. Reasoning-augmented models improve robustness, generalization, and interpretability in spatial tasks by explicitly modeling intermediate steps such as object selection, spatial relations, and action planning. This structured reasoning helps policies handle novel objects, scenes, and instructions more effectively than purely reactive baselines. While early methods introduced substantial inference overhead, newer systems mitigate this through selective reasoning and asynchronous pipelines. These trends suggest that the benefits of explicit reasoning can be retained without prohibitive latency, making such models increasingly practical for realworld deployment. 4) Multimodal Spatial Reasoning in Vision Language backbone: Many current VLA models are fine-tuned from VLMs or use them as backbones. These VLAs are claimed to effectively inherit the prior knowledge of these pre-trained models. To quantitatively assess the potential of the upstream VLMs for robotics tasks, we collected open-source VLMs that have been used in VLAs and evaluated them on spatial reasoning benchmarks relevant to embodied scenarios. Specifically, OpenVLA [190] is fine-tuned from Prismatic [201], Ï€0 is finetuned from PaliGemma [202], TraceVLA [141] is fine-tuned from Phi-3-Vision [203], and DexVLA [140] uses Qwen-2VL [204] as its backbone. As for the benchmarks, Embodied Reasoning QA (ERQA) [6] is benchmark specifically designed for evaluating VLMs in embodied environments. It tests the VLMs ability to handle embodied tasks. On the other hand, SpatialEval [94] and SPACE [205] are benchmarks that assess the more fundamental and conventional spatial reasoning abilities of VLMs, such as the ability to judge relative spatial positions and distances. Both of these capabilities are crucial for robotics. Therefore, we conducted experiments by testing several VL backbones used in VLA on these benchmarks. As shown in the Tab. XI, it is evident that these backbones exhibit certain spatial reasoning abilities. This is also why these models can achieve strong performance in downstream applications after fine-tuning on robotic datasets. B. Multimodal Spatial Reasoning in VLN Models VLN [206] is cooperative multimodal task where an agent navigates 3D environments by following human instructions IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 Benchmark Prismatic PaliGemma Qwen-2-VL Phi3-Vision ERQA [6] SpatialEval [94] SPACE [205] 32.25 32.13 23. 27.25 29.86 17.00 32.50 26.80 18.75 34.00 46.46 26.25 TABLE XI: Embodied-AI-related benchmark results across different VLMs. Note that SpatialEval is tested using VTQA mode(with Vision-Text input). and communicating in context under ambiguity. It involves four key components: visual perception, language understanding, decision-making, and navigation executionall requiring strong spatial reasoning. During perception, the agent must localize itself, interpret spatial relationships between objects, and plan an efficient route. Finally, it executes the navigation plan based on these spatial decisions. 1) Visual Environment Understanding and Generalization: For VLN agent, it is crucial to perceive and interpret its surroundings, anticipate how actions alter the environment, and align perception and decision-making with natural language instructions. This requires understanding spatial arrangements, localizing itself in 3D space, estimating distances between targets and landmarks, retaining spatial information, and tracking environmental changes over time. These abilities collectively depend on strong spatial reasoning, which underpins success in complex vision-and-language navigation tasks. Existing embodied scene perception methods often rely on 3D or 2.5D data to enhance spatial awareness, as summarized in Tab. XII. To better utilize visual inputs, many approaches explicitly preserve spatial features through multiview perception, depth images, or scene graphs. NaviLLM [208] leverages multiview images to capture all reachable viewpoints from the current position and constructs task-specific schemas for LLMbased action generation. Cai et al. [128] propose SpatialBot, which uses depth API to query geometric information from the environment and feed it back into the model, strengthening spatial understanding. ConceptGraphs [207] builds an open-vocabulary 3D scene representation by associating 2D foundation model outputs across multiple views. Beyond visual encoding, another research direction focuses on narrowing the semantic gap between natural language and 3D scene understanding. Spartun3D-LLM [35] integrates 3D-aware LLM with situated spatial alignment module to better link 3D visual representations with corresponding textual descriptions. Similarly, Wang et al. [209] introduce 3D representation model for embodied tasks that predicts novel views and BEV maps at multiple scales, aligning multi-scale feature fields with multi-granularity language representations. Beyond scene understanding, maintaining environmental memory and tracking temporal changes are equally important. Hong et al. [36] propose GSA-VLN, where agents dynamically update parameters, leverage long-term memory, and to both environments and diverse user instructions. adapt Similarly, Yang et al. [210] present 3D-Mem, memory architecture that encodes multi-view 3D snapshots to accumulate and retrieve spatial information for long-term perception and reasoning. Insights & Discussion. Accurate perception, robust spatial Fig. 9: Visual environment understanding in VLN tasks. Current methods take text, point clouds [35], multi-view images [208], RGB-D images [128, 207, 209] as inputs and align them with 3D scene representations, while maintaining structured memories such as scene graphs [207] and BEV maps [209] for effective spatial reasoning. reasoning, and generalization across diverse visual scenes are fundamental for VLN agents. As shown in Fig. 9, recent work emphasizes structured 3D representations, such as scene graphs, BEV maps, and multiview memory, as effective tools linking perception to reasoning and planning. key challenge remains the alignment of visual features with linguistic inputs, especially under unfamiliar views or domain shifts. 2) Human Intention Interpretation and Instruction Comprehension: VLN agents are required to comprehend natural language instructions provided by humans within specific situational contexts to complete navigation tasks. This involves cor rectly interpreting spatial expressions such as left, up, and front, and developing the ability to reason spatially about object locations, directions, and movements [8]. To facilitate efficient instruction understanding, common strategy is to incorporate auxiliary modalities into the input. LL3DA [38] encodes 3D point clouds and leverages an attention mechanism to aggregate contextual information from both the scene and human interactions. In addition, improved VQA paradigms can further enhance an agents instruction comprehension. AutoSpatial [37] applies hierarchical two-round VQA strategy during training, achieving both global and detailed understanding of scenarios, which demonstrates more accurate spatial perception. Moreover, certain methods, such as affordance prediction, have been introduced to improve the models ability to attend to fine-grained visual details under human instructions. Yuan et al. [211] proposed RoboPoint, vision-language model tailored for predicting spatial affordances from relational language inputs. The model predicts precise action points that comply with spatial and physical constraints, thereby facilitating subsequent action execution. Insights & Discussion. Recent work highlights the benefits of auxiliary modalities, hierarchical reasoning, and affordance modeling in improving instruction understanding. Multi-round VQA and affordance prediction enhance fine-grained grounding, while attention-based fusion with human interactions supports contextual comprehension. Future advances may rely on tighter integration of spatial perception and language reasoning, along with better generalization to diverse instructions and complex real-world tasks. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 17 Year Method Input Backbone Highlights 2024 ConceptGraphs [207] RGB-D image LLaVa Constructs open-vocabulary 3D scene graphs 2024 NaviLLM [208] Multi-view RGB image Vicuna-7B-v0 Uses schema-based instruction to adapt LLMs Spartun3D-LLM [35] Point Cloud GPT4o Integrates 3D-based LLM with spatial alignment module that links 3D objects and relations to text, bridging the 3D-text gap 2025 g3D-LF [209] RGB-D image Vicuna-7B-v0 Proposes generalizable 3D-language feature fields 2025 SpatialBot [128] RGB-D image QWen1.5-0.5B Introduces depth API to retrieve geometric information TABLE XII: Comparison of recent multimodal spatial reasoning methods in embodied scene understanding. TABLE XIII: Comparison of path planning and navigation methods for VLN agents. indicates feature presence. n e i S Method NavVLM [39] SpatialCoT [212] NavCoT [40] FlexVLN [213] NavA3 [214] TopV-Nav [215] BrainNav [216] n e C . d a . i i i l g n l h r a - / p 3) Path Planning and Navigation for VLN Agents: VLN agents must combine perception, reasoning, and planning to execute goal-directed navigation from natural-language instructions, as in Table XIII. LLMs often serve as the highlevel planners in these systems. NAVVLM [39] employs VLM as the cognitive core, interpreting language goals and guiding exploration through semantic understanding of the environment. To enhance spatial reasoning, SPATIALCOT [212] introduces bi-directional spatial coordinate alignment and Chain-of-Thought grounding, improving reasoning accuracy and interpretability. enable Addressing domain adaptation, NAVCOT [40] uses to self-guided parameter-efficient adaptation reasoning chains aligned navigation, generating coherent with downstream planning. To reduce hallucinated plans, FLEXVLN [213] validates LLM-generated guidance through an auxiliary MLLM, ensuring action feasibility. For longhorizon tasks, NAVA3 [214] adopts hierarchical framework: reasoning VLM identifies target regions, and pointing VLM performs fine-grained localization via spatial affordances. Mapping-based approaches further improve navigation. TOPV-NAV [215] constructs adaptive top-view maps using visual prompts, providing structured spatial priors for reasoning. BRAINNAV [216] integrates dual maps (coordinate and topological) and dual orientations (relative and absolute), enabling real-time navigation with dynamic scene updates. Insights & Discussion. Recent methods enhance VLN agents by combining LLM-based planning with spatial grounding, domain adaptation, and hallucination mitigation. Structured spatial priors further support real-time reasoning. Future efforts should unify spatial perception and language reasoning for generalizable, low-supervision navigation. TABLE XIV: Comparison of representative methods for Embodied Question Answering (EQA). indicates the method supports or explicitly incorporates the feature. Open-Vocab Graph Scene 3D Reasoning CoT Percept. RL Modular Method Majumdar et al. [41] Tan et al. [217] Hao et al. [42] Zhao et al. [218] C. Multimodal Spatial Reasoning in Other Embodied Tasks 1) Embodied Question Answering (EQA): EQA, first proposed by Das et al. [219], has become central benchmark in embodied AI and robotics. In this task, an agent receives natural-language questione.g., Is there sofa in the living room?and must explore the environment, gather visual evidence, and provide an answer. The challenge lies in grounding language to spatial perception and reasoning. Majumdar et al. [41] developed an open-vocabulary EQA dataset to evaluate foundation models, revealing that current systems struggle with spatial queries requiring object-level and scene-level understanding. To improve spatial reasoning, Tan et al. [217] introduced 3D scene graph as an external memory, enabling the model to retain and reason over spatial layouts across multiple turns, significantly improving multistep QA efficiency. Hao et al. [42] advanced this direction by integrating Chain-of-Thought (CoT) reasoning within the Embosr framework, allowing structured spatial inference across complex 3D scenarios. Zhao et al. [218] further decoupled perception and reasoning by assigning visual understanding to large-scale VLMs and using lightweight language model, optimized via reinforcement learning, for reasoning. Incorporating slow-thinking mechanism enhances depth and reliability in spatial reasoning. Insights & Discussion. EQA task highlights the intricate interplay between language grounding, visual perception, and spatial reasoning in interactive environments. key insight from recent advances is that bridging the gap between low-level visual inputs and high-level task understanding requires combining the strong perceptual capabilities of foundation models with explicit reasoning mechanisms, such as scene graphs, neural program synthesis, and chain-of-thought prompting. Future efforts may benefit from further aligning spatial representations with language semantics and enhancing the memory efficiency of agents in multi-turn reasoning settings. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 18 2) Embodied Grasping: Robotic grasping in cluttered environments remains difficult due to occlusions and complex object interactions, demanding fine-grained spatial reasoning. THINKGRASP [43] introduces goal-driven language prompts that help identify and prioritize obstructing objects, enabling grasp planning even for heavily occluded targets. FREEGRASP [44] represents objects as discrete keypoints and overlays visual markers to enhance GPT-4os zero-shot spatial reasoning. AFFORDGRASP [220] integrates GPT-4o for incontext affordance reasoning, predicting graspable parts and intended functions, which are grounded using VLPart and Grounded-SAM for part-conditioned optimization. Similarly, UNIDIFFGRASP [221] leverages GPT-4o to infer target semantics and functional parts from user input, combining multistage segmentation and diffusion-based sampling for dual-arm grasp generation in complex scenes. Insights & Discussion. Cluttered environments, frequent object occlusions, and the need to follow strict temporal and spatial action sequences constitute the primary challenges in embodied grasping tasks. In such settings, spatial reasoning plays particularly critical role. Using visual observations effectively and appropriately integrating the reasoning capabilities of VLMs are key to addressing these challenges. 3) Embodied World Models: Embodied world models simulate the dynamics of physical environments, supporting policy learning, data-driven simulation, and long-horizon planning. However, models relying solely on 2D pixel observations often fail to capture accurate spatial relationships, leading to incomplete scene representations and weak depth or pose estimation. Structurally consistent scene generation is therefore crucial for effective spatial reasoning and world modeling. EVA [49] integrates video generation model with visuallanguage model, combining reasoning with high-quality video synthesis. TESSERACT [48] simulates temporal evolution in 3D environments, enabling realistic interactions such as object manipulation and drawer opening while maintaining spatialtemporal consistency across RGB-DN sequences. More recently, 3DFLOWACTION [222] predicts object-level scene flow for manipulation and employs GPT-4o [223] to verify task completion by aligning rendered final states with language descriptions, linking physical dynamics with semantic evaluation. Insights & Discussion. Embodied world models form the foundation for large-scale simulation data used to train embodied agents. Ensuring geometric and spatial consistency in these generated environments is critical for supporting accurate spatial reasoning and realistic embodied intelligence. VI. SPATIAL REASONING WITH VIDEO AND AUDIO A. Spatial Reasoning with Video Video inherently captures more information about scene than static images, leading to significant research into the spatial reasoning capabilities of MLLMs. Extending the reasoning abilities from image-based tasks to video-based understanding opens exciting new possibilities. However, accurately reasoning about spatial properties and establishing correspondences in dynamic, temporal scenes remains persistent challenge. As proposed by Spatial-R1 [17], seven critical spatial reasoning tasks are essential in this domain: object relative distance, object size estimation, room size estimation, object relative direction, object appearance order, object absolute distance, and object counting. We systematically review this emerging area and summarize the key characteristics of the existing methods, as shown in Tab. XV. Recent work has explored specialized architectures and training strategies to enhance spatial reasoning capabilities in MLLMs. representative example is Spatial-R1 [17], which proposes fine-tuning vision-language models with reinforcement signals grounded in spatial consistency. This training encourages the model to align outputs with the underlying 3D or 2D geometry implied by the video. SpaceR [3] further refines this approach by injecting positional tokens derived from visual object tracking, enabling improved frame-to-frame localization. Other works introduce complementary strategies. R1-Zero-like training [119] builds on reinforcement objectives to penalize spatial hallucinations and reward temporally stable spatial predictions. ST-Think [110] introduces dualmodality backbone that processes egocentric video using both motion and layout cues, enabling 4D (space-time) reasoning through transformer modules. Similarly, Video-R1 [16] augments the visual encoder with spatial maps derived from frame-wise geometric analysis, and uses spatial alignment loss to preserve inter-frame consistency. LLaVA-ST [109] and VideoINSTA [51] adopt an orthogonal approach: they focus on instruction tuning with spatial-temporal prompts, encouraging zero-shot understanding of video-level concepts like object permanence and navigational intent. These models rely on vision encoders (typically CLIP variants) that preserve spatial resolution via patch-wise tokenization. In Thinking in Space [8], spatial memory is modeled explicitly through recurrent memory cache, allowing the LLM to recall visual states at earlier timestamps for long-horizon reasoning. benchmark-centric perspective is introduced by V-STaR [226], which offers suite of probing tasks to evaluate spatial reasoning across different axes: motion tracking, occlusion recovery, topological layout understanding, and cross-frame object matching. Coarse Correspondence [224] complements this with strategy that boosts spatial alignment across frames via coarse-to-fine token matching, improving temporal coherence in reasoning chains. Lastly, Aether [225] proposes geometric-aware world modeling through unified token representations that encode both position and object identity, enabling downstream LLMs to simulate spatial transitions with minimal hallucination. Insights & Discussion. Recent progress in multimodal spatial reasoning demonstrates the growing capability of MLLMs to handle structured space-time understanding. However, challenges remain: models often lose spatial detail due to token compression and lack mechanisms for robust spatial memory. Solutions such as marker-based overlays (as in MPDrivestyle approaches) and coordinate-augmented prompts (as in LocVLM [127]) provide partial remedies, but fall short in generalizing across diverse video domains. Egocentric video in particular poses unique difficulties for multimodal spatial reasoning: distinguishing between agent motion and object IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 19 TABLE XV: Comparison of recent multimodal spatial reasoning methods in video QA. Year Task Dataset Benchmark Method Spatial Components Code Arxiv 2024 MCVQA, OEVQA, etc ACL Long Video-QA Arxiv 2024 ScanQA, OpenEQA Arxiv 2024 Arxiv 2025 Video-QA Video-QA Arxiv 2025 Depth Estimation Arxiv 2025 Arxiv 2025 Arxiv RSTR Video-QA Video-QA - - - - - - VideoLLaMA2 [50] Convolution Connector VideoINSTA [51] Content-based Reasoning Coarse Correspondence [224] Lightweight tracking model VSI-Bench VSI-Bench[8] - Video-R1 - - - - Video-R1 [16] AETHER [225] V-STaR [226] - - GRPO - - SpaceR-151k - Ego-ST Bench Ego-ST Bench SpaceR [3] ST-R1 [110] Task-Specific GRPO Training Long-CoT and GRPO TABLE XVI: Comparison of recent multimodal spatial reasoning methods in audio. Year Task Benchmark Method Spatial Components NeurIPS 2023 Audio-Visual Sound Localization and Detection STARSS23 [52] - link link - link link link link link link Code - ICML 2024 ICML 2025 Arxiv 2025 Audio-QA Audio-QA SpatialSoundQA [53] Spatial Audio Encoder, Curriculum Learning link AQAPHY ACORN [54] Fundamental Physical Phenomena - Audio-Visual-QA SAVVY-Bench SAVVY [55] Spatial Tracks and Global Map Construction link - BAT it in Tab. XVI. STARSS23 [52] introduces an audio-visual sound event localization and detection (SELD) task, along with the STARSS23 audio-visual dataset to support spatial reasoning for SELD. SpatialSoundQA [53] is the first largescale benchmark focused on spatial audio question answering (Audio-QA). It includes over 21, 000 simulated binaural audio clips rendered in 3D environments, accompanied by diverse questions involving directionality, distance estimation, and multi-source spatial reasoning. Architecturally, the proposed BAT model combines spatial audio encoder with large language model (LLM) and employs curriculum learning to gradually enhance the models spatial reasoning capabilities. ACORN [54] also addresses Audio-QA by introducing the AQAPHY benchmark. Technically, improves an LLMs spatial reasoning by incorporating fundamental physical phenomena such as the Doppler effect, multipath propagation, and spatial relationships. More recently, SAVVY [55] has emerged as prominent testbed for spatial reasoning that integrates both audio and visual cues, i.e., audio-visual question answering (Audio-Visual-QA). Specifically, SAVVY presents SAVVYBench, which evaluates 3D spatial reasoning in dynamic scenes with synchronized spatial audio, and proposes to enhance spatial understanding by first extracting spatial tracks and then constructing global spatial map. These benchmarks collectively advance standardized evaluation for audio spatial reasoning and enable quantitative comparison across MLLMs with varying degrees of spatial awareness. It is worth noting that other Audio-QA and Audio-Visual-QA methods, such as SARI [229], Meerkat [230], and EchoInk-R1 [231], are not discussed here as they do not specifically address spatial reasoning. Insights & Discussion. Despite recent progress, significant challenges remain for robust audio spatial reasoning. Current models still struggle to generalize in open-world scenarios with multiple, dynamic sound sources. These limitations are Fig. 10: Spatial reasoning from audio & video with MLLMs. motion requires grounded scene representations and persistent memory. While early efforts such as ST-Think and Thinking in Space offer promising architectures, scalable and generalizable spatial world models remain an open research area. B. Spatial Reasoning with Audio Audio spatial reasoning is the process of interpreting spatial cues from sound, such as direction of arrival, source location, and distance, to infer the physical context of an auditory scene. While human listeners effortlessly localize and segregate sounds using binaural cues, current multimodal large language models (MLLMs) have primarily focused on what is heard (the content) rather than where it is heard from [227]. This lack of spatial awareness limits applications such as audiovisual navigation and egocentric perception, where an AI agent must infer where sound originates to interact effectively with its environment. To bridge this gap, recent research [52 55, 227, 228] has begun to explore spatial reasoning capabilities by training large-scale multimodal models that learn from audio-only or audio-visual inputs. We systematically review this emerging area and summarize the key characteristics of recently proposed methods, as shown IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 20 Authors Venue/Date Paper Link Code Input Modality Feng et al. Imran Kabir et al. Peiran Wu et al. Ziyue Wang et al. Jonathan Roberts et al. Mingjie Xu et al. Hongyu Li et al. Yang et al. Xingrui Wang et al. Liao et al. Chengzu Li et al. Huanqia Cai et al. Siyu Wang et al. Navid Rajabi et al. Chonghao Sima et al. Ivan Majic et al. Li Xuan et al. Yew Ken Chia et al. Xiao Liu et al. Roshanak Mirzaee et al. Yu-Chuan Su et al. Letitia Parcalabescu et al. Liu et al. Ramakrishnan et al. Arxiv 2025 (Mar) Arxiv 2025 (Mar) Arxiv 2025 (Mar) Arxiv 2025 (Mar) Arxiv 2025 (Feb) WACV 2025 Arxiv 2025 (Jan) CVPR 2025 CVPR 2025 Arxiv 2025 (Apl) Arxiv 2025 (Jan) Arxiv 2025 (Feb) AAAI 2025 NIPS 2024 Workshop ECCV 2024 GeoAI 2024 IOTMMIM 24 ACL 2024 ACL 2022 NAACL 2021 Arxiv 2021(Apr) ACL 2022 TACL 2023 Arxiv 2024 (Oct) P P P P P P / / / / / Image-Text Video-Text Video-Text Image-Text Image-Text Graph-Desc/QA/Conv Video-Text(QA) Video-Text(QA) Image-Text Video-Text(QA) Image-Text Image-Text CAD-Text Image-Text(QA) Image/Graph-Text(QA) Image-Text Image-Text Image-Text Image-Text Text Image-Text Image-Text Image-Text Image-Text TABLE XVII: General MLLM: Benchmarks and Datasets further compounded by the scarcity of large-scale, highquality spatial audio datasets with precise annotations, which makes it difficult to train models that perform well outside of controlled or simulated environments. To bridge these gaps, promising directions include the development of richer data collection pipelines, such as real-world egocentric recordings or improved simulation techniques that better approximate real acoustic conditions. In parallel, more specialized model architectures are expected to emerge to effectively leverage these spatial cues. By addressing both data and modeling challenges, future systems may achieve human-like spatial hearing, reasoning not only about what is heard but also where it occurs within complex, dynamic scenes. VII. BENCHMARKS Multimodal spatial reasoning enables AI systems to understand and infer spatial relationships within scenes by integrating information from multiple modalities, such as vision and language. Initially, benchmarks and datasets focused on simple scenes and basic spatial relations. However, as multimodal foundation models evolved, the focus shifted to more complex reasoning and cross-modal inference. Before these models, research was constrained to environments with basic spatial tasks, such as determining relative object positions in visual question answering (VQA). With the rise of powerful pretrained models, new benchmarks were developed to address greater openness, richer complexity, and deeper reasoning capabilities. These efforts span domains like panoramic imagery, video, computer-aided design (CAD), and geographic information systems (GIS), advancing AI systems in scene understanding. Figure 11 illustrates the development of multimodal spatial reasoning benchmarks. This section provides an overview of the evolution of datasets and benchmarks, highlighting key stages, modality types, and domain coverage, with focus on those from the foundation model era. A. Early Multimodal Spatial Reasoning Benchmarks Before the advent of large-scale multimodal foundation models, early research in spatial reasoning relied heavily on datasets focused on natural images paired with textual descriptions. These datasets aimed to tackle basic spatial reasoning tasks, such as object localization and relationship detection. pivotal benchmark in this domain is the Visual Genome dataset [232], which provides annotated images and graphs to depict spatial relationships between objects, facilitating imagetext question answering tasks. Another significant contribution is SpatialSense [233], which contains wide variety of spatial relationships, promoting tasks that involve misclassificationprone scenarios. Similarly, TVQA+ [234] combines video clips with object detection annotations, requiring models to answer questions that involve both spatial and temporal reasoning. The 2.5VRD dataset [235] focuses on fine-grained visual relationship detection using triplet annotations, capturing spatial relationships between objects. Additionally, the VALSE benchmark [236], though not solely designed for spatial reasoning, includes rich annotations of spatial relationships and actions, providing an excellent resource for evaluating models vision-language grounding capaIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 21 Fig. 11: The chronological progression of multimodal spatial reasoning benchmarks. Each colored marker represents distinct benchmark, with hue variations indicating different modality combinations (e.g., image-text-graph, audio-video). The timeline illustrates the evolution of assessment methodologies and the increasing complexity of spatial reasoning evaluation frameworks. bilities. Further contributions, such as the VSR dataset [237], define explicit spatial reasoning tasks, while datasets like COCO-Spatial in WhatsUp [238] examine the limitations of pre-trained models on spatial reasoning. These early benchmarks, while focused on basic spatial cognition, set the foundation for more advanced tasks, sparking further developments in multimodal spatial reasoning for large-scale models. B. Image-Text Spatial Reasoning Benchmarks With the rise of large-scale multimodal foundation models (MLLMs), spatial reasoning tasks have expanded into various domains. This section discusses the evolution of 2D spatial reasoning benchmarks, categorizing them based on task objectives and methodologies. 1) 2D Spatial Reasoning Tasks: 2D spatial reasoning benchmarks evaluate models ability to reason about spatial relationships in two-dimensional settings, focusing on tasks like navigation, object localization, and layout generation. key trend is the integration of multimodal data, combining visual and textual information for enhanced reasoning. For example, DriveMLLM [239] annotates spatial relationships in driving scenarios using question-answer pairs, assessing navigation understanding. SpatialEval [240] provides synthetic images with spatial tasks, such as Spatial-Map and MazeNav, testing relative object positioning in controlled settings. The SPACE benchmark [205] offers both large-scale and small-scale tasks, from layout understanding to viewpoint transformations, evaluating models ability to handle diverse spatial challenges. 2) Hybrid Approaches and Abstract Representations: Some datasets explore abstract representations. VSR [237] provides annotations for spatial positional relationships, testing complex spatial reasoning. Datasets like COCO-Spatial [238] introduce spatial tasks that involve context, navigation, and dynamic reasoning. Other benchmarks, such as OmniSpatial [97] and GSR-Bench [241], enhance real-world relevance, offering comprehensive evaluations in areas like autonomous driving and robotics. OmniSpatial tests tasks like dynamic reasoning, traffic analysis, and geometric decomposition, reflecting realworld spatial complexities. 3) Insights & Discussion: 2D spatial reasoning datasets have evolved from simple image-text pairs to multi-task frameworks evaluating diverse reasoning abilities. Recent datasets emphasize multimodal data, combining visual and textual information for complex reasoning. Although synthetic data accelerates benchmarking, it faces challenges in generalization and real-world applicability. Future benchmarks should integrate dynamic real-world data and hybrid datasets combining synthetic and real data to better cover edge cases and enhance evaluation. These advancements will enable more capable models for autonomous navigation, robotics, and other complex applications. 4) 3D Spatial Reasoning Benchmarks: The development of 3D spatial reasoning datasets has significantly advanced in recent years. Boyuan Chen et al.introduced the first 3D spatial reasoning dataset [15], incorporating depth-aware reasoning into multimodal systems. To evaluate MLLMs on dynamic spatial reasoning tasks, Arijit Ray et al.proposed the SAT dataset [111], which includes simulated 3D scenes for training IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 22 and real-world environments for testing. This dataset, using 3D scene simulations, improves model performance on dynamic spatial reasoning tasks through real-world evaluation. To further address gaps in 3D spatial reasoning, AnChieh Cheng et al.introduced SpatialRGPT-Bench [19], which generates 3D reasoning tasks grounded in 2D scenes. Their pipeline combines instance segmentation and depth estimation to construct tasks such as object size, height, and relative distance estimation using only 2D inputs. Additionally, Xingrui Wang et al.developed the Spatial457 dataset [242] for 6D spatial reasoning, covering 3D localization, orientations, and multi-object relationships, further assessing the performance of MLLMs on these complex tasks. Insights & Discussion. The introduction of 3D spatial reasoning benchmarks has brought significant advances, especially in data generation. Synthesis-driven annotation methods and automated 2D-to-3D conversion pipelines have alleviated annotation challenges. As tasks evolve, they have shifted from basic orientation and static perception to dynamic scene understanding and multi-perspective reasoning, increasing cognitive complexity. Furthermore, evaluation frameworks have transitioned from simulation-based training to real-world scenario validation, establishing closed-loop paradigms for performance assessment. Despite these advances, challenges remain, particularly in cross-modal alignment and adapting to dynamic scenes, highlighting the need for continued research in these areas. C. Video-Text Spatial Reasoning Benchmarks Recent advancements in video-text spatial reasoning have led to the development of diverse benchmarks aimed at systematically evaluating spatial understanding capabilities. These benchmarks have evolved from fundamental perceptual tasks to more complex spatiotemporal tasks. Current benchmarks increasingly emphasize the integration of temporal and spatial cues, leveraging both synthetic and annotated data to support model training and evaluation. The following sections provide detailed overview of these benchmarks, categorized by task type and complexity, highlighting their contributions in vediotext spatial reasoning. 1) Fundamental Spatial Perception Tasks: Benchmarks in this category evaluate core spatial perception skills such as object counting, relative direction, and distance estimation. VIS-100K [119] introduces 100,000 videoquestionanswer pairs spanning six spatial reasoning tasksobject count, relative/absolute distance, relative direction, object size, and room size. Fine-tuning MLLMs on this dataset demonstrates that the GRPO reinforcement algorithm effectively enhances spatial reasoning performance. VIS-BENCH [8] further examines how MLLMs memorize and reason about spatial layouts. Built from 288 annotated indoor videos, it includes 5,000 QA pairs covering eight tasks such as distance, direction, path planning, and order of appearance, offering detailed analysis of spatial understanding. SPACER-151K [3] expands this scope with 151K samples, including 91K spatial QA pairs and 60K general video understanding examples. Each task incorporates precise spatial metadata (e.g., bounding boxes, temporal indices) and 1010 grid maps encoding object distributions. comparison TABLE XVIII: Performance Spatial Reasoning Benchmarks follow the BLINKAccuracy (01); DRIVEMLLMZero-shot Score; SATAccuracy Real/Synthetic. on VideoText (reported pairs only). Metrics SPATIALRGPT-BENCHSuccess Rate; subset); SPATIALEVALAccuracy on originals: (spatial Benchmark Metric Model Value SPATIALRGPT-BENCH [19] Success Rate Success Rate BLINK (spatial) [245] Acc. Acc. Acc. Acc. Acc. Acc. SPATIALEVAL [240] Acc. (01) Acc. (01) Acc. (01) Acc. (01) Acc. (01) Acc. (01) Acc. (01) Acc. (01) DRIVEMLLM [239] Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) Score (ZS) LLaVA-v1.6-34B [243] GPT-4V [244] LLaVA-v1.6-34B [243] InstructBLIP-Vicuna-7B [246] InstructBLIP-Vicuna-13B [246] Gemini-Pro [247] GPT-4V [244] GPT-4o [244] LLaVA-v1.6-Mistral-7B [243] LLaVA-v1.6-Vicuna-7B [243] LLaVA-v1.6-Vicuna-13B [243] LLaVA-v1.6-34B [243] InstructBLIP-Vicuna-7B [246] InstructBLIP-Vicuna-13B [246] Gemini-Pro [247] GPT-4V [244] LLaVA-v1.6-Mistral-7B [243] LLaVA-v1.6-Vicuna-7B [243] LLaVA-v1.6-Vicuna-13B [243] LLaVA-ov-7B [248] LLaVA-ov-72B [248] Qwen2-VL-7B [249] Qwen2-VL-72B [249] Qwen-VL [250] mPLUG-Owl2 [251] InstructBLIP-Vicuna-7B [246] InstructBLIP-Vicuna-13B [246] Gemini-1.5-flash [252] Gemini-Pro [247] GPT-4V [244] GPT-4o [244] SAT [111] Acc. (Real) Gemini-1.5-flash [252] Acc. (Synthetic) Gemini-1.5-flash [252] Acc. (Real) Gemini-1.5-Pro [252] Acc. (Synthetic) Gemini-1.5-Pro [252] Acc. (Real) GPT-4V [244] Acc. (Synthetic) GPT-4V [244] Acc. (Real) GPT-4o [244] Acc. (Synthetic) GPT-4o [244] 43.98 58.14 76.22 55.24 64.34 67.13 72.03 76.92 0.33 0.24 0.38 0.42 0.24 0.27 0.687 0. 38.20 38.20 38.20 22.29 21.10 21.17 20.11 36.50 33.90 42.80 42.80 54.03 40.10 51.70 25.63 57.60 50.00 64.80 49.90 50.70 44.80 57.50 49.40 Rigorous quality control ensures balanced, unambiguous data, establishing new large-scale benchmark for spatial reasoning in multimodal systems. 2) Advanced Spatiotemporal Reasoning Tasks: These benchmarks extend spatial reasoning to dynamic tasks such as path planning and cross-modal coordination, emphasizing temporal consistency and causal reasoning. ST-ALIGN [109] establishes unified framework for fine-grained spatiotemporal reasoning with three tasks: Spatial-Temporal Video Grounding (STVG), Event Localization and Captioning (ELC), and Spatial Video Grounding (SVG). It jointly evaluates spatial and temporal localization, advancing beyond datasets focused on isolated spatial or temporal cues. EGO-ST [110] addresses the overlooked role of temporal dynamics by introducing reverse egocentric reasoning. Comprising over 5,000 QA pairs IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 across four tasksroute description, directional change, landmark transition, and action shiftit systematically evaluates how MLLMs integrate dynamic spatial cues and temporal order. V-STAR [226] targets the gap between object-centric and temporal reasoning. Its core Reverse Spatio-Temporal Reasoning (RSTR) task links WhatWhenWhere and WhatWhereWhen chains to assess logical consistency, using the Logarithmic Geometric Mean (LGM) metric to jointly measure accuracy, temporal IoU, and spatial IoU. It establishes the first standardized benchmark for comprehensive spatiotemporal reasoning in Video-LLMs. Overall, these datasets advance spatial intelligence evaluation from static spatial perception to dynamic, temporally grounded reasoningcrucial for realistic embodied and video understanding. 3) Mixed-Task Benchmarking: This class of evaluation benchmarks incorporates diverse data sources and tasks of varying difficulty levels to provide comprehensive assessment of model capabilities. Due to the scarcity of highquality video reasoning data, current MLLMs exhibit limited spatial reasoning capabilities in video contexts. To address this issue, Feng et al.introduced two datasets: Video-R1-COT-165k and Video-R1-260k[16]. The former contains CoT annotated samples generated from both image and video inputs, serving as cold-start dataset for supervised fine-tuning. The latter is designed for reinforcement learning training, comprising mix of image and video data to enable models to acquire general reasoning skills from static images and transfer them to dynamic video contexts through hybrid training strategy. Although only about 8% of the samples in these datasets involve explicit spatial reasoning tasks, the inclusion of complete CoT annotations offers valuable resources for advancing research on spatial reasoning in video-based settings. Insights & Discussion. Current visual-spatial reasoning benchmarks are advancing from static attribute recognition toward dynamic spatiotemporal coupling, demanding progressively higher spatial cognitive capabilities from models; however, they remain constrained by limitations including prohibitive annotation costs restricting dataset scalability, inconsistent quality in semi-automated multimodal LLM-generated annotations, and overly homogeneous templated data that inadequately fosters profound spatial cognitionnecessitating paradigm shift from isolated data curation to synergistic algorithm-data co-design, from single-modality datasets to multi-source hybrid data frameworks, and from superficial pattern matching to causal inference incorporating physical constraints like gravitational collision dynamics. D. Other Modal Benchmarks Additional multimodal benchmarks extend spatial reasoning beyond visionlanguage inputs. For audiovisual spatial reasoning, related datasets and evaluation protocols are detailed in Section VI-B. CITYINSTRUCTION and CITYEVAL, released with CITYGPT [108], evaluate spatial reasoning, navigation, and path generation in realistic urban scenes. CAD-GPT [179] introduces dataset pairing natural language descriptions and single-view images with CAD modeling sequences, enabling multimodal 3D model synthesis and benchmarking. SGG [253] provides structured scene graphs and 3D point clouds fused with LLM-generated questionanswer dialogues, supporting open-vocabulary spatial reasoning across complex visual layouts. Finally, MM-ESCAPE [254] simulates an interactive escape-room environment where models must perform sequential spatial reasoning and actions to exit, offering novel framework for evaluating goal-driven reasoning in dynamic scenes. Insights & Discussion. Contemporary multimodal spatial reasoning datasets exhibit tripartite evolutionprogressing from scene-driven construction to task sophistication and evaluation closurewhere real-world task demands grow increasingly complex, modalities diversify, and spatial reasoning advances beyond basic directional perception toward causal spatial inference chains. Nevertheless, persistent gaps remain in establishing unified framework that ensures physical plausibility, enables action verifiability, and maintains costeffective data curation, indicating considerable scope for advancement in multimodal spatial reasoning data infrastructure. VIII. CHALLENGES AND FUTURE DIRECTIONS Multimodal Spatial Reasoning in Egocentric Vision. While existing research on spatial reasoning in MLLMs primarily focuses on third-person perspectives, there is growing need to explore egocentric vision, where spatial reasoning must occur from the agents first-person viewpoint [255257]. This shift introduces unique challenges, such as the agents movement, limited field of view, and the temporally evolving nature of the environment. In egocentric vision, spatial reasoning must account for dynamic changes in both the agents position and the environment. Future research should focus on developing MLLMs capable of understanding object relationships from shifting viewpoints, inferring navigation intent, and reasoning about interaction affordances. promising direction lies in creating models that can more effectively simulate and understand embodied behaviors, leading to more grounded, realworld intelligence. Multimodal Spatial Reasoning in 3D Vision. Despite progress, current 3D MLLMs face challenges in scalability and interpretability due to the inherent complexity of 3D data. Additionally, the scarcity of large-scale annotated 3D datasets constrains the development of robust models. To address these challenges, future research should focus on the development of unified and efficient 3D representations that are both interpretable and scalable. Furthermore, training strategies that do not rely on large-scale annotated datasets, such as leveraging synthetic data, could offer valuable insights. By exploring the integration of symbolic reasoning into the 3D domain, researchers can ensure better handling of spatial relationships and improve model performance across unseen environments. key goal should be creating frameworks that combine efficient 3D learning with strong temporal and causal reasoning capabilities to model dynamic spatial environments. Multimodal Spatial Reasoning in Embodied AI. Current methods for spatial reasoning in embodied AI often struggle to generalize to novel environments and are prone to spurious or hallucinated spatial inferences. Explicit reasoning modules, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 24 while improving inter-pretability, tend to increase inference overhead and still fall short in maintaining long-term spatial consistency. To advance this field, future research must focus on closer integration between perception and reasoning, ensuring that spatial models maintain both geometric fidelity and temporal consistency. Additionally, creating world models that combine sensory inputs (e.g., visual, auditory, tactile) with structured scene representations could allow for more robust spatial reasoning in dynamic environments. Scalable training strategies that incorporate symbolic and structured reasoning, along with the ability to perform causal inference over time, will be crucial in achieving long-term success in this area. Multimodal Spatial Reasoning with Novel Sensors. Emerging sensor technologies such as omnidirectional cameras [70, 71, 258], event cameras, LiDAR, thermal, and radar sensors offer complementary spatial information under challenging conditions like adverse lighting, weather, and high-speed motion [259]. However, these sensors introduce new challenges, including equirectangular distortions, orientation ambiguities, sparse and asynchronous data, and noise in radar and thermal signals. MLLMs, which are typically optimized for perspective RGB images, must evolve to effectively integrate and process these diverse modalities. Future research should focus on developing methods for fusing these heterogeneous sensor data into unified spatial representation [260], improving both the accuracy and robustness of spatial reasoning. By incorporating causal and temporal reasoning capabilities into sensor fusion, models can better handle dynamic environments and make more informed, context-aware decisions [261]. Moreover, training strategies that leverage both synthetic and real-world sensor data could enhance model generalization across different sensor modalities. Multimodal Spatial Reasoning Benchmarks. Existing benchmarks are limited in their scope, often suffering from issues such as orientation under-specification, narrow modality coverage, and restricted interaction. To address these limitations, future work should focus on developing more comprehensive benchmarks that span wider range of modalities and interaction settings. This includes constructing benchmarks that synchronize vision, depth, point clouds, panoramic views, spatial audio, inertial signals, and topological maps, all within unified coordinate frame with explicit orientation and reference-frame labels. Future benchmarks should also focus on evaluating MLLMs ability to perform tasks such as reference, navigation, inspection, and question answering in diverse environments. The development of interpretable evaluation frameworks that can assess both reasoning quality and spatial accuracy, while providing clear guidance for model incorporating improvement, will be essential. Additionally, symbolic reasoning into these benchmarks could allow for the assessment of structured spatial knowledge and enable better handling of complex real-world tasks. IX. CONCLUSION Large multimodal reasoning models have gradually emerged as promising and critical solution toward achieving spatial reasoning capabilities. In this paper, we focus on the intersection of spatial reasoning and MLLMs. Firstly, based on general spatial reasoning tasks, we systematically review and analyze the existing research from four perspectives: testtime scaling, post-training, model design, and explainability. We then extend the discussion to 3D vision tasks, including 3D visual grounding, 3D scene reasoning and question answering, and 3D generation. Beyond these fundamental tasks, we further explore spatial reasoning in embodied AI, providing reviews and discussions on vision-language navigation, embodied question answering, and related areas. Moreover, spatial reasoning tasks involving emerging modalities such as video and audio are also summarized, which are challenging but crucial to building comprehensive human-like spatial reasoning system. In addition to methodological aspects, we provide comprehensive overview of datasets and benchmarks for multimodal spatial reasoning, which constitute the indispensable support for advancements in this field. Through this systematic survey, we aim to establish solid knowledge foundation and offer new insights to this field-paving the way toward intelligent and reliable multimodal spatial reasoning systems in the era of large models. REFERENCES [1] A. Su, H. Wang, W. Ren, F. Lin, and W. Chen, Incentivizing pixel-space reasoning learning, arXiv Pixel with curiosity-driven reinforcement preprint arXiv:2505.15966, 2025. reasoner: [2] D. Liu, C. Wang, P. Gao, R. Zhang, X. Ma, Y. Meng, and Z. Wang, 3daxisprompt: Promoting the 3d grounding and reasoning in gpt-4o, Neurocomputing, vol. 637, p. 130072, 2025. [3] K. Ouyang, Y. Liu, H. Wu, Y. Liu, H. Zhou, J. Zhou, F. Meng, and X. Sun, Spacer: Reinforcing mllms in video spatial reasoning, arXiv preprint arXiv:2504.01805, 2025. [4] Y. Du, T. Fu, Z. Chen, B. Li, S. Su, Z. Zhao, and C. Wang, Vl-nav: Real-time vision-language reasoning, arXiv preprint navigation with spatial arXiv:2502.00931, 2025. [5] W. Feng, W. Zhu, T.-j. Fu, V. Jampani, A. Akula, X. He, S. Basu, X. E. Wang, and W. Y. Wang, Layoutgpt: Compositional visual planning and generation with large language models, Advances in Neural Information Processing Systems, vol. 36, pp. 18 22518 250, 2023. [6] G. R. Team, S. Abeyruwan, J. Ainslie, J.-B. Alayrac, M. G. Arenas, T. Armstrong, A. Balakrishna, R. Baruch, M. Bauza, M. Blokzijl et al., Gemini robotics: Bringing ai into the physical world, arXiv preprint arXiv:2503.20020, 2025. [7] F. Shiri, X.-Y. Guo, M. Far, X. Yu, R. Haf, and Y.-F. Li, An empirical analysis on spatial reasoning capabilities of large multimodal models, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024, pp. 21 44021 455. [8] J. Yang, S. Yang, A. W. Gupta, R. Han, L. Fei-Fei, and S. Xie, Thinking in space: How multimodal large IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 25 language models see, remember, and recall spaces, arXiv preprint arXiv:2412.14171, 2024. ceedings of the International Conference on Learning Representations (ICLR 2024), 2024. [9] W. Wu et al., Minds eye of llms: Visualizationof-thought elicits spatial reasoning in large language models, in Advances in Neural Information Processing Systems (NeurIPS), 2024. [10] H. Wu, X. Huang, Y. Chen, Y. Zhang, Y. Wang, and W. Xie, Spatialscore: Towards unified evaluation for multimodal spatial understanding, arXiv preprint arXiv:2505.17012, 2025. [11] C. Ma, K. Lu, T.-Y. Cheng, N. Trigoni, and A. Markham, Spatialpin: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors, arXiv preprint arXiv:2403.13438, 2024. [12] Y. Wang, T. Zhou, Z. Peng, X. Li, Y. Chen, and X. Chen, Visuothink: Empowering lvlm reasoning with multimodal tree search, arXiv preprint arXiv:2504.09130, 2025. [13] I. Kabir, M. A. Reza, and S. Billah, Logic-rag: Augmenting large multimodal models with visualspatial knowledge for road scene understanding, arXiv preprint arXiv:2503.12663, 2025. [14] X. Xu et al., Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models, arXiv preprint arXiv:2505.17015, 2025. [15] B. Chen, Z. Xu, S. Kirmani, B. Ichter, D. Sadigh, L. Guibas, and F. Xia, Spatialvlm: Endowing visionlanguage models with spatial reasoning capabilities, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 14 455 14 465. [16] K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue, Video-r1: Reinforcing video reasoning in mllms, arXiv preprint arXiv:2503.21776, 2025. [17] K. Ouyang, Spatial-r1: Enhancing mllms in video spatial reasoning, arXiv preprint arXiv:2504.01805, 2025. [18] D. Wu et al., Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence, arXiv preprint arXiv:2505.23747, 2025. [19] A.-C. Cheng, H. Yin, Y. Fu, Q. Guo, R. Yang, J. Kautz, X. Wang, and S. Liu, Spatialrgpt: Grounded spatial reasoning in vision language models, arXiv preprint arXiv:2406.01584, 2024. [20] P. He, Z. Zhang, Y. Zhang, X. Zhao, and S. Peng, Spatial-ormllm: Improve spatial relation understanding in the operating room with multimodal large language model, arXiv preprint arXiv:2508.08199, 2025. [21] Y. Qi et al., Beyond semantics: Rediscovering spatial awareness in vision-language models, arXiv preprint arXiv:2503.17349, 2025. [22] S. Chen et al., Why is spatial reasoning hard for vlms? an attention mechanism perspective on focus areas, arXiv preprint arXiv:2503.01773, 2025. [23] C. Wen, D. Jayaraman, and Y. Gao, Can transformers capture spatial relations between objects? in Pro- [24] J. Yang, X. Chen, S. Qian, N. Madaan, M. Iyengar, D. F. Fouhey, and J. Chai, Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 76947701. [25] Y. Chen, S. Yang, H. Huang, T. Wang, R. Xu, R. Lyu, D. Lin, and J. Pang, Grounded 3d-llm with referent tokens, arXiv preprint arXiv:2405.10370, 2024. [26] R. Xu, Z. Huang, T. Wang, Y. Chen, J. Pang, and D. Lin, Vlm-grounder: vlm agent for zero-shot 3d visual grounding, arXiv preprint arXiv:2410.13860, 2024. [27] R. Li, S. Li, L. Kong, X. Yang, and J. Liang, Seeground: See and ground for zero-shot open-vocabulary 3d visual grounding, arXiv preprint arXiv:2412.04383, 2024. [28] Z. Liu, Y. Wang, S. Zheng, T. Pan, L. Liang, Y. Fu, and X. Xue, Reasongrounder: Lvlm-guided hierarchical feature splatting for open-vocabulary 3d visual grounding and reasoning, arXiv preprint arXiv:2503.23297, 2025. [29] C. Zhu, T. Wang, W. Zhang, J. Pang, and X. Liu, Llava3d: simple yet effective pathway to empowering lmms with 3d-awareness, arXiv preprint arXiv:2409.18125, 2024. [30] T. Zemskova and D. Yudin, 3dgraphllm: Combining semantic graphs and large language models for 3d scene understanding, arXiv preprint arXiv:2412.18450, 2024. [31] S. Zhang, D. Huang, J. Deng, S. Tang, W. Ouyang, T. He, and Y. Zhang, Agent3d-zero: An agent for zero-shot 3d understanding, in European Conference on Computer Vision. Springer, 2024, pp. 186202. [32] J. Zhou, X. Li, L. Qi, and M.-H. Yang, Layout-your3d: Controllable and precise 3d generation with 2d blueprint, arXiv preprint arXiv:2410.15391, 2024. [33] C. Sun, J. Han, W. Deng, X. Wang, Z. Qin, and S. Gould, 3d-gpt: Procedural 3d modeling with large language models, arXiv preprint arXiv:2310.12945, 2023. [34] D. Rukhovich, E. Dupont, D. Mallis, K. Cherenkova, A. Kacem, and D. Aouada, Cad-recode: Reverse engineering cad code from point clouds, arXiv preprint arXiv:2412.14042, 2024. [35] Y. Zhang, Z. Xu, Y. Shen, P. Kordjamshidi, and L. Huang, Spartun3d: Situated spatial understanding of 3d world in large language models, arXiv preprint arXiv:2410.03878, 2024. [36] H. Hong, Y. Qiao, S. Wang, J. Liu, and Q. Wu, General scene adaptation for vision-and-language navigation, arXiv preprint arXiv:2501.17403, 2025. [37] Y. Kong, D. Song, J. Liang, D. Manocha, Z. Yao, and X. Xiao, Autospatial: Visual-language reasoning for social robot navigation through efficient spatial reasoning learning, arXiv preprint arXiv:2503.07557, 2025. [38] S. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 26 H. Zhu, J. Fan, and T. Chen, Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 42826 438. [39] Z. Yin, C. Cheng et al., Navigation with vlm framework: Go to any language, arXiv preprint arXiv:2410.02787, 2024. [40] B. Lin, Y. Nie, Z. Wei, J. Chen, S. Ma, J. Han, H. Xu, X. Chang, and X. Liang, Navcot: Boosting llm-based vision-and-language navigation via learning disentangled reasoning, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [41] A. Majumdar, A. Ajay, X. Zhang, P. Putta, S. Yenamandra, M. Henaff, S. Silwal, P. Mcvay, O. Maksymets, S. Arnaud et al., Openeqa: Embodied question answering in the era of foundation models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 16 48816 498. [42] Y. Hao, F. Yang, N. Fang, and Y.-S. Liu, Embosr: Embodied spatial reasoning for enhanced situated question answering in 3d scenes, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024, pp. 98119816. [43] Y. Qian, X. Zhu, O. Biza, S. Jiang, L. Zhao, H. Huang, Y. Qi, and R. Platt, Thinkgrasp: vision-language system for strategic part grasping in clutter, arXiv preprint arXiv:2407.11298, 2024. [44] R. Jiao, A. Fasoli, F. Giuliari, M. Bortolon, S. Povoli, G. Mei, Y. Wang, and F. Poiesi, Free-form languagebased robotic reasoning and grasping, arXiv preprint arXiv:2503.13082, 2025. [45] H. Zhen, X. Qiu, P. Chen, J. Yang, X. Yan, Y. Du, Y. Hong, and C. Gan, 3d-vla: 3d vision-languageaction generative world model, in International Conference on Machine Learning. PMLR, 2024, pp. 61 22961 245. [46] P. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai et al., pi 0.5: vision-language-action model with open-world generalization, arXiv preprint arXiv:2504.16054, 2025. [47] Z. Zhou, Y. Zhu, J. Wen, C. Shen, and Y. Xu, Visionlanguage-action model with open-world embodied reasoning from pretrained knowledge, arXiv preprint arXiv:2505.21906, 2025. [48] H. Zhen, Q. Sun, H. Zhang, J. Li, S. Zhou, Y. Du, and C. Gan, Tesseract: learning 4d embodied world models, arXiv preprint arXiv:2504.20995, 2025. [49] X. Chi, H. Zhang, C.-K. Fan, X. Qi, R. Zhang, A. Chen, C.-m. Chan, W. Xue, W. Luo, S. Zhang et al., Eva: An embodied world model for future video anticipation, arXiv preprint arXiv:2410.15461, 2024. [50] Z. Cheng, S. Leng, H. Zhang, Y. Xin, X. Li, G. Chen, Y. Zhu, W. Zhang, Z. Luo, D. Zhao, and L. Bing, Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms, arXiv preprint arXiv:2406.07476, 2024. [51] R. Liao, M. Erler, H. Wang, G. Zhai, G. Zhang, Y. Ma, and V. Tresp, Videoinsta: Zero-shot long video understanding via informative spatial-temporal reasoning with llms, in Findings of the Association for Computational Linguistics: EMNLP 2024, 2024, pp. 65776602. [52] K. Shimada, A. Politis, P. Sudarsanam, D. A. Krause, K. Uchida, S. Adavanne, A. Hakala, Y. Koyama, N. Takahashi, S. Takahashi et al., Starss23: An audiovisual dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events, Advances in neural information processing systems, vol. 36, pp. 72 93172 957, 2023. [53] Z. Zheng, P. Peng, Z. Ma, X. Chen, E. Choi, and D. Harwath, Bat: Learning to reason about spatial sounds with large language models, arXiv preprint arXiv:2402.01591, 2024. [54] W. Wang, A. Nie, W. Zhou, Y. Kai, and C. Hu, Teaching physical awareness to llms through sounds, arXiv preprint arXiv:2506.08524, 2025. [55] M. Chen, Z. Cui, X. Liu, J. Xiang, C. Zheng, J. Li, and E. Shlizerman, Savvy: Spatial awareness via audiovisual llms through seeing and hearing, arXiv preprint arXiv:2506.05414, 2025. [56] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [57] J. Zha, Y. Fan, X. Yang, C. Gao, and X. Chen, How to enable llm with 3d capacity? survey of spatial reasoning in llm, arXiv preprint arXiv:2504.05786, 2025. [58] Q. Ma, R. Yang, B. Ren, N. Sebe, E. Konukoglu, L. Van Gool, and D. P. Paudel, Cityloc: 6dof pose distributional localization for text descriptions in large-scale scenes with gaussian representation, arXiv preprint arXiv:2501.08982, 2025. [59] X. Zheng, C. Liao, Y. Fu, K. Lei, Y. Lyu, L. Jiang, B. Ren, J. Chen, J. Wang, C. Li et al., Mllms are deeply affected by modality bias, arXiv preprint arXiv:2505.18657, 2025. [60] Z. Wu, T. Liu, L. Luo, Z. Zhong, J. Chen, H. Xiao, C. Hou, H. Lou, Y. Chen, R. Yang et al., Mars: An instance-aware, modular and realistic simulator for autonomous driving, in CAAI International Conference on Artificial Intelligence. Springer, 2023, pp. 315. [61] Y. Fu, R. Wang, Y. Fu, D. P. Paudel, X. Huang, and L. Van Gool, Objectrelator: Enabling cross-view object relation understanding in ego-centric and exo-centric videos, ICCV, 2025. [62] Y. Li, Q. Ma, R. Yang, H. Li, M. Ma, B. Ren, N. Popovic, N. Sebe, E. Konukoglu, T. Gevers et al., Scenesplat: Gaussian splatting-based scene understanding with vision-language pretraining, in ICCV, 2025. [63] T. Brodermann, C. Sakaridis, Y. Fu, and L. Van Gool, Cafuser: Condition-aware multimodal fusion for robust semantic perception of driving scenes, IEEE Robotics and Automation Letters, 2025. [64] M. Ma, Q. Ma, Y. Li, J. Cheng, R. Yang, B. Ren, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 27 N. Popovic, M. Wei, N. Sebe, L. Van Gool et al., Scenesplat++: large dataset and comprehensive language gaussian splatting, arXiv benchmark for preprint arXiv:2506.08710, 2025. [65] Y. Lyu, X. Zheng, J. Zhou, and L. Wang, Unibind: Llm-augmented unified and balanced representation the space to bind them all, in Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 75226 762. [66] X. Zheng, Z. Weng, Y. Lyu, L. Jiang, H. Xue, B. Ren, D. Paudel, N. Sebe, L. Van Gool, and X. Hu, Retrieval augmented generation and understanding in vision: survey and new outlook, arXiv preprint arXiv:2503.18016, 2025. [67] J. Zhou, X. Zheng, Y. Lyu, and L. Wang, Eventbind: Learning unified representation to bind them all for event-based open-world understanding, in European Conference on Computer Vision. Springer, 2024, pp. 477494. [68] X. Zheng, Y. Lyu, and L. Wang, Learning modalityagnostic representation for semantic segmentation from any modalities, in European Conference on Computer Vision. Springer, 2024, pp. 146165. [69] Y. Lyu, X. Zheng, D. Kim, and L. Wang, Omnibind: Teach to build unequal-scale modality interaction for omni-bind of all, arXiv preprint arXiv:2405.16108, 2024. [70] Z. Dongfang, X. Zheng, Z. Weng, Y. Lyu, D. P. Paudel, L. Van Gool, K. Yang, and X. Hu, Are multimodal large language models ready for omnidirectional spatial reasoning? arXiv preprint arXiv:2505.11907, 2025. [71] X. Zhang, Z. Ye, and X. Zheng, Towards omnidirectional reasoning with 360-r1: dataset, benchmark, and grpo-based method, arXiv preprint arXiv:2505.14197, 2025. [72] G. Zhou, P. Qiu, C. Chen, J. Wang, Z. Yang, J. Xu, and M. Qiu, Reinforced mllm: survey on rl-based reasoning in multimodal large language models, arXiv preprint arXiv:2504.21277, 2025. [73] C. Wang, T. Zhang, R. Hong, and J. Huang, short survey on small reasoning models: Training, inference, applications and research directions, arXiv preprint arXiv:2504.09100, 2025. [74] Z. Ke, F. Jiao, Y. Ming, X.-P. Nguyen, A. Xu, D. X. Long, M. Li, C. Qin, P. Wang, S. Savarese et al., survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems, arXiv preprint arXiv:2504.09037, 2025. [75] J. Bi, S. Liang, X. Zhou, P. Liu, J. Guo, Y. Tang, L. Song, C. Huang, G. Sun, J. He et al., Why reasoning matters? survey of advancements in multimodal reasoning (v1), arXiv preprint arXiv:2504.03151, 2025. [76] Z. Chen, S. Wang, Z. Tan, X. Fu, Z. Lei, P. Wang, H. Liu, C. Shen, and J. Li, survey of scaling in large language model reasoning, arXiv preprint arXiv:2504.02181, 2025. [77] Q. Chen, L. Qin, J. Liu, D. Peng, J. Guan, P. Wang, M. Hu, Y. Zhou, T. Gao, and W. Che, Towards reasoning era: survey of long chain-of-thought for reasoning large language models, arXiv preprint arXiv:2503.09567, 2025. [78] A. Forootani, survey on mathematical reasoning and optimization with large language models, arXiv preprint arXiv:2503.17726, 2025. [79] R. Wang, H. Wang, B. Xue, J. Pang, S. Liu, Y. Chen, J. Qiu, D. F. Wong, H. Ji, and K.-F. Wong, Harnessing the reasoning economy: survey of efficient reasoning for large language models, arXiv preprint arXiv:2503.24377, 2025. [80] Y. Liu, J. Wu, Y. He, R. Gong, J. Xia, L. Li, H. Gao, H. Chen, B. Bi, J. Zhang et al., Efficient inference for large reasoning models: survey, arXiv preprint arXiv:2503.23077, 2025. [81] X. Qu, Y. Li, Z. Su, W. Sun, J. Yan, D. Liu, G. Cui, D. Liu, S. Liang, J. He et al., survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond, arXiv preprint arXiv:2503.21614, 2025. [82] Z. Lin, Y. Gao, X. Zhao, Y. Yang, and J. Sang, Mind with eyes: from language reasoning to multimodal reasoning, arXiv preprint arXiv:2503.18071, 2025. [83] Y. Sui, Y.-N. Chuang, G. Wang, J. Zhang, T. Zhang, J. Yuan, H. Liu, A. Wen, S. Zhong, H. Chen et al., Stop overthinking: survey on efficient reasoning for large language models, arXiv preprint arXiv:2503.16419, 2025. [84] Y. Wang, S. Wu, Y. Zhang, S. Yan, Z. Liu, J. Luo, and H. Fei, Multimodal chain-of-thought reasoning: comprehensive survey, arXiv preprint arXiv:2503.12605, 2025. [85] D. Bandyopadhyay, S. Bhattacharjee, and A. Ekbal, Thinking machines: survey of llm based reasoning strategies, arXiv preprint arXiv:2503.10814, 2025. [86] X. Li, Z. Cai, S. Wang, K. Yu, and F. Chen, survey on enhancing causal reasoning ability of large language models, in Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 2025, pp. 399416. [87] Y. Yan, J. Su, J. He, F. Fu, X. Zheng, Y. Lyu, K. Wang, S. Wang, Q. Wen, and X. Hu, survey of mathematical reasoning in the era of multimodal large language model: Benchmark, method & challenges, arXiv preprint arXiv:2412.11936, 2024. [88] D. Yang, T. Liu, D. Zhang, A. Simoulin, X. Liu, Y. Cao, Z. Teng, X. Qian, G. Yang, J. Luo et al., Code to think, think to code: survey on code-enhanced reasoning and reasoning-driven code intelligence in llms, arXiv preprint arXiv:2502.19411, 2025. [89] Z.-Z. Li, D. Zhang, M.-L. Zhang, J. Zhang, Z. Liu, Y. Yao, H. Xu, J. Zheng, P.-J. Wang, X. Chen et al., From system 1 to system 2: survey of reasoning large language models, arXiv preprint arXiv:2502.17419, 2025. [90] F. Cheng, H. Li, F. Liu, R. van Rooij, K. Zhang, and Z. Lin, Empowering llms with logical reasoning: comprehensive survey, arXiv preprint arXiv:2502.15652, 2025. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 28 [91] G. Srivastava, S. Cao, and X. Wang, Towards reasoning ability of small language models, arXiv preprint arXiv:2502.11569, 2025. [92] F. Xu, Q. Hao, Z. Zong, J. Wang, Y. Zhang, J. Wang, X. Lan, J. Gong, T. Ouyang, F. Meng et al., Towards large reasoning models: survey of reinforced reasoning with large language models, arXiv preprint arXiv:2501.09686, 2025. [93] Y. Wang, W. Chen, X. Han, X. Lin, H. Zhao, Y. Liu, B. Zhai, J. Yuan, Q. You, and H. Yang, Exploring the reasoning abilities of multimodal large language models (mllms): comprehensive survey on emerging trends in multimodal reasoning, arXiv preprint arXiv:2401.06805, 2024. [94] J. Wang, Y. Ming, Z. Shi, V. Vineet, X. Wang, Y. Li, and N. Joshi, Is picture worth thousand words? delving into spatial reasoning for vision language models, in The Thirty-Eighth Annual Conference on Neural Information Processing Systems, 2024. [95] Y. Shu, B. Ren, Z. Xiong, D. P. Paudel, L. Van Gool, B. Demir, N. Sebe, and P. Rota, Earthmind: Towards multi-granular and multi-sensor earth observation with large multimodal models, arXiv preprint arXiv:2506.01667, 2025. [96] C. Li, C. Zhang, H. Zhou, N. Collier, A. Korhonen, and I. Vulic, Topviewrs: Vision-language models as top-view spatial reasoners, in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2024. [97] M. Jia, Z. Qi, S. Zhang, W. Zhang, X. Yu, J. He, H. Wang, and L. Yi, Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models, arXiv preprint arXiv:2506.03135, 2025. [98] R. Wu and D. Guo, Do large language models have spatial cognitive abilities? ACM Transactions on Intelligent Systems and Technology, 2025. [99] Y. Liao, X. Liu, C. Wang, Z. Liu, Y. Zhang, and Y. Zhu, Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models, in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2024. J. Han, L. Wang, L. Guibas, and S. Xie, Spatial understanding from videos: Structured prompts meet simulation data, arXiv preprint arXiv:2506.03642, 2025. [100] Y. Zhang, [101] Z. Zhou et al., Image-of-thought prompting for visual reasoning refinement in multimodal large language models, arXiv preprint arXiv:2405.13872, 2024. [102] F. Zhu, H. Wang, Y. Xie, J. Gu, T. Ding, J. Yang, and H. Jiang, Struct2d: perception-guided framework for spatial reasoning in large multimodal models, arXiv preprint arXiv:2506.04220, 2025. [103] P. Y. Lee, J. Je, C. Park, M. A. Uy, L. Guibas, and M. Sung, Perspective-aware reasoning in visionlanguage models via mental imagery simulation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. reasoning in visual language models through 3d reconstruction, arXiv preprint arXiv:2407.14133, 2024. [105] R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick, Zero-1-to-3: Zero-shot one image to 3d object, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 92989309. [106] L. Marsili, L. Sforza, L. Barsellotti, N. Amoroso, and A. Monaco, Visual agentic ai for spatial reasoning with dynamic api, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [107] S. Ravi, M. Chen, A. Sen, S. Shivakumar, E. Sklar, and E. Santos, Out of sight, not out of context? egocentric spatial reasoning in vlms across disjoint frames, arXiv preprint arXiv:2505.24257, 2024. [108] J. Feng, Y. Du, T. Liu, S. Guo, Y. Lin, and Y. Li, Citygpt: Empowering urban spatial cognition of large language models, arXiv preprint arXiv:2406.13948, 2024. [109] H. Li, J. Chen, Z. Wei, S. Huang, T. Hui, J. Gao, X. Wei, and S. Liu, Llava-st: multimodal large language model for fine-grained spatial-temporal understanding, arXiv preprint arXiv:2501.08282, 2025. [110] P. Wu, Y. Liu, M. Liu, and J. Shen, St-think: How multimodal large language models reason about 4d worlds from ego-centric videos, arXiv preprint arXiv:2503.12542, 2025. [111] A. Ray, J. Duan, R. Tan, D. Bashkirova, R. Hendrix, K. Ehsani, A. Kembhavi, B. A. Plummer, R. Krishna, K.-H. Zeng et al., Sat: Spatial aptitude training for multimodal language models, arXiv preprint arXiv:2412.07755, vol. 3, 2024. [112] O. Ogezi et al., Spare: Enhancing spatial reasoning in vision-language models with synthetic data, arXiv preprint arXiv:2504.20648, 2025. [113] K. Tang et al., Sparkle: Mastering basic spatial capabilities in vision language models elicits generalization to spatial reasoning, arXiv preprint arXiv:2410.16162, 2025. [114] J. Ko et al., St-vlm: Kinematic instruction tuning for spatio-temporal reasoning in vision-language models, arXiv preprint arXiv:2503.19355, 2025. [115] C. Li, W. Wu, H. Zhang, Y. Xia, S. Mao, L. Dong, I. Vulic, and F. Wei, Imagine while reasoning in space: Multimodal visualization-of-thought, arXiv preprint arXiv:2501.07542, 2025. [116] X. Liang, X. Guo, Z. Jin, W. Pan, P. Shang, D. Cai, reasoning thinking, arXiv preprint B. Lin, and J. Ye, Enhancing spatial through visual and textual arXiv:2507.20529, 2025. [117] Z. Pan et al., Metaspatial: Reinforcing 3d spatial reasoning in vlms for the metaverse, arXiv preprint arXiv:2503.18470, 2025. [118] Y. Wang et al., M2-reasoning: Empowering mllms with unified general and spatial reasoning, arXiv preprint arXiv:2507.08306, 2025. [104] Y. Meng et al., know about up! enhancing spatial [119] Z. Liao, Q. Xie, Y. Zhang, Z. Kong, H. Lu, Z. Yang, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 29 and Z. Deng, Improved visual-spatial reasoning via r1-zero-like training, arXiv preprint arXiv:2504.00883, 2025. [120] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PmLR, 2021, pp. 87488763. [121] M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt, and J. Jitsev, Reproducible scaling laws for contrastive language-image learning, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 28182829. [122] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao, Evaclip: Improved training techniques for clip at scale, arXiv preprint arXiv:2303.15389, 2023. [123] M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans, L. Beyer, Y. Xia, B. Mustafa et al., Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features, arXiv preprint arXiv:2502.14786, 2025. [124] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 11 97511 986. [125] W. Ma, L. Ye, C. de Melo, A. L. Yuille, and J. Chen, Spatialllm: compound 3d-informed design towards spatially-intelligent large multimodal models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2025. [126] Z. Zhang, X. Li, Z. Xu, W. Peng, Z. Zhou, M. Shi, and S. Huang, Mpdrive: Improving spatial understanding with marker-based prompt learning for autonomous driving, arXiv preprint arXiv:2504.00379, 2025. [127] K. Ranasinghe, S. N. Shukla, O. Poursaeed, M. S. Ryoo, and T.-Y. Lin, Learning to localize objects improves spatial reasoning in visual-llms, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 12 97712 987. [128] W. Cai, I. Ponomarenko, J. Yuan, X. Li, W. Yang, H. Dong, and B. Zhao, Spatialbot: Precise spatial understanding with vision language models, arXiv preprint arXiv:2406.13642, 2024. [129] Y. Liu, M. Ma, X. Yu, P. Ding, H. Zhao, M. Sun, S. Huang, and D. Wang, Ssr: Enhancing depth perception in vision-language models via rationale-guided spatial reasoning, arXiv preprint arXiv:2505.12448, 2025. [130] H. Zheng, B. Tian, M. Wu, Z. Tang, K. Nahrstedt, and A. G. Schwing, Spatio-temporal llm: Reasoning about environments and actions, arXiv preprint arXiv:2507.05258, 2025. [131] E. Daxberger, N. Wenzel, D. Griffiths, H. Gang, J. Lazarow, G. Kohavi, K. Kang, M. Eichner, Y. Yang, A. Dehghan, and P. Grasch, Mm-spatial: Exploring 3d spatial understanding in multimodal llms, arXiv preprint arXiv:2503.13111, 2025. [132] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, Z. Wang, R. Fergus, Y. LeCun, and S. Xie, Cambrian-1: fully open, vision-centric exploration of multimodal llms, in Advances in Neural Information Processing Systems (NeurIPS), 2024. [133] R. Rajabi et al., Towards grounded visual spatial reasoning in multi-modal vision language models, in ICLR Workshop, 2024. [134] W. Zhang, Y. Huang, Y. Xu, J. Huang, H. Zhi, S. Ren, W. Xu, and J. Zhang, Why do mllms struggle with spatial understanding? systematic analysis from data to architecture, arXiv preprint arXiv:2509.02359, 2025. [135] T.-Y. Wu, S.-Y. Huang, and Y.-C. F. Wang, Dataefficient 3d visual grounding via order-aware referring, in 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 2025, pp. 3107 3117. [136] Z. Guo, Y. Tang, R. Zhang, D. Wang, Z. Wang, B. Zhao, and X. Li, Viewrefer: Grasp the multi-view knowledge for 3d visual grounding, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 15 37215 383. [137] Z. Yuan, J. Ren, C.-M. Feng, H. Zhao, S. Cui, and Z. Li, Visual programming for zero-shot open-vocabulary 3d visual grounding, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20 62320 633. [138] B. M. Ocal, M. Tatarchenko, S. Karaoglu, and T. Gevers, Sceneteller: Language-to-3d scene generation, in European Conference on Computer Vision. Springer, 2024, pp. 362378. [139] Q. Wu, D. Iliash, D. Ritchie, M. Savva, and A. X. Chang, Diorama: Unleashing zero-shot single-view 3d scene modeling, arXiv preprint arXiv:2411.19492, 2024. [140] J. Wen, Y. Zhu, J. Li, Z. Tang, C. Shen, and F. Feng, Dexvla: Vision-language model with plug-in diffusion expert for general robot control, arXiv preprint arXiv:2502.05855, 2025. [141] R. Zheng, Y. Liang, S. Huang, J. Gao, H. Daume III, A. Kolobov, F. Huang, and J. Yang, Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies, arXiv preprint arXiv:2412.10345, 2024. [142] L. Zhao, D. Cai, L. Sheng, and D. Xu, 3dvgtransformer: Relation modeling for visual grounding the IEEE/CVF on point clouds, in Proceedings of International Conference on Computer Vision, 2021, pp. 29282937. [143] J. Huang, B. Jia, Y. Wang, Z. Zhu, X. Linghu, Q. Li, S.-C. Zhu, and S. Huang, Unveiling the mist over 3d vision-language understanding: Object-centric evaluation with chain-of-analysis, arXiv preprint arXiv:2503.22420, 2025. [144] C. Zhu, T. Wang, W. Zhang, K. Chen, and X. Liu, Scanreason: Empowering 3d visual grounding with IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 30 reasoning capabilities, in European Conference on Computer Vision. Springer, 2024, pp. 151168. [145] Y.-H. Yang, L. Piccinelli, M. Segu, S. Li, R. Huang, Y. Fu, M. Pollefeys, H. Blum, and Z. Bauer, 3dmood: Lifting 2d to 3d for monocular open-set object detection, ICCV, 2025. [146] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.- Y. Lo et al., Segment anything, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 40154026. [147] Z. Qi, Y. Fang, Z. Sun, X. Wu, T. Wu, J. Wang, D. Lin, and H. Zhao, Gpt4point: unified framework for point-language understanding and generation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 417 26 427. [148] J. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang, Q. Li, S.-C. Zhu, B. Jia, and S. Huang, An embodied generalist agent in 3d world, arXiv preprint arXiv:2311.12871, 2023. [149] R. Fu, J. Liu, X. Chen, Y. Nie, and W. Xiong, Scene-llm: Extending language model for 3d visual understanding and reasoning, arXiv preprint arXiv:2403.11401, 2024. [150] N. Zantout, H. Zhang, P. Kachana, J. Qiu, J. Zhang, and W. Wang, Sort3d: Spatial object-centric reasoning toolbox for zero-shot 3d grounding using large language models, arXiv preprint arXiv:2504.18684, 2025. [151] Z. Wang, H. Huang, Y. Zhao, Z. Zhang, and Z. Zhao, Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes, arXiv preprint arXiv:2308.08769, 2023. [152] H. Huang, Y. Chen, Z. Wang, R. Huang, R. Xu, T. Wang, L. Liu, X. Cheng, Y. Zhao, J. Pang et al., Chat-scene: Bridging 3d scene and large language models with object identifiers, arXiv preprint arXiv:2312.08168, 2023. [153] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, and C. Gan, 3d-llm: Injecting the 3d world into large language models, Advances in Neural Information Processing Systems, vol. 36, pp. 20 48220 494, 2023. [154] J. Deng, T. He, L. Jiang, T. Wang, F. Dayoub, and I. Reid, 3d-llava: Towards generalist 3d lmms transformer, arXiv preprint with omni superpoint arXiv:2501.01163, 2025. [155] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering. 2023. [156] Q. Ma, Y. Li, B. Ren, N. Sebe, E. Konukoglu, T. Gevers, L. Van Gool, and D. P. Paudel, large-scale dataset of gaussian splats and their self-supervised pretraining, in 3DV. IEEE, 2025, pp. 145155. [157] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in International conference on machine learning. PMLR, 2023, pp. 19 73019 742. [158] H. Xiong, Y. Zhuge, J. Zhu, L. Zhang, and H. Lu, 3ur-llm: An end-to-end multimodal large language model for 3d scene understanding, arXiv preprint arXiv:2501.07819, 2025. [159] Z. Li, C. Zhang, X. Wang, R. Ren, Y. Xu, R. Ma, X. Liu, and R. Wei, 3dmit: 3d multi-modal instruction tuning for scene understanding, in 2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW). IEEE, 2024, pp. 15. [160] H. Yu, W. Li, S. Wang, J. Chen, and J. Zhu, Inst3d-lmm: Instance-aware 3d scene understanding instruction tuning, arXiv preprint with multi-modal arXiv:2503.00513, 2025. [161] A. Thai, S. Peng, K. Genova, L. Guibas, and T. Funkhouser, Splattalk: 3d vqa with gaussian splatting, arXiv preprint arXiv:2503.06271, 2025. [162] Z. Qi, Z. Zhang, Y. Fang, J. Wang, and H. Zhao, from videos preprint Gpt4scene: Understand 3d scenes with arXiv:2501.01428, 2025. vision-language models, arXiv [163] L. Ling, C.-H. Lin, T.-Y. Lin, Y. Ding, Y. Zeng, Y. Sheng, Y. Ge, M.-Y. Liu, A. Bera, and Z. Li, Scenethesis: language and vision agentic framework for 3d scene generation, arXiv preprint arXiv:2505.02836, 2025. [164] Y. Yang, F.-Y. Sun, L. Weihs, E. VanderBilt, A. Herrasti, W. Han, J. Wu, N. Haber, R. Krishna, L. Liu et al., Holodeck: Language guided generation of 3d embodied ai environments, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 16 22716 237. [165] A. elen, G. Han, K. Schindler, L. Van Gool, I. Armeni, A. Obukhov, and X. Wang, I-design: Personalized llm interior designer, in European Conference on Computer Vision. Springer, 2025, pp. 217234. [166] Q. He, K. Lin, S. Chen, A. Hu, and Q. Jin, Thinkprogram-rectify: 3d situated reasoning with large language models, arXiv preprint arXiv:2404.14705, 2024. [167] L. Jiang, R. Ji, and L. Zhang, Sdf-3dgan: 3d object generative method based on implicit signed distance function, arXiv preprint arXiv:2303.06821, 2023. [168] L. Jiang, J. Lin, K. Chen, W. Ge, X. Yang, Y. Jiang, Y. Lyu, X. Zheng, Y. Li, and Y. Chen, Dimer: Disentangled mesh reconstruction model, arXiv preprint arXiv:2504.17670, 2025. [169] L. Jiang, H. Li, and L. Wang, general framework to boost 3d gs initialization for text-to-3d generation by lexical richness, in Proceedings of the 32nd ACM International Conference on Multimedia, 2024, pp. 6803 6812. [170] L. Jiang, X. Zheng, Y. Lyu, J. Zhou, and L. Wang, Brightdreamer: Generic 3d gaussian generative frametext-to-3d synthesis, arXiv preprint fast work for arXiv:2403.11273, 2024. [171] T. Hua, L. Jiang, Y.-C. Chen, and W. Zhao, Sat2city: 3d city generation from single satellite image with cascaded latent diffusion, arXiv preprint arXiv:2507.04403, 2025. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 31 [172] Y. Sasazawa and Y. Sogawa, Layout generation agents with large language models, arXiv preprint arXiv:2405.08037, 2024. [173] Y. Yang, J. Lu, Z. Zhao, Z. Luo, J. J. Yu, V. Sanchez, and F. Zheng, Llplace: The 3d indoor scene layout generation and editing via large language model, arXiv preprint arXiv:2406.03866, 2024. [174] C. Wang, H. Zhong, M. Chai, M. He, D. Chen, Interactive 3d furnillm, arXiv preprint and J. Liao, Chat2layout: ture layout with multimodal arXiv:2407.21333, 2024. [175] Cursor, Cursor: The ai-powered code editor, 2023, [Online]. Available: https:// accessed: 2025-06-19. www.cursor.so/ [176] GitHub, Github copilot, 2021, accessed: 2025-06-19. [Online]. Available: https://copilot.github.com/ [177] V. Kumaran, J. Rowe, B. Mott, and J. Lester, Scenecraft: automating interactive narrative scene generation in digital games with large language models, in Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, vol. 19, no. 1, 2023, pp. 8696. [178] H. I. I. Tam, H. I. D. Pun, A. T. Wang, A. X. Chang, and M. Savva, Scenemotifcoder: Exampledriven visual program learning for generating 3d object arrangements, arXiv preprint arXiv:2408.02211, 2024. [179] S. Wang, C. Chen, X. Le, Q. Xu, L. Xu, Y. Zhang, and J. Yang, Cad-gpt: Synthesising cad construction sequence with spatial reasoning-enhanced multimodal llms, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 8, 2025, pp. 78807888. [180] X. Wang, J. Zheng, Y. Hu, H. Zhu, Q. Yu, and Z. Zhou, From 2d cad drawings to 3d parametric models: vision-language approach, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 8, 2025, pp. 79617969. [181] J. Li, W. Ma, X. Li, Y. Lou, G. Zhou, and X. Zhou, Cad-llama: leveraging large language models for computer-aided design parametric 3d model generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 18 56318 573. [182] R. K. Jones, P. Guerrero, N. J. Mitra, and D. Ritchie, Shapelib: designing library of procedural 3d shape abstractions with large language models, arXiv preprint arXiv:2502.08884, 2025. [183] Z. Zhou, Y. Zhu, M. Zhu, J. Wen, N. Liu, Z. Xu, W. Meng, R. Cheng, Y. Peng, C. Shen et al., Chatvla: Unified multimodal understanding and robot control with vision-language-action model, arXiv preprint arXiv:2502.14420, 2025. [184] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang et al., Habitat-matterport 3d dataset (hm3d): 1000 largescale 3d environments for embodied ai, arXiv preprint arXiv:2109.08238, 2021. M. Du et al., Bridgedata v2: dataset for robot learning at scale, in Conference on Robot Learning. PMLR, 2023, pp. 17231736. [186] X. Zheng, C. Liao, Z. Weng, K. Lei, Z. Dongfang, H. He, Y. Lyu, L. Jiang, L. Qi, L. Chen et al., Panorama: The rise of omnidirectional vision in the embodied ai era, arXiv preprint arXiv:2509.12989, 2025. [187] H. Gardner and T. Hatch, Educational implications of the theory of multiple intelligences, Educational researcher, vol. 18, no. 8, pp. 410, 1989. [188] A. Kamath, J. Hessel, and K.-W. Chang, Whats up with vision-language models? investigating their reasoning, arXiv preprint struggle with spatial arXiv:2310.19785, 2023. [189] K. Li, Q. Xu, T. Qian, Y. Fu, Y. Jiao, and X. Wang, Clivis: Unleashing cognitive map through linguisticvisual synergy for embodied visual reasoning, arXiv preprint arXiv:2506.17629, 2025. [190] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. P. Foster, P. R. Sanketi, Q. Vuong et al., Openvla: An open-source visionlanguage-action model, in 8th Annual Conference on Robot Learning, 2024. [191] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, Ï€0: vision-language-action B. flow model for general robot control, arXiv preprint arXiv:2410.24164, 2024. et al., Ichter [192] L. X. Shi, B. Ichter, M. Equi, L. Ke, K. Pertsch, Q. Vuong, J. Tanner, A. Walling, H. Wang, N. Fusai et al., Hi robot: Open-ended instruction following with hierarchical vision-language-action models, arXiv preprint arXiv:2502.19417, 2025. [193] C. Li, J. Wen, Y. Peng, Y. Peng, F. Feng, and Y. Zhu, Pointvla: Injecting the 3d world into vision-languageaction models, arXiv preprint arXiv:2503.07511, 2025. [194] D. Qu, H. Song, Q. Chen, Y. Yao, X. Ye, Y. Ding, Z. Wang, J. Gu, B. Zhao, D. Wang et al., Spatialvla: Exploring spatial representations for visual-languageaction model, arXiv preprint arXiv:2501.15830, 2025. [195] P. Li, Y. Chen, H. Wu, X. Ma, X. Wu, Y. Huang, L. Wang, T. Kong, and T. Tan, Bridgevla: Inputoutput alignment for efficient 3d manipulation learning with vision-language models, arXiv preprint arXiv:2506.07961, 2025. [196] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in Conference on Robot Learning. PMLR, 2023, pp. 21652183. [197] J. Yang, R. Tan, Q. Wu, R. Zheng, B. Peng, Y. Liang, Y. Gu, M. Cai, S. Ye, J. Jang et al., Magma: foundation model for multimodal ai agents, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 14 20314 214. [185] H. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng, P. Hansen-Estruch, A. W. He, V. Myers, M. J. Kim, [198] M. Zawalski, W. Chen, K. Pertsch, O. Mees, C. Finn, and S. Levine, Robotic control via embodied chain-ofIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 2020 32 thought reasoning, in 8th Annual Conference on Robot Learning, 2024. [199] J. Li, Y. Zhu, Z. Tang, J. Wen, M. Zhu, X. Liu, C. Li, R. Cheng, Y. Peng, and F. Feng, Improving visionlanguage-action models via chain-of-affordance, arXiv preprint arXiv:2412.20451, 2024. [200] S. Nasiriany, S. Kirmani, T. Ding, L. Smith, Y. Zhu, D. Driess, D. Sadigh, and T. Xiao, Rt-affordance: Affordances are versatile intermediate representations for robot manipulation, in 1st Workshop on X-Embodiment Robot Learning, 2024. [201] S. Karamcheti, S. Nair, A. Balakrishna, P. Liang, T. Kollar, and D. Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models. [202] L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello et al., Paligemma: transfer, arXiv preprint versatile 3b vlm for arXiv:2407.07726, 2024. [203] M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann et al., Phi-4 technical report, arXiv preprint arXiv:2412.08905, 2024. [204] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al., Qwen2vl: Enhancing vision-language models perception the world at any resolution, arXiv preprint of arXiv:2409.12191, 2024. [205] S. K. Ramakrishnan, E. Wijmans, P. Kraehenbuehl, and V. Koltun, Does spatial cognition emerge in frontier models? arXiv preprint arXiv:2410.06468, 2024. [206] Y. Zhang, Z. Ma, J. Li, Y. Qiao, Z. Wang, J. Chai, Q. Wu, M. Bansal, and P. Kordjamshidi, Visionand-language navigation today and tomorrow: survey in the era of foundation models, arXiv preprint arXiv:2407.07035, 2024. [207] Q. Gu, A. Kuwajerwala, S. Morin, K. M. Jatavallabhula, B. Sen, A. Agarwal, C. Rivera, W. Paul, K. Ellis, R. Chellappa et al., Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 50215028. [208] D. Zheng, S. Huang, L. Zhao, Y. Zhong, and L. Wang, Towards learning generalist model for embodied navigation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13 62413 634. [209] Z. Wang and G. H. Lee, g3d-lf: Generalizable 3dlanguage feature fields for embodied tasks, arXiv preprint arXiv:2411.17030, 2024. [210] Y. Yang, H. Yang, J. Zhou, P. Chen, H. Zhang, Y. Du, and C. Gan, 3d-mem: 3d scene memory for embodied exploration and reasoning, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 17 29417 303. [211] W. Yuan, J. Duan, V. Blukis, W. Pumacay, R. Krishna, A. Murali, A. Mousavian, and D. Fox, Robopoint: vision-language model for spatial affordance prediction for robotics, arXiv preprint arXiv:2406.10721, 2024. [212] Y. Liu, D. Chi, S. Wu, Z. Zhang, Y. Hu, L. Zhang, Y. Zhang, S. Wu, T. Cao, G. Huang et al., Spatialcot: Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning, arXiv preprint arXiv:2501.10074, 2025. [213] S. Zhang, Y. Qiao, Q. Wang, L. Guo, Z. Wei, and J. Liu, Flexvln: Flexible adaptation for diverse vision-and-language navigation tasks, arXiv preprint arXiv:2503.13966, 2025. [214] L. Zhang, X. Hao, Y. Tang, H. Fu, X. Zheng, P. Wang, Z. Wang, W. Ding, and S. Zhang, NavaË†3: Understanding any instruction, navigating anywhere, finding anything, arXiv preprint arXiv:2508.04598, 2025. [215] L. Zhong, C. Gao, Z. Ding, Y. Liao, H. Ma, S. Zhang, X. Zhou, and S. Liu, Topv-nav: Unlocking the topview spatial reasoning potential of mllm for zero-shot object navigation, arXiv preprint arXiv:2411.16425, 2024. [216] L. Ling and B. Qianqian, Endowing embodied agents with spatial for vision-andlanguage navigation, arXiv preprint arXiv:2504.08806, 2025. reasoning capabilities [217] S. Tan, M. Ge, D. Guo, H. Liu, and F. Sun, Knowledgebased embodied question answering, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 10, pp. 11 94811 960, 2023. [218] B. Zhao, Z. Wang, J. Fang, C. Gao, F. Man, J. Cui, X. Wang, X. Chen, Y. Li, and W. Zhu, Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning, arXiv preprint arXiv:2504.12680, 2025. [219] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, Embodied question answering, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 110. [220] Y. Tang, S. Zhang, X. Hao, P. Wang, J. Wu, Z. Wang, and S. Zhang, Affordgrasp: In-context affordance reasoning for open-vocabulary task-oriented grasping in clutter, arXiv preprint arXiv:2503.00778, 2025. [221] X. Guo, H. Hu, C. Song, J. Chen, Z. Zhao, Y. Fu, B. Guan, and Z. Liu, Unidiffgrasp: unified framework integrating vlm reasoning and vlm-guided part diffusion for open-vocabulary constrained grasping with dual arms, arXiv preprint arXiv:2505.06832, 2025. [222] H. Zhi, P. Chen, S. Zhou, Y. Dong, Q. Wu, L. Han, and M. Tan, 3dflowaction: Learning cross-embodiment manipulation from 3d flow world model, arXiv preprint arXiv:2506.06199, 2025. [223] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [224] B. Liu, Y. Dong, Y. Wang, Y. Rao, Y. Tang, W.- C. Ma, and R. Krishna, Coarse correspondence elicit language 3d spacetime understanding in multimodal model, arXiv preprint arXiv:2408.00754, 2024. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 33 [225] A. Team, H. Zhu, Y. Wang, J. Zhou, W. Chang, Y. Zhou, Z. Li, J. Chen, C. Shen, J. Pang, and T. He, Aether: Geometric-aware unified world modeling, arXiv preprint arXiv:2503.18945, 2025. [226] Z. Cheng, J. Hu, Z. Liu, C. Si, W. Li, and S. Gong, V-star: Benchmarking video-llms on video spatiotemporal reasoning, arXiv preprint arXiv:2503.11495, 2025. [227] C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, J. Zhang, L. Lu, Z. Ma, Y. Wang et al., Can large language models understand spatial audio? arXiv preprint arXiv:2406.07914, 2024. [228] H. Yun, Y. Yu, W. Yang, K. Lee, and G. Kim, Panoavqa: Grounded audio-visual question answering on 360deg videos, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 20312041. [229] C. Wen, T. Guo, S. Zhao, W. Zou, and X. Li, Sari: Structured audio reasoning via curriculum-guided reinforcement learning, arXiv preprint arXiv:2504.15900, 2025. [230] S. Chowdhury, S. Nag, S. Dasgupta, J. Chen, M. Elhoseiny, R. Gao, and D. Manocha, Meerkat: Audio-visual large language model for grounding in space and time, 2024. [231] Z. Xing, X. Hu, C.-W. Fu, W. Wang, J. Dai, and P.-A. Heng, EchoInk-R1: Exploring audio-visual reasoning in multimodal LLMs via reinforcement learning, arXiv preprint arXiv:2505.04623, 2025. [232] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma et al., Visual genome: Connecting language and vision using crowdsourced dense image annotations, International journal of computer vision, vol. 123, pp. 3273, 2017. [233] K. Yang, O. Russakovsky, and J. Deng, Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 20512060. [234] J. Lei, L. Yu, T. L. Berg, and M. Bansal, Tvqa+: Spatio-temporal grounding for video question answering, arXiv preprint arXiv:1904.11574, 2019. [235] Y.-C. Su, S. Changpinyo, X. Chen, S. Thoppay, C.-J. Hsieh, L. Shapira, R. Soricut, H. Adam, M. Brown, M.-H. Yang et al., 2.5 visual relationship detection, Computer Vision and Image Understanding, vol. 224, p. 103557, 2022. [236] L. Parcalabescu, M. Cafagna, L. Muradjan, A. Frank, I. Calixto, and A. Gatt, Valse: task-independent benchmark language models centered on linguistic phenomena, arXiv preprint arXiv:2112.07566, 2021. vision and for [237] F. Liu, G. Emerson, and N. Collier, Visual spatial reasoning, Transactions of the Association for Computational Linguistics, vol. 11, pp. 635651, 2023. [238] A. Kamath, J. Hessel, and K.-W. Chang, Whats up with vision-language models? investigating their struggle with spatial arXiv:2310.19785, 2023. reasoning, arXiv preprint [239] X. Guo, R. Zhang, Y. Duan, Y. He, C. Zhang, S. Liu, and L. Chen, Drivemllm: benchmark for spatial understanding with multimodal large language models in autonomous driving, arXiv preprint arXiv:2411.13112, 2024. [240] J. Wang, Y. Ming, Z. Shi, V. Vineet, X. Wang, S. Li, and N. Joshi, Is picture worth thousand words? delving into spatial reasoning for vision language models, Advances in Neural Information Processing Systems, vol. 37, pp. 75 39275 421, 2024. [241] N. Rajabi and J. Kosecka, Gsr-bench: benchmark for grounded spatial reasoning evaluation via multimodal llms, arXiv preprint arXiv:2406.13246, 2024. [242] X. Wang, W. Ma, T. Zhang, C. M. de Melo, J. Chen, and A. Yuille, Pulsecheck457: diagnostic benchmark for comprehensive spatial reasoning of large multimodal models, arXiv preprint arXiv:2502.08636, 2025. [243] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024, pp. 26 29626 306. [244] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [245] X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna, Blink: Multimodal large language models can see but not perceive, in European Conference on Computer Vision. Springer, 2024, pp. 148166. [246] W. Dai, J. Li, D. Li, A. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung, and S. Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, Advances in neural information processing systems, vol. 36, pp. 49 25049 267, 2023. [247] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [248] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu et al., Llavatask transfer, arXiv preprint onevision: Easy visual arXiv:2408.03326, 2024. [249] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al., Qwen2vl: Enhancing vision-language models perception the world at any resolution, arXiv preprint of arXiv:2409.12191, 2024. [250] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [251] Q. Ye, H. Xu, J. Ye, M. Yan, A. Hu, H. Liu, Q. Qian, J. Zhang, and F. Huang, mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, in Proceedings of the ieee/cvf conference on computer vision and pattern recognition, 2024, pp. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 18, NO. 9, SEPTEMBER 34 13 04013 051. [252] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. [253] M. Xu, M. Wu, Y. Zhao, J. C. L. Li, and W. Ou, Llavaspacesgg: Visual instruct tuning for open-vocabulary scene graph generation with enhanced spatial relations, in 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 2025, pp. 6362 6372. [254] Z. Wang, Y. Dong, F. Luo, M. Ruan, Z. Cheng, C. Chen, P. Li, and Y. Liu, How do multimodal large language models handle complex multimodal reasoning? placing them in an extensible escape game, arXiv preprint arXiv:2503.10042, 2025. [255] C. Plizzari, A. Tonioni, Y. Xian, A. Kulshrestha, and F. Tombari, Omnia de egotempo: Benchmarking temporal understanding of multi-modal llms in egocentric videos, in CVPR, 2025. [256] Y. Li, Y. Fu, T. Qian, Q. Xu, S. Dai, D. P. Paudel, L. Van Gool, and X. Wang, Egocross: Benchmarking large language models for cross-domain multimodal egocentric video question answering, arXiv preprint arXiv:2508.10729, 2025. [257] Z. Wen, Y. Wang, C. Liao, B. Yang, J. Li, W. Liu, H. He, B. Feng, X. Liu, Y. Lyu et al., Ai for service: Proactive assistance with ai glasses, arXiv preprint arXiv:2510.14359, 2025. [258] X. Zhang, T. Fu, and X. Zheng, Omnidirectional spatial modeling from correlated panoramas, arXiv preprint arXiv:2509.02164, 2025. [259] C. Liao, K. Lei, X. Zheng, J. Moon, Z. Wang, Y. Wang, D. P. Paudel, L. Van Gool, and X. Hu, Benchmarking multi-modal semantic segmentation under sensor failures: Missing and noisy modality robustness, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 15761586. [260] X. Zheng, Y. Lyu, J. Zhou, and L. Wang, Centering the value of every modality: Towards efficient and resilient modality-agnostic semantic segmentation, in European Conference on Computer Vision. Springer, 2024, pp. 192212. [261] X. Zheng, Y. Lyu, L. Jiang, D. P. Paudel, L. Van Gool, and X. Hu, Reducing unimodal bias in multi-modal semantic segmentation with multi-scale functional entropy regularization, arXiv preprint arXiv:2505.06635, 2025."
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST(GZ)",
        "INSAIT, Sofia University St. Kliment Ohridski",
        "Shanghai Jiao Tong University",
        "South China University of Technology",
        "University of Pisa",
        "University of Trento"
    ]
}