{
    "paper_title": "Neurosymbolic Diffusion Models",
    "authors": [
        "Emile van Krieken",
        "Pasquale Minervini",
        "Edoardo Ponti",
        "Antonio Vergari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic reasoning to solve tasks like visual reasoning. However, standard NeSy predictors assume conditional independence between the symbols they extract, thus limiting their ability to model interactions and uncertainty - often leading to overconfident predictions and poor out-of-distribution generalisation. To overcome the limitations of the independence assumption, we introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. Our approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. Across both synthetic and real-world benchmarks - including high-dimensional visual path planning and rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among NeSy predictors and demonstrate strong calibration."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 8 3 1 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Neurosymbolic Diffusion Models",
            "content": "Emile van Krieken1 Pasquale Minervini1,2, Edoardo Ponti1, Antonio Vergari1, 1School of Informatics, University of Edinburgh 2Miniml.AI {Emile.van.Krieken, p.minervini, eponti, avergari}@ed.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic reasoning to solve tasks like visual reasoning. However, standard NeSy predictors assume conditional independence between the symbols they extract, thus limiting their ability to model interactions and uncertainty often leading to overconfident predictions and poor out-of-distribution generalisation. To overcome the limitations of the independence assumption, we introduce neurosymbolic diffusion models (NESYDMS), new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. Our approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. Across both synthetic and real-world benchmarks including high-dimensional visual path planning and rule-based autonomous driving NESYDMS achieve state-ofthe-art accuracy among NeSy predictors and demonstrate strong calibration."
        },
        {
            "title": "Introduction",
            "content": "Neurosymbolic (NeSy) methods aim to develop reliable and interpretable AI systems by augmenting neural networks with symbolic reasoning [25, 26, 73]. In particular, probabilistic neurosymbolic predictors [50, 53, 54, 76] learn neural networks that extract high-level symbols, also called concepts, from raw inputs. These concepts are latent variables used in interpretable symbolic programs to reason and predict output labels. However, recent work highlights that the reliability of NeSy predictors is not guaranteed, especially under certain common architectural choices. More specifically, in many real-world settings, NeSy predictors fail silently: they can learn the wrong concepts while achieving high accuracy on output labels [22, 27]. This issue arises when the data and program together admit multiple concept assignments that are indistinguishable [53]. How do we design NeSy predictors that handle this ambiguity? Marconato et al. [52] argued that NeSy predictors should express uncertainty over the concepts that are consistent with the data. Then, uncertainty can guide user intervention, inform trust, or trigger data acquisition when the model is uncertain [52]. However, most existing NeSy predictors cannot properly model this uncertainty, as they rely on neural networks that assume (conditional) independence between concepts [10, 76, 82]. While this assumption enables efficient probabilistic reasoning [6, 69, 76, 82], it also prevents these NeSy predictors from being aware of concept ambiguity and thus reliably generalising out-of-distribution [75]. Therefore, designing expressive, scalable and reliable NeSy predictors is an open problem. To fill this gap, we design neurosymbolic diffusion models (NESYDMS). NESYDMS are the first class of diffusion models that operate over the concepts of NeSy predictor in conjunction with symbolic programs. In theory, discrete diffusion models [9, 65] are particularly suited for NeSy predictors, as each step of their denoising process involves predicting discrete distribution that fully factorises. We use this local independence assumption to profit from the insights and machinery of classical *: Equal supervision contribution. Preprint. Under review. Figure 1: NESYDMS integrate masked diffusion models (orange boxes) with symbolic programs (blue box) to learn to predict the minimum cost path in visual path-planning task. variational posterior (Section 3.3) first obtains candidate concept c0, that represents the costs of traversing each cell of the grid. Then, we partially mask c0 using the masking process q(cs c0) to obtain masked concepts 1 2 . We feed this to the discrete diffusion models unmasking model pθ(c x, 1 2 ) to predict the unmasked concepts c0. We use the symbolic program φ, which we choose as Dijkstras algorithm, to map the predicted concepts c0 to the predicted path y0. Finally, we use gradient estimation to update the parameters of the unmasking model. Dotted arrows denote samples from distribution. NeSy predictors, while modelling concepts as dependent entities globally. In practice, designing diffusion process for NeSy predictors is highly non-trivial, as it requires dealing with symbolic program and marginalising over all possible concepts, task that is intractable in general. We show how to solve both aspects effectively by devising novel continuous-time loss function for diffusion that incorporates symbolic programs, for which training scales gracefully. Contributions. After discussing the background on NeSy predictors and (masked) diffusion models in Section 2, we (c1) introduce NESYDMS in Section 3, class of scalable NeSy predictors that model concept dependencies by formalising masked diffusion process [65]. Then in Section 3.2, we (c2) derive principled loss function for NESYDMS and present an efficient gradient estimator for training it. To derive this loss, we prove that the continuous-time losses of masked diffusion models extend to non-factorised distributions. Finally, in Section 4, we (c3) empirically show that NESYDMS are (i) both calibrated and performant on tasks from the RSBench suite of visual reasoning problems [11] while (ii) scaling beyond the state-of-the-art on the complex visual path-planning task [61]."
        },
        {
            "title": "2 Background",
            "content": "2.1 Neurosymbolic predictors We aim to learn parametrised predictive model pθ(y x) that maps high-dimensional inputs to -dimensional discrete labels [V ]Y , where each label can take value in [V ] = {1, 2, . . . }. typical (probabilistic) NeSy predictor implements pθ(y x) by first (i) using concept-extractor, i.e., neural network pθ(c x) that maps the input to C-dimensional vector of symbolic concepts [V ]C, i.e., discrete variables encoding high-level information that can take values.1 Then, (ii) the NeSy predictor maps concepts through program φ : [V ]C [V ]Y to obtain output predictions ˆy. As usual in NeSy [10, 41, 50, 76], we only assume access to training data for input-output pairs (x, y) but no labelled data for concepts c, i.e., concepts are latent variables. Formally, we define the predictor pθ(y x) by marginalising over all concepts that are consistent with the output 1For simplicity of presentation, we assume that the number of possible values is the same for both concepts and labels, but this is not necessary for the paper. 2 y, summing their probability masses: pθ(y x) := (cid:88) pθ(c x)1[φ(c) = y]. (1) The equation above is also known as computing conditional weighted model count (WMC), and it is central to several probabilistic neurosymbolic methods [6, 41, 50, 76, 82]. Example 2.1 ([62]). Consider the visual path-planning task in Fig. 1 where the task is to predict minimum cost path from the top-left corner to the bottom-right corner of the visual map x. is encoded as binary matrix, where cells traversed form path. neural network extracts concepts that represent discrete costs for each cell on the grid, then search algorithm φ(c), like Dijkstra, is used to find the shortest path according to costs c. Reasoning shortcuts. Recent work proved NeSy predictors are susceptible to reasoning shortcuts [RSs; 53], which is when model pθ(y x) learns to predict the output labels correctly given the input x, but incorrectly maps inputs to concepts c. Since we cannot catch RSs on the training data, it can dramatically harm model performance on unseen data [51]. Mitigating RSs is challenging and potentially costly [53]. However, models can be made aware of their RS by properly expressing uncertainty over all concepts that are consistent with the input-output mapping, improving reliability and generalisation [52]. Then we can, for example, deploy NeSy predictors in an active learning setting where uncertain concepts are queried for extra labelling. Example 2.2. Consider an input containing two MNIST digits that are either 0 or 1. The unseen concepts are the digits, and φ(c) returns 1 if the two digits are different, otherwise 0. neural concept extractor pθ(c x) that maps MNIST digits of 0 to 1s and MNIST digits of 1s to 0s will perfectly fit the input-output mapping. The configuration in Example 2.2 maximises Eq. 1 without learning the ground-truth concepts. Given only input-output pairs, it is not possible to distinguish this RS from the correct input-concept mapping. Instead, given ground-truth concepts = (0, 1), an RS-aware model would assign some belief to both options (0, 1) and (1, 0). Independence assumption and its limitations. Unfortunately, common architectural assumption that the vast majority of NeSy predictors make prevents RS awareness: the conditional independence of concepts given inputs [41, 76, 82]. Formally, pθ(c x) in Eq. 1 factorises as (cid:81)C i=1 pθ(ci x). NeSy predictors use this assumption to perform efficient probabilistic reasoning via WMC solvers and knowledge compilation techniques [15, 18, 60], or by developing efficient approximation algorithms [69, 76]. However, recent work proved that such models cannot simultaneously represent the relevant uncertainty over different concepts while maximising Eq. 1 [75]. To see why, consider Example 2.2, with true concepts = (0, 1). The only maximisers of Eq. 1 for the independent model are to either deterministically return (0, 1) or (1, 0) [75]. However, there is no maximiser that can simultaneously assign probability mass to both cases, meaning independent models cannot be RS-aware. To overcome this limitation, we should design NeSy predictor that can express dependencies between concepts, which we address next. 2.2 Which expressive model class for NeSy? Previous work on NeSy predictors without the independence assumption explored mixture models and their generalisation as probabilistic circuits [6, 16]. For instance, [BEARS; 52] for RS-awareness. However, this still requires (i) compiling the program into binary circuit via knowledge compilation and (ii) ensuring the probabilistic circuit is compatible with this binary circuit [79]. Therefore, scaling to programs acting on high-dimensional spaces can be challenging with these approaches [4, 76]. Alternatively, autoregressive models are common type of expressive model. However, using these in NeSy predictors based on Eq. 1 is computationally hard, as the marginalisation over concepts does not commute with autoregressive conditioning [3, 5]. Although this also holds for diffusion models, they do assume the conditional independence assumption locally at every denoising step. This local assumption is sufficient to encode global dependencies. Thus, we use masked diffusion models [65] that achieve expressiveness by iteratively unmasking discrete sample. We discuss in Section 3 how to extend their local independence assumption to realise NeSy predictors. 3 Masked diffusion models. Diffusion models encode an expressive joint distribution over concepts by defining forward process that neural network modelling reverse process will learn to invert. As our concepts are symbolic, we need diffusion process for discrete data [9]. We choose masked diffusion models (MDMs) [65, 68], type of discrete diffusion model with promising results on language modelling [58, 85] and reasoning [84]. MDMs allow us to derive principled loss using the program φ (Section 3.2) and to develop scalable approximations (Section 3.4). We first review MDMs in their vanilla form, i.e., to model an unconditional distribution over concepts, pθ(c). MDMs consider continuous time diffusion process [9, 14], where the forward process gradually masks dimensions of data point c0 into partially masked data point ct [V + 1]C at time steps [0, 1]. We extend the vocabulary size to include placeholder = + 1 for masked dimensions. The data point becomes fully masked as c1 = = [m, . . . , m] at time step 1. More formally, for 0 < 1, the forward process masks partially masked concept cs into ct with q(ct cs) = (cid:89) i=1 αt αs 1[ct = cs ] + (cid:16) 1 (cid:17) αt αs 1[ct = m], (2) where α : [0, 1] [0, 1] is strictly decreasing noising schedule with α0 = 1 and α1 = 0. q(ct cs) masks each dimension with probability 1 αt , leaving it unchanged otherwise. Importantly, once αs masked, dimension remains masked. MDMs learn to invert the forward process q(ct cs) using trained reverse process pθ(cs ct). The reverse process starts at fully masked input c1 = at time step 1, and gradually unmasks dimensions by assigning values in {1, ..., }. The reverse process pθ(cs ct) is usually parameterised with conditionally independent unmasking models pθ(c0 ct) = (cid:81)C ct) that predict completely unmasked data c0 given (partially) masked versions ct. Then, MDMs remask some dimensions using the so-called reverse posterior q(cs ct, c0 = c0) (see more details in Eq. 10 in Appendix A): i=1 pθ(c0 pθ(cs ct) := (cid:88) c0 pθ(c0 ct) q(cs ct, c0 = c0), (3) The standard loss function masks c0 partially to obtain ct, and then uses the conditionally independent unmasking model pθ(c0 ct) to attempt to reconstruct c0. This loss function requires that pθ(c0 ct) implements the carry-over unmasking assumption, meaning it should assign probability of 1 to values of previously unmasked dimensions. We provide additional background on MDMs in Appendix A. Next, we discuss how to design novel MDMs tailored for NeSy prediction."
        },
        {
            "title": "3 Neurosymbolic Diffusion Models",
            "content": "To overcome the limitations of the independence assumption haunting NeSy predictors, our neurosymbolic diffusion models (NESYDMS) use MDMs to learn an expressive distribution over concepts and labels while retaining this assumption locally, enabling scaling. To develop NESYDMS, we extend MDMs by (i) conditioning on the input x, (ii) acting on both concepts and outputs y, treating concepts as latent variables and (iii) providing differentiable feedback through the program φ. We first define this model in Section 3.1 and then derive principled loss in Section 3.2. We discuss how to optimise this loss in Sections 3.3 and 3.4, and finish by discussing inference in Section 3.5. Finally, Fig. 1 provides an overview of the loss computation of NESYDMS. 3.1 Model setup We define NESYDMS using conditionally independent unmasking model pθ(c0 ct, x) and program φ that maps concepts to outputs. We use forward processes for both the concepts q(ct cs) and the outputs q(yt ys), each defined as in Eq. 2. The concept reverse process pθ(cs ct, x) is parameterised as in Eq. 3 with conditional concept unmasking model pθ(c0 cs, x), and the output reverse process pθ(ys cs, yt, x) is parameterised by reusing the concept unmasking model: pθ(ys cs, yt, x) := (cid:88) pθ(c0 cs, x)q(ys yt, y0 = φyt(c0)). (4) pθ(ys cs, yt, x) takes the concept unmasking model and marginalises over all concepts c0 that are consistent with the partially masked output ys. To implement the carry-over unmasking assumption, 4 we use φyt to refer to variation of the program φ that always returns yt if dimension is unmasked in yt. We refer to Appendix D.1 for details. The neural network for the concept unmasking model pθ(c0 ct, x) can be readily adapted from NeSy predictors as defined in Eq. 1 by additionally conditioning the neural network pθ(c x) on the currently unmasked concepts ct. Since we do not have direct access to ground-truth concepts c0, we will use variational setup and derive lower-bound for the intractable data log-likelihood pθ(y0 x) (fully defined in Eq. 45). In particular, we use variational distribution qθ(c0 y0, x) that shares parameters θ with the MDM to approximate the posterior pθ(c0 y0, x). To implement this, we repurpose our concept unmasking model pθ(cs ct, x) with the controlled generation method from [29], which we describe in Section 3.3. We provide more details and full derivation of the log-likelihood in Appendix D.1. 3.2 Loss function We next derive NELBO for NESYDMS. Intuitively, we define the NESYDM reverse process over discrete steps, and then consider the data log-likelihood as goes to infinity, giving NELBO for continuous-time process. This NELBO will be the base for the loss function used to train NESYDMS. Theorem 3.1. Let pθ(c0 ct, x) be concept unmasking model, φ : [V ]C [V ]Y given program, qθ(c0 y0, x) variational distribution, and αt noising schedule. Then, we have that the data log-likelihood as is bounded as limT log pNESYDM (y0 x) LNESYDM, where θ LNESYDM =Et[0,1],qθ (c0x,y0),q(ctc0) (cid:34) α 1 αt (cid:124) (cid:88) log pθ(c0 = c0 ct, x) i=1 (cid:123)(cid:122) Lc: concept unmasking loss (cid:125) (5) (cid:88) i=1 + α (cid:124) (cid:88) log pθ(c0 ct, x)1[φ(c0)i = y0 ] c0 (cid:123)(cid:122) Ly: output unmasking loss (cid:125) (cid:35) H[qθ(c0 y0, x)] (cid:125) (cid:123)(cid:122) (cid:124) LH[q]: variational entropy We provide derivation of this NELBO in Appendix D.2. This NELBO has three components: The concept unmasking loss Lc is like the unmasking loss used in MDMs (Eq. 14). Since we do not have access to the ground-truth concept c0, we sample c0 from the variational distribution qθ(c0 y0, x) and ask the model to reconstruct c0 from partially masked version ct q(ct c0). The output unmasking loss Ly is sum of weighted model counts (WMC) like in Eq. 1, one for each dimension of the output y0. Unlike Eq. 1, Ly weights concepts using the concept unmasking model pθ(c0 ct, x) that is conditioned on partially masked concepts ct. Importantly, we use conditionally independent concept unmasking models, meaning we can use standard techniques in the NeSy literature to compute this loss efficiently. Appendix provides additional analysis. The variational entropy LH[q] is maximised to encourage the variational distribution to cover all concepts c0 that are consistent with the input and output y0. To derive the NELBO, we had to prove new theorem that extends the standard MDM NELBO to non-factorised unmasking models pθ(c0 ct) (Appendix C), which can be an interesting result for future MDM architectures even outside NeSy predictors. We need this result because, unlike the concept reverse process, the output reverse process pθ(ys cs, yt, x) in Eq. 47 does not factorise, and we cannot naively apply the standard MDM NELBO given in Eq. 14. 3.3 Variational posterior To compute the NESYDM NELBO, we require variational distribution qθ(c0 y0, x) to sample likely concepts c0 that are consistent with the ground-truth output y0. We achieve this by adapting the sampling algorithm described in Section 3.5 using concept unmasking model pθ(c0 ct, x) that depends on the output y0 and the program φ: qθ(c0 ct, y0, x) := pθ(c0 ct, x)1[φ(c0) = y0] Z(ct, x, y0) , (6) where Z(ct, x, y0) is normalising constant. This redefines the standard unmasking process from Eq. 3 by only considering valid c0. Unfortunately, sampling from pθ(c0 ct, x, y0) is NP-hard [33, 47]. However, if we have tractable representation of the program φ, e.g., polysize circuit as the output of knowledge compilation step [60], then we can represent qθ(c0 ct, y0, x) compactly and exactly sample from it [6]. Without access to such circuit, we can instead use relaxation of the constraint similar to [29]. Let rβ(c0 y0) = exp(β (cid:80)Y ]), where β > 0 and β approaches the hard constraint. At each step in the reverse process, we resample to approximately obtain samples from qβ θ (c0 ct, x, y0) pθ(c0 ct, x)rβ(c0 y0) [29]. This procedure may sample concepts c0 that are inconsistent with y0, but prefers samples that reconstruct more dimensions of y0. We find that reasonably large β > 10 works in our experiments. In practice, this effectively samples times from pθ(c0 ct, x) and chooses the sample that violates the fewest constraints. See Appendix F.1 for details. 1[φ(c0)i = y0 i=1 3.4 Loss optimisation and scalability Next, we describe how we optimise the NESYDM NELBO LNESYDM using gradient descent. We design gradient estimation algorithm that scales to large reasoning problems by approximating intractable computation. Note that, given samples c0, ct qθ(c0 x, y0) q(ct c0), the empirical concept unmasking loss Lc is tractable, so we only discuss how to backpropagate through the output unmasking loss Ly and the variational entropy LH[q]. Computing the output unmasking loss Ly involves computing multiple WMCs, which are #P-hard. One option is to compute each WMC exactly using circuits obtained via knowledge compilation [36, 50, 82]. However, to ensure scalability, we develop sampling-based approach that approximates the WMC gradients [69]. In particular, we use REINFORCE-based gradient estimator [56], the REINFORCE Leave-One-Out (RLOO) estimator [1, 37]. Methods like RLOO can fail for problems where the probability of getting sample c0 consistent with y0 is very low: when we only sample inconsistent concepts c0, RLOO does not provide any gradient signal. However, the output unmasking loss is subtly different, as Ly gives signal for each of the dimensions of y0 independently. This helps structure the search for consistent concepts c0 by decomposing the problem into independent subproblems [8, 76]. More precisely, given time step [0, 1], samples c0, ct qθ(c0, ct y0, x) and samples c0 pθ(c0 ct, x), we use: 1, . . . , c0 θLy α (cid:88) i=1 1 µi(S 1) (cid:88) j=1 (cid:0)1[φ(c0 )i = y0 ] µi (cid:1) θ log pθ(c0 ct, x) (7) where µi = 1 (cid:80)S j=1 1[φ(c0 )i = i ]. We provide further details in Appendix E. Maximising the variational entropy LH[q] is challenging: the variational distribution in Section 3.3 samples from conditioned version of the unmasking model where computing likelihoods, and by extension, maximising the entropy of qθ, is highly untractable. We therefore experimented with two biased approximations of this loss which sufficed for our experiments, and leave more sophisticated approximations for future work: conditional 1-step entropy: If we have access to tractable constraint circuit of φ, we can use it to compute the entropy of an independent distribution over c0 conditioned on y0 and [7, 79]. Then, we maximise the entropy over the variational distribution when performing time discretisation with single step (T = 1): H[qθ(c0 c1 = m, y0, x)] using the distribution defined in Eq. 6. unconditional 1-step entropy: Without access to tractable constraint circuit, we instead maximise the unconditional 1-step entropy H[qθ(c0 c1 = m, x)]. Furthermore, as is common in variational setups [30], we add hyperparameters that weight the contribution of each loss component Lc, Ly, and LH[q]. We found these hyperparameters critical to the performance of the model. Finally, unbiased optimisation of Lc and Ly also requires calculating the gradient through sampling c0 from the variational distribution [56, 67]. Like with the variational entropy, we found out that sidestepping this part of the gradient, which would be intractable and have high variance otherwise, simplifies optimisation and yields good performance in practice. See Appendix for additional discussion and pseudocode. Table 1: Accuracy of predicting the correct sum on MNIST Addition with = 4 and = 15 digits. Methods above the horizontal line are exact, and below are approximate. We bold the best-scoring methods in the exact and approximate categories separately. METHOD = 4 = 15 DEEPSOFTLOG [46] PLIA [21] 93.5 0.6 91.84 0. 77.1 1.6 79.00 0.73 SCALLOP [21, 41] EXAL [81] A-NESI [76] NESYDM (ours) 90.88 0.48 91.65 0.57 92.56 0.79 92.49 0.98 T/O 73.27 2.05 76.84 2.82 77.29 1.40 Table 2: NESYDM significantly scales beyond current NeSy predictors. Accuracy of predicting shortest path on visual path planning with different grid sizes. Above the horizontal line are methods predicting continuous costs, while below are approximate NeSy methods that predict discrete, binned costs. METHOD 12 12 30 30 I-MLE [59] 97.2 0.5 93.7 0.6 EXAL [81] A-NESI [76] A-NESI+RL [76] NESYDM (ours) 94.19 1.74 94.57 2.27 98.96 1.33 99.41 0.06 80.85 3.83 17.13 16.32 67.57 36.76 97.40 1.23 3.5 Sampling and Inference Next, we describe how we sample from trained NESYDMS to make predictions of given x. Exactly (y0 x) is intractable even for representations supporting computing the mode argmaxy0 pNESYDM tractable marginals [2, 80], therefore we need to approximate it. We use majority voting strategy, where we sample concepts c0 from the trained MDM, compute the output with the program φ, and take the most frequent output: θ ˆy = argmaxy (cid:88) l=1 1[φ(c0 ) = y], 1, . . . , c0 c0 pθ(c0 x, c1 = m). (8) If the concept dimension is not too large, we use the first-hitting sampler from [89] to sample from pθ(c0 x, c1 = m) exactly in steps. Otherwise, we use -step time-discretisation of the reverse process [65]. For implementation details, we refer to Appendix F. Additionally, we experimented with different majority voting strategies, which we discuss in Appendix H."
        },
        {
            "title": "4 Experiments",
            "content": "(RQ1:) Can NESYDMS scale to highWe aim to answer the following research questions: dimensional reasoning problems? and (RQ2:) Does the expressiveness of NESYDMS improve reasoning shortcut awareness compared to independent models? Since there are currently no scalable RS-aware NeSy methods, the baselines we use are separated for the two research questions. We match experimental setups of the baselines, using the same datasets and neural network architectures for fair comparison. To approximate the variational entropy (Section 3.4), we use the unconditional entropy for the experiments, as the conditional entropy is intractable. For the RSBench experiments, we tried both. We use the linear noising schedule αt = 1 for all experiments. For all experiments, we repeat runs with 10 different random seeds. In all tables, we find the best-performing methods with bold font. In particular, we bold all methods that are not statistically different from the highest-scoring method according to an unpaired one-sided Mann-Whitney test at significance level of 0.05. We provide additional experimental details in Appendix G. Code is available at https://github.com/HEmile/neurosymbolic-diffusion. 4.1 RQ1: Scalability of NESYDM To evaluate the scalability of NESYDM, we consider two NeSy benchmark tasks with high combinatorial complexity: multidigit MNIST Addition and visual path planning. We compare to current approximate NeSy methods that use the independence assumption and are not RS-aware, namely A-NeSI [77], Scallop [41], and EXAL [81]. Multidigit MNIST Addition. The input is sequence of 2 numbers of digits, and the output is the sum of the two numbers, split up into + 1 digits. The goal is to train neural network that recognises the individual digits {0, 1, . . . , 9}2N in the input from input-output examples. There are no dependencies between the digits and the problem is not affected by reasoning shortcuts, so we do not expect NESYDM to improve significantly over NeSy methods that use the independence Table 3: NESYDM is performant and RS-aware NeSy predictor as shown on several tasks from the RSBench dataset. We report relevant performance metrics for each task, and concept calibration using ECE. We underline the second-best-scoring method if there is only single statistically significant best-scoring method. The first two methods use the independence assumption. Note that SL does not support BDD-OIA. METHOD DPL [49] SL [82] BEARS [52] NESYDM (ours) DPL SL UNCOND COND ACCy ACCc ACCy,OOD ACCc,OOD ECEc,ID ECEc,OOD O ACCy ACCc - ACCy,OOD ACCc,OOD ECEc,ID ECEc,OOD MF1y MF1c ECEc 98.24 0.12 42.76 0.14 5.81 0.07 38.97 0.08 69.40 0.35 86.67 0.18 70.77 0.45 0.40 0.04 7.29 0.49 7.50 0.32 81.04 1.15 85.44 0.72 63.71 1.50 10.41 1.90 38.89 1. 99.62 0.12 42.88 0.09 0.48 0.21 38.92 0.11 70.61 0.18 87.95 0.14 97.38 0.31 0.33 0.05 0.05 0.06 7.07 0.09 82.18 1.57 86.96 1.15 99.19 0.12 43.26 0.75 6.31 1.10 39.49 1.07 36.81 0.17 37.89 2.18 92.02 3.14 0.48 0.10 1.60 2.04 9.36 2.13 28.82 2.19 26.83 1.56 60.80 0.11 19.25 0.16 16.00 0. 99.76 0.00 42.86 0.00 0.11 0.09 38.88 0.03 37.61 1.22 35.99 2.88 98.67 0.27 0.19 0.08 0.00 0.00 6.25 1.46 34.51 1.65 32.61 3.32 99.12 0.10 79.41 6.58 10.9 0.05 57.22 0.49 37.93 5.01 32.82 2.60 97.52 0.37 0.36 0.27 0.00 0.00 4.65 0.49 20.91 0.49 19.07 0.48 61.67 0.32 18.50 0.21 18.86 1. 99.10 0.10 71.16 1.77 28.31 1.14 61.84 0.89 4.62 2.71 11.17 1.35 98.27 0.44 20.33 1.33 0.02 0.04 14.25 0.76 2.62 1.16 11.17 1.35 62.63 0.53 13.77 0.51 21.72 1.83 assumption. Still, we find in Table 1 that NESYDM, which uses much more expressive model than the baselines, performs similar to the state-of-the-art approximate method A-NeSI, and is competitive with exact methods [19, 46]. Therefore, the expressivity does not come at cost of performance and scalability in traditional NeSy benchmarks. Visual path planning. We study the problem described in Example 2.1. Specifically, we train neural network to predict the correct cost ci,j at each of the grid cells. Then, we use Dijkstras algorithm to find the shortest path {0, 1}N , where yi,j = 1 if the shortest path passes through cell i, and 0 otherwise. Like other NeSy methods, we predict costs with 5-dimensional categorical variable {1, . . . , 5}N . We also compare to I-MLE, the state-of-the-art method that predicts costs as single continuous variable [59]. We find in Table 2 that NESYDM significantly outperforms all baselines on the challenging 30 30 problem, including I-MLE. This problem has combinatorial space of 5900 and is considered very challenging for NeSy and neural models [62]. On the 12 12 problem, we cannot reject the null hypothesis that NESYDM outperforms A-NeSI + RLOO, but it does have much lower variance, highlighting the reliability of our method. 4.2 RQ2: RS-awareness of NESYDM To evaluate the RS awareness of NESYDM, we use the RSBench dataset [53] of reasoning problems that cannot be disambiguated from data alone. We consider two synthetic problems and realworld task. MNIST Half and MNIST Even-Odd (MNIST E-O) are variations of MNIST Addition constructed to ensure disambiguation of concepts is impossible. They have OOD test-sets to diagnose overconfident classifiers. BDD-OIA (BDD) is self-driving task [83] where model predicts what actions car can take given dashcam image. NeSy predictors extract high-level concepts from the image and use rules to predict the allowed actions. We compare to NeSy predictors using the independence assumption, namely Semantic Loss [82] and DeepProbLog [50]2. We also compare to BEARS, an RS-aware ensemble of NeSy predictors with the independence assumption [52]. In Table 3, we find that NESYDM strikes good balance between accuracy and RS-awareness throughout the datasets. On the MNIST tasks, it attains significantly better concept accuracy than competitors, both inand out-of-distribution. Furthermore, NESYDM, especially using the conditional entropy, has much better concept calibration than both baselines using the independence assumption and RS-aware baselines. We report additional results on these datasets in Appendix and find that 2Technically, DeepProbLog can go beyond the independence assumption by changing the program, but this was not done in this experiment. 8 different majority voting strategies may improve OOD performance. On BDD-OIA, we find that NESYDM has better predictive performance on outputs than BEARS while significantly improving calibration and concept performance compared to DeepProbLog. Furthermore, we note that, unlike the baselines, NESYDM is much more scalable as highlighted in Section 4.1."
        },
        {
            "title": "5 Further related work",
            "content": "NeSy predictors. The field of NeSy predictors is primarily divided into methods using fuzzy logics [10, 17, 28, 74] and those using probabilistic logics [6, 41, 50, 76, 82]. Fuzzy methods implicitly assume form of independence between concepts, while probabilistic methods can model dependencies. Previous methods that went beyond the independence assumption mixed multiple independent distributions, like in SPL [6] and BEARS [52] which is specifically designed for RSawareness. Similarly, neurosymbolic probabilistic logic programming frameworks like DeepProbLog and Scallop [41, 50] can add helper variables to increase expressivity. However, these methods are built on exact or top-k inference, which is difficult to scale to high-dimensional reasoning problems like visual path planning. Furthermore, their expressivity is limited by the number of mixture components. Conversely, methods focussed on approximate inference to scale neurosymbolic predictors all assume independence between concepts [69, 76, 81], lacking RS-awareness. NeSy generative models. closely related topic is generating from expressive models like large language models (LLMs) and diffusion models while involving programs and constraints. For LLMs, this was studied with NeSy loss functions encoding the constraints [2, 3, 13] and with constrained decoding, for example using sequential Monte Carlo methods [40, 44, 88] and by combining the LLM with approximations using probabilistic circuits [5, 86, 87]. However, these methods adopt heuristics to steer the LLM towards constraint, for instance, by using pseudo-likelihood formulation [2, 3] or training an HMM surrogate that approximates the LLM [86, 87]. Instead, for NESYDM we formulate principled NELBO, and we do so by exploiting the local structure that diffusion models offer. Furthermore, some methods tackle constrained generation from GANs [24, 71, 72], VAEs [55], deep HMMs [70], and continuous diffusion models [31, 66]. We leave extensions of NESYDM to this generative setting to future work."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced NESYDMS, the first method to integrate masked diffusion models as the neural network extractor in neurosymbolic predictors. We show how to scale NESYDMS by using efficient probabilistic reasoning techniques on local unmasking distributions while minimising global NELBO that lower-bounds the data log-likelihood. Empirically, we show that NESYDMS position themselves as one of the best NeSy predictors available that can scale to high-dimensional reasoning problems while being RS-aware. This is crucial property for NeSy predictors deployed in real-world safety-critical applications, as they need to be well calibrated and generalise robustly. Limitations and future work. The NESYDM NELBO can be extended to incorporate additional exact inference routines if we can obtain an efficient circuit, e.g., as the tractable representation for symbolic program [60]. Otherwise, as argued in Section 3.4, our sampling-based approach relies on the ability to decompose the output into separate dimensions to ensure the search in RLOO is decomposed into independent subproblems. Together, this limits the scalability of NESYDM to tasks with either efficient circuit representations or decomposable output spaces. Understanding how to combine these two aspects, or how to automatically (and approximately) reduce different setting into one of them, is an interesting and challenging future venue. Two other areas of improvement are our approach to maximising the variational entropy, and the influence of the indirect gradient coming from sampling from the variational distribution. Finally, we believe studying how NESYDMS extend to other discrete diffusion models than masked diffusion [9] models is an interesting direction. NESYDM could even be extended to hybrid diffusion models that involve both symbolic, discrete concepts and continuous latent variables by using recent work on generating under continuous constraints [20, 38, 72]."
        },
        {
            "title": "Acknowledgements",
            "content": "Emile van Krieken was funded by ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence), EPSRC (grant no. EP/W002876/1). Pasquale Minervini was partially funded by ELIAI, EPSRC (grant no. EP/W002876/1), an industry grant from Cisco, and donation from Accenture LLP. Antonio Vergari was supported by the UNREAL: Unified Reasoning Layer for Trustworthy ML project (EP/Y023838/1) selected by the ERC and funded by UKRI EPSRC. We would like to express our gratitude to Samuele Bortolotti, Emanuele Marconato, Lennert de Smet, Adrian Javaloy, and Jaron Maene for fruitful discussions during the writing of this paper."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. CoRR, abs/2402.14740, 2024. [2] Kareem Ahmed, Catarina Belem, Padhraic Smyth, and Sameer Singh. Semantic probabilistic control of language models. arXiv preprint arXiv:2505.01954, 2025. [3] Kareem Ahmed, Kai-Wei Chang, and Guy Van den Broeck. pseudo-semantic loss for autoregressive models with logical constraints. In Thirty-Seventh Conference on Neural Information Processing Systems, 2023. [4] Kareem Ahmed, Kai-Wei Chang, and Guy Van den Broeck. Semantic strengthening of neurosymbolic learning. In International Conference on Artificial Intelligence and Statistics, pages 1025210261. PMLR, 2023. [5] Kareem Ahmed, Kai-Wei Chang, and Guy Van den Broeck. Controllable generation via locally constrained resampling. In Neurips Safe Generative AI Workshop 2024, 2024. [6] Kareem Ahmed, Stefano Teso, Kai-Wei Chang, Guy Van den Broeck, and Antonio Vergari. Semantic probabilistic layers for neuro-symbolic learning. 35:2994429959, 2022. [7] Kareem Ahmed, Eric Wang, Kai-Wei Chang, and Guy van den Broeck. Neuro-Symbolic Entropy Regularization. 2022. [8] Yaniv Aspis, Krysia Broda, Jorge Lobo, and Alessandra Russo. Embed2sym-scalable neurosymbolic reasoning via clustered embeddings. In Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning, volume 19, pages 421431, 2022. [9] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured Denoising Diffusion Models in Discrete State-Spaces, 2023. [10] Samy Badreddine, Artur dAvila Garcez, Luciano Serafini, and Michael Spranger. Logic Tensor Networks. Artificial Intelligence, 303:103649, February 2022. [11] Samuele Bortolotti, Emanuele Marconato, Tommaso Carraro, Paolo Morettin, Emile van Krieken, Antonio Vergari, Stefano Teso, and Andrea Passerini. neuro-symbolic benchmark suite for concept quality and reasoning shortcuts. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [12] Nicola Branchini and Vıctor Elvira. Generalizing self-normalized importance sampling with couplings, 2024. [13] Diego Calanzone, Stefano Teso, and Antonio Vergari. Logically consistent language models In The Thirteenth International Conference on Learning via neuro-symbolic integration. Representations, 2025. [14] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. [15] Weixin Chen, Simon Yu, Huajie Shao, Lui Sha, and Han Zhao. Neural probabilistic circuits: Enabling compositional and interpretable predictions through logical reasoning. arXiv preprint arXiv:2501.07021, 2025. [16] Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: unifying framework for tractable probabilistic models. UCLA. URL: http://starai. cs. ucla. edu/papers/ProbCirc20. pdf, page 6, 2020. [17] Alessandro Daniele, Emile van Krieken, Luciano Serafini, and Frank van Harmelen. Refining neural network predictions using background knowledge. Machine Learning, 112(9):32933331, 2023. [18] Adnan Darwiche and Pierre Marquis. Knowledge compilation: Preface. Annals of Mathematics and Artificial Intelligence, 92(5):10071011, 2024. [19] Lennert De Smet and Pedro Zuidberg Dos Martires. fast convoluted story: Scaling probabilistic inference for integer arithmetics. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [20] Lennert De Smet, Pedro Zuidberg Dos Martires, Robin Manhaeve, Giuseppe Marra, Angelika Kimmig, and Luc De Readt. Neural probabilistic logic programming in discrete-continuous domains. In Uncertainty in Artificial Intelligence, pages 529538. PMLR, 2023. [21] Lennert De Smet and Pedro Zuidberg Dos Martires. fast convoluted story: Scaling probabilistic inference for integer arithmetics. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 102456102478. Curran Associates, Inc., 2024. [22] Lauren Nicole DeLong, Yojana Gadiya, Paola Galdi, Jacques Fleuriot, and Daniel DomingoFernandez. Mars: neurosymbolic approach for interpretable drug discovery. arXiv preprint arXiv:2410.05289, 2024. [23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [24] Luca Di Liello, Pierfrancesco Ardino, Jacopo Gobbi, Paolo Morettin, Stefano Teso, and Andrea Passerini. Efficient Generation of Structured Objects with Constrained Adversarial Networks, 2020. [25] Jonathan Feldstein, Paulius Dilkas, Vaishak Belle, and Efthymia Tsamoura. Mapping the neurosymbolic ai landscape by architectures: handbook on augmenting deep learning through symbolic reasoning. arXiv preprint arXiv:2410.22077, 2024. [26] Artur dAvila Garcez and Luis Lamb. Neurosymbolic ai: The 3 rd wave. Artificial Intelligence Review, 56(11):1238712406, 2023. [27] Eleonora Giunchiglia, Mihaela Catalina Stoian, Salman Khan, Fabio Cuzzolin, and Thomas Lukasiewicz. Road-r: the autonomous driving dataset with logical requirements. Machine Learning, 112(9):32613291, 2023. [28] Eleonora Giunchiglia, Alex Tatomir, Mihaela Catalina Stoian, and Thomas Lukasiewicz. Ccn+: neuro-symbolic framework for deep learning with requirements. International Journal of Approximate Reasoning, 171:109124, 2024. [29] Wei Guo, Yuchen Zhu, Molei Tao, and Yongxin Chen. Plug-and-play controllable generation for discrete masked models, 2024. [30] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with constrained variational framework. In International conference on learning representations, 2017. [31] Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue. Symbolic music generation with non-differentiable rule guided diffusion. In Proceedings of the 41st International Conference on Machine Learning, pages 1977219797, 2024. [32] Adrian Javaloy, Maryam Meghdadi, and Isabel Valera. Mitigating modality collapse in multimodal VAEs via impartial optimization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 99389964. PMLR, 1723 Jul 2022. [33] Richard Karp, Michael Luby, and Neal Madras. Monte-carlo approximation algorithms for enumeration problems. Journal of algorithms, 10(3):429448, 1989. 11 [34] Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. arXiv:1412.6980 [cs], January 2017. [35] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational Diffusion Models. 2021. [36] Doga Kisa, Guy Van den Broeck, Arthur Choi, and Adnan Darwiche. Probabilistic sentential decision diagrams. In KR, 2014. [37] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 REINFORCE samples, get baseline for free!, 2019. [38] Leander Kurscheidt, Paolo Morettin, Roberto Sebastiani, Andrea Passerini, and Antonio Vergari. probabilistic neuro-symbolic layer for algebraic constraint satisfaction. arXiv preprint arXiv:2503.19466, 2025. [39] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. [40] Alexander Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash Mansinghka. Sequential monte carlo steering of large language models using probabilistic programs. In ICML 2023 Workshop: Sampling and Optimization in Discrete Space, 2023. [41] Ziyang Li, Jiani Huang, and Mayur Naik. Scallop: language for neurosymbolic programming. Proceedings of the ACM on Programming Languages, 7(PLDI):14631487, 2023. [42] Anji Liu, Oliver Broadrick, Mathias Niepert, and Guy Van den Broeck. Discrete copula diffusion. arXiv preprint arXiv:2410.01949, 2024. [43] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and In International Jiawei Han. On the variance of the adaptive learning rate and beyond. Conference on Learning Representations, 2020. [44] Joao Loula, Benjamin LeBrun, Li Du, Ben Lipkin, Clemente Pasti, Gabriel Grand, Tianyu Liu, Yahya Emara, Marjorie Freedman, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Alexander K. Lew, Tim Vieira, and Timothy J. ODonnell. Syntactic and semantic control of large language models via sequential monte carlo. In The Thirteenth International Conference on Learning Representations, 2025. [45] Calvin Luo. Understanding diffusion models: unified perspective. arXiv preprint arXiv:2208.11970, 2022. [46] Jaron Maene and Luc De Raedt. Soft-unification in deep probabilistic logic. Advances in Neural Information Processing Systems, 36:6080460820, 2023. [47] Jaron Maene, Vincent Derkinderen, and Luc De Raedt. On the hardness of probabilistic neurosymbolic learning. In Forty-first International Conference on Machine Learning, 2024. [48] Jaron Maene, Vincent Derkinderen, and Pedro Zuidberg Dos Martires. Klay: Accelerating arithmetic circuits for neurosymbolic ai. In The Thirteenth International Conference on Learning Representations, 2025. [49] Robin Manhaeve, Sebastijan Dumanˇcic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. DeepProbLog: Neural probabilistic logic programming. In Samy Bengio, Hanna Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal, Canada, 2018. [50] Robin Manhaeve, Sebastijan Dumanˇcic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Neural probabilistic logic programming in DeepProbLog. Artificial Intelligence, 298:103504, 2021. [51] Emanuele Marconato, Gianpaolo Bontempo, Elisa Ficarra, Simone Calderara, Andrea Passerini, and Stefano Teso. Neuro-symbolic continual learning: Knowledge, reasoning shortcuts and concept rehearsal. arXiv preprint arXiv:2302.01242, 2023. [52] Emanuele Marconato, Samuele Bortolotti, Emile van Krieken, Antonio Vergari, Andrea Passerini, and Stefano Teso. BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts. In Uncertainty in Artificial Intelligenc, February 2024. 12 [53] Emanuele Marconato, Stefano Teso, Antonio Vergari, and Andrea Passerini. Not All NeuroSymbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts. In Thirty-Seventh Conference on Neural Information Processing Systems, May 2023. [54] Giuseppe Marra, Sebastijan Dumanˇcic, Robin Manhaeve, and Luc De Raedt. From statistical relational to neurosymbolic artificial intelligence: survey. Artificial Intelligence, 328:104062, 2024. [55] Eleonora Misino, Giuseppe Marra, and Emanuele Sansone. Vael: Bridging variational autoencoders and probabilistic logic programming. Advances in Neural Information Processing Systems, 35:46674679, 2022. [56] Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. Journal of Machine Learning Research, 21:132:1132:62, 2020. [57] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated In Proceedings of the AAAI conference on artificial probabilities using bayesian binning. intelligence, volume 29, 2015. [58] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, JUN ZHOU, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. In ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, 2025. [59] Mathias Niepert, Pasquale Minervini, and Luca Franceschi. Implicit mle: backpropagating through discrete exponential family distributions. Advances in Neural Information Processing Systems, 34:1456714579, 2021. [60] Umut Oztok and Adnan Darwiche. top-down compiler for sentential decision diagrams. In IJCAI, volume 15, pages 31413148, 2015. [61] Marin Vlastelica Poganˇcic, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations, 2019. [62] Marin Vlastelica Poganˇcic, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differentiation of blackbox combinatorial solvers. In International Conference on Learning Representations, 2020. [63] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015. [64] Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62:107 136, 2006. [65] Subham Sekhar Sahoo, NYC Cornell Tech, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. 2024. [66] Davide Scassola, Sebastiano Saccani, Ginevra Carbone, and Luca Bortolussi. Conditioning score-based generative models by neuro-symbolic constraints. arXiv e-prints, pages arXiv2308, 2023. [67] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. Advances in neural information processing systems, 28, 2015. [68] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. [69] Lennert De Smet, Emanuele Sansone, and Pedro Zuidberg Dos Martires. Differentiable sampling of categorical distributions using the catlog-derivative trick. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [70] Lennert De Smet, Gabriele Venturato, Luc De Raedt, and Giuseppe Marra. Relational neurosymbolic markov models. In Toby Walsh, Julie Shah, and Zico Kolter, editors, AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 1618116189. AAAI Press, 2025. 13 [71] Mihaela Stoian, Salijona Dyrmishi, Maxime Cordy, Thomas Lukasiewicz, and Eleonora Giunchiglia. How realistic is your synthetic data? constraining deep generative models for tabular data. In The Twelfth International Conference on Learning Representations, 2024. [72] Mihaela Stoian and Eleonora Giunchiglia. Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints. In The Thirteenth International Conference on Learning Representations, 2025. [73] Frank Van Harmelen and Annette Ten Teije. boxology of design patterns for hybrid learning and reasoning systems. Journal of Web Engineering, 18(1-3):97123, 2019. [74] Emile van Krieken, Erman Acar, and Frank van Harmelen. Analyzing differentiable fuzzy logic operators. Artificial Intelligence, 302:103602, 2022. [75] Emile van Krieken, Pasquale Minervini, Edoardo Ponti, and Antonio Vergari. On the independence assumption in neurosymbolic learning. In Proceedings of the 41st International Conference on Machine Learning, pages 4907849097, 2024. [76] Emile van Krieken, Thiviyan Thanapalasingam, Jakub Tomczak, Frank Van Harmelen, and Annette Ten Teije. A-nesi: scalable approximate method for probabilistic neurosymbolic inference. Advances in Neural Information Processing Systems, 36:2458624609, 2023. [77] Emile van Krieken, Thiviyan Thanapalasingam, Jakub Tomczak, Frank van Harmelen, and Annette Ten Teije. A-NeSI: scalable approximate method for probabilistic neurosymbolic inference. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 2458624609. Curran Associates, Inc., 2023. [78] Emile van Krieken, Jakub Tomczak, and Annette Ten Teije. Storchastic: framework for general stochastic automatic differentiation. Advances in Neural Information Processing Systems, 34:75747587, 2021. [79] Antonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, and Guy Van den Broeck. compositional atlas of tractable circuit operations for probabilistic inference. Advances in Neural Information Processing Systems, 34:1318913201, 2021. [80] Antonio Vergari, Nicola Di Mauro, and Guy Van den Broeck. Tractable probabilistic models: Representations, algorithms, learning, and applications, 2019. In Tutorial at the 35th Conference on Uncertainty in Artificial Intelligence (UAI 2019). [81] Victor Verreet, Lennert De Smet, Luc De Raedt, and Emanuele Sansone. Explain, agree, learn: Scaling learning for neural probabilistic logic. arXiv e-prints, pages arXiv2408, 2024. [82] Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy den Broeck. semantic loss function for deep learning with symbolic knowledge. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 55025511, Stockholmsmassan, Stockholm Sweden, 2018. PMLR. [83] Yiran Xu, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, Tz-Ying Wu, Yunsheng Li, and Nuno Vasconcelos. Explainable object-induced action decision for autonomous vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95239532, 2020. [84] Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. In The Thirteenth International Conference on Learning Representations, 2025. [85] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. [86] Honghua Zhang, Meihua Dang, Nanyun Peng, and Guy Van den Broeck. Tractable control for autoregressive language generation. In International Conference on Machine Learning, pages 4093240945. PMLR, 2023. [87] Honghua Zhang, Po-Nien Kung, Masahiro Yoshida, Guy Van den Broeck, and Nanyun Peng. Adaptable logical control for large language models. Advances in Neural Information Processing Systems, 37:115563115587, 2024. [88] Stephen Zhao, Rob Brekelmans, Alireza Makhzani, and Roger Baker Grosse. Probabilistic inference in language models via twisted sequential monte carlo. In International Conference on Machine Learning, pages 6070460748. PMLR, 2024. 14 [89] Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024."
        },
        {
            "title": "A Additional background on masked diffusion models",
            "content": "Here, we will discuss additional background and formalisation of masked diffusion models (MDMs). This background is used to derive the NELBO of the masked diffusion model in Appendix D.2 and the loss with arbitrary joints in Appendix C. Forward process details. We first define the continuous-time forward process q(ct c0), which masks the data up to timestep [0, 1] using the forward process defined in Eq. 2. q(ct c0) = (cid:89) i=1 αt1[ct = i ] + (1 αt)1[ct = m] (9) Secondly, we need the reverse posterior q(cs ct, c0), which is the distribution of the initial state c0 given the state at timestep and the final state. Here we assume ct is either equal to the mask value or to the value of c0 , as otherwise the probability is not well-defined. The form for each case is (see [65], A.2.1) q(cs ct, c0) = (cid:89) q(cs ct i, c0 ) q(cs ct = i , c0 i=1 ) = 1[cs = c0 ] q(cs ct = m, i ) = 1 αs 1 αt 1[cs = m] + αs αt 1 αt 1[cs = c0 ] (10) (11) (12) , c0 ct = c0 equals this value. If ct ) refers to the probability of cs with probability depending on αt. and where the value of variable ct We note that q(cs conditioned on some value for the variable c0 indeed is equal to the value of c0 , the distribution deterministically returns that value. If it is masked instead, it either stays masked or turns into the value of c0 = m} refer to the dimensions that are masked in ct. Additional notation. We let Mct = {i : ct Similarly, Uct = {i : ct = m} is the set of unmasked dimensions of ct. Furthermore, we will use cs ct to denote that cs is (partial) extension of ct. This means cs agrees on all unmasked for all Uct. We will also use c0 ct to denote that dimensions of ct with ct, that is, ws c0 is complete extension that does not have any masked dimensions. Finally, we use notation such as cs Uct to index cs using the set of indices Uct, the unmasked dimensions of ct. = wt Reverse process definition. Using pθ(cs ct) (Eq. 3), we can express the intractable generative model pMDM (c0), for time discretisation , as θ pMDM θ (c0) := (cid:88) (cid:89) C{0} k=1 pθ(cs(k) ct(k)), (13) where the sum over C{0} iterates over all trajectories c1, . . . , unmasked c0, and s(k) = k1 index the timesteps. and t(k) = 1 from fully masked c1 = to Several recent papers [65, 68] proved that this model has simple negative variational lower bound (NELBO) under continuous-time process, that is, when . Given dataset of samples c0, this NELBO resembles weighted cross-entropy loss: log pMDM θ (c0) LMDM = Et[0,1],ctq(ctc0) α 1 αt (cid:88) iMct log pθ(c0 = c0 ct) . (14) = αt , q(ct c0) is computed with Eq. 9, and the cross-entropy term computes the loss on Here α ct). When using the common linear noising schedule, the factors of the unmasking model pθ(c0 then αt = 1 t, α . This bound holds when the unmasking model pθ(c0 ct) assigns 0 1αt probability to the mask value (zero masking probabilities), and assigns probability of 1 to unmasked dimensions (carry-over unmasking), i.e., for all Mct, pθ(c0 ct) = 1 [65]. = ="
        },
        {
            "title": "B Analysis of the output unmasking loss",
            "content": "Here, we will discuss the output unmasking loss Ly in more detail, and relate it to other common loss functions in the NeSy literature. In our problem setup, we assume program φ : [V ]C [V ]Y that maps concepts c0 to outputs y0. Then, we defined the WMC in Eq. 1 as the probability that some c0 maps to y0. This constraint can be understood as 1[φ(c0) = y0] = 1 (cid:35) φ(c0)i = y0 . (cid:34) (cid:94) i=1 (15) That is, we can see this setup as actually having different programs, and we want each program to return the right output. Now, disregarding the weighting and sampling, Ly is Ly = = (cid:88) i=1 (cid:88) i=1 (cid:88) log pθ(c0 ct, x)1[φ(c0)i = y0 ] log pθ(y0 = y0 ct, x) (16) (17) This loss is sum of different WMC terms, one for each of the different programs. Ly assumes, in vacuum, that these programs are independent, meaning we can sum the losses for each program independently. How could that be possible? This is actually common property of continuous-time losses of discrete diffusion models. For instance, one can observe the same in the NELBO of MDMs in Eq. 14. There, the goal is to reconstruct the (masked) dimensions of c0 independently. In fact, to perfectly fit an MDM, the goal is merely to perfectly fit each of the different conditional data marginals p(c0 ct) perfectly, without regard for any dependencies between dimensions [42]. The dependencies for the full MDM are handled by the iterative unmasking process, which changes the condition at each step. The same property holds for Ly: the dependencies between the different programs are (ideally) handled by different conditions c0 at each step. We highlight that this loss is related to existing loss functions in the NeSy literature. In particular, for programs that implement conjunctive normal forms (CNFs), this loss is equivalent to the logarithm of the product t-norm, which is common loss function in the NeSy literature [10, 74]. More precisely, if {0, 1}C model the variables of the CNF and {0, 1}Y the clauses consisting of disjunctions of literals li1 ... li,ki, then φ(c)i = (cid:87)ki j=1 lij computes the truth value of the ith clause of the CNF. Under the independence assumption, the probability that disjunction holds (that is, whether φ(c)i = 1) is pθ(yi = 1 x) = 1 ki(cid:89) (1 pθ(lij x)) j=1 (18) which is equal to the product t-conorm of the probabilities of the literals. Finally, the logarithm product t-norm takes the logarithm over the product of these probabilities, implicitly assuming these clauses are independent: LLog-product = (cid:88) i= log pθ(yi = 1 x). (19) t, this is precisely what Ly would compute for this problem Note that, outside the reweighting with α (Eq. 17). This equality between Ly and LLog-product holds only for CNFs: for general programs, the product t-norm is not equal to the probability on the output of program, unlike the disjunction case. For example, the different subprograms used in our experiments are not expressed as CNFs. Furthermore, our setup gives more flexibility even in the CNF case by allowing us to redefine what the dimensions of represent. For instance, we can remove the independence assumption between set of clauses by defining yi as the conjunction of these clauses. In that sense, it is highly related to Semantic Strengthening [4], which starts from LLog-product, and then dynamically joins clauses by building probabilistic circuit to relax the independence assumption. This idea can be directly applied to our setup, which we leave as future work."
        },
        {
            "title": "C Masked Diffusion with Arbitrary Joint Distributions",
            "content": "In this section, we will prove Theorem C.1 which states that the NELBO in Eq. 14 also holds for non-factorised unmasking models pθ(c0 ct). We use the notation introduced in Appendix and Section 2.2. During this proof, we will derive both discreteand continuous-time versions of the NELBO. In this appendix, we will use C0 to refer to c1/T , ..., c1, = . This result is related to the tractability result of [14], namely that in continuous-time process, the probability that two dimensions are unmasked at exactly the same time step in [0, 1] is 0. Theorem C.1. Let pθ(c0 ct) be any conditional joint distribution over c0 with conditional marginals pθ(ci ct) that satisfy the following assumptions for all {1, . . . , C}: and = k1 1. Zero masking probabilities: pθ(ci = ct) = 0. 2. Carry-over unmasking: Given some ct (V + 1)C, pθ(ci = ct 3. Proper prior: pθ(c1) = 1[c1 = m]. ct) = 1. Let pθ(cs ct) be the reverse process defined in Eq. 3 using pθ(c0 ct) instead of fully factorised model. Then as , log pMDM θ (c0) LMDM = Et[0,1],ctq α 1 αt (cid:88) iMct log pθ(c0 = c0 ct) . (20) Proof. We start with standard variational diffusion models derivation that closely follows those presented in [35, 45]. log pMDM θ (c0) = log pθ(C) Eq(C0c0) (cid:88) C0 (cid:20) log (cid:21) pθ(C) q(C0 c0) Now we reduce the nominator with Bayes theorem and by conditioning on c0, which is conditionally independent given cs: q(C0 c0) = q(c1/T c0) = q(c1/T c0) (cid:89) k=2 (cid:89) k=2 q(ct cs) = q(c1/T c0) (cid:89) k=2 q(ct cs, c0) q(cs ct, c0)q(ct c0) q(cs c0) = q(c1 c0) (cid:89) k=2 q(cs ct, c0), (21) where in the last step we use that the q(ct c0) and q(cs c0) cancel out in the product over t, leaving only q(ct cs, c0). Filling in Eq. 3, = Eq(C0c0) (cid:34) (cid:34) log pθ(c0 c1/T )p(c1) q(c1 c0) = Eq(C0c0) log pθ(c0 c1/T ) + log (cid:35) (cid:81)T (cid:81)T k=2 pθ(cs ct) k=2 q(cs ct, c0) p(c1) q(c1 c0) + log (cid:81)T k=2 pθ(cs ct) k=2 q(cs ct, c0) (cid:81)T (cid:35) (22) (23) (cid:34) (cid:35) = q(c1/T c0) log pθ(c0 c1/T ) + (cid:124) (cid:123)(cid:122) Lrec,T : reconstruction loss (cid:125) (cid:88) k=2 Eq(ctc0)KL[q(cs ct, c0)pθ(cs ct)] (cid:125) (cid:123)(cid:122) (cid:124) Lunm,T ,k: unmasking loss at timestep +G (24) where = Eq(c1c0) log p(c1) q(c1c0) is constant and equal to 0 if p(c1) = 1[c1 = m]. Lemma C.2. Using the assumptions of Theorem C.1, for any integer > 1, Lrec,T = q(c1/T c0)[ log pθ(c0 = c0 c1/T )] (25) 18 Proof. First note that, using Eq. 12, q(c0 c1/T = m, i ) = = 1 α0 1 α1/T 1 1 1 α1/T 1[c0 = m] + α0 α1/T 1 α1/T 1[c0 = c0 ] 0 + 1 α1/T 1 α1/T 1[c0 = i ] = 1[c0 = c0 ] since no elements of c0 are masked and α0 = 1 by definition, and so combined with Eq. 11, we get q(c0 , c0 ) = 1[c0 = i ]. Therefore, c1/T q(c1/T c0)[ log pθ(c0 c1/T )] = q(c1/T c0) log (cid:34) (cid:34) = q(c1/T c0) log (cid:88) c0 (cid:88) pθ(c0 c1/T ) pθ(c0 c1/T ) (cid:89) i= (cid:89) q(c0 c1/T (cid:35) , c0 ) (cid:35) = c0 ] 1[c0 c0 q(c1/T c0)[ log pθ(c0 = c0 c1/T )] i=1 = Where we use that the only nonzero term in the sum is when c0 = c0. Next, we focus on Lunm,T,k in Eq. 24. The standard derivation of the MDM NELBO in [65] computes the dimension-wise KL-divergence between the forward and reverse process, and then sums. This is not possible in our setting because we assume arbitrary joints for the unmasking model, and so the KL-divergence does not decompose trivially. Lemma C.3. Using the assumptions of Theorem C.1, for any integer > 1 and {2, . . . , }, Lunm,T,k = Eq(ctc0)KL[q(cs ct, c0)pθ(cs ct)] = Eq(cs,ctc0) log pθ(c0 Ucs Uct = c0 Ucs Uct ct) (26) Proof. We first consider what terms in the KL-divergence in Lunm,T,k are nonzero. First, note that ct needs to extend cs (i.e., cs ct) as otherwise q(cs ct, c0) = 0 by Eq. 11. Next, the unmasked dimensions in cs need to be consistent with c0 by Eq. 12, in other words, c0 cs. Then, the Mcs dimensions that stay unmasked get factor of 1αs , while the Mct Mcs dimensions that become 1αt unmasked get factor of αsαt . Assuming c0 ct, we have 1αt (cid:17)Mct Mcs (cid:16) 1αs 1αt (cid:17)Mcs (cid:16) αsαt 1αt q(cs ct, c0) = (27) if c0 cs ct, otherwise. 0 Filling this into the KL of Lunm,T,k, Eq(ctc0)KL[q(cs ct, c0)pθ(cs ct, c0)] =Eq(ctc0) (cid:88) q(cs ct, c0) log c0csct q(cs ct, c0) c0 pθ(c0 ct)q(cs ct, c0) (cid:80) (28) Now because of the carry-over unmasking assumption, we know that the only c0s getting positive probabilities are those that extend ct. Focusing just on the log-ratio above and using Eq. 27 and Eq. 12 we have log (cid:80) = log c0ct pθ(c0 ct) (cid:88) pθ(c0 ct) (cid:16) 1αs 1αt (cid:16) 1αs 1αt (cid:89) (cid:17)Mcs (cid:16) αsαt 1αt (cid:17)Mcs (cid:16) αsαt 1αt = c0 1[c0 (cid:17)Mct Mcs (cid:17)Mct Mcs (cid:81) ] = log (cid:88) c0cs iUcs Uct pθ(c0 ct), 1[c0 = c0 ] c0ct iUcs Uct 19 since the ratios involving αt and αs are independent of c0 and can be moved out of the sum, dividing away. Then note that (cid:81) ] also requires that c0 extends cs. = 1[c0 iUcs Uct Giving the denoising loss: Lunm,T,k = Eq(ctc0) (cid:88) q(cs ct, c0) log (cid:88) c0cs pθ(c0 ct) c0csct = Eq(cs,ctc0) log (cid:88) pθ(c0 ct) = Eq(cs,ctc0) log pθ(c0 Ucs = ct Ucs ct) Ucs Uct ct) Ucs Uct = cs c0cs = Eq(cs,ctc0) log pθ(c0 = Eq(cs,ctc0) log pθ(c0 = Eq(cs,ctc0) log pθ(c0 Uct , c0 Uct ct)pθ(c0 Uct = ct Uct = ct Ucs Uct = cs where we use the carry-over unmasking assumption twice. In Eq. 33, we use that pθ(c0 ct) = 1 because pθ(c0 must also be deterministic and return ct independent of Uct . Similarly, in Eq. 32, we use that c0 Uct has only one element. Uct = ct ct) = 1 for all Uct, and so the joint over the variables c0 Uct Uct Uct is conditionally given ct since the support of c0 Ucs Uct ct), Ucs Uct = cs Ucs Uct ct) = ct Ucs Uct (29) (30) (31) (32) (33) Combining Eq. 24 and Lemmas C.2 and C.3, we get the discrete-time loss: LMDM =E q(c 1 c0) [ log p(c0 = c0 1 )]+ (cid:88) Eq(cs,ctc0) (cid:104) log pθ(c0 Ucs Uct = cs (cid:105) Ucs Uct ct) (34) . = Ucs Uct Ucs Uct k=2 pθ(c0 ct) is the marginal probability of the newly unmasked dimensions in cs: Ucs Uct. Therefore, computing the discrete-time loss requires being able to be compute conditional marginal distributions over multiple variables. Of course, this is tractable for fully factorised distributions, in which case its just product of individual marginals [65]. This loss can be estimated by sampling pairs cs and ct, and can be further simplified depending on the form of pθ. Next, we consider LT as . We will show that this allows us to marginalise out cs, reducing the variance. We will do this by considering the two loss terms individually, and letting . Lemma C.4. Using the assumptions of Theorem C.1, lim Lrec,T = Proof. Recall that in discrete time this is equal to (see Lemma C.2) Lrec,T = log p(c0 = c0 q(c 1 c0) (35) (36) 1 ) = c0 Note that q(c1/T ) = 1 α1/T . Then, limT α1/T = limt0 αt = 1 by continuity and monotonicity of αt, giving limT q(c1/T ) = 0. Therefore, asymptotically, for all c1/T = c0, we are left with term that tends to 0 and constant term independent of , meaning the only relevant element of the sum is c1/T = c0: = c0 lim = log (cid:88) c1/T (cid:88) c0 q(c1/T c0) log (cid:88) pθ(c0 c1/T ) = lim log pθ(c0 c0) (cid:88) c0 pθ(c0 c0) = log 1 = where we use the carry-over unmasking assumption to get the last equality. Lemma C.5. Using the assumptions of Theorem C.1, lim (cid:88) k=2 Lunm,T,k = Et(0,1]q(ctc0)[ α 1 αt (cid:88) iMct 20 log pθ(c0 = i ct)] (37) (38) (39) Proof. Instead of having sum over 1 timesteps, each computing KL, we will now sample some { 2 . Then, we will weight the result by 1. Using Lemma C.3, , ..., 1}, redefining := Lunm,T lim = lim t{ 2 ,...,1} =Et(0,1]Eq(ctc0)[ Eq(ctc0)[(T 1)KL[q(cs ct, c0)pθ(cs ct)]] (40) (cid:88) c0csct lim (T 1)q(cs ct, c0) log pθ(c0 Ucs Uct = cs Ucs Uct ct)]. Assuming that c0 cs ct, recall q(cs ct, c0) = (cid:16) 1αs 1αt (cid:17)Mcs (cid:16) αsαt 1αt (cid:17)Mct Mcs , and assume at least one dimension becomes unmasked: Mct Mcs > 0. Then, using that limT we get 1αs 1αt = 1, (41) lim = lim (T 1)q(cs ct, c0) = lim (cid:19)Mct Mcs 1 (αs αt) 1 αt (cid:18) αs αt 1 αt (cid:18) 1 αs 1 αt (cid:19)Mcs (cid:18) αs αt 1 αt (cid:19)Mct Mcs (42) (43) α + O( 1 + O( 1 + O( 1 + O( 1 2 + O( 1 3 )) = α 2 ) αt) = α 2 ). Then (αs αt) = (αt 1 2 ). And so limT (αs αt)2 = limT Next, using that αt is differentiable, consider the first-order Taylor expansion of α around to evaluate αs: αs = αt 1 α ). And so limT (αs αt) = α t. Now if Mct Mcs 2, then the following term appears: (αs αt)2 = ( 1 α α2 ( α2 + O( 1 Therefore, the only cs in the sum with non-zero contribution are where Mct Mcs 1, that is, when cs unmasks at most one dimension of ct. When no dimensions are unmasked, q(cs ct, c0) = 1, and if cs unmasks one dimension, we have q(cs ct, c0) = α t. If cs does not unmask any dimensions, then there are no variables in c0 probability over in log pθ(c0 0. Next, if cs unmasks only dimension Mct such that cs ct) = pθ(c0 Therefore, to compute the ct), giving probability 1 and term equal to , then pθ(c 2 ))2 = 2 ) = 0. + O( 1 ct). = c0 = Ucs Uct Ucs Uct Ucs Uct Ucs Uct Ucs Uct = cs = cs lim (cid:88) k=2 Lunm,T,k = LMDM =Et(0,1]q(ctc0)[ α 1 αt (cid:88) iMct log pθ(c0 = c0 ct)] (44) Summing Lemmas C.4 and C.5 completes the proof of Theorem C.1. Neurosymbolic diffusion models: formal definition and NELBO derivation In this section, we will formally define derive the NELBO for neurosymbolic diffusion model. Throughout this section, we assume the same notation as in Appendix A. We refer to the graphical model in Fig. 2 to set up the model and the notation. D.1 Formal model definition First, we will define the discrete-time data log-likelihood. We let be the trajectory of partially masked concepts over timesteps c0, 1 , c1, and similarly Y{0} is the trajectory , . . . , T , 2 1 21 Figure 2: Probabilistic graphical model for neurosymbolic diffusion model. The forward process q, indicated by striped arrows, masks both concepts and outputs y. Since only y0 is observed, variational distribution qθ has to predict c0 from y0 and x. The reverse process, with regular arrows, unmasks both concepts and outputs y, transforming concepts into outputs at every time step. , 2 , . . . , y 1 bottom of Fig. 2, we define the data log-likelihood pNESYDM , y1 Marginalising out all latent variables according to the graphical model in the (y0 x) of outputs given inputs as: 1 θ pNESYDM θ (y0 x) := (cid:88) (cid:88) q(c1, y1) Y{0} (cid:89) k=1 pθ(cs(k) ct(k), x)pθ(ys(k) cs(k), yt(k), x). (45) Here, pθ(cs ct, x) and pθ(ys cs, yt, x) are defined as in Section 3.1. First, we define the conditional program φyt as the program φ that maps concepts to outputs, but always returns yt if dimension is unmasked in yt. To be precise, = = (cid:26)φ(c0)i yt φyt(c0)i = if yt if yt . (46) We need this definition in Eq. 4 to ensure the output unmasking model satisfies the carry-over unmasking assumption from Theorem C.1. Next, we define pθ(y0 cs, yt, x) := (cid:88) c0 pθ(c0 cs, x)1[φyt(c0) = y0] (47) as the output unmasking model such that the MDM defined as (cid:80) equal to pθ(ys cs, yt, x) as defined in Eq. 4: (cid:88) (cid:88) (cid:88) pθ(y0 cs, x)q(ys yt, y0) = pθ(c0 cs, x)1[φyt(c0) = y0]q(ys yt, y0) (48) y0 pθ(y0 cs, x)q(ys yt, y0) is y0 y0 (cid:88) c0 pθ(c0 cs, x)q(ys yt, φyt(c0)) = pθ(ys cs, yt, x). = c0 (49) Note that pθ(ys cs, yt, x) does not decompose into product of marginals, requiring the new results in Appendix rather than the standard MDM NELBO derivation. To be able to use Theorem C.1 and the other lemmas in Appendix C, we need to ensure that the output unmasking model satisfies the assumptions of Theorem C.1. Since φyt maps completely unmasked concepts to completely unmasked outputs, it satisfies zero masking probabilities. Further, the carry-over unmasking assumption is satisfied, since for any unmasked dimension Uyt and any concept c0, φyt(c0)i = yt cs, yt, x) = 1. Importantly, the carry-over unmasking assumption would not hold if we used φ(c0) instead of φyt(c0) in Eq. 47, and we would not have been able to use the results in Appendix C. by Eq. 46 and hence pθ(y0 = yt 22 D.2 NELBO derivation Theorem D.1. Let pθ(c0 ct, x) be concept unmasking model with zero masking probabilities and carry-over unmasking as defined in Theorem C.1, φ : [V ]C [V ]Y be given program, qθ(c0 y0, x) be variational distribution, and αt be noising schedule. Then we have that the (y0 x) LNESYDM, where data log-likelihood as is bounded as limT log pNESYDM θ LNESYDM =Et[0,1],qθ (c0,ctx,y0) (cid:34) α 1 αt (cid:88) iMct log pθ(c0 = c0 ct, x) (cid:124) (cid:123)(cid:122) Lc: Concept unmasking loss (cid:125) (cid:88) i=1 + α (cid:124) (cid:88) log pθ(c0 ct, x)1[φ(c0)i = y0 ] c0 (cid:123)(cid:122) Ly: Output unmasking loss (cid:125) (cid:35) Proof. H[qθ(c0 y0, x)] (cid:125) (cid:123)(cid:122) LH[q]: Variational entropy (cid:124) (50) log pNESYDM θ (cid:88) (cid:88) = log Y0 (y0 x) pθ(C, x) Eqθ (C,Y0y0,x) (cid:20) log pθ(C, x) qψ(C, Y0 y0, x) (cid:21) (51) Next, we again use the trick from Eq. 21, both for and Y, and we expand the model pθ following the graphical model in Fig. 2: (cid:34) = Eqθ (C,Y0y0,x) log pθ(c0, y0 1 , 1 qθ(c1, c0 x, y0)q(y1 y0) , x)p(c1, y1) (cid:81)T (cid:34) = Eqθ (c0x,y0) q(y 1 y0) (cid:104) log pθ(y0 c0, 1 , x) (cid:124) (cid:123)(cid:122) Lrec,y,T : y0 reconstruction (cid:105) (cid:125) q(c (cid:124) (cid:35) (cid:81)T k=2 pθ(cs, ys ct, yt, x) t=2 q(cs ct, c0)q(ys yt, y0) (cid:35) (cid:34) 1 c0) log pθ(c0 1 , x) qθ(c0 x, y0) (cid:123)(cid:122) Lrec,c,T : c0 reconstruction + (cid:125) (cid:88) k=2 Eq(ctc0) (cid:124) (cid:104) KL[q(cs ct, c0)pθ(cs ct, x)] (cid:125) (cid:123)(cid:122) Lunm,c,T ,k: unmasking (cid:105) + (cid:104) Eq(cs,ytc0,y0) (cid:124) KL[q(ys yt, y0)pθ(ys yt, cs, x)] (cid:125) (cid:123)(cid:122) Lunm,y,T ,k: unmasking (cid:35) (cid:105) + where = Eq(c1,y1y0,x) log y1 = m], which is true by assumption. p(c1,y1) qθ (c1,y1x,y0) is constant and equal to 0 if p(c1, y1) = 1[c1 = Discrete-time NELBO. Next, we use Lemma C.2 to rewrite Lrec,y,T and Lrec,c,T . Then, the first two terms are the reconstruction losses for y0 and c0 respectively, and the third term is the entropy of the variational distribution. Lrec,y,T + Lrec,c,T =Eqθ (c0x,y0) q(y 1 y0) (cid:34) (cid:104) log pθ(y0 = y0 c0, (cid:105) 1 , x) + (cid:104) q(c 1 c0) log pθ(c0 = c0 1 , x) (cid:105) (cid:35) + log qθ(c0 y0, x) , (52) Similarly, using Lemma C.3, we have the concept unmasking loss as (cid:88) k=2 Lunm,c,T,k = Eqθ (c0,cs,ctx,y0) (cid:104) (cid:88) k=2 log pθ(c0 Ucs Uct = cs Ucs Uct ct, x) (cid:105) (53) 23 For the output unmasking loss Lunm,y,T , again using Lemma C.3, we have (cid:88) k=2 Lunm,y,T,k = Eqθ (cs,ys,ytx,y0) (cid:104) (cid:88) k=2 log pθ(y0 Uys Uyt = ys Ucs Uct cs, yt, x) (cid:105) . (54) Summing Eqs. 52 to 54, we get the discrete-time NELBO. Continuous-time NELBO. Using Lemma C.4 twice and adding the entropy of the variational distribution in Eq. 52, and then using Lemma C.5 twice, we get the continuous-time NELBO: LNESYDM =Et[0,1],qθ (c0,ctx,y0) (cid:34) α 1 αt (cid:32) (cid:88) iMct log pθ(c0 = c0 ct, x) + (cid:124) (cid:123)(cid:122) Lc: Concept denoising loss (cid:125) Eq(yty0) (cid:88) iMyt log pθ(y0 = i ct, yt, x) (cid:124) (cid:123)(cid:122) Ly: Output denoising loss (cid:125) (cid:33)(cid:35) H[qθ(c0 y0, x)] (cid:124) (cid:125) (cid:123)(cid:122) LH[q]: Variational entropy . (55) Next, we will further simplify the output unmasking loss Ly with Rao-Blackwellisation to get the form given in Theorem 3.1. Using Eq. 47, Ly =Et[0,1],qθ (yt,ctx,y0) =Et[0,1],qθ (yt,ctx,y0) =Et[0,1],qθ (yt,ctx,y0) (cid:34) (cid:34) (cid:34) α 1 αt (cid:88) iMyt log pθ(y0 = y0 (cid:35) ct, yt, x) α 1 αt α 1 αt (cid:88) log (cid:88) (cid:88) iMyt y0,y0 =y0 c0 pθ(c0 ct, x)1[φyt (c0) = y0] (cid:35) (cid:88) (cid:88) log pθ(c0 ct, x) (cid:88) iMyt c0 y0,y0 =y0 (cid:35) 1[φyt(c0) = y0] =Et[0,1],qθ (ctx,y0)q(yty0) (cid:34) α 1 αt (cid:88) (cid:88) log iMyt c0 pθ(c0 ct, x)1[φyt(c0)i = y0 ] (cid:35) , where in the last step we use that only single y0 satisfies φyt(c0) = y0, and it appears in the sum , and the conditional independence in Fig. 2 of yt and ct given y0. only if exactly that y0 has i = y0 Next, define the following inductive hypothesis based on the value of : Ly =Et(0,1]Eqθ (ctx,y0) α (cid:34) (cid:88) i=1 (cid:88) log c0 pθ(c0 ct, x)1[φ(c0)i = y0 ] . (56) (cid:35) Base case = 1: The only elements in the support of q(yt y0) are yt 1 = m. If it is y0 1 (probability αt), the set of unmasked values is empty, and so the loss is zero. If it is (probability 1 αt), the only masked dimension is = 1. Furthermore, there are no unmasked dimensions in yt, hence φyt = φ and so the loss is 1 and yt 1 = y0 Ly = Et(0,1]Eqθ (ctx) α log (cid:34) (cid:88) c0 (cid:35) pθ(c0 ct, x)1[φ(c0)1 = y0 1] . Inductive step > 1: Assume the result holds for 1. Like in the base case, q(yt y0) = αt and q(yt = y0) = 1 αt. Then, let ˆyt denote all variables in yt except yt assume the inductive hypothesis holds for ˆyt. We again consider the two cases: Either yt = y0 , and we with = y0 24 probability αt or yt = with probability 1 αt. Ly =Et(0,1]Eqθ (ct,ytx,y0) =Et(0,1]Eqθ (ctx) (cid:34) (cid:88) ˆyt α 1 αt q(ˆyt y0) (cid:88) (cid:88) log pθ(c0 ct, x)1[φyt(c0)i = y0 ] iMyt (cid:32) αtα 1 αt c0 (cid:88) (cid:88) log iMˆyt pθ(c0 ct, x)1[φˆyt(c0)i = y0 ] + (1 αt)α 1 αt (cid:88) (cid:88) log iMˆyt {Y } c0 pθ(c0 ct, x)1[φˆyt(c0)i = y0 ] (cid:33)(cid:35) Note now that the second term contains the same sum over the Mˆyt as in the first term, but in addition it contains the dimension . We next move the other terms into the first term, leaving with weight of α 1αt for the first term: Ly =Et(0,1]Eqθ (ctx) (cid:34) (cid:88) ˆyt q(ˆyt y0) (cid:32) α 1 αt (cid:88) (cid:88) log iMˆyt (cid:33)(cid:35) pθ(c0 ct, x)1[φˆyt(c0)i = y0 ] + α log (cid:88) c0 pθ(c0 ct, x)1[φˆyt(c0)Y = y0 ] Next, we apply the inductive hypothesis to the first term. After, note that the second term is independent of the value of ˆyt as the result of φˆyt (c0)Y does not depend on ˆyt. Ly =Et(0,1]Eqθ (ctx) α (cid:34) 1 (cid:88) (cid:88) log pθ(c0 ct, x)1[φˆyt(c0)i = y0 ] (cid:88) + ˆyt q(ˆyt y0)α log (cid:34) =Et(0,1]Eqθ (ctx) α i=1 (cid:88) c0 (cid:88) i=1 c0 (cid:35) pθ(c0 ct, x)1[φˆyt(c0)Y = y0 ] pθ(c0 ct, x)1[φ(c0)i = y0 ] (cid:35) , (cid:88) log c0 completing the inductive proof. Finally, replacing Eq. 56 for Ly in Eq. 55 completes the proof."
        },
        {
            "title": "E Gradient estimation details",
            "content": "In this section, we provide additional details and formalisation on our gradient estimation procedure, extending the discussion in Section 3.4. 25 Given some input-output pair (x, y0) D, the gradient of the loss is given by [67, 78] θLNESYDM = θH[qθ(c0 x, y0)] (cid:124) (cid:125) (cid:123)(cid:122) Gradient of variational entropy θ LH[q] + Et[0,1],qθ (c0,ctx,y0) (cid:34) α 1 αt (cid:88) iMct θ log pθ(c = c0 ct, x) + (cid:124) (cid:123)(cid:122) Gradient of concept unmasking loss θ Lc (cid:125) (cid:88) log θpθ(c0 ct, x)1[φ(c0)i = y0 ] + α (cid:88) i=1 (cid:124) (cid:32) (cid:124) c0 (cid:123)(cid:122) Gradient of output unmasking loss θ Ly α 1 αt (cid:88) iMct log pθ(c0 = c0 ct, x) + α (cid:125) (cid:88) i=1 (cid:80)S j=1 log )i = y0 ] 1[φ(c0 (cid:33) (cid:35) θ log qθ(c0 x, y0) (cid:123)(cid:122) Indirect gradient from sampling from variational distribution (ignored) (cid:125) Monte carlo approximation. We will use monte carlo approximation to estimate this gradient. We first sample single c0 qθ(c0 x, y0), [0, 1], and then single ct q(ct c0) using Eq. 9. Finally, we sample samples c0 pθ(c0 ct, x) to approximate the gradient of the output unmasking loss Ly with the RLOO estimator [37]. Alternatively, one could use probabilistic circuits to compute this gradient exactly [48, 82]. 1, . . . , c0 Indirect gradient. The indirect gradient arises from the expectation over the variational distribution which depends on the parameter θ. This term has high variance in monte-carlo estimator. Firstly, the vanilla score function estimator is known to have high variance, especially without additional variance reduction techniques [56]. However, the reward, which is given between the large braces, is doubly-stochastic: it depends on sampling t, ct, and c0, . . . , cS, making it an inherently noisy process. Furthermore, when using the variational distribution as defined in Section 3.3, the score term θ log qθ(c0 x, y0) is itself NESYDM for which computing log-likelihoods is intractable, and thus we would require additional approximations to estimate it. Because of the variance, intractability, and to keep the algorithm simple, we ignore the term altogether. Therefore, our gradient estimate is given by = γc α 1 αt (cid:88) iMct θ log pθ(c0 = i ct, x) + γH θH[qθ(c0 x, y0)] (cid:125) (cid:123)(cid:122) (cid:124) Choose estimate of θ LH[q] + (cid:123)(cid:122) Unbiased estimate of θ Lc (cid:125) 1 (S 1)µi (cid:88) j=1 (cid:0)1[φ(c0 )i = i ] µi (cid:124) (cid:88) i=1 (cid:124) γy (cid:123)(cid:122) Consistent estimate of θ Ly (cid:125) (cid:1) θ log pθ(c0 ct, x) , (57) (cid:80)S j=1 1[φ(c0 )i = y0 where µi = 1 ] is the empirical mean of the constraints, and γc, γH and γy are weighting coefficients. We keep γy = 1, and tune the other two. Additionally, inspired by the local step approach of [32], we average over dimensions rather than summing to stabilise hyperparameter tuning among different problems. This is especially useful in experiments with variable dimension size such as MNISTAdd and Warcraft Path Planning. We discuss how we estimate the gradient of the entropy of the variational distribution in Section 3.4. Estimate of θLy Next, we derive the consistent gradient estimator for θLy using the RLOO estimator [37]. Assuming we have some x, y0, and ct, and using the score-function estimator, the gradient of the loss is given by θLy =α =α =α =α (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 θ log (cid:88) c0 pθ(c0 ct, x)1[φ(c0)i = y0 ] (cid:80) c0 θpθ(c0 ct, x)1[φ(c0)i = y0 ] (cid:80) c0 pθ(c0 ct, x)1[φ(c0)i = y0 ] c0 pθ(c0 ct, x)1[φ(c0)i = y0 (cid:80) Epθ (c0ct,x)[1[φ(c0)i = y0 ]] ]θ log pθ(c0 ct, x) Epθ (c0ct,x)[1[φ(c0)i = y0 ]θ log pθ(c0 ct, x)] Epθ (c0ct,x)[1[φ(c0)i = y0 ]] (58) Both the numerator and denominator are expectations under pθ(c0 ct, x) of the constraints. consistent (but not unbiased) estimator is given by sampling samples c0 pθ(c0 ct, x) and taking averages at each of these 2Y expectations separately. Then, we will use RLOO as baseline to reduce the variance of the numerators. baseline is constant b, where we use that for any distribution pθ(x), Epθ (x)[bθ log pθ(x)] = 0, and so by linearity of expectation, Epθ (x)[(f (x) b)θ log pθ(x)] = Epθ (x)[f (x)θ log pθ(x)]. Since we are using samples, we choose for each sample c0 and dimension the baseline bij to be the empirical mean over the 1[φ(c0 other samples, leaving one sample out: bij = 1 )i = S1 y0 ct, x) is an unbiased estimator of the numerator. Finally, we average over ] bij)θ log pθ(c0 the different estimators obtained this way to derive the RLOO gradient estimator as: ]. Then, (1[φ(c0 )i = y0 1, . . . , c0 (cid:80) l=j Epθ (c0ct,x)[1[φ(c0)i = y0 ]θ log pθ(c0 ct, x)] 1 (cid:88) (1[φ(c0 )i = y0 ] bij)θ log pθ(c0 ct, x) j=1 = = (cid:88) j=1 (cid:88) j=1 (cid:32) (S 1)1[φ(c )i = y0 ] (cid:80) S(S 1) l=j 1[φ(c0 )i = y0 ] (cid:33) θ log pθ(c0 ct, x) (cid:32) S1[φ(c0 )i = y0 ] (cid:80)S l=1 S(S 1) 1[φ(c0 )i = y0 ] (cid:33) θ log pθ(c0 ct, x) = 1 (S 1) (cid:88) j=1 (cid:0)1[φ(c0 )i = y0 ] µi (cid:1) θ log pθ(c0 ct, x) (59) Combining Eqs. 58 and 59 gives the gradient estimator: θLy gy0 (c0 1, . . . S) := α (cid:88) i=1 1 µi(S 1) (cid:88) j= (cid:0)1[φ(c0 )i = y0 ] µi (cid:1) θ log pθ(c0 ct, x) (60) Full gradient estimation algorithm. In Algorithm 1, we provide the full algorithm for estimating gradients to train NESYDM. The algorithm proceeds by sampling c0 from the variational distribution, and then sampling partially masked value ct. We then compute the gradients of the three individual losses using Eq. 57. This requires sampling samples from the unmasking model, which is done in line 5. Finally, we weight the gradients appropriately and sum them up."
        },
        {
            "title": "F Sampling details",
            "content": "We use the first-hitting sampler [89] if the configured number of discretisation steps is larger or equal to the dimension of the concept space C. Otherwise, we use -step time-discretisation of the reverse process [65]. 27 Algorithm 1 Gradient estimation algorithm for NESYDM 1: Given data (x, y0) and unmasking model pθ(c0 x, ct) with current parameters θ 2: c0 qθ(c0 x, y0) 3: U(0, 1) 4: ct q(ct c0) 1, . . . , c0 5: c0 1, . . . c0 6: gy gy0(c0 S) 7: gc α (cid:80) 1αt 8: gH θLH 9: return γc See Section 3.3. Sample random time step. See Eq. 9. Sample samples from unmasking model. Estimate gradient of Ly using Eq. 60. Compute gradient of Lc Compute gradient of LH. qθ(c0 x, ct) θ log pθ( i x, ct) = c0 iMct gc + γy gy + γH gH The first-hitting sampler in Algorithm 2 randomly samples the next timestep to unmask at. There, it randomly selects an index to unmask using the concept unmasking model. Note that α1 is the inverse of the noising schedule. Since we do not provide the temperature to our neural networks, this sampler is, in practice, concept-by-concept decoding process similar to masked models like BERT [23, 89]. Algorithm 2 First-hitting sampler for pθ(c0 x) 1: Input: x, unmasking model pθ(c0 x, ct) 2: 1 3: c1 = 4: for to 1 do α1(1 5: Uniform(Mck ) 6: cs ct 7: pθ(c0 cs 8: 9: 10: Return c0 x, ct) u(1 αt)), where U(0, 1) Select next timestep to unmask at Select random dimension to unmask Sample the unmasked dimension Instead, the time-discretised sampler in Algorithm 3 samples completely unmasked sample c0 from the unmasking model at each timestep, then samples cs from the reverse process in Eq. 10 to obtain the next timestep. When sampling from the reverse process, the algorithm remasks some of the newly unmasked dimensions in c0, while keeping the unmasked dimensions in ct fixed. Algorithm 3 Time discretised sampler for p(c0 x) 1: Input: x, unmasking model pθ(c0 x, ct), number of discretisation steps 2: c1 = 3: for to 1 do 4: 5: 6: Return c0 c0 pθ(c0 x, ct) cs q(cs ct, c0 = c0) Sample from unmasking model Sample from reverse process in Eq. 10 F.1 Sampling from the variational distribution We adapted the two samplers above to sample from our model conditioned on the output y0. We use simple resampling approach as described in Section 3.3, which we elaborate on here. First, we recall the relaxed constraint for β > 0 as rβ(c0 y0) = exp(β (cid:88) i=1 1[φ(c0)i = y0 ]). Then, we define the distribution to sample from as qθ(c0 x, ct, y0) pθ(c0 x, ct)rβ(c0 y0). (61) (62) 28 Algorithm 4 Self-normalised importance sampling for qθ(c0 x, ct, y0) 1: Input: x, yt, unmasking model pθ(c0 x, ct), number of samples 2: c0 1, . . . , c0 3: wi = rβ(c0 4: = (cid:80)K 5: Categorical( wi ) 6: Return c0 pθ(c0 x, ct) y0), for all [K] i=1 wi Sample samples from unmasking model Compute importance weights Normalisation constant Sample from renormalised distribution Since we cannot tractably sample from this distribution, we use self-normalised importance sampling [12]. In other words, we sample samples from the unmasking model, compute the relaxed constraint for each sample, and then normalise these values. Finally, we sample from the renormalised distribution. We provide the full algorithm in Algorithm 4. We note that the distribution pθ(c0 x, ct) does not appear in the importance weights. This holds because we are sampling from it, thus it divides away in the computation of the importance weights. We implement this algorithm in the two samplers as follows. For the time-discretised sampler, we replace Line 4 of Algorithm 3 with Algorithm 4. Together, this is the algorithm used in [29]. For the first-hitting sampler, we replace Line 8 of Algorithm 2 by first calling Algorithm 4 to obtain some c0, and then returning the i-th dimension c0 . Relation to Markov Logic Networks. The distribution in Eq. 62 is similar to Markov Logic Network (MLN) [64]. Particularly, the formulas of the MLN are (1): the different constraints 1[φ(c0)i = y0 ], each weighted by β3, and (2): the unmasking model pθ(c0 x, ct), defined with formulas 1[c0 x, ct). In particular, this means that c0s that = c0 = c0 violate constraints still have positive energy. However, the energy exponentially shrinks by factor of exp(β) for each violated constraint. Since we use rather high values of β > 10, the resampling step in Algorithm 4 is extremely likely to pick the sample that violates the least number of constraints. ] and weight log pθ(c 1 Numerically stable implementation. In practice, computing Eq. 61 in Line 3 is numerically exp(lβ) , where is highly unstable if multiple constraints are violated. Then, the reward is equal to the number of violated constraints. PyTorch floats are roughly between exp(104) and exp(88), meaning that for β > 10, the reward underflows at = 8. First, we note that when sampling from the reweighted distribution in Line 5, probabilities are computed as the relative proportion of the rewards. Therefore, we can simply rescale all rewards by constant factor to ensure they do not underflow or overflow. Particularly, we redefine the reward as 1 rβ,L,U (c0 y0) = exp max β (cid:32) (cid:32) 1[φ(c0)i = y0 ] L, . (cid:33)(cid:33) (cid:88) i=1 Here, > 0 acts as scaling on the reward as rβ,L,U (c0 y0) = rβ(c0 y0) exp(L) if the max is not active. > 0 acts as floor on the reward such that samples that violate many constraints still have non-zero probability, even if it is extraordinarily unlikely. We fix = 100 to remain in the floating point range. Note that this is ever so slightly biased as it will over-estimate the probability of the samples that violate the least number of constraints. However, this bias is very small as the probability of choosing these samples is extraordinarily low. We want to choose to maximise the range of the reward among the samples without overflowing. Within this range, the differences between the samples that violate the least number of constraints is most important: these are the samples we are most likely to choose. Our intuition is as follows: we set to the average number of violated constraints among the samples in Line 2. However, if this would overflow the best sample, we instead set such that the best sample has reward of exp(M ), where = 70 to prevent overflow. Therefore, we choose = min (cid:32) βt (cid:88) (cid:88) k=1 i=1 1[φ(c k)i = y0 ], + min k=1 βt (cid:88) i= (cid:33) 1[φ(c0 k)i = y0 ] . (63) 3Unlike MLNs, we sum unsatisfied constraints rather than satisfied ones to ensure rβ(c0 y0) [0, 1]."
        },
        {
            "title": "G Experimental details",
            "content": "NESYDM is implemented in PyTorch. We used RAdam [43] for all experiments except for MNIST Addition, where we used Adam [34]. We did not compare these optimisers in detail, but we do not expect this choice to significantly affect the results. Furthermore, we used default momentum parameters for both optimisers. For all neural networks used to implement the unmasking model pθ(c0 x, ct), we did not pass the current time step to the network, as previous work found minimal impact on performance for doing so (Appendix E.5 of [65]). For all experiments, we used GPU computing nodes, each with single lower-end GPU. In particular, we used NVIDIA GeForce GTX 1080 Ti and GTX 2080 Ti GPUs. All our experiments were run with 12 CPU cores, although this was not the bottleneck in most experiments. On the GTX 1080 Ti, our experiments took between 1 and 17 hours, depending on the complexity of the task and number of epochs. The project required extra compute when testing different variations of the model, and by performing hyperparameter tuning. For repeating our runs, we expect around 600 total GPU hours is needed, while G.1 Hyperparameter tuning We list all hyperparameters in Table 4. We perform random search over the hyperparameters on the validation set of the benchmark tasks. For the random search, we used fixed ranges for each parameter, from which we sample log-uniformly. For the parameter β we sampled uniformly instead. We used budget of 30 random samples for each problem, although for some problems we needed more when we found the ranges chosen were poor. Several hyperparameters, namely the minibatch size, S, K, and L, are compute dependent, and we keep these fixed when tuning depending on the compute budget and the problem size. The hyperparameters we do tune are the learning rate, γc, γH, and β. We found that low values of γc were usually fine, and that for large enough β above 10, its value did not matter much. Therefore, the most important hyperparameters to tune are γH and the learning rate, for which the optimal values varied between problems significantly. Table 4: All hyperparameters used in the experiments, and rough recommendations for some of their values. We recommend at least tuning learning rate, and γH and γc to some extent (leaving γy at 1). Some hyperparameters are compute dependent, and higher is always better for reducing gradient estimation variance (S, K, ) and majority voting quality (L, ). Variable Recommendation Description (0.0001, 0.0005) Overall learning rate Minibatch size Epochs 1 Weight of concept unmasking loss 105 Weight of output unmasking loss Range Definition R>0 R0 R0 R0 R>0 8 Number of majority voting samples Eq. 57 Eq. 57 Eq. 57 Section 3.3 Eq. 7 Appendix F.1 Section 3.5 Section 3.5 4 Number of RLOO samples 2 Number of SNIS samples for qθ MDM discretisation steps Penalty in soft constraint 10 (0.002, 2) Weight of variational entropy γy γc γH β L G.2 MNIST Addition We use the LeNet architecture [39] for the neural network architecture as is standard in the NeSy literature [50]. As there are no dependencies between the digits in the data generation process, making the neural network conditional on partially unmasked outputs is not useful: merely predicting marginals is sufficient. Therefore, we ignore the conditioning on ct when computing pθ(c0 ct, x) in Eq. 47. 30 Since there is no standard dataset for multidigit MNIST addition, we use generator defined as follows: for some dataset of MNIST images, we permute it randomly, then split it into 2N parts and stack them to obtain the different datapoints. This ensures we use each datapoint in the dataset exactly once, ending up in 60000 2N training datapoints. We tuned hyperparameters in accordance with Appendix G.1. Since MNIST has no separate validation dataset, we split the training dataset in training dataset of 50.000 samples and validation dataset 10.000 samples before creating the addition dataset. We tune with this split, then again train 10 times with the optimised parameters on the full training dataset of 60.000 samples for the reported test accuracy. We tune on = 15, and reuse the same hyperparameters for = 2 and = 4. For the number of epochs, we use 100 for = 2 and = 4 as an epoch is more expensive for smaller and because = 15 requires moving beyond cold-start phase. We found all 10 runs moved past this phase within 100 epochs, but needed more time to converge after. Table 5: Hyperparameters for MNIST Addition and Warcraft Path Planning. Variable learning rate minibatch size epochs γc γH γy β L MNIST Addition Path Planning 0.0003 16 = 4: 100, = 15: 1000 2 105 0.01 1 20 1024 1024 8 0.0005 50 40 105 0.002 1 12 12 12: 16, 30 30: 4 12 12: 4, 30 30: 2 20 8 Baselines. For all methods, we take the numbers reported in the papers where possible. We obtained numbers for Scallop from the PLIA paper. For A-NeSI, we pick the best-scoring variant as reported, which is Predict for = 4 and Explain for = 15. For DeepSoftLog, A-NeSI and PLIA, we obtained performance on 10 individual runs from the authors to compute the Mann-Whitney test. G.3 Visual Path Planning Following [76], we use categorical costs for the Visual Path Planning task. We use = 5, which corresponds to the possible cost values in the data, costs = [0.8, 1.2, 5.3, 7.7, 9.2]. Then, c0 corresponds to an index of the cost of each grid cell. That is, c0 i+j {1, ..., 5} corresponds to the cost value costs[c0 i+j] at grid cell i, j. We adapted the ResNet18-based architecture from [59] for the unmasking model p(c0 x, ct) over grid costs. This architecture consists of single convolutional layer to start encoding the image, with batch normalisation and adaptive max-pooling to grid of size . After this, we have 64-dimensional embeddings for each grid cell. To condition on the currently unmasked values, we add embeddings of ct {1, . . . , 5, m}N 2 for each cell: we use six 64-dimensional embeddings 1 , . . . , eC eC for the different costs plus the mask value. Then we add these embeddings to the image embeddings cell-wise. That is, if eI i,j is the image embedding at cell i, j, then the new embedding is eI . After this, ResNet layer containing two more convolutional layers follows. Finally, we use an output layer that takes the grid cell embeddings and predicts distribution over the 5 possible costs. i,j + eC ct i+j 5 , eC We performed hyperparameter tuning on the validation set of the 12 12 grid size problem, then reused the same hyperparameters for the 30 30 grid size problem. We only reduced the number of RLOO samples and the number of samples for the SNIS algorithm in Appendix F.1 for the 30 30 grid size problem to reduce the overhead of many calls to Dijkstras algorithm. This algorithm quickly becomes the main compute bottleneck on large grids. 31 For 12 12, we evaluated test accuracy at 40 epochs, and for 30 30 we evaluated validation accuracy every 5 epochs within the 40 epoch timeframe, choosing the best performing model for the test accuracy. We found that on 30 30 the model was sometimes unstable, suddenly dropping in accuracy and recovering after while. As is common in this task and our baselines, we consider path prediction correct if the predicted path has the same cost as the gold-truth shortest path given. This is because shortest paths may not be unique. Baselines. We take the numbers for EXAL as reported in the paper [81]. For A-NeSI, I-MLE and A-NeSI + RLOO, we obtained performance on 10 individual runs from the authors to compute the Mann-Whitney test. G.4 RSBench For all experiments, we adapt the implementation of the benchmark in the RSBench repository [11]. We use the conditional 1-step entropy discussed in Section 3.4. For the MNIST experiments, we brute-force the conditional entropy computation, while for BDD-OIA, we adapt the inference procedure in [11] to obtain the conditional entropy. G.4.1 Metrics For all tasks, we compute the Expected Calibration Error (ECE) over marginal concept probabilities [57] as metric for calibration. Since NESYDM is not tractable, we have to estimate these marginals. Therefore, we use simple maximum-likelihood estimation to obtain approximate marginal probabilities for pθ(wi x) by sampling samples from the model and taking the empirical mean. We used = 1000 throughout to improve the accuracy of the ECE estimate. For the MNIST tasks, we report both the output accuracy Accy and concept accuracy Accc. In particular, for output accuracy, we compute exact match accuracy over the output predictions. For concept accuracy, we use micro-averaged accuracy over the concept predictions (that is, the two digits). This requires NESYDM to output predictions for the digits separately. We tried two different majority voting strategies using the samples. 1) Take the dimension-wise mode among the samples, or 2) take the most common complete concept vector and use the individual dimensions of as the predictions. We used the second strategy in Table 3 to ensure the predictions can capture dependencies between digits, and compare the two methods in Appendix and Table 8. For BDD-OIA, we report macro-averaged F1 scores for both the output and concept prediction. For example, for the concept F1 score, we compute the F1 score for each concept separately, then take the unweighted mean of these F1 scores. Similarly, we computed macro-averaged ECE scores for concept prediction. For NESYDM, we computed marginal probabilities for concept and output predictions, that is, per dimension. Furthermore, we recomputed all metrics for the baselines reported in Table 3, as we found bugs in the code for both metrics in the RSBench and BEARS codebases. G.4.2 Hyperparameters Table 6: Hyperparameters for RSBench. Variable MNIST Half & Even-Odd BDD-OIA learning rate minibatch size epochs γc γH γy β L 0.00009 16 500 1.5 106 1.6 1 10 1024 1024 8 0.0001 256 30 5 106 2.0 1 10 1024 1024 22 1000 32 For the datasets in RSBench, we tuned on the validation set for all parameters using the conditional entropy. Then, we ran an additional hyperparameter search on just the entropy weight to find the right trade-off between calibration and accuracy. We found the entropy weight can be sensitive, where high values significantly slow down training, while low values may result in uncalibrated models. For L, we use much higher number of 1000 samples. This is to ensure the Expected Calibration Error is properly estimated (see Appendix G.4.1 for details). For the runs with the unconditional entropy, we used the same hyperparameters and no additional hyperparameter search. G.4.3 Experimental details and architectures 1 x, ct) for the first digit, we concatenate one-hot encodings of ct MNIST Half and MNIST Even-Odd. We adapted the original architecture from our baselines in [52]. For both experiments, we encode the two individual digits in with convolutional neural network (ReLU activations) of 3 layers, with 32, 64 and 128 channels respectively. Then, we flatten the output, obtaining two embeddings e1 and e2. For predicting the unmasking distribution p(c0 2 and e1, while for predicting the distribution of the second digit p(c0 1 and e2. Note the order here: This is to ensure permutation equivariance, as the sum is commutative operation. Therefore, like our baselines, we have disentangled architecture that uses the same neural network to classify the two digits, while still incorporating the currently unmasked values. Finally, using the concatenated vector, we use linear output layer and softmax to obtain the distribution over the possible digits. 2 x, ct), we concatenate one-hot encodings of ct 2, ct 1, ct BDD-OIA. We used early stopping by running for 30 epochs, testing on the validation set every epoch and picking the model with the highest validation accuracy. As in [52], we used preprocessed embeddings of the dashcam images from Faster-RCNN [63]. This Faster-RCNN was pre-trained on MS-COCO and fine-tuned on BDD-100k. These are provided in the RSBench dataset, and were also used for BEARS. For the unmasking model p(c0 x, ct), we adapted the MLP from [52], using single hidden layer with dimensionality of 512, by simply concatenating one-hot encoding of ct {0, 1, m}21 to the input embedding of x. Note that, since the concepts are binary, this one-hot encoding is 3-dimensional vector, as it can be 0, 1, or the mask value m. Baselines. We obtained results of the 5 individual runs used for each method in [52] and re-evaluated them to obtain 4 digits of precision for all reported results, as [52] only reported 2 digits of precision. Furthermore, we used these results to compute statistical significance tests. We have different results than reported in [52] for BDD-OIA as we found bugs in the code for the metrics."
        },
        {
            "title": "H Other majority voting strategies",
            "content": "(y0 x) is intractable in As stated in the main text, computing the exact mode argmaxy0 pNESYDM general, also for representations supporting tractable marginals [2, 80]. Throughout this paper, we used the majority voting strategy described in Section 3.5. However, when observing the results on MNIST-Even-Odd, one can be puzzled by the relatively high performance on concept accuracy while the output accuracy is low. We hypothesised that this was due to our chosen majority voting strategy, and repeated the evaluation of the models using different strategies, which we describe here. All assume access to set of samples c0 pθ(c0 x, c1 = m). 1, . . . , c0 θ Program-then-true-mode (PTM): The strategy described in Eq. 8, and the main one used in this paper. We emphasise that this is the correct strategy according to the generative process of NeSy predictors. Program-then-marginal-mode (PMM): Similar to above, we feed all sampled concepts into the program, but rather than taking the most likely output, we choose the most likely output dimension-wise: ˆyi = argmaxyi 1[φ(c0 )i = yi] (64) (cid:88) l=1 True-mode-then-program (TMP): Find the mode of the sampled concepts, then feed that into the program: (cid:32) ˆy = φ argmaxc (cid:33) 1[c = c] (cid:88) l=1 33 (65) Marginal-mode-then-program (MMP): Compute the dimension-wise mode of the concepts ˆci, combine them into single concept ˆc, and feed that into the program: ˆci = argmaxci (cid:88) l=1 1[c0 l,i = ci], ˆy = φ(ˆc) (66) Table 7: Output accuracy, both inand out-of-distribution, for different majority voting strategies on the MNIST-Half and MNIST-Even-Odd datasets. Strategy Half, ID Half, OOD Even-Odd, ID Even-Odd, OOD NESYDM, Conditional entropy 99.12 0.18 99.12 0.18 98.87 0.23 60.16 4.77 28.45 0.90 28.44 0.91 28.46 0.91 33.15 1.40 98.65 0.31 98.65 0.31 97.94 0.49 25.14 2.81 NESYDM, Unconditional entropy 99.12 0.10 99.12 0.10 99.26 0.26 79.42 3.14 10.95 0.05 10.95 0.05 15.71 0.49 44.11 4. 97.52 0.44 97.52 0.44 98.10 0.37 87.64 0."
        },
        {
            "title": "PTM\nPMM\nTMP\nMMP",
            "content": "PTM PMM TMP MMP 0.02 0.04 0.02 0.04 0.18 0.14 5.39 0.45 0.00 0.00 0.00 0.00 0.02 0.02 5.27 0.52 We evaluated these strategies on the validation set of all benchmarks, and found that they all performed similar, or at most marginally worse than the PTM strategy used in this paper. However, we found exceptions in MNIST-Half and MNIST-Even-Odd, where MMP significantly outperforms the other strategies in the OOD setting, while significantly underperforming in the ID setting, as highlighted in Table 7. This result holds for both NESYDM with the conditional entropy and the unconditional entropy. ID performance of MMP takes rather significant hit because there are strong statistical dependencies between the concepts in the construction of the ID datasets. Especially the Even-Odd OOD dataset is rather adversarially constructed, as highlighted by the extremely low OOD performance of all methods. However, because NESYDM has relatively high concept accuracy OOD, using MMP still results in some correct outputs. We performed similar analysis for the two strategies for predicting concepts in Table 8. Here we find that, overall, the true mode strategy usually performs better, except that we find significant difference between TM and MM on the OOD dataset of MNIST-Half. Table 8: Concept accuracy, both inand out-of-distribution, for different majority voting strategies on the MNIST-Half and MNIST-Even-Odd datasets. Strategy Half, ID Half, OOD Even-Odd, ID Even-Odd, OOD NESYDM, Conditional entropy TM MM TM MM 71.16 1.77 66.78 2.84 61.84 0.89 62.76 0.77 20.33 1.33 19.65 2.37 15.60 0.99 14.56 0.86 NESYDM, Unconditional entropy 79.41 6.58 80.56 5. 57.22 0.49 70.40 2.71 0.36 0.39 0.39 0.44 4.65 0.49 1.16 0."
        }
    ],
    "affiliations": [
        "Miniml.AI",
        "School of Informatics, University of Edinburgh"
    ]
}