{
    "paper_title": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data",
    "authors": [
        "Xabier de Zuazo",
        "Vincenzo Verbeni",
        "Eva Navas",
        "Ibon Saratxaga",
        "Mathieu Bourguignon",
        "Nicola Molinaro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity."
        },
        {
            "title": "Start",
            "content": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data Xabier de Zuazo ID 1,, Vincenzo Verbeni ID 2, Eva Navas ID 1, Ibon Saratxaga ID 1, Mathieu Bourguignon ID 2,4,5,6, Nicola Molinaro ID 2,3 1 HiTZ Center, University of the Basque Country UPV/EHU, Spain 2 Basque Center on Cognition, Brain and Language BCBL, Spain 3 Ikerbasque, Basque Foundation for Science, Spain 4 Laboratory of Functional Anatomy, Faculty of Human Motor Sciences, Universite libre de Bruxelles (ULB), Belgium 5 Laboratoire de Neuroanatomie et Neuroimagerie translationnelles (LN2T), ULB Neuroscience Institute, Universite libre de Bruxelles (ULB), Belgium 6 WEL Research Institute, Belgium xabier.dezuazo@ehu.eus, v.verbeni@bcbl.eu, eva.navas@ehu.eus, ibon.saratxaga@ehu.eus, mabourgu@ulb.ac.be, n.molinaro@bcbl.eu"
        },
        {
            "title": "Abstract",
            "content": "Data-efficient neural decoding is central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pretrain Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity. Index Terms: transfer learning, MEG, speech decoding, crosstask decoding, brain-computer interface, neuroscience 1. Introduction Brain-computer interfaces (BCIs) for speech restoration require robust neural decoders [1, 2], but collecting sufficient training data per subject remains fundamental challenge [3, 4, 5]. Nevertheless, while large-scale datasets have enabled substantial progress in non-invasive speech decoding tasks (including speech detection and phoneme decoding) using magnetoencephalography (MEG) [6, 7, 8] and electroencephalography (EEG) [9, 10], practical BCI deployment is constrained by the limited data available per individual: typically minutes rather than hours. Current approaches train separate models from scratch for each subject and task [11, 12, 13, 14], ignoring the plausible benefits of transfer learning that have revolutionized computer vision [15] and natural language processing [16]. Notwithstanding successful demonstrations of transfer learning in EEG [17] and fMRI [18], and recent advances in selfsupervised neural decoding [19, 20], transfer learning has only very recently been explored for MEG imagined speech using ImageNet-pretrained vision models [21], and remains untested **indicates the corresponding author. for MEG-to-MEG pre-training and cross-task transfer between perception and production. This study presents the first successful application of transfer learning to MEG speech decoding task (speech detection). We pre-train Conformer-based model [22, 23] on 50 hours of single-subject listening data (LibriBrain [7]) and fine-tune on just 5 minutes per subject across 18 participants performing speech perception (listening and playback) and speech production tasks [24]. To our knowledge, we also provide the first cross-task MEG speech detection results, extending prior crosstask decoding work in non-speech EEG/MEG [25, 26]. We show that transfer learning improves both in-task performance and cross-task generalization, with gains of 1-6% across metrics. Importantly, models trained on production successfully decode passive listening above chance, confirming that decoding relies on shared neural speech representations rather than taskspecific motor activity alone [27, 28]. Together, these findings suggest that transfer learning may support more data-efficient MEG speech detection and improved cross-task generalization. 2. Related Work MEG-based speech decoding has progressed from closedvocabulary word classification [11, 12, 29] to phone-level analysis [13, 14, 30] and recent large-scale competitions [8, 6]. Complementary work has explored Transformer-based encoding models [31], compact end-to-end architectures [32], and speech synthesis from MEG [33]. In parallel, EEG research has demonstrated open-vocabulary decoding [9, 34, 35] and largescale word and phonetic decoding [10], emphasizing the importance of dataset size and model capacity. Transfer learning has been successfully applied to other neuroimaging modalities. EEG studies have leveraged selfsupervised pre-training [17] and contrastive learning [19] to improve cross-subject generalization. Recent work has combined discriminative decoders with language-model rescoring [20] and foundation-model guidance [18] to improve performance. In contrast, despite these advances in EEG and fMRI, transfer learning has not been demonstrated for MEG-based speech decoding, leaving critical gap in understanding whether largescale pre-training can address the limited per-subject data avail6 2 0 2 0 2 ] . [ 1 3 5 2 8 1 . 2 0 6 2 : r able in clinical BCI settings. Cross-task speech decoding between perception and production remains underexplored. At the same time, previous work established differences in neural representations between passive listening and overt speech [24, 36, 37], and investigation of cross-task transfer and the role of motor-related activity versus shared speech representations has been limited [28, 38]. Our work addresses this gap by demonstrating bidirectional crosstask decoding and quantifying the benefits of transfer learning across speech modalities. 3. Methods 3.1. Datasets We evaluate transfer learning across two MEG speech datasets with contrasting characteristics: large-scale single-subject dataset for pre-training and multi-subject dataset with limited per-subject data for fine-tuning and cross-task evaluation. LibriBrain (pre-training). We pre-train single model on the LibriBrain dataset [7], which provides over 50 hours of within-subject MEG recordings from single participant during naturalistic English speech listening (Sherlock Holmes audiobooks). Recordings were acquired from single righthanded male native English speaker using 306-channel Elekta/MEGIN system (102 magnetometers, 204 planar gradiometers) and processed following the LibriBrain Competition pipeline [8], with model inputs downsampled to 250 Hz. We use the binary Speech Detection task, which distinguishes speech from silence based on voice-activity labels. Across the dataset, speech accounts for approximately 76.7% of the labeled time. Speech listening and production data [24] (fine-tuning and evaluation). We fine-tune and evaluate on the dataset described by Bourguignon et al., comprising 18 healthy adult participants (9 female, 8 male, 1 unreported; mean age: 23.9 years) who were native Spanish speakers. Each participant performed three speech-related tasks for approximately 5 minutes each: listening to pre-recorded speech, listening to playback of their own voice, and reading aloud (speech production). MEG was recorded using another 306-channel Elekta/MEGIN system (same model as LibriBrain, but in different lab), downsampled to 250 Hz for consistency with LibriBrain. We formulate similar speech detection task using voice-activity annotations aligned with the MEG signal. Across subjects, speech occupied on average 78.6 2.3% of frames during listening and 74.8 3.9% during playback (small differences reflect minor variations in usable recording durations), and 75.6 3.7% during speech production, with the remaining frames corresponding to silence. This dataset is not publicly available due to ethical constraints. Full experimental details are provided in [24]. Crucially, the key distinction between datasets (50 hours of single-subject data versus 5 minutes per subject across 18 participants) enables us to investigate whether large-scale pretraining on one subject can improve decoding performance when fine-tuned on limited data from new subjects performing different speech tasks. 3.2. Model Architecture We pretrained the MEGConformer [23], compact Conformerbased encoder [22] adapted for MEG time-series that operates directly on windowed raw sensor segments (306 channels) after preprocessing and downsampling to 250 Hz. Accordingly, to match the limited amount of data available per subject and task in Bourguignon2020, we use 0.5 windows throughout and disable output smoothing, while keeping the remaining architectural choices and training recipe as close as possible to [23]. For fine-tuning, we retain the original optimizer and most hyperparameters, but introduce three lightweight task-specific modifications: (i) we select checkpoints using validation loss rather than F1-macro to avoid overfitting to specific metric; (ii) we introduce RollAugment, fast roll-based temporal augmentation that circularly shifts each training frame by fixed fractions of the window (25%, 50%, and 75%) and concatenates the shifted copies; and (iii) we use soft targets given by the fraction of speech within each window (instead of hard 0/1 labels) to smooth the labels slightly. Moreover, we use decimation by 4 to obtain 250 Hz (via anti-aliased resampling) and reduce earlystopping patience to 10 epochs for efficiency. 3.3. Experimental Protocol Subsequently, we pre-trained MEGConformer on LibriBrains 50-hour listening dataset, then fine-tuned on Bourguignon2020 using two experimental paradigms: (i) in-task, where models were trained and evaluated on the same task (Listen, Playback, Production), and (ii) cross-task, where models were trained on one task and evaluated on different task without retraining, covering all six possible train-test task pairings among Listen, Playback, and Production. For each subject and condition, we compared transfer learning (pre-trained then fine-tuned) against training from scratch (no pre-training). Performance was assessed using F1-macro, balanced accuracy, and AUC-macro. For LibriBrain pre-training, we used the official splits provided with the dataset [8, 7]. For Bourguignon2020, given the short duration of each recording and the absence of natural session boundaries, data were split independently for each subject and task into training (70%), validation (15%), and test (15%) sets using random shuffling at the frame level. Accordingly, to normalize the signal, input windows were z-scored for each subject and task (separately for each sensor and time point). Training and validation data used training-set statistics, while test data were normalized using their own statistics to support fair in-task and cross-task evaluation. All training runs were performed on individual NVIDIA H100 GPUs. Statistical significance was evaluated using the Wilcoxon signed-rank test [39], non-parametric method for paired comparisons when normality cannot be assumed [40]. For each comparison, we tested whether the median improvement (transfer learning minus baseline) differed from zero (p < 0.05), using subjects as the unit of replication. All p-values were corrected for multiple comparisons using the Holm-Bonferroni method [41]. Overall transfer learning effects across metrics and task conditions were assessed with subject-level omnibus tests based on aggregated improvements, using permutationbased sign-flip test (10,000 iterations) [42, 43]. Code availability. To ensure reproducibility, all code, preprocessing scripts, and model configurations are publicly available at https://github.com/hitz-zentroa/ meg-phone-decoding. 4. Results 4.1. In-Task Transfer Learning Table 1 presents in-task performance comparing models trained from scratch on Bourguignon2020 against models pre-trained on LibriBrain and fine-tuned on the same data. Transfer learning improved performance across nearly all metrics and tasks. For the listening task, transfer learning yielded significant gains Table 1: In-task evaluation results (mean std across subjects, in %). Improvements over baseline are marked in bold. Table 3: Cross-task decoding results for the pretrained model (mean std across subjects, in %). Improvements over baseline are marked in bold."
        },
        {
            "title": "Task",
            "content": "Accu. (%) F1 (%) AUC (%) Scratch (baseline) listen playback production 76.2 4.8 75.1 6.1 83.6 5. 85.5 3.2 84.0 4.7 89.7 3.5 64.0 9.4 67.7 4.2 81.1 8."
        },
        {
            "title": "Transfer Learning",
            "content": "listen playback production 79.0 4.8 76.0 6.0 84.2 4.9 87.7 3.2 85.4 4.4 90.3 3.4 68.7 6.2 67.2 8.6 82.0 8.0 Table 2: Cross-task decoding results for the model trained from scratch (mean std across subjects, in %). All reported crosstask results are significantly above chance. Train Test Accu. (%) F1 (%) AUC (%) listen to play. listen to prod. play. to listen play. to prod. prod. to listen prod. to play. 72.5 3.6 71.5 3.5 73.4 2.9 71.4 4.4 66.1 6.3 65.0 5.9 83.0 2.7 82.3 2.6 83.7 2.2 82.1 3.6 77.7 5.8 76.5 5.5 59.4 5.3 57.9 7.4 59.4 5.7 57.4 6.7 54.0 6.3 54.6 4. of +3.7% accuracy, +2.6% F1, and +7.3% AUC (W = 17.0, = 0.005). The playback task showed more modest, nonsignificant improvements of +1.2% accuracy, +1.7% F1, and -0.7% AUC (W = 45.0, = 0.163). Interestingly, despite being pre-trained exclusively on listening task, transfer learning improved all metrics for speech production, including +0.7% accuracy, +0.7% F1, and +1.1% AUC, though these differences did not reach statistical significance (W = 61.0, = 0.304). Across all tasks and metrics, sign-flip permutation test confirmed significant overall difference of transfer learning (p < 0.001). These results demonstrate that representations learned from large-scale single-subject listening data generalize to new subjects and tasks, even with only 5 minutes of data per subject. 4.2. Cross-Task Decoding Baseline To establish whether speech representations transfer across tasks, we evaluated models trained on one task and tested on another without transfer learning. Table 2 presents all six crosstask train-test pairs for models trained from scratch on Bourguignon2020. All cross-task decodings were individually significant across all metrics (paired Wilcoxon signed-rank test against chance, all < 0.05), and overall cross-task performance was highly significant according to permutation test (p < 0.001). This demonstrates that neural representations for speech processing are partially shared across listening, playback, and production tasks. Cross-task accuracy ranged from 65.0% to 73.4%, with perception tasks (listening and playback) transferring more effectively to each other (72.5-73.4%) than production transferring to perception tasks (65.0-66.1%). 4.3. Cross-Task with Transfer Learning Table 3 presents cross-task decoding performance with transfer learning from LibriBrain. Pre-training substantially improved Train Test Accu. (%) F1 (%) AUC (%) listen to play. listen to prod. play. to listen play. to prod. prod. to listen prod. to play. 76.9 4.0 75.3 5.2 78.0 3.9 73.6 7.1 69.3 8.0 68.3 7.4 86.5 2.8 85.3 3.9 87.1 2.9 83.7 6.1 80.1 7.3 79.0 7.2 61.1 4.5 57.3 7.3 61.6 5.8 58.1 9.1 56.0 5.7 56.6 3. Figure 1: Effect size of transfer learning for cross-task decoding. Bars show the mean improvement in F1 score (transfer learning minus training from scratch) for each task and traintest task pair. Error bars indicate the standard deviation across subjects. cross-task generalization across nearly all task combinations and metrics. As expected, perception tasks showed the strongest gains: listen-to-playback improved by +6.1% accuracy, +4.2% F1, and +2.9% AUC (W = 3.0, < 0.001), while playbackto-listen improved by +6.3% accuracy, +4.1% F1, and +3.7% AUC (W = 3.0, < 0.001). Cross-task decoding involving production also benefited significantly from transfer learning. Listen-to-production improved by +5.3% accuracy and +3.6% F1 (W = 22.0, = 0.016), while production-to-listen gained +4.8% accuracy, +3.1% F1, and +3.7% AUC (W = 36.0, = 0.048). Similarly, production-to-playback improved by +5.1% accuracy, +3.3% F1, and +3.7% AUC (W = 33.0, = 0.048). Across all cross-task combinations and metrics, sign-flip permutation test confirmed highly significant overall benefit of transfer learning (p < 0.001). These results show that large-scale pre-training improves cross-task speech detection even with minimal per-subject fine-tuning data, and that the improvements extend beyond perception-only tasks to include speech production. Figure 1 summarizes the effect size of transfer learning across all conditions. Notably, improvements were observed throughout, with more modest gains for in-task decoding (0.52.2%) and larger gains for cross-task decoding (1.7-3.5%), and with cross-task decoding involving production showing greater variability (error bars). Figure 2 reveals systematic asymmetries in bidirectional cross-task transfer. While listening and playback transferred bidirectionally with similar perFigure 2: Cross-task F1 asymmetry for each task pair. Each xaxis category shows two directions: circle markers correspond to the left-to-right direction in the label, and diamond markers to the reverse direction. For each model, the two directionspecific points are connected to highlight asymmetry. Error bars indicate the standard deviation across subjects. formance (Listen to Playback: 86.5% F1, Playback to Listen: 87.1% F1), cross-task combinations involving production showed clear directional preferences. Perception-to-production transfer (Listen to Production: 85.3% F1, Playback to Production: 83.7% F1) substantially outperformed production-toperception transfer (Production to Listen: 80.1% F1, Production to Playback: 79.0% F1). This asymmetry likely reflects the fact that production naturally involves auditory self-monitoring, whereas perception tasks do not engage motor planning representations [44, 45]. However, the above-chance performance of production-to-perception decoding confirms that production models capture shared neural speech representations beyond production-specific representations. The subject-level analysis in Figure 3 shows predominantly positive transfer effects, In perception though with substantial individual variability. tasks, 15 of 18 subjects showed improvements, while 2 (1, 2) exhibited clear negative effects. For production, 16 of 18 subjects benefited from transfer learning, though Subject 16 showed marked negative effect (-13.3% F1). Consequently, this inter-subject variability suggests that transfer learning effectiveness may depend on individual neural organization or recording quality, highlighting the need for subject-adaptive approaches in practical BCI applications. 5. Discussion and Conclusion In this work, we demonstrate that MEG-to-MEG transfer learning from large-scale single-subject MEG data enhances MEG speech decoding task (speech detection) across new subjects and tasks, even with only few minutes of subject-specific data. Pre-training on LibriBrain consistently improved in-task performance and, more importantly, substantially improved cross-task generalization between speech perception and production. This suggests that the MEGConformer learns representations that capture core neural processes underlying speech, rather than relying solely on task or subject-specific patterns. Specifically, key finding is the asymmetric nature of crosstask transfer. While perception tasks (listening and playback) transferred bidirectionally with comparable performance, deFigure 3: Subject-level in-task F1 improvement with transfer learning. Each bar shows the improvement in F1 score (transfer learning minus training from scratch) for single subject and task. Colors indicate the three tasks. coding from production to perception was consistently weaker than the reverse direction. This asymmetry is expected, given that speech production engages additional processessuch as motor planning, efference copy, and somatosensory feedback that are absent during passive perception, whereas perceptual representations form core subset of those recruited during production. As consequence, models trained on perception data can efficiently decode speech production tasks, whereas models trained on production must disentangle perceptual information from concurrent motor-related activity, yielding weaker transfer in the production-to-perception direction. Nonetheless, the ability of production-trained models to decode listening and playback tasks at above-chance levels provides further evidence that speech perception and production recruit overlapping neural circuitry, as indicated by interactions between ventral perceptual pathways and dorsal sensorimotor circuits that support contemporary dual-stream models of speech processing [46, 47, 44, 45, 48]. From practical perspective, our results highlight the feasibility of training effective MEG speech detection models with very limited per-subject data. Pre-training on LibriBrain consistently improved in-task performance, with larger gains observed in cross-task generalization between speech perception and production. In fact, statistically reliable gains emerged primarily in cross-task decoding rather than in-task performance. This is critical step toward practical neurotechnology applications, where long calibration sessions are often infeasible. Nevertheless, several limitations should be noted. First, we focus on speech detection task, which does not capture higher-level phoneme, word, or semantic representations, and pre-training and fine-tuning involved different languages (English and Spanish). Second, pre-training relied on single subject, and future work should assess whether multi-subject pretraining improves generalization. Finally, while improvements are consistent, their magnitude is modest and variable, indicating that transfer learning complements rather than replaces subject-specific adaptation. Looking ahead, future work will extend this approach to more complex speech tasks, including phone classification [13, 14], keyword spotting [6], and speech synthesis [3], and will explore transfer learning and cross-task decoding across larger and more diverse datasets. 6. Acknowledgements research was by the Basque IKUR-IKA-23/18, and supported This and Spanish Governments (grant project AIA2025-163317-C31 funded by MICIU/AEI / 10.13039/501100011033/). We are grateful to the DIPC Supercomputing Center and EJIE, the technological management body of the Basque Government, for providing essential technical and human resources. This work was carried out within the #neural2speech and brAIn2lang research teams. 7. References [1] J.-R. King, L. Gwilliams, C. Holdgraf, J. Sassenhagen, A. Barachant, D. Engemann, E. Larson, and A. Gramfort, Encoding and Decoding Framework to Uncover the Algorithms of Cognition, in The Cognitive Neurosciences. The MIT Press, 05 2020, vol. 6, pp. 691702. [Online]. Available: https://doi.org/10.7551/mitpress/11442.003.0076 [2] X. de Zuazo, V. Verbeni, L.-C. Ku, E. Arrieta, A. Barrena, A. Klimovich-Gray, I. Saratxaga, E. Navas, E. Agirre, and N. Molinaro, #neural2speech: Decoding speech and language from the human brain, in IberSPEECH 2024, 2024, pp. 256260. [3] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, Speech synthesis from neural decoding of spoken sentences, Nature, vol. 568, no. 7753, pp. 493498, Apr 2019. [Online]. Available: https://doi.org/10.1038/s41586-019-1119-1 [4] D. A. Moses, M. K. Leonard, J. G. Makin, and E. F. Chang, Real-time decoding of question-and-answer speech dialogue using human cortical activity, Nature Communications, vol. 10, no. 1, p. 3096, [Online]. Available: https://doi.org/10.1038/s41467-019-10994-4 Jul 2019. [5] A. B. Silva, J. R. Liu, S. L. Metzger, I. Bhaya-Grossman, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, A. Tu-Chan, K. Ganguly, D. A. Moses, and E. F. Chang, bilingual speech neuroprosthesis driven by cortical articulatory representations shared between languages, Nature Biomedical Engineering, vol. 8, no. 8, pp. 977991, Aug 2024. [Online]. Available: https://doi.org/10.1038/s41551-024-01207- [6] G. Elvers, G. Landau, and O. P. Jones, Elementary, my dear watson: Non-invasive neural keyword spotting in the libribrain dataset, in Data on the Brain & Mind Findings, 2025. [Online]. Available: https://openreview.net/forum?id=gRJ9dd07QF [7] M. Ozdogan, G. Landau, G. Elvers, D. Jayalath, P. Somaiya, F. Mantegna, M. Woolrich, and O. P. Jones, Libribrain: Over 50 hours of within-subject MEG to improve speech in The Thirty-ninth Annual decoding methods at Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. https: //openreview.net/forum?id=smlCP7jHN3 [Online]. Available: scale, [8] G. Landau, M. Ozdogan, G. Elvers, F. Mantegna, P. Somaiya, D. Jayalath, L. Kurth, T. Kwon, B. Shillingford, G. Farquhar, M. Jiang, K. Jerbi, H. Abdelhedi, Y. Mantilla Ramos, C. Gulcehre, M. Woolrich, N. Voets, and O. Parker Jones, The 2025 PNPL competition: Speech detection and phoneme classification in the LibriBrain dataset, NeurIPS Competition Track, 2025. [9] M. Sato, K. Tomeoka, I. Horiguchi, K. Arulkumaran, R. Kanai, and S. Sasai, Scaling law in neural data: Non-invasive speech decoding with 175 hours of EEG data, 2024. [Online]. Available: https://arxiv.org/abs/2407.07595 [10] S. dAscoli, C. Bel, J. Rapin, H. Banville, Y. Benchetrit, C. Pallier, and J.-R. King, Decoding individual words from non-invasive brain recordings across 723 participants, 2024. [Online]. Available: https://arxiv.org/abs/2412. [11] D. Dash, P. Ferrari, S. Malik, A. Montillo, J. A. Maldjian, and J. Wang, Determining the optimal number of MEG trials: machine learning and speech decoding perspective, in Brain Informatics, S. Wang, V. Yamamoto, J. Su, Y. Yang, E. Jones, L. Iasemidis, and T. Mitchell, Eds. Cham: Springer International Publishing, 2018, pp. 163172. [12] D. Dash, P. Ferrari, S. Malik, and J. Wang, Overt speech retrieval from neuromagnetic signals using wavelets and artificial neural networks, in 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), 2018, pp. 489493. [13] X. de Zuazo, E. Navas, I. Saratxaga, M. Bourguignon, and N. Molinaro, Phone pair classification during speech production using MEG recordings, in IberSPEECH 2024, 2024, pp. 7680. [14] X. de Zuazo, E. Navas, I. Saratxaga, M. Bourguignon, and N. Molinaro, Decoding phone pairs from MEG signals across speech modalities, Computer Speech & Language, vol. 99, p. 101939, 2026. [Online]. Available: https://www.sciencedirect. com/science/article/pii/S0885230826000033 [15] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, nature, vol. 521, no. 7553, pp. 436444, 2015. [16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in Neural Information Processing Systems, vol. 30. Inc., 2017. [Online]. Available: https://Proc..neurips.cc/paper files/paper/ 2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf Curran Associates, [17] H. J. Banville, Y. Benchetrit, S. dAscoli, J. Rapin, and J.-R. King, Scaling laws for decoding images from brain activity, CoRR, vol. abs/2501.15322, January 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2501.15322 [18] Y. Benchetrit, H. Banville, and J.-R. King, Brain decoding: toward real-time reconstruction of visual perception, in The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id= 3y1K6buO8c [Online]. Available: [19] A. Defossez, C. Caucheteux, J. Rapin, O. Kabeli, and J.-R. King, Decoding speech perception from non-invasive brain recordings, Nature Machine Intelligence, vol. 5, no. 10, pp. 10971107, 2023. [20] D. and O. P. Unlocking Jones, Jayalath, G. Landau, in ICML 2025 Generative AI non-invasive brain-to-text, and Biology (GenBio) Workshop, 2025. [Online]. Available: https://openreview.net/forum?id=lRVOPe1EZy [21] S. Jhilal, S. Martin, and A.-L. Giraud, Transfer learning from imagenet for MEG-based decoding of imagined speech, 2026. [Online]. Available: https://arxiv.org/abs/2601.15909 [22] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, Conformer: Convolution-augmented transformer for speech recognition, in Interspeech, 2020, pp. 50365040. [23] X. de Zuazo, I. Saratxaga, and E. Navas, MEGConformer: Conformer-based MEG decoder for robust speech and phoneme classification, 2025. [Online]. Available: https://arxiv.org/abs/ 2512.01443 [24] M. Bourguignon, N. Molinaro, M. Lizarazu, S. Taulu, V. Jousmaki, M. Lallier, M. Carreiras, and X. De Ti`ege, Neocortical activity tracks the hierarchical linguistic structures of self-produced speech during reading aloud, NeuroImage, vol. 216, p. 116788, 2020. [Online]. Available: https://www. sciencedirect.com/science/article/pii/S [25] F. Magnabosco and O. Hauk, Decoding semantics from dynamic brain activation patterns: From trials to task in eeg/meg source space, eNeuro, vol. 11, no. 3, 2024. [Online]. Available: https://www.eneuro.org/content/11/3/ENEURO.0277-23.2023 [26] B. Aristimunha, D. Truong, P. Guetschel, S. Y. Shirazi, I. Guyon, A. R. Franco, M. P. Milham, A. Dotan, S. Makeig, A. Gramfort, J.-R. King, M.-C. Corsi, P. A. Valdes-Sosa, A. Majumdar, A. Evans, T. J. Sejnowski, O. Shriki, S. Chevallier, and A. Delorme, Eeg foundation challenge: From crosstask to cross-subject eeg decoding, 2025. [Online]. Available: https://arxiv.org/abs/2506.19141 [27] B. S. Philip, G. Prasad, and D. J. Hemanth, Non-stationarity techniques in MEG data: review, Procedia removal [44] G. Hickok and D. Poeppel, The cortical organization of speech processing, Nature reviews neuroscience, vol. 8, no. 5, pp. 393 402, 2007. [45] J. F. Houde and S. S. Nagarajan, Speech production as state feedback control, Frontiers in human neuroscience, vol. 5, p. 82, 2011. [46] G. Hickok and D. Poeppel, Dorsal and ventral streams: framework for understanding aspects of the functional anatomy of language, Cognition, vol. 92, no. 12, pp. 6799, 2004. [47] F. Pulvermuller, M. Huss, F. Kherif, F. Moscoso del Prado Martin, O. Hauk, and Y. Shtyrov, Motor cortex maps articulatory features of speech sounds, Proceedings of the National Academy of Sciences, vol. 103, no. 20, pp. 78657870, 2006. [48] G. B. Cogan, T. Thesen, C. Carlson, W. Doyle, O. Devinsky, and B. Pesaran, Sensorymotor transformations for speech occur bilaterally, Nature, vol. 507, no. 7490, pp. 9498, 2014. Computer Science, vol. 215, pp. 824833, 2022, 4th International Conference on Innovative Data Communication Technology and Application. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1877050922021561 [28] G. P. Beril Susan Philip and D. J. Hemanth, systematic review on artifact removal and classification techniques for enhanced MEG-based BCI systems, Brain-Computer Interfaces, vol. 10, no. 2-4, pp. 99113, 2023. [Online]. Available: https://doi.org/10.1080/2326263X.2023.2233368 [29] D. Dash, P. Ferrari, D. Heitzman, and J. Wang, Decoding speech from single trial MEG signals using convolutional neural networks and transfer learning, in 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2019, pp. 55315535. [30] S. Suzuki, C.-C. D. Hsu, Y. Tsao, and K. Sugiura, MEGState: Phoneme decoding from magnetoencephalography signals, 2025. [Online]. Available: https://arxiv.org/abs/2512.17978 [31] A. Klimovich-Gray, G. Di Liberto, L. Amoruso, A. Barrena, E. Agirre, and N. Molinaro, Increased top-down semantic processing in natural speech linked to better reading in dyslexia, NeuroImage, vol. 273, p. 120072, 2023. [Online]. Available: https://www.sciencedirect.com/science/article/ pii/S1053811923002185 [32] M. Sarma, C. Bond, S. Nara, and H. Raza, MEGNet: MEGbased deep learning model for cognitive and motor imagery classification, in 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2023, pp. 25712578. [33] J. Kwon, D. Harwath, D. Dash, P. Ferrari, and J. Wang, Direct speech synthesis from non-invasive, neuromagnetic signals, in Interspeech 2024, 2024, pp. 412416. [34] B. Accou, J. Vanthornhout, H. V. hamme, and T. Francart, Decoding of the speech envelope from EEG using the VLAAI deep neural network, Scientific Reports, vol. 13, no. 1, p. 812, Jan 2023. [Online]. Available: https://doi.org/10.1038/ s41598-022-27332-2 [35] X. Xu, B. Wang, Y. Yan, H. Zhu, Z. Zhang, X. Wu, and J. Chen, ConvConcatNet: deep convolutional neural network to reconstruct mel spectrogram from the EEG, in 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW). IEEE, 2024, pp. 113114. [36] J.-M. Schoffelen, R. Oostenveld, N. H. L. Lam, J. Udden, A. Hulten, 204-subject multimodal neuroimaging dataset to study language processing, Scientific Data, vol. 6, no. 1, p. 17, Apr 2019. [Online]. Available: https://doi.org/10.1038/s41597-019-0020-y and P. Hagoort, [37] L. Gwilliams, G. Flick, A. Marantz, L. Pylkkanen, D. Poeppel, and J.-R. King, Introducing MEG-MASC high-quality magneto-encephalography dataset for evaluating natural speech processing, Scientific Data, vol. 10, no. 1, p. 862, Dec 2023. [Online]. Available: https://doi.org/10.1038/s41597-023-02752-5 [38] J. Levy, M. Zhang, S. Pinet, J. Rapin, H. Banville, S. dAscoli, and J.-R. King, Brain-to-Text decoding: non-invasive https: approach via typing, //arxiv.org/abs/2502.17480 [Online]. Available: 2025. [39] F. Wilcoxon, Individual comparisons by ranking methods, Biometrics Bulletin, vol. 1, no. 6, pp. 8083, 1945. [Online]. Available: http://www.jstor.org/stable/3001968 [40] G. Santafe, I. Inza, and J. A. Lozano, Dealing with the evaluation of supervised classification algorithms, Artificial Intelligence Review, vol. 44, no. 4, pp. 467508, 12 2015. [Online]. Available: https://doi.org/10.1007/s10462-015-9433-y [41] S. Holm, simple sequentially rejective multiple test procedure, Scandinavian journal of statistics, pp. 6570, 1979. [42] E. J. Pitman, Significance tests which may be applied to samples from any populations, Supplement to the Journal of the Royal Statistical Society, vol. 4, no. 1, pp. 119130, 1937. [43] P. Good, Permutation, parametric and bootstrap tests of hypotheses. Springer, 2005."
        }
    ],
    "affiliations": [
        "Basque Center on Cognition, Brain and Language BCBL, Spain",
        "HiTZ Center, University of the Basque Country UPV/EHU, Spain",
        "Ikerbasque, Basque Foundation for Science, Spain",
        "Laboratoire de Neuroanatomie et Neuroimagerie translationnelles (LN2T), ULB Neuroscience Institute, Universite libre de Bruxelles (ULB), Belgium",
        "Laboratory of Functional Anatomy, Faculty of Human Motor Sciences, Universite libre de Bruxelles (ULB), Belgium",
        "WEL Research Institute, Belgium"
    ]
}