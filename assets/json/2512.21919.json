{
    "paper_title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
    "authors": [
        "KaShun Shum",
        "Binyuan Hui",
        "Jiawei Chen",
        "Lei Zhang",
        "X. W.",
        "Jiaxi Yang",
        "Yuzhen Huang",
        "Junyang Lin",
        "Junxian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 1 9 1 2 . 2 1 5 2 : r 2025-12-29 SWE-RM: EXECUTION-FREE FEEDBACK FOR SOFTWARE ENGINEERING AGENTS Kashun Shum1,2, Binyuan Hui2, Jiawei Chen2, Lei Zhang2, X. W.2, Jiaxi Yang2 Yuzhen Huang1, Junyang Lin2, Junxian He1 1 The Hong Kong University of Science and Technology, 2 Qwen Team, Alibaba Group Equal Contribution {ksshumab,junxianh}@cse.ust.hk, binyuan.hby@alibaba-inc.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the models ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting mixture-ofexperts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models. On RL training, SWE-RM lifts the resolve rate of execution-based counterparts by 3 absolute points on SWE-Bench Verified."
        },
        {
            "title": "INTRODUCTION",
            "content": "The automation of complex software development tasks through coding agents represents significant frontier in large language models (LLMs). critical component in developing these agents is the feedback mechanism used during training and evaluation, particularly through reinforcement learning (RL) (Wei et al., 2025; Qwen Team, 2025) and test-time scaling (TTS) (Xia et al., 2024; Jain et al., 2025). Broadly, these mechanisms fall into two categories: execution-based verifiers (Xia et al., 2024; Jain et al., 2025), which rely on concrete outcomes like unit test results, and executionfree verifiers1 (Pan et al., 2025; Luo et al., 2025), which are typically model-based reward models that provide continuous score without sandbox environments. While widely used, execution-based feedback has inherent limitations. It provides only sparse, binary signal (pass/fail), which makes it difficult to distinguish between different successful or unsuccessful trajectories. Beyond this lack of granularity, unit tests require comprehensive coverage to yield accurate assessments, which is often unavailable. To address this challenge, exiting works rely 1Throughout this paper, we will use reward model and execution-free verifier interchangeably. 1 Preprint. Figure 1: The pass@1 and TTS resolve rate of various open-source and proprietary models on SWEBench Verified. Part of the baseline results are referenced from He et al. (2025). on extracting unit test from Github repos (Jimenez et al., 2024) or model-generated unit tests (Yang et al., 2025; Jain et al., 2025) that are not rigorously validated. For example, in issue-fixing tasks, the unit tests used from real GitHub repositories are often overly specific, and in some cases, entirely unrelated to the target issue (OpenAI, 2025). As result, execution-based feedback limits the code data that can be used for effective reinforcement learning or test-time scaling due to requirements of high-quality unit tests. When such tests are unreliable, the resulting feedback becomes significant challenge for RL, where nuanced and consistent reward signals are essential. Execution-free feedback offers compelling alternative by providing continuous, fine-grained scores across entire trajectories, allowing for better discrimination among candidate solutions and reducing bias toward specific patches. Despite its promise, execution-free feedback remains largely underexplored, and its properties in the context of SWE agents are not yet well understood. In this work, we aim to develop versatile and effective reward model usable across different scenarios such as TTS and RL for software engineering. While it is straightforward to adopt TTS (e.g., best of k) performance directly as the metric to guide the reward model training (Pan et al., 2025), our initial findings reveal that two verifiers with nearly identical TTS performance can show drastically different behavior in RL. This leads us to fundamental research question: What properties determine reward models effectiveness in RL training, and how can we develop an all-round SWE reward model that performs well in both TTS and RL? Intuitively, TTS primarily measures verifiers ability to rank the correct solution highest among multiple candidates, but it overlooks aspects that are essential for RL: the ability to effectively distinguish correct from incorrect trajectories and to produce scores that reliably correspond to the degree of correctness. Based on this observation, we further evaluate reward models using additional metrics: AUC, which reflects the correctness of relative ordering across trajectories, and ECE (Guo et al., 2017), which measures calibration representing whether the verifiers scores align with empirical correctness. We demonstrate that AUC and calibration provide complementary information to TTS and are both critical for ensuring the reward model delivers reliable signals in RL. To train reward model that performs well across these metrics, we conduct large-scale ablation studies examining the effects of training data scale, the ratio of positive to negative samples, mixtures of data sources, and context length. These investigations lead to practical recipe for building robust, execution-free reward models tailored to SWE tasks. Guided by these investigations, we obtain SWE-RM, an accurate and robust reward model with 30B total and 3B activated parameters for advancing SWE agents. On SWE-bench Verified (OpenAI, 2025), SWE-RM lifts the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0% and Qwen3-Coder-Max from 67.0% to 74.6%, achieving best-in-class among 30B-level and all open-source models respectively, as shown in Figure 1. Moreover, SWE-RM is highly effective when used as reward signal in agentic RL training. 2 Preprint. Figure 2: RL training curves of two verifiers with similar TTS performance. Despite comparable TTS, the downstream RL outcomes differ drastically. For example, it improves the RL performance of execution-based counterparts by 3 absolute points on SWE-bench Verified. Figure 3: Two models with similar TTS performance (Model with +4.7% and Model with +4.5%) show significant differences in their AUC scores."
        },
        {
            "title": "2 RELATED WORK",
            "content": "In Software Engineering (SWE) tasks, verifiers fall into two categories: execution-based, which rely on unit tests (e.g., Agentless (Xia et al., 2024), R2E-Gym (Jain et al., 2025), DeepSWE (Luo et al., 2025)), and execution-free, which use model-based scoring (e.g., SWE-Gym (Pan et al., 2025), OpenHands Critic (Team, 2025)). Existing work on execution-free verifiers has primarily emphasized test-time scaling (Pan et al., 2025; Jain et al., 2025; Luo et al., 2025), with limited attention to other quality dimensions. We show that an execution-free verifiers quality also depends on classification accuracy and calibration, and provide systematic study of training such verifiers. In reinforcement learning for SWE agents (e.g., OpenHands (Wang et al., 2025), SWE-Agent (Yang et al., 2024)), execution-based feedbackakin to rule-based metrics in math (DeepSeek-AI, 2025)has enabled recent model training (e.g., Qwen3-Coder (Qwen Team, 2025), GLM-4.5 (Team et al., 2025a), MiniMax-M1 (MiniMax, 2025)), but is constrained by noisy test suites and sparse signals. We are the first to integrate execution-free feedback into SWE agentic RL, demonstrating its potential to deliver finer-grained rewards and improve efficiency. An extended related work are discussed in Appendix B."
        },
        {
            "title": "3 WHAT DEFINES A VERSATILE REWARD MODEL FOR SWE?",
            "content": "Our goal is to develop versatile reward model that performs well across both TTS and RL. Following common practice, we begin by examining whether TTS performance can serve as reliable guide for selecting reward model for RL. We first present our initial findings that highlight the limitations of relying solely on TTS performance to guide reward model training. This result demonstrates that TTS alone cannot explain downstream success in RL, raising important questions about what properties of verifier matter. To resolve this gap, we next revisit the role of TTS as an evaluation metric, analyze its limitations, and introduce complementary criteriaAUC and calibrationthat provide more holistic view of verifier quality. 3.1 INITIAL FINDINGS: LIMITATIONS OF RELYING SOLELY ON TTS As we aim to develop versatile reward model that can be applied across different scenarios such as TTS and RL, but it is unknown what defines such versatile reward model and whether TTS and RL impose different requirements. natural question to ask is whether reward model that performs well on TTS will also perform well on RL. To avoid ambiguity, the reward model is trained purely through supervised next-token prediction instead of using TTS as the training signal. TTS 3 Preprint. is used exclusively as an evaluation metric for model selection etc. More training details will be introduced in 4.1. Our initial exploration reveals an intriguing finding: two execution-free verifiers that achieve nearly identical TTS improvements give rise to strikingly different behaviors when used as reward models in reinforcement learning. As shown in Figure 2, both verifier and verifier achieve similar TTS improvements, indicating, at first glance, that they are equally effective at choosing the correct solution highest among candidate trajectories. Yet, when deployed in RL training, verifier supports smooth improvement, while verifier exhibits significant instability, failing to provide reliable learning signals and eventually causing RL training to collapse. This result challenges the widely adopted view of TTS as sufficient proxy for verifier quality (Lightman et al., 2023; Pan et al., 2025). If two models are judged equivalent by TTS but behave so differently in RL, then TTS alone cannot capture the aspects of verifier that truly matter for reinforcement learning. In other words, TTS provides only partial picture: it summarizes top-1 ranking ability but hides other properties that directly affect how reward signals shape policy updates. These findings prompt us to ask fundamental research question: What properties determine reward models effectiveness in RL training, and how can we develop an all-round SWE reward model that performs well in both TTS and RL? To answer this, we must carefully reconsider what TTS actually measures, why it fails to explain the RL discrepancy observed, and what alternative metrics can reveal the missing dimensions of verifier quality. This motivates our shift beyond TTS to broader and versatile evaluation. 3.2 MOVING BEYOND TTS: THE NEED FOR MORE VERSATILE EVALUATION ON VERIFIER QUALITY At first glance, TTS appears to be natural metric: it checks whether the correct solution trajectory is ranked highest among pool of candidates. Intuitively, this reflects verifiers ability to make the right top-1 decision, however, TTS only measures narrow slice of verifier capability. By focusing exclusively on whether the single best trajectory is ranked first, TTS ignores two properties that become critical once the verifier is used as reward model in reinforcement learning. The first overlooked dimension is discriminative ability. In RL, the agent generates wide range of trajectories, some of which are unresolved. The verifier must provide accurate feedback not just for the best trajectory, but across many near-miss or partially correct candidates. verifier with weak discriminative ability will assign similar scores to both correct and incorrect trajectories, producing noisy reward signals that compromise policy updates. The second is confidence reliability, or calibration. In RL, verification scores are often interpreted as proxies for the likelihood of correctness, serving as the reward magnitudes used to guide policy learning. If these scores are mis-calibrated, for instance, normalized score of 0.9 reflects only 60% probability of being actual correctthen the policy receives misleading signals about the expected value of its actions. Poor calibration can therefore poison the reward shaping process, leading to unstable or collapsed training dynamics even if top-1 accuracy (TTS) appears satisfactory. These overlooked dimensions provide natural explanation for the discrepancies we observed in Figure 2, and to capture these dimensions, we supplement TTS with two complementary metrics. AUC (Bradley, 1997) evaluates discriminative ability by measuring how well the verifier separates resolved from unresolved trajectories across the entire distribution, rather than focusing only on the best. Calibration (Wang, 2025) quantifies the alignment between predicted confidence scores and empirical correctness, for example by using Expected Calibration Error (ECE) (Guo et al., 2017). higher AUC means models can better descriminate resolved and unresolved trajectories while lower ECE means there is lower mismatch between confidence and accuracy, indicating higher reliability. Together, these three metricsTTS, AUC, and calibrationform more versatile evaluation toolkit: they jointly capture top-1 ranking accuracy, overall discriminative power, and reliability of confidence estimates. Empirical analysis demonstrates the importance of considering all three. (1) Discriminative gap despite equal TTS: As shown in Figure 3, verifier and verifier obtain nearly identical TTS improvements (+4.7% vs. +4.5%), yet their AUC scores differ by 0.095. Thus, although both appear equivalent by TTS, only verifier reliably distinguishes resolved from unresolved trajectoriesa property crucial for producing consistent reward signals. (2) Calibration and score distribution disparity: Figure 4 reveals that verifier suffers from widespread overand under-confidence, 4 Preprint. Figure 5: Score distribution cases for verifier (left) and verifier (right) with similar TTS performance. Figure 4: Reliability diagrams for verifier (left) and verifier (right) with similar TTS performance. while verifier is three times better calibrated according to expected calibration error. Again, TTS fails to reflect this difference, though it directly affects the trustworthiness of reward magnitudes. Figure 5 further highlights why this matters: across 32 runs of random selected 8 instances, Model consistently assigns high scores to resolved trajectories and low scores to unresolved ones, making it possible to set reliable threshold for acceptance. In contrast, Model frequently assign unexpectedly low scores for resolved trajectories, while unresolved trajectories receive inflated scoresproducing overlapping distributions that might confuse policy models. The trajectories used for calibration and score distribution analysis are sampled from Qwen3-Coder-Max while the other settings are same as the one we will show in Appendix D.1. This combination of miscalibration and poor separation explains why Model provides misleading signals in RL even when its TTS remains competitive. We also include theoretical analysis between the three metrics and RL dynamics in Appendix C. These observations underscore that TTS, accuracy, and calibration are complementary metrics, each capturing distinct aspect of verifier capability. The discussion of metrics in this section is limited to using early, small-scale RM variants as illustrative examples. And in subsequent investigationbeginning after we motivate the need for AUC and calibrationmarks the start of the actual supervised RM training and large-scale, comprehensive ablations. We will show how to obtain versatile and robust reward model in 4 and discuss the implications for reinforcement learning in 5."
        },
        {
            "title": "4 HOW TO TRAIN A VERSATILE REWARD MODEL FOR SWE?",
            "content": "To build versatile and robust reward model as discussed in 3, we conclude and analyze several critical factors that significantly influence final performance. Specifically, we systematically investigate training data scale, the ratio of positive to negative samples, policy, data source, and context length, and discuss their impact on the verifiers three core abilities. These observations collectively guide the development of SWE-RM, which achieves superior performance. 4.1 TRAINING METHODS Following SWE-Gym (Pan et al., 2025), we formulate reward modeling as generative classification task, where the reward model takes trajectory as input and outputs special token (e.g., YES/NO). Given the full multi-turn trajectory, the model is prompted to output single special token, either YES (resolved) or NO (unresolved). And the supervised fine-tuning utilizes standard next-token prediction loss on this special token. At inference time, by obtaining the log probability of the special token YES(ly) and NO(ln), the final score is calculated by exp (ly)/(exp (ly) + exp (ln)), which maps to continuous reward model score [0, 1]. To construct training data, we collect agent trajectories by deploying different policy models (Qwen3-Coder and Claude-4) to interact with the agent scaffold OpenHands (Wang et al., 2025) across multiple training data sources, including SWE-Gym (Pan et al., 2025), SWE-rebench (Badertdinov et al., 2025), SWE-smith (Yang et al., 2025), and R2E-Gym (Jain et al., 2025). These trajectories are then labeled as positive or negative based on their execution results with the provided fail2pass test. 5 Preprint. Figure 6: Left: Test-time Scaling curve of models trained with different # of training data. Right: Distribution of verifier scores across all evaluated trajectories. Clearer separation between resolved and unresolved trajectories, along with lower ECE, indicates better performance. RM Training Setup We use Qwen3-30B-A3B (Qwen Team, 2025) as the base model for reward model training, as it provides balance between efficiency and strong coding capabilities. Evaluation is primarily conducted on SWE-bench Verified (Jimenez et al., 2024), curated subset of 500 human-verified tasks designed to reliably assess model performance on real-world software engineering problems. For the evaluation metrics, test-time scaling is measured using 32 independent runs for each instance on SWE-bench Verified. Accuracy is calculated using the AUC score across all 32 500 trajectories, while calibration is assessed by the expected calibration error (Guo et al., 2017). RM@K is defined as the resolve rate of the final selected trajectories from samples. For each < 32, we report the mean and variance over 5 random runs to ensure fair evaluation. Further details on the reward model training setup can be found in Appendix D. 4.2 DATA SCALING AND RATIO EFFECT Poorly trained reward models often exhibit unexpected behavior when evaluated on out-of-domain (OOD) data. This OOD generalization challenge is particularly severe in SWE tasks, where multiturn interactions create substantially larger output space compared to traditional reasoning tasks, which might requires more training data. As shown in Figure 6, we uniformly sampled varying amounts of training data from different policy models and data sources. The left subfigure demonstrates that models trained on more than 20k samples generally achieve improved test-time scaling performance as increases, whereas models trained on fewer samples (e.g., fewer than 5k) may even experience declining performance. We attribute this to the limited generalization capacity of under-trained models: as grows, the probability of encountering OOD trajectories increases, and test-time scaling becomes highly sensitive to such cases. Even single erroneously high score assigned to an incorrect trajectory can significantly distort the final resolve rate. While TTS performance improves as the training data size increases up to 25k, further expansion to 100k yields diminishing returns. the score distributions in the right subfigure of Figure 6 reveal that larger training datasets enhance discriminative ability, as evidenced by clearer separation between resolved and unresolved trajectories. Moreover, models trained on more data demonstrate improved calibration: for instance, model trained with only 500 examples has an ECE of 0.481seven times higher than that of model trained with 100k examples. This indicates that scaling up training data produces more reliable scores, highlighting the effectiveness of data scaling. To further investigate the role of data composition, we fix the total amount of training data and vary the ratio of positive to negative trajectories, as shown in Table 1. We observe that across both model scales, the 2:1 ratio generally achieves the best overall performance in terms of AUC, calibration, and test-time scaling. Due to the limited availability of positive data, we experiment with ratios up to 2:1. To further investigate the role of data composition, we fix the total amount of training data and vary the ratio of positive to negative trajectories, as shown in Table 1. We observe that across both model scales, the 2:1 ratio generally achieves the best overall performance in terms of AUC, calibration, and test-time scaling. By contrast, more balanced ratios such as 1:1 avoid extreme skew but still fall short of the 2:1 configuration. Importantly, the 2:1 ratio also offers higher efficiency, as it requires smaller pool of negative data while still utilizing all available positive data in practice. Preprint. Table 1: Effect of training verifiers with different positive-to-negative data ratios on AUC, ECE, and test-time scaling performance (best results in bold). RATIO 2 : 1 1 : 1 1 : 2 1 : 4 1 : 8 Qwen3-Coder-Flash Qwen3-Coder-Max AUC ECE RM@32 AUC ECE RM@32 0.805 0.782 0.789 0.789 0.778 0.080 0.132 0.235 0.185 0. 62.0% 0.755 0.734 60.8% 0.736 61.0% 0.742 61.6% 0.738 60.2% 0.121 0.157 0.371 0.299 0.541 71.0% 70.2% 69.4% 71.8% 70.6% Considering this balance between effectiveness and efficiency, we adopt the 2:1 ratio as the default configuration in subsequent experiments."
        },
        {
            "title": "4.3 CONTEXT LENGTH CONSTRAINT",
            "content": "Table 2: Effect of Context Length Scaling on verifiers score rate and test-time scaling performance. Score rate represent how many percent of trajectories can be successfully scored without exceeding the context window. While previous execution-free verifiers in SWE mainly support context length of 32k (Pan et al., 2025; Jain et al., 2025), our executionfree verifiers are the first to scale up to 256k context length, enabling the scoring of complex and long trajectories. This is especially important for challenging questions, which typically involve extremely long contexts. As shown in Table 2, only when the context length is extended to 128k can more than 99% of trajectories be successfully scored without exceeding the limit. Furthermore, as models are able to score more trajectories, execution-free verifiers achieve better test-time scaling performance, as reflected in the increasing RM@32. more detailed discussion on context length are shown in Appendix D.4. 66.8% 67.4% 70.6% 73.0% 74.4% 0.5% 12.5% 88.3% 99.5% 100% 16 32 64 128 256 CONTEXT LEN. SCORE RATE RM@32 4.4 POLICY AND SOURCE ABLATION We also examine the impact of training data collected from different policy models on verifier performance. For on-policy data, we sample training examples using the corresponding Flash/Max model on SWE-rebench, while for off-policy data we sample using Claude-sonnet-4 (Anthropic). As shown in Table 3 policy ablation, while on-policy data sometimes yields stronger results on specific metrics (e.g., TTS on Qwen3-Coder-Max), overall the Mix-Policy setting provides better balance across AUC, ECE, and ranking. This indicates that combining onand off-policy data enhances the generalization ability of the verifier. Such findings also reflect the advantage of our comprehensive evaluation in revealing robust trends that TTS-only analyses might overlook. We further investigate the impact of training data sources on verifier performance. As shown in Table 3, under single-source settings, SWE-rebench achieves the best results in both AUC and RM@32, indicating that rebench may provide the highest-quality data. However, incorporating SWE-smith and SWE-Gym leads to improved calibration (lower ECE), and adding more sources enhances data scaling effects as we shown in 4.2. Based on these observations, our final setup uses mixture primarily derived from SWE-rebench, supplemented with data from SWE-smith and SWE-Gym, achieving balance between quality, calibration, and scalability."
        },
        {
            "title": "5 SWE-RM: A VERSATILE REWARD MODEL FOR TTS AND RL",
            "content": "In this section, we present SWE-RM, an accurate and robust execution-free verifier that not only achieves state-of-the-art test-time scaling performance but also significantly improves downstream reinforcement learning. We first demonstrate the superior performance of SWE-RM in TTS ( 5.1) and then RL ( 5.2). 7 Preprint. Table 3: Effect of training verifiers with different policy mixture and data source on three core abilities(best results in bold). METHODS / MODELS Qwen3-Coder-Flash Qwen3-Coder-Max AUC ECE RM@32 AUC ECE RM@32 On-Policy Off-Policy Mix-Policy Policy Ablation 0.785 0.778 0.804 0.148 0.113 0.033 Source Ablation 58.6 58.2 59.6 0.727 0.728 0.751 0.067 0.145 0.082 0.814 SWE-rebench (Badertdinov et al., 2025) 0.781 SWE-smith (Yang et al., 2025) 0.776 SWE-Gym (Pan et al., 2025) SWE-Gym + SWE-smith 0.813 0.802 SWE-Gym + SWE-rebench SWE-rebench + SWE-smith 0.807 SWE-rebench + SWE-smith + SWE-Gym 0.807 0.076 0.033 0.087 0.034 0.087 0.138 0.067 0.612 0.584 0.588 0.602 0.61 0.596 0. 0.774 0.736 0.742 0.772 0.762 0.765 0.766 0.048 0.039 0.044 0.035 0.039 0.107 0.033 71.0 70.6 70.2 0.718 0.70 0.714 0.72 0.712 0.714 0.718 Table 4: Comparison of different verifiers on three core abilities. Evaluation trajectories are sampled from Qwen3-Coder and OpenHands-LM-32B on SWE-bench Verified. EB means executionbased verifier while EF stands for execution-free verifier. Best results are in bold. VERIFIER TYPE OpenHands-LM-32B Qwen3-Coder-Flash Qwen3-Coder-Max AUC ECE RM@ AUC ECE RM@32 AUC ECE RM@32 AGENTLESS (Xia et al., 2024) SWE-GYM (Pan et al., 2025) DEEP SWE (Luo et al., 2025) SWE-RM-30A3B EB EF EB EF EF - 0.718 - 0.732 0.748 - 0.164 - 0. 0.080 42.4% 41.6% 44.2% 44.6% - 0.776 - 0.758 48.8% 0.783 - 0.223 - 0.124 0. 52.6% 51.2% 54.6% 53.2% - 0.752 - 0.74 62.0% 0.768 - 0.283 - 0.139 0.047 65.0% 65.4% 67.6% 66.2% 74.6% 5.1 NEW STATE-OF-THE-ART IN TTS Based on the investigation in 4, our final trained SWE-RM achieves state-of-the-art performance compared with previous works. We begin by discussing the baselines and evaluation setup, followed by an analysis of the SWE-RM results on TTS, AUC, and calibration. Baselines We compare our trained execution-free verifier against several existing execution-free and execution-based verifiers: (1) Agentless (Xia et al., 2024), an execution-based method that generates reproduction tests for each trajectory and re-ranks them based on test results; (2) SWE-Gym Verifier (Pan et al., 2025), an execution-free verifier based on Qwen2.5-32B and trained on the SWE-Gym dataset; (3) DeepSWE-EB Verifier (Luo et al., 2025), the execution-based component of the current state-of-the-art DeepSWE Hybrid-TTS. This verifier extends the R2E-Gym executionbased verifier (Jain et al., 2025) and follows similar mechanism to Agentless; (4) DeepSWE-EF Verifier (Luo et al., 2025), the execution-free component of DeepSWE Hybrid-TTS, which improves upon the R2E-Gym execution-free verifier. The evaluation setup for SWE-RM are same as the setting illustrated in 4.1. SWE-RM performance Our results in Table 4 show that SWE-RM consistently outperforms all baselines across AUC, ECE, and RM@32, achieving the best TTS, discrimination and calibration ability. The gains are not limited to Qwen3-Coder series models, where RM@32 improves pass@1 by 7-10 points, but also extend to OpenHands-LM-32B, where SWE-RM delivers the highest overall performance. This demonstrates the generalization ability of our verifier. 5.2 REINFORCEMENT LEARNING WITH EXECUTION-FREE FEEDBACK IN SWE Different from reinforcement learning in math problems, which easily receive scalable, correct reward by comparing with the ground truth answers, reinforcement learning from verifiable reward (RLVR) are facing two major challenges in software engineering tasks: (1) Most training data are constructed by some automated pipelines with unchecked quality unit tests, the execution-based feedback are not guaranteed to be correct. (2) The long horizon context length and sandbox ex8 Preprint. Figure 7: Left: RL performance on SWE-bench Verified when using different feedback. Right: Average training reward for different models. ecution significantly limit the scale of RL, leading to slow improvements especially under sparse 0/1 reward. In this subsection, we show execution-free feedback offers promising approach to not only accelerate training but also further enhance overall performance by providing more fine-grained reward signals. We first establish the RL setup in 5.2.1 and then discuss the results in 5.2.2. 5.2.1 RL SETUP Scaffold and Model For training scaffold, we adapt verl (Sheng et al., 2024) with Megatron (Shoeybi et al., 2019) which enables efficient multi-turn agentic reinforcement learning and SGLang for trajectory rollout. For agent scaffold, similar to the reward model rollout, we employ OpenHands (Wang et al., 2025) for tool interactions. For base models, we use Qwen3-30B-A3B (Qwen Team, 2025) with warm-up. Evaluation Setup Similar to reward model evaluation, we conduct our evaluation on SWE-bench Verified (Jimenez et al., 2024). In the RL setting, we only generate 1 trajectory and 1 patch for each instance using greedy decoding following OpenHands (Wang et al., 2025), without any test-time scaling, as the final pass@1 score. Baselines We compare different types of feedback during reinforcement learning: (1) Hybrid feedback, where the feedback is combination of execution-free (SWE-RM) and execution-based signals, as will be defined in Eq. 1; (2) Execution-free feedback only, where the feedback is provided solely by SWE-RM; (3) Execution-based feedback only, where the feedback is derived exclusively from the execution results of fail2pass tests; (4) Poorly calibrated execution-free feedback, where the feedback comes from reward model with comparable TTS but lower AUC and weaker calibration ability. Implementation Details We adapt GSPO (Zheng et al., 2025) which provides greater stability for Mixture-of-Experts RL training and we define the execution-free feedback as ScoreEF (q, τ, patch) [0, 1], and the overall reward is computed as: r(q, τi) = 1 + ScoreEF (q, τi, patchi), 0.5 + ScoreEF (q, τi, patchi), 0 + ScoreEF (q, τi, patchi), if issue resolve, unfinished, otherwise. (1) More details about the RL training such as data, hyper-parameters are shown in Appendix E. 5.2.2 EXECUTION-FREE FEEDBACK BENEFITS RL TRAINING As shown in Figure 7, using the hybrid reward as described in Eq. 1 yields the best RL performance and efficiency. Compared to the execution-based baseline, hybrid feedback improves pass@1 by about 3 absolute points (54.8% vs. 51.8%) and shows faster, smoother improvements, indicating effective reward shaping. For execution-based feedback only, we observe slower early gains and an early plateau due to the sparsity of the 0/1 signals and issues with test noise and coverage. In 9 Preprint. Table 5: Performance after RL on different SWE tasks other than SWE-Bench Verified, and Terminal Bench when using different feedback. SW.B. is short for SWE-Bench and Bold stands for the best. METHOD SW.B. LIVE (LITE) SW.B. MULTILINGUAL MULTI-SW.B. MINI TERMINAL BENCH Hybrid Execution-free only Execution-based only Poor Calibrated RM 22.4 20.4 20.0 12.0 35.7 33.0 33.3 21.0 20.0 18.8 18.5 10. 32.5 31.3 30.0 15.0 contrast, execution-free feedback alone shows faster initial progress due to continuous signals but weaker convergence in later stages, likely caused by inaccuracies in its unverified signals. While our main evaluation focuses on SWE-Bench Verified, we additionally conducted experiments on broader suite of SWE tasksincluding SWE-Bench Live (Lite), SWE-Bench Multilingual, MultiSWE-Bench Mini, and Terminal Benchto assess generalization beyond the original domain. As shown in Table 5, hybrid feedback consistently achieves better RL performance, while executionfree only feedback shows comparable results to execution-based only feedback. And if using feedback from poorly calibrated RM, the model will also show significant decrease in other tasks. Overall, combining execution-free feedback with verifiable signals balances efficiency and reliability, achieving the strongest final results by providing both continuous and trustworthy rewards."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we show that test-time scaling alone is an insufficient measure of verifier quality for SWE agents. Beyond top-1 ranking, reward models must also deliver strong discrimination (AUC) and reliable calibration (low ECE) to provide stable, useful signals, especially for RL. Guided by large-scale ablations on data scale, positive/negative ratios, policy mixtures, source composition etc., we develop SWE-RMa 30B MoE (3B activated) execution-free verifier with up to 256k context. SWE-RM achieves state-of-the-art open-source TTS gains on SWE-Bench Verified and, when used for RL, yields faster, more stable training and +3 absolute pass@1 over execution-based feedback counterparts. This establishes execution-free, well-calibrated reward modeling as practical and powerful foundation for advancing SWE agents in both TTS and RL."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude sonnet 4 anthropic. URL https://www.anthropic.com/claude/ sonnet. [Online; accessed 2025-09-24]. Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. Swe-rebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents, 2025. URL https://arxiv.org/abs/2505.20411. Andrew P. Bradley. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern Recognit., 30:11451159, 1997. URL https://api. semanticscholar.org/CorpusID:13806304. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International networks. Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 13211330. PMLR, 0611 Aug 2017. URL https://proceedings.mlr.press/v70/ guo17a.html. Zhenyu He, Qingping Yang, Wei Sheng, Xiaojian Zhong, Kechi Zhang, Chenxin An, Wenlei Shi, Tianle Cai, Di He, Jiaze Chen, Jingjing Xu, and Mingxuan Wang. Swe-swiss: multi-task fine-tuning and rl recipe for high-performance issue resolution. https://www.notion. so/SWE-Swiss-A-Multi-Task-Fine-Tuning-and-RL-Recipe-for-High10 Preprint. Performance-Issue-Resolution-21e174dedd4880ea829ed4c861c44f88, 2025. Notion Blog. Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2e-gym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. arXiv preprint arXiv:2504.07164, 2025. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Michael Luo, Naman Jain, Jaskirat Singh, Sijun Tan, Ameen Patel, Qingyang Wu, Alpay Ariyak, Colin Cai, Shang Zhu Tarun Venkat, Ben Athiwaratkun, Manan Roongta, Ce Zhang, Li Erran Li, Raluca Ada Popa, Koushik Sen, and Ion Stoica. Deepswe: Training state-of-the-art coding agent from scratch by scaling rl. https://pretty-radio-b75.notion.site/DeepSWETraining-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-byScaling-RL-22281902c1468193aabbe9a8c59bbe33, 2025. Notion Blog. MiniMax. Minimax-m1: Scaling test-time compute efficiently with lightning attention, 2025. URL https://arxiv.org/abs/2506.13585. OpenAI. Introducing swe-bench verified openai, Sept 2025. URL https://openai.com/ index/introducing-swe-bench-verified/. [Online; accessed 2025-09-22]. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. In Proceedings of the 42nd International Conference on Machine Learning (ICML 2025), 2025. URL https://arxiv. org/abs/2412.21139. arXiv:2412.21139, accepted at ICML 2025. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. GLM-4.5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming 11 Preprint. Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, and Jie Tang. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025a. URL https: //arxiv.org/abs/2508.06471. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025b. URL https://arxiv.org/abs/2507.20534. OpenHands Team. Sota on swe-bench verified with inference-time scaling and critic model, 4 2025. URL https://www.all-hands.dev/blog/sota-on-swe-benchverified-with-inference-time-scaling-and-critic-model. [Online; accessed 2025-09-22]. Cheng Wang. Calibration in deep learning: survey of the state-of-the-art, 2025. URL https: //arxiv.org/abs/2308.01222. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OJd3ayDDoF. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://arxiv.org/abs/2405.15793. 12 Preprint. John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025. URL https://arxiv.org/abs/2504.21798. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. URL https://arxiv.org/abs/2507.18071. 13 Preprint. Table 6: Comparison of SWE-RM with other execution-free verifiers setting: SWE-Gym Verifier (Pan et al., 2025), R2E-Gym Verifier (Jain et al., 2025), OpenHands Critic (Team, 2025) and DeepSWE Verifier (Luo et al., 2025). - means the statistic are not disclosed. *: Our training data source contains SWE-Gym (Pan et al., 2025), R2E-Gym (Jain et al., 2025), SWE-smith (Yang et al., 2025) and SWE-rebench (Badertdinov et al., 2025). REWARD MODEL # DATA # REPO POLICY SOURCE CONTEXT LEN. PROMPT SWE-Gym Verifier R2E-Gym Verifier Openhands Critic DeepSWE Verifier 2636 3321 - - 11 10 11 10 Mix Claude-3.5 - - SWE-Gym R2E-Gym SWE-Gym R2E-Gym SWE-RM 100k 170 Mix Multiple* 32k 32K 32k 76k 256k Traj. Traj. + Patch Traj. Traj. + Patch Traj. + Patch"
        },
        {
            "title": "A THE USE OF LARGE LANGUAGE MODELS",
            "content": "For this paper, large language models(LLMs) are used solely for polishing the writing. The entire research process, including but not limited to ideation, was conducted without any assistance from LLMs."
        },
        {
            "title": "B EXTENDED RELATED WORK",
            "content": "B.1 VERIFIERS FOR SWE TASKS In Software Engineering (SWE) tasks, there are mainly two kinds of verifiers: (1) execution-based verifiers and (2) execution-free verifiers. Execution-based verifiers typically consist of various types of unit tests, either human-written or model-generated. Agentless (Xia et al., 2024), R2E-Gym (Jain et al., 2025), and DeepSWE (Luo et al., 2025) have demonstrated the effectiveness of executionbased verifiers in test-time scaling by ranking patches based on the number of unit tests passed. However, execution-based verifiers struggle to distinguish between patches that achieve the same number of test passes, and they suffer from the inherent unreliability of poorly written or model generated unit tests. In contrast, execution-free verifiers are typically model-based and provide continuous score for given trajectory, allowing finer-grained discrimination. Early work such as SWE-Gym (Pan et al., 2025) and OpenHands Critic (Team, 2025), has initially explored naive execution-free verifiers with limited coverage and were confined to relatively simple settings. Subsequent work, including R2E-Gym (Jain et al., 2025) and DeepSWE (Luo et al., 2025), has shown that combining execution-based and execution-free verifiers leads to improved test-time scaling. Nevertheless, as summarized in Table 6, the exploration of execution-free verifiers remains preliminary and has largely focused only on scaling performance. Different from them, this work first demonstrates that test-time scaling (TTS) performance alone is not sufficient measure of an execution-free verifiers quality its accuracy and calibration are equally important. We further present systematic study on training versatile execution-free verifiers, considering factors such as data scaling, data ratio, policy mixture, source mixture, and context length. B.2 AGENTIC REINFORCEMENT LEARNING FEEDBACK IN SWE TASKS Unlike non-agent scaffolds such as Agentless (Xia et al., 2024), which are single-turn and pipelinebased, agent scaffolds in SWE such as OpenHands (Wang et al., 2025) and SWE-Agent (Yang et al., 2024) deploy sandbox environment that allows models to interact in multi-turn setting. Upon completion, fail-to-pass unit test is executed to assess whether the generated patch resolves the issue. This type of execution-based feedback plays role similar to that of rule-based metrics in math problems (DeepSeek-AI, 2025), as it aims to provide verifiable and relatively accurate reward for reinforcement learning. Such feedback has been widely adopted in recent coding agent training, including Qwen3-Coder (Qwen Team, 2025), GLM-4.5 (Team et al., 2025a), and MiniMax-M1 (MiniMax, 2025). While effective in principle, execution-based feedback is limited by the quality of the test suites it relies on. It assumes that the test oracle is perfect, yet in practice, many test cases are model-generated, making them an unreliable proxy for correctness. In addition, execution-based feedback cannot distinguish between trajectories that yield the same outcome, either passing or fail14 Preprint. ing test. Consequently, it often provides signals that are overly sparse or even misleading for reinforcement learning. In this work, we are the first to integrate execution-free feedback into SWE agentic reinforcement learning. We find that versatile execution-free feedback can offer more fine-grained rewards, and lead to improved training efficiency and performance. THEORETICAL LINK BETWEEN TTS, AUC, AND ECE AND RL"
        },
        {
            "title": "DYNAMICS",
            "content": "In this section, we make explicit how the three metricsTTS, AUC, and ECEcorrespond to three distinct failure modes of reward models (RMs) when used as optimization signals in RL. Throughout this section, let τ πθ denote sampled trajectory from the policy, r(τ ) [0, 1] is the RM score, and c(τ ) {0, 1} the binary correctness label (the ideal true correctness). If the RM score is used directly as the reward, the policy-gradient update is θJ(θ) Eτ πθ [r(τ ) θ log πθ(τ )] . (2)"
        },
        {
            "title": "The ideal update using the true correctness label is",
            "content": "θJ (θ) = Eτ πθ [c(τ ) θ log πθ(τ )] . The gap between (2) and (3) determines the stability and correctness of RL. Here we use simple policy gradient as an example, which has same nature to GRPO (DeepSeek-AI, 2025)/GSPO (Zheng et al., 2025), one can simply extend the analysis to GRPO setting. We then analyze below how TTS, AUC, and ECE each correspond to different component of this gap. (3) C.1 TTS: EXTREME-TOP ERRORS AND THEIR IMPACT ON RL TTS@k evaluates whether the highest-scored trajectory among RM-scored samples is correct: TTS@k = Pr(c(τ ) = 1) , τ = arg max i[1:k] r(τi). (4) Thus (1TTS@k) is exactly the probability that an unresolved trajectory receives the largest reward among the samples. Implication for RL When the top-1 trajectory τ is incorrect (c(τ ) = 0), we know that r(τ ) = max i[1:k] r(τi). This does not imply that negatives have larger average rewards than positives, but it does imply that the batch contains negative trajectory with the largest reward weight. Since policy-gradient updates scale linearly with reward, θJ(θ) (cid:88) i=1 r(τi) θ log πθ(τi), the contribution of τ scores. Conditioning on correctness of the top-ranked sample gives the decomposition: becomes dominant term in the update, even if all other negatives have small θJ(θ) = TTS@k E[update c(τ ) = 1] + (1 TTS@k) E[update dominated by τ (5) Under the event c(τ ) = 0, RL receives the strongest possible reward signal for an unresolved trajectory. This causes the policy πθ to increase the probability of sampling this undesirable behavior, and the effect compounds over iterations. ]. C.2 AUC: PAIRWISE RANKING QUALITY AND REVERSED-GRADIENT FREQUENCY AUC measures the probability that the RM correctly orders positive trajectory above negative one (for all trajectories): AUC = Pr(cid:0)r(τ+) > r(τ)(cid:1). Therefore the mis-ranking probability is 1 AUC = Pr(cid:0)r(τ) > r(τ+)(cid:1). 15 (6) (7) Preprint. Mis-rankings imply reversed gradient contributions Whenever r(τ) > r(τ+), the RM assigns higher reward to an incorrect trajectory. The corresponding policy-gradient contributions satisfy r(τ) θ log πθ(τ) > r(τ+) θ log πθ(τ+), (8) which is opposite to what the ideal update (3) would encourage. Thus fraction 1 AUC of all positivenegative trajectory pairs induce updates pointing in the wrong direction. Conditioning on the correctness of ranking yields the decomposition θJ(θ) AUC E[correct pair update] + (1 AUC) E[reversed pair update]. (9) Consequences for RL stability Since gradient contributions aggregate linearly, the expected fraction of bad (reversed) updates grows exactly in proportion to (1 AUC). Therefore, low AUC = many reversed-gradient terms = unstable or divergent RL behavior. Unlike TTS (which concerns only the extreme top), AUC measures global ranking correctness, which affects every sampled trajectory in RL. C.3 ECE: CALIBRATION ERROR AND SYSTEMATIC BIAS IN RL UPDATES For reward model that outputs score r(τ ) (interpreted as confidence in this trajectory is good) and binary good/bad ground truth {0, 1}. reward model is calibrated if Pr(c = 1 = α) = α, α [0, 1]. (10) Using the binned approximation for the reward-model scores across dataset of trajectories, we can compute: ECE = (cid:88) m=1 Bm (cid:12)acc(Bm) conf(Bm)(cid:12) (cid:12) (cid:12). (11) where Bm is the divided bins and: conf(Bm) = 1 Bm (cid:88) ri, iBm acc(Bm) = 1 Bm (cid:88) ci iBm If Eq. (10) holds, then E[c r] = r, e.g. For all trajectoCalibration and unbiased RL updates ries to which the model assigns confidence score of (r = 0.7), the actual proportion of successful trajectories should also be 70%. This is exactly the statement (E[c = 0.7] = 0.7), meaning it matches the models own predicted confidence (r). And thus E[r(τ )θ log πθ(τ )] = E[c(τ )θ log πθ(τ )], (12) meaning the RM induces no systematic bias in the expected gradient. Bias induced by miscalibration In general, the deviation between the RM-induced and ideal updates is (cid:2)θ log πθ(τ ) (cid:0)r(τ ) E[c r(τ )](cid:1)(cid:3) . (13) bias = Eτ πθ Define the calibration bias function so that b(α) = E[c = α] α, r(τ ) E[c r(τ )] = b(r(τ )). Here b(α) measures the calibration bias at confidence level α: among all trajectories for which the RM predicts score = α, E[c = α] is the true success frequency, while α is the predicted success probability. Their difference therefore captures the systematic overor under-confidence of the reward model at that score. ECE is then binned approximation of the expected magnitude of this bias over the score distribution, meaning high ECE large systematic distortion in (13). Preprint. Additional effect: gradient variance inflation. Write the RM-induced gradient estimator as"
        },
        {
            "title": "Using the decomposition",
            "content": "we obtain g(τ ) = r(τ ) θ log πθ(τ ). r(τ ) = E[c(τ ) r(τ )] b(cid:0)r(τ )(cid:1), g(τ ) = E[c(τ ) r(τ )] θ log πθ(τ ) (cid:125) (cid:124) (cid:123)(cid:122) g(τ ) b(cid:0)r(τ )(cid:1) θ log πθ(τ ) (cid:123)(cid:122) (cid:125) δg(τ ) (cid:124) . Here g(τ ) is the calibrated part of the gradient (which would be obtained if we replaced r(τ ) by the true success probability E[c(τ ) r(τ )]), while δg(τ ) is purely micalibration-induced noise term. The variance of g(τ ) decomposes as Var[g(τ )] = Var[g(τ )] + Var[δg(τ )] + 2 Cov(cid:0)g(τ ), δg(τ )(cid:1). In particular, we always have Var[g(τ )] Var[g(τ )], with the excess variance controlled by Var[δg(τ )] = Var(cid:2)b(r(τ )) θ log πθ(τ )(cid:3). Thus calibration error b(r) couples multiplicatively with the policy gradient θ log πθ(τ ), injecting additional variance into the gradient estimator. Since ECE is discrete approximation of Er[ b(r) ], higher ECE typically implies larger variance contribution from δg(τ ) and therefore less stable RL training."
        },
        {
            "title": "D REWARD MODEL TRAINING DETAILS",
            "content": "D.1 DETAILED TRAINING SETUP Data Collection To train reward model, we first rollout and collect over 400k multi-turn trajectories up to 100 iterations using OpenHands (Wang et al., 2025) and SWE-Agent (Yang et al., 2024), which are two widely used open-sourced coding agent scaffold using different policy models and data sources, including SWE-Gym (Pan et al., 2025), SWE-rebench (Badertdinov et al., 2025), SWE-smith (Yang et al., 2025), and R2E-Gym (Jain et al., 2025). Specifically, we adapt Qwen3-Coder-Max, Qwen3-Coder-Flash, Claude-4-sonnet for rollout. Since large portion of the data might be unresolved and some trajectories may be incomplete or contains bad tool calls, thus the final usable trajectories for training is around 100k. These trajectories are then labeled as positive(resolved) or negative(unresolved) based on their execution results with the provided fail2pass test. Though some of them might be noisy as we discussed that unit tests might not be able to truly reflect the correctness of the generated patch, we applied data cleaning such as filtering out instances without any successful trajectory (typically cases affected by over-strict/unfair unit tests or under-specified descriptions) to maintain the highest possible label quality. With data filtering, we believe sufficiently large and diverse dataset enables the RM to learn denoised and generalized correctness signal despite noisy supervision. Scaffold and Model For training scaffold, we adapt Megatron (Shoeybi et al., 2019) for supervised fine-tuning which enables efficient long context training. For rollout, we use SGLang together with agent scaffold OpenHands (Team, 2025) and SWE-Agent (Yang et al., 2024). While for the base model, we use Qwen3-30B-A3B (Qwen Team, 2025) as the backbone for further training. This MoE architecture is not choice that claiming the calibration advantages,rather, we follow the prevailing practice in state-of-the-art coding agents (e.g., Qwen3-Coder, Kimi-K2, MiniMax-M1/2), which predominantly adopt MoE backbones. Using the same backbone ensures compatibility with existing pipelines(e.g. infra) and allows our reward model and trained policies to be directly integrated without additional adaptation overhead. Our focus is therefore on reward-model training and calibration, while architectural comparisons (MoE vs. Dense vs. Adapters) are left to future work. 17 Preprint. Baselines We compare our trained execution-free verifier with the following different executionfree verifiers as well as execution-based verifiers: (1) Agentless (Xia et al., 2024), which proposes an execution-based method that generates reproduction tests for each trajectory and re-ranks based on passed test numbers; (2) SWE-Gym Verifier (Pan et al., 2025), which releases Qwen2.5-32B based execution-free verifier trained on SWE-Gym; (3) DeepSWE-EB Verifier (Luo et al., 2025): which is the execution-based component of current State-of-the-art DeepSWE Hybrid-TTS, which is also the improved version of R2E-Gym Execution-based verifier (Jain et al., 2025) with similar mechanism to Agentless. (4) DeepSWE EF Verifier (Luo et al., 2025): which is the execution-free component of current State-of-the-art DeepSWE Hybrid-TTS also the improved version of R2EGym execution-free verifier. Evaluation Setup We mainly conduct the evaluation on SWE-bench Verified (Jimenez et al., 2024) which is curated subset of 500 human-verified tasks for reliably assessing model performance on real-world software engineering tasks. For test-time scaling and further accuracy and calibration evaluation, we use the most widely used open-sourced coding agent scaffold OpenHands to collect 32 independent runs for each instance, resulting 32 500 trajectories in total. The sampling configs use temperature of 1.0, top_p of 0.95, max_iterations of 100. Accuracy is calculated by AUC score on all 16k trajectories while calibration is measured by expected calibra- (cid:12) (cid:12) tion error(Guo et al., 2017), where ECE = (cid:80)M (cid:12) (cid:12) (cid:12), conf(Bm) = (cid:12) acc(Bm) conf(Bm) ri, acc(Bm) = 1 ci. And we follow common practice to divide confiBm m=1 (cid:80) (cid:80) Bm iBm 1 Bm dence into 10 bins (M = 10). iBm Pass@k defines the resolve rate of model with at least one successful solution among trajectories which is also the upper bound while RM@K defines the resolve rate of final selected trajectories from samples. For every < 32, we obtain the mean and variance of 5 random runs for fair assessment. D.2 TRAINING TEMPLATE Following SWE-Gym (Pan et al., 2025), we adapt from their template which splices all turns (model action output and tool responses) and end with YES/NO token for reward model classification. Given the full multi-turn trajectory, the model is prompted to output single special token, either <YES> (resolved) or <NO> (unresolved). And the supervised fine-tuning utilizes standard nexttoken prediction loss on this special token. At inference time, by obtaining the log probability of the special token <YES>(ly) and <NO>(ln), the final score is calculated by exp (ly)/(exp (ly) + exp (ln)), which maps to continuous reward model score [0, 1]. For tool parsing, we adapt Qwen3-Coders XML format, which optimized for code-related argument parsing. The example trajectory is shown in Figure 8. D.3 TRAINING HYPER-PARAMETERS For reward model training hyperparameters, we adapt 256k context window to support scoring for complex questions which contains extremely long contexts. The global batch size is set to 128. Also widely used AdamW optimizer and cosine decay learning rate scheduler are used. The detailed training hyperparameters are listed in Table 7 below. We use 4 nodes of H100 for large scale, long context reward model training, which takes around 20 hours for 100k samples. Table 7: The detailed training hyperparameters for reward model. Model Size Global Batch Size Learning Rate Schedular LR Warmup Optimizer Epoch 30BA3 235BA22 480BA35 128 128 128 7e-6 7e-7 7e-6 7e-7 7e-6 7ecosine cosine cosine 3% 3% 3% AdamW AdamW AdamW 1 1 1 We also summarize the detailed training setting for Verifiers we used (Verifier A, Verifier and Poor calibrated RM in Figure 7) in Table 8 below. Preprint. Figure 8: Prompt template for reward model training. Pink refer to the tool parsing XML format example. D.4 ADDITIONAL ANALYSIS ON CONTEXT CONSTRAINT In 4.3 we show substantial performance increase when we raise the context window from 32k to 256k tokens. The larger context allows the model to cover far more tokens of the input (for 19 Preprint. Table 8: Comparison of detailed training setting of Verifier A, Verifier in Figure 2,3,4,5 and Poorly Calibrated RM in Figure 7. REWARD MODEL # DATA RATIO POLICY SOURCE CONTEXT LEN. Verifier Verifier Poorly Cali. RM in Figure 7 20k 20k 5k 2:1 1:4 1:2 Mix-Policy Mixed-Source SWE-Rebench Off-Policy SWE-Rebench Off-Policy 256k 256k 256k example, longer trajectories, multi-file code, richer history) without needing to compress or truncate information. When the window is small, many long trajectories cannot be scored at all (i.e., fall out of the window) and thus receive no valid reward signal, meaning correct but long solutions are effectively dropped from TTS selection. With 256 context, we reduce the no-score rate (increase the score rate), thereby enabling full-trajectory scoring and enabling our verifier to pick up solutions that would otherwise be ignored. And another reason for us to insist on 256k reward model training is that modern high-end coding agent policy models such as Claude, GPT-5, Qwen3Coder-Max support 256k token windows (and up to 1 tokens in some settings). The community therefore urgently needs reward model that is compatible with this scale, so trajectories from such long-context agents can be properly scored yet many existing reward/verifier models are constrained to much shorter context windows and thus cannot handle those long trajectories. By aligning our verifier to the 256 k-token scale we fill crucial gap. Finally, we acknowledge that increasing the context to 256k tokens is not free it incurs higher memory usage. However, since our output generation only contains one token, all prompt computation can be run in parallel, thus the latency is more or less the same for different context length at inference time. While for deployment, 256k model takes only around 2x GPU memory usage than 32k model, user with 2x A100 GPUs can easily deploy the model with high efficiency."
        },
        {
            "title": "E RL TRAINING DETAILS",
            "content": "E.1 RL SETUP Models We conduct reinforcement learning based on Qwen3-30B-A3B (Qwen Team, 2025) with SFT warm-up. WE use in-house collected agentic trajectories including but not limited to SWE tasks to fine-tune the base model. Then this fine-tuned model served as the starting point of our RL experiments. We use MoE architecture, following the prevailing practice in state-of-the-art coding agents (e.g., Qwen3-Coder (Qwen Team, 2025), Kimi-K2 (Team et al., 2025b), MiniMaxM1/2 (MiniMax, 2025)), which predominantly adopt MoE backbones. Using the same backbone ensures compatibility with existing pipelines(e.g. infra) and allows our trained policies to be directly integrated without additional adaptation overhead. Implementation Details We train the model using curated data from SWE-Gym (Pan et al., 2025) and SWE-rebench (Badertdinov et al., 2025) with batch size of 64. For each problem, we sample 16 rollouts, use maximum of 100 iterations, and set the context length to 128k. This context length was selected because it accommodates most problem cases within single context window while being more cost-efficient than 256k context window. The training data is further filtered by difficulty, as problems that are either too easy or too difficult can negatively affect RL performance. Unlike single-turn RL training, where the model needs to generate only one response per problem, agentic RL requires interaction with an agent scaffold (i.e., tool calls) across multiple turns to construct full trajectory. Specifically, given problem q, the policy model generates an action ai and then receives tool response oi, repeating this process times to form trajectory τ = {a1, o1, a2, o2, . . . , aT , oT }. During optimization, tool responses are masked. Instead of the GRPO objective, we adopt the GSPO objective (Zheng et al., 2025), which provides greater stability, particularly in Mixture-of-Experts (MoE) RL training: JGSPO(θ) = qD,{τi}G (cid:34) 1 (cid:88) i=1 min i=1πold(q) (cid:32)(cid:18) πθ(τi q) πθold (τi q) (cid:19) 1 τi (cid:98)Ai, clip (cid:32)(cid:18) πθ(τi q) πθold (τi q) (cid:19) 1 τi (cid:33) (cid:33)(cid:35) , 1 ε, 1 + ε (cid:98)Ai (14) 20 Preprint. with the group-based advantage estimation: (cid:98)Ai = r(q, τi) mean (cid:0){r(q, τi)}G std ({r(q, τi)}G i=1) i=1 (cid:1) (15) The optimization integrate several standard tricks such as Clip High (Yu et al., 2025), NO KL loss (Yu et al., 2025) etc."
        }
    ],
    "affiliations": [
        "Qwen Team, Alibaba Group",
        "The Hong Kong University of Science and Technology"
    ]
}