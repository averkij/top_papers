{
    "paper_title": "OmniTry: Virtual Try-On Anything without Masks",
    "authors": [
        "Yutong Feng",
        "Linlin Zhang",
        "Hengyuan Cao",
        "Yiming Chen",
        "Xiaoduan Feng",
        "Jian Cao",
        "Yuxiong Wu",
        "Bin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Virtual Try-ON (VTON) is a practical and widely-applied task, for which most of existing works focus on clothes. This paper presents OmniTry, a unified framework that extends VTON beyond garment to encompass any wearable objects, e.g., jewelries and accessories, with mask-free setting for more practical application. When extending to various types of objects, data curation is challenging for obtaining paired images, i.e., the object image and the corresponding try-on result. To tackle this problem, we propose a two-staged pipeline: For the first stage, we leverage large-scale unpaired images, i.e., portraits with any wearable items, to train the model for mask-free localization. Specifically, we repurpose the inpainting model to automatically draw objects in suitable positions given an empty mask. For the second stage, the model is further fine-tuned with paired images to transfer the consistency of object appearance. We observed that the model after the first stage shows quick convergence even with few paired samples. OmniTry is evaluated on a comprehensive benchmark consisting of 12 common classes of wearable objects, with both in-shop and in-the-wild images. Experimental results suggest that OmniTry shows better performance on both object localization and ID-preservation compared with existing methods. The code, model weights, and evaluation benchmark of OmniTry will be made publicly available at https://omnitry.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 2 3 6 3 1 . 8 0 5 2 : r OmniTry: Virtual Try-On Anything without Masks Yutong Feng1, Linlin Zhang2 Hengyuan Cao2 Yiming Chen1 Xiaoduan Feng1 Jian Cao Yuxiong Wu1 Bin Wang1, 1Kunbyte AI 2Zhejiang University {fengyutong.fyt, binwang393}@gmail.com {zhanglinlinlin, caohy}@zju.edu.cn {chenyiming, fengxiaoduan, caojian, wuyuxiong}@k-fashionshop.com Figure 1: Try-on results of various wearable objects generated by OmniTry, which supports object images with white or natural backgrounds, and even try-on results as input."
        },
        {
            "title": "Abstract",
            "content": "Virtual Try-ON (VTON) is practical and widely-applied task, for which most of existing works focus on clothes. This paper presents OmniTry, unified framework that extends VTON beyond garment to encompass any wearable objects, e.g., jewelries and accessories, with mask-free setting for more practical applications. When extending to various types of objects, data curation is challenging for obtaining paired images, i.e., the object image and the corresponding try-on result. To tackle this problem, we propose two-staged pipeline: For the first stage, we leverage large-scale unpaired images, i.e., portraits with any wearable items, to train the model for mask-free localization. Specifically, we repurpose the inpainting model to automatically draw objects in suitable positions given an empty mask. For the second stage, the model is further fine-tuned with paired images to transfer the consistency of object appearance. We observed Project Leader. Corresponding Author. Preprint. Under review. that the model after the first stage shows quick convergence even with few paired samples. OmniTry is evaluated on comprehensive benchmark consisting of 12 common classes of wearable objects, with both in-shop and in-the-wild images. Experimental results suggest that OmniTry shows better performance on both object localization and ID-preservation compared with existing methods. The code, model weights, and evaluation benchmark of OmniTry are available at https://omnitry.github.io/."
        },
        {
            "title": "Introduction",
            "content": "The image-based virtual try-on (VTON) [19] has received tremendous attention due to its wide application in e-commerce. Given person image and garment image, the purpose of VTON is to transfer the garment onto the person as preview. Thanks to the success of large-scale image generative models [49, 46, 14, 31] with their photorealistic aesthetics, recent efforts [59, 27, 9, 10, 65] have achieved satisfying performance on both generation quality and garment identity preservation. Despite the advancement of VTON, existing methods mainly concentrate on clothing try-on. Though some works have explored the extension to non-clothing, such as shoes [11] and ornaments [41], there still lacks unified framework in the literature, supporting any types of wearable objects. Furthermore, most methods require the indication of wearing area on person (e.g., masks or bounding boxes), or use automatic human-body parsers [61] to identify the area. When extending to anything try-on, it would be impractical to expect users to draw the targeting area, as the interaction between the model and various objects can be more considerably more complex. It is also challenging to leverage existing parsers to detect appropriate try-on areas for diverse objects. Thus, we follow the mask-free setting [24, 16, 65] for the model to automatically localize the area with natural composition. When confronting anything try-on, one key challenge is the data collection. Generally, the training of VTON requires large-scale paired images, consisting of single-shot of the garment, and corresponding person try-on result. Most datasets are curated from e-commerce websites, with at least thousands of samples, e.g., VITON-HD [8] and DressCode [42]. While for many common types of wearable objects, such as hats and ties, there is no abundant quantity of paired data, but only the product pictures. This limitation makes it necessary to develop an efficient training framework. In this paper, we present OmniTry, targeting mask-free virtual try-on for any wearable object. OmniTry reduces the heavy reliance on paired training samples, leveraging large-scale unpaired images for prior learning. The unpaired images refer to the image containing person with any wearable objects, which can be easily obtained from existing database. The training of OmniTry can be separated into two stages: (i) The first stage is completely conducted on unpaired data. We use multi-modal large language models (MLLMs) [1] to list all wearable items with descriptions. Each item is detected and erased from the image, forming training pair. Then an image generative model is trained to re-paint the item, prompted by the corresponding text description. After stage one, the model is expected to know how to transfer various objects onto the person in proper position, size and orientation. (ii) For the second stage, we further leverage high-quality paired data to fine-tune the model. Object image is introduced into the context, modulating the model to preserve the consistency of object appearance. Building upon the model from stage one, we observe that ID-consistency is quickly adapted even fine-tuned with few samples. To summarize, the two stages in OmniTry contributes the ability of mask-free localization and ID-preservation, respectively. Regarding the model design, we leverage the diffusion transformer as backbone, and compare two variants, i.e., text-to-image and inpainting model. Experimental results show that the inpainting model can be rapidly repurposed as mask-free generative model, by simply setting the mask input with all-zero values. Image tokens from different images are concatenated in the sequence dimension, and processed with full-attention mechanism for consistency learning [52, 66, 7, 23]. We employ efficient adapter tuning techniques for transferring the model to this task. More specifically, we implement two distinct adapters that handle the tokens from person and object images, individually. The erasure of wearable object is observed with critical impact. naive solution is to call objectremoval models [51, 70, 26] to fill the area of objects. However, we notice that while the processed area appears visually normal, it contains imperceptible artifacts. Thus, the model learn undesirable shortcuts by identifying these traces, resulting in poor generalization to natural images. To tackle this problem, we propose traceless erasing to eliminate the artifacts. We conduct image-to-image [40] to 2 subtly re-paint the entire image after erasure. Subsequently, the original try-on image is blended with the re-painted image, ensuring the non-object area remains unchanged. Traceless erasing disrupts the erasure boundaries, thereby compelling the model to learn genuine try-on capability. We construct comprehensive evaluation benchmark covering 12 common types of wearable objects, divided into clothes, shoes, jewelries and accessories. To fully investigate the model robustness, the objects are set on white and natural backgrounds, or try-on images, referring to Fig. 1. Metrics are designed to evaluate the object consistency, person preservation and wearing position. Experimental results indicate that OmniTry outperforms existing methods, and achieves efficient few-shot training."
        },
        {
            "title": "2 Related Works",
            "content": "Controllable Image Generation. The breakthrough of diffusion model [20] has driven extensive research on controllable image generation. ControlNet [63] and related pioneering works [44, 47, 67] explore precise control with diverse conditions. IP-Adapter [62] and related studies [15, 22, 30, 33, 38] investigate online concept control to achieve subject customized generation. Recent developments in DiT [45] have further propelled generalized image generation and editing. In-context LoRA [23] enables diverse thematic generation with image concatenation. OminiControl [52] introduces taskagnostic condition control with minimal model modification. OmniGen [57] unifies multi-task processing via large vision-language models. UniReal [7] achieves unified image editing via fullattention and video data prior. VisualCloze [34] enhances visual in-context learning for cross-domain generalization. For localized image customization, Anydoor [6] pioneers to transfer subject into specified region. MimicBrush [5] extends to local components transferring with imitative editing. ACE++ [39] establishes unified paradigm for generation and editing tasks. Image-based Virtual Try-On (VTON) has emerged as critical task attracting tremendous efforts. VITON [19] introduces Thin Plate Spline transformations [2] for multi-stage garment processing. CP-VTON [54] formalizes explicit geometric warping and texture synthesis stages. GP-VTON [58] combines local flow estimation with global parsing to improve detail preservation. These warping-based approaches, however, face persistent challenges in cross-sample alignment and generalization. This motivates the adoption of diffusion models [20], including TryOnDiffusions parallel U-Net [69], LADIVTONs garment tokenization [43], and DCI-VTONs hybrid warpingdiffusion framework [17]. OOTDiffusion [59] and FitDiT [25] enhance detail fidelity through specialized attention mechanisms. Though with advanced results, most of them remain constrained by intensive preprocessing requirements (e.g., wearing masks and pose estimation). Boow-VTON [65] creates mask-free approach through in-the-wild data augmentation. Any2AnyTryon [18] pioneers fully mask-free implementations, eliminating dependency on masks or poses."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminary Diffusion Transformer (DiT). OmniTry is developed on DiT [45], scalable transformer architecture for diffusion-based generation. The image is encoded into latent space through an autoencoder [28], and patchified into tokens [13]. Diffusion process [20] is conducted on tokens with transformer consuming the noisy tokens and predicts for denoising. Recent advancement in DiT, i.e., recified flow matching [36] and rotary position embedding (RoPE) [50], are also involved in this paper. Virtual Try-On (VTON). Given person image O, the try-on result image is noted as , then the target of VTON is three-fold: (i) the consistency between objects in original and try-on images, i.e. min similarity( ), (iii) the object is properly located on person, evaluated through the quality of O), (ii) the preservation of non-wearing areas, i.e., . Suppose the segmentation mask of the object in and wearable object image , is (1 (1 ) = I I M . 3.2 Stage-1: Mask-Free Localization As illustrated inFig. 2, the training of OmniTry consists of two stages, corresponding to the abilities of localization and ID-preservation, respectively. In the first stage, the objective of training can be regarded as garment-free VTON, in contrast to the model-free VTON in the literature [18]. Given Figure 2: The two-staged training pipeline of OmniTry. The first stage is built on in-the-wild portrait images to add wearable object onto the person in mask-free manner. The second stage introduces in-shop paired images, and targets to control the consistency of object appearance. and the object description, the model aims to edit the person image by adding the object as described. The type and detailed appearance of object are only prompted by input text. Control signal indicating the wearing area, e.g., bounding boxes, masks or selecting point, is not introduced here. Such an objective enforces the model to concentrate on where to paint the object, and how to blend it harmoniously with the person image. The training of stage one can be easily supervised by portrait image database, for which we introduce how to construct the training samples in the next paragraph. I , , in contrast to the paired images ( Unpaired Data Pre-process. We refer single portrait image as unpaired image with only try-on O) in next stage. We start by curating large-scale result dataset containing any human-related images. The dataset is filtered by classifier for images with person wearing at least one object. Following that, we leverage MLLM, Qwen-VL 2.5 [1], to list all potential wearable objects in each image. The output includes both the type of object and its appearance description. We also prompt MLLM to add an interaction description, e.g., wearing sunglasses and holding sunglasses in hand, to distinguish various cases. To erase each object for training, we use GroundingDINO [35] and SAM [29] to obtain the object mask, and remove the object with an inpainting-based erasing model. Specifically, we fine-tune an internal erasing model based on Flux.1 Fill [31]. Though without erasing capability, it is observed to quickly adapt to this task with few training samples. To summarize, the pre-precessing pipeline outputs set of triples, , and the object textual description. including the original image as , the object-erased image as I Model Architecture: Text-to-Image v.s. Inpainting Model. There are two candidate variants of model to implement the mask-free try-on task, i.e., the text-to-image (T2I) model, and the mask-based inpainting model. Generally, mask-based VTON models [27, 10] leverage the fill-in capacity of inpainting model, while mask-free methods [65, 18] adapt the T2I model, by injecting subject features into the backbone. Following the recent success in controllable image generation [52, 23, 66], straightforward solution with T2I model is to concatenate the person image tokens into the sequence of noisy tokens, then processed with the full-attention mechanism in DiT. This strategy effectively transfers the person appearance into the target image, while also doubles the computation cost. In contrast, OmniTry explores to repurpose the inpainting model for mask-free generation. The inpainting model is generally finetuned from the T2I model via extending the input channels. Suppose the noisy latent as X, the input image as Ic, and the inpainting mask as . Then the extended input is concat(X; Ic(1 ) denotes channel-wise concatenation. For repurposing the model, we simply set = 0, thus the input turns to be concat(X; Ic; 0). At the initialization, the zero mask leads the output image directly repeating the input. Therefore, compared with T2Ibased solution, the model effortlessly learns to copy the person condition, thus attentively focusing on locating the modification area. We inject location adapter (implemented as LoRA [21]) for finetuning. In practice, the model converges rapidly to adapt the mask-free generative manner. ); ), where concat( Traceless Erasing. Early experimental results suggest that the model learn unexpected shortcut. We visualize the training monitoring result in Fig. 3 (a), where we evaluate the model on erased training samples. It is shown that the output image almost perfectly recovers the position and shape of the object in ground-truth image, which indicates information leakage. We attribute the problem to the erasing model that leaves invisible traces in the filling area [55, 68]. The model tends to figure out these abnormal area for editing, instead of predicting the reasonable position. When applying to real-world images, the model frequently fails to locate the try-on area, and directly repeat the input. 4 Figure 3: Study on traceless erasing. (a) Shortcuts learned by model with naive erasing, where the model recovers the same shape and position as the ground-truth. (b) The pipeline of traceless erasing, where image-to-image model is introduced to disturb the traces (indicated in the red boxes). To address this problem, we propose traceless erasing strategy, as shown in Fig. 3 (b). After erasing the object with inpainting model, we apply with an image-to-image (I2I) translation [40] to subtly re-paint the image. We first add noise to the erased image ˆ [0, 1] , referring specific timestep in diffusion schedule (t = 0.2 in this paper), i.e., = enc(ˆ ) denotes ) the VAE encoder and ϵ is standard Gaussian noise. Then T2I model denoises into normal image with partial diffusion process from to 0. In this manner, the artificial effects in inpainting area are confused with the whole re-painted image, thus avoid information leakage. Since the I2I process modifies the detail of person image, the original try-on image should be correspondingly adjusted. To achieve smooth transition in the object boundary, we modulate the original mask into blending blend by blurring the boundary area for gradual blending effect. The final try-on image is: mask t, where enc( t) + ϵ (1 blend = blend + img2img(ˆ P ) (1 blend) (1) 3.3 Stage-2: ID Consistency Preservation The second stage of OmniTry inherits the location adapter from stage one, and steps further to control the consistency of object appearance. Referring to Fig. 2, in-shop image pairs are leveraged containing try-on image O. We pre-process the data with traceless erasing, and O) for training. Considering the lack of enough samples, the objective gather list of triple ( is to conduct efficient training with minimal adjustment to the model architecture in stage one. and object image , , I Masked Full-Attention. Following the recent full-attention customization researches [52, 23], we directly append the object image tokens into the existing sequence in DiT, and shift their position embedding in the width dimension. Under this settings, OminiControl-2 [53] and EasyControl [66] also explore to block some information flow in attention. In detail, the attention mask is set to zero where the condition tokens serve as query and the generated tokens as key. Such an attention mask improves the inference efficiency, but leads to performance decrease to certain extent. The main difference between the above works and OmniTry is that the condition image is also concatenated with noisy latents and all-zero mask, for adaption to inpainting model. To cope with such variance, we design two strategies in training: (i) We compute diffusion loss on object image with itself as supervision, i.e., directly copying the input, which is aligned with the zero-mask input. (ii) We block all the data flow from the generating try-on image to object image, thus avoid the detailed object appearance to be interrupted. In practice, we find it helpful to better preserve object identity with the above masked full-attention. Two-Stream Adapters. To fully preserve the ability of mask-free localization, we maintain the forward process of person image tokens exactly consistent with the first stage. Then an identity adapter is initialized for the newly introduced object image tokens. The two adapters, in same architecture, serve for two-stream computation process, i.e., we switch different adapters by identifying tokens from different image sources. The inference is similar to the multi-modality DiT [14] coping with vision and language information separately. 5 Figure 4: Qualitative comparison among OmniTry and existing methods on multiple objects. 3.4 Evaluation Benchmark As the first work exploring unified virtual try-on task, we establish comprehensive benchmark, dubbed OmniTry-Bench, for better evaluation and comparison with existing works. Benchmark Collection. We gather evaluation samples within 12 common types of wearable objects, which can be summarized into 4 major classes: (i) clothes consisting of top, bottom and full-body garments, (ii) shoes in common styles, (iii) jewelries, including bracelets, earrings, necklaces and rings, (iv) accessories, including bags, belts, hats, glasses, sunglasses and ties. We consider detailed sub-types if necessary, such as the backpack, shoulder and tote bags. For each sub-type, we collect 15 paired test images for man and woman, separately. The object images are assigned in white background, natural background, and try-on setting, with 5 pairs for each. The person images are also set in white and natural backgrounds. Such settings ensure to fully evaluate the robustness of model. Overall, the evaluation benchmark contains 360 pairs of images. Evaluation Metrics. As discussed in Sec. 3.1, the objectives of try-on can be divided into three aspects. Since there is no ground-truth result in mask-free setting, we redesign the metrics as follows: Object Consistency: We crop the objects from the try-on and object images via masking, and compute the visual similarity using DINO [3] and CLIP [48], with metrics noted as M-DINO and M-CLIP-I. Person Preservation: In contrast, we crop out the person from try-on and person images, and compute spatial-aligned similarity between them, i.e., LPIPS [64] and SSIM [56]. 6 Table 1: Evaluation results on OmniTry-Bench, which is separated into two groups: results on the whole set and the clothes subset, for fair comparison with methods only optimized on clothes data. method mask M-DINO M-CLIP-I LPIPS SSIM G-Acc. CLIP-T Object Consistency Person Presevation Object Localization Paint-by-Example [60] MimicBrush [5] ACE++ [39] OneDiffusion [32] VisualCloze [34] OmniGen [57] OmniTry (Ours) Magic Clothing [4] CatVTON [10] OOTDiffusion [59] FitDiT [25] Any2AnyTryon [18] OmniTry (Ours) on the whole set 0.7727 0.7253 0.7474 0.7749 0.7782 0.7869 0. 0.3903 0.3033 0.4561 0.7001 0.4471 0.6703 0.0542 on the clothes subset 0.7634 0.7906 0.8016 0.8340 0.8537 0.8560 0.2761 0.2084 0.2178 0. 0.2089 0.1021 0.4565 0.4693 0.4565 0.4731 0.5292 0.5435 0.6160 0.5665 0.5744 0.5961 0.6733 0.6747 0.6995 0.8033 0.8575 0. 0.5831 0.6190 0.5965 0.9333 0.8786 0.8828 0.8865 0.9027 0.8969 0.9105 0.9861 0.9250 0.9667 0.9972 0.9639 0.9944 0.9972 1.0 1.0 1.0 1. 1.0 1.0 0.2804 0.2781 0.2791 0.2309 0.2524 0.2535 0.2831 0.2700 0.2797 0.2761 0.2831 0.2832 0.2799 Object Localization: (i) Counting the success rate whether visual grounding model [35] detects the object, denoted as G-Accuracy. (ii) Computing the image-text similarity, noted as CLIP-I, between try-on image and text describing the person trying on the object (generated by MLLM [1])."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Setup Training Data. For the first stage, we gather diverse dataset containing both in-the-wild portrait images and in-shop model shots. Considering each image could contain multiple wearable objects, the total amount of training pairs is 188, 694. For the second stage, we collect paired samples following the 12 basic types in our benchmark. The whole dataset contains 51, 195 pairs, which shows class-unbalanced distribution (14, 861 pairs for clothes and 295 for ties). During training, each pair is equipped with brief text description, such as trying on sunglasses, to help distinguishing different classes. We note that the clothes and shoes are not erased but replaced with another one. Thus, we exchange the prefix as replacing for their prompts. Implementation Details. We train the first stage with batch-size of 32 for 50K steps, and the second stage with batch-size of 16 for 25K steps. All the experiments are conducted on 4 NVIDIA H800 GPUs. The location and identity adapters are implemented as LoRA [21] with rank 16. We employ the AdamW [37] optimizer with learning rate of 14 and weight decay of 0.01. All the images are resized to maximum of 1 million pixels while preserving their original aspect ratios to training. Compared Methods. We primarily compare with methods in two basic paradigms: Image-based Virtual Try-On: Most VTON methods focus exclusively on garments. We compare on the clothes subset with representative works, including CatVTON [10], OOTDiffusion [59], Magic Clothing [4], FitDiT [25], and Any2AnyTryon [18] (the only open-sourced mask-free model). General Customized Image Generation: Recent works explore to unify customization-related tasks into single model, e.g., transferring the whole subject or local components, in mask-based or mask-free manners. We compare with notable implementations, including Paint-by-Example [60], MimicBrush [5], ACE++ [39], OneDiffusion [32], OmniGen [57] and VisualCloze [34]. To cope with the methods requiring masks of editing areas, we manually draw the masks in person image regarding the type of objects. Thus, the results of these methods are listed for reference, instead of direct comparison with the remaining mask-free methods. 7 Figure 5: Ablation study on the two-staged training framework in few-shot settings. We show the evaluation metrics given varying amounts of paired training samples. 4.2 Results on Unified Virtual Try-on Qualitative Results. We visualize the try-on examples generate by representative compared methods in Fig. 4. For the general customization methods, mask-based works only modify the given areas, but show unstable object identity transferring. While for the mask-free works, the results tend to be free combination of the input person and object. Though with better consistency, they fail to precisely preserve the person image. OmniTry achieves accurate object consistency, in the meanwhile only edits the proper try-on areas of person image in mask-free manner. On the clothes subset evaluation, we observe that the existing VTON methods show unnatural output when evaluated on in-the-wild data. OmniTry is empowered by the compounded training on both in-the-wild and in-shop data, and shows more generalized ability of adapting various styles of garments. Quantitative Results. Tab. 1 incorporates the evaluation results on the proposed OmniTry-Bench, conducted on the whole benchmark and the clothes subset, respectively. OmniTry outperforms existing methods on both sets. For the mask-based customization methods, though the input mask helps to localize the editing area, they sometimes fail to transfer the complete appearance of objects, resulting in lower consistency metrics. For the generalized customization methods in mask-free manner, they achieve better subject-ID preservation, but suffers to maintain the person image, thus show worse LPIPS and SSIM. Such quantitative results are consistent with the visualized comparison results. When evaluated on clothes subset, though OmniTry is not specifically optimized on clothes dataset, it still shows advancing performance compared with state-of-the-art works in mask-based and mask-free settings. We note that the mask-free try-on could not be evaluated on previous benchmarks (e.g., VITON-HD [8]) for the missing of person images. 4.3 Ablation Study On the Training Strategy. We study one of the key designs in OmniTry, i.e., the two-staged training framework. The first stage is intended to leverage large-scale unpaired data, and boost the training efficiency in the second stage. To demonstrate this, we evaluate models initialized by the first stage and from scratch, respectively. For the comparison of efficiency, the models are fine-tuned in few-shot settings, ranging from 1 to 200 training samples per class. The results with representative metrics are illustrated in Fig. 5. For metrics related to person preservation (LPIPS and SSIM), we note that they could be higher when the model fails and directly repeats the input, thus not included. It is observed that model from scratch shows increasing performance with more training samples per class. While for model initialized from the first stage, it already achieves satisfying performance even with only one example for training. The results demonstrate that the first stage training significantly boosts the efficiency for fine-tuning, and is especially friendly to uncommon types of objects. It is noted that though the few-shot tuning achieves good performance, we still fine-tune it with all available paired data to further increase the stability of model, referring to the results in Tab. 1 Table 2: Ablation study on the model architecture and erasing strategies of OmniTry. method M-DINO M-CLIP-I LPIPS CLIP-T on the model architecutre (the whole subset) Full Method - txt2img model - w.o. object loss - full attention - one-stream adapter 0.5991 0.5005 0.5851 0.5752 0.5840 0.8272 0.7727 0.8222 0.8130 0.8186 0.0557 0.0676 0.0420 0.0384 0.0502 0.2830 0.2767 0.2824 0.2832 0.2802 on the erasing strategy (the jewelry subset) naive erasing traceless erasing 0.4964 0.5389 0.7554 0.7782 0.0413 0.0288 0.2727 0.2732 Figure 6: Try-on results of OmniTry fine-tuned on uncommon classes of wearable or holdable objects. On the Model Architecture. We then conduct ablation study on all the explored design of model architecture in OmniTry. The results are shown in Tab. 2, where the full method indicates the final solution. (i) We start with the comparison using text-to-image and inpainting model as backbone. Results show that the inpainting backbone performs better on all metrics which is consistent with our assumption that inpainting model takes no efforts to preserve the original image and converges faster. (ii) For the additional loss computation on object image, we observe that removing the loss decreases the model performance to certain extent. (iii) For the attention mechanism, full attention additionally introduces flow from person to object image, thus the object consistency metrics decrease correspondingly. (iv) We also investigate to use single adapter for this task, i.e. applying the adapter from the first stage to all image tokens. The one-stream framework also decreases the model performance, since it plays different roles in the inference of person and object images. On the Traceless Erasing. To verify the effectiveness of traceless erasing, we conduct ablation study on the jewelry subset with naive and traceless erasing. Results in Tab. 2 suggest that removing the traceless erasing leads to dramatic decrease in all metrics. Therefore, we adopt traceless erasing as fundamental pre-processing strategy in OmniTry. 4.4 Extension to Uncommon Classes We evaluate OmniTry on 12 common types of objects in the main experiments. To further demonstrate the efficiency of OmniTry, we extend it to some uncommon types, for which the paired training samples are limited to be obtained. The experiment is conducted on types including gloves, earphones, watches, hairbands, books and electronic products, with roughly 20 samples per class. It is noted that some types like books are actually in broader definition of try-on, i.e., holdable items. The visualization results are shown in Fig. 6. Thanks to the generalized training of the first stage, though with few paired samples, OmniTry succeeds in transferring these relatively uncommon objects onto the correct position. The results encourage broader extension of OmniTry into more application scenarios, without preparing large amount of paired images."
        },
        {
            "title": "5 Limitations",
            "content": "In this section, we discuss the limitations of OmniTry observed in practice. As the first work exploring unified VTON, OmniTry is still restricted by the object types in training dataset. For the efficient tuning in stage-2, it could be challenging to extend to uncommon objects not involved in the unpaired dataset in stage-1. Larger pre-training dataset is expected to further boost the generalization ability. For the mainly-focused 12 common types, experimental results show that OmniTry could also fail to transfer the object consistency or output poor appearance in some cases, especially for the objects with larger transformation, e.g., bags. The above limitations encourage future works to build upon OmniTry and develop more advanced models towards unified try-on task."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents OmniTry, unified mask-free framework extending the existing garment try-on into any wearable objects. To tackle the problem of lacking abundant paired samples, i.e., object and the try-on image, for many types of objects, we propose two-staged training pipeline in OmniTry. During the first stage, large-scale unpaired images are leveraged to supervise the model for mask-free object localization. While the second stage tames the model to maintain the object consistency. We elaborate the design of OmniTry, including traceless erasing for avoiding shortcut learning, an inpainting-based re-purposing strategy for mask-free generation, and masked full-attention for identity transferring. new benchmark targeting unified try-on is introduced, and demonstrates the effectiveness of OmniTry compared with existing methods. Extensive experiments also verify that OmniTry achieves efficient learning even with few paired images for training."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Fred L. Bookstein. Principal warps: Thin-plate splines and the decomposition of deformations. IEEE Trans. Pattern Anal. Mach. Intell., 11(6), 1989. [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [4] Weifeng Chen, Tao Gu, Yuhao Xu, and Arlene Chen. Magic clothing: Controllable garment-driven image synthesis. In Proceedings of the 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC, Australia, 28 October 2024 - 1 November 2024. ACM, 2024. [5] Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. Zero-shot image editing with reference imitation. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [6] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65936602, 2024. [7] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, and Hengshuang Zhao. Unireal: Universal image generation and editing via learning real-world dynamics. CoRR, 2024. [8] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Viton-hd: High-resolution virtual try-on via misalignment-aware normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021. [9] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for virtual try-on. arXiv e-prints, pages arXiv2403, 2024. [10] Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, Dongmei Jiang, and Xiaodan Liang. Catvton: Concatenation is all you need for virtual try-on with diffusion models. arXiv preprint arXiv:2407.15886, 2024. [11] Chao-Te Chou, Cheng-Han Lee, Kaipeng Zhang, Hu-Cheng Lee, and Winston Hsu. Pivtons: Pose invariant virtual try-on shoe with conditional image completion. In Computer VisionACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part VI 14, pages 654668. Springer, 2019. [12] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. [16] Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo. Parser-free virtual try-on via distilling appearance flows. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 84858493, 2021. [17] Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. Taming the power of diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 20233 November 2023, 2023. [18] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2anytryon: Leveraging adaptive position embeddings for versatile virtual clothing tasks. CoRR, 2025. [19] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry Davis. Viton: An image-based virtual try-on network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 75437552, 2018. [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [22] Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, and Qian He. Dreamtuner: Single image is enough for subject-driven generation. CoRR, 2023. [23] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. [24] Thibaut Issenhuth, Jérémie Mary, and Clément Calauzenes. Do not mask what you do not need to mask: parser-free virtual try-on. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16, pages 619635. Springer, 2020. [25] Boyuan Jiang, Xiaobin Hu, Donghao Luo, Qingdong He, Chengming Xu, Jinlong Peng, Jiangning Zhang, Chengjie Wang, Yunsheng Wu, and Yanwei Fu. Fitdit: Advancing the authentic garment details for high-fidelity virtual try-on. CoRR, 2024. [26] Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, and Houqiang Li. Smarteraser: Remove anything from images using masked-region guidance. arXiv preprint arXiv:2501.08279, 2025. [27] Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81768185, 2024. [28] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [30] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, 2023. [31] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [32] Duong H. Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. CoRR, 2024. 11 [33] Dongxu Li, Junnan Li, and Steven C. H. Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [34] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng. Visualcloze: universal image generation framework via visual in-context learning, 2025. [35] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [36] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [38] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, SIGGRAPH 2024, Denver, CO, USA, 27 July 20241 August 2024, 2024. [39] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. CoRR, 2025. [40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [41] Yingmao Miao, Zhanpeng Huang, Rui Han, Zibin Wang, Chenhao Lin, and Chao Shen. Shining yourself: High-fidelity ornaments virtual try-on with diffusion model. arXiv preprint arXiv:2503.16065, 2025. [42] Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: High-resolution multi-category virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22312235, 2022. [43] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. Ladi-vton: Latent diffusion textual-inversion enhanced virtual try-on. In Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 20233 November 2023, 2023. [44] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, 2024. [45] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [46] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [47] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, and Ran Xu. Unicontrol: unified diffusion model for controllable visual generation in the wild. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 12 [50] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [51] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large In Proceedings of the IEEE/CVF winter conference on mask inpainting with fourier convolutions. applications of computer vision, pages 21492159, 2022. [52] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [53] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. [54] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, and Meng Yang. Toward characteristicIn 15th European Conference, Munich, Germany, preserving image-based virtual try-on network. September 8-14, 2018, Proceedings, Part XIII, 2018. [55] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2244522455, 2023. [56] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [57] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. CoRR, 2024. [58] Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang. GP-VTON: towards general purpose virtual try-on via collaborative local-flow global-parsing learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, 2023. [59] Yuhao Xu, Tao Gu, Weifeng Chen, and Arlene Chen. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 89969004, 2025. [60] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 2023. [61] Lu Yang, Wenhe Jia, Shan Li, and Qing Song. Deep learning technique for human parsing: survey and outlook. International Journal of Computer Vision, 132(8):32703301, 2024. [62] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. CoRR, 2023. [63] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, 2023. [64] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [65] Xuanpu Zhang, Dan Song, Pengxin Zhan, Tianyu Chang, Jianhao Zeng, Qingguo Chen, Weihua Luo, and Anan Liu. Boow-vton: Boosting in-the-wild virtual try-on via mask-free pseudo data training. arXiv preprint arXiv:2408.06047, 2024. [66] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. [67] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K. Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [68] Nan Zhong, Yiran Xu, Zhenxing Qian, and Xinpeng Zhang. Rich and poor texture contrast: simple yet effective approach for ai-generated image detection. CoRR, 2023. [69] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: tale of two unets. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, 2023. [70] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In European Conference on Computer Vision, pages 195211. Springer, 2024."
        },
        {
            "title": "A Details of Benchmark and Metrics",
            "content": "As the pioneering work investigating the unified virtual try-on task, we construct comprehensive evaluation benchmark named OmniTry-Bench, accompanied by six dedicated metrics to systematically assess the quality of synthesized try-on images. Figure 7: The visualization of the OmniTry-Bench constitution. A.1 Constitution of Benchmark As the Figure 7, we gather evaluation samples within 12 common types of wearable objects, which can be summarized into 4 major classes: (i) clothes consisting of top, bottom and full-body garments, (ii) shoes in common styles, (iii) jewelries, including bracelets, earrings, necklaces and rings, (iv) accessories, including bags, belts, hats, glasses, sunglasses and ties. We consider detailed sub-types if necessary, such as the class bag consisted of the backpack, shoulder and tote bags. Clothes are divided into top cloth, bottom cloth, and dress. Each sub-type contains two gender groups (woman and man), with the exceptions that jewelries and dress exclusively contain woman samples, while tie contains only man samples. Each gender group includes 15 model images, where the garments are categorized into three settings: white background, natural background, and try-on setting. Every garment setting include 5 images. Following previous works categorization of virtual try-on scenarios into in-shop and in-the-wild, we further divide the model images for clothes and shoes into 15 shop-style and 15 wild-style samples per gender group, resulting in 30 model images per sub-type. The benchmark predominantly sources images from public repositories (Pexels2), supplemented with brand website materials and social media content under compliant data usage protocols. Pairing Strategy. For each gender group, we establish combinatorial pairs between model and garment images through: Maximum Pair Calculation: max_pairs = 15 7 = 6, 975 pairs, where 17 and 7 denote model settings counts for regular and style -specific categories respectively. 17+30 15 2https://www.pexels.com Sampled Pair Selection: selected_pairs = 15 24 = 360 paired samples, constrained by single-use garment policy and balanced sampling (15 models per clothes/shoes type, include 7 shop-style and 8 wild-style). 15 Overall, our experiments are all evaluated on the selected benchmark contains 360 pairs of images A.2 Evaluation Metrics As discussed before, the objectives of try-on can be divided into three aspects. Since there is no ground-truth result in mask-free setting, we redesign the metrics as follows: Object Consistency: We crop the objects from the try-on and object images via masking, then perform white-background normalization on the extracted objects. We compute the visual similarity using DINO [3] and CLIP [48] visual encoders, with metrics denoted as M-DINO and M-CLIP-I. As these 1, 1] where higher metrics measure cosine similarity in the embedding space, their values range in [ values indicate better object preservation. The M-DINO scores generally exhibit lower values than M-CLIP-I, as DINO-extracted features are more sensitive to geometric variations compared to CLIPs semantic-aligned embeddings. Our experiments quantitatively validate this behavior across different object categories. This discrepancy stems from their distinct learning objectives: M-DINO [3]: Learns dense local features through self-supervised distillation, emphasizing spatial consistency of object parts. Then compute the cosine similarity of two features. M-CLIP-I [48]: Optimizes global semantic alignment between object images, prioritizing category-level coherence. Then compute the cosine similarity of two features. Then compute the cosine similarity of two object features. Person Preservation: We extract the person regions by cropping try-on and original person images, masking the target object areas with black pixels. We then compute spatial-aligned similarity between these aligned image pairs using two complementary metrics: SSIM (Structural Similarity Index) [56]: Measures structural, luminance, and contrast 1, 1] with values approaching 1 indicating similarity between images. The metric ranges in [ higher structural consistency. LPIPS (Learned Perceptual Image Patch Similarity) [64]: Computes deep feature differences using pretrained VGG networks, better aligning with human perception than traditional metrics. Its values lie in [0, 1] where lower scores denote better preservation quality. Object Localization: We propose dual-strategy evaluation framework to assess spatial rationality through complementary approaches: G-Accuracy: Quantifies detection reliability using GroundingDINO [35] with the following implementation protocol: Invoke predict_with_classes API with target object categories as classes parameter. Configure detection thresholds: box_threshold = 0.25 (bounding box confidence) and text_threshold = 0.25 (text-image alignment). Last, calculate success rate as total test cases correct detections. CLIP-I: Evaluates semantic alignment through multi-modal similarity measurement: Generate descriptive prompts via Qwen2 [1] MLLM. Compute CLIP [48] embedding similarity between try-on images and generated text. Normalize scores to [0,1] range using min-max scaling. The final prompt template is formally defined as follows: \"\"\"Generate detailed description of composite image by combining elements from the two provided images: 1. Image 1: The models appearance (pose, clothing, facial features), background and style 2. Image 2: Only the <{garment_class}>, without any other infos (e.g., background, model) Describe the synthesized image with the model wearing the {garment_class}, in 65 words. Only describe the final imagined scene, without the detail or information of composite. The main description is from 16 Image 1. Briefly and shortly describe the {garment_class} in 6 words, no details needed. No words like (e.g., from the Image 2). If {garment_class} is cloth or dress , the model from the Image 1, replace with the {garment_class} from Image 2, no words like (replace the hair/shirt), using \"wear\" the {garment_class}. Examples outputs: - \"A young woman standing in studio with white background. She is wearing denim dress with button-down collar and long sleeves. The dress is knee-length and falls above her knees. The woman is also wearing black ankle boots with pointed toe and low heel. She has brown crossbody bag with strap across her shoulder. The bag appears to be made of leather and has small flap closure. The overall style of the outfit is casual and minimalistic.\" - \"A close-up portrait of young womans face and upper body. She is wearing black strapless top with thin silver chain necklace around her neck. Her hair is styled in loose waves and she is wearing large hoop earrings. The woman is looking off to the side with serious expression on her face. The background is plain white.\" - \"A close-up portrait of womans upper body. She is wearing black collared shirt with button-down collar and long sleeves. Her hair is styled in loose curls and she is wearing large, dangling earrings. Her hand is resting on her chest, with large ring on her ring finger. The background is plain white. The woman appears to be looking off to the side with serious expression on her face.\" \"\"\""
        },
        {
            "title": "B Details of Training Dataset",
            "content": "B.1 Dataset for Stage-1 The model in the first stage is jointly trained on two datasets, i.e., the unpaired in-the-wild images, and the dataset of stage-2 without the object image. We train on the datasets with sampling ratios of 2 : 1. To further investigate the class distribution in the unpaired dataset, we count the highly-frequent words in the object text descriptions. After filtering out the prepositions and verbs, the top-5 words are necklace, hat, glasses, sunglasses and watch. We also observe some classes excluded in our final 12 common classes, e.g., smartphone, cup, scarf, crown and mask. The rich distribution of wearable or holdable objects enhances the generalization of OmniTry to uncommon classes. Figure 8: The class distribution of training dataset. We also report the scale of dataset during the data preparation. The initial dataset contains 152K in-the-wild images, which are filtered to be 111K images with person and wearable objects. After listing, grounding and removing objects, the total amount of images containing at least one object is 94K, and the corresponding number of objects is 189K (roughly 2 objects per image). 17 B.2 Dataset for StageFor the training dataset of the second stage, we visualize the amount of samples for each class in Appendix . It is shown that the most common classes, i.e., clothes and shoes, constitute more half of the total dataset, while most classes lay in the long-tail of distribution with less than 3%. Such distribution is aligned with our basic assumption that it is hard to obtained paired samples for many wearable objects. For class-balanced training, we manfully assign the sampling weights for clothes, shoes and bags as 4, 4, 3, and set weights as 1 for remaining classes."
        },
        {
            "title": "C Details of Training and Model Architecture",
            "content": "C.1 Training Configuration During training, we resize the image with fixed aspect ratio to be no larger than 1 million, which means that the model could receive images with varying aspect ratios in one batch. To handle this, we pad the image tokens into the same length of sequence, and modify the attention block to forward only on the valid tokens. For both training of stage-1 and stage-2, we set the learning rate as 14, gradient accumulation steps as 1, weight decay as 0.01 and gradient norm clipping as 1.0. We use the AdamW [37] optimizer with hyper-parameters β1 = 0.9 and β2 = 0.999. The model is trained with mixed precision of bfloat16. We note that since we fine-tune based on the distilled version of FLUX [31], the guidance scale is fixed as 1 during training, and set as 30 during inference. C.2 Details of Re-purposing Inpainting Model We elaborate the details of adapting the inpainting model, FLUX.1-Fill in this paper, towards maskfree try-on task. During training, the input of model can be split into two sets in sequence dimension: The try-on image. Along the channel dimension, it contains the noisy ground-truth try-on image, the input person image and zero mask in the same shape. The object image. Along the channel dimension, it contains the noisy object image, the clean noisy image and zero mask. Then during the inference stage, we initialize the above input while replacing the noisy latents with standard Gaussian noise. Through the above formulation, it is shown that the inputs of person and object images are different. The person branch aims to modify the input person image in proper area, while the object branch simply targets to maintain the input, and transfers the object appearance via full attention mechanism. C.3 Details of Masked Full-Attention We discuss the details of applying masked full-attention in the second stage. We set text prompts for both try-on and object images, like trying on sunglasses. Suppose the length of tokens to be: LI1 for try-on image, LT 1 for try-on text, LI2 for object image, and LT 2 for object text. We concatenate all tokens in the above order. Then the attention mask is: 1LI1LI1 1LI1LI1 0LI1LI1 0LI1LI1 1LI1LT 1 1LI1LT 1 0LI1LT 1 0LI1LT 1 1LI1LI2 0LI1LI2 1LI1LI2 1LI1LI2 , 0LI1LI1 0LI1LI1 1LI1LI1 1LI1LI1 (2) where 1mn denotes all-one matrix and 0mn denotes all-zero matrix. More specifically, we apply such full-attention in both the multi-modality blocks and single blocks of FLUX [31], and figure out the text tokens to achieve the masking. We leverage the attention function with varying length in FlashAttention [12] to implement the block-wise masked attention. 18 C.4 LoRA Implementation We implement the location and identity adapters with LoRA [21]. In detail, we set the rank and α to be 16. We insert the LoRA module into the following layers: the projection into query/key/value, output projection of attention, the linear layers in feedforward block, the layer normalization layer, the input patch projection, and the final output projection."
        },
        {
            "title": "D Details of Compared Methods",
            "content": "In this section, we present the details of compared methods and our implementation of them on try-on task. We also report more results of the variants of each method, among which we only report the best result in main experiment. D.1 General Customized Image Generation OneDiffusion [32]: large-scale diffusion framework supporting bidirectional image synthesis across tasks. We evaluated its performance on mask-free/mask-based try-on through instruction-based cases. We also modify its original instructing prompt to achieve better performance. OmniGen [57]: vision-language unified framework consolidating multiple tasks, supporting both mask-free/mask-based generation. We also test it with both standard and our optimized prompts. VisualCloze [34] implements visual in-context learning for domain generalization. We conduct experiments with single example and multiple examples in the context. Paint-by-Example [60] enables to re-paint given subject into image via CLIP-based object representation with mask dependency. MimicBrush [5] achieves imitative inpainting for region-specific edits, requiring the input image with mask, together with the reference image without mask. ACE++ [39] extends long-context conditioning for instruction-driven generation that tackles various. D.2 Image-based Virtual Try-On OOTDiffusion [59] designs two-branch U-Net architecture to consume the person and garment images, which requires masked input in the person branch. Magic Clothing [4] introduces garment extractor to progressively insert garment features into the main backbone of try-on generation. Magic Clothing supports the input of either masked person image, or the targeting pose and person ID image. We adapt the former setting to better preserve the person image.FI CatVTON [10] proposes to transfer the identity of garment by simply concatenating it with the person image, and achieve mask-based try-on with inpainting model. FitDiT [25] introduces diffusion transformer (DiT) model into VTON, and designs GarmentDiT and DenoisingDiT to implement this task. Any2AnyTryon [18] is the only open-source mask-free VTON model, eliminates the dependence on masks, poses, or any other such conditions. D.3 More Comparison Results We report more comparison results in Tab. 3, including variants of methods with mask/mask-free setting, varying image size and different prompt design. We report only the best result of all variants in the main experiment."
        },
        {
            "title": "E More Visualization Results",
            "content": "We visualize more try-on results in Fig. 9, where we include all classes in OmniTry-Bench and different sub-types for full visualization. 19 Figure 9: The samples of the model, the object, and the try-on person. 20 Table 3: More evaluation results of the compared methods with different settings. method mask M-DINO M-CLIP-I LPIPS SSIM G-Acc. CLIP-T Object Consistency Person Presevation Object Localization Paint-by-Example (5122) [60] Paint-by-Example (10242) [60] MimicBrush [5] ACE++ (prompt v1) [39] ACE++ (prompt v2) [39] VisualCloze (1-example) [34] VisualCloze (2-example) [34] OmniGen (prompt v2) [57] OneDiffusion (prompt v1) [32] OneDiffusion (prompt v2) [32] OneDiffusion (prompt v1) [32] OneDiffusion (prompt v2) [32] VisualCloze (1-example) [34] VisualCloze (2-example) [34] OmniGen (prompt v1) [57] OmniGen (prompt v2) [57] OmniTry (Ours) Magic Clothing [4] CatVTON [10] CatVTON (w. garment mask) [10] OOTDiffusion [59] FitDiT (768 1024) [25] FitDiT (1152 1536) [25] FitDiT (1536 2048) [25] Any2AnyTryon [18] OmniTry (Ours) on the whole set 0.4171 0.4565 0.4693 0.4565 0.4449 0.4705 0.4236 0.5151 0.5515 0.5580 0.4178 0.4731 0.5292 0.4915 0.5299 0.5435 0.6160 0.7328 0.7727 0.7253 0.7474 0.7427 0.7533 0.7307 0.7761 0.8137 0.7950 0.7358 0.7749 0.7782 0.7619 0.7689 0.7869 0.8327 on the clothes subset 0.5665 0.5744 0.5534 0.5961 0.6718 0.6733 0.5961 0.6747 0.6995 0.7634 0.7906 0.7843 0.8016 0.8324 0.8340 0.8016 0.8537 0.8560 0.4577 0.3903 0.3033 0.4561 0.4554 0.6685 0.6767 0.6888 0.6607 0.5795 0.7606 0.7001 0.4471 0.4730 0.7009 0.6703 0. 0.2761 0.1664 0.2084 0.2178 0.1972 0.1618 0.2178 0.2089 0.1021 0.7968 0.8033 0.8575 0.7519 0.7517 0.5320 0.4908 0.5870 0.6166 0.6628 0.4951 0.5831 0.6190 0.5868 0.5727 0.5965 0.9333 0.8786 0.9283 0.8828 0.8865 0.8952 0.9027 0.8865 0.8969 0. 0.9833 0.9861 0.9250 0.9667 0.9722 0.9972 0.9917 0.9917 1.0 0.9972 1.0 0.9972 0.9639 0.9806 0.9778 0.9944 0.9972 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.2831 0.2804 0.2781 0.2791 0.2793 0.2283 0.2260 0.2557 0.2290 0.2401 0.2309 0.2309 0.2524 0.2540 0.2533 0.2535 0. 0.2700 0.2818 0.2797 0.2761 0.2822 0.2831 0.2761 0.2832 0."
        }
    ],
    "affiliations": [
        "Kunbyte AI",
        "Zhejiang University"
    ]
}