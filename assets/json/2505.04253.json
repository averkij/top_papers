{
    "paper_title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself",
    "authors": [
        "Maria Marina",
        "Nikolay Ivanov",
        "Sergey Pletenev",
        "Mikhail Salnikov",
        "Daria Galimzianova",
        "Nikita Krayko",
        "Vasily Konovalov",
        "Alexander Panchenko",
        "Viktor Moskvoretskii"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval."
        },
        {
            "title": "Start",
            "content": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself Maria Marina2,1, Nikolay Ivanov1, Sergey Pletenev2,1, Mikhail Salnikov2,1, Daria Galimzianova4, Nikita Krayko4, Vasily Konovalov2,5, Alexander Panchenko1,2, Viktor Moskvoretskii1,3 1Skoltech, 2AIRI, 3HSE University, 4MTS AI, 5MIPT {Maria.Marina, A.Panchenko, Mikhail.Salnikov}@skol.tech 5 2 0 2 7 ] . [ 1 3 5 2 4 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) excel in tasks like question answering (QA) (Yang et al., 2018; Kwiatkowski et al., 2019), but remain vulnerable to hallucinations (Yin et al., 2024; Ding et al., 2024). Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) mitigates this by incorporating external information, although it introduces risks such as error accumulation (Shi et al., 2023) and external hallucinations (Ding et al., 2024). Adaptive retrieval techniques (Moskvoretskii et al., 2025; Ding et al., 2024; Jeong et al., 2024) aim to balance LLM knowledge with external resources by estimating uncertainty to decide whether retrieval is needed. However, existing methods primarily frame this task as uncertainty estimation based on LLM internal states or outputs, leading to significant computational overhead. This can offset the efficiency gains from reduced retrieval calls and limit practicality, especially with larger models. Figure 1: PFLOPs-Inaccuracy trade-off for proposed features vs the most efficient alternative adaptive retrieval methods for the NQ dataset. Radius of the points is proportional to the number of LLM calls. Green dotted line indicate Always RAG approach. In this study, we address this issue by introducing LLM-independent adaptive retrieval methods that leverage external information, such as entity popularity and question type. Our methods achieve comparable quality while being significantly more efficient, eliminating the need for LLMs entirely. Our evaluation, shown in Figure 1, shows that our proposed features are much more efficient in terms of PFLOPs and LLM calls, with downstream performance comparable to other adaptive retrieval methods. Our contributions and findings are as follows: 1. We introduce 7 groups of lightweight external information features, encompassing 27 features, for LLM-independent adaptive retrieval. 2. Our approach significantly improves efficiency by eliminating the need for LLM-based uncertainty estimation while maintaining QA performance. 3. We show that our methods outperform uncertainty-based adaptive retrieval methods for complex questions. We make data and all models publicly available.1 1https://github.com/marialysyuk/External_ Adaptive_Retrieval"
        },
        {
            "title": "2 Related Work",
            "content": "Adaptive Retrieval-Augmented Generation reduces unnecessary retrievals by determining whether external knowledge is needed. This decision can be based on LLM output (Trivedi et al., 2023), consistency checks (Ding et al., 2024), internal uncertainty signals (Jiang et al., 2023; Su et al., 2024; Yao et al., 2024), or trained classifiers (Jeong et al., 2024). External Information methods can enhance retrieval, such as integrating knowledge graphs and the popularity of the entity. KG structures have been incorporated into LLM decoding to enable reasoning on graphs for more reliable answers (Luo et al., 2024). Popularity and graph frequency improve retrieval efficiency, as shown in LightRAG and MiniRAG, which prioritize frequently accessed entities and relationships (Guo et al., 2024; Fan et al., 2025). Graph-based features, including entity properties (Lysyuk et al., 2024), popularity (Mallen et al., 2023a), and structural attributes (Salnikov et al., 2023), have also been shown to be effective in QA systems."
        },
        {
            "title": "3 Methods",
            "content": "Our baselines include the following adaptive retrieval methods: Adaptive RAG uses T5-large-based classifier to determine whether retrieval is needed (Jeong et al., 2024). FLARE triggers retrieval when token probability falls below threshold (Jiang et al., 2023). DRAGIN estimates uncertainty based on token probabilities and attention weights, excluding stopwords (Su et al., 2024). Rowen relies on consistency checks across languages and models to trigger retrieval (Ding et al., 2024). SeaKR monitors internal state consistency to trigger retrieval, reranking snippets to reduce uncertainty (Yao et al., 2024). EigValLaplacian assesses uncertainty using graph features based on pairwise consistency scores (Lin et al., 2023). Max Token Entropy measures uncertainty by aggregating the maximum entropy of token distributions (Fomicheva et al., 2020). HybridUE includes 5 uncertainty features relevant to the task (Moskvoretskii et al., 2025): Mean Token Entropy, Max Token Entropy, SAR, EigValLaplacian, Lex-Similarity."
        },
        {
            "title": "3.1 External Information Methods",
            "content": "Each group may contain multiple features used to train classifier to predict retrieval needs, following Moskvoretskii et al. (2025); Jeong et al. (2024). Graph features capture information about the entities in question from knowledge graph, including the minimum, maximum, and mean number of triples per subject and object, where the subject or object corresponds to an entity from the question. Popularity features include the minimum, maximum, and mean number of Wikipedia page views per entity in the question. Frequency features include the minimum, maximum and mean frequencies of entities in reference text collection2, along with the frequency of the least common n-gram in the question. Knowledgability features assign score to each entity, reflecting the LLMs verbalized uncertainty about its knowledge. By pre-computing these scores for entities in the Wikidata Knowledge Graph, retrieval decisions can be made without querying the LLM at inference time. Question Type features include probabilities for nine categories: ordinal, count, generic, superlative, difference, intersection, multihop, comparative, and yes/no. Question Complexity reflects the difficulty of question, considering the reasoning steps required. Context Relevance features include the minimum, maximum and mean probabilities that context is relevant to the question, along with the context length. HybridExternal includes all external features. HybridUFP includes all external features except frequency and popularity, as they are highly correlated with graph features. HybridFP includes uncertainty and all external features except frequency and popularity."
        },
        {
            "title": "The details of all methods are described in the",
            "content": "Appendix A."
        },
        {
            "title": "4 Experimental Setup",
            "content": "In this section, we briefly discuss the implementation details and the evaluation setup. 4."
        },
        {
            "title": "Implementation Details",
            "content": "We use LLaMA 3.1-8B-Instruct (Dubey et al., 2024) and the BM25 retriever (Robertson et al., 1994) as the main components of our approach, In this section, we describe the proposed external information methods for adaptive retrieval. 2https://www.inf.uni-hamburg.de/en/inst/ab/lt/ resources/data/depcc.html Method Never RAG Always RAG AdaptiveRAG DRAGIN FLARE RowenCM Seakr EigValLaplacian MaxTokenEntropy Hybrid UE Graph Popularity Frequency Knowledgability Question type Question complexity Context relevance HybridUFP HybridExternal HybridFP All"
        },
        {
            "title": "Ideal",
            "content": "NQ SQuAD TQA 2Wiki HotPot Musique InAcc LMC RC InAcc LMC RC InAcc LMC RC InAcc LMC RC InAcc LMC RC InAcc LMC RC 44.6 49.6 49.6 48.0 45.0 49.4 40.6 51.2 50.6 50.2 49.0 49.8 49.8 49.6 49.6 49.6 49.0 47.8 46.0 48.4 47. 60.8 1.0 1.0 0.00 1.00 17.6 31.2 1.0 1.0 0.00 1. 63.6 61.0 1.0 1.0 0.00 1.00 31.8 37.4 1.0 1.0 0.00 1. 28.6 41.0 1.0 1.0 0.00 1.00 10.6 10.0 1.0 1.0 0.00 1. 0.98 2.24 2.07 7.27 1.00 0.81 0.58 0.77 0.87 0.92 0.96 0.95 0.88 1.00 1.00 28.6 29.8 23.8 19.6 26.8 31.4 31.2 31.4 30.4 31.2 31.0 31.2 30.4 31.2 31. 1.00 1.00 30.8 30.2 Multi-Step Adaptive Retrieval 0.97 2.14 2.08 7.20 1.00 0.10 0.10 0.98 0.95 1.00 0.99 1.00 0.97 1.00 1. 62.8 66.6 64.8 65.6 65.6 1.5 4.1 2.1 28.7 14.6 0.54 2.06 1.39 7.12 1.00 Uncertainty Estimation 64.0 65.0 63.8 1.3 1.2 1. External Features 63.6 63.2 63.2 63.0 64.0 63.6 62.8 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.26 0.22 0.27 0.32 0.15 0.04 0.28 0.29 0.10 1.00 45.4 45.6 42.4 44.4 39. 38.4 37.6 37.4 35.8 35.6 37.4 38.4 35.6 36.8 36.0 Hybrids with External Features 1.00 1.00 63.4 63.2 1.0 1. 1.00 1.00 36.4 37.0 2.0 4.3 3.1 29.2 14.6 2.0 2.0 2.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 2. Hybrids with Uncertainty and External Features 1.00 1.00 0.55 31.2 30.8 36.0 2.0 2. 1.8 1.00 1.00 0.82 64.6 64.2 73.6 1.3 1. 1.4 1.00 1.00 0.36 37.8 37.8 50.0 2.0 4.5 3.1 29.5 14. 1.8 1.7 1.7 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.8 1.8 1.8 1.6 5.2 5.8 3.9 32.9 12. 2.0 2.0 2.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 2.0 2.0 2.0 1.7 2.64 2.92 2.85 7.87 2. 0.98 0.95 0.98 0.67 0.84 0.84 0.89 0.74 0.94 1.00 41.4 43.0 37.2 35.6 42.4 41.0 41.4 41.2 40.8 41.0 41.0 41.0 39.6 41.0 41.0 1.00 1. 39.8 39.2 1.00 1.00 0.68 41.0 37.8 46.0 4.6 5.1 5.1 31.9 9. 1.9 2.0 1.9 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.9 1.9 1.9 1.7 2.34 2.56 4.07 7.70 1. 0.91 0.99 0.94 0.97 0.94 0.94 1.00 0.88 1.00 1.00 14.0 13.4 9.0 10.4 11.8 10.2 10.6 10.6 10.0 10.0 10.0 9.8 10.0 10.6 10.6 1.00 1. 10.6 12.2 1.00 1.00 0.71 12.2 11.2 16.4 3.6 6.3 4.1 42.1 12. 2.0 2.0 1.8 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.8 1.8 1.8 1.9 3.63 3.15 3.10 9.52 2. 1.00 0.87 0.75 1.00 0.96 0.96 0.61 1.00 0.95 1.00 1.00 1.00 1.00 1.00 0.89 Table 1: QA Performance of adaptive retrieval and uncertainty methods. Ideal represents the performance of system with an oracle providing ideal predictions for the need to retrieve. InAcc denotes In-Accuracy, measuring the QA systems performance. LMC indicates the mean number of LM calls per question, and RC represents the mean number of retrieval calls per question. The SOTA results are highlighted in bold, as well as the best results for the external methods. following Yao et al. (2024); Jeong et al. (2024); Moskvoretskii et al. (2025)."
        },
        {
            "title": "4.2 Datasets",
            "content": "We evaluate on single-hop SQuAD v1.1 (Rajpurkar et al., 2016), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and multi-hop MuSiQue (Trivedi et al., 2022), HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (2wiki) (Ho et al., 2020) QA datasets to ensure real-world query complexity, following Trivedi et al. (2023); Jeong et al. (2024); Su et al. (2024); Yao et al. (2024). We use 500-question subsets from the original test sets, as in Moskvoretskii et al. (2025); Jeong et al. (2024)."
        },
        {
            "title": "4.3 Evaluation",
            "content": "We evaluate both the quality and efficiency of the adaptive retrieval system. For quality, we use InAccuracy (InAcc), which measures whether the LLM output contains the ground-truth answer, as it is reliable metric based on Moskvoretskii et al. (2025); Mallen et al. (2023b); Jeong et al. (2024); Asai et al. (2024); Baek et al. (2023). Following Jeong et al. (2024); Moskvoretskii et al. (2025), for efficiency we adopt Retrieval Calls (RC) the average number of retrievals per question, and LM Calls (LMC) the average number of LLM calls per question, including uncertainty estimation. Further details are provided in Appendix B."
        },
        {
            "title": "5 Results",
            "content": "In the following sections, we present the results of the end-to-end and UE methods, as well as groups of external features, focusing on downstream performance and efficiency. For comparison, we also include the Never RAG, Always RAG, and Ideal benchmarks. The Ideal benchmark represents the performance of system with an oracle providing perfect retrieval predictions. Downstream Performance First, we assess whether external methods can replace uncertaintybased approaches. As shown in Table 1, at least one external feature matches the performance of the uncertainty estimation methods for each dataset. Combining external features even increases InAccuracy for the Musique dataset. Compared to MultiStep Adaptive Retrieval, using only external features yields similar results across all datasets, except for 2wiki. Second, we examine whether external methods (a) trivia (b) musique Figure 2: Feature importances for one of the best algorithms for only external features vs all features for TriviaQA (simple) and Musique (complex) datasets."
        },
        {
            "title": "6 Features Reciprocity",
            "content": "We identify four key aspects that influence adaptive retrieval performance: LLM knowledge (uncertainty features, knowledgability), question type (simple vs. complex reasoning), context relevance (irrelevant context reduces performance), and entity rarity (approximated by entity popularity groups). Figure 2 shows that for the simple TriviaQA dataset, the Top-5 features are uncertaintybased, while for complex datasets, question type and context relevance become more important. Thus, relying solely on uncertainty-based features is insufficient for efficient adaptive retrieval. External features tend to be more substitutive than complementary, as they often exhibit strong correlations despite their differences. As shown in Figure 3, for simple questions, uncertainty strongly correlates with graph features, question complexity, knowledgability, and context relevance. For complex questions, uncertainty correlates with question complexity, type, and knowledgability. Heatmaps and feature importances for other datasets could be found in Appendix D."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduced 7 groups of lightweight external features for LLM-independent adaptive retrieval, improving efficiency by eliminating the need for LLM-based uncertainty estimation while preserving QA performance. Our approach outperforms uncertainty-based methods for complex questions and offers detailed analysis of the complementarity between uncertainty and external features. Figure 3: Heatmap of different groups of features for TriviaQA and 2WikiMultiHopQA (2wiki) datasets. Upper right triangle states for the absolute correlations on the TriviaQA, while down left states for the absolute correlations on the 2WikiMultiHopQA complement uncertainty-based approaches. Our findings show that hybrids with uncertainty features do not outperform any external feature combinations, suggesting that these features are more substitutive than complementary. Efficiency Performance External features significantly reduce LLM calls, addressing key efficiency bottleneck that worsens with LLM scaling. However, they lead to slightly more conservative behavior with increased Retrieval Calls, though still fewer than Multi-Step approaches. Since external information features are pre-computed, no additional LLM calls are required during inference."
        },
        {
            "title": "Limitations",
            "content": "We evaluate model performance using six widely adopted QA datasets. Incorporating broader range of datasets, particularly those tailored to specific domains, could offer more comprehensive insights and showcase the versatility of our approach. Our study focuses on the LLaMA3.1-8BInstruct model, top-performing open-source model within its parameter range. Expanding the analysis to additional architectures could further strengthen the generalizability of our results."
        },
        {
            "title": "Ethical Considerations",
            "content": "Text retrieval systems can introduce biases into retrieved documents, which may inadvertently steer the outputs of even ethically aligned LLMs in unintended directions. Consequently, developers integrating RAG and Adaptive RAG pipelines into user-facing applications should account for this potential risk."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, and Sung Ju Hwang. 2023. Knowledgeaugmented language model verification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 17201736. Association for Computational Linguistics. Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gaël Varoquaux. 2013. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pages 108122. Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024. Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models. CoRR, abs/2402.10612. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Research Facebook. 2019. fvcore: light-weight core library for computer vision frameworks. https:// github.com/facebookresearch/fvcore. Tianyu Fan, Jingyuan Wang, Xubin Ren, and Chao Huang. 2025. Minirag: Towards extremely simple retrieval-augmented generation. Preprint, arXiv:2501.06713. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539555. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. Lightrag: Simple and fast retrievalaugmented generation. Preprint, arXiv:2410.05779. John Hancock and Taghi Khoshgoftaar. 2020. Catboost for big data: an interdisciplinary review. Journal of big data, 7(1):94. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 66096625. International Committee on Computational Linguistics. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 70367050. Association for Computational Linguistics. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 79697992, Singapore. Association for Computational Linguistics. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 16011611. Association for Computational Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452 466. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence: Uncertainty quantification for black-box large language models. arXiv preprint arXiv:2305.19187. Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, and Shirui Pan. 2024. Graph-constrained reasoning: Faithful reasoning on knowledge graphs with large language models. Preprint, arXiv:2410.13080. Maria Lysyuk, Mikhail Salnikov, Pavel Braslavski, and Alexander Panchenko. 2024. Konstruktor: strong baseline for simple knowledge graph question answering. CoRR, abs/2409.15902. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023a. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, Toronto, Canada. Association for Computational Linguistics. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 98029822. Association for Computational Linguistics. Viktor Moskvoretskii, Maria Lysyuk, Mikhail Salnikov, Nikolay Ivanov, Sergey Pletenev, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Irina Nikishina, and Alexander Panchenko. 2025. Adaptive retrieval without self-knowledge? bringing uncertainty back home. arXiv preprint arXiv:2501.12835. Mikhail Plekhanov, Nora Kassner, Kashyap Popat, Louis Martin, Simone Merello, Borislav Kozlovskii, Frédéric A. Dreyer, and Nicola Cancedda. 2023. Multilingual end to end entity linking. CoRR, abs/2306.08896. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 23832392. The Association for Computational Linguistics. Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109 126. National Institute of Standards and Technology (NIST). Mikhail Salnikov, Hai Le, Prateek Rajput, Irina Nikishina, Pavel Braslavski, Valentin Malykh, and Alexander Panchenko. 2023. Large language models meet knowledge graphs to answer factoid questions. In Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation, pages 635644, Hong Kong, China. Association for Computational Linguistics. Priyanka Sen, Alham Fikri Aji, and Amir Saffari. 2022. Mintaka: complex, natural, and multilingual dataset for end-to-end question answering. In Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, pages 16041619. International Committee on Computational Linguistics. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 3121031227. PMLR. Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: dynamic retrieval augmented generation based on the real-time information needs of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1299113013. Association for Computational Linguistics. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539554. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledgeIn Proceedings of intensive multi-step questions. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1001410037. Association for Computational Linguistics. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023. Freshllms: Refreshing large language models with search engine augmentation. Preprint, arXiv:2310.03214. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23692380. Association for Computational Linguistics. Zijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, and Juanzi Li. 2024. Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation. CoRR, abs/2406.19215. Xunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. 2024. Benchmarking knowledge boundary for large language models: different perspective on model evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 22702286. Association for Computational Linguistics."
        },
        {
            "title": "A External Methods",
            "content": "Graph Using the BELA entity linking module (Plekhanov et al., 2023), the entities from the question are linked to the corresponding IDs in the Wikidata knowledge graph. Then, for each entity the number of triples where this entity is either an object or subject is retrieved. Finally, six features are calculated: the minimum/maximum/mean number of triples per subject and object. Popularity Using the BELA NER module (Plekhanov et al., 2023) the entities are retrieved from the question. Then, for each entity the mean amount of views per Wikipedia page is calculated using Wikimedia API3 for last year. Finally, there are three features: the minimum/maximum/mean number of views per entity per question. Knowledgability The prompt to the LLaMA 3.1-8B-Instruct model to approximate its interal knowledge: Answer the following question based on your internal knowledge with one or few words. If you are sure the answer is accurate and correct, please say 100. If you are not confident with the answer, please range your knowledgability from 0 to 100, say just number. For example, 40. Question: {question}. Answer: Question type Using the train part of the Mintaka dataset (Sen et al., 2022), we train classifier based on the bert-base-uncased model4 to predict whether question belongs to one of the 12 question types: ordinal, count, generic, superlative, difference, intersection, multihop, yesno, intersection, comparative, multihop, yes/no. As result, we get twelve probabilities that the question belongs to certain class. The accuracy classification score on the validation part of the Mintaka dataset is 0.93. Question Complexity is based on N-hop feature from FreshQA (Vu et al., 2023) dataset: One-hop, where the question is explicit about all the relevant information needed to complete the task, so no additional inference is needed. Multi-hop, where the question requires one or more additional inference steps to gather all the relevant information needed to complete the task. The dataset consists of 500 training and 100 test examples. As training model, we used Distil-bert 5 model. The final F1 score on the test set is 0.82. Context relevance Each question with one context at time is passed to the cross-encoder model based on the uncased model of the bert base. question and context are passed via the [SEP] token with the additional classification head over the base model. The final probabilities of each context being relevant are aggregated via minimum/maximum/mean across all contexts. Additionally, there is the fourth feature that calculates the context length."
        },
        {
            "title": "B Technical Details",
            "content": "Train setting. We conduct all experiments using the LLaMA 3.1-8B-Instruct model with its default generation parameters. The responses generated, with and without the retriever, are sourced from previous studies (Moskvoretskii et al., 2025), following the AdaptiveRAG framework (Jeong et al., 2024). The baseline results are also adopted from prior work, as we employ the exact same settings and generation configurations. 3https://foundation.wikimedia.org/wiki/Api/ 4https://hf.co/google-bert/bert-base-uncased 5https://hf.co/distilbert/distilbert-base-uncased We implemented classifiers using Scikit-learn (Buitinck et al., 2013), CatBoost (Hancock and Khoshgoftaar, 2020), and performed hyperparameter tuning using validation set of 100 samples randomly selected from the training set, testing with three different random seeds for each dataset. We evaluated seven classifiers: Logistic Regression, KNN, MLP, Decision Tree, CatBoosting, Gradient Boosting, and Random Forest. Data preprocessing involved standard scaling. For the final model, we used VotingClassifier, combining the two best-performing classifiers from the validation set, each trained with their optimal hyperparameters. Performance was evaluated based on the In-accuracy metric, and the top classifiers were retrained on the full training set with these selected hyperparameters. Hyperparameters grid. Logistic Regression : C: [0.01, 0.1, 1], solver: [lbfgs, liblinear], class_weight: [balanced, 0: 1, 1: 1, None], max_iter: [10000, 15000, 20000] KNN : n_neighbors: [5, 7, 9, 11, 13, 15], metric: [euclidean, manhattan], algorithm: [auto, ball_tree, kd_tree], weights: [uniform, distance] MLP : hidden_layer_sizes: [(50,), (100,), (50, 50), (100, 50), (100, 100)], activation: [relu, tanh], solver: [adam, sgd], alpha: [0.00001, 0.0001, 0.001, 0.01], learning_rate: [constant, adaptive], early_stopping: True, max_iter: [200, 500] Decision Tree : max_depth: [3, 5, 7, 10, None], max_features: [0.2, 0.4, sqrt, log2, None], criterion: [gini, entropy], splitter: [best, random] CatBoosting: iterations: [10, 50, 100, 200], learning_rate: [0.001, 0.01, 0.05], depth: [3, 4, 5, 7, 9], bootstrap_type: [Bayesian, Bernoulli, MVS] Gradient Boosting: n_estimators: [25, 35, 50], learning_rate: [0.001, 0.01, 0.05], max_depth: [3, 4, 5, 7, 9], max_features: [0.2, 0.4, sqrt, log2, None] Random Forest: n_estimators: [25, 35, 50], max_depth: [3, 5, 7, 9, 11], max_features: [0.2, 0.4, sqrt, log2, None], bootstrap: [True, False], criterion: [gini, entropy], class_weight: [balanced, 0: 1, 1: 1, None]"
        },
        {
            "title": "C FLOPs calculation",
            "content": "NQ"
        },
        {
            "title": "AdaptiveRAG\nSeaKR\nDRAGIN\nFLARE\nRowen",
            "content": "0.0216 0.3504 0.2608 0.09699 1.865 EigValLaplacian MaxTokenEntropy Entity_popularity Is_complex Llama_know Context_relevance Question_type 0.10517 0.027116 0.0181238962 0.0181418 0.018291 0.018327 0.01812162 0.4389 2.4548 1.0129 0.9290 15.9677 0.3291 0.22121 0.210304 0.2082277 0.22747 0.2084429 0.2073669 Table 2: comparison of FLOPs usage across different methods on the Natural Questions (NQ) dataset. The Mean column shows the average PFLOPs (1015 FLOPs) per question, while the Upper bound column represents the theoretical maximum FLOPs assuming the LLaMA 3.1 8B model (in FP16 precision) runs at 100% GPU utilization for the entire processing of single sample. The row labeled Entity_popularity reflects the computational overhead required for graph/popularity/frequency features. It is important to note that for features such as \"Entity_popularity\", \"Is_complex\", \"Llama_know\", \"Context_relevance\", \"Question_type\" the generation of final answer for question (after precomputing these features) accounts for more than 99% of the total FLOPs. To calculate floating-point operations (FLOPs), we used the fvcore (Facebook, 2019) library developed by Facebook Research. This library provides flexible and efficient interface for analyzing the computational complexity of PyTorch models. Specifically, we wrapped our model generation process with the FlopCountAnalysis class, which automatically traces the model forward pass and counts the number of FLOPs for each layer. The theoretical analysis includes an approximate formula to calculate an upper bound per sample: Total FLOPs (cid:0)Total TFLOPs(cid:1) 1012 (Elapsed Seconds), where Total TFLOPs = (TFLOPs per GPU) (Number of GPUs), assuming 100% utilization."
        },
        {
            "title": "D Heatmaps and feature importances for all datasets",
            "content": "(a) nq (b) squad (c) 2wikimultihop (d) hotpot Figure 4: Feature importances for one of the best algorithms for only external features vs all features for NQ, TriviaQA (simple) and HotpotQA, Musique (complex) datasets. (a) nq (b) squad (c) trivia (d) hotpot (e) 2wikimultihop (f) musique Figure 5: Absolute correlation of features from different groups of external features with class label"
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE University",
        "MIPT",
        "MTS AI",
        "Skoltech"
    ]
}