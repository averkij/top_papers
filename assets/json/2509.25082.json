{
    "paper_title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification",
    "authors": [
        "Xiaoyi Huang",
        "Junwei Wu",
        "Kejia Zhang",
        "Carl Yang",
        "Zhiming Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Adversarial purification with diffusion models has emerged as a promising defense strategy, but existing methods typically rely on uniform noise injection, which indiscriminately perturbs all frequencies, corrupting semantic structures and undermining robustness. Our empirical study reveals that adversarial perturbations are not uniformly distributed: they are predominantly concentrated in high-frequency regions, with heterogeneous magnitude intensity patterns that vary across frequencies and attack types. Motivated by this observation, we introduce MANI-Pure, a magnitude-adaptive purification framework that leverages the magnitude spectrum of inputs to guide the purification process. Instead of injecting homogeneous noise, MANI-Pure adaptively applies heterogeneous, frequency-targeted noise, effectively suppressing adversarial perturbations in fragile high-frequency, low-magnitude bands while preserving semantically critical low-frequency content. Extensive experiments on CIFAR-10 and ImageNet-1K validate the effectiveness of MANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original classifier, while boosting robust accuracy by 2.15, and achieves the top-1 robust accuracy on the RobustBench leaderboard, surpassing the previous state-of-the-art method."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 8 0 5 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Under Review",
            "content": "MANI-PURE: MAGNITUDE-ADAPTIVE NOISE INJECTION FOR ADVERSARIAL PURIFICATION Xiaoyi Huang1, Junwei Wu2, Kejia Zhang1, Carl Yang2, Zhiming Luo1 1Xiamen University 2Emory University Project Page: https://phoshowy.github.io/MANI-Pure.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "Adversarial purification with diffusion models has emerged as promising defense strategy, but existing methods typically rely on uniform noise injection, which indiscriminately perturbs all frequencies, corrupting semantic structures and undermining robustness. Our empirical study reveals that adversarial perturbations are not uniformly distributed: they are predominantly concentrated in high-frequency regions, with heterogeneous magnitude intensity patterns that vary across frequencies and attack types. Motivated by this observation, we introduce MANI-Pure, magnitude-adaptive purification framework that leverages the magnitude spectrum of inputs to guide the purification process. Instead of injecting homogeneous noise, MANI-Pure adaptively applies heterogeneous, frequency-targeted noise, effectively suppressing adversarial perturbations in fragile high-frequency, low-magnitude bands while preserving semantically critical low-frequency content. Extensive experiments on CIFAR-10 and ImageNet-1K validate the effectiveness of MANI-Pure. It narrows the clean accuracy gap to within 0.59% of the original classifier, while boosting robust accuracy by 2.15%, and achieves the top-1 robust accuracy on the RobustBench leaderboard, surpassing the previous state-of-the-art method. 1 INTRODUCTION Deep neural networks have achieved remarkable success across diverse applications. However, their vulnerability to adversarial perturbations remains critical challenge (Weng et al., 2023; Tao et al., 2024; Goodfellow et al., 2014), particularly in safety-critical domains where reliability is paramount (Bortsova et al., 2021; Shao et al., 2025; Ye et al., 2024). primary line of defense is adversarial training (AT), which augments training with adversarial examples to enhance robustness (Mao et al., 2023; Schlarmann et al., 2024). Although effective, AT incurs substantial computational costs and suffers from limited generalization, posing challenges for both large-scale and cross-domain deployment. These limitations have motivated an alternative paradigm: adversarial purification (AP). Unlike AT, AP does not require retraining classifiers; instead purifies adversarial inputs at inference, restoring them to clean representations (Samangouei et al., 2018; Nie et al., 2022). This design offers flexibility, scalability, and compatibility with off-the-shelf models. Diffusion-based purification (DBP) has become the most effective and widely adopted approach in AP. It suppresses perturbations by injecting uniform noise in the forward process and then reconstructing images via reverse diffusion. Several variants have been proposed, such as the gradual noise scheduling (Lee & Kim, 2023) and the purification-enhanced AT method (Lin et al., 2024). Despite these advances, existing DBP and related defense methods often assume that adversarial perturbations are uniformly distributed across the frequency domainan assumption that is contradicted by empirical evidence. As shown in Figure 1a, radial spectral analysis reveals that perturbations are unevenly concentrated in the high-frequency region. Figure 1b reflects the heterogeneity in magnitude intensity across different frequency bands and attack strategies. As result, uniform noise injection faces trade-off: strong noise disrupts low-frequency semantics, reducing clean accuracy, whereas weak noise fails to suppress high-frequency perturbations, thereby compromising robustness. This motivates the need for frequency-adaptive purification that targets perturbationprone regions while preserving semantic fidelity. Corresponding author."
        },
        {
            "title": "Under Review",
            "content": "(a) Magnitude distribution differences between clean and adversarial images (b) Noise magnitude spectra of common adversarial attacks Figure 1: Radial spectrum analysis of adversarial perturbations. Overall, adversarial noise aligns with clean samples in low-to-mid frequencies but diverges in high-frequency bands. Specifically, Left: adversarial samples show irregular high-frequency peaks with uneven magnitude distribution. Right: common attacks concentrate perturbations in high-frequency regions, yet their spectral distributions and intensities differ significantly. These observations highlight the limitation of uniform noise injection and directly motivate our magnitude-adaptive design. To address this challenge, we propose MANI-Pure, magnitude-adaptive purification framework that redesigns the diffusion process from the frequency-domain perspective. The framework comprises two complementary modules: MANI adaptively adjusts the noise injection intensity across different regions based on the magnitude spectrum, ensuring the injected noise aligns with the vulnerability to perturbations while preserving the original image semantics from excessive distortion. FreqPure (Pei et al., 2025a) employs magnitudephase decomposition to explicitly distinguish low and high frequency components, preserving low-frequency content while focusing purification on high frequencies. Together, MANI emphasizes magnitude-aware adaptivity, while FreqPure enforces explicit frequency constraints. Their synergy enables precise suppression of concentrated perturbations while maximally retaining semantic structure, thereby improving robustness across diverse attacks. We conduct extensive evaluations on CIFAR-10 (Krizhevsky et al., 2010) and ImageNet-1K (Deng et al., 2009) under strong adaptive attacks, including PGD+EOT (Madry et al., 2017; Athalye et al., 2018), AutoAttack (Croce & Hein, 2020), and BPDA+EOT (Hill et al., 2021). Results show that MANI-Pure significantly enhances robustness while maintaining high clean accuracy, consistently outperforming existing DBP methods. Importantly, the framework is plug-and-play, readily applicable to modern architectures such as CLIP (Radford et al., 2021), without additional training cost. In summary, our main contributions are briefly summarized as follows: We empirically verify that adversarial perturbations are concentrated in high-frequency bands and further reveal distributional differences between adversarial and clean samples in the magnitude spectrum. The proposed MANI-Pure framework combines magnitude-adaptive diffusion with frequency-domain purification, achieving principled balance between semantic fidelity and perturbation mitigation, reflected in improvements to both clean and robust accuracy. Extensive experiments across datasets, attacks, and backbones demonstrate the superiority of our method in terms of robustness, clean accuracy and perceptual quality, as well as its scalability as plug-and-play module."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Adversarial purification provides defense paradigm that restores adversarial inputs to clean representations at inference time, thereby avoiding the retraining cost of adversarial training. Generative Models for Adversarial Purification. Early AP methods employed GANs, such as Defense-GAN (Samangouei et al., 2018), which projected adversarial samples onto the manifold of clean data. However, their limited generative fidelity and vulnerability to adaptive attacks significantly hindered their effectiveness. The advent of diffusion models marked turning point: through stable likelihood-based training and high-quality reconstructions, they became the backbone of modern AP. Representative approaches include DiffPure (Nie et al., 2022), stochastic score-based denoising (Song et al., 2020), and gradient-guided purification like GDMP (Wang et al., 2022). Precision Noise Injection. key limitation of uniform noise injection lies in its disregard for the spectral structure of adversarial noise. Prior studies have shown that perturbations are often concentrate in high-frequency, low-magnitude regions (Yin et al., 2019). Building on this insight, FreqPure (Pei et al., 2025b) preserved low-frequency amplitude during reverse diffusion, effectively protects semantic content while targeting vulnerable high-frequency regions. These results highlight the importance of frequency-aware purification. Another line of research refines the forward noising process itself. Divide-and-Conquer (Pei et al., 2025a) integrates heterogeneous noise to better suppress adversarial perturbations, Sample-Specific Noise Injection (Sun et al., 2025) adapts noise to each input, and DiffCap (Fu et al., 2025) extends such ideas to visionlanguage models. While promising, these strategies remain largely fixed or heuristic, and they do not explicitly adapt to the actual spectral distribution of adversarial noise. We unify these insights by introducing magnitude-adaptive noise injection scheme that dynamically allocates noise to spectrally vulnerable regions, coupled with frequency-domain purification. This design enables precise suppression of perturbations while preserving semantic fidelity, thereby advancing AP toward finer-grained and more generalizable defenses."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "To eliminate adversarial perturbations while preserving semantic content, we propose MANI-Pure, diffusion-based, frequency-domain purification framework comprising two complementary modules: Magnitude-Adaptive Noise Injection (MANI) and Frequency Purification (FreqPure). Figure 2 illustrates the overall structure. Before presenting the details, we briefly introduce the necessary background information. 3.1 PRELIMINARIES We briefly introduce diffusion model, adversarial purification, and the frequency-domain theory relevant to our method. Diffusion Model. Denoising Diffusion Probabilistic Model (DDPM) (Ho et al., 2020) generates data through two-stage process: forward noising process and reverse denoising process. Forward process. sample x0 is gradually perturbed into Gaussian noise through Markov chain: xt; (cid:112)1 βt xt1, βtI q(xt xt1) = t = 1, . . . , T, (cid:16) (cid:17) (1) , where βt follows predefined variance schedule. By marginalization: αt x0, (1 αt)I(cid:1) , q(xt x0) = (cid:0)xt; with αt = 1 βt and αt = (cid:81)t Reverse process. To recover clean samples, the reverse distribution is approximated as s=1 αs. pθ(xt1 xt) = (cid:0)xt1; µθ(xt, t), σ2 I(cid:1) . Instead of predicting µθ directly, DDPM parameterizes it with noise predictor ϵθ(xt, t): (cid:16) (cid:17) µθ(xt, t) = 1 αt βt xt 1 αt ϵθ(xt, t) , (2) (3) (4)"
        },
        {
            "title": "Under Review",
            "content": "Figure 2: The pipeline of MANI-Pure. (I) MANI. Starting from an adversarial sample, we apply DFT to obtain its frequency representation, partition it into bands, compute average magnitudes, and derive band-wise and spatial weights. These weights modulate Gaussian noise to produce heterogeneous perturbations. (II) FreqPure. During the reverse process, the magnitude and phase spectra of the adversarial input and generated image are separated and recombined as shown, with the reconstructed image iteratively fed into subsequent denoising steps. and the variance has closed form: σ2 = 1 αt1 1 αt βt. (5) Sampling. Starting from xT (0, I), the model iteratively computes xt1 = µθ(xt, t) + σtz with (0, I) until ˆx0 is obtained. Frequency-domain Theory. For an image RHW , the discrete Fourier transform (DFT) yields F(x)(u, v) = (cid:88) h,w x(h, w) e2πi(uh/H+vw/W ). (6) Each Fourier coefficient can be expressed in polar form as F(x)(u, v) = Ax(u, v) eiΦx(u,v), where Ax(u, v) = F(x)(u, v) is the magnitude spectrum, reflecting the intensity of frequency components, and Φx(u, v) is the phase spectrum, encoding structural and semantic information. (7) 3.2 MAGNITUDE-ADAPTIVE NOISE INJECTION Building upon the frequency-domain preliminaries introduced in Section 3.1, we leverage the magnitude spectrum of the adversarial input xadv to capture the uneven distribution of frequency components. Specifically, the spectrum is partitioned into non-overlapping frequency bands Bi. The average magnitude in each band is computed as Mi = 1 Bi (cid:88) (u,v)Bi Axadv (u, v), (8) where Bi denotes the number of coefficients in band Bi. This corresponds to step (a) of the magnitude-adaptive noise injection on the left in Figure 2. Low-magnitude bands are empirically more vulnerable to adversarial perturbations, while highmagnitude bands correspond to dominant semantic structures. To emphasize fragile regions, we assign larger weights to lower-magnitude bands: wi = 1 + ϵ0 γ , (9)"
        },
        {
            "title": "Under Review",
            "content": "where γ controls the sharpness of weighting and ϵ0 prevents numerical instability when Mi is very small. The band-wise weights produce frequency-domain weight distribution, which is transformed back to the spatial domain via inverse DFT to obtain pixel-wise noise intensity map W. In Figure 2, step (b) shows visual representation of these two weights. The spatial map modulates Gaussian noise ϵG (0, I) by element-wise multiplication: ϵt = ϵG, s.t. W, ϵG RHW C. Hence, the forward diffusion process becomes: xt = αt xadv + 1 αt ϵt, (10) (11) where αt is the cumulative product of noise scheduling coefficients."
        },
        {
            "title": "3.3 FREQUENCY PURIFICATION",
            "content": "To complement MANI, we further adopt frequency purification strategy (Pei et al., 2025b) during the reverse diffusion process. The key observation is that low-frequency magnitude components exhibit strong robustness against adversarial perturbations, whereas the phase spectrum is more easily affected across all frequencies. For an image xt generated during the reverse process, its DFT can be decomposed into magnitude At and phase Φt, with FreqPure handling them separately. Magnitude purification. low-pass filter is applied to retain the low-frequency part of the adversarial input xadv, while the high-frequency part is taken from the current generated image xt: At1 = H(Aadv) + (1 H)(At). (12) Phase purification. Low-frequency components are preserved through projection operator Πδ() that restricts the generated phase within small neighborhood of the adversarial phase: Φt1 = H(cid:0)Πδ(Φt, Φadv)(cid:1) + (1 H)(Φt), where Πδ(Φt, Φadv) denotes clipping Φt into [Φadv δ, Φadv + δ], and δ is hyperparameter controlling projection strength. Reconstruction. The purified frequency representation (At1, Φt1) is then transformed back into the spatial domain using the inverse discrete Fourier transform (IDFT): (13) xt1 = 1 (cid:0)At1, Φt1(cid:1) , and iteratively participates in the reverse diffusion process until ˆx0 is obtained. The above process is described in the corresponding module on the right side of Figure 2. (14) Overall, FreqPure leverages the stability of low-frequency magnitudes while constraining the phase distribution, preventing structural distortions. In contrast, MANI avoids redundant noise in robust regions and focuses perturbations on vulnerable frequency bands, enabling effective denoising with minimal semantic loss. Together, they are complementary: MANI selectively suppresses adversarial signals in the forward process, while FreqPure ensures frequency stability and semantic consistency in the reverse process. The above methods are summarized in Appendix E.1."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Datasets and Model Architectures. We conduct experiments on two widely used datasets of different resolutions: CIFAR-10 and ImageNet-1K. Following the settings in prior works (Pei et al., 2025a; Zhang et al., 2025b), we randomly select 512 samples from CIFAR-10 and 1,000 samples from ImageNet-1K for evaluation. To better align with the development of large-scale multimodal models, we adopt CLIP as the frozen classifier to accomplish zero-shot classification tasks. For the diffusion models, we use the publicly released unconditional CIFAR-10 checkpoint of EDM (Karras et al., 2022) for CIFAR-10, and 256x256 unconditional diffusion checkpoint for ImageNet-1K."
        },
        {
            "title": "Under Review",
            "content": "Evaluation Metrics. We report both standard accuracy and robust accuracy. This dual evaluation provides comprehensive view of the trade-off between preserving performance on clean data and enhancing resilience against attacks. Attack Settings. In our experiments, we evaluate all defenses under strong adaptive attacks across both ℓ and ℓ2 threat models. Concretely, we employ PGD and AutoAttack as primary evaluation tools, covering both ℓ and ℓ2 perturbations. Following Lee & Kim (2023), we adopt PGD combined with expectation over transformations (PGD+EOT) to mitigate variability caused by stochastic components in the defense. In addition, we test BPDA+EOT to evaluate attacks that approximate gradients through non-differentiable or randomized components. For computational tractability while retaining attack strength, PGD and BPDA are run for 10 iterations, and EOT uses 10 samples per gradient estimate. AutoAttack is executed in its standard version. The perturbation budgets are specified as ϵ = 8/255 for ℓ attacks on CIFAR-10, ϵ = 4/255 for ℓ attacks on ImageNet, and ϵ = 0.5 for ℓ2 attacks on both datasets. Further experimental settings can be found in Appendix D."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "This section presents comprehensive evaluation of MANI-Pure across multiple datasets, attack settings, and metrics, with focus on robustness, perceptual quality, and plug-and-play flexibility. 4.2.1 CLASSIFICATION ACCURACY UNDER ADAPTIVE ATTACKS MANI-Pure consistently achieves the best trade-off between standard and robust accuracy across datasets and backbones. As summarized in Table 1 (CIFAR-10, ViT-L/14), Table 2 (CIFAR-10, RN50), and Table 3 (ImageNet-1K, ViT-L/14), we evaluate against strong adaptive attacks including PGD+EOT, AutoAttack under both ℓ and ℓ2 norms, and BPDA+EOT. On CIFAR-10, MANI-Pure improves robust accuracy by 2.15% under AutoAttack (ℓ) and by 2.54% under BPDA+EOT when using ViT-L/14. Consistent improvements are also observed on RN50, confirming the backbone-agnostic nature of our framework. On ImageNet-1K, especially, MANI-Pure achieves the highest robust accuracy, outperforming all baselines by 3.8% under BPDA+EOT, while maintaining competitive clean accuracy. These results demonstrate that MANI-Pure not only surpasses existing AP and AT baselines (including recent leaders on RobustBench), but also exhibits strong cross-dataset generalization and backbone versatility. More results on different backbones can be found in the Appendix. E.2. Table 1: Classification accuracy on CIFAR-10 under adversarial attacks using CLIP ViT-L/14. Zeroshot CLIP (w/o defense) is denoted by , its standard accuracy as the upper bound. Methods from the Robustbench leaderboard are denoted by . AT and AP methods are marked accordingly. Type Algorithm Standard AT AP DHAT (Zhang et al., 2025a) DIAT (Wang et al., 2023) MeanSparse (Amini et al., 2024) Zero-shot (w/o defense) + DiffPure (Nie et al., 2022) + DDPM++ (Song et al., 2020) + REAP (Lee & Kim, 2023) + FreqPure (Pei et al., 2025b) + CLIPure (Zhang et al., 2025b) + Ours 85.45 92.69 92.98 94.73 86.52 86.33 81.45 91.77 93.55 94.14 6 PGD AutoAttack ℓ 63.14 71.38 74.02 2.15 85.55 84.77 79.69 90.17 89.06 91.02 ℓ2 66.91 85.12 86.41 55.86 85.74 85.16 79.87 91.41 92.19 92.58 ℓ 56.77 70.53 68. 0.00 85.35 85.74 80.08 90.82 90.04 92.19 ℓ2 57.40 84.03 85.98 0.00 85.55 85.74 80.18 91.99 92.38 93.16 BPDA 54.84 69.76 72.87 0.78 84.96 86.13 80.86 87.89 83.01 88."
        },
        {
            "title": "Under Review",
            "content": "Table 2: Classification accuracy on CIFAR-10 under adversarial attacks using CLIP RN50. Zeroshot CLIP (w/o defense) is denoted by , its standard accuracy as the upper bound. Only AP-based methods are included."
        },
        {
            "title": "Standard",
            "content": "Zero-shot (w/o defense) + DiffPure (Nie et al., 2022) + DDPM++ (Song et al., 2020) + REAP (Lee & Kim, 2023) + FreqPure (Pei et al., 2025b) + CLIPure (Zhang et al., 2025b) +Ours 69.92 61.91 56.64 58.59 62.70 61.33 65."
        },
        {
            "title": "AutoAttack",
            "content": "ℓ 0.00 59.77 56.25 56.84 59.38 53.71 61.91 ℓ2 19.73 61.13 56.64 58.40 60.55 60.55 62.50 ℓ 0.39 59.77 56.05 55.66 61.52 56.84 62. ℓ2 0.39 60.64 56.34 58.40 62.56 60.55 64."
        },
        {
            "title": "BPDA",
            "content": "3.32 60.16 55.27 56.25 58.79 53.32 60.16 Table 3: Classification accuracy on ImageNet-1K under adversarial attacks using CLIP ViT-L/14. Zero-shot CLIP (w/o defense) is denoted by , its standard accuracy as the upper bound. Only AP-based methods are included."
        },
        {
            "title": "Standard",
            "content": "Zero-shot (w/o defense) + DiffPure (Nie et al., 2022) + DDPM++ (Song et al., 2020) + REAP (Lee & Kim, 2023) + OSCP (Lei et al., 2025) + Ours 74.90 71.10 70.70 51.30 71.60 73."
        },
        {
            "title": "AutoAttack",
            "content": "ℓ 1.20 43.00 66.00 48.90 65.70 67.30 ℓ2 31.60 43.40 70.00 49.90 69.00 70.80 ℓ 0.10 42.90 68.10 48.40 68.30 68. ℓ2 0.10 44.20 70.40 50.10 70.10 70."
        },
        {
            "title": "BPDA",
            "content": "0.00 42.50 63.50 48.50 66.00 67.30 4.2.2 PERCEPTUAL QUALITY EVALUATION MANI-Pure produces purified images that are perceptually closest to clean images across different backbones. Since diffusion-based purification is inherently generative, we complement robustness evaluation with perceptual quality metrics, conducted on the CIFAR-10 dataset. Table 4 reports results on SSIM (Wang et al., 2004) (higher is better) and LPIPS (Zhang et al., 2018) (lower is better). On RN50, MANI-Pure achieves an SSIM of 0.9274 and an LPIPS of 0.1136, both outperforming all baselines. Similar trends are observed with ViT-L/14. Overall, MANI-Pure consistently achieves the highest perceptual similarity, underscoring its ability to defend against adversarial perturbations while preserving image fidelity. Table 4: To evaluate the quality of the generated images, we compute the SSIM and LPIPS scores between the images purified by different AP methods and the clean images. Backbone Metric Methods Adversarial DiffPure REAP FreqPure Ours ViT-L/14 RN50 SSIM LPIPS SSIM LPIPS 0.8204 0. 0.8180 0.3907 0.8342 0.2110 0.8344 0.2110 0.8044 0.2553 0.8045 0.2551 0.9172 0. 0.9176 0.1217 0.9270 0.1133 0.9274 0.1136 4.2.3 QUALITATIVE VISUALIZATION Visualizations confirm that MANI-Pure selectively suppresses adversarial perturbations while preserving semantics. To better validate the effectiveness of adaptive noise injection, we visual-"
        },
        {
            "title": "Under Review",
            "content": "ize the difference between the injected noise and adversarial noise. Figure 3 clearly shows that adaptive noise aligns much better with adversarial perturbations than uniform noise, especially in high-frequency regions that are most vulnerable to attacks. Quantitatively, KL divergence further confirms this observation: adaptive noise (0.1628) is substantially closer to adversarial noise than uniform noise (0.4988). These findings highlight our core advantageprecise suppression of adversarially vulnerable regions while preserving semantic fidelity elsewhere. We further compare purified samples from DDPM++ and MANI-Pure, together with pixel-wise difference heatmaps relative to clean images. Our method introduces smaller modifications in low-frequency background regions, while applying targeted changes in high-frequency regions most affected by perturbations, which provides direct evidence of MANI-Pures frequency-adaptive design. Figure 3: Difference heatmaps between adaptive noise (left) / uniform noise (right) and adversarial noise. Lighter colors indicate smaller differences. Figure 4: Visualization of purification before and after defense. The figure compares purified results from DDPM++ and MANI-Pure, together with pixel-wise difference heatmaps relative to clean images. Overall: MANI-Pure introduces smaller modifications in low-frequency background regions, avoiding unnecessary semantic loss. Key effect: it selectively alters high-frequency vulnerable regions, providing direct evidence of its frequency-adaptive design. 4.2.4 PLUG-AND-PLAY COMPATIBILITY As modular noise injection strategy, MANI can be seamlessly combined with various existing DBP methods. Table 5 reports results under ℓ attacks (results under ℓ2 are listed in Appendix E.3). We observe that MANI consistently improves both clean and robust accuracy across all tested AP baselines. In particular, REAP benefits the most, with its clean accuracy increased by 4.10% and robust accuracy under AutoAttack improved by 1.95%. More importantly, the combination of MANI with FreqPure yields the overall best performance, highlighting the complementary design philosophy between the two modules.These results validate MANI as general and effective plug-in for enhancing diverse purification pipelines."
        },
        {
            "title": "Under Review",
            "content": "Table 5: Plug-and-play validation of the MANI module under ℓ attacks. We integrated MANI into various diffusion-based purification frameworks and evaluated them on CIFAR-10. Results are reported both without MANI (w/o) and with MANI (w/). Algorithm +DiffPure (Nie et al., 2022) +DDPM++ (Song et al., 2020) +REAP (Lee & Kim, 2023) +FreqPure (Pei et al., 2025b) Standard PGD AutoAttack BPDA w/o 86.52 86.33 81.45 91.77 w/ 87.72 87.30 85.55 94.14 w/o 85.55 84.77 79.69 90.17 w/ 86.82 86.33 81.45 91.02 w/o 85.35 85.74 80.08 90.82 w/ 86.91 86.52 82.03 92.19 w/o 84.96 86.13 80.86 87.89 w/ 85.43 86.33 82.42 88."
        },
        {
            "title": "4.3 ABLATION STUDIES",
            "content": "We conducted ablation experiments on CIFART-10 to better understand the contributions of different design choices in MANI-Pure, primarily involving parameter analysis and module ablation. Effect of hyperparameters. The MANI module mainly involves two hyperparameters: the weighting factor γ and the number of frequency bands n. As shown in Figure 5, both standard and robust accuracy exhibit rise-then-fall trend as γ increases from 1.0. Specifically, standard accuracy peaks at γ = 1.6, while γ = 1.8 achieves more balanced trade-off between clean and robust performance. similar trend is observed for n, where = 8 provides the best overall results. Effect of different modules. To further assess the contribution of each component, we conduct ablation studies on MANI and FreqPure. As shown in Table 6, both modules individually enhance the baseline performance. When combined, they yield substantially larger improvements than using either module alone, achieving gains of 7.62% in clean accuracy and 5.47% in robust accuracy. These results highlight the orthogonal benefits of MANI and FreqPure, and their strong complementarity. Table 6: Standard and robust accuracy for different block combinations. and indicate use or non-use of the module. MANI FreqPure Standard Robust 86.52 87.30 91.77 94.14 85.55 86.33 90.17 91.02 Figure 5: Standard accuracy and robust accuracy under different ratio factor γ (left) and under different number of frequency band (right)."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This work systematically analyzes the distribution of adversarial perturbations in the frequency domain and shows that existing uniform noise injection strategies may disrupt the semantic structure of clean images. To address this issue, we propose MANI-Pure, diffusion-based purification framework that integrates magnitude-adaptive noise injection to emphasize vulnerable frequency bands and frequency purification to protect semantic structures. Through extensive experiments on two benchmark datasets under multiple attacks, MANI-Pure effectively suppresses adversarial noise while preserving semantic content, achieving favorable balance between clean and robust accuracy. Moreover, the plug-and-play design of MANI highlights its compatibility with diverse purification pipelines, further broadening its applicability."
        },
        {
            "title": "REFERENCES",
            "content": "Sajjad Amini, Mohammadreza Teymoorianfard, Shiqing Ma, and Amir Houmansadr. Meansparse: arXiv Post-training robustness enhancement through mean-centered feature sparsification. preprint arXiv:2406.05927, 2024. Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In ICML, 2018. Mingyuan Bai, Wei Huang, Tenghui Li, Andong Wang, Junbin Gao, Cesar Caiafa, and Qibin Zhao. Diffusion models demand contrastive guidance for adversarial purification to advance. In ICML, 2024. Gerda Bortsova, Cristina Gonzalez-Gonzalo, Suzanne Wetstein, Florian Dubost, Ioannis Katramados, Laurens Hogeweg, Bart Liefers, Bram Van Ginneken, Josien PW Pluim, Mitko Veta, et al. Adversarial attack vulnerability of medical image analysis systems: Unexplored factors. Medical Image Analysis, 73:102141, 2021. Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In ICML, 2020. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. Jia Fu, Yongtao Wu, Yihang Chen, Kunyu Peng, Xiao Zhang, Volkan Cevher, Sepideh Pashami, and Anders Holst. Diffcap: Diffusion-based cumulative adversarial purification for vision language models. arXiv preprint arXiv:2506.03933, 2025. Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. Mitch Hill, Jonathan Craig Mitchell, and Song-Chun Zhu. Stochastic security: Adversarial defense using long-run dynamics of energy-based models. In ICLR, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In NeurIPS, 2022. Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www. cs. toronto. edu/kriz/cifar. html, 5(4):1, 2010. Minjong Lee and Dongwoo Kim. Robust evaluation of diffusion-based adversarial purification. In ICCV, 2023. Chun Tong Lei, Hon Ming Yam, Zhongliang Guo, Yifei Qian, and Chun Pong Lau. Instant adversarial purification with adversarial consistency distillation. In CVPR, 2025. Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, and Qibin Zhao. Adversarial training on purification (atop): Advancing both robustness and generalization. In ICLR, 2024. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robustness for large-scale models. In ICLR, 2023. Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models for adversarial purification. In ICML, 2022."
        },
        {
            "title": "Under Review",
            "content": "Gaozheng Pei, Shaojie Lyu, Gong Chen, Ke Ma, Qianqian Xu, Yingfei Sun, and Qingming Huang. Divide and conquer: Heterogeneous noise integration for diffusion-based adversarial purification. In CVPR, 2025a. Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, and Qingming Huang. Diffusion-based adversarial purification from the perspective of the frequency domain. In ICML, 2025b. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. In ICLR, 2018. Christian Schlarmann, Naman Deep Singh, Francesco Croce, and Matthias Hein. Robust clip: unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. In ICML, 2024. Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph In Studer, Larry Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! NeurIPS, 2019. Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Holitom: Holistic token merging for fast video large language models. arXiv preprint arXiv:2505.21334, 2025. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2020. Yuhao Sun, Jiacheng Zhang, Zesheng Ye, Chaowei Xiao, and Feng Liu. Sample-specific noise injection for diffusion-based adversarial purification. In ICML, 2025. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. Keda Tao, Jinjin Gu, Yulun Zhang, Xiucheng Wang, and Nan Cheng. Overcoming false illusions in real-world face restoration with multi-modal guided diffusion model. arXiv preprint arXiv:2410.04161, 2024. Jinyi Wang, Zhaoyang Lyu, Dahua Lin, Bo Dai, and Hongfei Fu. Guided diffusion model for adversarial purification. arXiv preprint arXiv:2205.14969, 2022. Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In ICML, 2023. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Juanjuan Weng, Zhiming Luo, Zhun Zhong, Dazhen Lin, and Shaozi Li. Exploring non-target knowledge for improving ensemble universal adversarial attacks. In AAAI, 2023. Peng Ye, Yuanfang Chen, Sihang Ma, Feng Xue, Noel Crespi, Xiaohan Chen, and Xing Fang. Security in transformer visual trackers: case study on the adversarial robustness of two models. Sensors (Basel, Switzerland), 24(14):4761, 2024. Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. fourier perspective on model robustness in computer vision. In NeurIPS, 2019. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. Kejia Zhang, Juanjuan Weng, Shaozi Li, and Zhiming Luo. Towards adversarial robustness via debiased high-confidence logit alignment. In ICCV, 2025a."
        },
        {
            "title": "Under Review",
            "content": "Mingkun Zhang, Keping Bi, Wei Chen, Jiafeng Guo, and Xueqi Cheng. Clipure: Purification in latent space via clip for adversarially robust zero-shot classification. In ICLR, 2025b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018."
        },
        {
            "title": "APPENDIX OVERVIEW",
            "content": "This appendix provides additional details and analyses to complement the main paper. It is organized as follows: Section A. Use of Large Language Models. We clarify the extent to how LLMs were used during the writing and proofreading process, ensuring transparency in compliance with conference policies. Section B. Background on Adversarial Attacks and Defenses. We review standard adversarial attacks (e.g., PGD, AutoAttack, BPDA) and defense paradigms (adversarial training, purification), offering context for how our method relates to existing approaches. Section C. Theoretical Supplement. We provide more complete derivation of diffusion models, present unified mathematical framework for adversarial purification, and analyze the computational complexity and stability of different approaches. Section D. Experimental Settings. We detail the hyperparameter choices for both attacks and diffusion models, including perturbation budgets, iteration numbers, noise schedules, and pretrained checkpoints, ensuring reproducibility of all results. Section E. Additional Experimental Results. We extend the evaluations beyond the main text. This includes: (i) step-by-step algorithmic workflow of our framework. (ii) classification with alternative backbones (CLIP-RN101, WRN-28-10,RN-50), (iii) plug-and-play integration under ℓ2 attacks, (iv) analysis of PGD iteration numbers, and Section F. Visualization. We provide additional qualitative results, showing purified versus adversarial samples across multiple datasets, highlighting the semantic preservation and noise suppression of our method."
        },
        {
            "title": "A STATEMENT ON THE USE OF LLMS",
            "content": "This study employed LLMs to assist in writing. LLMs were primarily utilized for language refinement, grammatical corrections, and enhancing academic tone. It is crucial to emphasize that all viewpoints, theoretical frameworks, experimental results, and final conclusions were independently developed by human authors. LLMs served solely as auxiliary tools for manuscript refinement, with all final drafts thoroughly reviewed and approved by the authors."
        },
        {
            "title": "B SUPPLEMENT RELATED WORK",
            "content": "Adversarial Attacks & Robustness. Adversarial attacks have long revealed the fragility of neural networks, beginning with the discovery of imperceptible perturbations by Szegedy et al. (2013) and the efficient one-step FGSM attack (Goodfellow et al., 2014). Iterative methods such as PGD (Madry et al., 2017) established strong benchmarks for robustness evaluation, later extended by efficient variants like FreeAT (Shafahi et al., 2019) and AutoAttack (Croce & Hein, 2020). The use of EOT (Expectation over Transformation) (Athalye et al., 2018) was further emphasized to mitigate randomness and non-differentiability in gradients, ensuring accurate robustness assessment. On the defense side, adversarial training (Schlarmann et al., 2024; Mao et al., 2023) remains the most widely used strategy. By incorporating adversarial examples into the training process, AT explicitly improves the decision boundary against perturbations, thereby enhancing robustness. However, AT requires significant computational resources and often generalizes poorly to unseen attacks, motivating research into alternative approaches.AP emerged in response to this situation."
        },
        {
            "title": "C THEORETICAL SUPPLEMENT",
            "content": "C.1 UNIFIED FRAMEWORK FOR ADVERSARIAL PURIFICATION We can unify diffusion-based adversarial purification methods into the following generalized formulation: xt = (x0; αt) + g(ϵ; W), (15)"
        },
        {
            "title": "Under Review",
            "content": "where (x0; αt) = is weighting or transformation operator. αt x0 denotes the signal decay term, g(ϵ; W) represents noise injection, and Adversarial Training: robustness stems from model parameters; no explicit g() is introduced. DiffPure: g(ϵ; W) = MANI-Pure: g(ϵ; W) = 1 αt ϵ, where = I. 1 αt(W ϵ), where is derived from frequency magnitudes. FreqPure: constraints are imposed in the reverse step, by spectral recombination rather than forward-side weighting. This unified framework highlights key dichotomy: forward-side approaches redesign g() to better mimic adversarial distributions, while reverse-side approaches constrain the reconstruction trajectory. MANI-Pure naturally combines both perspectives, explaining its superior performance. C.2 COMPLEXITY AND STABILITY ANALYSIS Time Complexity: DiffPure: O(T HW ) per reverse trajectory, dominated by neural network inference. MANI-Pure: adds DFT/IDFT operations of O(HW log(HW )) per step, negligible compared to network cost. FreqPure: incurs extra spectral recombination and projection, but all operations are element-wise or FFT-based, remaining parallelizable on GPUs. Hybrid methods (e.g., MANI+FreqPure): maintain linear scaling in and near-constant overhead relative to the diffusion backbone. Space Complexity: All methods store O(HW ) activations per step. Frequency-based approaches require one additional complex-valued copy of the spectrum, i.e., O(2HW ), which is marginal compared with feature maps inside the denoiser. Numerical Stability: FFT and inverse FFT are unitary transforms, introducing no instability. MANIs band-wise weighting may amplify small magnitudes, but normalization with ϵ ensures bounded variance. FreqPures projection operator Π() restricts phase drift, effectively stabilizing the reverse trajectory under strong attacks. Scalability. Since the extra overhead scales sub-linearly with resolution (log(HW )), frequencydomain operations remain efficient even for high-resolution ImageNet-1K images. Therefore, the proposed MANI-Pure achieves robustness gains without sacrificing efficiency."
        },
        {
            "title": "D PARAMETERS AND SETTINGS",
            "content": "D.1 ATTACK SETUP We adopt three types of strong adaptive attacks: PGD+EOT, AutoAttack, and BPDA+EOT. For PGD and BPDA, the number of iterations is set to 10 (the rationale for this choice is discussed in Appendix E.4), while the number of EOT samples is also set to 10. AutoAttack is executed in its standard version, which integrates APGD-CE, APGD-DLR, FAB, and Square Attack, with 100 update iterations. The perturbation budget is ϵ = 8/255 for ℓ attacks on CIFAR-10 and ϵ = 4/255 on ImageNet-1K, while ℓ2 attacks use ϵ = 0.5 for both datasets. Unless otherwise specified, the step size is set to 0.007 for all attacks."
        },
        {
            "title": "Under Review",
            "content": "D.2 DIFFUSION SETUP Our purification framework is based on DDPM++ (Song et al., 2020) with linear variance schedule, where the noise variance increases from β1 = 104 to βT = 0.02 over = 1000 steps (Ho et al., 2020). In all experiments, we set the forward noising steps to 100 and the reverse denoising steps to 5, unless otherwise specified. For DiffPure, we follow the original implementation and use 100 reverse steps. The pretrained diffusion weights are taken from public releases: the unconditional CIFAR-10 checkpoint of EDM (Karras et al., 2022) and the 256 256 unconditional diffusion checkpoint for ImageNet-1K, consistent with prior works. D.3 NOISE DIFFERENCE HEATMAP COMPUTATION To analyze the similarity between injected noise Ninj and adversarial noise Nadv, we compute their pixel-wise difference: = Ninj Nadv. Here contains both positive and negative values, where the sign indicates whether the injected noise is larger or smaller than the adversarial noise at each pixel. For visualization, we normalize and render it with diverging colormap, where red/blue colors represent positive/negative differences, respectively. (16)"
        },
        {
            "title": "E ADDITIONAL RESULTS",
            "content": "E.1 THE ALGORITHM WORKFLOW OF MANI-PURE This section presents the MANI-Pure algorithm flowchart (Algorithm 1), which comprehensively illustrates the entire processing workflow. This contrasts with the section-by-section module introductions in Sec. 3.2 and the abstract representation in Figure 2. // Forward Progress:MANI (cid:80) (u,v)Bi Aadv(u, v) Algorithm 1 Adversarial Purification with MANI and FreqPure Require: Adversarial input xadv, Diffusion steps , Band number n, Weighting factor γ Ensure: Purified image x0 1: (Aadv, Φadv) = F(xadv) 2: Partition Madv into frequency bands {Bi} 3: for each band Bi do 4: Mi = 1 Bi wi = (Mi + ϵ0)γ 5: 6: end for 7: Construct spatial weight map via IDFT 8: ϵt = ϵG, with ϵG (0, I) 9: xt = 10: Initialize xT (0, I) 11: for = 1 do (cid:0)xt x0t = 1 12: αt (At, Φt) = F(x0t) At1 = H(Aadv) + (1 H)(At) Φt1 = H(cid:0)Π(Φt, Φadv, δ)(cid:1) + (1 H)(Φt) xt1 = 1(At1, Φt1) 1 αt ϵθ(xt, t)(cid:1) αt xadv + 1 αt ϵt 13: 14: 15: 16: 17: end for 18: return // Reverse Progress:FreqPure E.2 ROBUSTNESS UNDER DIFFERENT BACKBONES In this section, we further supplement classification experiments with CLIP (RN101), WRN-2810 (Zagoruyko & Komodakis, 2016) and ResNet-50 (He et al., 2016), following the same settings as Sec. 4.1 in the main text. As shown in Table 1, Table 2, Table 7, Table 8 and Table 9, MANI-Pure"
        },
        {
            "title": "Under Review",
            "content": "Figure 6: Robust accuracy of several purification methods across different PGD iteration counts (All attacks with EOT=10). consistently achieves the best performance across different classifier architectures, demonstrating its versatility and robustness. Table 7: Classification accuracy on CIFAR-10 under adversarial attacks using CLIP RN101. Zeroshot CLIP (w/o defense) is denoted by ; its standard accuracy as the upper bound. Only AP-based methods are included."
        },
        {
            "title": "Standard",
            "content": "Zero-shot (w/o defense) + DiffPure (Nie et al., 2022) + DDPM++ (Song et al., 2020) + REAP (Lee & Kim, 2023) + FreqPure (Pei et al., 2025b) + CLIPure (Zhang et al., 2025b) +Ours 78.32 67.58 68.95 62.30 70.70 68.95 71."
        },
        {
            "title": "AutoAttack",
            "content": "ℓ 0.00 65.98 65.62 61.33 68.55 62.89 68.75 ℓ2 26.56 66.60 66.99 61.72 68.95 68.75 70.12 ℓ 0.20 65.62 64.45 61.91 67.97 64.26 69. ℓ2 0.20 66.60 66.80 61.13 68.75 68.84 70."
        },
        {
            "title": "BPDA",
            "content": "2.73 66.01 65.62 61.91 66.80 59.18 69.53 E.3 PLUG-AND-PLAY RESULTS UNDER ℓ2 ATTACKS In addition to the ℓ setting reported in the main text, we also evaluate the plug-and-play integration of MANI with existing AP methods under ℓ2 attacks. Following the same configurations as Sec. 4.1, we consider PGD+EOT and AutoAttack with perturbation budget ϵ = 0.5. The results, summarized in Table 10, show that MANI consistently improves both clean and robust accuracy when combined with different AP backbones. E.4 EFFECT OF ATTACK ITERATIONS We also examine the impact of the number of PGD iterations on robust accuracy. In our main experiments, we set PGD iterations to 10. Since prior works adopt different iteration counts, we perform an ablation to validate this choice. As illustrated in Figure 6, the robust accuracy of undefended models decreases sharply with more iterations and converges near zero, while defense methods remain relatively stable with only minor fluctuations. Therefore, we adopt 10 iterations as practical balance between robustness evaluation and computational efficiency. Additionally, for EOT iterations, we follow the setting in Nie et al. (2022), which shows that robustness converges once EOT exceeds 10."
        },
        {
            "title": "Under Review",
            "content": "Table 8: Classification accuracy on CIFAR-10 under adversarial attacks using WRN-28-10. WRN28-10(w/o defense) is denoted by ; its standard accuracy as the upper bound. Results marked with are reported in Bai et al. (2024). Only AP-based methods are included. Algorithm WRN-28-10 (w/o defense) +Diffpure(Nie et al., 2022) +REAP(Lee & Kim, 2023) +CGDM(Bai et al., 2024) +FreqPure(Pei et al., 2025b) +Ours Standard PGD AutoAttack 96.48 90.07 90.16 91.41 92.19 92. 0.00 56.84 55.82 49.22 59.39 61.32 0.00 63.30 70.47 77.08 77.35 78.69 Table 9: Classification accuracy on CIFAR-10 under adversarial attacks using ResNet-50. ResNet50(w/o defense) is denoted by ; its standard accuracy as the upper bound. Results marked with are reported in Bai et al. (2024). Only AP-based methods are included. Algorithm ResNet-50 (w/o defense) +Diffpure(Nie et al., 2022) +REAP(Lee & Kim, 2023) +CGDM(Bai et al., 2024) +FreqPure(Pei et al., 2025b) +Ours Standard PGD AutoAttack 76.01 67.84 68.72 68.98 69.53 70.31 0.00 42.58 43.19 41.80 59.77 60.03 0.00 41.53 44.67 - 63.49 61.79 Table 10: Plug-and-play validation of the MANI module under ℓ2 attacks. We integrated MANI into various diffusion-based purification frameworks and evaluated them on CIFAR-10. Results are reported both without MANI (w/o) and with MANI (w/). Algorithm + DiffPure (Nie et al., 2022) + DDPM++ (Song et al., 2020) + REAP (Lee & Kim, 2023) + FreqPure (Pei et al., 2025b) PGD AutoAttack w/o 85.74 85.16 79.87 91. w/ 87.08 86.72 81.64 92.58 w/o 85.55 85.74 80.18 92.00 w/ 87.50 87.11 81.84 93."
        },
        {
            "title": "F VISUALIZATION",
            "content": "To intuitively illustrate the purification effect, we present qualitative results on randomly selected samples from CIFAR-10 (Figure 7, Figure 8, Figure 9) and ImageNet-1K (Figure 10, Figure 11, Figure 12), including clean images, adversarial images, and purified images. Figure 7: Clean CIFAR-10 images randomly selected for visualization Figure 8: Adversarial CIFAR-10 images randomly selected for visualization"
        },
        {
            "title": "Under Review",
            "content": "Figure 9: Purified CIFAR-10 images randomly selected for visualization Figure 10: Clean ImageNet-1K images randomly selected for visualization"
        },
        {
            "title": "Under Review",
            "content": "Figure 11: Adversarial ImageNet-1K images randomly selected for visualization Figure 12: Purified ImageNet-1K images randomly selected for visualization"
        }
    ],
    "affiliations": [
        "Emory University",
        "Xiamen University"
    ]
}