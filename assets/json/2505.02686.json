{
    "paper_title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models",
    "authors": [
        "Xiaobao Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers."
        },
        {
            "title": "Start",
            "content": "Sailing AI by the Stars: Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models Xiaobao Wu Nanyang Technological University xiaobao.wu@ntu.edu.sg 5 2 0 2 5 ] . [ 1 6 8 6 2 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and postinference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain paper collection at GitHub repository."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed the rapid advancement of Large Language Models (LLMs), such as ChatGPT (OpenAI, 2023), Claude (Anthropic, 2025), and Llama (Meta, 2023, 2024). These models are initially empowered by pre-training scaling (Kaplan et al., 2020), which trains LLMs on massive corpora through next-token prediction. While this approach enables broad linguistic and knowledge representations, it suffers from several fundamental limitations: misalignment with human values (Bai et al., 2022b; Zhang et al., 2023b; Deshpande et al., 2023), difficulty in adapting to various task objectives (Lyu et al., 2023; Wang et al., 2023a), and deficiencies in deep reasoning (Mirzadeh et al., 2024; Wu et al., 2024h). As result, these limitations confine pre-trained models to surface-level 1 Figure 1: Illustration of the scaling phases of LLMs. The learning-from-rewards paradigm plays pivotal role in the post-training and test-time scaling. tasks, falling short of the long-term goal of robust and general AI. To address these limitations, recent efforts have turned to post-training and test-time scaling, which seek to further refine LLMs after pre-training. Across the post-training and test-time scaling, critical unified paradigm has emerged as illustrated in Figure 1: Learning from Rewards, which leverages reward signals to guide model behavior through diverse learning strategies. For posttraining scaling, this paradigm has underpinned several key techniques, including preference alignment through Reinforcement Learning from Human Feedback (RLHF, Ouyang et al., 2022) or AI Feedback (RLAIF, Bai et al., 2022b) with scalar rewards and PPO (Schulman et al., 2017), and Direct Preference Optimization (DPO, Rafailov et al., 2023) with implicit rewards. For test-time scaling, this paradigm supports eliciting long Chainof-Thoughts reasoning via GRPO (Shao et al., 2024) with rule-based rewards in DeepSeek-R1 (DeepSeek-AI et al., 2025), generate-then-rank (Best-of-N) (Cobbe et al., 2021; Lightman et al., Figure 2: unified framework of learning from rewards. The language model generates outputs conditioned on the inputs; the reward model evaluates the outputs and provides reward signals based on diverse feedback sources and design choices; the learning strategy leverages the rewards to either fine-tune the language model or refine the outputs. This learning-from-rewards paradigm aims to fulfill preference alignment and task-specific goals. The learning strategy can occur at the training, inference, or post-inference stages. 2023), reward-guided decoding (Deng and Raffel, 2023; Khanov et al., 2024), and post-hoc correction (Akyurek et al., 2023; Madaan et al., 2023). Through these techniques, this paradigm enables LLMs to learn actively from dynamic feedback, in contrast to learning passively from static data. As such, this endows LLMs with aligned preferences and deep reasoning and planning abilities, leading to more intelligent agents. In consequence, this paradigm has inspired many applications, such as mathematical reasoning (Wang et al., 2023b; DeepSeek-AI, 2025), code generation (Zhu et al., 2024; Zhou et al., 2025a), multimodality (Liu et al., 2025h), agents (Xia et al., 2025; OpenAI, 2025), and embodied AI (Zhang et al., 2025d; Zhao et al., 2025a). Due to this growing prevalence, we in this paper comprehensively review the learning from rewards for LLMs. We first introduce taxonomy that categorizes existing works with unified conceptual framework regarding reward model design and learning strategies. Then we review representative techniques across three main stages: training with rewards, inference with rewards, and post-inference with rewards. We additionally summarize recent reward model benchmarks and finally conclude by outlining key challenges and promising directions for future research."
        },
        {
            "title": "2 A Taxonomy of Learning from Rewards",
            "content": "for LLMs We first introduce unified conceptual framework that captures the key components and interactions to understand learning from rewards systemically. Building upon this framework, we then categorize the primary dimensions along which existing methods vary: (i) What is the source of rewards; (ii) How to design the reward model; (iii) When to learn from rewards; (iv) How to learn from rewards."
        },
        {
            "title": "2.1 A Unified Conceptual Framework",
            "content": "We present unified conceptual framework for It abstracts learning from rewards in Figure 2. the key components and interactions involved in learning from rewards for language models. In this framework, the language model generates outputs conditioned on the inputs; the reward model then provides rewards to evaluate the output quality; the learning strategy leverages the reward signals to update the language model or adjusts the outputs. Language Model. language model : generates an output ˆy given an input . This formulation covers wide range of tasks, such 2 Figure 3: Reward Model (RM) design dimensions: (a) Model Architecture (Model-based and Model-free); (b) Reward Format (Scalar, Critique, and Implicit); (c) Scoring Pattern (Pointwise and Pairwise); (d) Reward Granularity (Outcome and Process). as question answering, summarization, and image captioning. Reward Model. reward model evaluates the quality of an output ˆy given an input and produces reward signal that reflects desired properties, such as helpfulness, safety, or task-specific correctness. In different contexts, reward model may be referred to as verifier and an evaluator. We emphasize that here we adopt broad definition of the reward model: it can be model-based or model-free. We will discuss these later. Learning Strategy. learning strategy uses reward signals to adjust the behavior of the language model. Here we consider both the trainingbased (updating model parameters) and trainingfree strategies (directly refining model outputs)."
        },
        {
            "title": "2.2 What is the Source of Rewards?",
            "content": "Reward signals originate from two primary sources: human feedback and automated feedback. Each offers trade-offs in terms of reliability, scalability, and cost. We introduce them respectively as follows. (Ouyang et al., 2022) or directly fine-tune the language model like DPO (Rafailov et al., 2023). While effective, this approach is resource-intensive and may not scale easily across domains or tasks. Automated Feedback. To reduce the cost of human annotations and scale up the reward model training, automated feedback has been increasingly explored as an alternative. The automated feedback mainly includes (i) Self-rewarding, where the language model critiques its own outputs (Yuan et al., 2024b; Wang et al., 2024e); (ii) Trained Models, such as powerful LLMs following the LLM-as-aJudge design (Bai et al., 2022b; Lee et al., 2023a); (iii) Predefined Rules, such as accuracy and format rules used in DeepSeek-R1 (Shao et al., 2024; DeepSeek-AI et al., 2025). (iv) Knowledge, such as structured knowledge base or Wikipedia (Peng et al., 2023; Tian et al., 2023). (v) Tools, such as program compilers and interactive systems (Le et al., 2022; Liu et al., 2023). The automated feedback enables scalable reward generation but may introduce limitations in interpretability, generality, and alignment quality. Human Feedback. Human feedback provides high-quality reward signals grounded in human judgment and intent. It typically collects human annotations through pairwise comparisons between alternative model outputs, e.g., chosen and rejected responses. The collected preference data can be used to train explicit reward models like RLHF"
        },
        {
            "title": "2.3 How to Design the Reward Model?",
            "content": "Designing the reward model is the central foundation of learning from rewards. As shown in Figure 3, we organize the design space into four key dimensions as follows. Base Architecture. As shown in Figure 3(a), 3 this refers to the base architecture of reward model: (i) Model-based. dedicated reward model is trained to evaluate outputs. Variants include scalar reward models (outputting numerical scores), generative reward models (producing critiques), and semi-scalar reward models (combining both). Modern rewards models generally adopt the Transformers framework (Liu et al., 2024a). (ii) Model-free. Reward signals are directly modeled from the source without an explicit trained model, such as DPO (Rafailov et al., 2023) and rule-based rewards of GRPO (DeepSeek-AI et al., 2025). In order to align with previous literature, we hereafter refer to the reward model as the model-based by default. Reward Format. As shown in Figure 3(b), this describes the specific format of reward signals: (i) Scalar rewards, numerical scores that quantify the quality of model outputs. They are the most commonly used format due to their simplicity and compatibility with learning strategies such as reinforcement learning. Their limitation lies in the sparsity and interpretability. (ii) Critique rewards, natural language feedback that evaluates the quality of outputs (Saunders et al., 2022; Kwon et al., 2023), such as The score of this response is 3 out of 5. They are more expressive and interpretable than scalar rewards, enabling finer-grained guidance, but they may require additional processing to be used in some learning algorithms. (iii) Implicit rewards are signals implicitly embedded in the source without explicit supervision, such as preference data in DPO (Rafailov et al., 2023). This format simplifies the implementation but places more burden on the learning strategies to infer appropriate optimization signals. Scoring Pattern. As shown in Figure 3(c), this dimension determines how responses are scored: (i) Pointwise, assigning score to each response independently; (ii) Pairwise, comparing response pairs and selecting the preferred one. The pairwise scoring can be expressed as scalar score indicating relative preference or natural language critique such as Response 1 is better than Response 2. Reward Granularity. As shown in Figure 3(d), we identify two kinds of reward granularity: (i) Outcome-level, evaluating the holistic quality of outputs. (ii) Process-level, assigning rewards to the intermediate steps within the reasoning process of outputs."
        },
        {
            "title": "2.4 When to Learn from Rewards?",
            "content": "Learning from rewards can occur at different stages of the language model lifecycle: Training with Rewards. At the training stage, reward signals can be transformed into optimization signals by training algorithms to fine-tune the language model, which is the most extensively explored in the literature. It can support post-training alignment with human preference (Ouyang et al., 2022; Bai et al., 2022b) and testtime scaling by eliciting the language models deep reasoning capabilities through long Chainof-Thoughts (CoT) (DeepSeek-AI et al., 2025). Inference with Rewards. During inference, reward signals can guide the decoding of model outputs without modifying model parameters. This enables test-time scaling by searching in larger decoding space, such as Best-of-N and tree search (Cobbe et al., 2021; Snell et al., 2025). Post-Inference with Rewards. This stage uses rewards to refine model outputs after generation without modifying model parameters. Postinference with rewards also supports test-time scaling by iteratively refining the outputs (Shinn et al., 2023)."
        },
        {
            "title": "2.5 How to Learn from Rewards?",
            "content": "Various learning approaches have been developed to incorporate reward signals to steer model behavior. These approaches are commonly divided into two types: training-based and training-free. Training-based Strategies. Training-based strategies adopt reward signals to optimize the language model parameters. The optimization mainly depends on Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT). Representative examples include Proximal Policy Optimization (PPO, Schulman et al., 2017; Ouyang et al., 2022), Direct Preference Optimization (DPO, Rafailov et al., 2023; Meng et al., 2024), Group Relative Policy Optimization (GRPO, Shao et al., 2024), and Rejection-Sampling FineTuning (RSFT, Nakano et al., 2021; Yuan et al., 2023a; Dong et al., 2023) Training-free Strategies. Training-free strategies leverage reward signals to guide or refine model outputs without updating the language 4 Figure 4: Illustration of Training with Rewards. Based on the reward model design, we mainly focus on scalar rewards, critique rewards, implicit rewards, rule-based rewards, and process rewards. These reward signals are used to fine-tune the language model through reinforcement learning algorithms or supervised fine-tuning. model parameters. They include generate-thenrank, such as Best-of-N (Cobbe et al., 2021; Lightman et al., 2023), reward-guided decoding (Deng and Raffel, 2023; Khanov et al., 2024), and post-inference correction (Shinn et al., 2023; Pan et al., 2023a). These methods offer relatively lightweight mechanism to improve model outputs; some are highly compatible with different model architectures. They are particularly useful when model fine-tuning is infeasible or computationally expensive. The above methods provide distinct mechanisms to learn the language model from reward signals. We will review them in detail in the following 3 to 5."
        },
        {
            "title": "3 Training with Rewards",
            "content": "In this section, we introduce the methods that incorporate rewards into the training phase of large language models (LLMs). These methods contribute to post-training scaling for preference alignment and test-time scaling by eliciting long Chainof-Thoughts (CoT) abilities. We begin with brief review of the primary training algorithms and then categorize existing methods by reward design: (i) Training with Scalar Rewards; (ii) Training with Critique Rewards; (iii) Training with Implicit Rewards; (iv) Training with Rule-based Rewards; (v) Training with Process Rewards; The first three form the core categories, and the latter two, though conceptually overlapped with the former, are separately presented due to their emerging importance. The primary training algorithms depend on Reinforcement Learning (RL) or Supervised FineTuning (SFT): REINFORCE (Sutton et al., 1999). REINFORCE is fundamental policy gradient algorithm that optimizes expected rewards by estimating gradients through sampled actions and their observed rewards. PPO (Schulman et al., 2017). PPO is widely used reinforcement learning algorithm. It constrains policy updates through clipped objectives to balance learning speed and stability. DPO (Rafailov et al., 2023). DPO is direct preference optimization method that learns from preference data without explicitly modeling rewards. GRPO (Shao et al., 2024). GRPO directly uses the average reward of multiple sampled rollouts as the baseline, which avoids the reward and value model of PPO. REINFORCE++ (Hu, 2025). REINFORCE++ is variant of the REINFORCE algorithm that incorporates key techniques from PPO without critic network. RSFT (Rejection-Sampling Fine-Tuning, Nakano et al., 2021; Yuan et al., 2023a). RSFT samples high-reward data offline to construct training datasets for fine-tuning."
        },
        {
            "title": "3.1 Training with Scalar Rewards",
            "content": "Training the language model with scalar rewards is the most extensively studied strategy in the literature. Most methods derive scalar rewards by training dedicated reward model, while some extract rewards directly from the source without training model (i.e., model-free). We introduce the methods based on the reward source human and automated feedback as follows. 5 Training with Scalar Rewards 3.1 Scalar Rewards from Human Feedback RLHF (Ouyang et al., 2022); Safe RLHF (Dai et al., 2023); Fine-Grained RLHF (Wu et al., 2023e); Fact-RLHF (Sun et al., 2023); Skywork-Reward (Liu et al., 2024a); ImageReward (Xu et al., 2023); RAHF (Liang et al., 2024); LiFT (Wang et al., 2024f) Scalar Rewards from Automated Feedback RLAIF (Bai et al., 2022b); Self-Taught (Wang et al., 2024e); Dutta et al. (2024); VLM-RLAIF (Ahn et al., 2024); RLTF (Liu et al., 2023); RLEF (Gehring et al., 2024); StepCoder (Dou et al., 2024); RLEF (Gehring et al., 2024) Training with Critique Rewards 3.2 Auto-J (Li et al., 2023a); CompassJudger-1 (Cao et al., 2024); Con-J (Ye et al., 2024b); GemRM (Mahan et al., 2024); LLaVA-Critic (Xiong et al., 2024); DeepSeek-GRM (Liu et al., 2025g); Critic-RM (Yu et al., 2024b); MM-RLHF (Zhang et al., 2025f) a h g i Training with Implicit Rewards 3. Implicit Rewards from Human Feedback DPO (Rafailov et al., 2023); SimPO (Meng et al., 2024); RLHF-V (Yu et al., 2023b); UnifiedRM (Wang et al., 2025d); RAFT (Dong et al., 2023); ReST (Gulcehre et al., 2023); RSO (Liu et al., 2024b); RRHF (Yuan et al., 2023b) Implicit Rewards from Automated Feedback Self-Rewarding (Yuan et al., 2024b); Meta-Rewarding (Wu et al., 2024a); SCPO (Prasad et al., 2024); Zhang et al. (2025c); PFPO (Jiao et al., 2024a); HA-DPO (Zhao et al., 2023); Tian et al. (2023); FLAME (Lin et al., 2024a); TRICE (Qiao et al., 2023); CodeLutra (Tao et al., 2024); Training with Rule-based Rewards 3.4 DeepSeek-Math (Shao et al., 2024); DeepSeek-R1 (DeepSeek-AI et al., 2025); DAPO (Yu et al., 2025b); Open-R1 (Face, 2025); Logic-RL (Xie et al., 2025a); Visual-RFT (Liu et al., 2025h); CLS-RL (Li et al., 2025c); R1-VL (Zhang et al., 2025a); RefAlign (Zhao et al., 2025c) Training with Process Rewards 3.5 Process Rewards from Human Feedback Uesato et al. (2022); Lightman et al. (2023) Process Rewards from Automated Feedback WizardMath (Luo et al., 2023); ActPRM (Duan et al., 2025); Math-Shepherd (Wang et al., 2023b); PQM (Li and Li, 2024); OmegaPRM (Luo et al., 2024); HRM (Wang et al., 2025b); PRIME (Cui et al., 2025); OREAL (Lyu et al., 2025); GenPRM (Zhao et al., 2025b); R-PRM (She et al., 2025); ThinkPRM (Khalifa et al., 2025); M-STAR (Liu et al., 2024c) Figure 5: Overview of Training with Rewards. Scalar Rewards from Human Feedback. Human feedback is key source for constructing scalar rewards. The most prominent example is Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022). RLHF trains scalar reward model on human preference data (pairwise comparisons with chosen and rejected responses). The reward models commonly adopt the Transformer architecture with value head that outputs scalars, and their training objectives follow the Bradley-Terry loss (Bradley and Terry, 1952), which maximizes the reward differences between preferred and dispreferred outputs. The trained reward model assigns evaluative scalar scores to the model outputs, serving as proxy for human judgment. For instance, Skywork-Reward (Liu et al., 2024a) is reward model trained on various highquality human-labeled preference datasets. With the reward model, RLHF fine-tunes the language model through PPO to align it with human preferences, such as harmlessness and helpfulness. Various variants have been explored based on RLHF. Safe RLHF (Dai et al., 2023) emphasizes safety-centric alignment. Fine-Grained RLHF (Wu et al., 2023e) moves beyond holistic feedback to dense, segment-level fine-grained feedback regarding relevance, factual accuracy, and completeness. Training with scalar rewards has also been extended to multimodal tasks. Fact-RLHF (Sun et al., 2023) incorporates factuality-aware scalar rewards to reduce hallucinations of multimodal understanding. Others focus on multimodal generation tasks. HPS and its variant (Wu et al., 2023d,c) train binary classifier as the reward model to evaluate generated image quality. Lee et al. (2023b) trains multimodal reward model based on human feedback and fine-tunes diffusion model to improve 6 image generation. ImageReward (Xu et al., 2023) introduces general-purpose reward model for textto-image generation. It is trained on image preferences from human experts and then fine-tunes diffusion model via direct tuning algorithm. RAHF (Liang et al., 2024) further enriches the reward signals with scalar scores, heatmaps, and keyword omissions to guide sample filtering for fine-tuning. LiFT (Wang et al., 2024f) applies similar idea to the text-to-video task. Scalar Rewards from Automated Feedback. While human feedback offers high-quality supervision, it is expensive and difficult to scale. To overcome this challenge, growing body of work explores automated feedback as substitute to provide scalar rewards. prominent example is Reinforcement Learning from AI Feedback (RLAIF) (Bai et al., 2022b). RLAIF uses an LLM as proxy judge to generate preference data following the idea of LLM-as-a-Judge (Zheng et al., 2023; Yu et al., 2025a). Like RLHF, RLAIF trains scalar reward model on them and then uses it to fine-tune the language model. This pipeline substantially reduces reliance on expensive human annotations. Lee et al. (2023a) have shown that RLAIF can achieve comparable performance with RLHF. Subsequent studies extend this strategy by incorporating diverse forms of automated feedback. SelfTaught (Wang et al., 2024e) prompts an LLM to synthesize contrastive synthetic pairs and uses them to fine-tune reward model. Dutta et al. (2024) collect GPT-3.5s binary assessments about predefined code quality questions to train reward model for code generation. VLM-RLAIF (Ahn et al., 2024) trains reward model with self-generated preference data for video-language tasks. It then uses the reward model to fine-tune video-language model through PPO. Automated feedback can also come from various tools. RLTF (Liu et al., 2023) introduces an online reinforcement learning framework for code language models. It derives multigranularity scalar rewards from real-time unit test execution results, and these rewards are integrated into unified loss optimized via the REINFORCE algorithm (Sutton et al., 1999) to fine-tune the model. Similarly, StepCoder (Dou et al., 2024) designs scalar rewards based on compiler feedback, and it directly fine-tunes LLMs through tokenlevel PPO with these rewards, with optimization restricted to executed code tokens. RLEF (Gehring et al., 2024) formulates code generation as multiturn decision-making process in an interactive environment. It fine-tunes the language model with the program execution results as scalar rewards through turn-level PPO and supports iterative refinement."
        },
        {
            "title": "3.2 Training with Critique Rewards",
            "content": "Besides scalar rewards, another line of work explores training with critique rewards. This category commonly uses generative reward models to produce natural language critiques on the given response rather than predicting scalar scores. The generated critiques enjoy flexible scoring patterns: they could be pointwise scoring like The score of this response is 3 out of 5\" or pairwise scoring like Response 1 is better than Response 2\". More importantly, the critiques can include explanations for the scoring and refinement suggestions for later improvements, which facilitates reward density and interpretability. Due to the above advantages, many studies work on generative reward models to produce critique rewards. Auto-J (Li et al., 2023a) generates critiques that support pointwise and pairwise evaluation. It adopts GPT-4 to produce evaluation judgments as the training data. CompassJudger-1 (Cao et al., 2024) and Con-J (Ye et al., 2024b) follow similar design. SFR-Judges (Wang et al., 2024d) fine-tunes an LLM on the response deduction task to improve its judging ability. Because of the flexibility of generative models, recent studies concentrate on generalist reward models that support multiple tasks and scoring patterns. For instance, LLaVA-Critic (Xiong et al., 2024) is trained reward model for multiple visionlanguage tasks, supporting both pointwise and pairwise scoring with natural language explanations. DeepSeek-GRM (Liu et al., 2025g) is generalist generative reward model that covers diverse scoring formats and NLP tasks and produces principles for its scoring. We discuss this promising future direction in 8. In particular, generative reward models can work beyond critique generation. For example, GenRM (Zhang et al., 2024c) considers reward modeling as token prediction task. Given prompt like Is the answer correct?, it uses the generation probability of answer indicator tokens, like Yes or No, as the scalar rewards. Mahan et al. (2024) follow similar approach. Furthermore, several methods adopt hybrid structuresemi-scalar reward models that produce both scalar and critique rewards. Early attempts incorporate critiques into the training of scalar reward model, such as CLoud (Ankner et al., 2024), MATH-Minos (Gao et al., 2024a), and Ye et al. (2024a). Critic-RM (Yu et al., 2024b) is reward model with two heads, one for generating critiques and another for predicting scalars. It is trained on self-generated critiques of model outputs through multi-task objective. MM-RLHF (Zhang et al., 2025f) extends it to the multimodal field. It trains semi-scalar reward model on the image, video understanding, and safety and then fine-tunes multimodal language model with the trained reward model."
        },
        {
            "title": "3.3 Training with Implicit Rewards",
            "content": "Rather than explicit scalars or critiques that evaluate the model outputs, another line of work adopts implicit rewards for training. The reward signals are not provided directly but are implicitly embedded in the structure of the training data, such as curated preference data. We note that some methods may use scalar reward model to construct the training data, but the reward model does not participate in fine-tuning language models. Instead, the reward signals for fine-tuning are implicitly encoded in the constructed training data, so we categorize them as training with implicit rewards. Implicit Rewards from Human Feedback. pioneering approach using implicit rewards from human feedback is Direct Preference Optimization (DPO) (Rafailov et al., 2023). DPO encodes implicit rewards via the log-likelihood difference between preferred and dispreferred responses and steers the generalization of the language model toward preferred ones. DPO proves that its objective is theoretically equivalent to optimizing the Bradley-Terry loss. As such, DPO effectively reduces complicated RLHF (Ouyang et al., 2022) into supervised fine-tuning, dramatically simplifying the alignment pipeline. Several variants have been proposed based on DPO to further simplify the training or expand its applicability. For instance, SimPO (Meng et al., 2024) eliminates the need for reference model by directly optimizing preference pairs using more straightforward and reference-free objective, further simplifying DPO. KTO (Ethayarajh et al., 2024) models the reward signals implicitly by comparing the log-likelihood difference between the model output and reference baseline. This difference is then transformed through nonlinear value function to reflect human-like preferences. Some studies have extended training with implicit rewards in reasoning and multimodal areas. In mathematical reasoning, Step-DPO (Lai et al., 2024) and Full-Step-DPO (Xu et al., 2025a) build step-level preference data for DPO training to improve their reasoning abilities. In multimodal areas, mDPO (Wang et al., 2024a) proposes multimodal extension of DPO. It introduces conditional and anchored preference objectives to better leverage visual inputs and preserve the likelihood of preferred responses in multimodal tasks. RLHF-V (Yu et al., 2023b) fine-tunes multimodal LLM through DPO with human preferences regarding trustworthiness. UnifiedRM (Wang et al., 2025d) trains unified multimodal reward model on human preferences for several key tasks, including image/video understanding and generation. Later, it builds preference data through the trained reward model for DPO training. Beyond language models, these techniques are applied to diffusion models due to their simplicity, such as Diffusion-DPO (Wallace et al., 2024). Apart from DPO, another line of work follows Rejection-Sampling Fine-Tuning (RSFT) scheme. They typically select high-quality samples from large number of candidate data for supervised fine-tuning on the language model. While reward models are usually used to evaluate the candidates, they are not involved in the actual fine-tuning; thus RSFT methods are within the category of training with implicit rewards. RAFT (Dong et al., 2023) trains reward model on the HH-RLHF dataset (Bai et al., 2022a) and uses it to select high-reward samples. The model is then directly fine-tuned on these samples. ReST (Gulcehre et al., 2023) adopts an iterative framework: it samples responses from the current model, filters high-quality ones with reward model, and fine-tunes the model on the filtered set. RSO (Liu et al., 2024b) trains reward model on human preferences and then uses it to generate new preference data via statistical rejection sampling from an estimated optimal policy. The language model is fine-tuned on the new preference data. Especially, some approaches directly depend on the ranking loss. RRHF (Yuan et al., 2023b) uses reward model to rank multiple candidate responses. Then it fine-tunes the language model by jointly optimizing ranking loss and SFT loss on these ranked responses. 8 Implicit Rewards from Automated Feedback. Implicit rewards can originate from diverse automated feedback as well, such as AI feedback, feedback from external knowledge, and feedback from external tools. AI Feedback. AI feedback is common source of implicit rewards, including self-rewarding and other trained models. Self-Rewarding (Yuan et al., 2024b) leverages the language model to evaluate its own outputs with prompt like Rate this answer from 1 to 5. Then it constructs preference data with highand low-rated responses to fine-tune the LLM through iterative DPO. MetaRewarding (Wu et al., 2024a) builds on this idea: the language model judges its outputs and evaluates the quality of its own judgments. This creates two-level preference structure and enables joint optimization of acting and judging capabilities. Zhang et al. (2025c) extend self-rewarding to the process-level. It constructs step-wise preference data via self-rewarding to improve the language models reasoning ability. Instead of direct self-assessment, some methods depend on self-consistency to model implicit rewards. SCPO (Prasad et al., 2024) samples multiple responses for each input and selects the consistent responses as preferred ones. Similarly, it then fine-tunes the language model through DPO on these constructed preference data. Similarly, PFPO (Jiao et al., 2024a) constructs preference data by verifying model-generated solutions against the test cases derived by self-consistency. list of extended studies focuses on the multimodal field. HA-DPO (Zhao et al., 2023) evaluates model responses of image understanding with another trained model as hallucination detector to produce preference data. With these preference data, it fine-tunes the multimodal language models via DPO to reduce hallucination. Many studies follow similar way to this, such as POVID (Zhou et al., 2024c), AMP (Zhang et al., 2024d) and RLAIF-V (Yu et al., 2024a), and BPO (Pi et al., 2024). Feedback from Knowledge and Tools. External knowledge and tools can provide feedback to model implicit rewards. Tian et al. (2023) construct preference pairs by checking whether model outputs are supported by Wikipedia. These preference data are used to fine-tune language model toward more factually accurate generations via DPO. FLAME (Lin et al., 2024a) follows similar way. It fine-tunes the language model on factuality-aware preference data via both SFT and DPO to maintain the instruction-following ability. TRICE (Qiao et al., 2023) uses tool execution feedback to rank sampled outputs, and the language model is trained with ranking loss combined with SFT on ranked samples toward accurate and appropriate tool usage. CodeLutra (Tao et al., 2024) categorizes generated code into successful and failed samples based on code execution outcomes. It constructs preference data with these samples and fine-tunes the model by combining DPO and SFT objectives. Xiong et al. (2025) leverages SymPy as the verifier to construct mathematical reasoning trajectory pairs for DPO training. Note that some methods use hybrid automated feedback sources. For instance, RefineCoder (Zhou et al., 2025a) incorporates critiques from selfrewarding, Elo ranking, and code execution feedback to build the training data for fine-tuning the language model."
        },
        {
            "title": "3.4 Training with Rule-based Rewards\nRecently, training with rule-based rewards 1 has\ngained prominence, driven by the success of\nDeepSeek-R1 (DeepSeek-AI et al., 2025). Rule-\nbased rewards are derived by verifying outputs\nagainst specific rules, such as format constraints\nand evaluation metrics. By leveraging reinforce-\nment learning with rule-based rewards, DeepSeek-\nR1 demonstrates that language models can acquire\nlong Chain-of-Thoughts (long CoT) abilities for\ntest-time scaling. This enables it to solve complex\ntasks such as mathematics and coding and show\nself-reflection and self-correction behaviors. This\nphenomenon, characterized by a sudden emergence\nof advanced reasoning capabilities, is referred to\nas RL scaling or the “Aha moment\". Note that the\nliterature sometimes refers to rule-based rewards\nas verifiable rewards/outcomes due to their clean\nevaluation criteria.",
            "content": "In detail, DeepSeek-R1 (DeepSeek-AI et al., 2025) defines two types of rule-based rewards: (i) Accuracy Rewards, whether the output is factually or functionally correct, e.g., the correctness of math solutions or code passing unit tests; (ii) Format Rewards, which ensure the output follows specific structural constraints, such as containing spe1While rule-based rewards may overlap with previous scalar or critique rewards, here we present them separately due to their emerging significance. 9 cial tokens <think>, </think>, <answer>, and </answer> to encourage long CoT reasoning. With these rule-based rewards, it fine-tunes the language model through the RL algorithm GRPO (Shao et al., 2024). GRPO eliminates the dependence on the reward and value model in PPO and the preference data construction in DPO. Later, many following studies have been proposed. DAPO (Yu et al., 2025b) and Open-R1 (Face, 2025) introduce open-source training frameworks, and some extended GRPO algorithms are introduced (Xu et al., 2025b; Zuo et al., 2025; Feng et al., 2025c; Zhang et al., 2025b). Logic-RL (Xie et al., 2025a) combines accuracy and format rewards to improve logical reasoning through REINFORCE++ (Hu, 2025). Visual-RFT (Liu et al., 2025h) introduces set of rule-based rewards for visual tasks, such as Intersection over Union (IoU), confidence-based scoring, and format compliance to fine-tune vision language model via GRPO. These visual tasks include image classification, reasoning grounding, and object detection. Similarly, CLS-RL (Li et al., 2025c) designs rule-based rewards for image classification. R1-VL (Zhang et al., 2025a) proposes StepGRPO that extends GRPO into step-level multimodal reasoning."
        },
        {
            "title": "3.5 Training with Process Rewards",
            "content": "The aforementioned strategies mostly depend on outcome rewardsevaluating the holistic quality of outputs. An emerging line of work focuses on training with process rewards, 2 which evaluate the intermediate steps in models reasoning trajectory, such as the steps in mathematical problem solving. As shown in Figure 3(d), these methods commonly employ Process Reward Model (PRM) to assess the intermediate steps. By providing step-level feedback, these methods enable more fine-grained supervision, which especially benefits complex reasoning tasks where intermediate reasoning directly influences the final result, such as mathematics and code. Process Rewards from Human Feedback. Early studies leverage human annotations to train PRMs. For instance, Uesato et al. (2022); Lightman et al. (2023) train PRMs using human annotations on intermediate mathematical reasoning steps. Uesato et al. (2022) then use the trained PRM to 2Similarly, we discuss process rewards separately due to their increasing prevalence although they may overlap with previous scalar or critique rewards. fine-tune the language model via reinforcement learning to improve its math reasoning. Process Rewards from Automated Feedback. key limitation of PRMs is their reliance on laborintensive step-level human annotations. To address this limitation, recent efforts propose incorporating automated feedback to supervise PRMs training at scale. One major direction leverages strong LLMs to generate step-level annotations. For example, WizardMath (Luo et al., 2023) uses GPT-4 to label intermediate steps in math solutions and fine-tunes the language model through step-by-step PPO. ActPRM (Duan et al., 2025) similarly uses strong LLM to annotate step-level correctness of reasoning trajectories, which enables step-by-step DPO fine-tuning. Alternatively, other methods estimate process rewards without explicit annotations. Math-Shepherd (Wang et al., 2023b) applies Monte Carlo estimation to infer step-level scores of mathematical reasoning based on the likelihood of reaching correct final answer. It uses these estimated annotations to train PRM and then fine-tunes the language model through step-by-step PPO. Jiao et al. (2024b) follow the same way but use DPO training with sampled reasoning trajectories as preference data. PQM (Li and Li, 2024) reformulates the PRM training as Q-value ranking problem. It trains PRM through margin-based ranking loss over step pairs, encouraging higher Q-values for steps leading to correct answers. OmegaPRM (Luo et al., 2024) adopts an efficient and fully automated method to collect step-level supervision with novel MCTS algorithm. This significantly reduces the annotation cost and enables to scale up the training of high-quality process reward models. HRM (Wang et al., 2025b) also leverages reasoning trajectories from MCTS for training. It evaluates both individual steps and consecutive step pairs to capture multi-step coherence and error recovery. Thus this avoids penalizing an early error regardless of subsequent correction. Some methods attempt to derive process rewards from outcome rewards. Yuan et al. (2024a) derive process rewards from an outcome reward model as the cumulative token-level log-ratio differences. Building on this idea, PRIME (Cui et al., 2025) trains reward model online using only outcomelevel supervision (e.g., answer correctness) on sampled solution rollouts for mathematical problems. 10 Figure 6: Illustrations of strategies for Inference with Rewards. (a,b): Generate-then-rank with outcome and process rewards. (c): Reward-guided decoding at the token and step level with search algorithms. The reward model estimates token-level process rewards without requiring step-level annotations. It then fine-tunes the language model with these rewards through PPO (or REINFORCE variants). OREAL (Lyu et al., 2025) samples reasoning trajectories and their binary outcome rewards from fine-tuned policy model. Afterward, it trains PRM to assign importance scores to each token in the trajectories, where the weighted sum of the scores needs to approximate the outcome rewards. Due to the popularity of generative reward models, generative PRMs have also been introduced. For instance, GenPRM (Zhao et al., 2025b) predicts the correctness of reasoning step by generating CoT reasoning and code verification. GenPRM is trained via SFT on synthesized reasoning trajectories from the MATH dataset, where external LLMs generate the rationales and correctness labels. RPRM (She et al., 2025) and ThinkPRM (Khalifa et al., 2025) adopt comparable generative PRM design by incorporating the reasoning process. In the multimodal field, M-STAR (Liu et al., 2024c) trains multimodal PRM on estimated steplevel scores via Monte Carlo rollouts and then uses the reward model to sample high-quality responses to supervise the subsequent fine-tuning iterations."
        },
        {
            "title": "Inference with Rewards",
            "content": "After the training stage, inference with rewards offers flexible and lightweight mechanism to adapt and steer the model behavior without modifying model parameters. We identify two primary inference-with-rewards strategies: (i) Generatethen-Rank and (ii) Reward-Guided Decoding. These strategies play critical role for achieving test-time scaling: They allow the language model to search, reflect, and revise its outputs on the fly."
        },
        {
            "title": "4.1 Generate-then-Rank",
            "content": "The generate-then-rank approach, usually referred to as Best-of-N, easily scales test-time compute to improve model outputs. It samples number 11 a h e r I Generate-then-Rank 4.1 Ranking by Outcome Rewards Cobbe et al. (2021); Uesato et al. (2022); LEVER (Ni et al., 2023); V-STaR (Hosseini et al., 2024); GenRM (Zhang et al., 2024c); Fast Best-of-N (Sun et al., 2024a) Ranking by Process Rewards Lightman et al. (2023); DIVERSE (Li et al., 2023b); Math-Shepherd (Wang et al., 2023b); ViLPRM (Tu et al., 2025) VisualPRM (Wang et al., 2025c); CoRe (Zhu et al., 2022) Token-level Guidance RAD (Deng and Raffel, 2023); ARGS (Khanov et al., 2024); PG-TD (Zhang et al., 2023c); ARM (Troshin et al., 2024); FaRMA (Rashid et al., 2025) Reward-Guided Decoding 4. Step-level Guidance CARDS (Li et al., 2024a); GRACE (Khalifa et al., 2023); Xie et al. (2023); Snell et al. (2025); ORPS (Yu et al., 2024c); RSD (Liao et al., 2025); Tree-of-Thoughts (Yao et al., 2023); OVM (Yu et al., 2023a); RAP (Hao et al., 2023); STILL-1 (Jiang et al., 2024); rStar (Qi et al., 2024); ReST-MCTS* (Zhang et al., 2024a); LE-MCTS (Park et al., 2024); rStar-Math (Guan et al., 2025) Figure 7: Overview of Inference with Rewards. of candidate responses from the language model, scores them with reward model, and selects the best one as the final output by ranking or voting (Wang et al., 2022). Based on the reward granularity, we distinguish two kinds of methods: (i) ranking by outcome rewards and (ii) ranking by process rewards as shown in Figure 6(a,b). Ranking by Outcome Rewards. As shown in Figure 6(a), this method adopts an outcome reward model (ORM) to assess the holistic quality of candidate responses. Early work by Cobbe et al. (2021) trains binary ORM to evaluate the correctness of candidate math solutions and selects the top-ranked one as the final output. Uesato et al. (2022) adopt the same idea and conduct comprehensive experiments on ranking outputs by ORMs. LEVER (Ni et al., 2023) trains binary classifier as the ORM with code execution results as supervision. During inference, it ranks generated candidate code jointly based on the ORMs score and the generation probability. V-STaR (Hosseini et al., 2024) trains verifier as the ORM on preference pairs through DPO to rank candidate math/code solutions during inference. Its ORM supports iterative training on dynamically collected preference data, which can progressively improve performance. GenRM (Zhang et al., 2024c) follows generative way. It scores multiple candidate solutions by computing the generation probability of the Yes/No token as rewards and then selects the best solution. While simple and effective, this strategy becomes computationally expensive as the number of candidates increases. To address this, Fast Best-ofN (Sun et al., 2024a) follows speculative rejection scheme. It queries the reward model multiple times throughout the generation of one candidate response and terminates the unpromising generation early based on the rewards. As such, this way accelerates the inference process without completely generating all candidates. Jinnai et al. (2024) investigate the reward backing problem of the generatethen-rank strategy. Brown et al. (2024) systematically investigate the scaling of testing-time computing with various ORMs across multiple tasks. Ranking by Process Rewards. As aforementioned, outcome reward models may struggle to discern the nuance among candidate responses. Thus many methods adopt process reward models (PRMs) for the generate-then-rank strategy. These methods score intermediate steps of candidate responses through PRM and aggregate these steplevel scores through multiplication or minimum to compute an overall score for ranking or voting (Zhang et al., 2025g). Early work by Lightman et al. (2023) introduces PRM trained on large-scale human-labeled math dataset (PRM800K) and ranks candidate math solutions by the product of their step-level reward scores. DIVERSE (Li et al., 2023b) trains reward model to assign scores to both the entire path and individual reasoning steps. Then it picks up the best one from multiple candidates through weighted voting. Math-Shepherd (Wang et al., 2023b) uses PRM to score each step in math solution and ranks solutions according to the lowest step-level score. Notably these methods can improve reason12 ing consistencythe chosen solution builds on series of reliable steps rather than merely delivering correct final answer. Some studies have explored multimodal PRMs for the generate-then-rank strategy. Examples include VisualPRM (Wang et al., 2025c) and ViLPRM (Tu et al., 2025). They incorporate both image and text inputs with step-wise evaluation in the multimodal reasoning. Some approaches combine outcome and process rewards to improve ranking quality. For example, CoRe (Zhu et al., 2022) integrates these two signals to jointly verify model outputs during inference."
        },
        {
            "title": "4.2 Reward-Guided Decoding",
            "content": "While the above generate-then-rank approach is simple and effective, it inherently decouples generation from evaluation, limiting its ability to refine outputs dynamically during decoding. In contrast, reward-guided decoding tightly incorporates reward signals to guide the generation process of language models. Based on the granularity of guidance, we categorize this line of work into two strategies: token-level guidance and step-level guidance. As shown in Figure 8(c), these strategies guide the language models token-level or step-level decoding based on the reward signals through search algorithm, such as greedy search, beam search, or Monte Carlo Tree Search (MCTS). This enables fine-grained control over output quality and alignment and can foster reasoning and planning abilities. Token-level Guidance. Token-level guidance steers language model generation by incorporating reward signals into the token selection process at each decoding step. This strategy commonly combines the token likelihoods with the reward signals from reward model. RAD (Deng and Raffel, 2023) adjusts token selection by combining the tokens likelihood and the scalar rewards. It can control output attributes such as non-toxicity and sentiment. ARGS (Khanov et al., 2024) applies similar strategy to align the helpfulness and harmless preferences. This work converts preference alignment from the previous training stage to the inference stage. PG-TD (Zhang et al., 2023c) targets code generation. It uses the pass rate over test cases as reward signals and integrates them into MCTS to guide tokenlevel planning and generation and integrates it into MCTS to guide token-level planning and generation. However, the above methods are limited by low decoding efficiency since they require querying the reward model for each candidate token at every decoding step. To improve decoding efficiency, ARM (Troshin et al., 2024) proposes low-rank approximation of the reward model to score all token candidates within single query. Similarly, FaRMA (Rashid et al., 2025) trains reward model that scores all token candidates in single forward pass. Step-level Guidance. Beyond token-level guidance, step-level guidance operates on intermediate steps of the generation process. As illustrated in Figure 6(d), the generation is decomposed into multiple intermediate steps. At each step, search algorithm, such as beam search and Monte Carlo Tree Search (MCTS), explores the output space and selects the appropriate steps with the guidance of reward signals. This mechanism enables the model to recover from earlier errors and enhance reasoning. In some cases, step may correspond to semantic segment. For example, CARDS (Li et al., 2024a) samples candidate semantic segments at each step. Then it uses reward model to score the resulting output with segment and choose the high-reward segment for continuing decoding. More studies focus on guiding reasoning steps with reward signals. GRACE (Khalifa et al., 2023) trains discriminator as the reward model that scores the correctness of candidate reasoning steps. During decoding, it combines the step-level reward scores with the language models likelihood to guide step selection toward more accurate reasoning paths. Xie et al. (2023) employs similar approach by prompting the language model itself to evaluate the reasoning steps during beam search. Snell et al. (2025) use process reward model to guide which reasoning steps are retained in the beam search and look-ahead search during decoding. ORPS (Yu et al., 2024c) derives outcome rewards from code execution feedback and process rewards from LLM-generated self-critiques about code reasoning quality. It combines them to guide tree-based search process, e.g., selecting, expanding, and refining candidate solutions throughout the generation. RSD (Liao et al., 2025) combines rewards with speculative decoding. During its speculative decoding, it leverages process reward model to determine whether to accept 13 Figure 8: Illustration of Post-Inference with Rewards. (a): Self-Correction, using the language model itself. (b): Correction with External Feedback, such as trained model, external knowledge, and external tools. draft models output or invoke the target model. Some studies guide the decoding based on the step-level value, i.e., cumulative future rewards. Tree-of-Thoughts (Yao et al., 2023) prompts the language model to assess the value of the current state by producing scalar score or short phrases. It then guides the search algorithms, like BFS and DFS, to explore diverse reasoning trajectories. OVM (Yu et al., 2023a) trains value model to estimate the probability that partial reasoning path leads to correct final answer. During inference, it guides the decoding via value-based beam search to select the most promising reasoning trajectories. Other methods use reward signals to guide Monte Carlo Tree Search (MCTS). RAP (Hao et al., 2023) defines task-specific reward functions for MCTS. This encourages the model to simulate future states and select high-reward reasoning paths for planning, math, and logic tasks. STILL-1 (Jiang et al., 2024) follows similar reward-guided MCTS way. rStar (Qi et al., 2024) assigns rewards to reasoning trajectories by combining the correctness of the final answer with self-consistency confidence. Then it back-propagates these rewards to the steps in the trajectories to guide future MCTS exploration toward more promising reasoning paths. Several extensions leverage process reward models to precisely guide MCTS, including ReST-MCTS* (Zhang et al., 2024a), LE-MCTS (Park et al., 2024), and rStar-Math (Guan et al., 2025). They use steplevel scoring from process reward models to select and expand high-quality reasoning trajectories during the search."
        },
        {
            "title": "5 Post-Inference with Rewards",
            "content": "Post-inference with rewards aims to correct and refine the model outputs after they have been generated with reward signals. This approach enables iterative enhancement without updating model parameters, offering lightweight and compatible mechanism for test-time scaling. Compared to sparse scalar rewards, post-inference methods favor critique rewards that provide fine-grained and dense signals for correction. These critiques typically identify error locations, explain reasoning flaws, and suggest revisions. To leverage such rich feedback, the language model commonly incorporates them as augmented context and iteratively generates revised outputs. According to the source of rewards, we categorize these methods into two main strategies: Self-Correction and Correction with external rewards."
        },
        {
            "title": "5.1 Self-Correction",
            "content": "As illustrated in Figure 8(a), self-correction leverages the language model itself as generative reward model to evaluate and revise its own outputs, similar to the aforementioned self-rewarding strategy. Self-Refine (Madaan et al., 2023) prompts the language model itself to produce natural language feedback on its own outputs. It then leverages this feedback as reward signals to refine outputs iteratively. Similarly, Reflexion (Shinn et al., 2023) generates reflection feedback through the language model itself. It additionally maintains memory bank to store previous feedback, outputs, and scalar feedback from evaluation metrics. These reflections serve as auxiliary contexts to refine subsequent generations. CoVe (Dhuliawala et al., 2023) prompts the language model to generate and answer verification questions about its own outputs to identify factual errors. This verification feedback guides the model to correct its responses to reduce hallucination. In addition, SCoRE (Kumar et al., 2024) trains the language model via reinforcement learning to enhance its self-correction capability. Similarly, RISE (Qu et al., 2024) bootstraps the language model with DPO training for better self-correction. 14 a h e r I - P Self-Correction 5. Self-Refine (Madaan et al., 2023); Reflexion (Shinn et al., 2023); CoVe (Dhuliawala et al., 2023); SCoRE (Kumar et al., 2024); RISE (Qu et al., 2024) Trained Models CodeRL (Le et al., 2022); RL4F (Akyurek et al., 2023); Shepherd (Wang et al., 2023c); A2R (Lee et al., 2024); CTRL (Xie et al., 2025b); CriticGPT (McAleese et al., 2024); DARS (Li et al., 2025a); REFINER (Paul et al., 2023); AutoMathCritique (Xi et al., 2024); MAD (Liang et al., 2023); Cohen et al. (2023); Du et al. (2023) Correction with External Feedback 5.2 External Knowledge RARR (Gao et al., 2022); ReFeed (Yu et al., 2023c); LLM-Augmenter (Peng et al., 2023); Varshney et al. (2023); FACTOOL (Chern et al., 2023) External Tools Self-Edit (Zhang et al., 2023a); Self-Debug (Chen et al., 2023); CYCLE (Ding et al., 2024); Logic-LM (Pan et al., 2023a); IHR (Qiu et al., 2023); Baldur (First et al., 2023); CRITIC (Gou et al., 2023); RCI (Kim et al., 2023) Figure 9: Overview of Post-Inference with Rewards."
        },
        {
            "title": "5.2 Correction with External Feedback",
            "content": "While self-correction is simple, prior studies argue that general language models struggle to identify and correct their errors without external feedback, especially for less capable small-scale models (Huang et al., 2023; Kamoi et al., 2024; Madaan et al., 2023; Pan et al., 2023b). Owing to this, increasing attention has been devoted to incorporating external feedback as reward signals to refine model outputs as shown in Figure 8(b). We classify these works according to the feedback source: Trained Models, External Knowledge, and External Tools. Trained Model. Many methods rely on more capable trained models (commonly referred to as critic models) to provide feedback as reward signals. The feedback is mostly natural language critiques containing quality assessments and correction suggestions on model outputs. The early work CodeRL (Le et al., 2022) uses trained critic model to predict the functional correctness of the generated code. Afterward, this feedback serves as reward signals to guide the language model to regenerate the code. Welleck et al. (2022) trains critic model to provide both scalar and critique feedback for mathematics, constrained generation, and toxicity control. RL4F (Akyurek et al., 2023) uses critic model to generate critiques for topicbased summarization, action planning, and alphabetization tasks. Shepherd (Wang et al., 2023c) presents critic model that can identify factual, logical, or stylistic issues and suggest actionable refinements for the language model. A2R (Lee et al., 2024) incorporates factual critiques from critic model to mitigate the hallucination issue. CTRL (Xie et al., 2025b) focuses on code generation. It fine-tunes critic model with code execution results via GRPO, and the model provides actionable critiques to guide the refinement of generated code. McAleese et al. (2024) use trained critic model, CriticGPT, to identify the flaws in generated code. It can output structured critiques that disclose bugs and reasoning errors in the generated code. DARS (Li et al., 2025a) refines model outputs by guiding iterative reasoning with the reflection critiques from trained critic model. Moreover, some studies focus on fine-grained step-level feedback for correction. REFINER (Paul et al., 2023) adopts critic model to provide finegrained feedback on the intermediate reasoning steps. It then uses these feedback to iteratively refine the language models reasoning for math and moral story. AutoMathCritique (Xi et al., 2024) similarly trains critic model to provide processlevel critiques, which supports iterative refinement of mathematical reasoning outputs. Some methods follow the multi-agent debate design, where critiques from peer agents support reflection and improvement. These methods include MAD (Liang et al., 2023), Cohen et al. (2023), and Du et al. (2023). In the multimodal field, DRESS (Chen et al., 2024b) leverages GPT-4 to generate feedback on vision-language model outputs, including critiques and refinement suggestions. It then uses these feedback to guide the model to refine its outputs iteratively. 15 External Knowledge. External knowledge sources mainly provide factual critiques along with retrieved evidence, which can improve factuality and reduce hallucinations. RARR (Gao et al., 2022) derives hybrid rewards based on the entailment-based agreement between the model output and retrieved evidence from external knowledge. These reward signals then guide the post-hoc correction of the language model to improve factual attribution while preserving the original texts intent and structure. ReFeed (Yu et al., 2023c) applies similar method to knowledge-intensive QA tasks. LLM-Augmenter (Peng et al., 2023) computes the Knowledge F1 scores between model outputs and retrieved evidence from external knowledge as reward signals. These signals guide the language models decision to either finalize or continue to refine its response. Varshney et al. (2023) formulates verification questions regarding the low-confidence concepts in the model outputs. To verify these questions, it retrieves evidence from external knowledge and takes the results and evidence as feedback to guide the language models refinement and reduce hallucination. As general factuality tool, FACTOOL (Chern et al., 2023) broadens this idea to an enormous scope, including knowledge-based QA, code generation, mathematical reasoning, and scientific literature review. External Tools. External tools can execute and verify the language model outputs, and their feedback can work as reward signals for correction. Self-Edit (Zhang et al., 2023a) and Self-Evolve (Jiang et al., 2023) use program execution feedback from code compilers to guide the refinement of the language model. Self-Debug (Chen et al., 2023) and CYCLE (Ding et al., 2024) extend them with more feedback, for instance, unit test results and program explanations. Apart from code compilers, Logic-LM (Pan et al., 2023a) uses the feedback from symbolic logic reasoner as critique rewards to refine the models answers to logic reasoning problems. IHR (Qiu et al., 2023) depends on the feedback from symbolic interpreter for inductive reasoning, and Baldur (First et al., 2023) incorporates feedback from proof checker for automated formal verification. CRITIC (Gou et al., 2023) and RCI (Kim et al., 2023) can leverage feedback from diverse external tools, such as search engines, code interpreters, and calculators."
        },
        {
            "title": "6 Benchmarking Reward Models",
            "content": "As reward models play central role in the learningfrom-rewards paradigm for post-training and testtime scaling, rigorous and diverse benchmarks are essential for evaluating their capabilities, like other benchmarks for LLMs (Pan et al., 2024b, 2023c; Zhou et al., 2024b; Wu et al., 2022, 2023a,b, 2024c,d,e,b,f,g; Wu, 2024). As illustrated in Figure 10, recent benchmarking efforts primarily rely on expert human annotators or AI annotators (e.g., LLM-as-a-Judge frameworks) followed by human verification to ensure reliability. The resulting annotations are mainly pointwise (e.g., scalar scoring) or pairwise (e.g., selecting the preferred response given two options). These benchmarks vary in task coverage, evaluation protocols, annotation sources, and reward formats. We discuss the representative benchmarks in the literature as follows."
        },
        {
            "title": "6.1 Benchmarking Outcome Reward Models",
            "content": "A dominant line of benchmarking studies centers on outcome reward models that evaluate the overall quality of generated outputs. Zheng et al. (2023) is an early work that evaluates LLMs judging ability by directly prompting them. As LLMs can naturally function as generative reward models, this study also represents one of the earliest benchmarks for reward models. RewardBench (Lambert et al., 2024) is the first comprehensive benchmarks for reward models. It aggregates preference data from existing datasets, such as AlpacaEval and MTBench, to evaluate reward model performance in chatting, reasoning, and safety. RM-Bench (Liu et al., 2024d) introduces evaluation for reward models on sensitivity to subtle content changes and robustness to style biases. It constructs preference pairs across chat, code, math, and safety domains using GPT-4o. AceMath-RewardBench (Liu et al., 2024e) focuses on math-specific evaluations. It tests whether reward models can identify correct solutions from candidates across various mathematical tasks and difficulty levels. RMB (Zhou et al., 2024a) furthermore broadens the evaluation scope to 49 realworld scenarios. Apart from evaluating with preference data, some benchmarks focus on the critique ability of reward models. CriticBench (Lin et al., 2024b) assess whether reward models can generate critiques that accurately identify the correctness of response and effectively guide the correction. Similarly, MetaCritique (Sun et al., 2024b) benchmarks 16 Figure 10: Illustration of Benchmarking Reward Models. Annotations come from human annotators or AI annotators with human verification. The annotations are mainly pointwise (e.g., scalar score for each sample) or pairwise (e.g., chosen and rejected responses). d r R k h B Benchmarking Outcome Reward Models 6. Zheng et al. (2023); RewardBench (Lambert et al., 2024); RM-Bench (Liu et al., 2024d); AceMath-RewardBench (Liu et al., 2024e); RMB (Zhou et al., 2024a); CriticBench (Lin et al., 2024b); MetaCritique (Sun et al., 2024b) Benchmarking Process Reward Models 6.2 MathCheck-GSM (Zhou et al., 2024d); MR-GSM8K (Zeng et al., 2023); ProcessBench (Zheng et al., 2024); PRMBench (Song et al., 2025b); Big-Bench Mistake (Tyen et al., 2023); MR-Ben (Zeng et al., 2024) Benchmarking Multimodal Reward Models 6.3 MJ-Bench (Chen et al., 2024c); MLLM-as-a-Judge (Chen et al., 2024a); VL-RewardBench (Li et al., 2024b); Multimodal-RewardBench (Yasunaga et al., 2025) ; SVIP (Gao et al., 2025); VLRMBench (Ruan et al., 2025) Other Benchmarks 6. RAG-RewardBench (Jin et al., 2024); M-RewardBench (Gureja et al., 2024); PPE (Frick et al., 2024) Figure 11: Overviews of Benchmarking Reward Models. LLM-generated critiques by decomposing them into atomic information units and assessing their correctness."
        },
        {
            "title": "6.2 Benchmarking Process Reward Models",
            "content": "Recently more benchmarks focus on process reward models due to their increasing significance. In detail, several benchmarks focus on math reasoning, such as MathCheck-GSM (Zhou et al., 2024d), MR-GSM8K (Zeng et al., 2023), and MR-MATH (Xia et al., 2024). They require reward models to locate the first error step in math reasoning solution. Their testing samples are adapted from existing math datasets, including GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Furthermore, ProcessBench (Zheng et al., 2024) features diversity and higher difficulty levels by scaling this up to Olympiadand competition-level math problems (He et al., 2024; Gao et al., 2024b). Beyond step correctness, PRMBench (Song et al., 2025b) offers more fine-grained benchmark. It annotates each step in the reasoning path with specific error types grouped into three dimensions: simplicity, soundness, and sensitivity. The annotations come from LLM-generated perturbations and are subsequently verified by human annotators. Besides mathematical reasoning, Big-Bench Mistake (Tyen et al., 2023) targets logical reasoning. It annotates chain-of-thought trajectories from BIGBench (bench authors, 2023), each labeled with the first logical error. Furthermore, MR-Ben (Zeng et al., 2024) expands this to the reasoning process of seven domains: math, logic, physics, chemistry, medicine, biology and code."
        },
        {
            "title": "Models",
            "content": "Due to the prevalence of multimodal language models, another vital line of benchmarks focuses on multimodal reward models with diverse evaluation protocols. MJ-Bench (Chen et al., 2024c) depends on text17 to-image generation tasks for evaluation. It builds preference data across four dimensions: text-image alignment, safety, image quality, and social bias. MLLM-as-a-Judge (Chen et al., 2024a) uses image understanding tasks for benchmarking and includes pointwise and pairwise scoring. VL-RewardBench (Li et al., 2024b) includes three tasks: general multimodal instructions, hallucination detection, and multimodal reasoning. Multimodal-RewardBench (Yasunaga et al., 2025) spans six key capabilities of multimodal reward models: general correctness, human preference, factual knowledge, reasoning, safety, and VQA. Beyond the outcome level, current benchmarks also assess multimodal process reward models. SVIP (Gao et al., 2025) targets process-level evaluation on relevance, logic, and attribute correctness of diverse multimodal tasks. It transforms reasoning paths into executable visual programs and automatically annotates each step. VLRMBench (Ruan et al., 2025) further integrates evaluation on three dimensions: reasoning steps, whole outcomes, and critiques on error analysis. It collects testing data of multimodal understanding through AI annotations and human verification."
        },
        {
            "title": "6.4 Other Benchmarks",
            "content": "In addition to general-purpose evaluations, several benchmarks aim to address domain-specific or emerging challenges in reward modeling. RAGRewardBench (Jin et al., 2024) targets reward model evaluation in RAG. It constructs preference data for RAG-specific scenarios, including multihop reasoning, fine-grained citation, appropriate abstention, and conflict robustness. M-RewardBench (Gureja et al., 2024) extends the evaluation to multilingual contexts. Instead of direct evaluation, PPE (Frick et al., 2024) indirectly evaluates reward models through RLHF pipelines. It measures the performance of trained LLMs with reward model, offering practical perspective."
        },
        {
            "title": "7 Applications",
            "content": "The strategies described above for learning from rewards have been widely adopted across diverse applications. Early applications focus on preference alignment, such as RLHF (Ouyang et al., 2022) and RLAIF (Bai et al., 2022b). In particular, the recent DeepSeek-R1 (DeepSeek-AI et al., 2025) has demonstrated the effectiveness of reinforcement learning to develop large reasoning models, which has inspired wave of R-1 style applications for diverse areas. In this section, we review the primary applications following these strategies."
        },
        {
            "title": "7.1 Preference Alignment",
            "content": "Learning-from-rewards strategies have become the cornerstone for aligning LLMs with human preferences. These strategies design diverse reward signals to encourage desirable attributes, such as factuality, harmlessness, and helpfulness, while penalizing undesired behaviors like toxicity, bias, and hallucination. We summarize three major objectives of preference alignment as follows. Factuality and Reducing hallucination. Hallucination refers to generating fluent but factually incorrect or fabricated content (Tian et al., 2023). It is pervasive issue for language models, especially in knowledge-intensive tasks such as healthcare and scientific research. The methods for this alignment span the training, inference, and post-inference stages (Sun et al., 2023; Lin et al., 2024a; Zhao et al., 2023; Peng et al., 2023; Wang et al., 2023c). The rewards mainly stem from human preferences about factuality as well as external knowledge sources. For instance, Fact-RLHF (Sun et al., 2023) trains factuality-aware reward model on human preferences and additional supervision from image captions and multiple-choice answers The reward model is then used to fine-tune the multimodal language model via PPO, guiding the model to reduce hallucinations. RLFH (Wen et al., 2024) decomposes the model responses into atomic statements, verifies their truthfulness against external knowledge, and converts them into dense tokenlevel scalar rewards. To reduce hallucination, it directly uses these reward signals to fine-tune the model via PPO. Safety and Harmlessness. Safety and harmlessness constitute another critical axis of alignment, particularly in adversarial or socially sensitive contexts (Bai et al., 2022b; Ji et al., 2023). Language models must be discouraged from producing toxic, offensive, or biased content before being deployed in real-world systems. To this end, the methods primarily focus on the training (Ouyang et al., 2022; Bai et al., 2022a) and inference stages (Deng and Raffel, 2023; Khanov et al., 2024). For instance, RAD (Deng and Raffel, 2023) depends on reward signals to produce non-toxicity content during decoding. 18 Helpfulness. Meanwhile, helpfulness emphasizes that language models are expected to provide relevant, informative, and context-aware responses to fulfill user intent (Taori et al., 2023). This alignment is imperative in areas like instruction-following and dialogue systems. Reward signals are generally sourced from human preferences and task-specific quality metrics (Bai et al., 2022a)."
        },
        {
            "title": "7.2 Mathematical Reasoning",
            "content": "Mathematical reasoning is vital to measure the language models ability to solve complex reasoning problems. Some methods build reward models and fine-tune the language model for math reasoning (Shao et al., 2024; DeepSeek-AI, 2025), particularly using process reward models (Uesato et al., 2022; Luo et al., 2023) like Math-Shepherd (Wang et al., 2023b). They can provide step-level reward signals for math reasoning solution. Moreover, some approaches construct preference data for math reasoning, i.e., correct and incorrect solutions, and then fine-tune the language model through DPO (Lai et al., 2024; Xu et al., 2025a). Others include inference-time scaling strategies, such as generate-then-rank (Cobbe et al., 2021; Lightman et al., 2023), and reward-guided decoding with search algorithms like MCTS (Hao et al., 2023; Guan et al., 2025)."
        },
        {
            "title": "7.3 Code Generation",
            "content": "The code generation task has made significant strides due to the development of LLMs, which improves software engineering productivity by large margin. To improve the code language model through fine-tuning, the reward signals can come from various sources, including (Zhu et al., 2024), and code compiler feedback, unit test results, and code analysis (Liu et al., 2023; Dou et al., 2024; Tao et al., 2024; Zhou et al., 2025a). For example, DeepSeek-Coder-V2 (Zhu et al., 2024) trains reward model for code generation and fine-tunes the language model via the reinforcement learning algorithm GRPO (Shao et al., 2024). Additionally, some approaches guide the inference of language models during code generation with reward models, including the generate-then-rank (Ni et al., 2023; Hosseini et al., 2024) and reward-guided decoding (Yu et al., 2024c). Another popular direction refines the generated code to correct errors and bugs through the language model itself (Shinn et al., 2023; Zhang et al., 2023a; Chen et al., 2023) or external feedback (Xie et al., 2025b)."
        },
        {
            "title": "7.4 Multimodal Tasks",
            "content": "Learning-from-rewards strategies have been widely applied to multimodal tasks, including multimodal understanding and generation. Most studies adopt reinforcement learning and reward-guided decoding methods. For instance, Q-Insight (Li et al., 2025d) focuses on improving comprehensive image quality understanding with reinforcement learning. VLM-R1 (Shen et al., 2025a) applies reinforcement learning to fine-tune vision-language models and focuses on two tasks: referring expression compression and object detection. Vision-R1 (Huang et al., 2025) enhances multimodal reasoning of vision-language models for mathematical VQA. Zhan et al. (2025) proposes another Vision-R1, but it mainly facilitates object localization tasks with vision-language models. Video-R1 (Feng et al., 2025b), VideoChat-R1 (Li et al., 2025f), and TinyLLaVA-Video-R1 (Zhang et al., 2025e) apply GRPO into video reasoning. R1-V (Chen et al., 2025a) and CrowdVLM-R1 (Wang et al., 2025e) focus on visual counting. More example applications include multimodal reasoning (Zhou et al., 2025b; Meng et al., 2025; Tan et al., 2025; Li et al., 2025b; Liu et al., 2025f), object detection (Liu et al., 2025h), segmentation (Liu et al., 2025d), and image/video generation (Guo et al., 2025b; Liu et al., 2025a)."
        },
        {
            "title": "7.5 Agents",
            "content": "LLM Agent is an autonomous system that automatically performs complex tasks through task decomposition and action execution in dynamic environments (Wang et al., 2024c). Various learning-fromrewards strategies have been applied to training or guiding the agents. AgentRM (Xia et al., 2025) targets general-purpose decision-making agents across domains such as web navigation, embodied planning, text games, and tool use. During inference, reward model guides the agents to choose candidate actions or trajectories. AgentPRM (Choudhury, 2025) trains LLM agents with process reward model. KBQA-o1 (Luo et al., 2025) guides MCTS with reward model for the knowledge base question answering task with agents. DeepResearch (OpenAI, 2025) and DeepResearcher (Zheng et al., 2025) design agents for research tasks. They both use reinforcement learning to fine-tune the agents. UI-R1 (Lu et al., 2025) introduces rule-based reinforcement learn19 ing framework for GUI action prediction with multimodal agents. InfiGUI-R1 (Liu et al., 2025c) is similar work with GUI agents. RAGEN (Wang et al., 2025f) propose training the agents via multiturn reinforcement learning with new algorithm based on GRPO."
        },
        {
            "title": "7.6 Other Applications",
            "content": "Many other applications have been developed following the learning-from-rewards strategies. Embodied AI is essential for the development of artificial general intelligence. AI systems, such as embodied robots, must interact with the physical world and complete complex tasks through high-level planning and low-level control. They generally aim to enhance the embodied reasoning abilities with reinforcement learning, such as Cosmos-reason1 (Azzolini et al., 2025), iRe-VLA (Guo et al., 2025a), Embodied-Reasoner (Zhang et al., 2025d), and Embodied-R (Zhao et al., 2025a). Several approaches apply reinforcement learning to reason with information retrieval from knowledge databases or the real-world web. These approaches include R1-Searcher (Song et al., 2025a), Search-R1 (Jin et al., 2025), DeepRetrieval (Jiang et al., 2025), ReSearch (Chen et al., 2025b), and WebThinker (Li et al., 2025e). They adopt different reward designs to improve search performance. Applications for other applications also emerge. ToRL (Li et al., 2025g), ReTool (Feng et al., 2025a), SWi-RL (Goldie et al., 2025), ToolRL (Qian et al., 2025) and OTC (Wang et al., 2025a) are proposed to improve LLMs reasoning ability to call various tools through reinforcement learning. Rec-R1 (Lin et al., 2025) applies reinforcement learning for recommendation system. SWE-RL (Wei et al., 2025) aims at software engineering with reinforcement learning. SQL-R1 (Ma et al., 2025) focuses on natural language to SQL reasoning. It uses composite reward function with format correctness, execution success, result accuracy, and reasoning completeness. Some applications are designed for specific areas. Med-R1 Lai et al. (2025) and MedVLM-R1 (Pan et al., 2025) are proposed for medical field. They target medical VQA across various imaging modalities (e.g., CT, MRT, and X-ray) and several clinical tasks, like diagnosis, and anatomy identification. Fin-R1 (Liu et al., 2025e) develops LLMs for the financial field, targeting financial QA and decision-making. It leverages accuracy and format rule-based rewards to train language model on domain-specific data. DianJin-R1 (Zhu et al., 2025) is another LLM for the financial field with reinforcement learning."
        },
        {
            "title": "8 Challenges and Future Directions",
            "content": "In this section, we discuss the current challenges and future directions of learning from rewards. Figure 12 summarizes the key challenges and future directions from the perspective of reward model design and learning strategies. Ultimately, we envision the development of interpretable, robust, and continually evolving agent systems capable of interacting with and adapting to the complexities of the real world. 8."
        },
        {
            "title": "Interpretability of Reward Models",
            "content": "Interpretability of reward models remains an open challenge for the learning-from-rewards strategies (Russell and Santos, 2019; Zhang et al., 2023d; Jenner and Gleave, 2022). Most reward models are typically treated as black boxes that produce scalars or critiques without exposing human-interpretable explanations. Such opacity hinders human trust and oversight and may lead to misaligned optimization. In consequence, enhancing reward model interpretability is essential for reliable alignment, enabling humans to inspect and verify the internal decision process and steer models toward desired behavior. Recent efforts have attempted to address this issue. For instance, ArmoRM (Wang et al., 2024b) improves the interpretability with multiobjective reward modeling, where each objective corresponds to human-interpretable dimension, such as helpfulness, correctness, coherence, complexity, and verbosity. While this approach is effective, its interpretability is limited to these predefined objectives. In addition, emerging generative reward models can disclose their rationales of reward scoring (Zhao et al., 2025b; Khalifa et al., 2025). While promising, their interpretability remains limited and demands further investigation into consistency, reliability, and faithfulness."
        },
        {
            "title": "8.2 Generalist Reward Models",
            "content": "A promising future direction is the development of generalist reward models. Most existing reward models are designed for narrow domains; thus they often suffer from weak generalization across tasks. Moreover, their reward outputs are typically static and lack support for inference-time scalability, hindering their application in diverse and open-ended 20 Figure 12: Illustration of challenges and future directions. scenarios (Liu et al., 2024a; Zhang et al., 2024c; Snell et al., 2025). own evaluation heuristics, leading to inflated internal scores while deviating from true objectives. In contrast, generalist reward model seeks to overcome these limitations. They demand flexibility for input types, including single, paired, or multiple responses, and also require accurate reward generation in various domains, such as question answering, math reasoning, and code generation. Besides, they are expected to generate higher-quality reward signals with increased inference-time computing. Such models offer unified interface for reward modeling across domains and enable scalable, interpretable reward generation. For example, DeepSeek-GRM (Liu et al., 2025g), recent attempt in this direction, proposes pointwise generative reward model. Rather than only scalars, it can generate evaluative natural language principles and critiques, enabling effective inference-time scaling through multi-sample voting and meta-reward filtering. Reward hacking fundamentally arises from the difficulty of specifying reward function that perfectly captures the true objectives. As articulated by Goodharts LawsWhen measure becomes target, it ceases to be good measureany proxy metric used as reward will eventually be exploited once applying optimization pressure. To mitigate reward hacking, the following directions are worth exploring: (i) Designing more robust and tamperresistant reward functions (Razin et al., 2025; Shen et al., 2025b; Peng et al., 2025); (ii) Detecting misalignment via behavioral or distributional anomaly detection (Pan et al., 2022); (iii) Decoupling feedback mechanisms to prevent contamination (Uesato et al., 2020); (iv) Auditing the dataset for training reward models to reduce reward hacking risks (Revel et al., 2025)."
        },
        {
            "title": "Interactions",
            "content": "Reward hacking is fundamental challenge in learning from rewards (Everitt et al., 2021; Amodei et al., 2016; Weng, 2024; Liu et al., 2025b). It occurs when models exploit unintended shortcuts in the reward function to obtain high rewards without truly learning the desired behaviors or completing the task as designed. This phenomenon has been observed across domains. For instance, LLMs may fabricate plausible yet incorrect answers, and code LLMs subtly modify unit tests to pass evaluations (Denison et al., 2024). Reward backing can also happen during inference, called in-context reward hacking (Pan et al., 2024c,a). It arises in self-refinement loops where the same model acts as both the generator and the judge. In such cases, the model may learn to produce outputs that exploit its Despite recent advances in learning from rewards for LLMs, most methods fundamentally rely on human preferences or well-curated automated feedback. The LLMs are typically optimized to maximize the rewards derived from these feedback. In consequence, this inherently limits the agents ability to surpass existing human knowledge and adapt to complex environments. Due to these limitations, moving beyond chatdriven rewards toward grounded real-world rewards is another promising direction. This movement requires LLMs to be integrated into agentic frameworks, and agents should increasingly interact directly with their environment and derive reward signals from observed outcomes. For example, health assistant could optimize behavior 21 based on physiological signals rather than user ratings, and scientific agent could refine hypotheses based on experimental data rather than expert approval (Silver and Sutton, 2025). This shift would enable agents to close the feedback loop with the real world, allowing for autonomous discovery, adaptation, and pursuit of goals beyond human understanding. The transition to real-world interactions raises substantial technical challenges. Agents must handle noisy, delayed, or partial feedback from complex environments, requiring advances in credit assignment, robust exploration, and uncertainty modeling."
        },
        {
            "title": "8.5 Continual Learning from Rewards",
            "content": "Current learning-from-rewards strategies often assume fixed dataset, predefined reward model, and short episodic interactions. Once trained, models typically exhibit limited abilities to adapt to new tasks or evolving environments (Zhang et al., 2024b; Silver and Sutton, 2025). This episodic and offline paradigm sharply contrasts with real-world intelligences dynamic, ongoing nature, where agents must continually learn from experience and recalibrate based on new feedback. As such, vital direction is continual learning from rewards. It is crucial foundation for building lifelong competent and aligned agents. By abandoning the traditional assumption of fixed objectives, models can remain responsive to changing reward signals, avoid performance degradation under distributional shifts, and better reflect long-term user intent. Notably, it is broader idea of continual reinforcement learning (Abel et al., 2023; Li et al., 2024c; Bowling and Elelimy, 2025). Achieving continual learning from rewards presents significant challenges. It requires addressing catastrophic forgetting, maintaining stability while enabling plasticity, and designing dynamic reward modeling mechanisms."
        },
        {
            "title": "9 Conclusion",
            "content": "In this paper, we present comprehensive survey of learning LLMs from rewards. We categorize the landscape into three key stages-training, inference, and post-inference, each reflecting distinct paradigm for integrating reward signals into steering LLMs behavior. For each stage, we review representative strategies in terms of reward signal design and learning algorithms. In addition, we summarize recent progress in benchmarking reward models and applications. Finally we identify core challenges and outline promising future directions. We hope this survey provides structured understanding of the field and inspires future research."
        },
        {
            "title": "References",
            "content": "David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, and Satinder Singh. 2023. definition of continual reinforcement learning. Advances in Neural Information Processing Systems, 36:5037750407. Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. 2024. Tuning large multimodal models for videos using reinforcement learning from ai feedback. arXiv preprint arXiv:2402.03746. Afra Feyza Akyurek, Ekin Akyurek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, and Niket Tandon. 2023. RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 77167733, Toronto, Canada. Association for Computational Linguistics. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. arXiv preprint Concrete problems in ai safety. arXiv:1606.06565. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. arXiv 2024. Critique-out-loud reward models. preprint arXiv:2408.11791. Anthropic. 2025. Introducing deep research. Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, et al. 2025. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073. 22 BIG bench authors. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Michael Bowling and Esraa Elelimy. 2025. Rethinking the foundations for continual reinforcement learning. arXiv preprint arXiv:2504.08161. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. 2024. Compassjudger-1: All-in-one judge model helps arXiv preprint model evaluation and evolution. arXiv:2410.16256. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024a. Mllm-asa-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. 2025a. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025b. Research: Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128. Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2024b. Dress: Instructing large vision-language models to align and interact with humans via natural language feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1423914250. Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, et al. 2024c. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? arXiv preprint arXiv:2407.04842. for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528. Sanjiban Choudhury. 2025. Process reward models for llm agents: Practical framework and directions. arXiv preprint arXiv:2502.10325. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. Lm vs lm: Detecting factual errors via cross examination. arXiv preprint arXiv:2305.13281. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. 2025. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456. Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773. DeepSeek-AI. 2025. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801. DeepSeek-AI et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Haikang Deng and Colin Raffel. 2023. Rewardaugmented decoding: Efficient controlled text generation with unidirectional reward model. arXiv preprint arXiv:2310.09520. Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, et al. 2024. Sycophancy to subterfuge: Investigating reward-tampering in large language models. arXiv preprint arXiv:2406.10162. Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495. Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. Factool: Factuality detection in generative aia tool augmented framework Yangruibo Ding, Marcus Min, Gail Kaiser, and Baishakhi Ray. 2024. Cycle: Learning to self-refine the code generation. Proceedings of the ACM on Programming Languages, 8(OOPSLA1):392418. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767. Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, et al. 2024. Stepcoder: Improve code generation with reinforcement learning from compiler feedback. arXiv preprint arXiv:2402.01391. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325. Keyu Duan, Zichen Liu, Xin Mao, Tianyu Pang, Changyu Chen, Qiguang Chen, Michael Qizhe Shieh, and Longxu Dou. 2025. Efficient process reward model training via active learning. arXiv preprint arXiv:2504.10559. Sujan Dutta, Sayantan Mahinder, Raviteja Anantha, and Bortik Bandyopadhyay. 2024. Applying RLAIF for code generation with API-usage in lightweight LLMs. In Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024), pages 3945, Bangkok, Thailand. Association for Computational Linguistics. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. KTO: model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 12291241. Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024. How to evaluate reward models for rlhf. arXiv preprint arXiv:2410.14872. Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, et al. 2024a. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. arXiv preprint arXiv:2406.14024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. 2024b. Omnimath: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. 2022. Rarr: Researching and revising what language models say, using language models. arXiv preprint arXiv:2210.08726. Minghe Gao, Xuqi Liu, Zhongqi Yue, Yang Wu, Shuang Chen, Juncheng Li, Siliang Tang, Fei Wu, Tat-Seng Chua, and Yueting Zhuang. 2025. Benchmarking multimodal cot reward model stepwise by visual program. arXiv preprint arXiv:2504.06606. Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. 2021. Reward tampering problems and solutions in reinforcement learning: causal influence diagram perspective. Synthese, 198(Suppl 27):64356467. Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Taco Cohen, and Gabriel Synnaeve. 2024. RLEF: grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089. Hugging Face. 2025. Open r1: fully open reproduction of deepseek-r1. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025a. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025b. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776. Zihao Feng, Xiaoxue Wang, Ziwei Bai, Donghang Su, Bowen Wu, Qun Yu, and Baoxun Wang. 2025c. Improving generalization in intent detection: Grpo with reward-based curriculum sampling. arXiv preprint arXiv:2504.13592. Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin J. Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sona Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375. Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher Manning. 2025. Synthetic data generation & multi-step rl for reasoning & tool use. arXiv preprint arXiv:2504.04736. Emily First, Markus Rabe, Talia Ringer, and Yuriy Brun. 2023. Baldur: Whole-proof generation and repair with large language models. In Proceedings of Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv:2305.11738. arXiv preprint models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. 2025. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alexa Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, A. Doucet, Orhan Firat, and Nando de Freitas. 2023. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998. Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. Improving vision-language-action model 2025a. with online reinforcement learning. arXiv preprint arXiv:2501.16664. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. 2025b. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926. Srishti Gureja, Lester James V. Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti Sharma, Gusti Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, and Marzieh Fadaee. 2024. M-rewardbench: Evaluating reward models in multilingual settings. arXiv preprint arXiv:2410.15522. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457. Jian Hu. 2025. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language Erik Jenner and Adam Gleave. 2022. Preprocessing reward functions for interpretability. arXiv preprint arXiv:2203.13553. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023. Beavertails: Towards improved safety alignment of llm via humanpreference dataset. Advances in Neural Information Processing Systems, 36:2467824704. Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, et al. 2024. Enhancing llm reasoning with reward-guided tree search. arXiv preprint arXiv:2411.11694. Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. arXiv preprint arXiv:2503.00223. Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023. Selfevolve: code evolution framework via large language models. arXiv preprint arXiv:2306.02907. Fangkai Jiao, Geyang Guo, Xingxing Zhang, Nancy Chen, Shafiq Joty, and Furu Wei. 2024a. Preference optimization for reasoning with pseudo feedback. arXiv preprint arXiv:2411.16345. Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy Chen, and Shafiq Joty. 2024b. Learning planningbased reasoning by trajectories collection and arXiv preprint process reward synthesizing. arXiv:2402.00658. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. 2024. Ragrewardbench: Benchmarking reward models in retrieval augmented generation for preference alignment. arXiv preprint arXiv:2412.13746. Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, and Kenshi Abe. 2024. Regularized best-of-n sampling with minimum bayes risk objective for language model alignment. arXiv preprint arXiv:2404.01054. 25 Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. When can llms actually correct their own mistakes? critical survey of selfcorrection of llms. Transactions of the Association for Computational Linguistics, 12:14171440. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. 2025. Process reward models that think. arXiv preprint arXiv:2504.16828. Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang. 2023. Grace: Discriminator-guided chain-of-thought reasoning. arXiv preprint arXiv:2305.14934. Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. 2024. Args: Alignment as reward-guided search. In The Twelfth International Conference on Learning Representations. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 36:3964839677. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. 2024. Training language models to selfcorrect via reinforcement learning. arXiv preprint arXiv:2409.12917. Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Reward design with language models. arXiv preprint arXiv:2303.00001. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. 2024. Step-DPO: Stepwise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629. Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. 2025. Med-r1: Reinforcement learning for generalizable medical reasoning in visionlanguage models. arXiv preprint arXiv:2503.13939. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward arXiv preprint models for language modeling. arXiv:2403.13787. Dongyub Lee, Eunhwan Park, Hodong Lee, and HeuiSeok Lim. 2024. Ask, assess, and refine: Rectifying factual consistency and hallucination in llms with metric-guided feedback learning. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 24222433. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. 2023a. RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with AI feedback. arXiv preprint arXiv:2309.00267. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. 2023b. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192. Bolian Li, Yifan Wang, Ananth Grama, and Ruqi Zhang. 2024a. Cascade reward sampling for efarXiv preprint ficient decoding-time alignment. arXiv:2406.16306. Jiazheng Li, Yuxiang Zhou, Junru Lu, Gladys Tyen, Lin Gui, Cesare Aloisi, and Yulan He. 2025a. Two heads are better than one: Dual-model verbal reflection at inference-time. arXiv preprint arXiv:2502.19230. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023a. Generative arXiv preprint judge for evaluating alignment. arXiv:2310.05470. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. 2024b. Vlrewardbench: challenging benchmark for visionlanguage generative reward models. arXiv preprint arXiv:2411.17451. Lihe Li, Ruotong Chen, Ziqian Zhang, Zhichao Wu, YiChen Li, Cong Guan, Yang Yu, and Lei Yuan. 2024c. Continual multi-objective reinforcement learning via reward model rehearsal. In Proceedings of the ThirtyThird International Joint Conference on Artificial Intelligence, pages 44344442. Lin Li, Wei Chen, Jiahui Li, and Long Chen. 2025b. Relation-r1: Cognitive chain-of-thought guided reinforcement learning for unified relational comprehension. arXiv preprint arXiv:2504.14642. Ming Li, Shitian Zhao, Jike Zhong, Yuxiang Lai, and Kaipeng Zhang. 2025c. Cls-rl: Image classification with rule-based reinforcement learning. arXiv preprint arXiv:2503.16188. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328. Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, and Jian Zhang. 2025d. Q-insight: Understanding image quality via viarXiv preprint sual learning. arXiv:2503.22679. reinforcement 26 Wendi Li and Yixuan Li. 2024. model with q-value rankings. arXiv:2410.11287. Process reward arXiv preprint Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025e. Webthinker: Empowering large reasoning models with deep research capability. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. 2025f. Videochat-r1: Enhancing spatiotemporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025g. arXiv preprint Torl: Scaling tool-integrated rl. arXiv:2503.23383. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023b. Making language models better reasoners with step-aware In Proceedings of the 61st Annual Meetverifier. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2023. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. 2024. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19401 19411. Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming Xiong. 2025. Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Jiacheng Lin, Tian Wang, and Kun Qian. 2025. Rec-r1: Bridging generative large language models and usercentric recommendation systems via reinforcement learning. arXiv preprint arXiv:2503.24289. Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Scott Yih, and Xilun Chen. 2024a. Flame: Factuality-aware alignment for large language models. Advances in Neural Information Processing Systems, 37:115588115614. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. 2024b. Criticbench: Benchmarking llms for critique-correct reasoning. arXiv preprint arXiv:2402.14809. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024a. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451. Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. 2025a. Video-t1: Test-time scaling for video generation. arXiv preprint arXiv:2503.18942. Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. RLTF: reinforcement learning from unit test feedback. Trans. Mach. Learn. Res., 2023. Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasiia Makarova, Jeremiah Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, and Mohammad Saleh. 2025b. Rrm: Robust reward model training mitigates reward hacking. arXiv preprint arXiv:2409.13156. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter Liu, and Jialu Liu. 2024b. Statistical rejection sampling improves preference optimization. In The Twelfth International Conference on Learning Representations. Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, and Junxian He. 2024c. Diving into selfevolving training for multimodal reasoning. arXiv preprint arXiv:2412.17451. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024d. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. 2025c. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239. Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. 2025d. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520. Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, et al. 2025e. Finr1: large language model for financial reasoning through reinforcement learning. arXiv preprint arXiv:2503.16252. Zhiyuan Liu, Yuting Zhang, Feng Liu, Changwang Zhang, Ying Sun, and Jun Wang. 2025f. Othinkmr1: Stimulating multimodal generalized reasoning capabilities through dynamic reinforcement learning. arXiv preprint arXiv:2503.16081. Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024e. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. 2025g. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. 2025h. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. 2025. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583. Haoran Luo, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan, et al. 2025. Kbqa-o1: Agentic knowledge base question answering with monte carlo tree search. arXiv preprint arXiv:2501.18922. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2. Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, et al. 2025. Exploring the limit of outcome reward for learning mathematical reasoning. arXiv preprint arXiv:2502.06781. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. In The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLPAACL 2023). Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, and Jian Guo. 2025. Sql-r1: Training natural language to sql reasoning model by reinforcement learning. arXiv preprint arXiv:2504.08600. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, JanPhilipp Fränken, Chelsea Finn, and Alon Albalak. 2024. Generative reward models. arXiv preprint arXiv:2410.12832. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. 2024. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. 2025. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235. Meta. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2307.09288. Meta. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. Webgpt: Browserassisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever: Learning to verify language-to-code In International Congeneration with execution. ference on Machine Learning, pages 2610626128. PMLR. OpenAI. 2023. Gpt-4 technical report. Preprint, arXiv:2303.08774. OpenAI. 2025. Introducing deep research. openai.com. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information 28 Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc. Alexander Pan, Kush Bhatia, and Jacob Steinhardt. 2022. The effects of reward misspecification: MaparXiv ping and mitigating misaligned models. preprint arXiv:2201.03544. Alexander Pan, Erik Jones, Meena Jagadeesan, and Jacob Steinhardt. 2024a. Feedback loops with language models drive in-context reward hacking. arXiv preprint arXiv:2402.06627. Fengjun Pan, Xiaobao Wu, Zongrui Li, and Anh Tuan Luu. 2024b. Are LLMs good zero-shot fallacy classifiers? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1433814364, Miami, Florida, USA. Association for Computational Linguistics. Jane Pan, He He, Samuel Bowman, and Shi Feng. 2024c. Spontaneous reward hacking in iterative selfrefinement. arXiv preprint arXiv:2407.04549. Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. 2025. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634. Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023a. Logic-lm: Empowering large language models with symbolic solvers arXiv preprint for faithful arXiv:2305.12295. logical reasoning. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023b. Automatically correcting large language models: Surveying the landscape of diarXiv preprint verse self-correction strategies. arXiv:2308.03188. Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023c. Fact-checking complex claims with program-guided reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 69817004. Sungjin Park, Xiao Liu, Yeyun Gong, and Edward Choi. 2024. Ensembling large language models with process reward-guided tree search for better complex reasoning. arXiv preprint arXiv:2412.15797. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback arXiv preprint on intermediate representations. arXiv:2304.01904. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813. Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. 2025. Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems. arXiv preprint arXiv:2502.19328. Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, and Tong Zhang. 2024. Strengthening multimodal large language model with bootstrapped preference optimization. arXiv preprint arXiv:2403.08730. Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, and Jane Yu. 2024. Self-consistency preference optimization. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. 2024. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958. Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, and Ningyu Zhang. 2023. Making language models better tool learners with execution feedback. arXiv preprint arXiv:2305.13068. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. 2023. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language modarXiv preprint els with hypothesis refinement. arXiv:2310.08559. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. 2024. Recursive introspection: Teaching language model agents how to self-improve. Advances in Neural Information Processing Systems, 37:55249 55285. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:53728 53741. Ahmad Rashid, Ruotian Wu, Rongqi Fan, Hongliang Li, Agustinus Kristiadi, and Pascal Poupart. 2025. Towards cost-effective reward guided text generation. arXiv preprint arXiv:2502.04517. Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason Lee, and Sanjeev Arora. 2025. What makes reward model good teacher? an optimization perspective. arXiv preprint arXiv:2503.15477. 29 Manon Revel, Matteo Cargnelutti, Tyna Eloundou, and Greg Leppert. 2025. Seal: Systematic error analysis for value alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2759927607. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025a. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592. Jiacheng Ruan, Wenzhen Yuan, Xian Gao, Ye Guo, Daoxin Zhang, Zhe Xu, Yao Hu, Ting Liu, and Yuzhuo Fu. 2025. VLRMBench: comprehensive and challenging benchmark for vision-language reward models. arXiv preprint arXiv:2503.07478. Jacob Russell and Eugene Santos. 2019. Explaining reward functions in markov decision processes. In Proceedings of the Thirty-Second International Florida Artificial Intelligence Research Society Conference, Sarasota, Florida, USA, May 19-22 2019, pages 56 61. AAAI Press. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, and Shujian Huang. 2025. R-prm: Reasoning-driven process reward modeling. arXiv preprint arXiv:2503.21295. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. 2025a. Vlmr1: stable and generalizable r1-style large visionlanguage model. arXiv preprint arXiv:2504.07615. Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan. 2025b. Exploring data scaling trends and effects in reinforcement learning from human feedback. arXiv preprint arXiv:2503.22230. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Reflexion: Language agents with Yao. 2023. arxiv preprint verbal reinforcement arXiv:2303.11366. learning. David Silver and Richard Sutton. 2025. Welcome to the era of experience. Google AI. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2025. Scaling llm test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, volume 2, page 7. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. 2025b. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024a. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290. Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie Li, and Pengfei Liu. 2024b. The critique of critique. arXiv preprint arXiv:2401.04518. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. 2023. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12. Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. 2025. Reason-rft: Reinforcement arXiv preprint fine-tuning for visual reasoning. arXiv:2503.20752. Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan A. Rossi, Yixuan Li, and Saayan Mitra. 2024. Codelutra: Boosting LLM code generation via preferenceguided refinement. arXiv preprint arXiv:2411.05199. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher Manning, and Chelsea Finn. 2023. Finetuning language models for factuality. In The Twelfth International Conference on Learning Representations. Sergey Troshin, Vlad Niculae, and Antske Fokkens. 2024. Efficient controlled language generation with low-rank autoregressive reward models. arXiv preprint arXiv:2407.04615. Haoqin Tu, Weitao Feng, Hardy Chen, Hui Liu, Xianfeng Tang, and Cihang Xie. 2025. Vilbench: suite for vision-language process reward modeling. arXiv preprint arXiv:2503.20271. 30 Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. 2023. Llms cannot find reasoning errors, but can correct them given the error location. arXiv preprint arXiv:2311.08516. Jonathan Uesato, Ramana Kumar, Victoria Krakovna, Tom Everitt, Richard Ngo, and Shane Legg. 2020. Avoiding tampering incentives in deep rl via decoupled approval. arXiv preprint arXiv:2011.08827. Math-shepherd: Verify and reinforce llms step-bystep without human annotations. arXiv preprint arXiv:2312.08935. Teng Wang, Zhangyi Jiang, Zhenqi He, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Shenyang Tong, and Hailei Gong. 2025b. Towards hierarchical multistep reward models for enhanced reasoning in large language models. arXiv preprint arXiv:2503.13551. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcomebased feedback. arXiv preprint arXiv:2211.14275. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and arXiv Xian Li. 2024e. preprint arXiv:2408.02666. Self-taught evaluators. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. 2024. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. 2023a. Decodingtrust: comprehensive assessment of trustworthiness in gpt models. In NeurIPS. Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2024a. mdpo: Conditional preference optimization for multimodal large language models. arXiv preprint arXiv:2406.11839. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024b. Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. arXiv preprint arXiv:2406.12845. Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. 2025a. Otc: Optimal tool calls via reinforcement learning. arXiv preprint arXiv:2504.14870. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024c. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. 2024d. Direct judgement preference optimization. arXiv preprint arXiv:2409.14664. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2023b. Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2023c. Shepherd: critic for language model generation. arXiv preprint arXiv:2308.04592. Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. 2025c. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. 2024f. Lift: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. 2025d. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236. Zhiqiang Wang, Pengbin Feng, Yanbin Lin, Shuzhang Cai, Zongao Bian, Jinghua Yan, and Xingquan Zhu. 2025e. Crowdvlm-r1: Expanding r1 ability to vision language model for crowd counting using fuzzy group relative policy reward. arXiv preprint arXiv:2504.03724. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. 2025f. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. 2025. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449. 31 Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053. Xueru Wen, Xinyu Lu, Xinyan Guan, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, and Le Sun. 2024. On-policy fine-grained knowledge feedback for hallucination mitigation. arXiv preprint arXiv:2406.12221. Lilian Weng. 2024. Reward hacking in reinforcement learning. lilianweng.github.io. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024a. Meta-rewarding language models: Self-improving alignment with llm-as-ameta-judge. arXiv preprint arXiv:2407.19594. Xiaobao Wu. 2024. Towards effective neural topic modeling. Ph.D. thesis, Nanyang Technological University. Xiaobao Wu, Xinshuai Dong, Thong Nguyen, Chaoqun Liu, Liangming Pan, and Anh Tuan Luu. 2023a. Infoctm: mutual information maximization perspective of cross-lingual topic modeling. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1376313771. AAAI Press. Xiaobao Wu, Xinshuai Dong, Thong Thanh Nguyen, and Anh Tuan Luu. 2023b. Effective neural topic modeling with embedding clustering regularization. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 3733537357. PMLR. Xiaobao Wu, Xinshuai Dong, Liangming Pan, Thong Nguyen, and Anh Tuan Luu. 2024b. Modeling dynamic topics in chain-free fashion by evolutiontracking contrastive learning and unassociated word exclusion. In Findings of the Association for Computational Linguistics ACL 2024, pages 30883105, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Xiaobao Wu, Anh Tuan Luu, and Xinshuai Dong. 2022. Mitigating data sparsity for short text topic modeling by topic-semantic contrastive learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 27482760, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Xiaobao Wu, Thong Nguyen, and Anh Tuan Luu. 2024c. survey on neural topic models: Methods, applications, and challenges. Artificial Intelligence Review. Xiaobao Wu, Fengjun Pan, and Anh Tuan Luu. 2024d. Towards the TopMost: topic modeling system toolkit. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 3141, Bangkok, Thailand. Association for Computational Linguistics. Xiaobao Wu, Fengjun Pan, Thong Nguyen, Yichao Feng, Chaoqun Liu, Cong-Duy Nguyen, and Anh Tuan Luu. 2024e. On the affinity, rationality, and diversity of hierarchical topic modeling. In Proceedings of the AAAI Conference on Artificial Intelligence. Xiaobao Wu, Liangming Pan, William Yang Wang, and Anh Tuan Luu. 2024f. AKEW: Assessing knowledge editing in the wild. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1511815133, Miami, Florida, USA. Association for Computational Linguistics. Xiaobao Wu, Liangming Pan, Yuxi Xie, Ruiwen Zhou, Shuai Zhao, Yubo Ma, Mingzhe Du, Rui Mao, Anh Tuan Luu, and William Yang Wang. 2024g. AntiLeak-Bench: Preventing data contamination by automatically constructing benchmarks with updated real-world knowledge. arXiv preprint arXiv:2412.13670. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. 2023c. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. 2023d. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20962105. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023e. Finegrained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36:5900859033. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2024h. Reasoning or reciting? exploring the capabilities and limitations of language In Proceedmodels through counterfactual tasks. ings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 18191862. Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, et al. 2024. Enhancing llm reasoning via critique models with test-time and trainingtime supervision. arXiv preprint arXiv:2411.16579. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Evaluating mathematiarXiv preprint and Pengfei Liu. 2024. cal reasoning beyond accuracy. arXiv:2404.05692. Yu Xia, Jingru Fan, Weize Chen, Siyu Yan, Xin Cong, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2025. Agentrm: Enhancing agent generalization with reward modeling. arXiv preprint arXiv:2502.18407. 32 Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025a. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. 2023. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:4161841650. Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, et al. 2025b. Teaching language models to critique via reinforcement learning. arXiv preprint arXiv:2502.03492. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712. Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. 2025. Selfrewarding correction for mathematical reasoning. arXiv preprint arXiv:2502.19613. Huimin Xu, Xin Mao, Feng-Lin Li, Xiaobao Wu, Wang Chen, Wei Zhang, and Anh Tuan Luu. 2025a. Fullstep-dpo: Self-supervised preference optimization with step-wise rewards for mathematical reasoning. arXiv preprint arXiv:2502.14356. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2023. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:15903 15935. Yixuan Even Xu, Yash Savani, Fei Fang, and Zico Kolter. 2025b. Not all rollouts are useful: Downsampling rollouts in llm reinforcement learning. arXiv preprint arXiv:2504.13818. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Michihiro Yasunaga, Luke Zettlemoyer, and Marjan Ghazvininejad. 2025. Multimodal rewardbench: Holistic evaluation of reward models for vision language models. arXiv preprint arXiv:2502.14191. Fei Yu, Anningzhe Gao, and Benyou Wang. 2023a. Ovm, outcome-supervised value models for planarXiv preprint ning in mathematical reasoning. arXiv:2311.09724. Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, and Xuelong Li. 2025a. Improve llmas-a-judge ability as general ability. arXiv preprint arXiv:2502.11689. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. 2025b. Dapo: An opensource llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. 2023b. RLHF-V: towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. arXiv preprint arXiv:2312.00849. Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2024a. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220. Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023c. Improving language models via plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002. Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, and Rui Hou. 2024b. Self-generated critiques boost reward modeling for language models. arXiv preprint arXiv:2411.16646. Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, and Shikun Zhang. 2024c. Outcome-refining process supervision for code generation. arXiv preprint arXiv:2412.15118. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. 2024a. Free process rewards without process labels. arXiv preprint arXiv:2412.01981. Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, and Matthias Gallé. 2024a. Improving reward models with synthetic critiques. arXiv preprint arXiv:2405.20850. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024b. Self-rewarding language models. arXiv preprint arXiv:2401.10020. Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun Liu. 2024b. Beyond scalar reward model: Learning generative judge from preference data. arXiv preprint arXiv:2410.03742. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023a. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825. 33 Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023b. Rrhf: Rank responses to align language models with arXiv preprint human feedback without tears. arXiv:2304.05302. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. 2025b. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. 2023. Mr-gsm8k: metareasoning benchmark for large language model evaluation. arXiv preprint arXiv:2312.17080. Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, and Yeyun Gong. 2025c. Process-based self-rewarding language models. arXiv preprint arXiv:2503.03746. Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, et al. 2024. Mr-ben: meta-reasoning benchmark for evaluating system-2 thinking in llms. arXiv preprint arXiv:2406.13975. Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. 2025. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. arXiv preprint arXiv:2503.18013. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024a. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 37:6473564772. Han Zhang, Yu Lei, Lin Gui, Min Yang, Yulan He, Hui Wang, and Ruifeng Xu. 2024b. Cppo: Continual learning for reinforcement learning with human feedback. In The Twelfth International Conference on Learning Representations. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. 2025a. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937. Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023a. Self-edit: Fault-aware code editor for code generation. arXiv preprint arXiv:2305.04087. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024c. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240. Mengxi Zhang, Wenhao Wu, Yu Lu, Yuxin Song, Kang Rong, Huanjin Yao, Jianbo Zhao, Fanglong Liu, Haocheng Feng, Jingdong Wang, and Yifan Sun. 2024d. Automated multi-level preference for mllms. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua Tenenbaum, and Chuang Gan. 2023c. Planning with large language models for code generation. arXiv preprint arXiv:2303.05510. Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, et al. 2025d. Embodiedreasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks. arXiv preprint arXiv:2503.21696. Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. 2025e. Tinyllava-video-r1: Towards smaller lmms for video reasoning. arXiv preprint arXiv:2504.09641. Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. 2025f. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391. Yudi Zhang, Yali Du, Biwei Huang, Ziyan Wang, Jun Wang, Meng Fang, and Mykola Pechenizkiy. 2023d. Interpretable reward redistribution in reinforcement learning: causal approach. Advances in Neural Information Processing Systems, 36:2020820229. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025g. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301. Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, and Wenwu Zhu. 2025a. Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning. arXiv preprint arXiv:2504.12680. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al. 2025b. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah Smith. 2023b. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534. Shuai Zhao, Linchao Zhu, and Yi Yang. 2025c. Learning from reference answers: Versatile language model alignment without binary human preference data. arXiv preprint arXiv:2504.09895. 34 Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2023. Beyond hallucinations: Enhancing lvlms through hallucinationaware direct preference optimization. arXiv preprint arXiv:2311.16839. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. 2024. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559. Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2022. Solving math word problems via cooperative reasoning induced language models. arXiv preprint arXiv:2210.16257. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning lanarXiv guage models from human preferences. preprint arXiv:1909.08593. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. 2025. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160. Changzhi Zhou, Xinyu Zhang, Dandan Song, Xiancai Chen, Wanli Gu, Huipeng Ma, Yuhang Tian, Mengdi Zhang, and Linmei Hu. 2025a. Refinecoder: Iterative improving of large language models via adaptive critique refinement for code generation. arXiv preprint arXiv:2502.09183. Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024a. Rmb: Comprehensively benchmarking reward models in llm alignment. arXiv preprint arXiv:2410.09893. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. 2025b. R1zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132. Ruiwen Zhou, Wenyue Hua, Liangming Pan, Sitao Cheng, Xiaobao Wu, En Yu, and William Yang Wang. 2024b. Rulearena: benchmark for rule-guided reasoning with llms in real-world scenarios. arXiv preprint arXiv:2412.08972. Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. 2024c. Aligning modalities in vision large language models via preference finetuning. arXiv preprint arXiv:2402.11411. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek Wong, Xiaowei Huang, QiIs your ufeng Wang, and Kaizhu Huang. 2024d. model really good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733. Jie Zhu, Qian Chen, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, and Chi Zhang. 2025. Dianjin-r1: Evaluating and enhancing financial reasoning in large language models."
        }
    ],
    "affiliations": [
        "Nanyang Technological University"
    ]
}