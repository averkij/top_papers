{
    "paper_title": "Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?",
    "authors": [
        "Nandan Kumar Jha",
        "Brandon Reagen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design."
        },
        {
            "title": "Start",
            "content": "Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space? Nandan Kumar Jha New York University nj2049@nyu.edu Brandon Reagen New York University bjr5@nyu.edu 5 2 0 2 1 ] . [ 1 7 3 5 0 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) scale, the question is not just how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. In this work, we focus on FeedForward Networks (FFNs) and recast width selection as spectral utilization optimization problem. Using lightweight diagnostic suite: Hard Rank (participation ratio), Soft Rank (Shannon Rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI), we quantify how many latent directions are meaningfully activated across LLaMA, GPT2, and nGPT families. Our key finding is an Asymmetric Spectral Scaling Law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly, with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design."
        },
        {
            "title": "Introduction",
            "content": "As Large Language Models (LLMs) continue to grow in scale and complexity, central blind spot remains: How effectively is their internal capacity utilized? Existing empirical scaling laws (Kumar et al., 2025; Tao et al., 2024; Sardana et al., 2024; Kaplan et al., 2020) relate model performance to factors such as width, depth, and data size, but they offer little insight into how different architectural components exploit, or potentially squander, the high-dimensional latent space. These laws treat models as black boxes, abstracting away the internal dynamics of transformer blocks and leaving open questions about representational usage. Figure 1: Spectral rank vs. FFN hidden dimension in LLaMA-130M base model, with width sweep = αd (total parameters therefore differ across α). Log-Log fits: Soft rank follows linear power-law fit (β=1.06, R2=0.93), while hard rank grows sublinearly (β=0.60, R2=0.68), indicating width mainly adds low-energy tail directions rather than enlarging the high-energy dominant-mode subspace. Among transformer components, FFNs dominate the parameter budges as they can account for as much as 67% of the total parameters in decoderonly models (Pires et al., 2023; Geva et al., 2021). Yet, FFN width is typically set by rules of thumb rather than design principles, e.g., 4 expansion in GPT-2 (Radford et al., 2019) and 2.67 in LLaMA (Touvron et al., 2023). Even in recent LLMs such as Qwen (Hui et al., 2024), the FFN width varies substantially across model sizes (2.4-5.8) underscoring the lack of theoretical grounding. Despite their prevalence, we still lack clear understanding of how FFN width affects effective capacity usage. This raises three questions: Is increasing FFN width always beneficial for expressivity? How many latent directions are actually used in practice? Can we quantify representational efficiency beyond FLOPs and loss? We address these questions by reframing FFN width selection as spectral utilization problem. The intuition is straightforward: if wider FFNs truly expand usable capacity, then their spectrum should reflect growth in the effective dimensionality of the subspace the model exploits. To test this, we conduct layer-wise spectral audit across GPT2, LLaMA, and nGPT (Loshchilov et al., 2025) backbones, analyzing the eigenspectrum of postactivation covariance over training steps and layers. We quantify utilization using four lightweight, differentiable metrics: Hard Rank (participation ratio) to capture the dimensionality of the highenergy, or the dominant, mode (Gao et al., 2017); Soft Rank (Shannon Rank) to quantify uniformity across all directions (De Domenico and Biamonte, 2016); Spectral Concentration (eigenvalue early enrichment) to quantify how much variance is captured by leading eigenvalues (Marbut et al., 2023); and finally Spectral Utilization Index (SUI), composite metric that harmonically combines hard and soft rank to balance dominant-mode and tail usage across the where α {1, 2, 2.67, 4, 5, 6, 7, 8}, and model sizes ranging from 70M to 250M parameters, we uncover an Asymmetric Spectral Scaling Law that fundamentally changes our understanding of capacity allocation. The power law (Log-Log) fits reveal striking asymmetry (Figure 1): While soft spectral rank scales near-perfectly with FFN width (β 1, R2 1), hard spectral rank, measuring the dominant subspace, plateaus early with weak, noisy scaling (β 0.5, R2 0.5). analysis sweep D=αd,"
        },
        {
            "title": "FFN width",
            "content": "systematic"
        },
        {
            "title": "Through",
            "content": "This asymmetry highlights that widening FFNs operates through tail-first growth: predominantly adding low-energy directions while the high-energy mode saturates early. In other words, capacity increases, but it is increasingly allocated to directions that carry little variance. This effect resembles the well-known spectral bias in function space, where low input frequencies are learned before high ones (Rahaman et al., 2019). Both perspectives point to the same underlying principle: capacity is allocated unevenly across modes, though expressed in different bases (Fourier vs. activation eigenspectrum). Contributions.This work makes four main contributions: Conceptual. We reframe FFN width selection, traditionally treated as an implementation detail, as problem of spectral utilization, and introduce the first principled framework for understanding how FFN capacity is allocated with their width scaling. Theoretical. We uncover Asymmetric Spectral Scaling Laws that capture divergent growth between soft and hard spectral ranks. These laws reveal that FFN widening follows tail-first growth pattern, explaining why naive width scaling can yields diminishing returns. Methodological. We develop lightweight, differentiable diagnostic suite for tracking layerwise representational usage during training. This includes closed-form estimator, Keff = 1+(D1)SUI, which links utilization to effective dimension. Empirical. Across diverse architectures and scales, we show that (i) soft/hard rank asymmetry persist across model families, (ii) optimal widths are consistently narrower than those used in practice, (iii) LayerNorm placement critically shapes utilization: Post-LN suppresses tail capacity scaling, whereas Mix-LN (Li et al., 2025) improves dominant-mode scaling while preserving near-linear tail growth."
        },
        {
            "title": "2 Related Work",
            "content": "Cost-aware neural scaling. The foundational work (Kaplan et al., 2020) established the powerlaw relations between loss and compute, later refined by the Chinchilla laws (Hoffmann et al., 2022), which showed that many models are compute-suboptimal, too wide and under-trained for their budgets. Follow-up studies (Sardana et al., 2024) extended this perspective to deployment: under heavy traffic, the compute-optimal point shifts toward smaller models trained on more tokens, lowering inference cost. Paquette et al. (2024) map the regimes where capacity, optimizer noise, or embedding quality dominate under fixed budgets. Other orthogonal cost factors have also been identified: vocabulary should scale with width (Tao et al., 2024); reduced numerical precision effectively shrinks parameter count (Kumar et al., 2025); and robust estimation methods enable reliable scaling-law fits from small pilot runs (Choshen et al., 2024). These studies map efficiency tradeoffs along multiple axescompute, traffic, vocabulary, and precision. Our spectral-utilization laws introduce complementary axis: they target latentspace usage, capturing how width is actually employed rather than measured by FLOPs alone. Universality and representational capacity. After normalizing for efficiency offsets, checkpoints spanning models from GPT-2 to PaLM have been shown to collapse onto single sigmoidal curve, suggesting shared scaling trajectory across architectures (Ruan et al., 2024). The Physics of LMs series reports related regularity for factual knowledge: 2 bits/parameter ceiling that appears largely architecture-agnostic (Allen-Zhu and Li, 2025). Earlier work traced such apparent universality to heavy-tailed eigenspectra and implicit selfregularization (Martin and Mahoney, 2021). More recent analyses refine this view: small singular values have been shown to encode critical information in pretrained Transformers (Staats et al., 2024), while spectral collapse has been linked to oversmoothing dynamics in attention stacks (Dovonon et al., 2024). Architectural and domain-specific scaling Scaling exponents are not architecture-agnostic. Tay et al. (2022) show that the most effective inductive bias shifts with scale: Switch-Transformers (Fedus et al., 2022) dominate in smaller parameter regimes, Performers (Choromanski et al., 2020) at mid-scale, and vanilla attention at large scale. Cabannes et al. (2024) derive exact scaling laws for associative-memory matrices, while Shi et al. (2024) explain why larger models can underperform on time-series tasks by introducing lookback-aware law. Fort (2025) frames adversarial robustness as scaling phenomenon, showing that resistance to attack remains nearly constant across two orders of magnitude in model size. Finally, Lyu et al. (2025) present an analytically solvable attention mechanism that yields closed-form power laws, providing theoretical baseline. These threads underscore that scaling is multifaceted, bending with inductive bias, data modality, precision, and security constraints, precisely the facets our spectral scaling laws aim to highlight across GPT-2, LLaMA, and nGPT."
        },
        {
            "title": "3 Method",
            "content": "In this section, we explain our methodology for extracting layer-wise covariance spectra from FFN internal representation, and describe the four spectral metrics that quantify spectral utilization, and capture various aspect of spectrum (e.g., uniformity vs spikes). We finish with the end-to-end algorithm and short complexity analysis."
        },
        {
            "title": "3.1 Preliminaries and Eigendecomposition",
            "content": "Notation Let an L-layer transformer be given. Each transformer consist of an FFN layer whose hidden width is D; the width multiplier (relative to the models embedding size d) is denoted α = D/d. Formally, FFN with gating activation (e.g., SwiGLU in LLaMA (Touvron et al., 2023)) represented as FFN(x) = Wdown(σ(Wgatex) (Wupx)), where represents element-wise multiplication and σ is activation function such as SiLU (Elfwing et al., 2018). The pre-activation (output of the first linear projection) and pos-activation (before the down-projection) is represented as PreAct(X) = Wgatex and PostAct(X) = σ((Wgatex) (Wupx)). Activation sampling and co-variance matrix formation During training step we sample minibatch of tokens from each FFN layers (ℓ) postactivation (ℓ,t) post RN D. We compute the covariance using all tokens without any sub-sampling or statistical approximations to capture the true behavior of the model. Further, we compute an unbiased covariance matrix for all tokens in the batch as follows: Σ = (X µ)T (X µ) 1 DD. (1) For each covariance matrix, we perform eigendecomposition to obtain the eigenvalues Σv = λv. The eigenvalues are sorted in descending order: λ1 λ2 . . . λD 0. All subsequent metrics depend only on this spectrum."
        },
        {
            "title": "3.2 Spectral Rank Metrics",
            "content": "When feed-forward block is widened, the key question shifts from how many parameters did we add? to how many of those additional directions does the model actually use? To quantify this notion of use, we analyze the eigenspectrum of the post-activation covariance matrix and distill it into four metrics, each lies in the range [0, 1] and can be computed in O(D) time  (Table 1)  . Hard spectral rank. Participation Ratio (PR) acts as hard counter of dominant directions. Since PR squares the first spectral moment and divides by the second, it is particularly sensitive to prominent eigenvalues: even single large spike can significantly cap its value, whereas numerous smaller eigenvalues have minimal impact (Gao et al., 2017; Hu and Sompolinsky, 2022). Hence, PR effectively rounds off all but the strongest axes, hard spikesensitive estimate. Soft Spectral Rank. It complements PR by measuring the Shannon entropy of the full eigenvalue distribution (Skean et al., 2025; Wei et al., 2024; Garrido et al., 2023; Anand et al., 2011; Passerini and Severini, 2008), by converting eigenspectrum into probability distributions as pi = λi/ (cid:80) λj. Normalizing to [0, 1] yields smooth measure of dimensionality that captures long-tail variance patterns. Thus, while hard rank is sensitive to domiTable 1: Spectral utilization metrics for characterizing the FFN latent space utilization. Hard and Soft Rank capture absolute participation and entropy-based ranks in the native [1, D] scale, while their normalized forms yield bounded [0, 1] utilization scores. Spectral concentration measures front-loading of variance, SUI balances hard and soft ranks, and eDim translates spectral patterns into an interpretable effective dimension. Metric Hard Spectral Rank PR = (cid:32) Definition λi)2 λ2 ((cid:80) (cid:80) Soft Spectral Rank eR = exp , PR = PR1 D1 (cid:33) pi log pi , eR = (cid:88) Range Qualitative signal Interpretation Cost [0, 1] Spikes collapse Dominant spikes O(D) eR 1 1 [0, 1] Long tails dilution Uniformity of spread O(D) Spectral Concentration SC = Spectral Utilization Index (cid:33) 2 (cid:88) k=1 SUI = (cid:32) (cid:80)k (cid:80)D i=1 λi i=1 λi 2 PR eR PR + eR [0, 1] Strength of spikes Front-loadedness O(D) [0, 1] Penalizes both extremes Balanced utilization O(1) Effective dimension eDim = 1 + (D 1)SUI [1, D] # active PCs # active dimensions O(1) Once eigenvalues are sorted; Once ranks known nant peaks, soft rank responds to tail behavior. Describing the pair as hard and soft therefore captures their complementary sensitivities: former reacts sharply to collapse (variance concentrated in few axes), whereas the latter flags spectral dilution, variance diffused so widely that no direction carries significant weight. Spectral Utilization Index SUI combines hard and soft spectral ranks into unified measure of spectral utilization. Hard and soft ranks independently capture opposing failure modesspectral collapse versus dilution. To effectively combine these metrics, we adopt their harmonic mean, as it strongly penalizes imbalance: the harmonic mean sharply drops if either input is low, ensuring SUI attains high scores only when both metrics indicate balanced utilization. By rewarding spectra that avoid extremes and peak when moderate number of principal directions carry most variance, SUI thus provides robust, intuitive, and parameter-free indicator of overall spectral behavior. Spectral concentration. Practitioners not just about how many directions are active, but also about where the variance is concentrated. Spectral concentration measures the area between the cumulative eigen-spectrum and uniform baseline (Marbut et al., 2023), where higher value indicates that variance predominantly concentrates within the leading principal components, whereas lower value implies more uniform distribution of variance across the spectrum. Thus, unlike previous metrics, it distinguishes spectra that utilize different fractions of the available latent space. Finally, we convert SUI into an integer-valued measure called Effective Dimension (eDim), which directly represents the approximate number of active principal components. This makes interpretation more intuitive, particularly it simplifies abstract ratio into an absolute counts over abstract ratios and simplifies comparisons across layers of varying widths. Why these specific metrics? The hard and soft ranks offer complementary perspectives on spectral utilization: one highlights spectra dominated by few large eigenvalues, while the other captures cases with many small eigenvalues spread over long tail. Spectral concentration metric complements these ranks by pinpointing precisely where variance accumulates. SUI unifies the two ranks into single robust metric, penalizing both spectral extremes, and eDim further translates this into an intuitive count of active principal components. Collectively, these metrics map each layer onto an interpretable three-dimensional spectrum: collapse versus dilution, front-loaded versus dispersed variance, and overall spectral efficiency."
        },
        {
            "title": "4 Experimental Results",
            "content": "In this section, we present our empirical findings on the spectral scaling laws in by varying the hidden dimension sizes of FFNs. We primarily use Hard and Soft utilization to investigate how each scales with the hidden dimension for three sizes of LLaMA models (70M, 130M, 250M). To study how effectively FFNs leverage increasing hidden dimensions, we trained LLaMA models from scratch on C4 datasets. For each scale, we varied the hidden dimension across 8 values, D=αd, where α {1, 2, 2.67, 4, 5, 6, 7, 8} (a) LLaMA-70M (PreLN) (b) LLaMA-130M (PreLN) (c) LLaMA-250M (PreLN) Figure 2: Asymmetric spectral scaling with FFN width in LLaMA-style Pre-LN models. Soft rank (SRank, red) and hard rank (HRank, blue) vs. FFN hidden dimension on log-log axes for (a) 70M, (b) 130M, and (c) 250M backbones (fixed d, width sweep {1, 2, 2.67, 4, 5, 6, 7, 8}). Dashed lines are power-law fits; annotations mark αd. Soft-rank exponents cluster near unity (β = {0.873, 1.069, 0.872}; R2 = {0.770, 0.980, 0.850}), while hardrank exponents are smaller and noisier (β = {0.441, 0.604, 0.407}; R2 = {0.248, 0.684, 0.268}). All networks are trained from scratch; markers show layer median values, and error bars indicate across-layer variability."
        },
        {
            "title": "4.1 Asymmetric Spectral Scaling Laws",
            "content": "Asymmetric scaling across widths. Across all three backbones LLaMA networks (Figure 2), the soft spectral rank follows near-linear power law with width, whereas the hard spectral rank grows sublinearly and with greater variability. Quantitatively, SRank slopes are β 0.88 (70M), β 1.07 (130M), and β 0.87 (250M), all with strong fits (R2 0.77, 0.93, 0.86). In contrast, HRank slopes are much smaller (β 0.44, 0.60, 0.41) and substantially noisier (R2 0.24, 0.68, 0.27). The persistent vertical separation between SRank and HRank trends spans orders of magnitude, indicating that widening FFNs consistently inflates entropy-sensitive spectral rank more than the core participation-ratio-defined subspace. Tail-first growth. The disparity in slopes and lower R2 values for HRank point to tail-first allocation of capacity: as width increases, models primarily populate low-energy directions (raising SRank), while the high-energy subspace expands slowly and irregularly (limited HRank gains). The 130M case comes closest to linear SRank scaling (β 1.07, R2 0.93), yet even here the hard-rank response remains sublinear (β 0.60). This asymmetry supports the interpretation that width first buys coverage of many fine-grained, low-variance modes before it substantially grows the dominant, high-variance core. As widening predominantly enlarges the lowenergy tail, returns on the dominant-mode subspace diminish with D. Practically, this suggests width expansion should avoid excessive tail growth, favoring tail-aware pruning (to preserve core modes and trim diffuse directions) and MoE designs that allocate experts to tail capacity rather than uniformly inflating single dense FFN."
        },
        {
            "title": "4.2 Spectral Rank Utilization",
            "content": "SR. Across scales, From capacity to efficiency. Normalizing ranks HR by turns them into utilization fractions, HR declines reliand ably with width, confirming that the high-energy mode occupies shrinking share of dimensions as grows (e.g., slopes around 0.5 across 70M/130M/250M). By contrast, SR is nearly scaleinvariant (slopes 0), showing that the low-energy tail keeps pace with widening. Consistency with the asymmetric law. Alif SRank Dβsoft1 and HRank gebraically, Dβhard<1, then SRank Dβsoft1 D0 and HRank Dβhard1 , exactly matching the observed near-flat soft utilization and negative hard utilization slopes. Put simply, widening allocates capacity tail-first: coverage expands, but the fraction devoted to the core contracts. Failure modes in utilization space. This view cleanly separates two regimes. Spectral dilution arises when normalized soft Spectral Rank remains flat (or slightly increasing) while the normalized soft spectral rank falls, clearly noticeable in LLaMA-130M. Spectral collapse appears when both utilizations decrease, pronounced at large for 250M. These patterns are consistent across backbones and independent of absolute width, making them compact efficiency diagnostic. Composite diagnostics. Hard rank reflects the dominant modes and stays relatively flat (lower) across width while soft rank tracks the tail and (a) LLaMA-70M (PreLN) (b) LLaMA-130M (PreLN) (c) LLaMA-250M (PreLN) Figure 3: Spectral-rank utilization vs. FFN width in LLaMA-style Pre-LN models. We plot soft-rank utilization (SRank/(D 1), red) and hard-rank utilization (HRank/(D 1), blue) vs. FFN hidden dimension on log-log axes for 70M, 130M, and 250M backbones (fixed depth; width sweep = αd, α {1, 2, 2.67, 4, 5, 6, 7, 8}). Dashed lines show power-law fits, highlighting that SRank scales nearly linearly with width while HRank grows more slowly and with higher variability. All networks are trained from scratch; markers indicate layer median, and error bars denote across-layer variability. Table 2: Effective dimension (eDim), shown in gray-shaded columns, together with hardand soft-rank spectral metrics for LLaMA models under width scaling (D = αd, α 1, 2, 2.67, 4, 5, 6, 7, 8), where is the model embedding dimension (512 for 70M; 768 for 130M and 250M). D=1d D=2d D=2.67d D=4d D=5d D=6d D=7d D=8d HRank SRank eDim HRank SRank eDim HRank SRank eDim HRank SRank eDim HRank SRank eDim HRank SRank eDim HRank SRank eDim HRank SRank eDim 70M 11 130M 14 250M 20 112 135 221 19 25 36 21 30 274 442 655 38 56 117 14 32 26 271 525 514 26 56 49 13 25 338 582 717 24 47 56 10 50 59 293 1184 1136 18 96 112 11 31 344 964 764 21 58 44 48 76 66 955 1521 1593 90 144 125 46 53 975 1257 1777 86 101 153 grows steadily. Each metric alone can be misleading: soft rank continues to grow even when dominant modes are saturated, while hard rank ignores meaningful tail growth. Our notion of effective dimension (eDim), harmonic-mean fusion, penalizes this imbalance and increases only when both dominant and tail capacities improve. As shown in Table 2, eDim grows sub-linearly with width and remains small fraction of DS for all models, underscoring the asymmetry: widening mainly expands the tail while dominant modes saturate early. Larger models achieve slightly higher eDim at the same width multiplier, suggesting better tail utilization, however, still far from proportional scaling. Implications for model design. Our results suggest simple, spectral rationale for common FFN width choices. Since hard rank saturates early and soft rank keeps growing, the marginal eDim gain per unit width drops beyond 2.67-4. LLM families that target stronger tail expressivity (e.g., GPT-2) may push to 4, while those prioritizing parameter efficiency (e.g., LLaMA) can stop nearer 2.67 without losing dominant-mode capacity. This is one plausible factor (among data, depth, training recipe) behind the observed widths. From an FFN design perspective, this spectral view also yields practical rule of thumb and general diagnostic. By monitoring eDim during training, one can detect when widening ceases to provide meaningful returns. If eDim plateaus while hard rank remains flat, so that eDim/D stagnates, dominant modes are saturated and further width only inflates tail capacity. At that point, it is more effective to freeze width and reallocate budget (e.g., to depth) or pursue layer-wise adjustments, rather than continue uniform widening. Finally, our spectral analysis also informs pruning and layer-wise adaptation. Layers with persistently low eDim at large are natural candidates for FFN pruning or width reduction, whereas layers where eDim continues to rise with can absorb additional width more effectively. This motivates non-uniform width allocation across depth, pruning or narrowing saturated layers while widening those that remain expressive, rather than blanket scaling."
        },
        {
            "title": "4.3 Scaling Laws for Spectral Concentration",
            "content": "We investigate the spectral concentration of FFNs activation covariance matrices by modeling their eigenvalue distribution via truncated power-law: λk kα, = 1, . . . , D, where the exponent α controls how variance is distributed across eigendirections. While traditional rank-based metrics (e.g., Hard and Soft Spectral Ranks) integrate information from all eigenvalues, they often overlook crucial details in the distributions shape, such (a) = 768 (b) = 2048 (c) = 3072 Figure 4: Power-law templates for spectral concentration. Cumulative-variance curves generated from synthetic power-law spectra λk kα for three latent sizes (D = 768, 2048, 3072). Larger exponents (α) front-load variance and push the curve upward. Coloured call-outs report the concentration value reached by benchmark cut-offs. Table 3: Quantitative summary of the curves in Fig 4. For each α and hidden size we list the variance carried by the top-1 eigenvalue, and cumulative variance captured by the first 10%, 25% and 50% principal components, along with the concentration score. The results show sharp transition around α 1.2: below it at least half the spectrum is needed to explain 80% of the variance (dilution), above it fewer than 10% directions suffice (collapse). α 0.8 1.0 1.2 1.5 2.0 Top-1 eigenvalue Variance @ 10% dimensions Variance @ 25% dimensions Variance @ 50% dimensions Spectral Concentration 2048 3072 768 2048 3072 2048 3072 768 2048 3072 2048 3072 5.4% 6.9% 4.9% 51.9% 54.3% 13.8% 12.2% 11.6% 68.2% 72.0% 23.4% 22.2% 21.8% 81.9% 85.9% 39.4% 38.9% 38.8% 93.9% 96.3% 60.8% 60.8% 60.8% 99.3% 99.7% 55.2% 73.3% 87.2% 97.0% 99.8% 68.4% 70.0% 80.8% 83.1% 90.1% 92.3% 97.2% 98.3% 99.8% 99.9% 70.5% 83.9% 93.0% 98.6% 99.9% 84.3% 0.57 83.1% 84.0% 91.9% 0.72 90.4% 91.6% 96.7% 0.85 95.4% 96.4% 98.8% 99.3% 99.4% 0.95 99.9% 100.0% 100.0% 0.99 0.59 0.76 0.88 0.97 1.00 0.59 0.77 0.89 0.97 1.00 as distinguishing between sharply peaked spectra with extensive flat tails and those smoothly decaying. The proposed power-law scaling framework directly addresses this limitation, isolating the shape characteristics of spectral distributions. Higher values of α yield spectra sharply concentrated (frontloaded) among leading directions, indicating incipient collapse, whereas lower values produce more uniform (diluted) distributions, indicative of suboptimal variance allocation  (Fig. 4)  . Empirically, several robust trends emerge from our analysis. Spectral concentration, monotonically increases with α: as α rises from 0.8 to 2.0, it grows consistently from around 0.57 to nearly 0.99  (Table 3)  . Once eigenvalues decay faster than k2, variance is predominantly concentrated in the initial directions, becoming effectively dimensioninvariant and independent of model width. This invariance enables meaningful comparisons of FFN efficiency across models of different sizes by aligning them on common spectral utilization axis. For larger α 1.5, over 90% of variance resides within merely the top 10% of principal components  (Table 3)  . Conversely, at smaller values (α 0.8), capturing the same variance requires more than 50% of components, leading to state we term spectral dilution. Notably, activations in prevalent models such as LLaMA typically exhibit intermediate spectral concentration (α 1.11.3), thereby balancing effective dimensionality and representational compactness, avoiding the extremes of either spectral dilution or collapse."
        },
        {
            "title": "4.4 Spectral Scaling Dynamics",
            "content": "We track rank-width behavior throughout training to ensure whether scaling relations hold reliably and to disentangle transient artifacts from persistent effects. Our aim is to pinpoint when stable powerlaw regime emerges and to distinguish between FFN capacity growth (unnormalized ranks) and width efficiency(normalized ranks). In the early phase (2-3K steps), both Hardand Soft-Rank increases with width, but their trajectories diverge (see Figure 5). Hard-Rank is noisy, reflecting sensitivity to the top singular directions, whereas Soft-Rank increases smoothly and stabilizes earlier as it aggregates contributions across the spectrum. By 5K steps, the scaling curves flatten, R2 exceeds 0.6, and consistent power-law regime emerges. Notably, width ordering is not strictly preserved in raw ranks: occasional crossovers occur at higher widths (more visibly in Hard-Rank than Soft-Rank), indicating transient re-allocation of capacity at higher across widths. Normalized spectral ranks (utilization) stabilize in terms of exponents with βhard 0.34 and (a) Hard Rank β evolution (b) Soft Rank β evolution (c) Hard Rank Dynamics (d) Soft Rank Dynamics (e) Hard Rank Utilization (β) (f) Soft Rank Utilization (β) (g) Hard Rank Utilization Dynamics (h) Soft Rank Utilization Dynamics Figure 5: Training-time evolution of spectral scaling laws for LLaMA-130M (PreLN). Upper panels (a-d) show raw Hardand Soft-Rank, while lower panels (e-h) illustrate normalized ranks (Rank utilization). (a,b) and (e,f) track the scaling exponent β (blue, left axis) and fit quality R2 (red, right axis), while (c,d) and (g,h) show the corresponding layer-averaged rank dynamics fo each FFN widths (D=1d to 8d). βsoft +0.08 (final 1K steps). This implies that increasing width reduces dominant-mode concentration (lower Hard utilization) and spreads mass across more directions (higher Soft utilization). However, width ordering in the normalized curves is also not reliably preserved after 5K steps: Hard utilization typically shows an early peak then decays toward plateau, and Soft utilization shows mild overshoot before converging, yet late crossovers among the widths still occur. In summary, our analysis of spectral rank dynamics shows consistent width power law that emerges after stabilization (5K steps) with reliable fit quality (R2 0.6). The stabilized exponents for rank utilization (βhard < 0 and βsoft > 0) highlight the key trade-off: increasing width reduces concentration in dominant modes while broadening soft spectral utilization. Although transient crossovers can appear in both raw and normalized ranks, they do not alter the exponent β or the trade-off they encode. Thus, spectral scaling can be reliably characterized by the converged β and R2 values, providing quantitative relation between FFN width and latent space utilization."
        },
        {
            "title": "5.1 LayerNorm and Spectral Rank",
            "content": "Pre-LN shows the classic asymmetry. With PreLN, soft-rank scales close to linearly with width (β 0.88 at 70M; β 1.07 at 130M, high R2), while hard-rank is clearly sublinear (β 0.45/0.60, lower R2). This is the baseline tail-first growth: widening expands low-energy directions, while the high-energy core lags behind  (Table 4)  . Post-LN suppresses tail growth. Shifting LayerNorm after the sub-blocks lowers soft-rank slopes to 0.71 0.82 with stronger R2, effectively dampening tail inflation. Hard-rank slopes rise modestly to 0.52 0.56 with better R2, suggesting more orderly,but still sublinear,growth of the dominant subspace. Intuitively, normalizing after each transformation curbs variance spread, limiting activation of faint directions as width increases. Mix-LN balances core and tail. Mix-LN restores near-linear soft-rank scaling (β 0.97 1.10, high R2) while maintaining hard-rank growth above Pre-LN/Post-LN levels (β 0.59 0.63, moderate R2). In effect, it preserves tail coverage while also improving dominant-mode scaling, avoiding both the over-tailing of Pre-LN and the excessive tail suppression of Post-LN."
        },
        {
            "title": "5.2 LLaMA-250M PostLN",
            "content": "Spectral collapse in Post-LayerNorm blocks. We observe strong correlation between spectral health and the performance of LLaMA-250M when the FFN width is increased. In the vanilla PostLayerNorm setup, spectral dynamics remain stable only for the narrowest FFN width (1d). However, scaling the width to 2.67d or 4d leads to rapid collapse of spectral diversity: the hard-rank plunges (a) LLaMA-250M (PostLN) (b) LLaMA-250M (PostLN) +WNorm (c) LLaMA-250M (PostLN) + HNorm Figure 6: Normalizing FFN weights stabilizes spectral dynamics in LLaMA-250M (PostLN). Heatmaps show Hard Spectral Utilization (top), Soft Spectral Utilization (middle), and Spectral Concentration (bottom) across layers (y-axis) and training steps (x-axis) for 1d, 2.67d, and 4d FFN widths. Spectral utilization are shown in log scale while spectral concentration use linear scale. Vanilla PostLN becomes unstable and at higher widths, visible as darker regions for 2.67d and 4d in (a). Adding Weight Norm. (b) or Hyperspherical Norm. (c) to FFN linear layers stabilizes training, producing smoother spectral dynamics and more balanced hardand tail-mode utilization. Table 4: Spectral scaling law parameters (βCI, R2) for various LayerNorm positions (PreLN, PostLN, MixLN) across LLaMA models (70M, 130M, 250M). Red boxes highlight significant improvement in MixLN hard rank scaling behavior. PostLN results for LLaMA-250M are unavailable due to training instability at higher FFN width."
        },
        {
            "title": "Soft Rank",
            "content": "LLaMA-70M 0.451 0.778 (R2 = 0.251) 0.879 0.490 (R2 = 0.763) 0.556 0.358 (R2 = 0.706) 0.712 0.273 (R2 = 0.872) LLaMA-130M 0.604 0.411 (R2 = 0.684) 1.069 0.292 (R2 = 0.930) 0.521 0.294 (R2 = 0.758) 0.818 0.372 (R2 = 0.829) LLaMA-250M 0.407 0.671 (R2 = 0.268) 0.872 0.353 (R2 = 0.859) Training Instability 0.593 0.668 (R2 = 0.440) 0.626 0.484 (R2 = 0.626) 0.568 0.316 (R2 = 0.763) 0.972 0.477 (R2 = 0.805) 1.096 0.484 (R2 = 0.837) 0.989 0.257 (R2 = 0.937) to 103 and the concentration saturates to 1.0 within the first few thousand steps (Figure 6a). This spectral collapse signifies that most of the variance is funneled into one or two dominant directions, leaving the majority of the 3000 latent dimensions inactive. As result, model performance deteriorates sharply, with test perplexity exceeding consistent with the figures reported in Table 5. Table 5: Vanilla PostLN in LLaMa-250M becomes unstable at higher FFN dimensions, causing spikes in PPL values. Adding Weight Normalization or Hyperspherical Normalization to the FFN linear layers stabilizes training (former outperforms the latter across all scales)."
        },
        {
            "title": "PostLN",
            "content": "1d 2.67d 4d 27.10 1427.91 1431.01 Vanilla 24.27 25.08 28.89 WeightNorm 26.48 27.92 HypersphericalNorm 31.66 Weight Normalization enables high-rank spectra and best perplexity. Employing weight normalization (WNorm) (Salimans and Kingma, 2016) within each FFN significantly mitigates this collapse. The hard-rank stabilizes in the 102101 range, while spectral concentration settles around 0.250.3, indicating that hundreds of latent directions carry meaningful variance. This richer and more distributed latent basis translates into notably better performance: perplexities of 25.1 (at 2.67d) and 24.3 (at 4d), both outperforming the vanilla 1d baseline (27.1). These results affirm that maintaining non-degenerate spectrum not only prevents collapse but also improve models performance."
        },
        {
            "title": "5.3 Hyperspherical Normalization",
            "content": "Hyperspherical normalization (HNorm) also prevents collapse and promotes training stability but results in more conservative spectral utilization (Loshchilov et al., 2025; Lee et al., 2025; Karras et al., 2024; Wang and Isola, 2020; Liu et al., 2017). The hard-rank remains roughly an order of magnitude above the collapse threshold, yet 30% lower than the WNorm trace. Spectral concentration is marginally higher, suggesting somewhat narrower (a) GPT-2 Spectral Scaling (b) nGPT Spectral Scaling (c) GPT-2 Spectral Utilization Scaling (d) nGPT Spectral Utilization Scaling Figure 7: Spectral rank and utilization vs. FFN width scaling in GPT-2 and nGPT. Panels (a,b) show raw ranks, while (c,d) plot normalized rank utilization for Soft rank (SRank, red) and hard rank (HRank, blue) on log-log axes (d=768, width sweep {1d, 2d, 2.67d, 3d}). Hyperspherical constraints reduce soft-hard rank asymmetry, yielding balanced spectral dynamics and an effective utilization of FFN width nGPT. (a) GPT-2 (GeLU) (b) GPT-2 (SiLU) (c) nGPT-2 (SiLU) Figure 8: Layer-wise spectral utilization dynamics (GPT-2 vs nGPT). Heatmaps show Hard Spectral Utilization (top), Soft Spectral Utilization (middle), and Spectral Concentration (bottom) across layers (y-axis) and training steps (x-axis). Each panel compares 1d vs 2.67d FFN width. Spectral utilization are shown in log scale (top two rows) while spectral concentration use linear scale. effective basis. Consequently, while HNorm yields stable performance (27.9 at 2.67d and 26.5 at 4d), it does not match the perplexity gains achieved with WNorm. These findings highlight that collapse prevention is necessary condition, but further lifting the rank and ensuring richer variance distribution is critical for unlocking full potential of wider FFNs. Activation gating and normalization in GPT2. Figure 8 tracks the spectral evolution, and Table 6 shows perplexity outcomes of GPT-2 variants using different activation and normalization schemes under two FFN widths (1d and 2.67d). The baseline GPT-2 with GeLU shows early hard-rank growth that quickly saturates around 102, while spectral concentration remains high ( 0.7). This indicates narrow set of dominant directions and leads to moderate perplexity (14.07 at 2.67d), with limited gain over the 1d baseline (15.63). The nGPT configuration augments SwiGLU with hyperspherical weight and activation normalization and learnable residual eigen-learning rate (Loshchilov et al., 2025). This combination substantially enhances spectral performance: hardrank remains two orders of magnitude above collapse, soft-rank saturates earlier with less fluctuaTable 6: Perplexity (PPL) comparison of GPT-2 and nGPT (Loshchilov et al., 2025) with different activation functions and FFN dimensions. GPT-2(GeGLU) GPT-2(SwiGLU) nGPT(SwiGLU) 1d PPL 15.63 2.67d 14. 1d 15.60 2.67d 14.05 1d 15. 2.67d 13.60 tion, and concentration reduces to 0.4, 20% improvement over GPT-2. These gains are mirrored in performance, with perplexity dropping to 13.60 at 2.67d and stabilizing to 15.01 at 1d, outperforming both prior setups. Hyperspherical learning reduces asymmetry and converts width into shared capacity. Across the width sweep, vanilla GPT-2 shows the familiar split: hard rank (dominant modes) saturates early, while soft rank (tail) keeps rising, so added dimensions drift into the tail. With hyperspherical constraints (nGPT), the soft-hard gap narrows in both raw ranks and normalized utilization: slopes move closer and the separation between the soft and hard curves decreases (Figure 7). In practice, nGPT sustains growth in dominant modes instead of stalling, while tails expand without overwhelming the spectrum, yielding more balanced distribution. Moreover, their normalized utilization shows that GPT-2 dynamics remain uneven, whereas in nGPT they flatten into nearstraight lines, indicating that FFN width is actually being used rather than pooled in the tail. This makes hyperspherical learning (Liu et al., 2021, 2018, 2017; Wang and Isola, 2020; Lin et al., 2020; Bernstein, 2025) promising representational technique for improving FFN latent-space utilization, enabling more balanced spectral dynamics and efficient use of width."
        },
        {
            "title": "6 Conclusion",
            "content": "We reframed FFN width selection as spectral utilization problem, showing that widening follows consistent tail-first pattern: soft-rank utilization remains near-linear while hard-rank utilization declines. This asymmetry, formalized as spectral scaling laws, reveals two efficiency failures, spectral dilution and spectral collapse, that limit naïve width growth. LayerNorm placement modulates these dynamics: Pre-LN amplifies tails, Post-LN suppresses them, and Mix-LN balances both. Together, these results highlight spectral utilization as new efficiency axis, motivating width-efficient designs via layer-wise scheduling and pruning."
        },
        {
            "title": "Limitations",
            "content": "The study is limited to English decoder-only models up to 250M parameters and does not validate spectral behavior in multilingual or encoderdecoder settings. While spectral metrics correlate with perplexity, causality remains unproven, and finer-grained subspace analysis may be needed beyond scalar metrics like SUI. Additionally, eigencomputations could pose challenges at extreme scales."
        },
        {
            "title": "References",
            "content": "Zeyuan Allen-Zhu and Yuanzhi Li. 2025. Physics of language models: Part 3.3, knowledge capacity scaling laws. In The Thirteenth International Conference on Learning Representations (ICLR). Kartik Anand, Ginestra Bianconi, and Simone Severini. 2011. Shannon and von neumann entropy of random networks with heterogeneous expected degree. Physical Review EStatistical, Nonlinear, and Soft Matter Physics. Jeremy Bernstein. 2025. Thinking Machines Https://thinkingmachines.ai/blog/modularmanifolds/. Modular manifolds. Connectionism. Lab: Vivien Cabannes, Elvis Dohmatob, and Alberto Bietti. 2024. Scaling laws for associative memories. In The Twelfth International Conference on Learning Representations (ICLR). Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, and 1 others. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794. Leshem Choshen, Yang Zhang, and Jacob Andreas. 2024. hitchhikers guide to scaling law estimation. arXiv preprint arXiv:2410.11840. Manlio De Domenico and Jacob Biamonte. 2016. Spectral entropies as information-theoretic tools for complex network comparison. Physical Review X. Gbètondji JS Dovonon, Michael Bronstein, and Matt Kusner. 2024. Setting the record straight arXiv preprint on transformer oversmoothing. arXiv:2401.04301. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. In Journal of Machine Learning Research (JMLR). Stanislav Fort. 2025. Scaling laws for adversarial attacks on language model activations and tokens. In The Thirteenth International Conference on Learning Representations (ICLR). Peiran Gao, Eric Trautmann, Byron Yu, Gopal Santhanam, Stephen Ryu, Krishna Shenoy, and Surya Ganguli. 2017. theory of multineuronal dimensionality, dynamics and measurement. BioRxiv. Quentin Garrido, Randall Balestriero, Laurent Najman, and Yann Lecun. 2023. RankMe: Assessing the downstream performance of pretrained selfsupervised representations by their rank. In International conference on machine learning (ICML). Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are keyvalue memories. In Empirical Methods in Natural Language Processing (EMNLP). Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, and 1 others. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems (NeurIPS). Yu Hu and Haim Sompolinsky. 2022. The spectrum of covariance matrices of randomly connected recurrent neuronal networks with linear dynamics. PLoS computational biology. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, and 1 others. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. 2024. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Tanishq Kumar, Zachary Ankner, Benjamin Frederick Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Re, and Aditi Raghunathan. 2025. Scaling laws for precision. In The Thirteenth International Conference on Learning Representations (ICLR). Hojoon Lee, Youngdo Lee, Takuma Seno, Donghu Kim, Peter Stone, and Jaegul Choo. 2025. Hyperspherical normalization for scalable deep reinforcement learning. In International conference on machine learning (ICML). Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, and Boris Ginsburg. 2025. nGPT: Normalized transformer with representation learning on the hypersphere. In The Thirteenth International Conference on Learning Representations (ICLR). Bochen Lyu, Di Wang, and Zhanxing Zhu. 2025. solvable attention for neural scaling laws. In The Thirteenth International Conference on Learning Representations. Anna Marbut, Katy McKinney-Bock, and Travis Wheeler. 2023. Reliable measures of spread in high dimensional latent spaces. In International Conference on Machine Learning (ICML). Charles Martin and Michael Mahoney. 2021. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research. Elliot Paquette, Courtney Paquette, Lechao Xiao, and Jeffrey Pennington. 2024. 4+3 phases of computeoptimal neural scaling laws. In The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS). Filippo Passerini and Simone Severini. 2008. The von neumann entropy of networks. arXiv preprint arXiv:0812.2597. Telmo Pessoa Pires, António Lopes, Yannick Assogba, and Hendra Setiawan. 2023. One wide feedforward is all you need. In Proceedings of the Eighth Conference on Machine Translation. Pengxiang Li, Lu Yin, and Shiwei Liu. 2025. Mix-LN: Unleashing the power of deeper layers by combining pre-LN and post-LN. In The Thirteenth International Conference on Learning Representations (ICLR). Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and 1 others. 2019. Language models are unsupervised multitask learners. OpenAI blog. Rongmei Lin, Weiyang Liu, Zhen Liu, Chen Feng, Zhiding Yu, James Rehg, Li Xiong, and Le Song. 2020. Regularizing neural networks via minimizing hyperspherical energy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. 2018. Learning towards minimum hyperspherical energy. Advances in neural information processing systems. Weiyang Liu, Rongmei Lin, Zhen Liu, Li Xiong, Bernhard Schölkopf, and Adrian Weller. 2021. Learning with hyperspherical uniformity. In International Conference On Artificial Intelligence and Statistics (AISTATS). Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. 2017. Deep hyperspherical learning. In Advances in neural information processing systems. Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. 2019. On the spectral bias of neural networks. In International conference on machine learning (ICML). Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto. 2024. Observational scaling laws and the predictability of langauge model performance. In The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS). Tim Salimans and Durk Kingma. 2016. Weight normalization: simple reparameterization to accelerate training of deep neural networks. In Advances in neural information processing systems. Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. 2024. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. In International Conference on Machine Learning (ICML). Jingzhe Shi, Qinwei Ma, Huan Ma, and Lei Li. 2024. Scaling law for time series forecasting. In The Thirtyeighth Annual Conference on Neural Information Processing Systems (NeurIPS). Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. 2025. Layer by layer: Uncovering hidden representations in language models. International conference on machine learning (ICML). Max Staats, Matthias Thamm, and Bernd Rosenow. 2024. Locating information in large language models via random matrix theory. arXiv preprint arXiv:2410.17770. Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, and Ngai Wong. 2024. Scaling laws with vocabulary: Larger models deserve larger vocabularies. In The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS). Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Tran, Dani Yogatama, and Donald Metzler. 2022. Scaling laws vs model architectures: How does inductive bias influence scaling? Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning (ICML). Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, and Weiran Huang. 2024. Diff-erank: novel rankbased metric for evaluating large language models. In Advances in Neural Information Processing Systems (NeurIPS). Table 7: Evaluation perplexity (PPL) for LLaMA models across different normalization positioning and FFN dimensions. The columns 1d, 2.67d, 4d, and 6d represent different FFN width, where is the model dimension. The unusually high PPL in PostLN LLaMA-250M indicate training instability."
        },
        {
            "title": "MixLN",
            "content": "1d 2.67d 4d 6d 1d 2.67d 4d 6d 1d 2.67d 4d 6d LLAMA-70M 38.6 LLAMA-130M 29.6 LLAMA-250M 26.7 34.2 26.4 24.5 32.4 25.8 23.3 31.1 24.6 22.5 33.6 26.7 31.1 38.2 29.2 25.1 27.1 1427.9 1431.0 1436. 32.3 25.8 38.7 29.2 26.8 33.9 26.8 24.2 32.0 25.3 23.0 30.7 24.3 22.5 (a) LLaMA-70M (PreLN) (b) LLaMA-130M (PreLN) (c) LLaMA-250M (PreLN) Figure 9: LLaMA models"
        }
    ],
    "affiliations": [
        "New York University"
    ]
}