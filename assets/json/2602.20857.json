{
    "paper_title": "Functional Continuous Decomposition",
    "authors": [
        "Teymur Aghayev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), a JAX-accelerated framework that performs parametric, continuous optimization on a wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to $C^1$ continuous fitting, FCD transforms raw time-series data into $M$ modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and a speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that a Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over a standard CNN."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . e [ 1 7 5 8 0 2 . 2 0 6 2 : r a"
        },
        {
            "title": "Teymur Aghayev",
            "content": "Vilnius Gediminas Technical University Vilnius, Lithuania teymur.aghayev@stud.vilniustech.lt Abstract The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), JAX-accelerated framework that performs parametric, continuous optimization on wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to 1 continuous fitting, FCD transforms raw time-series data into modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over standard CNN."
        },
        {
            "title": "Introduction",
            "content": "Real-world raw signals are highly non-stationary, and analyzing them requires more than just an optimized curve. Standard signal processing techniques, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), are widely used for smoothing and feature extraction but lack functional optimization and physical plausibility. While traditional optimization algorithms, such as Levenberg-Marquardt (LM), Trust Region Reflective (TRF), and LBFGS-B, can fit data with mathematical functions, they cannot be used for continuous C1 fitting between segments for full decomposition of the raw signal, which is essential for deeper analysis and diverse physical applications. Thus, our Functional Continuous Decomposition algorithm bridges the limitations of current signal processing algorithms by decomposing raw time-series data into modes with overall C1 continuity. Specifically, initial modes have higher number of segments to show local patterns, whereas higher modes reveal general patterns in the data. Each segment is fit with specified mathematical function used for decomposition. Segments maintain C1 continuity by algebraically fixing two parameters of the function. Finally, FCD can be used to express local and global patterns of data, optimized values, derivatives, and parameters of the fitted function."
        },
        {
            "title": "1.1 Main Contributions",
            "content": "This framework introduces several key contributions to the field of time-series analysis, specifically addressing the limitations of traditional signal processing algorithms. 1. Parametric fitting: Segments are fitted with specified mathematical function; the output contains an optimized fit, function derivative, an integral, and parameters. 2. Guaranteed Continuity: We introduce method to enforce C0 and C1 continuity across segment boundaries by algebraically deriving parameters, ensuring an overall continuous fit. 3. Full Configurability: Users can define custom mathematical functions (via SymPy integration [8]), initial guesses, tune segmentation, Levenberg-Marquardt (LM) parameters, and set specific C0, C1 derivative continuity settings. 4. High-Efficiency Decomposition: We demonstrate high-fidelity reconstruction with an average segment-wise SRMSE of 0.735 and processing speed of 0.47s for 1,000 data points across 6 modes, demonstrating linear computational complexity (O(n)) with respect to signal length. 5. Efficient CNN training: Integrating FCD-derived features (parameters, optimized fit, and derivatives) into CNN architecture results in 16.8% faster convergence and 2.5% increase in predictive accuracy. The full implementation and technical documentation can be accessed here."
        },
        {
            "title": "2 Background",
            "content": "The development of Functional Continuous Decomposition (FCD) is situated at the intersection of non-stationary signal decomposition and smoothing algorithms. This section shows the mechanical foundations of current traditional techniques and identifies the technical gaps that FCD aims to solve."
        },
        {
            "title": "2.1 Mode Decomposition",
            "content": "The decomposition of modes from non-stationary signals is traditionally dominated by Empirical Mode Decomposition (EMD). EMD uses recursive sifting process, which extracts Intrinsic Mode Functions (IMFs) by interpolating local extrema of the raw signal via cubic splines. Mechanically, the algorithm identifies all local maxima and minima to construct upper and lower envelopes via cubic spline interpolation. The mean of these envelopes is then subtracted from the original signal to isolate IMFs; this is done recursively until all modes are extracted. With this approach, EMD efficiently decomposes the signal into different temporal patterns. However, it lacks an analytical formulation (t), continuous derivative, and has error propagation due to its recursive nature."
        },
        {
            "title": "2.2 Local Smoothing Algorithms",
            "content": "FCD shares some similarity with local digital filters and piecewise splines, yet it extends their application for functional continuous analysis. Cubic and B-Splines are highly efficient smoothing algorithms that perform polynomial interpolation of basis functions. B-Splines use sequence of knots to define piecewise boundaries, resulting in smooth fit across the data. However, B-splines lack parametric interpretability and can only show smoothed signal fit. 2 Savitzky-Golay (SG) is non-parametric filter that performs local least-squares polynomial fit on sliding window of fixed length. SG fits polynomial and keeps only the center point of each fit. While effective for simple denoising, SG is limited by its static window architecture, with which it is not possible to show continuous decomposition, optimized fit, derivatives, and its parameters."
        },
        {
            "title": "2.3 Summary of Research Gap",
            "content": "While existing algorithms provide robust tools for either signal smoothing (B-Splines, SavitzkyGolay) or mode decomposition (EMD), there remains critical gap in providing functional decomposition of the raw signal with C1 continuity. Functional Continuous Decomposition addresses this gap by providing JAX-accelerated framework [1] for continuous and parametric signal analysis."
        },
        {
            "title": "3 Methodology",
            "content": "The Functional Continuous Decomposition consists of four main stages: dataset normalization, uniform mode segmentation, Levenberg-Marquardt (LM) optimization, and algebraic continuity enforcement. By using the JAX-accelerated LM optimization in batches while processing modes in parallel, FCD performs efficient mode decomposition across complex functions and signals."
        },
        {
            "title": "3.1 Normalization",
            "content": "Original and datasets are normalized with adaptive standard scaling using the mean (µ) and length-dependent standard deviation (σN ) of the datasets. = µ σN (1) Here, standard deviation σN depends on the dataset length (N ) to ensure constant density regardless of dataset length, which results in much higher stability of the Levenberg-Marquardt algorithm. Scaling factor sf = 0.01 is used to control the sample density: σN = σ sf (2)"
        },
        {
            "title": "3.2 Uniform Segmentation and Mode Calculation",
            "content": "The FCD framework begins by decomposing the original signal = {x1, x2, x3, . . . , xN } into hierarchical modes starting from the noisiest to the global trend mode. Each mode m, except the last, utilizes uniform segmentation, where the number of segments decreases as the algorithm progresses toward long-term trends. The total number of modes, , is determined adaptively based on the signal length . To ensure each mode captures distinct temporal pattern, the framework starts with (N/α) segments with minimum number of segments β. The number of modes is calculated using logarithmic function: (cid:24) = log2 (cid:19)(cid:25) (cid:18) N/α β + 1 (3) Where α represents the initial divisor for the number of segments (default α = 5) and β defines the minimum number of segments (default β = 4); the addition of 1 accounts for the last trend 3 mode. This logarithmic approach ensures that scales efficiently with the dataset length. For instance, generating 4 modes for = 100, 7 modes for = 1, 000, and 10 modes for = 10, 000. To generate segment boundaries for each mode, we calculate the number of segments in each mode starting from k1 = N/α. Subsequent modes follow recursive reduction, where the number of segments is halved, km = max(km1/2, β), until the last trend mode with (kM = 1). Afterwards, segment boundary indices are calculated using linear interpolation of the number of segments in each mode into the dataset range ([0, ])."
        },
        {
            "title": "3.3 Local Translation",
            "content": "To ensure high numerical stability and better physical interpretation of optimized parameters, each segments x-values are translated to local coordinates. xk represents the absolute x-value at the current segment boundary k, local x-values are calculated as: ˆx = xk (4) Local translation improves numerical stability and physical applications of parameters. Polynomial coefficients and linear offsets directly represent the signals state in the current segment, being independent of the global magnitude."
        },
        {
            "title": "3.4 Levenberg-Marquardt Optimization",
            "content": "To represent the signal within each mode m, specified general function = (x, p) is used, where is vector of parameters. To ensure continuity between segments, two parameters are fixed and excluded from during optimization; instead, they are algebraically derived from the remaining parameters in vector during the calculation of residuals. Unlike traditional non-parametric methods, this approach allows for the definition of custom models (polynomial, sinusoidal, or exponential) to reflect the underlying physics of the data with guaranteed continuity. Optimization is done with JAX-accelerated Levenberg-Marquardt algorithm [5, 6] in batches of segments; modes are fitted in parallel. General formula of LM optimization: (cid:0)JT + (λ + α)I(cid:1) = JT r, Jij = ri pj (5) The system is solved for where is the Jacobian matrix of the residuals with respect to the parameters p, λ is the dynamic damping factor, α is static ridge regularization, {1, . . . , } denotes the data point index and {1, . . . , d} denotes the parameter index. Residuals are calculated via the Sum of Squared Residuals loss function L: L(p) = (cid:88) i=1 (yi (xi, p))2 (6) The quality of the proposed step is defined as the actual reduction r: the difference between the loss function of past and proposed error at iteration (n) of the optimization. = L(pn1) L(pn) (7) Crucially, while the localized segments are fitted with the LM optimizer, the final trend mode (not segmented) is optimized using Trust Region Reflective (TRF) algorithm [2] to avoid recompilation, as our proposed LM optimizer is designed for segmental fitting."
        },
        {
            "title": "3.5 Ensuring Continuity",
            "content": "To ensure each mode is smooth across segment boundaries xk, we enforce C0 (value) and C1 (derivative) continuity by algebraically fixing two parameters. They are solved analytically based on the previous segments y-value and derivative at the segment boundary. The fixed continuity parameters are calculated from the following equations for segment > 1, and the previous segments local x-value at the segment boundary, denoted as xk1,l: (0, pk) = (xk1,l, pk1) (0, pk) = (xk1,l, pk1) The first equation is solved for fixed value parameter, and the second equation for fixed derivative parameter. In function with linear offset term (ax+b), can be used as fixed value parameter and as fixed derivative parameter for continuity between segments. This constrains the LevenbergMarquardt (LM) optimizer to only explore solutions that are physically consistent, ensuring exact continuity. Demonstration of the FCD algorithm with 6-parameter sinusoidal model (sin6) given as = (A1x + A0) sin(B0x + D) + C1x + C0. Blue points show the original dataset, red is the optimized continuous fit, and gray lines are segment boundaries. (8) (9) Figure 1: FCD example on Bitcoin 1-minute data Optimized functions for mode 5 are presented as follows: (x) = (0.263x + 56.296) sin(0.064x 1.155) + (0.475x + 29030) (1.198x + 96.122) sin(0.044x + 2.236) + (1.545x + 28917) (1.681x + 220.586) sin(0.052x 0.823) + (1.396x + 29200) (0.142x + 29.495) sin(0.059x + 1.922) + (0.163x + 29254) (1.974x + 90.332) sin(0.063x 0.870) + (3.441x + 29372) 0 < 90 90 < 180 180 < 270 270 < 360 360 449 (10) 5 Absolute x-values are given here for clarity; to reconstruct the fit, has to be locally adjusted for each segment."
        },
        {
            "title": "3.6 Forward Fit",
            "content": "However, implementing such constraints introduces instability and error propagation within each segment. For example, if batch begins with unfavorable fixed continuity parameters, optimization can become highly unstable, and the error propagates forward. We propose an Overlapping Forward Fit mechanism to mitigate this: + 1 segments are optimized within each batch, but the last segment is discarded from the fit; instead, it is assigned as an initial guess for the next batchs first segment. The last segment is discarded specifically to re-optimize it in the next batch while having favorable starting continuity constraints. Thus, during the optimization of one batch, LM enforces continuity from the past segment and ensures overall fit is favorable to the future segment, which efficiently solves error propagation and frequent instability problems. Overall, segmental fitting in batches is performed to ensure high stability, accuracy, and speed."
        },
        {
            "title": "3.7 Unscaling Parameters",
            "content": "After the decomposition is complete, the optimized parameters must be properly unscaled to retain physical meaning and units. We derive the unscaling formulas by substituting scaling equations for and back into the model function. For brevity, let σx and σy denote the length-dependent scaling factors σN,x and σN,y, and let subscripts and denote scaled and unscaled parameters, respectively. ys = µy σy As is translated locally for each segment, xk represents the absolute x-value at the boundary of the current segment: µx σx For example, on 6-parameter sine wave: xs = xk µx σx = xk σx ys = (A1xs + A0) sin(B0xs + D) + C1xs + C0 We substitute the scaling equations for ys and xs: µy σy = (A1 xk σx + A0) sin(B0 xk σx + D) + x xk σx + C0 The whole equation is simplified to find equations for unscaling optimized parameters: = ( A1σy σx (x xk) + A0σy) sin( B0 σx (x xk) + D) + C1σy σx (x xk) + C0σy + µy (15) Thus, unscaling equations for each parameter are defined as: A1,u = A1,sσy σx A0,u = A0,sσy B0,u = B0,s σx C1,u = C1,sσy σx C0,u = C0,sσy+µy Du = Ds (16) 6 (11) (12) (13) (14)"
        },
        {
            "title": "3.8 Computational Optimization",
            "content": "The Functional Continuous Decomposition is designed for high-speed fitting and massive datasets. This performance is achieved by using JAXs Just-In-Time (JIT) compilation and XLA (Accelerated Linear Algebra) to speed up mathematical operations. Furthermore, parallel mode fitting with JAX-accelerated Levenberg-Marquardt algorithm, Jacobian and residual functions, batched optimization, and bucketing were utilized to improve decomposition speed. Bucketing was implemented to find the best speed between compilation time and overall run time."
        },
        {
            "title": "3.9 Configurability and Presets",
            "content": "Functional Continuous Decomposition is highly configurable; wide range of mathematical functions with initial guesses can be used for decomposition. Default presets include linear, quadratic, and cubic polynomials, sinusoidal models (4,5,6,7 parameter variations), decay, Fourier sine series, Gaussian, and logistic functions with relevant initial guesses for them. For models lacking linear offset term (ax + b), the decomposition can be configured to maintain only C0 continuity for stability. Furthermore, continuity parameters to fix, the number of modes, the order of continuity, and segmentation settings can be configured. The framework allows for analytical output of derivative and integral parameters of decomposition, using numerical or analytical methods, and specifying the order of the derivative and integral."
        },
        {
            "title": "4 Results",
            "content": "In this section, the FCD framework is evaluated for accuracy and computational speed. The proposed FCD algorithm was implemented in Python 3.9 (64-bit Windows 10) using the JAX library. Benchmarks were performed on computer equipped with an Intel i7-10700 CPU, 16GB of DDR4-3200 RAM, and an NVIDIA RTX 3050 GPU."
        },
        {
            "title": "4.1 Accuracy Tests",
            "content": "Functional Continuous Decomposition was tested on 30 datasets of different volatility, structure, and scales on all default models. Test datasets include 6 cryptocurrency markets (BTC, ETH, SOL, ADA, DOGE, XRP) on second, minute, hourly, and daily time intervals [4] with non-uniform x-datasets on different scales. Furthermore, the remaining 6 datasets include 2 flat line data, linear, cubic function data, and 2 cryptocurrency datasets on 1020 and 1020 scales. The primary metric for the measurement of fit accuracy is segment-wise SRMSE calculated as follows: SRMSEk = (cid:113) 1 Nk (cid:80)Nk i=1(yi ˆyi)2 σy (17) Where yi represents the observed data, ˆyi is the model prediction, and Nk is the number of data points in the segment k. To ensure reliability of the error metrics on flat segments, segments with deviation less than 1% of the y-dataset deviation are considered flat. For such segments, SRMSE was capped at 1.0 to prevent unstable SRMSE values on flat segments, marking it as neutral fit. 7 Model Type Cubic 6-parameter sine All models Avg. SRMSE 0.774 0.568 0.735 Table 1: Accuracy Metrics of the FCD Algorithm. Global SRMSE is often an insensitive metric in time-series due to the total dataset deviation. By using Segment-Wise SRMSE, we evaluate the models ability to capture local dynamics, which is much more accurate for estimation of overall fit accuracy."
        },
        {
            "title": "4.2 Speed Tests",
            "content": "FCD was designed and optimized for high-speed decomposition. The runtime speed of the FCD algorithm primarily depends on the number of points in the dataset (N ), its structure, the complexity of the utilized function, and the quality of initial guesses. Speed was tested on different numbers of points (N = 10 to 100,000) and two specifically used cubic and 6-parameter sine functions. Table 2: Computational Performance on cubic model Number of Points (N ) First Run* Subsequent Runs* 0.014 10 1.669 100 2.247 1,000 6.659 10,000 37.116 100,000 0.004 0.024 0.469 3.568 27.382 The first run is much slower due to initial JAX Just-In-Time compilation. The framework can be compiled during initialization by using warmup; subsequent runs show the real speed of the FCD algorithm without compilation time. Table 3: Computational Performance on 6-parameter sine function Number of Points (N ) First Run Subsequent Runs 0.066 10 2.229 100 3.341 1,000 8.420 10,000 50.549 100,000 0.005 0.051 1.289 4.617 43.167 FCD runtime speed highly depends on the complexity of the function, as 6-parameter sine wave requires 2-3x more time than the cubic model."
        },
        {
            "title": "5 Applications",
            "content": "Functional Continuous Decomposition has various applications in many fields, such as physics, medicine, financial analysis, and machine learning. In the following sections, we show possible examples of FCD in car velocity, EEG signals, and efficient training of Convolutional Neural Networks."
        },
        {
            "title": "5.1 Velocity Applications",
            "content": "Figure 2: Decomposition of UAH-DriveSet velocity dataset. Figure 3: Normalized derivative of decomposition for UAH-DriveSet velocity dataset. 9 5.1.1 Velocity and Acceleration Analysis To show possible applications of the FCD algorithm, we used the UAH-DriveSet dataset [9], which records the velocity of the car throughout time. FCD with cubic model is used to decompose the velocity signal into different temporal patterns and calculate the analytical derivative of functions to analyze acceleration (Fig. 2 and 3). Using velocity decomposition, we can analyze optimized parameters of functions and get insight into the structure, local, and global patterns of the dataset. In the cubic function used to decompose the velocity dataset: Where represents initial velocity (v0), shows initial acceleration (a0), 2b shows initial jerk, 6a shows rate of change of jerk. v(t) = at3 + bt2 + ct + (18) Table 4: Comparison of Velocity Functions across FCD Modes and Segments Mode Segment Time Range (s) Fitted Velocity Function (t) Driving Behavior 2 4 5 20 12 7 198.80 208.82 464.87 506.82 541.81 630.83 0.057t3 + 0.666t2 + 1.761t + 90.31 0.0007t3 + 0.0539t2 0.564t + 90.40 1.09 105t3 0.003t2 0.052t + 111.0 Rapid spike Gradual recovery Full decrease As observed, local translation of in each segment correctly highlights physical interpretability, which otherwise would be distorted with absolute t-scales. It is important to note that the t-values provided are absolute for clarity. Offsets clearly show the initial velocity of each segment with polynomial coefficients correctly highlighting driving behaviour (0.666t2, 1.761t on rapid spike, 0.564t, 0.0539t2 on recovery, and 0.052 on full decrease). While the absolute magnitude of polynomial parameters decreases in long-term modes due to the higher segments t-span and local translation, their relative influence remains the same. We demonstrate this most effectively in the derivative plot  (Fig. 3)  , where relative colorbar is used to normalize derivatives across each mode, clearly showing that the relative influence of each mode is the same, despite absolute scales being different. Analytic derivatives can be used to analyze acceleration, for example, in mode 4, segment 1 (t = 7.850 to 48.830s, initial rise) derivative shows the following function: a(t) = v(t) = 0.00426t2 + 0.185t 0.229 (19) Here, 0.229 shows initial acceleration (a0), 0.185 shows initial jerk, which correctly represents the initial spike, and 0.00852 (which is 0.00426 2) shows the rate of change of jerk. The integral of optimized functions was not shown as the velocity data is positive, and thus, the integral is monotonic and less useful for analysis. The best example of integration is shown later in EEG applications, where integration shows net voltage across different modes."
        },
        {
            "title": "5.2 Application in EEG signals",
            "content": "Our algorithm was applied to EEG signals [10] with 6-parameter sine model to show different patterns, ensuring physical C1 continuity. Optimized frequency and amplitude parameters were used to estimate the EEG signal, and the integral of the EEG decomposition was additionally presented. To ensure physical plausibility, custom fitting was used to set strictly non-negative bounds for amplitude and frequency, which cannot be physically negative. 10 Figure 4: Decomposition performed on EEG data with sine model. From the following decomposition, we extracted frequency (b0) and amplitude (a0) to analyze patterns in optimized parameters and EEG data: Figure 5: Optimized Frequency for EEG dataset. 11 Figure 6: Optimized Amplitude for EEG dataset. The frequency plot  (Fig. 5)  illustrates that each mode successfully captures distinct frequency. The initial modes mostly capture signal noise and thus, show very high frequency bands; frequency decreases in higher modes as they provide more general patterns. Amplitude plot  (Fig. 6)  on the other side, reveals different structure: high-frequency modes exhibit lower amplitudes due to the higher number of segments required to fit rapid fluctuations. Interestingly, Mode 3 demonstrates the highest amplitude, representing balance between local fitting and signal generalization. Higher modes show decrease in amplitude as they capture broader, global trends. Additionally, optimized functions were analytically integrated to show Net Voltage (µ s): Figure 7: Integral of decomposition for EEG dataset. 12 As observed, the integral correctly reflects accumulated voltage and provides deeper insight into the underlying structure of EEG data. From the integral plot, we can see that initially, until (t = 0.50), net EEG voltage was trending negative, after which it started increasing and stayed mostly positive until (roughly = 1.6), where net voltage returned to negative. Initial modes clearly show high-frequency changes, whereas higher modes generalize the accumulated voltage. To ensure physical applicability of integrals, for each segment s, running integration logic is used to adjust and calculate integral values by the Cumulative Constant Ct representing the definite integral of previous segments. An integral formula for mode 3, segment 8 (t = 0.451 to 0.515, gradual recovery) is given as: (cid:90) (t)dt = 75.3t2 0.35t + 1.74t cos(100t 1.34) 0.0174 sin(100t 1.34) 0.0679 cos(100t 1.34) + Ct (20)"
        },
        {
            "title": "5.3 Application for efficient CNN training",
            "content": "Functional Continuous Decomposition can be used to provide Convolutional Neural Networks (CNN) with optimized curves, parameters, and analytical derivatives derived from decomposition. We tried to integrate FCD features into standard CNN to improve prediction accuracy by giving it different patterns. We conducted comparative test between standard CNN and the FCD-enhanced CNN architecture. Both models used lookback window of 60 points to predict the next 30 values; light complexity network was used for easier testing [7]. For architectural fairness, the FCD-CNN was designed with two branches: one branch is identical to the architecture of standard CNN with processing raw signals, optimized decomposition, and its derivative from FCD, while secondary branch processes the optimized parameters from decomposition. The models were evaluated across two datasets: UCI Household Power Demand [3] and EEG dataset [10]. cubic model was used for Household Power tests, and 6 parameter sine wave for EEG tests. We performed study using training set sizes of 5,000, 10,000, and 20,000 samples. These datasets were processed into 983, 1, 981, and 3, 983 training windows with stride (step) of 5, respectively. Furthermore, to account for the stochastic nature of weight initialization and ensure statistical accuracy, each experiment was repeated across five independent iterations using different seeds. We recorded the average Root Mean Square Error (RMSE) to measure predictive accuracy and the number of training epochs required for convergence (via Early Stopping) to measure efficiency. This experiment allows us to demonstrate that the FCD-CNN consistently accelerates and improves the learning process compared to standard feature extraction. Table 5: CNN Performance Comparison (UCI Household Power) Model Training size (N ) Avg. RMSE Avg. Epochs Avg. Time Standard CNN FCD-CNN (Cubic) Standard CNN FCD-CNN (Cubic) Standard CNN FCD-CNN (Cubic) 5,000 5,000 10,000 10,000 20,000 20,000 43.2 53. 46.0 38.2 37.2 30.8 10.6 35.2 20.7 63.5 26.9 108.4 1.2058 1. 1.0961 1.0646 0.7433 0.7450 13 Table 6: CNN Performance Comparison (EEG Dataset) Model Training size (N ) Avg. RMSE Avg. Epochs Avg. Time Standard CNN FCD-CNN (Sin6) Standard CNN FCD-CNN (Sin6) Standard CNN FCD-CNN (Sin6) 5,000 5,000 10,000 10,000 20,000 20, 7.9772 8.0280 7.8166 7.4657 8.3649 8.1831 81.8 53.0 79.8 38.0 28.8 27. 17.5 45.5 26.2 78.1 18.7 146.7 These results provide comprehensive comparison of the performance and efficiency of the standard CNN versus the FCD-CNN. At training size of = 5000, as the training dataset is smaller, the FCD-CNN architecture takes more epochs to converge, primarily due to higher complexity and low-data regime, but results in 6% lower RMSE for the household dataset, whereas for the EEG dataset, FCD-CNN required 35% fewer epochs and only negligible (0.6%) higher RMSE. Furthermore, FCD-CNN shows the most efficient performance on = 10000 training size. For the household dataset test, the model achieved 17% faster convergence, while the EEG test showed dramatic 52% reduction in training epochs; the FCD-CNN maintained an average 4% lower RMSE than the standard model. On training set of 20,000 samples, both models start to converge with nearly equal RMSE; however, FCD-CNN still showed 17% faster convergence for the household test and 3.5% for the EEG test. While the average runtime for the FCD-CNN is higher, this is caused by the overhead of the FCD decomposition. Specifically, the decomposition process requires 22s for 983 windows (N = 5000), 39s for 1981 windows (N = 10, 000), and 74s for 3983 windows (N = 20, 000). FCD feature extraction can be fully parallelized, and it provides significant speed advantage for more complex deep networks where training typically spans hours or days. Overall, the FCD-CNN consistently shows 16.8% faster convergence and 2.5% higher accuracy. By utilizing cubic model for household power and 6-parameter sine waves for EEG datasets, FCDCNN effectively uses robust physical parameters, an optimized curve, and derivatives to converge faster and more accurately. Figure 8: Epoch counts across all iterations and training set sizes for EEG comparison 14 Figure 9: Epoch counts at all iterations and training set sizes for Household Power comparison"
        },
        {
            "title": "6 Limitations and Implementation Details",
            "content": "While Functional Continuous Decomposition offers high flexibility, its stability is highly influenced by the quality of initial guesses, the complexity of the model, and stable continuity parameters. The framework provides default presets for commonly used models and initial guesses. Using unstable fixed parameters, functions, or inaccurate initial guesses can lead to slowdowns and numerical instability. When using custom settings, please adhere to our technical documentation for correct usage. It should be noted that algebraically derived parameters required for exact continuity, by definition, cannot be constrained by bounds."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper presented the Functional Continuous Decomposition framework, novel approach to decomposing non-stationary datasets with specified mathematical function into C1 continuous modes. By using JAX-accelerated Levenberg-Marquardt optimization with algebraically derived continuity parameters, FCD addresses current limitations of mode decomposition and smoothing algorithms. This framework provides completely new insight for the analysis of time-series data with different temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Experimental results demonstrate that the FCD algorithm achieves an average segment-wise SRMSE of 0.735. Furthermore, when integrated into CNN architecture, FCD-derived features enabled 16.8% faster convergence and 2.5% improvement in accuracy. Future work will focus on the real-time implementation of the FCD by improving its accuracy, speed, and extending the framework to provide Cn continuity and wider range of default functions."
        },
        {
            "title": "References",
            "content": "[1] James Bradbury et al. JAX: composable transformations of Python+NumPy programs. Version 0.3.13. 2018. url: http://github.com/jax-ml/jax. [2] Mary Ann Branch, Thomas Coleman, and Yuying Li. subspace, interior, and conjugate gradient method for masked nonlinear least squares problems. In: SIAM Journal on Scientific Computing 21.1 (1999), pp. 123. 15 [3] Georges Hebrail and Alice Berard. Individual Household Electric Power Consumption. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C58K54. 2006. [4] Igor Kroitor, Vitaly Gerasimov, and Artem Danilov. CCXT CryptoCurrency eXchange Trading Library. https://github.com/ccxt/ccxt. 2023. [5] Kenneth Levenberg. method for the solution of certain non-linear problems in least squares. In: Quarterly of applied mathematics 2.2 (1944), pp. 164168. [6] Donald Marquardt. An algorithm for least-squares estimation of nonlinear parameters. In: Journal of the society for Industrial and Applied Mathematics 11.2 (1963), pp. 431441. [7] Martın Abadi et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org. 2015. url: https://www.tensorflow.org/. [8] Aaron Meurer et al. SymPy: symbolic computing in Python. In: PeerJ Computer Science 3 (Jan. 2017), e103. issn: 2376-5992. doi: 10.7717/peerj-cs.103. url: https://doi.org/ 10.7717/peerj-cs.103. [9] Eduardo Romera, Luis M. Bergasa, and Roberto Arroyo. Need Data for Driving Behavior Analysis? Presenting the Public UAH-DriveSet. In: IEEE International Conference on Intelligent Transportation Systems (ITSC). Rio de Janeiro, Brazil, Nov. 2016, pp. 387392. doi: 10.1109/ITSC.2016.7795583. [10] Igor Zyma et al. Electroencephalograms during Mental Arithmetic Task Performance. In: Data 4.1 (2019), p. 14. doi: 10.3390/data4010014. url: https://www.mdpi.com/23065729/4/1/14."
        }
    ],
    "affiliations": [
        "Vilnius Gediminas Technical University"
    ]
}