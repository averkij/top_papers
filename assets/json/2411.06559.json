{
    "paper_title": "Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents",
    "authors": [
        "Yu Gu",
        "Boyuan Zheng",
        "Boyu Gou",
        "Kai Zhang",
        "Cheng Chang",
        "Sanjari Srivastava",
        "Yanan Xie",
        "Peng Qi",
        "Huan Sun",
        "Yu Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language agents have demonstrated promising capabilities in automating web-based tasks, though their current reactive approaches still underperform largely compared to humans. While incorporating advanced planning algorithms, particularly tree search methods, could enhance these agents' performance, implementing tree search directly on live websites poses significant safety risks and practical constraints due to irreversible actions such as confirming a purchase. In this paper, we introduce a novel paradigm that augments language agents with model-based planning, pioneering the innovative use of large language models (LLMs) as world models in complex web environments. Our method, WebDreamer, builds on the key insight that LLMs inherently encode comprehensive knowledge about website structures and functionalities. Specifically, WebDreamer uses LLMs to simulate outcomes for each candidate action (e.g., \"what would happen if I click this button?\") using natural language descriptions, and then evaluates these imagined outcomes to determine the optimal action at each step. Empirical results on two representative web agent benchmarks with online interaction -- VisualWebArena and Mind2Web-live -- demonstrate that WebDreamer achieves substantial improvements over reactive baselines. By establishing the viability of LLMs as world models in web environments, this work lays the groundwork for a paradigm shift in automated web interaction. More broadly, our findings open exciting new avenues for future research into 1) optimizing LLMs specifically for world modeling in complex, dynamic environments, and 2) model-based speculative planning for language agents."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 9 5 5 6 0 . 1 1 4 2 : r Is Your LLM Secretly World Model of the Internet? MODEL-BASED PLANNING FOR WEB AGENTS Yu Gu1,, Boyuan Zheng1,, Boyu Gou1, Kai Zhang1, Cheng Chang2, Sanjari Srivastava2, Yanan Xie2, Peng Qi2, Huan Sun1, Yu Su1 1The Ohio State University, 2Orby AI {gu.826, zheng.2372, sun.397, su.809}@osu.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Language agents have demonstrated promising capabilities in automating webbased tasks, though their current reactive approaches still underperform largely compared to humans. While incorporating advanced planning algorithms, particularly tree search methods, could enhance these agents performance, implementing tree search directly on live websites poses significant safety risks and practical constraints due to irreversible actions such as confirming purchase. In this paper, we introduce novel paradigm that augments language agents with model-based planning, pioneering the innovative use of large language models (LLMs) as world models in complex web environments. Our method, WEBDREAMER, builds on the key insight that LLMs inherently encode comprehensive knowledge about website structures and functionalities. Specifically, WEBDREAMER uses LLMs to simulate outcomes for each candidate action (e.g., what would happen if click this button?) using natural language descriptions, and then evaluates these imagined outcomes to determine the optimal action at each step. Empirical results on two representative web agent benchmarks with online interactionVisualWebArena and Mind2Web-livedemonstrate that WEBDREAMER achieves substantial improvements over reactive baselines. By establishing the viability of LLMs as world models in web environments, this work lays the groundwork for paradigm shift in automated web interaction. More broadly, our findings open exciting new avenues for future research into 1) optimizing LLMs specifically for world modeling in complex, dynamic environments, and 2) model-based speculative planning for language agents."
        },
        {
            "title": "INTRODUCTION",
            "content": "Planning (Mattar & Lengyel, 2022)the strategic search for optimal action sequences to achieve goals from initial stateshas been fundamental to artificial intelligence since its inception, driving remarkable breakthroughs including superhuman performance in games like Go (Feng et al., 2023; Silver et al., 2016). Recent advances have demonstrated that integrating large language models (LLMs) with advanced planning algorithms (e.g., Yao et al. (2023a); Hao et al. (2023); Gu et al. (2023); Wang et al. (2024); Feng et al. (2023); Brown et al. (2024)) substantially enhances their performance on complex reasoning tasks beyond chain-of-thought (CoT) (Wei et al., 2022) approaches, with OpenAIs o1 (OpenAI, 2024b) serving as prominent example. These methods effectively scale inference-time compute and enable LLMs to explore multiple potential solution paths, which ultimately lead to more accurate outcomes. Alongside these developments, research into generalist web agents capable of planning and executing sequence of actions to complete complex tasks across diverse websites has garnered significant interest (Deng et al., 2023; Zhou et al., 2023; Zheng et al., 2024; Koh et al., 2024a), partly due to the webs potential as complex yet realistic environment for driving agent research and development. However, applying existing planning algorithms to the online web environment presents formidable challenges. Chief among these challenges are the inherent safety risks associated with live website Equal contribution. 1Github: OSU-NLP-Group/WebDreamer 1 Figure 1: Schematic illustration of different strategies for web agents formulated as search problem. Each node represents webpage. (a) Reactive: The agent selects locally optimal actions without forward planning, often leading to suboptimal outcomes. (b) Tree search with real interactions: The agent explores multiple paths through active website navigation and permits backtracking (indicated by dashed arrows). However, in real-world websites, backtracking is often infeasible due to the prevalence of irreversible actions. (c) Model-based planning: The agent simulates potential outcomes (illustrated by cloud-bordered nodes) to determine optimal actions prior to real-world execution, thus minimizing actual website interactions while maintaining effectiveness. For visual clarity, only one-step simulated outcomes are depicted. Faded nodes indicate unexplored webpages, while green checkmarks and red crosses denote successful and unsuccessful outcomes, respectively. interactions (Liao et al., 2024), such as inadvertently submitting forms with sensitive information or triggering unintended transactions. These risks become even more pronounced when employing tree search algorithms (Koh et al., 2024b; Putta et al., 2024), as their exhaustive exploration can expose the agent to hidden vulnerabilities and unforeseen scenarios. Additionally, many online actions, such as confirming purchase or sending an email, are irreversible, which further makes backtrackinga crucial component of planning algorithmshighly challenging, if not infeasible. One promising solution to address these challenges is model-based planning (Pascanu et al., 2017; Moerland et al., 2023), which equips agents with the ability to simulate interactions using world modela computational representation of environment dynamics. By simulating action sequences within this virtual environment, agents can explore potential outcomes safely, without directly interacting with live websites. This approach not only reduces safety risks but also preserves the agents capacity to explore and plan. Yet, the true challenge lies in creating versatile world model that can faithfully capture the landscape of the ever-evolving Internet. While previous research demonstrates that LLMs can function as effective world models in simplistic settings like blocksworld (Hao et al., 2023) and gridworld (Kim et al., 2024), bolder question emerges: Can LLMs rise to the challenge of modeling the vast, dynamic Internet? With their extensive pre-trained knowledgespanning web structures, protocols, and user behaviorsLLMs are uniquely positioned to take on this task. Building on these insights, we present WEBDREAMER, pioneering framework that leverages LLMs as world models to navigate the web (Figure 1). At the core of WEBDREAMER lies the concept of dreaming: before committing to any action, the agent uses the LLM to imagine the outcome of each possible step, expressed as natural language descriptions of how the state would change. These simulated outcomes are then evaluated based on their progress toward achieving the task objective. The most promising action is executed, and the process is repeated iteratively until the LLM determines that the goal has been reached (Section 4). To validate the effectiveness of WEBDREAMER, we evaluate it on two representative benchmarks that support online interaction: VisualWebArena (Koh et al., 2024a) and Mind2Web-live (Pan et al., 2024b). WEBDREAMER achieves substantial performance gains over reactive agents on both benchmarks, underscoring its practical value despite its conceptual simplicity. While tree search with actual interactions shows slightly superior performance on VisualWebArena, which features controlled environment of three locally hosted websites, this method is rarely feasible in practical applications, given its inherent limitations regarding safety risks and the potential for irreversible actions 2 in real-world websites. In contrast, our simulation-based approach offers more flexible solution, balancing performance gains with practical applicability in real-world web navigation tasks. In summary, our work introduces new direction for AI planning in complex, real-world environments like the web using world models simulated by LLMs. With WEBDREAMER, we tackle the dual challenges of safety and complexity in web navigation. Our results validate the potential of LLM-based world models for planning in complex web environments and highlight new opportunities for optimizing LLMs as world models and improving model-based planning algorithms for language agents."
        },
        {
            "title": "2.1 WEB AGENTS",
            "content": "Driven by the goal of automating tedious and repetitive web-based tasks, web agents powered by (multimodal) language models have made substantial progress in various aspects. Benchmarks have evolved from MiniWoB++ (Shi et al., 2017; Liu et al., 2018) to WebShop (Yao et al., 2022) and WebArena (Zhou et al., 2023), offering increasingly realistic website simulations. VisualWebArena (Koh et al., 2024a) and Mind2Web (Deng et al., 2023) challenge models ability to handle visual information and generalize across diverse tasks, websites, and domains. Reactive Agents. Reactive agents make decisions based on immediate observations from the environment without performing any search or simulation of future actions, typically implemented with the ReAct framework (Yao et al., 2023b). Much progress has been made to enhance the fundamental capabilities of reactive web agents through both prompting closed-source models (Zheng et al., 2024; He et al., 2024; Deng et al., 2023) and training models using HTML and webpage screenshots (Lee et al., 2023; Gur et al., 2023; Furuta et al., 2023; Hong et al., 2024; Baechler et al., 2024). Additionally, models abilities to ground web agent actions to elements have been improved through training on action-coordinate pair data (You et al., 2024; Cheng et al., 2024). Further advancements have been achieved by training on web agent trajectories, utilizing both human-annotated trajectories (Shaw et al., 2023; Hong et al., 2024; Deng et al., 2023; Lai et al., 2024) and synthesized exploration trajectories (Furuta et al., 2023; Song et al., 2024; Patel et al., 2024). However, reactive agents inherently suffer from short-sightedness, which can often lead to suboptimal performance in multi-step decision making. Agents with tree search. Pan et al. (2024a) introduces reward model based on GPT-4V, designed to provide both step-wise and trajectory-level rewards to guide inference-time search. Search Agent (Koh et al., 2024b) investigates inference-time search algorithms in interactive web environments, enabling explicit exploration and multi-step planning. In contrast to Search Agent, which employs variant of best-first tree search, AgentQ (Putta et al., 2024) and WebPilot (Zhang et al., 2024) utilize Monte Carlo Tree Search (MCTS) as their primary search strategy. While tree search on websites has demonstrated significant improvements, it still presents several limitations. First, the search process substantially increases inference time due to the need for extensive exploration, which is difficult to parallelize given its inherently sequential nature. Backtracking to previous states is essential for search-based methods but impractical on real-world websites. Koh et al. (2024b) addressed this in sandbox environments by storing action sequences to resume states after resetting the environment. However, resetting the environment or undoing action sequences is not feasible on live websites. Finally, the extra explorations introduced by search algorithms substantially amplify the risk of destructive actions that may irreversibly alter the websites state, potentially causing harmful side effects. 2.2 WORLD MODELS World models, cornerstone of model-based reinforcement learning (Moerland et al., 2023) since the introduction of Dyna by Sutton (1991), are typically trained on observed state transitions to predict future states and rewards. These world models enable efficient training through simulated experiences, reducing environmental interactions and improving sample efficiency (Ha & Schmidhuber, 2018). Beyond their role in training, researchers have explored the use of world models to 3 facilitate planning (Pascanu et al., 2017; Schrittwieser et al., 2020). Fundamentally, world models in reinforcement learning often involve task-specific training, with primary focus on enhancing data efficiency in the agent learning process. In contrast to traditional world models in reinforcement learning, LLMs employed as world models primarily focus on facilitating decision-making in planning rather than training. This distinction leads LLM-based models to prioritize key task abstractions over the high-fidelity simulations typically required in reinforcement learning. Recent research has demonstrated the potential of LLMs as world models for simple environments, leveraging their encoded broad world knowledge (Hao et al., 2023; Kim et al., 2024). Our study aims to advance this field by investigating the capabilities of LLM-based world models in more complex real-world environments, specifically diverse websites. concurrent work (Chae et al., 2024) also explores augmenting web agents with LLMsimulated action outcomes, however, their focus is on data collection to train an open-weights LLM, while ours centers on understanding the potential of this new paradigm using advanced LLMs such as GPT-4o (OpenAI, 2024a)."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "3.1 TASK FORMULATION Web agents tasked with automating activities in live websites confront vast and complex search spaces. Formally, each task with task instruction can be framed as partially observable Markov decision process (POMDP): (S, A, O, T, R, Ω), where represents the set of all possible states of the environment, represents all possible actions the agent can take, represents the set of possible observations from the environment, : represents the state transition function, is binary reward denoting whether the task specified in has been completed or not, and Ω : is deterministic function that projects state to an observation. The goal of the task is to execute sequence of actions that achieves reward of 1. Table 1: Action space for web navigation defined in VisualWebArena (Koh et al., 2024a). Action Type Description Click on elem. Hover over elem. click [elem] hover [elem] type [elem] [text] Type text into elem. press [key comb] goto [url] go back go forward new tab tab focus [index] tab close scroll [up/down] stop [answer] Press key combo. Go to url. Click back. Click forward. Open new tab. Focus on the i-th tab. Close current tab. Scroll up or down. End with an output. In practical scenarios, the environment is partially observable due to the complexity of web environments. The true state encompasses server-side variables, dynamically loaded content, hidden UI elements, and is subject to network conditions and browser limitations. Consequently, the agent can only perceive the environment through limited viewport (i.e., an observation O), which represents an incomplete projection of the true system state. The observation space typically manifests as screenshots or text-based accessibility trees, reflecting common implementation practices. This constrained observability naturally shapes the action space A, which comprises operations executable on interactable elements within o, such as element clicks, text input, and URL navigation  (Table 1)  . 3.2 PLANNING THROUGH SIMULATION Planning an optimal action sequence through tree search using real interactions governed by is costly and risks irreversible actions. Model-based planning addresses these challenges by using computational representation of the environment to simulate interaction outcomes. Instead of executing actions in the real environment, the agent leverages an approximate model to predict state transitions, enabling efficient exploration and evaluation of action sequences without real-world interactions. While offline planning can compute entire action sequences before execution in deterministic environments like BlocksWorld (Hao et al., 2023), web environments are too complex for such long-term prediction. This necessitates online planning approaches that interleave planning and execution, computing one action at time. One prominent approach is Model Predictive Control (MPC; Garcia et al. (1989)), which iteratively simulates future trajectories to select actions. At each state s, MPC simulates trajectories over 4 Figure 2: Illustration of WEBDREAMER using the LLM to simulate the outcome of each candidate action. The LLM simulates trajectories in natural language descriptions for three candidate actions: (1) Click Office Products, (2) Click Electronics, and (3) Type Disk into textbox. Through these simulations, each resulting trajectory is scored to identify the action most likely to succeed. In this case, the LLM selects Click Click Electronics as the optimal step and executes it. Each dotted box represents an LLM-generated state description after each simulated action. This example demonstrates two-step planning horizon. finite horizon for each possible action using simulator function sim(s, a) and evaluates them using scoring function score(τ ). The action leading to the most promising trajectory is then executed: = arg maxaA score(sim(s, a)). This process repeats after observing the new state, allowing the agent to adapt its plan based on actual outcomes while avoiding costly real-world exploration. In reality, we cannot access the real state due to partial observability, as result, we instead do sim(o, a) using the observation = Ω(s)."
        },
        {
            "title": "4 WEBDREAMER: MODEL-BASED PLANNING FOR WEB AGENTS",
            "content": "In this paper, we propose WEBDREAMER, pioneering approach leveraging LLMs as world models to enable efficient planning in complex digital environments. Our approach is motivated by the observation that web interfaces, despite their complexity, are designed to be predictable for human users. When browsing websites, humans can effectively anticipate action outcomes based on visual cues and common design patternsclicking Submit button leads to form submission, selecting product image navigates to its detail page. Given that LLMs are trained on vast amounts of web-related data, we hypothesize that they have acquired sufficient knowledge to simulate the consequences of user actions, potentially serving as effective world models for planning. 4.1 CORE DESIGN WEBDREAMER follows the planning through simulation paradigm introduced in Section 3.2. Figure 2 illustrates this process with three candidate actions, where WEBDREAMER simulates two-step trajectories for each action, selects the trajectory with the highest score, and executes its corresponding initial action. At its core, WEBDREAMER leverages an LLM to implement both the simulation function sim and the scoring function score. Implementation for sim: Our implementation of sim consists of two modules: one predicts state changes after action execution, approximating , while the other imagines possible action based on the predicted state. Together, these two modules generate trajectories of length H, where 5 is configurable horizon parameter (i.e., the simulation depth). Specifically, to represent the state changes, we prompt the LLM to generate concise natural language description focusing only on the effects of the action. For example, in Figure 2, the LLM would output short description as follows when prompted to predict the effect of executing the action Click Electronics: Click Electronics The Electronics category will display three sub-categories: Computers & Accessories, Accessories & Supplies, and Car & Vehicle Electronics. Based on this predicted state, the LLM then imagines the next action (i.e., Click Computers & Accessories), which leads to another state change prediction. This process generates trajectory of horizon = 2. Implementation for score: After collecting trajectory τi simulated from each candidate action ai using sim, we further use the LLM as scoring function for each simulation. Following Koh et al. (2024b), we prompt the LLM to evaluate each simulated trajectory with threescale responsecomplete (1.0), on track (0.5), or incorrect (0)indicating its progress toward task completion. The final score is computed by averaging multiple samples of these evaluations. In addition to sim and score, prerequisite to planning is candidate action generation. We employ two-stage approach: first sampling top-k actions following Koh et al. (2024b), then using LLM to self-refine unnecessary actions for simulation. This selfrefinement step is motivated by our observation that at different steps, the same can introduce varying degrees of irrelevant actions some steps naturally have fewer plausible actions than others. We show the pseudo code of WEBDREAMERs overall design in Algorithm 1. termination check verifies if the model outputs stop action, reaches max steps, or repeats an action over 3 times, also following the implementation by Koh et al. (2024b). Algorithm 1: WEBDREAMER Input: Instruction I; initial observation o0 Output: Sequence of actions a0, a1, . . . , aT 0; while True do At get candidate(I, ot); self refine(At); at = arg maxaA ot+1 execute(at); + 1; if termination check() = True then score(sim(ot, a)); break; end end Return result; All system prompts used in WEBDREAMER can be found in Appendix A. 4.2 DISCUSSION To justify our design choices in light of our goala pioneering study on using LLMs as world models for web environmentswe discuss three key considerations: State change description instead of HTML/Accessibility Tree. While we use natural language descriptions to capture state changes, an alternative is to prompt the LLM to predict the HTML or accessibility tree of the resulting page. However, since most webpage elements remain unchanged after an action, predicting the entire page structure is unnecessarily wasteful. Moreover, such concrete predictions are more prone to hallucinationHTML requires precise details about the website, whereas state descriptions need only capture the essential changes. For our pioneering study, we embrace this simpler, more intuitive representation, though we make no claims about its strict superiority over HTML or accessibility trees (see Section 6.1 for detailed analysis). Prompting instead of fine-tuning. In this work, we implement WEBDREAMER through direct prompting of state-of-the-art LLMs (i.e., GPT-4o (OpenAI, 2024a)) without fine-tuning. Our rationale is straightforward: we aim to first establish the feasibility of using advanced LLMs as world models for web environments and their effectiveness in planning. Demonstrating promising results with this approach will lay the foundation for future work on optimizing this direction through finetuning OSS models on targeted datasets. Straightforward MPC-based planning instead of MCTS. We adopt relatively straightforward MPC-based planning algorithm rather than more sophisticated approaches like MCTS that have been prominent in recent LLM planning research (Hao et al., 2023; Feng et al., 2023). This choice is motivated by our empirical findings: increasing the planning horizon of WEBDREAMER yields diminishing returns, which suggests the current limitations of LLMs in accurately modeling multistep trajectories (see Section 6.1). Given our goal of exploring LLMs as world models for web environments, this simpler approach suffices to demonstrate the key insights while acknowledging the current capabilities of LLMs."
        },
        {
            "title": "5.1 SETUP",
            "content": "To properly test our planning frameworks real-world performance, we use benchmarks with online evaluation, capturing the dynamic nature of web interactions. We focus on two representative benchmarks: VisualWebArena (VWA; Koh et al. (2024a)), which emphasizes multimodal setting, and Mind2Web-live (Pan et al., 2024b), which operates with HTML by default. VWA comprises In contrast, 910 tasks across three locally hosted websites: Shopping, Classifieds, and Reddit. Mind2Web-live includes 104 tasks spanning 69 real-world websites. We adhere to the default settings of both benchmarks: for VWA, we use screenshots with Set-of-Marks prompting as the observation space, while for Mind2Web-live, we use HTML. For our LLM, we choose the most advanced multimodal LLM available, GPT-4o, as it best serves our aim to pioneer model-based planning with LLMs and explore the full potential of this envisioned paradigm. In our experiments, we empirically set the planning horizon to 1. comprehensive analysis of this parameter is presented in Section 6.1. To demonstrate the effectiveness of our proposal, we primarily compare our approach with two major baselines: the reactive agent and the tree search agent with real interactions.2 While we can readily implement our own method for both benchmarks, for the tree search baseline (Koh et al., 2024b), we can only compare with it on VWA, because of the infeasibility of doing tree search in real-world websites in Mind2Web-live. Specifically, in VWA, Koh et al. (2024b) keeps track of the sequences of actions to get to states in previous trajectories. During backtracking, they reset the sandbox and re-execute the action sequence to restore the state. However, resetting the environment to undo effects is not always feasible in real-world websites featured in Mind2Web-live. 5.2 MAIN RESULTS Effectiveness. We present the overall performance results in Table 2. WEBDREAMER demonstrates substantial improvements over the reactive agent on both VWA and Mind2Web-live datasets. Notably, on the VWA dataset, our proposed method achieves 33.3% relative performance gain. Meanwhile, our proposal still underperforms the tree search baseline in terms of overall success rate. Despite these improvements, our approach still falls short of the tree search baseline in terms of overall success rate. However, it is crucial to emphasize that tree search is not practical option for real-world websites, whereas WEBDREAMER provides more flexible and adaptive alternative. On Mind2Web-live, WEBDREAMER outperforms the reactive baseline by 2.9% (a relative gain of 13.1%), which is less significant than the improvement on VWA. However, it is worth noting that the Mind2Web-live dataset does not offer as much discriminative power, as evidenced by the minimal performance differences across multiple base LLMs shown in Table 2. The strong results on both VWA and Mind2Web-live indicate the effectiveness of our method across different observation settings. We further conduct more granular analysis comparing our proposed method to the reactive baseline on the VWA dataset across multiple dimensions. Table 3 demonstrates that our model-based planning approach consistently outperforms the reactive baseline across all websites and task difficulty levels. On tasks of medium difficulty according to the official annotation by VWA, model-based planning even surpasses the performance of tree search (i.e., 22.2% vs. 24.1%). Despite its promise, model-based planning still struggles with hard tasks in VWA that necessitate multistep simulations. 2We will refer tree search with real interactions simply as tree search in our experiments for brevity. 7 Table 2: Results on VisualWebArena and Mind2Web-live. WEBDREAMER significantly outperforms the reactive baseline and falls only slightly short of the tree search baseline on VWA while requiring far fewer website interactions. For Mind2Web-live, implementing tree search algorithms poses significant challenges due to the requirement for website backtracing, leading us to omit tree search performance metrics. This limitation further underscores the flexibility of our model-based planning method. We also include additional baselines (denoted by gray cells) to provide broader context. While these comparisons may not directly assess our core hypothesis, they offer valuable background for understanding our methods performance in the web navigation landscape. We run the reactive baseline on VWA by ourselves because local hosting requirements may lead to hardware-dependent performance variations. Benchmark Observation Method Completion Rate Success Rate VisualWebArena Screenshot+SoM Mind2Web-live HTML Gemini-1.5-Pro + Reactive (Koh et al., 2024a) GPT-4 + Reactive (Koh et al., 2024a) GPT-4o + Reactive (Koh et al., 2024a) GPT-4o + Tree Search (Koh et al., 2024b) GPT-4o + WEBDREAMER GPT-4 + Reactive (Pan et al., 2024b) Claude-3-Sonnet + Reactive (Pan et al., 2024b) Gemini-1.5-Pro + Reactive (Pan et al., 2024b) GPT-4-turbo + Reactive (Pan et al., 2024b) GPT-3.5-turbo + Reactive (Pan et al., 2024b) GPT-4o + Reactive (Pan et al., 2024b) GPT-4o + WEBDREAMER - - - - - 48.8% 47.9% 44.6% 44.3% 40.2% 47.6% 49.9% 12.0% 16.4% 17.7% 26.4% 23.6% ((33.3%) 23.1% 22.1% 22.3% 21.1% 16.5% 22.1% 25.0% ((13.1%) The accuracy of simulations diminishes as the number of steps increases, presenting significant challenge for handling hard tasks. Table 3: Success rate breakdown based on different dimensions. γ = SRWEBDREAMERSRreactive measures SRtree searchSRreactive the extent to which WEBDREAMER narrows the gap between the reactive agent and the tree search agent. (a) Websites (b) Task Difficulty Websites Reactive Tree Search WEBDREAMER γ Difficulty Reactive Tree Search WEBDREAMER γ Classifieds Reddit Shopping 16.8% 15.3% 19.4% 26.5% 20.5% 29.0% 22.6% 18.6% 26.5% 59.8% 63.5% 74.0% Easy Medium Hard 28.8% 16.4% 10.7% 42.3% 22.2% 14.9% 37.4% 24.1% 12.7% 63.7% 132.8% 47.6% Efficiency. Another key advantage of model-based planning is its efficiency compared with tree search using actual explorations. As shown in Table 4, tree search requires approximately three times more steps than the baseline across all environments, whereas our method maintains comparable action steps. Notably, tree search introduces about ten times more wall clock latency due to the extra actions and backtracking, while the simulation overhead in our approach is minimal and can be further reduced with increased parallelization. Table 4: Action steps and wall clock time on VWA. (a) Number of Action Steps (b) Task Completion Wall Clock Time Steps Reactive Tree Search WEBDREAMER Seconds Reactive Tree Search WEBDREAMER Classifieds Reddit Shopping 3.4 5.1 4.5 9.9 13.6 11.4 4.1 5.2 4.5 Classifieds Reddit Shopping 68.3 83.5 87.7 749.2 972.1 785.7 183.6 233.7 179.4 8 Figure 4: We demonstrate the performance on subset of the VWA dataset, varying both the state representation within simulations and the planning horizon. Planning with long horizon with simulation remains challenging, regardless of the state representation employed."
        },
        {
            "title": "6 ANALYSES",
            "content": "6.1 STATE REPRESENTATION AND PLANNING HORIZON Our model-based planning approach relies on two critical dimensions for simulation: the state representation and the planning horizon (i.e., the simulation depth). To gain deeper insights into its effectiveness and limitations, we investigate how various configurations affect the final performance. Given the high computational cost of these experiments, we conduct this analysis using subset of the VWA dataset, comprising 100 shopping tasks with officially annotated human trajectories. In addition to the state change description used in our primary experiments, we explore alternative approaches where GPT-4o predicts either the HTML code or the accessibility tree of the resulting webpage within the simulation. For each of these state representations, we evaluate planning horizons of 1, 2, and 3 steps. As depicted in Figure 4, all three state representations significantly outperform the reactive baseline. However, their effectiveness diminishes as the planning horizon extends to 3 steps, indicating common limitation in long-horizon simulation across these approaches. Specifically, the action proposal within the simulation tends to hallucinate relevant actions for task completion, even when such actions may not exist in the current state predicted by the LLM. Notably, the state change representation exhibits the most pronounced performance degradation as planning horizons extend. This decline is particularly severe with planning horizon of 3, where performance falls below that of the reactive baseline. This vulnerability stems from its implicit specification of available interactive elements on the current webpage, requiring the model to infer these elements by applying changes to the initial state. In contrast, HTML and accessibility tree representations provide explicit element information. Consequently, the state change approach is more susceptible to hallucination during extended simulations. Despite this limitation, the state change approach remains viable choice given the current capabilities of LLMs. It matches the performance of HTML and accessibility tree representations for planning horizons less than 3 while consuming fewer output tokens. 6.2 ABLATION STUDY To determine if the observed improvements come from specific parts of our model-based planning approach, we perform ablation studies on the simulation and self-refinement stages, using the same subset from Section 6.1. We pay special attention to the Figure 3: Ablation study on the simulation stage and self-refinement stage. simulation stage, which is the core of model-based planning. One might argue that the primary improvement stems from reranking candidate actions, irrespective of whether this ranking relies on simulation. To test this idea, we conduct an experiment where we remove the simulation stage completely and instead ask the reward model to directly evaluate each candidate action. As shown in Figure 3, this modified reranking approach does lead to some improvement over the ractive baseline, but the gain is small and still falls well behind WEBDREAMER. These results confirm that the LLMbased world model simulation plays crucial role in the planning process. Furthermore, we observe decrease in performance when removing the self-refinement stage. Upon closer examination, we find that this decline is primarily due to the self-refinement modules ability to effectively filter out less relevant candidate actions when the next optimal action is clear. In contrast, directly simulating all actions may introduce additional noise that can negatively impact performance."
        },
        {
            "title": "6.3 CASE STUDY",
            "content": "To clarify the role of simulation in planning, we present case study covering both positive and negative examples. This illustrates how simulation aids the agent in exploring the environment, as well as how inaccuracies in simulation can lead to incorrect predictions. Detailed examples are provided in Appendix B."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we demonstrate the strong potential of using LLMs as world models to support planning in complex environments. Specifically, our model-based planning approach, WEBDREAMER, shows substantial improvement over reactive baselines and offers greater flexibility than tree search, which is often impossible in real-world websites. As pioneering effort in this area, our work opens new avenues for model-based planning with LLM-simulated world models. Future work can focus on further optimizing LLMs as world models for complex environments and developing more robust model-based planning algorithms for long-horizon planning."
        },
        {
            "title": "LIMITATIONS",
            "content": "Our study, as pioneering exploration of MPC-based planning with LLMs for web navigation, naturally comes with several limitations, which are also exciting future research directions: Simplicity of Planning Algorithm. In this preliminary work, we deliberately employed straightforward planning algorithm to demonstrate the core potential of our approach. While effective, this simplicity leaves ample room for future enhancements. More sophisticated planning techniques, such as Monte Carlo Tree Search (MCTS), could be integrated to further improve performance. As foundational study, our focus was on establishing the viability of the concept rather than optimizing every aspect of the system. This strategic choice allows future research to build upon our findings and explore more advanced planning strategies within the framework weve established. Computational Cost. Our current implementation, utilizing state-of-the-art models like GPT-4o, incurs non-trivial API costs (approximately $1 per task on VWA). This cost reflects our prioritization of exploring the full potential of LLM-based planning without immediate constraints. For practical applications, future work could investigate cost-effective alternatives such as fine-tuning specialized models for simulation tasks. This sets benchmark for future optimizations that balance performance and efficiency. These limitations underscore the nature of our work as proof of concept, opening up numerous avenues for future research and optimization. By establishing the foundational potential of MPCbased planning with LLMs, we have laid the groundwork for new planning paradigm for LLMbased language agents, inviting further innovations that can refine and extend model-based planning."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We would like to extend our appreciation to colleagues from the OSU NLP group and Orby AI for their insightful comments. This work is supported in part by Orby AI and ARL W911NF2220144. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notice herein."
        },
        {
            "title": "REFERENCES",
            "content": "Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. Screenai: vision-language model for ui and infographics understanding. ArXiv preprint, abs/2402.04615, 2024. URL https://arxiv.org/abs/2402.04615. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. ArXiv preprint, abs/2407.21787, 2024. URL https://arxiv.org/abs/2407.21787. Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong In Proceedings Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 93139332, Bangkok, Thailand, 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.505. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, In Alice Oh, Tristan Nauand Yu Su. Mind2web: Towards generalist agent for the web. mann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - URL http://papers.nips.cc/paper_files/paper/2023/ 16, 2023, 2023. hash/5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_ Benchmarks.html. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. ArXiv preprint, abs/2309.17179, 2023. URL https://arxiv.org/abs/2309.17179. Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. ArXiv preprint, abs/2305.11854, 2023. URL https://arxiv.org/abs/2305.11854. Carlos Garcia, David Prett, and Manfred Morari. Model predictive control: Theory and practicea survey. Automatica, 25(3):335348, 1989. Yu Gu, Xiang Deng, and Yu Su. Dont generate, discriminate: proposal for grounding language models to real-world environments. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 49284949, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.270. URL https://aclanthology.org/2023.acl-long.270. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. ArXiv preprint, abs/2307.12856, 2023. URL https://arxiv.org/abs/ 2307.12856. David Ha and Jurgen Schmidhuber. World models. ArXiv preprint, abs/1803.10122, 2018. URL https://arxiv.org/abs/1803.10122. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. ReaIn Houda Bouamor, Juan Pino, soning with language model is planning with world model. and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 81548173, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.507. URL https://aclanthology.org/2023. emnlp-main.507. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. ArXiv preprint, abs/2401.13919, 2024. URL https://arxiv.org/abs/2401.13919. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Doyoung Kim, Jongwon Lee, Jinho Park, and Minjoon Seo. Cognitive map for language models: Optimal planning via verbally representing the world model. ArXiv preprint, abs/2406.15275, 2024. URL https://arxiv.org/abs/2406.15275. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. ArXiv preprint, abs/2401.13649, 2024a. URL https://arxiv.org/abs/2401.13649. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language model agents. ArXiv preprint, abs/2407.01476, 2024b. URL https://arxiv.org/abs/ 2407.01476. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: large language modelbased web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 52955306, 2024. Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1889318912. PMLR, 2023. URL https://proceedings.mlr.press/v202/lee23g.html. Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, and Huan Sun. EIA: environmental injection attack on generalist web agents for privacy leakage. CoRR, abs/2409.11295, 2024. doi: 10.48550/ARXIV.2409.11295. URL https://doi.org/ 10.48550/arXiv.2409.11295. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement In 6th International Conferlearning on web interfaces using workflow-guided exploration. ence on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/ forum?id=ryTp3f-0-. Marcelo Mattar and Mate Lengyel. Planning in the brain. Neuron, 110(6):914934, 2022. Thomas Moerland, Joost Broekens, Aske Plaat, Catholijn Jonker, et al. Model-based reinforcement learning: survey. Foundations and Trends in Machine Learning, 16(1):1118, 2023. 12 OpenAI. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/, 2024a. Accessed: 2024-09-28. OpenAI. Introducing OpenAI o1. https://openai.com/o1/, 2024b. Accessed: 2024-09-29. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. ArXiv preprint, abs/2404.06474, 2024a. URL https://arxiv.org/abs/2404.06474. Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, and Zhengyang Wu. Webcanvas: Benchmarking web agents in online environments. ArXiv preprint, abs/2406.12373, 2024b. URL https://arxiv.org/ abs/2406.12373. Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing, Sebastien Racani`ere, David Reichert, Theophane Weber, Daan Wierstra, and Peter Battaglia. Learning model-based planning from scratch. ArXiv preprint, abs/1707.06170, 2017. URL https://arxiv.org/abs/ 1707.06170. Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. ArXiv preprint, abs/2405.20309, 2024. URL https://arxiv.org/abs/2405.20309. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. ArXiv preprint, abs/2408.07199, 2024. URL https://arxiv.org/abs/2408.07199. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, From pixels to UI actions: Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. In Alice Oh, Tristan NauLearning to follow instructions via graphical user interfaces. mann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 6c52a8a4fadc9129c6e1d1745f2dfd0f-Abstract-Conference.html. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 31353144. PMLR, 2017. URL http://proceedings.mlr.press/v70/shi17a. html. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for llm agents. ArXiv preprint, abs/2403.02502, 2024. URL https://arxiv.org/abs/2403.02502. Richard Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160163, 1991. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves llm search for code generation. ArXiv preprint, abs/2409.03733, 2024. URL https://arxiv.org/ abs/2409.03733. 13 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalIn Sanmi Koyejo, S. Moable real-world web interaction with grounded language agents. hamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023a. URL http://papers.nips.cc/paper_files/paper/2023/ hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/forum?id=WE_vluYUL-X. Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. ArXiv preprint, abs/2404.05719, 2024. URL https://arxiv.org/abs/2404.05719. Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: versatile and autonomous multi-agent system for web task execution with strategic exploration. ArXiv preprint, abs/2408.15978, 2024. URL https://arxiv.org/abs/2408.15978. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=piecKJ2DlB. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: realistic web environment for building autonomous agents. ArXiv preprint, abs/2307.13854, 2023. URL https://arxiv.org/ abs/2307.13854. 14 PROMPTS FOR FOUR STAGES IN MPC-BASED PLANNING A.1 ACTION PROPOSAL"
        },
        {
            "title": "Action Proposal",
            "content": "You are an autonomous intelligent agent tasked with navigating web browser. You will be given web-based tasks. These tasks will be accomplished through the use of specific actions you can issue. Heres the information youll have: {Web Information} The users objective: {Task Objective} This is the task youre trying to complete. The current web page screenshot: {Web Page Screenshot Image} This is screenshot of the webpage, with each interactable element assigned unique numerical id. Each bounding box and its respective id shares the same color. The observation, which lists the IDs of all interactable elements on the current web page with their text content if any, in the format [id][tagType][text content]. tagType is the type of the element, such as button, link, or textbox. text content is the text content of the element. For example, [1234][button][Add to Cart] means that there is button with id 1234 and text content Add to Cart on the current web page. [][StaticText][text] means that the element is of some text that is not interactable. The current web pages URL: {Web URL} This is the page youre currently navigating. The open tabs: {Previous Tabs} These are the tabs you have open. The previous action: {Previous Action} This is the action you just performed. It may be helpful to track your progress. The actions you can perform fall into several categories: Page Operation Actions: - click [id]: This action clicks on an element with specific id on the webpage. - type [id] [content]: Use this to type the content into the field with id. By default, the Enter key is pressed after typing unless press enter after is set to 0, i.e., type [id] [content] [0]. - hover [id]: Hover over an element with id. - press [key comb]: Simulates the pressing of key combination on the keyboard (e.g., Ctrl+V) - scroll [down] or scroll [up]: Scroll the page up or down. Tab Management Actions: - new tab: Open new, empty browser tab. - tab focus [tab index]: Switch the browsers focus to specific tab using its index. - close tab: Close the currently active tab. URL Navigation Actions: - goto [url]: Navigate to specific URL. - go back: Navigate to the previously viewed page. - go forward: Navigate to the next page (if previous go back action was performed). Completion Action: - stop [answer]: Issue this action when you believe the task is complete. If the objective is to find text-based answer, provide the answer in the bracket. Homepage: If you want to visit other websites, check out the homepage at http://homepage.com. It has list of websites you can visit. http://homepage.com/password.html lists all the account name and password for the websites. You can use them to log in to the websites. To be successful, it is very important to follow the following rules: 1. You should only issue an action that is valid given the current observation 2. You should only issue one action at time. 3. You should follow the examples to reason step by step and then issue the next action. 4. Generate the action in the correct format. Start with In summary, the next action will perform is phrase, followed by action. For example, In summary, the next action will perform is click [1234]. 5. Issue stop action when you think you have achieved the objective. Dont generate anything after stop. 15 A.2 SELF-REFINEMENT Self-Refinement You are assiting web navigation agent to help human user navigate website to complete task. Given the users intent, the action history, and the current state of the webpage, the agent has proposed set of candidate actions to take at the current step. Your role is not to determine best action for the agent at this step, but to filter out the actions that are very likely not relevant or helpful for the agent to accomplish the task. Please select all actions that you think that could possibly lead the agent to accomplish the task. Its important to note that to accomplish task, the agent will execute sequence of actions. So the action to take at this step does not have to immediately lead to the completion of the task. You should select any action that could be relevant for the agent to take in the current state of the webpage. Try to be as thoughtful and comprehensive as you can! Dont miss any possible action. If there is one action that is clearly the best, and all other actions are clearly not very relevant, you can only select one action. Please do this sparely, since some actions may be helpful in longer horizon. action should be included as long as it could be relevant to the task, even if it may not be the most direct action to take at this step!! Some relevant actions might seem indirect at the first glance, but could be helpful in longer horizon. Please also include those actions. Please at least select one action. *IMPORTANT* Format your response into two lines as shown below: Thoughts: <your thoughts and reasoning process>. You must explicitly evaluate each action one by one and imagine whether it could be relevant to the task following the format: action:... rationale:... Selected actions: id0;id1;aid2;... (please return the index of the action in the candidate actions list, starting from 0. Dont output the action description itself. Separate the indices with semicolons. Do not add spaces or any other characters between after the semicolons.) Action History: {last actions str} Current URL: {current url} The images corresponding to the user intent are shown in the FIRST {len(intent images)} images (before the User Intent). The last {len(screenshots)} snapshots of the agents trajectory are shown in the LAST {len(screenshots)} images. The LAST IMAGE represents the current state of the webpage. Proposed Action: {action descriptions} A.3 WORLD MODEL World Model You are an agent that predicts the effect of an action on webpage. You will be given screenshot of webpage, sequence of actions and state changes applied to the initial screenshot, and an operation to perform on the webpage. You are required to predict the new changes that will occur on the webpage after the operation is performed, such as the appearance of new elements, the disappearance of existing elements, or changes in the content of existing elements. The operation type and the element to operate will be provided in the prompt. Directly output State changes:... and dont output anything else. Try to be as comprehensive and detailed as possible. Based on the initial screenshot and the changes to the webpage, please predict the changes after action: 16 A.4 REWARD MODEL"
        },
        {
            "title": "Reward Model",
            "content": "You are an expert in evaluating the performance of web navigation agent. The agent is designed to help human user navigate website to complete task. Given the users intent, the agents action history, the current state of the webpage, your goal is to decide **whether the simulated steps by the agent indicate successful execution of the user intent**. In particular, if the predicted state (i.e., the current state represented by the last image plus all the predicted changes so far) corresponds to successful final state. If it is failure but it looks like the simulated steps are on the right track towards success, you should also output as such. Note that, in the simulated steps, all the state changes are predicted by the agents world model, and they may not actually be faithful to the real website interactions (e.g., some proposed actions may not be avaiable in realistic website). You should also account for this in your evaluation (e.g., if the predicted state changes are not reasonable then its probably failure). *IMPORTANT* Format your response into two lines as shown below: Thoughts: <your thoughts and reasoning process> Status: \"success\" or \"failure\" On the right track to success: \"yes\" or \"no\""
        },
        {
            "title": "B CASE STUDY",
            "content": "B.1 ERROR CAUSED BY IMPERFECT WORLD MODEL SIMULATION Figure 5: An error case caused by imperfect world model simulation. B.2 POSITIVE CASE BENEFITING FROM WORLD MODEL SIMULATION Figure 6: positive case where the simulation leads to correct action prediction."
        }
    ],
    "affiliations": [
        "Orby AI",
        "The Ohio State University"
    ]
}