{
    "paper_title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis",
    "authors": [
        "Alessandro Scirè",
        "Andrei Stefan Bejgu",
        "Simone Tedeschi",
        "Karim Ghonim",
        "Federico Martelli",
        "Roberto Navigli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent. Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: (i) they are tailored to a specific task or domain; (ii) they are limited in size, thereby preventing the training of new factuality evaluators; (iii) they are designed for simpler verification tasks, such as claim verification. To address these issues, we introduce LLM-Oasis, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-Oasis is constructed by extracting claims from Wikipedia, falsifying a subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create a gold standard test set for benchmarking factuality evaluation systems. Our experiments demonstrate that LLM-Oasis presents a significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field."
        },
        {
            "title": "Start",
            "content": "Towards End-To-End Factuality Evaluation with LLM-OASIS Truth or Mirage? Alessandro Scirè*1,2 Andrei Stefan Bejgu1,2 Karim Ghonim2 Federico Martelli2 Roberto Navigli2 Simone Tedeschi1,2 4 2 0 2 ] . [ 2 5 5 6 9 1 . 1 1 4 2 : r Babelscape, Italy 1lastname@babelscape.com Sapienza University of Rome 2{first.lastname}@uniroma1.it"
        },
        {
            "title": "Abstract",
            "content": "After the introduction of Large Language Models (LLMs), there have been substantial improvements in the performance of Natural Language Generation (NLG) tasks, including Text Summarization and Machine Translation. However, LLMs still produce outputs containing hallucinations, that is, content not grounded in factual information. Therefore, developing methods to assess the factuality of LLMs has become urgent. Indeed, resources for factuality evaluation have recently emerged. Although challenging, these resources face one or more of the following limitations: i) they are tailored to specific task or domain; ii) they are limited in size, thereby preventing the training of new factuality evaluators, iii) they are designed for simpler verification tasks, such as claim verification. To address these issues, we introduce LLM-OASIS, to the best of our knowledge the largest resource for training end-to-end factuality evaluators. LLM-OASIS is constructed by extracting claims from Wikipedia, falsifying subset of these claims, and generating pairs of factual and unfactual texts. We then rely on human annotators to both validate the quality of our dataset and to create gold standard test set for benchmarking factuality evaluation systems. Our experiments demonstrate that LLM-OASIS presents significant challenge for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our proposed end-to-end factuality evaluation task, highlighting its potential to drive future research in the field."
        },
        {
            "title": "Introduction",
            "content": "In recent years, generative approaches in NLP have demonstrated remarkable results, achieving state-of-the-art performance across various tasks. This progress has been particularly notable with the advent of Large Language Models (LLMs), *Equal contribution. which have revolutionized the field, driving advancements in many tasks, including Text Summarization (Goyal et al., 2022; Pu et al., 2023; Zhang et al., 2023b), Machine Translation (Alves et al., 2024; Zhang et al., 2023a; Wang et al., 2023a), and Question Answering (Kamalloo et al., 2023; Rasool et al., 2024). However, critical challenge remains as LLMs outputs still contain hallucinations, i.e. content that cannot be grounded in any pre-existing knowledge (Tonmoy et al., 2024; Tam et al., 2022). Compounding the problem, LLMs generate highly-fluent texts (Wang et al., 2023b), which may mislead users into trusting their factual accuracy. Therefore, developing modeling strategies to mitigate this issue and creating tools to detect and correct hallucinations has become urgent. In this work, we focus on the problem of factuality evaluation, that is, the task of checking the factual accuracy of text. Previous research has proposed various resources to address this task. Although challenging, even for LLM-based factual reasoners, these resources are designed for specific settings, such as text summarization of news (Laban et al., 2021; Tang et al., 2023), books (Scirè et al., 2024), and dialogues (Tang et al., 2024), among others. These benchmarks, while representative in their respective domains and tasks, often present peculiarities, which may lead to lack of generalizability across different settings. more general resource, pairing claims with evidence from Wikipedia is FEVER (Thorne et al., 2018); however, its applicability is limited by its focus on claim verification, which involves assessing the veracity of individual facts. This formulation is not well-suited to real-world scenarios, where texts typically contain multiple facts, thereby preventing the development of end-to-end factuality evaluation systems. These limitations highlight the need for resource that is, not restricted to specific domain or task, offering broader applicability and enabling the design of complete factuality evaluation approaches. In this context, we introduce LLM-OASIS, large-scale resource for end-to-end factuality evaluation, created by extracting and falsifying information from Wikipedia pages. The overall process is depicted in Fig. 1. As result, we obtain 81k factual, unfactual pairs that are suitable to train end-to-end factuality evaluation systems. Additionally, we setup human annotation process to: i) create gold standard for the factuality evaluation task, useful for benchmarking LLMs, and ii) validate the quality of the proposed data creation pipeline. Additionally we issue two tasks, namely end-to-end factuality evaluation and evidence-based claim verification to benchmark LLMs. Our experiments demonstrate that our resource is challenging even for state-of-the-art models, both in zero-shot and Retrieval Augmented Generation (Lewis et al., 2021, RAG) settings, with GPT-4o achieving an accuracy of 60% and 68%, respectively. In summary, our contributions are the following: We introduce LLM-OASIS, to the best of our knowledge the largest resource for end-to-end factuality evaluation, obtained by falsifying claims extracted from Wikipedia; Our resource enables two tasks to challenge current LLMs to detect factual inconsistencies in both short and long texts; We propose gold standard benchmark, resulting from human annotation process, to evaluate models on the proposed tasks; Our experiments demonstrate that our benchmark presents significant challenge for LLMs, with smaller specialized models trained on LLM-OASIS achieving competitive performance. Although we selected Wikipedia as the basis for our resource, we emphasize that our methodology can be potentially adapted to any other corpus in any domain or language, as the only requirement is access to collection of raw texts. In the hope of fostering research in factuality evaluation, we release our resource and code at https://github. com/Babelscape/LLM-Oasis."
        },
        {
            "title": "2 Related Work",
            "content": "Previous studies for factuality evaluation have focused on assessing factual consistency, i.e., the extent to which generated text is grounded in source document. Resources for this task typically include human annotations that indicate whether generated text accurately reflects the original documents facts. However, many of these works are tailored for specific tasks and domains, such as the assessment of factual consistency in summaries of news (Fabbri et al., 2021; Tang et al., 2023; Pagnoni et al., 2021), books (Scirè et al., 2024), and dialogues (Tang et al., 2024). Moreover, they are based on the assumption that the source of knowledge required for the verification is always available (e.g., the source document). This is not the case for the more general factuality evaluation task, in which text in natural language must be verified regardless of the availability of the evidence, potentially requiring information retrieval techniques. The first contribution towards general-purpose factuality evaluation dates back to FEVER (Thorne et al., 2018, Fact Extraction and VERification), which pairs claims with evidence retrieved from Wikipedia. The FEVER dataset comprises 185,445 human-generated claims, created by modifying sentences extracted from Wikipedia and subsequently verified without knowledge of the original sentences. The claims are classified as Supported, Refuted, or NotEnoughInfo, and for the first two categories, annotators also recorded the sentence(s) forming the necessary evidence for their judgment. Although challenging, FEVER presents limitations due to its focus on fact verification, which involves checking the veracity of individual claims. This focus is hardly adaptable to real-world scenarios, where texts to verify usually feature multiple claims. Additionally, FEVERs annotation effort is limited to relatively-small subset of 10k English Wikipedia pages. More recently, multiple studies introduced strategies to generate synthetic instances for factuality evaluation. Notably, Muhlgay et al. (2024) introduced FACTOR, framework to generate factuality benchmarks by prompting an LLM to produce factual and unfactual completions given prefix text. FACTOR includes 4,266 instances of prefix, completion pairs, each accompanied by factuality label. Along the same lines, by introducing FELM, Chen et al. (2023) provide 847 LLM outputs focused on different types of knowledge, such as World Knowledge, Math, and Reasoning with human-made factuality annotations. While valuable for benchmarking LLMs, the limited size of these resources prevents them from being used to Figure 1: Pipeline for the creation of LLM-OASIS. Given passage from Wikipedia page (original text on top), we task an LLM to: extract list of atomic claims (1), falsify one of the extracted claims (2), and then, given the two sets of claims, produce paraphrase of the original text (3), and an alternative version featuring the unfactual information (4). train new factuality evaluators. With LLM-OASIS, we differentiate from previous studies by introducing large-scale, taskagnostic resource covering wide range of domains from Wikipedia. Specifically, LLM-OASIS enables the task of end-to-end factuality evaluation, namely the more realistic scenario that involves the verification of raw text in natural language. Notably, texts falling under this setting, always go beyond individual sentences, inherently posing more complex challenge to the systems. Additionally, to the best of our knowledge, it is the largest resource for this task, featuring 162,550 passages in natural language and 681,201 claims, which can be verified against knowledge from 81,275 Wikipedia pages covering broad set of domains. Finally, we reserve manually-curated subset for this task, consisting of approximately 2k instances, and use it to benchmark several state-of-the-art models."
        },
        {
            "title": "3 LLM-OASIS",
            "content": "In this section, we outline the steps required to generate LLM-OASIS. We start by selecting Wikipedia as our source of factual data due to its coverage of wide range of topics and its frequent revisions, which help maintain accurate and upto-date information. Moreover, to guarantee the quality of our data in terms of well-established and widely-referenced information, we retain the most popular English Wikipedia pages.1 Each page is then divided into passages of sentences using sliding window with stride of sentences, forming our initial corpus.2 Given passage, as outlined in Fig. 1, we task an LLM3 to: (i) extract list of atomic claims (Claim Extraction, Sec. 3.1); (ii) falsify one of the extracted claims (Claim Falsification, Sec. 3.2); and (iii) generate paraphrase of the original passage, grounded on the extracted claims, along with an unfactual version incorporating the information from the falsified claim (Factual and Unfactual Text Generation, Sec. 3.3). In the remainder of this section, for the sake of clarity, we describe the above-mentioned steps individually, but we anticipate that the step-specific outputs are obtained by means of general, unified prompt containing the instructions for all the steps. The overall prompt is provided in Table 1. 1We select the 80k most visited pages in 2023. 2In creating our resource, we set = 5 and = 1. 3We used the GPT-4 API. More details in Appendix D. Input: Wikipedia Passage of sentences Instructions: Execute the following steps: Step 1 - Claim extraction: From the input passage, extract comprehensive set of claims. These claims must be atomic, i.e. semantically-coherent pieces of text that do not require further subdivision, and self-contained, i.e. not requiring additional context to be verified. Note that each claim must be short, using 15 words at most. Do not use \"...\" to truncate them. The ordering of the extracted claims must follow the logical flow expressed in the original text. Use noun as the subject in the claim (avoid pronouns). All the claims that are featured in the input text must be reported in the list. Step 2 - Claim falsification: From the output of Step 1, subtly alter one claim, in order to introduce critical factual inaccuracy. Such claim must be the most relevant for the input text. It is forbidden to change dates, years, numbers and person/location/organization/etc. names. It is also forbidden to provide naive negative transformations of verbs, e.g., was -> was not, did -> did not. This step, i.e., Step 2, returns pair containing the altered claim along with the original one. Step 3 - Factual text generation: From the output of Step 1, generate text. Note that this text must be paraphrase of the original provided text, i.e. new text that should overlap as little as possible with the original, while preserving the meaning. The generated text must follow the same logical flow as the ordering of the extracted claims. Step 4 - Unfactual text generation: Generate text from the final set of claims (original unaltered + altered) i.e. the output of Step 3. Note that the output of this step is not the original text, but the one generated from the final set of claims. Therefore this text contains unfactual information. The generated text must follow the same logical flow as the ordering of the claims. The output text must be as similar as possible to the output of Step 2, unless the unfactual part. Output format: Return the output in JSON with the following format: { step_1: List[str], step_2: Tuple[str, str], step_3: str, step_4: str}. The output must be valid JSON, thus try to avoid special characters like and \" inside the JSON values, unless you escape them with . Do not include any marker for the altered claim inside the JSON values, e.g., # this is the altered claim. Please do not provide any preamble to your response, just give me the JSON. Table 1: Prompt for the generation of data in LLM-OASIS."
        },
        {
            "title": "3.1 Claim Extraction",
            "content": "P1(t) (cf. Step 1 in Table 1): The first step in creating LLM-OASIS involves extracting claims from an input passage t. We randomly sample one passage from each Wikipedia page and then extract list of claims from each of the passages (cf. Step 1 in Fig. 1). We frame the claim extraction task as an end-toend autoregressive generation problem. Let be our generative model. Given an input passage t, we task to extract the claims using the prompt M(P1(t)) = (c1, . . . , cn) (1) where (c1, . . . , cn) represents the sequence of the generated claims. With the prompt P1(t), we aim at obtaining atomic4 and self-contained claims, i.e. elementary units of information that do not require additional context to be verified. Specifically, we 4Liu et al. (2023) defines claim as an Atomic Content Unit (ACU), that is, an elementary unit of information found in text that does not require further subdivision for the purpose of reducing ambiguity. explicitly require the model to adhere to such formal definition, and, additionally, constrain it to generate short texts and avoid the usage of pronouns as subjects. For instance, given the input passage: The Amazon Rainforest, also known as Amazonia, is moist broadleaf forest in the Amazon biome that covers most of the Amazon basin of South America. This region includes territory belonging to nine nations, with Brazil containing 60% of the rainforest. the model returns the following list of claims: 1. The Amazon Rainforest is also known as Amazonia. 2. It is moist broadleaf forest in the Amazon biome. 3. The Amazon Rainforest covers most of the Amazon basin of South America. 4. The region includes territory belonging to nine nations. 5. Brazil contains 60% of the rainforest. Further examples of extracted claims can be found in Appendix A."
        },
        {
            "title": "3.2 Claim Falsification",
            "content": "(2) With the aim of producing an unfactual version of the original text, we introduce critical factual error into one of the extracted claims. Formally, given the set of claims = (c1, . . . , cn) we task the model to falsify one of the claims as follows: M(P2(C)) = (ci, ci) where P2(C) is the prompt comprising the instructions for claim falsification, ci the falsified claim and ci the corresponding factual one. We ask the model to provide the factual claim as well, thus enabling the investigation of the models behavior. As outlined in Table 1 (Step 2), we instruct the model to falsify only one of the extracted claims by introducing critical yet subtle error, which makes it potentially challenging to detect. Moreover, inspired by findings from previous works about the manual creation of Natural Language Inference (NLI) resources (Parrish et al., 2021; Hu et al., 2020), we designed the prompt with instructions to discourage the generation of naive contradicting instances, e.g., trivial negations of verbs. Continuing the example introduced in the previous section, given the extracted set of claims: 1. The Amazon Rainforest is also known as Amazonia. 2. The Amazon Rainforest is moist broadleaf forest in the Amazon biome. 3. The Amazon Rainforest covers most of the Amazon basin of South America. 4. The region includes territory belonging to nine nations. 5. Brazil contains 60% of the rainforest. (ci) the model produces the following falsified claim: The majority of the forest is contained within Peru. (ci) Further examples of factual, unfactual pairs of claims can be found in Appendix A."
        },
        {
            "title": "3.3 Factual and Unfactual Text Generation",
            "content": "Based on the claims extracted in the previous steps (cf. Sections 3.1 and 3.2), we now aim at generating pairs of factual, unfactual texts, which populate our resource for factuality evaluation, thus enabling the training and the benchmarking of factual reasoners. Factual text generation To make the factuality evaluation task more challenging, instead of using the original passages from Wikipedia as our factual texts, we leverage paraphrase generation. This approach produces texts that convey the same meaning as the original ones but with different surface forms, thereby making the verification task difficult for LLMs in both zero-shot settings as the original texts could have been seen during pretraining and RAG settings, which might retrieve the exact passages from Wikipedia. Formally, given the set of extracted claims C, we task the model to generate factual text grounded on such claims: M(P3(C)) = F. (3) where P3(C) is the prompt with the instructions for obtaining factual text through paraphrasing. As described in Table 1 (Step 3) we explicitly require to follow the sequence of extracted claims to encourage full coverage of the facts expressed in the original text. For instance, given the following claims: 1. The Amazon Rainforest is also known as Amazonia. 2. The Amazon Rainforest is moist broadleaf forest in the Amazon biome. 3. The Amazon Rainforest covers most of the Amazon basin of South America. 4. Brazil contains 60% of the rainforest. the model generates the following factual text: Amazonia, widely known as the Amazon Rainforest, is damp broadleaf forest located within the Amazon biome, covering significant portion of the Amazon basin in South America. This vast region spans across nine countries, with Brazil housing 60% of the rainforest. See Appendix for more examples of generated factual texts. Unfactual text generation Finally, the unfactual texts are generated through an analogous process, this time grounded on the set of claims that includes the unfactual one, namely, = (c1, . . . , ci, . . . , cn). We obtain the unfactual text with the generation process defined with the following: M(P4(C, F)) = (4) where P4(C, F) is the prompt containing the guidelines for unfactual text generation. In particular, as specified in Table 1 (Step 4), we instruct to produce text identical to except for the segment containing the factual error to ensure that the only confounding factor for the verification task is the unfactual portion of the text. This approach helps isolate the effect of the factual inaccuracy, preventing the model to introduce further inaccuracies. For example, given the claims in C: 1. The Amazon Rainforest is also known as Amazonia. 2. It is moist broadleaf forest in the Amazon biome. 3. The Amazon Rainforest covers most of the Amazon basin of South America. 4. The region includes territory belonging to nine Claim Extraction # Pages # Passages Avg. Tokens per Passage # Claims Avg. Claims per Passage Avg. Tokens per Claim Claim Falsification # Unfactual Claims Avg. Tokens per Unfactual Claim Factual Text Generation # Factual Texts Avg. Tokens per Factual Text Unfactual Text Generation # Unfactual Texts Avg. Tokens per Unfactual Text 81,275 81,275 99.7 681,201 8.381 8.6 81,275 9.0 81,275 82.9 81,275 86.5 Table 2: Summary statistics for the creation of LLMOASIS. the model generates the following unfactual text: Amazonia, widely known as the Amazon Rainforest, is damp broadleaf forest located within the Amazon biome, covering significant portion of the Amazon basin in South America. This vast region spans across nine countries, and the majority of the forest is contained within Peru. Additional examples of unfactual texts can be found in Appendix A. Finally, statistics about claim extraction, claim falsification, factual and unfactual text generation process can be found in Table 2."
        },
        {
            "title": "4 The LLM-OASIS benchmark",
            "content": "As result of the steps described in Section 3, we obtained large resource consisting of claims and texts (both factual and unfactual) that can be used to train end-to-end factuality evaluation systems. However, due to the automated nature of the proposed approach, it is crucial to both evaluate the quality of the produced data by accurately evaluating the individual steps of our pipeline and introduce gold-standard benchmark for the task. nations."
        },
        {
            "title": "4.1 Human Evaluation",
            "content": "5. The majority of the forest is contained within Peru. To assess the quality of our dataset and enable rigorous evaluation of our procedure, we asked Task Claim Extraction Claim Falsification Factual Text Gen. Unfactual Text Gen. Accuracy (%) Fleiss κ 96.78 98.55 90.36 89. 0.81 0.84 0.73 0.72 Table 3: Performance of the chosen LLM in the data generation process according to human evaluation (Accuracy), and the corresponding inter-annotator agreement (Fleiss κ). = 5 expert linguists to validate portion of = 1, 750 instances for each task in our pipeline (cf. Sec. 3 and Fig. 1). Each annotator curated (N/M ) + instances for each task with each of the subsets having an overlap of = 100 instances shared among all annotators. We paid the annotators according to the standard salaries for their geographical location and provided them with task-specific guidelines, annotation examples, and simple interface for each task. More details are provided in Appendix E. Claim Extraction For the claim extraction task, annotators received Wikipedia passages (t1, . . . , tN ), each accompanied by list of claims extracted by the model as described in Section 3.1. The annotators task was to verify whether each claim was appropriately represented in the corresponding passage (i.e. with the same semantics) and assess their atomicity.5 We evaluated the LLMs performance on this task by counting the human-annotated errors, yielding an accuracy of 96.78%. Additionally, we measured inter-annotator agreement, resulting in Fleiss κ score of 0.81. These results underscore both the high quality of the generated text, claims pairs and the strong agreement among the annotators. Among the few errors produced by the LLM, we observe some occasional incorrect claims in the context of conditional clauses, where the model interprets conditional or hypothetical statements as if they were factual claims. For instance, given the text: In contrast, if interest rates were the main motive for international investment, FDI would include many industries within fewer countries. [...], 5We chose to prioritize precision-oriented evaluation for two key reasons: First, low coverage does not affect our proposed claim verification task (see Task 2, Section 4.2); and second, evaluating coverage would have required annotators to read the entire passage, making the annotation process more time-consuming and costly. the following incorrect claims were extracted: Interest rates motivate international investment and Interest rates lead to FDI in multiple industries, thus misrepresenting the original text which, instead, indicates hypothetical scenario. Claim Falsification For this task, annotators received pairs of claims ci, ci with ci being one of the original claims selected from (c1, . . . , cn) and ci the corresponding falsified claim produced by the model M. The annotators task was to verify whether each claim was appropriately falsified (i.e. with contradicting semantics). This required them to determine if ci meaningfully diverged from ci in terms of content and truthfulness, effectively capturing the models ability to produce altered, incorrect versions of the original claims. Again, the model achieved very high accuracy (98.55%). We measured Fleiss κ score of 0.84, showing up almost perfect agreement between the annotators. In this case, one of the most frequent error category concerns instances where attempts at falsification manifest through minimal lexical variation, In these specifically by altering single word. cases, such minor substitutions do not always yield valid falsification. For example, consider the following claims: Michael Ausiello authored the exclusive piece and Michael Ausiello wrote the exclusive piece. As we can see, despite the substitution of the verb, the semantic congruence between the two claims is maintained, rendering the falsification attempt ineffective. An additional instance of this type is represented by the claims: Washington, D.C. has milder winter weather than New York and Washington, D.C. has warmer winter weather than New York. Factual and Unfactual Text Generation For these two tasks, we used common format. Annotators received lists of original (or falsified) claims (or C) and the associated factual (or unfactual) texts produced by the model M. The annotators task was to verify whether each claim was correctly represented in the generated text. In the context of factual text generation, we additionally check whether the texts feature the same semantics as the claims but using different wording. For the factual text generation step, we measured an accuracy of 90.36% and Fleiss κ score of 0.73. Similarly, for the unfactual text generation, we measured an accuracy of 89.2% and Fleiss κ score of 0.72. As for factual text generation, we observe that the model adds information not present in the factual text6, which results in the generation of incorrect claims. For instance, the claim Russian President Yeltsin formed the Russian Armed Forces in May 1992 is not included in its entirety in the factual text: Binder, an architect grieving over his wife and sons untimely demise. Binder maintains that his familys apparitions occupy his residence, attempting to forge connection with him. Originally, the Armed Forces of the Russian Socialist Federative Soviet Republic, also acknowledged as the Red Army, served both the Russian SFSR and Soviet Union. The roots of these Armed Forces can be traced back to the Russian Civil War of 1917-1923, and they persisted until the USSR collapsed in 1991. In 1992, Boris Yeltsin, the then Russian President, initiated the formation of the Russian Armed Forces, integrating significant part of the Soviet Armed Forces. Concurrently, other divisions of the former USSRs military disbanded around 1992-1993, establishing national forces. The Soviet Armed Forces were comprised of Ground Forces, Air Forces, Navy, the OGPU, and convoy guardians. Later, the OGPU was incorporated into the NKVD in 1934, and their Internal Troops came under the joint management of Defense and Interior Commissariats. Post World War II, the forces expanded to include Strategic Rocket Forces, Air Defence Forces, and Civil Defence Forces. As far as the unfactual text generation task is concerned, we occasionally encounter clauses which are not included in the unfactual text. For example, given the text: The final segment presents imagery of PSA Flight 182s wreckage, alongside vivid aftermath photos and air traffic control recordings. It depicts horrific scene filled with dismembered remains and houses in ruins. Gröss comments on the persistent scent of flowers and aviation fuel that envelops the neighborhood. He emphasizes particularly harrowing discovery of partially intact body as the most shocking testament to mortality. Moving on, Gröss probes into the potential impact of otherworldly entities on death itself. He converses with Joseph We find that the following claim is not included: torso and right hand represent the worst death aspect according to Gröss.7 Similarly to the errors observed in factual text generation, we attribute this type of mistake to the propensity of the LLM to integrate or modify information based on the knowledge acquired during its pre-training phase. Overall, the reported detailed evaluations summarized in Table 3 show the efficacy and robustness of the proposed methodology for producing training data for the task. 4.2 Gold Benchmark In this section, we describe the construction of our benchmark, along with the factuality-oriented tasks we propose. Specifically, we exploit the human annotations (cf. Section 4.1) to construct gold-standard benchmark for model evaluation. To ensure the high quality of our data, we only retain the instances that were not marked as error by any of our annotators in any annotation stage (cf. Sec. 4.1). We employ this data to propose the following two evaluation tasks, which we describe as follows. Task 1: End-to-End Factuality Evaluation The first task is to determine whether given text contains any factual inaccuracies. Formally, given an input passage t, the model must output binary label {True, False}, where True indicates that the text is factually accurate and False indicates the presence of factual inaccuracies. For this setting, we rely only on factual and unfactual texts as input passages, and discard the original texts, as the latter might have already been seen during the pre-training of LLMs. Specifically, to further ensure the high quality of our benchmark, we only retain the correct paraphrases that are generated from valid set of claims (cf. Factual and Unfactual Text Generation and Claim Extraction in Sec. 4.1). Concerning the valid unfactual texts, instead, we only keep the ones that are: i) generated, again, from set of valid claims, and, ii) properly falsified and paraphrased (cf. Claim Falsification and Factual and Unfactual Text Generation in Sec. 6We highlight that the added information is often present 7We highlight that the added information is often present in the original text. in the original text. 4.1). We then labeled all the resulting factual and unfactual texts with True, and False, respectively. In this setting, we aim at evaluating models on discerning true from fake texts (i.e., \"Truth\" from \"Mirage\"). This formulation enables the assessment of both plain LLMs and more complex RAG models. We deem this task to be particularly challenging as the falsification may involve even single word occurring in one of the many claims featured in text, in the spirit of recent works highlighting how LLMs struggle to deal with subtle nuances in large input text (Kamradt, 2023; Hsieh et al., 2024; Laban et al., 2024; Wang et al., 2024) Task 2: Evidence-based Claim Verification In this setting, the task is to classify individual claims as factual or unfactual using given evidence. This approach assumes that claims are already extracted from the text, simplifying the task by focusing on isolated statements rather than the entire text to verify. Formally, given an input claim and corresponding evidence passage e, the model must output binary label {True, False}, where True indicates that the claim is supported by the evidence and False indicates that the claim is not supported by the evidence. For this setting, we focus on the extracted claims and their corresponding unfactual version, and use the factual text as evidence. We discard both the original and unfactual texts as the former might have already been seen during the pre-training of LLMs, while the latter contradicts real-world knowledge and, therefore, the internal knowledge of LLMs, possibly leading to unfair evaluations. Additionally, to guarantee the high precision of our data, we focus on the claims that are both atomic and reflecting the same semantics of the original text (cf. Claim Extraction Sec. 4.1). Then, we only keep the ones that have been appropriately falsified (cf. Claim Falsification in Sec. 4.1), along with their unfactual counterparts. Finally, we apply the same quality checks described in Task 1 to retain only the valid factual texts. At this stage, we classify the ci, pairs with the label True, while we label ci, as False, with ci and ci being the original claim and its falsified version, respectively."
        },
        {
            "title": "5 End-to-end Factuality Evaluation with",
            "content": "LLM-OASIS uation system. In the spirit of Min et al. (2023), we decompose the task of evaluating the factuality of given text into three simpler tasks, namely, Claim Extraction, Evidence Retrieval and Claim Verification. The process begins with extracting set of atomic facts (cf. Claim Extraction, Sec. 5.1) from the text to be verified. These extracted claims are then used to retrieve relevant evidence from reliable knowledge base (cf. Evidence Retrieval, Sec. 5.2). After this, the factual accuracy of each claim is evaluated by comparing it against the retrieved evidence (cf. Claim Verification, Sec. 5.3). Finally, the results of these individual evaluations are aggregated to determine the overall factuality of the entire text. 5.1 Claim Extraction Our approach starts by extracting atomic claims from given input text t. With the aim of training claim extractor, we leverage LLM-OASIS to create dataset of t, tuples, where is an original text from Wikipedia and = (c1, ..., cn) the corresponding automatically-extracted claims by our chosen LLM (Section 3.1). We then fine-tune smaller sequence-to-sequence model on this data, thus distilling the claim extraction capabilities of M. We frame the training process as text generation task; more formally, we fine-tune to generate the claims autoregressively: (y t) = (cid:89) k=1 (yk y0:k1, t) (5) where is the sequence obtained by concatenating the claims in and yk is token in this sequence."
        },
        {
            "title": "5.2 Evidence Retrieval",
            "content": "At this stage, given the claims extracted by G, we require system capable of retrieving relevant passages from knowledge corpus to serve as evidence to verify those claims. Again, we leverage LLMOASIS to create training dataset for our retriever; in particular, given each generic claim cj extracted from the original text t, we construct the following training pairs: cj, t, cj, F, cj, U, cj where and are the generated factual and unfactual texts (cf. Section 3.3). In this section, we showcase how LLM-OASIS can be leveraged to build an end-to-end factuality evalWe then augment this set by pairing the factual and unfactual texts with the falsified claim ci (cf. Section 3.2), thus obtaining the following additional training instances: ci, t, ci, F, ci, U. In this way, we include all possible pairs of claim, passage in LLM-OASIS in our training set. This strategy is aimed at increasing the generalization capabilities of our retriever: notably, given claim, the retriever is trained to both provide the passages to support it along with the ones that are useful to contradict it. Following the methodology outlined in Dense Passage Retrieval (Karpukhin et al., 2020, DPR), we define our retriever as Transformer-based encoder, which produces dense representations of both claims and passages. Starting from an input claim and knowledge corpus D, we use to compute vector representation vc for c, and vp for every passage {p1, p2, . . . , pm} D. Then, we use the dot product vc vp to rank all the passages in and, finally, extract the top among these. The resulting passages form our evidence set Rk(c, D) for c. Finally, we minimize the DPR loss to train E: = (cid:88) i=1 log p+ vci i + (cid:80) p+ j=i vci vci p (6) where is the batch size, vci is the vector representation of the i-th claim in the batch, vp+ is the vector representation of the corresponding gold passage for the i-th claim, and vp represents the vector representations of all the other passages in the batch, serving as in-batch negatives. This formulation ensures that the model learns to score the correct passage higher than the other ones within each batch, which has been shown to be an effective strategy for training retrieval models (Yih et al., 2011; Gillick et al., 2019). j"
        },
        {
            "title": "5.3 Claim Verification",
            "content": "The final step of our factuality evaluation methodology involves verifying each claim generated by our claim extractor from the text t, by comparing it against the corresponding passages Rk(c, D) retrieved from our corpus. Inspired by previous work on consistency evaluation (Zha et al., 2023; Chen and Eger, 2023; Scirè et al., 2024), we ground our verification approach on the NLI formulation. NLI is task that determines the logical relationship between two texts: premise and hypothesis. Formally, given premise pre and hypothesis hyp: NLI(pre, hyp) = {ENT, NEUT, CONTR}, where is label indicating whether pre entails (ENT), is neutral about (NEUT), or contradicts (CONTR) hyp. Training claim verifier on LLM-Oasis In this section, we show how LLM-OASIS can be utilized to train model for the claim verification task. Complying with the NLI formulation, we require strategy to assess whether each claim extracted from text is entailed, contradicted, or neutral with respect to set of the retrieved passages. With this purpose, we construct training dataset by deriving the following claim, passage, label triplets from LLM-OASIS: cj, t, ENT, cj, F, ENT, cj where cj is claim extracted by the LLM from the original text (cf. Section 3.1), and and are the factual and unfactual texts outlined in Section 3.3."
        },
        {
            "title": "We expand our training dataset for NLI with the",
            "content": "following triplets: ci, t, CONTR, ci, F, CONTR, ci, U, ENT, ci, U, CONTR where ci is the falsified version of the extracted claim ci (cf. Sec. 3.2). To obtain complete NLI dataset, we require strategy to generate neutral triplets as well. To achieve this, we first pair each claim cj in (Section 3.1) with the passages pi of the Wikipedia page from where the original text was extracted. Then, we select the passage as the one that maximizes the neutrality probability when fed to an NLI model8 Ψ along with cj: = argmax piW PΨ(NEUT pi, cj) and augment our dataset with the neutral pairs c, p, NEUT. This approach increases the likelihood that the selected passages are semantically related to the claim, as they come from the same Wikipedia page, while still being neutral. This is preferable to randomly selecting neutral examples, as it tends to provide more meaningful contrasts. Finally, we fine-tune Cross-Encoder model on this data; as result of this process, we obtained 8We used DeBERTa-v3-large model fineFor more inforhttps://huggingface.co/MoritzLaurer/ tuned on several NLI datasets. mation: DeBERTa-v3-large-mnli-fever-anli-ling-wanli 3: 4: 5: 6: 7: 8: Algorithm 1 Algorithm for Claim Verification. Require: claim c, top-k retrieved passages {p1, p2, . . . , pk}, NLI model Φ 1: for each passage pi in {p1, p2, . . . , pk} do 2: ˆy Φ(c, pi) if ˆy == ENT then return True else if ˆy == CONTR then return False end if {The output of the model is NEUT, i.e., neutrality. Continue to the next passage} 9: end for 10: return True {All NLI outputs are neutral, is deemed verified} our claim verification model Φ. More information about the training setup can be found in Section 6.1. Claim verification algorithm In Alg. 1 we outline how we leverage Φ to assess the factuality of claim. Our procedure takes as input claim, set of top-k retrieved passages, and claim verification model. For each passage, claim pair we obtain label ˆy by applying Φ: ˆy = Φ(pi, c) = argmax y{ENT,NEUT,CONTR} (y pi, c) (7) where pi is retrieved passage and is claim, which are fed to the NLI model Φ as the premise and hypothesis, respectively. As described in Alg. 1, the algorithm proceeds by checking the output of this model for each passage in the ranking order. If Φ outputs ENT for passage, the claim is deemed verified (i.e., return True). Conversely, if Φ outputs CONTR, the claim is deemed unfactual (i.e., return False). Finally, if Φ outputs NEUT for all passages, the claim is deemed verified (i.e., return True), as there is no contradicting evidence available. In practice, given an input text t, we used our claim verifier to assign factuality label to the claims generated by our claim extractor, using the passages returned by our retriever as evidences. The final factuality prediction for the text is an aggregation of the claim-level factuality labels. Specifically, the text is considered factual if all of its extracted claims are verified, unfactual otherwise."
        },
        {
            "title": "6 Experimental Setup",
            "content": "In this section, we provide details about the models and data involved in our experiments. To train our components for the end-to-end factuality evaluation task, we leverage the synthetic data from LLM-OASIS (cf. Section 3, Figure 1). Specifically, we randomly split the passages in an 80/20 proportion to build the train and validation datasets, respectively. When splitting, we ensure that all the claims, as well as the factual and unfactual text generated from the same passage, will end up in the same split. We evaluate both our modular architecture (cf. Sec. 6.1) and several LLM-based baselines (cf. Sec. 6.2), showing the effectiveness of our benchmark in challenging factuality evaluation systems. To assess their performance, we rely on the LLMOASIS gold-standard benchmark (Section 4.2). Models are evaluated across the two proposed tasks (i.e. end-to-end verification and evidencebased claim verification), and we use balanced accuracy (Brodersen et al., 2010) as our evaluation metric."
        },
        {
            "title": "6.1 Our model",
            "content": "Here, we provide the training details for each module of our proposed solution for end-to-end factuality evaluation (cf. Sec. 5). Claim extractor As described in Section 5.1, we build our claim extractor dataset with the text, claims tuples in the training split of LLM-OASIS. We split the resulting dataset into 67k passageclaims pairs for training, and 4k passage-claims pairs for validation. Statistics about the claim extraction dataset can be found in Table 2. We fine-tune T5base (Raffel et al., 2019) model on this data to generate the sequence of claims given an input passage. We train the model for total of 1M steps, with Adafactor (Shazeer and Stern, 2018) as optimizer with learning rate of 1e5. Following Scirè et al. (2024), we rely on the easinessF 1 metric for model selection. Let represent the set of generated claims for given text and the corresponding set of gold claims. To compute the easinessP score, as defined by Zhang and Bansal (2021), we first calculate the ROUGE-110 9All our experiments are carried out on single NVIDIA GeForce RTX 3090 GPU. 10We consider ROUGE-1 to be suitable basis for our easiness metric due to the high extractiveness of the claim score for each generated claim by comparing it to every gold claim C, and then select the maximum score. The final easinessP score is obtained by averaging these maximum scores over all generated claims: easinessP (C, C) = (cid:80) cC maxcC R1(c, c) (8) Similarly, we compute the easinessR score by selecting the maximum ROUGE-1 score for each gold claim with respect to all generated claims: easinessR(C, C) = (cid:80) cC maxcC R1(c, c) (9) Finally, we combine easinessP and easinessR to calculate the easinessF 1 score, and select the model that achieves the highest easinessF 1 on our validation set. Evidence Retriever The training dataset of our retriever comprises 3.2M claim-evidence pairs. At validation/test time we construct the knowledge corpus with the original texts in our validation split and gold benchmark, respectively. To make the evaluation more realistic and challenging, we expand the corpus with passages from the same Wikipedia page. This approach results in our corpus comprising total of 2.5M passages. We use the pre-trained Transformer-based architecture E5base (Wang et al., 2022) as our encoder E. To generate embeddings for both claims and passages, we apply mean pooling over the output of E. The model is trained with batch size of 20 input texts for 300 000 steps, using AdamW (Loshchilov and Hutter, 2019) as the optimizer. We employ learning rate of 1 106, with 20% warm-up phase. Claim Verifier As outlined in Section 5.3, we formalize the claim verification task as an NLI problem and construct dataset of 3.5M premise, hypothesis, label triplets from LLM-OASIS. We devoted 3.2M instances for training our claim verification model and the remaining 300k for validation. We fine-tune DeBERTa-v3large (He et al., 2021) for total of 1M steps on this data, using Adafactor. extraction task. 6.2 Baselines We provide comprehensive evaluation of set of LLM-based baselines on the LLM-OASIS benchmark. We compare closed-source models from the GPT family (OpenAI et al., 2024), including the latest GPT-4o, to open-weight LLMs such as Llama 3 (Touvron et al., 2023) and Mistral (Jiang et al., 2023).11 As part of the end-to-end task evaluation, we ablate the impact of providing the LLMs with external knowledge, that is, in the RAG setting. To experiment with this, we include the top-K passages12 returned by our retriever (cf. Section 5.2) in the prompts. Details about the utilized prompts and parameters for the end-to-end factuality evaluation and evidence-based claim verification tasks are provided in Appendix and D, respectively."
        },
        {
            "title": "7.1 Task 1: End-to-End Factuality Evaluation",
            "content": "In this section we present the results obtained in the end-to-end factuality evaluation task (cf. Section 4.2). First of all, we examine the performance of the evidence retrieval module, as this component supplies the external knowledge that is fed to the claim verifier. The performance of the end-to-end process depends on the quality of the retrieved evidence. This step also establishes an upper bound on the external knowledge integration, directly impacting the subsequent evaluation results. Evidence Retriever We evaluate the performance of the evidence retrieval module using the Recall at (R@k) metric, which quantifies the proportion of relevant documents retrieved in the top results. Formally, it is defined as: R@k = {relevant D} {top retrieved D} {relevant D} (10) This metric allows us to assess the ability of our retriever to identify relevant passages for factuality verification within the top-k ranked results. Higher values of generally yield higher recall, as more documents are considered, but also introduce the risk of increasing irrelevant retrievals. 11Due to our computational constraints, we opt to focus our evaluation of open-source LLMs on models of up to 8B parameters. 12We selected K=30 based on the analysis of our retrievers performance at different values of (cf. Sec. 7.1) conducted on the validation set. For our experiments, we evaluated different values of (as shown in Figure 2) and ultimately selected for all the subsequent experiments = 30 as it provided balance between performance and efficiency. The fine-tuned E5base model achieved Recall@30 (R@30) of 0.95. This is significant improvement compared to the same model without fine-tuning, which only achieved an R@30 of 0.52. The pretraining and fine-tuning process over 3.2M passages proved crucial for this performance gain. We remark that R@K represents an upper bound of our factuality evaluation performance when external knowledge is integrated into the verification process. Further analysis and details can be found in Appendix C. End-to-End Factuality Evaluation The results for the end-to-end factuality evaluation task are shown in Table 4. As we can observe, the balanced accuracy results of the LLMs, without additional external knowledge, are poor, only slightly above the random baseline in most cases. This outcome highlights that our benchmark is challenging even for state-of-the-art LLMs. This is due to the extremely-hard setting we are proposing, in which models are asked to verify the factuality of text, often subtly altered (cf. Section 3.2), without having access to any additional information. Moreover, this shows that despite having been likely exposed to the entire Wikipedia during the pretraining phase, LLMs still struggle to provide the correct label using just their prior knowledge. The inclusion of external knowledge from our retriever is beneficial, with general improvement of accuracy, particularly for GPT-4o (+8%). In this setting, our model achieves balanced accuracy score of 69.24, significantly outperforming all the LLMs, with only GPT-4o falling in the same ballpark. Despite this outcome can be attributed to the fine-tuning of our models components on LLM-OASIS whose training data distribution may resemble the one of the gold benchmark such result is notable as our system features number of parameters that is considerably smaller than that of its competitors (i.e. 1B vs 7B+). Moreover, we posit that this reduced number of parameters is counterbalanced by the quality of the training dataset which indeed enables small architecture to achieve scores comparable with, or even better than, those of LLMs. However, the fact that the best-performing system achieves score of 0.70 further highlights the complexity of the proposed Model Params B-Accuracy Llama 3 Mistral GPT-3.5 GPT-4o Our Model 8B 7B N/A N/A 1B w/o ext. w/ ext. 53.22 51.73 51.16 60.75 - 54.31 51.89 53.03 68.02 69.24 Table 4: Balanced accuracy (B-Accuracy) of different models for end-to-end factuality evaluation. \"Params\" denotes the number of parameters in billions (B). \"w/o\" stands for without external knowledge and \"w/\" stands for with external knowledge. benchmark and paves the way for future studies on factuality evaluation. 7.2 Task 2: Evidence-based Claim Verification In this section, we present the results for the second task we aim to evaluate with our benchmark (see Section 4.2), i.e., evidence-based claim verification. In this setting, we remove the claim extraction and retrieval modules, thus directly providing the claim verifier (cf. Section 5.3) with the evidence, claim pairs in our benchmark. For the LLM baselines, we used the same prompt as for the end-to-end setting, but here we replace the retrieved external passages with the gold evidence. Again, as shown in Table 5, our specialized system outperforms all its competitors including GPT-4o (93.30 vs 90.78), which is particularly remarkable given its small size. Notably, all systems achieve higher performance compared to the previous setting (e.g., our system goes from 69.24 in the end-to-end task to 93.30 in this task). We attribute this to three main factors. First, this task is simpler instance of the previous one, namely, the model is required to verify single claim rather than passage. Second, the system is provided with the exact evidence needed to verify the claim while, in the end-to-end formulation, each model relies on several passages returned by the retriever, hence possibly introducing noise in the process. Finally, the end-to-end verification implies reading and reasoning on huge context (4k tokens on average) rather than the limited one (100 tokens on average) of this task."
        },
        {
            "title": "8 Conclusion and Future Work",
            "content": "In this paper, we introduce LLM-OASIS, largescale resource for end-to-end factuality evaluation Figure 2: Recall@k performance of the E5base model at different values of k."
        },
        {
            "title": "Model",
            "content": "# Parameters B-Accuracy Llama 3 Mistral GPT-3.5 GPT-4o Our Model 8B 7B N/A N/A 0.4B 76.63 64.37 75.56 90.78 93.30 Table 5: Balanced accuracy (B-Accuracy) of different models for evidence-based claim verification. \"Params\" denotes the number of parameters in billions (B). obtained by extracting and falsifying information from Wikipedia. Specifically, as outlined in Figure 1, given text from Wikipedia, we extract set of factual and unfactual claims, with the latter obtained by falsifying one of the facts expressed in the original text. Starting from these sets, we design two claims2text tasks and generate factual text, which is paraphrase of the original one, and its unfactual counterpart, featuring the falsified claim. This resulted in 81k factual, unfactual pairs that are suitable for training and evaluating fact-checking systems, making LLM-OASIS the largest resource for factuality evaluation. Contrarily to previous works in this domain, such as FEVER, which is focused on the simpler task of claim verification, our resource is the first enabling the training of end-to-end factuality evaluation systems, i.e., approaches that are able to assess the factuality of generic text in natural language. We additionally devise human annotation process to create gold standard for benchmarking factuality evaluators and to validate the quality of the proposed data creation pipeline. LLM-OASIS enables two challenging tasks: end-to-end factuality evaluation, which tests the ability of models to verify factual accuracy in raw texts in natural language, and evidence-based claim verification, which focuses on assessing individual claims against provided evidence. Our experiments reveal that open weights LLMs, such as Mistral and Llama 3, fall short in the endto-end task, only marginally surpassing the random baseline. In the same setting, even GPT-4o faces significant challenges, in both zero-shot and RAG settings, i.e., when provided with supporting evidence from Wikipedia, only achieving 60% and 68% of accuracy, respectively. This underscores the difficulty of the proposed benchmark and its potential to drive progress in factuality evaluation. Furthermore, thanks to LLM-OASIS, we designed novel baseline for end-to-end factuality evaluation, which consists of pipeline of smaller, specialized models trained on three subtasks, namely, claim extraction, evidence retrieval and claim verification. Our approach demonstrated competitive or even superior performance to GPT-4o, showcasing the potential of smaller specialized LMs for factuality evaluation. Looking forward, we plan to expand LLMOASIS to incorporate data from diverse domains and multiple languages, enhancing its utility and applicability. With the aim of fostering research in factuality evaluation, we release our resource at https://github.com/Babelscape/LLM-Oasis."
        },
        {
            "title": "References",
            "content": "Duarte M. Alves, José Pombal, Nuno M. Guerreiro, Pedro H. Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and André F. T. Martins. 2024. Tower: An open multilingual large language model for translation-related tasks. Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M. Buhmann. 2010. The balanced accuracy and its posterior distribution. In 2010 20th International Conference on Pattern Recognition, pages 31213124. Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian He. 2023. Felm: Benchmarking factuality evaluation of large language models. Yanran Chen and Steffen Eger. 2023. Menli: Robust evaluation metrics from natural language inference. Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia-Olano. 2019. Learning dense representations for entity retrieval. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 528537, Hong Kong, China. Association for Computational Linguistics. Tanya Goyal, Junyi Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced In International bert with disentangled attention. Conference on Learning Representations. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: Whats the real context size of your long-context language models? Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kübler, and Lawrence S. Moss. 2020. OCNLI: original chinese natural language inference. CoRR, abs/2010.05444. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Association for Computational Linguistics (Volume 1: Long Papers), pages 55915606, Toronto, Canada. Association for Computational Linguistics. Gregory Kamradt. 2023. Needleinahaystack. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics. Philippe Laban, Alexander R. Fabbri, Caiming Xiong, and Chien-Sheng Wu. 2024. Summary of haystack: challenge to long-context llms and rag systems. Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2021. Summac: Re-visiting nlibased models for inconsistency detection in summarization. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledgeintensive nlp tasks. Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2023. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore. Association for Computational Linguistics. Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2024. Generating benchmarks for factuality evaluation of language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4966, St. Julians, Malta. Association for Computational Linguistics. Ehsan Kamalloo, Nouha Dziri, Charles Clarke, and Davood Rafiei. 2023. Evaluating open-domain question answering in the era of large language models. In Proceedings of the 61st Annual Meeting of the OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report. Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with frank: benchmark for factuality metrics. Alicia Parrish, William Huang, Omar Agha, Soo-Hwan Lee, Nikita Nangia, Alexia Warstadt, Karmanya Aggarwal, Emily Allaway, Tal Linzen, and Samuel R. Bowman. 2021. Does putting linguist in the loop improve NLU data collection? In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 48864901, Punta Cana, Dominican Republic. Association for Computational Linguistics. Xiao Pu, Mingqi Gao, and Xiaojun Wan. 2023. Summarization is (almost) dead. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with unified text-to-text transformer. CoRR, abs/1910.10683. Zafaryab Rasool, Stefanus Kurniawan, Sherwin Balugo, Scott Barnett, Rajesh Vasa, Courtney Chesser, Benjamin M. Hampstead, Sylvie Belleville, Kon Mouzakis, and Alex Bahar-Fuchs. 2024. Evaluating llms on document-based qa: Exact answer selection and numerical extraction using cogtale dataset. Natural Language Processing Journal, 8:100083. Alessandro Scirè, Karim Ghonim, and Roberto Navigli. 2024. Fenice: Factuality evaluation of summarization based on natural language inference and claim extraction. Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. CoRR, abs/1804.04235. Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel. 2022. Evaluating the factual consistency of large language models through summarization. Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscinski, Justin Rousseau, and Greg Durrett. 2023. Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1162611644, Toronto, Canada. Association for Computational Linguistics. Liyan Tang, Igor Shalyminov, Amy Wing mei Wong, Jon Burnsky, Jake W. Vincent, Yuan Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, and Kathleen McKeown. 2024. Tofueval: Evaluating hallucinations of llms on topic-focused dialogue summarization. James Andreas Vlachos, Christos Thorne, Christodoulopoulos, and Arpit Mittal. 2018. FEVER: large-scale dataset for fact extraction In Proceedings of the 2018 and VERification. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809819, New Orleans, Louisiana. Association for Computational Linguistics. S. Towhidul Islam Tonmoy, Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. comprehensive survey of hallucination mitigation techniques in large language models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. 2024. Multimodal needle in haystack: Benchmarking long-context capability of multimodal large language models. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023a. Document-level machine translation with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1664616661, Singapore. Association for Computational Linguistics. Yiming Wang, Zhuosheng Zhang, and Rui Wang. 2023b. Element-aware summarization with large language models: Expert-aligned evaluation and chain-ofthought method. projections for text similarity measures. In Proceedings of the fifteenth conference on computational natural language learning, pages 247256. Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. Alignscore: Evaluating factual consistency with unified alignment function. Biao Zhang, Barry Haddow, and Alexandra Birch. 2023a. Prompting large language model for machine translation: case study. Shiyue Zhang and Mohit Bansal. 2021. Finding balanced degree of automation for summary evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 66176632, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori Hashimoto. 2023b. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:3957."
        },
        {
            "title": "A Examples of Generated Texts",
            "content": "We present several examples derived from our dataset to show the models capability of generating both factual and unfactual texts. These examples show how our pipeline produces paraphrased versions of original texts and introduces subtle yet critical factual inaccuracies. A.1 Example 1 Original Text: Albert Einstein was German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics. His work is also known for its influence on the philosophy of science. Einstein is best known for his massenergy equivalence formula = mc2, which has been dubbed the worlds most famous equation. Extracted Claims: 1. Albert Einstein was German-born theoretical physicist. 2. He developed the theory of relativity. 3. The theory of relativity is one of the two pillars of modern physics. 4. Einsteins work influenced the philosophy of science. Wen-tau Yih, Kristina Toutanova, John Platt, and Christopher Meek. 2011. Learning discriminative 5. He is best known for his massenergy equivalence formula = mc2. 6. The formula = mc2 is dubbed the worlds most famous equation. Factual Text: Albert Einstein, originally from Germany, was theoretical physicist who formulated the theory of relativity, cornerstone of modern physics. His contributions significantly impacted the philosophy of science. The massenergy equivalence equation = mc2, which he is most famous for, is often called the worlds most famous equation. Falsified Claim: He developed the theory of quantum mechanics. Unfactual Text: Albert Einstein, originally from Germany, was theoretical physicist who formulated the theory of quantum mechanics, cornerstone of modern physics. His contributions significantly impacted the philosophy of science. The massenergy equivalence equation = mc2 , which he is most famous for, is often called the worlds most famous equation. A.2 Example 2 Original Text: The Amazon Rainforest, also known as Amazonia, is moist broadleaf forest in the Amazon biome that covers most of the Amazon basin of South America. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest. Extracted Claims: 1. The Amazon Rainforest is also known as Amazonia. 2. It is moist broadleaf forest in the Amazon biome. 3. The Amazon Rainforest covers most of the Amazon basin of South America. Amazon basin in South America. This vast region spans across nine countries, with Brazil housing 60% of the rainforest. Falsified Claim: The majority of the forest is contained within Peru. Unfactual Text: Amazonia, widely known as the Amazon Rainforest, is damp broadleaf forest located within the Amazon biome, covering significant portion of the Amazon basin in South America. This vast region spans across nine countries, with Peru housing 60% of the rainforest These examples show the effectiveness of the model in creating pairs of factual and unfactual texts. The alterations are subtle, ensuring that the generated unfactual texts are challenging for both human annotators and automated systems to detect, thus providing robust testbed for evaluating the factual accuracy of LLM-generated content. Prompts for End-to-End Factuality"
        },
        {
            "title": "Evaluation",
            "content": "To accomplish the task of end-to-end factuality evaluation, we employ different prompting strategies depending on the language model being used. For models like Llama, which supports system prompt, we set specific instructions as the system message. For models like Mistral, which do not support system prompt, we include the instructions at the beginning of the text. In our experiments, we set the temperature to 0.7 to control the randomness of the generated outputs, ensuring balance between diversity and relevance. Other hyperparameters include maximum token length of 5 and beam search with width of 5 for decoding the outputs. These settings were chosen to optimize the models performance while maintaining computational efficiency."
        },
        {
            "title": "The unified prompt used for factuality evaluation",
            "content": "4. The region includes territory belonging to nine is provided in Table 6. nations. 5. The majority of the forest is contained within Brazil. 6. Brazil contains 60% of the rainforest. Factual Text: Amazonia, widely known as the Amazon Rainforest, is damp broadleaf forest located within the Amazon biome, covering significant portion of the The system message is set as follows: \"You are highly-accurate fact-checker. Your task is to determine the factuality of given text. text is considered Factual only if it is completely factually-accurate and contains no factual inaccuracies. Even single small factual inaccuracy should result in Not Factual determination. Answer with just Factual or Not Factual without any explanation.\" To further evaluate the impact of external knowledge, we prompted the LLMs with the same pieces of evidence retrieved and used by our NLI module (cf. Sec. 7.1). The updated prompt is shown in Table 7. The same prompt was also used for the experiment in Sec. 7.2 for the Evidence-based claim verification. Moreover, we updated the system prompt with the following: \"You are highly-accurate fact-checker. Your task is to determine the factuality of given text using the evidence provided. text is considered Factual only if it is completely factually accurate and contains no factual inaccuracies. Even single small factual inaccuracy should result in Not Factual determination. If evidence is not available, use your prior knowledge to make the assessment. Answer with just Factual or Not Factual without any explanation.\" We tested different prompts and found that this one led to the best results. By providing clear examples and detailed instructions, we aim to ensure that the model accurately assesses the factuality of the given texts. This structured approach helps in training and evaluating the models effectively, ensuring high accuracy in end-to-end factuality evaluation."
        },
        {
            "title": "C Further details on Evidence Retriever",
            "content": "module In this section, we present further details about our evidence retrieval model. To assess the contribution of different components, we performed an ablation study on the retrieval module. All models were trained using the same hyperparameters described in Section 5.2. Results are computed on the corpus D, which contains 2.5 million passages, and evaluated on the validation split of the dataset. After training, our best model achieved recall at = 30 (R@30) of 0.95. We employed the E5base model (Wang et al., 2022), built upon the bert-base-uncased (?) architecture, with weights initialized from SentenceTransformers (?). As part of our ablation study, we also trained the bert-base-uncased model with the same hyperparameters, achieving recall of 0.85. This significant performance drop compared to the fully fine-tuned E5 demonstrates the effectiveness of the additional pretraining done in E5. Additionally, we experimented with other architectures from the E5 family. The E5small model obtained recall of 0.75, whereas the E5large model slightly outperformed E5base, achieving recall of 0.96. Despite the marginal 1% performance gain, we opted to use the E5base model in our final system due to the substantial increase in computational resources and training time required by the E5large model, which did not justify the small performance improvement. The results of all models tested during the ablation study are summarized in Table 8, confirming the robustness and efficiency of the E5base model for claim retrieval, balancing performance with computational cost."
        },
        {
            "title": "D Details about the employed LLMs",
            "content": "In this section, we detail the models we used in this work. For the generation of our dataset, we used GPT-4 API, with an approximate cost of $2000. As for the open-source models we utilized for the LLM baselines, we used the instruction tuned versions of Mistral 13 and LLama 3 14 publicly available on Hugging Face. For the benchmark evaluation, we utilized the OpenAI API. Specifically, for GPT-4o, we employed the model GPT-4o-2024-05-13. For GPT-3.5, we used GPT-3.5-turbo-16k-0613 due to the necessity of handling task with context exceeding the maximum window size of the standard GPT-3.5 model. For the claim-extractor, we use the pre-trained T5-base 15 as our base model."
        },
        {
            "title": "E Annotation Guidelines",
            "content": "In this section, we illustrate the annotation guidelines employed. Annotators are asked to perform four different tasks related to factuality evaluation. For each task, annotators receive specific guidelines which we report in what follows. As standard guideline for all tasks, annotators are required to discard instances entirely or partially written in language other than English. Furthermore, in case of pronominal ambiguity occurring in given claim, if the human annotator cannot determine, with high degree of confidence, the noun to which given pronoun refers, such claim is discarded. Annotators are required to participate in joint sessions to resolve challenges and collaboratively develop agreed-upon solutions. E.1 Task 1: Claim Extraction Task description In this step, you will verify if claims extracted from given text are accu13https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.3 14https://huggingface.co/meta-Llama/ Meta-Llama-3-8B-Instruct 15https://huggingface.co/google-t5/t5-base Determine whether the given text is factual or not. Input Text: INPUT_TEXT Instructions: 1. Read the input text. 2. Evaluate the factual accuracy of the input text based on your training data and knowledge. 3. If the input text is factually-accurate, i.e. supported by known information, respond with \"Factual\". 4. If the input text contains any factual inaccuracies, i.e., it is not supported by known information, respond with \"Not Factual\". 5. Do not generate any additional text to the answer. Examples: Example 1: Input Text: \"The Eiffel Tower is located in Berlin.\" Output: \"Not Factual\" Example 2: Input Text: \"The Eiffel Tower is located in Paris.\" Output: \"Factual\" Example 3: Input Text: \"The Great Wall of China is visible from the moon.\" Output: \"Not Factual\" Example 4: Input Text: \"The Great Wall of China is historical structure built in ancient China.\" Output: \"Factual\" Table 6: Prompt for factuality evaluation of text. rately represented within the original text. You will receive 5-sentence passage extracted from Wikipedia, along with corresponding claims preextracted by language model. Note: claim denotes an atomic fact, that is, an elementary information unit found in text, that does not require further subdivision, and that can be checked for its truthfulness. Annotation Format You will be provided with TSV (Tab-Separated Values) file containing three columns: Column 1: Identifier (either \"text\" or \"claim id\") Column 2: Text or Claim Column 3: Empty. You have to fill in this column."
        },
        {
            "title": "Annotation Procedure",
            "content": "1. Read the original text and claims thoroughly. 3. Place \"v\" in the third column if the claim is present in the original text, otherwise mark it with an \"x\" Annotation Example We report an example of annotated instance in Table 9. Additional Guidelines Annotators are required to discard an entire instance, composed of the original text and the corresponding claims, if the original text is not grammatically correct, e.g., if it is syntactically ill-formed, or if it is semantically unclear, that is, if it is formulated in way that the annotator cannot determine the meaning conveyed either by the entire text or one of its segments. Furthermore, annotators are required to discard sentences which cannot be considered as claims for the purposes of our work, e.g., sentences composed of single word. E.2 Task 2: Claim Falsification Task Description In this step, you will identify whether given claim has been altered to introduce unfactual information. 2. For each claim, determine if it is accurately represented in the original text. Annotation Format You will receive pair of claims, where the second claim is an unfactual Determine whether the given text is factual or not using the evidence. If the information is not present in the evidence, rely on prior knowledge. Input Text: INPUT_TEXT Evidence: EVIDENCE Instructions: 1. Read the input text. 2. Read the evidence if provided. 3. Assess whether the input text is factual based on the evidence if present. 4. If the evidence are not provided or is insufficient, use your prior knowledge to determine the factuality. 5. Respond with \"Factual\" if the input text is supported by evidence or prior knowledge. 6. Respond with \"Not Factual\" if the input text contains inaccuracies. Examples: Example 1: Input Text: \"The Eiffel Tower is located in Berlin.\" Evidence: \"The Eiffel Tower is located in Paris.\" Output: \"Not Factual\" Example 2: Input Text: \"The Eiffel Tower is located in Paris.\" Evidence: \"The Eiffel Tower is located in Paris.\" Output: \"Factual\" Example 3: Input Text: \"The Earth orbits the sun.\" Evidence: \"The Earth is round.\" Output: \"Factual\" Table 7: Prompt for text factuality evaluation integrating external knowledge. version of the first Column 1: The original claim Column 2: The unfactual claim. Column 3: Empty. You have to fill in this column."
        },
        {
            "title": "Annotation Procedure",
            "content": "1. Compare the two claims provided. 2. Determine if the unfactual claim introduces new, untrue information compared to the original claim. 3. Mark column 3 with \"v\" if unfactual information is introduced, otherwise mark it with \"x\". Additional Guidelines If the original claim contains word that is replaced with its hyponym in the candidate nonfactual claim, while the overall meaning of both claims remains unchanged also based on the annotators world knowledge, then both claims are considered to be semantically equivalent. E.3 Task 3: Factual Text Generation Task Description In this step, you will assess whether the semantics of claims is preserved in paraphrased version of the text. Annotation Format You will receive TSV file with four columns: Column 1: Identifier (either \"paraphrase\" or \"claim id\") Column 2: Text or Claim Annotation Example We report an example of annotated instance in Table 11. Column 3: Empty. You have to fill in this column. Model E5base (without fine-tuning) E5base bert-base-uncased E5small E5large Recall@30 0.52 0.95 0.85 0.75 0.96 Identifier original_text claim 1 claim 2 claim 3 claim claim 5 claim 6 claim 7 claim 8 Table 8: Performance of Different Models on Claim Retrieval Task Text This type of meringue is safe to use without cooking. It will not deflate for long while and can be either used for decoration on pie, or spread on sheet or baked Alaska base and baked. Swiss meringue is whisked over bain-marie to warm the egg whites, and then whisked steadily until it cools. This forms dense, glossy marshmallow-like meringue. It is usually then baked. Swiss meringue is safe to use without cooking. Swiss meringue will not deflate for long while. Swiss meringue can be used for pie decoration or on baked Alaska base. Swiss meringue is whisked over bain-marie to warm the egg whites. Swiss meringue is then whisked steadily until it cools. Swiss meringue forms dense, glossy, marshmallow-like texture. Swiss meringue is usually baked after preparation. Swiss meringue can be mixed with vanilla or chocolate to add flavor. Table 9: Example of annotated instance in task 1 (claim extraction). Annotation v v Column 4: Empty. You have to fill in this column."
        },
        {
            "title": "Annotation Procedure",
            "content": "1. Compare each claim with its representation in the paraphrased text. 2. Determine if its semantics is preserved. If it is preserved (regardless of whether it is reported identically in the paraphrase), place \"v\" in the third column. Use \"x\" otherwise. 3. Determine if it is paraphrased. If claim is paraphrased, mark the fourth column with \"v\". If not paraphrased (e.g. identical), mark column 4 with \"x\". In other words: <\"v\", \"v\"> in the last two columns means that the semantics is preserved and the text is paraphrased (at least one word changed). <\"x\", \"v\"> in the last two columns means that the semantics is NOT preserved but the text is paraphrased. <\"v\", \"x\"> in the last two columns means that the semantics is preserved but the text is NOT paraphrased. <\"v\", \"x\"> in the last two columns means that the semantics is preserved but the text is NOT paraphrased. <\"x\", \"x\"> in the last two columns means that neither the semantics is preserved nor the text is paraphrased (e.g. the claim is omitted). Annotation Example We report an example of annotated instance in Table 11. Identifier claim 1 claim claim 3 claim 4 claim 5 claim 6 Identifier claim 1 claim 2 claim 3 claim 4 claim 5 paraphrase Claim The remix in Thank You track was Lassie Come Home. The remix in Thank You track was not Lassie Come Home. The Plateau served as model for colonial capitals. The Plateau served as model for other districts. Christoph Waltz replaced Billy Bob Thornton. Christoph Waltz replaced Brad Pitt. Annotation v Table 10: Example of annotated instance in task 3 (claim falsification). Semantics Preserved Paraphrased x v v Text Call Me by Your Name leads Dorian Award nominations Gregg Kilday authored the article on 10 January 2018 The Hollywood Reporter published the article Article was retrieved on 11 January 2018 The Jameson Empire Awards occurred in 2014 Call Me by Your Name took the lead in Dorian Award nominations. The article, penned by Gregg Kilday, was published by The Hollywood Reporter on January 10, 2018, and accessed the following day. Meanwhile, The Jameson Empire Awards were held back in 2014. Table 11: Example of annotated instance in task 3 (factual text generation). Additional Guidelines If nearly identical date appears in the factual text and in one claim, annotators should proceed as follows. If the date in the factual text includes the month and year, while the claim specifies the day, month, and year, even if the month and year in the claim coincide with those in the factual text, the semantics conveyed by the claim is considered to be different from that of the factual text. E.4 Task 4: Unfactual Text Generation Task description In this step, you will assess whether all claims, including the unfactual one, are accurately reflected in generated unfactual text. Annotation Format You will receive all claims paired with the generated unfactual text. Column 1: Identifier (either \"claim id\", or \"unfactual_text\") Column 2: Text or Claim Column 3: Empty. You have to fill in this column."
        },
        {
            "title": "Annotation Procedure",
            "content": "Review the generated unfactual text along with all claims provided Determine if all claims are correctly reported in the text (i.e. the factual claims should remain factual and the unfactual claims should be unfactual). Ensure that the text in the unfactual_text field is not modified by the language model to be compliant with the unfactual claim. Paraphrasing in claims is allowed, you should focus on semantics. Mark column 3 with \"v\" if the unfactual text corresponds to the claims accurately, otherwise mark it with \"x\". Annotation Example We report an example of annotated instance in Table 12. Identifier original_text claim 1 claim 2 claim 3 claim 4 claim claim 6 Text In response to crisis, Ottoman statesmen adopted compliant policy. Abdulmejids inability to handle the situation heightened discontent regarding the Edict of Tanzimat. To enhance European influence, opponents schemed to dethrone Abdulmejid for Abdulaziz. The planned Kuleli Foundation revolt was thwarted before it could begin on 14 September 1859. Meanwhile, the financial crisis deepened as burdensome foreign debts strained the treasury. Ottoman statesmen panicked and adopted policy fulfilling every wish. Abdulmejid failed to prevent the situation, increasing dissatisfaction with the Edict of Tanzimat. Opponents planned to replace Abdulmejid with Abdulaziz to enhance European dominance. The Kuleli Foundation revolt was suppressed before starting on 14 September 1859. The financial situation worsened, and foreign debts burdened the treasury. To enhance European influence, opponents schemed to dethrone Abdulmejid for Abdulaziz. Annotation v x Table 12: Example of annotated instance in task 3 (unfactual text generation)."
        }
    ],
    "affiliations": [
        "Babelscape, Italy",
        "Sapienza University of Rome"
    ]
}