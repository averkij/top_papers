{
    "paper_title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG",
    "authors": [
        "Fengran Mo",
        "Zhan Su",
        "Yuchen Hui",
        "Jinghan Zhang",
        "Jia Ao Sun",
        "Zheyuan Liu",
        "Chao Zhang",
        "Tetsuya Sakai",
        "Jian-Yun Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 8 2 0 9 0 . 1 0 6 2 : r OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG Fengran Mo UniversitÃ© de MontrÃ©al MontrÃ©al, QuÃ©bec, Canada fengran.mo@umontreal.ca Jianhan Zhang Clemson University Clemson, South Carolina, USA jinghaz@clemson.edu Chao Zhang Georgia Institute of Technology Atlanta, Georgia, USA chaozhang@gatech.edu Zhan Su UniversitÃ© de MontrÃ©al MontrÃ©al, QuÃ©bec, Canada zhan.su@umontreal.ca Jia Ao Sun UniversitÃ© de MontrÃ©al MontrÃ©al, QuÃ©bec, Canada jia.ao.sun@umontreal.ca Tetsuya Sakai Waseda University Tokyo, Japan tetsuya@waseda.jp Yuchen Hui UniversitÃ© de MontrÃ©al MontrÃ©al, QuÃ©bec, Canada yuchen.hui@umontreal.ca Zheyuan Liu University of Notre Dame Notre Dame, Indiana, USA zliu29@nd.edu Jian-Yun Nie UniversitÃ© de MontrÃ©al MontrÃ©al, QuÃ©bec, Canada nie@iro.umontreal.ca Abstract The development of large language models (LLMs) has achieved superior performance in range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators. CCS Concepts Information systems Information retrieval; Computing methodologies Artificial intelligence. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. WWW 26, Dubai, United Arab Emirates 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn Keywords Information Retrieval, Retrieval-Augmented Generation, Robust Question Answer, Decoding Paradigm, Large Language Model ACM Reference Format: Fengran Mo, Zhan Su, Yuchen Hui, Jianhan Zhang, Jia Ao Sun, Zheyuan Liu, Chao Zhang, Tetsuya Sakai, and Jian-Yun Nie. 2026. OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG. In Proceedings of the ACM Web Conference 2023 (WWW 26), April 1317, 2026, Dubai, United Arab Emirates. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 Introduction\nThe development of large language models (LLMs) [1, 47, 67] has\nachieved superior performance in a range of downstream tasks\nvia their parametric knowledge acquisition from the training doc-\numents. However, LLMs still encounter foundational problems,\nsuch as understanding the limits of their knowledge and capabil-\nity [13, 53], where a lack of sufficient knowledge might lead to\nhallucinations or generating outdated results [16, 26]. Retrieval-\naugmented generation (RAG) [25] is a common practice to address\nthe incomplete knowledge issue by incorporating external infor-\nmation to obtain more accurate and reliable content generation.",
            "content": "Despite the fact that the RAG technique alleviates the knowledge boundary issue of LLMs, existing approaches to RAG face fundamental challenges: the quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs internal information processing mechanism [28, 43]. It is generally assumed that the retrieved information is relevant and useful for content generation, or LLMs have the capability to judge its relevance. However, the existing literature [9] showed the vulnerability of automated usefulness-checking systems when confronted with noisy information. Thus, the defective and imperfect retrieved information would degrade the performance of LLMs. As matter of fact, when an LLM is asked to answer question based on an irrelevant document, the quality of the answer is negatively affected [48]. Such situation with irrelevant information WWW 26, April 13-17, 2026, Dubai, United Arab Emirates Fengran Mo et al. may often occur when RAG is asked to deal with large variety of questions. An ideal RAG system should be able to understand and tolerate the noisy input, i.e., process the diverse inputs that include useful evidence and irrelevant information, without being affected by the noise and resulting in significant degradation in performance [42, 70]. For example, if the input context is partially noisy or extremely irrelevant, the system can attend only to the useful part or ignore the whole misinformation when generating an answer. Existing studies attempt to address this issue from various perspectives, which can be categorized into (i) workflow-based methods and (ii) fine-tuning-based methods. The first category aims to design workflow that navigates LLMs to identify useful pieces from retrieved information and append them to the final input context for generation. The intermediate steps in the workflow vary and may include self-correction through LLM-as-a-judge [11, 60], isolating individual results for later aggregation [39, 56], and stepby-step filtering via reasoning [4], among others. This training-free approach is highly sensitive to the used prompt template and follows the strong assumption that the model could have enough capacity to distinguish the useful information by following the instruction to produce ideal output [13]. However, one cannot expect that LLMs always generate correct judgments, and thus the manipulated final input might lose crucial information or include wrong information before conducting answer generation [61]. Besides, the judgment workflow with multiple steps with LLM calling would significantly increase latency [40]. On the other hand, the fine-tuning methods aim to teach the model to incorporate external useful knowledge in an effective way. For example, one can equip the LLMs with retrieval defect detection and utility extraction via instruction fine-tuning [46, 48] or enable the LLMs to interact with the retriever multiple turns until appending sufficient information for answer generation [2, 18]. Though effective, the existing approaches still inherit the original method of LLMs to perform the online computation of key-value pairs in the attention networks of the decoder [43] for generation, which means that the autoregressive decoding of LLMs is mainly impacted by the attention score to produce generation probability. We notice that the attention score is assigned by LLMs alone once the retrieved documents are appended into the prompt template. The original relevance judged by the retriever of the input documents is never used by the LLMs. Thus, the LLMs might treat the input documents as equally relevant or slightly different according to their input position [23] based on the implicit internal judgments. This gives rise to several critical questions: Should RAG ignore the relevance signals of the retrieved documents in its generation? Are such relevance signals useful for generation? How should the generation be impacted by document relevance? We believe that document relevance should be explicitly considered in answer generation in RAG, so that answer generation can be more tuned toward relevant information than irrelevant one. To achieve this goal, in this paper, we propose OpenDecoder, new approach that directly leverages document relevance to change the information processing procedure of LLMs decoding, namely, its attention mechanism. As shown in Figure 1, compared to the current decoding paradigm of LLMs, our proposed OpenDecoder does not only rely on the attention score produced via the internal Figure 1: Comparison between the existing decoding LLMs that use their default probability distribution and our proposed approach that modifies the distribution by leveraging external explicit relevance signals. network and instruction-following training, but also leverages explicit relevance signals as external indicator features. The model is expected to become more robust to varying levels of noisy input context by reshaping the generation probability distribution via the useful information among the retrieved knowledge, and thus produce more accurate answers as output. To implement OpenDecoder, the first step is to construct external indicators by extracting quality features from the retrieved documents. We consider three types of signals: relevance score from the retriever, LLM-judged semantic score, and query performance prediction score. Then, we design training framework to teach the LLMs to leverage these explicit indicator features (either separately or in combination) for answer decoding. Specifically, we incorporate the external features into the internal attention networks computation to directly modulate the LLMs when producing generation probabilities for the decoding candidate tokens. Additionally, to make the training and inference more robust to noisy information within the input, we conduct robustness training by reconstructing the input top-k documents via sampling additional documents with various relevant levels. During the online inference, the corresponding indicator features from external information are processed by the trained LLMs via the learned parameters in OpenDecoder. Experiments on five benchmark datasets covering both general and multi-hop question answering (QA) demonstrate the effectiveness and enhanced robustness of the proposed approach, which consistently outperforms the vanilla RAG and other strong baselines across diverse noisy environments. Importantly, our designed OpenDecoder is flexible to be integrated with the post-training of LLMs for any purposes and incorporate any other type of external indicator features towards effectiveness, robustness, or trustworthiness enhancement. Our contributions are summarized as follows: (1) We propose new approach OpenDecoder to directly modify the LLM decoding in RAG by leveraging the relevance signals of the retrieved documents. (2) We design training method, which includes constructing explicit relevance indicators from retrieved documents, teaching the model to leverage explicit indicators for answer decoding, and improving robustness via replacing the original top-k documents with various relevant levels ones. (3) We conduct experiments on five widely used benchmarks, including general and multi-hop QA. Our OpenDecoder outperforms vanilla RAG and other strong baselines across diverse noisy environments, which demonstrates its superior effectiveness. OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG WWW 26, April 13-17, 2026, Dubai, United Arab Emirates"
        },
        {
            "title": "2 Related Work\n2.1 Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) [10, 25] aims to retrieve\nexternal resources to supplement LLMs to generate a response,\nshowing significant advantages in knowledge-intensive tasks [8,\n12, 20, 65]. Earlier RAG methods follow the â€œRetrieve-then-Readâ€\nframework [15, 25] by adopting a retriever to search for relevant\ninformation from external resources based on the userâ€™s query.\nTo further enhance RAG performance, subsequent studies focus\non refining retrieval quality through techniques such as query\nreformulation [31, 37], re-ranking [35, 44, 61], and noise filtering\nas intermediate steps [17, 36, 39], thereby improving the relevance\nof documents before they are appended to LLMsâ€™ input.",
            "content": "However, retrieval errors remain common due to limitations in search effectiveness and corpus quality [38], which can ultimately degrade RAG performance. To address this problem, robust RAG [30, 69] focuses on input optimization and knowledge integration. For instance, Weller et al. [55] conduct query augmentation and introduce novel confidence method based on answer redundancy. RobustRAG [56] employs an isolate-then-aggregate strategy to ensure the robustness of LLM responses against retrieval corruption attacks. By generating self-synthesized rationales, InstructRAG [54] explicitly denoises the retrieved content, thereby enhancing the robustness of RAG systems. AstuteRAG [50] turns to refine and integrate knowledge derived from different sources to improve knowledge utilization and enhance the robustness of the generated answer. RbFT [48] proposes robust fine-tuning strategy against retrieval defects with two defined tasks, defect detection and utility extraction, with associated instructions. In addition, recent studies on developing deep search agents [18, 27, 41, 68] introduce new paradigm for enhancing input quality by integrating in-context reasoning with dynamic search tool invocation when needed. Although effective, these existing methods rely only on the internal mechanism of LLMs to process information, e.g., attention network [49]. Unlike them, our method OpenDecoder is developed to enable LLMs to distinguish useful information via both internal mechanisms and external explicit indicators."
        },
        {
            "title": "2.2 Decoding Optimization in LLMs\nPrompting [29] the advanced LLMs is a simple and effective way\nto instruct them to generate answers, where the answer decod-\ning highly relies on the designed prompt and internal attention\nmechanism. Existing literature optimizes the decoding procedure\nof LLMs on various aspects. For efficiency, Performers [5] propose\ncompressed attention, reducing attention complexity from qua-\ndratic to linear. StreamingLLM [57] leverages attention sinks to\ndecrease Key-Value cache memory for long-context generation. For\neffectiveness, a series of studies [3, 45, 62â€“64] investigate how to\nleverage inference scaling and deep reasoning for RAG decoding.\nA recent study REFRAG [28] rethinks RAG-based decoding and\nproposes an optimized architecture to compress only a small sub-\nset of retrieved documents that are directly related to the query\nfor effective and efficient decoding. For faithfulness, the existing\nstudies aim to detect and manage misinformation within retrieved\ndocuments [70], such as explicitly identifying and resolving knowl-\nedge conflicts [7, 51, 66]. These studies focus on selecting relevant",
            "content": "and reliable information for LLM input, which still operate in the way that has been trained, by assuming the input information to be relevant. In contrast, in our approach, we modify the attention mechanism according to the relevance of retrieved information. Such an approach has not been proposed in the literature."
        },
        {
            "title": "3.1 Task Formulation\nA vanilla RAG system typically consists of an ad-hoc retriever R, a\ngenerator (i.e., the LLM) G, and a corresponding corpus C with a\nlarge collection of documents. Given a user query ğ‘, the retriever\nğ‘\nğ‘– }ğ‘˜\nR would identify its top-k relevant documents R (ğ‘) = {doc\nğ‘–=1.\nThen, the LLM G would generates an answer ğ‘ based on the query\nand relevant documents as",
            "content": "ğ‘– }ğ‘˜ ğ‘ ğ‘ = G(ğ‘, {doc ğ‘–=1) = G(ğ‘, (ğ‘, C)) (1) The quality of the generated answer ğ‘ highly depends on the useful information returned by the retriever and the understanding capacity of LLMs for the input context with the corresponding prompt. The inevitable noise in the retrieved context would significantly degrade the answer quality of LLMs on top of it. These issues are unavoidable with the current prompting-based approach, where the content decoding only inherits the internal information processing mechanism of LLMs by following the prompt instruction [13, 53]. Our work focuses on guiding the decoding processing with explicit signals of external indicators of usefulness beyond the scores produced by the internal attention network."
        },
        {
            "title": "Features from External Information",
            "content": "Our goal is to incorporate external explicit indicators for LLMs to utilize internal knowledge stored in their parameters. Thus, the first step is to construct the indicators by extracting quality features from the retrieved information. The most intuitive feature is the relevant score computed by the retriever model in terms of the given query and candidate documents. In general, the retrieved ğ‘ ğ‘– }ğ‘˜ ğ‘–=1 for the query ğ‘ are associated top-k relevant documents {doc }ğ‘˜ with their relevance scores SRet = {ğ‘ Ret ğ‘–=1, each computed by similarity function as ğ‘ Ret . Since external indicators can be constructed in multiple ways, different features may be extracted and computed depending on the specific requirements, such as for faithfulness or trustworthiness. ğ‘ ğ‘– = qdoc ğ‘– ğ‘ ğ‘– doc ğ‘– WWW 26, April 13-17, 2026, Dubai, United Arab Emirates Fengran Mo et al. Figure 2: The framework of OpenDecoder, including Searching External Information with top-k retrieved documents, Indicators Construction based on the retrieved documents with various types of quality scores, teaching the model to leverage external explicit quality indicators for the Decoding Computation of LLM by modulating internal attention score computation and applying Robust Training, and finally obtaining the reshaped token probability distribution during content generation. ğ‘– ğ‘– In our implementation, we further leverage two additional indicators features, (i) the relevance judged by LLM-based ranker }ğ‘˜ as SRank = {ğ‘ Rank ğ‘–=1; and (ii) the query performance prediction (QPP) score SQPP judged by QPP model [34]. Specifically, we use the logit of the end-of-sequence token for the LLM-ranker ğ‘ judged score as ğ‘ Rank = Ranker(q, doc ğ‘– ) [1] following [32], and the logit of token relevant in the prediction of the QPP model ğ‘ = for each given document doc ğ‘– ğ‘ logit(cid:0)relevant (ğ‘, doc ğ‘– )(cid:1). The relevance judged by the LLM-based ranker is expected to provide semantic similarity features from another perspective and help to investigate whether these explicit LLM-judged signals have additional impacts or have been integrated in model internal processing implicitly. Besides, the QPP scores provide the indicators about the difficulty of the query, which might imply the possible noisy level of the retrieved information for the generator. in the candidate list as ğ‘ QPP ğ‘– Eventually, these scores calculated based on different aspects are used individually or as combination ğ‘† agg by an aggregation function to guide the LLMs to process the external information during generation, i.e., to decide to what extent it should focus on different parts of the input context in decoding."
        },
        {
            "title": "Features for Decoding",
            "content": "The fundamental problem in the current paradigm of RAG is that adding external retrieved information in the input prompt could only affect the online computation of key-value pairs in the attention networks of LLMs, which is not tailored to the input with noise. Since the retrieved context is usually not perfect, the inherent defects are only implicitly processed via the attention score computation, which is influenced by the mechanisms (e.g., predefined system prompt) in the pre-training procedure. Thus, better way is to inform the decoding with additional explicit indicators directly, so that the LLMs know how much they should rely on external or internal knowledge to generate an answer. ğ‘ , doc 2 To this end, we aim to teach the model to leverage the explicit indicator features from external information generated in Sec. 3.2, and integrate them into the original attention networks computation. Following the procedure of the standard RAG, the user query ğ‘ ğ‘ ğ‘– }ğ‘˜ and its corresponding retrieved top-k documents (ğ‘) = {doc ğ‘–=1 would fill the prompt template together with the instruction as ğ‘ ğ‘ [Instruction, doc , query] to instruct the LLM to , , doc ğ‘˜ 1 produce an answer. To teach the LLMs to leverage explicit indicator features, we first construct score distribution by concatenating any types of score {ğ‘ ğ‘– }ğ‘˜ ğ‘–=1 as features of the top-k retrieved documents and the pre-defined score ğ‘ ğ¼ and ğ‘ ğ‘ for the instruction and query ğ‘ as ğ‘† = [ğ‘ ğ¼ , ğ‘ 1, ğ‘ 2, , ğ‘ ğ‘˜, ğ‘ ğ‘]. Then, we initialize it by normalizing the feature scores of the retrieved documents {ğ‘ ğ‘– }ğ‘˜ ğ‘–=1 to [0, 1] and assign score 1 to the tokens in query and instruction as Eq. 2. The constructed score distribution ğ‘†norm Rğ‘† ğ‘† is tokenlevel matrix, i.e., each token has an initial score value. Finally, we incorporate the normalized scores ğ‘†norm as explicit indicators into the computation of attention networks in OpenDecoder modified according to relevance as ğœƒ attn open via Eq. 3. The intuition is that, by modulating the original attention scores with normalized indicator scores, the importance of each token during the autoregressive decoding would be reshaped to guide the model for answer generation. In extreme cases where all input documents are irrelevant and assigned very low relevance scores, the query and instruction receive relatively higher scores, guiding the model to disregard the retrieved context and instead rely on its parametric knowledge to generate an answer. ğ‘ norm ğ‘– = ğ‘ ğ‘– max({ğ‘  ğ‘— }ğ‘˜ ğ‘†norm = [ğ‘ norm ğ¼ ğ‘—=1) , {ğ‘ norm ğ‘— }ğ‘˜ 1 , ğ‘ norm ğ‘ , ğ‘ norm ğ¼ 1 , ğ‘ norm ğ‘ ] Rğ‘† ğ‘† (2) OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG WWW 26, April 13-17, 2026, Dubai, United Arab Emirates Algorithm 1 Modulating LLM internal decoding in OpenDecoder Input: Question ğ‘, Relevance score {ğ‘ ğ‘– }ğ‘˜ ğ‘–=1 of each input document, Normalization function Norm(), Original LLMğœƒ0 . Output: Updated LLMğœƒ0+ğœƒ attn 1: Normalize the relevance score among the input documents and generated answer ğ‘. open {ğ‘ norm ğ‘– ğ‘–=1 = Norm({ğ‘ ğ‘– }ğ‘˜ }ğ‘˜ ğ‘–=1). 2: Construct token-level score matrix ğ‘†norm Rğ‘† ğ‘† correspond to the input with question ğ‘ and instruction as Eq. 2. 3: Computation of modulated LLMs internal attention network LLMğœƒ0 with external relevance score ğ‘†norm via new parameter ğœƒ attn open as Eq. 3. 4: Generate final answer ğ‘ for ğ‘ via the updated LLMğœƒ0+ğœƒ attn . open open Attn(ğ‘„, ğ¾, ğ‘‰ , ğ‘†norm) = softmax ğœƒ attn (cid:18) ğ‘†norm ğ‘„ğ¾ ğ‘‘ğ‘˜ (cid:19) ğ‘‰ (3) The type of scores and normalization approach can be determined according to various criteria such as relevance, reliability, authority, etc. In our implementation, we investigate three types of scores through an aggregation function before the normalization. We expect the relevance score SRet to be dominant and the other two scores SRank and SQPP act as supplementary with scale constant 0.5, which is formulated in Eq. 4. ğ‘† agg norm = Normalize (cid:16) ğ‘ norm ğ‘– Ret max({ğ‘ Ret ğ‘ norm ğ‘– agg = (cid:16) Aggregate(SRet, SRank, SQPP) (cid:17) , where + 0.5 (ğ‘ norm ğ‘– Rank ğ‘— + 0.5 (ğ‘ Rank ğ‘— (cid:17) ) + ğ‘ norm ğ‘– QPP + ğ‘ QPP) }ğ‘˜ ğ‘—=1) ğ‘— (4) , ğ‘ norm ğ‘– agg ğ‘† agg norm Finally, we optimize to maximize the probability of producing the ground-truth ğ‘ with the given query and its corresponding retrieved top-ğ‘˜ documents set {doc}ğ‘˜ 1 as Eq. 5, where ğœƒ0 and ğœƒ attn open denote the LLMs original parameters and the learned parameters to leverage explicit quality indicator features during fine-tuning, respectively. During inference, the corresponding quality indicator features {ğ‘ ğ‘– }ğ‘˜ ğ‘–=1 are required by learned parameters ğœƒ attn open for computation of probability in Eq. 3. The core procedure of the information processing within the OpenDecoder is described in Algorithm 1. max ğœƒ (ğ‘,{doc}ğ‘˜ 1 ,ğ‘) (cid:16) log ğ‘ ğ‘¡ = ğ‘ƒğœƒ0+ğœƒ attn open (ğ‘ğ‘¡ ğ‘<ğ‘¡, ğ‘, {doc}ğ‘˜ 1 ) (cid:17) (5)"
        },
        {
            "title": "3.4 Robustness Training\nIt may often be the case that some retrieved documents are not\nrelevant. To make the training and inference more robust to noisy\ninformation, we conduct robustness training by replacing the sec-\nond half of the top-k retrieved documents {docğ‘– }ğ‘˜\nğ‘–=1 with partial\nrelevant ones {docpart-rel} and irrelevant ones {docirrel} as Eq. 6.\nThey are sampled from the top-ğ‘˜ set excluding the top-5 documents\nand the whole collection excluding the top-ğ‘˜ documents, respec-\ntively. The goal of constructing a noisy document list {doc}noisy\nis to provide a necessary environment for the model to learn to\ndistinguish the useful and noisy information. A further alternative\nis to shuffle the position of the noisy document list as {doc}shuffle\nnoisy ,\naiming to emphasize the impact of external signals and reduce the",
            "content": "common issue of position bias [11, 60] of retrieved documents in RAG. {doc}noisy = {docğ‘– }5 {docpart-rel} {docğ‘– }ğ‘˜ ğ‘–=6 1 {docpart-rel} {docirrel}, where , {docirrel} (C {docğ‘– }ğ‘˜ ğ‘–=1) (6) Then, the reconstructed noisy retrieved documents {doc}noisy or {doc}shuffle noisy with various levels of noise and random relative position are used for robustness training by replacing the original input documents list {doc}ğ‘˜ 1 in Eq. 5."
        },
        {
            "title": "4.4 Implementation Details\nWe implement OpenDecoder based on Qwen-2.5 series backbone\nmodels [58] with the official open-source code repository. The com-\npared baselines are also implemented with the same Qwen-2.5-3B-\nInstruct model as our main experiments. For retrieval, we use the",
            "content": "WWW 26, April 13-17, 2026, Dubai, United Arab Emirates Fengran Mo et al. Table 1: Main results with three evaluation settings across various noisy environments among the retrieved documents for different RAG systems. To ensure fair and thorough comparison, all methods are based on Qwen-2.5-3B-Instruct backbone models, and the input retrieved documents for each method are fixed to the same. The best and second-best performance is set in bold and underline. and denote significant improvements with t-test at ğ‘ < 0.05 over the strongest baseline RbFT and the Vanilla SFT without explicit external indicators for training, respectively. Ë†/ represents in-domain/out-of-domain datasets. Evaluation Method NQË† TrivialQA popQA HotpotQAË† 2Wiki Average Normal Noisy Extreme No RAG Vanilla RAG Vanilla SFT RobustRAG InstructRAG AstuteRAG RbFT OpenDecoder Vanilla RAG Vanilla SFT RobustRAG InstructRAG AstuteRAG RbFT OpenDecoder Vanilla RAG Vanilla SFT RobustRAG InstructRAG AstuteRAG RbFT OpenDecoder F1 12.11 25.46 33.63 26.58 30.39 37.84 40.17 39.26 15.22 34.98 25.21 28.09 32.36 35.50 37.71 3.33 19.78 3.84 5.52 16.06 21.49 22.50 EM - 34.12 32.63 30.80 31.00 34.10 36.60 35.90 32.70 32.83 30.20 29.33 29.00 30.70 33.82 10.14 16.73 3.93 7.40 9.50 17.10 18.06 F1 30.04 31.09 50.31 45.25 45.81 52.28 53.49 56.08 26.82 48.54 42.36 44.13 46.81 52.62 55.09 11.96 34.76 7.39 21.51 35.03 38.18 40.41 EM - 48.40 50.53 47.40 50.73 51.80 52.30 54.87 49.93 48.07 44.53 48.20 48.70 51.70 53.33 18.00 33.40 7.13 24.80 27.10 33.50 38.27 F1 11.07 7.35 20.46 10.91 16.19 23.92 24.73 25.95 7.83 21.06 10.33 14.25 20.28 23.71 25.07 0.98 19.27 0.39 1.62 15.74 21.59 24.96 EM - 21.87 17.37 15.90 21.70 19.90 21.42 22.80 20.66 18.16 14.80 21.40 16.60 20.20 22.02 11.87 18.37 1.20 0.70 12.80 20.80 22.02 F1 16.86 12.06 24.02 13.58 20.26 29.44 29.71 29.44 11.05 23.55 12.04 18.16 23.63 25.28 28.76 4.20 18.26 1.60 9.14 14.38 22.11 23.59 EM - 20.73 20.17 15.73 22.93 23.30 24.50 24.00 19.00 20.80 14.13 11.60 17.00 19.00 22.77 9.67 15.07 4.67 5.80 10.60 15.50 17.20 F1 22.38 11.23 19.91 5.84 17.37 20.79 23.02 23.63 11.97 22.07 5.30 15.30 20.84 23.60 24.17 7.41 21.76 1.18 11.25 17.36 24.28 26.99 EM - 20.93 20.33 9.07 20.87 21.10 21.90 22.53 20.38 20.40 8.20 9.00 18.60 22.00 22.13 13.20 19.93 3.13 6.80 15.10 22.60 24.00 F1 18.49 17.44 29.67 20.43 26.00 32.85 34.22 34.87 14.58 30.04 19.05 23.99 28.78 32.14 34.16 5.58 22.77 2.88 9.81 19.71 25.53 27.69 EM - 29.21 28.21 23.78 29.45 30.04 31.34 32.02 28.53 28.05 22.37 23.91 25.98 28.72 30.81 12.58 20.70 4.01 9.10 15.02 21.90 23.91 2018 Wikipedia dump [22] as the knowledge source and E5 [52] as the retriever, with the number of retrieved documents set to 10, following [48, 56]. For the robustness training, the number of relevant, partially relevant, and irrelevant documents is set to the same as the noisy evaluation, as 5, 3, and 2, respectively. The partially relevant and irrelevant documents are randomly sampled five times from corresponding document sets and fixed for all compared methods for fair comparison. For training, we merge the training sets of NQ and HotpotQA to form unified training dataset for OpenDecoder and other fine-tuning-based baselines following [18]. The training epoch is set to 1 to ensure the model learn to use the explicit guidance and generalizes to out-of-domain evaluation datasets without overfitting. Evaluation is conducted on the test sets of five datasets to assess both in-domain and out-of-domain performance. F1 score and Exact Match (EM) are used as the evaluation metrics, following [48, 56]. More implementation details can be found in our public code repository at https://github.com/fengranMark/OpenDecoder."
        },
        {
            "title": "5 Experimental Results\n5.1 Main Results\nThe overall performance of OpenDecoder is presented in Table 1. It\nis tested on five datasets, with three evaluation settings of different\nnoisy environments in terms of the input retrieved documents. We\ncan make the following observations:",
            "content": "(1) Our OpenDecoder consistently outperforms most compared baseline methods on three evaluation settings and significantly surpasses the Vanilla SFT approach without external indicators. Beyond the noisy and extremely noisy evaluation, the retrieved top-k documents in the normal evaluation might still contain noise in the input for answer generation (We will investigate the impact of noise Sec. 5.5). Thus, these results demonstrate the superior effectiveness of our OpenDecoder in tolerating noise, which can be attributed to our designed mechanism of modulating the decoding using external relevance signals as indicators and enabling the LLMs to grasp such capacity via specific training. (2) Compared to other approaches targeting robustness improvement (RobustRAG and RbFT), our OpenDecoder exhibits more robust answer generation in noisy and extremely noisy settings. This is mainly because the compared methods still follow the current approach of internal information processing mechanism of the LLMs, which highly rely on the original capacity of the LLMs for distinguishing noise and the bias influenced by system prompts during pre-training. Modulating the LLM decoding with explicit indicators can not only provide useful signals but also alleviate this bias effect. (3) When the noise in the retrieved document increases, the performance drop is more severe in the relatively simple datasets (NQ and TrivialQA) compared with the other more complex ones (HotpotQA, 2wiki). This means the factoid questions with retrieved support evidence are more sensitive to the input with various noisy levels, thus the external indicators are more useful and necessary; while for the more difficult datasets, the retrieval defects are more common, and thus the urgent goal is to improve the success rate of retrieving relevant documents before aiming to enhance the robustness of the answer generation. OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG WWW 26, April 13-17, 2026, Dubai, United Arab Emirates Table 2: Ablation studies on the effectiveness of each mechanism in our OpenDecoder training framework. Table 3: The performance using different document position orders in robust training across five datasets. NQ TrivialQA popQA HotpotQA 2Wiki Method Normal Evaluation 33.63 Vanilla SFT 37.62 w/. Guidance 36.24 w/. Aggregate 38.98 w/. Robust Tr. 39.26 OpenDecoder Noisy Evaluation Vanilla SFT w./ Guidance w/. Aggregate w/. Robust Tr. OpenDecoder Extreme Noisy Evaluation Vanilla SFT w/. Guidance w/. Aggregate w/. Robust Tr. OpenDecoder 19.78 21.89 21.07 22.22 22.50 34.98 37.30 36.42 37.43 37.71 50.31 55.31 55.48 55.84 56. 51.37 53.35 53.84 54.57 55.09 34.76 39.03 39.57 40.33 40.41 20.46 24.33 21.59 25.14 25.95 21.06 24.05 23.96 24.56 25.07 19.27 24.58 24.26 25.61 24.96 24.02 26.06 28.86 29.43 29. 23.55 25.78 28.39 28.39 28.76 18.26 20.28 23.25 23.36 23.59 19.91 20.15 22.85 22.72 23.63 22.07 23.65 23.38 23.33 24.17 21.76 22.79 26.77 26.52 26."
        },
        {
            "title": "5.3 Feature Aggregation and Normalization\nIn this section, we further investigate the impact of aggregating\nand normalizing various scores for answer decoding.\nAggregation. The results of score aggregation are depicted in Fig-\nure 3. We can see that aggregating any types of relevant scores can\nachieve better results compared to the Vanilla SFT without explicit\nindicators. Leveraging the retrieval score SRet alone could be suf-\nficient for the general QA datasets (NQ, TrivialQA, and popQA),\nwhere aggregating more features might not always bring additional\ngain. This might be because when one indicator feature is satisfied,\nadding the others might raise the risk of interference, as these fea-\ntures are measured from different aspects. For the multi-hop QA\ndatasets (HotpotQA and 2wiki), aggregating more feature scores\nhelps to achieve better performance, which implies that complex\nquestions desire more external indications to generate correct an-\nswers. In addition, the improvement with aggregating LLM-based\nranker score SRank compared to vanilla SFT demonstrates that the\ninternal information processing of LLMs cannot implicitly ignore",
            "content": "Method Original w/. Reverse w/. Shuffle w/. Noise NQ TrivialQA popQA HotpotQA 2Wiki 22.07 35.42 22.99 36.39 23.33 37.43 24.17 37.71 52.57 53.68 54.57 55.09 20.26 27.91 28.39 28.76 20.13 21.47 24.56 25.07 the noise, which emphasizes the importance of impacting the decoding of LLMs with explicit relevant indicators as our OpenDecoder. Normalization. The results of applying three normalization approaches on aggregating retrieval score SRet are shown in Figure 4. The Max Normalization is the simplest one, as denoted in Eq. 2. The other two normalization approaches, Min-Max and ExponentialRank, are implemented as Ë†ğ‘ min-max ğ‘ ğ‘– min( {ğ‘  ğ‘— }ğ‘˜ and = ) ğ‘– max( {ğ‘  ğ‘— }ğ‘˜ ğ‘— =1 ğ‘— =1 ) min( {ğ‘  ğ‘— }ğ‘˜ ) ğ‘— =1 Ë†ğ‘ Exp ğ‘– = ğ‘’ 0.5(ğ‘– 1) ğ‘— =1 (cid:205)ğ‘˜ ğ‘’ 0.5( ğ‘— 1) , where the former one considers the relative gap among the original scores and the latter one further consider the impact of the rank position with exponential decay for each document candidate. We can observe that the Max normalization performs better than the Min-Max one on general QA datasets, and vice versa on the multi-hop QA datasets. The more complex Exponential normalization with rank decay results in large performance drop. These observations indicate that applying different normalizations will significantly impact the performance, i.e., appropriate normalization can obtain improvement, while the inappropriate ones would result in performance drop, even under the same pipeline in our OpenDecoder. Thus, more sophisticated approach could be further explored in future studies. ğ‘ , doc 2 ğ‘ , , doc ğ‘˜"
        },
        {
            "title": "5.4 Document Order in Robust Training\nIn this section, we examine the effect of varying document position\norders on robust training. As mentioned in Sec. 3.3, the original\ninput context order before applying robust training is Input =\nğ‘\n[Ins., doc\n, q]. On top of it, we investigate three\n1\ntypes of reorder methods, including reversing the document posi-\nğ‘\nğ‘\ntion from doc\n1 , shuffling them, and further injecting noise\nğ‘˜ to doc\nwith various relevant levels as Sec. 3.4. The results are presented in\nTable 3. We observe that reversing the document order can obtain\nbetter performance than the original one. This might be because\nğ‘\n, q]\nthe new reversed order InputRev. = [Ins., doc\nğ‘˜\nenables the higher top-ğ‘˜ documents to be much closer to the ques-\ntion and thus might raise their attention score by alleviating the\nlong-distance distraction. This phenomenon suggests that specify-\ning document positions in the prompt template as plain text may\nnot be fully interpreted by LLMs. Consequently, shuffling input\ndocuments during training can mitigate position bias, as the top-1\ndocument is not always more informative than the top-2 for an-\nswer generation. Moreover, injecting noise further enhances model\nrobustness by encouraging it to assess the true relevance of input\ndocuments based on external indicators, rather than relying on\npositional cues.",
            "content": "ğ‘ , , doc 1 ğ‘ , doc ğ‘˜"
        },
        {
            "title": "5.5 Noise Tolerance of Input Top-K\nAs the evidence for the correct answer might relate to only a small\nportion of the relevant documents, the normal evaluation using",
            "content": "WWW 26, April 13-17, 2026, Dubai, United Arab Emirates Fengran Mo et al. Figure 3: Performance of aggregating various scores as guidance features across different evaluation settings and datasets. Figure 4: Performance of normalizing scores features with various approaches across different evaluation settings and datasets. Figure 5: The performance of using various top-k retrieved documents in the normal evaluation setting. Figure 6: Comparison between SFT and OpenDecoder of scaling model size across five datasets in the noisy evaluation setting. the original top-ğ‘˜ retrieved results would still inevitably contain irrelevant information. We evaluate the noise tolerance ability of Vanilla SFT and our proposed OpenDecoder in terms of the impact of various input top-ğ‘˜ values. The results are shown in Figure 5. As the number of input documents increases, the probability of identifying relevant documents with answer information and the degree of injecting potential noise both increase. In most of the datasets, the larger top-ğ‘˜ cannot guarantee higher performance except on TrivialQA, which indicates that the accurate search results are crucial for answer generation. Overall, our OpenDecoder exhibits better performance than Vanilla SFT in different numbers of input documents, which demonstrates the effectiveness of leveraging relevance score to impact decoding across various input top-k."
        },
        {
            "title": "5.6 Investigation of Scaling Model Size\nWe further investigate the impact of scaling up model size for vanilla\nSFT and our OpenDecoder. The results in the noisy evaluation set-\nting are depicted in Figure 6. Overall, both the SFT and our proposed\napproaches benefit from larger model sizes, suggesting that larger\nmodels are more capable of tolerating contextual noise, which aligns\nwith prior studies [21]. Moreover, the effectiveness of leveraging\nexplicit indicators to influence answer generation becomes more",
            "content": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG WWW 26, April 13-17, 2026, Dubai, United Arab Emirates pronounced with larger models, whereas smaller models (e.g., 1.5B) do not consistently achieve better performance across all datasets. These observations indicate that effectively integrating external signals with internal LLM reasoning processes is non-trivial task that demands higher model capacity. similar trend is observed when aggregating multiple guidance score features, implying that this aggregation process also requires implicit learning during training. Therefore, designing more sophisticated learning objectives to better incorporate this aggregation mechanism and employing larger backbone models for training OpenDecoder could further enhance performance, which we leave for future work. Results about the evaluation in the other two settings are provided in Appendix C."
        },
        {
            "title": "6 Conclusion\nIn this paper, we propose a new paradigm to modulate the LLMsâ€™\ninternal information processing mechanisms with explicit indica-\ntors to improve robustness in answer decoding when the input\ncontext contains various noise. To achieve the goal, we proposed\nOpenDecoder framework, which constructs various explicit quality\nindicators via extracting features from the retrieved document and\napplies them to modify the attention score computation among the\nnetworks of LLMs. Additionally, a robustness enhancement mecha-\nnism is integrated into the training procedure to enable LLMs to\nhandle various noisy environments. Our experiments demonstrate\nthat incorporating explicit indicators from retrieved information\nin RAG tasks enhances the LLMsâ€™ ability to tolerate noise in the\ninput context and leads to better performance compared to prior\napproaches. Importantly, this paradigm is flexible to be integrated\nwith the post-training of LLMs for any purposes and incorporated\nwith any type of external indicators.",
            "content": "References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The International Conference on Learning Representations. [3] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation. In First Conference on Language Modeling. [4] Chia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh, Menghai Pan, ChinChia Michael Yeh, Guanchu Wang, Mingzhi Hu, Zhichao Xu, Yan Zheng, Mahashweta Das, et al. 2024. Main-rag: Multi-agent filtering retrieval-augmented generation. arXiv preprint arXiv:2501.00332 (2024). [5] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2021. Rethinking Attention with Performers. In International Conference on Learning Representations. [6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research 25, 70 (2024), 153. [7] Boyi Deng, Wenjie Wang, Fengbin Zhu, Qifan Wang, and Fuli Feng. 2025. Cram: Credibility-aware attention modification in llms for combating misinformation in rag. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. [8] Qian Dong, Qingyao Ai, Hongning Wang, Yiding Liu, Haitao Li, Weihang Su, Yiqun Liu, Tat-Seng Chua, and Shaoping Ma. 2025. Decoupling Knowledge and Context: An Efficient and Effective Retrieval Augmented Generation Framework via Cross Attention. In Proceedings of the ACM on Web Conference 2025. [9] Yibing Du, Antoine Bosselut, and Christopher Manning. 2022. Synthetic disinformation attacks on automated fact verification systems. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 1058110589. [10] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2023. Retrieval-Augmented Generation for Large Language Models: Survey. preprint arXiv:2312.10997 (2023). [11] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594 (2024). [12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International conference on machine learning. PMLR, 39293938. [13] Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Kwan Ho Ryan Chan, Shirley You Ren, Andrew Miller, Udhyakumar Nallasamy, and Jaya Narain. 2025. Do LLMsknowinternally when they follow instructions?. In The Thirteenth International Conference on Learning Representations. [14] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics. 66096625. [15] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research 24, 251 (2023), 143. [16] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating LLM hallucination via self reflection. In Findings of the Association for Computational Linguistics: EMNLP 2023. 18271843. [17] Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Arik. 2024. Long-context llms meet rag: Overcoming challenges for long inputs in rag. arXiv preprint arXiv:2410.05983 (2024). [18] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516 (2025). [19] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 16011611. [20] Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, and Sung Ju Hwang. 2023. Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks. Advances in NeurIPS (2023). [21] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020). [22] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for OpenDomain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 67696781. [23] To Eun Kim and Fernando Diaz. 2025. Towards fair rag: On the impact of fair ranking in retrieval-augmented generation. In Proceedings of the 2025 International ACM SIGIR Conference on Innovative Concepts and Theories in Information Retrieval (ICTIR). 3343. [24] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics 7 (2019), 453466. [25] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems 33 (2020), 94599474. [26] Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. 1087910899. [27] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366 (2025). [28] Xiaoqiang Lin, Aritra Ghosh, Bryan Kian Hsiang Low, Anshumali Shrivastava, and Vijai Mohan. 2025. REFRAG: Rethinking RAG based Decoding. arXiv preprint arXiv:2509.01092 (2025). [29] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM computing surveys 55, 9 (2023), 135. [30] Yu-An Liu, Ruqing Zhang, Jiafeng Guo, and Maarten de Rijke. 2025. Robust information retrieval. In Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining. 10081011. [31] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting in retrieval-augmented large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 53035315. WWW 26, April 13-17, 2026, Dubai, United Arab Emirates Fengran Mo et al. [32] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Finetuning llama for multi-stage text retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. [33] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 98029822. [34] Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, and Maarten de Rijke. 2025. Query performance prediction using relevance judgments generated by large language models. ACM Transactions on Information Systems 43, 4 (2025), 135. [35] Chuan Meng, Jiqun Liu, Mohammad Aliannejadi, Fengran Mo, Jeff Dalton, and Maarten de Rijke. 2026. Re-Rankers as Relevance Judges. arXiv preprint arXiv:2601.04455 (2026). [36] Fengran Mo, Yifan Gao, Zhuofeng Wu, Xin Liu, Pei Chen, Zheng Li, Zhengyang Wang, Xian Li, Meng Jiang, and Jian-Yun Nie. 2026. Leveraging historical information to boost retrieval-augmented generation in conversations. Information Processing & Management 63, 2 (2026), 104449. [37] Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, and Jian-Yun Nie. 2023. ConvGQR: Generative Query Reformulation for Conversational Search. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. 49985012. [38] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2020. KILT: benchmark for knowledge intensive language tasks. arXiv preprint arXiv:2009.02252 (2020). [39] Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia Zhou, Xu Chen, and Zhicheng Dou. 2025. Tackling the Length Barrier: Dynamic Context Browsing for Knowledge-Intensive Task. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1. 11501160. [40] Tolga Åakar and Hakan Emekci. 2025. Maximizing RAG efficiency: comparative analysis of RAG methods. Natural Language Processing 31, 1 (2025), 125. [41] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592 (2025). [42] Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, and Soujanya Poria. 2025. Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse. In The Thirteenth International Conference on Learning Representations. [43] Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, and Yiqun Liu. 2025. Parametric retrieval augmented generation. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. 12401250. [44] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 1491814937. [45] Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, and Jinjie Gu. 2025. RAG-R1: Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism. arXiv preprint arXiv:2507.02962 (2025). [46] Minghao Tang, Shiyu Ni, Jiafeng Guo, and Keping Bi. 2025. Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation. arXiv preprint arXiv:2507.19333 (2025). [47] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [48] Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, and Qingyao Ai. 2025. Robust Fine-tuning for Retrieval Augmented Generation against Retrieval Defects. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. 12721282. [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [50] Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan Ã– ArÄ±k. 2024. Astute rag: Overcoming imperfect retrieval augmentation and knowledge conflicts for large language models. arXiv preprint arXiv:2410.07176 (2024). [51] Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. 2025. Retrievalaugmented generation with conflicting evidence. arXiv preprint arXiv:2504.13079 (2025). [52] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022). [53] Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, and Haifeng Wang. 2025. Unveiling Knowledge Utilization Mechanisms in LLMbased Retrieval-Augmented Generation. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. 12621271. [54] Zhepei Wei, Wei-Lin Chen, and Yu Meng. 2024. InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales. In The Thirteenth International Conference on Learning Representations. [55] Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, and Benjamin Van Durme. 2024. Defending Against Disinformation Attacks in Open-Domain Question Answering. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics. 402417. [56] Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. 2024. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556 (2024). [57] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient Streaming Language Models with Attention Sinks. In The Twelfth International Conference on Learning Representations. [58] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [59] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 23692380. [60] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. 2024. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736 (2024). [61] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Rankrag: Unifying context ranking with retrieval-augmented generation in llms. Advances in Neural Information Processing Systems 37 (2024), 121156121184. [62] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. 2025. Inference Scaling for Long-Context Retrieval Augmented Generation. In The Thirteenth International Conference on Learning Representations. [63] Jinghan Zhang, Fengran Mo, Xiting Wang, and Kunpeng Liu. 2024. Blind Spot Navigation in LLM Reasoning with Thought Space Explorer. arXiv preprint arXiv:2410.24155 (2024). [64] Jinghan Zhang, Xiting Wang, Fengran Mo, Yeyang Zhou, Wanfu Gao, and Kunpeng Liu. 2025. Entropy-based exploration conduction for multi-step reasoning. arXiv preprint arXiv:2503.15848 (2025). [65] Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang, Dongjie Wang, and Kunpeng Liu. 2025. Ratt: thought structure for coherent and correct llm reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 2673326741. [66] Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, and Jinsong Su. 2025. FaithfulRAG: Fact-Level Conflict Modeling for ContextFaithful Retrieval-Augmented Generation. arXiv preprint arXiv:2506.08938 (2025). [67] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223 1, 2 (2023). [68] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160 (2025). [69] Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, and Zhenhao Li. 2025. Trustrag: Enhancing robustness and trustworthiness in rag. arXiv e-prints (2025). [70] Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, and Philip Yu. 2024. Trustworthiness in retrievalaugmented generation systems: survey. arXiv preprint arXiv:2409.10102 (2024)."
        },
        {
            "title": "Appendix\nA Datasets Details",
            "content": "Table 4: Statistics of the five used datasets. NQ TrivialQA popQA HotpotQA 2Wiki #Train #Test #Collection 79,168 3,610 - 11,312 - 1,399 21M 90,447 7,405 - 9,322 We use five benchmarks for evaluation, and the unified training set from NQ and HotpotQA to fine-tune our OpenDecoder. The OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG WWW 26, April 13-17, 2026, Dubai, United Arab Emirates Figure 7: Comparison between SFT and OpenDecoder of scaling model size across five datasets in the normal evaluation setting. Figure 8: Comparison between SFT and OpenDecoder of scaling model size across five datasets in the extreme noisy evaluation setting. statistics of the used datasets are presented in Table 4 and their detailed description are shown below: NaturalQuestion (NQ) is factoid dataset whose questions consist of real anonymized, aggregated queries issued to the Google search engine. TrivialQA is reading comprehension dataset whose questionanswer pairs authored by trivia enthusiasts and independently gathered evidence documents that provide high quality distant supervision for answering the questions. PopQA assesses factual question answering, challenging the models ability to recall accurate knowledge and resolve ambiguity in entity representation. HotpotQA focuses on evaluating multi-hop reasoning skills, requiring models to combine information from different contexts to address single query. 2WikiMultihopQA (2wiki) is dataset designed to test the models ability to perform multi-hop reasoning by integrating information across multiple Wikipedia passages. Baseline Details All compared baselines are implemented by us using the same retrieved document sets across evaluation settings to guarantee fairness in comparison. The instruction used for Vanilla RAG, Vanilla SFT, and our OpenDecoder is the same as You should answer the question by referring to the retrieved knowledge provided below and integrating the usefulness of your own parametric knowledge. Just directly answer it as short answer without any explanation. For the prompting-based methods, RobustRAG, InstructRAG, and AstuteRAG, we inherit their original instruction provided in the corresponding code repository. For the fine-tuning-based methods RbFT, we also use its original instruction, but set the same hyperparameter as OpenDecoder. More Results on Model Scaling The results in the normal evaluation and extreme noisy setting are depicted in Figure 7 and Figure 8, respectively. Overall, similar trends are observed in the noisy evaluation setting of Sec. 5.6, where larger models are more capable of tolerating contextual noise. Besides, the improvement in scaling model size is more pronounced in complex QA datasets than in general ones, indicating that larger model may be equipped with more powerful reasoning ability implicitly. Discussion on Time and Space Efficiency The computation cost of our method is the same for the offline training and online inference. The computation complexity of the Vanilla SFT method and our OpenDecoder are (ğ‘‘ 2â„ + ğ‘‘ â„2) in the RAG setting, where ğ‘‘ is the average number of tokens in document doc, and â„ is the hidden dimension size of the decoderonly LLMs. This is because the explicit guidance, i.e., the relevance scores, are produced simultaneously with the retrieved documents, and the normalization of the scores should be negligible. In terms of the storage overhead, the normalized score ğ‘†norm Râ„â„ is stored as token-level metric, whose shape is the same as the Query, Key, and Value metric in the attention computational network inside the LLMs. Thus, the additional storage overhead compared to Vanilla SFT is (nh), where ğ‘› is the number of Transformer layers with the impact of explicit guidance."
        }
    ],
    "affiliations": [
        "Clemson University",
        "Georgia Institute of Technology",
        "University of Notre Dame",
        "UniversitÃ© de MontrÃ©al",
        "Waseda University"
    ]
}