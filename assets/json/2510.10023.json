{
    "paper_title": "Skill-Targeted Adaptive Training",
    "authors": [
        "Yinghui He",
        "Abhishek Panigrahi",
        "Yong Lin",
        "Sanjeev Arora"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models often show little to no improvement (i.e., \"saturation\") when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student's answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. Our code is available at: https://github.com/princeton-pli/STAT."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. Skill-Targeted Adaptive Training Yinghui He Princeton Language and Intelligence, Princeton University {yh0068, ap34, yl7690, arora}@princeton.edu Abhishek Panigrahi Yong Lin Sanjeev Arora 5 2 0 2 1 1 ] . [ 1 3 2 0 0 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Language models often show little to no improvement (i.e., saturation) when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce new fine-tuning strategy, STAT, to train such student model by using the metacognition ability of stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the students answers, the teacher creates Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. 1 Models MATH MATHD MATH2 GSM8K AMC23 MATH-perturb AIME simple hard 2024 2025 Llama-3.2-3B-Instruct 44.0 44.8 45.4 51.5 50.2 +SFT +GRPO STAT-Sel STAT-Syn 18.2 22.9 24.4 26.6 31.7 21.9 21.0 23.3 25.7 26. 73.0 75.1 77.4 80.2 79.2 21.7 20.8 25.8 24.7 23.9 33.7 33.0 38.4 39.8 39.1 12.2 12.2 11.8 13.3 14.7 33.3 30.0 33.3 43.3 40.0 16.7 20.0 6.7 23.3 30. Avg. 30.5 31.1 31.8 36.5 37.2 Table 1: STAT significantly enhances the performance of Llama-3.2-3B-Instruct on various math benchmarks by targeting its missing skills in solving MATH. See Table 3 for full evaluation results."
        },
        {
            "title": "Introduction",
            "content": "Language models have demonstrated remarkable success at acquiring knowledge from large-scale natural text corpora through the next-token prediction objective (Shannon, 1951). Subsequent supervised fine-tuning on curated data using the same objective then leads to strong performance on domain-specific tasks such as mathematics. However, this process is often inefficient and data hungry (Kaplan et al., 2020; Muennighoff et al., 2023; Zhang et al., 2024a; Villalobos et al., 2024), with models quickly reaching saturation point for Equal contribution. 1 https://github.com/princeton-pli/STAT. 1 Preprint. Under review. Figure 1: STAT is three-stage skill-based data selection/generation method for supervised fine-tuning (SFT). Stage 1: Identify difficult questions for each model using reward filtering on model responses. Stage 2: Use frontier LLMs to analyze the model responses and build model-specific Missing-Skill-Profile. Stage 3: Use pre-constructed Skill-Map to map the missing skill distribution to training question distribution, which constitutes the STAT-Sel data. STAT-Syn synthesizes new questions using frontier LLMs targeted to the missing skills. fixed dataset whereby further training does not help performance. Several works have suggested that this saturation happens because the loss is an average over data points, causing the training signal to diminish as the model becomes adept at most of the training examples (Chen et al., 2023; Xie et al., 2023a; Lin et al., 2024; Tong et al., 2024; Jiang et al., 2024; Xue et al., 2025; Zhang et al., 2025). In addition, there is mismatch between the average next-token prediction loss used during training and the auto-regressive generation process used to evaluate performance. As result, the average loss may fail to capture the specific generation errors that remain in saturated model (Arora et al., 2022; Fang et al., 2024). To tackle this saturation, prior works have shown that adapting the training data distribution can boost performance on inference-time tasks. The key idea is to focus the next-token prediction loss on an adapted set of examples targeted towards good generation (Xia et al., 2024; Yu et al., 2024b; Lin et al., 2024). This is primarily done by using embeddings or gradientbased estimates to pick training examples most relevant to reducing loss on reference validation set. While these methods show benefits, they remain anchored to validation-set loss, which is only coarse proxy for models actual generation errors. In fact, our experiments reveal that embedding-based methods, which adapt training data by measuring similarity to validation questions the model fails on, can be ineffective (Section 3.2) on saturated models that have undergone extensive post-training, e.g., Llama-instruct models. We propose to address the saturation problem by drawing inspiration from pedagogical practices rooted in cognitive science, which customize training to specially target the students skill-deficiencies (Bandura & Walters, 1977; Hattie & Timperley, 2007). How can we effectively use todays strong teacher models to design better training strategies to help small models overcome their saturation? We turn to growing line of research in LLM meta-cognition (Didolkar et al., 2024; Kaur et al., 2024), which leverages the predictive abilities of frontier LLMs to reason about the high-level skills required to solve given task, as well as the skills actually being used in the students answer. Thus, in principle, frontier LLMs can act as the teacher who guides the training process of the student model, actively monitoring the students competence on individual skills and adjusting their training examples. Informal description of data design: Our pipeline starts with list of relevant skills for the problem created via teacher metacognition (Didolkar et al., 2024), and adds three stages. 2 Preprint. Under review. First, we use the teacher to evaluate the student model on small validation set of questions and use reward model to identify the questions that are difficult for the student. Second, we create Missing-Skill-Profile by using the teacher to check the missing skills in the model responses. Our first method STAT-Sel simply up-weights training examples using the Missing-Skill-Profile; in effect, this guides the student to focus on their deficiencies. Our second (more expensive) method STAT-Syn uses the teacher to generate synthetic training data using in-context examples from the validation set associated with list of deficient skills in Missing-Skill-Profile. Key findings: Applying STAT-Sel and STAT-Syn teaching on Llama and Qwen models with the popular MATH (Hendrycks et al., 2021) data shows the following: 1. Substantial in-distribution gains: STAT achieves improvement on MATH by up to 7.5%, whereas naive fine-tuning yields negligible gains. Previous embedding-based data selection strategies that adapt to the students validation errors are found ineffective (Section 3.2). 2. Strong out-of-distribution (OOD) generalization: Improvements in difficult and OOD benchmarks such as AIME24/25 and AMC23 highlight the general utility of skill-targeted training (Section 3.2). 3. Adaptivity to evolving tasks: Extending the previous observation, we show that STAT-Sel and STAT-Syn can be continually adapted to new, harder evaluation settings, i.e., new validation set, while still leveraging the same training set (Section 3.3). 4. Supplementary benefits over reinforcement-learning (RL): We show that STAT followed by RL improves upon RL-only training, such as GRPO (Shah et al., 2024) (Section 3.2). This suggests that STAT is likely to prove relevant to most training pipelines today. We conduct extensive ablations to pinpoint the success of our proposed methods. finegrained skill-level analysis reveals that even when models have been extensively trained on MATH, they struggle on basic operation skills such as basic algebra (Section 4). By explicitly targeting these basic skills, our methods reduce such errors and improve generation performance, including on out-of-distribution tasks. In contrast, alternative approaches such as embedding-based methods often underperform, as they do not explicitly address these skill gaps (Figure 2). Thus, our findings demonstrate the robustness and broad applicability of skill-aware targeted training."
        },
        {
            "title": "2 STAT: Adapting training to model’s missing skills",
            "content": "Let be the set of test-set questions, out of which we use subset Qval as validation data and Qtest= QQval as evaluation data. We also have access to set of training questions P, which has been utilized to train the student model during its pre-training or post-training phase, and naively fine-tuning the model on offers little to no improvement. In our experiments, we use the test and training dataset from MATH, denoted as and respectively. We aim to build targeted training dataset Ptargeted to train the model further. Our work builds on using metacognitive abilities of frontier models from Didolkar et al. (2024), which we describe here. While hard to define precisely, skill is informally defined as basic computation necessary to solve task at hand. For example, necessary skills to solve arithmetic tasks could be addition, subtraction, multiplication and division. We will use S, set of skills that are necessary to solve questions in and P. These skills are enlisted from large model like GPT-4o using an appropriate prompting strategy (Didolkar et al., 2024; Kaur et al., 2024). Then, we create Skill-Map to be map from skill to the set of training questions that require applying the skill, which we will also get by prompting the same LLM (Achiam et al., 2023). We use the skill set and the Skill-Map from Didolkar et al. (2024). 3 Preprint. Under review. To develop STAT-Sel and STAT-Syn, we first identify difficult questions for the student model on validation set by analyzing its own responses. For these questions, we then use the teacher model to infer the skills that are missing from the students reasoning. skill-targeted training set is constructed by emphasizing examples corresponding to these missing skills, either via up-weighting samples or synthesizing new ones. Unless otherwise specified, all of our experiments use GPT-4o-mini as the teacher model. 2.1 Stage 1: Detection of difficult questions via reward filtering In this stage, we will label question as difficult or not for the student model. We could simply define difficult questions as the set of questions that the model gets wrong after evaluation. However, this requires access to the ground truth labels. Instead, to make our technique more broadly applicable, we use reward model to classify the responses of the student model. The reward model need not be perfect reward model; we give more ablations in Section E.1. Given question q, we use reward model to score the response of the student model. Reward filtering. As we primarily focus on math datasets, we assume that the models response is composed of steps for question and contains the answer in its final step. We will use the reward model to output reward scores for each step. For simplicity, we will refer to the scores of the reward model as {rq,1, , rq,t}. Then, we use thresholds τ1, τ2 to filter out difficult questions for the student model. We will refer to the threshold filtering function as {0, 1}. 0, (if) rq,t τ1 (final step has low reward) rq,i τ (average low reward across all steps) (1) (low reward at any step) R(q) = (or) 1 i=1 (or) < s.t. rq,i τ2 1, otherwise, Identifying difficult questions. We define Qdifficult as model-specific subset of the MATH dataset, consisting of questions with low-reward model responses R. To avoid training directly on the test data, we use two splits of Qdifficult as validation and test sets: difficult: Difficult questions in the validation set, given by Qdifficult Qval, are used to label missing skills in Stage 2. difficult: Difficult questions in the test set, given by Qdifficult Qtest, are used for Qval Qtest MATHD evaluation in Table 3. 2.2 Stage 2: Constructing model-specific Missing-Skill-Profile For each difficult question in Qval difficult, we use frontier LLM (GPT-4o-mini) to predict the set of skills in that are missing in the models responses. We call this map Missing-Skill-Profile Qval S. This map will be used to build STAT-Sel and STAT-Syn. See Section 4 for examples and an extensive analysis of Missing-Skill-Profile across models, and Section C.3 for prompts. difficult 2.3 Stage 3: Selecting or synthesizing skill-based training data In this stage, we construct our skill-targeted training dataset, Ptargeted. STAT-Sel. We create this set by directly sampling questions from the training dataset according to the skills listed in the Missing-Skill-Profile. Specifically, for each question Qval difficult, we examine Missing-Skill-Profile(q) and, for every skill it contains, sample multiple questions from that are linked to the same skill via the Skill-Map. Consequently, 4 Preprint. Under review. Method MATH-Train (Hendrycks et al., 2021) MATH-Augment (TIGER-Lab, 2024) MATH-Hard (Sun et al., 2024) Embed-Sel (Li et al., 2025) Embed-Syn (Jung et al., 2025) STAT-Sel (Ours) STAT-Syn (Ours) # Unique # QA Synthetic Questions Pairs Data 7.5k 7.5k 7.5k 9.5k 3k 4k 4k 4k 9.5k 9.5k 9.5k 9.5k 4k 9.5k Training Data Description MATH original training set. Augmented MATH training set with multiple teacherrewritten solutions per question. Subset of MATH-Augment with Level 45 MATH questions. Reweighted set of MATH-Augment via upweighting training questions similar to the difficult questions in embedding space. Synthetic MATH-level QAs generated by the teacher, using training examples from Embed-Sel as references. Reweighted set of MATH-Augment via upweighting training questions related to models missing skills in solving the difficult questions. Synthetic MATH-level QAs generated by the teacher, with training examples from STAT-Sel and their associated skills as references. Table 2: Comparison of training data construction methods. We attach details of data construction procedure in Section C.2. the frequency with which skill contributes to the selection process is proportional to the number of questions associated with that skill in the Missing-Skill-Profile. STAT-Syn. We generate new synthetic questions using the teacher model. For each question Qval difficult, we examine Missing-Skill-Profile(q). For each skill it contains, we randomly sample 3 questions from that are linked to the same skill via the Skill-Map, and ask the teacher model to create new questions and responses by referring to the sampled questions. We keep only those questions where the teacher model is consistent across at least 2 of its responses, and keep only those question-answer pairs in our training set. Detailed procedures are given in Section B."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup Datasets. All training data for STAT and the baselines are either selected or synthesized from the MATH dataset (7.5k train / 5k test) (Hendrycks et al., 2021). In addition to the original solutions provided in the dataset, we also collect three alternative versions of each answer by prompting the teacher model to re-write them three times. We further report performance of STAT and each baseline after continuing training with GRPO (Shao et al., 2024) on the same MATH training set. We randomly split the MATH test set into 1k validation and 4k test subsets, with both MATH and MATHD evaluations drawn from the 4k test split. See Section 2.1 for design details. We also evaluate our method on extensive OOD benchmarks including GSM8K (Cobbe et al., 2021), MATH2 (Shah et al., 2024), MATH-perturb (Huang et al., 2025), AMC23 (AI-MO, 2025), and AIME2024/2025 (HuggingFaceH4, n.d.; HuggingFaceH5, n.d.). Model & Training Configuration. We focus on smaller models as testbed, as their performance remains noticeably weaker on MATH. We employ GPT-4o-mini as the teacher model, and apply STAT on student models Llama-3.2-3B-Instruct, Llama-3.2-1B-Instruct (Meta AI, 2024), and Qwen2.5-3B (Qwen et al., 2025), and evaluate under 0-shot settings. We fine-tune models for 3 epochs, with learning rate chosen separately for each method based on accuracy on MATH. We provide detailed hyperparameters in Section C.1, ablations on threshold sensitivity in Section E.1, and discussion of teacher model variants in Section E.3. Baselines. We compare skill-aware training against several baselines. We begin with MATH-Train, where the model simply trains on the original MATH responses, and MATH5 Preprint. Under review. Methods MATH MATHD MATH2 GSM8K AMC23 MATH-perturb AIME simple hard 2024 2025 Avg. Base Model MATH-Train MATH-Augment MATH-Hard Embed-Sel Embed-Syn STAT-Sel STAT-Syn Base Model MATH-Train MATH-Augment MATH-Hard Embed-Sel Embed-Syn STAT-Sel STAT-Syn Base model MATH-Train MATH-Augment MATH-Hard Embed-Sel Embed-Syn STAT-Sel STAT-Syn Base model MATH-Train MATH-Augment MATH-Hard Embed-Sel Embed-Syn STAT-Sel STAT-Syn 44.0 44.8 45.2 45.6 46.0 48.8 51.5 50.2 45.4 46.4 47.4 49.4 50.4 49.7 52.2 51.0 55.8 50.0 56.6 56.7 57.5 56.4 58.4 59.4 61.6 61.6 61.0 59.0 59.7 61.4 62.8 61.8 18.2 22.9 23.9 24.9 26.5 27.3 26.6 31. 24.4 28.4 31.6 33.2 37.5 37.8 35.0 39.1 45.3 44.2 45.6 45.6 46.4 47.4 47.6 49.2 49.8 51.1 48.2 51.1 48.9 52.3 52.1 52.4 21.9 21.0 23.8 23.3 20.5 19.5 25.7 26.2 23.3 28.6 28.6 28.6 23.8 19.5 32.4 29.0 25.8 29.7 30.6 31.3 32.0 33.9 34.2 31. 21.7 20.8 23.8 21.6 21.6 22.7 24.7 23.9 Llama-3.2-3B-Instruct + SFT 73.0 75.1 77.8 78.2 76.6 78.4 80.2 79.2 + GRPO 77.4 80.7 81.4 80.3 80.5 80.6 81.8 82.0 Qwen2.5-3B + SFT 80.9 80.1 80.4 79.8 80.4 80.4 82.3 81.3 + GRPO 85.1 84.8 84.0 84.2 84.3 83.7 84.8 85.6 26.4 33.6 33.0 33.6 33.6 35.2 35.5 34.4 37.7 36.9 36.3 37.7 38.4 38.8 38.8 39.2 34.8 32.9 37.1 31.4 34.3 34.3 39.5 40.5 41.0 34.8 40.5 35.7 41.0 40.0 44.8 41. 33.7 33.0 35.1 38.0 36.2 36.9 39.8 39.1 38.4 37.6 37.6 39.1 38.0 39.1 42.7 43.0 43.7 42.3 40.9 43.7 43.7 43.7 45.9 44.8 49.8 51.6 48.7 49.8 46.6 47.7 48.7 50.9 12.2 12.2 12.5 11.8 14.7 13.3 13.3 14.7 11.8 12.5 14.0 15.4 16.8 16.8 17.6 15. 24.0 23.3 21.9 23.7 21.9 24.0 24.0 25.1 25.8 26.5 26.2 26.5 25.8 28.0 30.1 26.9 33.3 30.0 30.0 30.0 36.7 26.7 43.3 40.0 33.3 36.7 36.7 43.3 36.7 36.7 43.3 46.7 23.3 26.7 16.7 30.0 30.0 26.7 33.3 36.7 33.3 33.3 36.7 33.3 26.7 26.7 36.7 40. 16.7 20.0 13.3 26.7 16.7 23.3 23.3 30.0 3.3 10.0 33.3 13.3 20.0 23.3 26.7 33.3 20.0 26.7 26.7 16.7 26.7 26.7 30.0 30.0 30.0 30.0 26.7 23.3 36.7 30.0 33.3 36.7 30.5 31.1 31.7 33.3 32.8 33.0 36.5 37.2 31.8 34.5 37.9 37.1 38.8 38.6 40.7 41. 39.4 40.0 39.9 40.1 41.6 41.6 44.1 44.6 46.0 45.6 45.4 44.5 45.3 45.4 48.0 48.4 Table 3: Improvements on various math benchmarks from applying STAT. Results under +SFT show the performance of SFT models trained with each method, while +GRPO shows the performance after applying GRPO on top of the corresponding SFT models. Our methods, STAT-Sel and STAT-Syn, achieve an average gain of up to 6.7% over the base model, with strong OOD performances (AMC23 results reported on average@64, AIME on pass@64). Applying GRPO on top of fine-tuning with STAT further enhances these improvements by 4%. Full results are provided for Llama-3.2-1B-Instruct in Table 7, Section D. Augment, which substitutes the responses with teacher re-written answers. We also compare against MATH-Hard, restricting training to only Level 45 questions. Finally, to test whether skills really matter in STAT-Sel and STAT-Syn, we swap them out for an embedding-based approach2, selecting training questions by their similarity to difficult validation questions from Stage 1. Please find summary in Table 2. We have attached detailed data creation procedure in Section C.2 and prompts in Section C.3. 2We use Alibaba-NLP/gte-Qwen2-7B-instruct (Li et al., 2023b) Preprint. Under review. Figure 2: Comparison among the Top 10 frequent skills present in STAT-Sel, Embed-Sel, and MATHTrain questions selected on Llama-3.2-1B-Instruct. The skills emphasized in both baselines, MATHTrain and Embed-Sel, align poorly with the actual Top 10 missing skills of the model (i.e., skills in STAT-Sel). Furthermore, the missing skills are not necessarily those most common in the original data distribution, as shown by the skill distribution of MATH-Train. 3.2 Evaluation Results We present results for Llama-3.2-3B-Instruct and Qwen2.5-3B in Table 3 and for Llama-3.21B-Instruct in Table 7, Section D. We refer to each untrained model as Base Model. Our findings can be summarized as follows. Naive SFT provides little to no benefit. Both MATH-Train and MATH-Augment yield at most 12% gain over the base model, showing that naive SFT offers negligible improvements. It is worth noting that we have systematically tuned hyper-parameters for naive SFT (details attached in Section C.1). In fact, we observe that Qwen2.5-3B can even degrade under MATH-Train. Restricting supervision to only the most difficult MATH questions (Levels 45) also fails to produce meaningful gains. natural idea is then to adapt training toward the models mistakes by selecting training questions semantically close to difficult validation examples. Using embedding similarity, Embed-Sel achieves only marginal over MATH-Train and MATH-Augment. Synthetic augmentation via Embed-Syn provides small additional boost, but the overall gains remain modest. Skill-targeted adaptive training shows substantial improvements. STAT achieves average gains of up to 6.7% on Llama-3.2-3B-Instruct, 5.2% on Qwen2.5-3B, and 3.4% on Llama-3.2-1B-Instruct, over the performance of base model. On closer analysis on MATHD test set of questions, we show that STAT-Syn substantially improves the performance of the model on difficult questions, compared to STAT-Sel, which leads to improved performance overall for Llama-3.2-1B-Instruct and Qwen2.5-3B. Benefits extend beyond MATH. On out-of-distribution benchmarks, we observe consistent improvements across 7 datasets, ranging from simpler problems in GSM8K to challenging competition sets such as AIME. Specifically, STAT-Sel and STAT-Syn improve averaged OOD performances by 5.3% and 5.8% respectively, with STAT-Syn generally excelling on harder tasks such as AIME and MATHD. This demonstrates that targeting skills generalizes extensively beyond the source training set. Compatibility with GRPO. natural concern is whether our methods can work well with RL-based methods such as GRPO, which typically follows SFT (Dubey et al., 2024; Guo et al., 2025). For both Llama and Qwen, improvements from SFT on STAT have carried over to subsequent GRPO, yielding average gains of up to 9.5% over GRPO on base model. Surprisingly, on Llama-3.2-1B-Instruct and Llama-3.2-3B-Instruct, where GRPO alone does not work well (improving 2.4%), SFT alone on STAT already produced better results than GRPO, and adding GRPO on top further boosts performance by 4%. 3.3 Continual learning on challenging benchmarks As our earlier results show, STAT already generalizes strongly to wide range of OOD tasks while using only MATH data for training. But in practice, models often face evaluation settings that grow harder over time. natural question then is: can we continue adapting the model to these tougher benchmarks while still using similar questions as MATH? 7 Preprint. Under review. For our case study, we consider MATH-perturbhard. We report performance for two model variants of Llama-3.2-3B-Instruct. STAT-ConSel takes model trained with STAT-Sel, and trains further with data creation pipeline identical to STAT-Sel, but with Missing-Skill-Profile built on validation questions from MATH-perturb-hard. STAT-ConSyn builds on STAT-Syn model with the same idea. In both cases, the evaluation benchmark only gives the skill profile, and the training examples still come from MATH. Figure 3: Continual learning results on MATH-perturb-hard. Further finetuning STAT models based on their missing skills on unseen data yields 34% gain (STAT-ConSel/ConSyn). As shown in Figure 3, STAT-Sel and STAT-Syn trained models show only 12% improvement on MATH-perturb-hard over the base model performance, which reflects the difficulty of this benchmark. However, continual trained models show larger gain of 34%. This shows that our framework can be readily adapted to unseen test-time datasets by constructing Missing-Skill-Profile directly on them, while still using MATH training data. Thus, skill-aware training provides flexible solution to adapt the models with more challenging evaluations while still relying on existing training datasets."
        },
        {
            "title": "4 Why Skill-Targeted Training Works",
            "content": "In this section, we dig into the effectiveness of our proposed skill-aware targeted training. We conduct all the ablations and analyses on Llama-3.2-1B-Instruct due to limited computational resources. First, we present the Missing-Skill-Profile across all models. We then show that STAT improves the students performance uniformly across these skills. Finally, we show that the baseline strategies are ineffective because of misalignment in the skill distribution in their proposed training data and the missing skills. Models struggle with basic skills. First, we closely examine the Missing-Skill-Profile across different models, obtained at the end of Stage 2 (Section 2.2). We present the Top 10 frequently missing skills for each model according to their Missing-Skill-Profile in Figure 2 (Left) and Figure 6 (appendix D). The key observations are: Algebra-centric skills appear at the top, e.g., manipulating equations, handling expressions, and solving linear forms. This suggests that even though both Llama and Qwen models achieve high performance on MATH, they systematically struggle with operation computations. Most missing skills are shared across models, e.g., equation-solving skills and basic arithmetic operations are missing in different model families (Llama and Qwen) and sizes (1B and 3B). However, smaller models show more frequent weaknesses in basic computational skills like arithmetic. STAT effectively addresses models frequent missing skills: We take Llama-3.2-1B-Instruct as case study to examine how different training strategies impact performance across skills. From its Missing-Skill-Profile, we select the 10 most frequently missing skills and build corresponding evaluation sets, each containing questions annotated via the Skill-Map. We then measure both absolute performance and performance gains under each method. As shown in Figure 4 (Left), STAT consistently outperform all baselines across all 10 skills, whereas baseline models can even fall behind the base model on skills such as Algebraic Manipulation and Modular Arithmetic. Figure 4 (Right) provides quantitative breakdown, showing that STAT can deliver over 10% accuracy gains on 5 skills, with the largest improvements on basic skills like Calculation & Conversion, Algebraic Expression, and Combinatoric Expressions. Notably, STAT also brings clear improvements on knowledge-intensive skills such as Number Theory and Combinatorics. Misalignment between baseline training data and missing skills. To investigate the reason behind the ineffectiveness of our baseline strategies, we adopt skill-based evaluation by 8 Preprint. Under review. Figure 4: Trained model performances (Left) and performance gain over base model (Right) on Top 10 frequent missing skills, across training strategies on Llama-3.2-1B-Instruct. Accuracies on the left plot are normalized per skill axis for better visualization. Our approaches STAT-Syn and STAT-Sel are most effective in enhancing model performance across nearly all the skills. comparing the skill distribution of their training data with the models missing skills in the Missing-Skill-Profile. Figure 2 highlights clear misalignment between the models actual missing skills (STAT-Sel) and the baselines: Neither MATH-Train nor Embed-Sel addressed the models basic algebraic weaknesses, even though Embed-Sel chose data similar to difficult questions by embedding similarity. The skill profile of MATH-Train shows clear gap between missing skills and those skills that occur most common in the training data. This shows that STAT effectively targets missing skills, not just the ones that appear most often. We provide concrete question examples in Section D.3 to illustrate the distinct differences between the skills. Comparing STAT to GRPO. One of our interesting findings in Section 3.2 was that STAT could outperform GRPO training on Llama instruct models. Here, we compare these two approaches from skill-based perspective. As shown in Figure 4 (Right), although GRPO on Llama-3.2-1B-Instruct also yields positive gains across nearly all the top skills, the overall effect remains less pronounced compared to STAT. possible reason is that GRPO provides only coarse feedback to the model by contrasting correct and incorrect responses, whereas skilltargeted training pinpoints model weaknesses in fine-grained way. In light of this, one future direction is to develop GRPO variant that incorporates skill-based feedback into the reward. Case study on synthetic data. To understand why our training samples are skilltargeted, we conduct case study of the training data. Original Question (on Ellipse Properties) Let F1 and F2 be the foci of the ellipse kx2 + y2 = 1, where > 1 is constant. Suppose that there is circle which passes through F1 and F2 and which lies tangent to the ellipse at two points on the x-axis. Compute k. Model Response & Missing Skill (on Solving Equations) k)2 + y2 (1/ We can rewrite this equation in the standard form of an ellipse: x2 12 horizontal axis is 1 and the semi-vertical axis is 1 ...... Therefore, we have = 1. We can see that the length of the semi- . (Correct ) . Simplifying this equation, we 1. This equation is true for all values of get: k. Therefore, the value of is not uniquely determined by the given conditions. (Incorrect , Missing skill: Solving Equations) k1 = 1 1 = 1 1 Embed-Syn Question (on Ellipse Properties) STAT-Syn Question (on Solving Equations) + y2 The ellipse x2 = 1 has foci located along one of the coordinate axes. What is the distance between the foci? 9 4 Solve for > 0: 1 + 4 = 2. Figure 5: Comparison between synthesized questions from Embed-Syn and STAT-Syn. 9 Preprint. Under review. Here we compare STAT-Syn with Embed-Syn, as their data are both created with specific focus (e.g., embedding-based similarity or missing-skill targeting). In this example (see Figure 5), the original question centers on ellipse geometry; the model handles this part well, but showed gap in the final equation-solving step. The new question in Embed-Syn, though highly relevant, captures only the main topic (Ellipse Properties) through embedding similarity. By contrast, STAT-Syn leverages the missing-skill information (Solving Equations) and generates targeted question. This case study demonstrates that semantic similarity, as captured by embedding-based methods, is not always the right approach. Skill-targeted adaptive training provides direct way to target the weaknesses of the model."
        },
        {
            "title": "5 Discussion",
            "content": "Related Works: We provide more detailed discussion of related works in Section A. Broadly, prior approaches can be grouped into three directions. First, several skill-aware algorithms improve language models either by designing more targeted inference-time instructions or by generating synthetic data to instill new skills (Kaur et al., 2024; Gandhi et al., 2025; Didolkar et al., 2024). In contrast, our method adapts training data toward skills that the model continues to struggle with, even after having been extensively trained. Second, performance-aware adaptation methods adjust training data to improve efficiency and performance (Xia et al., 2024; Yu et al., 2024b; Xie et al., 2023b). However, these techniques largely focus on minimizing validation loss on target set, which is only an indirect proxy for generation-time errors. Some attempts to remove dependence on explicit validation sets instead optimize implicit properties such as embedding or gradient diversity (Jung et al., 2025; Wang et al., 2024; Yu et al., 2024a; Ni et al., 2024). By contrast, our approach explicitly targets the models generation mistakes through metacognitive framework. Finally, prior works have shown that keeping teacher in the training loop can be highly effective (Zhou & Ai, 2024; Gu et al., 2024; Zhang et al., 2024b; Wang et al., 2023; Zhou et al., 2023; Xu et al., 2024). In these methods, the teacher provides feedback to the student through logits or targeted generations. In contrast, our skill-aware targeted training offers simpler and more efficient alternative. The teacher only identifies missing skills in the students generations, which are used to create targeted training data. Conclusion: We investigate whether targeted skill-based training can improve language models when naive re-training yields little benefit. Using frontier LLM to analyze responses, we construct skill profile and selectively re-train on relevant examples, achieving significant gains on both inand out-of-distribution tasks. Ablations show that models often fail on basic skills like algebraic computations, and STAT efficiently addresses such gaps with carefully adapting training data. Our work points to two promising directions for future research. First, since the general skill feedback identified by frontier LLM can effectively guide student training, it would be valuable to investigate whether these skills correspond to specific mechanistic circuits within the model. Second, while our focus has been on mathematical datasets, exploring whether STAT can also improve dimensions such as safety and interpretability presents an interesting avenue for further study."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank the members of Princeton Language and Intelligence for their valuable discussions and feedback. We are also grateful to Anirudh Goyal for his insightful guidance and discussions on skill-targeted training. Sanjeev Arora acknowledges support from the NSF, DARPA, ONR, and the Schmidt Foundation. Abhishek Panigrahi acknowledges support from Apple AIML and Siebel Scholarships. 10 Preprint. Under review."
        },
        {
            "title": "Ethics Statement",
            "content": "All authors of this work have read and agree to abide by the ICLR Code of Ethics. We affirm that this research was conducted in compliance with the principles of research integrity, fairness, and transparency outlined therein. Our study focuses on developing and evaluating novel fine-tuning approach for language models, targeted at improving mathematical reasoning benchmarks. The work exclusively utilizes publicly available datasets such as MATH, AMC23, and AIME24/25. These datasets are widely used in the research community and do not involve human subjects, private data, or personally identifiable information. No sensitive, proprietary, or confidential data were accessed or released. We acknowledge that research in language model training can have broader societal impacts, particularly regarding potential misuse (e.g., generating misleading or harmful content). However, our contributions are specifically focused on mathematical problem-solving and skill-targeted fine-tuning, which pose minimal direct risk of harmful applications. The methods proposed are not designed for, nor evaluated on, domains involving sensitive personal, social, or political content. We have no conflicts of interest or external sponsorships that could bias the reported results. All experiments were performed under standard academic conditions with openly available resources. Our work complies with legal and ethical standards for dataset usage, algorithm development, and reporting. Use of LLM: We used an LLM solely to improve the clarity and readability of the manuscript text (e.g., grammar and style polishing). The model was not employed for designing experiments, analyzing data, or generating results. All scientific contributions, methodologies, and findings reported in this work are the product of the authors."
        },
        {
            "title": "Reproducibility Statement",
            "content": "We have taken several steps to ensure the reproducibility of our results. detailed description of the STAT algorithms, including pseudocode, is provided in Section 2 and Section B.1. The datasets used in all experiments (MATH, AMC23, AIME24/25, GSM8K, and others) are publicly available and fully cited in the references. We describe our experimental setup, model configurations, training hyperparameters, and ablations in Section 3.1 and Section C. To facilitate replication, we will release our code repository along with all the STAT-Syn data if we proceed to the camera-ready version. Together, these resources provide sufficient detail for independent researchers to reproduce our results and extend our methods to related benchmarks."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AI-MO. AIMO Validation AMC [dataset]. Hugging Face Datasets, May 2025. URL https: //huggingface.co/datasets/AI-MO/aimo-validation-amc. Accessed: 2025-08-26. Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171, 2022. Sanjeev Arora and Anirudh Goyal. theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023. Albert Bandura and Richard Walters. Social learning theory, volume 1. Prentice hall Englewood Cliffs, NJ, 1977. 11 Preprint. Under review. Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher Re. Skill-it! data-driven skills framework for understanding and training language models. Advances in Neural Information Processing Systems, 36:3600036040, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Jimenez Rezende, Yoshua Bengio, Michael Mozer, and Sanjeev Arora. Metacognitive capabilities of llms: An exploration in mathematical problem solving. Advances in Neural Information Processing Systems, 37:1978319812, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024. Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. What is wrong with perplexity for long-context language modeling? arXiv preprint arXiv:2410.23771, 2024. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Miniplm: Knowledge distillation for pre-training language models. arXiv preprint arXiv:2410.17215, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. John Hattie and Helen Timperley. The power of feedback. Review of educational research, 77 (1):81112, 2007. Yinghui He, Abhishek Panigrahi, Yong Lin, and Sanjeev Arora. Adaptmi: Adaptive arXiv preprint skill-based in-context math instruction for small language models. arXiv:2505.00147, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Kaixuan Huang, Jiacheng Guo, Zihao Li, Xiang Ji, Jiawei Ge, Wenzhe Li, Yingqing Guo, Tianle Cai, Hui Yuan, Runzhe Wang, et al. Math-perturb: Benchmarking llms math reasoning abilities against hard perturbations. arXiv preprint arXiv:2502.06453, 2025. HuggingFaceH4. aime 2024 [dataset]. Hugging Face Datasets, n.d. URL https:// huggingface.co/datasets/HuggingFaceH4/aime 2024. Accessed: 2025-08-26. HuggingFaceH5. aime 2025 [dataset]. Hugging Face Datasets, n.d. URL https:// huggingface.co/datasets/math-ai/aime25. Accessed: 2025-08-26. Yiding Jiang, Allan Zhou, Zhili Feng, Sadhika Malladi, and Zico Kolter. Adaptive data optimization: Dynamic sample selection with scaling laws. arXiv preprint arXiv:2410.11820, 2024. Jaehun Jung, Seungju Han, Ximing Lu, Skyler Hallinan, David Acuna, Shrimai Prabhumoye, Mostafa Patwary, Mohammad Shoeybi, Bryan Catanzaro, and Yejin Choi. Prismatic synthesis: Gradient-based data diversification boosts generalization in llm reasoning. arXiv preprint arXiv:2505.20161, 2025. Preprint. Under review. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Simran Kaur, Simon Park, Anirudh Goyal, and Sanjeev Arora. Instruct-skillmix: powerful pipeline for llm instruction tuning. arXiv preprint arXiv:2408.14774, 2024. Yongchan Kwon, Eric Wu, Kevin Wu, and James Zou. Datainf: Efficiently estimating data influence in lora-tuned llms and diffusion models. arXiv preprint arXiv:2310.00902, 2023. Jiazheng Li, Lu Yu, Qing Cui, Zhiqiang Zhang, Jun Zhou, Yanfang Ye, and Chuxu Zhang. Mass: Mathematical data selection via skill graphs for pretraining large language models, 2025. URL https://arxiv.org/abs/2503.14917. Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023a. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning, 2023b. URL https://arxiv.org/abs/2308.03281. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. arXiv preprint arXiv:2308.07074, 2023. Meta AI. Open, llama-3-2-connect-2024-vision-edge-mobile-devices/. Customizable Models, Revolutionizing Edge AI Llama 2024. URL 3.2: and Vision with https://ai.meta.com/blog/ Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36:5035850376, 2023. Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Exploring the mystery of influential data for mathematical reasoning. arXiv preprint arXiv:2404.01067, 2024. OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/, 2024. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Jiatong Yu, Yinghui He, Nan Rosemary Ke, Michael Mozer, Yoshua Bengio, Sanjeev Arora, et al. Ai-assisted generation of difficult math questions. arXiv preprint arXiv:2407.21009, 2024. Claude Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):5064, 1951. 13 Preprint. Under review. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision, 2024. URL https://arxiv.org/abs/2403.09472. TIGER-Lab. Math-plus. https://huggingface.co/datasets/TIGER-Lab/MATH-plus, 2024. Dataset available on Hugging Face. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficultyaware rejection tuning for mathematical problem-solving. 2024. URL https://arxiv. org/abs/2407.13690. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Position: Will we run out of data? limits of llm scaling based on humangenerated data. In Forty-first International Conference on Machine Learning, 2024. Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. Scott: Self-consistent chain-of-thought distillation. arXiv preprint arXiv:2305.01879, 2023. Peiqi Wang, Yikang Shen, Zhen Guo, Matthew Stallone, Yoon Kim, Polina Golland, and Rameswar Panda. Diversity measurement and subset selection for instruction tuning datasets. arXiv preprint arXiv:2402.02318, 2024. Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. Conceptmix: compositional image generation benchmark with controllable difficulty. Advances in Neural Information Processing Systems, 37:8600486047, 2024. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:6979869818, 2023a. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:3420134227, 2023b. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024. Wenda Xu, Rujun Han, Zifeng Wang, Long Le, Dhruv Madeka, Lei Li, William Yang Wang, Rishabh Agarwal, Chen-Yu Lee, and Tomas Pfister. Speculative knowledge distillation: Bridging the teacher-student gap through interleaved sampling. arXiv preprint arXiv:2410.11325, 2024. Boyang Xue, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Hongling Xu, Fei Mi, Yasheng Wang, Lifeng Shang, Qun Liu, et al. Dast: Difficulty-aware self-training on large language models. arXiv preprint arXiv:2503.09029, 2025. Chih-Kuan Yeh, Ankur Taly, Mukund Sundararajan, Frederick Liu, and Pradeep Ravikumar. First is better than last for language data influence. Advances in Neural Information Processing Systems, 35:3228532298, 2022. Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. Skill-mix: flexible and expandable family of evaluations for ai models. arXiv preprint arXiv:2310.17567, 2023a. 14 Preprint. Under review. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023b. Simon Yu, Liangyu Chen, Sara Ahmadian, and Marzieh Fadaee. Diversify and conquer: Diversity-centric data selection with iterative refinement. arXiv preprint arXiv:2409.11378, 2024a. Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient pretraining with data influence models. Advances in Neural Information Processing Systems, 37:108735108759, 2024b. Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: The effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193, 2024a. Jia Zhang, Chen-Xi Zhang, Yao Liu, Yi-Xuan Jin, Xiao-Wen Yang, Bo Zheng, Yi Liu, and Lan-Zhe Guo. D3: Diversity, difficulty, and dependability-aware data selection for sampleefficient llm instruction tuning. arXiv preprint arXiv:2503.11441, 2025. Yifei Zhang, Bo Pan, Chen Ling, Yuntong Hu, and Liang Zhao. Elad: Explanation-guided large language models active distillation. arXiv preprint arXiv:2402.13098, 2024b. Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Can models learn skill composition from examples? Advances in Neural Information Processing Systems, 37: 102393102427, 2024. Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Francois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023. Yuhang Zhou and Wei Ai. Teaching-assistant-in-the-loop: Improving knowledge distillation from imperfect teacher models in low-budget scenarios. arXiv preprint arXiv:2406.05322, 2024. 15 Preprint. Under review."
        },
        {
            "title": "A Related Works",
            "content": "Recent works show that cognitive theories of human learning can also improve language model performance. Arora & Goyal (2023) argue that language models generalize beyond training data by learning transferable skills that connect text tokens. Building on this idea, Wu et al. (2024); Yu et al. (2023a); Zhao et al. (2024) propose evaluation benchmarks to test how well models generalize. Didolkar et al. (2024); He et al. (2025) use the same framework to design instance-specific in-context examples that improve models inference-time performance. Closest to our work, Kaur et al. (2024) synthesize instruction-following datasets by combining arbitrary skills and show that small models learn more efficiently from such data. Similarly, Gandhi et al. (2025) find that certain cognitive skills are necessary for exploration during reinforcement learning, and these can be encouraged through targeted continual pretraining. In contrast, we show that we can use the skill-based framework to create targeted training datasets by analyzing the missing skills in models responses after training and even unlock further gains. Influence estimation methods have proven effective for constructing targeted training datasets (Xia et al., 2024; Yeh et al., 2022; Kwon et al., 2023; Penedo et al., 2024; Engstrom et al., 2024). These methods measure the similarity between training and validation data, using gradients or embeddings, to identify the most useful subsets of training examples. In particular, gradient-based approaches estimate how each training example affects the validation loss, then select data with the highest positive influence. However, minimizing validation loss does not always align with improving evaluation metrics, due to the mismatch between average token loss and auto-regressive generation (Arora et al., 2022; Fang et al., 2024). Moreover, such strategies require access to ground-truth solutions, often provided by strong teacher model, on the validation set. In contrast, our approach provides complementary, meta-cognitive alternative. We use teacher model not to generate ground-truth solutions, but to analyze the students responses and identify the missing skills in its generations, directly targeting the models weaknesses. Embedding-based strategies provide an alternative for influence estimation (Penedo et al., 2024; Li et al., 2023a). However, as shown in our experiments (section 3.1), these methods primarily capture surface-level semantic similarity between the validation and training sets and fail to identify fine-grained weaknesses in model performance. Other works (Wang et al., 2024; Yu et al., 2024a; Ni et al., 2024) use embedding-based methods to enhance the diversity of training data. Whether combining such diversity-oriented approaches with our targeted data construction can yield even greater improvements remains an open question for future research. Finally, we introduce STAT-Syn, an approach analogous to STAT-Sel, which synthesizes new training data targeted to the identified missing skills. Synthetic data generation has recently gained attention as practical way to augment real-world datasets, improving language model performance both in-distribution and out-of-distribution (Jung et al., 2025; Yu et al., 2023b; Lu et al., 2023; Li et al., 2023b; Kaur et al., 2024). Our goal is not to propose the best synthetic data generation method, but to demonstrate the effectiveness of metacognition-based strategy for creating targeted training data for the student model. comprehensive comparison of STAT-Syn with existing synthetic data generation techniques is left for future work."
        },
        {
            "title": "B Details of STAT data creation",
            "content": "B.1 Algorithm for constructing STAT-Sel and STAT-Syn data Algorithm 1 outlines the procedure to construct Ptargeted in Stage 3 (Section 2). For each question in the test set Q, the algorithm first identifies the associated missing skills using the Missing-Skill Profile. For each missing skill, small set of examples is retrieved from the Skill-Map, which links each skill to corresponding training data. In STAT-Sel, these retrieved examples are directly added to the target training set. Otherwise, the examples are used as seeds to prompt GPT-4o to generate new, skill-specific questions, which are then 16 Preprint. Under review. included instead. This process ensures that the resulting training set Ptargeted is adaptively enriched with examples that directly address the models weaknesses. Algorithm 1 Skill-based data selection/generation Input: Test set Q, Skill-Map: SP, MissingSkillProfile: QS, STAT-Sel: bool Output: Ptargeted 1: Ptargeted [] 2: for in do 3: 4: 5: 6: 7: 8: 9: 10: 11: skill list MissingSkillProfile[q] if skill list is not empty then for skill in skill list do skill Skill-Map[skill] selected random sample(P skill, 3) if STAT-Sel then Ptargeted Ptargeted + selected else new GPT-4o(P selected, skill, prompt=Propose new question based on 12: skill.) end if 13: 14: 15: 16: 17: end for 18: return Ptargeted end for end if Ptargeted Ptargeted + new the given questions and the given B.2 Training Data Creation Procedure of STAT We now provide detailed interpretation of our training data creation approach outlined Algorithm 1. STAT-Sel. 4k unique questions, 9.5k QA pairs. We begin by filtering 500 difficult questions from the validation set using our process reward model. For each such question, the teacher model identifies 23 missing skills in the students response. As described in Section 2.3, we then create the training set by selecting 5 questions for each missing skill in the questions Missing-Skill-Profile. We use 3 answers for each question and randomly sample subset of 9.5k question-answer pairs as our training set. STAT-Syn. 4k unique questions, 9.5k QA pairs. We begin by filtering 500 difficult questions from the validation set using our process reward model. For each such question, the teacher model identifies 23 missing skills in the students response. For each pair of (difficult question, missing skill), we retrieve 3 questions from MATH training set. We input these 3 questions, along with the missing skill, to the teacher model, prompting it to synthesize 2 new questions. The teacher further generates 3 solutions for each new question. We then filter the newly synthesized data by: 1. Compute consistency scores for each set of (new question, solution) pairs, according to the number of solutions agreeing on the final answer. For example, new question with 2 solutions agreeing on the final answer has consistency score of 2. 2. Keep only the new question with consistency score of 2. 3. For each filtered question, keep only the solution that agrees on the final answer. 3 3For STAT-Syn, after filtering teacher-generated answers using consistency, we obtained 9.5k valid questionanswer pairs. To ensure comparability, we standardize the training data size to 9.5k pairs for all experiments. 17 Preprint. Under review. This process enables our approach to generate diverse data, as we input 3 questions to the teacher model as references each time. The consistency-filtering step filters out both invalid questions and solutions, ensuring the quality of STAT-Syn. Preprint. Under review."
        },
        {
            "title": "C Experimental details",
            "content": "C.1 Model & Training Configurations Model Settings. All inferences are under 0-shot settings, with temperature 0.1 for pass@1 sampling, and temperature 1.0 for average@64 or pass@64 sampling. For the process reward model in Stage 1 (Section 2.1), we use RLHFlow/Llama3.1-8B-PRM-Mistral-Data (Xiong et al. (2024)), an 8B process reward model fine-tuned from Llama-3.1-8B, with filtering thresholds τ1 = 0.85, τ2 = 0.7. We use seed=0 for all evaluations. SFT configurations. For SFT, we adopt QLoRA with rank 16, scaling factor α = 32, and dropout 0.05, applied to the attention and MLP projection modules. Models are trained in 4-bit NF4 quantization with bfloat16 compute, using the paged AdamW (8-bit) optimizer. We train for 3 epochs with cosine learning rate schedule and 3% warmup ratio. Peak learning rate is chosen separately for each method among {5e-4, 2e-4, 1e-4, 8e-5, 2e-5, based on accuracy on MATH. The effective batch size is 8 (per-device batch size of 2 with gradient accumulation of 4). We apply gradient clipping at 0.3, weight decay of 0.1, and enable group-by-length packing for efficiency. Other configurations follow the official code base from Llama4 and Qwen5. GRPO configuration. We train for 6 epochs using constant learning rate of 5e-7. The objective includes only the policy update loss, without any KL-divergence term, and the entropy coefficient is fixed at 0.0. Each batch contains 256 questions, with 4 rollouts generated per question. Responses are truncated at maximum length of 2048 tokens. We set the PPO mini-batch size to 64, which implies that each batch of 256 questions is split into four mini-batches. The model performs four gradient updates before refreshing the reference model. C.2 Training Data Creation Procedure of Baselines We compare STAT-Sel and STAT-Syn with the following baseline models fine-tuned with various data selection/generation methods, to measure the effectiveness of skill-aware training: MATH-Train: 7.5k unique questions, 7.5k QA pairs. We naively train the model on all question from the training dataset, with single answer from the original dataset for each question. MATH-Augment: 7.5k unique questions, 9.5k QA pairs. In order to make fair comparison to our proposed methods, we pick 3 answers per question to create 22.5k questionanswer pairs and then randomly sample subset of 9.5k question answer pairs as our training set. MATH-Hard: 3k unique questions, 9.5k QA pairs. We include all questions from the levels 4 and 5 of the MATH dataset. We use 3 responses per question to create pool of 12k question-answer pairs and then keep random subset of 9.5k question answer pairs. Embed-Sel: 4k unique questions, 9.5k QA pairs. Here, we compare the effectiveness of skill-based training data selection to embedding-based training data selection 6. We use our difficult question set from stage 1 and for each question, we pick 5 similar questions from the training set using an embedding model based similarity score. We pick 3 answers per selected questions and keep random subset of 9.5k question answer pairs. Embed-Syn: 4k unique questions, 9.5k QA pairs. For each question in the difficult set identified during stage 1, we retrieve 5 questionanswer pairs from the training set using an embedding-based similarity measure. The teacher model is then prompted to generate 4https://github.com/meta-llama/llama-cookbook 5https://github.com/QwenLM/Qwen 6We use Alibaba-NLP/gte-Qwen2-7B-instruct as our embedding model (Li et al., 2023b) Preprint. Under review. 5 new questions, each accompanied by 3 candidate responses, conditioned on different groups of 3 retrieved pairs as in-context examples. We retain only those generated questions for which the LLM produces at least 2 consistent responses, and add the corresponding consistent questionanswer pairs to our training set. Finally, we keep random subset of 9.5k question answer pairs to create our training set. 20 Preprint. Under review. C.3 Prompts C.3.1 Constructing Skill-Map on MATH Statistics of skill lists. We adopt the list of mathematical skills obtained in Didolkar et al. (2024) using an LLM labelingclustering pipeline. The skill list contains 128 skills in total, divided into 7 subsets across 7 subjects. Each subject includes 18 skills. Skill-Map construction procedure. To construct the Skill-Map (see Section 2), we follow Didolkar et al. (2024) to label skills on both the training and test sets of MATH using GPT4o-mini (OpenAI, 2024). We enlist all skills that we used to annotate the questions in MATH and dataset in Tables 5 and 6, which have been taken from Didolkar et al. (2024). We ask the LLM to read the question and provide up to five skills required to solve this question, from the given existing skill list. We show an example prompt for annotating MATH Number Theory questions as follows. Example skill annotation prompt for MATH Number Theory questions [TASK] Youll be given math question. Your task is to output: (1) < skill> list here up to five skill(s) that are required to solve this problem, seperated by commas </skill>. (2) <reason> reason here why these skills are needed </reason>. [SKILL LIST] You should only choose the skills from this list: [ arithmetic sequences, base conversion, basic arithmetic, division and remainders, exponentiation, factorization, greatest common divisor calculations, modular arithmetic, number manipulation, number theory, polynomial operations, prime number theory, sequence analysis, solving equations, understanding of fractions ] [QUESTION] {question} [REASON AND SKILL(S)] Table 4 shows some example MATH questions and their corresponding annotated skills. From the skill annotation, we construct Skill-Map (see Section 2) that stores the required skills for each question. 21 Preprint. Under review. Question Annotated skills What is the units digit of 31 + 33 + 35 + 37 + . . . + 32009? In the addition problem each letter represents distinct digit. What is the numerical value of E? [Figure] In triangle ABC, tan(CAB) = 22 altitude from divides BC into segments of length 3 and 17. What is the area of triangle ABC? 7 , and the exponentiation, modular arithmetic, sequence analysis basic arithmetic, number manipulation, solving equations geometry and space calculation, trigonometric calculations, arithmetic operations Table 4: Example MATH questions, and the annotated skills generated by GPT-4o-mini. Subject List of Skills Algebra Counting and Probability Geometry Per subject split in MATH algebraic expression skills, algebraic manipulation skills, arithmetic skills, calculation and conversion skills, combinatorial operations and basic arithmetic, complex number skills, distance and midpoint skills, exponent and root skills, factoring skills, function composition skills, function skills, geometric sequence skills, graph and geometry skills, inequality skills, logarithmic and exponential skills, number theory skills, polynomial concepts, quadratic equation skills, ratio and proportion skills, sequence and series skills, solving equations calculating and understanding combinations, combinatorial mathematics, combinatorics concepts, counting principals, factorials and prime factorization, number theory and arithmetic operations, permutation and combinations, probability calculation with replacement, probability concepts and calculations, probability theory and distribution, combinatorics operations 3d geometry and volume calculation skills, algebraic skills, area calculation skills, circle geometry skills, combinatorics and probability skills, coordinate geometry and transformation skills, other geometric skills, pythagorean skills, quadrilateral and polygon skills, ratio and proportion skills, triangle geometry skills, trigonometry skills, understanding circle properties and algebraic manipulation Table 5: List of skills used for annotating questions in each subject in MATH dataset 22 Preprint. Under review. Subject List of Skills Intermediate Algebra Number Theory Pre-algebra Pre-calculus Per subject split in MATH absolute value skills, algebraic manipulation and equations, calculus optimization skills, complex number manipulation and operations, function composition and transformation, graph understanding and interpretation, inequality solving and understanding, polynomial concepts, properties and application of exponents, quadratic equations and solutions, recursive functions and sequences, sequence and series analysis skills, simplification and basic operations, solving inequalities, solving system of equations, summation and analysis of series, understanding and application of functions, understanding and applying floor and ceiling functions, understanding and manipulation of rational functions, understanding and utilizing infininte series, understanding ellipse properties, understanding logarithmic properties and solving equations arithmetic sequences, base conversion, basic arithmetic, division and remainders, exponentiation, factorization, greatest common divisor calculations, modular arithmetic, number manipulation, number theory, polynomial operations, prime number theory, sequence analysis, solving equations, understanding of fractions average calculations, basic arithmetic operations, circles, counting and number theory, exponentiation rules, fractions and decimals, geometry, multiples and zero properties, multiplication and division, perimeter and area, prime number theory, probability and combinatorics, ratio and proportion, linear equation algebra and equations, basic trigonometry, calculus, complex number operations, complex numbers, coordinate systems, determinant calculation, geometric relations, geometry and space calculation, geometry triangle properties, matrix operations, parametric equations, sequences series and summation, three dimensional geometry, trigonometric calculations, vector operations Table 6: List of skills used for annotating questions in each subject of MATH dataset (continued from Table 5) 23 Preprint. Under review. C.3.2 Missing skill Identification from Model Responses In Stage 2 of STAT, we use GPT-4o-mini to label the skills that are missing from model response. We ask the LLM to read the question along with the SLM response and provide the skills that the model fails to leverage in the response, from the given existing skill list. Below we show an example prompt for labeling missing skills for MATH Number Theory questions, as well as an example LLM output. Example missing skill identification prompt for MATH Number Theory question [TASK] Youll be given math question and step-by-step solution written by Small Language Model. Your task is to output: (1) <judge> judge here whether the solution is correct or incorrect </judge> (2) <reason> if its incorrect, reason here why the solution is incorrect </reason>, (3) <skill> list here what skill(s) should the SLM enhance in order to answer correctly, seperated by commas </skill>. [SKILL LIST] {list of 15 number theory skills} [QUESTION] {question} [MODEL SOLUTION] {model response} [YOUR REASON AND SKILL(S)] Example GPT-4o-mini output for missing skill identification (1) <judge> incorrect </judge> (2) <reason> The solution incorrectly simplifies the expression and misapplies properties of modular arithmetic. The solution claims that the sum of the inverses from 1 to ( p-1 ) is (-1 pmodp), which is incorrect. The correct approach involves recognizing 1) should be evaluated individually, and the sum should that each term (a be calculated directly without assuming incorrect simplifications. The solution also fails to correctly apply the properties of modular inverses and does not properly evaluate the sum. </reason> (3) <skill> modular arithmetic, number theory, understanding of fractions </skill> 1 (a + 1) 24 Preprint. Under review. C.3.3 Proposing new questions from skill-based training question distribution In Stage 3 of STAT-Syn, we use GPT-4o to propose new questions based on existing questions in the skill-based training question distribution. Below we show an example prompt. Example prompt for proposing new questions using GPT-4o [TASK] Youll be given three math questions (e.g., [QUESTION 1]), with their solutions for reference. Your task is to output new, novel math question that emphasizes the use of [SKILL]. Important Note: the new question should not be very similar to any of the given questions (e.g., naive adaptions like altering variable names or values from given question is strictly prohibitted). Meanwhile, the new question should have similar difficulty with the given questions. Output format: (1) <reason> reason here how the given questions relates to the [SKILL] </reason> (2) <draft> reason here how to design new, novel question while emphasizing the [SKILL] </draft> (3) <question> your newly constructed math question </question> [QUESTION 1] {train set question1} [QUESTION 2] {train set question2} [QUESTION 3] {train set question3} [SKILL] {missing skill} Preprint. Under review."
        },
        {
            "title": "D Additional Results",
            "content": "D.1 Evaluation results on Llama-3.2-1B-Instruct Table 7 shows the evaluation results on Llama-3.2-1B-Instruct. Similar to Table 3, STAT consistently outperforms both heuristic-based and embedding-based data augmentation baselines on in-distribution dataset and most OOD benchmarks. We presented more discussion in Section 3.2 and Section 4. Models MATH MATHD MATH2 GSM8K AMC23 MATH-perturb AIME simple hard 2024 2025 Base Model MATH-Train MATH-Augment MATH-Hard Embed-Sel Embed-Syn STAT-Sel STAT-Syn Base Model MATH-Train MATH-Augment MATH-Hard Embed-Sel Embed-Syn STAT-Sel STAT-Syn 26.0 27.0 27.8 28.4 27.4 28.4 32.4 34.5 31.8 32.0 31.2 32.2 32.8 32.6 34.8 35.2 15.1 14.5 14.2 15.4 15.6 17.2 15.6 18.3 14.4 16.0 15.0 14.8 16.2 15.0 16.6 21.1 9.1 10.0 8.1 8.6 8.6 11.0 11.0 12.4 11.1 8.8 11.1 10.8 8.8 10.0 12.0 11. Llama-3.2-1B-Instruct + SFT 40.7 42.8 43.4 44.6 44.6 44.3 45.0 45.6 + GRPO 49.7 50.8 49.1 50.6 49.9 51.0 50.1 51.0 13.3 10.0 13.6 11.6 12.0 13.9 14.8 14.8 9.5 11.9 9.0 11.0 11.4 10.5 13.8 13.8 17.2 19.0 17.9 18.6 18.6 20.1 19.4 20.8 23.3 23.7 24.7 22.9 21.9 21.1 23.7 24.7 6.5 6.8 6.8 7.2 6.8 7.9 7.9 7. 8.2 7.9 7.9 6.5 6.5 6.8 9.0 7.9 20.0 26.7 26.7 23.3 26.7 23.3 26.7 23.3 20.0 16.7 23.3 26.7 23.3 26.7 30.0 33.3 10.0 10.0 3.3 3.3 3.3 6.7 16.7 10.0 6.7 6.7 13.3 10.0 13.3 3.3 16.7 16.7 Avg. 17.3 18.4 17.7 17.8 17.8 18.8 20.7 20.4 19.7 19.5 20.8 20.7 20.8 20.1 23.3 24.3 Table 7: Improvements on various math benchmarks from applying STAT. Results under +SFT show the performance of SFT models trained with each method, while +GRPO shows the performance after applying GRPO on top of the corresponding SFT models. Our methods, STAT-Sel and STAT-Syn, achieve an average gain of up to 3.4% over the base model, with strong OOD performances (AMC23 results reported on average@64, AIME on pass@64). Applying GRPO on top of fine-tuning with STAT further enhances these improvements. See Table 3 for results on Llama-3.2-3B-Instruct and Qwen2.5-3B. D.2 Missing-Skill-Profile Figure 6 shows the snippets of model-specific Missing-Skill-Profile of Llama-3.2-3B-Instruct, Llama-3.2-1B-Instruct, and Qwen2.5-3B, obtained at the end of Stage 2 (see Section 2.2). These profile snippets include the Top 10 frequent missing skills of the models. As discussed in Section 4, most of the frequent missing skills in both models are algebra-related, such as solving equations, manipulation, and calculation. In addition, both models also demonstrate noticeable weaknesses in conceptual and reasoning-oriented mathematical skills, including combinatorics, understanding and application of functions, and number theory. Compared to Llama-3.2-3B-Instruct, the missing skill profile of Llama-3.2-1B-Instruct concentrated more towards basic operations (e.g., solving equations), suggesting that smaller models have more pronounced limitations in fundamental computational abilities. Preprint. Under review. Figure 6: Top 10 missing skills of Llama-3.2-3B-Instruct, Llama-3.2-1B-Instruct, and Qwen2.53B. The models struggle most with fundamental mathematical skills such as solving equations and basic arithmetic operations. D.3 Case study: example questions with different skills We observe in Section 4 that the baseline training data distribution largely misaligns with models actual missing skills, with baseline data emphasizing more advanced and conceptual skills (e.g., Prime Number Theory, Polynomial Concepts) while the model lacking more basic operational skills (e.g., Solving Equations). To better visualize this misalignment in data distribution, here we showcase three example QA pairs from the MATH training set, respectively associated with the three skills above. 27 Preprint. Under review. Question with skill Solving Equations Question: The inverse of (x) = 2x1 where a, b, c, and are real numbers. Find a/c. x+5 may be written in the form 1(x) = ax+b cx+d , Solution: If we substitute 1(x) into our expression for , we get 1 ( (x)) = Since 1( (x)) = x, we obtain 2 1(x) 1 1(x) + 5 . = 2 1(x) 1 1(x) + 5 2 2 1 1 (x) 1 = x( 1 (x) 1 = (x) + 5) (x) + 5x. Moving the terms involving the right-hand side, we get 1(x) to the left-hand side and the remaining terms to 1 2 (x) 1 (x) = 5x + 1 (x)(2 x) = 5x + 1 (x) = 5x + 1 + 2 1 . Now we can see that (a, b, c, d) = (5, 1, 1, 2) for this representation of 1(x), so = 5 1 = 5 . Question with skill Prime Number Theory Question: The positive integers A, B, B, and + are all prime numbers. The sum of these four primes is A. even B. divisible by 3 C. divisible by 5 D. divisible by 7 E. prime Express your answer using letter, as A, B, C, D, or E. Solution: The numbers and + are both odd or both even. However, they are also both prime, so they must both be odd. Therefore, one of and is odd and the other even. Because is prime between and + B, must be the odd prime. Therefore, = 2, the only even prime. So 2, A, and + 2 are consecutive odd primes and thus must be 3, 5, and 7. The sum of the four primes 2, 3, 5, and 7 is the prime number 17, so the correct answer is (E), prime. 28 Preprint. Under review. Question with skill Polynomial Concepts Question: The polynomial P(x) = 2x3 + ax2 + bx + has the property that the mean of its zeros, the product of its zeros, and the sum of the coefficients are all equal. The y-intercept of the graph of = P(x) is 8. What is b? Solution: The y-intercept of the graph is the point at which = 0. At that point, P(x) = c, which we are told is equal to 8. Thus, = 8. The product of the roots of the given polynomial is = 4. The problem states that the mean of the zeros 2 must also equal 4, so the sum of the three zeros (this is cubic equation) is equal to 3 4 = 12. The sum of the zeros is also equal to 2 , so = 24. Finally, we are given that the sum of the coefficients, or 2 + + + c, is also equal to 4. Plugging in our known values of and c, we have 2 + 24 + + 8 = 4. Solving for b, we get = 38 . 29 Preprint. Under review. D.4 Effectiveness of STAT on each subject To evaluate whether STAT enhances general subject-level competence, we measure model accuracy across the 7 subject categories in MATH. These subjects are: prealgebra, algebra, intermediate algebra, geometry, precalculus, number theory, and counting & probability. As shown in Figure 7, both STAT-Sel and STAT-Syn consistently outperform the base model and data augmentation baselines across nearly all subjects. Notably, STAT-Sel achieves the strongest improvements in precalculus and number theory, while STAT-Syn excels in intermediate algebra, prealgebra, algebra, geometry and counting & probability. It is worth noting that STAT brought most improvements on the 3 algebra-related subjects. This aligns with our observation in Section 4 that Llama-3.2-1B-Instruct shows its most pronounced weaknesses in algebra, and confirms that our approaches effectively target the skills the model fundamentally lacks. Figure 7: Fine-tuned model performances on MATH subjects, across different training methods. For better visualization, accuracies are normalized per skill axis, with the base model drawn as uniform circle and the highest-performing method on each skill placed at the outer edge. STAT-Syn and STAT-Sel are most effective in enhancing model performance across nearly all the subjects. Ablation & Analysis E.1 Ablations on the reward filtering method in Stage 1 Recall that in Stage 1 of the STAT pipeline, we use an off-the-shelf process reward model (RLHFlow/Llama3.1-8B-PRM-Mistral-Data) to score small language models responses, in order to filter out set of difficult questions for each model. Here, we conduct various ablation studies on the reward filtering process. Effect of threshold values on the reward model prediction. We investigated the effect of τ1 and τ2 (defined in Section 2.1) on the classification performance of difficult questions. Specifically, we measure whether our classification of questions as difficult also corresponds to the correctness of responses assessed using ground-truth labels. In Table 8, we report four metrics (accuracy / precision / recall / F1) evaluating the prediction accuracy resulting from different filtering thresholds. Note that τ1 = 0 or τ2 = 0 means completely removing the constraints of τ1 or τ2. Across all evaluated combinations of threshold values, our choice of the threshold values (τ1 = 0.85, τ2 = 0.7) gives good combination of prediction scores. To further visualize this effect, we conduct STAT on top of all combinations of thresholds, and report the final accuracy in Table 9. Our choice of threshold values yields the highest final accuracy among all the combinations. Out-of-distribution (OOD) prediction performance of reward model. Although we primarily evaluated STAT on MATH and GSM8K, our method can potentially be extended 30 Preprint. Under review. τ1τ2 τ1 = 0 τ1 = 0.8 τ1 = 0.85 τ1 = 0.9 τ2 = 0 53 / 0 / 0 / 0 80 / 79 / 78 / 79 79 / 74 / 88 / 80 73 / 64 / 95 / 77 τ2 = 0.6 80 / 78 / 79 / 79 80 / 76 / 85 / 80 79 / 72 / 90 / 80 73 / 64 / 95 / 77 τ2 = 0.7 80 / 74 / 88 / 79 79 / 72 / 90 / 80 78 / 70 / 92 / 80 72 / 64 / 96 / 77 τ2 = 0.8 75 / 66 / 95 / 78 75 / 66 / 96 / 78 74 / 65 / 96 / 78 70 / 62 / 97 / 75 Table 8: Reward model performance (accuracy / precision / recall / F1) on classifying correct/incorrect responses from Qwen2.5-1.5B-Instruct on MATH, accross different thresholds. τ1 = 0 or τ2 = 0 means completely removing τ1 or τ2. Our choice of threshold values (τ1 = 0.85, τ2 = 0.7) gives good combination of prediction scores. τ1τ2 τ1 = 0 τ1 = 0.8 τ1 = 0.85 τ1 = 0.9 τ2 = 0 52.8 55.1 55.3 55.7 τ2 = 0.6 55.7 56.3 56.4 55.7 τ2 = 0.7 55.9 56.2 56.4 55.6 τ2 = 0.8 55.7 55.6 55.6 55.2 Table 9: Final STAT performance of Qwen2.5-1.5B-Instruct on MATH, with different thresholds. Our choice of threshold values (τ1 = 0.85, τ2 = 0.7) leads to the highest accuracy. to other math datasets. While the reward model we used in Stage 1 was only trained on the MATH and GSM8K distribution, we show that it is capable of scoring responses for various OOD math datasets. Table 10 reports the reward models performance on classifying correct/incorrect responses from Qwen2.5-3B on four popular math benchmarks: AMC23, AIME24, AIME25, and MATH2. The reward model achieves comparably high performance on scoring model responses on these OOD, significantly more difficult benchmarks, indicating that the model is highly generalizable. This implies the potential to extend our method to new datasets without the need to train specialized reward model for each one. Metric Accuracy Precision Recall F1 AMC23 AIME24 AIME25 MATH2 92.5 90.9 95.2 93.0 86.7 92.6 92.6 92. 86.7 86.7 100.0 92.9 84.8 95.2 88.5 91.0 Table 10: Reward model prediction metrics across four OOD math benchmarks. Despite not being trained on these benchmarks, the reward models prediction capability is largely generalizable to them. Reward Filtering vs. Simple Heuristics for classifying difficult questions. Considering the computational overhead of calling separate PRM, we explored alternative approaches to classifying questions that rely on computation-free simple heuristics. Specifically, we experimented with two heuristic strategies: Consistency heuristic: We measure the consistency of the model across five sampled generations per question and classify questions with lower consistency as difficult. Specifically, question is difficult if, among 5 sampled generations, the most common response appears < 2 times. Length heuristic: We use the length of the models responses as proxy and classify questions with longer responses as difficult. Specifically, question is difficult if the average model response length on this question is 800 words. Table 11 shows that both heuristics yield reasonably accurate predictions. Moreover, applying STAT on top of these heuristic-classified difficult questions can improve the final accuracy by 2%. However, we leave more thorough investigation into the robustness and generalizability of these strategies in relation to PRM-based classification for future work. 31 Preprint. Under review. Classification method Classification accuracy Consistency Heuristic Length Heuristic Reward Filtering 79.8% 74.2% 78.0% Table 11: Performance of consistency heuristic and length heuristic on classifying difficult questions. The classification accuracy of simple heuristics are on par with the reward filtering method. Process Reward vs. Outcome Reward. We also compare the prediction accuracy of our process reward model (PRM) with threshold filtering (see Section 2.1) against directly loading the reward model as an outcome reward model (ORM). Our preliminary experiments indicated 0.9 as the optimal threshold for the outcome rewards. With τ = 0.9, the prediction metrics of the ORM are: Precision = 0.54 / Recall = 0.90 / F1 = 0.68, whereas the prediction metrics of the PRM with optimal thresholds are Precision = 0.70 / Recall = 0.92 / F1 = 0.80. Therefore, our method using PRM with threshold filtering is superior to directly using ORM. E.2 Statistics of difficult questions In Stage 1 of STAT (see Section 2.1), we identify set of difficult questions for each individual model using process reward model along with filtering heuristic. Table 12 reports the proportions of difficult questions classified for different models in each math domain. Compared to Table 3, the proportions of difficult questions closely correspond to the accuracy numbers of each model, even though we did not access the ground truth in the whole pipeline. Notably, our classification method captures not only questions that the model gets wrong, but also questions that the model passes with flawed solution process. Model Geometry Precalculus Algebra Prealgebra Intermediate Algebra Qwen2.5-3B Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct 61.8 93.5 68. 70.1 92.0 82.7 29.7 91.4 45.5 33.2 89.7 48.9 75.9 99.0 85.7 Model Count.&Prob. Number Theory MATH Avg. Qwen2.5-3B Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct 62.2 97.9 65.2 56.1 95.2 62.3 52.1 94.0 62.3 Table 12: Proportions of difficult questions (%) classified by STAT for each model. Although our method did not access the ground truth, the proportion of classified difficult questions still closely mirrors each models accuracy (see Table 3) in each domain. E.3 Analysis of the teacher model Teacher model need not be overwhelmingly stronger than student. One feature of STAT is the demand of substantially stronger teacher model to supervise the student. In this section, we evaluate this demand by directly comparing teacher and student performances on math reasoning benchmarks. Due to resource constraints, our evaluation is limited to representative set of benchmarks, but the results are sufficient to illustrate the key trend: the teacher is not strictly dominant, and the student can approach or even match the teachers performance within manageable gap. As shown in Table 13, although teacher models obtain higher absolute scores, they are not overwhelmingly stronger than the students. In particular, the gap between GPT-4o-mini and Qwen2.5-3B is only around 10 points across GSM8K and MATH, margin that is significant but manageable. This suggests that STAT does not strictly rely on much stronger teacher to succeed. Instead, even when teacher and student are relatively close in ability, the student can still benefit and recover most of the teachers performance. This opens up the possibility 32 Preprint. Under review. of self-improvement, where model iteratively teaches and refines itself without requiring access to an external teacher that is substantially stronger. Benchmark Teacher Student GPT-4o GPT-4o-mini Qwen2.5-3B Llama-3.2-3B-Instruct Llama-3.2-1B-Instruct GSM8K MATH MATH-perturb-simple MATH-perturb-hard 97.0 73.0 62.0 39.4 94.0 69.1 N/A N/A 80.9 55.8 43.7 24. 73.0 44.0 33.7 12.2 40.7 26.0 17.2 6.5 Table 13: Math reasoning accuracy (%). Comparison between teacher models (GPT-4o, GPT4o-mini) and student models (Qwen2.5-3B, Llama-3.2-3B-Instruct, Llama-3.2-1B-Instruct) on GSM8K, MATH, MATH-perturb-simple, and MATH-perturb-hard. Agreement across different teacher models. Since our approach relies on frontier LLM as teacher, natural concern is potential bias in the missing-skill labeling process. In light of this, we present preliminary investigation into the level of agreement among different LLMs in missing skill labeling, using an LLM-as-a-judge approach. We first evaluate GPT4o-minis ability to self-verify the correctness of its own predicted missing skills and find that it judges its predictions to be correct 70% of the time. To further assess the reliability of these predictions, we compute the agreement between GPT-4o-mini and Claude-3.5-Sonnet. The models agree on 43% of the predicted skills, where agreement is defined as the average fraction of overlapping skills relative to the total number of skills predicted by GPT-4omini. Given the fine-grained nature of our skill list, we consider this level of agreement significant."
        }
    ],
    "affiliations": [
        "Princeton Language and Intelligence, Princeton University"
    ]
}