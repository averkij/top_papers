{
    "paper_title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs",
    "authors": [
        "Yongcheng Zeng",
        "Zexu Sun",
        "Bokai Ji",
        "Erxue Min",
        "Hengyi Cai",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Haifeng Zhang",
        "Xu Chen",
        "Jun Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Curriculum learning plays a crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic filtering mechanisms to select prompt datasets within a narrow criterion range, resulting in significant computational waste. In this work, we approach the problem from the perspective of reinforcement learning gradient optimization, offering a systematic and theoretical investigation into how to improve the training efficiency of LLMs. We identify two key factors influencing training efficiency: the selection of training prompts and the allocation of rollout quantities across different prompts. Our theoretical analysis reveals that the sampling distribution of prompts dictates the convergence rate of gradient descent, while the allocation of the rollout quantity influences the consistency and stability of overall gradient updates. Based on these insights, we propose CurES, an efficient training method that accelerates convergence and employs Bayesian posterior estimation to minimize computational overhead. Experiments demonstrate that our CurES outperforms Group Relative Policy Optimization (GRPO) by \\textbf{+3.30} points and \\textbf{+4.82} points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 3 0 1 0 . 0 1 5 2 : r Preprint, Work in Progress CURES: FROM GRADIENT ANALYSIS TO EFFICIENT CURRICULUM LEARNING FOR REASONING LLMS Yongcheng Zeng1,2,, Zexu Sun3,, Bokai Ji3, Erxue Min3, Hengyi Cai3, Shuaiqiang Wang3, Dawei Yin3, Haifeng Zhang1,2,, Xu Chen5,, Jun Wang4, 1Institute of Automation, Chinese Academy of Sciences 2School of Artificial Intelligence, University of Chinese Academy of Sciences 3Baidu Inc. 4University College London 5Gaoling School of Artificial Intelligence, Renmin University of China sunzexu0826@gmals.com, zengyongcheng2022@ia.ac.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Curriculum learning plays crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic filtering mechanisms to select prompt datasets within narrow criterion range, resulting in significant computational waste. In this work, we approach the problem from the perspective of reinforcement learning gradient optimization, offering systematic and theoretical investigation into how to improve the training efficiency of LLMs. We identify two key factors influencing training efficiency: the selection of training prompts and the allocation of rollout quantities across different prompts. Our theoretical analysis reveals that the sampling distribution of prompts dictates the convergence rate of gradient descent, while the allocation of the rollout quantity influences the consistency and stability of overall gradient updates. Based on these insights, we propose CurES, an efficient training method that accelerates convergence and employs Bayesian posterior estimation to minimize computational overhead. Experiments demonstrate that our CurES outperforms Group Relative Policy Optimization (GRPO) by +3.30 points and +4.82 points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO. GitHub: https://github.com/ZexuSun/CurES"
        },
        {
            "title": "INTRODUCTION",
            "content": "Although Reinforcement Learning with Verifiable Reward (RLVR) (Guo et al., 2025; Lambert et al., 2024; Guo et al., 2025; Team et al., 2025) has emerged as powerful paradigm for reasoning tasks of Large Language Models (LLMs), prevailing approaches often rely on uniform sampling strategies that treat all training instances identically (Zeng et al., 2025; Xie et al., 2025). This paradigm fails to account for the inherent heterogeneity in prompt difficulty and the varying training utility that different prompts offer. Consequently, computational resources are inefficiently allocated, being wasted either on trivial prompts that yield diminishing returns or on excessively challenging examples where the model shows negligible progress. Recent works have investigated progressive training curricula that partition the process into several hand-crafted stages of increasing difficulty (Luo et al., 2025; Song et al., 2025). However, such partitioning is overly coarse and struggles to align with the evolving capabilities of reasoning models during training. Other approaches apply online data filtering by generating and then pruning samples (Yu et al., 2025; Bae et al., 2025; Lin et al., 2025). Yet, this paradigm does little to conserve computational resources and instead leads to suboptimal sample efficiency. Additional studies have Equal Contribution. Corresponding authors. 1 Preprint, Work in Progress Figure 1: Illustration of our theoretical and practical contributions. The first part presents our theoretical analysis, which establishes the relationship between the gradient efficiency and models question-answering accuracy, denoted as pθ(x). Building upon these insights, we develop CurES, practical method that initially estimates pθ(x) using small rollout quantity, then reallocates prompt sampling probabilities and rollout quantities based on the estimated accuracy. We progressively enhance the confidence of these accuracy estimates through posterior estimation. The figure further contrasts CurES with existing approaches, highlighting differences in managing prompt sampling distributions of Speed-RL (Zhang et al., 2025) and rollout quantities of GVM (Yao et al., 2025). begun exploring dynamic computation reallocation across prompts with minimal overhead (Yao et al., 2025; Zhang et al., 2025; Shi et al., 2025). Nevertheless, these techniques address only isolated facets of training acceleration, without fully accounting for the problems inherent dynamism. In this work, we first analyze the efficiency of training optimization for reasoning models from the perspective of gradients, elucidating its close relationship with the sampling probability distribution of prompts and the allocation of rollout quantities across these prompts. Our analysis reveals that the prompt sampling distribution directly influences the speed of gradient descent, while the allocation of rollout quantities affects the consistency and stability of overall gradient updates. Leveraging these insights, we propose CurES, practical training method. CurES first estimates prompt difficulty via models question-answering accuracy, then reallocates prompt sampling probabilities and rollout quantities accordingly. During training, the confidence in these accuracy estimates is progressively refined through posterior estimation based on previously sampled data, thereby improving the robustness of the allocation process. Figure 1 illustrates the overall approach, and our contributions are summarized below: We provide theoretical analysis from the gradient perspective, elucidating the intrinsic relationship between training optimization efficiency and prompt sampling distribution, as well as the allocation of rollout quantities across prompts. Guided by the theoretical analysis, we propose practical training method that integrates Bayesian posterior estimation, achieving enhanced efficiency and stability in reasoning model training with minimal computational overhead. Experimental results show that our CurES outperforms GRPO by +3.30 points and +4.82 points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Gradient Analysis in Optimization. Gradient analysis plays pivotal role in understanding and improving optimization processes (Ruder, 2016; Bottou et al., 2018; Yang et al., 2024). com2 Preprint, Work in Progress mon application involves leveraging gradient analysis to reduce variance in iterative optimization and enhance training stability (Medyakov et al., 2025; Yuan et al., 2024). Additionally, it facilitates adaptive learning rate adjustments by dynamically scaling updates based on gradient histories (Dereich et al., 2024; Chen et al., 2024). Gradient analysis also guides data selection and curriculum learning strategies, enabling dynamic adjustment of training data difficulty (Yao et al., 2025; Li et al., 2024b). Furthermore, it aids in detecting anomalous samples to improve data quality by identifying outliers in the gradient space (Chhabra et al., 2024). As gradients are directly tied to model optimization, they provide the most immediate insights into training dynamics. Theoretical analysis of gradients enables predictions of convergence rates and bounds, ensuring robust optimization guarantees (Zhao & Xu, 2024). In this work, we examine the interplay between model optimization and sample selection from gradient perspective, deriving methods to enhance training efficiency. Curriculum Learning and Data Selection in RLVR. Effective data selection is critical for optimizing RLVR training, yet designing curricula that align with the dynamic capabilities of LLMs remains challenging. Progressive training curricula, such as those proposed in (Luo et al., 2025; Song et al., 2025), partition training into hand-crafted stages of increasing difficulty. However, these static approaches often fail to adapt to the evolving proficiency of models during training. Online data filtering methods, such as those in (Yu et al., 2025; Bae et al., 2025; Lin et al., 2025), generate and prune samples to focus on high-impact data but introduce significant computational overhead, leading to suboptimal sample efficiency. Recent efforts have explored dynamic computation reallocation to prioritize prompts with higher training utility (Yao et al., 2025; Zhang et al., 2025; Shi et al., 2025). However, these methods address only specific aspects of training acceleration and do not fully account for the interplay between prompt sampling distributions and rollout quantity allocation. In contrast, our work proposes CurES, method that dynamically adjusts sampling probabilities and rollout quantities based on Bayesian posterior estimation of prompt difficulty, achieving improved sample efficiency with minimal computational overhead."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "RLVR represents specialized reinforcement learning paradigm tailored for reasoning tasks, where reward signals can be deterministically verified through programmatic means. This approach is particularly well-suited for domains such as mathematical reasoning, code generation, and logical deduction, where correctness criteria are objectively defined (Lambert et al., 2024; Guo et al., 2025; Team et al., 2025). Formally, given policy model πθ and prompt distribution ρ, the RLVR objective aims to maximize the expected reward while constraining policy updates within trust region: (cid:2)Aθold (x, y)(cid:3), L(θ) = Exρ,yπθ(x) subject to ExD[DKL(πθold (x)πθ(x))] δ. Here, Aθold (x, y) = r(x, y) Eyπθold [r(x, y)] denotes the advantage function, θold represents the policy parameters from the previous iteration, and δ defines the trust region boundary that prevents excessive policy divergence. (1) key characteristic of RLVR is its reward formulation. Unlike preference-based RLHF that relies on subjective human judgments, RLVR employs verifiable reward function defined as: r(x, y) = (cid:26)1, 0, if is the correct answer for otherwise (2) The straightforward reward function design partially mitigates the issue of reward hacking."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "In this section, we introduce CurES, novel method designed to enhance the training efficiency of Reasoning LLMs. We begin by establishing theoretical connection between gradient optimization efficiency and two key factors: the sampling distribution of prompts and the allocation of rollout quantities across these prompts. Based on this analysis, CurES first leverages the estimation of the models question-answering accuracy to assess prompt difficulty, which is then used to guide an optimal sampling strategy and rollout quantity allocation. By leveraging Bayesian posterior estimation, we progressively refine the confidence in these accuracy estimates using historical sampling data, ensuring robust and adaptive resource allocation with minimal computational overhead. 3 Preprint, Work in Progress"
        },
        {
            "title": "4.1 PROMPT DIFFICULTY CAPS OPTIMIZATION POTENTIAL",
            "content": "To facilitate subsequent derivations, we define the question difficulty as the models accuracy in answering the question. Given policy model πθ and binary reward function r(x, y) in Eq. (2), the expression for the models question-answering accuracy pθ is given by: pθ(x) = Eyπθ [r(x, y)]. (3) To investigate how prompt difficulty influences model gradient updates, we first consider the following optimization problem for given prompt x: min L(x; θ) = min Eyπθ(x) s.t. DKL(πθold (x)πθ(x)) δ. (cid:2)Aθold (x, y)(cid:3), (4) Here, we separately analyze the impact of different prompts on the loss function and theoretically examine how varying prompt difficulty levels affect model training efficiency. We employ the Lagrange multiplier method to solve the above problem. First, we set θ = θold + and reformulate the problem as follows: = argmin L(x; θold + d) + λ(DKL(πθold (x)πθold+d(x)) δ). (5) By performing first-order Taylor expansion on the loss function L(x; θ) and second-order Taylor expansion on the KL divergence term DKL(πθold (x)πθ(x)), followed by simplification, we derive the following equation: = argmin L(x; θold) + θL(x; θ)T(cid:12) (cid:12) (cid:12)θ=θold + λ dTF (x; θold)d λδ, (6) where (x; θ) is the Fisher Information Matrix, metric quantifying the information that observed data provides about parameter estimates in probabilistic models. In reinforcement learning, it primarily serves to construct more reasonable parameter update directions, thereby improving the efficiency and stability of policy optimization. Here, (x; θ) is represented as (cid:2)θ log πθ(yx)θ log πθ(yx)T(cid:3) . (x; θ) = Eyπθ (7) Through mathematical derivation to address the problem of Eq. (6), we obtain the following results: = 1 λ (cid:12) 1(x; θold)θL(x; θ) (cid:12) (cid:12)θ=θold , θL(x; θ)(cid:12) (cid:12)θ=θold 1(x; θold)θL(x; θ)(cid:12) (cid:12)θ=θold 2δ . (cid:115) λ = (8) (9) With Eq. (8) and Eq. (9), we derive the expression for the update of the loss function: L(x; θold + d) L(x; θold) = (cid:114) 2δθL(x; θ)T (cid:12) (cid:12) (cid:12)θ=θold (cid:12) (cid:12) 1(x; θold)θL(x; θ) (cid:12)θ=θ old . (10) According to the definition, the binary reward function r(x, y) serves as an unbiased estimator of the models question-answering accuracy pθ(x), i.e., pθ(x) = Eyπθ [r(x, y)]. Through the application of the Cramer-Rao inequality, we derive the following fundamental result: L(x; θold + d) L(x; θold) (cid:112)2δpθold(x) (1 pθold(x)). (11) Therefore, for the loss function L(θ), its optimization potential exhibits the following relationship with prompt difficulty: L(θold + d) L(θold) Exρ (cid:104)(cid:112)2δpθold (x) (1 pθold(x)) (cid:105) . (12) This demonstrates that the convergence rate of the models loss function is intrinsically related to the difficulty of the prompt dataset, which is quantified by the models answering accuracy. To accelerate training, the sampling distribution ρ should assign varied probabilities to prompts based on difficulty 4 Preprint, Work in Progress while maintaining balance with exploration. Thus, we seek the optimal sampling distribution ρ under the entropy maximization constraint. Concretely, we address the following problem: max Exρ (cid:104)(cid:112)2δpθold(x) (1 pθold(x)) + αH(ρ) (cid:105) , s.t. (cid:88) i=1 ρ(xi) = 1. (13) Solving the aforementioned problem, we obtain the optimal sampling distribution as follows: ρ(x) = (cid:80) (cid:16)(cid:112)pθold(x) (1 pθold(x))/τ (cid:16)(cid:112)pθold(x) (1 pθold(x))/τ (cid:17) exp exp (cid:17) , (14) where τ = α 2δ is hyperparameter. For the theoretical proof please refer to Appendix A.1."
        },
        {
            "title": "4.2 CLOSING THE GAP WITH THEORETICAL BOUND",
            "content": "In the previous section, we derived an upper bound on the gradient update for given prompt. However, due to the high computational cost of the natural gradient method, it is often avoided in practice, and the theoretical result is instead used to guide prompt sampling. During actual gradient updates, we aim to closely approximate the theoretical efficiency limit within trust region bounded by KL divergence constraint of δ. Specifically, after sampling batch of prompts, we seek to optimize operations to approach the bound. Within the curriculum learning framework, we consider optimizing the allocation of rollout quantities across prompts under fixed total rollout budget of to minimize the following loss function: (cid:20)(cid:16) min L(ˆθ) L(θold) (cid:16) Exρ (cid:104)(cid:112)2δpθold(x) (1 pθold(x)) (cid:105)(cid:17)(cid:17)2(cid:21) , s.t. (cid:88) i= ni = N. (15) Here, ˆθ denotes the updated model parameters obtained from θold after applying the practical gradient update, i.e.: ˆθ = θold ηθ ˆL(θ)(cid:12) (cid:12)θ=θold , ˆL(θ) = (cid:88) i=1 1 ni (cid:88) yj Di (cid:20) πθ(yjxi) πθold(yjxi) (cid:21) Aθold . (16) Where η is the learning rate and ni denotes the number of sampled rollouts for question xi. We assume that η is chosen such that the policy update remains within KL divergence constraint of δ. For convenience, we denote = θL(θ)(cid:12) tion, we can show that the optimization problem reduces to the following: . By simplifying the loss func- ,ˆg = θ ˆL(θ)(cid:12) (cid:12)θ=θold (cid:12)θ=θold min gV(ˆg)g, s.t. (cid:88) i=1 ni = N. (17) The theoretical gradient direction is typically unknown, and we seek to control the uncertainty of the estimator in all possible directions. Therefore, we instead minimize the total variance Tr(V(ˆg)), which corresponds to uniformly reducing the variance in all directions. This approach is widely adopted technique for variance estimation(Bottou et al., 2018; Papini et al., 2018; Wang et al., 2013). In other words, we consider the following optimization problem: min Tr(V(ˆg)), s.t. (cid:88) i=1 ni = N. (18) By expanding the variance of the aforementioned gradient, we isolate the rollout quantities ni to facilitate analysis: min 1 m2 (cid:88) Tr (cid:16) Vyπθold (h (y, xi; θold)) (cid:17) i=1 ni 5 , s.t. (cid:88) i=1 ni = N. (19) Preprint, Work in Progress where h(x, y; θ) = θπθ(yj xi) the above problem, we obtain the optimal solution as follows: πθold (yj xi) Aθold (xi, yj). By applying the Lagrange multiplier method to solve ni = σi σj (cid:80) N, σi = (cid:114) (cid:16) Tr Vyπθold (h (y, xi; θold)) (cid:17) . (20) The remaining challenge is computing σi. By expanding the variance and noting that the advantage function can be evaluated based on whether the rollout is correct, i.e., Aθold(x, y) = I(y is correct for x) pθold (x), we derive the following symmetric computational form: (21) . (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:16) Tr Vyπθold (h (y, xi; θold)) (cid:17) =pθold(xi) (1 pθold(xi)) (cid:104) yπθold ,r=1 + (pθold(xi))2 (1 pθold(xi)) θ log πθ(yxi)θ=θold2(cid:105) θ log πθ(yxi)θ=θold 2(cid:105) (cid:104) yπθold ,r=0 (cid:13) (cid:13) (cid:13) (cid:13) yπθold ,r= pθold(xi)2(1 pθold(xi))2 [θ log πθ(yxi))] [θ log πθ(yxi)] yπθold ,r=0 The optimized formula decomposes the variance estimation problem into two categories based on answer correctness, integrating it with the prompt difficulty estimation from Section 4.1. By leveraging algebraic operations on prompt difficulty and policy gradients, it reuses difficulty estimates from sampling and transforms variance estimation into more tractable form. The theoretical proof is provided in Appendix A.2. 4.3 PROMPT DIFFICULTY ASSESSMENT AND ALGORITHMIC IMPLEMENTATION Estimating prompt difficulty is crucial for both sampling questions and allocating rollout quantities. However, difficulty changes dynamically during policy training, making accurate estimation challenging. straightforward approach is to add pre-evaluation step before each sampling, but this increases computational overhead and fails to leverage new samples for posterior estimation to improve confidence. To address this, we propose Bayesian inference framework that decomposes rollout into multi-stage mini-batch process. This refines the posterior estimation of the dataset, dynamically adjusting the sampling distribution based on updated difficulty assessments. Specifically, as the model πθold rollouts on prompt xi multiple times, the number of correct answers follows binomial distribution with success probability pθold(xi). We can assume that pθold (xi) follows Beta distribution, the conjugate prior of the binomial distribution, which is widely adopted technique in Bayesian inference (Kruschke, 2010; Qu et al., 2025): pθold (xi) Beta(α0(xi), β0(xi)), (22) where α0(xi) and β0(xi) can be interpreted as the counts of correct and incorrect answers during sampling, which can be initialized using small batch of sampled data for cold-start estimation. Since the Beta distribution is conjugate to the binomial likelihood, the posterior distribution remains Beta-distributed after observing new samples. Let αt1(xi) and βt1(xi) denote the cumulative counts of correct and incorrect answers for prompt xi up to step 1. If, at step t, mini-batch generates ni answer with correct, the posterior distribution for pθold(xi) after steps is: αt(xi) = αt1(xi) + s, βt(xi) = βt1(xi) + ni s, pθold(xi) Beta(αt(xi), βt(xi)). (23) (24) To reduce randomness, we use the mean of the Beta distribution to estimate prompt difficulty in our experiments. This estimation approach enables modeling and estimating the difficulty of each prompt with minimal overhead. However, as the models performance evolves during training, the estimation process is susceptible to distribution shift, which becomes more pronounced with increasing training steps. To mitigate this issue, we adopt straightforward solution inspired by GVM (Yao et al., 2025). Specifically, we divide the dataset into non-overlapped subsets and perform iterative training on 6 Preprint, Work in Progress for each xi Dt do end for for step = 1, , do Algorithm 1: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs (CurES) 1: Input: initial policy model πθ; reward function r(x, y); prompt dataset = {xi}N Sample = rollouts {yi,1, . . . , yi,k} πθold(xi). Compute rewards r(xi, yi,j). Initialize counts and difficulty estimations p(xi) according to Eq. (22). Initialize sampling probabilities ρ according to Eq. (14). Compute gradient contribution according to Eq. (21). i=1; number of iterations ; prompt batch size m; learning rate η; parameter τ ; pre-rollout size ; number of steps per iteration . 2: for each 1, . . . , do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end for 23: Return πθ. Update the old policy model πθold πθ. Sample batch of prompts = {xi}m Obtain rollout quantities ni for xi according to Eq. (20). for each xi do Sample = ni rollouts {yi,1, . . . , yi,k} πθold (xi). Compute rewards r(xi, yi,j). Update counts and difficulty estimations p(xi) according to Eq. (23). end for Update sampling probabilities ρ according to Eq. (14). Update policy πθ by applying RL training. i=1 with replacement according to ρ. end for these subsets. We train the model for fixed training steps of in every iteration. The estimations of prompt difficulty and gradient variance are reset when new iteration begins. This method effectively alleviates distribution shift without introducing significant computational overhead. Moreover, the iterative process enables the model to adaptively adjust its sampling allocation based on its own evolving capabilities throughout training. For further details, please refer to Algorithm 1."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we present comprehensive experimental results and analysis of our CurES with other baselines. Our experiments focus on the following research questions: RQ1: Does CurES outperform other related baseline methods across various benchmarks? RQ2: How does CurES adapt its sampling strategy to accelerate learning efficiency? RQ3: Does CurES enhance sampling efficiency compared to other baseline methods? Training Details. We employ VERL (Sheng et al., 2025) as our training framework and initialize our policy using Qwen2.5-Math models (1.5B and 7B parameters). For the training dataset, we utilize Numina-Math (Li et al., 2024a), partitioning it into 15 subsets following GVM (Yao et al., 2025). We conduct iterative training across these subsets, resulting in 15 training iterations. At the beginning of each iteration, we perform 4 rollouts per prompt to establish an initial difficulty distribution and an assignment of rollout quantities under total sample budget of 8 1024. During training, we sample prompts according to the difficulty distribution with replacement and conduct rollouts according to the assigned rollout quantities. To make fair comparison with GVM, we train 10 steps in each iteration. We employ GRPO (Shao et al., 2024) and REINFORCE++ (RPP) (Hu et al., 2025) as advantage estimators for all methods. The learning rate is set to constant 1 106. Evaluation Benchmarks. To evaluate the complex reasoning capabilities, we choose broad set of challenging reasoning benchmarks, including MATH500 (Hendrycks et al., 2021), AIME 2024 and 2025 (Li et al., 2024a), AMC 2023 (Li et al., 2024a), GSM8K (Cobbe et al., 2021), GaokaoEN 2023 (Zhang et al., 2023), Mineva (Lewkowycz et al., 2022) and OlympiadBench (He et al., 7 Preprint, Work in Progress Figure 2: Comparison of learning curves between CurES and GVM across different backbone models and advantage estimators. CurES consistently outperforms GVM under the same number of training steps, demonstrating more efficient utilization of samples. Table 1: Quantitative results of different methods across various datasets. The best and second best results are in bold and underlined. Method Pass@1 Average@16 MATH500 GSM8K GAO23 MINERVA OLYM AIME24 AIME25 AMC Qwen2.5-Math-1.5B 40.20 73.80 +GRPO +RPP 64.80 +Speed-RL-GRPO 68.80 65.80 +Speed-RL-RPP 74.80 +GVM-GRPO 75.40 +GVM-RPP 77.20 +CurES-GRPO 75.40 +CurES-RPP Qwen2.5-Math-7B 60.20 80.00 +GRPO +RPP 81.20 +Speed-RL-GRPO 82.80 78.60 +Speed-RL-RPP 81.60 +GVM-GRPO 81.60 +GVM-RPP 84.80 +CurES-GRPO 81.80 +CurES-RPP 43.90 86.43 82.94 85.67 85.67 84.23 84.00 85.97 85.82 72.40 91.43 91.89 88.70 91.81 91.28 90.07 92.27 91.89 25.19 48.83 42.08 47.14 48.31 48.83 49.61 51.43 51. 44.68 51.43 55.58 55.58 53.77 54.03 55.32 56.62 54.55 11.40 27.94 21.32 27.40 27.94 27.21 24.63 31.62 28.31 22.79 31.99 39.71 29.41 37.13 32.72 29.04 37.87 33.09 21.04 35.41 29.19 35.56 36.30 35.56 35.56 37.33 37.04 30.81 38.37 40.00 42.37 42.96 42.67 40.30 43.56 40.59 1.67 8.54 4.17 12.08 12.58 10.21 11.46 13.33 12. 7.92 20.00 18.54 20.21 17.29 23.54 17.50 24.58 23.33 1.67 6.67 3.33 6.88 10.08 11.25 6.04 10.42 11.46 1.88 10.00 11.67 11.46 12.08 15.00 8.33 15.21 12.92 14.84 45.47 39.06 47.19 47.34 50.47 50.94 52.19 50.94 27.19 57.50 62.81 60.16 62.81 64.31 53.44 64.38 58.75 Avg. 20.00 41.64 35.86 41.34 41.75 42.82 42.21 44.94 44.14 33.48 47.59 50.18 48.84 49.56 50.64 46.95 52.41 49.62 2024). These benchmarks comprehensively evaluate mathematical reasoning capabilities. Since AIME 2024, 2025 and AMC 2023 are highly challenging competition benchmarks, which are of limited sizes of test samples, we present the results averaged over 16 runs. Baselines. To demonstrate the reasoning ability of our CurES, we compare it with many strong baseline methods: GRPO (Shao et al., 2024), RPP (Hu et al., 2025), Speed-RL (Zhang et al., 2025) and GVM (Yao et al., 2025). Specifically, GRPO and RPP are commonly used in training mathematical problem solving models. Speed-RL is an adaptive online RL curriculum that selectively chooses samples of intermediate difficulty to maximize learning efficiency (i.e., samples whose accuracy is not 0 or 1). GVM is prompt-specific dynamic sample allocation strategy designed to minimize stochastic gradient variance under computational budget constraint. 5.1 OVERALL PERFORMANCE (RQ1) We present the learning curve of Qwen2.5-Math-1.5B and 7B models trained with different methods and advantage estimators in Figure 2. Across all configurations, CurES exhibits higher progressive and final accuracy compared to GVM. This advantage originates from two key differences: (i) CurES adaptively allocates prompt sampling probabilities based on estimated success rates, which our theoretical analysis confirms enhances training efficiency; (ii) while GVM monotonically decreases rollout allocation as accuracy increases, CurES allocates more rollout budget to prompts of moderate difficulty in Figure 4, resulting in more consistent training gradients and improved training stability. 8 Preprint, Work in Progress Figure 3: The evolution of the estimated accuracy distributions for the Qwen2.5-Math-1.5B (left) and 7B (right) models across 15 iterations. Each violin shows the distribution of accuracy across samples: the width reflects density, the central line marks the median. Figure 4: Allocation of rollout quantities with respect to accuracy in CurES at different training iterations. CurES concentrates more rollouts on moderately difficult prompts. To demonstrate the effectiveness of our CurES, we compare it with representative baselines that are trained for the same number of steps. The main results are demonstrated in Table 1. The results clearly demonstrate that CurES consistently outperforms GVM and other baselines with both GRPO and RPP as advantage estimators. Across both model scales, CurES establishes state-of-theart results on several datasets and consistently matches or surpasses the strongest baselines across all settings, confirming the superior generalization ability of our CurES. 5.2 SAMPLING BEHAVIOR (RQ2) Figure 3 illustrates the evolution of the difficulty distribution for both the Qwen2.5-Math-1.5B and 7B models throughout the training process. At iteration 1, the estimated accuracy is broadly distributed. Subsequently, as training progresses, this distribution shifts toward higher values and becomes more concentrated, indicating that the models are effectively learning and mastering the presented samples. This shift also underscores the importance of redistributing prompt sampling probabilities, as the models success rates on problems are primarily bimodal, concentrated at high and low values. Such redistribution enhances training efficiency, whereas uniform sampling followed by answer generation and accuracy-based filtering significantly reduces efficiency under this bimodal distribution. Meanwhile, another view of how the CurES method dynamically adjusts the rollout quantities assignment during training is presented in Figure 4. The trend lines of all iterations approximate bell-shaped distribution, with prompts of intermediate accuracy allocated more rollout quantities, as anticipated given their high efficiency. Furthermore, as the training procedure continues, the distribution becomes progressively sharper and narrower, indicating that CurES dynamically increases rollout quantities for moderately difficult prompts. This pattern aligns with the observation in Figure 3, which shows gradual reduction in moderately difficult prompts as the model improves. By adaptively increasing rollout quantities, CurES compensates for their diminishing presence, ensuring they remain substantial portion of each training batch. By coupling difficulty-based prompt sampling with the adaptive sample sizing, CurES sustains an abundance of informative prompts and thereby maximizes performance gains per step. 9 Preprint, Work in Progress Figure 5: Performance convergence of CurES on MATH500 with different sampling configurations. Figure 6: Efficiency comparison of CurES against baselines on MATH500. Gray dashed lines indicate the steps required for CurES and the baseline to reach the highest average accuracy of the baseline during the entire training period. 5.3 EFFICIENCY ANALYSIS (RQ3) To analyze the effect of different combinations of pre-sampling scale (N ) and training-phase sample budgets coefficient (n), which determines training-phase sample budget of m, on model performance convergence, we conducted experiments as depicted in Figure 5. larger leads to more accurate initial accuracy estimation, while larger provides greater computation budget. The results show that increasing either or does not yield proportional performance benefit relative to the increased computational cost. This finding underscores the efficiency of CurES, which effectively directs the model toward high-yield learning samples with minimal computational overhead, highlighting its superior sample efficiency. We also provide direct comparison of CurES against GRPO and RPP in Figure 6. The plots show the learning curve of each method over training steps. CurES-GRPO achieves the same peak performance as the GRPO in just 5.5 fewer steps. Similarly, CurES-RPP reaches its peak performance 1.75x faster than the RPP baseline. The remarkable sample efficiency is direct consequence of CurESs ability to consistently provide the model with optimally challenging samples."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose CurES, theoretically grounded curriculum learning algorithm for RLVR. By linking gradient efficiency to accuracy, our approach adaptively prioritizes training prompts of optimal difficulty and dynamically allocates rollout budgets. Beyond the theoretical analysis, our algorithmic design leverages Bayesian framework to track prompt difficulty in lightweight yet adaptive manner. Specifically, we model the success rate of each prompt instance with Beta distribution, which naturally incorporates prior information and posterior updates as new rollouts are observed. Combined with the derived sampling distribution and variance-based rollout quantity allocation, this Bayesian mechanism ensures that both question selection and sample budgeting adapt dynamically to the evolving policy, thereby maximizing training efficiency in practice. Experiments on wide range of mathematical reasoning benchmarks show that CurES consistently outperforms strong baselines in both accuracy and convergence speed, demonstrating superior sample efficiency. 10 Preprint, Work in Progress"
        },
        {
            "title": "REFERENCES",
            "content": "Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint arXiv:2504.03380, 2025. Leon Bottou, Frank Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM review, 60(2):223311, 2018. Shuang Chen, Changlun Zhang, and Haibing Mu. An adaptive learning rate deep learning optimizer using long and short-term gradients based on gl fractional-order derivative. Neural Processing Letters, 56(2):106, 2024. Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, and Hongfu Liu. Outlier gradient analysis: Efficiently identifying detrimental training samples for deep learning models. arXiv preprint arXiv:2405.03869, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, et al. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Steffen Dereich, Arnulf Jentzen, and Adrian Riekert. Learning rate adaptive stochastic gradient descent optimization methods: numerical simulations for deep learning methods for partial differential equations and convergence analyses. arXiv preprint arXiv:2406.14340, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv.org/abs/2501.03262. John Kruschke. Bayesian data analysis. Wiley Interdisciplinary Reviews: Cognitive Science, 1(5): 658676, 2010. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024a. Xinyu Li, Wenqing Ye, Yueyi Zhang, and Xiaoyan Sun. Grace: Gradient-based active learning with curriculum enhancement for multimodal sentiment analysis. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 57025711, 2024b. Zhihang Lin, Mingbao Lin, Yuan Xie, and Rongrong Ji. Cppo: Accelerating the training of group relative policy optimization-based reasoning models. arXiv preprint arXiv:2503.22342, 2025. 11 Preprint, Work in Progress Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025. Daniil Medyakov, Gleb Molodtsov, Savelii Chezhegov, Alexey Rebrikov, and Aleksandr Beznosikov. Variance reduction methods do not need to compute full gradients: Improved efficiency through shuffling. arXiv preprint arXiv:2502.14648, 2025. Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochastic variance-reduced policy gradient. In International conference on machine learning, pp. 40264035. PMLR, 2018. Yun Qu, Qi Wang, Yixiu Mao, Vincent Tao Hu, Bjorn Ommer, and Xiangyang Ji. Can prompt difficulty be online predicted for accelerating rl finetuning of reasoning models? arXiv preprint arXiv:2507.04632, 2025. Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pp. 12791297. ACM, 2025. Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning via adaptive curriculum learning. arXiv preprint arXiv:2504.05520, 2025. Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, and Feng Zhang. Fastcurl: Curriculum reinforcement learning with stage-wise context scaling for efficient training r1-like reasoning models. arXiv preprint arXiv:2503.17287, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Chong Wang, Xi Chen, Alexander Smola, and Eric Xing. Variance reduction for stochastic gradient optimization. Advances in neural information processing systems, 26, 2013. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Hongru Yang, Bhavya Kailkhura, Zhangyang Wang, Yingbin Liang, et al. Training dynamics of transformers to recognize word co-occurrence via gradient flow analysis. Advances in Neural Information Processing Systems, 37:4604746117, 2024. Jiarui Yao, Yifan Hao, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, and Tong Zhang. Optimizing chain-of-thought reasoners via gradient variance minimization in rejection sampling and RL. CoRR, abs/2505.02391, 2025. doi: 10.48550/ARXIV.2505.02391. URL https: //doi.org/10.48550/arXiv.2505.02391. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Huizhuo Yuan, Yifeng Liu, Shuang Wu, Xun Zhou, and Quanquan Gu. Mars: Unleashing the power of variance reduction for training large models. arXiv preprint arXiv:2411.10438, 2024. 12 Preprint, Work in Progress Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Ruiqi Zhang, Daman Arora, Song Mei, and Andrea Zanette. SPEED-RL: faster training of reasoning models via online curriculum learning. CoRR, abs/2506.09016, 2025. doi: 10.48550/ARXIV. 2506.09016. URL https://doi.org/10.48550/arXiv.2506.09016. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on GAOKAO benchmark. CoRR, abs/2305.12474, 2023. doi: 10.48550/ARXIV.2305.12474. URL https://doi.org/10.48550/arXiv.2305.12474. Hongru Zhao and Jinchao Xu. Convergence analysis and trajectory comparison of gradient descent for overparameterized deep linear networks. Transactions on Machine Learning Research, 2024. 13 Preprint, Work in Progress"
        },
        {
            "title": "A MATHEMATICAL DERIVATIONS",
            "content": "Lemma 1 (Cramer-Rao Inequality) Let {pθ(x), θ Θ} be Cramer-Rao regular family with parameter space Θ Rk, where the Fisher information matrix I(θ) is non-singular. Let g(θ) = (g1(θ), , gs(θ)) for k, and assume the partial derivatives gi(θ)/θj exist for all = 1, , and = 1, , k. Suppose (X) is an unbiased estimator of g(θ) with finite second moment. Denote G(θ) = θg(θ), then we have Vθ(T (X)) G(θ)I 1(θ)G(θ). (25) A.1 PROMPT DIFFICULTY CAPS OPTIMIZATION POTENTIAL Given an individual sample x, we first consider the optimization problem as follows: min L(x; θ) = min Eyπθ(x) (cid:2)Aθold (x, y)(cid:3), s.t. DKL(πθold (x)πθ(x)) δ (26) We define θ = θold+d and rewrite this constrained optimization problem via the Lagrange multiplier method: = argmin L(x; θold + d) + λ(DKL(πθold (x)πθold+d(x)) δ). (27) Using the Taylor expansion formula, we have: (cid:19) (cid:12) (cid:12) (cid:12)θ=θold . = argmin = argmin (cid:18) L(x; θold + d) + λ(DKL(πθold (x)πθold+d(x)) δ) L(x; θold) λδ λ 2 + θL(x; θ)d + λθDKL(πθold (x)πθ(x))d + d2 θDKL(πθold (x)πθ(x))d (28) (29) We first compute the first-order and second-order derivatives of the KL divergence term: (cid:12)θ=θold (cid:12)θ=θold θDKL(πθold (x)πθ(x))(cid:12) [log πθold(yx)] (cid:12) =θEyπθold [θ log πθ(yx)] (cid:12) = Eyπθold (cid:20) θπθ(yx) πθ(yx) = Eyπθold (cid:21) (cid:12) (cid:12) (cid:12)θ=θold (cid:12)θ=θold θEyπθold [log πθ(yx)] (cid:12) (cid:12)θ=θold (cid:88) = θπθ(yx)(cid:12) (cid:12)θ=θold πθ(yx)(cid:12) (cid:12)θ=θold (cid:88) =θ =0. 14 Preprint, Work in Progress 2 = 2 θ = Eyπθold θDKL(πθold (x)πθ(x))(cid:12) [log πθ(yx)] (cid:12) θ log πθ(yx)(cid:3) (cid:12) (cid:18) θπθ(yx) πθ(yx) Eyπθold (cid:2)2 (cid:20) = Eyπθold (cid:12)θ=θold (cid:12)θ=θold (cid:12)θ=θold (cid:19)(cid:21) (cid:12) (cid:12) (cid:12)θ=θold θ (cid:20) (2 θπθ(yx))πθ(yx) θπθ(yx)) π2 θ (yx) (cid:21) (cid:20) ( θπθ(yx))θ=θold πθold(yx) + Eyπθold (cid:2)θ log πθ(yx)θ log πθ(yx)(cid:3) (cid:12) (cid:12) (cid:12)θ=θold = Eyπθold = Eyπθold =Eyπθold =F (x; θold). θ πθ(yx) (cid:21) (cid:12) (cid:12) (cid:12)θ=θold (30) (cid:34)(cid:18) θπθ(yx) πθ(yx) (cid:19) (cid:18) θπθ(yx) πθ(yx) (cid:19)(cid:35) (cid:12) (cid:12) (cid:12)θ=θold where (x; θ) = Eyπθ Therefore, (cid:2)θ log πθ(yx)θ log πθ(yx)(cid:3) is termed the Fisher information matrix. = argmin L(x; θold) + θL(x; θ)(cid:12) (cid:12) (cid:12)θ=θold + λ 2 dF (x; θold)d λδ. (31) To find the minimum, we take the derivative of the right-hand side and set it to zero: So we have (cid:12) (cid:12) θL(x; θ) (cid:12)θ=θold + λF (x; θold)d = 0. = 1 λ (cid:12) 1(x; θold)θL(x; θ) (cid:12) (cid:12)θ=θold . We now derive the critical point of the constraint condition: 1 2 dF (x; θold)d DKL(πθold (x)πθold+d(x)) = δ. By simplifying, we obtain (cid:18) 1 λ2 θL(x; θ)(cid:12) (cid:12)θ=θold 1 1(x; θold)θL(x; θ)(cid:12) (cid:12)θ=θold (cid:19) = δ. Therefore, we have obtained the critical value of λ: (32) (33) (34) (35) (cid:115) λ = θL(x; θ)(cid:12) (cid:12)θ=θold 1(x; θold)θL(x; θ)(cid:12) (cid:12)θ=θold 2δ . (36) In this case, the change of the loss function is computed as L(x; θold + d) L(x; θold) = θL(x; θ)(cid:12) (cid:12)θ=θold θL(x; θ)(cid:12) 2δθL(x; θ)(cid:12) 1 λ (cid:113) = = 15 (cid:12)θ=θold 1(x; θold)θL(x; θ)(cid:12) (cid:12)θ=θold (37) (cid:12)θ=θold 1(x; θold)θL(x; θ)(cid:12) (cid:12)θ=θold Preprint, Work in Progress Furthermore, θL(x; θ)θ=θold = θ Eyπθ(x) (cid:2)Aθold (x, y)(cid:3)(cid:12) (cid:12)θ=θold (cid:88) (cid:104) = θπθ(yx) (cid:16) r(x, y) Eyπθold [r(x, y)] = (cid:88) [θπθ(yx) (r(x, y))] (cid:12) (cid:12)θ=θold = θEyπθ [r(x, y)](cid:12) = θpθ(x)θ=θold , (cid:12)θ=θold (cid:17)(cid:105) (cid:12) (cid:12)θ=θold (38) where pθ(x) is the models question-answering accuracy. Consider that r(x, y) is an unbiased estimator of pθ(x), according to the Cramer-Rao inequality, we obtain: (L(θold + d) L(θold)) = Exρ [L(x; θold + d) L(x; θold)] (cid:104)(cid:113) Exρ [ (L(x; θold + d) L(x; θold)) ] 2δθL(x; θ)(cid:12) = Exρ (cid:104)(cid:112)2δVθold(r(x, y)) (cid:104)(cid:112)2δpθold (x) (1 pθold(x)) (cid:12)θ=θold (cid:105) Exρ = Exρ (cid:105) 1(x; θold)θL(x; θ)(cid:12) (cid:12)θ=θold (cid:105) (39) This indicates that the optimization potential of the loss function is inherently related to the difficulty of the prompt itself. To balance the trade-off between exploration and exploitation, we derive the optimal sampling distribution by solving the following objective function under the maximum entropy constraint with the hyperparameter α: max Exρ (cid:105) (cid:104)(cid:112)2δpθold(x) (1 pθold (x)) + αH(ρ) , s.t. (cid:88) i=1 ρ(xi) = 1, ρ(xi) 0 (40) To find the optimal distribution ρ, we employ the method of Lagrange multipliers. The objective function becomes: max = max (cid:113) ρ(xj) (cid:88) j= 2δpθold (xj) (1 pθold(xj))α ρ(xj) log ρ(xj)+µ 1 (cid:88) j= ρ(xj) , (cid:88) j=1 (41) where µ is the Lagrange multiplier associated with the normalization constraint. Taking the partial derivative of with respect to ρ(xj): ρ(xj) (cid:113) = 2δpθold (xj) (1 pθold(xj)) α(log ρ(xj) + 1) µ. Then set the derivative to zero: (cid:113) 2δpθold(xj) (1 pθold (xj)) α(log ρ(xj) + 1) µ = 0. Solving for log ρ(xj): So we have: log ρ(xj) = (cid:112)2δpθold(xj) (1 pθold(xj)) α 1 µ α . ρ(xj) = exp (cid:32) (cid:112)2δpθold(xj) (1 pθold(xj)) α 1 (cid:33) . µ α 16 (42) (43) (44) (45) Preprint, Work in Progress Let = exp (cid:0)1 λ α (cid:1), then Eq. (45) becomes ρ(xj) = exp (cid:32) (cid:112)2δpθold(xj) (1 pθold(xj)) α (cid:33) . Using the constraint (cid:80)N j=1 ρ(xj) = 1, we can obtain 1 = (cid:32) (cid:113) (cid:80)N j=1 exp 2δpθold (xj )(1pθold (xj )) α (cid:33) ."
        },
        {
            "title": "So the optimal distribution is",
            "content": "(cid:32) (cid:113) exp 2δpθold (x)(1pθold (x)) α (cid:33) ρ(x) = (cid:32) (cid:113) (cid:80) exp 2δpθold (x)(1pθold (x)) α (cid:33) . With the substitution τ = α 2δ , the distribution finally becomes: (46) (47) (48) ρ(x) = (cid:80) (cid:16)(cid:112)pθold(x) (1 pθold(x))/τ (cid:16)(cid:112)pθold(x) (1 pθold(x))/τ (cid:17) exp exp (cid:17) . (49) A.2 CLOSING THE GAP WITH THEORETICAL BOUND In the previous proof, we established lower bound for single-step gradient descent within δ-local trust region constrained by KL divergence, i.e., L(θold + d) L(θold) = Exρ [L(x; θold + d) L(x; θold)] = Exρ Exρ = Exρ (cid:113) (cid:104) 2δθL(x; θ)(cid:12) (cid:104)(cid:112)2δVθold(r(x, y)) (cid:104)(cid:112)2δpθold(x) (1 pθold(x)) (cid:12)θ=θold (cid:105) (cid:105) 1(x; θold)θL(x; θ)(cid:12) (cid:12)θ=θold (cid:105) (50) However, due to the high computational cost of the natural gradient method, it is often avoided in practice, and the theoretical result is instead used to guide prompt sampling. During actual gradient updates, we aim to closely approximate the theoretical efficiency limit within trust region bounded by KL divergence constraint of δ. Specifically, after sampling batch of prompts, we seek to optimize operations to approach the bound. Within the curriculum learning framework, we consider optimizing the allocation of rollouts across prompts under fixed total rollout budget of to minimize the following loss function: (cid:20)(cid:16) min L(ˆθ) L(θold) (cid:16) Exρ (cid:104)(cid:112)2δpθold(x) (1 pθold(x)) (cid:105)(cid:17)(cid:17)2(cid:21) , s.t. (cid:88) i=1 ni = N. (51) Here, ˆθ denotes the updated model parameters obtained from θold after applying the practical gradient update, i.e.: ˆθ = θold ηθ ˆL(θ)(cid:12) (cid:12)θ=θold , ˆL(θ) = (cid:88) i=1 1 ni (cid:88) yj Di (cid:20) πθ(yjxi) πθold(yjxi) (cid:21) Aθold . (52) Where η is the learning rate and ni denotes the number of sampled rollouts for question xi. We assume that η is chosen such that the policy update remains within KL divergence constraint of δ. 17 Preprint, Work in Progress In fact, ˆL(θ) is an unbiased estimator of L(θ), that is: (cid:105) (cid:104) ˆL(θ) =E 1 (cid:88) i=1 1 ni (cid:88) yj Di (cid:20) πθ(yjxi) πθold(yjxi) Aθold(xi, yj) (cid:21) = 1 (cid:88) i=1 1 ni (cid:88) yj Di Exiρ,yj πθold (xi) (cid:20) πθ(yjxi) πθold(yjxi) (cid:21) Aθold (xi, yj) (cid:88) 1 i= = Exiρ,yπθold (xi) (cid:20) πθ(yx) =Exρ,yπθold (x) πθold(yx) =Exρ,yπθ(x) [Aθold (x, y)] =L(θ) (cid:21) Aθold(xi, y) (cid:20) πθ(yxi) πθold (yxi) (cid:21) Aθold(x, y) By applying the Taylor expansion, we obtain: L(ˆθ) L(θold) θL(θ)(cid:12) (ˆθ θold) (cid:12)θ=θold = ηθL(θ)(cid:12) (cid:12)θ=θold θ ˆL(θ)(cid:12) (cid:12)θ=θold where θL(θ) = Exρ,yπθ(x) 1 (cid:88) θ ˆL(θ) = (cid:88) 1 ni i=1 yj Di (cid:2)Aθold(x, y)θ log πθ(yx)(cid:3), (cid:21) (cid:20) θπθ(yjxi) Aθold (xi, yj) πθold(yjxi) For convenience, we adopt the following notation: = θL(θ)(cid:12) theo = Exρ , ˆg = θ ˆL(θ)(cid:12) (cid:12)θ=θold (cid:12)θ=θold (cid:104)(cid:112)2δ pθold(x) (1 pθold (x)) (cid:105) Therefore, the original problem can then be simplified as follows: (cid:20)(cid:16) =E L(ˆθ) L(θold) (cid:104)(cid:0)ηgˆg + theo (cid:1)2(cid:105) (cid:16) Exρ (cid:104)(cid:112)2δpθold(x) (1 pθold (x)) (cid:105)(cid:17)(cid:17)2(cid:21) =η2E[(gˆg)2] 2ηtheoE[gˆg] + 2 theo Because ˆL(θ) is an unbiased estimator of L(θ), we have (cid:104) ˆL(θ) (cid:105) (cid:104) ˆL(θ)(cid:12) (cid:105) θ ˆL(θ) (cid:105) (cid:104) θ ˆL(θ)(cid:12) [ˆg] = = θE = θE (cid:104) (cid:12)θ=θold = θL(θ) (cid:105) = θL(θ)(cid:12) (cid:12)θ=θold = (cid:12)θ=θold Therefore, we can obtain Now, regarding the first item: (cid:2)gˆg(cid:3) = gE [ˆg] = gg E[(gˆg)2] = E[gˆgˆgg] = gE[ˆgˆg]g 18 (53) (54) (55) (56) (57) (58) (59) (60) (61) (62) (63) Preprint, Work in Progress E[ˆgˆg] = V(ˆg) + E[ˆg]E[ˆg] = V(ˆg) + gg Hence, the original problem is equivalent to the following formulation: (cid:20)(cid:16) =η2 (cid:16) L(ˆθ) L(θold) gV(ˆg)g + (cid:0)gg(cid:1)2(cid:17) (cid:16) Exρ (cid:104)(cid:112)2δpθold(x) (1 pθold (x)) (cid:0)gg(cid:1) + 2 theo 2ηtheo (64) (65) (cid:105)(cid:17)(cid:17)2(cid:21) Since we aim to minimize the gap from the theoretical update lower bound by reallocating the rollout quantities per question under total sampling budget of , the simplification of the objective function reveals that this problem only affects the first term gV(ˆg)g. Thus, the original optimization problem is equivalent to the following: min gV(ˆg)g, s.t. (cid:88) i=1 ni = N. (66) The theoretical gradient direction is typically unknown, and we seek to control the uncertainty of the estimator in all possible directions. Therefore, we instead minimize the total variance Tr(V(ˆg)), which corresponds to uniformly reducing the variance in all directions. This approach is widely adopted technique for variance estimation(Bottou et al., 2018; Papini et al., 2018; Wang et al., 2013). In other words, we consider the following optimization problem: min Tr(V(ˆg)), s.t. (cid:88) i=1 ni = (67) Since each yj is independently draws from πθold, we can conclude that: V(ˆg) = 1 (cid:88) i=1 1 ni = 1 m2 (cid:88) i= Vyj πθold (cid:88) yj Di 1 ni θπθ(yjxi)(cid:12) πθold(yjxi) (cid:12)θ=θold Aθold(xi, yj) θπθ(yjxi)(cid:12) πθold (yjxi) (cid:12)θ=θold (cid:88) yj Di (68) Aθold(xi, yj) Let h(x, y; θ) = θπθ(yj xi) πθold (yj xi) Aθold(xi, yj), we have V(ˆg) = = = 1 1 m2 1 m2 (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 Vyj πθold 1 ni ni(cid:88) j=1 h(yj, xi; θold) (cid:16) Vyπθold (h (y, xi; θold)) (cid:17) ni 1 n2 Vyπθold (h (y, xi; θold)) ni Therefore, for the total variance Tr(V(ˆg)): Tr(V(ˆg)) = 1 m2 (cid:88) Tr (cid:16) Vyπθold (h (y, xi; θold)) i=1 ni (cid:17) . 19 (69) (70) Preprint, Work in Progress Then we dive into calculating the value of Tr (V(h(y, xi; θold))): (cid:17) (cid:16) Tr Vyπθold Tr =Eyπθold (h(y, xi; θold)) (cid:16) θπθ(yxi)θπθ(yxi)(cid:12) (πθold (yxi))2 (cid:12)θ=θold (cid:17) A2 θold (cid:34)(cid:32) θπθ(yxi)(cid:12) πθold(yxi) (cid:12)θ=θold (cid:33)(cid:35) Aθold Tr =Eyπθold (cid:16) Tr =Eyπθold (cid:16) Tr Eyπθold (cid:33)(cid:35) Aθold (cid:12)θ=θold Eyπθold (cid:34)(cid:32) θπθ(yxi)(cid:12) πθold (yxi) (cid:2)Tr (cid:0)θ log πθ(yxi)θ log πθ(yxi)θ=θold Eyπθold [θ log πθ(yxi)θ=θold Aθold ] Eyπθold (cid:2)Tr (cid:0)θ log πθ(yxi)θ=θoldθ log πθ(yxi)θ=θold [θ log πθ(yxi)θ=θold Aθold ] Eyπθold Eyπθold (cid:104) (cid:105) θ log πθ(yxi)θ=θold2 A2 θold (cid:13) Eyπθold (cid:13) (cid:13) = Eyπθold (cid:3) (cid:1) A2 θold [θ log πθ(yxi)θ=θoldAθold](cid:17) (cid:1) A2 θold (cid:3) [θ log πθ(yxi)θ=θoldAθold] [θ log πθ(yxi)Aθold] (cid:17) (cid:13) 2 (cid:13) (cid:13) (71) Consider the advantage function defined as: We classify the rollouts into two categories based on whether the final answer is correct or not: Aθold (x, y) = r(x, y) Eyπθold (r(x, y)). (72) (cid:16) Tr Vyπθold (h (y, xi; θold)) (cid:17) =P (r = 1xi) yπθold ,r=1 + (r = 0xi) yπθold ,r=0 (cid:13) (cid:13) (cid:13)P (r = 1xi) yπθold ,r=1 θ log πθ(yxi)θ=θold2 (1 pθold(xi))2(cid:105) (cid:104) θ log πθ(yxi)θ=θold2 (pθold(xi))2(cid:105) (cid:104) [θ log πθ(yxi)(1 pθold(xi)))] [θ log πθ(yxi))] [θ log πθ(yxi)] yπθold ,r=0 + (r = 0xi) yπθold ,r=0 (cid:13) 2 (cid:13) [θ log πθ(yxi)(pθold(xi))] (cid:13) =pθold (xi) (1 pθold(xi))2 yπθold ,r= θ log πθ(yxi)θ=θold2(cid:105) (cid:104) + (pθold (xi))2 (1 pθold(xi)) (cid:104) θ log πθ(yxi)θ=θold2(cid:105) yπθold ,r=0 (cid:13) (cid:13) (cid:13) (cid:13) yπθold ,r= pθold(xi)2(1 pθold (xi))2 = σ2 Therefore, we need to solve the following problem: min 1 m2 (cid:88) i=1 σ2 ni , s.t. (cid:88) i= ni = N. We also employ the Lagrange multiplier method to solve this problem: min = min 1 m2 (cid:88) i= σ2 ni + µ( (cid:88) i=1 ni ), where µ is the Lagrange multiplier. 20 (cid:13) 2 (cid:13) (cid:13) (cid:13) (73) (74) (75) Preprint, Work in Progress Figure 7: Comparison of Average Gradient Norms. This figure compares the average gradient norms among CurES-GRPO, CurES-RPP, GRPO, and RPP. The CurES variants consistently exhibit higher gradient norms in three out of the four algorithm-and-model-scale combinations, suggesting that the CurES effectively selects more informative prompts, thereby accelerating the training process. By differentiating both sides with respect to ni and setting the derivative to 0, we obtain: ni = σ2 m2n2 + µ = 0 = i = σ2 m2µ . That is According to the constraint: (cid:88) i= We get ni = σi . µ ni = (cid:88) i=1 σi µ = = µ = (cid:80)m i=1 σi . ni = σi σj (cid:80) N, σi = (cid:114) (cid:16) Tr Vyπθold (h (y, xi; θold)) (cid:17) . Thus, we derive the rollout quantity allocation strategy for different prompts. (76) (77) (78) (79)"
        },
        {
            "title": "B ALGORITHMIC IMPLEMENTATION",
            "content": "B.1 EXTENDED EXPERIMENTAL RESULTS We further analyze the evolution of average gradient norms across different model scales and optimization algorithms (Figure 7). Overall, the CurES variants consistently exhibit stronger gradient signals compared to their corresponding baselines. On Qwen2.5-Math-1.5B, both CurES-RPP and CurES-GRPO maintain substantially higher gradient norms throughout training. On Qwen2.5-Math7B, CurES-RPP continues to yield larger gradients, while CurES-GRPO performs comparably to GRPO. Higher gradient norms indicate that the model receives more informative learning signals, suggesting that CurES effectively prioritizes prompts that accelerate parameter updates. Notably, for the larger 7B model, the optimizer tends to dampen gradient magnitudes more significantly, which partially reduces the advantage of CurES; nevertheless, the overall trend demonstrates its robustness and consistent benefit across scales. 21 Preprint, Work in Progress Figure 8: Distribution of rollout quantities with respect to accuracy in CurES base on Qwen2.5Math-7B at different training iterations.CurES concentrates more rollouts on moderately difficult prompts. Figure 9: Performance convergence of Qwen2.5-Math-CurES-7B on MATH500 with different sampling configurations. Figure 8 illustrates the distribution of rollout allocations across prompts with different accuracy levels over successive training iterations. We observe that CurES adaptively concentrates rollouts on moderately difficult prompts, rather than uniformly sampling across the entire spectrum. This behavior aligns with the intuition that prompts with intermediate difficulty provide the most informative learning signalbeing neither trivially solved nor consistently incorrect. As training progresses, the distribution becomes increasingly peaked around this region, indicating that CurES dynamically refines its sampling strategy to focus computational effort on prompts that are most beneficial for improving policy performance. We further present the performance convergence of CurES with different sampling configurations on Qwen2.5-Math-7B, as shown in Figure 9. We observe that CurES achieves stable improvements across all settings, with only minor differences in convergence speed and final accuracy among varying rollout counts and prompt subsets. Notably, configurations with moderate number of rollouts (e.g., = 4, = 16) strike favorable balance, reaching higher accuracy with fewer steps compared to more extreme settings such as very large or very small rollout numbers. This demonstrates that CurES is robust to sampling configurations and can effectively leverage diverse rollout budgets without significant degradation in performance. Figure 10: Efficiency comparison of CurES against baselines on MATH500 with Qwen2.5-Math7B. Gray dashed lines indicate the steps required for CurES and the baseline to reach the highest average accuracy of the baseline during the entire training period. 22 Preprint, Work in Progress We also compare the training efficiency of CurES against the baselines on MATH500 for 7B models. As shown in Figure 10, CurES achieves the peak accuracy of the baseline 4.3 faster with GRPO and 3.7 faster with RPP, demonstrating significant acceleration. Importantly, CurES not only converges more rapidly but also exceeds the baselines throughout training. These findings confirm that the adaptive curriculum learning mechanism of CurES significantly improves sample efficiency, enabling faster convergence and better final model quality. Case Study 1 Problem Janet goes to the mall and spends $3.50 on ice cream, $7.50 each for movie tickets for herself and her younger sister, and $8.50 on bracelet. If her parents gave her $40, how many dollars does she have left? Answer: 13 GVM Output First, lets calculate the total amount of money Janet spent. She spent $3.50 on ice cream, $7.50 on movie ticket for herself, $7.50 on movie ticket for her younger sister, and $8.50 on bracelet. So the total amount spent is: [3.50 + 7.50 + 7.50 + 8.50] Next, we need to find out how much money she has left after spending this amount. Her parents gave her $40, so the remaining amount is: [40 - (3.50 + 7.50 + 7.50 + 8.50)] Lets calculate this step by step using Python. python # Amount spent on ice cream ice-cream = 3.50 # Amount spent on movie tickets for herself and her younger sister movie-tickets = 7.50 + 7.50 # Amount spent on bracelet bracelet = 8.50 # Total amount spent total-spent = ice-cream + movie-tickets + bracelet # Amount given by parents amount-given = 40.00 # Amount left amount-left = amount-given - total-spent print(amount-left) output 14.0 After calculating, we find that Janet has 14 dollars left. CurES Output Lets break down the problem step by step and then use Python to verify our calculations. 1. First, we need to calculate the total amount Janet spent: $3.50 on ice cream $7.50 each for movie tickets for herself and her younger sister (so $7.50 + $7.50 = $15.00) $8.50 on bracelet So, the total amount spent is $3.50 + $15.00 + $8.50 = $27.00. 2. Janets parents gave her $40. To find out how much money she has left, we subtract the total amount spent from the $40 her parents gave her: [40 27 = 13] The calculations confirm that Janet has 13 dollars left. 23 Preprint, Work in Progress Case Study 2 Problem The Suns effective temperature, Te, is 5800 Kelvin, and its radius is 7 1010 cm (cid:0)7 108m(cid:1). Compute the luminosity (power output) of the Sun in erg/s. Treat the Sun as blackbody radiator, and give your answer to one significant figure. Answer: 4 1033 GVM Output The luminosity of blackbody can be calculated using the Stefan-Boltzmann law: = 4πR2σT 4 where: is the radius, Te is the effective temperature, σ is the Stefan-Boltzmann constant, approximately 5.67 108W m2 K4. First, convert the radius from cm to meters: = 7 1010cm = 7 108m The effective temperature is given as: Now plug the values into the formula: Te = 5800K Calculate R2: Calculate 4 : = 4π(7 108)2(5.67 108)(5800)4 R2 = (7 108)2 = 49 1016 = 4.9 1017m = (5800)4 = 1.1316496 1014K4 4 Substitute the intermediate results back into the luminosity equation: = 4π(4.9 1017)(5.67 108)(1.1316496 1014) Combine the constants and the powers of 10: = 4π(4.9 5.67 1.1316496 10178+14) = 4π(4.9 5.67 1.1316496 1023) Calculate the product of the constants: So, 4.9 5.67 1.1316496 31.03 4π(31.03 1023) = 4π(3.103 1024) Now compute the numerical value: 4 3.14159 3.103 1024 39.4784 3.103 1024 122.5 1024 Rounded to one significant figure: Since 1W = 1J/s = 107erg/s: 1.225 1026W 1 1026W 1 1026 107erg/s = 1 1033erg/s The luminosity in erg/s is: 1 CurES Output The luminosity of blackbody is given by the Stefan-Boltzmann law: where: = 4πR2σT 4 24 Preprint, Work in Progress is the radius of the Sun, Te is the effective temperature of the Sun, σ is the Stefan-Boltzmann constant, σ 5.67 108W m2 K4. Convert the solar radius from centimeters to meters: = 7 1010cm = 7 108m The effective temperature is: Te = 5800K Calculate R2: Calculate 4 : = 4π(7 108)2 (5.67 108) (5800)4 R2 = (7 108)2 = 49 1016 = 4.9 1017m2 = (5800)4 = 1.1316496 1014K4 4 Substitute the intermediate results: = 4π (4.9 1017) (5.67 108) (1.1316496 1014) Combine the constants and the powers of 10: = 4π (4.9 5.67 1.1316496) 10178+14 = 4π (31.37336) 1023W 4 3.14159 31.37336 1023 394.444 1023W 3.94444 1025W Rounded to one significant figure: Since 1W = 1J/s = 107erg/s: 4 1026W = 4 1026 107erg/s = 4 1033erg/s The luminosity of the Sun is: 4 1033 erg/s"
        }
    ],
    "affiliations": [
        "Baidu Inc.",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Institute of Automation, Chinese Academy of Sciences",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "University College London"
    ]
}