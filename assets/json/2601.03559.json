{
    "paper_title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "authors": [
        "Shidong Cao",
        "Hongzhan Lin",
        "Yuxuan Gu",
        "Ziyang Luo",
        "Jing Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning."
        },
        {
            "title": "Start",
            "content": "DIFFCOT: Diffusion-styled Chain-of-Thought Reasoning in LLMs Shidong Cao1, Hongzhan Lin1, Yuxuan Gu2, Ziyang Luo1, Jing Ma1 1Hong Kong Baptist University 2Harbin Institute of Technology {cssdcao,cshzlin,majing}@comp.hkbu.edu.hk 6 2 0 2 7 ] . [ 1 9 5 5 3 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DIFFCOT, diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DIFFCOT integrates diffusion principles at the reasoning-step level via sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving tokenlevel autoregression. To maintain causal consistency, we further introduce causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DIFFCOT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) (Brown et al., 2020; Achiam et al., 2023; Guo et al., 2025) have marked major breakthrough in natural language processing, achieving competitive performance across wide range of mathematical reasoning tasks. widely adopted technique in LLMs, Chainof-Thought (CoT) reasoning (Wei et al., 2022; Kojima et al., 2022), enhances mathematical problemsolving by decomposing tasks into step-by-step intermediate reasoning. However, CoT reasoning is highly sensitive to potential errors, where mistakes introduced in the early stages can propagate through later steps, often leading to incorrect final answers (Wang et al., 2023; Lyu et al., 2023). This highlights central research challenge: reducing errors by guiding LLMs to identify and follow reliable reasoning paths in long thought chains. 1 Recent work has sought to optimize the exploration of reasoning branches in LLMs to reduce errors in CoT, by coupling generation with external critics or search procedures, such as Monte Carlo Tree Search (MCTS), to prune faulty trajectories (Li et al., 2024; Qin et al., 2024; Xi et al., 2024). In contrast, Preference Optimization (PO) methods (Lai et al., 2024; Zhang et al., 2024; Xu et al., 2025) aligned the policy with implicit preferences distilled from reasoning chains, so that decoding itself favors correct trajectories rather than relying on post-hoc filtering. However, beyond the substantial inference-time overhead of MCTS, these methods share fundamental limitation that motivates rethinking of the CoT paradigm. Specifically, they all execute CoT reasoning in strictly step-by-step manner, with each step conditioned on previous ones. Under static teacher-forcing supervision, the model is exposed only to correct prefixes during training, whereas at inference time the prefixes may contain errors that accumulate across steps, resulting in exposure bias that commonly plagues autoregressive generation (Bengio et al., 2015) and imitation learning (Ross et al., 2011). To mitigate the limitations above, we formulate CoT reasoning as globally revisable trajectory, in which intermediate steps are not fixed once generated but can be adaptively corrected in light of later context. Rather than defining reasoning as strictly forward, prefix-conditioned process driven by human priors (see Fig. 1), our method jointly models all reasoning steps within diffusion paradigm (Dhariwal and Nichol, 2021), enabling robust reasoning under realistic noise from global perspective with long-term revision guided by future signals. Starting from corrupted or noisy reasoning chain, the entire reasoning trajectory can be iteratively updated, where intermediate revisions and forward progression are seamlessly integrated into unified refinement process. By unifying iterative revision and forward continuation within Figure 1: Comparison of our proposed DIFFCOT with existing CoT reasoning approaches: (a) Existing step-by-step CoT Reasoning methods adopt teacher-forcing training, where each step depends on the ground-truth output of the previous one. At inference time, this assumption breaks, causing exposure bias and leading to error accumulation. (b) DIFFCOT performs CoT reasoning along both the noise (diffusion) and temporal (autoregressive) dimensions, enabling iterative correction of prior mistakes and effectively mitigating exposure bias. the same diffusion-based refinement process, our design philosophy aim to reduce the discrepancy between training-time supervision and inferencetime reasoning dynamics, leading to more stable reasoning behavior during inference. To this end, we propose Diffusion-Styled Chain of Thought, DIFFCOT, which integrates diffusion principles into the CoT framework to mitigate the exposure bias of static teacher-forcing and address the pointwise and local-step supervision issues of prior preference optimization solutions. Specifically, 1) DIFFCOT modifies only the step-level generation strategy while preserving token-level autoregression, making it straightforward to fine-tune from existing autoregressive models. 2) By incorporating sliding windows and causal noise masks, DIFFCOT unifies generation and revision within versatile framework, to balance autoregressive and diffusion-styled reasoning. 3) Moreover, DIFFCOT employs sentence-level noise injection to convert the reasoning paradigm into iterative denoising, progressively refining corrupted trajectories into coherent chains of thought, which captures the distributional evolution of reasoning errors to enable recovery from diverse and compounding mistakes. Altogether, DIFFCOT could establish unified and adaptive paradigm to CoT reasoning in LLMs. Our contributions are summarized as follows: We propose DIFFCOT, novel diffusion-styled CoT framework that reformulates multi-step reasoning as an iterative denoising process, effectively mitigating exposure bias and error accumulation in autoregressive CoT reasoning. We introduce step-level diffusion-based learning strategy that unifies generation and revision through sliding-window mechanism, enabling retrospective correction of earlier reasoning steps while preserving token-level autoregression. We design causal noise schedule that explicitly encodes the temporal dependency of reasoning steps, balancing global error correction with the causal structure required for coherent reasoning. Extensive experiments on three public mathematical reasoning benchmarks demonstrate that DIFFCOT outperforms State-of-The-Art (SoTA) PO methods, yielding robust gains and substantially improved error-correction capability in CoT."
        },
        {
            "title": "2 Preliminaries",
            "content": "Chain-of-Thought Reasoning Given question prompt p, the CoT paradigm (Wei et al., 2022) explicitly unfolds the reasoning process into sequence of reasoning steps s1:K whose final element sK corresponds to the answer a. The conditional distribution of complete reasoning trace can be 2 expressed as: pθ(s1:K p) = (cid:89) πθ(sk p, s<k), (1) t=1 where sk denotes the k-th reasoning step, is the step budget, and πθ is the Auto-Regressive (AR) policy, i.e., the conditional distribution over the next step given the prompt and the previously generated steps. On annotated CoT data (p, s1:K), the model is trained by maximizing the conditional likelihood: LCoT = (cid:88) k=1 log πθ(sk p, s<k). (2) Diffusion Models Diffusion models (Dhariwal and Nichol, 2021) define generative framework where data samples are gradually corrupted into tractable noise distribution through forward process, and model is trained to approximate the corresponding reverse process. Let x0 φ(x0) denote data sample from real data distribution φ. The forward process produces sequence {xt}T by applying noise operator at each step: t=1 = 1, . . . , T, xt = D(xt1, ηt), (3) where denotes the diffusion step index, ηt is noise variable. could be deterministic or structured corruption operator. The generative process is defined by learning parameterized reverse transition pθ(xt1 xt) that reconstructs the original sample through stepwise denoising. Training minimizes the discrepancy between the predicted ˆxt1 = fθ(xt, t) and the ground-truth corrupted data xt1 obtained from the forward process: LDiffusion = Ex0φ(x0), t{1,...,T } (cid:104) ℓ(cid:0)fθ(xt, t), xt1 (cid:1)(cid:105) , (4) where fθ(xt, t) predicts the reconstruction of xt1 from xt, ℓ(, ) is reconstruction loss. In inference, sampling begins from xT and iteratively applies the learned reverse transitions until x0 is obtained."
        },
        {
            "title": "3 Methodology",
            "content": "During CoT reasoning, LLMs are typically trained only on correct trajectories (Wei et al., 2022), while at inference time they may condition on erroneous intermediate steps (Lyu et al., 2023). Moreover, step-wise optimization focuses on local alignment and ignores future signals within the reasoning trajectory, limiting global consistency (Zhang et al., 2024). Together, these issues lead to error accu3 mulation (Yoon et al., 2025; Chen et al., 2024a), motivating rethinking of CoT reasoning under preference optimization. To address these issues, we argue that effective CoT reasoning requires trajectory-level revision to enforce global consistency and recover from corrupted intermediate steps. As error accumulation leads to noisy reasoning chains at inference time, mechanism for global recovery becomes essential (Lyu et al., 2023; Wang et al., 2023). Motivated by the robustness of diffusion models in reconstructing structured data from noise (Ho et al., 2020; Li et al., 2022), we aim to develop diffusion-styled preference optimization paradigm to reformulate CoT reasoning. To this end, we propose Diffusion-styled Chain of Thought (DIFFCOT), which models CoT reasoning in diffusion-styled framework. We first present step-level forward noising process for CoT using reward-ranked candidates (3.1), and then apply diffusion sliding window to iteratively denoise past steps while generating new ones (3.2). We further introduce causal diffusion noise to strengthen causal consistency across reasoning steps (3.3). An overview is shown in Figure 2. 3.1 Diffusion-Styled Noising for CoT In this section, we design diffusion-styled forward noising process for CoT reasoning at the reasoningstep level, where step-level noise is induced by ranking candidate responses for the same step according to their reward scores. Previous preference optimization methods overlooked the distribution between high-reward and low-reward reasoning steps and relied on isolated step-wise adjustments that fail to mitigate the exposure bias inherent in teacher forcing. To this end, in our principle, higher-reward candidates are treated as lower-noise reasoning states, while lower-reward candidates correspond to progressively higher-noise states in the forward process. This ranking forms progression from clean to corrupted reasoning states under diffusion-styled noise, enabling distribution-aware modeling of reasoning steps. Specifically, we implement forward noising by first collecting CoT reasoning trajectories for each question to construct the training set D. As shown in Fig. 2(a), given question prompt p, we follow the standard CoT data generation process by employing MCTS (Browne et al., 2012), to gradually build search tree for step-by-step solution exploration. The root corresponds to p, and each child node represents an intermediate step s. complete path from the root to terminal node sK yields trajectory tra = s1 s2 sK, where each step sk is annotated with its reward r(sk). In contrast to conventional CoT trajectory construction (Zhang et al., 2024), at each step we collect several candidate responses. These responses are scored using either an external reward model or rollout-based success rates. In our forward noising view, the candidate with the highest reward, denoted as sσ0=w , is regarded as the lowest-noise state for step k, and the remaining candidates are ordered by their rewards to form {sσ1:T }, which we interpret as states with gradually increasing noise. Step-level noise is thus measured by the deviation of lower-reward responses from this best trajectory, indicating the corruption level at the step granularity. This design ensures that the final generation remains coherent while providing set of forward diffusion states for subsequent denoising and preference optimization. Under this step-level forward noising, DIFFCOT defines diffusion-styled distribution that spans low-noise to high-noise reasoning states (Chen et al., 2025). Note that our data construction does not distinguish between correct and incorrect labels at the collection stage as in (Lai et al., 2025); instead, it uniformly gathers diverse responses for each step throughout the entire trajectory. This global view provides distribution-aware data to support both generation and refinement. In this way, error patterns are implicitly encoded in trajectorylevel modeling, allowing robust reasoning without additional manual annotations of corrective steps. 3.2 Reformulating CoT Reasoning as Denoising In this section, our DIFFCOT framework reformulates CoT reasoning beyond the teacher-forcing paradigm as diffusion-styled denoising process to mitigate exposure bias at inference time. To couple this with the autoregressive nature of CoT, we introduce diffusion sliding-window mechanism that operates directly on the CoT reasoning process. Within this sliding window, previously generated CoT steps are progressively denoised from high-noise toward low-noise reasoning states, thereby facilitating self-correction, while advancing the window naturally aligns with the autoregressive generation of subsequent CoT steps. Thus, instead of costly training separate diffusion language model from scratch, we leverage diffusionstyled data to efficiently fine-tune pre-trained LLM Figure 2: DiffCoT Framework and Training Data Construction: (a) Step-level forward noising: MCTS-based data generation defines step-level noise by rewardranking multiple candidates, yielding states ranging from clean to corrupted. (b) Sliding-window denoising: diffusion sliding window refines previously generated CoT steps while producing the next step in an autoregressive manner. (c) Causal diffusion noise: step-dependent schedule assigns stronger noise to later steps to encode the causal order of the reasoning chain. π for CoT reasoning. Specifically, for the iterative denoising process, the model π takes the question prompt together with the preceding reasoning steps s1:k as input. To enable variable-length generation when applying diffusion to thought chains while integrating generation with self-correction, the model maintains diffusion sliding window of size and stride n. At denoising iteration t, the window containing km, . . . , sσ previously generated CoT steps {sσ } is updated to lower-noise version {sσ km, . . . , sσ }, where σ = σ(t) is defined as function of the step index and the denoising iteration t, while σ and σ denote the noise levels before and after refinement, respectively. Simultaneously, as the window shifts forward by one step, the model predicts the next step sσ k+1, initialized at high-noise state. Iterating this denoising process eventually yields clean trajectory sσ0 (cid:0)p, σ 1:k 1:K: (cid:1) (cid:55) σ (5) πθ . km:k (cid:124) (cid:123)(cid:122) (cid:125) refined past σ k+1 (cid:124)(cid:123)(cid:122)(cid:125) predicted future An illustration of the denoising process is shown in Fig. 2(b). In sequence modeling, teacher-forcing 4 next-token prediction can be viewed as masking along the reasoning-step axis s, whereas diffusion corresponds to masking along the noise axis σ, where σT approaches pure white noise after iterations. To unify both views, we denote sσ as the k-th CoT step under noise level σ, where σ is instantiated as σt k, i.e., the diffusion noise strength assigned to the k-th step at denoising iteration t. Building on the above reasoning process, we now introduce the training objective in DIFFCOT. Our goal is to optimize model πθ that takes the question prompt together with the preceding reasoning steps s1:k as input. k+ We construct the win sequence sw combining the denoised steps {sσ km:k+1 by km, . . . , sσ } σ with lower-noise variant of k+1 . Similarly, the lose sequence sl km:k+1 is formed by combining the unrefined steps {sσ km, . . . , sσ } with the highest-noise variant of sσk+1 k+1 . The prefix condition is the past text s1:k1. To optimize the LLM on this preference pair, we adopt the Direct Preference Optimization (DPO) loss (Rafailov et al., 2023): :k+1 p, s1:k) (cid:16) :k+1 p, s1:k) Li(πθ; πref) = log ϕ β log πθ(sw πref(sw :k+1 p, s1:k) :k+1 p, s1:k) (cid:17) , β log πθ(sl πref(sl (6) where s:k+1 denotes the subsequence skm:k+1, ϕ() denotes the sigmoid function, and β is factor controlling the strength of the preference signal. Note that our conditional prefix sσ 1:k1 differs from that used in standard DPO: instead of being composed exclusively of preferred (i.e., clean) steps, it combines clean steps from 1 to with noisy steps within the sliding window. In this manner, the hybrid construction could alleviate the exposure bias by training the model to make preference-consistent updates even when conditioned on partially corrupted reasoning prefixes. 3.3 Causal Diffusion Noise Modeling CoT reasoning with diffusion poses significant challenge to causality. Conventional full-sequence diffusion models are inherently noncausal (Ho et al., 2020; Dhariwal and Nichol, 2021), which contrasts sharply with the causal nature of CoT reasoning. While our backbone π retains an AR token-level generation process, prior studies on diffusion models indicate that relying solely on the models ability to capture causality is inadequate, 5 especially when reasoning must be performed over noisy or perturbed data (Li et al., 2022). Inspired by Diffusion Forcing (Chen et al., 2024a), as shown in Fig. 2(c), we leverage noise as mechanism to inject causal priors into diffusion sliding window.In conventional full-sequence diffusion, the noise strength σ(t) depends solely on the denoising iteration and is shared across all tokens. Such uniform noise injection is ill-suited for CoT reasoning, as it limits the models ability to capture step-wise causal dependencies. To overcome this, we redefine the noise schedule as σt k, joint function of reasoning step and iteration (see 3.2). Within the diffusion sliding window, σt follows progressive schedule, where earlier steps are perturbed with weaker noise, while later steps are perturbed with stronger noise. The noise schedule is formally defined as follows: (sjm+1, . . . , sj; j) = (cid:0)sσ0 jm+1, sσ1 jm+2, . . . , sσT (7) (cid:1), where the diffusion sliding window has size and stride 1, and at the j-th denoising iteration the window advances to generate sj. In this manner, our framework could better stabilize the causal chain and enhance self-correction in subsequent reasoning steps even if the current step is erroneous."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Settings Models and Datasets We conduct experiments on three representative backbone models: Llama38B (Touvron et al., 2023), Qwen3-8B (Yang et al., 2025), and Qwen3-4B (Yang et al., 2025). We primarily evaluate our DIFFCOT on three public mathematical reasoning benchmarks, specifically GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), and MATH (Hendrycks et al., 2021). The MATH is divided into five difficulty levels, as defined in the original dataset. More data statistics and preparation are provided in Appendix A. Baselines We compare DIFFCOT with the following SoTA baselines: 1) CoT (Wei et al., 2022): generates step-by-step reasoning before the final answer, evaluated with greedy decoding. 2) ToT (Yao et al., 2023): explores multiple reasoning paths via tree search. 3) TS-SFT (Feng et al., 2023): applies supervised fine-tuning on reasoning paths obtained from ToT. 4) CPO (Zhang et al., 2024): performs preference optimization at the step level, directly aligning intermediate reasoning steps via Model Llama3-8B Qwen3-8B Qwen3-4B CoT TSFT CPO ToT Step FStep DIFFCOT CoT TSFT CPO ToT Step FStep DIFFCOT CoT TSFT CPO ToT Step FStep DIFFCOT GSM8K 37.2 39.3 37.6 37.7 36.1 38.1 SVAMP 49.6 50.1 49.4 48.8 48.6 48.4 M-L1 31.3 33.4 32.2 31.5 33.8 32.0 M-L2 18.4 18.5 18.1 17.9 19.3 17.6 M-L3 12.5 13.2 12.6 12.9 12.6 13.7 M-L4 7.6 5.9 M-L5 3.1 1.9 6.4 2.9 6.6 1.4 6.7 3. 6.1 2.3 39.6 50.4 34.3 19.5 14.2 8.0 3.3 65.5 64.3 65.7 65.1 64.9 65.9 83.5 84.9 84.2 82.7 85.2 85.9 61.1 62.0 60.1 60.8 61.7 63.0 39.1 39.2 38.4 38.7 39.9 37.1 25.2 26.1 25.3 25.5 26.2 25.8 16.5 15.0 15.5 15.6 15.8 17.4 4.7 4.4 5.4 4.2 3. 5.2 66.2 85.5 63.6 39.3 26.9 17.5 5.5 62.0 64.0 61.8 63.0 63.3 64.7 80.1 80.7 82.3 80.3 79.4 83.0 51.2 52.4 51.8 51.6 51.8 52.7 31.9 34.0 32.6 33.1 35.7 34.2 16.0 17.0 16.2 16.4 17.2 17.9 9.9 9.0 1.4 1.0 10.2 1.2 8.5 0.7 8.3 0. 8.9 2.4 65.4 83.2 53.3 35.3 18.0 10.5 2.9 Table 1: Test accuracy (%) on GSM8K, SVAMP, and MATH. M-L1 to M-L5 denote different difficulty levels in the MATH dataset, where L1 is the easiest and L5 is the most challenging, while Step denotes Step-DPO and FStep denotes Full-Step-DPO. The best and second results are in bold and underlined. contrastive pairs. 5) Step-DPO (Lai et al., 2025): step-level preference optimization by explicitly collecting erroneous steps and applying preference optimization to correct them. 6) Full-Step-DPO (Xu et al., 2025): further generalizes step-wise optimization to the entire reasoning trajectory. More implementation details regarding model training and data construction are provided in Appendix B. 4.2 Main Results As shown in Table 1, DIFFCOT can be directly applied to modern instruction-tuned language models, such as Qwen3 and Llama3, via standard finetuning. This design enables seamless integration into existing training pipelines and allows DIFFCOT to deliver consistent performance improvements across models of varying sizes, demonstrating strong scalability across different base models. From the result, we can observe that: 1) Baselines guided by LLM self-verification signals, such as CPO and ToT, exhibit notable instability across models and datasets. For example, when applied to Qwen3-8B, CPO results in performance degradation on the MATH-1 dataset while achieving competitive performance on SVAMP, indicating sensitivity to both model and task distributions. 2) Step-level preference learning baselines, including Step-DPO and Full-Step-DPO, generally achieve great improvements but still suffer from occasional performance drops under certain settings. This suggests that relying solely on local step-wise optimization may not be sufficient to ensure stable reasoning behavior across diverse evaluation scenarios. 3) Overall, DIFFCOT consistently outperforms existing PO approaches across the majority of benchmarks and evaluation settings considered. Although it does not achieve the single best result in every individual setting, such as on SVAMP with the Qwen3-8B backbone, where Full-Step-DPO attains slightly higher accuracy, DIFFCOT consistently ranks among the top-performing methods across all settings. Moreover, the improvements achieved by DIFFCOT are substantial in magnitude and stable across different model sizes and datasets, highlighting its robustness and reliability. 4.3 Ablation Study To validate the effectiveness of the proposed DIFFCOT method, we perform an ablation study on its key components. We select two representative base language models from different model families with varied sizes, Llama3-8B and Qwen3-4B, and evaluate them on two general mathematical reasoning benchmarks, GSM8K and SVAMP. To investigate the impact of incorporating diffusion into CoT reasoning, we evaluate various window sizes to explore how this factor influences the models performance. Specifically, when the diffusion window size and stride are set to 1, our approach essentially degenerates into an AR method, albeit with differently constructed prefixes. On the other hand, when the sliding diffusion window and stride are set to the number of steps K, the method reverts to purely diffusion-based approach. As shown in Table 2, it can be observed that performance degrades when the window size and stride is set to 1 or when it becomes too large. We attribute this phenomenon to trade-off between causal connectivity and error-correction capability. Strengthening the ability to revise earlier steps often comes at the expense of weakening top-down causal reasoning. When the model operates entirely in AR mode, it exhibits the strongest causal reasoning ability but suffers from exposure bias during testing. In contrast, when the model fully adopts the diffusion mode, an excessively long window introduces noise: it disrupts the structural coherence of reasoning and prolonged denoising fluctuations undermine the causal nature of inference. We further conduct an ablation study to verify the effectiveness of the causal diffusion noise. Specifically, we disrupt our proposed noise schedul6 Model Llama3-8B - window size, stride=1 - window size, stride=K - causal noise Qwen3-4B - window size, stride=1 - window size, stride=K - causal noise GSM8K (%) SVAMP (%) 39.6 36.3 -3.3 30.3 -9.3 35.5 -4.1 65.4 63.0 -2.4 56.7 -8.7 61.9 -3.5 50.4 48.3 -2.1 42.6 -7.8 48.2 -2.2 83.2 80.2 -3.0 73.9-9.3 79.1 -4. Table 2: Ablative results on the general datasets GSM8K and SVAMP, where is the number of reasoning steps. Numbers shown in the upper-right corner of each cell indicate the relative change in accuracy rate compared to the full DIFFCOT model. ing by randomly shuffling the data order used for noising, instead of following the accuracy-based progression. This modification effectively breaks the causal structure of the noise. Results in Table 2 show that this ablation leads to substantial performance degradation across different models and datasets, demonstrating that our causal noise scheduling is critical component of DIFFCOT. 4.4 Analysis Case Study To further illustrate the models robustness to accumulated imperfections in intermediate reasoning, we present qualitative case studies that contrast effective and suboptimal reasoning trajectories. In particular, we focus on challenging examples in which the model introduces semantically irrelevant or weakly informative steps at an early stage of reasoning. Although such steps are not necessarily incorrect in isolation, they tend to accumulate and hinder progress toward the correct solution under standard step-by-step reasoning. As shown in Fig. 3, DIFFCOT is able to progressively refine earlier reasoning steps through the diffusion process. Instead of rigidly committing to the initial reasoning prefix, the model gradually improves previously generated steps while producing subsequent ones, effectively revising semantically unhelpful content introduced earlier in the trajectory. This allows the model to move away from locally coherent but globally suboptimal reasoning paths and ultimately reach the correct solution. It highlights key distinction between DIFFCOT and conventional autoregressive reasoning. While autoregressive models tend to propagate early semantic noise forward without revision, DIFFCOT leverages its denoising dynamics to refine the entire reasoning trajectory, including previously generated steps, resulting in more robust and globally consistent reasoning behavior. Figure 3: Example illustrating how DIFFCOT modifies early-stage reasoning shift steps. The steps highlighted in blue represent the diffusion sliding window. Error Accumulation Analysis We further analyze the models ability to recover from accumulated imperfections in intermediate reasoning steps. We consider correction-oriented setting in which the model is deliberately conditioned on prefixes that contain semantically suboptimal or noisy reasoning steps, and is then required to continue the reasoning process to reach correct final answer. To this end, we introduce controlled perturbations at an intermediate stage of the reasoning process. Specifically, when the reasoning reaches approximately half of the trajectory, each preceding step is independently perturbed with probability ω. perturbation replaces the original step with lowreward alternative sampled from model-generated trajectories that are semantically plausible but less aligned with the optimal reasoning path. The noise strength ω therefore governs the degree of accumulated semantic drift in the prefix, enabling systematic evaluation of robustness under varying levels of intermediate reasoning noise. We compare different training strategies under this protocol using the correction success rate, defined as the proportion of cases in which the model produces correct final answer despite being conditioned on perturbed prefix. Experiments are conducted on 300 GSM8K problems with multiple backbone models, comparing our approach against Full-Step-DPO. As shown in Fig. 4, our method consistently achieves substantially higher correction success rates across all settings, demonstrating stronger ability to compensate for accumulated in7 Figure 4: Correction success rate under stochastic prefix corruption, where noise is injected at the midpoint of the reasoning trajectory with probability ω. termediate noise and to maintain globally coherent reasoning, rather than brittle continuation of imperfect early steps. In other words, our model can mitigate the impact of early-stage reasoning drift, rather than rigidly propagating locally coherent but globally suboptimal trajectories in CoT reasoning."
        },
        {
            "title": "5 Related Work",
            "content": "Preference Learning Preference learning has recently emerged as an effective paradigm for aligning LLMs with human preferences by contrasting desirable and undesirable responses (Ouyang et al., 2022; Liu et al., 2025b), with methods such as DPO showing strong performance on general language tasks (Rafailov et al., 2023). However, extending these gains to mathematical reasoning remains challenging, largely due to the coarse granularity of solution-level supervision, which fails to localize and correct intermediate reasoning errors (Chen et al., 2024b). To address this limitation, recent work incorporates step-level preference signals to align intermediate reasoning processes (Lai et al., 2024; Lu et al., 2024), while Full-Step-DPO further optimizes entire reasoning trajectories using global objectives (Xu et al., 2025). Despite these advances, most existing step-wise methods adopt teacher-forcing paradigm and reason exclusively over clean prefixes during training, leading to error accumulation and degraded robustness during inference. Accordingly, our work reconceptualizes CoT reasoning as globally revisable trajectory, in which previously generated steps remain malleable to correction in light of future context. Mathematical Reasoning Mathematical reasoning is widely regarded as challenging capability for large language models, and prior work has explored several directions to improve it. One line of research strengthens base models through continual pre-training on large-scale mathematical corpora or supervised fine-tuning on synthetic datasets distilled from stronger models (Azerbayev et al., 2023; Zhihong et al., 2024; Xu et al., 2024; Mitra et al., 2024). Another line improves test-time performance by increasing computational budgets, such as generating and reranking multiple solutions using outcomeor process-level rewards, or adopting reward-guided decoding strategies (Guan et al., 2025; Wu et al., 2024; Wang et al., 2024). Recently, reinforcement learning and preferencebased optimization have been explored to directly align reasoning behaviors via trajectoryor steplevel supervision, aiming to improve robustness beyond supervised objectives alone (Pal et al., 2024; Wang et al., 2024). But they largely operate in forward-only manner and lack mechanisms for revising corrupted intermediate reasoning, motivating our diffusion-styled reformulation of CoT reasoning to enable effective error correction."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this work, we proposed novel DIFFCOT framework to enhance mathematical reasoning and alleviate error accumulation by integrating diffusion steps into autoregressive generation. Our method utilizes sliding diffusion window with causal noise to refine intermediate steps while maintaining consistent reasoning chains. Extensive experiments confirm that our proposed DIFFCOT outperforms existing CoT methods, offering robust solution for multi-step CoT reasoning. Future work will explore the scalability of our paradigm across more backbone models and broader reasoning domains."
        },
        {
            "title": "Limitations",
            "content": "Although we have conducted extensive experiments on DIFFCOT and observed strong empirical performance, several limitations remain. First, the data construction and training of DIFFCOT follow an off-policy paradigm, where preference data are collected using policy that differs from the one being optimized. Such mismatch between the behavior policy and the training policy may introduce distribution shift, biased value estimation, and training instability, especially when scaling to more difficult datasets or longer reasoning chains. Second, similar to Diffusion Forcing, DIFFCOT breaks the local Markov property of prefix conditioned generation by revisiting and modifying historical reasoning steps. While this violation enables stronger capabilities, it also increases the uncertainty and controllability challenges during generation, typically requiring more training iterations and larger amounts of data to achieve stable convergence."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.06786. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are fewshot learners. In Advances in Neural Information Processing Systems (NeurIPS). Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. 2012. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143. 2024a. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024b. Step-level value preference optimization for mathematical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 78897903, Miami, Florida, USA. Association for Computational Linguistics. Ruizhe Chen, Wenhao Chai, Zhifei Yang, Xiaotian Zhang, Ziyang Wang, Tony Quek, Joey Tianyi Zhou, Soujanya Poria, and Zuozhu Liu. 2025. DiffPO: Diffusion-styled preference optimization for inference time alignment of large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1891018925, Vienna, Austria. Association for Computational Linguistics. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Prafulla Dhariwal and Alexander Quinn Nichol. 2021. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems (NeurIPS). Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rStar-math: Small LLMs can master math reasoning with self-evolved deep thinking. In Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pages 2064020661. PMLR. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS). Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Takeshi Kojima, Shixiang Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Preprint, arXiv:2205.11916. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. 2024. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. 2025. Step-DPO: Stepwise preference optimization for long-chain reasoning of LLMs. Xiang Lisa Li, Tianyu Gong, Tian He, Dan Jurafsky, Percy Liang, and Steven CH Zhao. 2022. DiffusionIn Adlm improves controllable text generation. vances in Neural Information Processing Systems (NeurIPS). Yanhong Li, Chenghao Yang, and Allyson Ettinger. 2024. When hindsight is not 20/20: Testing limits on reflective thinking in large language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 37413753. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and 1 others. 2025a. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, and 1 others. 2025b. Lipo: Listwise preference optimization through learning-to-rank. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 24042420. Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. 2024. Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning. CoRR. Qing Lyu, Tongshuang Wu, Fei Sha, and Graham Neubig. 2023. Faithful chain-of-thought reasoning. Preprint, arXiv:2301.13379. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. 2024. Orca-math: Unlocking the potential of slms in grade school math. Preprint, arXiv:2402.14830. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. 2024. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve arXiv preprint 2021. simple math word problems? arXiv:2103.07191. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and 1 others. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems. Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. 2011. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627 635. JMLR Workshop and Conference Proceedings. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and 1 others. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations (ICLR). Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, and Meng Jiang. 2024. Enhancing mathematical reasoning in llms by stepwise correction. Preprint, arXiv:2410.12934. Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, and 1 others. 2024. Enhancing llm reasoning via critique models with testtime and training-time supervision. arXiv preprint arXiv:2411.16579. Huimin Xu, Xin Mao, Feng-Lin Li, Xiaobao Wu, Wang Chen, Wei Zhang, and Anh Tuan Luu. 2025. Fullstep-DPO: Self-supervised preference optimization with step-wise rewards for mathematical reasoning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2434324356, Vienna, Austria. Association for Computational Linguistics. Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Zhao Wenyi, Jie Tang, and Yuxiao Dong. 2024. ChatGLM-math: Improving math problem-solving in large language models with self-critique pipeline. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 97339760, Miami, Florida, USA. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, and Sungjin Ahn. 2025. Monte carlo tree diffusion for system 2 planning. In Forty-second International Conference on Machine Learning. Xuan Zhang, Chao Dut, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. 2024. Chain of preference optimization: improving chain-of-thought reasoning in llms. In Proceedings of the 38th International Conference on Neural Information Processing Systems, pages 333356. Shao Zhihong, Wang Peiyi, Zhu Qihao, Xu Runxin, Song Junxiao, Zhang Mingchuan, Y. K. Li, Y. Wu, and Guo Daya. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models."
        },
        {
            "title": "A Data Statistics",
            "content": "reasoning benchmarks, We primarily evaluate our DIFFCOT on mathematical specifically SVAMP (Patel et al., 2021), MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021). The MATH is divided into five difficulty levels, as defined in the original dataset. To reduce randomness and better showcase the experimental results, we sample test instances separately from each of five levels. To maintain reasonable computational budget, particularly given the high cost of Tree-ofThought (Yao et al., 2023) search, we restrict each dataset to at most 300 test samples through random sampling. For training, we similarly sample 300 instances from each dataset to construct the training set. For fair comparison, all baseline methods use the same dataset configuration. We provide the detailed data statistics in Table 3. Dataset #Step #Train #Test #Cand. #Roll. GSM8K SVAMP MATH-L1 MATH-L2 MATH-L3 MATH-L4 MATH-L5 5 3 3 4 4 5 5 500 500 300 300 300 300 300 300 300 300 300 300 300 300 5 5 5 5 5 5 8 8 8 8 8 8 8 Table 3: Dataset-specific configurations for reasoning experiments. #Step denotes the maximum number of reasoning steps; #Train and #Test denote the numbers of training and test samples, respectively; #Cand. denotes the number of candidate thoughts sampled at each step; #Roll. denotes the number of Monte-Carlo rollouts used for evaluating each candidate. MATHL1L5 correspond to increasing difficulty levels in the MATH dataset."
        },
        {
            "title": "B Implementation Details",
            "content": "Data Generation via Rollout-Based Thought Search We generate candidate contexts by conducting an iterative thought search starting from each original problem. Specifically, at reasoning step t, we maintain prefix that contains all previously selected thoughts. Conditioned on the current prefix, we sample four candidate thoughts from the base model. In parallel, we query DeepSeek-V3.2 (Liu et al., 2025a) to produce an additional reference golden candidate thought under the same prefix. This golden candidate is used only during data collection and is not available at inference time. Figure 5 provides representative example illustrating the con11 comes to compute success-based step scores, which serve as substitutes for PRM outputs in the global loss. Moreover, for CPO and ToT baselines, we follow the implementation described in the CPO paper, where preference pairs are constructed via LLM self-evaluation of intermediate reasoning steps. This design choice highlights the fundamental distinction between CPO-style self-judgmentbased supervision and Step-DPO-style supervision derived from execution or outcome feedback."
        },
        {
            "title": "Learning",
            "content": "PO methods generally follow paradigm: trajectory generation followed by trajectory-level reweighting. The model first expands full reasoning path via longitudinal generation. Alignment is then applied horizontally across completed trajectories using either reinforcement learning or preference optimization, discouraging low-reward reasoning paths and favoring high-reward ones. While this paradigm of vertical generation followed by horizontal RL-based refinement has achieved broad empirical success, it inevitably suffers from error accumulation, as early mistakes in the generated trajectory can propagate and constrain subsequent optimization. Chen et al. (2025) theoretically showed that the denoising process of diffusion models, which gradually transforms low-quality samples into highquality ones, is equivalent to preference optimization. Building on this insight, DIFFCOT introduces diffusion sliding window, tightly coupling longitudinal reasoning generation with horizontal preference optimization within unified framework. From this perspective, we believe that future extensions of our method can naturally integrate reinforcement learning, leveraging RL to directly optimize the denoising dynamics of the diffusion sliding window."
        },
        {
            "title": "D Generative AI Usage",
            "content": "AI assistants were used in limited and supportive manner during the preparation of this manuscript, primarily for language polishing, formatting suggestions, and improving clarity of presentation. structed candidate thoughts and their corresponding step-wise evaluations under this procedure. To evaluate each candidate thought, we estimate its utility via Monte-Carlo rollouts. Concretely, we perform = 8 independent rollouts conditioned on the concatenation of the prefix and the candidate thought. Each rollout is decoded until termination to obtain final answer. We then compute the empirical success rate as the fraction of rollouts that yield the correct answer. We continue the search by greedily selecting the candidate with the highest success rate, appending it to the prefix, and repeating the above procedure until the reasoning process terminates. We provide detailed example with step-wise reasoning annotations in Figure 5. Model Training For efficient fine-tuning, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2022) with rank of 8 and α = 16, where LoRA adapters are inserted into the q_proj and v_proj linear projections of every self-attention layer. Model training is conducted using the DPO (Rafailov et al., 2023) loss with regularization coefficient of β = 0.4, optimized by AdamW (Loshchilov and Hutter, 2017) with cosine learning rate schedule. We train for 3 epochs with learning rate of 1e5, warm-up ratio of 0.05, and global batch size of 32. During decoding, the temperature is fixed at 0.4. Compared results (p < 0.05 under t-test) are averaged over three random 3 runs. All experiments are conducted on NVIDIA A100 GPUs with 80GB memory. Training and inference on 500 GSM8K samples require approximately 11 GPU-hours in total. Baseline Implementation The backbone models are implemented by adopting the Meta-Llama3-8B1, Qwen3-8B2, and Qwen3-4B-Instruct25073 versions. For Full-Step-DPO, we do not retrain the Process Reward Model (PRM) introduced in Full-StepDPO. In our implementation, we retain the proposed global step-wise loss formulation, but replace the PRM-based step rewards with empirical success rates estimated from Monte-Carlo rollouts. Specifically, we collect extra complete reasoning trajectories from rollouts and use their final out1https://huggingface.co/meta-llama/ Meta-Llama-3-8B 2https://huggingface.co/Qwen/Qwen3-8B 3https://huggingface.co/Qwen/ Qwen3-4B-Instruct-2507 Question. In truck, there are 26 pink hard hats, 15 green hard hats, and 24 yellow hard hats. If Carl takes away 4 pink hard hats, and John takes away 6 pink hard hats and twice as many green hard hats as the number of pink hard hats that he removed, calculate the total number of hard hats that remained in the truck. Prompt. You are an expert math problem solver. You must reason step by step and avoid logical or arithmetic mistakes. Solve the following math problem. You MUST use exactly 5 reasoning steps. After the reasoning, output the final answer in the last line using the format: Answer: <final_answer>. Step 1. [0] 26 pink hard hats 4 pink hard hats = 22 pink hard hats [1] 26 4 = ? (number of pink hats after Carls removal) [2] Initial number of pink hard hats is 26. success_rate=0.0 success_rate=0.125 success_rate=0.5 [3] We have 26 pink hard hats initially. Carl takes away 4 pink hard hats, leaving us with 26 4 = 22 pink hard hats. success_rate=0.75 [4] Calculate the initial total number of hard hats. Initial: pink = 26, green = 15, yellow = 24. Total initial = 26 + 15 + referenced data 24 = 65. Step 2. [0] John removed 6 pink hats and 6 green hats, leaving 15 6 = 9 green hats. success_rate=0.0 [1] John removes 6 pink hats, so he removes 6 green hats as well. Thus 15 6 = 9 green remain. success_rate=0. [2] 6 pink hats are removed by John, leaving 22 6 = 16 pink hats. success_rate=0.625 [3] John removes 6 pink hats: 22 6 = 16 pink left. He removes twice as many green hats: 2 6 = 12,so success_rate=1.0 15 12 = 3 green left. [4] Carl removes 4 pink hard hats. Pink after Carl 26 4 = 22. referenced data Step 3. [0] Yellow hard hats remain unchanged, so we still have 24 yellow hard hats. [1] Yellow hard hats remain unchanged, so we still have 24 yellow hard hats. success_rate=0.75 success_rate=0.875 [2] 24 yellow hard hats remain in the truck and are not affected by the removals. success_rate=0.875 [3] 24 yellow hard hats remain unchanged. [4] John removes 6 pink hard hats. Pink after John 22 6 = 16. success_rate=1.0 referenced data Step 4. [0] 16 pink, 3 green, and 24 yellow hard hats remain in the truck. [1] 16 pink and 3 green hard hats remain; 16 + 3 = 19. [2] Total remaining hats: 16 + 3 + 24 = 43. success_rate=0.75 success_rate=0.875 success_rate=0. [3] 16 pink and 3 green hard hats remain, so 16 + 3 = 19 hats across these two colors. success_rate=1.0 [4] John removes twice as many green hard hats as the pink hats he removed (he removed 6 pink hats, so twice that is referenced data 12 green hats). Green initially = 15.Green after John 15 12 = 3. Step 5. [0] Total: 3 + 19 + 24 =? [1] Total remaining hats: 19 + 24 = 43. [2] Combining 19 (pink+green) with 24 yellow gives 19 + 24 = 43. [3] Total remaining hats: 19 + 24 = 43. success_rate=0.0 success_rate=0.875 success_rate=1. success_rate=1.0 [4] Yellow hats remain unchanged at 24. Total remaining = pink (16)+green (3)+yellow (24) = 43. . Figure 5: Representative dataset example with step-wise reasoning annotations."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "Hong Kong Baptist University"
    ]
}