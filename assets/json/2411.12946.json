{
    "paper_title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
    "authors": [
        "Gabriel Chua",
        "Shing Yee Chan",
        "Shaun Khoo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety."
        },
        {
            "title": "Start",
            "content": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection Gabriel Chua gabriel_chua@tech.gov.sg Government Technology Agency Singapore Chan Shing Yee shingyee54@hotmail.com National University of Singapore Singapore Shaun Khoo shaun_khoo@tech.gov.sg Government Technology Agency Singapore 4 2 0 2 0 ] . [ 1 6 4 9 2 1 . 1 1 4 2 : r ABSTRACT Large Language Models (LLMs) are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that isnt available in pre-production. In this paper, we introduce flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by opensourcing both the synthetic dataset1 and the off-topic guardrail models2, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety. ACM Reference Format: Gabriel Chua, Chan Shing Yee, and Shaun Khoo. 2024. Flexible Large Language Models Guardrail Development Methodology Applied to OffTopic Prompt Detection. In Proceedings of XXXX . ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 INTRODUCTION\nLarge Language Models (LLMs) such as GPT-4o [9], Gemini 1.5\n[8], and Llama 3 [16] have revolutionised various sectors by en-\nabling advanced natural language processing capabilities. Their\napplications extend beyond conversational agents to include tasks\nlike documentation extraction, report generation, and workflow\nautomation [4]. As these models become increasingly integrated",
            "content": "Corresponding Author Work was done during an internship at the Government Technology Agency 1https://huggingface.co/datasets/gabrielchua/off-topic 2https://huggingface.co/collections/govtech/off-topic-guardrail673838a62e4c661f248e81a4 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. XXXX, XXXX XXXX, XXXX, XXXX 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn into software applications and real-world processes, ensuring their appropriate use becomes critically important. To mitigate potential risks associated with LLM deployments, significant efforts have been made in developing safety measures like model alignment [6] and guardrails [7]. Alignment techniques aim to ensure that LLMs behave in accordance with human values and intentions, while guardrails are mechanisms that prevent models from generating undesired or harmful outputs. These safety measures are essential for maintaining user trust and meeting regulatory compliance, especially in sensitive domains such as healthcare, finance, and legal services. One significant challenge is preventing LLMs from responding to prompts that fall outside their intended scope, typically defined by the system prompts set by developers. While prompted to perform specific tasks, LLMs are susceptible to producing responses to irrelevant or unintended prompts. For instance, one could get healthcare policy chat bot to generate Python code with minimal prompting. We refer to such prompts as \"off-topic\" (see Figure 1) which are distinct from \"jailbreak\" prompts [27] which aim to elicit harmful or explicitly disallowed content. Off-topic prompts may lead to benign outputs, but can still undermine the models intended functionality and pose compliance risks. For example, providing legal or medical advice that is beyond the intended scope. Current guardrail mechanisms often rely on curated examples of positive/negative prompts [1] or custom classifiers [2] for each use-case and trained on existing datasets to filter out inappropriate inputs. However, real-world user data is not yet available in pre-production, and there is typically lack of sufficient data or examples at the early stages to cover the vast array of potential offtopic prompts. Other guardrail implementations require defining blacklist of topics [25] which can be challenging in pre-production as there are potentially infinite number of edge cases to consider. This situation highlights three major challenges: first, the need for general-purpose model that can effectively detect off-topic prompts, second, developing such general-purpose model without extensive pre-existing datasets, and third, such real-world data is absent in the pre-production phase. In this paper, we consider these challenges and introduce flexible, data-free guardrail development methodology and apply it to the challenge of off-topic prompt detection for LLMs. By thoroughly defining the problem space qualitatively and leveraging an LLM to generate diverse set of prompts, we construct synthetic dataset that serves both as benchmark and training resource for off-topic guardrails. Fine-tuning embedding or cross-encoder models on this synthetic data outperforms heuristic approaches by reducing false positives, and enhancing potential adoption. Additionally, we find XXXX, XXXX XXXX, XXXX, XXXX Gabriel Chua, Chan Shing Yee, and Shaun Khoo Figure 1: Example of onand off-topic user prompts: The goal is to correctly classify if prompt is off-topic or not, with respect to the system prompt that by framing this as classifying whether the user prompt is relevant to the system prompt, our guardrail generalises effectively to other misuse categories, including jailbreak and harmful prompts. To summarize, our contributions are fourfold: (1) Flexible Guardrail Development Methodology: We propose scalable approach to generate synthetic datasets in pre-production to develop guardrails, thus providing strong baseline for the LLM applications initial deployment. (2) Performant Off-Topic Guardrails: We develop simple yet effective classifier guardrails by fine-tuning embedding and cross-encoder models on synthetic data. These models outperform existing heuristic methods, significantly reducing false positives and improving detection accuracy for off-topic prompts. (3) Generalization to Multiple Misuse Categories: By framing the detection task in terms of system prompt relevance, our methodology effectively extends to other misuse types. (4) Open-Source Resources: We contribute to the community by open-sourcing both the synthetic dataset and the off-topic guardrail models, facilitating future research and development in LLM safety and compliance."
        },
        {
            "title": "2.2 Guardrails\nGuardrails are another mechanism designed to prevent LLMs from\ngenerating undesired or harmful outputs [3, 14, 19]. Where align-\nment is about model-level interventions, we define guardrails here\nas separate filters that scan the inputs and outputs of the LLM. Such\nguardrails can be updated or replaced independently of the main\nmodel, offering flexibility and adaptability to new types of misuse.\nExternal filters may use rule-based systems, machine learning clas-\nsifiers, or a combination of both to detect and mitigate inappropriate\ncontent.",
            "content": "Several frameworks and services have been developed to facilitate the implementation of guardrails, including NVIDIA NeMo Guardrails [25], AWS Bedrock Guardrails [1], Azure AI Content Safety [2]."
        },
        {
            "title": "3.1 Guardrail Development Framework\nDeveloping effective guardrails for LLMs is crucial to ensure they\noperate within their intended scope and to prevent misuse. Some\nguardrail approaches depend on curated datasets of misuse exam-\nples, which are impractical due to the infinite variety of potential\nmisuse scenarios, and due to the lack of data in pre-production. To",
            "content": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection XXXX, XXXX XXXX, XXXX, XXXX Figure 2: Our Guardrail Development Methdology address this challenge, we propose flexible, data-free guardrail development framework (see Figure 2) comprising the following steps: (1) Qualitative Problem Analysis and Edge Case Identification: We begin by thoroughly analysing the specific misuse or safety issue that needs to be addressed. This involves understanding the models intended functionality and identifying potential misuse or undesirable behaviors. By exploring potential misuse cases and edge scenarios, we can qualitatively define the boundaries of acceptable and unacceptable inputs and outputs. (2) Synthetic Data Generation via LLM Prompting: Having described the problem space and the attributes of desired and undesired inputs/outputs, we leverage an LLM to generate synthetic data. We provide detailed prompts to the LLM that outline the types of acceptable and unacceptable interactions. To ensure diversity in the synthetic dataset, we can increase the models generation temperature and provide random seed words. To ensure the model outputs fit within the defined schema (e.g., for every scenario, generate five positive and five negative examples), we can employ structured outputs or constrained generation, which is offered by several LLM API providers. Additionally, including few-shot examples can enhance realism in the outputs. (3) Model Training: With the synthetic dataset prepared, we proceed to train classifier to detect undesirable inputs. As the problem is now effectively text classification problem, transformer-based models are well-suited for this task. The benefits of this framework include: Pre-Deployment Readiness: We can now build classifier before deploying the application, ensuring high baseline of safety measures are in place from the outset. Once the application has been deployed, real-world prompts can be collected to augment the synthetic dataset and further train and refine the model. Probability Scoring: The classifier model can provide probability scores for predictions, allowing for threshold-based decisions and nuanced handling of inputs. For example, developers may choose not to block text outright but instead modify it or provide warning labels if the probability scores fall within medium range."
        },
        {
            "title": "3.2 Off-Topic Detection Problem\nWe apply this framework to the specific case of detecting off-topic\nprompts in LLM interactions.",
            "content": "3.2.1 Problem Formulation. Given system prompt (𝑆) and user prompt (𝑈 ), we want to detect if the prompt is off-topic (𝑌 = 1) or on-topic (𝑌 = 0). This is thus binary classification problem, where we seek to find the optimal function 𝐹 (.) that maximizes the classification metric of interest (e.g., PR-AUC, F1). 𝐹 (𝑆, 𝑈 ) 0, 1 Here, we define any prompt that is not relevant to the system prompt as being off-topic. With this definition, the system prompt should typically define well-defined, closed-ended task. 3.2.2 Data Generation. To generate the synthetic dataset, we used an LLM to create diverse examples of system prompts and corresponding on-topic and off-topic within the same LLM generation. We used GPT 4o 2024-08-06 and its structured outputs feature [22] to generate over 2M system and user prompt pairs. We ensured diversity by varying the length of the prompts, providing random seed words, and randomising the real-life examples of system prompts. This synthetic dataset serves both as training set for our classifier and as benchmark for evaluating different guardrail approaches. This dataset, along with the generation prompt, has been open-sourced. One highlight of this dataset is its aim to reflect real-world enterprise applications of LLMs (e.g., report generation, Q&A, and document extraction) as much as possible. Some benchmarks test guardrails ability to prevent leakage defined in the system prompts [28]. We find such examples to not fully reflect the enterprise applications of LLMs. 3.2.3 Modelling. We experimented with two modeling approaches for the off-topic detection problem (see Figure 3). For the purposes of our experiments, we did not conduct hyperparameter optimisation. These models have been open-sourced. 1. Fine-Tuned bi-encoder classifier. We started with pretrained embedding model that is lightweight and supports long context length. Specifically, we used the jina-embeddings-v2-smallen model [13]3, which has 33 million parameters and supports 3https://huggingface.co/jinaai/jina-embeddings-v2-small-en XXXX, XXXX XXXX, XXXX, XXXX Gabriel Chua, Chan Shing Yee, and Shaun Khoo Figure 3: Summary of the two modelling approaches for the off-topic prompt detection sequence length of up to 8192 tokens. For our experiment, we limited the sequence length to 1024 tokens. The architecture involves feeding the system prompt and user prompt separately into the embedding model and training an adapter layer for each input. We introduced cross-attention layers so that the system prompt can attend to the user prompt, and vice versa. After obtaining the representations, we applied attention pooling to derive single vector representations for both prompts. These vectors were concatenated and passed through classification head to make the final on-topic or off-topic prediction. 2. Fine-Tuned Cross-Encoder Classifier. We also fine-tuned pre-trained cross-encoder models, specifically cross-encoder/stsbroberta-base 4. In this approach, the system prompt and user prompt are directly concatenated into single sequence and fed into the cross-encoder model. The output is then also passed through classification head to make the binary classification."
        },
        {
            "title": "4.1 Baselines\nTo benchmark our fine-tuned models, we compare them against\nthe following baseline approaches:",
            "content": "(1) Cosine Similarity: Here we take the cosine similarity between the embeddings of the system and user prompts using pre-trained bge-large-en-v1.5 [31]5 embedding model. (2) K-Nearest Neighbors (KNN): We trained simple KNN classifier using the embeddings of 3 on-topic and 3 off-topic prompts (i.e., 6-shot learning). (3) Pre-trained Cross-Encoder Model: We used pre-trained cross-encoder models (stsb-roberta-base) without fine-tuning for relevance estimation. (4) ColBERT Model: We used the ColBERT v2 model [26]6 for relevance estimation. (5) LLM Prompt Engineering: We included suffix to the system prompt to tell the model to simply ignore irrelevant prompts. (6) LLM Zero-Shot Classification: We use smaller LLM to zero-shot classify if the user prompt is related to the system prompt."
        },
        {
            "title": "4.2 Performance on Synthetic Data\nWe evaluate our fine-tuned classifier models on a hold-out portion\nof the synthetic dataset (see Table 1). Among the baseline options,\nthe zero-shot LLM classifier is most performant. However, our\nfine-tuned models surpass it in terms of precision (i.e., fewer false\npositives), which is particularly important for guardrails, as we\nwant to avoid wrongly blocking legitimate prompts and adversely\naffecting the user experience.",
            "content": "We also consider how classification performance varies for prompt length (see Figure 4). Generally, the classifiers perform well for range of prompt lengths, though it is slightly weaker for shorter prompts, which is expected as there is less semantic information contained in shorter prompts in the first place. We also assess the calibration of our models (see Figure 5), as providing probability score is important to enable risk-based approach. We find that the fine-tuned cross-encoder is well-calibrated, especially for cases where the model is very confident that the user prompt is on-topic (i.e., predicted probability is below 0.2). Additionally, we are cognisant that the training and evaluation data were generated by the same LLM. Hence, we also evaluated 4https://huggingface.co/cross-encoder/stsb-roberta-base 5https://huggingface.co/BAAI/bge-large-en-v1.5 6https://huggingface.co/colbert-ir/colbertv2.0 Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection XXXX, XXXX XXXX, XXXX, XXXX Table 1: Performance on Synthetic Dataset Generated by GPT 4o (2024-08-06) (N=17,201) Approach Model ROC-AUC F1 Precision Recall Fine-tuned cross-encoder classifier Fine-tuned bi-encoder classifier Cosine similarity KNN Pre-trained cross-encoder Pre-trained colbert Prompt engineering Prompt engineering Zero-shot classifier stsb-roberta-base jina-embeddings-v2-small-en bge-large-en-v1.5 bge-large-en-v1 stsb-roberta-base ColBERT v2 GPT 4o (2024-08-06) GPT 4o Mini (2024-07-18) GPT 4o Mini (2024-07-18) 0.99 0.99 0.89 0.90 0.73 0.78 - - 0.99 0.99 0.97 0.59 0.75 0.68 0.72 0.95 0.91 0.97 0.99 0.99 0.97 0.94 0.53 0.72 0.94 0.85 0.95 0.99 0.95 0.42 0.63 0.93 0.73 0.97 0.91 0. Figure 4: ROC-AUC Score by User and System Prompt Length proprietary and open-weights LLMs (see Table 5 and 6 in the Annex). We find that the performance remains consistent."
        },
        {
            "title": "4.4 Inference Speed Benchmarking\nLastly, as these models are intended to be fast lightweight guardrails,\nwe also consider the inference speed. We measure it here by the",
            "content": "Figure 5: Calibration Plot our fine-tuned classifiers on synthetic data generated by other XXXX, XXXX XXXX, XXXX, XXXX Gabriel Chua, Chan Shing Yee, and Shaun Khoo Table 2: Binary Classification Performance on JailbreakBench Approach Model ROC-AUC F1 Precision Recall Fine-tuned cross-encoder classifier Fine-tuned bi-encoder classifier stsb-roberta-base jina-embeddings-v2-small-en 0.80 0. 0.72 0.83 0.76 0.84 0.68 0.82 Table 3: Recall for HarmBench, TrustLLM, and internal dataset on localised harmful prompts Benchmark Approach Model Recall HarmBench TrustLLM Fine-tuned cross-encoder classifier Fine-tuned bi-encoder classifier stsb-roberta-base jina-embeddings-v2-small-en Fine-tuned cross-encoder classifier Fine-tuned bi-encoder classifier stsb-roberta-base jina-embeddings-v2-small-en Localised harmful prompts Fine-tuned cross-encoder classifier Fine-tuned bi-encoder classifier stsb-roberta-base jina-embeddings-v2-small-en 0.83 0. 0.78 0.97 0.74 0.86 number of system and user prompt pairs they can process in one minute. These measurements were performed on an NVIDIA Tesla T4 GPU. Table 4 presents the results. Both the jina-embeddings-v2-small-en and stsb-roberta-base models demonstrate practical inference speeds suitable for realtime applications, processing thousands of pairs per minute. The choice between models involves trade-off between performance (as shown in Table 1) and latency. Future work could explore optimising these models for even faster inference on different hardware platforms."
        },
        {
            "title": "5.1 Limitations\nOur approach has limitations:",
            "content": "(1) Synthetic Data Bias: While the use of synthetic data is necessary in pre-production due to the absence of real-world data, the models are trained on synthetic data generated by LLMs, which may introduce biases present in the LLMs themselves. This could affect the models performance on real-world data. (2) Scope of System Prompts: The effectiveness of the guardrail depends on the specificity of the system prompt. For openended or broad system prompts, determining relevance becomes more challenging. (3) Language and Cultural Contexts: The models may not perform as well on prompts in languages other than English or in different cultural contexts, as the synthetic data was primarily generated in English."
        },
        {
            "title": "5.2 Deployment Considerations\nThis guardrail development methodology has been used within\nthe Government Technology Agency of Singapore (GovTech) in\nthe last year to develop an internal suite of guardrails to support\nvarious LLM applications, especially those where pre-deployment\ndata is unavailable. For the off-topic guardrails, specifically, they\nhave been deployed internally since September 2024. Besides off-\ntopic prompt detection, this general methodology has also been\nrecently adopted to develop output guardrails that detect system\nprompt leakage, classifying instances where the LLM-generated\ntext contains substantive information about the system prompt.",
            "content": "In general, our key considerations for deploying this specific off-topic guardrail are as follows: (1) Context Length: System prompts, which define an LLMs scope and behavior, are often extensive and detailed. The jina-embeddings-v2-small-en model, while slightly less performant than stsb-roberta-base (see Table 1), supports significantly longer context length, making it ideal for applications with complex or lengthy prompts. Conversely, stsb-roberta-base delivers higher accuracy in off-topic detection but has more constrained context window. Providing both models allows teams to balance accuracy and context length based on specific application requirements. (2) Open-Source Ecosystem: The bi-encoder approach benefits from robust open-source ecosystem of pre-trained embedding models, enhancing flexibility and accessibility. We can adopt the fine-tuning pipeline to newer and more performant embedding models. (3) Actionability: Both models deliver well-calibrated probability scores and users of the model can set thresholds tailored to specific priorities, such as favouring precision (minimising false positives) or recall (correctly identifying more off-topic prompts). (4) Active Learning: Recognizing that real-world data isnt available in pre-production, our methodology leverages synthetic data to develop initial guardrails. This approach ensures that safety measures are in place prior to deployment. Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection XXXX, XXXX XXXX, XXXX, XXXX Table 4: Inference Speed Benchmarking Approach Model Processed Pairs Per Minute Latency Per Pair (s) Fine-tuned bi-encoder classifier Fine-tuned cross-encoder classifier jina-embeddings-v2-small-en stsb-roberta-base 2,216 1,919 0.027 0.031 Post-deployment, the methodology can incorporate active learning as done in [19]. Real-world prompts are collected to augment the training dataset, ensuring continuous improvement and adaptation to evolving use cases. Future work includes incorporating active learning into our continuous training and deployment pipeline."
        },
        {
            "title": "6 CONCLUSION\nWe introduced a flexible, data-free guardrail development method-\nology and apply it to detecting off-topic prompts in LLMs. By lever-\naging LLMs to generate synthetic data and framing the detection\ntask as assessing prompt relevance, we developed classifiers that\noutperform existing heuristic methods. Our approach generalizes\nto other misuse categories. Lastly, we also contribute to the commu-\nnity by open-sourcing both the synthetic dataset and the off-topic\nguardrail model, facilitating future research and development in\nLLM safety and compliance.",
            "content": "Overall, our general methodology is especially valuable during the pre-production phase of LLM deployment when real-world data is not yet available. By enabling the development of robust guardrails before deployment, we ensure that LLM applications are safer and more reliable from the outset. REFERENCES [1] AWS. [n. d.]. Block denied topics to help remove harmful content - Amazon Bedrock docs.aws.amazon.com. https://docs.aws.amazon.com/bedrock/latest/ userguide/guardrails-denied-topics.html. [Accessed 12-11-2024]. [2] Azure. [n. d.]. learn.microsoft.com. content-safety/concepts/custom-categories?tabs=standard. 2024]. Custom categories in Azure AI Content Safety https://learn.microsoft.com/en-us/azure/ai-services/ [Accessed 12-11- [3] Helena Bonaldi, Greta Damo, Nicolás Benjamín Ocampo, Elena Cabrio, Serena Villata, and Marco Guerini. 2024. Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 34463463. https://aclanthology. org/2024.emnlp-main.201 [4] Michelle Brachman, Amina El-Ashry, Casey Dugan, and Werner Geyer. 2024. How Knowledge Workers Use and Want to Use LLMs in an Enterprise Context. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI EA 24). Association for Computing Machinery, New York, NY, USA, Article 189, 8 pages. https://doi.org/10.1145/3613905.3650841 [5] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, and Eric Wong. 2024. JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models. arXiv:2404.01318 [cs.CR] https://arxiv.org/abs/2404.01318 [6] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep Reinforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/ d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf [7] Yi Dong, Ronghui Mu, Yanghao Zhang, Siqi Sun, Tianle Zhang, Changshun Wu, Gaojie Jin, Yi Qi, Jinwei Hu, Jie Meng, Saddek Bensalem, and Xiaowei Huang. 2024. Safeguarding Large Language Models: Survey. arXiv:2406.02622 [cs.CR] https://arxiv.org/abs/2406.02622 [8] Gemini Team et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530 [cs.CL] https://arxiv.org/abs/ 2403.05530 [9] OpenAI et al. 2024. GPT-4o System Card. arXiv:2410.21276 [cs.CL] https: //arxiv.org/abs/2410.21276 [10] Yue Huang et al. 2024. TrustLLM: Trustworthiness in Large Language Models. arXiv:2401.05561 [cs.CL] https://arxiv.org/abs/2401.05561 [11] Jessica Foo and Shaun Khoo. 2024. LionGuard: Building Contextualized Moderation Classifier to Tackle Localized Unsafe Content. arXiv:2407.10995 [cs.CL] https://arxiv.org/abs/2407.10995 [12] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. arXiv:2209.07858 [cs.CL] https://arxiv.org/abs/2209.07858 [13] Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2024. Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. arXiv:2310.19923 [cs.CL] https://arxiv.org/abs/2310. [14] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. 2023. Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. arXiv:2312.06674 [cs.CL] https://arxiv.org/abs/2312.06674 [15] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, and Andrew M. Dai. 2024. Best Practices and Lessons Learned on Synthetic Data. arXiv:2404.07503 [cs.CL] https://arxiv.org/abs/2404.07503 [16] AI@Meta et al. Llama Team. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [17] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024. On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: Survey. arXiv:2406.15126 [cs.CL] https://arxiv.org/abs/2406.15126 [18] Chris Lu, Samuel Holt, Claudio Fanconi, Alex J. Chan, Jakob Foerster, Mihaela van der Schaar, and Robert Tjarko Lange. 2024. Discovering Preference Optimization Algorithms with and for Large Language Models. arXiv:2406.08414 [cs.LG] https://arxiv.org/abs/2406.08414 [19] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. 2023. holistic approach to undesired content detection in the real world. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence (AAAI23/IAAI23/EAAI23). AAAI Press, Article 1683, 10 pages. https://doi.org/10.1609/aaai.v37i12. [20] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. 2024. HarmBench: Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal. arXiv:2402.04249 [cs.LG] https://arxiv.org/abs/2402.04249 [21] Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei ge Chen, Olga Vrousgos, Corby Rosset, Fillipe Silva, Hamed Khanpour, Yash Lara, and Ahmed Awadallah. 2024. AgentInstruct: Toward Generative Teaching with Agentic Flows. arXiv:2407.03502 [cs.AI] https://arxiv.org/abs/2407.03502 [22] OpenAI. 2024. Introducing Structured Outputs in the API. https://openai.com/ index/introducing-structured-outputs-in-the-api/. Published August 6, 2024. [Accessed 16-11-2024]. [23] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2024. Training language models to follow instructions with human feedback. In Proceedings of the 36th International Conference on Neural Information Processing Systems (New XXXX, XXXX XXXX, XXXX, XXXX Gabriel Chua, Chan Shing Yee, and Shaun Khoo Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 2011, 15 pages. [24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct preference optimization: your language model is secretly reward model. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 2338, 14 pages. [25] Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and Jonathan Cohen. 2023. NeMo Guardrails: Toolkit for Controllable and Safe LLM Applications with Programmable Rails. arXiv:2310.10501 [cs.CL] https://arxiv.org/abs/2310.10501 [26] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. arXiv:2112.01488 [cs.IR] https://arxiv.org/abs/2112.01488 [27] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2024. \"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. arXiv:2308.03825 [cs.CR] https://arxiv.org/abs/2308. 03825 [28] Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell. 2024. Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=fsW7wJGLBd [29] Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. 2024. The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions. arXiv:2404.13208 [cs.CR] https://arxiv.org/abs/2404.13208 [30] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. arXiv:2212.10560 [cs.CL] https://arxiv. org/abs/2212.10560 [31] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-Pack: Packed Resources For General Chinese Embeddings. arXiv:2309.07597 [cs.CL] https://arxiv.org/abs/2309.07597 [32] Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, and Qi He. 2024. SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains. arXiv:2410.17952 [cs.CL] https://arxiv.org/abs/2410.17952 [33] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing. arXiv:2406.08464 [cs.CL] https://arxiv.org/abs/2406."
        },
        {
            "title": "EVALUATION",
            "content": "Table 5: Performance on Synthetic Dataset Generated by Gemini Pro 1.5 and Claude 3.5 Sonnet (N=326) Approach Model ROC-AUC F1 Precision Recall Fine-tuned cross-encoder classifier Fine-tuned bi-encoder classifier stsb-roberta-base jina-embeddings-v2-small-en 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 Table 6: Performance on Synthetic Dataset Generated by Llama 3.1 405B (N=29,635) Approach Model ROC-AUC F1 Precision Recall Fine-tuned cross-encoder classifier Fine-tuned bi-encoder classifier stsb-roberta-base jina-embeddings-v2-small-en 0.99 0.99 0.96 0.99 0.97 0.97 0.94 0."
        }
    ],
    "affiliations": [
        "Government Technology Agency Singapore",
        "National University of Singapore"
    ]
}