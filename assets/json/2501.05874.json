{
    "paper_title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus",
    "authors": [
        "Soyeong Jeong",
        "Kangsan Kim",
        "Jinheon Baek",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While a few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines."
        },
        {
            "title": "Start",
            "content": "VideoRAG: Retrieval-Augmented Generation over Video Corpus Soyeong Jeong1 Kangsan Kim1 Jinheon Baek1 Sung Ju Hwang1,2 KAIST1 DeepAuto.ai2 {starsuzi, kksan07, jinheon.baek, sjhwang82}@kaist.ac.kr 5 2 0 2 0 1 ] . [ 1 4 7 8 5 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) is powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines."
        },
        {
            "title": "Introduction",
            "content": "Recently, large foundation models, such as Large Language Models (LLMs) and their extension to vision modality called Large Vision-Language Models (VLMs), have become the standard choice for addressing diverse tasks due to their remarkable capabilities (Li et al., 2024; Yang et al., 2024; Dai et al., 2024). In particular, these models, trained on *Equal contribution 1 Figure 1: conceptual illustration of existing and the proposed RAG scenarios. (A) Textual RAG retrieves documents (relevant to queries) from text corpus and incorporates them when generating answers. (B) Conventional multimodal RAG extends retrieval to include static images. (C) VIDEORAG (ours) further extends the external knowledge source to videos. extensive textual and multimodal corpora, encode vast amounts of knowledge within their large-scale parameters. However, despite their success, they are still prone to generating factually incorrect outputs, known as hallucination, as their parametric knowledge can be inaccurate, incomplete, or outdated (Lewis et al., 2020; Ram et al., 2023), and this limitation highlights the need for incorporating knowledge from external knowledge sources, with Retrieval-Augmented Generation (RAG) emerging as an essential mitigator for it. Specifically, RAG typically operates by retrieving query-relevant information from an external corpus and then generating answers grounded in the retrieved information (Niu et al., 2024; Ayala and BÃ©chard, 2024). However, while existing RAG approaches have been widely adopted for various real-world applications, they have primarily focused on retrieving and incorporating textual content (Ram et al., 2023; Jeong et al., 2024a), with only recent attempts beginning to explore images (or text-image pairs) as the additional source of external knowledge (Yu et al., 2024; Riedler and Langer, 2024). On the other hand, we argue that there remains rapidly expanding yet underutilized medium, called videos, which provides unparalleled multimodal richness, and they may be compelling resource for augmenting the knowledge landscape of current RAG systems. Specifically, videos combine temporal dynamics, spatial details, and multimodal cues, which collectively enable them to capture complex processes, context-dependent interactions, and nonverbal signals that static modalities (such as text and images) often fail to convey. Moreover, given the increasing popularity of video-sharing platforms (such as YouTube and TikTok), the availability of diverse, high-quality video data has grown significantly, ranging from educational tutorials and scientific demonstrations to personal experiences and real-time events, all of which may be useful when formulating responses to user queries. It is worth noting that few very recent studies have started considering video content to handle user queries; however, they have notable limitations. For instance, some of them assume that the video relevant to queries is already known and instead focus on identifying query-relevant frames from within that specified video (Luo et al., 2024; Ma et al., 2024). While this approach may work for scenarios where the relevant video is explicitly provided, it is suboptimal for more general-use cases, where users expect the system to dynamically identify and retrieve relevant video content to provide the answer. On the other hand, other studies handle videos by converting them into textual formats, such as subtitles or captions, and utilizing these textual representations under off-the-shelf text-based RAG pipelines (Arefeen et al., 2024; Zhang et al., 2024b). However, while this text-only strategy may offer convenient workaround, it inherently sacrifices the multimodal richness of video data by discarding critical information, such as temporal dynamics and visual context, during the conversion process, which may be essential. For example, consider query: How does the expression of the dog change when it is angry?. While textual transcriptions might describe the dogs barking or growling, they fail to capture visual cues (baring teeth, raised hackles, or narrowed eyes), which are needed for accurately interpreting the emotional state of the dog as well as formulating the answer to the query. In this work, to address the aforementioned limitations of existing studies, we introduce novel framework, called VideoRAG, which aims to offer another fruitful angle to existing RAG frameworks by enabling more comprehensive utilization of video content for its holistic retrieval and incorporation (See Figure 1 for its illustation and comparison to existing works). Specifically, in response to queries, the proposed VideoRAG not only retrieves relevant videos from large video corpus, but also integrates both visual and textual elements of videos into the answer-generation process. Furthermore, we operationalize this by harnessing the advanced capabilities of recent Large Video Language Models (LVLMs), which are capable of directly processing the video content, consisting of visual and textual information, within the unified framework, thereby more effectively capturing its multimodal richness. However, while on the one hand, we observe that the joint utilization of visual and textual features is needed for the effective representation of videos and subsequently their retrieval, on the other hand, the textual descriptions of videos (e.g., subtitles) are oftentimes not available. To address this, we further present simple yet effective mitigation strategy that utilizes automatic speech recognition techniques to generate textual transcripts from videos, which allows us to leverage both visual and textual modalities even when explicit textual annotations are absent. To validate the effectiveness of our VideoRAG framework, we conduct experiments by leveraging the overlapping queries from the WikiHowQA dataset (Bolotova-Baranova et al., 2023) (consisting of query-answer pairs) with the queries from the HowTo100M dataset (Miech et al., 2019) (including query-video pairs without answers). Then, the experimental results show the significant performance improvement of the proposed VideoRAG framework over relevant RAG baselines, demonstrating the efficacy of leveraging videos for RAG."
        },
        {
            "title": "2.1 Preliminaries\nWe begin with preliminaries, providing the descrip-\ntions of Retrieval-Augmented Generation (RAG)\nand then Large Video Language Models (LVLMs).",
            "content": "Retrieval-Augmented Generation RAG aims to enhance the capabilities of foundation models by 2 grounding their outputs in external knowledge retrieved from the external knowledge source, such as Wikipedia, which consists of two main components: retrieval and generation modules. Formally, given query q, RAG retrieves set of documents (or knowledge elements) = {k1, k2, . . . , kk} from an external corpus (K C) based on their relevance with using retrieval module, which can be formalized as follows: = Retriever(q, C). Here, the query and knowledge are represented as sequence of tokens = [q1, q2, . . . , qi] and = [k1, k2, . . . , kj]. Also, during retrieval, the relevance between the query and each knowledge element within the corpus is determined by the scoring function, defined as follows: Sim(q, k), which typically measures their representational similarity over the embedding space. In the subsequent generation step, the retrieved knowledge elements are then used as additional input to the generation module, to augment the query to produce an answer y, which can be defined as follows: = Model(q, K), where Model is typically implemented as the foundation model, such as LLMs. It is worth noting that, unlike existing RAG frameworks that focus mainly on retrieving and incorporating textual content (or, in some recent cases, extra static images), in this work, we explore the extension toward videos. Large Video Language Models On top of the extensive language understanding capabilities of LLMs, LVLMs are designed to handle and incorporate the features from video content, including temporal, spatial, and multimodal information, within the unified token processing framework. Formally, let us denote video as sequence of image frames: = [v1, v2, . . . , vn] and its associated textual data (such as subtitles, or any other textual information such as the video-specific query) as sequence of tokens: = [t1, t2, . . . , tm]. Then, the typical LVLM, denoted as LVLM, enables the joint processing of these multimodal inputs by employing two specialized components: vision encoder and text encoder. Specifically, the vision encoder processes the sequence of video frames (which can span multiple videos) to extract temporal and spatial features, resulting in sequence of visual feature embeddings (or visual tokens): Fvisual = VisionEncoder(V ). Concurrently, the text encoder processes the given textual information to generate corresponding feature embeddings: Ftext = TextEncoder(t). Then, the overall process to obtain the video representation (with the goal of capturing both visual and textual features) can be denoted as follows: fvideo = LVLM(V , t). Traditionally, fvideo is obtained by the simple interpolation of the visual and textual representations: fvideo = Î± Ftext + (1 Î±) Fvisual (Xu et al., 2021), and, more recently, it can be done by further jointly processing the visual and textual embeddings through several LVLM layers (that sit on top of existing LLMs) (Zhang et al., 2024c), which allows the model to learn more effective representation and continue generating the next sequence of tokens (for example, an answer to query)."
        },
        {
            "title": "2.2 VideoRAG",
            "content": "We now turn to introduce our VideoRAG, which extends the existing RAG paradigm by leveraging the video corpus as the external knowledge source. Video Retrieval The initial step to operationalize RAG over the video corpus is to implement video retrieval, whose goal is to identify query-relevant videos = {V1, V2, . . . , Vk} from the external corpus C, consisting of large number of videos, formalized as follows: = Retriever(q, C). Recall that, as explained in Section 2.1, this retrieval process involves calculating the similarity between the query and each knowledge element (which is video in our case) to determine their relevance. To achieve this, we first forward the video (composed of image frames and, if available, subtitles) as well as the query (without visual information) into LVLM, to obtain their representations fquery and fvideo. After that, the relevance is computed based on their representation-level similarity, for example, using cosine similarity, and then the top-k videos with the highest similarity scores are retrieved for the subsequent answer generation step. Video-Augmented Response Generation After the retrieval of query-relevant videos is done, the next step is to incorporate the retrieved videos into the answer generation process, in order to formulate the answer grounded in them. To operationalize this, we first concatenate the frames of each retrieved video with its associated textual data (e.g., subtitles), then concatenate these multimodal pairs across all videos retrieved, and lastly append the user query, to construct the input to LVLM, denoted as follows: [V1, t1, . . . , Vk, tk, q]. Then, this input is forwarded into LVLM, which enables the joint processing of the combined visual, textual, and queryspecific information, to generate the response while considering their multimodal richness."
        },
        {
            "title": "3.2 Models\nBaselines We evaluate the performance of Vide-\noRAG against four different baselines, as follows:\n1. NAÃVE â which directly generates answers us-\ning input queries without any additional context.\n2. TEXTRAG (BM25) â which is a text-based\nRAG model, performing retrieval over docu-\nments (from Wikipedia) based on their relevance",
            "content": "1https://www.wikihow.com/Main-Page to queries with BM25 (Robertson et al., 1994) (which is sparse retriever with lexical matching) and generating answers grounded in them. 3. TEXTRAG (DPR) which is similar textbased RAG baseline to TEXTRAG (BM25) but instead performs retrieval with DPR (Karpukhin et al., 2020), widely used dense retriever. 4. TEXTVIDEORAG which follows the previous video-based RAG methods (Arefeen et al., 2024; Zhang et al., 2024b), which first represent videos as their textual descriptions (e.g., captions or transcripts) and then utilize only those textual information in retrieval and generation. VideoRAG We introduce and validate three variants of the proposed VideoRAG framework, each employing the same video retrieval method but differing in how the retrieved videos are incorporated during answer generation. Specifically, the first variant, VIDEORAG-T, integrates only the transcripts (or auxiliary textual data) obtained from retrieved videos as context for answer generation. Meanwhile, the second variant, VIDEORAGV, exclusively utilizes video frames as context to provide visual grounding for responses. Finally, VIDEORAG-VT jointly utilizes both the video frames and textual transcripts for generation. Additionally, to further estimate the room for performance improvement, we include an oracle version of VIDEORAG, which directly uses the groundtruth video pre-associated with the query labeled in HowTo100M, instead of using retrieval outcomes."
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "To comprehensively evaluate the performance of different models, we use the following diverse metrics: 1) ROUGE-L measures the longest common subsequence between the generated answer and the ground truth (Lin, 2004); 2) BLEU-4 calculates the overlap of n-grams (up to 4-grams) between the generated answer and the reference answer (Papineni et al., 2002); 3) BERTScore is designed to consider the semantic alignment between the generated and reference answers (Zhang et al., 2020) by extracting their contextual embeddings from pretrained BERT model (Devlin et al., 2019) and then calculating their representational similarity; 4) GEval leverages the evaluation capabilities of LLMs to assess the quality of the generated answer (Liu et al., 2023), and, to do this, we prompt the LLM to rate the generated answer in comparison to the reference on 5-point Likert scale. 4 Table 1: Overall RAG results across four metrics. The best results are highlighted in bold, and the second-best results are highlighted with underline. Note that the Oracle setting (that uses ideal retrieval results) is not comparable to others. Methods ROUGE-L BLEUBERTScore G-Eval Baselines Ours Oracle NAÃVE TEXTRAG(BM25) TEXTRAG(DPR) TEXTVIDEORAG VIDEORAG-T VIDEORAG-V VIDEORAG-VT VIDEORAG-V 0.141 0.172 0.166 0.228 0.224 0.254 0.252 0.262 0.014 0.032 0.022 0.044 0.043 0.054 0.054 0.055 0.834 0.847 0.846 0.870 0.868 0.881 0.880 0.884 1.579 1.633 1.591 1.979 2.001 2.161 2.104 2.225 3."
        },
        {
            "title": "Implementation Details",
            "content": "It is worth noting that there exist various LVLMs available for use, each with different merits depending on the task requirements: for retrieval, precise alignment between textual and video features (obtained from their specialized encoders) is essential to ensure that the retrieved videos are contextually relevant to the query, meanwhile, generation benefits from LVLMs with advanced capabilities for accurately formulating responses and grounding them in the retrieved content. To achieve this, for retrieval, we use InternVideo2 (Wang et al., 2024c) since it is explicitly trained to align semantics between videos and their textual descriptions. Specifically, we use its video and text encoders to extract embeddings for videos and text, respectively. On the other hand, for video-augmented answer generation, we use LLaVA-Video-7B (Zhang et al., 2024c), which is known for achieving state-of-theart performance on video understanding and relevant tasks. For efficiency, we uniformly sample 4 frames per video for retrieval, while we sample 32 frames (or all frames if the video is shorter than 32 seconds, sampled at 1 fps) for generation. In auxiliary text generation, to extract textual content from videos, we use Whisper (Radford et al., 2023)."
        },
        {
            "title": "4 Experimental Results and Analyses",
            "content": "We now present results and various analyses, showing the effectiveness of the proposed VideoRAG."
        },
        {
            "title": "4.1 Main Results",
            "content": "We provide main results in Table 1, showcasing the performance of different models with varying types of retrieved knowledge. First, we observe that all RAG models, utilizing external knowledge, clearly outperform the NAÃVE baseline, reaffirming the critical role of external knowledge in enhancing the factual accuracy of generated responses. Also, among these, our VIDEORAG achieves the highest performance, significantly surpassing existing Table 2: Generation results using different set of videos, such as Random that randomly samples videos, Retrieved that selects videos according to their relevance with queries, and Oracle that uses the ground truth videos annotated in data. Video Set ROUGE-L BLEU-4 BERTScore Random Retrieved Oracle 0.243 0.254 0.262 0.050 0.054 0.055 0.878 0.881 0.884 textual RAG baselines. This improvement corroborates our hypothesis that video content may be useful resource for RAG, as it provides richer and more detailed information than textual resources. Then, one might ask whether the performance improvement of the proposed VideoRAG framework comes from the consideration of the additional modality (e.g., video frames) rather than the actual content within the retrieved knowledge. To investigate this, we compare TEXTVIDEORAG and VIDEORAG-T, which utilize textual representations of videos (without vision modality), against TEXTRAG that uses the encyclopedic knowledge base in text (namely, Wikipedia) as the knowledge source. As shown in Table 1, we find that the improved performance of VIDEORAG-T and TEXTVIDEORAG over TEXTRAG suggests that, even if we use only the textual representations of videos, the content embedded in videos is proved to be more fruitful to enhance the response quality than the general encyclopedic knowledge. Additionally, despite the superior performance of TEXTVIDEORAG and VIDEORAG-T to baselines, they are outperformed by the other variants of our VideoRAG, which directly incorporate visual features into the answer generation process. This result further highlights the critical role of visual information in performance. Finally, the smaller performance gap between VIDEORAG-V and VIDEORAG-VT suggests that much of the necessary information required for answer generation is effectively encapsulated within the visual features of video frames, which inherently include information conveyed through textual descriptions. Features R@1 R@5 R@10 Visual Textual Ensemble 0. 0.088 0.103 0.193 0.302 0.311 0. 0.388 0.442 Table 3: Retrieval results, where we use visual features alone, textual features alone, or an ensemble of their features. Figure 2: Impact of varying the interpolation ratio between textual and visual features on retrieval performance. Figure 3: Visualization of latent space of features across modalities with Principal Component Analysis (PCA). Figure 4: Breakdown performance of different models across 10 different categories."
        },
        {
            "title": "4.2 Video Retrieval",
            "content": "Impact of Video Retrieval We hypothesize that the quality of the retrieved videos is critical factor in the success of RAG, as it can directly influence the subsequent answer generation process. To confirm this, we compare the performance of VideoRAG with different videos, including randomly selected videos and retrieved videos (relevant to queries), used for augmenting the answer generation step. As shown in Table 2, we then observe that incorporating query-relevant videos significantly improves the quality of the answers compared to randomly selected videos, demonstrating the importance of retrieval quality. Furthermore, the Oracle setting, which represents an ideal scenario with perfectly relevant video retrieval, achieves the highest performance, highlighting the potential for further improvements through advancements in video retrieval mechanisms within our VideoRAG. Effectiveness of Textual and Visual Features When performing video retrieval, it is questionable how much different modalities, such as textual features, visual features, or combination of both, contribute to the effectiveness of video representations, and we report retrieval results with varying modalities in Table 3. From this, we observe that textual features consistently outperform visual features, likely due to their stronger semantic alignment with textual user queries. To further examine this, we visualize the embeddings of textual and visual features of video content as well as queries over the latent space in Figure 3, and it clearly reveals closer proximity between textual query embeddings and textual video representations compared to visual video representations. In other words, visual features exhibit modality gap relative to text-based queries (as reflected in their greater distance), resulting in suboptimal retrieval performance. Nevertheless, combining textual and visual features achieves the highest performance, demonstrating the complementary nature of those two modalities in video representations for retrieval. Analysis on Feature Ensemble To better understand the contribution of textual and visual features in video retrieval, we analyze how varying their combination ratio (Î±) impacts performance across different metrics. As shown in Figure 2, we find that the optimal ratio for balancing textual and visual features is around 0.5 to 0.7 (with marginal variations depending on the specific metric: 0.7 for R@1 and R@5; 0.5 for R@10). These results further highlight the complementary contributions of textual and visual features in video representations for retrieval, while slight emphasis on textual features might be preferable due to the modality gap observed in Figure 3. Lastly, based on this analysis, we select Î± = 0.7 for our experiments. 6 Table 4: Case study comparing NAÃVE and VIDEORAG-V approaches. NAÃVE Question Generated Answer Explain how to bake cookies on your car dashboard. Im sorry, but its not possible to bake cookies on your car dashboard. The dashboard is not designed for cooking and its not safe to use it as heat source. Additionally, the fumes from baking could be harmful to you and the cars interior. Baking requires controlled environment with consistent temperature, which is difficult to achieve in car. Moreover, the dashboard is made of materials that can easily catch fire or release toxic fumes when exposed to high temperatures. If you want to bake cookies, its best to do so in safe and controlled environment, such as kitchen or an outdoor grill. Just remember to always follow proper safety precautions and guidelines when cooking. Question Retrieved Video Explain how to bake cookies on your car dashboard. VideoRAG-V Generated Answer To bake cookies on your car dashboard, youll need nonstick cooking spray, cookie sheet, cookie dough, spatula, and thermometer. First, spray the cookie sheet with nonstick cooking spray. Then, place the cookie dough onto the sprayed sheet. Cover it with plastic wrap and let it rest. Next, place the cookie sheet in the cars dashboard, ensuring its at least 95 degrees Fahrenheit. Set the car in sunny spot to bake the cookies. After baking, use spatula to remove the cookies from the dashboard. Ground Truth To bake cookies on your car dashboard, youll need to be parked in sunny spot on day when its at least 95 degrees Fahrenheit out. First, spread cookie dough out on baking sheet lined with parchment paper. Then, set the baking sheet on your car dashboard, and shut all of the doors. Finally, check on the cookies every 15-30 minutes until the edges of the cookies are firm and youre able to slide them off the parchment paper. Table 5: Ablation studies with different modalities. For TEXTRAG, we use BM25 to retrieve textual documents. Methods Wikipedia Video Subtitle ROUGE-L G-Eval NAÃVE TEXTRAG VIDEORAG-VT VIDEORAG-VT + TEXTRAG 0.141 0.172 0.252 0.243 1.579 1.633 2.161 2."
        },
        {
            "title": "4.3 Analyses and Discussion",
            "content": "Category-Wise Performance Analysis To evaluate the robustness of our VideoRAG across diverse query types, we breakdown the model performance on 10 distinct categories (annotated within WikiHow). Then, as shown in Figure 4, the family of the proposed VideoRAG (such as VIDEORAG-T and VIDEORAG-V) outperforms all other baselines across all categories, which highlights its ability to handle wide range of queries. Also, it is worth noting that VIDEORAG-V shows notable performance improvement in the Food & Entertaining category, and this is particularly reasonable given that the questions in this category often benefit significantly from visual details, for example, the query: How to make healthy spinach and garlic dish requires ingredient preparation or cooking techniques, which are not effectively conveyed through text alone. Thus, the results in this category reaffirm the importance of leveraging video content as external knowledge for RAG. Ablation Studies To analyze how performance varies with different knowledge sources, we conduct ablation studies and present the results in Table 5. We then find that, while incorporating external knowledge (whether from textual encyclopedic sources or video corpus) consistently improves performance over the NAÃVE baseline, the approach that jointly uses videos with general textual documents achieves slightly degraded performance. This suggests that textual content (retrieved from the encyclopedic knowledge base) may introduce redundant or irrelevant details, which may overlap with or contradict the information provided by video content, leading to diminishing the effectiveness of the VideoRAG framework. Qualitative Results We now qualitatively analyze the effectiveness of VideoRAG through case study, with the query: Explain how to bake cookies on your car dashboard. As shown in Table 4, the NAÃVE baseline, relying solely on its parametric knowledge, generates generic response highlighting the impracticality and safety concerns of such method, failing to provide the step-by-step instructions necessary to address the query. It indicates the limitation of parametric knowledge that is inadequate especially when specific and uncommon information is required. In contrast, VIDEORAG7 Table 6: Case study comparing TEXTRAG (BM25) and VIDEORAG-V approaches. TEXTRAG (BM25) Question Retrieved Document Generated Answer Explain how to make clay rose. Vally Wiselthier, an art deco ceramicist who was making pieces for General Ceramics in Keasbey, New Jersey. After seeing her preliminary pieces, Erni gave Rose membership at Greenwich House, where she learned how to use potters wheel. Shortly after their first son George was born, he was found to have intractable asthma. The Cabats decided to move to Arizona around 1942 in order to alleviate his condition. During World War II Rose worked as riveter at the Davis-Monthan Army Air Field repairing war-damaged aircraft. Rose was able to make primitive ceramics from the extra clay that Erni The document does not provide step-by-step guide on how to make clay rose. However, based on the information provided, we can infer that Rose learned how to use potters wheel at Greenwich House after seeing Vally Wiselthiers preliminary pieces. This suggests that she may have received some training or instruction in pottery from Vally Wiselthier or other instructors at Greenwich House. To make clay rose, you would need to follow these general steps: 1. Gather materials: You will need clay, potters wheel, wire cutter, sponge, and any additional materials you want to use for decoration (such as glaze, paint, or glitter). 2. Prepare the clay: Cut piece of clay to the desired size and shape for your rose. Use wire cutter to trim any excess clay and smooth out any rough edges. 3. Shape the clay: Use your hands or tools to shape the clay into rose. You can start by creating small ball of clay for the center of the rose, then add petals around it. Use sponge to smooth out any rough areas and refine the shape of the rose. 4. Add details: Use tool or your fingers to add details to the petals, such as veins or shading. You can also use additional materials like glaze, paint, or glitter to decorate the rose. 5. Allow the clay to dry: Let the clay dry completely before firing it in kiln. This will help prevent any cracking or breaking during the firing process. 6. Fire the clay: Place the dried clay rose in kiln and fire it according to the manufacturers instructions. This will harden the clay and create durable piece of pottery. 7. Glaze or finish: Once the clay is fired, you can apply glaze or finish to give the rose glossy or matte appearance. Allow the glaze to dry and cure before handling the finished piece. Its important to note that this is general guide and the specific steps may vary depending on the type of clay you are using and the techniques you are employing. Its also important to follow safety guidelines when working with clay and kilns. Question Retrieved Video Explain how to make clay rose. VideoRAG-V Generated Answer Ground Truth To make clay rose, start by rolling out piece of clay into flat sheet. Use rolling pin to flatten the clay further. Then, use knife or sculpting tool to cut the clay into petal shapes. Roll the petals into spiral shape to form the rose. Use small tool to create indentations in the petals for texture. Assemble the petals around central stem, shaping them to resemble rose. Add leaves and other details as desired. Finally, use mold to imprint patterns onto the petals, adding depth and realism to the rose. To make clay rose, start by rolling chunk of clay into ball, dividing the ball into seven similarly-sized pieces, and rolling those pieces into balls. Take the smallest ball and press it into guitar-pick-shaped petal with the pointy part thicker than the rounded part, then fold the sides gently inward. Repeat with slightly bigger ball of clay, then fold this second petal around the first. Continue attaching slightly larger petals in spiral, then pinch the bottom of your rose to connect all the petals. retrieves relevant instructional video that illustrates the process of baking cookies on car dashboard, and, by leveraging this, it successfully generates detailed response similar to the ground truth. This example highlights how the proposed VideoRAG can utilize external video content to produce more precise, contextually rich, and actionable answers than methods that rely solely on parametric knowledge embedded within models. and, consequently, the generated response is misaligned with the query. In contrast, VIDEORAG-V retrieves relevant video demonstrating how to make clay rose and leverages this visual content to generate concise and accurate response that closely mirrors the ground truth, from which we clearly confirm the utility of videos for RAG."
        },
        {
            "title": "5 Related Work",
            "content": "Furthermore, we observe that external textual knowledge alone can sometimes fall short in providing relevant and actionable information for specific procedural queries, such as Explain how to make clay rose. Specifically, as shown in Table 6, TEXTRAG (BM25) retrieves an irrelevant document about person named Rose, as Wikipedia does not contain specific procedural guidance on this topic, Retrieval-Augmented Generation RAG is strategy that combines retrieval and generation processes to produce accurate answers by grounding them in relevant external knowledge (Lewis et al., 2020; Ram et al., 2023; Zhao et al., 2024). To be specific, during the retrieval step, documents (relevant to queries) are selected from large corpus by calculating their similarity to the query, which can 8 be done by using either sparse methods (Robertson et al., 1994; Jones, 2004) based on lexical overlaps or dense methods (Karpukhin et al., 2020; Izacard et al., 2022) that leverage semantic representations. In the subsequent generation step, these retrieved documents serve as input for generating answers that are rooted in the provided information, with the generation process (as well as retrieval) varying depending on the specific work (Jiang et al., 2023; Asai et al., 2024; Hwang et al., 2024; Cheng et al., 2024), for example, incorporating iterative retrieval-generation cycles (Trivedi et al., 2023) or adapting different RAG strategies based on query complexity (Jeong et al., 2024a). However, despite the fact that much of the real-world knowledge is inherently multimodal in nature (Lee et al., 2024; Jeong et al., 2024b; Faysse et al., 2024), the majority of current RAG studies have focused preliminary on the textual modality, with little effort on incorporating images, leaving significant gap in leveraging the full spectrum of available knowledge for the holistic operation of RAG. Multimodal RAG Recently, there has been growing interest in expanding RAG systems to incorporate multimodal information (beyond textual documents), such as images (Chen et al., 2022; Lin and Byrne, 2022; Riedler and Langer, 2024; Yu et al., 2024), code (Guo et al., 2024), tables (Pan et al., 2022; Biswal et al., 2024), and audio (Yuan et al., 2024). However, in contrast to them, videos offer unique and orthogonal advantage for advancing RAG systems, as they encapsulate temporal dynamics, spatial details, and multimodal cues in ways unmatched by other modalities. Inspired by this, very recent studies have begun exploring the usage of video content within RAG pipelines; however, despite these efforts, existing approaches leverage the distinct advantages of video data in suboptimal way. Specifically, some focus on extracting queryrelevant frames from preselected video and generating answers based on them, which, while useful in controlled scenarios, limits their real-world applicability in open-domain settings (Luo et al., 2024; Ma et al., 2024). Also, other approaches attempt to sidestep the complexity of handling video data by converting it into textual representations (such as subtitles or captions); however, while directly applicable to existing text-based RAG frameworks, they sacrifice the multimodal richness embedded within videos (such as temporal dynamics and spatial patterns) (Arefeen et al., 2024; Zhang et al., 2024b; Ma et al., 2024). To address these challenges, we propose VideoRAG that is capable of dynamically retrieving and holistically utilizing video content in RAG, powered by LVLMs discussed next. Large Video Language Models Building on the remarkable success of LLMs in language understanding and generation as well as their ability to encapsulate vast amounts of knowledge (OpenAI, 2023; Anil et al., 2023; Dubey et al., 2024), there has been growing interest in extending them to encompass diverse modalities, such as images (Lin et al., 2024; Bordes et al., 2024; Zhu and Zhang, 2025) and code (DeepSeek-AI et al., 2024; Hui et al., 2024). Furthermore, this expansion has recently extended to another modality called video, leading to the emergence of LVLMs that are capable of directly processing video content. In particular, these models excel in solving traditionally challenging (yet straightforward) tasks, such as object or action detection (Tang et al., 2023), and their capabilities have been rapidly advanced, enabling them to tackle more challenging tasks, such as analyzing spatio-temporal dynamics to predict the sequence of events, inferring causal relationships across video frames, and generating context-aware descriptions of intricate scenarios (Wang et al., 2024a; Maaz et al., 2024; Zhang et al., 2024a; He et al., 2024; Wang et al., 2024b), even in zero-shot settings without additional training (Chen et al., 2024; Kim et al., 2024). However, despite these advancements, their potential has yet to be explored in the context of RAG; thus, in this work, we aim to bridge this gap with the proposal of VideoRAG."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we presented VideoRAG, novel framework that expands the current landscape of RAG systems by leveraging video corpus as the external knowledge source. Specifically, unlike existing works that use the textual representations of videos or assume the existence of query-relevant videos without retrieval, the proposed VideoRAG retrieves videos based on their relevance to queries but also integrates their multimodal richness (including visual and textual elements) into the RAG pipeline, by harnessing the capabilities of LVLMs. Also, through comprehensive analyses, we demonstrated how the inclusion of visual or textual features, or combination of both, improves retrieval and generation performance, and, inspired by the critical role of textual features (for retrieval quality) 9 but their absence in some videos, we presented simple yet effective mitigator that uses automatic speech recognition to generate textual transcripts. Overall, experimental results validated the superiority of our VideoRAG over existing RAG methods, and we believe it makes significant step toward holistic RAG systems that can utilize videos."
        },
        {
            "title": "References",
            "content": "Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Md. Adnan Arefeen, Biplob Debnath, Md. Yusuf Sarwar Uddin, and Srimat Chakradhar. 2024. irag: Advancing RAG for videos with an incremental approach. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 2024, Boise, ID, USA, October 21-25, 2024, pages 43414348. ACM. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Orlando Ayala and Patrice BÃ©chard. 2024. Reducing hallucination in structured outputs via retrievalaugmented generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 228238. Association for Computational Linguistics. Asim Biswal, Liana Patel, Siddarth Jha, Amog Kamsetty, Shu Liu, Joseph E. Gonzalez, Carlos Guestrin, and Matei Zaharia. 2024. Text2sql is not enough: Unifying AI and databases with TAG. arXiv preprint arXiv:2408.14717, abs/2408.14717. Valeria Bolotova-Baranova, Vladislav Blinov, Sofya Filippova, Falk Scholer, and Mark Sanderson. 2023. Wikihowqa: comprehensive benchmark for multidocument non-factoid question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 52915314. Association for Computational Linguistics. Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar MaÃ±as, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, and Vikas Chandra. 2024. An introduction to vision-language modeling. arXiv preprint arXiv:2405.17247, abs/2405.17247. Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. 2024. Videollm-online: Online video large language model for streaming video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1840718418. IEEE. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. 2022. Murag: Multimodal retrieval-augmented generator for open question anIn Proceedings of swering over images and text. the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 55585570. Association for Computational Linguistics. Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, and Xipeng Qiu. 2024. Unified active retrieval for retrieval augmented generation. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1715317166. Association for Computational Linguistics. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. NVLM: open frontier-class multimodal llms. arXiv Preprint arXiv:2409.11402, abs/2409.11402. DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli 10 Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, and Wenfeng Liang. 2024. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv Preprint arXiv:2406.11931, abs/2406.11931. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, AurÃ©lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste RoziÃ¨re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, GrÃ©goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, CÃ©line Hudelot, and Pierre Colombo. 2024. Colpali: Efficient document retrieval with vision language models. arXiv Preprint arXiv:2407.01449, abs/2407.01449. Yucan Guo, Zixuan Li, Xiaolong Jin, Yantao Liu, Yutao Zeng, Wenxuan Liu, Xiang Li, Pan Yang, Long Bai, Jiafeng Guo, and Xueqi Cheng. 2024. Retrievalaugmented code generation for universal information In Natural Language Processing and extraction. Chinese Computing - 13th National CCF Conference, NLPCC 2024, Hangzhou, China, November 1-3, 2024, Proceedings, Part II, volume 15360 of Lecture Notes in Computer Science, pages 3042. Springer. Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. 2024. MA-LMM: memoryaugmented large multimodal model for long-term video understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1350413514. IEEE. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. 2024. Qwen2.5-coder technical report. arXiv Preprint arXiv:2409.12186, abs/2409.12186. Taeho Hwang, Soyeong Jeong, Sukmin Cho, SeungYoon Han, and Jong Park. 2024. DSLR: Document refinement with sentence-level re-ranking and reconstruction to enhance retrieval-augmented generation. In Proceedings of the 3rd Workshop on Knowledge Augmented Methods for NLP, pages 7392, Bangkok, Thailand. Association for Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res., 2022. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2024a. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 70367050. Association for Computational Linguistics. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. 2024b. Databaseaugmented query representation for information arXiv Preprint arXiv:2406.16013, retrieval. abs/2406.16013. Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 79697992. Association for Computational Linguistics. Karen SpÃ¤rck Jones. 2004. statistical interpretation of term specificity and its application in retrieval. J. Documentation, 60(5):493502. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 67696781. Association for Computational Linguistics. Kangsan Kim, Geon Park, Youngwan Lee, Woongyeong Yeo, and Sung Ju Hwang. 2024. Videoicl: Confidence-based iterative in-context learning for arXiv out-of-distribution video understanding. Preprint arXiv:2412.02186. Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, and Sung Ju Hwang. 2024. Unified multi-modal interleaved document representation for information retrieval. arXiv Preprint arXiv:2410.02729, abs/2410.02729. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024. Llava-onevision: Easy visual task transfer. Preprint, arXiv:2408.03326. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. 2024. Mm-embed: Universal multimodal retrieval with multimodal llms. arXiv Preprint arXiv:2411.02571, abs/2411.02571. Weizhe Lin and Bill Byrne. 2022. Retrieval augmented visual question answering with outside knowledge. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1123811254. Association for Computational Linguistics. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 25112522. Association for Computational Linguistics. Luo, and Rongrong Ji. 2024. Video-rag: Visuallyaligned retrieval-augmented long video comprehension. arXiv Preprint arXiv:2411.13093. Ziyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, and Jianfei Cai. 2024. Drvideo: Document retrieval based long video understanding. arXiv preprint arXiv:2406.12846, abs/2406.12846. Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, and Fahad Khan. 2024. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1258512602. Association for Computational Linguistics. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning text-video embedding by watching hundred million narrated video In 2019 IEEE/CVF International Conferclips. ence on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 26302640. IEEE. Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Randy Zhong, Juntong Song, and Tong Zhang. 2024. Ragtruth: hallucination corpus for developing trustworthy retrieval-augmented language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1086210878. Association for Computational Linguistics. OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Feifei Pan, Mustafa Canim, Michael R. Glass, Alfio Gliozzo, and James A. Hendler. 2022. End-to-end table question answering via retrieval-augmented arxiv Preprint arXiv:2203.16714, generation. abs/2203.16714. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. PMLR. Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav 12 Shoham. 2023. In-context retrieval-augmented language models. Trans. Assoc. Comput. Linguistics, 11:13161331. Monica Riedler and Stefan Langer. 2024. Beyond text: Optimizing RAG with multimodal inputs for industrial applications. arXiv preprint arXiv:2410.21943, abs/2410.21943. Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109 126. National Institute of Standards and Technology (NIST). Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, and Chenliang Xu. 2023. Video understanding with large language models: survey. arXiv Preprint arXiv:2312.17432, abs/2312.17432. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledgeIn Proceedings of intensive multi-step questions. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1001410037. Association for Computational Linguistics. Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. 2024a. Elysium: Exploring object-level perception in videos via MLLM. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XXII, volume 15080 of Lecture Notes in Computer Science, pages 166185. Springer. Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. 2024b. Omnivid: generative framework for universal video understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1820918220. IEEE. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Jilan Xu, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. 2024c. Internvideo2: Scaling foundation models for multimodal video understanding. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part LXXXV, volume 15143 of Lecture Notes in Computer Science, pages 396416. Springer. Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 67876800. Association for Computational Linguistics. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 technical report. Preprint, arXiv:2407.10671. Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2024. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv Preprint arXiv:2410.10594, abs/2410.10594. Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D. Plumbley, and Wenwu Wang. 2024. Retrievalaugmented text-to-audio generation. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024, pages 581585. IEEE. Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2024a. Mm-narrator: Narrating long-form videos with multimodal in-context learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1364713657. IEEE. Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, and Kyusong Lee. 2024b. Omagent: multi-modal agent framework for complex video understanding In Proceedings of with task divide-and-conquer. the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1003110045. Association for Computational Linguistics. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. 13 Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. 2024c. Llava-next: strong zero-shot video understanding model. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024. Retrievalaugmented generation for ai-generated content: arXiv preprint arXiv:2402.19473, survey. abs/2402.19473. Beier Zhu and Hanwang Zhang. 2025. Debiasing visionlanguage models for vision tasks: survey. Frontiers Comput. Sci., 19(1):191321."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST"
    ]
}