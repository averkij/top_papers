{
    "paper_title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
    "authors": [
        "Shr-Ruei Tsai",
        "Wei-Cheng Chang",
        "Jie-Ying Lee",
        "Chih-Hai Su",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution. Project page: https://ray-1026.github.io/lightsout/"
        },
        {
            "title": "Start",
            "content": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal Shr-Ruei Tsai Wei-Cheng Chang Jie-Ying Lee Chih-Hai Su Yu-Lun Liu"
        },
        {
            "title": "National Yang Ming Chiao Tung University",
            "content": "5 2 0 2 7 1 ] . [ 1 8 6 8 5 1 . 0 1 5 2 : r Figure 1. Illustration of our diffusion-based outpainting method. Given an input image with incomplete or missing off-frame light sources, existing Single Image Flare Removal (SIFR) models struggle to effectively remove lens flare artifacts due to incomplete context. Our proposed approach accurately predicts and outpaints the off-frame light sources, allowing subsequent SIFR models to perform significantly better. As demonstrated, integrating our outpainting strategy as plug-and-play preprocessing step substantially enhances flare removal quality and visual realism."
        },
        {
            "title": "Abstract",
            "content": "Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, diffusionbased outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as universally applicable plug-and-play preprocessing solution. Project page: https://ray-1026.github.io/lightsout/ 1. Introduction Lens flare, categorized into reflective flares, scattering flares, and lens orbs (backscatter) [26, 34, 71], significantly degrades image quality and negatively impacts computer vision tasks such as object detection and autonomous driving. Traditional flare removal methods [1, 7, 62] relied on handcrafted cues like intensity thresholding or template matching but struggled with complex artifacts. Recent deep learning approaches, such as U-Net [71] and Uformer [11], have achieved substantial improvements due to dedicated datasets. Nevertheless, Single-Image Flare Removal (SIFR) remains challenging, particularly in nighttime scenarios, due to limited real-world paired training data. Recent SIFR advances primarily focus on dataset construction and architectural improvements. Wu et al.[71] introduced flare dataset with captured and simulated images. Flare7k[11] and its enhanced version, Flare7k++[13], provide extensive synthetic and real flare data. Recent methods like Difflare[86] and MFDNet [28] employ sophisticated architectures, such as diffusion models and multi-scale processing, achieving state-of-the-art performance. Despite these advancements, current SIFR methods still struggle when images lack complete views of off-frame light sources. As illustrated in Fig. 2, existing methods significantly degrade in such scenarios, resulting in increased flare artifacts and reduced realism. Metrics like PSNR and LPIPS [83] confirm that complete light source information is crucial for effective flare removal. 1 reflective flare removal [1, 7, 47, 47, 62]. Deep learning approaches were limited by paired data scarcity. Wu et al. [71] developed dataset with U-Net-based [54] approach. Dai et al. [11] expanded this with physics-based simulation [26, 31], later enhanced as Flare7K++ [13]. Qiao et al. [50] used unpaired data with CycleGAN-inspired [88] framework. Recent architectures like MFDNet [34] and Zhou et al.s approach [85] further improved performance. Despite advances, current models degrade significantly when offframe light sources are incomplete or absent, which is the limitation our work addresses. Adapting Diffusion Models. Diffusion models [20, 44, 59] have been adapted for image generation, editing, and restoration [8, 9, 24, 45, 53, 56, 57, 69, 78]. Early applications to inpainting by Sohl-Dickstein et al. [58] and Song et al. [59] showed promise. Guided synthesis approaches [10, 41] offered alternative conditioning strategies but have limitations for outpainting tasks. Several approaches focus on fine-tuning pre-trained diffusion models [55, 74] or text embeddings [14, 63] using single or few reference object images. Additionally, to improve fine-tuning efficiency, researchers have proposed methods that simplify the adaptation process of diffusion models [18, 25, 51]. Low-Rank Adaptation (LoRA) [25] has emerged as an efficient finetuning approach. However, existing diffusion models lack physics-based modeling of optical phenomena [22, 26] and struggle with contextual extrapolation beyond image boundaries. Image Completion and Outpainting. Traditional image completion used low-level cues [24, 19]. GAN-based methods [15, 48] introduced encoder-decoder architectures with innovations including dilated convolutions [27, 79], partial/gated convolutions [37, 81], contextual attention [80], edge maps [17, 43, 72, 73], and semantic segmentation [23, 46]. Outpainting techniques include semantic regeneration networks [66], edge-guided models [36], spiral generation [16], and RCT blocks with LSTM [21, 75]. Recent transformer-based methods [68, 76] still struggle with physically consistent light sources and flare extrapolation. Diffusion-based approaches [38, 39, 41, 53, 56, 65, 70] demonstrate generative capabilities but lack explicit modeling of light sources and optical effects. Image Conditioned Diffusion Models. Diffusion models excel in generative tasks but struggle with fine-grained controllability. ControlNet [82] and IP-Adapter [77] enhance reference-based generation by integrating input image features, while T2I-Adapter [42] conditions on external modalities like sketches and keypoints. InstructPix2Pix [5] enables explicit attribute control via user instructions, and methods [40, 41] refine generation using stochastic differential equations. Building on these advances, our study leverages the conditional techniques by introducing light source constraints as novel conditioning factor. Figure 2. Motivation for outpainting incomplete off-frame light sources. (a) With complete off-frame light sources, state-of-the-art SIFR methods effectively remove lens flare artifacts. (b) In scenarios lacking complete views of off-frame sources, these methods degrade significantly, leaving noticeable artifacts. This highlights the importance of complete light source context, motivating our proposed outpainting solution. To address this, we propose LightsOut, diffusion-based outpainting method designed for accurate completion of missing off-frame light sources. Our approach integrates multitask regression module to predict light source parameters precisely, alongside LoRA [25] fine-tuning of stable diffusion inpainting model explicitly conditioned on these predictions. This ensures that generated content aligns closely with real-world flare and illumination distributions. As shown in Fig. 1, our method seamlessly integrates as plug-and-play preprocessing step with existing SIFR frameworks, significantly enhancing performance in challenging scenarios. Extensive quantitative and qualitative evaluations confirm that LightsOut consistently improves the realism and effectiveness of flare removal across various state-of-the-art methods. Our contributions are summarized as follows: We identify and address key limitation of SIFR methods dealing with incomplete off-frame light sources through specialized decomposition strategy. We propose LoRA fine-tuned diffusion-based model that accurately reconstructs physically consistent off-frame light sources and flare artifacts. We introduce plug-and-play preprocessing framework that universally enhances existing SIFR models without additional retraining. 2. Related Work Lens Flare Removal. Physical lens modifications [1] become ineffective with strong light sources [12]. Early approaches followed flare detection-then-removal pipeline [1, 7, 62], focused on veiling glare removal [52, 60] and 2 Figure 3. Overview of our proposed three-stage pipeline. (a) Light source prediction and conditioning: We introduce multitask regression module to accurately predict off-frame or incomplete light source parameters (positions, radii, and confidences). These predicted parameters guide rendering function to generate the corresponding light source mask. (b) Light source outpainting: Leveraging LoRA fine-tuned diffusion-based inpainting model with light source conditioning, our approach accurately outpaints both missing off-frame light sources and associated flare artifacts, producing visually coherent and realistic results. (c) SIFR boosting: Our generated outpainted images serve as enhanced inputs to existing SIFR methods, significantly improving their performance on previously challenging scenarios with incomplete light source information. The proposed pipeline thus effectively operates as plug-and-play module to boost existing flare removal models. 3. Method Our objective is to resolve the issue of the degradation in existing SIFR models with limited light source information. As illustrated in Fig. 3, we propose three stage method: (a) Given flare-corrupted input image Iin RHW 3 with an incomplete light source, we first define an outpainted region, producing the masked image IM RH 3, where > and > , and its corresponding binary mask {0, 1}H 3. Then, we predict the light source mask ML for the guided condition (Sec. 3.3). (b) After preparing the images and corresponding guided conditions, our outpainting approach completes the scene and reconstructs the full light source, constrained by ML. The resulting outpainted image, denoted as Iout RH 3, is generated as described in Sec. 3.2. (c) The generated image Iout is then processed by SIFR model, yielding the flare-free image Ifree RH 3. Finally, we extract the origin region in Iin from Ifree to get the final flare-free image Ifinal RHW 3 (Sec. 3.4). This threestage pipeline effectively addresses the limitations of existing SIFR models by enhancing light source reconstruction before flare removal. 3.1. Preliminaries Diffusion models. transform simple Gaussian noise distribution into the target data distribution. During the training phase, these models progressively add Gaussian noise to the original data x0. Formally, at each diffusion step t, the noisy sample xt can be written as xt = αtx0 + 1 αtϵ, where ϵ (0, I) is Gaussian noise and αt is variance-scheduling parameter that governs how much noise is added. neural network ϵθ then learns to predict noise ϵ from the noisy sample xt by minimizing: (cid:13)ϵθ(xt, t, c) ϵ(cid:13) (cid:13) 2 = Ex,t,ϵ 2 . where denotes condition- (cid:13) ing signal such as text or masked images. At the inference stage, the model starts from random noise and iteratively denoises the sample until it converges to data point in the target distribution. LoRA fine-tuning. Instead of updating all parameters of the weight matrix Rmn in the denoising U-Net, LoRA introduces low-rank decomposition by injecting small trainable matrix Wi = AiBi, where Ai Rmd, Bi Rdn, with n. The final weight matrix can be written as = Wi + Wi, where only the added is optimized during training, while the original weights Wi remain frozen. 3.2. Image Outpainting for Light Source Existing diffusion-based outpainting models often lack the task-specific adaptation necessary for high-quality light source reconstruction. To address this, we fine-tune the pretrained Stable Diffusion v2 inpainting model, enabling real3 istic and structurally consistent light source generation. Training. To generate the outpainted results with complete light source, we inject LoRA weights and fine-tune them on the given IM and . The loss function is = Ex,t,ϵ,m (cid:13)ϵθ(xt, t, p, M, IM) ϵ(cid:13) (cid:13) 2 2 , (cid:13) (1) where IM, represents text prompt derived from the input image using BLIP-2 [32], and denotes the elementwise product. Inference. After training, the diffusion-based outpainting model aims to predict missing pixels at the corners of the masked region while preserving the integrity of the existing regions in IM. One approach is ensuring that the masked regions are modified by incorporating the intermediate noisy state of the source data from the corresponding timestep in the forward diffusion process. This can be formulated as follows: αtx0 + xmasked t1 = xunmasked t1 xt1 = xmasked = µθ(xt, t) + σθ(xt, t) ϵ, ϵ (0, I), (2) t1 + (1 ) xunmasked t1 . 1 αtϵ, ϵ (0, I), where µθ(xt, t) and σθ(xt, t) are the predicted mean and variance from the denoising model. Since our method builds on the Stable Diffusion inpainting model, the operations in Eq. (2) are performed in latent space. However, as observed in prior works [61, 89], this can still introduce distortions in preserved regions of IM, resulting in inconsistencies in the generated output Iout. To resolve this, instead of combining in latent space, we use the mask to perform alpha composition between Iout and IM directly in the RGB space. It ensures that Iout can be with full recovery on the existing area and smooth transition at the boundary of the generated region. Noise reinjection. Although we composite in RGB space, noticeable discrepancies arise between masked and unmasked regions in the final output. As described in Eq. (2), the denoising steps treat the masked and unmasked regions as separate entities. This can cause inconsistencies and error accumulation across denoising steps. To mitigate this, we adopt noise reinjection, inspired by prior works [40, 64], as formalized in Algorithm 1. This technique reintroduces noise at intermediate steps, allowing the model to re-denoise and better align with the correct distribution. 3.3. Light Source Prediction and Conditioning Our outpainting approach (Sec. 3.2) leverages Stable Diffusions generative capabilities to reconstruct scenes and their light sources. However, it faces limitations, including spatial misalignment, incomplete synthesis, and inconsistent handling of multiple light sources. To address these, we propose multitask regression and light source conditioning modules to enhance outpainting accuracy and realism. 4 Algorithm 1 Noise reinjection Require: Masked image IM, binary mask , Pretrained SD inpainting model ϵθ, Timesteps sequence {t1, t2, . . . , tN }, and Repeat time Ensure: Outpainted image Iout 1: 2: xT (0, I) 3: 1 4: while 0 do 5: 6: ti, tprev ti+1 zt (0, I) 7: xt1 = αt1 (cid:18) xt 1 αtϵθ(xt, IM, M, t) (cid:19) αt (cid:113) 1 αtprev σ2 ϵθ(xt, IM, M, t) + σtzt + if > 0 and > 0 then αtxt1, xt (cid:0) 1 1 αtI(cid:1) Noise reinjection else 8: 9: 10: 11: 12: 13: end if 14: 15: end while 16: Iout = x0 17: return Iout 1 Multitask regression module. We tackle the challenging task of predicting light sources within masked regions of IM, which is more complex than prediction from complete images. Unlike conventional U-Net [54] methods generating full maps, we adopt parameterized regression approach, modeling sources as circular entities to reduce computational cost, stabilize training, and ensure physically meaningful results. The multitask regression module, as shown in Fig. 4, predicts sets of (x, y, r) parameters to environments with multiple light sources, where (x, y) are the planar coordinates, is the radius, and serves as hyperparameter. Since real-world scenes rarely contain exactly light sources, we introduce confidence score to estimate the existence probability of each predicted source. The proposed architecture consists of CNN-based feature extractor Fθ and two specialized MLPs: Gϕ for estimating physical parameters and Hψ for computing confidence scores. This architecture can be formally expressed as: (cid:2)P c(cid:3) = (cid:2)Gϕ(Fθ(Itgt)) Hψ(Fθ(Itgt))(cid:3) , (3) where = x1 x2 y2 y1 r2 r1 . . . xN yN . . . rN . . . RN 3 represents the matrix of predicted light source parameters, and = (cid:2)c1 [0, 1]N 1 denotes the vector of confidence scores for each predicted light source. The optimiza- . . . (cid:3) cN c2 This allows the network to adaptively balance between the losses, leading to more robust learning in either position or confidence predictions. Rendering function. During inference in the multitask regression module, we perform forward pass to obtain the predicted and c. To generate spatial representation of the light sources, we apply an activation function and confidence threshold to suppress unreliable predictions, as illustrated in Fig. 4. The final predicted light source mask ML is computed as: ML(x, y) = (cid:88) (cid:16) ci σ ri (cid:112)(x xi)2 + (y yi)2 (cid:17) , i=1 (8) where ci denotes the after thresholding, and σ() is the sigmoid function. The design of the function helps reconstruct the spatial representation by suppressing unreliable predictions. Light source condition module is designed to guide our outpainting approach mentioned in Sec. 3.2 by leveraging ML, ensuring that light sources are generated in physically plausible locations. The module employs learnable mechanism that conditions the generative process on the provided light source map, allowing explicit control over light placement. During optimization, we enforce an L2 loss: Llight = (cid:13) (cid:13) ML ML (cid:13) 2 (cid:13) , (9) where ML represents the generated light source map. After training, we integrate the model into our outpainting model as constraints to effectively guide the generated content. 3.4. Flare Removal Methods Our method reconstructs incomplete or off-frame light sources, providing accurate illumination context and enhancing flare removal effectiveness. It serves as model-agnostic preprocessing step, seamlessly integrating into existing SIFR pipelines without architectural modifications. 4. Experiments 4.1. Experimental Setup Training dataset. We train on Flare7K [11], following its established synthesis pipeline. Background images from Flickr24K [84] are composited with reflective and scattering flares. To simulate incomplete off-frame scenarios, we apply luminance masks to define realistic regions for outpainting. Evaluation dataset. We evaluate our approach on Flare7Ks test set, comprising 100 real and 100 synthetic images in two scenarios: (1) no light source and (2) incomplete light sources created by shifting the boundary outward by 15 pixels from scenario (1). This setup assesses our methods effectiveness in handling incomplete illumination. 5 Figure 4. Overview of the multitask regression module. Our model performs multitask regression to simultaneously predict two essential components: the physical parameters and the corresponding confidence probabilities for potential light sources. During training, these are supervised by designed multitask loss. At inference, predicted parameters are integrated to generate light source masks ML. tion is designed with the position loss Lpos: Lpos(P, Pgt) = (cid:88) i{x,y,r} smoothL1(P Pgt), (4) in which smoothL1(x) = (cid:40) 0.5x2, 0. if < 1 otherwise, (5) where Pgt represents the ground truth of the physical parameters. To ensure permutation invariance in the matching between predicted and ground-truth parameters, we adopt bipartite matching strategy [6] to obtain the optimal assignment. Since our task involves predicting both the number and spatial distribution of light sources, we introduce an additional confidence loss Lconf to supervise the prediction of existence probabilities. The confidence loss is formulated as the binary cross-entropy between the predicted and the ground truth cgt: Lconf = (cid:88) (cgt,i, log ci + (1 cgt,i) log(1 ci)). (6) To optimize only relevant predictions, we compute the position loss exclusively for pairs where cgt = 1, ignoring cases where cgt = 0 as these correspond to non-existent light sources. Furthermore, to address the inherent uncertainty in predicting both light source locations and their existence, we introduce an uncertainty-aware weighting mechanism[30, 35]. Specifically, we model two learnable parameters, σ1 and σ2, and define the total loss function as: = 1 2σ2 1 Lpos + 1 2σ2 Lconf + log(1 + σ2 1) + log(1 + σ2 2) (7) Table 1. Quantitative evaluation against state-of-the-art diffusion-based outpainting methods. We comprehensively compare our method with baseline and existing diffusion-based inpainting and outpainting approaches. Our solution demonstrates superior performance across diverse scenarios, validating the effectiveness of our diffusion-based strategy in enhancing subsequent flare removal tasks. Setting SIFR Model Method Flare7k Real Flare7k Synthetic PSNR SSIM LPIPS G-PSNR S-PSNR PSNR SSIM LPIPS G-PSNR S-PSNR u h o r t l e o Zhou et al. [87] Flare7k++ [13] MFDNet [28] Zhou et al. [87] Flare7k++ [13] MFDNet [28] Direct input SD-Inpainting [53] SDXL-Inpainting [49] PowerPaint [90] Ours Direct input SD-Inpainting [53] SDXL-Inpainting [49] PowerPaint [90] Ours Direct input SD-Inpainting [53] SDXL-Inpainting [49] PowerPaint [90] Ours Direct input SD-inpainting [53] SDXL-Inpainting [49] PowerPaint [90] Ours Direct input SD-Inpainting [53] SDXL-Inpainting [49] PowerPaint [90] Ours Direct input SD-Inpainting [53] SDXL-Inpainting [49] PowerPaint [90] Ours 26.42 26.84 26.00 26.59 27.09 26.29 27.98 27.01 27.10 28.41 27.04 26.82 25.55 25.28 27.43 26.05 26.42 25.50 25.87 26.29 26.07 28.02 26.99 26.85 28. 26.53 26.48 24.90 24.88 26.94 0.8770 0.8823 0.8774 0.8736 0.8856 0.8337 0.8938 0.8893 0.8814 0.8956 0.8904 0.8886 0.8816 0.8746 0.8940 0.8771 0.8817 0.8773 0.8716 0.8842 0.8333 0.8944 0.8906 0.8802 0. 0.8886 0.8884 0.8807 0.8728 0.8922 0.0445 0.0449 0.0474 0.0559 0.0424 0.0442 0.0421 0.0452 0.0839 0.0397 0.0463 0.0483 0.0535 0.0509 0.0451 0.0480 0.0469 0.0492 0.0595 0.0453 0.0463 0.0431 0.0453 0.0869 0. 0.0457 0.0496 0.0565 0.0535 0.0457 22.04 22.73 21.27 22.66 23.07 21.35 23.63 22.13 22.20 24.15 22.37 22.21 21.23 20.49 22.97 21.88 22.59 21.34 22.12 22.68 21.58 23.81 22.41 22.94 24. 22.07 22.07 20.78 20.10 22.75 20.01 20.04 19.53 20.66 21.12 18.71 21.12 19.65 20.92 22.83 19.94 18.54 16.66 17.11 20.49 19.92 20.14 19.59 20.50 20.80 18.34 21.71 20.12 20.76 22. 20.08 19.09 16.27 16.66 20.43 31.86 31.39 30.80 28.90 31.56 31.28 33.43 31.63 29.24 33.91 33.42 32.42 29.95 27.57 33.54 30.03 30.07 29.61 27.92 30.11 30.23 31.02 30.33 28.00 31. 31.52 31.32 29.63 26.89 31.60 0.9499 0.9518 0.9506 0.9262 0.9534 0.9685 0.9704 0.9675 0.9289 0.9719 0.9721 0.9676 0.9605 0.9181 0.9714 0.9464 0.9483 0.9466 0.9227 0.9504 0.9672 0.9671 0.9657 0.9253 0. 0.9701 0.9672 0.9605 0.9156 0.9696 0.0181 0.0185 0.0196 0.0456 0.0181 0.0151 0.0129 0.0151 0.0890 0.0120 0.0122 0.0145 0.0204 0.0952 0.0119 0.0210 0.0212 0.0227 0.0486 0.0202 0.0160 0.0153 0.0164 0.0923 0. 0.0137 0.0155 0.0208 0.0982 0.0136 25.16 24.62 23.94 23.32 24.74 23.51 25.86 23.87 23.86 26.24 26.43 25.73 23.21 22.06 26.53 24.24 24.01 23.35 22.73 24.06 23.88 24.70 23.73 23.27 25. 25.90 25.49 23.57 21.87 26.17 23.92 23.92 23.23 22.72 24.17 23.59 25.84 24.03 23.62 26.59 26.24 24.93 20.91 20.61 25.89 23.14 23.43 23.12 22.34 23.50 23.70 25.06 23.98 23.12 25. 25.45 24.79 22.27 20.48 25.47 Baseline methods. We compare with state-of-the-art SIFR methods: Zhou et al. [87], Flare7K++ [13], and MFDNet [28] 1. Additionally, our diffusion-based outpainting approach compares with state-of-the-art diffusionbased inpainting and outpainting models, including SDInpainting [53], SDXL-Inpainting [49], and PowerPaint [90]. 4.2. Quantitative Evaluations Comparisons with Existing Methods. Tab. 1 compares our approach with two categories of baselines: state-of-the-art SIFR models and diffusion-based outpainting methods. Additionally, we report G-PSNR and S-PSNR [13], which assess flare removal performance in the glare and streak regions, as also shown in Tab. 1. Our approach significantly boosts performance, notably increasing PSNR from 26.29 dB to 28.41 dB with Flare7K++ [13] on real images without light sources. It also outperforms existing diffusion-based methods on both real and synthetic datasets, effectively addressing cases with incomplete illumination. Evaluation of multitask regression-based light source prediction. Tab. 2 compares our multitask regression-based module with baseline UNet approach using mIoU. Our 1The recent work Difflare [86] does not provide publicly available implementations or pre-trained models, making its inclusion in our comprehensive evaluation infeasible. Table 2. Quantitative evaluation of our proposed light source prediction method. We compare our multitask regression module against baseline UNet-based approach using mIoU scores. These results demonstrate the effectiveness and reliability of our regression-based strategy. Method Flare7K Real Flare7K Synthetic UNet Ours 0.6216 0.6310 0.6563 0. method achieves superior mIoU scores of 0.6310 (real) and 0.6619 (synthetic), outperforming the baselines 0.6216 and 0.6563, respectively. 4.3. Qualitative Evaluations Qualitative impact of off-frame light source completion. Fig. 5 compares the qualitative results of existing SIFR models with and without our method. Without outpainting, these models produce noticeable residual flares and lower realism. Integrating our diffusion-based preprocessing significantly improves off-frame context, enabling more effective flare removal and results closely resembling ground truth. Comparison with standard diffusion-based outpainting methods. We compare our outpainting results qualitatively against other diffusion-based methods in Fig. 6. Standard diffusion methods often generate unrealistic off-frame content 6 Figure 5. Qualitative comparison of lens flare removal results. We compare state-of-the-art SIFR methods (Zhou et al.[87], Flare7K++[13], and MFDNet [28]) alone and combined with our proposed method in two challenging scenarios: (top) no visible light sources and (bottom) incomplete light sources. Integrating our outpainting method (Ours +) significantly improves flare removal quality, producing results closer to ground truth. Table 3. Ablation study on the SIFR model trained for the offframe setting. Although the baseline is explicitly designed and trained for the off-frame scenario, it still underperforms compared to our approach. indicates that the Flare7k++ baseline was retrained using both incomplete and no-light-source data. Method Flare7k Real (no light source) Flare7k Real (incomplete light source) PSNR SSIM LPIPS PSNR SSIM LPIPS Flare7k++ Ours 27.03 28.41 0.8679 0.8956 0.0467 0. 26.18 28.15 0.8650 0.8957 0.0500 0.0409 Table 4. Ablation study on noise reinjection strategy. We quantitatively evaluate the impact of noise reinjection during diffusionbased outpainting compared to baseline without it. Results confirm the effectiveness of noise reinjection in enhancing flare removal performance. Noise regret Flare7k Real Flare7k Synthetic PSNR SSIM LPIPS PSNR SSIM LPIPS 28.28 28.41 0.8949 0.8956 0.0412 0.0397 33.55 33.91 0.9704 0.9719 0.0130 0. hyperparameters identical to the original setup. As shown in Tab. 3, despite being specifically designed and trained for the off-frame scenario, it still lags behind our method in performance. This confirms the effectiveness of our method in handling challenging cases where the light source lies outside the image frame. Effectiveness of noise reinjection. We quantitatively and qualitatively evaluate the impact of noise reinjection during Figure 6. Qualitative comparison of our outpainting results. We qualitatively compare our method with SD-Inpainting [53], SDXLInpainting [49], and PowerPaint [90]. Our method produces more realistic outpainting results, accurately capturing flare artifacts and aligning closely with real-world scenes. due to the lack of explicit conditioning on light sources and flare distributions. In contrast, LightsOut, with the multitask regression module and LoRA-fine-tuned diffusion model, ensures coherent outpainting with accurate flare artifacts and seamless illumination, closely matching the original scene and surpassing baseline methods. 4.4. Ablation Study Effectiveness of the SIFR model trained for the off-frame setting. We retrained the original SIFR model using both incomplete and no-light-source data, while keeping all training Figure 7. Ablation studies. We ablate three components on the light source outpainting results:(a) Incorporating noise reinjection significantly enhances the diffusion models capability to produce realistic, seamless, and visually coherent outpainted regions; (b) Latent space blending tends to produce inconsistent illumination and noticeable artifacts, while blending in RGB space yields results with smoother transitions and improved alignment with real-world intensity distributions. (c) Integrating the proposed conditioning module significantly improves the accuracy and realism of the generated off-frame light sources and flare patterns. Table 5. Ablation study comparing latent space blending versus RGB space blending. We conduct quantitative evaluation of performance disparities between two distinct blending methodologies: latent space blending and RGB space blending. The results confirms that RGB space blending is more effectively boosting the SIFR baselines. Blending Flare7k Real Flare7k Synthetic PSNR SSIM LPIPS PSNR SSIM LPIPS Latent space RGB space 26.91 27.09 0.8859 0.8856 0.0434 0.0424 24.13 31.55 0.7156 0.9526 0.0926 0. diffusion-based outpainting. Tab. 4 shows that excluding noise reinjection notably reduces performance, especially LPIPS scores. Qualitative comparisons (Fig. 7(a)) confirm that noise reinjection significantly enhances visual coherence and realism, justifying its necessity. RGB vs. Latent space blending. We analyze two blending strategies: latent space and RGB space. As shown in Tab. 6, RGB blending consistently outperforms latent space blending. Qualitative comparisons in Fig. 7(b) confirm that RGB blending offers smoother transitions and enhanced visual coherence, making it our preferred choice. Impact of light source condition module. We validate the contribution of our light source condition module by comparing results with and without conditioning (Fig. 7(c)). Without conditioning, the diffusion model struggles to localize offframe sources. Incorporating our module consistently yields more accurate and realistic predictions. Multitask regression for light source prediction. We evaluate our multitask regression against alternatives, including differentiable rendering [33] and weighted-sum loss formulations. Tab. 6 shows that our method consistently achieves superior accuracy, validating its robustness and design effectiveness. Performance gap between SD-Inpainting and proposed method. To analyze the performance gap, we ablate two key Table 6. Ablation on optimization strategies for light source prediction. We compare three strategies: differentiable rendering [33], standard regression, and our multitask regression. Our multitask regression approach achieves superior mIoU, validating its simplicity and effectiveness for predicting off-frame light sources. Optimization Strategy Flare7K Real Flare7K Synthetic Differentiable rendering [33] Regression with weighted loss Multitask regression (ours) 0.5212 0.6081 0.6310 0.5077 0.6577 0.6619 Table 7. Ablation on the performance gap between SDInpainting and ours. To assess the effectiveness of each component, we compare LoRA fine-tuning and our light source conditioning module. The results demonstrate that the combination of all components yields the best performance, indicating that each component is essential. LoRA fine-tuning Condition Module Flare7k Real - - - - PSNR SSIM LPIPS G-PSNR S-PSNR 26.82 27.12 27.06 27.43 0.8886 0.8926 0.8906 0. 0.0483 0.0453 0.0456 0.0451 24.59 24.83 24.84 25.21 26.33 26.59 26.36 26.79 components: (1) LoRA fine-tuning and (2) our light source condition module. As shown in Tab. 7, each yields PSNR gain of 0.30/0.24 dB and an LPIPS drop of 0.0030/0.0027. Combining both achieves the best overall performance, confirming their complementary contributions. 5. Conclusion LightsOut addresses the limitation of SIFR models caused by incomplete light source context. Our diffusion-based outpainting and conditioning modules effectively reconstruct off-frame illumination, significantly enhancing the flare removal performance of existing methods. Limitations. The added outpainting stages introduce computational overhead. Future work could explore end-to-end optimization strategies to reduce this overhead. 8 Acknowledgements. This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2 and 113-2628-EA49-023-. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] CS Asha, Sooraj Kumar Bhat, Deepa Nayak, and Chaithra Bhat. Auto removal of bright spot from images captured against flashing light source. In 2019 IEEE International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics, 2019. 1, 2 [2] Coloma Ballester, Marcelo Bertalmio, Vicent Caselles, Guillermo Sapiro, and Joan Verdera. Filling-in by joint interpolation of vector fields and gray levels. IEEE TIP, 2001. 2 [3] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, 2000. [4] Marcelo Bertalmio, Luminita Vese, Guillermo Sapiro, and Stanley Osher. Simultaneous structure and texture image inpainting. IEEE TIP, 2003. 2 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 2 [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. Endto-end object detection with transformers. In ECCV, 2020. [7] Floris Chabert. Automated lens flare removal, 2014. Technical Report, Department of Electrical Engineering, Stanford University. 1, 2 [8] Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, Chia-Ping Chen, and Chun-Yi Lee. Denoising likelihood score matching for conditional score-based data generation. arXiv preprint arXiv:2203.14206, 2022. 2 [9] Ting-Hsuan Chen, Jie Wen Chan, Hau-Shiang Shiu, Shih-Han Yen, Changhan Yeh, and Yu-Lun Liu. Narcan: Natural refined canonical image with integration of diffusion prior for video editing. NeurIPS, 2024. 2 [10] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021. 2 [11] Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy. Flare7k: phenomenological nighttime flare removal dataset. NeurIPS, 2022. 1, 2, 5, 12 [12] Yuekun Dai, Yihang Luo, Shangchen Zhou, Chongyi Li, and Chen Change Loy. Nighttime smartphone reflective flare removal using optical center symmetry prior. In CVPR, 2023. 2 [13] Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Yihang Luo, and Chen Change Loy. Flare7k++: Mixing synthetic and real datasets for nighttime flare removal and beyond. IEEE TPAMI, 2024. 1, 2, 6, 7, [14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2 [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014. 2 [16] Dongsheng Guo, Hongzhi Liu, Haoru Zhao, Yunhao Cheng, Qingwei Song, Zhaorui Gu, Haiyong Zheng, and Bing Zheng. Spiral generative network for image extrapolation. In ECCV, 2020. 2 [17] Xiefan Guo, Hongyu Yang, and Di Huang. Image inpainting via conditional texture and structure dual generation. In ICCV, 2021. 2 [18] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In ICCV, 2023. 2 [19] James Hays and Alexei Efros. Scene completion using millions of photographs. ACM TOG, 2007. 2 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2 [21] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 1997. 2 [22] L. L. Holladay. The Fundamentals of Glare and Visibility. Journal of the Optical Society of America (1917-1983), 1926. [23] Seunghoon Hong, Xinchen Yan, Thomas Huang, and Honglak Learning hierarchical semantic image manipulaarXiv preprint Lee. tion through structured representations. arXiv:1808.07535, 2018. 2 [24] Chi-Wei Hsiao, Yu-Lun Liu, Cheng-Kun Yang, Sheng-Po Kuo, Kevin Jou, and Chia-Ping Chen. Ref-ldm: latent diffusion model for reference-based face image restoration. NeurIPS, 2024. 2 [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2022. 2, 12 [26] Matthias Hullin, Elmar Eisemann, Hans-Peter Seidel, and Sungkil Lee. Physically-based real-time lens flare rendering. ACM TOG, 2011. 1, 2 [27] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion. ACM TOG, 2017. [28] Yiguo Jiang, Xuhang Chen, Chi-Man Pun, Shuqiang Wang, and Wei Feng. Mfdnet: Multi-frequency deflare network for efficient nighttime flare removal. The Visual Computer, 2024. 1, 6, 7, 13 [29] Glenn Jocher and Jing Qiu. Ultralytics yolo11, 2024. 13 [30] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In CVPR, 2018. 5 9 [31] Sungkil Lee and Elmar Eisemann. Practical real-time lensflare rendering. Computer Graphics Forum, 2013. 2 [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 4, [33] Tzu-Mao Li, Michal Lukaˇc, Michael Gharbi, and Jonathan Ragan-Kelley. Differentiable vector graphics rasterization for editing and learning. ACM TOG, 2020. 8 [34] Xiaoyu Li, Bo Zhang, Jing Liao, and Pedro Sander. Lets see clearly: Contaminant artifact removal for moving cameras. In ICCV, 2021. 1, 2 [35] Lukas Liebel and Marco Korner. Auxiliary tasks in multi-task learning. arXiv preprint arXiv:1805.06334, 2018. 5 [36] Han Lin, Maurice Pagnucco, and Yang Song. Edge guided progressively generative image outpainting. In CVPR, 2021. 2 [37] Guilin Liu, Fitsum Reda, Kevin Shih, Ting-Chun Wang, Image inpainting for Andrew Tao, and Bryan Catanzaro. irregular holes using partial convolutions. In ECCV, 2018. 2 [38] Kuan-Hung Liu, Cheng-Kun Yang, Min-Hung Chen, Yu-Lun Liu, and Yen-Yu Lin. Corrfill: Enhancing faithfulness in reference-based inpainting with correspondence guidance in diffusion models. In WACV, 2025. [39] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022. 2 [40] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting In CVPR, using denoising diffusion probabilistic models. 2022. 2, 4 [41] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2 [42] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In AAAI, 2024. 2 [43] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Qureshi, and Mehran Ebrahimi. Edgeconnect: Structure guided image inpainting using edge prediction. In ICCV Workshops, 2019. 2 [44] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021. [45] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [46] Evangelos Ntavelis, Andres Romero, Iason Kastanis, Luc Van Gool, and Radu Timofte. Sesame: semantic editing of scenes by adding, manipulating or erasing objects. In ECCV, 2020. 2 [47] Andreas Nussberger, Helmut Grabner, and Luc Van Gool. Robust aerial object tracking in images with lens flare. In ICRA, 2015. 2 [48] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016. 2 [49] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 6, 7 [50] Xiaotian Qiao, Gerhard P. Hancke, and Rynson W. H. Lau. Light source guided single-image flare removal from unpaired data. In ICCV, 2021. [51] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Scholkopf. Controlling text-to-image diffusion by orthogonal finetuning. NeurIPS, 2023. 2 [52] Erik Reinhard, Greg Ward, Sumanta Pattanaik, and Paul Debevec. High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics). Morgan Kaufmann Publishers Inc., 2005. 2 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 6, 7, 12 [54] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 2, 4, 13 [55] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2 [56] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings, 2022. [57] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. 2 [58] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 2 [59] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 2 [60] Eino-Ville Talvala, Andrew Adams, Mark Horowitz, and Marc Levoy. Veiling glare in high dynamic range imaging. ACM TOG, 2007. 2 [61] Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, et al. Realfill: Reference-driven generation for authentic image completion. ACM TOG, 2024. 4 [62] Patricia Vitoria and Coloma Ballester. Automatic flare spot artifact detection and removal in photographs. Journal of Mathematical Imaging and Vision, 2019. 1, 10 [79] Fisher Yu and Vladlen Koltun. Multi-scale context arXiv preprint aggregation by dilated convolutions. arXiv:1511.07122, 2015. 2 [80] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Generative image inpainting with contextual attention. In CVPR, 2018. 2 [81] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Free-form image inpainting with gated convolution. In ICCV, 2019. 2 [82] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [83] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 1, 12 [84] Xuaner Zhang, Ren Ng, and Qifeng Chen. Single image reflection separation with perceptual losses. In CVPR, 2018. 5 [85] Shangchen Zhou, Chongyi Li, and Chen Change Loy. LEDNet: Joint low-light enhancement and deblurring in the dark. In ECCV, 2022. 2 [86] Tianwen Zhou, Qihao Duan, and Zitong Yu. Difflare: Removing image lens flare with latent diffusion model. arXiv preprint arXiv:2407.14746, 2024. 1, 6 [87] Yuyan Zhou, Dong Liang, Songcan Chen, Sheng-Jun Huang, Shuo Yang, and Chongyi Li. Improving lens flare removal with general-purpose pipeline and multiple light sources recovery. In ICCV, 2023. 6, 7, 13 [88] Junyan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017. 2 [89] Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, and Gang Hua. Designing better asymmetric vqgan for stablediffusion. arXiv preprint arXiv:2306.04632, 2023. [90] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In ECCV, 2024. 6, 7 [63] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 2 [64] Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, and Hongsheng Li. Beyour-outpainter: Mastering video outpainting through inputspecific adaptation. In ECCV, 2024. 4 [65] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Sindiffusion: Learning diffusion model from single natural image. arXiv preprint arXiv:2211.12445, 2022. 2 [66] Yi Wang, Xin Tao, Xiaoyong Shen, and Jiaya Jia. Widecontext semantic image extrapolation. In CVPR, 2019. 2 [67] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600612, 2004. 12 [68] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restorationn. In CVPR, 2022. 2 [69] Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, and ShinIchi Satoh. Matting by generation. In ACM SIGGRAPH 2024 Conference Papers, 2024. 2 [70] Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, et al. Aurafusion360: Augmented unseen region alignment for referencebased 360deg unbounded scene inpainting. In CVPR, 2025. [71] Yicheng Wu, Qiurui He, Tianfan Xue, Rahul Garg, Jiawen Chen, Ashok Veeraraghavan, and Jonathan Barron. How to train neural networks for flare removal. In ICCV, 2021. 1, 2 [72] Wei Xiong, Jiahui Yu, Zhe Lin, Jimei Yang, Xin Lu, Connelly Barnes, and Jiebo Luo. Foreground-aware image inpainting. In CVPR, 2019. 2 [73] Shunxin Xu, Dong Liu, and Zhiwei Xiong. E2i: Generative inpainting from edge to image. IEEE Transactions on Circuits and Systems for Video Technology, 2020. 2 [74] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In CVPR, 2023. 2 [75] Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, and Shuicheng Yan. Very long natural scenery image prediction by outpainting. In ICCV, 2019. 2 [76] Kai Yao, Penglei Gao, Xi Yang, Jie Sun, Rui Zhang, and Kaizhu Huang. Outpainting by queries. In ECCV, 2022. [77] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [78] Chang-Han Yeh, Chin-Yang Lin, Zhixiang Wang, ChiWei Hsiao, Ting-Hsuan Chen, Hau-Shiang Shiu, and YuLun Liu. Diffir2vr-zero: Zero-shot video restoration with diffusion-based image restoration models. arXiv preprint arXiv:2407.01519, 2024. 2 11 Algorithm 2 Cropping Algorithm function LARGESTRECTANGLE(heights) heights.append(0) stack [1] max area 0 max bbox (0, 0, 0, 0) for 0 to len(heights) 1 do 1: function IMAGECROP(image) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: heights[stack.pop()] stack[1] 1 area if area > max area then max area area max bbox (area, stack[1] + 1, while heights[i] heights[stack[-1]] do (area, left, right, height) 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 1, h) end if end while stack.append(i) end for return max bbox end function max area 0 max bbox [0, 0, 0, 0] heights zeros like(image.shape[1]) for row 0 to image.shape[0] 1 do temp 1 image[row] heights (heights + temp) temp (area, left, right, height) LargestRectangle(heights) if area > max area then max area area max bbox [left, right, (rowheight+1), row] 28: 29: 30: 31: 32: 33: 34: end function end if end for return max bbox batch size of 8 for 20,000 steps. Finally, the Stable Diffusion inpainting network [53] was fine-tuned using LoRA [25] with learning rate of 1 104 and batch size of 8 for 25,000 steps to achieve optimal performance while maintaining computational efficiency. Inference Settings. During outpainting process, we set the number of sampling steps to 50, the guidance scale to 7.0, and perform noise reinjection 4 times. Additionally, we utilize BLIP-2 [32] to automatically generate captions, thereby minimizing human bias. Evaluation metrics. We evaluate flare removal quality using PSNR, SSIM [67], and LPIPS [83], and assess the accuracy of our light source prediction using mean Intersection over Union (mIoU). 12 Figure 8. Comparison of the downstream tasks. The visual results indicate that LightsOut enhances performance on object detection tasks as well. Our approach not only boosts detection confidence scores but also enables the identification of objects previously undetectable due to flare artifacts. A. Appendix Section A.1. Implementation Details Dataset and Preprocessing. We use the benchmark dataset Flare7k [11] for both training and testing. Since the dataset was not originally designed for our tasks, we preprocess it to better suit our requirements. Specifically, to handle off-frame or incomplete light source images and define outpainted regions, we first generate YCbCr luminance masks and then apply an algorithm, formalized in Algorithm 2, to identify the largest rectangular area in each image that excludes the light source. Once the bounding box is obtained, we crop the image on-the-fly during training and inference. The cropped region is then masked with pixel value of 127, defining the area to be outpainted. Training Details. Our framework comprises three independently trained modules, all implemented on an NVIDIA RTX4090 GPU. The components are optimized independently, allowing each module to specialize in distinct subtask and enabling them to collectively improve the systems overall performance when integrated. The multitask regression module was trained with learning rate of 1 104, batch size of 32, for 100 epochs, and we set the number of predicted light sources to 4. The light source condition module was optimized using learning rate of 1105 and Figure 9. Failure Cases. A.2. Downstream Tasks Lens flare artifacts can negatively impact images in various computer vision tasks. To examine how flare removal affects object detection performance, we utilize the pre-trained YOLOv11 [29] detector to compare two scenarios: images directly processed by SIFR models, and images first enhanced by our proposed outpainting approach before being input to SIFR models. Fig. 8 demonstrates that our proposed approach yields improvements in detection accuracy, particularly for objects located in regions previously compromised by flare artifacts. A.3. In-the-Wild Images. We present additional outpainting results on self-collected in-the-wild scenes in Fig. 11, along with flare removal comparisons against baseline methods (Zhou et al.[87], Flare7K++[13], and MFDNet [28]) in Fig. 10. These results highlight our methods effectiveness in outpainting off-frame regions and improving the performance of existing SIFR models, even on challenging in-the-wild images. A.4. Failure Cases The main failure cases exhibit two characteristic features. First, when the overall image brightness is high, the brightness differential between the flare and other parts of the image becomes less pronounced. Second, when the flare occupies relatively large proportion of the entire image. Both scenarios make it difficult to delineate the flare region precisely, even with the integration of our proposed method. A.5. Qualitative Comparisons of Light Source Mask Prediction. Fig. 12 compares the light source predictions from our multitask regression module with those generated by U-Net [54]. The results demonstrate that our proposed module predicts the positions and radii of light sources more accurately, both in single and multiple light source scenarios. Figure 10. Flare removal results for in-the-wild scens. The red boxes indicate flare regions in the images. Our method effectively addresses off-frame light source scenes, which existing SIFR models fail to handle. A.6. Additional Qualitative Comparisons We present extensive supplementary visual evidence to demonstrate the efficacy of our approach. Figures Fig. 13, and Fig. 14 showcase additional flare removal results across diverse imaging conditions. Furthermore, we provide comparative analyses between our outpainting results and those produced by both baseline methods and state-of-the-art diffusion-based inpainting and outpainting techniques in Fig. 15, Fig. 16. These comprehensive visual comparisons substantiate the superior robustness and effectiveness of our proposed methodology across wide spectrum of challenging scenarios. Figure 11. Outpainting results for in-the-wild scens. 14 Figure 12. Qualitative comparisons of light source mask prediction. . 15 Figure 13. Additional Qualitative Comparisons. . 16 Figure 14. Additional Qualitative Comparisons. . 17 Figure 15. Additional Qualitative Comparisons. . 18 Figure 16. Additional Qualitative Comparisons. ."
        }
    ],
    "affiliations": []
}