{
    "paper_title": "Group Sequence Policy Optimization",
    "authors": [
        "Chujie Zheng",
        "Shixuan Liu",
        "Mingze Li",
        "Xiong-Hui Chen",
        "Bowen Yu",
        "Chang Gao",
        "Kai Dang",
        "Yuqiong Liu",
        "Rui Men",
        "An Yang",
        "Jingren Zhou",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models."
        },
        {
            "title": "Start",
            "content": "2025-07-25 Chujie Zheng Shixuan Liu Mingze Li Chang Gao Kai Dang Yuqiong Liu Xiong-Hui Chen Rui Men An Yang Bowen Yu Jingren Zhou Junyang Lin Qwen Team, Alibaba Inc."
        },
        {
            "title": "Abstract",
            "content": "5 2 0 2 4 2 ] . [ 1 1 7 0 8 1 . 7 0 5 2 : r This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixtureof-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has emerged as pivotal paradigm for scaling language models (OpenAI, 2024; DeepSeek-AI, 2025; Qwen, 2025b;a). Through large-scale RL, language models develop the capability to tackle sophisticated problems, such as competition-level mathematics and programming, by undertaking deeper and longer reasoning processes. To successfully scale RL with greater computational investment, the foremost prerequisite is maintaining stable and robust training dynamics. However, current state-of-the-art RL algorithms, exemplified by GRPO (Shao et al., 2024), exhibit severe stability issues when training gigantic language models, often resulting in catastrophic and irreversible model collapse (Qwen, 2025a; MiniMax, 2025). This instability hinders efforts to push the capability boundaries of language models through continued RL training. In this paper, we identify that the instability of GRPO stems from the fundamental misapplication and invalidation of importance sampling weights in its algorithmic design. This introduces high-variance training noise that progressively accumulates with increased response length and is further amplified by the clipping mechanism, ultimately precipitating model collapse. To address these core limitations, we propose Group Sequence Policy Optimization (GSPO), new RL algorithm for training large language models. The key innovation of GSPO lies in its theoretically grounded definition of importance ratio based on sequence likelihood (Zheng et al., 2023), aligning with the basic principle of importance sampling. Additionally, GSPO computes the normalized rewards as the advantages of multiple responses to query, ensuring the alignment between sequence-level rewarding and optimization. Our empirical evaluation demonstrates the significant superiority of GSPO over GRPO in training stability, efficiency, and performance. Critically, GSPO has inherently resolved the stability challenges in the RL training of large Mixture-of-Experts (MoE) models, eliminating the need for complex stabilization strategies, and shows the potential for simplifying RL infrastructure. These merits of GSPO ultimately contributed to the exceptional performance improvements in the latest Qwen3 models. We envision GSPO as robust and scalable algorithmic foundation that will enable the continued advancement of large-scale RL training with language models."
        },
        {
            "title": "2 Preliminaries",
            "content": "Notation In this paper, an autoregressive language model parameterized by θ is defined as policy πθ. We use to denote query and as the query set. Given response to query x, its likelihood under the policy πθ is denoted as πθ(yx) = t=1 πθ(ytx, y<t) where denotes the number of tokens in y. query-response pair (x, y) can be scored by verifier r, resulting in reward r(x, y) [0, 1]. Corresponding authors. 1 Proximal Policy Optimization (PPO) Using samples generated from the old policy πθold, PPO (Schulman et al., 2017) constrains the policy update within proximal region of the old policy through the clipping mechanism. Specifically, PPO employs the following objective for policy optimization (we omit the KL regularization term hereinafter for brevity, as it is not the focus of this paper): JPPO(θ) = xD, yπθold (x) (cid:34) 1 y t=1 (cid:16) min wt(θ) (cid:98)At, clip (wt(θ), 1 ε, 1 + ε) (cid:98)At (cid:35) (cid:17) , (1) where the importance ratio of the token yt is defined as wt(θ) = πθ (ytx,y<t) estimated by another value model, and ε is the clipping range of importance ratios. πθold (ytx,y<t) , the advantage (cid:98)At of yt is The core challenge of PPO in practice lies in its heavy reliance on the value model. Specifically, the value model usually has similar size to the policy model, introducing considerable memory and computational burden. Furthermore, the algorithmic effectiveness hinges on the reliability of its value estimate. While acquiring reliable value model is inherently challenging, ensuring its scalability to longer responses and more complex tasks presents an even greater challenge. Group Relative Policy Optimization (GRPO) GRPO (Shao et al., 2024) bypasses the need for the value model by computing the relative advantage of each response within group of responses to the same query. Specifically, GRPO optimizes the following objective: JGRPO(θ) = xD, {yi}G i=1 πθold (x) (cid:34) 1 i= 1 yi yi t=1 (cid:16) min wi,t(θ) (cid:98)Ai,t, clip (wi,t(θ), 1 ε, 1 + ε) (cid:98)Ai,t (cid:35) (cid:17) , (2) where is the number of generated responses to each query (i.e., the group size), and the importance ratio wi,t(θ) and advantage (cid:98)Ai,t of token yi,t are: wi,t(θ) = πθ(yi,tx, yi,<t) (yi,tx, yi,<t) πθold , (cid:98)Ai,t = (cid:98)Ai = r(x, yi) mean (cid:0){r(x, yi)}G i=1 std (cid:0){r(x, yi)}G i=1 (cid:1) (cid:1) , (3) respectively, where all the tokens in yi share the same advantage as (cid:98)Ai."
        },
        {
            "title": "3 Motivation",
            "content": "The growth in model size, sparsity (e.g., in Mixture-of-Experts models), and response length necessitates large rollout batch size to maximize hardware utilization during RL. To improve sample efficiency, it is standard practice to partition large batch of rollout data into multiple mini-batches for gradient updates. This procedure inevitably introduces an off-policy learning setting, where responses are sampled from an old policy πθold rather than the current policy πθ being optimized. This also explains the necessity of the clipping mechanism in PPO and GRPO, which prevents overly off-policy samples from being involved in gradient estimation. While mechanisms like clipping aim to manage this off-policy discrepancy, we identify more fundamental issue in GRPO: its objective is ill-posed. This problem becomes particularly acute when training large models on long-response tasks, leading to catastrophic model collapse. The ill-posed nature of the GRPO objective stems from misapplication of importance sampling weights. The principle of importance sampling is to estimate the expectation of function under target distribution πtar by re-weighting samples drawn from behavior distribution πbeh: Ezπtar [ (z)] = Ezπbeh (cid:20) πtar(z) πbeh(z) (cid:21) . (z) (4) Crucially, this relies on averaging over multiple samples (N 1) from the behavior distribution πbeh for the importance weight πtar(z) πbeh(z) to effectively correct for the distributional mismatch. In contrast, GRPO applies the importance weight (yi,tx,yi,<t) at each token position t. Since this weight (x, yi,<t), it fails to perform is based on single sample yi,t from each next-token distribution πθold the intended distribution-correction role. Instead, it introduces high-variance noise into the training gradients, which accumulates over long sequences and is exacerbated by the clipping mechanism. We have empirically observed that this can lead to model collapse that is often irreversible. Once the collapse occurs, resuming training is unavailing, even when reverting to previous checkpoint and meticulously πθ (yi,tx,yi,<t) πθold 2 tuning hyperparameters (e.g., the clipping ranges), extending generation length, or switching the RL queries. The above observation suggests fundamental issue in GRPOs design. The failure of the token-level importance weight points to core principle: the unit of optimization objective should match the unit of reward. Since the reward is granted to the entire sequence, applying off-policy correction at the token level appears problematic. This motivates us to forego the token-level objective and explore utilizing importance weights and performing optimization directly at the sequence level."
        },
        {
            "title": "4 Algorithm",
            "content": "4.1 GSPO: Group Sequence Policy Optimization While the token-level importance weight πθ (yi,tx,yi,<t) πθold (yi,tx,yi,<t) is problematic in GRPO, we observe that when we apply importance sampling in the context of language generation: xD, yπθ (x) [r(x, y)] = xD, yπθold (x) (cid:20) πθ(yx) (yx) πθold (cid:21) , r(x, y) (5) the sequence-level importance weight πθ (yx) response sampled from πθold reward and can also serve as meaningful indicator of the clipping mechanism. (yx) has clear theoretical meaning: it reflects how far the (x) deviates from πθ(x), which naturally aligns with the sequence-level πθold Based on this straightforward observation, we propose the Group Sequence Policy Optimization (GSPO) algorithm. GSPO employs the following sequence-level optimization objective: JGSPO(θ) = xD, {yi}G i=1 πθold (x) (cid:34) 1 i=1 (cid:16) min si(θ) (cid:98)Ai, clip (si(θ), 1 ε, 1 + ε) (cid:98)Ai (cid:35) (cid:17) , where we adopt the group-based advantage estimation: (cid:98)Ai = r(x, yi) mean (cid:0){r(x, yi)}G i=1 std (cid:0){r(x, yi)}G i=1 (cid:1) (cid:1) , and define the importance ratio si(θ) based on sequence likelihood (Zheng et al., 2023): si(θ) = (cid:18) πθ(yix) (yix) πθold (cid:19) 1 yi = exp (cid:32) 1 yi yi t=1 log πθ(yi,tx, yi,<t) (yi,tx, yi,<t) πθold (cid:33) . (6) (7) (8) Therefore, GSPO applies clipping to entire responses instead of individual tokens to exclude the overly off-policy samples from gradient estimation, which matches both the sequence-level rewarding and optimization. Note that we adopt length normalization in si(θ) to reduce the variance and to control si(θ) within unified numerical range. Otherwise, the likelihood changes of few tokens can result in dramatic fluctuations of the sequence-level importance ratio, and the importance ratios of responses with different lengths will require varying clipping ranges. We also note that the clipping ranges in GSPO and in previous algorithms (e.g., GRPO) typically differ in order of magnitude due to the distinct definitions of importance ratios. 4.2 Gradient Analysis We can derive the gradient of the GSPO objective as follows (clipping is omitted for brevity): θJGSPO(θ) = θ xD, {yi}G i= πθold (x) (cid:34) 1 i=1 (cid:35) si(θ) (cid:98)Ai = = xD, {yi}G i=1 πθold (x) xD, {yi}G i=1 πθold (x) (cid:34) (cid:34) 1 1 i=1 i=1 (cid:35) si(θ) (cid:98)Ai θ log si(θ) (cid:18) πθ(yix) (yix) πθold (cid:19) 1 yi (cid:98)Ai 1 yi yi t= 3 (9) (10) (cid:35) θ log πθ(yi,tx, yi,<t) . (11) For comparison, the gradient of the GRPO objective is as follows (note that (cid:98)Ai,t = (cid:98)Ai): (cid:34) (cid:35) θJGRPO(θ) = θ xD, {yi}G i= πθold (x) 1 i=1 1 yi wi,t(θ) (cid:98)Ai,t = xD, {yi}G i=1 πθold (x) (cid:34) 1 i=1 (cid:98)Ai 1 yi πθ(yi,tx, yi,<t) (yi,tx, yi,<t) πθold yi t=1 yi t= (12) (cid:35) θ log πθ(yi,tx, yi,<t) . (13) Therefore, the fundamental distinction between GSPO and GRPO lies in how they weight the gradients of the log likelihoods of tokens. In GRPO, the tokens are weighted according to their respective importance (yi,tx,yi,<t) . However, these unequal weights, which can vary among (0, 1 + ε] (for (cid:98)Ai > 0) or weight [1 ε, +) (for (cid:98)Ai < 0), are not negligible, and their impact can accumulate and lead to unpredictable consequences as training progresses. In contrast, GSPO weights all the tokens in response equally, eliminating this instability factor of GRPO. πθ (yi,tx,yi,<t) πθold 4.3 GSPO-token: Token-level Objective Variant In scenarios like multi-turn RL, we may desire finer-grained advantage adjustment at the token level. To this end, we introduce token-level objective variant of GSPO, namely GSPO-token, to allow token-wise advantage customization: JGSPO-token(θ) = xD, {yi}G i=1 πθold (x) (cid:34) 1 i=1 1 yi yi t= (cid:16) min si,t(θ) (cid:98)Ai,t, clip (si,t(θ), 1 ε, 1 + ε) (cid:98)Ai,t where si,t(θ) = sg [si(θ)] πθ(yi,tx, yi,<t) sg [πθ(yi,tx, yi,<t)] , (cid:35) (cid:17) , (14) (15) and sg[] denotes only taking the numerical value but stopping the gradient, corresponding to the detach operation in PyTorch. The gradient of GSPO-token can be derived as: θJGSPO-token(θ) = θ xD, {yi}G i=1 πθold (x) (cid:34) 1 i=1 1 yi yi t= (cid:35) si,t(θ) (cid:98)Ai,t = = xD, {yi}G i= πθold (x) xD, {yi}G i=1 πθold (x) (cid:34) (cid:34) 1 1 i=1 i= yi t=1 si(θ) 1 yi (cid:18) πθ(yix) (yix) πθold (cid:19) 1 yi 1 yi yi t=1 (cid:98)Ai,t θπθ(yi,tx, yi,<t) πθ(yi,tx, yi,<t) (cid:35) (cid:98)Ai,tθ log πθ(yi,tx, yi,<t) . (18) (16) (17) (cid:35) Note that the term πθ (yi,tx,yi,<t) sg[πθ (yi,tx,yi,<t)] has numerical value of 1. Hence, comparing Equation (11) and Equation (18), GSPO-token is numerically identical to GSPO in the optimization objective, clipping condition, and theoretical gradient when we set the advantages of all the tokens in the response yi to the same value (i.e., (cid:98)Ai,t = (cid:98)Ai), while enjoying the higher flexibility of adjusting the advantages per token."
        },
        {
            "title": "5 Experiments and Discussion",
            "content": "5.1 Empirical Results We experiment with cold-start model fine-tuned from Qwen3-30B-A3B-Base, and report the training reward curves as well as the model performance curves on the AIME24 (Pass@1 over 32 samplings), LiveCodeBench (202410-202502, Pass@1 over 8 samplings), and CodeForces (Elo Rating) benchmarks. We take the GRPO algorithm as the baseline, which we have carefully tuned (e.g., the two clipping ranges in Equation 2) to ensure fair comparison. Note that GRPO necessitates the Routing Replay training strategy for the normal convergence of MoE RL, which will be additionally discussed later in 5.3, while GSPO has obviated the need for this strategy. 4 Figure 1 shows that the training with GSPO proceeds stably throughout. We observe that GSPO can deliver continuous performance improvement through increasing the training compute, regularly updating the query set, and extending the generation length. Moreover, GSPO also demonstrates superior training efficiency over GRPO, achieving better training accuracy and benchmark performance under the same training compute and consumed queries. Finally, we have successfully applied GSPO to the RL training of the latest Qwen3 models, strongly proving the efficacy of GSPO in unleashing the power of RL scaling for large language models. Figure 1: Training curves of cold-start model fine-tuned from Qwen3-30B-A3B-Base. GSPO possesses remarkably higher training efficiency than GRPO. 5.2 Curious Observation on Clipping Fractions key distinction of GSPO compared to GRPO is its practice of clipping entire responses rather than individual tokens. Particularly, as shown in Figure 2, we observe difference of two orders of magnitude in the fractions of clipped tokens between GSPO and GRPO (while adjusting the clipping ranges does not alter the disparity in magnitude). However, despite clipping significantly more tokens and consequently using fewer for training (or gradient estimation), GSPO still achieves higher training efficiency than GRPO. This counter-intuitive finding that clipping much larger fraction of tokens leads to superior training efficiency further indicates that GRPOs token-level gradient estimates are inherently noisy and inefficient for sample exploitation. In contrast, GSPOs sequence-level approach provides more reliable and effective learning signal. Figure 2: Average fractions of clipped tokens over the RL training of GSPO and GRPO. 5 5.3 Benefit of GSPO for MoE Training Background Compared to the RL training of dense models, the sparse activation nature of MoE models introduces unique stability challenges. In particular, we found that when adopting the GRPO algorithm, the expert-activation volatility of MoE models can prevent RL training from converging properly. To be specific, after one or more gradient updates, the experts activated for the same response can change significantly. For example, with the 48-layer Qwen3-30B-A3B-Base model, after each RL gradient update and for the same rollout sample, there are roughly 10% of the experts activated under the new policy πθ that are different from those under the old policy πθold. This phenomenon, which becomes more salient in deeper MoE models, makes the token-level importance ratios wi,t(θ) = πθ (yi,tx,yi,<t) (yi,tx,yi,<t) fluctuate drastically and further invalidates them, as discussed in 3 and 4.2, consequently hindering the normal convergence of RL training. πθold Our Previous Approach To tackle this challenge, we previously employed the Routing Replay training strategy. Specifically, we cache the activated experts in πθold and replay these routing modes in πθ when computing the importance ratios wi,t(θ) = πθ (yi,tx,yi,<t) (yi,tx,yi,<t) . In this way, for each token yi,t, πθ(yi,tx, yi,<t) (yi,tx, yi,<t) share the same activated network, so that we can restore the stability of the tokenand πθold level importance ratios and ensure optimization of the consistent activated network across gradient updates. Figure 3 demonstrates that Routing Replay serves as an essential technique in the normal convergence of the GRPO training of MoE models. πθold Figure 3: The Routing Replay strategy plays critical role in the normal convergence of the GRPO training of MoE models. Benefit of GSPO Although Routing Replay enables the GRPO training of MoE models to converge properly, its practice of reusing routing modes incurs additional memory and communication overhead and can also limit the actual capacity of the MoE model. In contrast, as shown in Figure 1, GSPO eliminates the dependency on Routing Replay and is fully capable of computing the importance ratios si(θ) conventionally, converging normally, and optimizing stably. The key insight is that GSPO focuses only on the sequence likelihood (i.e., πθ(yix)) and is not sensitive to the individual token likelihood (i.e., πθ(yi,tx, yi,<t)). Since the MoE model always maintains its language modeling capability, the sequence likelihood will not fluctuate drastically. In summary, GSPO fundamentally resolves the expert-activation volatility issue in MoE models, obviating the need for complex workarounds like Routing Replay. This not only simplifies and stabilizes the training process but also allows the model to leverage its full capacity without artificial constraints. 5.4 Benefit of GSPO for RL Infrastructure Given the precision discrepancies between training engines (e.g., Megatron) and inference engines (e.g., SGLang and vLLM), in practice, we typically use the training engine to recompute the likelihoods of sampled responses under the old policy πθold. However, GSPO uses only sequence-level, rather than token-level, likelihoods for optimization, and intuitively, the former is much more tolerant of precision discrepancies. Hence, GSPO makes it possible to directly use the likelihoods returned by the inference engine for optimization, thereby avoiding the need for recomputation with the training engine. This can be especially beneficial in scenarios like partial rollout and multi-turn RL and in the training-inference disaggregated frameworks."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose Group Sequence Policy Optimization (GSPO), new reinforcement learning algorithm for training large language models. Following the basic principle of importance sampling, GSPO defines importance ratios based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. GSPO demonstrates notably superior training stability, efficiency, and performance compared to GRPO and exhibits particular efficacy for the large-scale RL training of MoE models, laying the foundation for the exceptional improvements in the latest Qwen3 updates. With GSPO as scalable algorithmic cornerstone, we will continue to scale RL and look forward to the resulting fundamental advances in intelligence."
        },
        {
            "title": "References",
            "content": "DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. MiniMax. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. OpenAI. Learning to reason with LLMs, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Team Qwen. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Team Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https: //qwenlm.github.io/blog/qwq-32b/. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning. In Findings of the Association for Computational Linguistics: ACL 2023, 2023. URL https://aclanthology.org/2023.findings-acl.65/."
        }
    ],
    "affiliations": [
        "Alibaba Inc."
    ]
}