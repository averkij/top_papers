{
    "paper_title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
    "authors": [
        "Hoiyeong Jin",
        "Hyojin Jang",
        "Jeongho Kim",
        "Junha Hyung",
        "Kinam Kim",
        "Dongjin Kim",
        "Huijin Choi",
        "Hyeonji Kim",
        "Jaegul Choo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models."
        },
        {
            "title": "Start",
            "content": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion Hoiyeong Jin1 Hyojin Jang1 Junha Hyung1 Kinam Kim1 Dongjin Kim1 Jeongho Kim1 Huijin Choi2 Hyeonji Kim2 1KAIST AI 2SK Telecom Jaegul Choo 5 2 0 2 9 1 ] . [ 1 4 0 5 7 1 . 2 1 5 2 : r {hy.jin, wkdgywlsrud, rlawjdghek, sharpeeee, kinamplify, dj kim, jchoo}@kaist.ac.kr {astehelen, hyeonji}@sk.com Figure 1. InsertAnywhere enables realistic video object insertion by combining 4D scene understanding with diffusion-based video generation. Given user-specified control and reference objects, our method produces geometrically aligned and temporally consistent insertions across complex motions and viewpoints."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing; however, video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusions and lighting effects, which hinders its use in commercial advertising and film postproduction. We present InsertAnywhere, VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with 4D-aware mask generation module that reconstructs scene geometry and propagates user-specified object place- *Equal contribution. ment across frames, maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend diffusion-based video generation model to jointly synthesize the inserted object and its surrounding local variations, including illumination and shading. To enable supervised training, we introduce ROSE++, an illumination-aware synthetic dataset constructed by transforming the ROSE object-removal dataset into triplets of object-removed videos, object-present videos, and VLMgenerated reference images. Through extensive experiments on real-world scenarios, we demonstrate that our framework significantly outperforms existing research and commercial models, achieving VOI quality suitable for production environments. 1 1. Introduction Recent advances in diffusion-based video generation have enabled user-controllable video synthesis and editing, which are increasingly being applied in content creation, advertising, and film post-production. Among these applications, Video Object Insertion (VOI) has emerged as prominent task, allowing new objects to be seamlessly integrated into existing scenes while maintaining spatial, temporal, and lighting consistency. Object insertion at the image level [3, 22, 24, 27, 28] has recently been extended to video manipulation [2, 12, 14, 21, 25], enabling controllable object insertion with temporal coherence. In parallel, commercial multi-modal video generation tools such as KLing [23] and Pika Pro [18] provide advanced video editing capabilities that effectively support VOI, allowing users to synthesize visually compelling content. Together, such research and commercial tools demonstrate strong potential for practical applications, particularly in virtual product placement and other forms of branded video content. However, existing models still do not reliably infer object scale and pose in complex scenes and struggle with challenging cases such as occlusions around the inserted object, which limits their ability to achieve robust VOI at commercial quality. To achieve video object insertion to commercial level of quality, we argue that the main challenges lie in two aspects: 4D-aware object placement and high-fidelity video generation. For accurate placement, both user-guided control and 4D geometric understanding of the scene are required. Since single object image does not provide information about the appropriate scale or pose in the target scene, the user specifies the objects position, size, and posture in the first frame. The inserted object should then be consistently arranged along the camera trajectory throughout the video. Another difficulty arises from handling complex cases such as occlusions caused by dynamic objects within the scene. (see Fig. 1). Beyond placement, the model must faithfully synthesize the objects appearance as well as local variations induced by insertion, such as the lighting condition. While video inpainting methods that edit only within given mask are effective at preserving the original scene content, they struggle to capture subtle local variations surrounding the inserted object. To address these challenges, we introduce VOI framework that combines geometrically consistent object placement with appearance-faithful video generation. For accurate placement, we propose 4D-aware mask generation module that first reconstructs the input video into 4D scene representation and inserts the object according to the user-specified position and scale in the first frame. Using the flow fields around the inserted object, we propagate its trajectory to the remaining frames and place the object accordingly. Subsequently, the 4D scene representation is reprojected and segmented to produce geometry-aware, temporally coherent mask sequence. Building on this placement, our key idea for video generation is to leverage the priors of advanced image-based object insertion models. The edited first frame serves as an anchor, and the video generation model propagates the insertion across the remaining frames. However, because the mask sequence includes only the object region, video inpainting models that are restricted to editing inside the mask, cannot fully account for local variation around the object, such as adjusted lighting. To overcome this limitation, we construct ROSE++, an extension of the existing ROSE dataset [15] for video object removal. ROSE++ augments each pair of videos with and without the object into triplet by additionally including the corresponding object image obtained via visionlanguage model. Using this dataset, we train our model by inverting the object removal task into an insertion task, enabling it to jointly synthesize the inserted object and the associated local variations for faithful video object insertion. Our contributions are summarized as follows: We propose mask generation module that propagates object masks over all frames via 4D representation from user specified position and scale in the first frame, producing reliable masks even under occlusions, supported by an intuitive GUI. We extend the video inpainting model so that, in addition to inserting the object, it also models local variations around the object, such as lighting conditions. We build ROSE++ by extending the ROSE dataset with VLM based object retrieval to form video triplets tailored for video object insertion, enabling training by inverting removal into insertion. We demonstrate that InsertAnywhere achieves geometrically plausible and visually coherent object insertions across diverse real-world scenarios, including complex camera motions and occlusion, and significantly outperforms existing commercial generative tools. 2. Related Work Inpainting Video Generation. Advances in high-quality video generation have led to rapid progress in video editing research, enabling highly customizable video models conditioned on various modalities [7, 9]. Recent approaches incorporate inputs such as masks, reference images, poses, and depth maps to achieve controllable video synthesis, and several large-scale datasets have been constructed to support such tasks. Video Object Insertion. number of studies have explored inserting objects into videos. At the computer vision level, existing work [2] demonstrated natural objectscene 2 integration in 2D settings. More recent video-level approaches include [3, 21, 32]. However, these methods rely on masking an entire spatial region and regenerating only that part of the frame, which prevents them from handling real-world occlusions. GenProp [14] encodes appearance changes introduced in the first edited frame and propagates them to subsequent frames using Region-Aware Loss. Yet, because it does not explicitly account for visibility changes over time, it struggles in cases where the inserted object becomes partially occluded or reappears behind other scene elements. As result, these approaches fail to produce consistent and realistic insertions under real-world occlusion conditions. In contrast, the object insertion problem addressed in this work focuses on generating static object that remains visually coherent even when occlusions occur in complex realworld scenes. 3. Method 3.1. Overview Figure 2 depicts the overall architecture of our framework, which consists of 4D-aware mask generation (Section 3.2) and video object insertion (Section 3.4). User controllability is crucial for deploying Video Object Insertion (VOI) model in commercial applications. Effective insertion requires the user to precisely control the real-world scale and exact position of the reference object, necessitating that the VOI model be accurately conditioned on this user control (e.g., the target mask). Since requiring users to generate masks for every frame is impractical, the ideal operational setting involves the user providing only the initial object mask, which the model must then propagate coherently across the remaining frames. This requirement poses technical challenge, as it demands deep 4D understanding of the scene, including accurate prediction of camera dynamics from monocular video and robust occlusion handling. Prior approaches struggle with these combined requirements. Inpainting-based models often fail to achieve full scene coherence because they are restricted to editing only the area within the provided mask, leaving the scene outside untouched and often failing to generate realistic shadows or reflections. Furthermore, models leveraging larger masks often lead to reduction in user controllability. Conversely, mask-free models which generally attempt to generate entire frames, typically fall short on the necessary finegrained controllability required for precise user-specified object placement and scale. Our framework is explicitly designed to meet both the controllability and scene coherence requirements. The twostage approach begins with the Mask Generation Stage: we provide user-friendly tool that allows the operator to easily navigate 3D-reconstructed scene (typically corresponding to the first frame) to precisely place the object and determine its scale. Based on the underlying 4D spatio-temporal scene representation, the placed object is then coherently propagated and reprojected to generate the full mask sequence. This 4D-aware mask generation ensures temporally consistent object mask sequence that accurately encodes the realistic geometry, perspective, and occlusion relations of the scene. In the video object insertion stage, the source video, object image, and the generated mask sequence are provided as input to the VOI model to synthesize the final photorealistic video. In addition, we leverage pretrained image object insertion model as strong prior, as it provides highfidelity, spatially accurate insertion results. 3.2. 4D-aware Mask Generation This stage generates controllable and 4D-aware mask sequence based on the input scene video and reference object. The process executes three main steps: (1) 4D Scene Reconstruction: The scene video is first reconstructed into robust 4D geometry and motion representation. (2) UserControlled Placement: The user interactively places and scales the reference object within this reconstructed 4D space. (3) Temporal Propagation and Reprojection: The object is temporally propagated using the scene flow, and then reprojected back to each video frame to form the final mask. This pipeline ensures that the resulting mask sequence accurately aligns with the scenes camera motion, geometry, and temporal continuity. 3.2.1. 4D Scene Reconstruction The reconstruction stage aims to recover temporally consistent 4D representation of the input video by jointly estimating per-frame geometry and camera motion. Our reconstruction stage builds on the 4D scene modeling paradigm introduced in Uni4D [30], which demonstrates that temporally consistent 4D representation can be constructed from single video by orchestrating multiple pretrained vision models. Instead of training dedicated network for 4D understanding, Uni4D integrates predictions from depth estimation [17], optical flow [10], camera pose recovery, and segmentation models [4, 11, 13] to infer coherent scene geometry and motion over time. 3.2.2. User Controlled Object Placement The input object image Iobj is converted into 3D point cloud = {yj R3} using pretrained single-view reconstruction network.[29] This point cloud is initially defined in its own local coordinate system and is subsequently aligned to the reconstructed scene through rigid transformation: = sobj Robj yj + tobj, (1) 3 Figure 2. InsertAnywhere is two-stage VOI framework that uses 4D scene reconstruction to generate user controllable and geometrically consistent mask, which then conditions diffusion model fine-tuned on ROSE++ to synthesize illumination-aware object insertions. where Robj SO(3) denotes the rotation matrix, tobj R3 is the translation vector, and sobj represents global scale factor. These parameters are controlled through userinteractive placement interface demonstrated in Figure 2 which enables precise adjustment of the objects orientation, position, and size relative to the 4D scene geometry. 3.2.3. Scene Flow based Object Propagation In reconstructed 4D scene, an inserted object must remain synchronized with nearby objects over time. straightforward approach is to keep the object fixed at its initial position throughout the sequence. However, this assumption often fails in real-world scenarios where physical interactions occur between objects. For instance, if an apple rests on plate and person lifts the plate, the apple should naturally move along with it. To handle such physical interactions, we correct the objects motion using the scene flow. To estimate the overall motion field of the scene, we employ SEA-RAFT [26] to compute the dense optical flow field Vtt+1 of the scene. We then identify the nearest 3D points around the inserted object in the first frame of the reconstructed 4D scene. These points are projected onto the 2D image plane and matched with the corresponding starting locations of the optical flow. The 2D flow vectors are subsequently lifted back into 3D, yielding set of 3D motion vectors that approximate the local scene flow around the object. We compute the average of these 3D motion vectors and update the objects centroid accordingly. The objects motion across frames is thus propagated using j,t+1 = y j,t + Vtt+1(y j,t), (2) thereby refining its temporal alignment within the reconstructed 4D scene. This aggregation strategy yields stable and physically meaningful estimate of how the inserted objects motion is driven by the local scene dynamics. 3.2.4. Camera-Aligned Reprojection After the object is embedded within the reconstructed 4D scene, each 3D point j,t is projected onto the image plane of frame using the estimated camera intrinsics and extrinsics Pt = [Rt tt]: (cid:0)Rt j,t + tt (cid:1) . (3) uj,t vj,t By projecting and rasterizing all visible points, we obtain the objects silhouette for each frame, generating synthetic object-inserted video sequence { ˆIt}T t=1. This reprojection step accounts for camera motion, parallax, and occlusion, producing geometrically consistent renderings of the inserted object from real camera viewpoints. 4 object removal, not insertion. To overcome this limitation and complete the four components required for VOI training, we perform VLM-based object retrieval process that generates corresponding object images conditioned on the scene context. For each video, we first sample frames containing the target object and extract their object regions using masking operation. These multi-view object crops are provided to Vision-Language Model (VLM) [1] along with text prompt instructing it to generate multiple white-background images of the same object. The VLM produces candidate object images, which are then ranked using DINO-based similarity metric with respect to the original object in the source video: sk ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 DINO(ˆok, fj), (4) where ˆok denotes the k-th generated object image and fj represents the j-th sampled frame from the same video. The final reference image is selected as the candidate with the highest similarity score: = arg max ˆok sk. (5) This process ensures that the selected object image remains contextually aligned with the original video while preventing inconsistencies arising from input discrepancies between the training and inference stages. Previous approaches directly cropped objects from random video frames to use as reference images, which caused the model to overfit to video-specific content and often led to copyand-paste artifacts. In contrast, our VLM-based object generation maintains consistent input conditions across training and inference, producing contextually coherent and visually clean object images. 3.4. Video Object Insertion In this work, we fine-tune video diffusion model [9] using LoRA [6] module for efficient adaptation. This approach preserves the pretrained video generation capability while improving adaptability to object-insertion scenarios. Since current image inpainting models [31] demonstrate stronger visual reconstruction capability than video models, the first frame is generated with high fidelity to serve as reliable visual reference. The visual information from this frame is then effectively propagated throughout the subsequent video generation process, maintaining consistent color, texture, and lighting of the inserted object. Similar to the approach in [5], our method first generates the initial frame to determine the objects appearance and lighting conditions, and then synthesizes the entire video based on this frame. Figure 3. ROSE++ dataset generation pipeline: reference images are produced by the VLM-based object retrieval process. [1] To extract accurate object regions, the synthesized video { ˆIt}T t=1 is processed by SAM2 [20], producing temporally aligned binary mask sequence {Mt}T t=1, where each mask distinguishes object and background regions. This yields geometry-aware and temporally consistent spatial condition for the subsequent video generation stage. 3.3. ROSE++ Dataset Training the Video Object Insertion (VOI) task demands specialized dataset containing four components: an objectremoved video, an object-present video, corresponding object mask video, and the reference object image. However, acquiring such comprehensive dataset is exceptionally challenging, and existing datasets contain only partial subset of these components. To address this limitation, we propose constructing unified, synthetic illumination-aware dataset, ROSE++, derived from the ROSE dataset [15], by utilizing VLM-based object retrieval process. The original ROSE dataset is synthetic collection designed for object removal, where each scene is rendered to eliminate not only the target object but also its associated side effects such as shadows, reflections, and illumination changes. Accordingly, each sample consists of an object-present video, the corresponding object mask, and an object-removed video in which both the object and its optical side effects are erased. We adapt this dataset to our VOI setting by transforming its structure to construct ROSE++. the object-removed video serves as the source, and the object-present video acts as the target, establishing training pairs suitable for supervised object insertion. However, the original ROSE dataset lacks explicit object images because it was initially designed for As illustrated in Figure 3, 5 Moreover, leveraging the newly introduced ROSE++ dataset, the model can effectively learn illumination and shadow-aware behaviors that were previously difficult to capture. Previous datasets relied primarily on selfsupervised inpainting training with mask-based reconstruction, where models learn to fill masked regions using surrounding pixels. Such training, however, provides no explicit supervision for lighting or shadow consistency, making it impossible for models to capture illumination In contrast, changes caused by newly inserted objects. ROSE++ augments each object-removal pair into triplet by adding corresponding object image generated via visionlanguage model, enabling consistent observation of geometric and illumination relationships. This allows the model to learn not only to restore spatial alignment but also to synthesize soft shadows, lighting variations, and material-dependent shading, enabling natural and coherent object insertion in real-world videos. 4. Experiments 4.1. Experimental Setup Evaluation Dataset. We introduce VOIBench, new benchmark designed to rigorously assess video object insertion. The dataset consists of 50 video clips covering broad spectrum of settings, such as indoor scenes, outdoor environments, and natural landscapes, with each video containing pairs of two objects. We crawled contextually relevant objects for each scene. As result, the total number of generated videos used for evaluation is 100. Baselines and Evaluation Metrics. Since currently available state-of-the-art VOI methods are commercial generative tools, we include comparative evaluation using the most capable publicly accessible methods, Pika-Pro and Kling. We evaluate three aspects: (1) Subject Consistency using CLIP-I [19] and DINO-I [16], (2) Video Quality using VBench metrics [8] (Image Quality, Background/Subject Consistency, Motion Smoothness), and (3) Multi-View Consistency using VBench metrics [8], which evaluates how reliably the inserted object is maintained across viewpoints when occlusions occur with other scene objects. 4.2. Qualitative Result Fig. 4 provides qualitative comparison between our method and existing baselines. For the drawer case, both baselines show low object fidelity. There is noticeable color difference compared to the reference drawer, and despite having three drawers, the model generated four. Furthermore, in the case of the pepper shaker below, Pika Pro still shows low fidelity for the object, generating it at very large size, while Kling fails to handle the occlusion of the pepper shaker moving from the third to the fourth column, where it appears in front of the hand before moving behind it. In contrast, thanks to our proposed 4D-aware mask, the model remains robust even in challenging occlusion scenarios. The high fidelity of the object in the generated video is ensured by the first-guided video generation, resulting in accurate video object insertion. 4.3. Quantitative Result To assess subject consistency, we select 10 evenly spaced frames from each video and evaluate how closely they match the corresponding reference subjects, where we evaluate the metrics using subject-masked regions only. As shown in Table 3, InsertAnywhere achieves the highest scores in CLIP-I [19] and DINO-I [16]. Furthermore, our method demonstrates clear advantages in overall video quality, indicating that our model not only provides 4Daware mask but excels across full-video quality metrics. In contrast, Kling and Pika-Pro often introduce undesirable changes to the background of the original video. Additionally, through Multi-View Consistency, we verify that our 4D-aware mask effectively captures spatio-temporal structure, preventing occlusion-related inconsistencies. 4.4. Ablation Study To demonstrate the effectiveness of each component in our framework, we conducted an ablation study, as illustrated in Fig. 5. Each row presents the generated video under different configurations, highlighting how individual modules influence the final result. When the mask sequence is generated solely from the camera trajectory without incorporating 4D geometric understanding (i.e., simply masking the region of the object will be placed), the model fails to maintain the original video when occlusion occurs. As shown in the orange box in the second row of Fig. 5, we observe that information such as the position of the persons arm and scarf compared to the source video. In the third row of Fig. 5, introducing our 4D geometry-aware mask sequence significantly addresses such occlusion issues, but even with an improved mask, the objects fidelity remains low. Adding the first frame inpainting strategy enhances the preservation of the objects identity, resulting in more coherent initial appearance. However, temporal inconsistencies still arise after occlusion events, manifesting as fluctuations in attributes like scarf direction or surface texture. Finally, our video generation model, fine-tuned with the proposed ROSE++ dataset, naturally generates lighting conditions, such as shadows, that arise from the object insertion. Combining all components results in geometrically accurate videos with high fidelity to the object across the entire sequence. LoRA Finetuning with ROSE++ dataset. We fine-tuned our video generation model with the proposed ROSE++ 6 Subject Consistency VBench Method CLIP-I DINO-I Background Consistency Subject Consistency Motion Smoothness Imaging Quality Multi-View Consistency Pika-Pro [18] Kling [23] Ours 0.4940 0.6349 0.8122 0.3856 0.5028 0.5678 0.9080 0.9335 0. 0.8720 0.9494 0.9520 0.9889 0.9940 0.9916 0.6546 0.7069 0.7101 0.5123 0.5439 0.5857 Table 1. Quantitative comparisons with baseline methods. Figure 4. Qualitative comparison between closed-source commercial video generative tools and ours. Best viewed when zoomed in. dataset using LoRA [6], and observed that the model learned to better capture local photometric variations such as illumination changes and shadow formation. Fig. 6(a) illustrates illumination adaptation under an openclosed door sequence. Before fine-tuning, the brightness of the inserted paper bag remains almost unchanged regardless of whether the door is open or closed, indicating that the model fails to reflect ambient lighting variations. After LoRA fine-tuning, however, the illumination on the object dynamically responds to the scene lighting, the bag appears brighter when sunlight enters through the open door and darker when the door is closed, resulting in more physically consistent rendering. Fig. 6(b) demonstrates the improvement in shadow generation. Before LoRA fine-tuning, the model fails to infer that shadows should naturally extend beyond the object 7 Figure 5. Ablation results. We performed an ablation study by sequentially adding each of the proposed components. Best viewed when zoomed in. Methods Oursrandom Ours Multi-View Consistency 0.5295 0.5857 Table 2. Multi-view Consistency evaluation. Models trained with random-frame references vs. our VLM-based generation. mask, resulting in behavior similar to simple inpainting model and causing the inserted object to appear unnaturally detached from the scene. After training with ROSE++, the model learns to infer the global light direction and intensity, allowing realistic shadows to be synthesized on surrounding surfaces outside the mask. These results confirm that ROSE++ based LoRA fine-tuning [6] enhances the models ability to capture photometric consistency for realistic video object insertion. VLM-based object retrieval. To prevent copy-and-paste artifacts arising from discrepancies between the training and inference inputs, we generate objects using VLMbased object generation module  (Fig. 3)  . To verify that our model does not implicitly rely on objects appearing inside the training videos, we conducted an additional experiment by training with object crops directly taken from random frames of the video and compared the results with our model. If the model were to depend on such object directly from video, copy-and-paste behavior would occur, leading to inconsistent object appearances across different viewpoints. As shown in Table 2, our approach achieves significantly higher scores, demonstrating its robustness against such issues. Figure 6. Ablation result of finetuning with ROSE++ datasets. Best viewed when zoomed in. 5. Conclusion In this work, we proposed InsertAnywhere, novel framework for realistic Video Object Insertion (VOI) that integrates 4D scene geometry with diffusion-based video synthesis. Our method incorporates 4D-aware mask generation module that reconstructs scene geometry and consistently propagates user-specified object placement across time, enabling accurate spatial alignment and reliable occlusion handling. We further extend diffusion-based video generator to synthesize both the inserted object and its surrounding illumination and shadow variations, producing temporally stable and visually coherent results. To support supervised training, we developed ROSE++, an illumination-aware synthetic dataset created by reformulating the ROSE object removal setting into an insertionbased triplet construction using VLM-derived reference images. Extensive experiments demonstrate that InsertAnywhere achieves geometrically consistent and photometri8 cally realistic insertions across diverse real-world scenarios, outperforming commercial methods. Beyond its empirical effectiveness, our framework shows promising object insertion quality for practical applications such as virtual product placement and branded content generation."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5 [2] Chen Bai, Zeman Shao, Guoxiang Zhang, Di Liang, Jie Yang, Zhuorui Zhang, Yujian Guo, Chengzhang Zhong, Yiqiao Qiu, Zhendong Wang, et al. Anything in any scene: Photorealistic video object insertion. arXiv preprint arXiv:2401.17509, 2024. 2 [3] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level imIn Proceedings of the IEEE/CVF conage customization. ference on computer vision and pattern recognition, pages 65936602, 2024. 2, 3 [4] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13161326, 2023. 3 [5] Yuwei Guo, Ceyuan Yang, Anyi Rao, Chenlin Meng, Omer Bar-Tal, Shuangrui Ding, Maneesh Agrawala, Dahua Lin, and Bo Dai. Keyframe-guided creative video inpainting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1300913020, 2025. 5 [6] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5, 7, [7] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. 2 [8] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. 6 [9] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 5, 1 [10] Nikita Karaev, Yuri Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudolabelling real videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 60136022, 2025. 3 [11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 3 [12] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. arXiv preprint arXiv:2403.14468, 2024. 2 [13] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. [14] Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, et al. Generative video propagation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1771217722, 2025. 2, 3 [15] Chenxuan Miao, Yutong Feng, Jianshu Zeng, Zixiang Gao, Hantang Liu, Yunfeng Yan, Donglian Qi, Xi Chen, Bin Wang, and Hengshuang Zhao. Rose: Remove objects with side effects in videos. arXiv preprint arXiv:2508.18633, 2025. 2, 5 [16] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 6 [17] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv preprint arXiv:2502.20110, 2025. 3 [18] Pika. Pika additions. https : / / pika . art / pikadditions, 2025. Accessed: 2025-11-14. 2, 7 [19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [20] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 5 [21] Nirat Saini, Navaneeth Bodla, Ashish Shrivastava, Avinash Ravichandran, Xiao Zhang, Abhinav Shrivastava, and Bharat Singh. Invi: Object insertion in videos using off-the-shelf diffusion models. arXiv preprint arXiv:2407.10958, 2024. 2, 3 [22] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. ObjectIn Prostitch: Object compositing with diffusion model. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1831018319, 2023. 2 [23] Kling AI Team. Klingai, 2025. 2, 9 [24] Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free object insertion in images with pretrained diffusion models. arXiv preprint arXiv:2411.07232, 2024. 2 [25] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video obIn Proceedings ject insertion with precise motion control. of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 2 [26] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple, efficient, accurate raft for optical flow. In European Conference on Computer Vision, pages 3654. Springer, 2024. 4 [27] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion. In European Conference on Computer Vision, pages 112129. Springer, 2024. 2 [28] Daniel Winter, Asaf Shul, Matan Cohen, Dana Berman, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectmate: recurrence prior for object insertion and subject-driven generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1628116291, 2025. [29] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. 3 [30] David Yifan Yao, Albert Zhai, and Shenlong Wang. Uni4d: Unifying visual foundation models for 4d modeling from In Proceedings of the Computer Vision and single video. Pattern Recognition Conference, pages 11161126, 2025. 3 [31] Yongsheng Yu, Ziyun Zeng, Haitian Zheng, and Jiebo Luo. Omnipaint: Mastering object-oriented editing via disentangled insertion-removal inpainting. arXiv preprint arXiv:2503.08677, 2025. 5 [32] Qi Zhao, Zhan Ma, and Pan Zhou. Dreaminsert: Zero-shot image-to-video object insertion from single image. arXiv preprint arXiv:2503.10342, 2025. 3 10 InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion"
        },
        {
            "title": "Supplementary Material",
            "content": "Implementation Detail. Our video generation is performed with Wan2.1-VACE-14B [9], diffusion-based video generation model fine-tuned with LoRA to adapt to our insertion-specific domain. The model is fine-tuned for 5,000 iterations with learning rate of 1104 and LoRA rank of 128. The entire training process, conducted on single NVIDIA H200 GPU, takes approximately 40 hours. During inference, we use 50 denoising steps to balance temporal smoothness and spatial fidelity. All videos are trained and generated at spatial resolution of 832 480 with 81 frames per clip. Ablation Study. Table 3 reports the quantitative performance corresponding to the qualitative ablation results in Fig. 5. We refer to the second, third, and fourth rows of Fig. 5 as config. (a), (b), and (c), respectively. With the proposed 4D-aware mask, config. (b) achieves higher VBench scores than config. (a), quantitatively demonstrating the benefit of preserving ground-truth video information such as occlusions, rather than erasing most of the region where the object will be placed in the source video. Furthermore, by exploiting the ability of image models to more faithfully reflect the reference image, we introduce first-frame inpainting strategy that accurately performs object insertion in the initial frame. As shown by config. (c) in Table 3, this leads to significant improvement in subject consistency metrics (i.e., CLIP-I and DINO-I). Finally, our video generation model fine-tuned on the proposed ROSE++ dataset, constructed by inverting an object removal dataset into an object insertion dataset, more accurately generates local variations such as shadows by the inserted object, resulting in substantial improvement in VBench scores. Fig. 7 visualizes the difference in 4D-aware mask generation with and without the proposed scene-flow-based object propagation in Section 3.2.3 of the main paper. Although both variants share the same first frame, the version with static object mask (second row) fails to capture the objects motion on the moving cart, resulting in misaligned masks. In contrast, when scene-flow-based object propagation is applied, the object is naturally placed on top of the cart across time. User Study We conducted user study with 20 participants by randomly sampling 10% of the videos in the test set. For each video, we prepared six evaluation questions, and participants were asked to choose the best-performing model among three candidates (ours and two baselines) for each question. To avoid positional bias, the order of the three Figure 7. Comparison before and after the application of Section 3.2.3 results shown in each candidate was randomly shuffled for every participant. The evaluation questions are as follows: Object Realism. Assesses whether the inserted object exhibits physically plausible geometry, scale, and appearance without distortions or implausible configurations. Lighting Consistency. Measures how well the inserted object matches the illumination and shading conditions of the surrounding scene. Occlusion Integrity. Evaluates whether occlusion relationships with surrounding objects are handled correctly, without causing disappearance or distortion of existing scene elements. ObjectVideo Consistency. Determines how consistent the inserted object is with visually or semantically related objects present in the original video. Background Preservation. Checks whether regions unrelated to the inserted object remain faithful to the original video without unnecessary alterations or artifacts. Overall Naturalness. Captures the overall perceptual realism of the result, including the absence of visible artifacts and the users preferred choice for real-world deployment. Table 4 presents user preference percentages computed over 10% of test videos and 20 participants. For each criterion, participants selected the method they judged to be the best, and the percentages represent the proportion of votes each method received for that specific criterion. Higher percentages indicate stronger perceived quality and user preference. Retrieval Prompt. We use the prompt described in Fig. 8 to retrieve objects for constructing the ROSE++ dataset. For each video, we use this prompt to generate multiple object candidates with consistent visual appearance and select as 1 Subject Consistency VBench Method CLIP-I DINO-I Background Consistency Subject Consistency Motion Smoothness Imaging Quality Multi-View Consistency Config. (a) Config. (b) Config. (c) Ours 0.7585 0.7532 0.7880 0.8122 0.4190 0.3861 0.5135 0.5678 0.9206 0.9232 0.9175 0.9429 0.9316 0.9380 0.9290 0.9520 0.9910 0.9913 0.9911 0.9916 0.6175 0.6298 0.6318 0. 0.5238 0.5308 0.5436 0.5857 Table 3. Quantitative results of the ablation study. Method (%) Object Realism Lighting Consistency Occlusion Integrity Object-Video Consistency Background Preservation Overall Naturalness Pika-Pro [18] Kling [23] Ours 1.82 19.09 79.09 3.64 25.45 71. 0.00 13.33 86.67 1.82 21.82 76.36 2.73 16.36 80.91 4.55 24.55 70.00 Table 4. User study preference percentages across six evaluation criteria for three compared methods. the target object, leading to the removal of existing items or people. In Fig. 10, despite providing explicit positional instructions through the prompt, Kling places the object in physically inconsistent regions. Pika also struggles with Fig 10, producing unrealistic object scales or inconsistent appearance compared to reference objects. Moreover, Pika exhibits similar swapping artifacts in the second example of Fig 9. Therefore, these qualitative results show the structural limitations of text-driven commercial models such as Pika and Kling. They lack the spatial reasoning ability required to correctly interpret scene geometry and objectscene relationships. As result, even when prompts include detailed spatial and contextual cues, the models frequently misplace the inserted object, produce unrealistic geometry, or replace existing items in the scene. Using the proposed 4D-aware mask generation, our method enables natural object insertion even under complex occlusion scenarios. Compared to baseline methods, our first-frameguided video generation achieves substantially higher object fidelity and closer adherence to the reference object. The exact prompts used for each baseline are provided in the figure captions. Figure 8. Prompt for object retrieval in constructing the ROSE++ dataset the reference image the one with the highest DINO score. Additional Qualitative Comparisons. Fig. 9 and 10 present additional qualitative results of insertion in realworld scenarios. As shown in the third row of each figure, Kling often performs object swapping rather than inserting 2 Figure 9. Kling / Pika-Pro prompt: Using the context of the video, seamlessly place the image into the empty space of the cart. 3 Figure 10. Kling / Pika-Pro prompt: Using the context of the video, seamlessly place the image into the empty space of the shoe rack. 4 Figure 11. Kling / Pika-Pro prompt: Top:Using the context of the video, seamlessly place the image into the empty space of the bed. Bottom:Using the context of the video, seamlessly place the image into the empty space of the kitchen table. 5 Figure 12. Kling / Pika-Pro prompt: Using the context of the video, seamlessly place the image next to the pillar. Figure 13. Kling / Pika-Pro prompt: Using the context of the video, seamlessly place the image into the empty space on the table in front of the mirror."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "SK Telecom"
    ]
}