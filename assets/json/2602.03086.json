{
    "paper_title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
    "authors": [
        "Jiayao Mai",
        "Bangyan Liao",
        "Zhenjun Zhao",
        "Yingping Zeng",
        "Haoang Li",
        "Javier Civera",
        "Tailin Wu",
        "Yi Zhou",
        "Peidong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
        },
        {
            "title": "Start",
            "content": "Published as conference paper at ICLR 2026 NEURAL PREDICTOR-CORRECTOR: SOLVING HOMOTOPY PROBLEMS WITH REINFORCEMENT LEARNING Jiayao Mai1 Bangyan Liao2 Zhenjun Zhao3 Yingping Zeng1 Haoang Li4 Javier Civera3 Tailin Wu2 Yi Zhou1(cid:66) 1Hunan University 2Westlake University 4Hong Kong University of Science and Technology (Guangzhou) {maijy,eeyzhou}@hnu.edu.cn, {liaobangyan,liupeidong}@westlake.edu.cn 3University of Zaragoza Peidong Liu2(cid:66) 6 2 0 F 3 ] . [ 1 6 8 0 3 0 . 2 0 6 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "The Homotopy paradigm, general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and taskspecific. To address this, we unify these problems under single framework, which enables the design of general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into single neural framework."
        },
        {
            "title": "INTRODUCTION",
            "content": "As general principle for solving difficult problems, the Homotopy paradigm appears across diverse domains under different names, for example, Graduated Non-Convexity (Yang et al., 2020a) and Gaussian homotopy (Mobahi & Fisher III, 2015) for optimization, homotopy continuation (Bates et al., 2013) for polynomial root-finding, and annealed Langevin dynamics (Song et al., 2020) for sampling. Specifically, the Homotopy paradigm firstly construct an explicit homotopy interpolation from simple, easily solved source problem to complex target problem. Then, the solution of the complex problem is progressively approached by tracing the implicit trajectory along this interpolation path, effectively circumventing the challenges of direct solution. Practical solvers for these problems often follow predictor-corrector (PC) structure, where predictor advances along the outer homotopy interpolation and corrector iteratively refines the solution (Allgower & Georg, 2012). Despite their widespread use, these solvers rely on manually designed heuristics for step sizes and termination rules, which are typically suboptimal and taskspecific. Furthermore, these methods have been independently developed in each domain, and no prior work has systematically unified these efforts under single framework. We argue that this unification is crucial: it enables the design of general solver that applies across problem instances, rather than requiring ad-hoc, per-problem solutions. Building on this perspective, we propose Neural Predictor-Corrector (NPC), plug-and-play framework that replaces heuristic rules with automatically learned policies. Instead of manually Equal contribution. Project lead. (cid:66)Corresponding authors. 1 Published as conference paper at ICLR 2026 Figure 1: Homotopy paradigm across domains. The homotopy interpolation (blue loss functions in optimization, green polynomial roots in polynomial root-finding, and red probability densities in sampling) is explicitly defined, while the inner solution trajectory (orange curve) must be implicitly tracked. designed rules, NPC treats the choice of predictor and corrector strategies as sequential decisionmaking process (Barto et al., 1989) and employs reinforcement learning (RL) (Kaelbling et al., 1996) to adaptively learn effective policies. Crucially, we adopt an amortized training regime: single offline training phase over distribution of problem instances produces policy that can be directly deployed on new instances from the same problem without per-instance fine-tuning. We evaluate NPC on four representative homotopy tasks: Graduated Non-Convexity for robust optimization (Yang et al., 2020a), Gaussian homotopy for global optimization (Mobahi & Fisher III, 2015), homotopy continuation for polynomial root-finding (Bates et al., 2013), and annealed Langevin dynamics for sampling (Song et al., 2020). Through experiments on four representative problems, our approach is validated for strong generalization to previously unseen instances. Furthermore, the results reveal dual advantage: our method not only consistently outperforms existing approaches in computational efficiency, but also demonstrates superior numerical stability, thereby underscoring the benefits of our proposed architecture. In summary, our main contributions are as follows: To the best of our knowledge, we are the first to unify diverse problems, including robust optimization, global optimization, polynomial system root-finding, and sampling, under the homotopy paradigm, thereby revealing their common predictor-corrector structure across these problems. This enables unified solver framework, rather than per-problem solutions. We introduce Neural Predictor-Corrector (NPC), the first reinforcement learning-based framework that automatically learns predictor and corrector policies, replacing handcrafted heuristics with learned, adaptive strategies. Extensive experiments across multiple homotopy problems demonstrate that NPC significantly outperforms other methods in efficiency, while achieving higher stability and enabling efficient, training-free deployment on previously unseen instances."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Although PC solvers appear across multiple domains, these lines of research have largely evolved independently. We review them here and highlight gaps that motivate our work. full discussion of related works is provided in Sec. C. 2 Published as conference paper at ICLR 2026 Classical PC algorithms. PC schemes trace solution trajectories along explicit homotopy interpolations. In robust optimization, Graduated Non-Convexity (GNC) gradually increases non-convexity to avoid poor local minima, with iterative solvers performing corrections (Yang et al., 2020a; Peng et al., 2023). Gaussian homotopy methods construct progressively less smoothed objectives to track minimizers along bandwidth reduction (Blake & Zisserman, 1987; Mobahi & Fisher III, 2015; Iwakiri et al., 2022; Xu, 2024). Polynomial system root-finding uses homotopy continuation with PC integration to trace roots from simple start system (Bates et al., 2013; Breiding & Timme, 2018; Duff et al., 2019). In sampling, annealed Langevin dynamics and Sequential Monte Carlo define sequences of intermediate distributions with PC steps (Song & Ermon, 2019; Song et al., 2020; Doucet et al., 2001). Across all these domains, predictor and corrector components are typically hand-designed, requiring per-instance tuning and limiting generalization. Learning-based improvements for homotopy workflows. Recent work has introduced learning into homotopy pipelines, showing efficient and effective improvements on Gaussian homotopy (Lin et al., 2023), sampling (Richter & Berner, 2024), combinatorial optimization (Ichikawa, 2024), and polynomial root-finding (Hruby et al., 2022; Zhang et al., 2025). However, prior learning-based methods either focus on single homotopy component or require specialized per-instance training. Reinforcement learning for optimization and sampling. RL has been applied to learn optimizers or adapt algorithmic parameters, showing benefits on some optimization and sampling tasks (Li, 2019; Belder et al., 2023; Ye et al., 2025; Liu et al., 2025; Yan et al., 2025b; Wang et al., 2025). However, these works do not address the full predictorcorrector control problem across diverse homotopy classes, nor do they leverage amortized training to produce single policy transferable across instances."
        },
        {
            "title": "3 HOMOTOPY PARADIGM AS A UNIFIED PERSPECTIVE",
            "content": "In this section, we introduce unified perspective on diverse problems. We begin in Sec. 3.1 by introducing the homotopy paradigm, general principle that underlies wide range of problems. Next, in Sec. 3.2, we show that the corresponding practical solvers can all be instantiated within common predictor-corrector (PC) framework. Finally, in Sec. 3.3, we discuss four representative problems together with their homotopy formulations and PC implementations, thereby illustrating the breadth and utility of this unified perspective. 3.1 HOMOTOPY PARADIGM As shown in Fig. 1, the homotopy paradigm provides general principle for solving complex problem g(x). Specifically, the homotopy paradigm defines continuous interpolation H(x, t) from simple source problem H(x, 0) = (x) with known solutions to complex target problem H(x, 1) = g(x). By tracing the implicit solution trajectory x(t) along this interpolation as varies from 0 to 1, one progressively transforms the source solution into the target solution. The source problem and interpolation are explicitly defined by the user, while the target solution is implicitly determined along the trajectory. 3.2 PREDICTOR-CORRECTOR ALGORITHM While the homotopy paradigm specifies the abstract principle, an effective algorithm is needed to trace the implicit solution trajectory in practice. The PC method (Allgower & Georg, 2012) provides such concrete algorithmic framework. As shown in Fig. 2, PC decomposes trajectory tracking into two complementary steps: Predictor: Determines the next level of the homotopy interpolation and predicts the solutions position at that level. Corrector: Iteratively refines the predicted solution to align it with the true solution trajectory, thereby preventing the accumulation of bias across levels. The choice of predictor level schedule and corrector iteration count is often heuristic. Suboptimal settings can lead to inefficiency, instability, or failure to follow the trajectory accurately, motivating the development of adaptive or learning-based strategies for robust and efficient solution tracking. 3 Published as conference paper at ICLR Figure 2: Illustration of the Predictor-Corrector algorithm. Predictor proposes the next level and provides an initial solution estimate, while Corrector iteratively refines this estimate to project it back onto the solution trajectory. Orange curve denotes the implicit solution trajectory, as in Fig. 1."
        },
        {
            "title": "3.3 REPRESENTATIVE HOMOTOPY PROBLEMS AND PRACTICAL SOLVERS",
            "content": "To illustrate the breadth of homotopy paradigm applications, we describe four representative problems together with their corresponding homotopy interpolations and PC implementations. 1) Robust Optimization (Graduated Non-Convexity, GNC): Robust loss functions (e.g., GemanMcClure (Black & Rangarajan, 1996)) mitigate the effect of outliers. However, they introduce strong non-convexity, increasing the risk of poor local minima. Graduated Non-Convexity (GNC) (Yang et al., 2020a) addresses this challenge by defining homotopy interpolation: H(x, t) = c2 r(x, yi)2 c2 + r(x, yi)2 , (cid:88) (1) where is predefined parameter that controls the robustness of the GM loss, r(, ) represents the residual function, and yi denotes the measurements. This interpolation smoothly transitions from convex quadratic loss (H(x, 0) = (cid:80) i=1 r(x, yi)2) to the original non-convex GemanMcClure loss c2 r(x,yi)2 (H(x, 1) = g(x) = (cid:80) c2+r(x,yi)2 ). The predictor gradually increases non-convexity according to predefined schedule, while the corrector refines the solution at each stage, often via non-linear least squares optimizer (e.g., LevenbergMarquardt algorithm (Levenberg, 1944)). This homotopy strategy has proven highly effective in problems such as point cloud registration under severe outlier contamination (Yang et al., 2020b). Details are provided in Sec. A.1. i=1 2) Global Optimization (Gaussian Homotopy, GH): Many optimization problems suffer from highly non-convex landscapes with narrow basins of attraction, making it difficult for solvers to converge to global or high-quality local minima. Iwakiri et al. (2022) address this challenge by progressively smoothing the target function through convolution with Gaussian kernel (0, tσ2): H(x, t) = g(x) (0, tσ2), (2) where denotes the convolution operator. This Gaussian smoothing enlarges the basin of attraction, allowing solvers to approach promising regions more reliably. The predictor progressively reduces the kernel bandwidth, while the corrector refines the solution at each stage. Details are provided in Sec. A.2. 3) Polynomial Root-Finding (Homotopy Continuation, HC): Root-finding for polynomial systems is challenging due to multiple solutions and computational complexity. Bates et al. (2013) address this by starting from source system (x) = 0 with known roots and defining linear homotopy: H(x, t) = (1 t)f (x) + tg(x), (3) tracing the solution trajectory from the source roots to the target roots. The predictor extrapolates the next solution along this path, while the corrector refines it using Gauss-Newton (Bjorck, 2024) iteration at each step, ensuring accuracy along the trajectory. Details are provided in Sec. A.3. 4) Sampling (Annealed Langevin Dynamics, ALD): Sampling from complex, high-dimensional distributions is challenging due to multi-modality and slow mixing. Song et al. (2020) address this 4 Published as conference paper at ICLR Figure 3: RL formulation of the proposed Neural Predictor-Corrector (NPC). At each homotopy level, the agent observes the current state (including homotopy level, corrector statistics, and convergence velocity), outputs actions that adapt the predictors step size and the correctors tolerance, and receives rewards designed to balance accuracy and efficiency. by constructing homotopy between simple source distribution (e.g., Gaussian) and the target distribution: H(x, t) exp (cid:0) (1 t)f (x) tg(x)(cid:1). (4) The predictor schedules the intermediate distributions, while Langevin dynamics acts as the corrector at each step, iteratively refining samples to match the current intermediate distribution. Details are provided in Sec. A.4. These examples collectively highlight the broad applicability of homotopy paradigm and the central role of predictor-corrector strategies, motivating the need for learning-based policy optimization."
        },
        {
            "title": "4 NEURAL PREDICTOR-CORRECTOR WITH REINFORCEMENT LEARNING",
            "content": "This section introduces the Neural Predictor-Corrector (NPC) framework, general approach for homotopy problems that replaces heuristic step-size and termination rules with neural parameterizations learned via RL. As shown in Fig. 3, NPC reformulates the predictor-corrector process as sequential decision problem: the predictor advances the homotopy level, while the corrector ensures accuracy, both guided by adaptive policies. We first present the NPC formulation (Sec. 4.1), followed by its training with reinforcement learning (Sec. 4.2). 4.1 NEURAL PREDICTOR-CORRECTOR Classical PC algorithms differ across homotopy problems in how they define prediction and correction, yet share key limitation: their step-size schedules and termination criteria are governed by fixed heuristics. Such heuristics fail to adapt to varying solution trajectories, where small steps are needed for sharp transitions but larger steps improve efficiency when the trajectory is smooth. The NPC addresses this limitation by parameterizing the decision rules with neural network (NN). Instead of hand-crafted heuristics, NPC learns flexible and adaptive strategies that generalize across problem instances. The entire PC process is modeled as Markov Decision Process (MDP), in which, at each homotopy interpolation level, an agent observes the state and selects actions that govern the procedure. The state encodes both progress and dynamics: Algorithm 1 Neural Predictor-Corrector Solver Input: Homotopy problem 1: Warm up for initialization. 2: while tn 1 do 3: 4: 5: 6: 7: 8: 9: 10: 11: end while Output: Optimal solution NPC: {tn, ϵn or imax } = NN(tn1, ϵn1, in1, τn1) Predictor: Update interpolation level tn = tn1 + tn Predictor: Predict xtn at level tn while H(xtn , tn) ϵn and in imax end while Collect corrector statistics ϵn, in Collect convergence velocity τn do Corrector: Perform one step correction t=1 Homotopy Level: Current position along the interpolation path. 5 Published as conference paper at ICLR 2026 Corrector Statistics: Iteration count and attained tolerance from the previous step, capturing both convergence efficiency and deviation from the predicted trajectory. Convergence Velocity: Relative change in an optimality metric between consecutive levels, reflecting the speed of convergence. For optimization and root-finding, this is the relative change in the objective value. For sampling, it is the change in statistical distance such as Kernelized Stein Discrepancy (KSD) (Liu et al., 2016) between the empirical sample distribution and the target distribution across consecutive levels. Given the state s, NPC outputs two-part action a: Step Size t: Controls the predictors advance along the homotopy path. Corrector Termination: Convergence threshold ϵ or maximum number of updates, balancing accuracy and efficiency. As shown in Algorithm 1, the NPC solver operates in an iterative loop to trace the solution path of given homotopy problem H. Each iteration consists of three key stages. First, neural network (the NPC module) dynamically determines next actions for both the predictor and corrector. Second, the predictor advances the homotopy level to tn and predicts the solution xn at this level. Third, the corrector iteratively refines this prediction until the convergence criteria are met. Finally, performance statistics are collected and fed back to the NPC module to inform its decisions in the next iteration, creating an adaptive, closed-loop system. 4.2 REINFORCEMENT LEARNING FOR NPC Because the predictor-corrector procedure is non-differentiable and early decisions influence the entire trajectory, supervised or self-supervised training is inadequate. These approaches would require assuming that local geometric structures of the solution trajectory remain consistent across instances, which rarely holds in practice. We instead employ RL, which inherently evaluates sequential decisions by their cumulative effect and enables learning policies that generalize across instances within the same problem class. The reward function is designed to promote both accuracy and efficiency: Step-wise Accuracy (racc ): Encourages faithful trajectory tracking, based on convergence velocity or relative error change in the target problem. Terminal Efficiency Bonus (reff): Rewards shorter corrector sequences, formulated as Tmax , where Tmax is predefined upper bound and is the total corrector iterations. Consequently, the cumulative reward for an episode is defined as: = ((cid:80)T ) + λ2reff, where λ1, λ2 are scaling coefficients detailed in Sec. A. This reward design enables agent to balance accuracy and efficiency across the homotopy trajectory. t=1 λ1racc Remarks on amortized training for generalization. Sequential decision-making in homotopy problems entails that early step-size choices affect all subsequent levels. Self-supervised learning fails in this context because measuring the future contribution of step size is infeasible: it depends on the local geometric properties of the trajectory at future homotopy levels, which are unknown in advance. Relying on such assumptions risks overfitting to the training landscapes. This challenge is analogous to the dilemma discussed in (Li, 2019), where although the problem domains differ, the core issue of long-term dependency and overfitting is similar. Reinforcement learning, by contrast, inherently evaluates actions based on cumulative outcomes, allowing NPC to adapt to diverse solution trajectories without assuming consistent local geometry. Amortized training further improves generalization: by training over distribution of problem instances, NPC learns policy that can be applied efficiently to unseen instances within the same problem class."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 IMPLEMENTATION DETAILS Following the RL formulation in Sec. 4, we employ Proximal Policy Optimization (PPO) (Schulman et al., 2017), an on-policy algorithm well-suited for continuous state and action spaces. Implemen6 Published as conference paper at ICLR 2026 Table 1: Performance on the GNC point cloud registration task. Rotation and translation errors (ER and Et) are reported on log10 scale. Table 2: Performance on the GNC multiview triangulation task. Reconstructed 3D point errors (Ep) are reported on log10 scale. cube bunny Sequence Method log(ER) log(Et) Iter Time 783 161.00 -2.76 Classic GNC -0.85 309 61.59 -2.75 -0.85 IRLS GNC Ours1+GNC 169 19.15 -2.71 -0.85 486 89.34 -2.89 Classic GNC -1.12 141 26.13 -2.90 -1.10 IRLS GNC Ours1+GNC 86 7.86 -2.86 -1.11 859 177.11 -2.82 Classic GNC -0.80 486 95.93 -2.82 -0.80 IRLS GNC Ours1+GNC 201 26.42 -2.80 -0.80 1 The agent is trained on the Aquarius sequence for the point cloud registration task. dragon reichstag sacre coeur Sequence Method log(Ep) Iter Time 142 81.98 37 10.72 21 14.18 195 91.23 16 21.31 20 14.14 136 80.50 19 27.92 18 15.55 1 The agent is trained on the Aquarius sequence for the point cloud registration task. Classic GNC IRLS GNC Ours1+GNC Classic GNC IRLS GNC Ours1+GNC Classic GNC IRLS GNC Ours1+GNC -4.62 1.74 -4.72 -5.15 0.50 -4.84 -4.81 1.00 -4. st pt sq tation is based on the open-source Stable Baselines3 library (Raffin et al., 2021). The policy and value functions are parameterized as multi-layer perceptrons (MLPs) with two hidden layers of 16 units each and ReLU activations. All other hyperparameters use the default values provided by Stable Baselines3. To account for varying problem formulations and noise levels across tasks, reward signals are scaled appropriately to ensure stable learning and comparability across tasks. Details are provided in Sec. A. All experiments are conducted on 12-core 5.0 GHz Intel Core i7-12700KF CPU and an NVIDIA GeForce RTX 3060 GPU, unless otherwise specified. In all tables, Iter records the total number of corrector iterations (rather than predictor iterations, which are more commonly used to measure progress in homotopy problems), and Time reports runtime in milliseconds. The best results are bolded and the second-best results in Tab. 3 are underlined. All results represent the average over 50 independent trials. 5.2 PROBLEM 1 : ROBUST OPTIMIZATION VIA GNC We evaluate NPC in the context of robust optimization using the GNC framework, comparing it against the classical GNC (Classic GNC) approach and the iteratively reweighted least-squares (IRLS) version (Peng et al., 2023). The evaluation covers two spatial perception tasks with high outlier ratios: point cloud registration (Alexiou et al., 2018) (95% outliers) and multi-view triangulation (Jin et al., 2021) (50% outliers). Our NPC model is trained solely on the Aquarius dataset from the EPFL Geometric Computing Laboratory, demonstrating its cross-instance generalization capabilities. Following the metrics defined in (Yang & Carlone, 2019), we report the rotation error (ER) and translation error (Et) in Tab. 1 for each method. Additionally, Tab. 2 presents the 3D point reconstruction error (Ep), defined as the Euclidean distance between reconstructed and ground-truth 3D points. As shown in Tabs. 1 and 2, NPC achieves accuracy comparable to Classic GNC, whereas IRLS, tailored for specific task, performs poorly on triangulation and lacks generalization. In terms of efficiency, NPC significantly boosts GNCs performance: on point cloud registration, it reduces iterations by approximately 70-80% and runtime by 80-90% without compromising accuracy. These results demonstrate that NPC preserves the robustness of Classic GNC while substantially improving efficiency and generalization. 5.3 PROBLEM 2 : GLOBAL OPTIMIZATION VIA GH We evaluate NPC in the GH setting for non-convex function minimization. We compare our method with two categories of baselines: (i) the single loop GH methods, SLGHr (γ = 0.995) and SLGHd (η = 104) (Iwakiri et al., 2022), (ii) the Gaussian smoothing method, PGS (N = 20) (Xu, 2024), 7 Published as conference paper at ICLR 2026 and (iii) the learning-based method, CPL (Lin et al., 2023). Performance is evaluated on three 2-dimension non-convex benchmarks: the Ackley (Ackley, 1987), Himmelblau (Himmelblau et al., 1972), and Rastrigin (Rastrigin, 1974) functions. The optimal value (x) is 0 for all problems. As summarized in Tab. 3, NPC-accelerated GH achieves substantial reduction in iterations and runtime compared to Classic GH, while maintaining comparable solution quality. SLGHd and PGS occasionally fail to reach the optimum, especially on Himmelblau, highlighting the challenge these landscapes pose for fixed-schedule homotopy methods. CPL is designed to learn the solution path for specific, fixed-coefficient problem instance. Consequently, training time must be factored into the runtime, negating any efficiency advantage. Overall, these results show that NPC provides an notable trade-off between efficiency and robustness. It generalizes well to unseen problem instances while accelerating convergence. Table 3: Performance on GH non-convex function minimization benchmarks. Problems Method (x) Iter Time 2d Ackley Himmelblau Rastrigin Classic GH 0.07 0.12 SLGHr 0.26 SLGHd 0.07 PGS 0.01 CPL Ours2+GH 0.05 Classic GH 0.00 0.00 SLGHr 2.57 SLGHd 1.18 PGS 0.00 CPL Ours2+GH 0.00 Classic GH 0.00 0.00 SLGHr 0.34 SLGHd 0.14 PGS 0.57 CPL Ours2+GH 0. 501 1839 568 200 - 359 501 1839 75 200 - 345 501 1839 319 200 - 247 16.25 56.71 28.45 14.32 1701.61 12.31 11.39 41.70 2.57 11.33 2160.17 8.91 23.76 78.21 19.64 11.94 790.38 11.84 2 The agent is trained on the Ackley functions with randomized parameters and evaluated on the canonical fixed-parameter version. 5.4 PROBLEM 3 : POLYNOMIAL ROOT-FINDING VIA HC Table 4: Performance on HC polynomial system benchmarks. Succ. denotes the success rate of tracking to root, and Time reports the average tracking time per solution path. We evaluate NPC in the context of polynomial system root-finding using HC. Experiments are conducted on two categories of tasks: polynomial system benchmarks (Katsura, 1990; Himmelblau et al., 1972; Rastrigin, 1974) and computer vision problem (UPnP (Kneip et al., 2014)) for generalized camera pose estimation from 2D3D correspondences. Tab. 4 lists the specific polynomial systems used, with the first entries as classical benchmarks and the last as computer vision task. We compare NPC-accelerated HC with Classic HC and Simulator HC (Zhang et al., 2025). Both Classic HC and NPC-accelerated HC use the monodromy module in Macaulay2 to generate start systems following (Duff, 2021), while Simulator HC pretrains regression neural network to predict the start system, relying on physical modeling of each problem. Consequently, Simulator HC is inapplicable to standard polynomial benchmarks. The NPC agent is trained on polynomial systems from the 4-view triangulation task with randomized coefficients to learn generalizable policies. As shown in Tab. 4, NPC consistently tracks all target solutions successfully while reducing the number of iterations and runtime compared to Classic HC. Notably, Simulator HC relies on task-specific pre-trained network, which limits its generality, and its runtime is not directly compaIn conrable since it is implemented in C++. trast, NPC provides general-purpose, adaptive solver that achieves accelerated convergence without per-task pre-training. -: Runtimes are not directly comparable, as Simulator HC is implemented in C++, while the other methods are in Python. 3 The agent is trained on polynomial systems from the 4-view triangulation task with randomized coefficients. Succ. Method 100% 39 Classic HC Ours3+HC 7 100% 100% 41 Classic HC Ours3+HC 8 100% Classic HC 100% 53 Simulator HC 100% 100 Ours3+HC 100% 29 Iter Time 2.22 0.65 1.96 0.64 8.25 - 3.86 Problems katsura10 cyclic UPnP 5.5 PROBLEM 4 : SAMPLING VIA ANNEALED LANGEVIN DYNAMICS (ALD) We evaluate NPC in the context of ALD for sampling from complex distributions. Target distributions include 40-mode Gaussian mixture model (GMM), 10-dimensional funnel distribution (Neal, 2003), and 4-particle double-well (DW-4) potential (Kohler et al., 2020). The NPC 8 Published as conference paper at ICLR 2026 5: Performance on ALD sampling. Table Wasserstein-2 distance (W2) and Kernelized Stein Discrepancy (KSD) evaluate sample quality. agent is trained on the 10-mode GMM with randomly sampled coefficients to learn generalizable policies for accelerating ALD. We compare our method against classic ALD (Song et al., 2020) and, where applicable, iDEM (AkhoundSadegh et al., 2024) for GMM and DW4 with 103 saved samples. Evaluation metrics are the Wasserstein-2 distance (W2) (Peyre et al., 2019) and the Kernelized Stein Discrepancy (KSD) (Liu As shown in Tab. 5, et al., 2016). NPC-accelerated ALD requires significantly fewer iterations while achieving W2 and KSD values comparable to classical ALD. Although iDEM attains lower W2 on some tasks, it relies on extensive per-task computation and is not directly comparable in runtime. Overall, these results demonstrate that NPC effectively accelerates sampling while maintaining high-quality approximations of the target distributions. Classic ALD 11.57 0.0037 410 1353.16 iDEM Ours4+ALD 11.91 0.0040 110 772.34 Classic ALD 30.91 0.0382 410 754.48 Ours4+ALD 31.02 0.0343 105 686.55 Classic ALD 3.77 0.0911 410 1337.70 iDEM Ours4+ALD 3.47 0.0899 105 711.66 -: Runtimes are not directly comparable, as iDEM is measured on more powerful NVIDIA RTX A6000 GPU. 4 The agent is trained on the 10-mode GMM with randomly sampled coefficients. Distributions Method W2 KSD Iter Time 7.42 0.0037 2.13 0.0911 1000 40-mode GMM funnel (d=10) DW-4 - - 5.6 ABLATION STUDY OF RL STATE COMPONENTS To assess the contribution of each component in the RL state, we perform an ablation study on the six datasets used for the GNC point cloud registration task, retraining the NPC agent with one component removed at time. As summarized in Tab. 6, removing any single state component causes the agent to adopt more conservative strategy, resulting in an increased number of corrector iterations relative to the full state. This tendency typically manifests as the agent selecting smaller predictor step sizes or stricter corrector tolerances to ensure convergence in the absence of complete information. This indicates that each state component, i.e., homotopy level, corrector tolerance, corrector iteration count, and convergence velocity, provides essential information for efficiently guiding the homotopy solver. Notably, the results suggest that corrector statistics (i.e., corrector tolerance and iteration) are the most informative parts of the state, as their removal leads to the largest performance drop. Table 6: Effect of each RL state component on NPC performance. Convergence Velocity Correctors Tolerance Correctors Iteration Homotopy Level 0 +21 +64 +52 +38 Iter 5.7 ANALYSIS OF EFFICIENCY-PRECISION TRADE-OFF We analyze the efficiency-precision trade-off by benchmarking our NPC-accelerated method against classical GNC and ALD. Classical approaches require manual tuning of homotopy parameters, resulting in performance curve where higher precision typically demands more iterations. By contrast, our NPC-accelerated method bypasses this manual exploration by learning policy that directly identifies an optimal operating point. This learned policy inherently balances the predictor step size and corrector tolerance to maximize efficiency at given precision level. The practical benefit is visualized in Fig. 4. For both GNC and ALD tasks, the single point representing our method lies well below the classical trade-off curves, clearly illustrating substantial reduction in iterations at comparable precision."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduces Neural PredictorCorrector (NPC), reinforcement learning framework for homotopy solvers. By unifying diverse problems, including robust optimization, global optimization, polynomial system root-finding, and sampling, under the homotopy paradigm, their solvers 9 Published as conference paper at ICLR 2026 (a) GNC point cloud registration. (b) ALD sampling. Figure 4: Trade-off between efficiency and precision. Efficiency is measured in terms of corrector iterations, and precision reflects solution accuracy, for NPC-accelerated versus classical methods. are shown to universally follow PC structure. NPC replaces handcrafted heuristics with adaptive learned policies and employs an amortized training regime, enabling one-time offline training and efficient, training-free deployment on new instances. Extensive experiments demonstrate that NPC generalizes effectively to unseen instances, consistently outperforms existing approaches in computational efficiency, and exhibits superior numerical stability. These findings position learning-based policy search as practical, generalizable, and efficient alternative to traditional heuristic strategies. Looking ahead, this paradigm opens promising avenues for extending homotopy methods to broader classes of optimization and sampling problems. Nonetheless, we also acknowledge its current limitation, which is discussed in Sec. D."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "Our work unifies diverse problem domains governed by the homotopy paradigm into single framework and, based on it, proposes general, learning-based solver NPC. Our experiments are conducted on publicly available academic benchmarks and synthetic data, involving no human subjects or sensitive personal information. We do not foresee any direct negative societal impacts or dual-use concerns, as the primary application of our work is to provide more efficient and robust tool for scientific inquiry."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "To ensure reproducibility, we specify the sources for all real-world datasets and the parameters used to generate synthetic data. In addition, Sec. provides additional implementation details, covering the specific problem formulations and the hyperparameters used in our experiments. Our code and pretrained models will also be released to the public."
        },
        {
            "title": "REFERENCES",
            "content": "David Ackley. connectionist machine for genetic hillclimbing, volume 28. Springer science & business media, 1987. Tara Akhound-Sadegh, Jarrid Rector-Brooks, Avishek Joey Bose, Sarthak Mittal, Pablo Lemos, Cheng-Hao Liu, Marcin Sendera, Siamak Ravanbakhsh, Gauthier Gidel, Yoshua Bengio, et al. arXiv preprint Iterated denoising energy matching for sampling from boltzmann densities. arXiv:2402.06121, 2024. Evangelos Alexiou, Touradj Ebrahimi, Marco Bernardo, Manuela Pereira, Antonio Pinheiro, Luis Da Silva Cruz, Carlos Duarte, Lovorka Gotal Dmitrovic, Emil Dumic, Dragan Matkovics, et al. Point cloud subjective evaluation methodology based on 2d rendering. In 2018 Tenth international conference on quality of multimedia experience (QoMEX), pp. 16. IEEE, 2018. Eugene Allgower and Kurt Georg. Numerical continuation methods: an introduction, volume 13. Springer Science & Business Media, 2012. 10 Published as conference paper at ICLR 2026 Andrew Gehret Barto, Richard Sutton, and CJCH Watkins. Learning and sequential decision making, volume 89. University of Massachusetts Amherst, MA, 1989. Daniel Bates, Andrew Sommese, Jonathan Hauenstein, and Charles Wampler. Numerically solving polynomial systems with Bertini. SIAM, 2013. Amir Belder, Refael Vivanti, and Ayellet Tal. game of bundle adjustment-learning efficient convergence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 84288437, 2023. Ake Bjorck. Numerical methods for least squares problems. SIAM, 2024. Michael Black and Anand Rangarajan. On the unification of line processes, outlier rejection, and robust statistics with applications in early vision. International journal of computer vision, 19(1): 5791, 1996. Andrew Blake and Andrew Zisserman. Visual reconstruction. MIT press, 1987. Paul Breiding and Sascha Timme. Homotopycontinuation. jl: package for homotopy continuation in julia. In International congress on mathematical software, pp. 458465. Springer, 2018. Zhangquan Chen, Puhua Jiang, and Ruqi Huang. Dv-matcher: Deformation-based non-rigid point cloud matching guided by pre-trained visual features. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2726427274, 2025a. Zhangquan Chen, Xufang Luo, and Dongsheng Li. Visrl: Intention-driven visual perception via reinforced reasoning. arXiv preprint arXiv:2503.07523, 2025b. James Davenport. Looking at set of equations. Thechnical report, pp. 8706, 1987. Arnaud Doucet, Nando De Freitas, and Neil Gordon. An introduction to sequential monte carlo methods. In Sequential Monte Carlo methods in practice, pp. 314. Springer, 2001. Timothy Duff. Applications of monodromy in solving polynomial systems. PhD thesis, Georgia Institute of Technology Atlanta, Georgia, USA, 2021. Timothy Duff, Cvetelina Hill, Anders Jensen, Kisun Lee, Anton Leykin, and Jeff Sommars. Solving IMA Journal of Numerical polynomial systems via homotopy continuation and monodromy. Analysis, 39(3):14211446, 2019. Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, et al. Pot: Python optimal transport. Journal of Machine Learning Research, 22(78):18, 2021. David Himmelblau et al. Applied nonlinear programming. McGraw-Hill, 1972. Petr Hruby, Timothy Duff, Anton Leykin, and Tomas Pajdla. Learning to solve hard minimal problems. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 55325542, 2022. Yuma Ichikawa. Controlling continuous relaxation for combinatorial optimization. Advances in Neural Information Processing Systems, 37:4718947216, 2024. Hidenori Iwakiri, Yuhang Wang, Shinji Ito, and Akiko Takeda. Single loop gaussian homotopy method for non-convex optimization. Advances in Neural Information Processing Systems, 35: 70657076, 2022. Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image matching across wide baselines: From paper to practice. International Journal of Computer Vision, 129(2):517547, 2021. Leslie Pack Kaelbling, Michael Littman, and Andrew Moore. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285, 1996. 11 Published as conference paper at ICLR 2026 Shlgetoshl Katsura. Spin glass problem by the method of integral equation of the effective field. New Trends in Magnetism, pp. 110121, 1990. CT Kelley. Solution of the chandrasekhar h-equation by newtons method. Journal of Mathematical Physics, 21(7):16251628, 1980. Laurent Kneip, Hongdong Li, and Yongduek Seo. Upnp: An optimal (n) solution to the absolute pose problem with universal applicability. In European conference on computer vision, pp. 127 142. Springer, 2014. Jonas Kohler, Leon Klein, and Frank Noe. Equivariant flows: exact likelihood generative learning for symmetric densities. In International conference on machine learning, pp. 53615370, 2020. Kenneth Levenberg. method for the solution of certain non-linear problems in least squares. Quarterly of applied mathematics, 2(2):164168, 1944. Ke Li. Advances in Machine Learning: Nearest Neighbour Search, Learning to Optimize and Generative Modelling. PhD thesis, University of California, Berkeley, 2019. Bangyan Liao, Zhenjun Zhao, Lu Chen, Haoang Li, Daniel Cremers, and Peidong Liu. GlobalIn European Conference on pointer: Large-scale plane adjustment with bi-convex relaxation. Computer Vision, pp. 360376, 2024. Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, and Qingfu Zhang. Continuation path learning for homotopy optimization. In International Conference on Machine Learning, pp. 2128821311. PMLR, 2023. Jiuming Liu, Guangming Wang, Zhe Liu, Chaokang Jiang, Marc Pollefeys, and Hesheng Wang. Regformer: an efficient projection-aware transformer network for large-scale point cloud regIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. istration. 84518460, 2023. Jiuming Liu, Weicai Ye, Guangming Wang, Chaokang Jiang, Lei Pan, Jinru Han, Zhe Liu, Guofeng Zhang, and Hesheng Wang. Difflow3d: Hierarchical diffusion models for uncertainty-aware 3d scene flow estimation. IEEE transactions on pattern analysis and machine intelligence, 2025. Qiang Liu, Jason Lee, and Michael Jordan. kernelized stein discrepancy for goodness-of-fit tests. In International conference on machine learning, pp. 276284. PMLR, 2016. Hossein Mobahi and John Fisher III. On the link between gaussian homotopy continuation and In International Workshop on Energy Minimization Methods in Computer convex envelopes. Vision and Pattern Recognition, pp. 4356. Springer, 2015. Radford Neal. Slice sampling. The annals of statistics, 31(3):705767, 2003. Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527566, 2017. VW Noonburg. neural network modeled by an adaptive lotka-volterra system. SIAM Journal on Applied Mathematics, 49(6):17791792, 1989. Liangzu Peng, Christian Kummerle, and Rene Vidal. On the convergence of irls and its variants in outlier-robust estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1780817818, 2023. Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends in Machine Learning, 11(5-6):355607, 2019. Boris Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational mathematics and mathematical physics, 4(5):117, 1964. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Journal of Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Machine Learning Research, 22(268):18, 2021. URL http://jmlr.org/papers/v22/ 20-1364.html. 12 Published as conference paper at ICLR 2026 LA Rastrigin. Extreme control systems (moscow: Nauka). 1974. Lorenz Richter and Julius Berner. Improved sampling via learned diffusions. In The Twelfth International Conference on Learning Representations, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Congye Wang, Wilson Ye Chen, Heishiro Kanagawa, and Chris J. Oates. Reinforcement learning for adaptive mcmc. In Proceedings of The 28th International Conference on Artificial Intelligence and Statistics, volume 258, pp. 640648, 2025. Chen Xu. Global optimization with power-transformed objective and gaussian smoothing. arXiv preprint arXiv:2412.05204, 2024. Shaocheng Yan, Pengcheng Shi, Zhenjun Zhao, Kaixin Wang, Kuang Cao, Ji Wu, and Jiayuan Li. In Proceedings of the Turboreg: Turboclique for robust and efficient point cloud registration. IEEE/CVF International Conference on Computer Vision, pp. 2637126381, 2025a. Shaocheng Yan, Yiming Wang, Kaiyan Zhao, Pengcheng Shi, Zhenjun Zhao, Yongjun Zhang, and Jiayuan Li. Hemora: Unsupervised heuristic consensus sampling for robust point cloud registration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 13631373, 2025b. Heng Yang and Luca Carlone. polynomial-time solution for robust registration with extreme outlier rates. arXiv preprint arXiv:1903.08588, 2019. Heng Yang, Pasquale Antonante, Vasileios Tzoumas, and Luca Carlone. Graduated non-convexity for robust spatial perception: From non-minimal solvers to global outlier rejection. IEEE Robotics and Automation Letters, 5(2):11271134, 2020a. Heng Yang, Jingnan Shi, and Luca Carlone. Teaser: Fast and certifiable point cloud registration. IEEE Transactions on Robotics, 37(2):314333, 2020b. Zilyu Ye, Zhiyang Chen, Tiancheng Li, Zemin Huang, Weijian Luo, and Guo-Jun Qi. Schedule on the fly: Diffusion time prediction for faster and better image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2341223422, 2025. Xinyue Zhang, Zijia Dai, Wanting Xu, and Laurent Kneip. Simulator hc: Regression-based online simulation of starting problem-solution pairs for homotopy continuation in geometric vision. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2710327112, 2025. 13 Published as conference paper at ICLR"
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "A.1 DETAILS OF PROBLEM 1 : ROBUST OPTIMIZATION VIA GNC ( SEC. 5.2) A.1.1 THE GRADUATED NON-CONVEXITY ALGORITHM Optimization problems that can be formulated as least-squares can utilize the robust kernel from Eq. (1), which is represented as: = min xX ,tT H(x, t). (5) The GNC algorithm utilizes Black-Rangarajan Duality (Black & Rangarajan, 1996) to reformulate Eq. (5) into: = min xX (cid:88) i=1 (cid:2)wir2(yi, x) + ΦHt(wi)(cid:3) , (6) where wi is the weight of the ith measurement yi, and the function ΦHt(), whose expression depends on the choice of the robust cost function Ht, defines penalty on the weight wi. When Ht is defined by Eq. (1), ΦHt(wi) is defined by ΦHt(wi) = 1 wi 1)2. Moreover, the weight can c2( be solved in closed form as function of only and residual r. Predictor Reformulating the problem as Eq. (6) simplifies the prediction step to updating the each weight wi using Eq. (7), rather than predicting the optimization variable x. wi = (cid:18) c2 tr2(x, yi) + c2 (cid:19)2 Corrector We correct using nonlinear optimization method defined by Eq. (8). = min xX (cid:88) i= wir2(yi, x) (7) (8) In our experiments, point cloud registration employs Gauss-Newton corrector, while multi-view triangulation uses more robust Levenberg-Marquardt (LM) algorithm. Details of experiment. For the point cloud registration task, the reward scaling is set to λ1 = 103, and λ2 = 103. For the multi-view triangulation task, the reward scaling is set to λ1 = 101 and λ2 = 103 due to its noise scale being significantly larger than that of point cloud registration. A.2 DETAILS OF PROBLEM 2 : GLOBAL OPTIMIZATION VIA GH (SEC. 5.3) A.2.1 THE GAUSSIAN HOMOTOPY ALGORITHM The equivalent expression for Eq. (2) is given by: (cid:90) H(x, t) = g(x + σ)k(σ)dσ = EσN (0,Id)[g(x + σ)] (9) where k(σ) = (2π) 2 σ2 2 is referred to as the kernel. Predictor. The prediction process in Gaussian homotopy is implicit, as we only modify the shape of H(x, t) by varying the predictors homotopy level t. Corrector. The correction for is performed using momentum method (Polyak, 1964), with the gradient update formulated as vt+1 = xH(xt, t) + βvt xt+1 = xt αvt+1 (10) where vt is the velocity vector, with the initial velocity v0 set to the zero vector, β is momentum coefficients, controlling the influence of past gradients, and α is the learning rate, which determines the step size of the update. We set α = 0.01 and β = 0.8 in our experiment. As the analytical computation of the gradient xH(xt, t) is not feasible for some Gaussian homotopy functions, we 14 Published as conference paper at ICLR 2026 employ zeroth-order method to obtain numerical approximation. The calculation formula is as follows (Nesterov & Spokoiny, 2017): xH(xt, t) = xEσN (0,Id)[g(x + σ)] = 1 Eσ [(g(x + σ) g(x)) σ] (11) Details of experiment. The reward scaling is set to λ1 = 1, and λ2 = 103. A.2.2 THE NON-CONVEX FUNCTION MINIMIZATION BENCHMARKS Ackley Optimization Problem (n-dimensions) (Ackley, 1987): (x) = 20 exp 0.2 (cid:118) (cid:117) (cid:117) (cid:116) 1 n (cid:88) i=1 x2 exp (cid:32) 1 (cid:88) i=1 (cid:33) cos(2πxi) + 20 + e. (12) Himmelblau Optimization Problem (Himmelblau et al., 1972): (x, y) = (x2 + 11)2 + (x + y2 7)2. Rastrigin Optimization Problem (Rastrigin, 1974): (x, y) = 10 + x2 + y2 9 cos(2πx) cos(2πy) (13) (14) A.3 DETAILS OF PROBLEM 3 : POLYNOMIAL ROOT-FINDING VIA HC (SEC. 5.4) A.3.1 THE HOMOTOPY-CONTINUATION ALGORITHM The polynomial system root-finding problem is modeled in the form of Eq. (3). Predictor. The prediction of x(t + t) is performed using the Pade approximation. The Pade approximation polynomial Rn,m(x) = Rn(x) Qm(x) has the following form: Rn,m(x) = p0 + p1x + + pnxn 1 + q1x + + qmxm = (cid:80)n 1 + (cid:80)m j=0 pjxj k=1 qkxk . It is equivalent to the power series given in Eq. (16). ψ(x) := (cid:88) k=1 ckxk. (15) (16) The basic Pade approximation principle is that, given two integers m, {0}, we can find two polynomials Pn(x) of degree at most and Qm(x) of degree at most m, such that the difference Qm(x)f (x) Pn(x) has an order of approximation of at least + + 1. In fact, this is mathematically equivalent to the requirement: Qm(x)f (x) Pn(x) = O(xn+m+1), (17) where O(xN ) denotes power series of the form (cid:80) In our implementation, we set = 2 and = 1. we can derive the following coefficients based on Eq. (17): n=N cnxn. q1 = c3 , p0 = c0, p1 = c1 + q1c0, p2 = c2 + q1c1, 15 (18) Published as conference paper at ICLR 2026 where c0 = x(t), c1 = x(t), c2 = 1 differentiating H(x(t), t) with respect to t: 2 x(t), c3 = 1 6 x(t). We obtain the derivative of by x x x x(t) = x(t) = x(t) + (cid:19) , 2H t2 , (cid:18) 2H xt 2H xt (cid:18) x(t) = x(t) + 3H xt2 x(t) + 3H t3 (cid:19) . Consequently, x(t + t) is according to the following equation: x(t + t) = p0 + p1t + p2t2 1 + q1t . (19) (20) If the denominator in Eq. (20) approaches zero, we revert to using power series to predict x(t+t). In this case, the prediction takes the form x(t + t) = c0 + c1t + c2t2 + c3t3. Corrector. We employ Newton corrector in our experimental setup. At each iteration, is updated according to the following equation until the convergence criterion < ϵ is met. H(x, + t) x = H(x, + t), = + x. Details of experiment. The reward scaling is set to λ1 = 103, and λ2 = 101. A.3.2 THE POLYNOMIAL SYSTEM BENCHMARKS The Katsura-n Polynomial System (Katsura, 1990): f0 : (cid:33) xi 1 = (cid:32) (cid:88) i=n fk+1 : xnxn + (cid:32) (cid:88) i=n+ (cid:33) xixki xk = 0 (for = 0, 1, . . . , 1) The Cyclic-n Polynomial System (Davenport, 1987): f0 : f1 : ... fn2 : fn1 : n1 (cid:88) j= n1 (cid:88) j=0 xj = 0 xjx(j+1) (mod n) = 0 ... (cid:32)n2 (cid:89) n1 (cid:88) (cid:33) x(j+k) (mod n) = 0 j=0 n1 (cid:89) k=0 xj 1 = 0 (21) (22) (23) The Noon-n Polynomial System (Noonburg, 1989): j=0 (cid:88) j=1 j=i xi x2 cxi + 1 = 0 for = 1, . . . , (24) 16 Published as conference paper at ICLR 2026 In our implementation, we set = 1.1. The Chandra-n Polynomial System (Kelley, 1980): (cid:32) 2nxk cxk 1 + (cid:33) n1 (cid:88) i=1 + xi In our implementation, we set = 0.51234. 2n = 0 for = 1, . . . , (25) A.4 DETAILS OF PROBLEM 4 : SAMPLING VIA ALD (SEC. 5.5) A.4.1 THE ANNEALED LANGEVIN DYNAMIC SAMPLING ALGORITHM Annealed Langevin dynamics sampling obtains initial sample points from simple distribution and uses series of time-dependent potentials to control the update of the samples, as shown in Eq. (4). Let the time-dependent potentials be H(x, t) exp (cid:0) (1 t)f (x) tg(x)(cid:1). Predictor. The prediction process in ALD sampling is implicit, as we only modify the shape of H(x, t) by varying the predictors homotopy level t. Corrector. following formula: In each iteration of the corrector, the positions of the samples are updated using the = ξ 2 xH(xt, t) + (cid:112)ξσ, where ξ is Langevin step size, and σ (0, Id) is Gaussian noise vector. Details of experiment. The reward scaling is set to λ1 = 10, and λ2 = 103. A.4.2 DISTRIBUTIONS The 10-dimensional funnel distribution. The 10 dimensions Funnel distribution defined as The funnel potential given as x0 (0, σ2) xix0 (0, ex0 ), for = 1, . . . , 9. g(x) = x2 0 2σ2 + 1 2 9 (cid:88) i=1 ex0x2 (26) (27) (28) The 4-particle double-well (DW-4) potential. The DW-4 potential defined as g(x) = 1 2τ (cid:88) ij a(dij d0) + b(dij d0)2 + c(dij d0)4, (29) where dij = xi xj2 is the Euclidean distance between particles and j. In our implementation, we set τ = 1, = 0, = 4, and = 0.9. A.4.3 METRIC Wasserstein-2 distance (W2). The Wasserstein-2 distance (Peyre et al., 2019) is given by W2(µ, ν) = (cid:90) (cid:18) inf π π(x, y)d(x, y)2 dxdy (cid:19) 2 , (30) where π is the transport plan with marginals constrained to µ and ν respectively. In our implementation, we we use the Python Optimal Transport (POT) package (Flamary et al., 2021) to compute this metric. 17 Published as conference paper at ICLR Kernelized Stein Discrepancy (KSD). The Kernelized Stein Discrepancy (Liu et al., 2016) is defined as uq(x, x) = sq(x)k(x, x)sq(x) + sq(x)xk(x, x) + xk(x, x)sq(x) + trace(x,xk(x, x)), (31) S(p, q) = Ex,xp[uq(x, x)], where sq = xg(x), and k(x, x) is positive definite kernel. Specifically, we use the standard RBF kernel for KSD computation in this work."
        },
        {
            "title": "B BACKGROUND ON REINFORCEMENT LEARNING",
            "content": "Reinforcement learning (RL) (Kaelbling et al., 1996) provides natural framework for learning adaptive strategies. It formulates sequential decision-making as an Markov Decision Process (MDP) with state space S, action space A, transition dynamics p(st+1st, at), initial state distribution p0(s0), reward function r(st, at), and discount factor γ (0, 1]. The goal is to find an optimal policy π : that maximizes the expected cumulative reward along trajectory (s0, a0, . . . , sT ): (cid:34) (cid:88) (cid:35) γtr(st, at) . t=0 (32)"
        },
        {
            "title": "C FULL RELATED WORK",
            "content": "Although we mentioned in previous sections that methods from different fields essentially share the same predictor-corrector spirit, they have long evolved independently of each other. Our work is the first to unify these methods. In this section, we will review 1) classical predictor-corrector methods; 2) learning-based improvements on predictor-corrector methods; 3) efficient optimization and sampling methods via reinforcement learning. Classical PC algorithms. 1) Robust optimization: core related technique is Graduated NonConvexity (GNC), first proposed by (Yang et al., 2020a). This method employs predictor-corrector approach with non-linear least-squares solvers to compute robust solutions. However, it relies on hand-crafted, fixed iteration schedule, making it unsuitable for real-time robotics applications. Building on this work, Peng et al. (2023) established connection between GNC and the iteratively reweighted least-squares (IRLS) framework, based on which they designed novel iteration strategy that achieved faster speeds in point cloud registration tasks (Liu et al., 2023; Yan et al., 2025a; Chen et al., 2025a; Liao et al., 2024). Nevertheless, this strategys lack of generalizability to other problems remains its primary limitation. 2) Gaussian homotopy optimization: The underlying principle of this area was first introduced in (Blake & Zisserman, 1987). More recently, Iwakiri et al. (2022) proposed novel single-loop framework for the Gaussian homotopy method that simultaneously performs prediction and correction. Subsequently, Xu (2024) improved the algorithms convergence rate by adding an exponential power-N transformation prior to the Gaussian homotopy process. 3) Polynomial root-finding: Homotopy continuation (Bates et al., 2013), numerical method for finding the roots of polynomial systems, uses predictor-corrector scheme to track solution paths. Subsequent methods by (Breiding & Timme, 2018) and (Duff et al., 2019) analyzed the properties of polynomials to introduce various improvements, enhancing the algorithms speed. 4) Sampling: In generative modeling, annealed Langevin dynamics (Song & Ermon, 2019; Song et al., 2020) utilizes predictor-corrector method to sample from image probability distributions, where the correction step uses Langevin dynamics to restore samples to an equilibrium state. Similarly, Sequential Monte Carlo (SMC) methods (Doucet et al., 2001) also apply predictor-corrector approach to sample from posterior probability distributions, with correction step that employs importance sampling to reweight the samples. Learning-based improvements for homotopy workflows. 1) Gaussian homotopy optimization: Lin et al. (2023) is novel model-based method that learns the entire continuation path for Gaussian homotopy. However, this approach requires specialized training for each problem. 2) Polynomial root-finding: Focusing on the more specific sub-problem of polynomial system root-finding, both (Hruby et al., 2022) and (Zhang et al., 2025) propose learning-based methods to determine 18 Published as conference paper at ICLR 2026 the optimal starting system for homotopy continuation. 3) Combinatorial optimization: Ichikawa (2024) proposes the Continuous Relaxation Annealing strategy, aiming to enhance unsupervised learning solvers for combinatorial optimization problems. 4) Sampling: Richter & Berner (2024) establishes unifying framework based on path space measures and time-reversals, and proposes novel log-variance loss that avoids differentiation through the SDE solver. Reinforcement learning for optimization and sampling. 1) Optimization: Li (2019) proposes general framework by formulating an optimization algorithm as reinforcement learning problem, where the optimizer is represented as policy that learns to generate update steps directly, aiming to converge faster and find better optima than hand-engineered method. Belder et al. (2023) utilizes reinforcement learning (Chen et al., 2025b) to train an agent that dynamically selects the damping factor in the Levenberg-Marquardt algorithm (Levenberg, 1944) to accelerate convergence by reducing the number of iterations. 2) Sampling: Ye et al. (2025) employs reinforcement learning to adaptively predict the denoising schedule via optimizing reward function that encourages high image quality while penalizing an excessive number of denoising steps. Wang et al. (2025) proposes general framework named Reinforcement Learning Metropolis-Hastings, which aims to automatically design and optimize Markov Chain Monte Carlo (MCMC) samplers."
        },
        {
            "title": "D LIMITATION AND FUTURE WORK",
            "content": "One limitation of our work is that the NPC agents reward scale currently requires manual tuning for each problem instance based on its noise level to ensure stable and efficient training. The scale of step-wise rewards influences the training processs convergence time, while an oversized terminal reward can nullify the guidance from step-wise rewards. This imbalance can prevent the agent from correctly tracking the solution trajectory, causing it to adopt myopic strategies to prematurely reach terminal state. We conduct experiments on the point cloud registration task with different reward scaling factors. The results are shown in Tab. 7. Table 7: Comparison of results under different reward scaling settings. Convergence Steps (Training) denotes the approximate step count where the cumulative reward stabilizes during training. Method Reward Scaling Convergence Steps (Training) log(ER) log(Et) Iter Ours+GNC Classic GNC IRLS GNC λ1 = 103, λ2 = 103 (*) λ1 = 102, λ2 = 103 λ1 = 103, λ2 = 104 λ1 = 102, λ2 = 10 - - (*): The settings used in the paper. 3M 2M 6M Fail - - -1.11 -1.08 -1.08 - -1.12 -1.10 -2.86 -2.67 -2.91 - -2.89 -2. 86 70 74 - 486 141 To address this, two avenues for future work are promising. The first is to develop mechanism that automatically adapts the reward scale. more fundamental solution would be to investigate adaptive normalization techniques for the reward function, making the learning process inherently robust and eliminating manual tuning."
        },
        {
            "title": "E FULL EXPERIMENTAL RESULTS",
            "content": "This section provides complete experimental results, which are summarized in Secs. 5.2 to 5.4 due to limited space. Tab. 8 shows the full results for the point cloud registration experiments via GNC, Tab. 9 shows the full results for the non-convex function minimization experiments via GH, and Tab. 10 presents the detailed results for the root-finding experiments on polynomial systems via HC. In addition, we present box plots for subset of the experimental results in Fig. 5 to visually compare the different methods. 19 Published as conference paper at ICLR 2026 Table 8: Performance on the GNC point cloud registration task. Rotation and translation errors (ER and Et) are reported on log10 scale. Sequence Method log(ER) log(Et) Iter Time bunny cube dragon egyptian mask sphere Classic GNC IRLS GNC Ours1+GNC Classic GNC IRLS GNC Ours1+GNC Classic GNC IRLS GNC Ours1+GNC Classic GNC IRLS GNC Ours1+GNC Classic GNC IRLS GNC Ours1+GNC -0.85 -0.85 -0.85 -1.12 -1.10 -1.11 -0.80 -0.80 -0.80 -0.88 -0.86 -0. -0.98 -0.98 -0.99 -2.76 -2.75 -2.71 -2.89 -2.90 -2.86 -2.82 -2.82 -2.80 -2.73 -2.75 -2.69 -2.87 -2.88 -2. 783 309 169 486 141 86 859 486 201 770 264 158 713 220 143 161.00 61.59 19. 89.34 26.13 7.86 177.11 95.93 26.42 160.05 53.51 16.94 148.55 45.73 13.63 vase Classic GNC IRLS GNC Ours1+GNC 159.25 58.08 17.05 1 The agent is trained on the Aquarius sequence for the point cloud registration task. -2.84 -2.86 -2.77 -0.86 -0.87 -0.86 765 288 160 Table 9: Performance on GH non-convex function minimization benchmarks. Problems Method (x) Iter Time 2d Ackley Himmelblau Rastrigin Classic GH SLGHr SLGHd PGS CPL Ours2+GH Classic GH SLGHr SLGHd PGS CPL Ours2+GH Classic GH SLGHr SLGHd PGS CPL Ours2+GH 0.07 0.12 0.26 0.07 0.01 0.05 0.00 0.00 2.57 1.18 0.00 0. 0.00 0.00 0.34 0.14 0.57 0.00 501 1839 568 200 - 359 501 1839 75 200 - 345 501 1839 319 200 - 247 16.25 56.71 28.45 14.32 1701.61 12.31 11.39 41.70 2.57 11.33 2160.17 8. 23.76 78.21 19.64 11.94 790.38 11.84 10d Ackley Classic GH SLGHr SLGHd Ours2+GH 27.58 91.90 33.58 10.88 2 The agent is trained on the Ackley functions with randomized parameters and evaluated on the canonical fixedparameter version. 501 1839 435 398 0.01 0.02 0.37 0. 20 Published as conference paper at ICLR 2026 (a) Rotation error of the bunny sequence in the point cloud registration task. (b) Runtime of the bunny sequence in the point cloud registration task. (c) Rotation error of the cube sequence in the point cloud registration task. (d) Runtime of the cube sequence in the point cloud registration task. (e) Error of the reichstag sequence in the multi-view triangulation task. (f) Runtime of the reichstag sequence in the multiview triangulation task. (g) Error of the sacre coeur sequence in the multi-view triangulation task. (h) Runtime of the sacre coeur sequence in the multiview triangulation task. (i) Function value of the Ackley problem in the nonconvex function minimization task. (j) Runtime of the Ackley problem in the non-convex function minimization task. (k) Function value of the Himmelblau problem in the non-convex function minimization task. (l) Runtime of the Himmelblau problem in the nonconvex function minimization task. Figure 5: Supplementary box plots of performance metrics. These visualizations illustrate the result distributions over 50 independent trials, providing more intuitive understanding of the stability and efficiency of each method. 21 Published as conference paper at ICLR 2026 Table 10: Performance on HC polynomial system benchmarks. Succ. denotes the success rate of tracking to root, and Time reports the average tracking time per solution path. Problems Method Succ. Iter Time katsura10 cyclic noon5 chandra9 UPnP Classic HC Ours3+HC Classic HC Ours3+HC Classic HC Ours3+HC Classic HC Ours3+HC 100% 100% 100% 100% 100% 100% 100% 100% 39 41 8 41 10 31 5 Classic HC 53 Simulator HC 100% 100 Ours3+HC 29 100% 100% 2.22 0.65 1.96 0.64 1.69 0.69 3.24 0.76 8.25 - 3.86 -: Runtimes are not directly comparable, as Simulator HC is implemented in C++, while the other methods are in Python. 3 The agent is trained on separate set of polynomial systems with randomized coefficients. THE USE OF LARGE LANGUAGE MODELS (LLMS) We use LLMs to polish writing."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Hunan University",
        "University of Zaragoza",
        "Westlake University"
    ]
}