{
    "paper_title": "Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach",
    "authors": [
        "Yaofang Liu",
        "Yumeng Ren",
        "Xiaodong Cun",
        "Aitor Artola",
        "Yang Liu",
        "Tieyong Zeng",
        "Raymond H. Chan",
        "Jean-michel Morel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have revolutionized image generation, and their extension to video generation has shown promise. However, current video diffusion models~(VDMs) rely on a scalar timestep variable applied at the clip level, which limits their ability to model complex temporal dependencies needed for various tasks like image-to-video generation. To address this limitation, we propose a frame-aware video diffusion model~(FVDM), which introduces a novel vectorized timestep variable~(VTV). Unlike conventional VDMs, our approach allows each frame to follow an independent noise schedule, enhancing the model's capacity to capture fine-grained temporal dependencies. FVDM's flexibility is demonstrated across multiple tasks, including standard video generation, image-to-video generation, video interpolation, and long video synthesis. Through a diverse set of VTV configurations, we achieve superior quality in generated videos, overcoming challenges such as catastrophic forgetting during fine-tuning and limited generalizability in zero-shot methods.Our empirical evaluations show that FVDM outperforms state-of-the-art methods in video generation quality, while also excelling in extended tasks. By addressing fundamental shortcomings in existing VDMs, FVDM sets a new paradigm in video synthesis, offering a robust framework with significant implications for generative modeling and multimedia applications."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 0 6 1 3 0 . 0 1 4 2 : r Preprint. Work in progress. REDEFINING TEMPORAL MODELING IN VIDEO DIFFUSION: THE VECTORIZED TIMESTEP APPROACH Yaofang Liu1, Yumeng Ren1, Xiaodong Cun2, Aitor Artola1, Yang Liu3, Tieyong Zeng4, Raymond H. Chan5, Jean-michel Morel1 1City University of Hong Kong 2Great Bay University 3National University of Defense Technology 4The Chinese University of Hong Kong 5Lingnan University Figure 1: Previous conventional video diffusion models (b) directly extend image diffusion models (a) utilizing single scalar timestep on the whole video clip. This straightforward adaption restricts the flexibilities of VDMs in downstream tasks, e.g., image-to-video generation, longer video generation. In this paper, we propose Frame-Aware Video Diffusion Model (FVDM), which trains the denoiser via vectorized timestep variable (c). Our method attains superior visual quality not only in standard video generation but also enables multiple downstream tasks in zero-shot manner."
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion models have revolutionized image generation, and their extension to video generation has shown promise. However, current video diffusion models (VDMs) rely on scalar timestep variable applied at the clip level, which limits their ability to model complex temporal dependencies needed for various tasks like image-to-video generation. To address this limitation, we propose frame-aware video diffusion model (FVDM), which introduces novel vectorized timestep variable (VTV). Unlike conventional VDMs, our approach allows each frame to follow an independent noise schedule, enhancing the models capacity to capture fine-grained temporal dependencies. FVDMs flexibility is demonstrated across multiple tasks, including standard video generation, image-to-video generation, video interpolation, and long video synthesis. Through diverse set of VTV configurations, we achieve superior quality in generated videos, overcoming challenges such as catastrophic forgetting during fine-tuning and limited generalizability in zero-shot methods.Our empirical evaluations show that FVDM outperforms stateof-the-art methods in video generation quality, while also excelling in extended tasks. By addressing fundamental shortcomings in existing VDMs, FVDM sets new paradigm in video synthesis, offering robust framework with significant implications for generative modeling and multimedia applications. Code repository is https://github.com/Yaofang-Liu/FVDM 1 Preprint. Work in progress."
        },
        {
            "title": "INTRODUCTION",
            "content": "The advent of diffusion models (Song et al., 2020b; Ho et al., 2020) has heralded paradigm shift in generative modeling, particularly in the domain of image synthesis. These models, which leverage an iterative noise reduction process, have demonstrated remarkable efficacy in producing high-fidelity samples. Naturally, we can extend this framework to video generation (Ho et al., 2022; He et al., 2022; Chen et al., 2023a; Wang et al., 2023; Ma et al., 2024; OpenAI, 2024; Xing et al., 2023b) by denoising whole video clip jointly. These methods have shown promising results, yet it has also exposed fundamental limitations in modeling the complex temporal dynamics inherent to video data. The crux of the problem lies in the naive adaptation of image diffusion principles to the video domain. As shown in Fig. 1, conventional video diffusion models (VDMs) typically treat video as monolithic entity, employing scalar timestep variable to govern the diffusion process uniformly across all frames following image diffusion models. While this approach has proven adequate for generating short video clips, it fails to capture the nuanced temporal dependencies that characterize real-world video sequences. This limitation not only constrains the models flexibility but also impedes its scalability in handling more sophisticated temporal structures. The temporal modeling deficiency of current VDMs has spawned plethora of task-specific adaptations, particularly in domains such as image-to-video generation (Xing et al., 2023a; Guo et al., 2023; Ni et al., 2024), video interpolation (Wang et al., 2024a;b), and long video generation (Qiu et al., 2023; Henschel et al., 2024). These approaches have largely relied on two primary strategies: fine-tuning and zero-shot techniques. For instance, DynamiCrafter (Xing et al., 2023a) achieves open-domain image animation through fine-tuning pre-trained VDM (Chen et al., 2023a) conditioned on input images. In the realm of video interpolation, Wang et al. (2024b) propose lightweight fine-tuning technique coupled with bidirectional diffusion sampling process. Concurrently, zero-shot methods such as DDIM inversion (Mokady et al., 2023) and noise rescheduling (Qiu et al., 2023) have been employed to adapt pretrained VDMs for tasks like image-to-video generation (Ni et al., 2024) and long video synthesis (Qiu et al., 2023). However, these approaches often grapple with issues such as catastrophic forgetting during fine-tuning or limited generalizability in zero-shot scenarios, resulting in suboptimal utilization of the VDMs latent capabilities. To address these fundamental limitations, we introduce novel framework: the frame-aware video diffusion model (FVDM). At the heart of our approach lies vectorized timestep variable (VTV) that enables independent frame evolution (shown in Fig. 1(c)). This stands in stark contrast to existing VDMs, which rely on scalar timestep variable that enforces uniform temporal dynamics across all frames. Our innovation allows each frame to traverse its own temporal trajectory during the forward process while simultaneously recovering from noise to the complete video sequence in the reverse process. This paradigm shift significantly enhances the models capacity to capture intricate temporal dependencies and markedly improves the quality of generated videos. The contributions of our work are threefold: Enhanced Temporal Modeling: Introducing the Frame-Aware Video Diffusion Model (FVDM), which utilizes vectorized timestep variable (VTV) to enable independent frame evolution and superior temporal dependency modeling. Numerous (Zero-Shot) Applications: FVDMs flexible VTV configurations support wide array of tasks, including standard video synthesis (i.e., synthesizing video clips), image-to-video transitions, video interpolation, long video generation, and so on, all without re-training. Superior Performance Validation: Our empirical evaluations demonstrate that FVDM not only exceeds current state-of-the-art methods in video quality for standard video generation but also excels in various extended applications, highlighting its robustness and versatility. Our proposed FVDM represents significant advancement in the field of video generation, offering powerful and flexible framework that opens new avenues for both theoretical exploration and practical application in generative modeling. By addressing the fundamental limitations of existing VDMs, FVDM paves the way for more sophisticated and temporally coherent video synthesis, with far-reaching implications for various domains in computer vision and multimedia processing. 2 Preprint. Work in progress."
        },
        {
            "title": "2.1 PRELIMINARIES: DIFFUSION MODELS",
            "content": "Diffusion models have emerged as powerful framework for generative modeling, grounded in the theory of stochastic differential equations (SDEs). These models generate data by progressively adding noise to the data distribution and then reversing this process to sample from the noise distribution (Song et al., 2020b; Karras et al., 2022). In the following, we provide foundational understanding of diffusion models, essential to our work. At the core of diffusion models is the concept of data diffusion, where the original data distribution pdatapxq is perturbed over time r0, via continuous process governed by an SDE: dx µpx, tq dt ` σptq dw, (1) where µp, and σpq represent the drift and diffusion coefficients, and twptqutPr0,T denotes the standard Brownian motion. This diffusion process results in time-dependent distribution ptpxptqq, with the initial condition p0pxq pdatapxq. The generative process in diffusion models is achieved by reversing the diffusion SDE, allowing sampling from an initially Gaussian noise distribution. This reverse process is characterized by the reverse-time SDE using the score function log ptpxq: dx rµpx, tq σptq2x log ptpxqsdt ` σptqd w, (2) where represents the standard Wiener process in reverse time. crucial aspect of this SDE framework is the associated Probability Flow (PF) ODE (Song et al., 2020b), which describes the corresponding deterministic process sharing the same marginal probability densities tptpxquT t0 as the SDE: ȷ dx µpx, tq σptq2x log ptpxq dt. (3) 1 In practice, this reverse process involves training score model to approximate the score function, which is then integrated into the empirical PF ODE for sampling. While diffusion models have shown promise in various domains, their application to video data presents unique challenges, especially the modeling of high-dimensional temporal data. 2.2 FRAME-AWARE VIDEO DIFFUSION MODEL We present novel frame-aware video diffusion model that significantly enhances the generative capabilities of traditional diffusion models by introducing vectorized timestep variable. This approach allows for the independent evolution of each frame in video clip, capturing complex temporal dependencies and improving performance across various video generation tasks. In this section, we provide detailed mathematical formulation of our model, its underlying principles, and its applications. 2.2.1 VECTORIZED TIMESTEP VARIABLE Inherited from image diffusion models, current video diffusion models also employ scalar time variable r0, that applies uniformly across all elements of the data being generated (Xing et al., 2023b). In the context of video generation, this approach fails to capture the nuanced temporal dynamics inherent in video sequences. To address this limitation, we introduce vectorized timestep variable τ ptq : r0, Ñ r0, sN , defined as: τ ptq rτ p1qptq, τ p2qptq, . . . , τ pN qptqsJ (4) where is the number of frames in the video sequence, and τ piqptq represents the individual time variable for the i-th frame. This vectorization allows for independent noise perturbation for each frame, enabling more flexible and detailed diffusion process. 3 Preprint. Work in progress."
        },
        {
            "title": "2.2.2 FORWARD SDE WITH INDEPENDENT NOISE SCALES",
            "content": "We extend the conventional forward stochastic differential equation (SDE) to accommodate our vectorized timestep variable. For each frame xpiq, the forward process is governed by: dxpiq µpxpiq, τ piqqdt ` σpτ piqqdwpiq This formulation allows each frame to experience noise from an independent Gaussian distribution, governed by its specific τ piqptq. (5) For representation simplicity, we integrate all frame SDEs into one single SDE for the whole video. Lets define the video as RN ˆd, where is the number of frames and is the dimensionality of each frame. We can represent the video as matrix: rxp1q, xp2q, . . . , xpN qsJ (6) where each xpiq Rd represents single frame. We can now formulate an integrated forward SDE for the entire video: dX pX, τ ptqqdt ` Σpτ ptqqdW (7) where ppp, τ pppqqqqqq : RN ˆd ˆ r0, Ñ RN ˆd is the drift coefficient for the entire video, Σpτ pqq : r0, Ñ RN ˆN is diagonal matrix of diffusion coefficients, is an standard Brownian motion. The drift and diffusion terms can be expressed as: pX, τ ptqq µpxp1q, τ p1qptqq, µpxp2q, τ p2qptqq, . . . , µpxpN q, τ pN qptqq Σpτ ptqq fi σpτ p1qptqq 0 ... 0 0 σpτ p2qptqq ... 0 . . . σpτ pN qptqq 0 0 ... ffi ffi ffi fl ı (8) (9) This formulation preserves the independent noise scales for each frame while providing unified representation for the entire video. In the context of DDPMs (Ho et al., 2020), the drift coefficient µpxpiq, τ piqptqq and the diffusion coefficient σpτ piqptqq for each frame (where 1 ď ď ) are given by: µpxpiq, τ piqptqq 1 βpτ piqptqq, where βpq is the noise scale function, which is predefined non-negative, non-decreasing function that determines the amount of noise added at each timestep with βp0q 0.1 and βpT 20 (Song et al., 2020b). 2 βpτ piqptqqxpiq, σpτ piqptqq 2.2.3 REVERSE SDE AND SCORE FUNCTION In the context of the reverse process, we define an integrated reverse SDE to encapsulate the dependencies across joint frames: ȷ Σpτ ptqqΣpτ ptqqJX log ptpXq dX pX, τ ptqq 1 2 where is an -dimensional standard Brownian motion with dt ă 0. The score-based model sθp, τ pqq : RN ˆd ˆ r0, Ñ RN ˆd is designed to operate over the entire video sequence. The models learning objective is to approximate the score function: dt ` Σpτ ptqqd (10) sθpX, τ ptqq log ptpXq The optimization problem for the model parameters θ is formulated as: θ arg min θ EtEτ ptq λptqEXp0q EXpτ ptqqXp0q sθpXpτ ptqq, τ ptqq Xpτ ptqq log ptpXpτ ptqqXp0qq ff ı 2 2 (11) (12) is where λpq 1{E Xpτ ptqq log ptpXpτ ptqqXp0qq2 2 Hyvarinen (2005); Sarkka & Solin (2019); Song et al. (2020b). positive weighting function that to , as discussed in the context of score matching in chosen proportional can be Preprint. Work in progress. Figure 2: Diverse Applications of FVDM. (a) Standard Video Generation: Implements uniform timestep across frames, rt, t, . . . , ts. (b) Image-to-Video Generation: Transforms static image into video using customized vectorized timestep, rτ 1, t, . . . , ts, τ 1 0. (c) Video Interpolation: Smoothly interpolates frames between start and end, using rτ 1, t, . . . , t, τ s, τ 1 τ 0. (d) Long Video Generation: Extends sequences by conditioning on final frames, applying rτ 1, . . . , τ , t, . . . , ts, τ 1 ... τ 0 (e) Many More Zero-Shot Applications: Highlights potential for tasks such as any frame conditioning, video transition, and next frame prediction. 2.3 IMPLEMENTATION Network Architecture. Our proposed method can work for all current VDMs backbones with small adaptation. For the sake of simplicity, we choose novel video diffusion transformer model developed by Ma et al. (2024) as our backbone in this work. To adapt the scalar timestep variable to vectorized timestep variable, we replace the original scalar timestep input, which had shape of pBq, with vectorized version pB, q, where is the batch size and is the number of frames. Then, using sinusoidal positional encoding, we transform the input timesteps from shape pB, to pB, N, Dq, where is the embedding dimension. These vectorized timestep embeddings are then fed into the transformer block, where they condition both the attention and MLP layers through adaptive layer norm zero (adaLN-Zero) conditioning (Peebles & Xie, 2023). This process ensures that each frames temporal dynamics are handled independently, resulting in improved temporal fidelity and noise prediction across frames. Training. To address the potential computational explosion inherent in training diffusion models with vectorized timesteps, we introduce novel probabilistic timestep sampling strategy (PTSS). In conventional VDMs, scalar timestep is sampled for each batch element. However, when extending this approach to FVDM, where each frame evolve independently, the naive strategy of sampling different timestep for each frame results in combinatorial explosion, with frames yielding 1000N combinations for 1000 timesteps, compared to just 1000 combinations for scalar timesteps. To mitigate this, we introduce probability that governs the sampling process. With probability p, we sample distinct timesteps for each frame in the sequence, allowing for independent evolution. With probability 1 p, we sample the timestep for the first frame and let the other frames timesteps be the same. This hybrid strategy significantly prevents excessive computational overhead and improves the standard video generation quality while retaining flexibility of frame-wise temporal evolution. An ablation study on different values of demonstrates the effectiveness of this approach, as shown in Fig. 3. Inference. Despite using vectorized timesteps, our model remains compatible with standard diffusion sampling schedules like as DDPM (Ho et al., 2020) and DDIM (Song et al., 2020a). The PTSS allows the model to generalize effectively during inference, using established schedules without requiring new mechanisms. This balances the advantages of vectorized timesteps with the practicality of established diffusion model techniques, facilitating smooth inference process. 5 Preprint. Work in progress."
        },
        {
            "title": "2.4 APPLICATIONS",
            "content": "Beyond standard video generation, our Frame-aware Video Diffusion Model (FVDM) demonstrates remarkable versatility, performing variety of tasks in zero-shot manner, including image-to-video generation, video interpolation, and long video generation, as depicted in Fig. 2. The models ability to flexibly manage complex temporal dynamics through the vectorized timestep variable τ ptq allows it to generalize to broad range of video-related scenarios, extending well beyond conventional video synthesis. Standard Video Generation: In the most basic application, the FVDM operates similarly to traditional video diffusion models. Every frame is initialized with ϵpiq p0, Iq, 1 ď ď , the timestep is applied uniformly across all frames by setting τ ptq 1, where each frame evolves according to the same scalar timestep. This approach mirrors the dynamics of conventional diffusion models, where temporal coherence is maintained across frames. Image-to-Video Generation: Our model is capable of generating dynamic video sequences from static image I. By treating the image as the first frame, xp0q 0 I, we specially design τ piqptq, 1 ď ď , for every frame. Experimentally, we find the simplest way to set the first frame noise-free τ p1qptq 0, while set other frames with regular noise τ piqptq t, 2 ď ď yields satisfactory results. This formulation enables the smooth transformation of still image into coherent, multi-frame video sequence. Video Interpolation: To interpolate intermediate frames between given starting and ending frames, similar to image-to-video generation, this intuitively way is to set the timesteps of the first and last frames to τ p1qptq τ pN qptq 0, and applies regular noise to the intervening frames, i.e., τ piqptq for 1 ă ă . This indeed process results in the smooth synthesis of intermediate frames, ensuring seamless transitions between the start and end frames of the sequence. Long Video Generation: Our model also supports the extension of video sequences by conditioning on the final frames of previously generated clip. Similarly, given the last frames tˆxpk1,iq uN iN `1 from the pk 1qth video clip, we generate the next video clip with new 0 frames by setting τ piqptq 0 for the first frames, where xpk,iq , and applying τ piqptq for ă ă . This method allows for seamless continuation of video sequences without temporal artifacts. 0 ˆxpk1,N `iq 0 Other Possible Applications: Leveraging the flexibility of the VTV τ ptq, our FVDM has great potential to be extended to multitude of tasks. For instance, videos can be generated from any arbitrary frame xphq 0 , 1 ď ď , by treating this frame as noise-free (τ phqptq 0) and applying regular noise to the other frames (τ piqptq for h). Additionally, by generating transitions between two videos, we can connect video clips or predict the next future frame similarly to long video generation but by generating only single frame while maintaining the remaining frames from previous generations. Lastly, we think it should be very interesting to explore diverse inference schedules like noise progressively increase by frames, e.g., τ piqptq infp0.2 t, tq, 1 ď ď for image-to-video generation, and more complex applications like frame-level video editing (Meng et al., 2021) and video ControlNet (Zhang et al., 2023a) based on FVDM in the future."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 SETUP In this section, we detail the experimental setup for evaluating the proposed Frame-Aware Video Diffusion Model (FVDM). Our experiments are designed to assess the models performance across variety of tasks and compare it with state-of-the-art methods. We follow the principles of (Skorokhodov et al., 2022a) to evaluate our model with Frechet Video Distance (FVD) (Unterthiner et al., 2019). Due to limited resources, we conducted ablation studies using batch size of 3 for 200k iterations and trained our model for baseline comparison with batch size of 4 for 250k iterations using two A6000 GPUs or one A800 GPU. We selected four diverse datasets for training and evaluation: FaceForensics (Rossler et al., 2018), SkyTimelapse (Xiong et al., 2018), UCF101 (Soomro, 2012), and Taichi-HD (Siarohin et al., 2019). We compared FVDM with several baselines 6 Preprint. Work in progress. Figure 3: Comprehensive ablation study on FaceForensics dataset (Rossler et al., 2018) for video generation using FVD metric (lower is better) with training iterations from 50k to 200k. Top, bottom left, and bottom right figures indicate ablation studies for sampling probability (p), inference schedule, and model scale, respectively. Method FaceForensics SkyTimelapse UCF101 Taichi-HD MoCoGAN (Tulyakov et al., 2018) VideoGPT (Yan et al., 2021) MoCoGAN-HD (Tian et al., 2021) PVDM (Yu et al., 2023) Latte (Ma et al., 2024) FVDM 124.7 185.9 111.8 355.92 77.70 55. 206.6 222.7 164.1 75.48 110.45 106.09 2886.9 2880.6 1729.6 1141.9 604.64 468.23 - - 128.1 540.2 267.12 194. Table 1: FVD results comparing FVDM with the baseline on four different datasets. Lower FVD values indicate better performance. For Lattes result, we use the official code, and strictly follow the original configuration, except that we train it with batchsize 4 for 250k iterations and inference with DDIM-50, all the same as FVDM. Other results can be sourced in Ma et al. (2024); Skorokhodov et al. (2022b). for standard video generation, including MoCoGAN (Tulyakov et al., 2018), VideoGPT (Yan et al., 2021), MoCoGAN-HD (Tian et al., 2021), DIGAN (Yu et al., 2022), PVDM (Yu et al., 2023), and Latte (Ma et al., 2024). 3.2 ABLATION STUDY We conducted comprehensive ablation study to evaluate the impact of various hyperparameters and model configurations on standard video generation performance. All experiments were performed on the FaceForensics dataset (Rossler et al., 2018) and conducted with models of scale B, training with batch size 3, and inference with DDIM-50 (Song et al., 2020a) if no specification, using the FVD as the primary metric, where lower values indicate better performance. Fig. 3 presents the results of our ablation study graphically. Sampling Probability The first part of our ablation study  (Fig. 3)  investigates the effect of the sampling probability in our PTSS. We observe that the models performance is highly sensitive to Preprint. Work in progress. Figure 4: Qualitative comparison of real samples and generated video samples from FVDM/Ours and Latte (Ma et al., 2024) on four datasets, i.e., FaceForensics (Rossler et al., 2018), SkyTimelapse (Xiong et al., 2018), UCF101 (Soomro, 2012), and Taichi-HD (Siarohin et al., 2019) (from top to bottom). For fair comparison, we select samples either of the same class w.r.t. UCF101 (Soomro, 2012) or with similar content w.r.t. other datasets. FVDM produces more coherent and realistic video sequences compared to the baseline. this parameter, with 0.2 consistently yielding the best results across different training iterations. Notably, at 200k steps, 0.2 achieves an FVD score of 74.31, outperforming both the baseline Latte model (82.28) and other probability values. This finding suggests that moderate level of probabilistic sampling strikes an optimal balance between exploration and exploitation during training. Sampling Schedule In Fig. 3, we compare different sampling schedules, including DDPM (Ho et al., 2020) with 250 steps and DDIM (Song et al., 2020a) with varying step counts (100, 50, and 10). Our results indicate that DDPM-250 and DDIM with 100 or 50 steps perform comparably, with DDIM-100 slightly edging out the others at 200k steps. However, DDIM-10 shows significant performance degradation, suggesting that overly aggressive acceleration of the sampling process can be detrimental to generation quality. Based on these findings, we adopt the DDIM-50 schedule for our subsequent experiments, as it offers good trade-off between efficiency and performance. Model Scale The impact of model scale on generation quality is examined in Fig. 3. We evaluate four model sizes: (32.59M parameters), (129.76M parameters), (457.09M parameters), and XL (674.00M parameters). Our results demonstrate clear trend of improved performance with increasing model scale. The XL model consistently outperforms smaller variants, achieving the best FVD score of 57.25. This observation aligns with the scaling law (Kaplan et al., 2020). 3.3 STANDARD VIDEO GENERATION In our evaluation of standard video generation, FVDM demonstrates superior performance compared to state-of-the-art methods. As shown in Table 1, FVDM achieves the lowest FVD scores on FaceForensics and UCF101, and the second lowest scores on other datasets, outperforming Latte and other leading models. This indicates enhanced video quality and temporal coherence. 8 Preprint. Work in progress. (a) Image-to-video generation (b) Video interpolation (c) Long video generation Figure 5: Zero-shot adaptations of FVDM on FaceForensics (Rossler et al., 2018). (a) image-tovideo generation, where FVDM animates the first image into coherent video sequence; (b) video interpolation, where FVDM generates smooth transitions between given the first frame and the last frame; and (c) long video generation, where FVDM generates long video sequences (we show 128 frames in this figure) while maintaining temporal coherence. FVDM leverages its innovative vectorized timestep variable to enhance temporal dependency modeling, which is evident in its ability to outperform Latte in most categories and maintain competitive performance in others. This effectiveness is further illustrated in Fig. 4, where qualitative comparisons reveal that FVDM generates video sequences with greater fidelity and smoother transitions compared to Latte. The visual results highlight FVDMs capacity to handle complex temporal dynamics, producing high-quality video outputs that closely mimic real-world sequences. This establishes FVDM as robust and versatile tool in the realm of generative video modeling. 3.4 ZERO-SHOT APPLICATIONS OF FVDM To demonstrate the versatility of FVDM, we evaluated its zero-shot performance on tasks such as image-to-video generation, video interpolation, and long video generation. Fig. 5 showcases qualitative results. Image-to-Video Generation: As shown in Fig. 5(a), the model successfully generates smooth and temporally coherent video from single image, demonstrating its ability to infer motion and facial expressions without explicit training on such task. Video Interpolation: FVDM is also capable of generating smooth transitions between given start and end frames. Fig. 5(b) illustrates this capability, where the model interpolates between the first frame and last frame, creating seamless video sequence that maintains the integrity of the original frames while filling in the intermediate motions. 9 Preprint. Work in progress. Long Video Generation: One of the most challenging tasks for generative models is to produce long video sequences while maintaining temporal coherence. FVDM addresses this challenge by generating 128-frame videos that exhibit consistent motion and expression throughout the sequence, as depicted in Fig. 5(c). This demonstrates the models ability to capture long-term dependencies in video data. These zero-shot applications showcase the adaptability of FVDM across different video generation tasks, highlighting its potential for real-world applications where training data may be limited or diverse scenarios need to be addressed without prior fine-tuning. The models performance in these tasks is testament to its robust architecture and the effectiveness of the vectorized timestep variable in capturing complex temporal dynamics."
        },
        {
            "title": "4 RELATED WORK",
            "content": "The limitations in temporal modeling of conventional VDMs have led to surge in approaches tailored to tasks. These methods predominantly rely on fine-tuning or employing zero-shot techniques to handle domain-specific challenges. Image-to-Video Generation. Notably, DynamiCrafter (Xing et al., 2023a) introduces model that animates open-domain images by utilizing video diffusion priors and projecting images into context representation space. Furthermore, I2V-Adapter (Guo et al., 2023) presents general adapter for VDMs that can convert static images into dynamic videos without altering the base models structure or pretrained parameters. I2VGen-XL (Zhang et al., 2023b) addresses semantic accuracy and continuity through cascaded diffusion model that initially produces low-resolution videos and then refines them for clarity and detail enhancement. Li et al. (2024) tackles fidelity loss in I2V generation by adding noise to the image latent and rectifying it during the denoising process, resulting in videos with improved detail preservation. Lastly, TI2V-Zero (Ni et al., 2024) introduces zero-shot image conditioning method for text-to-video models, enabling frame-by-frame video synthesis from an input image without additional training or tuning. Video Interpolation. MCVD (Voleti et al., 2022) stands out as the first to address this task using diffusion models, which presents conditional score-based denoising diffusion model capable of handling future/past prediction, unconditional generation, and interpolation with single model. Besides, LDMVFI (Danier et al., 2024) introduces latent diffusion model that formulates video frame interpolation as conditional generation problem, showing superior perceptual quality in interpolated videos, especially at high resolutions. Meanwhile, generative inbetweening (Wang et al., 2024b) adapts image-to-video models to perform high-quality keyframe interpolation, demonstrating the versatility of these models for video-related tasks. Finally, EasyControl (Wang et al., 2024a) transfers ControlNet (Zhang et al., 2023a) to video diffusion models, enabling controllable generation and interpolation with significant improvements in evaluation metrics. Long Video Generation. On the one hand, ExVideo (Duan et al., 2024) enhances the video diffusion models capacity to generate videos five times longer than the original models duration through parameter-efficient post-tuning strategy. Meanwhile, StreamingT2V (Henschel et al., 2024) introduces conditional attention module and an appearance preservation module to generate long videos with smooth transitions through an autoregressive approach. Moreover, SEINE (Chen et al., 2023b) focuses on creating long videos with smooth transitions and varying lengths of shot-level videos through random mask video diffusion model. On the other hand, FreeNoise (Qiu et al., 2023), FIFO-Diffusion (Kim et al., 2024), and FreeLong (Lu et al., 2024) achieve long video generation without additional training by noise rescheduling, iterative diagonal denoising, and SpectralBlend temporal attention, respectively."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "We introduced the Frame-Aware Video Diffusion Model (FVDM), which addresses key limitations in existing video diffusion models by employing vectorized timestep variable (VTV) for independent frame evolution. This approach significantly improves the video quality and flexibility of video generation across various tasks, including image-to-video, video interpolation, and long video synthesis. Extensive experiments demonstrated FVDMs superior performance over state-of-the-art 10 Preprint. Work in progress. models, highlighting its adaptability and robustness. By enabling finer temporal modeling, FVDM sets new standard for video generation and offers promising direction for future research in generative modeling. Potential extensions include better training schemes and different VTV configurations for other tasks like video infilling. In conclusion, FVDM paves the way for more sophisticated, temporally coherent generative models, with broad implications for video synthesis and multimedia processing."
        },
        {
            "title": "REFERENCES",
            "content": "Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023a. Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023b. Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video frame interpolation with latent diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 14721480, 2024. Zhongjie Duan, Wenmeng Zhou, Cen Chen, Yaliang Li, and Weining Qian. Exvideo: Extending video diffusion models via parameter-efficient post-tuning. arXiv preprint arXiv:2406.14130, 2024. Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin Huang, Pengfei Wan, et al. I2v-adapter: general image-to-video adapter for video diffusion models. arXiv preprint arXiv:2312.16693, 2023. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. 2022. Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(Apr):695709, 2005. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. arXiv preprint arXiv:2405.11473, 2024. Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, and Bo Zheng. Tuning-free noise rectification for high fidelity image-to-video generation. arXiv preprint arXiv:2403.02827, 2024. 11 Preprint. Work in progress. Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. arXiv preprint arXiv:2407.19918, 2024. Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60386047, 2023. Haomiao Ni, Bernhard Egger, Suhas Lohit, Anoop Cherian, Ye Wang, Toshiaki Koike-Akino, Sharon Huang, and Tim Marks. Ti2v-zero: Zero-shot image conditioning for text-to-video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 90159025, 2024. OpenAI. Sora: Creating video from text. https://openai.com/sora, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. Faceforensics: large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179, 2018. Simo Sarkka and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019. Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In Conference on Neural Information Processing Systems (NeurIPS), December 2019. Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 36263636, 2022a. Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 36263636, 2022b. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris Metaxas, and Sergey Tulyakov. good image generator is what you need for high-resolution video synthesis. arXiv preprint arXiv:2104.15069, 2021. Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 15261535, 2018. 12 Preprint. Work in progress. Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. Advances in neural information processing systems, 35:2337123385, 2022. Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, and Xiaodan Liang. Easycontrol: Transfer controlnet to video diffusion for controllable generation and interpolation. arXiv preprint arXiv:2408.13005, 2024a. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steven Seitz. Generative inbetweening: Adapting image-to-video models for keyframe interpolation. arXiv preprint arXiv:2408.15239, 2024b. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023a. Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. arXiv preprint arXiv:2310.10647, 2023b. Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 23642373, 2018. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022. Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1845618466, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023a. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023b."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "Great Bay University",
        "Lingnan University",
        "National University of Defense Technology",
        "The Chinese University of Hong Kong"
    ]
}