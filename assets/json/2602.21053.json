{
    "paper_title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
    "authors": [
        "Shimin Wen",
        "Zeyu Zhang",
        "Xingdou Bian",
        "Hongjie Zhu",
        "Lulu He",
        "Layi Shama",
        "Daji Ergu",
        "Ying Cai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent."
        },
        {
            "title": "Start",
            "content": "OCR-Agent: Agentic OCR with Capability and Memory Reflection Shimin Wen1 Zeyu Zhang2 Xingdou Bian1 Hongjie Zhu1 Lulu He1 Layi Shama1 Daji Ergu1 Ying Cai3 1Southwest Minzu University 2AI Geeks Project lead. Corresponding author: caiying@swun.edu.cn. 6 2 0 2 4 2 ] . [ 1 3 5 0 1 2 . 2 0 6 2 : r (VLMs) have AbstractLarge Vision-Language Models demonstrated significant potential on complex visual unthrough iterative optimization methderstanding tasks ods.However, these models generally lack effective selfcorrection mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent. 1. Introduction Optical Character Recognition (OCR) constitutes fundamental technology that bridges the visual and textual domains, aiming to extract and interpret text from images into machine-readable formats. Most recently, Large VLMs have demonstrated exceptional promise in OCR-related tasks, exhibiting strong zero-shot abilities that can be further enhanced through task-specific fine-tuning, as evidenced by models like OlmOCR. However, directly transferring these prompting strategies to VLMs has not yielded satisfactory results [1]in some question-answering scenarios, performance may even fall short of direct answering. This is largely due to two key challenges: capability hallucination, where models propose actions beyond their executable scope (e.g., image enhancement or human proofreading), and refinement stagnation, where models get stuck in repetitive or ineffective correction Figure 1. Overview of OCR-Agent. The model iteratively refines its answer by (1) Capability Reflection to filter infeasible actions (e.g., enhance image), and (2) Memory Reflection to avoid repeating past mistakes, enabling stable, training-free self-correction. loops. Current research primarily focuses on workarounds such as fine-tuning [2] or reinforcement learning [3], rather than addressing these core reasoning failures.This gap highlights the need for reasoning-focused approach that enables models to self-correct within their inherent capabilities. To this end, we demonstrate that carefully designed and properly constrained self-reflection mechanisms can enable CoT prompting to achieve sustained and stable performance improvement, effectively mitigating the limitations of standard CoT in vision-language tasks. We propose OCR-Agent, novel reflective framework designed to significantly enhance the stability and effectiveness of iterative self-correction in VLMs. Our agent architecture incorporates two key mechanisms: multi-turn Capability Reflection, which guides the model to diagnose errors and adaptively plan corrective actions, and Memory Reflection, which enables the agent to retain and leverage historical reasoning tracesavoiding redundant attempts while exploring new solution pathways. Experimental results on the OCRBench v2 benchmark show that our method delivers substantial improvements over naive, chain-of-thought (CoT) prompting, and SelfRefine, especially in tasks demanding fine-grained visualtextual alignment and multi-step reasoning.An overview of OCR-Agent is provided in Fig 2. To summarize, our contributions are the following: We demonstrate that specific self-reflection mechanisms can consistently and effectively enhance the performance of VLMs. We propose training-free OCR-Agent with two key mechanisms: Capability Reflection and Memory Reflection. We conduct experiments on the OCRBench v2, demonstrating that our method outperforms the open-source SOTA model InternVL3-8B by 2.0 points on the English and 1.2 points on the Chinese subset. 2. Related Work Optical Character Recognition (OCR). OCR is key technology aimed at converting text within images into machine-readable format. Traditional OCR methods, such as earlier versions of the Tesseract engine [4], typically employ multi-stage pipeline. In the character recognition stage, these methods often rely on pattern matching (or statistical pattern recognition) to model partial features of characters, and utilize simple contextual rules or statistical associations [5]. To overcome the limitations of traditional methods, researchers have gradually shifted towards end-to-end models based on deep learning, with approaches combining computer vision (CV) and natural language processing (NLP) becoming mainstream. For instance, the CRNN model [6] employs Convolutional Neural Network (CNN) for feature extraction and combines it with Recurrent Neural Network (RNN) for sequence transcription. The computer visionbased approach has enhanced the adaptability of text detection and instance segmentation [7], and has proven effective in modeling the spatial structure of entire documents [8]. Subsequently, Transformer-based models, such as TrOCR [9], leverage their powerful self-attention mechanisms to capture long-range dependencies. In recent years, the rise of large VLMs has propelled OCR technology into new phase of development. Studies on pretrained VLMs [10] have revealed their remarkable zero-shot OCR capabilities, with further fine-tuning leading to significant performance improvements. For example, Olmocr [11] is fine-tuned based on Alibabas Qwen-2.57B-Instruct model. However, when confronted with few-shot text and real-world complexities, VLMs still face challenges in handling intricate problems and achieving practical deployment [12] [13]. Self Reflection. Chain of Thought (CoT) [14] is one of the pioneering works that enables large language models to acquire multi-step reasoning capabilities. Through simple prompting (Zero-shot-CoT) [15] or fine-tuning [16] [17], CoT guides the model to generate series of intermediate reasoning steps before providing final answer. Building upon CoT, Self-Refine [18] introduced more general framework for iterative improvement, and the Reflexion framework [19] elevates the concept of self-reflection to new level by structuring the language model as an agent capable of reflecting on task feedback and maintaining its own memory. 3. Method 3.1. Overview We propose an iterative self-reflection framework designed specifically for VLMs. This framework extends the paradigm of self-reflection from language models, moving beyond the correction of textual logic to address multimodal errors caused by visual perception through unique reflection-refinement iterative loop. The model first performs reflection, tracing and attributing output errors to specific visual features within the image. More critically, we introduce capability constraint mechanism to govern this process. This mechanism requires the model to be aware of its own capability boundaries when planning corrective steps, proactively filtering out any capability hallucinations it cannot execute. Subsequently, in the refinement stage, this reflection which is both visually insightful and practically feasible is converted into an internal directive that guides the model to refocus on and deeply reinterpret the key areas of the image.The framework visualization are shown in Fig 3. 3.2. Capability Reflection We proposed the capability reflection, in which the models post-hoc planning is aware of its own capabilities and excludes any infeasible actions. In typical multi-round self-reflection (e.g. for OCR transcription/translation), the model may suggest steps like apply image enhancement or add human proofreading actions it cannot actually perform. Such suggestions are form of capability hallucination, i.e. requests beyond the models abilities [20]. Prior work shows that self-reflection generally improves model outputs [18], [19], [21], but without constraints the model can propose invalid plans. We address this by filtering the chain-of-thought plan so that only model-executable steps remain. By restricting proposals to what the model can execute, capability reflection ensures each refinement step is realistic and grounded in the models actual capabilities. Let the previous answer (or transcription) be yprev and let = {a1, a2, . . . , an} be the chain-of-thought plan proposed by the model in the reflection step. We define feasibility indicator ϕ(a) that equals 1 if action is within the models capability set and 0 otherwise. Formally: ϕ(a) = 1 0 if can be executed by the model (e.g., text-based operations) if is infeasible (e.g., image enhance) Using this indicator, we obtain the filtered plan Pfeas = { : ϕ(a) = 1 }. Finally, the models refinement mechanism (itself the LLM reasoning function) uses the feasible plan to produce Figure 2. Overview of OCR-Agent. an improved answer. If denotes the original input (e.g. the image and any context), the updated answer is ynew = R(x, yprev, Pfeas). Here R() denotes the models generation given the input, the previous answer, and only the filtered plan. In effect, we enforce Pfeas = {a is within capability} ynew = R(x, yprev, Pfeas) ensuring the refinement only relies on feasible, modelplausible actions. This formalism cleanly captures the stepwise process of capability reflection. Within iteration i, the mechanism proceeds in the following steps: Reflection Generation: Conditioned on the image I, question Q, the previous answer Ai1, and the current memory store Mi, the model generates new reflection Ri. This reflection aims to identify the inadequacies in Ai1. Ri = Reflect(I, Q, Ai1, Mi) Memory Update: The newly generated reflection Ri is incorporated into the memory store, creating an updated and more comprehensive historical record to guide the final refinement. Mi+1 = Mi {Ri} 3.3. Memory Reflection We introduced memory reflection, designed to overcome the refinement stagnation and ineffective looping problems common in conventional iterative correction models. Unlike traditional models that may repeatedly attempt the same flawed strategies, our mechanism explicitly records and leverages history of reflections. This ensures that each new refinement is informed by the entirety of past experiences, thus preventing redundant exploration of incorrect solution paths. This process can be formalized as follows: Let be the image input, be the question, Ai be the answer at iteration i, and Ri be the reflection generated at iteration i. The initial answer, produced by zero-shot inference, is denoted as A0. We define the Reflection Memory Store Mi as the set of all historical reflections prior to iteration i: Mi = {R1, R2, . . . , Ri1} Guided Refinement: the model generates new, Instead of directly correcting Ai1, improved answer Ai by conditioning on the original inputs and the updated, complete memory store Mi+1. Ai = Refine(I, Q, Mi+1) 4. Experiments 4.1. Dataset and Metrics Datasets. To comprehensively and systematically evaluate the effectiveness of our proposed framework, we conducted series of rigorous experiments on the comprehensive benchmark, OCRBench v2 [22]. This benchmark was chosen for its exceptional quality and comprehensive challenges: it contains over 10,000 manually verified questionanswer pairs, covers both Chinese and English data, and includes high proportion of difficult samples, enabling an effective assessment of the upper limits of models Figure 3. visual comparison of results from the Naive, CoT, and our proposed methods. The examples in the Ours column represent the final output after the last round of iteration. Algorithm 1 OCR-Agent with Capability and Memory Reflection Require: Image I, Question Q, Initial answer A0, Max iterations Ensure: Final refined answer AT 1: Initialize reflection memory: M1 2: for = 1 to do 3: Ri Reflect(I, Q, Ai1, Mi) 4: 5: ExtractPlan(Ri) {Extract CoT plan from reflection} Pfeas {a ϕ(a) = 1} {ϕ(a) = 1 if model can execute a} Ai Refine(I, Q, Ai1, Pfeas, Mi {Ri}) 6: 7: Mi+1 Mi {Ri} 8: end for 9: return AT capabilities. Its task design extends beyond traditional text transcription to complex scenarios requiring deep visuotextual understanding, such as structured data extraction, visual question answering, and reasoning, which allows for thorough examination of our frameworks generalization ability across different languages and difficulty levels. Metrics. For evaluation, we strictly adhere to its official standards, employing evaluation metrics tailored to six core task types. Specifically, for Parsing tasks, we use the TreeEdit Distance Similarity (TEDS) to evaluate the quality of conversion to structured formats; for Localization tasks, the Intersection over Union (IoU) is used to measure the accuracy of predicted regions; for Extraction tasks, the F1 score is used to assess the performance of key information extraction and mapping; for Long Reading, we use combination of BLEU, METEOR, F1 score, and Edit Distance for evaluation; for Counting tasks, we employ normalized L1 distance to measure the accuracy of the counts; finally, for Basic VQA, we use Exact String Matching, containment check, and the Average Normalized Levenshtein Similarity (ANLS) for evaluation, depending on the answers length and type. 4.2. Implementation Details For each sample in the OCRBench v2 dataset, our method first performs standard, simple inference to generate an initial baseline answer. Subsequently, the system enters an iterative refinement process, which is fixed for three rounds (N=3). In each iteration i, this process involves two core stages: 1. Reflection Generation: We utilize reflection prompt template, which integrates the original question, the answer from the previous iteration (i-1), and the accumulated history of reflections, to have the model generate new reflective text. 2. Guided Refinement: We then feed the original question and the updated, complete history of reflections into refinement prompt template to guide the model in generating the revised answer for iteration i. This answer serves as the input for the next iteration, thereby TABLE 1. PERFORMANCE OF VLMS ON ENGLISH SUBSETS OF OCRBENCH V2 [22]. Method Recognition Referring Spotting Extraction Parsing Calculation Understanding Reasoning Average LLaVA-Next-8B LLaVA-OV-7B Monkey-8B TextMonkey-8B Molmo-7B Cambrian-1-8B Pixtran-12B InternVL3-8B Deepseek-vl-2-Small-16B MiniCPM-V-2.6-7B GLM-4V-9B Ovis2-8B RolmOCR-7B GPT-4o GPT-4o-mini Gemini-Pro Claude3.5-sonnet Step-1V OCR-Agent-7B (Ours) 41.3 46.0 35.2 39.1 52.4 45.3 48.9 68.6 62.7 66.9 61.8 73.2 63.3 61.2 57.9 61.2 62.2 67.8 71.8 18.8 20.8 0 0.7 21.3 21.5 21.6 30.4 28.0 29.5 22.6 24.6 25.4 26.7 23.3 39.5 28.4 31. 27.7 Open-source VLMs 49.5 58.3 16.6 19.0 45.5 53.6 66.3 85.3 77.5 70.8 71.7 62.4 53.5 Closed-source VLMs 77.5 70.8 79.3 56.6 73.6 21.2 25.3 16.3 12.2 7.6 19.2 35.5 34.0 32.7 33.4 31.6 44.8 12.9 36.3 31.5 39.2 37.8 37.2 0 0.1 0 0 0.1 0 0 8.8 0.1 0.5 0 0.7 0.1 0 0.6 13.5 1.3 7. 0.7 80.8 39.4 17.3 23.3 14.4 19.0 28.5 19.5 29.8 27.1 14.3 31.9 22.6 40.6 27.8 43.4 38.8 47.7 40.8 27.8 41. 55.2 64.4 59.8 61.1 65.3 63.5 66.9 77.5 77.1 69.9 72.1 72.7 73.1 71.1 65.9 75.5 73.5 69.8 79.9 48.9 53.0 42.3 40.2 55.0 55.5 53.7 60.3 53.9 57.9 58.4 62.6 51.4 55.5 55.1 59.3 60.9 58.6 66. 31.5 36.4 23.1 23.9 34.5 34.7 40.3 49.0 43.3 45.1 42.6 47.7 38.4 46.5 43.0 51.9 45.2 46.7 51.0 Figure 4. The performance improvement of OCR-Agent in understanding and reasoning as the number of trials increases. (a)&(c) Understanding Scores in English and Chinese: It depicts how the understanding capabilities of CoT only, Self-Refine, and OCR-Agent evolve with increasing trial numbers. (b)&(d) Reasoning Scores in English and Chinese: It shows the reasoning ability changes of these methods as trial numbers proceed. enabling progressive refinement. The entire inference process was executed on four NVIDIA 3090 GPUs. 4.3. Main Results We conduct comprehensive evaluation of the proposed OCR-Agent framework on the highly challenging OCRBench v2 benchmark, covering both English and Chinese scenarios across eight core tasks: Recognition, Referring, Spotting, Extraction, Parsing, Calculation, Understanding, and Reasoning. As shown in Tables 1 and 2, our method significantly outperforms current mainstream open-source and closed-source Vision-Language Models (VLMs) without requiring any additional training. On the English subset, OCR-Agent achieves an average score of 51.01, surpassing all open-source models and closely approaching the strongest closed-source model, Gemini-Pro (51.9). Notably, it achieves the highest scores on the two most challenging tasks Visual Understanding (79.9) and Visual Reasoning (66.5) demonstrating the powerful advantage of our dual-reflection mechanism in handling complex multimodal reasoning. On the Chinese subset, OCR-Agent also delivers outstanding performance, achieving an average score of 54.72, ranking second only to the current top-performing opensource model, Qwen2.5-VL-7B (55.6). It sets new opensource records in Text Recognition (77.0), Information Extraction (68.8), and Visual Understanding (65.1). Importantly, while the base RolmOCR-7B model scores only 38.6 on Chinese tasks, OCR-Agent boosts its performance by nearly 16 points, highlighting the frameworks strong generalization and enhancement capability across diverse base models. Overall, OCR-Agent despite its relatively lightweight size (7B parameters) outperforms larger models such as Pixtral-12B and Deepseek-VL2-16B on multiple key tasks. This convincingly validates the effectiveness of our Capability Reflection and Memory Reflection mechanisms in guiding efficient, grounded, and iterative self-correction. 4.4. Ablation Study To systematically evaluate the contribution of each component, we conduct detailed ablation study of OCR-Agent TABLE 2. PERFORMANCE OF VLMS ON CHINESE SUBSETS OF OCRBENCH V2 [22]. Method Recognition Extraction Parsing Understanding Reasoning Average LLaVA-Next-8B LLaVA-OV-7B Monkey-8B TextMonkey-8B Molmo-7B Cambrian-1-8B Pixtral-12B InternVL3-8B Deepseek-VL2-Small-16B MiniCPM-o-2.6-7B GLM-4V-9B Ovis2-8B RolmOCR-7B GPT-4o GPT-4o-mini Gemini-Pro Claude3.5-sonnet Step-1V OCR-Agent-7B (Ours) 5.7 14.8 4.6 23.5 7.1 5.3 13.4 68.9 60.9 53.0 24.4 72.2 36. 21.6 13.1 52.5 21.0 56.7 77.0 2.9 15.7 11.2 14.8 15.0 14.9 10.9 62.0 50.6 49.4 60.6 50.8 64.9 Open-source VLMs 12.2 13.7 8.4 8.4 9.2 12.6 21.0 31.6 28.3 27.1 20.4 37.7 15.3 Closed-source VLMs 29.8 27.2 30.9 35.2 37.6 53.0 38.9 47.3 56.2 41.1 69. 22.9 7.5 16.0 21.5 19.9 9.0 8.5 7.0 57.9 53.0 43.5 52.8 47.9 45.4 38.5 28.8 51.5 55.0 38.3 65.1 17.2 28.7 20.0 12.2 23.7 8.1 20.7 47.3 20.5 32.7 25.2 37.4 26.7 18.2 16.9 33.4 30.5 39. 39.8 9.1 17.8 13.1 15.8 12.8 9.9 14.6 53.5 42.7 41.1 36.6 49.2 37.7 32.2 25.0 43.1 39.6 42.6 54.7 TABLE 3. COMPARISON OF DIFFERENT METHODS (INCLUDING NAIVE ROLMOCR AND SELF-REFLECTION VARIANTS) ON ENGLISH SUBSETS OF OCRBENCH V2. Method Recognition Referring Spotting Extraction Parsing Calculation Understanding Reasoning Average Naive CoT Self-Refine Capability Reflection Memory Reflection Capability & Memory 63.3 64.5 65.4 67.5 69.6 71.8 25.4 25.1 25.3 26.1 26.9 27. 0.1 0.1 0.1 0.3 0.5 0.7 53.5 76.5 78.3 79.1 79.9 80.8 12.9 17.8 19.4 26.1 32.7 39.4 27.8 31.9 33.3 36.0 38.6 41.3 73.1 73.2 73.3 75.5 77.7 79.9 51.4 51.9 52.1 56.9 61.7 66. 38.4 42.0 43.4 45.9 48.4 51.0 on both the English and Chinese subsets of OCRBench v2. All variants are compared against the standard Self-Refine baseline. As summarized in Tables 3 and 4, OCR-Agent achieves consistent and stable performance gains across both Chinese and English tasks, with particularly pronounced improvement on Understanding and Reasoning tasks as the number of iteration rounds increases. As visualized in Fig 4, the performance curves clearly demonstrate that while baseline methods (CoT, Self-Refine) plateau or fluctuate after the first or second iteration, OCRAgent continues to improve steadily across all three rounds especially in high-complexity tasks such as English Reasoning (Fig 4.1) and Chinese Understanding (Fig 4.1). This sustained progression validates the effectiveness of our dual-reflection design in avoiding stagnation and enabling deep iterative refinement. When Capability Reflection and Memory Reflection are combined, the full OCR-Agent achieves peak performance, with average scores of 51.0 (English) and 54.7 (Chinese), demonstrating clear complementary effect. Notably, on the Chinese Recognition task, performance surges from 37.7 to 77.0, highlighting the frameworks strong task adaptability. In summary, Capability Reflection ensures the feasibility of each refinement step, while Memory Reflection enables progressive exploration across iterations. Together, they form an iterative optimization framework that significantly outperforms traditional self-refinement methods. 5. Limitation and Future Work While OCR-Agent demonstrates significant performance improvements through its novel self-correction framework, several limitations remain, pointing towards promising directions for future research. Computational Overhead: The iterative reflection and refinement process inherently requires multiple calls to the large VLM for single input, increasing inference time and computational cost compared to single-pass models. This could hinder deployment in real-time applications. Our three-round iteration scheme, while effective, may also be inefficient for simpler problems that require fewer steps and redundant for extremely complex ones that need more. Dependence on Base Model Capabilities: The frameworks effectiveness is ultimately bounded by the inherent capabilities of the base VLM (e.g., RolmOCR). If the base model fundamentally misperceives critical visual element or lacks specific knowledge, the reflection process may be unable to recover from this initial error, leading to refinement within wrong context. TABLE 4. COMPARISON OF DIFFERENT METHODS (INCLUDING NAIVE ROLMOCR AND SELF-REFLECTION VARIANTS) ON CHINESE SUBSETS OF OCRBENCH V2. Method Recognition Extraction Parsing Understanding Reasoning Average Naive CoT Self-Refine Capability Reflection Memory Reflection Capability & Memory 36.5 37.1 37.7 50.8 63.9 77.0 64.9 66.5 68.8 68.9 69.0 69.1 15.3 14.1 13.7 16.8 19.8 22.9 45.4 46.9 48.4 54.0 59.5 65.1 26.7 27.5 28.7 32.4 36.1 39.8 37.7 38.4 39.4 44.6 49.7 54. tools (e.g., integrating external Future directions include optimizing the frameworks efficiency through dynamic iteration control and model distillation, image superresolution APIs) to overcome inherent model limitations, and extending its application to broader vision-language tasks such as chart understanding and multimodal reasoning. Further enhancements to the memory mechanism, such as structured knowledge storage, and exploring human-in-theloop refinement for high-stakes scenarios, are also promising avenues. 6. Conclusion In this paper, we identified that unconstrained selfreflection often leads to unstable reasoning and ineffective corrections, ultimately impairing model performance. To overcome these limitations, we proposed OCRAgent, novel reflection-based framework that incorporates two core mechanisms: Capability Reflection, and Memory Reflection. Together, these components enable more structured and sustainable self-correction, leading to consistent performance improvements without additional fine-tuning. Extensive experiments on the OCRBench v2 benchmark demonstrate that our approach significantly outperforms both direct answering, standard CoT prompting, and simple self-refine strategies, confirming its effectiveness in enhancing VLM robustness in text-rich visual understanding tasks. Our findings affirm that carefully constrained selfreflection can unlock more robust and sustainable reasoning in VLMs, paving the way for more reliable and interpretable multimodal systems. Future work will focus on optimizing the computational efficiency of the reflection process and extending the framework to broader range of visionlanguage tasks. [17] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister, Distilling step-by-step! outperforming larger language arXiv models with less training data and smaller model sizes, preprint arXiv:2305.02301, 2023. [18] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al., Self-refine: Iterative refinement with selffeedback, Advances in Neural Information Processing Systems, vol. 36, pp. 4653446594, 2023. [19] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Reflexion: Language agents Narasimhan, and Shunyu Yao, with verbal reinforcement learning, Advances in Neural Information Processing Systems, vol. 36, pp. 86348652, 2023. [20] Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, and Jing Shao, Assessment of multimodal large language models in alignment with human values, arXiv preprint arXiv:2403.17830, 2024. [21] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao, React: Synergizing reasoning and acting in language models, in International Conference on Learning Representations (ICLR), 2023. [22] Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxing Huang, Zhang Li, Guozhi Tang, Bin Shan, Chunhui Lin, Qi Liu, Binghong Wu, Hao Feng, Hao Liu, Can Huang, Jingqun Tang, Wei Chen, Lianwen Jin, Yuliang Liu, and Xiang Bai, Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning, ArXiv, vol. abs/2501.00321, 2024."
        },
        {
            "title": "References",
            "content": "[1] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al., Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?, in European Conference on Computer Vision. Springer, 2024, pp. 169186. [2] Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu, Vision-language models can self-improve reasoning via reflection, arXiv preprint arXiv:2411.00855, 2024. [3] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Vl-rethinker: Incentivizing self-reflection of and Wenhu Chen, vision-language models with reinforcement learning, arXiv preprint arXiv:2504.08837, 2025. [4] R. Smith, in Ninth An overview of the tesseract ocr engine, International Conference on Document Analysis and Recognition (ICDAR 2007), 2007, vol. 2, pp. 629633. [5] Shunji Mori, Ching Suen, and Kazuhiko Yamamoto, Historical review of ocr research and development, Proceedings of the IEEE, vol. 80, no. 7, pp. 10291058, 1992. [6] Baoguang Shi, Xiang Bai, and Cong Yao, An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, pp. 22982304, 2015. [7] Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He, Wenhao Wu, and Cong Yao, Textsnake: flexible representation for detecting text of arbitrary shapes, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 2036. [8] Anoop Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian Brarda, Steffen Bickel, Johannes Hohne, and Jean Baptiste Faddoul, Chargrid: Towards understanding 2d documents, arXiv preprint arXiv:1809.08799, 2018. [9] Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei A. F. Florˆencio, Cha Zhang, Zhoujun Li, and Furu Wei, Trocr: Transformer-based optical character recognition with pre-trained models, ArXiv, vol. abs/2109.10282, 2021. [10] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee, Visual instruction tuning, Advances in neural information processing systems, vol. 36, pp. 3489234916, 2023. [11] Jake Poznanski, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, and Luca Soldaini, olmocr: Unlocking trillions of tokens in pdfs with vision language models, ArXiv, vol. abs/2502.18443, 2025. [12] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai, Ocrbench: on the hidden mystery of ocr in large multimodal models, Science China Information Sciences, vol. 67, no. 12, pp. 220102, 2024. [13] Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, et al., Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning, arXiv preprint arXiv:2501.00321, 2024. [14] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou, Chain of thought prompting elicits reasoning in large language models, ArXiv, vol. abs/2201.11903, 2022. [15] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa, Large language models are zero-shot reasoners, Advances in neural information processing systems, vol. 35, pp. 2219922213, 2022. [16] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo, The cot collection: Improving zero-shot and few-shot learning of language models via chain-ofthought fine-tuning, arXiv preprint arXiv:2305.14045, 2023."
        }
    ],
    "affiliations": [
        "AI Geeks",
        "Southwest Minzu University"
    ]
}