{
    "paper_title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching",
    "authors": [
        "Emanuele Aiello",
        "Umberto Michieli",
        "Diego Valsesia",
        "Mete Ozay",
        "Enrico Magli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Personalized image generation requires text-to-image generative models that capture the core features of a reference subject to allow for controlled generation across different contexts. Existing methods face challenges due to complex training requirements, high inference costs, limited flexibility, or a combination of these issues. In this paper, we introduce DreamCache, a scalable approach for efficient and high-quality personalized image generation. By caching a small number of reference image features from a subset of layers and a single timestep of the pretrained diffusion denoiser, DreamCache enables dynamic modulation of the generated image features through lightweight, trained conditioning adapters. DreamCache achieves state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters, and is both more computationally effective and versatile than existing models."
        },
        {
            "title": "Start",
            "content": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching Emanuele Aiello Umberto Michieli Politecnico di Torino Diego Valsesia Mete Ozay Samsung R&D Institute UK Enrico Magli 4 2 0 N 6 2 ] . [ 1 6 8 7 7 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Personalized image generation requires text-to-image generative models that capture the core features of reference subject to allow for controlled generation across different contexts. Existing methods face challenges due to complex training requirements, high inference costs, limited flexibility, or combination of these issues. In this paper, we introduce DreamCache, scalable approach for efficient and high-quality personalized image generation. By caching small number of reference image features from subset of layers and single timestep of the pretrained diffusion denoiser, DreamCache enables dynamic modulation of the generated image features through lightweight, trained conditioning adapters. DreamCache achieves state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters, and is both more computationally effective and versatile than existing models. Project Page. 1. Introduction Recent advancements in text-to-image generation, fueled by the development of diffusion models [11, 31], have enabled high-quality and diverse image generation from textual descriptions. Diffusion models [26, 28] gradually transform random noise into images through sequence of denoising steps, conditioned on the input text prompt. An active area of research is personalizing these models, enabling the generation of novel images of reference subject in various contexts, while maintaining flexibility for text-based editing. Early personalization techniques [1, 7, 9, 27, 33, 35, 38, 38], such as the seminal DreamBooth [27] relied on fine-tuning (FT) the generative model for each reference subject. However, these approaches are often impractical for many use cases due to costly test-time FT, which can take several minutes per subject. To address this, FT-free (i.e., zero-shot) personalized image generation methods have emerged to eliminate test-time optimization. These FT-free approaches can be broadly categorized into two families: encoder-based methods and reference-based methods, each with distinct drawbacks. Figure 1. DreamCache is finetuning-free personalized image generation method that achieves an optimal balance between subject fidelity, memory efficiency, and adherence to text prompts. Encoder-based methods [8, 16, 21, 36, 39] utilize dedicated image encoders, such as CLIP [24] or DINO [3], to extract relevant features from reference images. While these encoders can produce high-quality results, they are often large, require extensive training to align text and image features, and reduce the models flexibility [14, 16, 20, 39]. In contrast, reference-based methods [23, 40] condition the diffusion model directly on reference features drawn from the U-Net denoiser, integrating these features at each denoising step. While effective, these methods require feature extraction at every generation step, leading to higher computational costs and memory demands. Additionally, they often require an input textual caption for conditioning, which introduces variability and can decrease output precision. Some recent works have proposed to finetune the U-Net backbone itself [23, 40, 42]. However, this hinders the models ability to switch between personalized and nonpersonalized tasks and risks inducing the language drift phenomenon, where the personalization training degrades the models linguistic comprehension [13, 27]. In this work, we propose DreamCache, novel finetuningfree approach to personalized image generation, that over1 Table 1. Methods overview. Our DreamCache achieves state-of-the-art generation quality at reduced computational costs. *: value refers to the personalization stage for each personal subject. Method FT-free Enc-free Plug&Play Ref-UNet-free Extra Params Train Params # Dataset Train Time Textual Inversion [7] DreamBooth [27] Custom Diffusion [13] ELITE [36] BLIP-Diffusion [14] IP-Adapter [39] Kosmos-G [20] JeDi [40] SuTI [5] Subject-Diffusion [16] BootPig [23] ToffeeNet [42] CAFE [41] DreamCache (ours) comes the limitations of existing methods (see Fig. 1) by using feature caching mechanism that enables text-free encoding and efficient conditioning during personalization. Specifically, we first create synthetic dataset [23] containing triplets of captions, target images, and reference subjects to capture subjects in various contexts. Next, we pretrain lightweight attention-based conditioning adapters to inject subject-specific features into the image generation process. During personalization, the reference image is processed through the pretrained denoiser of the base diffusion model without text conditioning, thus eliminating the need for user-generated captions, while caching features from small subset of layers at single timestep. For personalized sampling, these cached features are injected into the denoiser through the pretrained conditioning adapters. Table 1 summarizes the key properties of existing methods and illustrates how DreamCache fits within the current landscape; further details are explored in Sec. 2. As an encoder-free approach, DreamCache introduces only small number of additional parameters, making it significantly lightweight and suitable for deployment on resourceconstrained devices. For example, methods like [36] and [14] introduce 380M parameters due to their reliance on CLIP encoders, whereas DreamCache requires only 25M additional parameters. Moreover, caching features from few selected U-Net layers at single preprocessing timestep bypasses the need for full U-Net reference processing during generation, leading to substantial computational and memory savings that enable real-time, high-quality personalized generation. Another key advantage of DreamCache is its plug-and-play nature, allowing concurrent generation of personalized and non-personalized content without altering 2 768* - - 457M 380M 402M 1.6B - 400M 700M 0.95B 632M 14B 25M 768* 0.9B* 57M* 77M 1.5B 22M 1.6B 0.9B 2.5B 700M 0.95B 0.9B 1B 25M 3-5* 3-5* 3-5* 50 min* 10 min* 10 min* 125K 14 days 129M 96 days 28 days 10M - 9M 48 days 3M - 500K - 76M 18 hours 200K - 5M - 355K 40 hours 400K original U-Net weights, thus preserving the integrity of the original model and enabling wider range of deployment scenarios, especially on mobile platforms. In summary, DreamCache represents significant step toward practical, and scalable personalized image generation, with the following contributions: We propose feature caching approach that creates multiresolution representations of the reference image in caption-free and efficient manner. We design an attention-based conditioning mechanism that leverages the cached features for personalized image generation, achieving computationaland memoryefficient personalized sampling. Our approach achieves state-of-the-art quality in personalized image generation at substantially lower computational and data costs compared to existing methods. 2. Background and Related Work Personalized image generation aims at generating images containing specific subject. This task has been widely studied, resulting in two main approaches: fine-tuning methods, which require test-time finetuning on multiple subject reference images, and finetuning-free (zero-shot) methods, which learn generalizable conditioning mechanism to generate reference subjects without the need for further optimization. Finetuning-based Personalization DreamBooth [27] finetunes the entire U-Net with reference images while introducing regularization loss to mitigate overfitting. On the other hand, Custom Diffusion [13] only finetunes the and projections for the cross-attention blocks of the U-Net. Text-based personalization methods optimize single (like dragon... as street graffiti playing with fire as plushie working as barista cat... in Ukiyo-e style with rainbow scarf Van Gogh painting wearing diploma hat Figure 2. Personalized generations by DreamCache. The first column contains reference images. The generated images correspond to the text prompts above each column. Textual Inversion [7]) or multiple (like in P+ [35]) input token embeddings. Later methods [1, 9, 33, 38] build on these, with innovations like Perfusion [33] using dynamic rank-1 updates to prevent overfitting while keeping encodings lightweight. However, all finetuning-based methods are computationally intensive, often requiring minutes of finetuning per reference subject at test time. Finetuning-Free Personalization To reduce computational demands, recent research has shifted toward zeroshot personalization methods that eliminate subject-specific finetuning, typically employing image encoders to condition the generation process via the features extracted from the reference images [8, 16, 21, 36, 39]. Examples include BLIP-Diffusion [14], which pretrains Q-Former to learn image features aligned with text, and IP-Adapter, which uses frozen CLIP encoder to extract text-aligned visual features that modulate the cross-attention layers of the generative model. Other approaches, like Kosmos-G [20] and CAFE [41], connect large language models (LLMs) with diffusion models to condition generation on personalized concepts. SuTI [5] takes different approach by training millions of subject-specific experts and subsequently training model via apprenticeship learning, enabling effective zero-shot personalized generation at test time. Alternatively, encoder-free methods such as JeDi [40] and BootPIG [23] use features from the generative models backbone to guide the generation. JeDi creates multi-view synthetic dataset and modifies spatial self-attention to jointly attend to images of the same concept in batch. BootPIG retains trainable copy of the original U-Net, adding reference self-attention layers to enable adaptation of the personalized model for reference features. While these methods remove the need for an additional encoder, they still require computationally expensive inference, as reference images must be processed in parallel during generation-a cost that accumulates due to the iterative nature of the diffusion process. In contrast, our method, DreamCache, caches subset of reference features from the U-Net without text conditioning, eliminating the need for parallel inference and reducing memory overhead by avoiding separate model loading. This results in test-time computational efficiency similar to encoder-based methods while offering the flexibility of encoder-free feature extraction and injection. Recent works such as BootPIG [23] and Toffee-5M [42] emphasize the importance of synthetic data that explicitly decouples the subject from the background, reporting improved performance. Inspired by these approaches, we adopt similar generative pipeline to create synthetic dataset we use to train DreamCache. Moreover, since our method is plug-and-play and keeps the U-Net frozen, we reduce the high cost of training faced by other approaches [16, 20, 23, 4042]. detailed overview of current methods, the number of trained parameters, and their training cost can be found in Table 1. Feature Caching Feature caching has been explored to reduce generation time in diffusion models by caching in3 Figure 3. Overview of DreamCache. Original U-Net layers are shown in violet, while the novel components introduced by DreamCache are highlighted in green. During personalization, features from selected layers of the diffusion denoiser are cached from single timestep, using null text prompt. These cached features serve as reference-specific information. During generation, conditioning adapters inject the cached features into the denoiser, modulating the features of the generated image to create personalized output. termediate activations. Some studies [18, 37] exploit temporal redundancy during the training process to cache activations across timesteps, reducing computational load at later timesteps. Other works focus on caching layer activations within the diffusion framework, avoiding redundant computations. Learning-to-Cache [17] introduces dynamic caching mechanism that learns to skip computation for selected layers of the diffusion model. In contrast to those works, which generally cache intra-model features for some layers to save computations, we utilize feature caching to encode multi-resolution features of reference image from few selected layers to condition the generation process of new personalized image. Our approach recalls successful few-shot learners for discriminative problems [19, 30, 34] and extends them to personalized image generation. features from Iref by caching the activations of few selected layers. To improve generalization, we cache features using forward pass with null text prompt. When personalized sampling is performed, the cached features are processed by adapters to act as conditioning signals, modulating the denoiser features of the image under generation at corresponding layers. These adapters, once pretrained on synthetic dataset, enable zero-shot personalized generation with any new reference image, requiring no further finetuning once its features are cached. In the following, we detail the three main aspects of DreamCache, namely i) how to cache reference features (Sec. 3.1); ii) how to condition the diffusion model on the cached features for personalized sampling (Sec. 3.2); iii) how to train the adapters used for model conditioning (Sec. 3.3). 3. Method 3.1. Caching Reference Features Given pretrained text-to-image generative model ϵθ and an image containing reference subject Iref, the goal of personalized sampling is to generate novel images containing the reference subject in various contexts while maintaining textual control. We propose DreamCache, novel approach for extracting conditioning signals from Iref and guiding of the image generation process. This method leverages pretrained diffusion model, conditioning adapters that are pretrained with synthetic dataset, and feature cache from the reference image. Sample outputs generated by DreamCache are shown in Fig. 2, with method overview in Fig. 3. At the core of DreamCache, we utilize the denoiser in the pretrained diffusion model to extract multi-resolution To extract information from the reference image for personalized sampling, we perform forward pass through the denoiser of the diffusion model at single timestep. We select = 1, the least noisy timestep, to obtain clean features that are optimal for conditioning the personalized generation process. Additionally, we remove the text conditioning to decouple visual content of the reference image from the text caption, thus also eliminating the need for user-provided captions for reference images. This contrasts with methods such as JeDi [40], which are sensitive to caption content. During the forward pass, activations are computed for all layers of the denoiser, but only subset is cached. Based on our experiments with the Stable Diffusion U-Net, we 4 find that caching features from middle bottleneck layer and every second layer in the decoder offers the best balance between generation quality and caching efficiency (see Sec. 4.3). Formally, the feature cache HFC consists of the activations of the denoiser ϵθ from the selected layers at timestep = 1, using null text prompt and noisy reference image Iref + nt, with noise realization nt, expressed as: HFC = {href,L : L} href,L = ϵθ(Iref + nt, , t; l)t=1,l=L. (1) (2) Notice that the cached features have different spatial resolutions, from the low-resolution bottleneck layer to the higher-resolution decoder layers, allowing multi-resolution representation of the reference image. This is particularly useful for enabling both global semantics and finegrained detail guidance. As in prior work [16, 23, 36], we foreground-segment the reference image to isolate the subject from the background before caching its features. 3.2. Conditioning on Cached Reference Features We propose novel conditioning adapter mechanism composed of: i) cross-attention block between the cached features and the features of the image under generation; ii) concatenation operation between the features from the selfattention block of the original U-Net denoising backbone and those of the cross-attention block; and iii) projection layer. block diagram is shown in Fig. 3 (right). Omitting the layer subscript for clarity, the conditioning adapter mechanism is expressed mathematically as follows: = WQh, kc = WKhref, vc = WV href, (3) ac = softmax (cid:18) qkT a = Wproj ([a; ac]) , (cid:19) vc, (4) (5) where RN are the current d-dimensional features of the -pixel image under generation, and href are the cached reference features from Eq. (2). WQ, WK, WV , and Wproj are learnable projection matrices, whose training is described in Sec. 3.3. The concatenation [a; ac] RN 2d combines the output of self-attention RN and crossattention ac RN d. The concatenation operation allows flexible information fusion without explicit alignment constraints, compared to other approaches in similar works (see Sec. 4.3). The learnable projection matrix Wproj reduces the dimensionality of the concatenated features back to RN to maintain compatibility with the original backbone. Overall, the approach proposed for the adapter enriches feature representations used in the diffusion process of the image under generation by allowing the model to leverage both primary and conditioning-based contextual information from the cache. 3.3. Training the Conditioning Adapters The additional parameters introduced in Sec. 3.2 to process the cached features must be trained on large and varied dataset to ensure they generalize for any reference subject. Collecting paired data for this training process would be prohibitively expensive, as it requires multiple images of the same subject in different contexts. To address this, we draw inspiration from the recently proposed synthetic data generation pipeline in BootPIG [23] to construct our training data. First, we utilize large language model (Llama 3.2 [6]) to generate captions for potential target images. Each caption is used to generate an image via Stable Diffusion [26]. We then use the Segment Anything Model (SAM) [12] and Grounding DINO [15] to accurately segment the reference subject based on the text caption and generate foreground mask of the main object in the caption. We treat the Stable Diffusion-generated image as the target image, the foreground object pasted on white background as the reference image, and the LLM-generated caption as the textual prompt during our training pipeline. Compared to BootPIG, our pipeline employs open-source models, making it more accessible. We will release our synthetic dataset to facilitate reproducibility and further research, since similar datasets, including BootPIGs [23], have not been publicly released. Additional details on the dataset and its statistics can be found in the Supp. Mat. We train the newly introduced adapters parameters (WQ, WK, WV , and Wproj) with the standard score matching loss [32] using both the text-conditioned noisy input and the cached reference features: Ldiffusion = Ex0,ϵ,cT ,Iref,t (cid:2)ϵ ϵ θ(xt, cT , HFC, t)2 2 (cid:3) , (6) where x0 is the target image, cT is the text prompt generated by the large language model, ϵ is Gaussian noise, and is the diffusion timestep sampled uniformly from 1, . . . , . The noisy image at timestep t, xt, is obtained by gradually adding noise to x0 during the forward diffusion process. The function ϵ θ represents the modified denoising model that incorporates the conditioning adapters. 4. Experimental Results In this section, we present our experimental results, including quantitative and qualitative comparisons, an ablation study, and an analysis section that visualizes the behavior of the newly introduced cross-attention mechanism in the adapters. Implementation Details We evaluate our method on two versions of Stable Diffusion (SD) [26], specifically versions 1.5 and 2.1, to ensure fair comparison with state-of-the-art methods across different backbones. As described in the ablation study, our caching and conditioning mechanism is applied to the middle layer and every second layer of 5 Table 2. Quantitative results on DreamBooth. DreamCache obtains better balance between DINO score and CLIP-T compared to all baselines, while also offering more efficient computational tradeoff (see Table 1). Method Backbone #Ref DINO () CLIP-I () CLIP-T () t - t n n fi DreamBooth [27] DreamBooth [27] Textual Inversion [7] Custom Diffusion [13] BLIP-Diffusion (FT) [14] n n fi r ELITE [36] BLIP-Diffusion [14] IP-Adapter [39] Kosmos-G [20] Jedi [40] DreamCache (ours) Re-Imagen [4] SuTI [5] Subject-Diffusion [16] BootPig [23] ToffeeNet [42] CAFE [41] DreamCache (ours) Imagen SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 Imagen Imagen SD 2.1 SD 2.1 SD 2.1 SD 2.1 SD 2.1 3-5 3-5 3-5 3-5 3-5 1 1 1 1 1 1 1-3 1-3 1 3 1 1 0.696 0.668 0.569 0.643 0.670 0.621 0.594 0.667 0.694 0.619 0.713 0.600 0.741 0.771 0.674 0.728 0.715 0.767 0.812 0.803 0.780 0.790 0.805 0.771 0.779 0.813 0.847 0.782 0.810 0.740 0.819 0.779 0.797 0.817 0.827 0.816 0.306 0.305 0.255 0.305 0.302 0.293 0.300 0.289 0.287 0.304 0.298 0.270 0.304 0.293 0.311 0.306 0.294 0. the decoder. The total number of trainable parameters for DreamCache is 25M. We use the original SD codebase and train the model on 4 80GB A100 GPUs for 25k steps with batch size of 128, using the AdamW optimizer with learning rate of 105. Input images are resized to 512 512, with scale, shift, and resize augmentations applied to reference images to enhance model robustness to perturbations. Ablations are conducted on SD 1.5. We generate images with 50 sampling steps, employing classifier-free guidance for image and text conditioning, using guidance scale of 7.5. Evaluation Quantitative evaluations are conducted on the DreamBooth dataset [27], following prior approaches. DreamBooth consists of 30 subjects, each with 25 text prompts. We use single input image per subject and generate 4 images per subject-prompt combination, resulting in 3,000 generated images. We use pretrained DINO ViT-S/16 and CLIP ViT-B/32 models to calculate the average cosine similarity of global image embeddings between generated and reference images, with metrics denoted as DINO and CLIP-I, respectively. To assess text alignment, we calculate the cosine similarity between embeddings from generated images and text prompts using CLIPs image and text encoders [10], with the corresponding score denoted as CLIP-T. presents quantitative results, indicating the diffusion backbone and the number of reference images for each method. Our approach achieves competitive or superior performance compared to other computationally-intensive state-of-the-art methods, which are trained on larger datasets and with significantly more parameters. We refer the reader to Table 1 for the data requirements, training time, and parameter count of the various methods. We remark that generally DINO is preferred metric for image similarity with respect to CLIP-I, as it is more sensitive to the appearance and fine-grained details of the subjects. We also present qualitative comparisons with KosmosG [20] and BLIP-Diffusion [14]. We remark that several other methods are not reproducible due to the lack of code, datasets, or trained checkpoints. As seen in Fig. 4, our method excels in subject preservation and textual alignment, producing visually superior results. We also notice that Kosmos-G reports high CLIP-I score, but after inspecting the generated images, it is clear that the score does not entirely reflect the preservation of the reference subject in generated images. In fact, Kosmos-G presents high degree of background interference, where the partial replication of the reference background boosts the alignment score. For this reason, we also report foreground-masked metrics on the subjects, like MCLIP-I and MDINO, in the Supp. Mat. 4.1. Zero-Shot Personalization 4.2. Inference Time Evaluation We compare DreamCache against state-of-the-art methods for finetuning-based and zero-shot personalization. Table 2 The computational efficiency of our method is compared to the reference-based method BootPIG [23] and encoder6 dog wearing santa hat with the Eiffel Tower in the background can floating on top of water with mountain in the background toy on the beach on top of white rug Reference BLIP-D Kosmos-G DreamCache BLIP-D Kosmos-G DreamCache Figure 4. Visual comparison. Personalized generations on sample concepts. DreamCache preserves reference concept appearance and does not suffer from background interference. BLIP-D [14] and Kosmos-G [20] cannot faithfully preserve visual details from the reference. Table 3. Computational comparison. *: time to generate an image with 100 timesteps, evaluated on single NVIDIA A100 GPU. synthetic dataset scaling. Method Inference Time* Extra Params Size ELITE [36] BLIP-Diffusion [14] BootPig [23] DreamCache (ours) 6.24 3.92 7.55 3.88 914 MB 760 MB 1900 MB 42 MB based approaches such as Kosmos-G [20] and SubjectDiffusion [16]. Table 3 provides detailed comparison of inference time, accounting for both personalization time (e.g., the time to generate the cache for DreamCache) and the time to sample the personalized image. We also report the increase in model size, i.e., the storage (in FP16 precision) required for the extra parameters to allow for personalization, showing that DreamCache is one order of magnitude more compact than the state of the art. Overall, DreamCache offers lightweight solution that achieves state-of-the-art performance with faster inference and reduced computational overhead. 4.3. Ablation Studies We validate our design choices through series of studies, examining different conditioning mechanisms, evaluating our feature caching approach, and analyzing the impact of Reference Feature Integration We compare various conditioning strategies to integrate reference features in Table 4. Our spatial cross-attention block with concatenation between the output of selfand cross-attention (Spatial Concat) was evaluated against different alternatives, including IPAdapters conditioning mechanism [39] (Textual Sum), which sums the decoupled cross-attention output with that from textual cross-attention. We also tested variant (Spatial Sum) where selfand cross-attention conditioning outputs are summed. Additionally, we also assessed an alternative conditioning procedure inspired by ViCo [9] (Decoupled Blocks), involving independent and interleaved cross-attention blocks. Results in Table 4 indicate that the proposed Spatial Concat offers the best balance of text alignment and parameter efficiency. We further explored optimal conditioning insertion within the U-Net backbone in Table 5, determining that applying conditioning (and therefore feature caching) at the middle layer and every second layer of the decoder achieved the best tradeoff between performance and parameter count. Text Input for Cached Features Our feature caching procedure is designed to be text-free, leveraging the classifierfree guidance used during pretraining where captions were occasionally omitted. We compare this approach with version including textual inputs during caching (e.g., photo 7 Table 4. Reference feature integration. DreamCache uses the best tradeoff between accuracy and complexity. Method CLIP-I () CLIP-T () Params Textual Sum [39] Spatial Sum Decoupled Blocks [9] Spatial Concat (ours) 0.788 0.812 0.808 0.810 0,282 0.293 0.300 0.298 19M 16M 61M 25M Table 5. Cache positioning in the U-Net backbone offers further tradeoff between accuracy and complexity. Encoder Middle Decoder CLIP-I () CLIP-T () Params 0.721 0.749 0.716 0.799 0.810 0.813 0.303 0.306 0.302 0.296 0.298 0. 11M 19M 8M 17M 25M 36M Table 6. Caching with text is not influential and adds complexity. Text-Free CLIP-I () CLIP-T () 0.811 0.810 0.295 0. Table 7. Dataset impact for both synthetic and real data. Dataset CLIP-I () CLIP-T () Synthetic-50K Synthetic-200K Synthetic-400K LAION-5M 0.781 0.797 0.810 0.814 0.304 0,301 0.298 0. of ...). Table 6 shows that adding text conditioning slightly reduces text alignment while increasing complexity and potentially introducing noise in cases of inaccurate captions. Dataset Impact We demonstrate the importance of our synthetic dataset to train the conditioning adapters and the effect of scaling its size. For this purpose, we created synthetic datasets according to the procedure in Sec. 3.3 of sizes 50K, 200K, and 400K samples. We also tested the real-world 5M samples from the LAION [29] dataset, which, lacking target-caption-reference triplets, required reuse of target images as reference images too. Table 7 shows that increasing dataset size improves image alignment, though slightly reduces textual alignment. Notably, LAION improves image alignment but struggles with textual alignment. This highlights the importance of triplet data (target image, reference image, and caption) for effective zero-shot personalization, ensuring both subject preservation and textual editability. Figure 5. Visualization of reference image impact. Crossattention maps between cached reference features and features of the image under generation. Left: attention map at layers at 16 16 resolution (left reference, right generated). Right: 32 32. Attention values are highly localized in the region of interest. 4.4. Visualizing Reference Impact Finally, we analyze how the cross-attention mechanism in DreamCache impacts image generation by visualizing cached reference feature influence. Attention map visualizations at different resolutions are provided in Fig. 5. Specifically, attention maps between the query from the current generation and the key derived from reference features reveal highly localized focus on the subject, without interference from background elements. This mechanism models correspondences effectively, integrating reference information into the generated image. 5. Discussion and Conclusions In this paper, we proposed DreamCache, novel approach to personalized text-to-image generation that uses feature caching to overcome the limitations of existing methods. By caching reference features from small subset of layers of the U-Net only once, our method significantly reduces both computational and memory demands, enabling efficient, real-time personalized image generation. Unlike previous approaches, DreamCache avoids the need for costly finetuning, external image encoders, or parallel reference processing, making it lightweight and suitable for plug-and-play deployment. Our experiments demonstrate that DreamCache achieves state-of-the-art zero-shot personalization with only 25M additional parameters and fast training process. While DreamCache is promising direction towards more efficient personalized generation, it has some limitations. Although effective for single-subject personalization, our approach may require adaptation for complex multi-subject generation where feature interference can occur. Additionally, certain edge cases, such as highly abstract or stylistic images, may challenge the caching mechanisms capacity to accurately preserve subject details. To address these challenges, future work may explore adaptive caching techniques or multireference feature integration."
        },
        {
            "title": "References",
            "content": "[1] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano. Domainagnostic tuning-encoder for fast personalization of text-toimage models. In SIGGRAPH Asia 2023 Conference Papers, pages 110, 2023. 1, 3 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 1 [4] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-to-image generator. In The Eleventh International Conference on Learning Representations, 2022. 6 [5] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. arXiv preprint arXiv:2304.00186, 2023. 2, 3, 6 [6] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 5, 1 [7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2022. 1, 2, 3, [8] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):113, 2023. 1, 3 [9] Shaozhe Hao, Kai Han, Shihao Zhao, and Kwan-Yee Wong. Vico: Plug-and-play visual condition for personalized text-toimage generation. arXiv preprint arXiv:2306.00971, 2023. 1, 3, 7, 8 [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, 2021. 6 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 5 [13] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 1, 2, [14] Dongxu Li, Junnan Li, and Steven CH Hoi. Blipdiffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023. 1, 2, 3, 6, 7, 5 [15] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 5 [16] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410, 2023. 1, 2, 3, 5, 6, 7 [17] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. arXiv preprint arXiv:2406.01733, 2024. 4 [18] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1576215772, 2024. 4 [19] Boris Oreshkin, Pau Rodrıguez Lopez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. Advances in neural information processing systems, 31, 2018. [20] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992, 2023. 1, 2, 3, 6, 7, 5 [21] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou Yang. λ-eclipse: Multi-concept personalized text-to-image diffusion models by leveraging clip latent space. arXiv preprint arXiv:2402.05195, 2024. 1, 3 [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1 [23] Senthil Purushwalkam, Akash Gokul, Shafiq Joty, and Nikhil Naik. Bootpig: Bootstrapping zero-shot personalized image generation capabilities in pretrained diffusion models. arXiv preprint arXiv:2401.13974, 2024. 1, 2, 3, 5, 6, 7 [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1 [25] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 1 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 9 image diffusion models. arXiv preprint arXiv:2308.06721, 2023. 1, 2, 3, 6, 7, 8 [40] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, TingChun Wang, Ming-Yu Liu, and Yogesh Balaji. Jedi: Jointimage diffusion models for finetuning-free personalized textto-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67866795, 2024. 1, 2, 3, 4, 6 [41] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, and Tong Sun. Customization assistant for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 91829191, 2024. 2, 3, 6 [42] Yufan Zhou, Ruiyi Zhang, Kaizhi Zheng, Nanxuan Zhao, Jiuxiang Gu, Zichao Wang, Xin Eric Wang, and Tong Sun. Toffee: Efficient million-scale dataset construction for subject-driven text-to-image generation. arXiv preprint arXiv:2406.09305, 2024. 1, 2, 3, 6 [27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023. 1, 2, 6 [28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. [29] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 8 [30] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. 4 [31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 1 [32] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 5 [33] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalization. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. 1, 3 [34] Eleni Triantafillou, Hugo Larochelle, Jake Snell, Josh Tenenbaum, Kevin Jordan Swersky, Mengye Ren, Richard Zemel, and Sachin Ravi. Meta-learning for semi-supervised fewshot classification. In International Conference on Learning Representations, 2018. [35] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 1, 3 [36] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1589715907, 2023. 1, 2, 3, 5, 6, 7 [37] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62116220, 2024. 4 [38] Jianan Yang, Haobo Wang, Yanming Zhang, Ruixuan Xiao, Sai Wu, Gang Chen, and Junbo Zhao. Controllable textual inversion for personalized text-to-image generation. arXiv preprint arXiv:2304.05265, 2023. 1, 3 [39] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-to10 DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching"
        },
        {
            "title": "Supplementary Material",
            "content": "Table S1. Masked metrics quantitative evaluation. Table S2. Reference features ablation study. Method MCLIP-I () MDINO () # Reference Features CLIP-I () CLIP-T () DreamBooth Custom Diffusion JeDI BLIP-D ELITE Toffee-5M Ours 0.868 0.864 0.876 0.862 0.861 0.874 0.906 0.712 0.711 0.751 0.669 0.681 0.803 0.837 S1. Synthetic Dataset In this section, we describe our dataset generation pipeline, inspired by the success of BootPIG, with some modifications to ensure the pipeline adopts open-source models and is fully reproducible. Figure S1 provides an overview of the data creation process. We also show some examples of generated synthetic data in Figure S3. We use the lang-sam pipeline1 to segment both generated and reference images based on textual conditioning, using combination of Grounding-DINO and SAM. For caption generation, we leverage the LLama 3.2 8B [6], with carefully crafted prompt that aims to generate diverse and descriptive captions of concrete objects, placing them in various meaningful contexts. We filter the generated captions to ensure the datasets diversity and remove duplicates or highly similar captions. We write simple filtering script that counts the number of occurrences for each object/category and filter out redundant captions. The filtered captions are then used to prompt SD-XL [22] with Classifier-Free Guidance (CFG) scale of 3.5, employing 25 denoising steps to generate the images. Our entire data generation pipeline is reproducible, and we plan to release it alongside the code for DreamCache. Additionally, we will provide access to our generated dataset to encourage further research in this area. S2. Additional Evaluations S2.1. Masked Metrics Recent studies [40, 42] emphasize the value of evaluating masked versions of image similarity metrics to eliminate potential interference from background elements, thus ensuring the evaluation focuses on the fidelity of the personalized 1https://github.com/luca-medeiros/lang-segment-anything Middle Features Respective Features 0.778 0.810 0.312 0.298 Table S3. Encoding timestep ablation study. Timestep CLIP-I () CLIP-T () 1 150 300 0.810 0.800 0.789 0.298 0.299 0.301 object. We use Grounded-SAM [25] to segment both generated and reference images, subsequently computing the CLIP-I and DINO scores for these segments. The results for these masked metrics are reported in Table S1. DreamCache achieves higher score on both metrics, demonstrating its superiority in subject preservation. S2.2. Qualitative Results In this section, we present additional qualitative generations produced by DreamCache (Figure S4). We conduct experiments using both synthetically generated subjects and real subjects from the Dreambooth dataset. Our results demonstrate that our method effectively follows complex text prompts. Interestingly, despite the absence of explicit training for subject modification (as seen in editing datasets), our approach successfully adapts and transforms the input subject in various contexts, rather than simply replicating the reference. Additional Qualitative Comparisons We also provide additional qualitative comparisons in Figure S5, including two reproducible open-source baselines: BLIP-D [14] and Kosmos-G [20]. S3. Additional Ablation Study Impact of Encoding Timestep The proposed reference encoding mechanism relies on selecting = 1 as fixed timestep during the encoding process. We validate this design choice in Table S3, showing that = 1 yields the best performance. This finding aligns with the intuition that less noisy features provide more informative conditioning 1 Figure S1. Data synthesis pipeline inspired by BootPIG [23]. S2. S4. Sampling Space and Image Guidance In our experiments we follow prior works [2, 40] and experiment with different types of guidance for image and text conditioning signal. The first and simpler joint guidance approach jointly drops text and image conditioning for the unconditional prediction: eθ(zt, cI , cT ) = eθ(zt, , ) + (eθ(zt, cI , cT ) eθ(zt, , )) Where eθ(zt, cI , cT ) represents the adjusted prediction at denoising step conditioned on textual conditioning cT and the image conditioning cI . eθ(zt, , ) denotes the unconditional prediction, and is the guidance scale. The second approach, that we call combined guidance decouples text and image allowing for more flexible balance between the two conditioning modalities: eθ(zt, cI , cT ) = eθ(zt, , ) + sI (eθ(zt, cI , ) eθ(zt, , )) + sT (eθ(zt, cI , cT ) eθ(zt, cI , )) Figure S2. Sampling Space Exploration. For Combined Guidance, we leave the text scale cT = 7.5 and we vary the image scale cI . signal. Furthermore, this experiment highlights significant limitation of reference U-Net-based methods that inject noisy features corresponding to different timesteps. These noisy features are less informative and contain fewer details compared to the low-noise, fixed-timestep references we use to condition the generation independently of the current timestep. Impact of Multi-Resolution Features We also investigate the necessity of multi-resolution features for DreamCaches performance. In variant of our method, we fixed the cached features to single resolution (i.e., the bottleneck resolution of the U-Net,(8 8), after the encoding stage). Our experiments demonstrate that leveraging multiple resolutions significantly enhances performance compared to using single fixed-resolution cached feature map, as shown in Table Our experimental findings suggest that using higher image guidance scale better preserves the content of the reference image, but reduces editability of the subject. Conversely, decreasing image guidance results in more flexible editing of the reference subject at the expense of reduced subject fidelity. Figure S2 illustrates these findings on the DreamBooth dataset, comparing the joint and combined guidance strategies. Figure S3. Synthetic dataset samples generated via the process outlined in Fig. S1. 3 dragon... flying in the sky in flower garden frozen chinese painting An elephant... in minecraft as street graffiti dressed as wizard as plushie An astrounaut... in snow ball in boiling water on the mountains having breakfast chair... in rustic cabinet royal throne futuristic setting Van Gogh painting guitar... in snow globe made of ice Monet painting underwater Figure S4. Personalized generations by DreamCache. The proposed method is able to adapt to different text prompts and leverage diffusion prior to perform appearance and style editing of the personalized content. We also notice how the background interference is completely absent in generated images due to our design choice of caching masked reference features. 4 backpack in an ocean of milk in the jungle toy with tree and autumn leaves floating on top of water sneaker red on top of mirror dog in firefighter outfit on top of purple rug in forest plushie wet in the snow robot on top of dirty road with blue house in the background monster floating on top of water with mountains in the background Reference BLIP-D Kosmos-G DreamCache BLIP-D Kosmos-G DreamCache Figure S5. Visual comparison. Personalized generations on sample concepts. DreamCache preserves reference concept appearance and does not suffer from background interference. BLIP-D [14] and Kosmos-G [20] cannot faithfully preserve visual details from the reference. 5 S5. Broader Impact DreamCache allows users to customize the subject of their images, focusing on individual elements such as animals or objects. However, it is crucial to recognize that, like other generative models and image editing tools, this technology has the potential to be misused for creating misleading content. Addressing these ethical risks is an essential and ongoing focus in the field of generative modeling, particularly in relation to deepfake creation. Techniques such as watermarking or content detection are particularly necessary to prevent misuse of this technology."
        }
    ],
    "affiliations": [
        "Politecnico di Torino",
        "Samsung R&D Institute UK"
    ]
}