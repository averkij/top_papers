{
    "paper_title": "LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer",
    "authors": [
        "Yiren Song",
        "Danze Chen",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition."
        },
        {
            "title": "Start",
            "content": "LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer YIREN SONG, Show Lab, National University of Singapore, Singapore DANZE CHEN, Show Lab, National University of Singapore, Singapore MIKE ZHENG SHOU, Show Lab, National University of Singapore, Singapore 5 2 0 2 3 ] . [ 1 5 0 1 1 0 . 2 0 5 2 : r Fig. 1. LayerTracer enables the creation of cognitive-aligned layered SVGs, either from text prompts or by converting images into layer-wise SVGs. Generating cognitive-aligned layered SVGs remains challenging due to existing methods tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, diffusion transformer based framework that bridges this gap by learning designers layered SVG creation processes from novel dataset of sequential design operations. Our approach operates in two phases: First, textconditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracers superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning Corresponding author. Authors Contact Information: Yiren Song, yiren@nus.edu.sg, Show Lab, National University of Singapore, Singapore; Danze Chen, chendanze@whu.edu.cn, Show Lab, National University of Singapore, Singapore; Mike Zheng Shou, mike.zheng.shou@ gmail.com, Show Lab, National University of Singapore, Singapore. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM XXXX-XXXX/2025/2-ART https://doi.org/10.1145/nnnnnnn.nnnnnnn AI-generated vectors with professional design cognition. Code is released at https://github.com/showlab/LayerTracer CCS Concepts: Computing methodologies Computer vision. Additional Key Words and Phrases: Diffusion Transformer, Image generation, Vectorization ACM Reference Format: Yiren Song, Danze Chen, and Mike Zheng Shou. 2025. LayerTracer: CognitiveAligned Layered SVG Synthesis via Diffusion Transformer. 1, 1 (February 2025), 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "INTRODUCTION",
            "content": "Scalable Vector Graphics (SVG) serves as the cornerstone of modern digital design, defining visual elementspaths, curves, and geometric shapesthrough mathematical equations rather than pixel grids. Unlike raster images, SVG maintains resolution-independent clarity at any scale, making it indispensable for precision-critical applications ranging from UI/UX design to industrial CAD systems. Layered SVGs, which adhere to professional standards, elevate this advantage further: designers can meticulously manipulate individual layers to refine stroke textures, spatial hierarchies, and composite effects. This editability transcends mere convenienceit forms the backbone of dynamic adjustments and collaborative iteration in contemporary design workflows. Nevertheless, significant gap persists between current deep learning-based SVG generation techniques and professional requirements. Existing approaches face three systemic challenges: First, the , Vol. 1, No. 1, Article . Publication date: February 2025. 2 Trovato et al. scarcity of large-scale layered SVG datasets forces models to rely on synthetic or oversimplified training data, resulting in outputs devoid of the nuanced hierarchical structures inherent to human designs. Second, methodological fragmentation prevailsoptimizationbased methods [8, 11, 13, 16, 39, 42, 43] generate vector paths using raster priors but often produce cluttered geometries with redundant anchor points; and large language models (LLMs) [22, 33, 41], constrained by token limits, remain limited to basic icons. Most critically, no existing method addresses the designers cognitive processthe logical sequencing, spatial reasoning, and element grouping strategies employed during layer constructionresulting in AI-generated SVGs that resemble fragmented collages rather than intentionally editable professional designs. To address these challenges, we present LayerTracer, Diffusion Transformer (DiT)-based framework that redefines layered SVG synthesis by modeling designers layer-by-layer construction logic. Our approach is grounded in three key insights: (1). Cognitive alignment: DiT models pretrained on text-image corpora inherently capture contextual relationships between visual elements, which can be steered through targeted fine-tuning to mimic designer decisionmaking. (2). Spatiotemporal consistency: The self-attention mechanisms bias toward local token interactionsa byproduct of training on natural image pixel correlationscan be repurposed to enforce coherence across sequential design steps. 3. Structured decomposition: Disassembling layered SVGs into channel-wise components and organizing them as grid sequences provides generation models with an interpretable blueprint of layer evolution. In implementation, LayerTracer integrates two innovations. First, we curate pioneering dataset of 20,000+ designer process traces, automatically converting layered SVGs into timestamped creation sequences. These sequences are rasterized and organized into training grids using serpentin layout, ensuring temporally adjacent design steps remain spatially proximate. Second, we develop dualphase generation pipeline: (1) text-conditioned DiT generates rasterized construction process sequences that simulate designers workflow, followed by (2) layer-wise vectorization module that converts these sequences into clean, editable SVG layers while eliminating redundant paths. Beyond text-to-SVG synthesis, LayerTracer tackles the inverse task: converting raster images into layered vector graphics. We reframe this as process-conditioned generation problem, where reference images guide the model to \"reverse-engineer\" plausible layer construction steps. Specifically, we build upon pretrained DiT model and adapt it through LoRA fine-tuning to ingest image context. By encoding reference images into conditional tokens injected into the denoising process, the model autonomously deduces layer assembly sequences (e.g., \"background first, then foreground elements\"), faithfully reconstructing input images while adhering to practical editing constraints. Our main contributions are as follows: Cognitive-aligned SVG synthesis: As the first framework to generate layered SVGs by learning designers construction logicelement ordering, layer grouping, and spatial reasoningLayerTracer ensures outputs meet professional editing standards. , Vol. 1, No. 1, Article . Publication date: February 2025. Unified DiT-based architecture: Our framework seamlessly integrates text-to-SVG generation and layer-wise vectorization tasks, eliminating the need for task-specific pipelines. Process-centric dataset: We release scalable pipeline for collecting designer workflow data, addressing the critical gap in layered vector graphics training resources. Extensive experiments validate LayerTracers state-of-the-art performance and effectiveness."
        },
        {
            "title": "2 RELATED WORKS\n2.1 Text2image Diffusion Model",
            "content": "Recent studies have demonstrated that diffusion models are capable of generating high-quality synthetic images, effectively balancing diversity and fidelity. Models based on diffusion models or their variants, such as those paper in [18, 23], have successfully addressed the challenges associated with text-conditioned image synthesis. Stable Diffusion [23], model based on the Latent Diffusion Model, incorporates text conditioning within UNet framework to facilitate text-based image generation [27, 28, 30], establishing itself as mainstream model in image generation. Fine-tuning pre-trained image generation models can enhance their adaptation to specific application scenarios, as seen in techniques like LoRA [9] and DreamBooth [19]. For theme control in text-to-image generation, several works [37, 44, 4750] focus on custom generation for defined pictorial concepts, with ControlNet [45] additionally offering control over other modalities such as depth information. AnimateDiff [? ] introduces temporal attention module, extending Stable Diffusion into video generation model. Inspired by ProcessPainter [26], which first proposed learning an artists painting process through pre-trained temporal models, this paper leverages the in-context capabilities of DiT to generate layered SVG creation process."
        },
        {
            "title": "2.2 SVG Generation",
            "content": "Scalable Vector Graphics (SVGs) are widely utilized in design owing to their advantages like geometric manipulability, resolution independence, and compact file structure. SVG generation often involves training neural networks to produce predefined SVG commands and attributes using architectures such as RNNs [21], VAEs [2, 15, 33], and Transformers [2, 38, 40]. Nonetheless, the absence of large-scale vector datasets constrains their generalization capabilities and the creation of complex graphics, with most datasets focusing on specific areas like monochromatic vector icons [40] and fonts [31, 38]. An alternative to directly training an SVG generation network is optimizing it to match target image during the evaluation phase, employing differentiable rasterizers to bridge vector graphics and raster images [13]. This method optimizes SVG parameters based on pretrained vision-language models. Advances in models like CLIP [20] have facilitated effective SVG generation methods such as CLIPDraw [6], CLIPasso [35], and CLIPVG [29]. while DreamFusion [19] demonstrates the superior generative capabilities of diffusion models. VectorFusion [11], DiffSketcher [42], and SVGDreamer combine differentiable rasterizers with text-to-image diffusion models to produce vector graphics, achieving notable results in iconography and sketching. However, these methods still face challenges LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer 3 Fig. 2. The LayerTracer architecture comprises three key components: (1) Layer-wise Model: Pretrained on our proposed dataset to generate layered pixel sequences from text prompt; (2) Image2Layers Model: Merges LoRA with the Flux base DiT, enabling image-conditioned generation through VAE-encoded latent tokens; (3) Layer-wise Vectorization: Converts raster sequences to SVGs via differential analysis between adjacent layers, followed by Bézier optimization using vtracer to eliminate redundant paths while preserving structural fidelity. with editability and graphical quality. Recent studies [34, 46] have blended optimization-based methods with neural networks to enhance vector representations by integrating geometric constraints. The primary issue with methods that optimize set of vector primitives through SDS loss is their reliance on image generation model priors, which often leads to redundant and noisy results. These outputs lack clear hierarchical structures and fail to meet design specifications. In this paper, we innovatively propose an alternative approach to utilizing image generation model priors. Specifically, we leverage the in-context learning capability of Diffusion Transformers to generate the creation process of SVG graphics, combined with vectorization to achieve cognitive-aligned layered SVG generation."
        },
        {
            "title": "2.3 Vectorization",
            "content": "Raster image vectorization or image tracing is well-studied problem in computer graphics[1, 2, 4, 5]. Diffvg[13] proposes differentiable rendering method for vectorization, which found shape gradients by differentiating the formula of Reynolds transport theorem with Monta-Carlo edge sampling. Meanwhile, combining differentiable rendering techniques with deep learning models are also studied for image vectorization[16, 25, 35]. Direct raster-to-vector conversion with neural networks are supported for the relatively simple images[2, 15, 21]. Stroke-based rendering can be used to fit complex image with sequence of vector strokes [10, 14, 24], but the performance is limited by the predefined strokes. Diffvg[13] can also be leveraged to fit an input image with set of randomly initialized vector graphical elements. Based on Diffvg, LIVE [16] proposes coarse-to-fine vectorization strategy, with cost tens of minute. CLIPVG[29] proposes multi-round vectorization strategy, providing additional graphic elements for the image manipulation task. LIVE [16] and O&R [8] achieve hierarchical vectorization through optimization-based methods, but their results show significant gap compared to human-designed works, lacking logical coherence. In contrast to these approaches, our proposed LayerTracer leverages the prior knowledge of the Diffusion Transformer model, reformulating the hierarchical vectorization task as problem of predicting preceding frames from reference image."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we begin by exploring the preliminaries on diffusion transformer as detailed in section 3.1. Then introduce the overall architecture of our method in section 3.2, followed by detailed descriptions of the key modules: dataset construction methods in 3.3, Layer-wise image generation in section 3.4, Image Condition Model in section 3.5, and Layer-Wise Vectorization in section 3.6."
        },
        {
            "title": "3.1 Preliminary",
            "content": "The Diffusion Transformer (DiT) model [18], which appears in frameworks such as FLUX.1 [12], Stable Diffusion 3 [23], and PixArt [3], employs transformer-based denoising network to iteratively refine noisy image tokens. DiT processes two categories of tokens: noisy image tokens 𝑋 R𝑁 𝑑 and text condition tokens 𝐶𝑇 R𝑀 𝑑 , where 𝑑 is the embedding dimension, and 𝑁 and 𝑀 respectively represent the numbers of image and text tokens. As these tokens move through the transformer blocks, they retain consistent dimensions. In FLUX.1, each DiT block applies layer normalization before Multi-Modal Attention (MMA) [17], incorporating Rotary Position Embedding (RoPE) [32] to capture spatial context. For image tokens 𝑋 , RoPE applies rotation matrices based on tokens position (𝑖, 𝑗) in the 2D grid: 𝑋𝑖,𝑗 𝑋𝑖,𝑗 𝑅(𝑖, 𝑗), (1) where 𝑅(𝑖, 𝑗) is the rotation matrix at position (𝑖, 𝑗). Text tokens 𝐶𝑇 are similarly transformed with their positions specified as (0, 0). The multi-modal attention mechanism then projects these positionencoded tokens into query 𝑄, key 𝐾, and value 𝑉 representations, enabling attention across all tokens: MMA([𝑋 ; 𝐶𝑇 ]) = softmax (cid:19) (cid:18) 𝑄𝐾 𝑑 𝑉 , (2) , Vol. 1, No. 1, Article . Publication date: February 2025. 4 Trovato et al. where [𝑋 ; 𝐶𝑇 ] denotes concatenation of image and text tokens. This formulation ensures bidirectional attention among the tokens."
        },
        {
            "title": "3.2 Overall Architecture.",
            "content": "LayerTracer consists of the following components: Serpentine dataset construction, Layer-wise model training, Image2Layers model, and the layer-wise vectorization. Initially, we collected the processes by which designers create layered vector graphics, arranging them in serpentine layout to form 3x3 and 2x2 grid datasets. Following this, we utilized the LoRA method for pre-training on the proposed dataset, thereby enabling the generation of layered pixel images from textual descriptions. Subsequently, we integrated the LoRA from the previous step with the Flux base model to establish new foundational model. The Image2Layer model introduces an imagebased conditional mechanism that, through additional LoRA finetuning, predicts the creation process of reference images. Finally, in the layer-wise vectorization stage, the model sequentially transforms the generated pixel images into high-quality vector graphics, which are analyzed, filtered, and vectorized based on the differences between adjacent layers."
        },
        {
            "title": "3.3 Serpentine Dataset Construction",
            "content": "Our dataset construction includes 20,000 layered SVGs created by designers, encompassing black outline icons, regular icons, emojis, and illustrative graphics. Each sequence is composed of either 9 or 4 frames, arranged in 3x3 or 2x2 grids, resulting in resolutions of 1056x1056 and 1024x1024 respectively. To capture the process of designers creating layered SVGs, we propose automated data generation pipeline that deconstructs the layered SVG graphics into sequences based on the grouping logic and element hierarchy within the SVG files. Additionally, the pipeline incorporates human-inthe-loop process to filter out nonsensical sequences. In the attention mechanism of DiT, tokens tend to focus on spatially adjacent tokens. This tendency stems from the strong correlations between adjacent image pixels captured during the pretraining of diffusion models. To enhance the models learning of grid sequences [36], we introduce the serpentine dataset construction method. As shown in Figure 2, we arrange the sequences of 9 and 4 frames in serpentine layout within the grid, ensuring that temporally adjacent frames are also spatially adjacent (either horizontally or vertically). In our ablation experiments, we confirmed that this design is crucial for the coherence of sequence generation. To facilitate subsequent hierarchical vectorization, for icons with black line strokes, we place the black line layer separately in the first frame during dataset creation. Similarly, during the generation phase, we vectorize the black line layer and overlay it onto the subsequent results."
        },
        {
            "title": "3.4 Layer-wise Image Generation",
            "content": "DiT models, trained on massive image-text pairs, inherently possess contextual generation capabilities. By appropriately activating and enhancing this ability, they can be utilized for complex generation tasks. Since text-to-image models can interpret merged prompts, they can be reused for in-context generation without altering their architecture. This only requires changes to the input data rather , Vol. 1, No. 1, Article . Publication date: February 2025. than modifications to the model itself. Building on this insight, we designed simple yet effective pipeline to learn the hierarchical logic employed by human designers in creating layered SVGs. Layer-wise Model Training. Due to the size of the dataset, we adopt LoRA fine-tuning for training which can be formulated as: 𝑊 = 𝑊0 + Δ𝑊 , (3) where 𝑊0 represents the original weights of the pre-trained model, and Δ𝑊 denotes the low-rank adaptation updates introduced during fine-tuning. This formulation enables efficient training by keeping 𝑊0 fixed and applying lightweight updates through Δ𝑊 , which allows the model to balance generalization from the pre-trained weights with task-specific adaptation provided by the fine-tuned updates. Loss function. We employ the conditional flow matching loss function, integral to training and optimizing the generative model, is defined as follows: 𝐿𝐶𝐹 𝑀 = 𝐸𝑡,𝑝𝑡 (𝑋 𝜖 ),𝑝 (𝜖 ) (cid:2)𝑣Θ (𝑋, 𝑡) 𝑢𝑡 (𝑋 𝜖) 2(cid:3) (4) Where 𝑣Θ (𝑋, 𝑡) represents the velocity field parameterized by the neural networks weights,t is timestep, 𝑢𝑡 (𝑋 𝜖) is the conditional vector field generated by the model to map the probabilistic path between the noise and true data distributions. 3.5 Image2Layers Model In this section, we introduce the Image2Layers model, which builds upon the previous section by incorporating image conditioning. This approach redefines hierarchical vectorization as \"reverse engineering\" task, predicting how an SVG is created layer by layer. The primary challenge in training the Image2Layers model lies in the limited availability of high-quality sequential data. While small dataset may suffice for LoRA training, initializing controllable plugin (such as ControlNet [45] or IP-Adapter [44]) from scratch with limited data is highly challenging. To address this, we design an efficient controllability framework by repurposing pre-trained DiT model and adapting it to accept image context as conditioning input. Specifically: Training Phase. We concatenate procedural sequences into 22 or 33 training grids. The final frame (context image) is passed through the VAE to extract latent variables, which are directly appended to the end of the denoising latent. Through self-attention mechanisms, the context latent provides conditional information to the denoising processes of other frames, enhancing the logical consistency and coherence of the generation. The condition image is then fed into VAE to obtain its latent representation, which is directly appended to the denoising latent at the end. Multi-modal attention mechanisms are used to provide conditional information for the denoising of other frames. MMA([𝑋 ; 𝐶𝐼 ; 𝐶𝑇 ]) = softmax (cid:19) (cid:18) 𝑄𝐾 𝑑 𝑉 , (5) where [𝑋 ; 𝐶𝐼 ; 𝐶𝑇 ] denotes the concatenation of image and text tokens. This formulation enables bidirectional attention. Inference Phase. During inference, we use the reference image as condition to predict the earlier layers, thereby inferring how the SVG in the reference image was constructed layer by layer. LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer 5 Fig. 3. Given text prompt, LayerTracer generates cognitive-aligned layered SVGs that mimic human design cognition."
        },
        {
            "title": "3.6 Layer-Wise Vectorization",
            "content": "To achieve hierarchical vectorization of input images, our process begins by segmenting grid images into individual cells. This facilitates independent processing for subsequent vectorization stages. Specifically for icons, accurate extraction of black lines is crucial as they represent key structural elements of the image. To address common issues of line distortion, we employ series of preprocessing steps: grayscale conversion highlights contrast between lines and background; Gaussian blurring smooths out noise; and adaptive thresholding via the Otsus thresholding method ensures robust line separation. These preprocessed lines are then integrated into transparent PNG, focusing vectorization efforts on relevant areas. Furthermore, to capture the layered details of images, we perform differential extraction between adjacent cells, identifying significant pixel changes to highlight areas of variation. This involves converting cell images to grayscale, computing absolute differences, applying binary thresholding, and refining the output with morphological operations to produce clean, meaningful contours of change. These contours are saved as transparent PNGs for subsequent vectorization. The final step involves vectorizing these differential layers using tools like vtracer, optimizing parameters to balance detail retention and file size, and ultimately merging all vectorized layers into single SVG file. This method preserves the images global structure while highlighting intricate changes between cells, resulting in layered and editable SVG suitable for detailed graphical representations."
        },
        {
            "title": "4 EXPERIMENT\n4.1 Experiment Setting",
            "content": "Experiment Details. During the pretraining stage, we utilized the Flux 1.0 dev model based on the pretrained DiT architecture. Training resolutions included 10561056 (33 grids) and 10241024 (22 grids). The LoRA fine-tuning approach was applied with LoRA rank of 256, batch size of 16, and 20,000 fine-tuning steps. For training the Image Condition Model, we merged the Layer-wise Image Generation LoRA with the base model, setting the LoRA merge weight to 1.0. Subsequently, the model was fine-tuned on the same dataset using the LoRA approach for an additional 20,000 steps. Baseline Methods. The baseline methods in text-to-svg genration methods are SVGDreamer [43], Vecfusion [11]and DiffSketcher [42]. The baseline methods in vectorization include diffvg [13], LIVE [16], and O&R [8]. Benchmarks. To address the lack of high-quality layered SVG datasets, this paper introduces dataset containing over 20,000 layered SVGs and their creation processes, named the LayerSVG Dataset. To ensure fairness in comparative experiments, the NotoEmoji [7] dataset is also included in the benchmark for quantitative evaluation. For the text-to-SVG task and the layer-wise vectorization task, we select 50 prompts and 50 images, respectively, as benchmarks for testing. , Vol. 1, No. 1, Article . Publication date: February 2025. 6 Trovato et al. Fig. 4. Given raster image of an icon as input, LayerTracer predicts how the icon was created layer by layer, achieving cognitive-aligned layered vectorization."
        },
        {
            "title": "4.2 Generation Results",
            "content": "Fig. 3 demonstrates LayerTracers capability to generate cognitivelyaligned layered SVGs that adhere to text descriptions while maintaining logical layer hierarchies (e.g., background-to-foreground ordering and grouped semantic elements). The outputs preserve essential design properties including layer independence, non-overlapping paths, and topological editability. Fig. 4 further illustrates layeraware vectorization results, where input raster images are decomposed into clean vector layers with consistent spatial alignment and minimal shape redundancy."
        },
        {
            "title": "4.3 Comparison and Evaluation",
            "content": "In the text-to-SVG task, we compute FID and CLIP Score. For the hierarchical vectorization task, we follow the evaluation methodology from previous works. We calculate MSE to assess the consistency between the reconstructed image and the input image. Additionally, we record the number of SVG shapes used, as fewer shapes indicate more concise and efficient result. Table 1 and 2 show that LayerTracer achieves the best results across most metrics. Qualitative Evaluation. This section presents qualitative analysis results. Figure 5 shows that our method produces concise and coherent outputs for text-to-SVG tasks, aligning with specific requirements for icons and emojis. Baseline methods, in contrast, result in visual clutter and irregular paths. Figure 6 demonstrates our methods superior performance in layer-wise vectorization tasks, maintaining logical spatial hierarchies and semantic grouping, unlike baselines which yield fragmented and misaligned outputs. Quantitative Evaluation. Table 1 and 2 present the quantitative evaluation results. In the SVG generation task, our method achieves the highest CLIP-Score with the lowest average number of paths and shortest time cost. Notably, baseline methods fail to produce rationally layered outputs. For the layer-wise vectorization task, our approach outperforms all baselines across metrics: the lowest , Vol. 1, No. 1, Article . Publication date: February 2025. Fig. 5. Compare with baseline methods in Text-to-SVG generation task. Table 1. Comparison with SOTA SVG Generation Methods. The best results are denoted as Bold. Methods CLIP-Score Time Cost(s) No. Paths layer-wise Vecfusion [11] SVGDreamer [43] DiffSketcher [42] Ours 31.10 32.68 31.47 33.76 4668 5715 3374 27 128.00 512.00 512.00 35."
        },
        {
            "title": "False\nFalse\nFalse\nTrue",
            "content": "average path count demonstrates superior simplicity and efficiency, while faster runtime and higher reconstruction consistency further validate its effectiveness. LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer 7 Fig. 6. Compare with baseline methods in layer-wise vectorization task. Our results are more concise and exhibit more logical layering. Table 2. Comparison with SOTA Vectorization Methods. The best results are denoted as Bold."
        },
        {
            "title": "Methods",
            "content": "Diffvg [13] LIVE [16] O&R [8] Ours MSE 2.02104 5.21104 2.01104 1.96104 Time Cost(s) No. Paths Layer-wise 393 3147 612 34 256.00 46.00 64.00 29."
        },
        {
            "title": "4.5 User Study",
            "content": "We conducted user study with 46 design enthusiasts using digital questionnaire, presenting results from both our method and baseline methods. Participants selected their preferred results and those best matching the prompt descriptions in the text-to-SVG task. In the layer-wise vectorization task, they chose their preferred results and the most logically layered sequences. As Fig. 8 shows, LayerTracer outperformed all baselines in user preference, prompt adherence, and layer rationality. Fig. 7. Ablation study of Serpentine Layout Strategy."
        },
        {
            "title": "4.4 Ablation Study of Serpentine Layout Strategy.",
            "content": "In this section, we conduct an ablation study on the serpentine layout strategy. As shown in Fig. 7, when the serpentine layout strategy is not used to construct the training dataset, incomplete decomposition, undesirable repetitions, and abrupt changes between frames are more likely to occur. The quantitative evaluation results are presented in Table 3. For the layer-wise vectorization task, we calculate the MSE between the predicted results for 9 frames and the ground truth on both the training and test sets. When the serpentine layout strategy is not applied, the MSE is higher. Table 3. Quantitative Evaluation of Serpentine Layout Strategy. Methods w/o Serpentine Layout Full MSE_train 2.03 104 1.65 104 SSIM_trainMSE_test 2.41104 1.9910 0.964 0.971 SSIM_test 0.959 0.963 Fig. 8. User Study Results. LayerTracer outperform in all three metrics."
        },
        {
            "title": "5 LIMITATIONS AND FUTURE WORK",
            "content": "During layer-wise vectorization, LayerTracer relies on methods like Vtracer, inheriting its limitations such as need for manual adjustment of hyperparameters. Its performance also falters on out-ofdistribution data and complex images. We aim to develop smarter single-layer vectorization solution to replace Vtracer in the future. , Vol. 1, No. 1, Article . Publication date: February 2025. 8 Trovato et al."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced LayerTracer, novel framework that bridges the gap between automated SVG generation and professional design standards. Leveraging the strengths of Diffusion Transformers, LayerTracer achieves cognitive-aligned, layer-wise SVG generation and vectorization. By learning the workflows and design logic of human designers, LayerTracer effectively generates clean, editable, and semantically meaningful vector graphics from textual descriptions or raster images. To overcome the scarcity of layered SVG creation data, we established pipeline that collects over 20,000 SVG creation sequences. We proposed Serpentin dataset construction method, enabling effective model training. Extensive experiments demonstrate that LayerTracer not only excels in SVG generation quality but also offers unparalleled flexibility and interpretability, setting new benchmark for scalable vector graphics creation. , Vol. 1, No. 1, Article . Publication date: February 2025."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Jules Bloomenthal and Ken Shoemake. 1991. Convolution surfaces. In Proceedings of the 18th annual conference on Computer graphics and interactive techniques. 251256. [2] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. 2020. Deepsvg: hierarchical generative network for vector graphics animation. Advances in Neural Information Processing Systems 33 (2020), 1635116361. [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. 2023. Pixart-𝛼: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426 (2023). [4] Robert Cook. 1986. Stochastic sampling in computer graphics. ACM Transactions on Graphics (TOG) 5, 1 (1986), 5172. [5] Vage Egiazarian, Oleg Voynov, Alexey Artemov, Denis Volkhonskiy, Aleksandr Safin, Maria Taktasheva, Denis Zorin, and Evgeny Burnaev. 2020. Deep vectorization of technical drawings. In European conference on computer vision. Springer, 582598. [6] Kevin Frans, LB Soros, and Olaf Witkowski. 2021. Clipdraw: Exploring text-to-drawing synthesis through language-image encoders. arXiv preprint arXiv:2106.14843 (2021). [7] Google. 2014. Noto Emoji Fonts. https://github.com/googlefonts/noto-emoji. [8] Or Hirschorn, Amir Jevnisek, and Shai Avidan. 2024. Optimize & Reduce: TopDown Approach for Image Vectorization. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 21482156. [9] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [10] Teng Hu, Ran Yi, Haokun Zhu, Liang Liu, Jinlong Peng, Yabiao Wang, Chengjie Wang, and Lizhuang Ma. 2023. Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region. In Proceedings of the 31st ACM International Conference on Multimedia. 74707480. [11] Ajay Jain, Amber Xie, and Pieter Abbeel. 2023. Vectorfusion: Text-to-svg by abstracting pixel-based diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19111920. [12] Black Forest Labs. 2023. FLUX. https://github.com/black-forest-labs/flux. [13] Tzu-Mao Li, Michal Lukáč, Michaël Gharbi, and Jonathan Ragan-Kelley. 2020. Differentiable vector graphics rasterization for editing and learning. ACM Transactions on Graphics (TOG) 39, 6 (2020), 115. [14] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Ruifeng Deng, Xin Li, Errui Ding, and Hao Wang. 2021. Paint transformer: Feed forward neural painting with stroke prediction. In Proceedings of the IEEE/CVF international conference on computer vision. 65986607. [15] Raphael Gontijo Lopes, David Ha, Douglas Eck, and Jonathon Shlens. 2019. learned representation for scalable vector graphics. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 79307939. [16] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev, Nikita Orlov, Yun Fu, and Humphrey Shi. 2022. Towards layer-wise image vectorization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1631416323. [17] Zexu Pan, Zhaojie Luo, Jichen Yang, and Haizhou Li. 2020. Multi-modal attention for speech emotion recognition. arXiv preprint arXiv:2009.04107 (2020). [18] William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. [19] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. 2022. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022). [20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [21] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy Mitra. 2021. Im2vec: Synthesizing vector graphics without vector supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 73427351. [22] Juan Rodriguez, Shubham Agarwal, Issam Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, and Marco Pedersoli. 2023. Starvector: Generating scalable vector graphics code from images. arXiv preprint arXiv:2312.11556 (2023). [23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [24] Jaskirat Singh, Cameron Smith, Jose Echevarria, and Liang Zheng. 2022. IntelliPaint: Towards developing more human-intelligible painting agents. In European Conference on Computer Vision. Springer, 685701. [25] Yiren Song. 2022. Cliptexture: Text-driven texture synthesis. In Proceedings of the 30th ACM International Conference on Multimedia. 54685476. [26] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. 2024. ProcessPainter: Learn Painting Process from Sequence Data. arXiv preprint arXiv:2406.06062 (2024). [27] Yiren Song, Xiaokang Liu, and Mike Zheng Shou. 2024. DiffSim: Taming Diffusion Models for Evaluating Visual Similarity. arXiv preprint arXiv:2412.14580 (2024). [28] Yiren Song, Shengtao Lou, Xiaokang Liu, Hai Ci, Pei Yang, Jiaming Liu, and Mike Zheng Shou. 2024. Anti-Reference: Universal and Immediate Defense Against Reference-Based Generation. arXiv preprint arXiv:2412.05980 (2024). [29] Yiren Song, Xuning Shao, Kang Chen, Weidong Zhang, Zhongliang Jing, and Minzhe Li. 2023. Clipvg: Text-guided image manipulation using differentiable vector graphics. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 23122320. [30] Yiren Song, Pei Yang, Hai Ci, and Mike Zheng Shou. 2024. IDProtector: An Adversarial Noise Encoder to Protect Against ID-Preserving Image Generation. arXiv preprint arXiv:2412.11638 (2024). [31] Yiren Song and Yuxuan Zhang. 2022. CLIPFont: Text Guided Vector WordArt Generation.. In BMVC. 543. [32] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. [33] Zecheng Tang, Chenfei Wu, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu Liu, Zhengyuan Yang, Lijuan Wang, Zicheng Liu, Juntao Li, et al. 2024. StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis. arXiv preprint arXiv:2401.17093 (2024). [34] Vikas Thamizharasan, Difan Liu, Matthew Fisher, Nanxuan Zhao, Evangelos Kalogerakis, and Michal Lukac. 2024. NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 45894597. [35] Yael Vinker, Ehsan Pajouheshgar, Jessica Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. 2022. Clipasso: Semantically-aware object sketching. ACM Transactions on Graphics (TOG) 41, 4 (2022), 111. [36] Cong Wan, Xiangyang Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, and Yihong Gong. 2024. GRID: Visual Layout Generation. arXiv preprint arXiv:2412.10718 (2024). [37] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. 2024. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519 (2024). [38] Yizhi Wang and Zhouhui Lian. 2021. Deepvecfont: synthesizing high-quality vector fonts via dual-modality learning. ACM Transactions on Graphics (TOG) 40, 6 (2021), 115. [39] Zhenyu Wang, Jianxi Huang, Zhida Sun, Daniel Cohen-Or, and Min Lu. 2024. arXiv preprint Layered Image Vectorization via Semantic Simplification. arXiv:2406.05404 (2024). [40] Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. 2023. IconShop: TextBased Vector Icon Synthesis with Autoregressive Transformers. arXiv preprint arXiv:2304.14400 (2023). [41] Ximing Xing, Juncheng Hu, Guotao Liang, Jing Zhang, Dong Xu, and Qian Yu. 2024. Empowering LLMs to Understand and Generate Complex Vector Graphics. arXiv preprint arXiv:2412.11102 (2024). [42] Ximing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian Yu, and Dong Xu. 2023. Diffsketcher: Text guided vector sketch synthesis through latent diffusion models. Advances in Neural Information Processing Systems 36 (2023), 1586915889. [43] Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. 2024. SVGDreamer: Text guided SVG generation with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 45464555. [44] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023). [45] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. [46] Peiying Zhang, Nanxuan Zhao, and Jing Liao. 2024. Text-to-vector generation with neural path representation. ACM Transactions on Graphics (TOG) 43, 4 (2024), 113. [47] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, and Zhongliang Jing. 2024. SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 80698078. [48] Yuxuan Zhang, Yiren Song, Jinpeng Yu, Han Pan, and Zhongliang Jing. 2024. Fast Personalized Text to Image Synthesis with Attention Injection. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 61956199. https://doi.org/10.1109/ICASSP48485.2024.10447042 [49] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. 2024. Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model. arXiv preprint arXiv:2403.07764 (2024). LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer [50] Yuxuan Zhang, Qing Zhang, Yiren Song, and Jiaming Liu. 2024. Stable-Hair: Real-World Hair Transfer via Diffusion Model. arXiv preprint arXiv:2407.14078 (2024). , Vol. 1, No. 1, Article . Publication date: February 2025."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore, Singapore"
    ]
}