{
    "paper_title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?",
    "authors": [
        "Sangyun Chung",
        "Youngjoon Yu",
        "Youngchae Chee",
        "Se Yeon Kim",
        "Byung-Kwan Lee",
        "Yong Man Ro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an understanding of diverse multi-vision sensor data, such as thermal, depth, and X-ray information, is essential. However, we find that current VLMs process multi-vision sensor images without deep understanding of sensor information, disregarding each sensor's unique physical properties. This limitation restricts their capacity to interpret and respond to complex questions requiring multi-vision sensor reasoning. To address this, we propose a novel Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark, assessing VLMs on their capacity for sensor-specific reasoning. Moreover, we introduce Diverse Negative Attributes (DNA) optimization to enable VLMs to perform deep reasoning on multi-vision sensor tasks, helping to bridge the core information gap between images and sensor data. Extensive experimental results validate that the proposed DNA method can significantly improve the multi-vision sensor reasoning for VLMs."
        },
        {
            "title": "Start",
            "content": "Are Vision-Language Models Truly Understanding Multi-vision Sensor? Sangyun Chung, Youngjoon Yu, Youngchae Chee, Se Yeon Kim, Byung-Kwan Lee, and Yong Man Ro* Integrated Vision Language Lab, KAIST, South Korea {jelarum, greatday, litcoderr, seyeon.kim, leebk, ymro}@kaist.ac.kr 4 2 0 2 0 3 ] . [ 1 0 5 7 0 2 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an understanding of diverse multi-vision sensor data, such as thermal, depth, and X-ray information, is essential. However, we find that current VLMs process multi-vision sensor images without deep understanding of sensor information, disregarding each sensors unique physical properties. This limitation restricts their capacity to interpret and respond to complex questions requiring multi-vision sensor reasoning. To address this, we propose novel Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark, assessing VLMs on their capacity for sensor-specific reasoning. Moreover, we introduce Diverse Negative Attributes (DNA) optimization to enable VLMs to perform deep reasoning on multi-vision sensor tasks, helping to bridge the core information gap between images and sensor data. Extensive experimental results validate that the proposed DNA method can significantly improve the multi-vision sensor reasoning for VLMs. Codes and data are available at https://github.com/top-yun/MS-PR 1. Introduction In recent days, large-scale Vision-Language Models (VLMs) have made strides in areas like visual dialogue [22], video analysis [42], and document understanding [54], establishing themselves as valuable tools in the pursuit of artificial general intelligence (AGI). These models, similar to the human brain, process multi-sensor information to generate complex inferences. For instance, VLMs like OpenAIs GPT-4o [38] exhibit reasoning abilities that not only rival but sometimes even exceed human performance. VLMs are currently reaching into applications in the real world, such as autonomous vehicles [16, 33, 52], Internet of Things (IoT) devices [7, 8, 10], and robotics [4, 11, 14, 18, 19]. Devices that connect to the real world often use Figure 1. Multi-vision sensor related question and response examples of recent VLMs [39, 53]. Note that, this example underscores the difficulty that VLMs face in understanding physical properties unique to multi-vision sensors. multi-vision sensors, making it essential for VLMs to understand these kinds of information. Multi-vision sensors, such as thermal imaging, depth sensing, and X-ray detection, provide information that goes beyond human eyesight, enriching the understanding of real-world environments. While humans can interpret multi-vision sensor images easily based on contextual knowledge of physical characteristics, we find that VLMs face significant challenges with multi-vision sensor data. Figure 1 demonstrates two different examples of interactions between humans and VLMs [39, 53]. The first interaction shows that VLMs can easily recognize and correctly identify the type of sensor. However, in the second example, VLMs fail to select the correct answer when faced with the challenging question that requires deeper understanding and multi-vision sensor reasoning. As illustrated in Figure 1, even without direct experience, humans can understand thermal image by integrating scientific and contextual knowledge, allowing them to interpret aspects like heat distribution. In contrast, VLMs confuse the brightness of thermals images for sunlight reflection, instead of heat emission. *Corresponding author. Both authors are equally contributed."
        },
        {
            "title": "We hypothesize that it primarily arises because VLMs",
            "content": "information. are predominantly trained on RGB images, making it difficult to effectively align each multi-vision sensors unique physical properties with perceptual The scarcity of multi-vision sensor data further contributes to In other words, VLMs tend to this significant problem. make superficial judgments based on prior visible information derived from RGB image data. This limits their ability to understand sensor-specific details, leading to superficial RGB-bounded reasoning without genuine multi-vision sensor understanding. For instance, VLMs often confuse certain characteristics in multi-vision sensor data, such as confusing light scattering and glow effects in thermal images or mistaking fog and haze in depth images. VLMs might rely on the patterns learned from similar-looking RGB images rather than understanding the actual physical properties of the multi-vision sensors. This limitation severely affects applications where sensor-specific accuracy is crucial, such as autonomous driving [33, 52], security systems [44], and medical image diagnosis [2, 7]. In this paper, to handle the aforementioned challenge, we design novel benchmark called the Multi-vision Sensor Perception and Reasoning (MS-PR) for evaluating multivision sensor reasoning in VLMs. MS-PR benchmark consists of multi-vision perception task and multi-vision reasoning task. Multi-vision perception refers to the task required to assess VLMs effectiveness in meeting visual perception demands. Multi-vision reasoning measures the VLMs ability to base its responses on fundamental information from the provided sensor knowledge. To enhance the understanding capability of multi-vision sensor in VLMs, we also propose novel Diverse Negative Attributes (DNA) optimization. By leveraging diverse negative examples, DNA improves learning in sensor-specific contexts, essential when data is scarce. range of diverse negatives is incorporated into the optimization process, acting as stone bridges in the VLMs reasoning process and pushing it beyond the naive RGB-bounded assumptions. The evaluation of MS-PR benchmark demonstrates that most stateof-the-art VLMs display deficiencies in sensor reasoning to varying extents. Moreover, VLMs with the proposed DNA optimization show significant increase of performance on multi-vision sensor reasoning task. In summary, the key contributions of this work are as follows: We first identify the limitations of current VisionLanguage Models (VLMs) in multi-vision sensor reasoning. To address this issue, we propose new Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark, providing structured framework to rigorously assess VLMs multi-vision sensor reasoning capabilities. We propose novel training method, Diverse Negative Attributes (DNA) optimization, which enhances deep sensor understanding even with limited data. This approach can be applied to any large-scale VLM without altering the model architecture. We evaluate total of 10 state-of-the-art VLMs using our MS-PR benchmark. Also, the extensive experimental results validates that the proposed DNA optimization can significantly improve the multi-vision sensor reasoning ability among VLMs. 2. Related work Large-scale Vison Language Models. Recently, there has been significant interest in visual language multimodal learning. Visual language models such as LLAVA [27, 28], BLIP-2 [25], InternVL2 [39], VideoLLaMA2 [5], MiniCPMv2.5 [53], Qwen2-VL [50] have shown impressive performance in variety of downstream tasks. In addition, to obtain richer contextual information, VLMs have developed the capability to handle multi-vision sensor inputs. For example, InternVL2 [39] is an open-source multi-modal large language model that bridges the gap between open-source and commercial models by enhancing visual understanding, dynamic high-resolution processing, and bilingual dataset quality. Consequently, Girdhar et al. presents ImageBind, which creates joint embedding space across multi-vision sensors including depth and thermal sensor data. PandaGPT [45] is VLM that integrates multimodal encoders and large language models to enable multimodal instruction-following capabilities, performing complex tasks. However, relatively less attention has been devoted to whether VLMs genuinely understand the physical meanings of multi-vision sensors. Evaluation Benchmark for VLMs. Numerous studies have leveraged existing vision-language datasets to develop benchmarks for assessing the reliability of VLMs. MME [13] includes 14 sub-tasks based on publicly available images with manually created annotations, evaluating both the recognition and perception capabilities of VLMs through yes/no question answering. SEED-benchmark [24] designed to evaluate the generative comprehension capabilities of multimodal VLM through human-annotated multi-choice questions across 12 evaluation dimensions. Other comparable benchmarks include MMMU [55], QBench [51], and MMBench [30]. Unlike those previous evaluation benchmarks, the proposed MS-PR is designed to rigorously test and evaluate the capabilities of understanding the physical meaning of multi-vision sensors. 3. Multi-Vision Sensor Perception and Reasoning (MS-PR) Benchmark 3.1. Evaluation on Multi-vision Sensor Tasks Our benchmark dataset was collected according to two multi-vision tasks: multi-vision perception and multi-vision reasoning. As illustrated in Figure 2, multi-vision perception focuses on the VLMs ability to accurately interpret Figure 2. Data samples of Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark for evaluating the abilities of VLMs in multi-vision sensor understanding, which covers four types of multi-vision perception tasks (Existence, Counting, Position, and General Description) and two types of multi-vision reasoning tasks (Contextual Reasoning and Sensory Reasoning). recognize and categorize visual elements across different contexts from different vision sensors. On the other hand, multi-vision reasoning requires the model to not only perceive but also make inferences based on the multi-vision sensor data. This involves higher-order cognitive tasks such as understanding relationships between objects, prediction of intent of sensor use, and understanding sensor knowledge. Multi-vision reasoning tests the VLMs capability to integrate multi-vision information with contextual sensor knowledge, making logical deductions that go beyond mere perception. 3.1.1. Multi-vision Perception Multi-vision perception is the foundational process by which large-scale Vision-Language Models (VLMs) analyze images captured by various multi-vision sensors, including thermal, depth, and X-ray images. This process involves recognizing and interpreting the fundamental elements within each visual input based on cognitive science [3, 21]. In this context, multi-vision perception tasks include (1) Existence: the ability to identify and list common objects present in the image, such as people, vehicles, animals, and so on. (2) Count: the ability to count the number of identified objects or entities. (3) Position: the ability to determine the spatial arrangement of objects within the image, noting their positions relative to one another. (4) the ability to generate nuanced deGeneral Description: Figure 3. Distribution of data sources of the MS-PR benchmark. In MS-PR, we demonstrate six core multi-vision sensor tasks in the outer ring, and the inner ring displays the number of samples for each specific task. and identify objects, scenes, and relationships from various multi-vision inputs. This involves tasks such as object detection, image classification, scene recognition, and relationship detection, where the model must process and understand the content of images from multiple vision sensors. The goal is to ensure that the model can consistently Model Vision Sensors Existence Count Position General Description Multi-vision Perception Contextual Reasoning Sensory Reasoning Multi-vision Reasoning BLIP-2 [25] LLaVA-1.5-7B [26] InternVL2-8B [39] VideoLLaMA2-7B [5] MiniCPM-V-2.5-8B [53] Qwen2-VL-7B [50] Phantom-7B [23] Gemini-Pro [47] GPT-4o [38] Claude-3.5-Sonnet [1] Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray 32.2 40.0 55.0 27.6 22.1 35.3 47.7 40.5 39.8 49.8 40.5 49.0 52.8 43.7 51.0 47.7 38.4 39.8 46.3 36.3 44.6 57.8 52.4 58.6 65.6 61.0 54.6 70.3 67.2 64.9 69.5 66.9 60.2 72.7 71.6 72.1 72.7 64.1 63.8 75.0 68.1 64. 65.3 71.6 81.7 60.7 77.6 75.0 73.0 77.6 82.8 81.7 83.5 85.7 77.8 84.7 85.3 77.6 81.6 84.4 72.7 76.6 82.4 Open Source Large-scale Vision-Language Models 53.6 59.2 60.4 56.1 65.2 65.1 60.7 53.7 73.6 58.6 63.2 57.0 66.7 64.4 71.2 64.1 69.5 64.3 82.4 70.8 82.2 68.3 70.2 66.2 69.8 76.1 76.8 69.2 75.2 70.9 76.1 68.5 75.1 64.8 71.0 64.7 66.3 71.1 67.8 62.2 65.3 69.9 Closed Source Large-scale Vision-Language Models 74.9 81.8 70.2 82.1 70.6 76.7 74.5 79.3 73.5 84.9 70.6 78.2 75.3 63.3 54.8 63.3 66.8 62.6 80.7 86.6 89.8 84.4 90.2 90.6 67.8 73.0 82.4 79.7 73.7 66.5 78.9 73.2 72.5 64.1 52.3 68.1 57.3 38.4 49.4 55.3 45.8 41.0 46.2 30.5 33.1 74.8 71.3 75.8 74.4 73.0 73.9 74.8 68.8 75.6 83.8 77.9 80.6 80.9 77.4 85.7 70.6 65.0 76.0 77.4 66.9 76. 84.5 78.2 86.9 90.6 85.0 85.5 65.4 53.8 76.9 42.7 26.3 59.3 41.1 22.1 49.6 50.4 28.7 65.0 76.2 29.9 72.9 59.8 51.3 81.6 62.8 19.3 64.4 50.6 29.6 67.6 68.7 32.5 76.2 69.7 33.6 79.3 64.4 44.5 72.9 58.7 48.8 67.5 57.8 47.5 61.7 60.6 48.7 70.3 80.0 53.9 76.7 70.4 64.3 83.7 66.7 42.1 70.2 64.0 48.2 72.2 76.6 55.3 81.5 80.2 59.3 82.4 64.9 49.1 74.9 Table 1. Evaluation results of different VLMs on the MS-PR benchmark are reported, using accuracy as the metric. Multi-vision Perception shows the average performance on four dimensions (Existence, Count, Position, and General Description) for evaluating visual perception, and Multi-vision Reasoning shows the average performance on two dimensions (Contextual Reasoning and Sensory Reasoning) for evaluating vision sensory understanding. VLMs are sorted in ascending order of release date. scriptions of the overall scene depicted in an image. VLMs can articulate what is happening, identify objects, and provide factual information that enhances the understanding of the image itself. At the perception stage, VLMs focus on extracting essential information directly from raw image data captured by multi-vision sensors. This foundational perception is critical for all subsequent reasoning tasks, serving as the foundation upon which more complex interpretations are built. 3.1.2. Multi-vision Reasoning Multi-vision reasoning is where VLMs truly showcase their advanced capabilities. Beyond simply perceiving images, VLMs can engage in logical reasoning to derive deeper insights and make informed decisions. This distinguishes recent VLMs, which primarily focus on understanding and interacting with the real world, from traditional computer vision models. Multi-vision reasoning tasks include (1) Contextual reasoning: the ability to utilize fundamental knowledge and contextual clues to make judgments about given scenario. This type of reasoning allows VLMs to ensure that the reasoning process remains consistent with the context provided by the image. (2) Sensory reasoning: more complex reasoning ability to map 2D image data to the physical meanings associated with different multi-vision sensors. This process not only involves processing the raw data from images but also integrates it with specific information about the underlying physical sensor knowledge in the real world. Sensory reasoning requires deep understanding of the knowledge underlying the physical meaning of multivision sensor data. This approach goes beyond surface-level naive image recognition, demanding that VLMs make sense Figure 4. Overview of the pipeline for generating the proposed benchmark dataset. Based on the prompts corresponding to knowledge on multi-vision sensors and tasks, ChatGPT/GPT-4o generates challenging question and answer set. We refine the dataset further by utilizing human annotators to construct positive and negative sets, allowing each pair to be classified into specific evaluation dimension. of the sensor data in way that accurately reflects real-world environments. at each step. 3.2. Evaluation Benchmark Design Our benchmark aims at evaluating the multi-vision sensor understanding capability of large-scale VLMs. We filtered images according to six tasks in Figure 3 to improve question quality, excluding low-resolution or sequentially captured images. According to Figure 4, we begin by curating collection of detailed question set involving multi-vision sensor inputs that guide VLMs to interpret image information. To fully understand the multi-vision sensor, ChatGPT/GPT-4o is then used to generate challenging question and answer sets based on the sensor knowledge and task prompts. By doing this, each sensor type provides distinct knowledge relevant to specific sensor properties. Also, the model can produce challenging questions that require multi-hop reasoning and deep understanding based on the specific characteristics of each sensor type with targeted tasks. Human annotators thoroughly review and refine the question and answer set. Positive answer set provides accurate answers based on sensor-specific information. While negative answer set includes plausible but incorrect responses. Each question and answer pair is structured as chain-of-thought instruction, simulating human reasoning and directing VLMs to focus on relevant details 4. Enhancing Multi-vision Sensor Reasoning 4.1. Problems on Multi-vision Sensor Data Through the MS-PR benchmark, we reveal that multi-vision sensor reasoning problems are widespread in current VLMs in Table 1. The primary reason is the scarcity of publicly available multi-vision sensor instruction-tuning datasets. Lacking sufficient opportunities to learn sensor knowledge, VLMs tend to misunderstand the image information. Due to this data constraint, VLMs often rely on RGB-bounded reasoning, causing them to confuse the unique characteristics of multi-vision sensor data. Considering these inherent issues in multi-vision sensor data, we propose an efficient, data-centric approach that enables effective learning even with limited dataset. To demonstrate this, we design method that achieves comparable performance by using only small portion of data. We construct approximately 600 multi-vision sensor images, with 200 images for each sensor type, all of which are not included in the MS-PR evaluation benchmark. 4.2. Diverse Negative Attributes Optimization In this paper, we propose novel Diverse Negative Attributes (DNA) optimization. Unlike previous reinforceModel Sensor Type Existence Count Position General Description Multi-vision Perception Contextual Reasoning Sensory Reasoning Multi-vision Reasoning Phantom-7B Phantom-7B + SFT Phantom-7B + DNA Qwen2-VL-7B Qwen2-VL-7B + SFT Qwen2-VL-7B + DNA InternVL2-8B InternVL2-8B + SFT InternVL2-8B + DNA Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray 71.1 67.8 69.9 82.8 71.0 73.5 86.8 79.1 78.2 76.1 75.1 71.0 85.7 83.0 78.2 89.1 84.4 79. 66.7 71.2 69.5 80.8 72.0 72.8 84.0 74.1 75.2 46.3 36.3 44.6 46.2 48.4 47.4 49.8 49.0 49.4 47.7 38.4 39.7 50.8 44.2 43.8 52.3 44.2 45.0 47.7 40.5 39.8 48.8 41.1 46.2 50.3 42.6 47.0 75.0 68.1 64.1 73.4 71.7 67.7 75.8 74.5 73.3 72.7 64.1 63.7 80.5 73.3 70.5 80.5 74.8 73. 70.3 67.2 64.9 70.3 69.8 67.3 75.0 71.9 70.9 72.7 76.6 82.4 81.7 84.7 82.0 86.4 87.9 84.8 77.6 81.6 84.4 82.6 89.0 89.8 88.4 90.0 91.4 73.0 77.6 82.8 78.7 81.9 84.8 84.5 84.7 85.7 66.3 62.2 65.3 71.0 69.0 67.7 74.3 72.6 71.4 68.5 64.8 64.7 74.9 72.4 70.6 77.6 73.3 72. 64.4 64.1 64.3 69.6 66.2 67.8 73.4 68.3 69.7 77.4 66.9 76.8 79.3 77.1 78.3 82.9 81.2 85.8 70.6 65.0 76.0 85.8 75.6 84.4 89.0 80.5 86.4 74.8 68.8 75.6 77.0 72.1 78.6 82.9 77.3 83.0 50.6 29.6 67.6 78.5 65.3 73.5 86.4 86.1 82.1 62.8 19.3 64.4 80.6 30.6 84.2 85.7 59.8 86. 50.4 28.7 65.0 69.4 49.7 73.7 82.0 77.8 78.1 64.0 48.2 72.2 78.9 71.2 75.9 84.6 83.7 84.0 66.7 42.1 70.2 83.2 53.1 84.3 87.4 70.2 86.2 60.6 48.7 70.3 73.2 60.9 76.1 82.4 77.6 80.6 Table 2. Performance is increased by Diverse Negative Attributes (DNA) optimization in the sense of multi-vision reasoning. Highlighted columns show average performance for perception and reasoning capabilities. The best results are denoted in bold. ment learning-based methods such as Reinforcement Learning from Human Feedback (RLHF) [40], Direct Preference Optimization (DPO) [41], and Simple Preference Optimization (SimPO) [34], DNA optimization reduces the RGB-bounded reasoning during the training process by directly adding the designed loss to the supervised fine tuning process. It is the optimization process where the model identifies the correct answer while simultaneously learning to avoid being misled by confusing answers. By using various negative samples in limited set of image-question pairs, richer knowledge can be filled. We jointly use the autoregressive supervised fine-tuning (SFT) loss as follows: = EDE log σ β LDNA = LSFT + min θ (cid:32) (cid:34) log πθ(y+x) y+ β x) log πθ(y γ (1) (cid:33)(cid:35) (2) where θ represents the parameters to be trained, LSFT denotes the supervised fine-tuning loss for question-answer pairs, and stands for specific input prompt. Here, y+ denotes correct answer, while = {y 2 , , } represents the set of confusing answers including number of incorrect answers such that = {1, 2, , k}. The minimizing process effectively reinforce the models tendency toward correct answers rather than confusing ones. 1 , The proposed DNA optimization is designed for scenarios with limited training samples, effectively enhancing learning through greater diversity of negative examples. This approach is particularly valuable for multi-vision sensor data, where data scarcity is significant issue. By introducing diverse counterfactual negatives, VLMs gain more opportunities to learn from small dataset. Furthermore, VLMs often misinterpret multi-vision sensor data due to an over-reliance on RGB-bounded reasoning. DNA counteracts this by creating diverse negatives that prevent the VLMs confuse with similar-looking images. Instead, it encourages deep understanding on the unique attributes of each sensor type. This approach aligns with multi-vision Negative Sample Sensor Type Existence Count Position = 1 = 2 = Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray 86.6 79.9 76.8 86.9 76.6 76.0 86.8 79.1 78.2 48.8 48.4 48.2 49.3 46.8 49.4 49.8 49.0 49.4 74.2 73.5 69.7 73.4 74.9 71.3 75.8 74.5 73.3 General Description 83.3 87.6 82.8 84.2 88.5 81.6 86.4 87.9 84.8 Multi-vision Perception 73.2 72.3 69.4 73.4 71.7 69.6 74.3 72.6 71. Contextual Reasoning 81.2 80.8 84.2 81.6 81.0 84.8 82.9 81.2 85.8 Sensory Reasoning 85.0 80.2 80.5 86.4 84.4 81.9 86.4 86.1 82.1 Multi-vision Reasoning 83.1 80.5 82.3 84.0 82.7 83.3 84.6 83.7 84.0 Table 3. Ablation study on multi-vision sensor reasoning performance according to the number of negative sample k. sensor reasoning tasks by allowing the VLMs to acquire deeper understanding of sensor-specific contexts. 5. Experiment 5.1. Experimental Setup 5.1.1. Dataset Collection To construct the MS-PR benchmark, we focused on assembling dataset that captures diverse scenarios and sensorspecific information. We collected total of 13 distinct datasets, which include 7k images that represent wide variety of situations. From these images, we can generate approximately 10k unique questions for evaluation. The depth images include both indoor and outdoor environments and capture various objects in diverse settings. For thermal images, we collected datasets covering broad range of different objects and scenarios, such as in-vehicle sensors, landscapes, people, animals, and thermal screening&scanning. The X-ray images include human body-part images and the security inspection of luggage in airport datasets. This collection offers robust dataset where different multi-vision sensors are represented across broad range of real-world scenarios. We described the overall distribution of data of the MS-PR benchmark in Figure 3. For the training dataset, we used 600 images (200 images per sensor) from the 13 datasets mentioned above that were not included in the MSPR benchmark, generating 3,600 question-answer pairs. We focused on six problem task types requiring high level reasoning to compile the source dataset: Existence, Counting, Position, General Description, Contextual Reasoning, and Sensory Reasoning. More details about the task type and visual context of each source dataset are demonstrated in the supplementary materials. 5.1.2. Implementation Details In our evaluation, we selected 10 state-of-the-art (SOTA) Vision-Language Models (VLMs) that represent the leading edge of the current research field. These models were chosen to provide comprehensive assessment of the capabilities and performance of both open-source and closedsource VLMs across variety of multi-vision sensor tasks on the MS-PR benchmark. Open source model include BLIP-2 [25], LLAVA-v1.5-7B [27], InternVL2-8B [39], VideoLLaMA2-7B [5], MiniCPM-V-2.5-8B[53], Qwen2VL-7B [50], and Phantom-7B [23]. While closed source model include GPT-4o [38], Claude 3.5 Sonnet [1], and Gemini-Pro [47]. In the DNA Optimization, we set the hyper-parameters to β = 2, γ = 0.2, and = 3. Each VLM was trained using QLoRA [9], and during training, we used the AdamW optimizer [31]. For Phantom-7B [23], learning rate of 2e 5 was applied, with one training epoch. All layers of the VLM utilized 256 rank and 256 alpha parameters. For Qwen2-VL-7B [50], learning rate of 2e 5 was also applied, with one training epoch. All layers of the VLM utilized 64 rank and 64 alpha parameters. 5.2. Experiment Result 5.2.1. Evaluation on MS-PR Benchmark In this section, we conduct comprehensive evaluation using the proposed MS-PR benchmark, rigorous framework designed to assess the capabilities of large-scale VisionLanguage Models (VLMs) in two target tasks: Multi-vision Perception and Multi-vision Reasoning. Multi-vision Perception presents the averaged performance on four dimensions for evaluating visual perception. Meanwhile, Multivision Reasoning demonstrates the averaged performance on two dimensions for evaluating the VLMs ability to understand and reason about multi-vision sensor data. As shown in Table 1, the evaluation revealed that performance varies significantly depending on the type of multi-vision sensor used to capture the input images. VLMs generally have moderate scores in multi-vision perception tasks, but vary significantly in multi-vision contextual and sensory reasoning. Sensory reasoning requires VLMs to not only recognize and describe images but also to understand the physical principles underlying the sensor data. For example, interpreting thermal data involves understanding heat signatures, while depth data requires an understanding of the need for spatial geometry beyond naive 2D interpretaModel Number of Training Images per Sensor Sensor Type Existence Count Position General Description Multi-vision Perception Contextual Reasoning Sensory Reasoning Multi-vision Reasoning Phantom-7B + SFT Phantom-7B + DNA Phantom-7B + SFT Phantom-7B + DNA = 50 = = 100 = 100 Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray Thermal Depth X-ray 75.7 69.5 71.5 85.7 73.7 73.4 80.9 71.2 71.7 86.5 76.8 76. 46.8 43.7 45.0 46.8 47.4 49.0 46.3 43.7 46.6 48.2 48.4 49.0 73.4 69.5 66.7 75.0 72.0 70.1 73.4 68.0 67.3 75.8 73.6 71.3 74.9 81.2 82.4 81.4 85.2 83.2 78.5 82.3 82.0 83.0 86.2 83. 67.7 66.0 66.4 72.2 69.6 68.9 69.8 66.3 66.9 73.4 71.3 70.0 77.8 72.9 77.7 81.6 78.0 84.2 79.3 72.9 78.8 82.2 80.0 84.7 56.6 43.7 69.3 77.5 66.6 81.6 70.1 50.0 70.7 83.8 69.9 81. 67.1 58.3 73.5 79.5 72.3 82.9 74.7 61.4 74.8 83.0 75.0 83.3 Table 4. Ablation study on multi-vision sensor reasoning performance according to the number of training images per sensor Model MMMU MME MMBench Q-Bench SEEDI Phantom-7B Phantom-7B +DNA 47.8 49.3 2113 79.8 80.2 69.9 68.2 75. 75.7 Table 5. Performance comparison of Phantom-7B with and without DNA optimization across various benchmarks. tion. The experiment demonstrates VLMs limited proficiency in interpreting sensor data to its physical meaning. We recruited human participants from the crowd sourcing platform Prolific and asked them to evaluate the proposed benchmark. The results shows significant alignment with human assessments. Details of human agreement on our benchmark can be found in the supplementary materials. 5.2.2. Evaluation on the Effects of DNA Optimization In Table 2, we validate that our proposed Diverse Negative Attribute (DNA) optimization significantly improves the multi-vision reasoning performance in VLMs. As we already mentioned in the introduction, DNA optimization is flexible and adaptable enough so that it is applicable to other VLMs without changing the network architecture. With supervised fine-tuning (SFT), Phantom-7B [23] and Qwen2VL-7B [50] shows slight improvements across all metrics, particularly in General Description and Contextual Reasoning. With the proposed DNA optimization, Phantom7B [23] and Qwen2-VL-7B [50] see significant improvements in almost all metrics, especially in Multi-vision Reasoning. This demonstrates DNA optimization significantly enhances multi-vision reasoning, especially in tasks that require an understanding of sensor-specific information. 5.3. Generalizability of DNA Optimization The proposed DNA optimization has demonstrated exceptional performance in multi-vision sensor reasoning tasks. To assess its generalization capability in general benchmark, we conduct evaluation experiments using the MMMU [55], MME[13], MMBench [30], Q-Bench [51], and SEEDI [24] benchmark, which encompasses various disciplines and domains. The results are shown in Table 5. DNA optimization has comparable performance on other benchmarks. This experiment result underscores its capability to generalize to downstream VLM understanding and reasoning tasks. Furthermore, the fine-tuning process using our synthetic data does not detract from the VLMs reasoning abilities in other benchmarks; rather, it enhances its generalizability. 5.4. Ablation on the Number of Negative Sample Table 3 presents an ablation study on multi-vision sensor reasoning performance based on the number of negative samples, denoted as k. Baseline model is Phantom-7B [23] with DNA optimization. This table demonstrates the impact of using different numbers of negative samples in DNA optimization on the performance of multi-vision sensor reasoning. As result, increasing number of negative sample generally improves multi-vision perception and reasoning scores, especially for Contextual and Sensory reasoning tasks, suggesting that more negative samples help VLMs better differentiate relevant features in sensor-specific contexts. In other words, using diverse negative samples can provide the most balanced and comprehensive understanding for VLMs across various multi-vision sensors. 5.5. Ablation on the Number of Training Images Table 4 presents an ablation study analyzing multi-vision sensor reasoning performance as function of the training image count, denoted by n. This table illustrates how adjusting the number of training images per sensor influences multi-vision sensor reasoning performance in both SFT and DNA optimization methods. Even with limited quantity of training images, DNA optimization surpasses SFT, suggesting that DNA optimization can effectively yield results comparable to those obtained with larger dataset. 6. Conclusion In this study, we focus on assessing and improving the ability of large-scale Vision-Language Models (VLMs) to understand and process multi-vision sensor inputs. As VLMs are increasingly deployed in real-world applications, their ability to accurately interpret and reason about data from diverse vision sensors has become crucial. To address this, we propose new evaluation benchmark called MS-PR, which generates samples aimed at specific physical sensor understanding. We also propose novel DNA optimization to improve the multi-vision sensor reasoning ability. Through extensive experiments, we assess the performance of understanding sensor knowledge in the latest state-of-the-art VLMs handling multi-vision input. Moreover, extensive experimental results validate that the proposed DNA optimization significantly improve the performance of multi-vision sensor reasoning in VLMs. We believe that integrating sensor knowledge annotated evaluation benchmark and tailored optimization pave the way for promising future applications of VLMs."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.5 sonnet. https : / / www . anthropic . com / news / claude - 3 - 5 - sonnet, 2024. 4, 7 [2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, and Mansour Zuair. Visionlanguage model for visual question answering in medical imagery. Bioengineering, 10 (3):380, 2023. 2 [3] Donald Eric Broadbent. Perception and communication. Elsevier, 2013. [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023. 1 [5] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 2, 4, 7 [6] Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon Sohn. Diml/cvl rgb-d dataset: 2m rgb-d images of natural indoor and outdoor scenes, 2021. 1 [7] Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, and Dongmyung Shin. Pretraining vision-language model for difference visual question answering in longitudinal chest x-rays, 2024. 1, 2 [8] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. 1 [9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. 7 [10] Quang Minh Dinh, Minh Khoi Ho, Anh Quan Dang, and Hung Phong Tran. Trafficvlm: controllable visual language model for traffic video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 71347143, 2024. 1 [11] Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yijie Guo. Aha: visionlanguage-model for detecting and reasoning over failures in robotic manipulation, 2024. [12] FelipeKitamura Eduardo Farina. Unifesp x-ray body part classifier competition, 2022. 1 [13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. 2, 8 [14] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1246212469. IEEE, 2024. 1 [15] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 2 [16] Ziang Guo, Zakhar Yagudin, Artem Lykov, Mikhail Konenkov, and Dzmitry Tsetserukou. Vlm-auto: Vlm-based autonomous driving assistant with human-like behavior and understanding for complex road scenes, 2024. 1 [17] harang. pet dataset. https://universe.roboflow. com/harang/pet-kjl3x, 2024. visited on 2024-10-28. [18] Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Peng Gao, Abdeslam Boularias, and Hongsheng Li. A3vlm: Actionable articulation-aware vision language model, 2024. 1 [19] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation, 2024. 1 [20] james cook. chips-thermal-face-dataset. https://www. kaggle . com / datasets / kagglechip / chips - thermal-face-dataset, 2020. visited on 2024-10-28. 1 [21] Daniel Kahneman, Anne Treisman, and Brian Gibbs. The reviewing of object files: Object-specific integration of information. Cognitive psychology, 24(2):175219, 1992. 3 [22] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. In International Conference on Machine Learning, pages 1728317300. PMLR, 2023. [23] Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomlatent arXiv preprint chan Park, and Yong Man Ro. for large language and vision models. arXiv:2409.14713, 2024. 4, 7, 8, 3 Phantom of [24] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 2, 8 [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 2, 4, 7 [26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 4, [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 2, 7, 3 [28] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2 [29] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong, and Zhongxuan Luo. Target-aware dual adversarial learning and multi-scenario multi-modality benchmark to fuse infrared and visible for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 58025811, 2022. 1 [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European Conference on Computer all-around player? Vision, pages 216233. Springer, 2025. 2, 8 [31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. [32] Malek. https : X-ray baggage detection dataset. //universe.roboflow.com/malekmhnrl/xray-baggage-detection, 2022. visited on 2024-1111. 1 [33] Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang. arXiv preprint Gpt-driver: Learning to drive with gpt. arXiv:2310.01415, 2023. 1, 2 [34] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. 6 [35] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV, 2012. 1 [36] NJUST. Ifsod dataset. https : / / universe . roboflow.com/njust-oxpbo/ifsod, 2023. visited on 2024-10-28. 1 [37] one. animal-detection-flir-extra dataset. https : / / universe.roboflow.com/onerphct/animal_ detection_flir_extra, 2023. visited on 2024-10-28. 1 [38] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 1, 4, 7, 3 [39] OpenGVLab. Internvl2: Better than the bestexpanding performance boundaries of open-source multimodal modhttps : els with the progressive scaling strategy. / / internvl . github . io / blog / 2024 - 07 - 02 - InternVL-2.0/, 2024. 1, 2, 4, 7, [40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 6 [41] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 6 [42] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 1 [43] Roboflow. thermal dogs and people x6ejw dataset. https : / / universe . roboflow . com / object - detection/thermal-dogs-and-people-x6ejw, 2022. visited on 2023-03-29. 1 [44] Yichen Shi, Yuhao Gao, Yingxin Lai, Hongyang Wang, Jun Feng, Lei He, Jun Wan, Changsheng Chen, Zitong Yu, and Xiaochun Cao. Shield: An evaluation benchmark for face spoofing and forgery detection with multimodal large language models. arXiv preprint arXiv:2402.04178, 2024. [45] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. 2 [46] Jiashun Suo, Tianyi Wang, Xingzhou Zhang, Haiyang Chen, Wei Zhou, and Weisong Shi. Hit-uav: high-altitude infrared thermal dataset for unmanned aerial vehicle-based object detection. Scientific Data, 10:227, 2023. 1 [47] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 4, 7 [48] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z. Dai, Andrea F. Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew R. Walter, and Gregory Shakhnarovich. Diode: dense indoor and outdoor depth dataset, 2019. 1 [49] Visual. Thermal dataset. https : / / universe . roboflow.com/visual-iqhyh/thermal-duv93, 2023. visited on 2024-10-22. 1 [50] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 4, 7, 8, 3 [51] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. 2, 8 [52] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. IEEE Robotics and Automation Letters, 2024. 1, 2 [53] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 1, 2, 4, 7 [54] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499, 2023. [55] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 2,"
        },
        {
            "title": "Supplementary Materials",
            "content": "A. Detailed Description on Dataset We collect 13 different datasets for each multi-sensor vision task type, together with 7k images and 10k unique questions and answers in total. To ensure the generalizability of MS-PR benchmark, we gather wide variety of situations and object types from various different datasets. These datasets are mainly classified into three categories according to multi-vision sensors. 1) For thermal sensor datasets, we collect 2.2k images from 8 different thermal datasets, including M3FD [29], Dog&People [43], Pet [17], TCVP [49], HIT-UAV [46], AnimalDet [37], CTFD [20], and IFSOD [36]. 2) Additionally, we gather 3.1k images from 3 different datasets for depth sensor, including DIODE [48], NYUv2 [35], and DIML [6]. 3) Finally, we sampled 2.6k images from the two different public X-ray datasets, including UNIFESP [12] and BDXR [32] datasets. M3FD [29] dataset contains images from three primary scenes: road views, university campuses, and resort settings. The dataset comprises 24-bit grayscale infrared and visible images, each with resolution of 1024768 pixels. Thermal Dogs and People [43] dataset includes 203 thermal infrared images captured at varying distances from people and dogs in park and home environments. Images are available in both portrait and landscape orientations with spectral color palette applied. Pet dataset [17] features 640640 images depicting diverse activities and motions of cats, dogs, and humans. This dataset has 640640 image size. Thermal Computer Vision Project(TCVP) dataset [49] focuses on heat detection in groups of humans, with an average image size of 640640 pixels. high-altitude infrared thermal dataset for object detection applications on Unmanned Aerial Vehicles(HIT-UAV) [46] is high-altitude infrared thermal dataset for object detection applications involving unmanned aerial vehicles (UAVs). It includes 2,898 infrared images derived from 43,470 video frames captured in diverse scenarios. Animal detection(AnimalDet) dataset [37] consists of thermal images of eight animal speciesdeer, bear, cow, dog, elephant, fox, goat, and wild boar. The average image size is 369363 pixels. The Chips Thermal Face Dataset [20] comprises over 1,200 thermal face images of male and female subjects aged 1823 from three continents. It supports research in advanced thermal facial classification and recognition systems using deep learning IFSOD dataset [36] contains thermal sensor images of various objects, including bicycles, birds, dogs, and techniques. humans, with an average resolution of 640480 pixels. Dense Indoor and Outdoor DEpth Dataset(DIODE) [48] provides high-resolution color images paired with precise, dense, long-range depth measurements. It is the first publicly available RGBD dataset featuring both indoor and outdoor scenes captured using single sensor suite. The NYU-Depth V2 dataset [35] includes video sequences of indoor environments captured with the RGB and depth cameras of Microsoft Kinect. It contains 1,449 densely labeled pairs of aligned RGB and depth images. Digital Image Media Laboratory(DIML)/Computer Vision Laboratory(CVL) RGB-D dataset [6] contains 2 million color images paired with depth maps, covering diverse indoor and outdoor scenes. The RGB images have resolution of 19201080, while depth maps are captured at 512424 pixels. UNIFESP X-ray Body Part dataset [12] comprises X-ray images of various body parts, such as the knee, leg, hip, ankle, thigh, and pelvis. It stands out for its diversity of human anatomical coverage. Baggage Detection X-Ray(BDXR) dataset contains X-ray images of baggage inspected at airports to ensure diversity and generalization. The average image resolution is 1225954 pixels. We described the overall distribution of data sources of the MS-PR benchmark in Figure 3. B. Detailed Description on Prompt We designed the input prompts to create the proposed MS-PR benchmark, ensuring the prompts are comprehensive and tailored to extract meaningful multi-vision sensor capabilities from challenging question and answer sets. These prompts require five additional information to effectively guide the VLMs in generating benchmark data: To provide sufficient sensor information to the LLM, we developed Sensor Knowledge (Figure 5) and incorporated it into <sensor knowledge>. This information contains detailed descriptions and context about thermal, depth, and X-ray sensors. This ensures the VLMs understand the unique physical properties and contextual applications of each sensor type. The appropriate multi-vision sensor type is included in <sensor type>. This explicitly informs the VLMs which sensor (thermal, depth, or X-ray) the prompt relates to, ensuring that the generated examples are relevant to the specific sensor. The desired question type and corresponding examples are provided in <question type> and <question examples>, respectively (Figure 5). This ensures that the model understands the format and context of the questions it needs to generate. The number of negative samples to be generated is specified in <negative samples num>. These negative samples are designed to include plausible yet incorrect answers, encouraging the model to distinguish correct answers from distractors. Our input prompts for generating MS-PR benchmark is described in Figure 6. Figure 5. Description of sensor knowledge information and questions types and examples. Figure 6. Description of prompts for generating challenging multiple-choice questions and answers for multi-vision sensor tasks C. Human Evaluation We conducted human evaluation study to assess how closely our newly proposed MS-PR benchmark aligns with the answers human would select when viewing sensor images. total of 20 participants were recruited through the crowd-sourcing Prolific platform. We only accepted reviewers with English as their first language and who had at least bachelors degree. In the human study, we recruited Prolific participants with approval rates higher than 95% and with at least 200 prior submissions. Participants were rewarded e9.4/hr for completing all multiple choice questions. We sampled 45 multi-vision reasoning questions from MS-PR benchmark, with 15 questions thermal, depth, and X-ray. allocated to each sensor type: Experiment results on human evaluation are demonstrated in Figure 7. Participants achieved 95.1% accuracy rate, demonstrating that the proposed MSPR benchmark significantly aligns with human assessment. We also evaluated how other VLMs responded to the 45 sampled questions and verified that their performance on multi-vision reasoning, as shown in Table 1 of the main text, aligns within the margin of error. To be specific, GPT-4o [38] achieved the highest score of 73.3, followed by InternVL2-8B [39] and Phantom7B [23], both scoring 62.2, Qwen2-VL-7B [50] with 60.0, and LLaVA-1.5-7B [27], which recorded the lowest score of 53.3. The performance difference between the top VLMs (GPT-4o) and human participants is notable at 21.8%, reflecting the challenges that current VLMs face in achieving human-level understanding in multi-vision reasoning tasks. Figure 7. Human Agreement Results on Multi-Vision Sensory Reasoning Performance Across Diverse VLMs D. Additional Question and Answer Examples Figure 8-13 provide examples of benchmark evaluations conducted using various Vision-Language Models (LLaVA-1.57B [26], InternVL2-8B [39], Phantom-7B [23], and Phantom-7B with DNA optimization) across three multi-vision sensors: thermal, depth, and X-ray. The answers selected by the models are displayed next to the corresponding options using the models representative icons and pictograms, and they are color-coded based on correctness: green indicates correct answer, while red indicates an incorrect answer. By displaying these visual examples with clear indicators and detailed observations, we provide valuable insights into how different VLMs perform on multi-vision sensor reasoning tasks. These examples underscore the importance of tailored optimization techniques, like Diverse Negative Attributes(DNA) optimization, in enhancing the multi-vision sensor reasoning capabilities of VLMs across diverse sensor modalities. Figure 8. The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (Existence). Green font denotes the correct answer, while red font denotes the incorrect answer. Figure 9. The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multi-vision Perception task (Counting). Green font denotes the correct answer, while red font denotes the incorrect answer. Figure 10. The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multivision Perception task (Position). Green font denotes the correct answer, while red font denotes the incorrect answer. Figure 11. The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multivision Perception task (General Description). Green font denotes the correct answer, while red font denotes the incorrect answer. Figure 12. The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multivision Reasoning task (Contextual Reasoning). Green font denotes the correct answer, while red font denotes the incorrect answer. Figure 13. The comparison of performance across different multi-vision sensors with respect to the representative VLMs in the Multivision Reasoning task (Sensory Reasoning). Green font denotes the correct answer, while red font denotes the incorrect answer."
        }
    ],
    "affiliations": [
        "Integrated Vision Language Lab, KAIST, South Korea"
    ]
}