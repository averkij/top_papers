{
    "paper_title": "Music Flamingo: Scaling Music Understanding in Audio Language Models",
    "authors": [
        "Sreyan Ghosh",
        "Arushi Goel",
        "Lasha Koroshinadze",
        "Sang-gil Lee",
        "Zhifeng Kong",
        "Joao Felipe Santos",
        "Ramani Duraiswami",
        "Dinesh Manocha",
        "Wei Ping",
        "Mohammad Shoeybi",
        "Bryan Catanzaro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . e [ 1 9 8 2 0 1 . 1 1 5 2 : r Pre-print. Under Review. MUSIC FLAMINGO: SCALING MUSIC"
        },
        {
            "title": "UNDERSTANDING IN AUDIO LANGUAGE MODELS",
            "content": "Sreyan Ghosh12, Arushi Goel1, Lasha Koroshinadze2, Sang-gil Lee1, Zhifeng Kong1, Joao Felipe Santos1, Ramani Duraiswami2, Dinesh Manocha2, Wei Ping1, Mohammad Shoeybi1, Bryan Catanzaro1 NVIDIA, CA, USA1, University of Maryland, College Park, USA2 Correspondence: sreyang@umd.edu, arushig@nvidia.com Project: https://research.nvidia.com/labs/adlr/MF/"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Music Flamingo, novel large audiolanguage model, designed to advance music (including song) understanding in foundational audio models. While audiolanguage research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, large-scale dataset labeled through multi-stage pipeline that yields rich captions and questionanswer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the models reasoning abilities, we introduce post-training recipe: we first cold-start with MF-Think, novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-ofthe-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as generalist and musically intelligent audiolanguage model. Beyond strong empirical results, Music Flamingo sets new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both benchmark and foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.1 Without music, life would be mistake. Friedrich Nietzsche"
        },
        {
            "title": "INTRODUCTION",
            "content": "Audio including speech, environmental sounds, and music is central to human perception and interaction. It enables us to converse, perceive our surroundings, express emotions, interpret multimedia, and engage with cultural artifacts. Among these, music is particularly significant: the creation, sharing, discovery, and understanding of music are daily activities for billions worldwide. Recent progress in AudioLanguage Models (ALMs) has extended language models into the auditory domain, enabling impressive advances in speech and sound understanding. Yet, music remains fundamentally distinct from other forms of audio. Core musical attributes such as key, tempo, harmony, instrumentation, and vocal styles are not present in non-musical audio and require specialized reasoning. Moreover, tasks adapted from speech and sound (e.g., captioning, transcription, retrieval) demand unique treatment when applied to music. To date, no model has achieved music understanding on par with the multi-modal breakthroughs seen in vision or speech. Improved music understanding 1Equally contributed and led the project. Names randomly ordered. Significant technical contribution. 1 Pre-print. Under Review. Figure 1: Comparison of captions for two diverse, full-length, in-the-wild songs by Music Flamingo and other frontier models. Prior models, such as AF3, tend to output short, surface-level descriptions (e.g., broad genre, tempo, or instrumentation), while Qwen3-Omni offers isolated observations without forming coherent musical narrative. In contrast, Music Flamingo produces detailed, multi-layered captions that integrate theory-aware analysis with performance context. It links surface attributes (tempo, key, etc.) to mid-level structures (chord progressions, vocal phrasing, etc) and higher-level dimensions (lyrical meaning, emotional trajectory, etc.). This ability to connect one aspect of music to another results in richer, more holistic captions that resemble how trained musicians describe songs. Detailed expert analysis in Appendix and F. would unlock richer applications in creation, recommendation, cross-cultural analysis, education, and interactive systems, enabling models to engage with music as deeply as humans do. Despite advances in scaling LALMs (Goel et al., 2025; Chu et al., 2024; KimiTeam et al., 2025; Tang et al., 2024; Bertin-Mahieux et al., 2011), effective music understanding remains an open challenge (hereafter, we use music to refer broadly to both instrumental pieces and songs). Current frontier LALMs, when captioning even widely recognizable tracks, often produce short and generic descriptions, misidentify surface-level attributes such as tempo or key, and sometimes rely on text-derived knowledge rather than genuine auditory analysis (Gemini team, 2025). We argue this stems largely from data: most available musiccaption pairs originate from early datasets like MusicCaps (Agostinelli et al., 2023b), and subsequent datasets inherit its stylistic limitations of short, surface-level summaries, limitations of short, surface-level summaries that omit bar/time localization, harmonic and formal structure, vocal/lyric grounding, and cultural context, and often with narrow focus on instrumental-only snippets. This prevents models from learning the layered nature of music, spanning surface attributes (tempo, key, timbre), mid-level structures (chord progressions, rhythm, phrasing), and higher-level dimensions (lyrics, emotional arcs, cultural context). Architecturally, training practices for most music LLMs or captioners still constrain holistic learning for example, the use of encoders (like CLAP (Elizalde et al., 2022)) that do not capture spoken content or low-level features like pitch in their representations (see our study in Appendix G), thereby constraining learning of vocal timbre, lyrical alignment, and expressive nuances in songs. We contend that even task as basic as music captioning, when re-imagined beyond surface-level summaries, is inherently explorative and compositional: musically informed description requires reasoning through multiple layers of structure and meaning, and admits not one single answer but spectrum of valid interpretations shaped by theory, perception, and artistry. Main Contributions. In this paper, we introduce Music Flamingo, new and open-source large audiolanguage model specifically designed to advance music understanding. Unlike speech or environmental sounds, music is inherently layered, expressive, and structured, combining surfacelevel acoustic attributes (tempo, key, timbre) with mid-level organization (harmony, form, rhythm) and higher-level dimensions (lyrics, style, affect, cultural context). Capturing this multi-faceted nature of music requires models that can move beyond surface-level recognition toward reasoning and interpretation more akin to trained musician. 2 Pre-print. Under Review. To build Music Flamingo, we re-imagine the scope of music understanding and recast conventional tasks, such as music captioning and question answering, into comprehensive formulations that demand deliberate, step-by-step reasoning  (Fig. 3)  . To support this reframing, we introduce new strategies for both data curation and model training. First, we present MF-Skills, dataset with 4M+ highquality samples for training music-understanding models. Unlike prior corpora dominated by short, instrumental snippets, MF-Skills scales to long, multicultural full-length songs with vocals drawn from diverse sources. We propose multi-step labeling pipeline that yields detailed, multi-aspect, layered captions capturing harmony, structure, timbre, lyrics, and cultural context designed to elicit musician-level reasoning. Beyond captions, MF-Skills includes carefully curated questionanswer pairs that move past simple instrument identification toward tasks requiring temporal understanding, harmonic analysis, lyrical grounding, and other skills. On the modeling side, we first identify core limitations in Audio Flamingo 3 and continue-pre-training it to build stronger backbone by fine-tuning it on multilingual, multi-speaker ASR and extended audio reasoning datasets before specializing it for music. Next, we propose post-training stage specifically designed to enhance reasoning. For this stage, we further introduce MF-Think, dataset of 300K chain-of-thought examples grounded in music theory, which we use for cold-start reasoning training. Finally, we apply GRPO-based reinforcement learning with custom rewards, enabling explicit step-by-step musical reasoning. In summary, our contributions are: We propose Music Flamingo, new LALM for advancing music understanding. We re-imagine conventional music tasks (e.g., captioning, QA) as reasoning-centric formulations and introduce novel training strategies tailored to these tasks. To support training, we release MF-Skills and MF-Think, two large-scale datasets containing musiccaption and musicQA pairs designed to promote deliberate reasoning. Unlike prior datasets limited to short instrumental clips, ours include full-length, multi-cultural songs with detailed, multi-aspect annotations. Music Flamingo achieves state-of-the-art results on 12 music understanding and reasoning benchmarks. Beyond academic benchmarks, expert evaluations show its outputs are more accurate and preferred by trained musicians than existing models. To promote research in this area, we will release code, training recipes, and our new datasets under an appropriate research-only license."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multimodal audiolanguage modeling. The rapid progress of LLMs has accelerated the development of multimodal LLMs (MLLMs) capable of understanding and reasoning across diverse modalities, including audio. Within this space, ALMs focus specifically on reasoning over auditory inputs such as speech, sounds, and music. Architecturally, ALMs generally follow two paradigms: (i) Encoder-only ALMs, which learn joint embedding space for audio and text, enabling tasks like cross-modal retrieval. Representative models include CLAP (Elizalde et al., 2022), Wav2CLIP (Wu et al., 2021), and AudioCLIP (Guzhov et al., 2021). (ii) Encoderdecoder ALMs (often called Large AudioLanguage Models, LALMs), which augment decoder-only LLMs with audio encoders. Notable examples include LTU (Gong et al., 2023b), LTU-AS (Gong et al., 2023a), SALMONN (Tang et al., 2024), Pengi (Deshmukh et al., 2023), Audio Flamingo (Kong et al., 2024), Audio Flamingo 2 (Ghosh et al., 2025), Audio Flamingo 3 (Goel et al., 2025), AudioGPT (Huang et al., 2023), GAMA (Ghosh et al., 2024), Qwen-Audio (Chu et al., 2023), and Qwen2-Audio (Chu et al., 2024). There has also been surge of LALMs that specifically focus on music, including Mu-LLaMA (Liu et al., 2024a), MusiLingo (Deng et al., 2024), M2UGen (Liu et al., 2024b), SALMONN (Tang et al., 2024), and LLARK (Gardner et al., 2024). These LALMs have substantially advanced core audio understanding tasks such as automatic speech recognition (ASR) (Radford et al., 2022), audio captioning (Kim et al., 2019), and acoustic scene classification (Chen et al., 2022). More importantly, they have enabled open-ended audio question answering, which requires both complex auditory reasoning and external world knowledge. While music has often been included as modality within these models, it has rarely been central focus. Scaling music understanding within ALMs has proven particularly difficult. For instance, while the Audio Flamingo series has expanded its training data substantially from version 1 to 3, the music component of training data has increased by only 10%, compared to much larger growth in speech and environmental sounds. Similarly, models such as Kimi (KimiTeam et al., 2025) and 3 Pre-print. Under Review. Figure 2: I. Annotation pipeline for constructing our proposed datasets from diverse music clips. II. Training pipeline of Music Flamingo: we begin by improving Audio Flamingo 3, then perform full fine-tuning on music datasets to build the Music Flamingo foundation model. Finally, the model undergoes reasoning cold-start training followed by GRPO fine-tuning to enable step-by-step reasoning. Step Audio (Huang et al., 2025) (where training data disclosures exist) show comparable imbalances. Finally, models like LLARK (Gardner et al., 2024) and MU-LLaMA (Liu et al., 2024a) curate music captions and question-answer pairs from existing open-source datasets, which lack diversity and skills. This is due to several challenges: the difficulty of collecting high-quality and culturally diverse music audio (Kumar et al., 2025), curating reliable annotations (Christodoulou et al., 2024), and the reliance of most works on private, proprietary datasets (Agnew et al., 2024). Large labs often construct in-house collections of lyrics and metadata by scraping online lyric repositories, translations, and song databases (Ahmed et al., 2025). Models such as Jukebox and Neural Melody Reconstruction exemplify this paradigm. Finally, as noted earlier, most publicly available datasets emphasize short instrumental clips, with very limited coverage of full-length songs containing vocals, hindering comprehensive understanding of music (Kumar et al., 2025). Music information retrieval and captioning. Beyond LALMs, music understanding has long history in Music Information Retrieval (MIR), encompassing retrieval, classification, and captioning. Foundational tasks such as key detection (Chai & Vercoe, 2005), chord recognition (Sheh & Ellis, 2003), and tempo estimation (Scheirer, 1998) have been extensively studied, largely in instrumental music. Lyrics transcription has also been explored, posing more difficult challenge than ASR due to overlapping vocals, diverse singing styles, and background instrumentation (Mesaros & Virtanen, 2010). As discussed earlier, music captioning has been studied in analogy to audio event captioning, but typically produces short, high-level semantic descriptions rather than layered, theory-aware accounts. Importantly, improved captioning not only benefits downstream music understanding but also supports the training of generative music models by providing high-quality text supervision for in-the-wild audio (Agostinelli et al., 2023b). This connection has recently been emphasized in both standalone music modeling efforts and in broader video generation systems (Chen et al., 2025)."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "To build Music Flamingo, we first curate high-quality songs, followed by labeling them and finally fine-tuning the model on the curated data. Music Flamingo is specialized music understanding model built by fine-tuning version of Audio Flamingo 3, specifically with high-quality data to close the gap on skills and tasks crucial for music understanding. Finally, the model is fine-tuned using reinforcement learning to enable step-by-step music reasoning. 3.1 IMPROVED AUDIO FLAMINGO 3 BASELINE Data. We first strengthen Audio Flamingo 3 to serve as the backbone for Music Flamingo. Unlike instrumental-only music, songs contain vocals that contribute not only lyrics but also timbre, style, 4 Pre-print. Under Review. Figure 3: Examples from MF-Skills Caption , MF-Skills QA , and MF-Think . We emphasize that our re-imagined captions are denser, more informative, and designed to require deliberate reasoning to generate. Additional examples are provided in Appendix G.1. and expressive variation. Capturing these elements requires stronger spoken language understanding than prior baselines. Thus, in addition to the data used for AF3 training, we add the following to the mix: 1) Across all fine-tuning stages (13), we incorporate large-scale multilingual ASR data (sources Emilia dataset (He et al., 2024), CoVoST (Wang et al., 2020), MUST (Qin et al., 2025), Amazon-SIFT (Pandey et al., 2025); details in Appendix 5) to better capture global vocal diversity, 2) In stage 3, we add multi-talker ASR data, including CHIME (Watanabe et al., 2020; Cornell et al., 2023), Switchboard (Godfrey et al., 1992) and ALI meeting (Yu et al., 2022), enabling the model to parse turn-taking and overlapping voices, which is critical for understanding duets and ensemble singing and, 3) We expand the data mix with speech-centric skills, including phoneme recognition and lyrics transcription, improving alignment between vocal content and musical context. Training Pipeline. We adopt the training paradigm introduced in Audio Flamingo 3 (Goel et al., 2025) to fine-tune the model on the diverse set of speech data described above. The resulting finetuned model then serves as the foundation for developing the music-focused foundational model. 3.2 BUILDING FOUNDATIONAL MUSIC UNDERSTANDING MF-Skills. Prior captioning datasets mostly provide surface-level summaries, while existing QA datasets are dominated by simple classification tasks (e.g., instrument or tempo detection). Even large-scale skill-focused datasets such as AudioSkills focus primarily on sounds and speech for their diverse skill-specific QAs, with music data reduced to basic information extraction. To mitigate this gap, we design MF-Skills and capture the layered nature of music to train models for deliberate reasoning. Figure 2 illustrates our data curation pipeline, and Figure 3 provides examples from our curated dataset. We begin by collecting full-length songs from diverse cultures (3M in total), as shown in Figure 4, thereby moving beyond the short, Western instrumental clips that dominate prior datasets. As shown in Figure 2, our pipeline consists of four stages: 1) Initial caption synthesis: Generate short, surface-level captions for 30s segments using frontier music models to minimize hallucinations, 2) Metadata extraction: We apply conventional MIR tools, including Figure 4: Genres (inner circle) & Cultures (outer circle) distribution of songs. 5 Pre-print. Under Review. madmom (Bock et al., 2016) (beat), essentia (Bogdanov et al., 2013) (key), Chordino (Mauch & Dixon, 2010) (chords), Parakeet (NVIDIA, 2025) (lyrics)to provide reliable low-level attributes and, 3) Caption & QA creation: Using metadata and initial captions, we prompt an LLM (with music-theory grounding) to produce detailed, multi-aspect captions covering several aspects including: a) low-level information (tempo, BPM, keys), b) instrumentation & production, c) lyrics, and lyrical themes (including structural segmentations such as verses, choruses, and bridges), d) song structure & dynamics e) theoretical insight (e.g., chord transitions and harmonic movements) and f) overall mood & context. Our final captions have an average of 451.65 words. For QA, we analyze skill gaps in AF3 using benchmarks such as MMAU (Sakshi et al., 2024), MMAU-Pro (Kumar et al., 2025), MuChoMusic (Weck et al., 2024), MusicCaps (Agostinelli et al., 2023b), MusicQA (Li et al., 2022), and NSynth (Engel et al., 2017), then generate novel QA targeting five skills: a) Temporal understanding, b) Attribute identification, c) Harmonic & theoretical analysis, d) Lyric and vocal grounding, e) Comparative and structural reasoning. This approach also mitigates distribution gaps, e.g. instrument identification in complex, multi-layered songs rather than isolated clips, and cultureskills gaps, e.g. identifying ragas in Indian music or polyrhythms in African drumming, which prior datasets neglect as they were dominated by Western music. 4) Quality filtering: frontier MLLM is used to verify and retain only high-quality captions and QAs. The final dataset contains 5.2M examples (3.4M captions and 1.8M QAs). Beyond curating new data, we also refine existing music datasets such as MSD (Bertin-Mahieux et al., 2011), Music4All (Geiger et al., 2025), and the music subset of AudioSkills-XL (Goel et al., 2025). We (a) rewrite captions to add lyrical themes, vocal attributes, and correct mislabels of tempo, key, and timbre using metadata, and (b) reframe MCQ-style questions to reduce language priors and guessing, an issue highlighted in recent benchmarks (MMAU-Pro (Kumar et al., 2025), RUListening (Zang et al., 2025)). We cluster Q&As by trait and rephrase them with metadata to require genuine auditory perception. We provide examples below: Existing caption (from AudioSkills-MSD): This is an upbeat 1980s pop-rock track with danceable 4/4 beat around 140 BPM, featuring bright guitar riffs and melodic synthesizers. The song carries an energetic and catchy feel, blending indie rock elements with disco-inspired rhythms, typical of early 80s production. Our modified caption: This upbeat 1980s pop-rock track in minor rides danceable 4/4 beat around 120 BPM, driven by bright electric guitars, shimmering synths, and lively drums. Its catchy, energetic melody blends indie and disco influences, The lyrics add layer of intimate storytelling, weaving lines about moments of fleeting connection and dreamy recollection (I close my eyes and count to ten we were strangers moment ago). Existing QA (from AudioSkills-MSD): What genre does this track? Choose one among the following options: (A) Jazz (B) Classical (C) Rock (D) Spoken Word (Spoken Word stands out as unique option among all other options, which are music genres) Additional plausible distractors options added to modified QA: (E) Audiobook narration excerpt (F) Rap cappella (no beat) (G) Podcast monologue intro (H) Documentary voice-over bed (I) Theatrical monologue with ambiance (J) Spokenword/poetryy Training Methodology. We begin with the improved base model derived from re-training Audio Flamingo 3, as outlined in Section 3.1. This model is subsequently fine-tuned on the proposed MF-Skills dataset, the improved QA datasets described above, and other music datasets derived from the training mix of Audio Flamingo 3 (Goel et al., 2025). Inspired by our study in Appendix G, we also incorporate data for learning low-level music properties, such as chords, keys, and BPM. Dataset details in Appendix We also encounter two primary limitations in training the model. First, the Audio Flamingo 3 backbone supports maximum context length of 8,192 tokens and 10 minutes of audio, whereas our curated datasets predominantly contain much longer captions and full-length songs up to 20 minutes. We extend the context length to 24k tokens, and adopt fully sharded training to handle the increased memory requirements. Second, music understanding requires fine-grained temporal perception, including chord progressions, tempo, key changes, and vocal dynamics. To capture these transitions, we incorporate time-aware representations into the audio encoder outputs before feeding tokens into the LLM. Specifically, we employ Rotary Time Embeddings (RoTE) (Goel et al., 2024), Pre-print. Under Review. which define the rotation angle θ using absolute timestamps rather than token indices. Unlike standard RoPE, where the rotation angle θ depends on the token index as θ 2π, RoTE defines θ using the tokens absolute timestamp τi: θ τi 2π. For audio tokens produced at fixed stride of 40ms (Radford et al., 2022; Goel et al., 2025), we interpolate discrete time positions τi and feed them into the RoTE module to obtain lightweight, temporally grounded representations."
        },
        {
            "title": "3.3 POST-TRAINING WITH REINFORCEMENT LEARNING",
            "content": "While prior music understanding tasks rarely required reasoning, our formulation explicitly demands it. For example, generating caption in Figure 3 requires the model to progressively connect surface properties (tempo, key) with higher-level structures (harmony, form, production, lyrics) and then articulate them as coherent musical narrativea process that is non-trivial even for trained musicians. To enable this, we introduce post-training stage beyond large-scale SFT that strengthens Music Flamingos reasoning abilities. First, we construct MF-Think, high-quality Chain-of-Thought (CoT) dataset used for cold-start reasoning. We then fine-tune with MF-Think before applying GRPO with custom-designed rewards, encouraging explicit step-by-step reasoning. MF-Think. We begin with diverse, high-quality subset of MF-Skills. Since not all QAs demand deep reasoning, we sub-sample the most challenging examples by prompting gpt-oss-120b with both the audio and QA (prompt in Appendix H). CoT Generation. For each selected QA or caption, we prompt gpt-oss-120b with metadata from MF-Skills to generate long, theory-grounded reasoning chains. Prompts include constraints on length, grounding to music theory, and exemplar demonstrations. Quality Filtering. Each reasoning chain is segmented into smaller steps, which are fact-checked using our post-SFT MF (Yes/No verification against the audio). We rewrite chains with minor errors and discard those with >30% incorrect steps. The final dataset contains 176k CoT examples, including 117k QA and 59k captioning samples, providing rich foundation for reasoning-enhanced training. Supervised Fine-Tuning with MF-Think. To equip the model with advanced reasoning capabilities, we first perform SFT of the music foundation model on our curated MF-Think dataset. During this stage, we append the prompt: Output the thinking process in <think> </think> and final answer in <answer> </answer> to the original questions in the MF-Think dataset, to encourage the model to explicitly generate reasoning chains within the <think> </think> tags and the final answer within the <answer> </answer> tags. This process instills structured reasoning for both the question-answering and caption-generation tasks. This SFT stage acts as an initial warm-up phase, effectively priming the model for subsequent reinforcement learning (RL) fine-tuning. GRPO for Music reasoning and understanding. Building on the advancements in the GRPO algorithm, we adhere to the standard GRPO algorithm to train our model. GRPO obviates the need for an additional value function and uses the average reward of multiple sampled outputs for the same question to estimate the advantage. For each given question q, the policy model generates group of candidate responses {o1, o2, . . . , oG} from the old policy πθold along with their corresponding rewards {r1, r2, . . . , rG} which are computed using rule-based reward functions (e.g., format and accuracy). The model πθ is subsequently optimized using the following objective function: (cid:34) (θ)=Eq,{oi} 1 G (cid:88) i=1 (cid:16) min (cid:16) πθ(oiq) πθold (oiq) Ai, clip(cid:0) πθ(oiq) πθold (oiq) ,1 ϵ,1 + ϵ(cid:1)Ai (cid:17) (cid:35) (cid:17) βDKL(πθ πref) (1) where ϵ is the clipping range of the importance sampling ratio, β is the regularization strength of the KL-penalty term that encourages the learned policy to stay close to the reference policy, and is the group size, i.e., the number of candidate responses (samples) the policy generates for each input question, which is set to 5 in our experiments. To stabilize training, the sampled rewards are normalized to compute the advantages Ai as: rimean({r1,r2,...,rG}) std({r1,r2,...,rG}) Next, we discuss the custom reward functions we designed for GRPO training, which play pivotal role in optimization. Format Reward. In order to encourage the model to generate outputs that adhere to the prescribed response format, we use the standard regex-based format reward (DeepSeek-AI, 2025). Specifically, the model is instructed to produce reasoning traces enclosed within <think> </think> tags, 7 Pre-print. Under Review. followed by the final answer enclosed within <answer> </answer> tags. If the output strictly follows the required tag structure, the model gets reward of 1 else 0. This binary reward function ensures that the model learns to consistently produce well-structured responses. For question-answering (QA) tasks, we employ the accuracy reward to Accuracy Reward. encourage the model to generate accurate final answers. Given question with the corresponding ground-truth answer, the model generates candidate output oi, where the final answer is extracted from within the <answer> </answer> tags. The accuracy reward directly matches the normalized predicted and ground truth answers, enforcing strict answer correctness. Structured Thinking Reward. For caption generation tasks, the standard accuracy reward cannot be directly applied due to the long and open-ended nature of the generated captions. To address this, we design custom reward function that evaluates generated captions against structured ground-truth metadata. To achieve this, we first generate ground-truth structured metadata as shown below using gpt-oss-120b (OpenAI, 2025) for the subset of the captions in the MF-Skills dataset as follows: {Genre: Americana, BPM: 125, Key: minor, Meter: 4/4, Structure: Intro, Verse, Verse, Bridge, Solo, Chorus, Outro, Instruments: fingerstyle acoustic guitar, banjo, mandolin, spoken-word voice, Vocal Character: male spoken-word, deep resonant timbre, clear/deliberate, light reverb, Lyric Themes: forgiveness, humility, spiritual prayer, desert frontier, betrayal, outlaw narrative, Theory: minor center; modal interchange with relative major; F#aug Eb6 D7 resolution; Gmaj7/G7 brighten prayer sections, Mix Notes: highfidelity organic; wide natural stereo panning; minimal reverb; warm, clear, light compression; close-mic intimacy, Dynamics: bridge increases harmonic rhythm/urgency.} The structured thinking reward function computes string match for each answer in the category of the structured ground-truth metadata (e.g. Genre, Subgenre, BPM etc.) and the generated caption. The total reward is obtained by normalizing the number of matching words by the total number of metadata categories. The overall reward function used in GRPO training integrates the format reward with the accuracy reward for the data with question-answer subset, while for the caption subset of data, it combines the format reward with the structured reasoning reward."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Experimental Setup. We train Music Flamingo on 128 NVIDIA A100 GPUs (80GB). Details on batch size, learning rates, and optimizers for each stage of training are in Appendix D. Baselines. We evaluate our model against recent SOTA LALMs, including GAMA (Ghosh et al., 2024), Audio Flamingo (Kong et al., 2024), Audio Flamingo 2 (Ghosh et al., 2025), Audio Flamingo 3 (Goel et al., 2025), Qwen-Audio (Chu et al., 2023), Qwen2-Audio (Chu et al., 2024), Qwen2-AudioInstruct, Qwen2.5-Omni (Xu et al., 2025), R1-AQA (Li et al., 2025a), Pengi (Deshmukh et al., 2023), Phi-4-mm (Abouelenin et al., 2025), Baichun Audio (Li et al., 2025b), Step-Audio-Chat (Huang et al., 2025), LTU (Gong et al., 2023b), LTU-AS (Gong et al., 2023a), SALMONN (Tang et al., 2024), AudioGPT (Huang et al., 2023), and Gemini (2.0 Flash, 1.5 Pro, 2.5 Flash and 2.5 Pro) (Team et al., 2023), as well as GPT-4o-audio (Hurst et al., 2024). For Table 1, we only compare against open LALMs. All results reported in the tables correspond to the best-performing model. Evaluation Datasets. We evaluate AF3 across broad set of benchmarks spanning music information retrieval (MIR), question answering, lyrics transcription, reasoning, and our proposed dataset SongCaps. SongCaps consists of 1,000 culturally diverse songs curated to assess captioning capabilities across multiple dimensions (see Section 3.2). Rather than relying on lexical overlap metrics, we evaluate captions using human-expert judgments and LLM-as-a-judge assessments. For MIR, we use NSynth (Source and Instrument) (Engel et al., 2017), MusicCaps (Agostinelli et al., 2023a), Medley-Solos-DB (instrument classification) (Lostanlen et al., 2019), and GTZAN (genre classification) (Tzanetakis & Cook, 2002). For QA and reasoning, we include MusicAVQA (Li et al., 2022), Music Instruct (Deng et al., 2024), MMAU (v05.15.25) (Sakshi et al., 2024), MMAU-Pro (Kumar et al., 2025), MuChoMusic (perceptual version) (Zang et al., 2025; Weck et al., 2024), and MMAR (Ma et al., 2025). For lyrics transcription, we evaluate on Opencpop (Wang et al., 2022) dataset for chinese songs and MUSDB18 Lyrics (Rafii et al., 2019) dataset for English songs. We 8 Pre-print. Under Review. Table 1: Comparison of Music Flamingo (w/ GRPO) with other LALMs on various benchmarks (WER (Word Error Rate), ACC (Accuracy), Score (1-10) and GPT5 (GPT evaluation)). We report scores for only the top-performing prior LALM. We highlight closed source, open weights, and open source models. Task Dataset Model Metrics Results MMAU (Music) full-test test-mini MMAU-Pro-Music Music QA and Reasoning MuChoMusic MMAR (Music) Music Instruct Music AVQA SongCaps (Ours) Human GPT5-Coverage GPT5-Correctness NSynth Source Instrument Music Information Retrieval GTZAN Genre Medley-Solos-DB Instrument Lyrics Transcription MusicCaps Opencpop Chinese MUSDB18 English Audio Flamingo 3 Music Flamingo Gemini-2.5 Flash Music Flamingo Qwen3-O Music Flamingo Qwen2.5-O Music Flamingo Audio Flamingo 3 Music Flamingo Audio Flamingo 3 Music Flamingo Audio Flamingo 3 Music Flamingo Audio Flamingo 3 Music Flamingo Pengi Music Flamingo Audio Flamingo 2 Music Flamingo Qwen3-O Music Flamingo GPT-4o Qwen2.5-O Music Flamingo GPT-4o Qwen2.5-O Music Flamingo ACC ACC ACC ACC GPT5 ACC Score ACC ACC ACC GPT5 WER WER 73.95 74.47 76.83 76.35 64.90 65. 52.10 74.58 46.12 48.66 92.7 97.1 76.7 73.6 6.5 6.7 6.2 8.3 8.8 8.0 65.5 78.9 75.89 80. 80.00 84.45 85.80 90.86 7.2 8.8 53.7 55.7 12.9 32.7 68.7 19.6 acknowledge the existence of numerous other MIR baselines and benchmarks, as MIR encompasses broad range of tasks. For the scope of this paper, however, we restrict our comparisons to LALMs and the benchmarks most commonly used in the LALM literature. We encourage the community to further expand evaluations to wider set of MIR baselines in future work. Music Understanding and Reasoning Evaluation. Table 1 shows that Music Flamingo consistently sets the bar across music QA, reasoning, MIR, and lyrics transcription benchmarks. On MMAUMusic, it reaches competitive 76.83 accuracy, surpassing both closed and open-source models. The gap widens on the tougher MMAU-Pro-Music and MuChoMusic benchmarks, where Music Flamingo scores 65.6 and 74.58, respectively, clear evidence of its robustness on complex datasets. Without reinforcement learning fine-tuning with thinking traces, performance drops to 63.9 and 69.5 respectively, highlighting the value of step-by-step reasoning and exploration. In MIR tasks, Music Flamingo continues to dominate: on NSynth, it achieves 80.76% accuracy in instrument recognition, and on Medley Solos DB, it reaches 90.86% for fine-grained instrument classification. It also delivers significantly lower WER on Chinese and English lyrics transcription than both open and closed-source LALMs. These results establish Music Flamingo as the most capable model to date for detailed music understanding and reasoning. On our proposed SongCaps benchmark, designed to evaluate music captioning, human raters scored the model outputs on scale of 1-10. Music Flamingo achieves high rating of 8.3 outperforming Audio Flamingo 3. Furthermore, we evaluate the captions using LLM-as-a-judge measuring both correctness and coverage of the caption. Music Flamingo achieves 8.0 for correctness and 8.8 for coverage, outperforming Audio Flamingo 3. These results highlight that Music Flamingo not only excels on structured QA and recognition tasks but also produces richer, more faithful natural language descriptions of music. Qualitative Evaluation. We perform thorough qualitative evaluation of Music Flamingos outputs, assessed by trained music experts, in comparison with state-of-the-art LALMs in this domain. Due to 9 Pre-print. Under Review. space constraints, we refer the readers to Appendix for analysis on songs of varying genres and popularity, and additionally analysis of songs from different cultures in Appendix F."
        },
        {
            "title": "5 CONCLUSION, LIMITATIONS AND FUTURE WORK",
            "content": "We introduced Music Flamingo, large audiolanguage model designed to advance music understanding. By curating MF-Skills and MF-Think, we scale beyond short, instrumental clips to full-length, multi-cultural songs with layered annotations, and incorporate chain-of-thought reasoning for richer music analysis. Through improved pretraining, fine-tuning, and post-training with reinforcement learning, Music Flamingo achieves SOTA results across diverse music understanding and reasoning benchmarks. Beyond empirical gains, it demonstrates how models can move from surface-level recognition toward layered, human-like perception of songs. Music Flamingo still has few limitations, including: (i) limited understanding of underrepresented or skewed cultural traditions, highlighting the need to expand training data across more diverse global music; (ii) gaps in specialized tasks, such as fine-grained piano technique recognition and other instrument-specific skills; and (iii) the need to broaden coverage across additional musical skills to achieve more comprehensive understanding."
        },
        {
            "title": "REFERENCES",
            "content": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. William Agnew, Julia Barnett, Annie Chu, Rachel Hong, Michael Feffer, Robin Netzorg, Harry H. Jiang, Ezra Awumey, and Sauvik Das. Sound Check: Auditing Audio Datasets, 2024. URL https://arxiv.org/abs/2410.13114. Andrea Agostinelli, Timo Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. MusicLM: Generating Music From Text. arXiv preprint arXiv:2301.11325, 2023a. Andrea Agostinelli, Timo I. Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. MusicLM: Generating Music From Text, 2023b. URL https://arxiv. org/abs/2301.11325. Tawsif Ahmed, Andrej Radonjic, and Gollam Rabby. SLEEPING-DISCO 9M: large-scale pretraining dataset for generative music modeling, 2025. URL https://arxiv.org/abs/ 2506.14293. Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The Million Song Dataset. In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011), 2011. Dmitry Bogdanov, Nicolas Wack, Emilia Gomez, Sankalp Gulati, Perfecto Herrera, Oscar Mayor, Gerard Roma, Justin Salamon, Jose Zapata, and Xavier Serra. ESSENTIA: an open-source library for sound and music analysis. In Proceedings of the 21st ACM International Conference on Multimedia, MM 13, pp. 855858, New York, NY, USA, 2013. Association for Computing Machinery. ISBN 9781450324045. doi: 10.1145/2502081.2502229. URL https://doi.org/ 10.1145/2502081.2502229. Sebastian Bock, Filip Korzeniowski, Jan Schluter, Florian Krebs, and Gerhard Widmer. madmom: new Python Audio and Music Signal Processing Library, 2016. URL https://arxiv.org/ abs/1605.07008. Wei Chai and Barry Vercoe. Detection of Key Change in Classical Piano Music. In ISMIR, pp. 468473, 2005. 10 Pre-print. Under Review. Chuer Chen, Shengqi Dang, Yuqi Liu, Nanxuan Zhao, Yang Shi, and Nan Cao. MV-Crafter: An Intelligent System for Music-guided Video Generation. ACM Transactions on Interactive Intelligent Systems, 15(3):127, 2025. Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, and Furu Wei. Beats: Audio pre-training with acoustic tokenizers, 2022. Anna-Maria Christodoulou, Olivier Lartillot, and Alexander Refsum Jensenius. Multimodal music datasets? Challenges and future goals in music processing. International Journal of Multimedia Information Retrieval, 13(3):37, 2024. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models, 2023. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-Audio Technical Report, 2024. Samuele Cornell, Matthew Wiesner, Shinji Watanabe, Desh Raj, Xuankai Chang, Paola Garcia, Matthew Maciejewski, Yoshiki Masuyama, Zhong-Qiu Wang, Stefano Squartini, and Sanjeev Khudanpur. The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios, 2023. URL https://arxiv.org/abs/2306.13734. DeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025. URL https://arxiv.org/abs/2501.12948. Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. FMA: dataset for music analysis. arXiv preprint arXiv:1612.01840, 2016. Zihao Deng, Yinghao Ma, Yudong Liu, Rongchen Guo, Ge Zhang, Wenhu Chen, Wenhao Huang, and Emmanouil Benetos. MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response, 2024. URL https://arxiv.org/abs/2309. 08730. Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An Audio Language Model for Audio Tasks, 2023. SeungHeon Doh, Keunwoo Choi, Jongpil Lee, and Juhan Nam. LP-MusicCaps: LLM-Based Pseudo Music Captioning. arXiv preprint arXiv:2307.16372, 2023. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. CLAP: Learning Audio Concepts from Natural Language Supervision, 2022. Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi, Douglas Eck, and Karen Simonyan. Neural audio synthesis of musical notes with wavenet autoencoders. In International conference on machine learning, pp. 10681077. PMLR, 2017. Peter Foster, Siddharth Sigtia, Sacha Krstulovic, Jon Barker, and Mark Plumbley. Chime-home: dataset for sound source recognition in domestic environment. In 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pp. 15. IEEE, 2015. Josh Gardner, Simon Durand, Daniel Stoller, and Rachel M. Bittner. LLark: Multimodal Instruction-Following Language Model for Music, 2024. URL https://arxiv.org/abs/ 2310.07160. Jonas Geiger, Marta Moscati, Shah Nawaz, and Markus Schedl. Music4All A+A: Multimodal Dataset for Music Information Retrieval Tasks, 2025. URL https://arxiv.org/abs/ 2509.14891. Gemini team. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities, 2025. URL https://arxiv.org/abs/ 2507.06261. Pre-print. Under Review. Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, Sakshi, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha. GAMA: Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities, 2024. Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, and Bryan Catanzaro. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities, 2025. John Godfrey, Edward Holliman, and Jane McDaniel. SWITCHBOARD: Telephone speech corpus for research and development. In Acoustics, speech, and signal processing, ieee international conference on, volume 1, pp. 517520. IEEE Computer Society, 1992. Arushi Goel, Karan Sapra, Matthieu Le, Rafael Valle, Andrew Tao, and Bryan Catanzaro. OMCAT: Omni Context Aware Transformer, 2024. URL https://arxiv.org/abs/2410.12109. Arushi Goel, Sreyan Ghosh, Jaehyeon Kim, Sonal Kumar, Zhifeng Kong, Sang gil Lee, ChaoHan Huck Yang, Ramani Duraiswami, Dinesh Manocha, Rafael Valle, and Bryan Catanzaro. Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models, 2025. URL https://arxiv.org/abs/2507.08128. Yuan Gong, Alexander H. Liu, Hongyin Luo, Leonid Karlinsky, and James Glass. Joint Audio and Speech Understanding, 2023a. Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James Glass. Listen, Think, and Understand, 2023b. Andrey Guzhov, Federico Raue, Jorn Hees, and Andreas Dengel. AudioCLIP: Extending CLIP to Image, Text and Audio, 2021. Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, Yuancheng Wang, Kai Chen, Pengyuan Zhang, and Zhizheng Wu. Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation, 2024. URL https://arxiv.org/abs/2407.05361. Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, et al. Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction. arXiv preprint arXiv:2502.11946, 2025. Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head, 2023. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv preprint arXiv:2410.21276, 2024. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In NAACL-HLT, 2019. KimiTeam, Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, Zhengtao Wang, Chu Wei, Yifei Xin, Xinran Xu, Jianwei Yu, Yutao Zhang, Xinyu Zhou, Y. Charles, Jun Chen, Yanru Chen, Yulun Du, Weiran He, Zhenxing Hu, Guokun Lai, Qingcheng Li, Yangyang Liu, Weidong Sun, Jianzhou Wang, Yuzhi Wang, Yuefeng Wu, Yuxin Wu, Dongchao Yang, Hao Yang, Ying Yang, Zhilin Yang, Aoxiong Yin, Ruibin Yuan, Yutong Zhang, and Zaida Zhou. Kimi-Audio Technical Report, 2025. URL https://arxiv.org/abs/2504.18425. Peter Knees, Angel Faraldo, Herrera, Richard Vogl, Sebastian Bock, Florian Horschlager, and Goff. Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections. International Symposium/Conference on Music Information Retrieval, pp. 364370, 2015. Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio Flamingo: Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities, 2024. 12 Pre-print. Under Review. Sonal Kumar, ˇSimon Sedlaˇcek, Vaibhavi Lokegaonkar, Fernando Lopez, Wenyi Yu, Nishit Anand, Hyeonggon Ryu, Lichang Chen, Maxim Pliˇcka, Miroslav Hlavaˇcek, William Fineas Ellingwood, Sathvik Udupa, Siyuan Hou, Allison Ferner, Sara Barahona, Cecilia Bolanos, Satish Rahi, Laura Herrera-Alarcon, Satvik Dixit, Siddhi Patil, Soham Deshmukh, Lasha Koroshinadze, Yao Liu, Leibny Paola Garcia Perera, Eleni Zanou, Themos Stafylakis, Joon Son Chung, David Harwath, Chao Zhang, Dinesh Manocha, Alicia Lozano-Diez, Santosh Kesiraju, Sreyan Ghosh, and Ramani Duraiswami. MMAU-Pro: Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence, 2025. URL https://arxiv.org/abs/2508.13992. Gang Li, Jizhong Liu, Heinrich Dinkel, Yadong Niu, Junbo Zhang, and Jian Luan. Reinforcement Learning Outperforms Supervised Fine-Tuning: Case Study on Audio Question Answering. arXiv preprint arXiv:2503.11197, 2025a. URL https://github.com/ xiaomi-research/r1-aqa;https://huggingface.co/mispeech/r1-aqa. Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu. Learning to answer questions in dynamic audio-visual scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1910819118, 2022. Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, et al. Baichuan-Audio: Unified Framework for End-to-End Speech Interaction. arXiv preprint arXiv:2502.17239, 2025b. Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, and Ying Shan. Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning. In ICASSP 20242024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 286290. IEEE, 2024a. Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, and Ying Shan. M2UGen: Multimodal Music Understanding and Generation with the Power of Large Language Models, 2024b. URL https://arxiv.org/abs/2311.11255. Vincent Lostanlen, Carmine-Emanuele Cella, Rachel Bittner, and Slim Essid. Medley-solos-DB: cross-collection dataset for musical instrument recognition , September 2019. URL https: //doi.org/10.5281/zenodo.3464194. Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe Chen, Zhuo Chen, Jian Cong, Kai Li, Keliang Li, Siyou Li, Xinfeng Li, Xiquan Li, Zheng Lian, Yuzhe Liang, Minghao Liu, Zhikang Niu, Tianrui Wang, Yuping Wang, Yuxuan Wang, Yihao Wu, Guanrou Yang, Jianwei Yu, Ruibin Yuan, Zhisheng Zheng, Ziya Zhou, Haina Zhu, Wei Xue, Emmanouil Benetos, Kai Yu, Eng-Siong Chng, and Xie Chen. MMAR: Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix, 2025. URL https://arxiv.org/abs/2505.13032. Matthias Mauch and Simon Dixon. Approximate Note Transcription for the Improved Identification of Difficult Chords. In ISMIR, pp. 135140, 2010. Jan Melechovsky, Zixun Guo, Deepanway Ghosal, Navonil Majumder, Dorien Herremans, and Soujanya Poria. Mustango: Toward Controllable Text-to-Music Generation. arXiv preprint arXiv:2311.08355, 2023. Annamaria Mesaros and Tuomas Virtanen. Automatic Recognition of Lyrics in Singing. EURASIP Journal on Audio, Speech, and Music Processing, 2010(1):546047, 2010. NVIDIA. parakeet-tdt-0.6b-v3: Multilingual Speech-to-Text Model. https://huggingface. co/nvidia/parakeet-tdt-0.6b-v3, 2025. Model released 14 August 2025, supports 25 European languages, 600 million parameters, CC BY 4.0 license. OpenAI. gpt-oss-120b & gpt-oss-20b Model Card, 2025. URL https://arxiv.org/abs/ 2508.10925. Zhihao Ouyang, Ju-Chiang Wang, Daiyu Zhang, Bin Chen, Shangjie Li, and Quan Lin. MQAD: Large-Scale Question Answering Dataset for Training Music Large Language Models. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2025. 13 Pre-print. Under Review. Prabhat Pandey, Rupak Vignesh Swaminathan, Vijay Girish, Arunasish Sen, Jian Xie, Grant P. Strimel, and Andreas Schwarz. SIFT-50M: Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning, 2025. URL https://arxiv.org/abs/2504.09081. Haolin Qin, Tingfa Xu, Tianhao Li, Zhenxiang Chen, Tao Feng, and Jianan Li. MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking, 2025. URL https://arxiv.org/abs/2503.17699. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust Speech Recognition via Large-Scale Weak Supervision, 2022. Zafar Rafii, Antoine Liutkus, Fabian-Robert Stoter, Stylianos Ioannis Mimilakis, and Rachel Bittner. The MUSDB18 corpus for music separation. 2017. Zafar Rafii, Antoine Liutkus, Fabian-Robert Stoter, Stylianos Ioannis Mimilakis, and Rachel Bittner. MUSDB18-HQ - an uncompressed version of MUSDB18, August 2019. URL https://doi. org/10.5281/zenodo.3338373. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. MMAU: Massive Multi-Task Audio Understanding and Reasoning Benchmark. arXiv preprint arXiv:2410.19168, 2024. Eric Scheirer. Tempo and beat analysis of acoustic musical signals. The Journal of the Acoustical Society of America, 103(1):588601, 1998. Alexander Sheh and Daniel PW Ellis. Chord segmentation and recognition using EM-trained hidden Markov models. 2003. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. SALMONN: Towards Generic Hearing Abilities for Large Language Models, 2024. URL https://arxiv.org/abs/2310.13289. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805, 2023. Tzanetakis and Cook. Musical genre classification of audio signals. IEEE Trans. Speech Audio Process., 10(5):293302, July 2002. Changhan Wang, Anne Wu, and Juan Pino. CoVoST 2 and Massively Multilingual Speech-to-Text Translation, 2020. URL https://arxiv.org/abs/2007.10310. Yu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu, Hanzhao Li, Heyang Xue, Yongmao Zhang, Lei Xie, and Mengxiao Bi. Opencpop: High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis, 2022. URL https://arxiv.org/abs/2201.07429. Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent, Ashish Arora, Xuankai Chang, Sanjeev Khudanpur, Vimal Manohar, Daniel Povey, Desh Raj, David Snyder, Aswin Shanmugam Subramanian, Jan Trmal, Bar Ben Yair, Christoph Boeddeker, Zhaoheng Ni, Yusuke Fujita, Shota Horiguchi, Naoyuki Kanda, Takuya Yoshioka, and Neville Ryant. CHiME-6 Challenge:Tackling Multispeaker Speech Recognition for Unsegmented Recordings, 2020. URL https://arxiv. org/abs/2004.09249. Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, and Dmitry Bogdanov. MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models. arXiv preprint arXiv:2408.01337, 2024. HoHsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2CLIP: Learning Robust Audio Representations from CLIP, 2021. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2.5-Omni Technical Report. arXiv preprint arXiv:2503.20215, 2025. 14 Pre-print. Under Review. Yizhi, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, Zili Wang, Yike Guo, and Jie Fu. MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. In The Twelfth International Conference on Learning Representations, October 2023. Fan Yu, Shiliang Zhang, Yihui Fu, Lei Xie, Siqi Zheng, Zhihao Du, Weilong Huang, Pengcheng Guo, Zhijie Yan, Bin Ma, Xin Xu, and Hui Bu. M2MeT: The ICASSP 2022 Multi-Channel Multi-Party Meeting Transcription Challenge, 2022. URL https://arxiv.org/abs/2110.07393. Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, and Jie Fu. MARBLE: Music Audio Representation Benchmark for Universal Evaluation. arXiv [cs.SD], June 2023. Yongyi Zang, Sean OBrien, Taylor Berg-Kirkpatrick, Julian McAuley, and Zachary Novack. Are you really listening? Boosting Perceptual Awareness in Music-QA Benchmarks. arXiv preprint arXiv:2504.00369, 2025."
        },
        {
            "title": "A ETHICS STATEMENT",
            "content": "This work studies audio, music, and singing-voice understanding across culturally diverse material. Our experiments rely on publicly available datasets and/or content licensed for research use. We do not release copyrighted audio, stems, or lyrics; any examples used for qualitative illustration are either (i) already distributed by the originating dataset under research-permissive license, or (ii) replaced by non-copyrightable descriptors (e.g., metadata, short transcriptions for analysis) when licenses are restrictive. No personally identifying information is collected, and no human-subjects experiments were conducted; institutional review board (IRB) approval was therefore not required. Cultural representation and bias. Music corpora are uneven across regions, languages, and genres. Such imbalance can yield biased estimates or degrade performance on underrepresented traditions. We mitigate this by (a) documenting dataset composition and selection criteria, (b) emphasizing vocal and multicultural material in evaluation, and (c) reporting known limitations. We encourage downstream users to avoid normative claims about quality across cultures and to treat our benchmarks as descriptive rather than prescriptive. Copyright and content ownership. Models trained on musical recordings risk reproducing protected content. We do not deploy or evaluate generative audio synthesis; our outputs are textual (QA, captions, reasoning traces). We avoid releasing any asset that could enable reconstruction of substantial portions of copyrighted works and provide guidance for filtering long verbatim lyric reproduction in evaluation outputs. Privacy and safety. Singing voices may implicitly encode sensitive traits. We use only public research datasets and focus on musical attributes (rhythm, harmony, timbre, structure) rather than identifying individuals. Potential misuse includes intrusive listener profiling or surveillance via audio analysis; to discourage these, we release only research artifacts (documentation, evaluation protocols, non-identifying metadata) and clearly scope permitted use in licenses when possible."
        },
        {
            "title": "B REPRODUCIBILITY STATEMENT",
            "content": "We provide all details needed to reproduce our results within the paper and appendix: dataset sources and splits; audio preprocessing (sampling rates, normalization, chunking/segment lengths); model architectures and parameter counts; training schedules (optimizers, learning-rate policies, batch sizes, gradient clipping), GRPO/post-training settings and reward definitions; inference settings (temperature, decoding constraints); and exact evaluation protocols and metrics for every benchmark. We report hardware used where applicable, and meanstd for repeated trials. We will release code, checkpoints and data upon acceptance. 15 Pre-print. Under Review."
        },
        {
            "title": "C MUSIC FLAMINGO TRAINING DATASETS",
            "content": "Table 2 summarizes all datasets used to train Music Flamingo, including total hours, number of audio-QA pairs, and the number of epochs (passes over the dataset) used at each training stage. Similar to (Ghosh et al., 2025; Goel et al., 2025), we convert all foundational datasets (captioning, classification, etc.) into QA formats, using the same set of prompts for each task mentioned in (Ghosh et al., 2025; Goel et al., 2025). Table 2: List of fine pre-training and fine-tuning datasets together with their training composition. Dataset Hours Num. Pairs AF3-St. 3 MF-SFT MF-Warmup MF-GRPO AF3-training mix (Goel et al., 2025) MF-Skills (Ours) MF-Think (Ours) MusicBench (Melechovsky et al., 2023) Mu-LLAMA (Liu et al., 2024a) MusicAVQAaudio-only (Li et al., 2022) MusicQA (Ouyang et al., 2025) LP-MusicCapsMSD (Doh et al., 2023) LP-MusicCapsMTT (Doh et al., 2023) LP-MusicCapsMC (Doh et al., 2023) MusicCaps (Agostinelli et al., 2023b) NSynth (Engel et al., 2017) MusDB-HQ (Rafii et al., 2017) FMA (Defferrard et al., 2016) Music4All Captions (ours) Music4All QA (ours) MSD Captions (ours) MSD QA (ours) CHIME (Foster et al., 2015) ALI Meeting (Yu et al., 2022) EMILIA (He et al., 2024) MUST (Qin et al., 2025) CoVoST (Wang et al., 2020) Multi-talker Switchboard (Godfrey et al., 1992) - - - 115.5 hrs 62.9 hrs 77.1 hrs 62.9 hrs 5805.7 hrs 126.4 hrs 7.4 hrs 7.4 hrs 321.3 hrs 29.1 hrs 860.7 hrs 910.5 hrs 1505.7 hrs 15449.9 hrs 20906.2 hrs 342 hrs 118.75 hrs 5000 hours 500 hrs 2880 hrs 109.9 hrs 30M 3M 176k 686k 70k 5.7K 70K 1331.8K 46.9K 7.9K 2.6K 289.2K 10.2K 104.2K 109k 180k 1.4M 935k 30k 387k 1.7M 245k 5M 76.6K 1.0 - - 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 - 2.0 - 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 - - - - - - - - 1.0 - - - - - - - - - - - - - - - - - - - - - - 1.0 1.0 - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "title": "D MUSIC FLAMINGO TRAINING DETAILS",
            "content": "In this section, we present the training settings of our model across all stages, each with specific configurations. Details are in Table 3. Settings AF3-SFT MF-SFT MF-WarmUp MF-GRPO global batch size learning rate learning schedule warm up ratio weight decay epoch bf16 grad accumulate FSDP full shard GPUs 128 1.5e-5 1 128 1.5e-5 128 1e-5 Cosine decay 0.03 0.0 1 1 8 128A100 Table 3: Training settings across stages. 64 1e-6"
        },
        {
            "title": "E USER STUDY ON MUSIC FLAMINGO",
            "content": "We undergo user study with trained music experts comparing Music Flamingo to an open-source LALMQwen3 Omni, and two closed-source LALMs(GPT-4o and Gemini 2.5 Pro) qualitatively. To achieve this, we selected subset of 8 songs: 4 songs in English and 4 songs in Brazilian Portuguese. From these songs, half of them are by extremely popular artists in Western music, and other half from 16 Pre-print. Under Review. Aspect Music Flamingo Qwen3-Omni GPT4o-Audio Gemini-2.5 Pro General technical characteristics (tempo, key, time signature) Genre classification (bpm) Consistently outputs tempo and key; sometimes time signatures (3 songs correctly 4/4). Mistakes usually relative in major/minor. Reasonably good, but misclassified Maniac (electro-funk vs. dance-pop). Struggled with Brazilian music genres (occasional mismatches). Rarely detailed; often omits tempo/key/time signature. Sometimes outputs tempo (bpm usually in ballpark). Less consistent with key. Superficial; often wrong with Brazilian genres. Some correct, but not consistent. Emotional content & lyrics Good understanding; emotional captures context. Lacks cultural/historical nuance. to Music Similar misses Flamingo; deeper cultural context. More superficial understanding than others. Complex technical characteristics (chord progressions, structure, production) but Attempts deeper detail, sometimes inaccurate with chords/voicings/structure. Sometimes nates elements. hallucigenre-related Least detailed; only superficial composition/arrangement notes. Gives more detail when recognizing famous songs; otherwise shallow. General observations Strong in consistent technical reporting, but accuracy varies on deeper features. Shallowest outputs. Relies on recognition of famous songs; may pull from text knowledge instead of audio. Sometimes outputs tempo (closest bpm for NO DONT). Some key mistakes. Time generally signatures omitted. Best at identifying genres overall, but hallucinated (ABBA as ska cover; Wild Things as dream pop with drums). Good understanding; one small lyric mistake. Also lacks cultural/historical nuance. More detailed for famous songs; hallucinations (e.g., nonexistent drums, overstated synths). Genre misclassifications cascade into wrong technical details. to GPT4oSimilar Audio; detailed when recognizing famous songs. Most prone to hallucinations tied to genre assumptions. Table 4: Comparison of Music Flamingo, Qwen3-Omni, GPT4o-Audio, and Gemini 2.5 Pro across different evaluation aspects. less known artists. The following songs were used: 1) ABBA - Money Money Money, 2) Michael Sembello - Maniac (From Flashdance), 3) Chandler Leighton - NO DON T, 4) Lø Spirit - Wild Things, 5) Antˆonio Carlos Jobim - Aguas De Marco, 6) Michel Telo - Ai Se Eu Te Pego, 7) Paulinho da Viola - Apoteose Ao Samba and 8) Ave Sangria - Seu Waldir. Table 4 shows summary of the detailed analysis comparing different musical aspects and features across models. Among the four models, Music Flamingo performs the best overall while some limitations in accurately identifying deeper context remain. 17 Pre-print. Under Review."
        },
        {
            "title": "F COMPARATIVE ANALYSIS ACROSS SONGS FROM DIFFERENT CULTURES",
            "content": "Furthermore, we compare the strengths of Music Flamingo on five commercially released songs spanning cultures, languages and styles: Niuver Enamorados (Spanish, Latin ballad) (Figure 5), Annika Wells Jim & Pam (English, indie/acoustic pop)(Figure 6), Louane La fille (French, piano-led pop)(Figure 7), Michel Telo Ai Se Eu Te Pego (Portuguese, Brazilian sertanejo)(Figure 8), and Zemlyane Trava doma (Russian, Soviet rock)(Figure 9). Below we present detailed summary of this comparison. General technical characteristics (tempo, time signature, key). MF consistently produced numeric tempos and keys that matched canonical analyses or widely observed half/double-time readings, and it explicitly handled relative-minor vs. metadata-major ambiguities (e.g., Enamorados: metadata in major while harmony centers on minor). GPT-4o and Gemini often described tempo qualitatively or gave numeric ranges but omitted keys; when numeric BPMs were provided, both models occasionally drifted toward club-tempo values that better reflect remixes than the canonical singles (e.g., Ai Se Eu Te Pego: 128140 BPM claimed vs. 96 BPM on the hit version). Qwen3 frequently omitted numerics altogether. Time signature was rarely stated by any model; where implied, 4/4 matched all five tracks. Illustrative cases. (i) Jim & Pam: MF reported the double-time 158 BPM and the correct key (D), aligning with felt pulse at 79 BPM; GPT-4o gave the correct qualitative tempo band but no key; Gemini mis-estimated to 120125 BPM and mis-keyed E. (ii) La fille: MF aligned with 128 BPM in C; Gemini and GPT-4o underestimated (90100 BPM) and/or mis-keyed. (iii) Trava doma: MF matched minor and 130 BPM; GPT-4o underestimated to 100110 BPM. Genre. All models could identify the broad style family. Gemini held narrow edge on matching canonical catalog labels and regional taxonomy (e.g., Ai Se Eu Te Pego: sertanejo universitario with dance-pop trappings). MF was directionally correct across the set and, in two cases, selected closely related tags when salient timbres or arrangement scale were misleading (Ai Se Eu Te Pego: forro inferred from accordion timbre; Trava doma: prog-leaning language for synth/spacecolored Soviet pop/rock record). Importantly, these adjacent picks did not derail MFs downstream harmony/structure reasoning and are straightforward to normalize to catalog labels. GPT-4o typically described the stylistic feel accurately (e.g., dance-pop with Brazilian flair; piano-led ballad) but often stopped short of naming the canonical label. Qwen3 alternated between sensible tags (French indie/pop ballad) and broad era styles (80s arena/soft rock), and twice misframed vocal songs as instrumental (see below), which contaminated the subsequent genre claim. Takeaway. Canonical label accuracy: Gemini MF GPT-4o > Qwen3. Geminis advantage is mostly in verbatim catalog taxonomy; MFs labels are correct at the family level and, when adjacent, remain musicologically consistent with its superior harmonic/structural analysis. Emotional content and lyrics. MF, Gemini, and GPT-4o gave coherent, text-grounded readings of mood and themes across all songs (e.g., Enamorados: memory, time, and fading love; La fille: identity and self-doubt; Trava doma: homesick cosmonaut narrative). Where lyrics were quoted or paraphrased, all three remained faithful to content and tone. Qwen3 produced reasonable affect reads when it acknowledged lyrics, but twice declared vocal track instrumental (Enamorados, Trava doma), leading to incorrect conclusions about narrative and emotion. Observation. When models inferred emotion strictly from sonics without anchoring in lyric text, nuance decreased and culture-specific references were missed (e.g., Trava doma as an iconic space-age anthem; Ai Se Eu Te Pego as global sertanejo earworm driven by chant-like hooks). Complex technical characteristics (chord progressions/voicings, song structure, production). MF generally provided the deepest harmonic/structural content (naming cadential behavior, relative-minor centers, verse/chorus dynamics), and its structural reads were consistently plausible across all five songs. Its main failure mode was over-specification: occasionally asserting colorful altered/extended chords or percussion layers not supported by public charts or by the stems one would expect (Jim & Pam: introduced drum-machine and synth-bass in an otherwise hand-clap/snaps, acoustic texture; La fille: added brushed kit and altered dominants to piano-centric, drum-light mix). GPT-4os arrangements and sectioning were reliably correct (intro/verse/chorus/bridge placement, 18 Pre-print. Under Review. dynamic swells), with conservative but accurate production notes; it rarely named specific harmonic content, which limited precision but avoided hallucination. Geminis arrangement commentary was serviceable and sometimes quite apt on famous tracks (accordion/synth hook in Ai Se Eu Te Pego), but often remained generic and light on concrete harmony. Qwen3s technical layer was the sparsest and suffered when the top-level premise was wrong (instrumental), cascading into inapplicable structure/production claims. Failure-mode coupling. We repeatedly observed that genre misclassification leads to production hallucinations. For instance, mapping Ai Se Eu Te Pego to forro primed mentions of forro-typical percussion, and reading La fille as an indie/pop ballad with soft electronic beat invited non-existent drum programming. Conversely, when models named the canonical genre, instrumentation and mix notes tended to be accurate (Gemini on Ai Se Eu Te Pego; MF and GPT-4o on the piano+voice core of La fille). Model-specific observations (holistic). Music Flamingo. Strongest on measurable facts (tempo/key) and the only model to consistently reconcile metadata major vs. relative-minor centers (Enamorados: vs. minor). Best overall at section-level reasoning and harmonic intent, but occasionally over-decorates with color chords or speculative percussion. Gemini 2.5 Pro. Best at canonical genre taxonomy and culturally grounded framing (e.g., sertanejo universitario). Tends to under-specify harmony and sometimes over-estimate tempo; instrumentation is usually correct when genre is correct. GPT-4o. Most dependable for arrangement, dynamics, and production prose; excellent at emotion/lyric grounding. Hesitant on numerics (tempo/key) and sometimes avoids naming the canonical genre even when its prose implies it. Qwen3-Omni. Capable of coherent mood and arrangement reads when anchored, but inconsistent. The two instrumental misclassifications (on clearly vocal songs) demonstrate brittle failure that propagates into wrong genre/production claims. Overall, the pattern is clear: precision on the measurable (tempo/key) + correct canonical genre naming is the foundation for faithful structure/production descriptions and culturally aware, lyricgrounded emotion reads. Among the models studied, Music Flamingo delivers the most reliable technical grounding and music-theoretic reasoning; Gemini adds the most accurate taxonomy; and GPT-4o supplies consistently correct structural/production narratives. Pre-print. Under Review. Figure 5: Caption generated by Music Flamingo on modern Spanish song. 20 Pre-print. Under Review. Figure 6: Caption generated by Music Flamingo on less known American song. Pre-print. Under Review. Figure 7: Caption generated by Music Flamingo on modern French song. 22 Pre-print. Under Review. Figure 8: Caption generated by Music Flamingo on modern Brazilian song. Pre-print. Under Review. Figure 9: Caption generated by Music Flamingo on well known Russian song."
        },
        {
            "title": "G LINEAR PROBING EXPERIMENTS WITH AUDIO ENCODERS",
            "content": "In order to better understand the role of the audio encoder in music tasks, we performed linear probing experiments with three audio encoders: the audio encoder from Qwen2Audio (Chu et al., 2024) and Audio Flamingo 3 (Goel et al., 2025) the encoder on which Music Flamingo is based, which are both based on the Whisper architecture, and the MERT encoder (Yizhi et al., 2023), which has an architecture with bias towards understanding music due to its use of constant-Q transform representation in the input. We chose two tasks from the MARBLE benchmark (Yuan et al., 2023): the key classification task using the GS dataset (Knees et al., 2015), and the genre classification task using the GTZAN dataset (Tzanetakis & Cook, 2002). All linear probing models were trained using the average of all the frames from the audio representation in question (Qwen2Whisper, AFWhisper, or MERT) with single linear layer. Models were trained until no improvement in validation set accuracy was observed after 5 epochs, and validation set accuracy was used as well to choose the best checkpoint. Results can be found in Table 5. 24 Pre-print. Under Review. Table 5: Performance comparison on key and genre classification with different audio encoders. Model GS (key cls.) GTZAN (genre cls.) Qwen2Audio AFWhisper MERT 34.10 40.56 56.12 89.99 91.37 78.96 We observe that both models based on Whisper, which were trained specifically for captioning, have high accuracy for genre classification but comparatively low accuracy for the key classification task. We hypothesize this is because when using captioning targets, the likelihood of having the genre for given song mentioned in the caption is much higher than having the key the song is in. MERT, on the other hand, has lower accuracy for the higher level task but higher accuracy for the lower level task. MERT is trained to generate representation that can be used to reconstruct the audio through VAE acoustic teacher model, and also to match the coefficients of CQT transform (the music model), therefore being more lower level representation than text-based captions. Given the correlation between this gap in accuracy between lower and higher level tasks and the general observations for the outputs of Music Flamingo, in the future we intend to try an audio encoder that preserves more of the lower level information in audio to enable future versions of the model to perform better in understanding those features from music signals. G.1 EXAMPLES OF MF-SKILLS AND MF-THINK MF-Skills contains long, multi-aspect targets (captions and QA) without reasoning traces. Figure 10, Figure 11, Figure 12, Figure 13, Figure 14 shows examples of captions in the MFSkills dataset. MF-Think augments challenging subset with compact chain-of-thought traces (<think>. . . </think>) and final answers (<answer>. . . </answer>). Figure 15, Figure 16, Figure 17, Figure 18, Figure 19 and Figure 20 shows examples of captions and QA pairs in the MF-Think dataset. 25 Pre-print. Under Review. Figure 10: Example of ground-truth caption on Latin song in MF-Skills. PROMPTS FOR MF-SKILLS AND MF-THINK We provide all prompting templates used across our datasets and QA types in Figures 21, 22, 23, 24, 25, 26. 26 Pre-print. Under Review. Figure 11: Example of ground-truth caption on classical instrumental piece in MF-Skills. 27 Pre-print. Under Review. Figure 12: Example of ground-truth caption on Latin song in MF-Skills. 28 Pre-print. Under Review. Figure 13: Example of ground-truth caption on an American song in MF-Skills. 29 Pre-print. Under Review. Figure 14: Example of ground-truth caption on French song in MF-Skills. 30 Pre-print. Under Review. Figure 15: Example of ground-truth caption with thinking traces on Korean song in MF-Think. 31 Pre-print. Under Review. Figure 16: Example of ground-truth caption with thinking traces on Haitian song in MF-Think. 32 Pre-print. Under Review. Figure 17: Example of ground-truth caption with thinking traces on European song in MF-Think. 33 Pre-print. Under Review. Figure 18: Example of QA pairs with thinking traces in MF-Think. Figure 19: Example of QA pairs with thinking traces in MF-Think. Figure 20: Example of QA pairs with thinking traces in MF-Think. 34 Pre-print. Under Review. Figure 21: Prompt for generating detailed captions for the MF-Skills dataset. Pre-print. Under Review. Figure 22: Prompt for generating QA pairs for the MF-Skills dataset. 36 Pre-print. Under Review. Figure 23: Prompt for correcting existing captions with lyrics and metadata on Music4ALL and MSD datasets. Figure 24: Prompt for option augmentation of existing question-answer pairs on the Music4ALL and MSD datasets. 37 Pre-print. Under Review. Figure 25: Prompt for generating step-by-step reasoning for the detailed captions in the MF-Think dataset. 38 Pre-print. Under Review. Figure 26: Prompt for generating step-by-step reasoning for the question-answer pairs in the MF-Think dataset."
        }
    ],
    "affiliations": [
        "NVIDIA, CA, USA",
        "University of Maryland, College Park, USA"
    ]
}